Inducing Information Extraction Systems for New Languages
via Cross-Language Projection
Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT 84112
riloff@cs.utah.edu
Charles Schafer and David Yarowsky
Department of Computer Science
Johns Hopkins University
Baltimore, MD 21218
{cschafer,yarowsky}@cs.jhu.edu
Abstract
Information extraction (IE) systems are costly to
build because they require development texts, pars-
ing tools, and specialized dictionaries for each ap-
plication domain and each natural language that
needs to be processed. We present a novel
method for rapidly creating IE systems for new lan-
guages by exploiting existing IE systems via cross-
language projection. Given an IE system for a
source language (e.g., English), we can transfer its
annotations to corresponding texts in a target lan-
guage (e.g., French) and learn information extrac-
tion rules for the new language automatically. In
this paper, we explore several ways of realizing both
the transfer and learning processes using off-the-
shelf machine translation systems, induced word
alignment, attribute projection, and transformation-
based learning. We present a variety of experiments
that show how an English IE system for a plane
crash domain can be leveraged to automatically cre-
ate a French IE system for the same domain.
1 Introduction
Information extraction (IE) is an important appli-
cation for natural language processing, and recent
research has made great strides toward making IE
systems easily portable across domains. However,
IE systems depend on parsing tools and specialized
dictionaries that are language specific, so they are
not easily portable across languages. In this re-
search, we explore the idea of using an information
extraction system designed for one language to au-
tomatically create a comparable information extrac-
tion system for a different language.
To achieve this goal, we rely on the idea of cross-
language projection. The basic approach is the fol-
lowing. First, we create an artificial parallel cor-
pus by applying an off-the-shelf machine translation
(MT) system to source language text (here, English)
to produce target language text (here, French). Or
conversely, in some experiments we generate a par-
allel corpus by applying MT to a French corpus
to produce artificial English. We then run a word
alignment algorithm over the parallel corpus. Next,
we apply an English IE system to the English texts
and project the IE annotations over to the corre-
sponding French words via the induced word align-
ments. In effect, this produces an automatically an-
notated French corpus. We explore several strate-
gies for transferring the English IE annotations to
the target language, including evaluation of the
French annotations produced by the direct projec-
tion alone, as well as the use of transformation-
based learning to create French extraction rules
from the French annotations.
2 Information Extraction
The goal of information extraction systems is to
identify and extract facts from natural language text.
IE systems are usually designed for a specific do-
main, and the types of facts to be extracted are de-
fined in advance. In this paper, we will focus on the
domain of plane crashes and will try to extract de-
scriptions of the vehicle involved in the crash, vic-
tims of the crash, and the location of the crash.
Most IE systems use some form of extraction
patterns to recognize and extract relevant informa-
tion. Many techniques have been developed to gen-
erate extraction patterns for a new domain automat-
ically, including PALKA (Kim & Moldovan, 1993),
AutoSlog (Riloff, 1993), CRYSTAL (Soderland et
al., 1995), RAPIER (Califf, 1998), SRV (Freitag,
1998), meta-bootstrapping (Riloff & Jones, 1999),
and ExDisco (Yangarber et al, 2000). For this
work, we will use AutoSlog-TS (Riloff, 1996b) to
generate IE patterns for the plane crash domain.
AutoSlog-TS is a derivative of AutoSlog that auto-
matically generates extraction patterns by gathering
statistics from a corpus of relevant texts (within the
domain) and irrelevant texts (outside the domain).
Each extraction pattern represents a linguistic ex-
pression that can extract noun phrases from one of
three syntactic positions: subject, direct object, or
object of a prepositional phrase. For example, the
following patterns could extract vehicles involved
in a plane crash: ?<subject> crashed?, ?hijacked
<direct-object>?, and ?wreckage of <np>?.
We trained AutoSlog-TS using AP news stories
about plane crashes as the relevant text, and AP
news stories that do not mention plane crashes as
the irrelevant texts. AutoSlog-TS generates a list
of extraction patterns, ranked according to their as-
sociation with the domain. A human must review
this list to decide which patterns are useful for the
IE task and which ones are not. We manually re-
viewed the top patterns and used the accepted pat-
terns for the experiments described in this paper. To
apply the extraction patterns to new text, we used a
shallow parser called Sundance that also performs
information extraction.
3 Cross-Language Projection
3.1 Motivation and Previous Projection Work
Not all languages have received equal investment
in linguistic resources and tool development. For
a select few, resource-rich languages such as En-
glish, annotated corpora and text analysis tools are
readily available. However, for the large majority
of the world?s languages, resources such as tree-
banks, part-of-speech taggers, and parsers do not
exist. And even for many of the better-supported
languages, cutting edge analysis tools in areas such
as information extraction are not readily available.
One solution to this NLP-resource disparity is
to transfer linguistic resources, tools, and do-
main knowledge from resource-rich languages to
resource-impoverished ones. In recent years, there
has been a burst of projects based on this paradigm.
Yarowsky et al (2001) developed cross-language
projection models for part-of-speech tags, base
noun phrases, named-entity tags, and morpholog-
ical analysis (lemmatization) for four languages.
Resnik et al (2001) developed related models for
projecting dependency parsers from English to Chi-
nese. There has also been extensive work on the
cross-language transfer and development of ontolo-
gies and WordNets (e.g., (Atserias et al, 1997)).
3.2 Mechanics of Projection
The cross-language projection methodology em-
ployed in this paper is based on Yarowsky et al
(2001), with one important exception. Given the
absence of available naturally occurring bilingual




































 
 
 
 
LOCATION
VICTIM
tuant ses  20 occupants
was crushed Thursday evening in the south?east of Haiti  ,
A two?motor aircraft Beechcraft of the Air?Saint?Martin company
Un avion bi?moteur Beechcraft de la compagnie Air?Saint?Martin
VEHICLE
killing its 20 occupants 
s? est ?cras? jeudi soir dans le sud?est   d?   Haiti    ,
.
.
Figure 1: French text word aligned with its English
machine translation (extractions highlighted)
corpora in our target domain, we employ commer-
cial, off-the-shelf machine translation to generate
an artificial parallel corpus. While machine transla-
tion errors present substantial problems, MT offers
great opportunities because it frees cross-language
projection research from the relatively few large
existing bilingual corpora (such as the Canadian
Hansards). MT allows projection to be performed
on any corpus, such as the domain-specific plane-
crash news stories employed here. Section 5 gives
the details of the MT system and corpora that we
used.
Once the artificial parallel corpus has been cre-
ated, we apply an English IE system to the English
texts and transfer the IE annotations to the target
language as follows:
1. Sentence align the parallel corpus.1
2. Word-align the parallel corpus using the
Giza++ system (Och and Ney, 2000).
3. Transfer English IE annotations and noun-
phrase boundaries to French via the mecha-
nism described in Yarowsky et al (2001),
yielding annotated sentence pairs as illustrated
in Figure 1.
4. Train a stand-alone IE tagger on these pro-
jected annotations (described in Section 4).
4 Transformation-Based Learning
We used transformation-based learning (TBL)
(Brill, 1995) to learn information extraction rules
for French. TBL is well-suited for this task because
it uses rule templates as the basis for learning, which
can be easily modeled after English extraction pat-
terns. However, information extraction systems typ-
ically rely on a shallow parser to identify syntactic
elements (e.g., subjects and direct objects) and verb
1This is trivial because each sentence has a numbered an-
chor preserved by the MT system.
constructions (e.g., passive vs. active voice). Our
hope was that the rules learned by TBL would be ap-
plicable to new French texts without the need for a
French parser. One of our challenges was to design
rule templates that could approximate the recogni-
tion of syntactic structures well enough to duplicate
most of the functionality of a French shallow parser.
When our TBL training begins, the initial state is
that no words are annotated. We experimented with
two sets of ?truth? values: Sundance?s annotations
and human annotations. We defined 56 language-
independent rule templates, which can be broken
down into four sets designed to produce different
types of behavior. Lexical N-gram rule templates
change the annotation of a word if the word(s) im-
mediately surrounding it exactly match the rule. We
defined rule templates for 1, 2, and 3-grams. In
Table 1, Rules 1-3 are examples of learned Lexi-
cal N-gram rules. Lexical+POS N-gram rule tem-
plates can match exact words or part-of-speech tags.
Rules 4-5 are Lexical+POS N-gram rules. Rule 5
will match verb phrases such as ?went down in?,
?shot down in?, and ?came down in?.
One of the most important functions of a parser is
to identify the subject of a sentence, which may be
several words away from the main verb phrase. This
is one of the trickest behaviors to duplicate without
the benefit of syntactic parsing. We designed Sub-
ject Capture rule templates to identify words that
are likely to be a syntactic subject. As an example,
Rule 6 looks for an article at the beginning of a sen-
tence and the word ?crashed? a few words ahead2,
and infers that the article belongs to a vehicle noun
phrase. (The NP Chaining rules described next will
extend the annotation to include the rest of the noun
phrase.) Rule 7 attempts relative pronoun disam-
biguation when it finds the three tokens ?COMMA
which crashed? and infers that the word preceding
the comma is a vehicle.
Without the benefit of a parser, another challenge
is identifying noun phrase boundaries. We designed
NP Chaining rule templates to look at words that
have already been labelled and extend the bound-
aries of the annotation to cover a complete noun
phrase. As examples, Rules 8 and 9 extend loca-
tion and victim annotations to the right, and Rule 10
extends a vehicle annotation to the left.
2? is a start-of-sentence token. w
4?7
means that the item
occurs in the range of word
4
through word
7
.
Rule Condition Rule Effect
1. w
1
=crashed w
2
=in w
3
is LOC.
2. w
1
=wreckage w
2
=of w
3
is VEH.
3. w
1
=injuring w
2
is VIC.
4. w
1
=NOUN w
2
=crashed w
1
is VEH.
5. w
1
=VERB w
2
=down w
3
=in w
4
is LOC.
6. w
1
=? w
2
=ART w
4?7
=crashed w
2
is VEH.
7. w
2
=COMMA w
3
=which w
4
=crashed w
1
is VEH.
8. w
1
=in w
2
=LOCATION w
3
=NOUN w
3
is LOC.
9. w
1
=VERB w
2
=VICTIM w
3
=NOUN w
3
is VIC.
10. w
1
=ART w
2
=VEHICLE w
1
is VEH.
Table 1: Examples of Learned TBL Rules
(LOC.=location, VEH.=vehicle, VIC.=victim)
5 Resources
The corpora used in these experiments were ex-
tracted from English and French AP news stories.
We created the corpora automatically by searching
for articles that contain plane crash keywords. The
news streams for the two languages came from dif-
ferent years, so the specific plane crash events de-
scribed in the two corpora are disjoint. The En-
glish corpus contains roughly 420,000 words, and
the French corpus contains about 150,000 words.
For each language, we hired 3 fluent university
students to do annotation. We instructed the anno-
tators to read each story and mark relevant entities
with SGML-style tags. Possible labels were loca-
tion of a plane crash, vehicle involved in a crash,
and victim (any persons killed, injured, or surviv-
ing a crash). We asked the annotators to align their
annotations with noun phrase boundaries. The an-
notators marked up 1/3 of the English corpus and
about 1/2 of the French corpus.
We used a high-quality commercial machine
translation (MT) program (Systran Professional
Edition) to generate a translated parallel corpus for
each of our English and French corpora. These will
henceforth be referred to as MT-French (the Systran
translation of the English text) and MT-English (the
Systran translation of our French text).
6 Experiments and Evaluation
6.1 Scoring and Annotator Agreement
We explored two ways of measuring annotator
agreement and system performance. (1) The
exact-word-match measure considers annotations to
match if their start and end positions are exactly the
same. (2) The exact-NP-match measure is more for-
giving and considers annotations to match if they
both include the head noun of the same noun phrase.
The exact-word-match criterion is very conservative
because annotators may disagree about equally ac-
ceptable alternatives (e.g., ?Boeing 727? vs. ?new
Boeing 727?). Using the exact-NP-match measure,
?Boeing 727? and ?new Boeing 727? would con-
stitute a match. We used different tools to identify
noun phrases in English and French. For English,
we applied the base noun phrase chunker supplied
with the fnTBL toolkit (Ngai & Florian, 2001). In
French, we ran a part-of-speech tagger (Cucerzan
& Yarowsky, 2000) and applied regular-expression
heuristics to detect the heads of noun phrases.
We measured agreement rates among our human
annotators to assess the difficulty of the IE task. We
computed pairwise agreement scores among our 3
English annotators and among our 3 French anno-
tators. The exact-word-match scores ranged from
16-31% for French and 24-27% for English. These
relatively low numbers suggest that the exact-word-
match criterion is too strict. The exact-NP-match
agreement scores were much higher, ranging from
43-54% for French and 51-59% for English3.
These agreement numbers are still relatively low,
however, which partly reflects the fact that IE is a
subjective and difficult task. Inspection of the data
revealed some systematic differences of approach
among annotators. For example, one of the French
annotators marked 4.5 times as many locations as
another. On the English side, the largest disparity
was a factor of 1.4 in the tagging of victims.
6.2 Monolingual English & French Evaluation
As a key baseline for our cross-language projec-
tion studies, we first evaluated the AutoSlog-TS
and TBL training approaches on monolingual En-
glish and French data. Figure 2 shows (1) English
training by running AutoSlog-TS on unannotated
texts and then applying its patterns to the human-
annotated English test data, (2) English training and
testing by applying TBL to the human-annotated
English data with 5-fold cross-validation, (3) En-
glish training by applying TBL to annotations pro-
duced by Sundance (using AutoSlog-TS patterns)
and then testing the TBL rules on the human-
annotated English data, and (4) French training and
testing by applying TBL to human annotated French
data with 5-fold cross-validation.
Table 2 shows the performance in terms of Pre-
cision (P), Recall (R) and F-measure (F). Through-
3Agreement rates were computed on a subset of the data
annotated by multiple people; systems were scored against the
full corpus, of which each annotator provided the standard for
one third.
out our experiments, AutoSlog-TS training achieves
higher precision but lower recall than TBL training.
This may be due to the exhaustive coverage pro-
vided by the human annotations used by TBL, com-
pared to the more labor-efficient but less-complete
AutoSlog-TS training that used only unannotated
data.
English TEST
140K words
(English)
  SUNDANCE
English (plain)
English (plain)
(English)
  SUNDANCE
4/5Eng TEST Eng TEST1/5
French TEST1/5
French TEST4/5
  TBLES
S1
ES1TS1Train TBL
(1)
(3)
Test TBL
(4) (French)  TBL TF0
Train TBL
Test TBL
English TESTS0
140K words
280K words
280K words
Autoslo
g?TS
Autos
log?T
S
[+ 280K words irrel. text]
(2) (English)  TBL T0
112K words 28K words
Train TBL
Test TBL
[+ 280K words irrel. text]
[cross validation]
[cross validation]
16K words
64K words
Figure 2: Monolingual IE Evaluation pathways4
Monolingual Training Route P R F
English
(1) Train AutoSlog-TS on English-plain (ASE)
S0: Apply ASE to English Test .44 .42 .43
(2) Train TBL on 4/5 of English-Test (TBLE)
T0: Apply TBLE to 1/5 of English Test .35 .62 .45
(perform in 5-fold cross-validation)
(3) Train AutoSlog-TS on English-plain (ASE)
S1: Apply ASE to English-plain .31 .40 .35
TS1 Train TBL on Sundance annotations
ES1: Apply TBLES to English Test
French
(4) Train TBL on 4/5 of French-Test (TBLF)
TF0: Apply TBLF to 1/5 of French Test .47 .66 .54
(perform in 5-fold cross-validation)
Table 2: Monolingual IE Baseline Performance
6.3 TBL-based IE Projection and Induction
As noted in Section 5, both the English and French
corpora were divided into unannotated (?plain?)
and annotated (?antd? or ?Tst?) sections. Figure
3 illustrates these native-language data subsets in
white. Each native-language data subset alo has
a machine-translated mirror in French/English re-
spectively (shown in black), with an identical num-
ber of sentences to the original. By word-aligning
these 4 native/MT pairs, each becomes a potential
vehicle for cross-language information projection.
Consider the pathway TE1?P1 ? TF1 as a rep-
resentative example pathway for projection. Here
an English TBL classifier is trained on the 140K-
word human annotated data and the learned TBL
rules are applied to the unannotated English sub-
corpus. The annotations are then projected across
the Giza++ word alignments to their MT-French
mirror. Next, a French TBL classifier (TBL1) is
trained on the projected MT-French annotations and
the learned French TBL rules are subsequently ap-
plied to the native-French test data.
An alternative path (TE4 ? P4 ? French-Test)
is more direct, in that the English TBL classifier
is applied immediately to the word-aligned MT-
English translation of the French test data. The MT-
English annotations can then be directly projected
to the French test data, so no additional training
is necessary. Another short direct projection path
(PHA2 ? THA2 ? French-test) skips the need to
train an English TBL model by projecting the En-
glish human annotations directly onto MT-French
texts, which can then be used to train a French TBL
system which can be applied to the French test data.
HUMAN
Annotators
  TBL (English)
French TBL
Training and
Transfer to
Test Data
English
Annotation
P1
English (plain)
P    2HA
TBL1
T    2HA
TBL2h
English (antd)
TB
L 
Tr
ai
ni
ng
T  1
ET  1
F
Cross?
Language
Projection
P3
TBL3
French (plain)
T  3F
T   3E
P4
MT?English Tst
T   4E
(plain)
(plain)
MT?English
MT?French
MT?French
(annotated) French Test
Figure 3: TBL-based IE projection pathways
Table 3 shows the results of our TBL-based ex-
periments. The top performing pathway is the
TE4 ? P4 two-step projection pathway shown in
Figure 3. Note the F-measure of the best pathway
is .45, which is equal to the highest F-measure for
monolingual English and only 9% lower than the F-
measure for monolingual French.
4The irrelevant texts are needed to train AutoSlog-TS, but
not TBL.
Projection and Training Route P R F
TE1: Apply TBLE to English-plain
P1: Project to MT-French(English-Plain) .69 .24 .36
TF1: Train TBL & Apply to FrTest
? Use human Annos from Eng Antd
Pha2: Project to MT-French(English Antd) .56 .29 .39
Tha2: Train TBL & Apply to FrTest
TE3: Apply TBLE to MT-Eng(FrenchPlain)
P3: Project to French-Plain .49 .34 .40
TF3: Train TBL & Apply to FrTest
TE4: Apply TBLE to MT-Eng(FrenchTest)
P4: Direct Project to French-Test .49 .41 .45
Table 3: TBL-based IE projection performance
6.4 Sundance-based IE Projection and
Induction
Figure 4 shows the projection and induction model
using Sundance for English IE annotation, which is
almost isomorphic to that using TBL. One notable
difference is that Sundance was trained by apply-
ing AutoSlog-TS to the unannotated English text
rather than the human-annotated data. Figure 4 also
shows an additional set of experiments (SMT 3 and
SMT 4) in which AutoSlog-TS was trained on the
English MT translations of the unannotated French
data. The motivation was that native-English extrac-
tion patterns tend to achieve low recall when applied
to MT-English text (given frequent mistranslations
such as ?to crush? a plane rather than ?to crash? a
plane). By training AutoSlog-TS on the sentences
generated by an MT system (seen in the SMT 3 and
SMT 4 pathways), the F-measure increases.5
French TBL
Training and
Transfer to
Test Data
English
Annotation
P2P1
T1
T2
S1
S2
  SUNDANCE
(English)
English (plain)
TBL1
TBL2
English (antd)
Projection
Language
Cross?
P4
(MT?English)
SUNDANCE
MT?English Tst
P     3
S     4
P     4
MT
MT MT
French (plain)
S4
S     3MT
S3
P3
TBL3
T3 TBL3m
T     3MT
Au
tos
log
?T
S
Au
to
slo
g 
 ?T
S
(plain)
MT?French
MT?English
(plain)
MT?French
(annotated) French Test
Figure 4: Sundance-based projection pathways
5This is a ?fair? gain, in that the MT-trained AutoSlog-TS
patterns didn?t use translations of any of the French test data.
Projection and Training Route P R F
AutoSlog-TS trained on native English (AS E)
S2: Apply ASE to English-Antd
P2: Project to MT-French(English-Antd) .39 .24 .29
T2: Train TBLFP2 & Apply to FrTest
S(1+2): Apply ASE to English Antd+Plain
P (1+2): Project to MT-French(Eng-Ant+Pl) .43 .23 .30
T (1+2): Train TBLFP1+2 & Apply to FrTest
S3: Apply ASE to MT-Eng(FrenchPlain)
P3: Project to French-Plain .45 .04 .07
T3: Train TBLFP3 & Apply to FrTest
S4: Apply ASE to MT-Eng(FrenchTest)
P4: Direct Project to French-Test .48 .07 .13
AutoSlog-TS trained on MT English (AS MTE)
SMT 3: Apply ASMTE to MT-Eng(FrPlain)
PMT 3: Project to French-Plain .46 .25 .32
TMT 3: Train TBLFMT3 & Apply to FrTest
SMT 4: Apply ASMTE to MT-Eng(FrTest)
PMT 4: Direct Project to French-Test .55 .28 .37
Table 4: Sundance-based IE projection performance 6
Table 4 shows that the best Sundance pathway
achieved an F-measure of .37. Overall, Sundance
averaged 7% lower F-measures than TBL on com-
parable projection pathways. However, AutoSlog-
TS training required only 3-4 person hours to review
the learned extraction patterns while TBL training
required about 150 person-hours of manual IE an-
notations, so this may be a viable cost-reward trade-
off. However, the investment in manual English IE
annotations can be reused for projection to new for-
eign languages, so the larger time investment is a
fixed cost per-domain rather than per-language.
6.5 Analysis and Implications
? For both TBL and Sundance, the P1, P2 and
P3-family of projection paths all yield stand-alone
monolingual French IE taggers not specialized for
any particular test set. In contrast, the P4 series of
pathways (e.g. PMT 4 for Sundance), were trained
specifically on the MT output of the target test data.
Running an MT system on test data can be done au-
tomatically and requires no additional human lan-
guage knowledge, but it requires additional time
(which can be substantial for MT). Thus, the higher
performance of the P4 pathways has some cost.
? The significant performance gains shown by
Sundance when AutoSlog-TS is trained on MT-
English rather than native-English are not free be-
cause the MT data must be generated for each new
language and/or MT system to optimally tune to
6S(1+2) combines the training data in S1 (280K) and S2
(140K), yielding a 420K-word sample.
its peculiar language variants. No target-language
knowledge is needed in this process, however, and
reviewing AutoSlog-TS? patterns can be done suc-
cessfully by imaginative English-only speakers.
? In general, recall and F-measure drop as the
number of experimental steps increases. Averaged
over TBL and Sundance pathways, when compar-
ing 2 and 3-step projections, mean recall decreases
from 26.8 to 21.8 (5 points), and mean F-measure
drops from 32.6 to 28.8 (3.8 points). Viable extrac-
tion patterns may simply be lost or corrupted via too
many projection and retraining phases.
? One advantage of the projection path families
of P1 and P2 is that no domain-specific documents
in the foreign language are required (as they are in
the P3 family). A collection of domain-specific En-
glish texts can be used to project and induce new IE
systems even when no domain-specific documents
exist in the foreign language.
6.6 Multipath Projection
Finally, we explored the use of classifier combina-
tion to produce a premium system. We considered a
simple voting scheme over sets of individual IE sys-
tems. Every annotation of a head noun was consid-
ered a vote. We tried 4 voting combinations: (1) the
systems that used Sundance with English extraction
patterns, (2) the systems that used Sundance with
MT-English extraction patterns, (3) the systems that
used TBL trained on English human annotations,
(4) all systems. For each combination of n sys-
tems, n answer sets were produced using the voting
thresholds Tv = 1..n. For example, for Tv = 2 ev-
ery annotation receiving >= 2 votes (picked by at
least 2 individual systems) was output in the answer
set. This allowed us to explore a precision/recall
tradeoff based on varying levels of consensus.
Figure 5 shows the precision/recall curves. Vot-
ing yields some improvement in F-measure and pro-
vides a way to tune the system for higher preci-
sion or higher recall by choosing the Tv threshold.
When using all English knowledge sources, the F-
measure at Tv=1 (.48) is nearly 3% higher than the
strongest individual system. Figure 5 also shows
the performance of a 5th system (5), which is a
TBL system trained directly from the French anno-
tations under 5-fold cross-validation. It is remark-
able that the most effective voting-based projection
system from English to French comes within 6% F-
measure of the monolingually trained system, given
that this cross-validated French monolingual system
was trained directly on data in the same language
and source as the test data. This suggests that cross-
language projection of IE analysis capabilities can
successfully approach the performance of dedicated
systems in the target language.
Precision
Recall
(5)
(2)
(1) (3) (4)
(5) TBL Trained from French Annotations 
(4) English TBL + Sundance pathways
(3) English TBL pathways
(2) Sundance?MT pathways
(1) Sundance pathways
[under 5?fold cross?validation]
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
Figure 5: Precision/Recall curves for voting systems. Each
point represents performance for a particular voting threshold.
In all cases, precision increases and recall decreases as the
threshold is raised.
French Test-Set Performance P R F
Multipath projection from all English resources .43 .54 .48
Table 5: Best multipath English-French Projection Per-
formance (from English TBL and Sundance pathways)
7 Conclusions
We have used IE systems for English to automati-
cally derive IE systems for a second language. Even
with the quality of MT available today, our results
demonstrate that we can exploit translation tools to
transfer information extraction expertise from one
language to another. Given an IE system for a
source language, an MT system that can translate
between the source and target languages, and a word
alignment algorithm, our approach allows a user to
create a functionally comparable IE system for the
target language with very little human effort. Our
experiments demonstrated that the new IE system
can achieve roughly the same level of performance
as the source-language IE system. French and En-
glish are relatively close languages, however, so
how well these techniques will work for more dis-
tant language pairs is still an open question.
Additional performance benefits could be
achieved in two ways: (1) put more effort into
obtaining better resources for English, or (2)
implement (minor) specializations per language.
While it is expensive to advance the state of the art
in English IE or to buy annotated data for a new
domain, these additions will improve performance
not only in English but for other languages as
well. On the other hand, with minimal effort
(hours) it is possible to custom-train a system
such as Autoslog/Sundance to work relatively
well on noisy MT-English, providing a substantial
performance boost for the IE system learned for the
target language, and further gains are achieved via
voting-based classifier combination.
References
J. Atserias, S. Climent, X. Farreres, G. Rigau and H. Rodriguez.
1997. Combining multiple methods for the automatic con-
struction of multilingual WordNets. In Proceedings of the
International Conference on Recent Advances in Natural
Language Processing.
E. Brill. 1995. Transformation-based error-driven learning and
natural language processing: A case study in part of speech
tagging. Computational Linguistics, 21(4):543?565.
M. E. Califf. 1998. Relational learning techniques for natural
language information extraction. Ph.D. thesis, Tech. Rept.
AI98-276, Artificial Intelligence Laboratory, The University
of Texas at Austin.
S. Cucerzan and D. Yarowsky. 2000. Language independent
minimally supervised induction of lexical probabilities. In
Proceedings of ACL-2000, pages 270-277.
D. Freitag. 1998. Toward general-purpose learning for in-
formation extraction. In Proceedings of COLING-ACL?98,
pages 404-408.
J. Kim and D. Moldovan. 1993. Acquisition of semantic pat-
terns for information extraction from corpora. In Proceed-
ings of the Ninth IEEE Conference on Artificial Intelligence
for Applications, pages 171?176.
G. Ngai and R. Florian. 2001. Transformation-based learning
in the fast lane. In Proceedings of NAACL, pages 40-47.
F. J. Och and H. Ney. 2000. Improved statistical alignment
models. In Proceedings of the 38th Annual Meeting of the
Association for Computational Linguistics, pages 440?447.
E. Riloff. 1993. Automatically Constructing a dictionary for
information extraction tasks. In Proceedings of the Eleventh
National Conference on Artificial Intelligence, pages 811?
816.
E. Riloff. 1996b. Automatically generating extraction patterns
from untagged text. In Proceedings of the Thirteenth Na-
tional Conference on Artificial Intelligence, pages 1044?
1049. AAAI Press/MIT Press.
E. Riloff and R. Jones. 1999. Learning dictionaries for infor-
mation extraction by multi-level bootstrapping. In Proceed-
ings of the Sixteenth National Conference on Artificial Intel-
ligence, pages 474?479.
S. Soderland, D. Fisher, J. Aseltine, and W. Lehnert. 1995.
CRYSTAL: Inducing a conceptual dictionary. In Proceed-
ings of the Fourteenth International Joint Conference on Ar-
tificial Intelligence, pages 1314?1319.
R. Yangarber, R. Grishman, P. Tapanainen, and S. Huttunen.
2000. Automatic acquisiton of domain knowledge for infor-
mation extraction. In Proceedings of COLING-2000, pages
940-946.
Yarowsky, D., G. Ngai and R. Wicentowski. 2001. Inducing
multilingual text analysis tools via robust projection across
aligned corpora. In Proceedings of HLT-01, pages 161?168.
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 717?727, Prague, June 2007. c?2007 Association for Computational Linguistics
Effective Information Extraction with Semantic Affinity Patterns and
Relevant Regions
Siddharth Patwardhan and Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT 84112
{sidd,riloff}@cs.utah.edu
Abstract
We present an information extraction system
that decouples the tasks of finding relevant
regions of text and applying extraction pat-
terns. We create a self-trained relevant sen-
tence classifier to identify relevant regions,
and use a semantic affinity measure to au-
tomatically learn domain-relevant extraction
patterns. We then distinguish primary pat-
terns from secondary patterns and apply the
patterns selectively in the relevant regions.
The resulting IE system achieves good per-
formance on the MUC-4 terrorism corpus
and ProMed disease outbreak stories. This
approach requires only a few seed extraction
patterns and a collection of relevant and ir-
relevant documents for training.
1 Introduction
Many information extraction (IE) systems rely on
rules or patterns to extract words and phrases based
on their surrounding context (e.g., (Soderland et al,
1995; Riloff, 1996; Califf and Mooney, 1999; Yan-
garber et al, 2000)). For example, a pattern like
?<subject> was assassinated? can reliably identify
a victim of a murder event. Classification-based IE
systems (e.g., (Freitag, 1998; Freitag and McCal-
lum, 2000; Chieu et al, 2003)) also generally de-
cide whether to extract words based on properties of
the words themselves as well as properties associ-
ated with their surrounding context.
In this research, we propose an alternative ap-
proach to IE that decouples the tasks of finding a rel-
evant region of text and finding a desired extraction.
In a typical pattern-based IE system, the extraction
patterns perform two tasks: (a) they recognize that
a relevant incident has occurred, and (b) they iden-
tify and extract some information about that event.
In contrast, our approach first identifies relevant re-
gions of a document that describes relevant events,
and then applies extraction patterns only in these rel-
evant regions.
This decoupled approach to IE has several po-
tential advantages. First, even seemingly good pat-
terns can produce false hits due to metaphor and id-
iomatic expressions. However, by restricting their
use to relevant regions of text, we could avoid such
false positives. For example, ?John Kerry attacked
George Bush? is a metaphorical description of a ver-
bal tirade, but could be easily mistaken for a physi-
cal attack. Second, IE systems are prone to errors of
omission when relevant information is not explicitly
linked to an event. For instance, a phrase like ?the
gun was found...? does not directly state that the the
gun was used in a terrorist attack. But if the gun is
mentioned in a region that clearly describes a terror-
ist attack, then it can be reasonably inferred to have
been used in the attack. Third, if the IE patterns are
restricted to areas of text that are known to be rel-
evant, then it may suffice to use relatively general
patterns, which may be easier to learn or acquire.
Our approach begins with a relevant sentence
classifier that is trained using only a few seed pat-
terns and a set of relevant and irrelevant documents
(but no sentence-level annotations) for the domain of
interest. The classifier is then responsible for identi-
fying sentences that are relevant to the IE task. Next,
we learn ?semantically appropriate? extraction pat-
717
terns by evaluating candidate patterns using a se-
mantic affinity metric. We then separate the pat-
terns into primary and secondary patterns, and ap-
ply them selectively to sentences based on the rel-
evance judgments produced by the classifier. We
evaluate our IE system on two data sets: the MUC-
4 IE terrorism corpus and ProMed disease outbreak
articles. Our results show that this approach works
well, often outperforming the AutoSlog-TS IE sys-
tem which benefits from human review.
2 Motivation and Related Work
Our research focuses on event-oriented information
extraction (IE), where the goal of the IE system
is to extract facts associated with domain-specific
events from unstructured text. Many different ap-
proaches to information extraction have been devel-
oped, but generally speaking they fall into two cate-
gories: classifier-based approaches and rule/pattern-
based approaches.
Classifier-based IE systems use machine learning
techniques to train a classifier that sequentially pro-
cesses a document looking for words to be extracted.
Examples of classifier-based IE systems are SRV
(Freitag, 1998), HMM approaches (Freitag and Mc-
Callum, 2000), ALICE (Chieu et al, 2003), and Re-
lational Markov Networks (Bunescu and Mooney,
2004). The classifier typically decides whether a
word should be extracted by considering features as-
sociated with that word as well as features of the
words around it.
Another common approach to information ex-
traction uses a set of explicit patterns or rules
to find relevant information. Some older sys-
tems relied on hand-crafted patterns, while more
recent systems learn them automatically or semi-
automatically. Examples of rule/pattern-based ap-
proaches to information extraction are FASTUS
(Hobbs et al, 1997), PALKA (Kim and Moldovan,
1993), LIEP (Huffman, 1996), CRYSTAL (Soder-
land et al, 1995), AutoSlog/AutoSlog-TS (Riloff,
1993; Riloff, 1996), RAPIER (Califf and Mooney,
1999), WHISK (Soderland, 1999), ExDisco (Yan-
garber et al, 2000), SNOWBALL (Agichtein and
Gravano, 2000), (LP)2 (Ciravegna, 2001), subtree
patterns (Sudo et al, 2003), predicate-argument
rules (Yakushiji et al, 2006) and KnowItAll
(Popescu et al, 2004).
One commonality behind all of these approaches
is that they simultaneously decide whether a context
is relevant and whether a word or phrase is a desir-
able extraction. Classifier-based systems rely on fea-
tures that consider both the word and its surround-
ing context, and rule/pattern-based systems typi-
cally use patterns or rules that match both the words
around a candidate extraction and (sometimes) prop-
erties of the candidate extraction itself.
There is a simplicity and elegance to having a sin-
gle model that handles both of these problems at the
same time, but we hypothesized that there may be
benefits to decoupling these tasks. We investigate an
alternative approach that involves two passes over a
document. In the first pass, we apply a relevant re-
gion identifier to identify regions of the text that ap-
pear to be especially relevant to the domain of inter-
est. In the second pass, we apply extraction patterns
inside the relevant regions. We hypothesize three
possible benefits of this decoupled approach.
First, if a system is certain that a region is rele-
vant, then it can be more aggressive about searching
for extractions. For example, consider the domain
of terrorist event reports, where a goal is to identify
the weapons that were used. Existing systems gen-
erally require rules/patterns to recognize a context
in which a weapon is explicitly linked to an event
or its consequences (e.g., ?attack with <np>?, or
?<np> caused damage?). However, weapons are
not always directly linked to an event in text, but
they may be inferred through context. For instance,
an article may mention that a weapon was ?found?
or ?used? without explicitly stating that it was in-
volved in a terrorist event. However, if we know in
advance that we are in a relevant context, then we
can reliably infer that the weapon was, most likely,
used in the event.
Second, some patterns may seem to be relevant
locally, but they can be deemed irrelevant when the
global context is considered. For example, consider
these sentences from the MUC-4 terrorism corpus:
D?Aubuisson unleashed harsh attacks on
Duarte ...
Other brave minds that advocated reform
had been killed before in that struggle.
Locally, patterns such as ?<subject> unleashed
718
attacks? and ?<subject> had been killed? seem
likely to identify the perpetrators and victims of a
physical attack. But when read in the full context
of these sentences, it becomes clear that they are not
related to a specific physical attack.
Third, decoupling these tasks may simplify the
learning process. Identifying relevant regions
amounts to a text classification task, albeit the goal is
to identify not just relevant documents, but relevant
sub-regions of documents. Within a relevant region
the patterns may not need to be as discriminating.
So a more general learning approach may suffice.
In this paper, we describe an IE system that con-
sists of two decoupled modules for relevant sentence
identification and extraction pattern learning. In
Section 3, we describe the self-trained sentence clas-
sifier, which requires only a few seed patterns and
relevant and irrelevant documents for training. Sec-
tion 4 describes the extraction pattern learning mod-
ule, which identifies semantically appropriate pat-
terns for the IE system using a semantic affinity mea-
sure. Section 5 explains how we distinguish Primary
patterns from Secondary patterns. Section 6 presents
experimental results on two domains. Finally, Sec-
tion 7 lists our conclusions and future work.
3 A Self-Trained Relevant Sentence
Classifier
Our hypothesis is that if a system can reliably iden-
tify relevant regions of text, then extracting informa-
tion only from these relevant regions can improve IE
performance. There are many possible definitions
for relevant region (e.g., Salton et al (1993), Callan
(1994)), and exploring the range of possibilities is
an interesting avenue for future work. For our ini-
tial investigations of this idea, we begin by simply
defining a sentence as our region size. This has the
advantage of being an easy boundary line to draw
(i.e., it is relatively easy to identify sentence bound-
aries) and it is a small region size yet includes more
context than most current IE systems do1.
Our goal is to create a classifier that can determine
whether a sentence contains information that should
be extracted. Furthermore, we wanted to create a
classifier that does not depend on manually anno-
1Most IE systems only consider a context window consisting
of a few words or phrases on either side of a potential extraction.
tated sentence data so that our system can be eas-
ily ported across domains. Therefore, we devised a
method to self-train a classifier using a training set
of relevant and irrelevant documents for the domain,
and a few seed patterns as input. However, this re-
sults in an asymmetry in the training set. By defini-
tion, if a document is irrelevant to the IE task, then
it cannot contain any relevant information. Con-
sequently, all sentences in an irrelevant document
must be irrelevant, so these sentences form our ini-
tial irrelevant sentences pool. In contrast, if a doc-
ument is relevant to the IE task, then there must be
at least one sentence that contains relevant informa-
tion. However, most documents contain a mix of
both relevant and irrelevant sentences. Therefore,
the sentences from the relevant documents form our
unlabeled sentences pool.
Figure 1 shows the self-training procedure, which
begins with a handful of seed patterns to initiate the
learning process. The seed patterns should be able
to reliably identify some information that is relevant
to the IE task. For instance, to build an IE system for
terrorist incident reports, we used seed patterns such
as ?<subject> was kidnapped? and ?assassination
of <np>?. The patterns serve as a simple pattern-
based classifier to automatically identify some rel-
evant sentences. In iteration 0 of the self-training
loop (shown as dotted lines in Figure 1), the pattern-
based classifier is applied to the unlabeled sentences
to automatically label some of them as relevant.
Next, an SVM (Vapnik, 1995) classifier2 is
trained using these relevant sentences and an equal
number of irrelevant sentences randomly drawn
from the irrelevant sentences pool. We artificially
created a balanced training set because the set of ir-
relevant sentences is initially much larger than the
set of relevant sentences, and we want the classi-
fier to learn how to identify new relevant sentences.
The feature set consists of all unigrams that appear
in the training set. The SVM is trained using a lin-
ear kernel with the default parameter settings. In a
self-training loop, the classifier is then applied to the
unlabeled sentences, and all sentences that it classi-
fies as relevant are added to the relevant sentences
pool. The classifier is then retrained with all of the
2We used the freely available SVMlight (Joachims, 1998)
implementation: http://svmlight.joachims.org
719
pattern?based
classifier
relevant
sentences
seed
patterns
SVM
classifier
unlabeled
sentences
SVM
training
irrelevant
sentences
irrelevant 
documents
Figure 1: The Training Process to Create a Relevant Sentence Classifier
relevant sentences and an equal number of irrelevant
sentences, and the process repeats. We ran this self-
training procedure for three iterations and then used
the resulting classifier as our relevant sentence clas-
sifier in the IE experiments described in Section 6.3.
4 Learning Semantic Affinity-based
Extraction Patterns
One motivation for creating a relevant region classi-
fier is to reduce the responsibilities of the extraction
patterns. Once we know that we are in a domain-
relevant area of text, patterns that simply identify
words and phrases belonging to a relevant seman-
tic class may be sufficient. In this section, we de-
scribe a method to automatically identify semanti-
cally appropriate extraction patterns for use with the
sentence classifier.
In previous work (Patwardhan and Riloff, 2006),
we introduced a metric called semantic affinity
which was used to automatically assign event roles
to extraction patterns. Semantic affinity measures
the tendency of a pattern to extract noun phrases
that belong to a specific set of semantic categories.
To use this metric for information extraction, a
mapping must be defined between semantic cate-
gories and the event roles that are relevant to the
IE task. For example, one role in the terrorism do-
main is physical target, which refers to physical ob-
jects that are the target of an attack. Most phys-
ical targets fall into one of two general semantic
categories: BUILDING or VEHICLE. Consequently,
we define the mapping ?Target ? BUILDING, VE-
HICLE?. Similarly, we might define the mapping
?Victim ? HUMAN, ANIMAL, PLANT? to charac-
terize possible victims of disease outbreaks. Each
semantic category must be mapped to a single event
role. This is a limitation of our approach for do-
mains where multiple roles can be filled by the same
class of fillers. However, sometimes a general se-
mantic class can be partitioned into subclasses that
are associated with different roles. For example, in
the terrorism domain, both perpetrators and victims
belong to the general semantic class HUMAN. But
we used the subclasses TERRORIST-HUMAN, which
represents likely perpetrator words (e.g., ?terrorist?,
?guerrilla?, and ?gunman?) and CIVILIAN-HUMAN,
which represents ordinary people (e.g., ?photogra-
pher?,?rancher?, and ?tourist?), in order to generate
different semantic affinity estimates for the perpetra-
tor and victim roles.
To determine the semantic category of a noun, we
use the Sundance parser (Riloff and Phillips, 2004),
which contains a dictionary of words that have se-
mantic category labels. Alternatively, a resource
such as WordNet (Fellbaum, 1998) could be used
to obtain this information. All semantic categories
that cannot be mapped to a relevant event role are
mapped to a special Other role.
To estimate the semantic affinity of a pattern p
for an event role rk, the system computes f(p, rk),
which is the number of pattern p?s extractions that
have a head noun belonging to a semantic category
mapped to rk. These frequency counts are obtained
by applying each pattern to the training corpus and
collecting its extractions. The semantic affinity of a
pattern p with respect to an event role rk is formally
defined as:
sem aff(p, rk) =
f(p, rk)
?|R|
i=1 f(p, ri)
log2 f(p, rk) (1)
where R is the set of event roles {r1, r2, . . . , r|R|}.
Semantic affinity is essentially the probability that
a phrase extracted by pattern p will be a semanti-
cally appropriate filler for role rk, weighted by the
log of the frequency.3 Note that it is possible for a
3This formula is very similar to pattern ranking metrics used
by previous IE systems (Riloff, 1996; Yangarber et al, 2000),
although not for semantics.
720
pattern to have a semantic affinity for multiple event
roles. For instance, a terrorism pattern like ?attack
on <np>? may have a semantic affinity for both
Targets and Victims.
To generate extraction patterns for an IE task, we
first apply the AutoSlog (Riloff, 1993) extraction
pattern generator to the training corpus exhaustively,
so that it literally generates a pattern to extract every
noun phrase in the corpus. Then for each event role,
we rank the patterns based on their semantic affinity
for that role.
Figure 2 shows the 10 patterns with the highest se-
mantic affinity scores for 4 event roles. In the terror-
ism domain, we show patterns that extract weapons
and perpetrator organizations (PerpOrg). In the dis-
ease outbreaks domain, we show patterns that ex-
tract diseases and victims. The patterns rely on shal-
low parsing, syntactic role assignment (e.g., subject
(subject) and direct object (dobj) identification), and
active/passive voice recognition, but they are shown
here in a simplified form for readability. The por-
tion in brackets (between < and >) is extracted, and
the other words must match the surrounding con-
text. In some cases, all of the matched words are
extracted (e.g., ?<# birds>?). Most of the highest-
ranked victim patterns recognize noun phrases that
refer to people or animals because they are common
in the disease outbreak stories and these patterns do
not extract information that is associated with any
competing event roles.
5 Distinguishing Primary and Secondary
Patterns
So far, our goal has been to find relevant areas
of text, and then apply semantically appropriate
patterns in those regions. Our expectation was
that fairly general, semantically appropriate patterns
could be effective if their range is restricted to re-
gions that are known to be relevant. If our relevant
sentence classifier was perfect, then performing IE
only on relevant regions would be ideal. However,
identifying relevant regions is a difficult problem in
its own right, and our relevant sentence classifier is
far from perfect.
Consequently, one limitation of our proposed ap-
proach is that no IE would be performed in sentences
that are not deemed to be relevant by the classifier,
Top Terrorism Patterns
Weapon PerpOrg
<subject> exploded <subject> claimed
planted <dobj> panama from <np>
fired <dobj> <np> claimed responsibility
<subject> was planted command of <np>
explosion of <np> wing of <np>
<subject> was detonated kidnapped by <np>
<subject> was set off guerillas of <np>
set off <dobj> <subject> operating
hurled <dobj> kingpins of <np>
<subject> was placed attacks by <np>
Top Disease Outbreak Patterns
Disease Victim
cases of <np> <# people>
spread of <np> <# cases>
outbreak of <np> <# birds>
<#th outbreak> <# animals>
<# outbreaks> <subject> died
case of <np> <# crows>
contracted <dobj> <subject> know
outbreaks of <np> <# pigs>
<# viruses> <# cattle>
spread of <np> <# sheep>
Figure 2: Top-Ranked Extraction Patterns
and this could negatively affect recall. We addressed
this issue by allowing reliable patterns to be applied
to all sentences in the text, irrespective of the output
of the sentence classifier. For example, the pattern
?<subject> was assassinated? is a clear indicator
of a murder event, and does not need to be restricted
by the sentence classifier4. We will refer to such
reliable patterns as Primary Patterns. In contrast,
patterns that are not necessarily reliable and need to
be restricted to relevant regions will be called Sec-
ondary Patterns.
To automatically distinguish Primary Patterns
from Secondary Patterns, we compute the condi-
tional probability of a pattern p being relevant,
Pr(relevant | p), based on the relevant and irrele-
vant documents in our training set. We then define
an upper conditional probability threshold ?u to sep-
arate Primary patterns from Secondary Patterns. If
a pattern has a high correlation with relevant docu-
ments, then our assumption is that it is generally a
reliable pattern that is not likely to occur in irrele-
vant contexts.
On the flip side, we can also use this condi-
tional probability to weed out patterns that rarely
4In other words, if such a pattern matches a sentence that is
classified as irrelevant, then the classifier is probably incorrect.
721
appear in relevant documents. Such patterns (e.g.,
?<subject> held?, ?<subject> saw?, etc.) could
potentially have a high semantic affinity for one of
the semantic categories, but they are not likely to be
useful if they mainly occur in irrelevant documents.
As a result, we also define a lower conditional proba-
bility threshold ?l that identifies irrelevant extraction
patterns.
The two thresholds ?u and ?l are used with seman-
tic affinity to identify the most appropriate Primary
and Secondary patterns for the task. This is done by
first removing from our extraction pattern collection
all patterns with probability less than ?l. For each
event role, we then sort the remaining patterns based
on their semantic affinity score for that role and se-
lect the top N patterns. Next, we use the ?u prob-
ability threshold to separate these N patterns into
two subsets. Patterns with a probability above ?u
are considered to be Primary patterns for that role,
and those below become the Secondary patterns.
6 Experiments and Results
6.1 Data Sets
We evaluated the performance of our IE system on
two data sets: the MUC-4 terrorism corpus (Sund-
heim, 1992), and a ProMed disease outbreaks cor-
pus. The MUC-4 IE task is to extract information
about Latin American terrorist events. We focused
our analysis on five MUC-4 string roles: perpetrator
individuals, perpetrator organizations, physical tar-
gets, victims, and weapons. The disease outbreaks
corpus consists of electronic reports about disease
outbreak events. For this domain we focused on two
string roles: diseases and victims5.
The MUC-4 data set consists of 1700 documents,
divided into 1300 development (DEV) texts, and
four test sets of 100 texts each (TST1, TST2, TST3,
and TST4). We used 1300 texts (DEV) as our train-
ing set, 200 texts (TST1+TST2) for tuning, and 200
texts (TST3+TST4) as a test set. All 1700 docu-
ments have answer key templates. For the training
set, we used the answer keys to separate the doc-
uments into relevant and irrelevant subsets. Any
document containing at least one relevant event was
considered relevant.
5The ?victims? can be people, animals, or plants that are
affected by a disease.
For the disease outbreak domain the data set
was collected from ProMed-mail6, an open-source,
global electronic reporting system for outbreaks
of infectious diseases. We collected thousands of
ProMed reports and created answer key templates
for 245 randomly selected articles. We used 125 as
a tuning set, and 120 as the test set. We used 2000
different documents as the relevant documents for
training. Most of the ProMed articles contain email
headers, footers, citations, and other snippets of non-
narrative text, so we wrote a ?zoner? program7 to
automatically strip off some of this extraneous in-
formation.
To obtain irrelevant documents, we collected
4000 biomedical abstracts from PubMed8, a free
archive of biomedical literature. We collected twice
as many irrelevant documents because the PubMed
articles are roughly half the size of the ProMed arti-
cles, on average. To ensure that the PubMed articles
were truly irrelevant (i.e. did not contain any disease
outbreak reports) we used specific queries to exclude
disease outbreak abstracts.
The complete IE task involves the creation of
answer key templates, one template per incident9.
Template generation is a complex process, requir-
ing coreference resolution and discourse analysis to
determine how many incidents were reported and
which facts belong with each incident. Our work fo-
cuses on extraction pattern learning and not template
generation, so we evaluated our systems directly on
the extractions themselves, before template genera-
tion would take place. This approach directly mea-
sures how accurately the patterns find relevant infor-
mation, without confounding factors from the tem-
plate generation process. For example, if a coref-
erence resolver incorrectly decides that two extrac-
tions are coreferent and merges them, then only one
extraction would be scored. We used a head noun
scoring scheme, where an extraction is considered
to be correct if its head noun matches the head noun
in the answer key10. Also, pronouns were discarded
from both the system responses and the answer keys
since no coreference resolution is done. Duplicate
6http://www.promedmail.org
7The term zoner was introduced by Yangarber et al (2002).
8http://www.pubmedcentral.nih.gov
9Many of the stories have multiple incidents per article.
10For example, ?armed men? will match ?5 armed men?.
722
extractions (e.g., the same string extracted by differ-
ent patterns) were conflated before being scored, so
they count as just one hit or one miss.
6.2 Relevant Sentence Classifier Results
First, we evaluated the performance of the relevant
sentence classifier described in Section 3. We auto-
matically generated seed patterns from the training
texts. AutoSlog (Riloff, 1993) was used to gener-
ate all extraction patterns that appear in the train-
ing documents, and only those patterns with fre-
quency > 50 were kept. These were then ranked
by Pr(relevant | p), and the top 20 patterns were
chosen as seeds. In the disease outbreak domain, 54
patterns had a frequency > 50 and probability of 1.0.
We wanted to use the same number of seeds in both
domains for consistency, so we manually reviewed
them and used the 20 most domain-specific patterns
as seeds.
Due to the greater stylistic differences between
the relevant and irrelevant documents in the disease
outbreak domain (since they were gathered from dif-
ferent sources), we decided to make the classifier for
that domain more conservative in classifying docu-
ments as relevant. To do this we used the prediction
scores output by the SVM as a measure of confi-
dence in the classification. These scores are essen-
tially the distance of the test examples from the sup-
port vectors of the SVM. For the disease outbreaks
domain we used a cutoff of 1.0 and in the terrorism
domain we used the default of 0.
Since we do not have sentence annotated data,
there is no direct way to evaluate the classifiers.
However, we did an indirect evaluation by using the
answer keys from the tuning set. If a sentence in
a tuning document contained a string that occurred
in the corresponding answer key template, then we
considered that sentence to be relevant. Otherwise,
the sentence was deemed irrelevant. This evaluation
is not perfect for two reasons: (1) answer key strings
do not always appear in relevant sentences.11, and
(2) some arguably relevant sentences may not con-
tain an answer key string (e.g., they may contain a
pronoun that refers to the answer, but the pronoun it-
self is not the desired extraction). However, judging
11This happens due to coreference, e.g., when multiple oc-
currences of an answer appear in a document, some of them
may occur in relevant sentences while others do not.
Irrelevant Relevant
Acc Rec Pr F Rec Pr F
Terrorism
Iter #1 .84 .93 .89 .91 .41 .55 .47
Iter #2 .84 .90 .91 .90 .54 .51 .53
Iter #3 .82 .85 .92 .89 .63 .46 .53
Disease Outbreaks
Iter #1 .75 .96 .76 .85 .21 .66 .32
Iter #2 .71 .76 .82 .79 .58 .48 .53
Iter #3 .63 .60 .85 .70 .72 .41 .52
Table 1: Relevant Sentence Classifier Evaluation
the relevance of sentences without relying on answer
keys is also tricky, so we decided that this approach
was probably good enough to get a reasonable as-
sessment of the classifier. Using this criterion, 17%
of the sentences in the terrorism articles are relevant,
and 28% of the sentences in the disease outbreaks
articles are relevant.
Table 1 shows the accuracy, recall, precision, and
F scores of the SVM classifiers after each self-
training iteration. The classifiers generated after the
third iteration were used in our IE experiments. The
final accuracy is 82% in the terrorism domain, and
63% for the disease outbreaks domain. The preci-
sion on irrelevant sentences is high in both domains,
but the precision on relevant sentences is relatively
weak. Despite this, we will show in Section 6.3 that
the classifier is effective for the IE task. The rea-
son why the classifier improves IE performance is
because it favorably alters the proportion of relevant
sentences that are passed along to the IE system. For
example, an analysis of the tuning set shows that re-
moving the sentences deemed to be irrelevant by the
classifier increases the proportion of relevant sen-
tences from 17% to 46% in the terrorism domain,
and from 28% to 41% in the disease outbreaks do-
main.
We will also see in Section 6.3 that IE recall only
drops a little when the sentence classifier is used,
despite the fact that its recall on relevant sentences
is only 63% in terrorism and 72% for disease out-
breaks. One possible explanation is that the an-
swer keys often contain multiple acceptable answer
strings (e.g., ?John Kennedy? and ?JFK? might both
be acceptable answers). On average, the answer
keys contain approximately 1.64 acceptable strings
per answer in the terrorism domain, and 1.77 accept-
able strings per answer in the disease outbreaks do-
723
Terrorism
Patterns App Rec Pr F Rec Pr F
PerpInd PerpOrg
ASlogTS All .49 .35 .41 .33 .49 .40
ASlogTS Rel .41 .50 .45 .27 .58 .37
Target Victim
ASlogTS All .64 .42 .51 .52 .48 .50
ASlogTS Rel .57 .49 .53 .48 .54 .51
Weapon
ASlogTS All .45 .39 .42
ASlogTS Rel .40 .51 .45
Disease Outbreaks
Disease Victim
ASlogTS All .51 .27 .36 .48 .35 .41
ASlogTS Rel .46 .31 .37 .44 .38 .41
Table 2: AutoSlog-TS Results
main. Thus, even if the sentence classifier discards
some relevant sentences, an equally acceptable an-
swer may be found in a different sentence.
6.3 Information Extraction Results
We first conducted two experiments with an exist-
ing IE pattern learner, AutoSlog-TS (Riloff, 1996)
to give us a baseline against which to compare our
results. The ?All? rows in Table 2 show these results,
where ?All? means that the IE patterns were applied
to all of the sentences in the test set. AutoSlog-TS12
produced F scores between 40-51% on the MUC-4
test set, and 36-41% on the ProMed test set. The
terrorism scores are competitive with the MUC-4
scores reported by Chieu et al (2003), although they
are not directly comparable because those scores are
based on template generation. Since we created the
ProMed test set ourselves, we are the first to report
results on it13.
Next, we evaluated the performance of AutoSlog-
TS? extraction patterns when they are applied only in
the sentences deemed to be relevant by our relevant
sentence classifier. The purpose of this experiment
was to determine whether the relevant sentence clas-
sifier can be beneficial when used with IE patterns
known to be of good quality. The ?Rel? rows in Ta-
12AutoSlog-TS was trained on a much larger data set of 4,958
ProMed and 10,191 PubMed documents for the disease out-
breaks domain. AutoSlog-TS requires a human review of the
top-ranked patterns, which resulted in 396 patterns for the ter-
rorism domain and 125 patterns for the disease outbreaks do-
main.
13Some previous work has been done with ProMed articles
(Grishman et al, 2002a; Grishman et al, 2002b), but we are not
aware of any IE evaluations on them.
Disease Victim
Patterns App Rec Pr F Rec Pr F
ASlogTS All .51 .27 .36 .48 .35 .41
SA-50 All .51 .25 .34 .47 .41 .44
SA-50 Rel .49 .31 .38 .44 .43 .43
SA-50 Sel .50 .29 .36 .46 .41 .44
SA-100 All .57 .22 .32 .52 .33 .40
SA-100 Rel .55 .28 .37 .49 .36 .41
SA-100 Sel .56 .26 .35 .51 .34 .41
SA-150 All .66 .20 .31 .55 .27 .37
SA-150 Rel .61 .26 .36 .51 .31 .38
SA-150 Sel .63 .24 .35 .53 .29 .37
SA-200 All .68 .19 .30 .56 .26 .36
SA-200 Rel .63 .25 .35 .52 .30 .38
SA-200 Sel .65 .23 .34 .54 .28 .37
Table 3: ProMed Disease Outbreak Results
ble 2 show the scores for this experiment. Precision
increased substantially on all 7 roles, although with
some recall loss. This shows that a sentence classi-
fier that has a high precision on irrelevant sentences
but only a moderate precision on relevant sentences
can be useful for information extraction.
Tables 3 and 4 show the results of our IE system,
which uses the top N Semantic Affinity (SA) pat-
terns and the relevant sentence classifier. We also
show the AutoSlog-TS results again in the top row
for comparison. The best F score for each role is
shown in boldface. We used a lower probability
threshold ?l of 0.5 to filter out irrelevant patterns.
We then ranked the remaining patterns based on se-
mantic affinity, and evaluated the performance of the
top 50, 100, 150, and 200 patterns. The App column
indicates how the patterns were applied: for All they
were applied in all sentences in the test set, for Rel
they were applied only in the relevant sentences (as
judged by our sentence classifier). For the Sel con-
dition, the Primary patterns were applied in all sen-
tences but the Secondary patterns were applied only
in relevant sentences. To separate Primary and Sec-
ondary patterns we used an upper probability thresh-
old ?u of 0.8.
Looking at the rows with the All condition, we
see that the semantic affinity patterns achieve good
recall (e.g., the top 200 patterns have a recall over
50% for most roles), but precision is often quite low.
This is not surprising because high semantic affin-
ity patterns do not necessarily have to be relevant to
the domain, so long as they recognize semantically
appropriate things.
724
PerpInd PerpOrg Target Victim Weapon
Patterns App Rec Pr F Rec Pr F Rec Pr F Rec Pr F Rec Pr F
ASlogTS All .49 .35 .41 .33 .49 .40 .64 .42 .51 .52 .48 .50 .45 .39 .42
SA-50 All .24 .29 .26 .20 .42 .27 .42 .43 .42 .41 .43 .42 .53 .46 .50
SA-50 Rel .19 .32 .24 .18 .60 .28 .38 .48 .42 .37 .52 .43 .41 .56 .48
SA-50 Sel .20 .33 .25 .20 .54 .29 .42 .50 .45 .38 .52 .44 .43 .53 .48
SA-100 All .40 .30 .34 .30 .43 .35 .56 .38 .45 .45 .37 .41 .55 .43 .48
SA-100 Rel .36 .39 .38 .25 .59 .35 .52 .45 .48 .40 .47 .44 .45 .51 .48
SA-100 Sel .38 .40 .39 .27 .55 .36 .56 .46 .50 .41 .47 .44 .47 .49 .48
SA-150 All .50 .27 .35 .34 .39 .37 .62 .30 .40 .50 .33 .40 .55 .39 .45
SA-150 Rel .46 .39 .42 .28 .58 .38 .56 .37 .45 .44 .45 .45 .45 .50 .47
SA-150 Sel .48 .39 .43 .31 .55 .40 .60 .37 .46 .46 .44 .45 .47 .47 .47
SA-200 All .73 .08 .15 .42 .43 .42 .64 .29 .40 .54 .32 .40 .64 .17 .27
SA-200 Rel .67 .15 .24 .34 .61 .43 .58 .36 .45 .47 .43 .45 .52 .29 .37
SA-200 Sel .71 .12 .21 .36 .58 .45 .61 .35 .45 .48 .43 .45 .53 .22 .31
Table 4: MUC-4 Terrorism Results
Next, we can compare each All row with the Rel
row immediately below it. We observe that in every
case precision improves, often dramatically. This
demonstrates that our sentence classifier is having
the desired effect. However, observe that the preci-
sion gain comes with some loss in recall points.
Clearly, this drop in recall is due to the answers
embedded inside relevant sentences incorrectly clas-
sified as irrelevant. To counter this, we apply the Pri-
mary patterns to all the sentences. Thus, if we com-
pare each Rel row with the Sel row immediately be-
low it, we see the effect of loosening the reins on the
Primary patterns (the Secondary patterns are still re-
stricted to the relevant sentences). In most cases, the
recall improves with a relatively small drop in preci-
sion, or no drop at all. In the terrorism domain, the
highest F score for four of the five roles occurs under
the Sel condition. In the disease outbreaks domain,
the best F score for diseases occurs in the Rel con-
dition, while the best score for victims is achieved
under both the All and the Sel conditions.
Finally, we note that the best F scores produced
by our information extraction system are higher than
those produced by AutoSlog-TS for all of the roles
except Targets and Victims, and our best perfor-
mance on Targets is only slightly lower. These re-
sults are particularly noteworthy because AutoSlog-
TS requires a human to manually review the patterns
and assign event roles to them. In contrast, our ap-
proach is fully automated.
These results validate our hypothesis that decou-
pling the processes of finding relevant regions and
applying semantically appropriate patterns can cre-
ate an effective IE system.
7 Conclusions
In this work, we described an information extraction
system based on a relevant sentence classifier and
extraction patterns learned using a semantic affin-
ity metric. The sentence classifier was self-trained
using only relevant and irrelevant documents plus a
handful of seed extraction patterns. We showed that
separating the task of relevant region identification
from that of pattern extraction can be effective for in-
formation extraction. In addition, we observed that
the use of a relevant sentence classifier is beneficial
for an IE system.
There are several avenues that need to be explored
for future work. First, it would be interesting to see
if the use of richer features can improve classifier
performance, and if that in turn improves the perfor-
mance of the IE system. We would also like to ex-
periment with different region sizes and study their
effect on information extraction. Finally, other tech-
niques for learning semantically appropriate extrac-
tion patterns could be investigated.
Acknowledgments
This research was supported by NSF Grant IIS-
0208985, Department of Homeland Security Grant
N0014-07-1-0152, and the Institute for Scientific
Computing Research and the Center for Applied
Scientific Computing within Lawrence Livermore
National Laboratory. We are grateful to Sean Igo
and Rich Warren for annotating the disease out-
breaks corpus.
725
References
E. Agichtein and L. Gravano. 2000. Snowball: Extract-
ing Relations from Large Plain-Text Collections. In
Proceedings of the Fifth ACM Conference on Digital
Libraries, pages 85?94, San Antonio, TX, June.
R. Bunescu and R. Mooney. 2004. Collective Informa-
tion Extraction with Relational Markov Networks. In
Proceeding of the 42nd Annual Meeting of the Associ-
ation for Computational Linguistics, pages 438?445,
Barcelona, Spain, July.
M. Califf and R. Mooney. 1999. Relational Learning
of Pattern-matching Rules for Information Extraction.
In Proceedings of the Sixteenth National Conference
on Artificial Intelligence, pages 328?334, Orlando, FL,
July.
J. Callan. 1994. Passage-Level Evidence in Document
Retrieval. In Proceedings of the 17th Annual Interna-
tional ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, pages 302?310,
Dublin, Ireland, July.
H. Chieu, H. Ng, and Y. Lee. 2003. Closing the
Gap: Learning-Based Information Extraction Rivaling
Knowledge-Engineering Methods. In Proceedings of
the 41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 216?223, Sapporo, Japan,
July.
F. Ciravegna. 2001. Adaptive Information Extraction
from Text by Rule Induction and Generalisation. In
Proceedings of Seventeenth International Joint Con-
ference on Artificial Intelligence, pages 1251?1256,
Seattle, WA, August.
C. Fellbaum, editor. 1998. WordNet: An electronic lexi-
cal database. MIT Press.
D. Freitag and A. McCallum. 2000. Information Ex-
traction with HMM Structures Learned by Stochas-
tic Optimization. In Proceedings of the Seventeenth
National Conference on Artificial Intelligence, pages
584?589, Austin, TX, August.
D. Freitag. 1998. Toward General-Purpose Learning
for Information Extraction. In Proceedings of the
36th Annual Meeting of the Association for Computa-
tional Linguistics and 17th International Conference
on Computational Linguistics, pages 404?408, Mon-
treal, Quebec, August.
R. Grishman, S. Huttunen, and R. Yangarber. 2002a. In-
formation Extraction for Enhanced Access to Disease
Outbreak Reports. Journal of Biomedical Informatics,
35(4):236?246, August.
R. Grishman, S. Huttunen, and R. Yangarber. 2002b.
Real-Time Event Extraction for Infectious Disease
Outbreaks. In Proceedings of the 3rd Annual Human
Language Technology Conference, San Diego, CA,
March.
J. Hobbs, D. Appelt, J. Bear, D. Israel, M. Kameyama,
M. Stickel, and M. Tyson. 1997. FASTUS: A Cas-
caded Finite-state Transducer for Extracting Informa-
tion for Natural-Language Text. In E. Roche and
Y. Schabes, editors, Finite-State Language Processing,
pages 383?406. MIT Press, Cambridge, MA.
S. Huffman. 1996. Learning Information Extraction
Patterns from Examples. In S. Wermter, E. Riloff,
and G. Scheler, editors, Connectionist, Statistical, and
Symbolic Approaches to Learning for Natural Lan-
guage Processing, pages 246?260. Springer, Berlin.
T. Joachims. 1998. Text Categorization with Support
Vector Machines: Learning with Many Relevant Fea-
tures. In Proceedings of the Tenth European Confer-
ence on Machine Learning, pages 137?142, April.
J. Kim and D. Moldovan. 1993. PALKA: A System for
Lexical Knowledge Acquisition. In Proceedings of
the Second International Conference on Information
and Knowledge Management, pages 124?131, Wash-
ington, DC, November.
S. Patwardhan and E. Riloff. 2006. Learning Domain-
Specific Information Extraction Patterns from the
Web. In Proceedings of the ACL 2006 Workshop on
Information Extraction Beyond the Document, pages
66?73, Sydney, Australia, July.
A. Popescu, A. Yates, and O. Etzioni. 2004. Class Ex-
traction from the World Wide Web. In Ion Muslea,
editor, Adaptive Text Extraction and Mining: Papers
from the 2004 AAAI Workshop, pages 68?73, San Jose,
CA, July.
E. Riloff and W. Phillips. 2004. An Introduction to the
Sundance and AutoSlog Systems. Technical Report
UUCS-04-015, School of Computing, University of
Utah.
E. Riloff. 1993. Automatically Constructing a Dictio-
nary for Information Extraction Tasks. In Proceedings
of the Eleventh National Conference on Artificial In-
telligence, pages 811?816, Washington, DC, July.
E. Riloff. 1996. Automatically Generating Extrac-
tion Patterns from Untagged Text. In Proceedings of
the Thirteenth National Conference on Articial Intelli-
gence, pages 1044?1049, Portland, OR, August.
G. Salton, J. Allan, and C. Buckley. 1993. Approaches
to Passage Retrieval in Full Text Information Systems.
In Proceedings of the 16th Annual International ACM
SIGIR Conference on Research and Development on
Information Retrieval, pages 49?58, Pittsburgh, PA,
June.
726
S. Soderland, D. Fisher, J. Aseltine, and W. Lehnert.
1995. CRYSTAL: Inducing a Conceptual Dictionary.
In Proceedings of the Fourteenth International Joint
Conference on Artificial Intelligence, pages 1314?
1319, Montreal, Canada, August.
S. Soderland. 1999. Learning Information Extraction
Rules for Semi-Structured and Free Text. Machine
Learning, 34(1-3):233?272, February.
K. Sudo, S. Sekine, and R. Grishman. 2003. An Im-
proved Extraction Patterns Representation Model for
Automatic IE Pattern Acquisition. In Proceedings of
the 41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 224?231, Sapporo, Japan,
July.
B. Sundheim. 1992. Overview of the Fourth Message
Understanding Evaluation and Conference. In Pro-
ceedings of the Fourth Message Understanding Con-
ference (MUC-4), pages 3?21, McLean, VA, June.
V. Vapnik. 1995. The Nature of Statistical Learning The-
ory. Springer, New York, NY.
A. Yakushiji, Y. Miyao, T. Ohta, Y. Tateisi, and J. Tsu-
jii. 2006. Construction of Predicate-argument Struc-
ture Patterns for Biomedical Information Extraction.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, pages 284?
292, Sydney, Australia, July.
R. Yangarber, R. Grishman, P. Tapanainen, and S. Hut-
tunen. 2000. Automatic Acquisition of Domain
Knowledge for Information Extraction. In Proceed-
ings of the 18th International Conference on Computa-
tional Linguistics, pages 940?946, Saarbru?cken, Ger-
many, August.
R. Yangarber, W. Lin, and R. Grishman. 2002. Unsu-
pervised Learning of Generalized Names. In Proceed-
ings of the 19th International Conference on Compu-
tational Linguistics, pages 154?160, Taipei, Taiwan,
August.
727
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 151?160,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
A Unified Model of Phrasal and Sentential Evidence
for Information Extraction
Siddharth Patwardhan and Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT 84112
{sidd,riloff}@cs.utah.edu
Abstract
Information Extraction (IE) systems that
extract role fillers for events typically look
at the local context surrounding a phrase
when deciding whether to extract it. Of-
ten, however, role fillers occur in clauses
that are not directly linked to an event
word. We present a new model for event
extraction that jointly considers both the
local context around a phrase along with
the wider sentential context in a proba-
bilistic framework. Our approach uses a
sentential event recognizer and a plausible
role-filler recognizer that is conditioned on
event sentences. We evaluate our system
on two IE data sets and show that our
model performs well in comparison to ex-
isting IE systems that rely on local phrasal
context.
1 Introduction
Information Extraction (IE) systems typically use
extraction patterns (e.g., Soderland et al (1995),
Riloff (1996), Yangarber et al (2000), Califf
and Mooney (2003)) or classifiers (e.g., Freitag
(1998), Freitag and McCallum (2000), Chieu et al
(2003), Bunescu and Mooney (2004)) to extract
role fillers for events. Most IE systems consider
only the immediate context surrounding a phrase
when deciding whether to extract it. For tasks such
as named entity recognition, immediate context is
usually sufficient. But for more complex tasks,
such as event extraction, a larger field of view is
often needed to understand how facts tie together.
Most IE systems are designed to identify role
fillers that appear as arguments to event verbs
or nouns, either explicitly via syntactic relations
or implicitly via proximity (e.g., John murdered
Tom or the murder of Tom by John). But many
facts are presented in clauses that do not contain
event words, requiring discourse relations or deep
structural analysis to associate the facts with event
roles. For example, consider the sentences below:
Seven people have died
. . . and 30 were injured in India after terror-
ists launched an attack on the Taj Hotel.
. . . in Mexico City and its surrounding sub-
urbs in a Swine Flu outbreak.
. . . after a tractor-trailer collided with a bus
in Arkansas.
Two bridges were destroyed
. . . in Baghdad last night in a resurgence of
bomb attacks in the capital city.
. . . and $50 million in damage was caused by
a hurricane that hit Miami on Friday.
. . . to make way for modern, safer bridges
that will be constructed early next year.
These examples illustrate a common phenomenon
in text where information is not explicitly stated
as filling an event role, but readers have no trou-
ble making this inference. The role fillers above
(seven people, two bridges) occur as arguments to
verbs that reveal state information (death, destruc-
tion) but are not event-specific (i.e., death and de-
struction can result from a wide variety of incident
types). IE systems often fail to extract these role
fillers because these systems do not recognize the
immediate context as being relevant to the specific
type of event that they are looking for.
We propose a new model for information ex-
traction that incorporates both phrasal and senten-
tial evidence in a unified framework. Our uni-
fied probabilistic model, called GLACIER, consists
of two components: a model for sentential event
recognition and a model for recognizing plausi-
ble role fillers. The Sentential Event Recognizer
offers a probabilistic assessment of whether a sen-
tence is discussing a domain-relevant event. The
151
Plausible Role-Filler Recognizer is then condi-
tioned to identify phrases as role fillers based upon
the assumption that the surrounding context is dis-
cussing a relevant event. This unified probabilistic
model allows the two components to jointly make
decisions based upon both the local evidence sur-
rounding each phrase and the ?peripheral vision?
afforded by the sentential event recognizer.
This paper is organized as follows. Section
2 positions our research with respect to related
work. Section 3 presents our unified probabilistic
model for information extraction. Section 4 shows
experimental results on two IE data sets, and Sec-
tion 5 discusses directions for future work.
2 Related Work
Many event extraction systems rely heavily on the
local context around words or phrases that are can-
didates for extraction. Some systems use extrac-
tion patterns (Soderland et al, 1995; Riloff, 1996;
Yangarber et al, 2000; Califf and Mooney, 2003),
which represent the immediate contexts surround-
ing candidate extractions. Similarly, classifier-
based approaches (Freitag, 1998; Freitag and Mc-
Callum, 2000; Chieu et al, 2003; Bunescu and
Mooney, 2004) rely on features in the immedi-
ate context of the candidate extractions. Our work
seeks to incorporate additional context into IE.
Indeed, several recent approaches have shown
the need for global information to improve IE per-
formance. Maslennikov and Chua (2007) use dis-
course trees and local syntactic dependencies in
a pattern-based framework to incorporate wider
context. Finkel et al (2005) and Ji and Grish-
man (2008) incorporate global information by en-
forcing event role or label consistency over a doc-
ument or across related documents. In contrast,
our approach simply creates a richer IE model for
individual extractions by expanding the ?field of
view? to include the surrounding sentence.
The two components of the unified model pre-
sented in this paper are somewhat similar to our
previous work (Patwardhan and Riloff, 2007),
where we employ a relevant region identification
phase prior to pattern-based extraction. In that
work we adopted a pipeline paradigm, where a
classifier identifies relevant sentences and only
those sentences are fed to the extraction module.
Our unified probabilistic model described in this
paper does not draw a hard line between rele-
vant and irrelevant sentences, but gently balances
the influence of both local and sentential contexts
through probability estimates.
3 A Unified IE Model that Combines
Phrasal and Sentential Evidence
We introduce a probabilistic model for event-
based IE that balances the influence of two kinds
of contextual information. Our goal is to create
a model that has the flexibility to make extraction
decisions based upon strong evidence from the lo-
cal context, or strong evidence from the wider con-
text coupled with a more general local context. For
example, some phrases explicitly refer to an event,
so they almost certainly warrant extraction regard-
less of the wider context (e.g., terrorists launched
an attack).1 In contrast, some phrases are poten-
tially relevant but too general to warrant extrac-
tion on their own (e.g., people died could be the
result of different incident types). If we are confi-
dent that the sentence discusses an event of inter-
est, however, then such phrases could be reliably
extracted.
Our unified model for IE (GLACIER) combines
two types of contextual information by incorpo-
rating it into a probabilistic framework. To deter-
mine whether a noun phrase instance NP
i
should
be extracted as a filler for an event role, GLACIER
computes the joint probability that NP
i
:
(1) appears in an event sentence, and
(2) is a legitimate filler for the event role.
Thus, GLACIER is designed for noun phrase ex-
traction and, mathematically, its decisions are
based on the following joint probability:
P (EvSent(S
NP
i
),PlausFillr(NP
i
))
where S
NP
i
is the sentence containing noun phrase
NP
i
. This probability estimate is based on con-
textual features F appearing within S
NP
i
and in
the local context of NP
i
. Including F in the joint
probability, and applying the product rule, we can
split our probability into two components:
P (EvSent(S
NP
i
),PlausFillr(NP
i
)|F ) =
P (EvSent(S
NP
i
)|F )
? P (PlausFillr(NP
i
)|EvSent(S
NP
i
), F )
These two probability components, in the expres-
sion above, form the basis of the two modules in
1There are always exceptions of course, such as hypothet-
ical statements, but they are relatively uncommon.
152
our IE system ? the sentential event recognizer and
the plausible role-filler recognizer. In arriving at
a decision to extract a noun phrase, our unified
model for IE uses these modules to estimate the
two probabilities based on the set of contextual
features F . Note that having these two probability
components allows the system to gently balance
the influence from the sentential and phrasal con-
texts, without having to make hard decisions about
sentence relevance or phrases in isolation.
In this system, the sentential event recog-
nizer is embodied in the probability compo-
nent P (EvSent(S
NP
i
)|F ). This is essentially
the probability of a sentence describing a rel-
evant event. Similarly, the plausible role-
filler recognizer is embodied by the probabil-
ity P (PlausFillr(NP
i
)|EvSent(S
NP
i
), F ). This
component, therefore, estimates the probability
that a noun phrase fills a specific event role, as-
suming that the noun phrase occurs in an event
sentence. Many different techniques could be used
to produce these probability estimates. In the rest
of this section, we present the specific models that
we used for each of these components.
3.1 Plausible Role-Filler Recognizer
The plausible role-filler recognizer is similar to
most traditional IE systems, where the goal is to
determine whether a noun phrase can be a legiti-
mate filler for a specific type of event role based on
its local context. Pattern-based approaches match
the context surrounding a phrase using lexico-
syntactic patterns or rules. However, most of these
approaches do not produce probability estimates
for the extractions. Classifier-based approaches
use machine learning classifiers to make extrac-
tion decisions, based on features associated with
the local context. Any classifier that can generate
probability estimates, or similar confidence val-
ues, could be plugged into our model.
In our work, we use a Na??ve Bayes classifier as
our plausible role-filler recognizer. The probabili-
ties are computed using a generative Na??ve Bayes
framework, based on local contextual features sur-
rounding a noun phrase. These clues include lexi-
cal matches, semantic features, and syntactic rela-
tions, and will be described in more detail in Sec-
tion 3.3. The Na??ve Bayes (NB) plausible role-
filler recognizer is defined as follows:
P (PlausFillr(NP
i
)|EvSent(S
NP
i
), F ) =
1
Z
P (PlausFillr(NP
i
)|EvSent(S
NP
i
)) ?
?
f
i
?F
P (f
i
|PlausFillr(NP
i
),EvSent(S
NP
i
))
where F is the set of local contextual features
and Z is the normalizing constant. The prior
P (PlausFillr(NP
i
)|EvSent(S
NP
i
)) is estimated
from the fraction of role fillers in the training data.
The product term in the equation is the likelihood,
which makes the simplifying assumption that all
of the features in F are independent of one an-
other. It is important to note that these probabil-
ities are conditioned on the noun phrase NP
i
ap-
pearing in an event sentence.
Most IE systems need to extract several differ-
ent types of role fillers for each event. For in-
stance, to extract information about terrorist inci-
dents a system may extract the names of perpetra-
tors, victims, targets, and weapons. We create a
separate IE model for each type of event role. To
construct a unified IE model for an event role, we
must specifically create a plausible role-filler rec-
ognizer for that event role, but we can use a single
sentential event recognizer for all of the role filler
types.
3.2 Sentential Event Recognizer
The task at hand for the sentential event recognizer
is to analyze features in a sentence and estimate
the probability that the sentence is discussing a
relevant event. This is very similar to the task per-
formed by text classification systems, with some
minor differences. Firstly, we are dealing with
the classification of sentences, as opposed to en-
tire documents. Secondly, we need to generate a
probability estimate of the ?class?, and not just
a class label. Like the plausible role-filler recog-
nizer, here too we employ machine learning clas-
sifiers to estimate the desired probabilities.
3.2.1 Na??ve Bayes Event Recognizer
Since Na??ve Bayes classifiers estimate class prob-
abilities, we employ such a classifier to create a
sentential event recognizer:
P (EvSent(S
NP
i
)|F ) =
1
Z
P (EvSent(S
NP
i
))
?
?
f
i
?F
P (f
i
|EvSent(S
NP
i
))
where Z is the normalizing constant and F is the
set of contextual features in the sentence. The
153
prior P (EvSent
S(NP
i
)
) is obtained from the ra-
tio of event and non-event sentences in the train-
ing data. The product term in the equation is the
likelihood, which makes the simplifying assump-
tion that the features in F are independent of one
another. The features used by the model will be
described in Section 3.3.
A known issue with Na??ve Bayes classifiers is
that, even though their classification accuracy is
often quite reasonable, their probability estimates
are often poor (Domingos and Pazzani, 1996;
Zadrozny and Elkan, 2001; Manning et al, 2008).
The problem is that these classifiers tend to overes-
timate the probability of the predicted class, result-
ing in a situation where most probability estimates
from the classifier tend to be either extremely close
to 0.0 or extremely close to 1.0. We observed this
problem in our classifier too, so we decided to ex-
plore an additional model to estimate probabilities
for the sentential event recognizer. This second
model, based on SVMs, is described next.
3.2.2 SVM Event Recognizer
Given the all-or-nothing nature of the probability
estimates that we observed from the Na??ve Bayes
model, we decided to try using a Support Vector
Machine (SVM) (Vapnik, 1995; Joachims, 1998)
classifier as an alternative to Na??ve Bayes. One
of the issues with doing this is that SVMs are not
probabilistic classifiers. SVMsmake classification
decisions using on a decision boundary defined by
support vectors identified during training. A deci-
sion function is applied to unseen test examples
to determine which side of the decision bound-
ary those examples lie. While the values obtained
from the decision function only indicate class as-
signments for the examples, we used these val-
ues to produce confidence scores for our sentential
event recognizer.
To produce a confidence score from the SVM
classifier, we take the values generated by the deci-
sion function for each test instance and normalize
them based on the minimum and maximum values
produced across all of the test instances. This nor-
malization process produces values between 0 and
1 that we use as a rough indicator of the confidence
in the SVM?s classification. We observed that we
could effect a consistent recall/precision trade-off
by using these values as thresholds for classifica-
tion decisions, which suggests that this approach
worked reasonably well for our task.
3.3 Contextual Features
We used a variety of contextual features in both
components of our system. The plausible role-
filler recognizer uses the following types of fea-
tures for each candidate noun phrase NP
i
: lexical
head ofNP
i
, semantic class ofNP
i
?s lexical head,
named entity tags associated with NP
i
and lexico-
syntactic patterns that represent the local context
surrounding NP
i
. The feature set is automatically
generated from the texts. Each feature is assigned
a binary value for each instance, indicating either
the presence or absence of the feature.
The named-entity features are generated by the
freely available Stanford NER tagger (Finkel et
al., 2005). We use the pre-trained NER model
that comes with the software to identify person,
organization and location names. The syntac-
tic and semantic features are generated by the
Sundance/AutoSlog system (Riloff and Phillips,
2004). We use the Sundance shallow parser to
identify lexical heads, and use its semantic dictio-
naries to assign semantic features to words. The
AutoSlog pattern generator (Riloff, 1996) is used
to create the lexico-syntactic pattern features that
capture local context around each noun phrase.
Our training sets produce a very large number
of features, which initially bogged down our clas-
sifiers. Consequently, we reduced the size of the
feature set by discarding all features that appeared
four times or less in the training set.
Our sentential event recognizer uses the same
contextual features as the plausible role-filler rec-
ognizer, except that features are generated for
every NP in the sentence. In addition, it uses
three types of sentence-level features: sentence
length, bag of words, and verb tense, which are
also binary features. We have two binary sentence
length features indicating that the sentence is long
(greater than 35 words) or is short (shorter than 5
words). Additionally, all of the words in each sen-
tence in the training data are generated as bag of
words features for the sentential model. Finally,
we generate verb tense features from all verbs ap-
pearing in each sentence. Here too we apply a fre-
quency cutoff and eliminate all features that ap-
pear four times or less in the training data.
4 IE Evaluation
4.1 Data Sets
We evaluated the performance of our IE system on
two data sets: the MUC-4 terrorism corpus (Sund-
154
heim, 1992), and a ProMed disease outbreaks cor-
pus (Phillips and Riloff, 2007; Patwardhan and
Riloff, 2007). The MUC-4 data set is a standard
IE benchmark collection of news stories about ter-
rorist events. It contains 1700 documents divided
into 1300 development (DEV) texts, and four test
sets of 100 texts each (TST1, TST2, TST3, and
TST4). Unless otherwise stated, our experiments
adopted the same training/test split used in pre-
vious research: the 1300 DEV texts for training,
200 texts (TST1+TST2) for tuning, and 200 texts
(TST3+TST4) as the blind test set. We evaluated
our system on five MUC-4 string roles: perpetra-
tor individuals, perpetrator organizations, physi-
cal targets, victims, and weapons.
The ProMed corpus consists of 120 documents
obtained from ProMed-mail2, a freely accessible
global electronic reporting system for outbreaks
of diseases. These 120 documents are paired with
corresponding answer key templates. Unless oth-
erwise noted, all of our experiments on this data
set used 5-fold cross validation. We extracted two
types of event roles: diseases and victims3.
Unlike some other IE data sets, many of the
texts in these collections do not describe a rele-
vant event. Only about half of the MUC-4 arti-
cles describe a specific terrorist incident4, and only
about 80% of the ProMed articles describe a dis-
ease outbreak. The answer keys for the irrelevant
documents are therefore empty. IE systems are es-
pecially susceptible to false hits when they can be
given texts that contain no relevant events.
The complete IE task involves the creation of
answer key templates, one template per incident
(many documents in our data sets describe multi-
ple events). Our work focuses on accurately ex-
tracting the facts from the text and not on tem-
plate generation per se (e.g., we are not concerned
with coreference resolution or which extraction
belongs in which template). Consequently, our ex-
periments evaluate the accuracy of the extractions
individually. We used head noun scoring, where
an extraction is considered to be correct if its head
noun matches the head noun in the answer key.5
2http://www.promedmail.org
3The ?victims? can be people, animals, or plants.
4With respect to the definition of terrorist incidents in the
MUC-4 guidelines (Sundheim, 1992).
5Pronouns were discarded from both the system responses
and the answer keys since we do not perform coreference res-
olution. Duplicate extractions (e.g., the same string extracted
multiple times from the same document) were conflated be-
fore being scored, so they count as just one hit or one miss.
4.2 Baselines
We generated three baselines to use as compar-
isons with our IE system. As our first baseline,
we used AutoSlog-TS (Riloff, 1996), which is a
weakly-supervised, pattern-based IE system avail-
able as part of the Sundance/AutoSlog software
package (Riloff and Phillips, 2004). Our previous
work in event-based IE (Patwardhan and Riloff,
2007) also used a pattern-based approach that ap-
plied semantic affinity patterns to relevant regions
in text. We use this system as our second base-
line. As a third baseline, we trained a Na??ve Bayes
IE classifier that is analogous to the plausible role-
filler recognizer in our unified IE model, except
that this baseline system is not conditioned on the
assumption of having an event sentence. Conse-
quently, this baseline NB classifier is akin to a tra-
ditional supervised learning-based IE system that
uses only local contextual features to make extrac-
tion decisions. Formally, the baseline NB classi-
fier uses the formula:
P (PlausFillr(NP
i
)|F ) =
1
Z
P (PlausFillr(NP
i
))
?
?
f
i
?F
P (f
i
|PlausFillr(NP
i
))
where F is the set of local features,
P (PlausFillr(NP
i
)) is the prior probability,
and Z is the normalizing constant. We used the
Weka (Witten and Frank, 2005) implementation
of Na??ve Bayes for this baseline NB system.
New Jersey, February, 26. An outbreak of Ebola has
been confirmed in Mercer County, New Jersey. Five teenage
boys appear to have contracted the deadly virus from an
unknown source. The CDC is investigating the cases and is
taking measures to prevent the spread. . .
Disease: Ebola
Victims: Five teenage boys
Location: Mercer County, New Jersey
Date: February 26
Figure 1: A Disease Outbreak Event Template
Both the MUC-4 and ProMed data sets have
separate answer keys rather than annotated source
documents. Figure 1 shows an example of a doc-
ument and its corresponding answer key template.
To train the baseline NB system, we identify all
instances of each answer key string in the source
document and consider every instance a positive
training example. This produces noisy training
data, however, because some instances occur in
155
PerpInd PerpOrg Target Victim Weapon
P R F P R F P R F P R F P R F
AutoSlog-TS .33 .49 .40 .52 .33 .41 .54 .59 .56 .49 .54 .51 .38 .44 .41
Sem Affinity .48 .39 .43 .36 .58 .45 .56 .46 .50 .46 .44 .45 .53 .46 .50
NB .50 .36 .34 .35 .35 .46 .40 .53 .49 .51 .50 .50 .50 1.00 .05 .10
NB .70 .41 .25 .31 .43 .31 .36 .58 .42 .48 .58 .37 .45 1.00 .04 .07
NB .90 .51 .17 .25 .56 .15 .24 .67 .30 .41 .75 .23 .36 1.00 .02 .04
Table 1: Baseline Results on MUC-4
Disease Victim
P R F P R F
AutoSlog-TS .33 .60 .43 .36 .49 .41
Sem Affinity .31 .49 .38 .41 .47 .44
NB .50 .20 .73 .31 .29 .56 .39
NB .70 .23 .67 .34 .37 .52 .44
NB .90 .34 .59 .43 .47 .39 .43
Table 2: Baseline Results on ProMed
undesirable contexts. For example, if the string
?man? appears in an answer key as a victim, one
instance of ?man? may refer to the actual vic-
tim in an event sentence, while another instance
of ?man? may occur in a non-event context (e.g.,
background information) or may refer to a com-
pletely different person.
We report three evaluation metrics in our exper-
iments: precision (P), recall (R), and F-score (F),
where recall and precision are equally weighted.
For the Na??ve Bayes classifier, the natural thresh-
old for distinguishing between positive and nega-
tive classes is 0.5, but we also evaluated this clas-
sifier with thresholds of 0.7 and 0.9 to see if we
could effect a recall/precision trade-off. Tables 1
and 2 present the results of our three baseline sys-
tems. The NB classifier performs comparably to
AutoSlog-TS and Semantic Affinity on most event
roles, although a threshold of 0.90 is needed to
reach comparable performance on ProMed. The
relatively low numbers across the board indicate
that these corpora are challenging, but these re-
sults suggest that our plausible role-filler recog-
nizer is competitive with other existing IE sys-
tems. In Section 4.4 we will show how our unified
IE model compares to these baselines. But before
that (in the next section) we evaluate the quality of
the second component of our IE system: the sen-
tential event recognizer.
4.3 Sentential Event Recognizer Models
The sentential event recognizer is one of the core
contributions of this research, so in this section we
evaluate it by itself, before we employ it within the
unified framework. The purpose of the sentential
event recognizer is to determine whether a sen-
tence is discussing a domain-relevant event. For
our data sets, the classifier must decide whether a
sentence is discussing a terrorist incident (MUC-
4) or a disease outbreak (ProMed). Ideally, we
want such a classifier to operate independently
from the answer keys and the extraction task per
se. For example, a terrorism IE system could be
designed to extract only perpetrators and victims
of terrorist events, or it could be designed to ex-
tract only targets and locations. The job of the sen-
tential event recognizer remains the same: to iden-
tify sentences that discuss a terrorist event. How to
train and evaluate such a system is a difficult ques-
tion. In this section, we present two approaches
that we explored to generate the training data: (a)
using the IE answer keys, and (b) using human
judgements.
4.3.1 Sentence Annotation via Answer Keys
We have argued that the event relevance of a sen-
tence should not be tied to a specific set of event
roles. However, the IE answer keys can be used
to identify some sentences that describe an event,
because they contain an answer string. So we can
map the answer strings back to sentences in the
source documents to automatically generate event
sentence annotations.6 These annotations will be
noisy, though, because an answer string can appear
in a non-event sentence, and some event sentences
may not contain any answer strings. The alterna-
tive, however, is sentence annotations by humans,
which (as we will discuss in Section 4.3.2) is chal-
lenging.
4.3.2 Sentence Annotation via Human
Judgements
For many sentences there is a clear consensus
among people that an event is being discussed. For
example, most readers would agree that sentence
(1) below is describing a terrorist event, while sen-
6A similar strategy was used in previous work (Patward-
han and Riloff, 2007) to generate a test set for the evaluation
of a relevant region classifier.
156
Evaluation on Answer Keys Evaluation on Human Annotations
Event Non-Event Event Non-Event
Acc Pr Rec F Pr Rec F Acc Pr Rec F Pr Rec F
MUC-4 (Terrorism)
An
s NB .80 .57 .55 .56 .86 .87 .87 .81 .46 .60 .52 .91 .85 .88
SVM .80 .68 .42 .52 .84 .93 .88 .83 .55 .44 .49 .88 .91 .90
H
u
m NB .82 .64 .48 .55 .85 .92 .88 .85 .56 .57 .57 .91 .91 .91
SVM .79 .64 .41 .50 .83 .91 .87 .84 .62 .51 .56 .90 .91 .91
ProMed (Disease Outbreaks)
An
s NB .75 .62 .61 .61 .81 .82 .82 .72 .43 .58 .50 .86 .77 .81
SVM .74 .78 .31 .44 .74 .95 .83 .76 .51 .26 .35 .80 .92 .86
H
u
m NB .73 .61 .46 .52 .77 .86 .81 .79 .56 .57 .56 .87 .86 .86
SVM .70 .62 .32 .42 .73 .89 .81 .79 .62 .42 .50 .84 .90 .87
Table 3: Sentential Event Recognizers Results (5-fold Cross-Validation)
Evaluation on Human Annotations
Event Non-Event
Acc Pr Rec F Pr Rec F
NB .83 .50 .70 .58 .94 .86 .90
SVM .89 .83 .39 .53 .89 .98 .94
Table 4: Sentential Event Recognizer Results for
MUC-4 using 1300 Documents for Training
tence (2) is not. However it is difficult to draw a
clear line. Sentence (3), for example, describes an
action taken in response to a terrorist event. Is this
a terrorist event sentence? Precisely how to define
an event sentence is not obvious.
(1) Al Qaeda operatives launched an at-
tack on the Madrid subway system.
(2) Madrid has a population of about
3.2 million people.
(3) City officials stepped up security in
response to the attacks.
We tackled this issue by creating detailed an-
notation guidelines to define the notion of an
event sentence, and conducting a human annota-
tion study. The guidelines delineated a general
time frame for the beginning and end of an event,
and constrained the task to focus on specific inci-
dents that were reported in the IE answer key. We
gave the annotators a brief description (e.g., mur-
der in Peru) of each event that had a filled answer
key in the data set. They only labeled sentences
that discussed those particular events.
We employed two human judges, who anno-
tated 120 documents from the ProMed test set,
and 100 documents from the MUC-4 test set. We
asked both judges to label 30 of the same docu-
ments from each data set so that we could compute
inter-annotator agreement. The annotators had an
agreement of 0.72 Cohen?s ? on the ProMed data,
and 0.77 Cohen?s ? on the MUC-4 data. Given
the difficulty of this task, we were satisfied that
this task is reasonably well-defined and the anno-
tations are of good quality.
4.3.3 Event Recognizer Results
We evaluated the two sentential event recognizer
models described in Section 3.2 in two ways:
(1) using the answer key sentence annotations for
training/testing, and (2) using the human annota-
tions for training/testing. Table 3 shows the re-
sults for all combinations of training/testing data.
Since we only have human annotations for 100
MUC-4 texts and 120 ProMed texts, we performed
5-fold cross-validation on these documents. For
our classifiers, we used the Weka (Witten and
Frank, 2005) implementation of Na??ve Bayes and
the SVMLight (Joachims, 1998) implementation
of the SVM. For each classifier we report overall
accuracy, and precision, recall and F-scores with
respect to both the positive and negative classes
(event vs. non-event sentences).
The rows labeled Ans show the results for mod-
els trained via answer keys, and the rows labeled
Hum show the results for the models trained with
human annotations. The left side of the table
shows the results using the answer key annotations
for evaluation, and the right side of the table shows
the results using the human annotations for evalua-
tion. One expects classifiers to perform best when
they are trained and tested on the same type of
data, and our results bear this out ? the classifiers
that were trained and tested on the same kind of
annotations do best. The boldfaced numbers rep-
resent the best accuracies achieved for each do-
main. As we would expect, the classifiers that are
both trained and tested with human annotations
(Hum) show the best performance, with the Na??ve
Bayes achieving the best accuracy of 85% on the
157
PerpInd PerpOrg Target Victim Weapon
P R F P R F P R F P R F P R F
AutoSlog-TS .33 .49 .40 .52 .33 .41 .54 .59 .56 .49 .54 .51 .38 .44 .41
Sem Affinity .48 .39 .43 .36 .58 .45 .56 .46 .50 .46 .44 .45 .53 .46 .50
NB (baseline) .36 .34 .35 .35 .46 .40 .53 .49 .51 .50 .50 .50 1.00 .05 .10
GLACIER
NB/NB .90 .39 .59 .47 .33 .51 .40 .39 .72 .51 .52 .54 .53 .47 .55 .51
NB/SVM .40 .51 .58 .54 .34 .45 .38 .42 .72 .53 .55 .58 .56 .57 .53 .55
NB/SVM .50 .66 .47 .55 .41 .26 .32 .50 .62 .55 .62 .36 .45 .64 .43 .52
Table 5: Unified IE Model on MUC-4
MUC-4 texts, and the SVM achieving the best ac-
curacy of 79% on the ProMed texts.
The recall and precision for non-event sentences
is much higher than for event sentences. This clas-
sifier is forced to draw a hard line between the
event and non-event sentences, which is a difficult
task even for people. One of the advantages of our
unified IE model, which will be described in the
next section, is that it does not require hard deci-
sions but instead uses a probabilistic estimate of
how ?event-ish? a sentence is.
Table 3 showed that models trained on human
annotations outperform models trained on answer
key annotations. But with the MUC-4 data, we
have the luxury of 1300 training documents with
answer keys, while we only have 100 documents
with human annotations. Even though the answer
key annotations are noisier, we have 13 times as
much training data.
So we trained another sentential event recog-
nizer using the entire MUC-4 training set. These
results are shown in Table 4. Observe that using
this larger (albeit noisy) training data does not ap-
pear to affect the Na??ve Bayes model very much.
Compared with the model trained on 100 manu-
ally annotated documents, its accuracy decreases
by 2% from 85% to 83%. The SVM model, on
the other hand, achieves an 89% accuracy when
trained with the larger MUC-4 training data, com-
pared to 84% accuracy for the model trained from
the 100 manually labeled documents. Conse-
quently, the sentential event recognizer models
used in our unified IE framework (described in
Section 4.4) are trained with this 1300 document
training set.
4.4 Evaluation of the Unified IE Model
We now evaluate the performance of our unified IE
model, GLACIER, which allows a plausible role-
filler recognizer and a sentential event recognizer
to make joint decisions about phrase extractions.
Tables 5 and 6 present the results of the unified
Disease Victim
P R F P R F
AutoSlog-TS .33 .60 .43 .36 .49 .41
Sem Affinity .31 .49 .38 .41 .47 .44
NB (baseline) .34 .59 .43 .47 .39 .43
GLACIER
NB/NB .90 .41 .61 .49 .38 .52 .44
NB/SVM .40 .31 .66 .42 .32 .55 .41
NB/SVM .50 .38 .54 .44 .42 .47 .44
Table 6: Unified IE Model on ProMed
IE model on the MUC-4 and ProMed data sets.
The NB/NB systems use Na??ve Bayes models for
both components, while the NB/SVM systems use
a Na??ve Bayes model for the plausible role-filler
recognizer and an SVM for the sentential event
recognizer. As with our baseline system, we ob-
tain good results using a threshold of 0.90 for our
NB/NB model (i.e., only NPs with probability ?
0.90 are extracted). For our NB/SVM models, we
evaluated using the default threshold (0.50) but ob-
served that recall was sometimes low. So we also
use a threshold of 0.40, which produces superior
results. Here too, we used the Weka (Witten and
Frank, 2005) implementation of the Na??ve Bayes
model and the SVMLight (Joachims, 1998) imple-
mentation of the SVM.
For the MUC-4 data, our unified IE model us-
ing the SVM (0.40) outperforms all 3 baselines
on three roles (PerpInd, Victim, Weapon) and
outperforms 2 of the 3 baselines on the Target
role. When GLACIER outperforms the other sys-
tems it is often by a wide margin: the F-score
for PerpInd jumped from 0.43 for the best base-
line (Sem Affinity) to 0.54 for GLACIER, and the
F-scores for Victim and Weapon each improved
by 5% over the best baseline. These gains came
from both increased recall and increased precision,
demonstrating that GLACIER extracts some infor-
mation that was missed by the other systems and
is also less prone to false hits.
Only the PerpOrg role shows inferior per-
formance. Organizations perpetrating a terrorist
158
event are often discussed later in a document, far
removed from the main event description. For ex-
ample, a statement that Al Qaeda is believed to
be responsible for an attack would typically ap-
pear after the event description. As a result, the
sentential event recognizer tends to generate low
probabilities for such sentences. We believe that
addressing this issue would require the use of dis-
course relations or the use of even larger context
sizes. We intend to explore these avenues of re-
search in future work.
On the ProMed data, GLACIER produces results
that are similar to the baselines for theVictim role,
but it outperforms the baselines for the Disease
role. We find that for this domain, the unified IE
model with the Na??ve Bayes sentential event rec-
ognizer is superior to the unified IE model with
the SVM classifier. For the Disease role, the F-
score jumped 6%, from 0.43 for the best base-
line systems (AutoSlog-TS and the NB baseline)
to 0.49 for GLACIER
NB/NB
. In contrast to the
MUC-4 data, this improvement was mostly due
to an increase in precision (up to 0.41), indicating
that our unified IE model was effective at elimi-
nating many false hits. For the Victim role, the
performance of the unified model is comparable
to the baselines. On this event role, the F-score
of GLACIER
NB/NB
(0.44) matches that of the best
baseline system (Sem Affinity, with 0.44). How-
ever, note that GLACIER
NB/NB
can achieve a 5%
gain in recall over this baseline, at the cost of a 3%
precision loss.
4.5 Specific Examples
Figure 2 presents some specific examples of ex-
tractions that are failed to be extracted by the
baseline models, but are correctly identified by
GLACIER because of its use of sentential evidence.
Observe that in each of these examples, GLACIER
correctly extracts the underlined phrases, in spite
of the inconclusive evidence in the local contexts
around them. In the last sentence in Figure 2, for
example, GLACIER correctly makes the inference
that the policemen in the bus (which was traveling
on the bridge) are likely the victims of the terrorist
event. Thus, we see that our system manages to
balance the influence of the two probability com-
ponents to make extraction decisions that would
be impossible to make by relying only on the local
phrasal context. In addition, the sentential event
recognizer can also help improve precision by pre-
THE MNR REPORTED ON 12 JANUARY THAT HEAVILY
ARMED MEN IN CIVILIAN CLOTHES HAD INTERCEPTED
A VEHICLE WITH OQUELI AND FLORES ENROUTE FOR
LA AURORA AIRPORT AND THAT THE TWO POLITICAL
LEADERS HAD BEEN KIDNAPPED AND WERE REPORTED
MISSING.
PerpInd: HEAVILY ARMED MEN
THE SCANT POLICE INFORMATION SAID THAT THE
DEVICES WERE APPARENTLY LEFT IN FRONT OF THE TWO
BANK BRANCHES MINUTES BEFORE THE CURFEW BEGAN
FOR THE 6TH CONSECUTIVE DAY ? PRECISELY TO
COUNTER THE WAVE OF TERRORISM CAUSED BY DRUG
TRAFFICKERS.
Weapon: THE DEVICES
THOSE WOUNDED INCLUDE THREE EMPLOYEES OF THE
GAS STATION WHERE THE CAR BOMB WENT OFF AND
TWO PEOPLE WHO WERE WALKING BY THE GAS STATION
AT THE MOMENT OF THE EXPLOSION.
Victim: THREE EMPLOYEES OF THE GAS STATION
Victim: TWO PEOPLE
MEMBERS OF THE BOMB SQUAD HAVE DEACTIVATED
A POWERFUL BOMB PLANTED AT THE ANDRES AVELINO
CACERES PARK, WHERE PRESIDENT ALAN GARCIA WAS
DUE TO PARTICIPATE IN THE COMMEMORATION OF THE
BATTLE OF TARAPACA.
Victim: PRESIDENT ALAN GARCIA
EPL [POPULAR LIBERATION ARMY] GUERRILLAS BLEW
UP A BRIDGE AS A PUBLIC BUS, IN WHICH SEVERAL
POLICEMEN WERE TRAVELING, WAS CROSSING IT.
Victim: SEVERAL POLICEMEN
Figure 2: Examples of GLACIER Extractions
venting extractions from non-event sentences.
5 Conclusions
We presented a unified model for IE that balances
the influence of sentential context with local con-
textual evidence to improve the performance of
event-based IE. Our experimental results showed
that using sentential contexts indeed produced bet-
ter results on two IE data sets. Our current model
uses supervised learning, so one direction for fu-
ture work is to adapt the model for weakly super-
vised learning. We also plan to incorporate dis-
course features and investigate even wider con-
texts to capture broader discourse effects.
Acknowledgments
This work has been supported in part by the De-
partment of Homeland Security Grant N0014-07-
1-0152. We are grateful to Nathan Gilbert and
Adam Teichert for their help with the annotation
of event sentences.
159
References
R. Bunescu and R. Mooney. 2004. Collective In-
formation Extraction with Relational Markov Net-
works. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 438?445, Barcelona, Spain, July.
M. Califf and R. Mooney. 2003. Bottom-Up Rela-
tional Learning of Pattern Matching Rules for In-
formation Extraction. Journal of Machine Learning
Research, 4:177?210, December.
H. Chieu, H. Ng, and Y. Lee. 2003. Closing the
Gap: Learning-Based Information Extraction Rival-
ing Knowledge-Engineering Methods. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 216?223, Sap-
poro, Japan, July.
P. Domingos and M. Pazzani. 1996. Beyond Inde-
pendence: Conditions for the Optimality of the Sim-
ple Bayesian Classifier. In Proceedings of the Thir-
teenth International Conference on Machine Learn-
ing, pages 105?112, Bari, Italy, July.
J. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating Non-local Information into Information
Extraction Systems by Gibbs Sampling. In Proceed-
ings of the 43rd Annual Meeting of the Association
for Computational Linguistics, pages 363?370, Ann
Arbor, MI, June.
D. Freitag and A. McCallum. 2000. Informa-
tion Extraction with HMM Structures Learned by
Stochastic Optimization. In Proceedings of the Sev-
enteenth National Conference on Artificial Intelli-
gence, pages 584?589, Austin, TX, August.
D. Freitag. 1998. Toward General-Purpose Learning
for Information Extraction. In Proceedings of the
36th Annual Meeting of the Association for Compu-
tational Linguistics and 17th International Confer-
ence on Computational Linguistics, pages 404?408,
Montreal, Quebec, August.
H. Ji and R. Grishman. 2008. Refining Event Ex-
traction through Cross-Document Inference. In Pro-
ceedings of ACL-08: HLT, pages 254?262, Colum-
bus, OH, June.
T. Joachims. 1998. Text Categorization with Sup-
port Vector Machines: Learning with Many Rele-
vant Features. In Proceedings of the Tenth European
Conference on Machine Learning, pages 137?142,
April.
C. Manning, P. Raghavan, and H Schu?tze. 2008. Intro-
duction to Information Retrieval. Cambridge Uni-
versity Press, New York, NY.
M. Maslennikov and T. Chua. 2007. A Multi-
resolution Framework for Information Extraction
from Free Text. In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics, pages 592?599, Prague, Czech Republic,
June.
S. Patwardhan and E. Riloff. 2007. Effective Informa-
tion Extraction with Semantic Affinity Patterns and
Relevant Regions. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 717?727, Prague, Czech Re-
public, June.
W. Phillips and E. Riloff. 2007. Exploiting Role-
Identifying Nouns and Expressions for Informa-
tion Extraction. In Proceedings of International
Conference on Recent Advances in Natural Lan-
guage Processing, pages 165?172, Borovets, Bul-
garia, September.
E. Riloff and W. Phillips. 2004. An Introduction to the
Sundance and AutoSlog Systems. Technical Report
UUCS-04-015, School of Computing, University of
Utah.
E. Riloff. 1996. Automatically Generating Extraction
Patterns from Untagged Text. In Proceedings of the
Thirteenth National Conference on Articial Intelli-
gence, pages 1044?1049, Portland, OR, August.
S. Soderland, D. Fisher, J. Aseltine, and W. Lehnert.
1995. CRYSTAL: Inducing a Conceptual Dictio-
nary. In Proceedings of the Fourteenth International
Joint Conference on Artificial Intelligence, pages
1314?1319, Montreal, Canada, August.
B. Sundheim. 1992. Overview of the Fourth Message
Understanding Evaluation and Conference. In Pro-
ceedings of the Fourth Message Understanding Con-
ference (MUC-4), pages 3?21, McLean, VA, June.
V. Vapnik. 1995. The Nature of Statistical Learning
Theory. Springer, New York, NY.
I. Witten and E. Frank. 2005. Data Mining - Practical
Machine Learning Tools and Techniques. Morgan?
Kaufmann, San Francisco, CA.
R. Yangarber, R. Grishman, P. Tapanainen, and S. Hut-
tunen. 2000. Automatic Acquisition of Domain
Knowledge for Information Extraction. In Proceed-
ings of the 18th International Conference on Com-
putational Linguistics, pages 940?946, Saarbru?cken,
Germany, August.
B. Zadrozny and C. Elkan. 2001. Obtaining Cal-
ibrated Probability Estimates from Decision Trees
and Na??ve Bayesian Classiers. In Proceedings of the
Eighteenth International Conference on Machine
Learning, pages 609?616, Williamstown, MA, June.
160
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 948?957,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Toward Completeness in Concept Extraction and Classification
Eduard Hovy and Zornitsa Kozareva
USC Information Sciences Institute
4676 Admiralty Way
Marina del Rey, CA 90292
hovy@isi.edu, zkozareva@gmail.com
Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT 84112
riloff@cs.utah.edu
Abstract
Many algorithms extract terms from text to-
gether with some kind of taxonomic clas-
sification (is-a) link. However, the general
approaches used today, and specifically the
methods of evaluating results, exhibit serious
shortcomings. Harvesting without focusing on
a specific conceptual area may deliver large
numbers of terms, but they are scattered over
an immense concept space, making Recall
judgments impossible. Regarding Precision,
simply judging the correctness of terms and
their individual classification links may pro-
vide high scores, but this doesn?t help with the
eventual assembly of terms into a single coher-
ent taxonomy. Furthermore, since there is no
correct and complete gold standard to measure
against, most work invents some ad hoc evalu-
ation measure. We present an algorithm that is
more precise and complete than previous ones
for identifying from web text just those con-
cepts ?below? a given seed term. Comparing
the results to WordNet, we find that the algo-
rithm misses terms, but also that it learns many
new terms not in WordNet, and that it clas-
sifies them in ways acceptable to humans but
different from WordNet.
1 Collecting Information with Care
Over the past few years, many algorithms have been
published on automatically harvesting terms and
their conceptual types from the web and/or other
large corpora (Etzioni et al, 2005; Pasca, 2007;
Banko et al, 2007; Yi and Niblack, 2005; Snow et
al., 2005). But several basic problems limit the even-
tual utility of the results.
First, there is no standard collection of facts
against which results can be measured. As we show
in this paper, WordNet (Fellbaum, 1998), the most
obvious contender because of its size and popularity,
is deficient in various ways: it is neither complete
nor is its taxonomic structure inarguably perfect. As
a result, alternative ad hoc measures are invented
that are not comparable. Second, simply harvesting
facts about an entity without regard to its actual sub-
sequent organization inflates Recall and Precision
evaluation scores: while it is correct that a jaguar
is a animal, mammal, toy, sports-team, car-make,
and operating-system, this information doesn?t help
to create a taxonomy that, for example, places mam-
mal and animal closer to one another than to some
of the others. ((Snow et al, 2005) is an exception
to this.) As a result, this work may give a mislead-
ing sense of progress. Third, entities are of differ-
ent formal types, and their taxonomic treatment is
consequently different: some are at the level of in-
stances (e.g., Michelangelo was a painter) and some
at the level of concepts (e.g., a painter is a human).
The goal of our research is to learn terms for en-
tities (objects) and their taxonomic organization si-
multaneously, from text. Our method is to use a
single surface-level pattern with several open posi-
tions. Filling them in different ways harvests differ-
ent kinds of information, and/or confirms this infor-
mation. We evaluate in two ways: against WordNet,
since that is a commonly available and popular re-
source, and also by asking humans to judge the re-
sults since WordNet is neither complete nor exhaus-
tively taxonomized.
In this paper, we describe experiments with two
rich and common portions of an entity taxonomy:
Animals and People. The claim of this paper is: It is
possible to learn terms automatically to populate a
targeted portion of a taxonomy (such as below An-
948
imals or People) both at high precision compared
to WordNet and including additional correct ones as
well. We would like to also report on Recall rela-
tive to WordNet, but given the problems described
in Section 4, this turns out to be much harder than
would seem.
First, we need to define some basic terminology:
term: An English word (for our current purposes, a
noun or a proper name).
seed term: A word we use to initiate the algorithm.
concept: An item in the classification taxonomy we
are building. A concept may correspond to several
terms (singular form, plural form, the term?s syn-
onyms, etc.).
root concept: A concept at a fairly general (high)
level in the taxonomy, to which many others are
eventually learned to be subtypes/instances of.
basic-level concept: A concept at the ?basic level?,
corresponding approximately to the Basic Level cat-
egories defined in Prototype Theory in Psychology
(Rosch, 1978). For our purposes, a concept corre-
sponding to the (proto)typical level of generality of
its type; that is, a dog, not a mammal or a dachshund;
a singer, not a human or an opera diva.
instance: An item in the classification taxonomy
that is more specific than a concept; only one exam-
ple of the instance exists in ?the real world? at any
time. For example, Michelangelo is an instance, as
well as Mazda Miata with license plate 3HCY687,
while Mazda Miata is not.
classification link: We use a single relation, that,
depending on its arguments, is either is a type of
(when both arguments are concepts), or is an in-
stance of or is an example of (when the first argu-
ment is an instance/example of the second).
Section 2 describes our method for harvesting;
Section 3 discusses related work; and Section 4 de-
scribes the experiments and the results.
2 Term and Relation Extraction using the
Doubly-Anchored Pattern
Our goal is to develop a technique that automatically
?fills in? the concept space in the taxonomy below
any root concept, by harvesting terms through re-
peated web queries. We perform this in two alter-
nating stages.
Stage 1: Basic-level/Instance concept collec-
tion: We use the Doubly-Anchored Pattern DAP de-
veloped in (Kozareva et al, 2008):
DAP: [SeedTerm1] such as [SeedTerm2] and <X>
which learns a list of basic-level concepts or in-
stances (depending on whether SeedTerm2 ex-
presses a basic-level concept or an instance).1 DAP
is very reliable because it is instantiated with ex-
amples at both ?ends? of the space to be filled (the
higher-level (root) concept SeedTerm1 and a basic-
level term or instance (SeedTerm2)), which mutu-
ally disambiguate each other. For example, ?pres-
idents? for SeedTerm1 can refer to the leader of a
country, corporation, or university, and ?Ford? for
SeedTerm2 can refer to a car company, an automo-
bile pioneer, or a U.S. president. But when the two
terms co-occur in a text that matches the pattern
?Presidents such as Ford and <X>?, the text will
almost certainly refer to country presidents.
The first stage involves a series of repeated re-
placements of SeedTerm2 by newly-learned terms
in order to generate even more seed terms. That is,
each new basic-level concept or instance is rotated
into the pattern (becoming a new SeedTerm2) in a
bootstrapping cycle that Kozareva et al called reck-
less bootstrapping. This procedure is implemented
as exhaustive breadth-first search, and iterates until
no new terms are harvested. The harvested terms are
incorporated in a Hyponym Pattern Linkage Graph
(HPLG) G = (V,E), where each vertex v ? V is
a candidate term and each edge (u, v) ? E indi-
cates that term v was generated by term u. A term
u is ranked by Out-Degree(u) =
P
?(u,v)?E
w(u,v)
|V |?1
,
which represents the weighted sum of u?s outgoing
edges normalized by the total number of other nodes
in the graph. Intuitively, a term ranks highly if it
is frequently discovering many different terms dur-
ing the reckless bootstrapping cycle. This method is
very productive, harvesting a constant stream of new
terms for basic-level concepts or instances when the
taxonomy below the initial root concept SeedTerm1
is extensive (such as for Animals or People).
1Strictly speaking, our lowest-level concepts can be in-
stances, basic-level concepts, or concepts below the basic level
(e.g., dachsund). But for the sake of simplicity we will refer to
our lowest-level terms as basic-level concepts and instances.
949
Stage 2: Intermediate level concept collection:
Going beyond (Kozareva et al, 2008), we next apply
the Doubly-Anchored Pattern in the ?backward? di-
rection (DAP?1), for any two seed terms represent-
ing basic-level concepts or instances:
DAP?1: <X> such as [SeedTerm1] and [SeedTerm2]
which harvests a set of concepts, most of them inter-
mediate between the basic level or instance and the
initial higher-level seed.
This second stage (DAP?1) has not yet been de-
scribed in the literature. It proceeds analogously.
For pairs of basic-level concepts or instances be-
low the root concept that were found during the first
stage, we instantiate DAP?1 and issue a new web
query. For example, if the term ?cats? was harvested
by DAP in ?Animals such as dogs and <X>?, then
the pair < dogs, cats > forms the new Web query
?<X> such as dogs and cats?. We extract up to 2
consecutive nouns from the <X> position.
This procedure yields a large number of discov-
ered concepts, but they cannot all be used for fur-
ther bootstrapping. In addition to practical limita-
tions (such as limits on web querying), many of them
are too general?more general than the initial root
concept?and could derail the bootstrapping process
by introducing terms that stray every further away
from the initial root concept. We therefore rank the
harvested terms based on the likelihood that they
will be productive if they are expanded in the next
cycle. Ranking is based on two criteria: (1) the con-
cept should be prolific (i.e., produce many lower-
level concepts) in order to keep the bootstrapping
process energized, and (2) the concept should be
subordinate to the root concept, so that the process
stays within the targeted part of the search space.
To perform ranking, we incorporate both the har-
vested concepts and the basic-level/instance pairs
into a Hypernym Relation Graph (HRG), which we
define as a bipartite graph HRG = (V,E) with two
types of vertices. One set of vertices represents the
concepts (the category vertices (V
c
), and a second
set of vertices represents the basic-level/instance
pairs that produced the concepts (the member pair
vertices (V
mp
)). We create an edge e(u, v) ? E
between u ? V
c
and v ? V
mp
when the con-
cept represented by u was harvested by the basic-
level/instance pair represented by v, with the weight
of the edge defined as the number of times that the
lower pair found the concept on the web.
We use the Hypernym Relation Graph to rank
the intermediate concepts based on each node?s In-
Degree, which is the sum of the weights on the
node?s incoming edges. Formally, In-Degree(u) =
?
?(u,v)?E
w(u, v). Intuitively, a concept will be
ranked highly if it was harvested by many different
combinations of basic-level/instance terms.
However, this scoring function does not deter-
mine whether a concept is more or less general than
the initial root concept. For example, when har-
vesting animal categories, the system may learn the
word ?species?, which is a very common term asso-
ciated with animals, but also applies to non-animals
such as plants. To prevent the inclusion of over-
general terms and constrain the search to remain
?below? the root concept, we apply a Concept Posi-
tioning Test (CPT): We issue the following two web
queries:
(a) Concept such as RootConcept and <X>
(b) RootConcept such as Concept and <X>
If (b) returns more web hits than (a), then the con-
cept passes the test, otherwise it fails. The first (most
highly ranked) concept that passes CPT becomes the
new seed concept for the next bootstrapping cycle.
In principle, we could use all the concepts that pass
the CPT for bootstrapping2. However, for practical
reasons (primarily limitations on web querying), we
run the algorithm for 10 iterations.
3 Related Work
Many algorithms have been developed to automat-
ically acquire semantic class members using a va-
riety of techniques, including co-occurrence statis-
tics (Riloff and Shepherd, 1997; Roark and Char-
niak, 1998), syntactic dependencies (Pantel and
Ravichandran, 2004), and lexico-syntactic patterns
(Riloff and Jones, 1999; Fleischman and Hovy,
2002; Thelen and Riloff, 2002).
The work most closely related to ours is that of
(Hearst, 1992) who introduced the idea of apply-
ing hyponym patterns to text, which explicitly iden-
tify a hyponym relation between two terms (e.g.,
2The number of ranked concepts that pass CPT changes in
each iteration. Also, the wildcard * is important for counts, as
can be verified with a quick experiment using Google.
950
?such authors as <X>?). In recent years, sev-
eral researchers have followed up on this idea using
the web as a corpus. (Pasca, 2004) applies lexico-
syntactic hyponym patterns to the Web and use the
contexts around them for learning. KnowItAll (Et-
zioni et al, 2005) applies the hyponym patterns to
extract instances from the Web and ranks them by
relevance using mutual information. (Kozareva et
al., 2008) introduced a bootstrapping scheme using
the doubly-anchored pattern (DAP) that is guided
through graph ranking. This approach reported a
significant improvement from 5% to 18% over ap-
proaches using singly-anchored patterns like those
of (Pasca, 2004) and (Etzioni et al, 2005).
(Snow et al, 2005) describe a dependency path
based approach that generates a large number of
weak hypernym patterns using pairs of noun phrases
present in WordNet. They build a classifier using
the different hypernym patterns and find among the
highest precision patterns those of (Hearst, 1992).
Snow et al report performance of 85% precision
at 10% recall and 25% precision at 30% recall for
5300 hand-tagged noun phrase pairs. (McNamee et
al., 2008) use the technique of (Snow et al, 2005)
to harvest the hypernyms of the proper names. The
average precision on 75 automatically detected cat-
egories is 53%. The discovered hypernyms were
intergrated in a Question Answering system which
showed an improvement of 9% when evaluated on a
TREC Question Answering data set.
Recently, (Ritter et al, 2009) reported hypernym
learning using (Hearst, 1992) patterns and manually
tagged common and proper nouns. All hypernym
candidates matching the pattern are acquired, and
the candidate terms are ranked by mutual informa-
tion. However, they evaluate the performance of
their hypernym algorithm by considering only the
top 5 hypernyms given a basic-level concept or in-
stance. They report 100% precision at 18% recall,
and 66% precision at 72% recall, considering only
the top-5 list. Necessarily, using all the results re-
turned will result in lower precision scores. In con-
trast to their approach, our aim is to first acquire au-
tomatically with minimal supervision the basic-level
concepts for given root concept. Thus, we almost
entirely eliminate the need for humans to provide
hyponym seeds. Second, we evaluate the perfor-
mance of our approach not by measuring the top-
ranked 5 hypernyms given a basic-level concept, but
considering all harvested hypernyms of the concept.
Unlike (Etzioni et al, 2005), (Pasca, 2007) and
(Snow et al, 2005), we learn both instances and con-
cepts simultaneously.
Some researchers have also worked on reorga-
nizing, augmenting, or extending semantic concepts
that already exist in manually built resources such
as WordNet (Widdows and Dorow, 2002; Snow et
al., 2005) or Wikipedia (Ponzetto and Strube, 2007).
Work in automated ontology construction has cre-
ated lexical hierarchies (Caraballo, 1999; Cimiano
and Volker, 2005; Mann, 2002), and learned seman-
tic relations such as meronymy (Berland and Char-
niak, 1999; Girju et al, 2003).
4 Evaluation
The root concepts discussed in this paper are An-
imals and People, because they head large taxo-
nomic structures that are well-represented in Word-
Net. Throughout these experiments, we used as the
initial SeedTerm2 lions for Animals and Madonna
for People (by specifically choosing a proper name
for People we force harvesting down to the level of
individual instances). To collect data, we submitted
the DAP patterns as web queries to Google, retrieved
the top 1000 web snippets per query, and kept only
the unique ones. In total, we collected 1.1 GB of
snippets for Animals and 1.5 GB for People. The
algorithm was allowed to run for 10 iterations.
The algorithm learns a staggering variety of terms
that is much more diverse than we had antici-
pated. In addition to many basic-level concepts or
instances, such as dog and Madonna respectively,
and many intermediate concepts, such as mammals,
pets, and predators, it also harvested categories that
clearly seemed useful, such as laboratory animals,
forest dwellers, and endangered species. Many other
harvested terms were more difficult to judge, includ-
ing bait, allergens, seafood, vectors, protein, and
pests. While these terms have an obvious relation-
ship to Animals, we have to determine whether they
are legitimate and valuable subconcepts of Animals.
A second issue involves relative terms that are
hard to define in an absolute sense, such as native
animals and large mammals.
A complete evaluation should answer the following
three questions:
951
? Precision: What is the correctness of the har-
vested concepts? (How many of them are sim-
ply wrong, given the root concept?)
? Recall: What is the coverage of the harvested
concepts? (How many are missing, below a
given root concept?)
? How correct is the taxonomic structure
learned?
Given the number and variety of terms obtained,
we initially decided that an automatic evaluation
against existing resources (such as WordNet or
something similar) would be inadequate because
they do not contain many of our harvested terms,
even though many of these terms are clearly sensi-
ble and potentially valuable. Indeed, the whole point
of our work is to learn concepts and taxonomies that
go above and beyond what is currently available.
However, it is necessary to compare with
something, and it is important not to skirt the issue
by conducting evaluations that measure subsets of
results, or that perhaps may mislead. We therefore
decided to compare our results against WordNet and
to have human annotators judge as many results as
we could afford (to obtain a measure of Precision
and the legitimate extensions beyond WordNet).
Unfortunately, it proved impossible to measure
Recall against WordNet, because this requires as-
certaining the number of synsets in WordNet be-
tween the root and its basic-level categories. This
requires human judgment, which we could not af-
ford. We plan to address this question in future
work. Also, assessing the correctness of the learned
taxonomy structure requires the manual assessment
of each classification link proposed by the system
that is not already in WordNet, a task also beyond
our budget to complete in full. Some results?for
just basic-level terms and intermediate concepts, but
not among intermediate-level concepts?are shown in
Section 4.3.
We provide Precision scores using the following
measures, where terms refers to the harvested terms:
Pr
WN
=
#terms found in WordNet
#terms harvested by system
Pr
H
=
#terms judged correct by human
#terms harvested by system
NotInWN = #terms judged correct by human but
not in WordNet
We conducted three sets of experiments. Ex-
periment 1 evaluates the results of using DAP to
learn basic-level concepts for Animals and instances
for People. Experiment 2 evaluates the results of
using DAP?1 to harvest intermediate concepts be-
tween each root concept and its basic-level concepts
or instances. Experiment 3 evaluates the taxonomy
structure that is produced via the links between the
instances and intermediate concepts.
4.1 Experiment 1: Basic-Level Concepts and
Instances
In this section we discuss the results of harvest-
ing the basic-level Animal concepts and People in-
stances. The bootstrapping algorithm ranks the har-
vested terms by their Out-Degree score and consid-
ers as correct only those with Out-Degree > 0. In
ten iterations, the bootstrapping algorithm produced
913 Animal basic-level concepts and 1, 344 People
instances that passed this Out-Degree criterion.
4.1.1 Human Evaluation
The harvested terms were labeled by human
judges as either correct or incorrect with respect to
the root concept. Table 1 shows the Precision of the
top-ranked N terms, with N shown in increments
of 100. Overall, the Animal terms yielded 71%
(649/913) Precision and the People terms yielded
95% Precision (1,271/1,344). Figure 1 shows that
higher-ranked Animal terms are more accurate than
lower-ranked terms, which indicates that the scor-
ing function did its job. For People terms, accuracy
was very high throughout the ranked list. Overall,
these results show that the bootstrapping algorithm
generates a large number of correct instances of high
quality.
4.1.2 WordNet Evaluation
Table 1 shows a comparison of the harvested
terms against the terms present in WordNet.
Note that the Precision measured against WordNet
(Pr
WN
) for People is dramatically different from
the Precision based on human judgments (Pr
H
).
This can be explained by looking at the NotInWN
column, which shows that 48 correct Animal terms
952
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 100  200  300  400  500  600  700  800  900
Pr
ec
is
io
n
Rank
Animal Basic-level Concepts
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 200  400  600  800  1000  1200
Pr
ec
is
io
n
Rank
People Instances
Figure 1: Ranked Basic-Concepts and Instances.
and 986 correct People instances are not present in
WordNet (primarily, for people, because WordNet
contains relatively few proper names). These results
show that there is substantial room for improvement
in WordNet?s coverage of these categories. For Ani-
mals, the precision measured against WordNet is ac-
tually higher than the precision measured by human
judges, which may indicate that the judges failed to
recognize some correct animal terms.
Pr
WN
Pr
H
NotInWN
Animal .79 .71 48
People .23 .95 986
Table 1: Instance Evaluation.
4.1.3 Evaluation against Prior Work
To assess how well our algorithm compares with
previous semantic class learning methods, we com-
pared our results to those of (Kozareva et al, 2008).
Our work was inspired by that approach?in fact, we
use that previous algorithm as the first step of our
bootstrapping process. The novelty of our approach
is the insertion of an additional bootstrapping stage
that iteratively learns new intermediate concepts us-
ing DAP?1 and the Concept Positioning Test, fol-
lowed by the subsequent use of the newly learned
intermediate concepts in DAP to expand the search
space beyond the original root concept. This leads
to the discovery of additional basic-level terms or in-
stances, which are then recycled in turn to discover
new intermediate concepts, and so on.
Consequently, we can compare the results pro-
duced by the first iteration of our algorithm (be-
fore intermediate concepts are learned) to those of
(Kozareva et al, 2008) for the Animal and People
categories, and then compare again after 10 boot-
strapping iterations of intermediate concept learn-
ing. Figure 2 shows the number of harvested con-
cepts for Animals and People after each bootstrap-
ping iteration. Bootstrapping with intermediate con-
cepts produces nearly 5 times as many basic-level
concepts and instances than (Kozareva et al, 2008)
obtain, while maintaining similar levels of precision.
The intermediate concepts help so much because
they steer the learning process into new (yet still cor-
rect) regions of the search space after each iteration.
For instance, in the first iteration, the pattern ?ani-
mals such as lions and *? harvests about 350 basic-
level concepts, but only animals that are mentioned
in conjunction with lions are learned. Of these, an-
imals typically quite different from lions, such as
grass-eating kudu, are often not discovered.
However, in the second iteration, the intermediate
concept Herbivore is chosen for expansion. The pat-
tern ?herbivore such as antelope and *? discovers
many additional animals, including kudu, that co-
occur with antelope but do not co-occur with lions.
Table 2 shows examples of the 10 top-ranked
basic-level concepts and instances that were learned
for 3 randomly-selected intermediate Animal and
People concepts (IConcepts) that were acquired dur-
ing bootstrapping. In the next section, we present an
953
 0
 500
 1000
 1500
 2000
 2500
 3000
 3500
 1  2  3  4  5  6  7  8  9  10
#I
te
m
s 
Le
ar
ne
d
Iterations
Animal Intermediate Concepts
Animal Basic-level Concepts
 0
 500
 1000
 1500
 2000
 2500
 3000
 3500
 4000
 1  2  3  4  5  6  7  8  9  10
#I
te
m
s 
Le
ar
ne
d
Iterations
People Intermediate Concepts
People Instances
Figure 2: Learning Curves.
evaluation of the intermediate concept terms.
4.2 Experiment 2: Intermediate Concepts
In this section we discuss the results of harvesting
the intermediate-level concepts. Given the variety of
the harvested results, manual judgment of correct-
ness required an in-depth human annotation study.
We also compare our harvested results against the
concept terms in WordNet.
4.2.1 Human Evaluation
We hired 4 annotators (undergraduates at a dif-
ferent institution) to judge the correctness of the in-
termediate concepts. We created detailed annota-
tion guidelines that define 14 annotation labels for
each of the Animal and People classes, as shown
in Table 3. The labels are clustered into 4 major
PEOPLE
IConcept Instances
Dictators: Adolf Hitler, Joseph Stalin, Benito Mussolini, Lenin,
Fidel Castro, Idi Amin, Slobodan Milosevic,
Hugo Chavez, Mao Zedong, Saddam Hussein
Celebrities: Madonna, Paris Hilton, Angelina Jolie, Britney ,
Spears, Tom Cruise, Cameron Diaz, Bono,
Oprah Winfrey, Jennifer Aniston, Kate Moss
Writers: William Shakespeare, James Joyce, Charles Dickens,
Leo Tolstoy, Goethe, Ralph Waldo Emerson,
Daniel Defoe, Jane Austen, Ernest Hemingway,
Franz Kafka
ANIMAL
IConcept Basic-level Terms
Crustacean: shrimp, crabs, prawns, lobsters, crayfish, mysids,
decapods, marron, ostracods, yabbies
Primates: baboons, monkeys, chimpanzees, apes, marmosets,
chimps, orangutans, gibbons, tamarins, bonobos
Mammal: mice, whales, seals, dolphins, rats, deer, rabbits,
dogs, elephants, squirrels
Table 2: Learned People and Animals Terms.
types: Correct, Borderline, BasicConcept, and Not-
Concept. The details of our annotation guidelines,
the reasons for the intermediate labels, and the anno-
tation study can be found in (Kozareva et al, 2009).
ANIMAL
TYPE LABEL EXAMPLES
Correct GeneticAnimal reptile,mammal
BehavioralByFeeding predator, grazer
BehaviorByHabitat saltwater mammal
BehaviorSocialIndiv herding animal
BehaviorSocialGroup herd, pack
MorphologicalType cloven-hoofed animal
RoleOrFunction pet, parasite
Borderline NonRealAnimal dragons
EvaluativeTerm varmint, fox
OtherAnimal critter, fossil
BasicConcept BasicAnimal dog, hummingbird
NotConcept GeneralTerm model, catalyst
NotAnimal topic, favorite
GarbageTerm brates, mals
PEOPLE
TYPE LABEL EXAMPLES
Correct GeneticPerson Caucasian, Saxon
NonTransientEventRole stutterer, gourmand
TransientEventRole passenger, visitor
PersonState dwarf, schizophrenic
FamilyRelation aunt, mother
SocialRole fugitive, hero
NationOrTribe Bulgarian, Zulu
ReligiousAffiliation Catholic, atheist
Borderline NonRealPerson biblical figures
OtherPerson colleagues, couples
BasicConcept BasicPerson child, woman
RealPerson Barack Obama
NotConcept GeneralTerm image, figure
NotPerson books, events
Table 3: Intermediate Concept Annotation Labels
We measured pairwise inter-annotator agreement
across the four labels using the Fleiss kappa (Fleiss,
1971). The ? scores ranged from 0.61?0.71 for
Animals (average ?=0.66) and from 0.51?0.70 for
People (average ?=0.60). These agreement scores
seemed good enough to warrant using these human
judgments to estimate the accuracy of the algorithm.
The bootstrapping algorithm harvested 3, 549 An-
imal and 4, 094 People intermediate concepts in ten
iterations. After In-Degree ranking was applied,
954
we chose a random sample of intermediate concepts
with frequency over 1, which was given to four hu-
man judges for annotation. Table 4 summarizes the
labels assigned by the four annotators (A
1
? A
4
).
The top portion of Table 4 shows the results for all
the intermediate concepts (437 Animal terms and
296 People terms), and the bottom portion shows the
results only for the concepts that passed the Concept
Positioning Test (187 Animal terms and 139 People
terms). Accuracy is computed in two ways: Acc1 is
the percent of intermediate concepts labeled as Cor-
rect; Acc2 is the percent of intermediate concepts
labeled as either Correct or Borderline.
Without the CPT, accuracies range from 53?66%
for Animals and 75?85% for People. After ap-
plying the CPT, the accuracies increase to 71?84%
for animals and 82?94% for people. These results
confirm that the Concept Positioning Test is effec-
tive at removing many of the undesirable terms.
Overall, these results demonstrate that our algorithm
produced many high-quality intermediate concepts,
with good precision.
Figure 3 shows accuracy curves based on the
rankings of the intermediate concepts (based on In-
Degree scores). The CPT clearly improves accu-
racy even among the most highly ranked concepts.
For example, the Acc1 curves for animals show that
nearly 90% of the top 100 intermediate concepts
were correct after applying the CPT, whereas only
70% of the top 100 intermediate concepts were cor-
rect before. However, the CPT also eliminates many
desirable terms. For People, the accuracies are still
relatively high even without the CPT, and a much
larger set of intermediate concepts is learned.
Animals People
A
1
A
2
A
3
A
4
A
1
A
2
A
3
A
4
Correct 246 243 251 230 239 231 225 221
Borderline 42 26 22 29 12 10 6 4
BasicConcept 2 8 9 2 6 2 9 10
NotConcept 147 160 155 176 39 53 56 61
Acc1 .56 .56 .57 .53 .81 .78 .76 .75
Acc2 .66 .62 .62 .59 .85 .81 .78 .76
Animals after CPT People after CPT
A
1
A
2
A
3
A
4
A
1
A
2
A
3
A
4
Correct 146 133 144 141 126 126 114 116
Borderline 11 15 9 13 6 2 2 0
BasicConcept 2 8 9 2 0 1 7 7
NotConcept 28 31 25 31 7 10 16 16
Acc1 .78 .71 .77 .75 .91 .91 .82 .83
Acc2 .84 .79 .82 .82 .95 .92 .83 .83
Table 4: Human Intermediate Concept Evaluation.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 50  100  150  200  250  300  350  400
Pr
ec
is
io
n
Rank
Animal Intermediate Concepts
noCPTC
noCPTCB
withCPTC
withCPTCB
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 50  100  150  200  250  300
Pr
ec
is
io
n
Rank
People Intermediate Concepts
noCPTC
noCPTCB
withCPTC
withCPTCB
Figure 3: Intermediate Concept Precision at Rank N.
4.2.2 WordNet Evaluation
We also compared the intermediate concepts har-
vested by the algorithm to the contents of WordNet.
The results are shown in Table 5. WordNet contains
20% of the Animal concepts and 51% of the People
concepts learned by our algorithm, which confirms
that many of these concepts were considered to be
valuable taxonomic terms by the WordNet develop-
ers. However, our human annotators judged 57%
of the Animal and 84% of the People concepts to
be correct, which suggests that our algorithm gen-
erates a substantial number of additional concepts
that could be used to enrich taxonomic structure in
WordNet.
955
Pr
WN
Pr
H
NotInWN
Animal .20 (88/437) .57 (248/437) 204
People .51 (152/296) .85 (251/296) 108
Table 5: WordNet Intermediate Concept Evaluation.
4.3 Experiment 3: Taxonomic Links
In this section we evaluate the classification (taxon-
omy) that is learned by evaluating the links between
the intermediate concepts and the basic-level con-
cept/instance terms. That is, when our algorithm
claims that isa(X,Y), how often is X truly a subcon-
cept of Y? For example, isa(goat, herbivore) would
be correct, but isa(goat, bird) would not. Again,
since WordNet does not contain all the harvested
concepts, we conduct both a manual evaluation and
a comparison against WordNet.
4.3.1 Manual and WordNet Evaluations
Creating and evaluating the full taxonomic struc-
ture between the root and the basic-level or instance
terms is future work. Here we evaluate simply the
accuracy of the taxonomic links between basic-level
concepts/instances and intermediate concepts as har-
vested, but not between intermediate concepts. For
each pair, we extracted all harvested links and deter-
mined whether the same links appear in WordNet.
The links were also given to human judges. Table 6
shows the results.
ISA Pr
WN
Pr
H
NotInWN
Animal .47(912/1940) .88 (1716/1940) 804
People .23 (318/908) .94 (857/908) 539
Table 6: WordNet Taxonomic Evaluation.
The results show that WordNet lacks nearly half
of the taxonomic relations that were generated by
the algorithm: 804 Animal and 539 People links.
5 Conclusion
We describe a novel extension to the DAP approach
for discovering basic-level concepts or instances and
their superconcepts given an initial root concept. By
appropriate filling of different positions in DAP, the
algorithm alternates between ?downward? and ?up-
ward? learning. A key resulting benefit is that each
new intermediate-level term acquired restarts har-
vesting in a new region of the concept space, which
allows previously unseen concepts to be discovered
with each bootstrapping cycle.
We also introduce the Concept Positioning Test,
which serves to confirm that a harvested concept
falls into the desired part of the search space rela-
tive to either a superordinate or subordinate concept
in the growing taxonomy, before it is selected for
further harvesting using the DAP.
These algorithms can augment other term harvest-
ing algorithms recently reported. But in order to
compare different algorithms, it is important to com-
pare results to a standard. WordNet is our best can-
didate at present. But WordNet is incomplete. Our
results include a significantly large number of in-
stances of People (which WordNet does not claim
to cover), a number comparable to the results of (Et-
zioni et al, 2005; Pasca, 2007; Ritter et al, 2009).
Rather surprisingly, our results also include a large
number of basic-level and intermediate concepts for
Animals that are not present in WordNet, a category
WordNet is actually fairly complete about. These
numbers show clearly that it is important to conduct
manual evaluation of term harvesting algorithms in
addition to comparing to a standard resource.
Acknowledgments
This research was supported in part by grants from
the National Science Foundation (NSF grant no. IIS-
0429360), and the Department of Homeland Se-
curity, ONR Grant numbers N0014-07-1-0152 and
N00014-07-1-0149. We are grateful to the anno-
tators at the University of Pittsburgh who helped
us evaluate this work: Jay Fischer, David Halpern,
Amir Hussain, and Taichi Nakatani.
References
M. Banko, M. Cafarella, S. Soderland, M. Broadhead,
and O.Etzioni. 2007. Open information extraction
from the web. In Proceedings of International Joint
Conference on Artificial Itelligence, pages 2670?2676.
M. Berland and E. Charniak. 1999. Finding Parts in Very
Large Corpora. In Proc. of the 37th Annual Meeting of
the Association for Computational Linguistics.
S. Caraballo. 1999. Automatic Acquisition of a
Hypernym-Labeled Noun Hierarchy from Text. In
Proc. of the 37th Annual Meeting of the Association
for Computational Linguistics, pages 120?126.
P. Cimiano and J. Volker. 2005. Towards large-scale,
open-domain and ontology-based named entity classi-
fication. In Proceeding of RANLP-05, pages 166?172.
956
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from the
web: an experimental study. Artificial Intelligence,
165(1):91?134, June.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database (Language, Speech, and Communication).
May.
M.B. Fleischman and E.H. Hovy. 2002. Fine grained
classification of named entities. In Proceedings of the
COLING conference, August.
J.L. Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. Psychological Bulletin,
76(5):378?382.
R. Girju, A. Badulescu, and D. Moldovan. 2003. Learn-
ing semantic constraints for the automatic discovery of
part-whole relations. In HLT-NAACL.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In COLING, pages 539?545.
Z. Kozareva, E. Riloff, and E. Hovy. 2008. Seman-
tic class learning from the web with hyponym pat-
tern linkage graphs. In Proceedings of ACL-08: HLT,
pages 1048?1056. Association for Computational Lin-
guistics.
Z. Kozareva, E. Hovy, and E. Riloff. 2009. Learning and
evaluating the content and structure of a term taxon-
omy. In AAAI-09 Spring Symposium on Learning by
Reading and Learning to Read.
G. Mann. 2002. Fine-grained proper noun ontologies for
question answering. In COLING-02 on SEMANET,
pages 1?7.
P. McNamee, R. Snow, P. Schone, and J. Mayfield. 2008.
Learning named entity hyponyms for question answer-
ing. In Proceedings of the Third International Joint
Conference on Natural Language Processing.
P. Pantel and D. Ravichandran. 2004. Automatically la-
beling semantic classes. In HLT-NAACL, pages 321?
328.
M. Pasca. 2004. Acquisition of categorized named en-
tities for web search. In Proceedings of CIKM, pages
137?145.
M. Pasca. 2007. Weakly-supervised discovery of named
entities using web search queries. In CIKM, pages
683?690.
S. Ponzetto and M. Strube. 2007. Deriving a large scale
taxonomy from wikipedia. In Proceedings of the 22nd
National COnference on Artificial Intelligence (AAAI-
07), pages 1440?1447.
E. Riloff and R. Jones. 1999. Learning Dictionaries for
Information Extraction by Multi-Level Bootstrapping.
In Proceedings of the Sixteenth National Conference
on Artificial Intelligence.
E. Riloff and J. Shepherd. 1997. A Corpus-Based Ap-
proach for Building Semantic Lexicons. In Proceed-
ings of the Second Conference on Empirical Methods
in Natural Language Processing, pages 117?124.
A. Ritter, S. Soderland, and O. Etzioni. 2009. What is
this, anyway: Automatic hypernym discovery. In Pro-
ceedings of AAAI-09 Spring Symposium on Learning
by Reading and Learning to Read, pages 88?93.
B. Roark and E. Charniak. 1998. Noun-phrase Co-
occurrence Statistics for Semi-automatic Semantic
Lexicon Construction. In Proceedings of the 36th
Annual Meeting of the Association for Computational
Linguistics, pages 1110?1116.
E. Rosch, 1978. Principles of Categorization, pages 27?
48.
R. Snow, D. Jurafsky, and A. Y. Ng. 2005. Learning
syntactic patterns for automatic hypernym discovery.
In NIPS.
M. Thelen and E. Riloff. 2002. A Bootstrapping Method
for Learning Semantic Lexicons Using Extraction Pat-
tern Contexts. In Proceedings of the 2002 Conference
on Empirical Methods in Natural Language Process-
ing, pages 214?221.
D. Widdows and B. Dorow. 2002. A graph model for un-
supervised lexical acquisition. In Proceedings of the
19th international conference on Computational lin-
guistics, pages 1?7.
J. Yi and W. Niblack. 2005. Sentiment mining in web-
fountain. In ICDE ?05: Proceedings of the 21st In-
ternational Conference on Data Engineering, pages
1073?1083.
957
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 355?362, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Identifying Sources of Opinions with Conditional Random Fields and
Extraction Patterns
Yejin Choi and Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853
{ychoi,cardie}@cs.cornell.edu
Ellen Riloff and Siddharth Patwardhan
School of Computing
University of Utah
Salt Lake City, UT 84112
{riloff,sidd}@cs.utah.edu
Abstract
Recent systems have been developed for
sentiment classification, opinion recogni-
tion, and opinion analysis (e.g., detect-
ing polarity and strength). We pursue an-
other aspect of opinion analysis: identi-
fying the sources of opinions, emotions,
and sentiments. We view this problem as
an information extraction task and adopt
a hybrid approach that combines Con-
ditional Random Fields (Lafferty et al,
2001) and a variation of AutoSlog (Riloff,
1996a). While CRFs model source iden-
tification as a sequence tagging task, Au-
toSlog learns extraction patterns. Our re-
sults show that the combination of these
two methods performs better than either
one alone. The resulting system identifies
opinion sources with 79.3% precision and
59.5% recall using a head noun matching
measure, and 81.2% precision and 60.6%
recall using an overlap measure.
1 Introduction
In recent years, there has been a great deal of in-
terest in methods for automatically identifying opin-
ions, emotions, and sentiments in text. Much of
this research explores sentiment classification, a text
categorization task in which the goal is to classify
a document as having positive or negative polar-
ity (e.g., Das and Chen (2001), Pang et al (2002),
Turney (2002), Dave et al (2003), Pang and Lee
(2004)). Other research efforts analyze opinion ex-
pressions at the sentence level or below to recog-
nize opinions, their polarity, and their strength (e.g.,
Dave et al (2003), Pang and Lee (2004), Wilson et
al. (2004), Yu and Hatzivassiloglou (2003), Wiebe
and Riloff (2005)). Many applications could ben-
efit from these opinion analyzers, including prod-
uct reputation tracking (e.g., Morinaga et al (2002),
Yi et al (2003)), opinion-oriented summarization
(e.g., Cardie et al (2004)), and question answering
(e.g., Bethard et al (2004), Yu and Hatzivassiloglou
(2003)).
We focus here on another aspect of opinion
analysis: automatically identifying the sources of
the opinions. Identifying opinion sources will
be especially critical for opinion-oriented question-
answering systems (e.g., systems that answer ques-
tions of the form ?How does [X] feel about [Y]??)
and opinion-oriented summarization systems, both
of which need to distinguish the opinions of one
source from those of another.1
The goal of our research is to identify direct and
indirect sources of opinions, emotions, sentiments,
and other private states that are expressed in text.
To illustrate the nature of this problem, consider the
examples below:
S1: Taiwan-born voters favoring independence...
1In related work, we investigate methods to identify the
opinion expressions (e.g., Riloff and Wiebe (2003), Wiebe and
Riloff (2005), Wilson et al (2005)) and the nesting structure
of sources (e.g., Breck and Cardie (2004)). The target of each
opinion, i.e., what the opinion is directed towards, is currently
being annotated manually for our corpus.
355
S2: According to the report, the human rights
record in China is horrendous.
S3: International officers believe that the EU will
prevail.
S4: International officers said US officials want the
EU to prevail.
In S1, the phrase ?Taiwan-born voters? is the di-
rect (i.e., first-hand) source of the ?favoring? sen-
timent. In S2, ?the report? is the direct source of
the opinion about China?s human rights record. In
S3, ?International officers? are the direct source of
an opinion regarding the EU. The same phrase in
S4, however, denotes an indirect (i.e., second-hand,
third-hand, etc.) source of an opinion whose direct
source is ?US officials?.
In this paper, we view source identification as an
information extraction task and tackle the problem
using sequence tagging and pattern matching tech-
niques simultaneously. Using syntactic, semantic,
and orthographic lexical features, dependency parse
features, and opinion recognition features, we train a
linear-chain Conditional Random Field (CRF) (Laf-
ferty et al, 2001) to identify opinion sources. In ad-
dition, we employ features based on automatically
learned extraction patterns and perform feature in-
duction on the CRF model.
We evaluate our hybrid approach using the NRRC
corpus (Wiebe et al, 2005), which is manually
annotated with direct and indirect opinion source
information. Experimental results show that the
CRF model performs well, and that both the extrac-
tion patterns and feature induction produce perfor-
mance gains. The resulting system identifies opinion
sources with 79.3% precision and 59.5% recall us-
ing a head noun matching measure, and 81.2% pre-
cision and 60.6% recall using an overlap measure.
2 The Big Picture
The goal of information extraction (IE) systems is
to extract information about events, including the
participants of the events. This task goes beyond
Named Entity recognition (e.g., Bikel et al (1997))
because it requires the recognition of role relation-
ships. For example, an IE system that extracts in-
formation about corporate acquisitions must distin-
guish between the company that is doing the acquir-
ing and the company that is being acquired. Sim-
ilarly, an IE system that extracts information about
terrorism must distinguish between the person who
is the perpetrator and the person who is the victim.
We hypothesized that IE techniques would be well-
suited for source identification because an opinion
statement can be viewed as a kind of speech event
with the source as the agent.
We investigate two very different learning-based
methods from information extraction for the prob-
lem of opinion source identification: graphical mod-
els and extraction pattern learning. In particular, we
consider Conditional Random Fields (Lafferty et al,
2001) and a variation of AutoSlog (Riloff, 1996a).
CRFs have been used successfully for Named En-
tity recognition (e.g., McCallum and Li (2003),
Sarawagi and Cohen (2004)), and AutoSlog has per-
formed well on information extraction tasks in sev-
eral domains (Riloff, 1996a). While CRFs treat
source identification as a sequence tagging task, Au-
toSlog views the problem as a pattern-matching task,
acquiring symbolic patterns that rely on both the
syntax and lexical semantics of a sentence. We hy-
pothesized that a combination of the two techniques
would perform better than either one alone.
Section 3 describes the CRF approach to identify-
ing opinion sources and the features that the system
uses. Section 4 then presents a new variation of Au-
toSlog, AutoSlog-SE, which generates IE patterns to
extract sources. Section 5 describes the hybrid sys-
tem: we encode the IE patterns as additional features
in the CRF model. Finally, Section 6 presents our
experimental results and error analysis.
3 Semantic Tagging via Conditional
Random Fields
We defined the problem of opinion source identifi-
cation as a sequence tagging task via CRFs as fol-
lows. Given a sequence of tokens, x = x1x2...xn,
we need to generate a sequence of tags, or labels,
y = y1y2...yn. We define the set of possible label
values as ?S?, ?T?, ?-?, where ?S? is the first to-
ken (or Start) of a source, ?T? is a non-initial token
(i.e., a conTinuation) of a source, and ?-? is a token
that is not part of any source.2
A detailed description of CRFs can be found in
2This is equivalent to the IOB tagging scheme used in syn-
tactic chunkers (Ramshaw and Marcus, 1995).
356
Lafferty et al (2001). For our sequence tagging
problem, we create a linear-chain CRF based on
an undirected graph G = (V,E), where V is the
set of random variables Y = {Yi|1 ? i ? n},
one for each of n tokens in an input sentence;
and E = {(Yi?1, Yi)|1 < i ? n} is the set
of n ? 1 edges forming a linear chain. For each
sentence x, we define a non-negative clique poten-
tial exp(
?K
k=1 ?kfk(yi?1, yi, x)) for each edge, and
exp(?K?k=1 ??kf ?k(yi, x)) for each node, where fk(...)
is a binary feature indicator function, ?k is a weight
assigned for each feature function, and K and K ?
are the number of features defined for edges and
nodes respectively. Following Lafferty et al (2001),
the conditional probability of a sequence of labels y
given a sequence of tokens x is:
P (y|x) = 1Zx
exp
?
X
i,k
?k fk(yi?1, yi, x)+
X
i,k
??k f ?k(yi, x)
?
(1)
Zx =
X
y
exp
?
X
i,k
?k fk(yi?1, yi, x) +
X
i,k
??k f ?k(yi, x)
?
(2)
where Zx is a normalization constant for each
x. Given the training data D, a set of sen-
tences paired with their correct ?ST-? source la-
bel sequences, the parameters of the model are
trained to maximize the conditional log-likelihood
?
(x,y)?D P (y|x). For inference, given a sentence x
in the test data, the tagging sequence y is given by
argmaxy?P (y?|x).
3.1 Features
To develop features, we considered three properties
of opinion sources. First, the sources of opinions are
mostly noun phrases. Second, the source phrases
should be semantic entities that can bear or express
opinions. Third, the source phrases should be di-
rectly related to an opinion expression. When con-
sidering only the first and second criteria, this task
reduces to named entity recognition. Because of the
third condition, however, the task requires the recog-
nition of opinion expressions and a more sophisti-
cated encoding of sentence structure to capture re-
lationships between source phrases and opinion ex-
pressions.
With these properties in mind, we define the fol-
lowing features for each token/word xi in an input
sentence. For pedagogical reasons, we will describe
some of the features as being multi-valued or cate-
gorical features. In practice, however, all features
are binarized for the CRF model.
Capitalization features We use two boolean fea-
tures to represent the capitalization of a word:
all-capital, initial-capital.
Part-of-speech features Based on the lexical cat-
egories produced by GATE (Cunningham et al,
2002), each token xi is classified into one of a set
of coarse part-of-speech tags: noun, verb, adverb,
wh-word, determiner, punctuation, etc. We do the
same for neighboring words in a [?2,+2] window
in order to assist noun phrase segmentation.
Opinion lexicon features For each token xi, we in-
clude a binary feature that indicates whether or not
the word is in our opinion lexicon ? a set of words
that indicate the presence of an opinion. We do the
same for neighboring words in a [?1,+1] window.
Additionally, we include for xi a feature that in-
dicates the opinion subclass associated with xi, if
available from the lexicon. (e.g., ?bless? is clas-
sified as ?moderately subjective? according to the
lexicon, while ?accuse? and ?berate? are classified
more specifically as ?judgments?.) The lexicon is
initially populated with approximately 500 opinion
words 3 from (Wiebe et al, 2002), and then aug-
mented with opinion words identified in the training
data. The training data contains manually produced
phrase-level annotations for all expressions of opin-
ions, emotions, etc. (Wiebe et al, 2005). We col-
lected all content words that occurred in the training
set such that at least 50% of their occurrences were
in opinion annotations.
Dependency tree features For each token xi, we
create features based on the parse tree produced by
the Collins (1999) dependency parser. The purpose
of the features is to (1) encode structural informa-
tion, and (2) indicate whether xi is involved in any
grammatical relations with an opinion word. Two
pre-processing steps are required before features can
be constructed:
3Some words are drawn from Levin (1993); others are from
Framenet lemmas (Baker et al 1998) associated with commu-
nication verbs.
357
1. Syntactic chunking. We traverse the depen-
dency tree using breadth-first search to identify
and group syntactically related nodes, produc-
ing a flatter, more concise tree. Each syntac-
tic ?chunk? is also assigned a grammatical role
(e.g., subject, object, verb modifier, time,
location, of-pp, by-pp) based on its con-
stituents. Possessives (e.g., ?Clinton?s idea?)
and the phrase ?according to X? are handled as
special cases in the chunking process.
2. Opinion word propagation. Although the
opinion lexicon contains only content words
and no multi-word phrases, actual opinions of-
ten comprise an entire phrase, e.g., ?is really
willing? or ?in my opinion?. As a result, we
mark as an opinion the entire chunk that con-
tains an opinion word. This allows each token
in the chunk to act as an opinion word for fea-
ture encoding.
After syntactic chunking and opinion word propa-
gation, we create the following dependency tree fea-
tures for each token xi:
? the grammatical role of its chunk
? the grammatical role of xi?1?s chunk
? whether the parent chunk includes an opinion
word
? whether xi?s chunk is in an argument position
with respect to the parent chunk
? whether xi represents a constituent boundary
Semantic class features We use 7 binary fea-
tures to encode the semantic class of each word
xi: authority, government, human, media,
organization or company, proper name,
and other. The other class captures 13 seman-
tic classes that cannot be sources, such as vehicle
and time.
Semantic class information is derived from named
entity and semantic class labels assigned to xi by the
Sundance shallow parser (Riloff, 2004). Sundance
uses named entity recognition rules to label noun
phrases as belonging to named entity classes, and
assigns semantic tags to individual words based on
a semantic dictionary. Table 1 shows the hierarchy
that Sundance uses for semantic classes associated
with opinion sources. Sundance is also used to rec-
ognize and instantiate the source extraction patterns
PROPER NAMEAUTHORITY LOCATION
CITY
COUNTRY
PLANET
PROVINCE
PERSON NAME
PERSON DESC
NATIONALITY
TITLE
COMPANY
GOVERNMENT
MEDIA
ORGANIZATION
HUMAN
SOURCE
Figure 1: The semantic hierarchy for opinion
sources
that are learned by AutoSlog-SE, which is described
in the next section.
4 Semantic Tagging via Extraction
Patterns
We also learn patterns to extract opinion sources us-
ing a statistical adaptation of the AutoSlog IE learn-
ing algorithm. AutoSlog (Riloff, 1996a) is a super-
vised extraction pattern learner that takes a train-
ing corpus of texts and their associated answer keys
as input. A set of heuristics looks at the context
surrounding each answer and proposes a lexico-
syntactic pattern to extract that answer from the text.
The heuristics are not perfect, however, so the result-
ing set of patterns needs to be manually reviewed by
a person.
In order to build a fully automatic system that
does not depend on manual review, we combined
AutoSlog?s heuristics with statistics from the an-
notated training data to create a fully automatic
supervised learner. We will refer to this learner
as AutoSlog-SE (Statistically Enhanced variation
of AutoSlog). AutoSlog-SE?s learning process has
three steps:
Step 1: AutoSlog?s heuristics are applied to every
noun phrase (NP) in the training corpus. This
generates a set of extraction patterns that, col-
lectively, can extract every NP in the training
corpus.
Step 2: The learned patterns are augmented with
selectional restrictions that semantically con-
strain the types of noun phrases that are legiti-
mate extractions for opinion sources. We used
358
the semantic classes shown in Figure 1 as se-
lectional restrictions.
Step 3: The patterns are applied to the training cor-
pus and statistics are gathered about their ex-
tractions. We count the number of extrac-
tions that match annotations in the corpus (cor-
rect extractions) and the number of extractions
that do not match annotations (incorrect extrac-
tions). These counts are then used to estimate
the probability that the pattern will extract an
opinion source in new texts:
P (source | patterni) =
correct sources
correct sources + incorrect sources
This learning process generates a set of extraction
patterns coupled with probabilities. In the next sec-
tion, we explain how these extraction patterns are
represented as features in the CRF model.
5 Extraction Pattern Features for the CRF
The extraction patterns provide two kinds of infor-
mation. SourcePatt indicates whether a word
activates any source extraction pattern. For exam-
ple, the word ?complained? activates the pattern
?<subj> complained? because it anchors the ex-
pression. SourceExtr indicates whether a word is
extracted by any source pattern. For example, in the
sentence ?President Jacques Chirac frequently com-
plained about France?s economy?, the words ?Pres-
ident?, ?Jacques?, and ?Chirac? would all be ex-
tracted by the ?<subj> complained? pattern.
Each extraction pattern has frequency and prob-
ability values produced by AutoSlog-SE, hence we
create four IE pattern-based features for each token
xi: SourcePatt-Freq, SourceExtr-Freq,
SourcePatt-Prob, and SourceExtr-Prob,
where the frequency values are divided into three
ranges: {0, 1, 2+} and the probability values are di-
vided into five ranges of equal size.
6 Experiments
We used the Multi-Perspective Question Answering
(MPQA) corpus4 for our experiments. This corpus
4The MPQA corpus can be freely obtained at
http://nrrc.mitre.org/NRRC/publications.htm.
consists of 535 documents that have been manu-
ally annotated with opinion-related information in-
cluding direct and indirect sources. We used 135
documents as a tuning set for model development
and feature engineering, and used the remaining 400
documents for evaluation, performing 10-fold cross
validation. These texts are English language ver-
sions of articles that come from many countries and
cover many topics.5
We evaluate performance using 3 measures: over-
lap match (OL), head match (HM), and exact match
(EM). OL is a lenient measure that considers an ex-
traction to be correct if it overlaps with any of the an-
notated words. HM is a more conservative measure
that considers an extraction to be correct if its head
matches the head of the annotated source. We report
these somewhat loose measures because the annota-
tors vary in where they place the exact boundaries
of a source. EM is the strictest measure that requires
an exact match between the extracted words and the
annotated words. We use three evaluation metrics:
recall, precision, and F-measure with recall and pre-
cision equally weighted.
6.1 Baselines
We developed three baseline systems to assess the
difficulty of our task. Baseline-1 labels as sources
all phrases that belong to the semantic categories
authority, government, human, media,
organization or company, proper name.
Table 1 shows that the precision is poor, suggest-
ing that the third condition described in Section 3.1
(opinion recognition) does play an important role in
source identification. The recall is much higher but
still limited due to sources that fall outside of the se-
mantic categories or are not recognized as belong-
ing to these categories. Baseline-2 labels a noun
phrase as a source if any of the following are true:
(1) the NP is the subject of a verb phrase containing
an opinion word, (2) the NP follows ?according to?,
(3) the NP contains a possessive and is preceded by
an opinion word, or (4) the NP follows ?by? and at-
taches to an opinion word. Baseline-2?s heuristics
are designed to address the first and the third condi-
tions in Section 3.1. Table 1 shows that Baseline-2
is substantially better than Baseline-1. Baseline-3
5This data was obtained from the Foreign Broadcast Infor-
mation Service (FBIS), a U.S. government agency.
359
Recall Prec F1
OL 77.3 28.8 42.0
Baseline-1 HM 71.4 28.6 40.8
EM 65.4 20.9 31.7
OL 62.4 60.5 61.4
Baseline-2 HM 59.7 58.2 58.9
EM 50.8 48.9 49.8
OL 49.9 72.6 59.2
Baseline-3 HM 47.4 72.5 57.3
EM 44.3 58.2 50.3
OL 48.5 81.3 60.8
Extraction Patterns HM 46.9 78.5 58.7
EM 41.9 70.2 52.5
CRF: OL 56.1 81.0 66.3
basic features HM 55.1 79.2 65.0
EM 50.0 72.4 59.2
CRF: OL 59.1 82.4 68.9
basic + IE pattern HM 58.1 80.5 67.5
features EM 52.5 73.3 61.2
CRF-FI: OL 57.7 80.7 67.3
basic features HM 56.8 78.8 66.0
EM 51.7 72.4 60.3
CRF-FI: OL 60.6 81.2 69.4
basic + IE pattern HM 59.5 79.3 68.0
features EM 54.1 72.7 62.0
Table 1: Source identification performance table
labels a noun phrase as a source if it satisfies both
Baseline-1 and Baseline-2?s conditions (this should
satisfy all three conditions described in Section 3.1).
As shown in Table 1, the precision of this approach
is the best of the three baselines, but the recall is the
lowest.
6.2 Extraction Pattern Experiment
We evaluated the performance of the learned extrac-
tion patterns on the source identification task. The
learned patterns were applied to the test data and
the extracted sources were scored against the manual
annotations.6 Table 1 shows that the extraction pat-
terns produced lower recall than the baselines, but
with considerably higher precision. These results
show that the extraction patterns alone can identify
6These results were obtained using the patterns that had a
probability > .50 and frequency > 1.
nearly half of the opinion sources with good accu-
racy.
6.3 CRF Experiments
We developed our CRF model using the MALLET
code from McCallum (2002). For training, we used
a Gaussian prior of 0.25, selected based on the tun-
ing data. We evaluate the CRF using the basic fea-
tures from Section 3, both with and without the IE
pattern features from Section 5. Table 1 shows that
the CRF with basic features outperforms all of the
baselines as well as the extraction patterns, achiev-
ing an F-measure of 66.3 using the OL measure,
65.0 using the HM measure, and 59.2 using the
EM measure. Adding the IE pattern features fur-
ther increases performance, boosting recall by about
3 points for all of the measures and slightly increas-
ing precision as well.
CRF with feature induction. One limitation of
log-linear function models like CRFs is that they
cannot form a decision boundary from conjunctions
of existing features, unless conjunctions are explic-
itly given as part of the feature vector. For the
task of identifying opinion sources, we observed
that the model could benefit from conjunctive fea-
tures. For instance, instead of using two separate
features, HUMAN and PARENT-CHUNK-INCLUDES-
OPINION-EXPRESSION, the conjunction of the two
is more informative.
For this reason, we applied the CRF feature in-
duction approach introduced by McCallum (2003).
As shown in Table 1, where CRF-FI stands for the
CRF model with feature induction, we see consis-
tent improvements by automatically generating con-
junctive features. The final system, which com-
bines the basic features, the IE pattern features,
and feature induction achieves an F-measure of 69.4
(recall=60.6%, precision=81.2%) for the OL mea-
sure, an F-measure of 68.0 (recall=59.5%, preci-
sion=79.3%) for the HM measure, and an F-measure
of 62.0 (recall=54.1%, precision=72.7%) for the EM
measure.
6.4 Error Analysis
An analysis of the errors indicated some common
mistakes:
? Some errors resulted from error propagation in
360
our subsystems. Errors from the sentence bound-
ary detector in GATE (Cunningham et al, 2002)
were especially problematic because they caused
the Collins parser to fail, resulting in no depen-
dency tree information.
? Some errors were due to complex and unusual
sentence structure, which our rather simple fea-
ture encoding for CRF could not capture well.
? Some errors were due to the limited coverage of
the opinion lexicon. We failed to recognize some
cases when idiomatic or vague expressions were
used to express opinions.
Below are some examples of errors that we found
interesting. Doubly underlined phrases indicate in-
correctly extracted sources (either false positives
or false negatives). Opinion words are singly
underlined.
False positives:
(1) Actually, these three countries do have one common
denominator, i.e., that their values and policies do not
agree with those of the United States and none of them
are on good terms with the United States.
(2) Perhaps this is why Fidel Castro has not spoken out
against what might go on in Guantanamo.
In (1), ?their values and policies? seems like a rea-
sonable phrase to extract, but the annotation does not
mark this as a source, perhaps because it is some-
what abstract. In (2), ?spoken out? is negated, which
means that the verb phrase does not bear an opinion,
but our system failed to recognize the negation.
False negatives:
(3) And for this reason, too, they have a moral duty to
speak out, as Swedish Foreign Minister Anna Lindh,
among others, did yesterday.
(4) In particular, Iran and Iraq are at loggerheads with
each other to this day.
Example (3) involves a complex sentence structure
that our system could not deal with. (4) involves an
uncommon opinion expression that our system did
not recognize.
7 Related Work
To our knowledge, our research is the first to auto-
matically identify opinion sources using the MPQA
opinion annotation scheme. The most closely re-
lated work on opinion analysis is Bethard et al
(2004), who use machine learning techniques to
identify propositional opinions and their holders
(sources). However, their work is more limited
in scope than ours in several ways. Their work
only addresses propositional opinions, which are
?localized in the propositional argument? of cer-
tain verbs such as ?believe? or ?realize?. In con-
trast, our work aims to find sources for all opinions,
emotions, and sentiments, including those that are
not related to a verb at all. Furthermore, Berthard
et al?s task definition only requires the identifica-
tion of direct sources, while our task requires the
identification of both direct and indirect sources.
Bethard et al evaluate their system on manually
annotated FrameNet (Baker et al, 1998) and Prop-
Bank (Palmer et al, 2005) sentences and achieve
48% recall with 57% precision.
Our IE pattern learner can be viewed as a cross
between AutoSlog (Riloff, 1996a) and AutoSlog-
TS (Riloff, 1996b). AutoSlog is a supervised learner
that requires annotated training data but does not
compute statistics. AutoSlog-TS is a weakly super-
vised learner that does not require annotated data
but generates coarse statistics that measure each pat-
tern?s correlation with relevant and irrelevant docu-
ments. Consequently, the patterns learned by both
AutoSlog and AutoSlog-TS need to be manually re-
viewed by a person to achieve good accuracy. In
contrast, our IE learner, AutoSlog-SE, computes
statistics directly from the annotated training data,
creating a fully automatic variation of AutoSlog.
8 Conclusion
We have described a hybrid approach to the problem
of extracting sources of opinions in text. We cast
this problem as an information extraction task, using
both CRFs and extraction patterns. Our research is
the first to identify both direct and indirect sources
for all types of opinions, emotions, and sentiments.
Directions for future work include trying to in-
crease recall by identifying relationships between
opinions and sources that cross sentence boundaries,
and relationships between multiple opinion expres-
sions by the same source. For example, the fact that
a coreferring noun phrase was marked as a source
in one sentence could be a useful clue for extracting
the source from another sentence. The probability or
the strength of an opinion expression may also play
a useful role in encouraging or suppressing source
extraction.
361
9 Acknowledgments
We thank the reviewers for their many helpful com-
ments, and the Cornell NLP group for their advice
and suggestions for improvement. This work was
supported by the Advanced Research and Develop-
ment Activity (ARDA), by NSF Grants IIS-0208028
and IIS-0208985, and by the Xerox Foundation.
References
C. Baker, C. Fillmore & J. Lowe. 1998. The Berkeley
FrameNet Project. In Proceedings of the COLING-ACL.
S. Bethard, H. Yu, A. Thornton, V. Hativassiloglou & D. Juraf-
sky. 2004. Automatic extraction of opinion propositions and
their holders. In Proceedings of AAAI Spring Symposium on
Exploring Attitude and Affect in Text.
D. Bikel, S. Miller, R. Schwartz & R. Weischedel. 1997.
Nymble: A High-Performance Learning Name-Finder. In
Proceedings of the Fifth Conference on Applied Natural Lan-
guage Processing.
E. Breck & C. Cardie. 2004. Playing the Telephone Game:
Determining the Hierarchical Structure of Perspective and
Speech Expressions. In Proceedings of 20th International
Conference on Computational Linguistics.
C. Cardie, J. Wiebe, T. Wilson & D. Litman. 2004. Low-
level annotations and summary representations of opinions
for multiperspective QA. In New Directions in Question An-
swering. AAAI Press/MIT Press.
M. Collins. 1999. Head-driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, University of Pennsylvania.
H. Cunningham, D. Maynard, K. Bontcheva & V. Tablan. 2002.
GATE: A Framework and Graphical Development Environ-
ment for Robust NLP Tools and Applications. In Proceed-
ings of the 40th Anniversary Meeting of the Association for
Computational Linguistics.
S. Das & M. Chen. 2001. Yahoo for amazon: Extracting market
sentiment from stock message boards. In Proceedings of the
8th Asia Pacific Finance Association Annual Conference.
K. Dave, S. Lawrence & D. Pennock. 2003. Mining the peanut
gallery: Opinion extraction and semantic classification of
product reviews. In International World Wide Web Confer-
ence.
J. Lafferty, A. K. McCallum & F. Pereira. 2001. Conditional
Random Fields: Probabilistic Models for Segmenting and
Labeling Sequence Data. In Proceedings of 18th Interna-
tional Conference on Machine Learning.
B. Levin. 1993. English Verb Classes and Alternations: A
Preliminary Investigation. University of Chicago Press.
A. K. McCallum. 2002. MALLET: A Machine Learning for
Language Toolkit. http://mallet.cs.umass.edu.
A. K. McCallum. 2003. Efficiently Inducing Features of Con-
ditional Random Fields. In Conference on Uncertainty in
Artificial Intelligence.
A. K. McCallum & W. Li. 2003. Early Results for Named
Entity Recognition with Conditional Random Fields, Feature
Induction and Web-Enhanced Lexicons. In Conference on
Natural Language Learning.
S. Morinaga, K. Yamanishi, K. Tateishi & T. Fukushima 2002.
Mining Product Reputations on the Web. In Proceedings of
the 8th Internatinal Conference on Knowledge Discover and
Data Mining.
M. Palmer, D. Gildea & P. Kingsbury. 2005. The Proposition
Bank: An Annotated Corpus of Semantic Roles. In Compu-
tational Linguistics 31.
B. Pang, L. Lee & S. Vaithyanathan. 2002. Thumbs up? sen-
timent classification using machine learning techniques. In
Proceedings of the 2002 Conference on Empirical Methods
in Natural Language Processing.
B. Pang & L. Lee. 2004. A sentimental education: Sentiment
analysis using subjectivity summarization based on mini-
mum cuts. In Proceedings of the 42nd Annual Meeting of
the Association for Computational Linguistics.
L. A. Ramshaw & M. P. Marcus. 1995. Nymble: A High-
Performance Learning Name-Finder. In Proceedings of the
3rd Workshop on Very Large Corpora.
E. Riloff. 1996a. An Empirical Study of Automated Dictionary
Construction for Information Extraction in Three Domains.
In Artificial Intelligence, Vol. 85.
E. Riloff. 1996b. Automatically Generating Extraction Patterns
from Untagged Text. In Proceedings of the 13th National
Conference on Artificial Intelligence.
E. Riloff & J. Wiebe. 2003. Learning extraction patterns for
subjective expressions. In Proceesings of 2003 Conference
on Empirical Methods in Natural Language Processing.
E. Riloff & W. Phillips. 2004. An Introduction to the Sun-
dance and AutoSlog Systems Technical Report UUCS-04-
015, School of Computing, University of Utah.
S. Sarawagi & W. W. Cohen. 2004. Semi-Markov Condi-
tional Random Fields for Information Extraction 18th An-
nual Conference on Neural Information Processing Systems.
P. Turney. 2002. Thumbs up or thumbs down? semantic orien-
tation applied to unsupervised classification of reviews. In
Proceedings of the 40th Annual Meeting of the Association
for Computational Linguistics.
T. Wilson, J. Wiebe & R. Hwa. 2004. Just how mad are you?
finding strong and weak opinion clauses. In Proceedings of
the 9th National Conference on Artificial Intelligence.
T. Wilson, P. Hoffmann, S. Somasundaran, J. Kessler, J. Wiebe,
Y. Choi, C. Cardie, E. Riloff & S. Patwardhan. 2005. Opin-
ionFinder: A system for subjectivity analysis. Demonstra-
tion Description in Conference on Empirical Methods in
Natural Language Processing.
J. Yi, T. Nasukawa, R. Bunescu & W. Niblack. 2003. Sentiment
Analyzer: Extracting Sentiments about a Given Topic using
Natural Language Processing Techniques. In Proceedings of
the 3rd IEEE International Conference on Data Mining.
H. Yu & V. Hatzivassiloglou. 2003. Towards answering opin-
ion questions: Separating facts from opinions and identify-
ing the polarity of opinion sentences. In Proceedings of the
Conference on Empirical Methods in Natural Language Pro-
cessing.
J. Wiebe, E. Breck, C. Buckley, C. Cardie, P. Davis, B. Fraser,
D. Litman, D. Pierce, E. Riloff & T. Wilson. 2002. NRRC
Summer Workshop on Multiple-Perspective Question An-
swering: Final Report.
J. Wiebe & E. Riloff. 2005. Creating subjective and objective
sentence classifiers from unannotated texts. Sixth Interna-
tional Conference on Intelligent Text Processing and Com-
putational Linguistics.
J. Wiebe, T. Wilson & C. Cardie. 2005. Annotating expressions
of opinions and emotions in language. Language Resources
and Evaluation, 1(2).
362
Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 34?35,
Vancouver, October 2005.
OpinionFinder: A system for subjectivity analysis
Theresa Wilson?, Paul Hoffmann?, Swapna Somasundaran?, Jason Kessler?,
Janyce Wiebe??, Yejin Choi?, Claire Cardie?, Ellen Riloff?, Siddharth Patwardhan?
?Intelligent Systems Program, University of Pittsburgh, Pittsburgh, PA 15260
?Department of Computer Science, University of Pittsburgh, Pittsburgh, PA 15260
?Department of Computer Science, Cornell University, Ithaca, NY 14853
?School of Computing, University of Utah, Salt Lake City, UT 84112
{twilson,hoffmanp,swapna,jsk44,wiebe}@cs.pitt.edu,
{ychoi,cardie}@cs.cornell.edu, {riloff,sidd}@cs.utah.edu
1 Introduction
OpinionFinder is a system that performs subjectivity
analysis, automatically identifying when opinions,
sentiments, speculations, and other private states are
present in text. Specifically, OpinionFinder aims to
identify subjective sentences and to mark various as-
pects of the subjectivity in these sentences, includ-
ing the source (holder) of the subjectivity and words
that are included in phrases expressing positive or
negative sentiments.
Our goal with OpinionFinder is to develop a sys-
tem capable of supporting other Natural Language
Processing (NLP) applications by providing them
with information about the subjectivity in docu-
ments. Of particular interest are question answering
systems that focus on being able to answer opinion-
oriented questions, such as the following:
How is Bush?s decision not to ratify the
Kyoto Protocol looked upon by Japan and
other US allies?
How do the Chinese regard the human
rights record of the United States?
To answer these types of questions, a system needs
to be able to identify when opinions are expressed in
text and who is expressing them. Other applications
that would benefit from knowledge of subjective lan-
guage include systems that summarize the various
viewpoints in a document or that mine product re-
views. Even typical fact-oriented applications, such
as information extraction, can benefit from subjec-
tivity analysis by filtering out opinionated sentences
(Riloff et al, 2005).
2 OpinionFinder
OpinionFinder runs in two modes, batch and inter-
active. Document processing is largely the same for
both modes. In batch mode, OpinionFinder takes a
list of documents to process. Interactive mode pro-
vides a front-end that allows a user to query on-line
news sources for documents to process.
2.1 System Architecture Overview
OpinionFinder operates as one large pipeline. Con-
ceptually, the pipeline can be divided into two parts.
The first part performs mostly general purpose doc-
ument processing (e.g., tokenization and part-of-
speech tagging). The second part performs the sub-
jectivity analysis. The results of the subjectivity
analysis are returned to the user in the form of
SGML/XML markup of the original documents.
2.2 Document Processing
For general document processing, OpinionFinder
first runs the Sundance partial parser (Riloff and
Phillips, 2004) to provide semantic class tags, iden-
tify Named Entities, and match extraction patterns
that correspond to subjective language (Riloff and
Wiebe, 2003). Next, OpenNLP1 1.1.0 is used to tok-
enize, sentence split, and part-of-speech tag the data,
and the Abney stemmer2 is used to stem. In batch
mode, OpinionFinder parses the data again, this time
to obtain constituency parse trees (Collins, 1997),
which are then converted to dependency parse trees
(Xia and Palmer, 2001). Currently, this stage is only
1http://opennlp.sourceforge.net/
2SCOL version 1g available at http://www.vinartus.net/spa/
34
available for batch mode processing due to the time
required for parsing. Finally, a clue-finder is run to
identify words and phrases from a large subjective
language lexicon.
2.3 Subjectivity Analysis
The subjectivity analysis has four components.
2.3.1 Subjective Sentence Classification
The first component is a Naive Bayes classifier
that distinguishes between subjective and objective
sentences using a variety of lexical and contextual
features (Wiebe and Riloff, 2005; Riloff and Wiebe,
2003). The classifier is trained using subjective and
objective sentences, which are automatically gener-
ated from a large corpus of unannotated data by two
high-precision, rule-based classifiers.
2.3.2 Speech Events and Direct Subjective
Expression Classification
The second component identifies speech events
(e.g., ?said,? ?according to?) and direct subjective
expressions (e.g., ?fears,? ?is happy?). Speech
events include both speaking and writing events.
Direct subjective expressions are words or phrases
where an opinion, emotion, sentiment, etc. is di-
rectly described. A high-precision, rule-based clas-
sifier is used to identify these expressions.
2.3.3 Opinion Source Identification
The third component is a source identifier that
combines a Conditional Random Field sequence
tagging model (Lafferty et al, 2001) and extraction
pattern learning (Riloff, 1996) to identify the sources
of speech events and subjective expressions (Choi
et al, 2005). The source of a speech event is the
speaker; the source of a subjective expression is the
experiencer of the private state. The source identifier
is trained on the MPQA Opinion Corpus3 using a
variety of features. Because the source identifier re-
lies on dependency parse information, it is currently
only available in batch mode.
2.3.4 Sentiment Expression Classification
The final component uses two classifiers to iden-
tify words contained in phrases that express pos-
itive or negative sentiments (Wilson et al, 2005).
3The MPQA Opinion Corpus can be freely obtained at
http://nrrc.mitre.org/NRRC/publications.htm.
The first classifier focuses on identifying sentiment
expressions. The second classifier takes the senti-
ment expressions and identifies those that are pos-
itive and negative. Both classifiers were developed
using BoosTexter (Schapire and Singer, 2000) and
trained on the MPQA Corpus.
3 Related Work
Please see (Wiebe and Riloff, 2005; Choi et al,
2005; Wilson et al, 2005) for discussions of related
work in automatic opinion and sentiment analysis.
4 Acknowledgments
This work was supported by the Advanced Research
and Development Activity (ARDA), by the NSF
under grants IIS-0208028, IIS-0208798 and IIS-
0208985, and by the Xerox Foundation.
References
Y. Choi, C. Cardie, E. Riloff, and S. Patwardhan. 2005. Identi-
fying sources of opinions with conditional random fields and
extraction patterns. In HLT/EMNLP 2005.
M. Collins. 1997. Three generative, lexicalised models for sta-
tistical parsing. In ACL-1997.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and la-
beling sequence data. In ICML-2001.
E. Riloff and W. Phillips. 2004. An Introduction to the Sun-
dance and AutoSlog Systems. Technical Report UUCS-04-
015, School of Computing, University of Utah.
E. Riloff and J. Wiebe. 2003. Learning extraction patterns for
subjective expressions. In EMNLP-2003.
E. Riloff, J. Wiebe, and W. Phillips. 2005. Exploiting sub-
jectivity classification to improve information extraction. In
AAAI-2005.
E. Riloff. 1996. An Empirical Study of Automated Dictionary
Construction for Information Extraction in Three Domains.
Artificial Intelligence, 85:101?134.
R. E. Schapire and Y. Singer. 2000. BoosTexter: A boosting-
based system for text categorization. Machine Learning,
39(2/3):135?168.
J. Wiebe and E. Riloff. 2005. Creating subjective and objec-
tive sentence classifiers from unannotated texts. In CICLing-
2005.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recognizing
contextual polarity in phrase-level sentiment analysis. In
HLT/EMNLP 2005.
F. Xia and M. Palmer. 2001. Converting dependency structures
to phrase structures. In HLT-2001.
35
Unsupervised Learning of Contextual Role Knowledge for Coreference
Resolution
David Bean
Attensity Corporation, Suite 600
Gateway One 90 South 400 West
Salt Lake City, UT 84101
dbean@attensity.com
Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT 84112
riloff@cs.utah.edu
Abstract
We present a coreference resolver called
BABAR that uses contextual role knowledge to
evaluate possible antecedents for an anaphor.
BABAR uses information extraction patterns
to identify contextual roles and creates four
contextual role knowledge sources using unsu-
pervised learning. These knowledge sources
determine whether the contexts surrounding
an anaphor and antecedent are compatible.
BABAR applies a Dempster-Shafer probabilis-
tic model to make resolutions based on ev-
idence from the contextual role knowledge
sources as well as general knowledge sources.
Experiments in two domains showed that the
contextual role knowledge improved corefer-
ence performance, especially on pronouns.
1 Introduction
The problem of coreference resolution has received con-
siderable attention, including theoretical discourse mod-
els (e.g., (Grosz et al, 1995; Grosz and Sidner, 1998)),
syntactic algorithms (e.g., (Hobbs, 1978; Lappin and Le-
ass, 1994)), and supervised machine learning systems
(Aone and Bennett, 1995; McCarthy and Lehnert, 1995;
Ng and Cardie, 2002; Soon et al, 2001). Most compu-
tational models for coreference resolution rely on prop-
erties of the anaphor and candidate antecedent, such as
lexical matching, grammatical and syntactic features, se-
mantic agreement, and positional information.
The focus of our work is on the use of contextual role
knowledge for coreference resolution. A contextual role
represents the role that a noun phrase plays in an event
or relationship. Our work is motivated by the observa-
tion that contextual roles can be critically important in
determining the referent of a noun phrase. Consider the
following sentences:
(a) Jose Maria Martinez, Roberto Lisandy, and Dino
Rossy, who were staying at a Tecun Uman hotel,
were kidnapped by armed men who took them to an
unknown place.
(b) After they were released...
(c) After they blindfolded the men...
In (b) ?they? refers to the kidnapping victims, but in (c)
?they? refers to the armed men. The role that each noun
phrase plays in the kidnapping event is key to distinguish-
ing these cases. The correct resolution in sentence (b)
comes from knowledge that people who are kidnapped
are often subsequently released. The correct resolution in
sentence (c) depends on knowledge that kidnappers fre-
quently blindfold their victims.
We have developed a coreference resolver called
BABAR that uses contextual role knowledge to make
coreference decisions. BABAR employs information ex-
traction techniques to represent and learn role relation-
ships. Each pattern represents the role that a noun phrase
plays in the surrounding context. BABAR uses unsu-
pervised learning to acquire this knowledge from plain
text without the need for annotated training data. Train-
ing examples are generated automatically by identifying
noun phrases that can be easily resolved with their an-
tecedents using lexical and syntactic heuristics. BABAR
then computes statistics over the training examples mea-
suring the frequency with which extraction patterns and
noun phrases co-occur in coreference resolutions.
In this paper, Section 2 begins by explaining how
contextual role knowledge is represented and learned.
Section 3 describes the complete coreference resolution
model, which uses the contextual role knowledge as well
as more traditional coreference features. Our corefer-
ence resolver also incorporates an existential noun phrase
recognizer and a Dempster-Shafer probabilistic model to
make resolution decisions. Section 4 presents experimen-
tal results on two corpora: the MUC-4 terrorism cor-
pus, and Reuters texts about natural disasters. Our re-
sults show that BABAR achieves good performance in
both domains, and that the contextual role knowledge
improves performance, especially on pronouns. Finally,
Section 5 explains how BABAR relates to previous work,
and Section 6 summarizes our conclusions.
2 Learning Contextual Role Knowledge
In this section, we describe how contextual role knowl-
edge is represented and learned. Section 2.1 describes
how BABAR generates training examples to use in the
learning process. We refer to this process as Reli-
able Case Resolution because it involves finding cases
of anaphora that can be easily resolved with their an-
tecedents. Section 2.2 then describes our representation
for contextual roles and four types of contextual role
knowledge that are learned from the training examples.
2.1 Reliable Case Resolutions
The first step in the learning process is to generate train-
ing examples consisting of anaphor/antecedent resolu-
tions. BABAR uses two methods to identify anaphors
that can be easily and reliably resolved with their an-
tecedent: lexical seeding and syntactic seeding.
2.1.1 Lexical Seeding
It is generally not safe to assume that multiple occur-
rences of a noun phrase refer to the same entity. For
example, the company may refer to Company X in one
paragraph and Company Y in another. However, lex-
ically similar NPs usually refer to the same entity in
two cases: proper names and existential noun phrases.
BABAR uses a named entity recognizer to identify proper
names that refer to people and companies. Proper names
are assumed to be coreferent if they match exactly, or if
they closely match based on a few heuristics. For exam-
ple, a person?s full name will match with just their last
name (e.g., ?George Bush? and ?Bush?), and a company
name will match with and without a corporate suffix (e.g.,
?IBM Corp.? and ?IBM?). Proper names that match are
resolved with each other.
The second case involves existential noun phrases
(Allen, 1995), which are noun phrases that uniquely spec-
ify an object or concept and therefore do not need a
prior referent in the discourse. In previous work (Bean
and Riloff, 1999), we developed an unsupervised learn-
ing algorithm that automatically recognizes definite NPs
that are existential without syntactic modification be-
cause their meaning is universally understood. For exam-
ple, a story can mention ?the FBI?, ?the White House?,
or ?the weather? without any prior referent in the story.
Although these existential NPs do not need a prior ref-
erent, they may occur multiple times in a document. By
definition, each existential NP uniquely specifies an ob-
ject or concept, so we can infer that all instances of the
same existential NP are coreferent (e.g., ?the FBI? always
refers to the same entity). Using this heuristic, BABAR
identifies existential definite NPs in the training corpus
using our previous learning algorithm (Bean and Riloff,
1999) and resolves all occurrences of the same existential
NP with each another.1
2.1.2 Syntactic Seeding
BABAR also uses syntactic heuristics to identify
anaphors and antecedents that can be easily resolved. Ta-
ble 1 briefly describes the seven syntactic heuristics used
by BABAR to resolve noun phrases. Words and punctua-
tion that appear in brackets are considered optional. The
anaphor and antecedent appear in boldface.
1. Reflexive pronouns with only 1 NP in scope.
Ex: The regime gives itself the right...
2. Relative pronouns with only 1 NP in scope.
Ex: The brigade, which attacked ...
3. Some cases of the pattern ?NP to-be NP?.
Ex: Mr. Cristiani is the president ...
4. Some cases of ?NP said [that] it/they?
Ex: The government said it ...
5. Some cases of ?[Locative-prep] NP [,] where?
Ex: He was found in San Jose, where ...
6. Simple appositives of the form ?NP, NP?
Ex: Mr. Cristiani, president of the country ...
7. PPs containing ?by? and a gerund followed by ?it?
Ex: Mr. Bush disclosed the policy by reading it...
Table 1: Syntactic Seeding Heuristics
BABAR?s reliable case resolution heuristics produced
a substantial set of anaphor/antecedent resolutions that
will be the training data used to learn contextual role
knowledge. For terrorism, BABAR generated 5,078 res-
olutions: 2,386 from lexical seeding and 2,692 from
syntactic seeding. For natural disasters, BABAR gener-
ated 20,479 resolutions: 11,652 from lexical seeding and
8,827 from syntactic seeding.
2.2 Contextual Role Knowledge
Our representation of contextual roles is based on infor-
mation extraction patterns that are converted into simple
caseframes. First, we describe how the caseframes are
represented and learned. Next, we describe four con-
textual role knowledge sources that are created from the
training examples and the caseframes.
2.2.1 The Caseframe Representation
Information extraction (IE) systems use extraction pat-
terns to identify noun phrases that play a specific role in
1Our implementation only resolves NPs that occur in the
same document, but in retrospect, one could probably resolve
instances of the same existential NP in different documents too.
an event. For IE, the system must be able to distinguish
between semantically similar noun phrases that play dif-
ferent roles in an event. For example, management suc-
cession systems must distinguish between a person who
is fired and a person who is hired. Terrorism systems
must distinguish between people who perpetrate a crime
and people who are victims of a crime.
We applied the AutoSlog system (Riloff, 1996) to our
unannotated training texts to generate a set of extraction
patterns for each domain. Each extraction pattern repre-
sents a linguistic expression and a syntactic position in-
dicating where a role filler can be found. For example,
kidnapping victims should be extracted from the subject
of the verb ?kidnapped? when it occurs in the passive
voice (the short-hand representation of this pattern would
be ?<subject> were kidnapped?). The types of patterns
produced by AutoSlog are outlined in (Riloff, 1996).
Ideally we?d like to know the thematic role of each ex-
tracted noun phrase, but AutoSlog does not generate the-
matic roles. As a (crude) approximation, we normalize
the extraction patterns with respect to active and passive
voice and label those extractions as agents or patients.
For example, the passive voice pattern ?<subject> were
kidnapped? and the active voice pattern ?kidnapped
<direct object>? are merged into a single normalized
pattern ?kidnapped <patient>?.2 For the sake of sim-
plicity, we will refer to these normalized extraction pat-
terns as caseframes.3 These caseframes can capture two
types of contextual role information: (1) thematic roles
corresponding to events (e.g, ?<agent> kidnapped? or
?kidnapped <patient>?), and (2) predicate-argument re-
lations associated with both verbs and nouns (e.g., ?kid-
napped for <np>? or ?vehicle with <np>?).
We generate these caseframes automatically by run-
ning AutoSlog over the training corpus exhaustively so
that it literally generates a pattern to extract every noun
phrase in the corpus. The learned patterns are then nor-
malized and applied to the corpus. This process produces
a large set of caseframes coupled with a list of the noun
phrases that they extracted. The contextual role knowl-
edge that BABAR uses for coreference resolution is de-
rived from this caseframe data.
2.2.2 The Caseframe Network
The first type of contextual role knowledge
that BABAR learns is the Caseframe Network
(CFNet), which identifies caseframes that co-occur in
anaphor/antecedent resolutions. Our assumption is that
caseframes that co-occur in resolutions often have a
2This normalization is performed syntactically without se-
mantics, so the agent and patient roles are not guaranteed to
hold, but they usually do in practice.
3These are not full case frames in the traditional sense, but
they approximate a simple case frame with a single slot.
conceptual relationship in the discourse. For example,
co-occurring caseframes may reflect synonymy (e.g.,
?<patient> kidnapped? and ?<patient> abducted?)
or related events (e.g., ?<patient> kidnapped? and
?<patient> released?). We do not attempt to identify
the types of relationships that are found. BABAR
merely identifies caseframes that frequently co-occur in
coreference resolutions.
Terrorism Natural Disasters
murder of <NP> <agent> damaged
killed <patient> was injured in <NP>
<agent> reported <agent> occurred
<agent> added cause of <NP>
<agent> stated <agent> wreaked
<agent> added <agent> crossed
perpetrated <patient> driver of <NP>
condemned <patient> <agent> carrying
Figure 1: Caseframe Network Examples
Figure 1 shows examples of caseframes that co-occur
in resolutions, both in the terrorism and natural disaster
domains. The terrorism examples reflect fairly obvious
relationships: people who are murdered are killed; agents
that ?report? things also ?add? and ?state? things; crimes
that are ?perpetrated? are often later ?condemned?. In the
natural disasters domain, agents are often forces of na-
ture, such as hurricanes or wildfires. Figure 1 reveals that
an event that ?damaged? objects may also cause injuries;
a disaster that ?occurred? may be investigated to find its
?cause?; a disaster may ?wreak? havoc as it ?crosses? ge-
ographic regions; and vehicles that have a ?driver? may
also ?carry? items.
During coreference resolution, the caseframe network
provides evidence that an anaphor and prior noun phrase
might be coreferent. Given an anaphor, BABAR iden-
tifies the caseframe that would extract it from its sen-
tence. For each candidate antecedent, BABAR identifies
the caseframe that would extract the candidate, pairs it
with the anaphor?s caseframe, and consults the CF Net-
work to see if this pair of caseframes has co-occurred in
previous resolutions. If so, the CF Network reports that
the anaphor and candidate may be coreferent.
2.2.3 Lexical Caseframe Expectations
The second type of contextual role knowledge learned
by BABAR is Lexical Caseframe Expectations, which are
used by the CFLex knowledge source. For each case-
frame, BABAR collects the head nouns of noun phrases
that were extracted by the caseframe in the training cor-
pus. For each resolution in the training data, BABAR also
associates the co-referring expression of an NP with the
NP?s caseframe. For example, if X and Y are coreferent,
then both X and Y are considered to co-occur with the
caseframe that extracts X as well as the caseframe that
extracts Y. We will refer to the set of nouns that co-occur
with a caseframe as the lexical expectations of the case-
frame. Figure 2 shows examples of lexical expectations
that were learned for both domains.
Terrorism
Caseframe: engaged in <NP>
NPs: activity, battle, clash, dialogue, effort, fight, group,
shoot-out, struggle, village, violence
Caseframe: ambushed <patient>
NPs: company, convoy, helicopter, member, motorcade,
move, Ormeno, patrol, position, response, soldier,
they, troops, truck, vehicle, which
Natural Disasters
Caseframe: battled through <NP>
NPs: flame, night, smoke, wall
Caseframe: braced for <NP>
NPs: arrival, battering, catastrophe, crest, Dolly, epidemics,
evacuate, evacuation, flood, flooding, front, Hortense,
hurricane, misery, rains, river, storm, surge, test, typhoon.
Figure 2: Lexical Caseframe Expectations
To illustrate how lexical expectations are used, suppose
we want to determine whether noun phrase X is the an-
tecedent for noun phrase Y. If they are coreferent, then
X and Y should be substitutable for one another in the
story.4 Consider these sentences:
(S1) Fred was killed by a masked man with a revolver.
(S2) The burglar fired the gun three times and fled.
?The gun? will be extracted by the caseframe ?fired
<patient>?. Its correct antecedent is ?a revolver?, which
is extracted by the caseframe ?killed with <NP>?. If
?gun? and ?revolver? refer to the same object, then it
should also be acceptable to say that Fred was ?killed
with a gun? and that the burglar ?fired a revolver?.
During coreference resolution, BABAR checks (1)
whether the anaphor is among the lexical expectations for
the caseframe that extracts the candidate antecedent, and
(2) whether the candidate is among the lexical expecta-
tions for the caseframe that extracts the anaphor. If either
case is true, then CFLex reports that the anaphor and can-
didate might be coreferent.
2.2.4 Semantic Caseframe Expectations
The third type of contextual role knowledge learned
by BABAR is Semantic Caseframe Expectations. Se-
mantic expectations are analogous to lexical expectations
except that they represent semantic classes rather than
nouns. For each caseframe, BABAR collects the seman-
tic classes associated with the head nouns of NPs that
were extracted by the caseframe. As with lexical expec-
tions, the semantic classes of co-referring expressions are
4They may not be perfectly substitutable, for example one
NP may be more specific (e.g., ?he? vs. ?John F. Kennedy?).
But in most cases they can be used interchangably.
collected too. We will refer to the semantic classes that
co-occur with a caseframe as the semantic expectations
of the caseframe. Figure 3 shows examples of semantic
expectations that were learned. For example, BABAR
learned that agents that ?assassinate? or ?investigate a
cause? are usually humans or groups (i.e., organizations).
Terrorism
Caseframe Semantic Classes
<agent> assassinated group, human
investigation into <NP> event
exploded outside <NP> building
Natural Disasters
Caseframe Semantic Classes
<agent> investigating cause group, human
survivor of <NP> event, natphenom
hit with <NP> attribute, natphenom
Figure 3: Semantic Caseframe Expectations
For each domain, we created a semantic dictionary by
doing two things. First, we parsed the training corpus,
collected all the noun phrases, and looked up each head
noun in WordNet (Miller, 1990). We tagged each noun
with the top-level semantic classes assigned to it in Word-
Net. Second, we identified the 100 most frequent nouns
in the training corpus and manually labeled them with
semantic tags. This step ensures that the most frequent
terms for each domain are labeled (in case some of them
are not in WordNet) and labeled with the sense most ap-
propriate for the domain.
Initially, we planned to compare the semantic classes
of an anaphor and a candidate and infer that they might be
coreferent if their semantic classes intersected. However,
using the top-level semantic classes of WordNet proved
to be problematic because the class distinctions are too
coarse. For example, both a chair and a truck would be la-
beled as artifacts, but this does not at all suggest that they
are coreferent. So we decided to use semantic class in-
formation only to rule out candidates. If two nouns have
mutually exclusive semantic classes, then they cannot be
coreferent. This solution also obviates the need to per-
form word sense disambiguation. Each word is simply
tagged with the semantic classes corresponding to all of
its senses. If these sets do not overlap, then the words
cannot be coreferent.
The semantic caseframe expectations are used in two
ways. One knowledge source, called WordSem-CFSem,
is analogous to CFLex: it checks whether the anaphor and
candidate antecedent are substitutable for one another,
but based on their semantic classes instead of the words
themselves. Given an anaphor and candidate, BABAR
checks (1) whether the semantic classes of the anaphor
intersect with the semantic expectations of the caseframe
that extracts the candidate, and (2) whether the semantic
classes of the candidate intersect with the semantic ex-
pectations of the caseframe that extracts the anaphor. If
one of these checks fails then this knowledge source re-
ports that the candidate is not a viable antecedent for the
anaphor.
A different knowledge source, called CFSem-CFSem,
compares the semantic expectations of the caseframe that
extracts the anaphor with the semantic expectations of the
caseframe that extracts the candidate. If the semantic ex-
pectations do not intersect, then we know that the case-
frames extract mutually exclusive types of noun phrases.
In this case, this knowledge source reports that the candi-
date is not a viable antecedent for the anaphor.
2.3 Assigning Evidence Values
Contextual role knowledge provides evidence as to
whether a candidate is a plausible antecedent for an
anaphor. The two knowledge sources that use semantic
expectations, WordSem-CFSem and CFSem-CFSem, al-
ways return values of -1 or 0. -1 means that an NP should
be ruled out as a possible antecedent, and 0 means that the
knowledge source remains neutral (i.e., it has no reason
to believe that they cannot be coreferent).
The CFLex and CFNet knowledge sources provide
positive evidence that a candidate NP and anaphor might
be coreferent. They return a value in the range [0,1],
where 0 indicates neutrality and 1 indicates the strongest
belief that the candidate and anaphor are coreferent.
BABAR uses the log-likelihood statistic (Dunning, 1993)
to evaluate the strength of a co-occurrence relationship.
For each co-occurrence relation (noun/caseframe for
CFLex, and caseframe/caseframe for CFNet), BABAR
computes its log-likelihood value and looks it up in the

2 table to obtain a confidence level. The confidence
level is then used as the belief value for the knowledge
source. For example, if CFLex determines that the log-
likelihood statistic for the co-occurrence of a particular
noun and caseframe corresponds to the 90% confidence
level, then CFLex returns .90 as its belief that the anaphor
and candidate are coreferent.
3 The Coreference Resolution Model
Given a document to process, BABAR uses four modules
to perform coreference resolution. First, a non-anaphoric
NP classifier identifies definite noun phrases that are exis-
tential, using both syntactic rules and our learned existen-
tial NP recognizer (Bean and Riloff, 1999), and removes
them from the resolution process. Second, BABAR per-
forms reliable case resolution to identify anaphora that
can be easily resolved using the lexical and syntactic
heuristics described in Section 2.1. Third, all remain-
ing anaphora are evaluated by 11 different knowledge
sources: the four contextual role knowledge sources just
described and seven general knowledge sources. Finally,
a Dempster-Shafer probabilistic model evaluates the ev-
idence provided by the knowledge sources for all can-
didate antecedents and makes the final resolution de-
cision. In this section, we describe the seven general
knowledge sources and explain how the Dempster-Shafer
model makes resolutions.
3.1 General Knowledge Sources
Figure 4 shows the seven general knowledge sources
(KSs) that represent features commonly used for corefer-
ence resolution. The gender, number, and scoping KSs
eliminate candidates from consideration. The scoping
heuristics are based on the anaphor type: for reflexive
pronouns the scope is the current clause, for relative pro-
nouns it is the prior clause following its VP, for personal
pronouns it is the anaphor?s sentence and two preced-
ing sentences, and for definite NPs it is the anaphor?s
sentence and eight preceding sentences. The semantic
agreement KS eliminates some candidates, but also pro-
vides positive evidence in one case: if the candidate and
anaphor both have semantic tags human, company, date,
or location that were assigned via NER or the manually
labeled dictionary entries. The rationale for treating these
semantic labels differently is that they are specific and
reliable (as opposed to the WordNet classes, which are
more coarse and more noisy due to polysemy).
KS Function
Gender filters candidate if gender doesn?t agree.
Number filters candidate if number doesn?t agree.
Scoping filters candidate if outside the anaphor?s scope.
Semantic (a) filters candidate if its semantic tags
don?t intersect with those of the anaphor.
(b) supports candidate if selected semantic
tags match those of the anaphor.
Lexical computes degree of lexical overlap
between the candidate and the anaphor.
Recency computes the relative distance between the
candidate and the anaphor.
SynRole computes relative frequency with which the
candidate?s syntactic role occurs in resolutions.
Figure 4: General Knowledge Sources
The Lexical KS returns 1 if the candidate and anaphor
are identical, 0.5 if their head nouns match, and 0 other-
wise. The Recency KS computes the distance between
the candidate and the anaphor relative to its scope. The
SynRole KS computes the relative frequency with which
the candidates? syntactic role (subject, direct object, PP
object) appeared in resolutions in the training set. Dur-
ing development, we sensed that the Recency and Syn-
role KSs did not deserve to be on equal footing with the
other KSs because their knowledge was so general. Con-
sequently, we cut their evidence values in half to lessen
their influence.
3.2 The Dempster-Shafer Decision Model
BABAR uses a Dempster-Shafer decision model (Stefik,
1995) to combine the evidence provided by the knowl-
edge sources. Our motivation for using Dempster-Shafer
is that it provides a well-principled framework for com-
bining evidence from multiple sources with respect to
competing hypotheses. In our situation, the competing
hypotheses are the possible antecedents for an anaphor.
An important aspect of the Dempster-Shafer model is
that it operates on sets of hypotheses. If evidence indi-
cates that hypotheses C and D are less likely than hy-
potheses A and B, then probabilities are redistributed to
reflect the fact that fA, Bg is more likely to contain the
answer than fC, Dg. The ability to redistribute belief val-
ues across sets rather than individual hypotheses is key.
The evidence may not say anything about whether A is
more likely than B, only that C and D are not likely.
Each set is assigned two values: belief and plausibil-
ity. Initially, the Dempster-Shafer model assumes that all
hypotheses are equally likely, so it creates a set called 
that includes all hypotheses.  has a belief value of 1.0,
indicating complete certainty that the correct hypothesis
is included in the set, and a plausibility value of 1.0, in-
dicating that there is no evidence for competing hypothe-
ses.5 As evidence is collected and the likely hypotheses
are whittled down, belief is redistributed to subsets of .
Formally, the Dempster-Shafer theory defines a proba-
bility density function m(S), where S is a set of hypothe-
ses. m(S) represents the belief that the correct hypothe-
sis is included in S. The model assumes that evidence also
arrives as a probability density function (pdf) over sets of
hypotheses.6 Integrating new evidence into the existing
model is therefore simply a matter of defining a function
to merge pdfs, one representing the current belief system
and one representing the beliefs of the new evidence. The
Dempster-Shafer rule for combining pdfs is:
m
3
(S) =
X
X\Y =S
m
1
(X)  m
2
(Y )
1 ?
X
X\Y =;
m
1
(X)  m
2
(Y )
(1)
All sets of hypotheses (and their corresponding belief
values) in the current model are crossed with the sets of
hypotheses (and belief values) provided by the new evi-
dence. Sometimes, however, these beliefs can be contra-
dictory. For example, suppose the current model assigns
a belief value of .60 to fA, Bg, meaning that it is 60%
sure that the correct hypothesis is either A or B. Then
new evidence arrives with a belief value of .70 assigned
5Initially there are no competing hypotheses because all hy-
potheses are included in  by definition.
6Our knowledge sources return some sort of probability es-
timate, although in some cases this estimate is not especially
well-principled (e.g., the Recency KS).
to fCg, meaning that it is 70% sure the correct hypothe-
sis is C. The intersection of these sets is the null set be-
cause these beliefs are contradictory. The belief value
that would have been assigned to the intersection of these
sets is .60*.70=.42, but this belief has nowhere to go be-
cause the null set is not permissible in the model.7 So this
probability mass (.42) has to be redistributed. Dempster-
Shafer handles this by re-normalizing all the belief values
with respect to only the non-null sets (this is the purpose
of the denominator in Equation 1).
In our coreference resolver, we define  to be the set
of all candidate antecedents for an anaphor. Each knowl-
edge source then assigns a probability estimate to each
candidate, which represents its belief that the candidate is
the antecedent for the anaphor. The probabilities are in-
corporated into the Dempster-Shafer model using Equa-
tion 1. To resolve the anaphor, we survey the final be-
lief values assigned to each candidate?s singleton set. If
a candidate has a belief value  .50, then we select that
candidate as the antecedent for the anaphor. If no candi-
date satisfies this condition (which is often the case), then
the anaphor is left unresolved. One of the strengths of the
Dempster-Shafer model is its natural ability to recognize
when several credible hypotheses are still in play. In this
situation, BABAR takes the conservative approach and
declines to make a resolution.
4 Evaluation Results
4.1 Corpora
We evaluated BABAR on two domains: terrorism and
natural disasters. We used the MUC-4 terrorism cor-
pus (MUC-4 Proceedings, 1992) and news articles from
the Reuter?s text collection8 that had a subject code cor-
responding to natural disasters. For each domain, we
created a blind test set by manually annotating 40 doc-
uments with anaphoric chains, which represent sets of
noun phrases that are coreferent (as done for MUC-6
(MUC-6 Proceedings, 1995)). In the terrorism domain,
1600 texts were used for training and the 40 test docu-
ments contained 322 anaphoric links. For the disasters
domain, 8245 texts were used for training and the 40 test
documents contained 447 anaphoric links.
In recent years, coreference resolvers have been evalu-
ated as part of MUC-6 and MUC-7 (MUC-7 Proceedings,
1998). We considered using the MUC-6 and MUC-7 data
sets, but their training sets were far too small to learn reli-
able co-occurrence statistics for a large set of contextual
role relationships. Therefore we opted to use the much
7The Dempster-Shafer theory assumes that one of the hy-
potheses in  is correct, so eliminating all of the hypotheses
violates this assumption.
8Volume 1, English language, 1996-1997, Format version 1,
correction level 0
Terrorism Disasters
Anaphor Rec Pr F Rec Pr F
Def. NPs .43 .79 .55 .42 .91 .58
Pronouns .50 .72 .59 .42 .82 .56
Total .46 .76 .57 .42 .87 .57
Table 2: General Knowledge Sources
Terrorism Disasters
Anaphor Rec Pr F Rec Pr F
Def. NPs .45 .71 .55 .46 .84 .59
Pronouns .63 .73 .68 .57 .79 .66
Total .53 .73 .61 .51 .82 .63
Table 3: General + Contextual Role Knowledge Sources
larger MUC-4 and Reuters corpora.9
4.2 Experiments
We adopted the MUC-6 guidelines for evaluating coref-
erence relationships based on transitivity in anaphoric
chains. For example, if fNP
1
, NP
2
, NP
3
g are all coref-
erent, then each NP must be linked to one of the other two
NPs. First, we evaluated BABAR using only the seven
general knowledge sources. Table 2 shows BABAR?s
performance. We measured recall (Rec), precision (Pr),
and the F-measure (F) with recall and precision equally
weighted. BABAR achieved recall in the 42-50% range
for both domains, with 76% precision overall for terror-
ism and 87% precision for natural disasters. We suspect
that the higher precision in the disasters domain may be
due to its substantially larger training corpus.
Table 3 shows BABAR?s performance when the four
contextual role knowledge sources are added. The F-
measure score increased for both domains, reflecting a
substantial increase in recall with a small decrease in pre-
cision. The contextual role knowledge had the greatest
impact on pronouns: +13% recall for terrorism and +15%
recall for disasters, with a +1% precision gain in terror-
ism and a small precision drop of -3% in disasters.
The difference in performance between pronouns and
definite noun phrases surprised us. Analysis of the data
revealed that the contextual role knowledge is especially
helpful for resolving pronouns because, in general, they
are semantically weaker than definite NPs. Since pro-
nouns carry little semantics of their own, resolving them
depends almost entirely on context. In contrast, even
though context can be helpful for resolving definite NPs,
context can be trumped by the semantics of the nouns
themselves. For example, even if the contexts surround-
ing an anaphor and candidate match exactly, they are not
coreferent if they have substantially different meanings
9We would be happy to make our manually annotated test
data available to others who also want to evaluate their corefer-
ence resolver on the MUC-4 or Reuters collections.
Pronouns Definite NPs
Rec Pr F Rec Pr F
No CF KSs .50 .72 .59 .43 .79 .55
CFLex .56 .74 .64 .42 .73 .53
CFNet .56 .74 .64 .43 .74 .54
CFSem-CFSem .58 .76 .66 .44 .76 .56
WordSem-CFSem .61 .74 .67 .45 .76 .56
All CF KSs .63 .73 .68 .45 .71 .55
Table 4: Individual Performance of KSs for Terrorism
Pronouns Definite NPs
Rec Pr F Rec Pr F
No CF KSs .42 .82 .56 .42 .91 .58
CFLex .48 .83 .61 .44 .88 .59
CFNet .45 .82 .58 .43 .88 .57
CFSem-CFSem .51 .81 .62 .44 .87 .58
WordSem-CFSem .52 .79 .63 .43 .86 .57
All CF KSs .57 .79 .66 .46 .84 .59
Table 5: Individual Performance of KSs for Disasters
(e.g., ?the mayor? vs. ?the journalist?).
We also performed experiments to evaluate the impact
of each type of contextual role knowledge separately. Ta-
bles 4 and 5 show BABAR?s performance when just one
contextual role knowledge source is used at a time. For
definite NPs, the results are a mixed bag: some knowl-
edge sources increased recall a little, but at the expense
of some precision. For pronouns, however, all of the
knowledge sources increased recall, often substantially,
and with little if any decrease in precision. This result
suggests that all of contextual role KSs can provide use-
ful information for resolving anaphora. Tables 4 and 5
also show that putting all of the contextual role KSs in
play at the same time produces the greatest performance
gain. There are two possible reasons: (1) the knowl-
edge sources are resolving different cases of anaphora,
and (2) the knowledge sources provide multiple pieces of
evidence in support of (or against) a candidate, thereby
acting synergistically to push the Dempster-Shafer model
over the belief threshold in favor of a single candidate.
5 Related Work
Many researchers have developed coreference resolvers,
so we will only discuss the methods that are most closely
related to BABAR. Dagan and Itai (Dagan and Itai, 1990)
experimented with co-occurrence statistics that are sim-
ilar to our lexical caseframe expectations. Their work
used subject-verb, verb-object, and adjective-noun rela-
tions to compare the contexts surrounding an anaphor and
candidate. However their work did not consider other
types of lexical expectations (e.g., PP arguments), seman-
tic expectations, or context comparisons like our case-
frame network.
(Niyu et al, 1998) used unsupervised learning to ac-
quire gender, number, and animacy information from res-
olutions produced by a statistical pronoun resolver. The
learned information was recycled back into the resolver
to improve its performance. This approach is similar to
BABAR in that they both acquire knowledge from ear-
lier resolutions. (Kehler, 1997) also used a Dempster-
Shafer model to merge evidence from different sources
for template-level coreference.
Several coreference resolvers have used supervised
learning techniques, such as decision trees and rule learn-
ers (Aone and Bennett, 1995; McCarthy and Lehnert,
1995; Ng and Cardie, 2002; Soon et al, 2001). These
systems rely on a training corpus that has been manually
annotated with coreference links.
6 Conclusions
The goal of our research was to explore the use of contex-
tual role knowledge for coreference resolution. We iden-
tified three ways that contextual roles can be exploited:
(1) by identifying caseframes that co-occur in resolu-
tions, (2) by identifying nouns that co-occur with case-
frames and using them to cross-check anaphor/candidate
compatibility, (3) by identifying semantic classes that co-
occur with caseframes and using them to cross-check
anaphor/candidate compatability. We combined evidence
from four contextual role knowledge sources with ev-
idence from seven general knowledge sources using a
Dempster-Shafer probabilistic model.
Our coreference resolver performed well in two do-
mains, and experiments showed that each contextual role
knowledge source contributed valuable information. We
found that contextual role knowledge was more beneficial
for pronouns than for definite noun phrases. This sug-
gests that different types of anaphora may warrant differ-
ent treatment: definite NP resolution may depend more
on lexical semantics, while pronoun resolution may de-
pend more on contextual semantics. In future work, we
plan to follow-up on this approach and investigate other
ways that contextual role knowledge can be used.
7 Acknowledgements
This work was supported in part by the National Sci-
ence Foundation under grant IRI-9704240. The inven-
tions disclosed herein are the subject of a patent applica-
tion owned by the University of Utah and licensed on an
exclusive basis to Attensity Corporation.
References
J. Allen. 1995. Natural Language Understanding. Ben-
jamin/Cummings Press, Redwood City, CA.
C. Aone and S. Bennett. 1995. Applying Machine Learning
to Anaphora Resolution. In IJCAI-95 Workshop on New Ap-
proaches to Learning for NLP.
D. Bean and E. Riloff. 1999. Corpus-Based Identification of
Non-Anaphoric Noun Phrases. In Proc. of the 37th Annual
Meeting of the Association for Computational Linguistics.
I. Dagan and A. Itai. 1990. Automatic Processing of Large
Corpora for the Resolution of Anaphora References. In Pro-
ceedings of the Thirteenth International Conference on Com-
putational Linguistics (COLING-90), pages 330?332.
T. Dunning. 1993. Accurate methods for the statistics of sur-
prise and coincidence. Computational Linguistics, 19(1):61?
74.
B. Grosz and C. Sidner. 1998. Lost Intuitions and Forgotten In-
tentions. In M. Walker, A. Joshi, and E. Prince, editors, Cen-
tering Theory in Discourse, pages 89?112. Clarendon Press.
B. Grosz, A. Joshi, and S. Weinstein. 1995. Centering: A
Framework for Modeling the Local Coherence of Discourse.
Computational Linguistics, 21(2):203?226.
J. Hobbs. 1978. Resolving Pronoun References. Lingua,
44(4):311?338.
A. Kehler. 1997. Probabilistic Coreference in Information Ex-
traction. In Proceedings of the Second Conference on Em-
pirical Methods in Natural Language Processing.
S. Lappin and H. Leass. 1994. An algorithm for pronominal
anaphora resolution. Computational Linguistics, 20(4):535?
561.
J. McCarthy and W. Lehnert. 1995. Using Decision Trees for
Coreference Resolution. In Proc. of the Fourteenth Interna-
tional Joint Conference on Artificial Intelligence.
G. Miller. 1990. Wordnet: An On-line Lexical Database. In-
ternational Journal of Lexicography, 3(4).
MUC-4 Proceedings. 1992. Proceedings of the Fourth Mes-
sage Understanding Conference (MUC-4).
MUC-6 Proceedings. 1995. Proceedings of the Sixth Message
Understanding Conference (MUC-6).
MUC-7 Proceedings. 1998. Proceedings of the Seventh Mes-
sage Understanding Conference (MUC-7).
V. Ng and C. Cardie. 2002. Improving Machine Learning Ap-
proaches to Coreference Resolution. In Proceedings of the
40th Annual Meeting of the Association for Computational
Linguistics.
G. Niyu, J. Hale, and E. Charniak. 1998. A statistical approach
to anaphora resolution. In Proceedings of the Sixth Workshop
on Very Large Corpora.
E. Riloff. 1996. An Empirical Study of Automated Dictionary
Construction for Information Extraction in Three Domains.
Artificial Intelligence, 85:101?134.
W. Soon, H. Ng, and D. Lim. 2001. A Machine Learning Ap-
proach to Coreference of Noun Phrases. Computational Lin-
guistics, 27(4):521?541.
M. Stefik. 1995. Introduction to Knowledge Systems. Morgan
Kaufmann, San Francisco, CA.
Proceedings of ACL-08: HLT, pages 1048?1056,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Semantic Class Learning from the Web with Hyponym Pattern Linkage
Graphs
Zornitsa Kozareva
DLSI, University of Alicante
Campus de San Vicente
Alicante, Spain 03080
zkozareva@dlsi.ua.es
Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT 84112
riloff@cs.utah.edu
Eduard Hovy
USC Information Sciences Institute
4676 Admiralty Way
Marina del Rey, CA 90292-6695
hovy@isi.edu
Abstract
We present a novel approach to weakly super-
vised semantic class learning from the web,
using a single powerful hyponym pattern com-
bined with graph structures, which capture
two properties associated with pattern-based
extractions: popularity and productivity. In-
tuitively, a candidate is popular if it was dis-
covered many times by other instances in the
hyponym pattern. A candidate is productive
if it frequently leads to the discovery of other
instances. Together, these two measures cap-
ture not only frequency of occurrence, but also
cross-checking that the candidate occurs both
near the class name and near other class mem-
bers. We developed two algorithms that begin
with just a class name and one seed instance
and then automatically generate a ranked list
of new class instances. We conducted exper-
iments on four semantic classes and consis-
tently achieved high accuracies.
1 Introduction
Knowing the semantic classes of words (e.g., ?trout?
is a kind of FISH) can be extremely valuable for
many natural language processing tasks. Although
some semantic dictionaries do exist (e.g., Word-
Net (Miller, 1990)), they are rarely complete, espe-
cially for large open classes (e.g., classes of people
and objects) and rapidly changing categories (e.g.,
computer technology). (Roark and Charniak, 1998)
reported that 3 of every 5 terms generated by their
semantic lexicon learner were not present in Word-
Net. Automatic semantic lexicon acquisition could
be used to enhance existing resources such as Word-
Net, or to produce semantic lexicons for specialized
categories or domains.
A variety of methods have been developed for
automatic semantic class identification, under the
rubrics of lexical acquisition, hyponym acquisition,
semantic lexicon induction, semantic class learn-
ing, and web-based information extraction. Many
of these approaches employ surface-level patterns to
identify words and their associated semantic classes.
However, such patterns tend to overgenerate (i.e.,
deliver incorrect results) and hence require addi-
tional filtering mechanisms.
To overcome this problem, we employed one sin-
gle powerful doubly-anchored hyponym pattern to
query the web and extract semantic class instances:
CLASS NAME such as CLASS MEMBER and *.
We hypothesized that a doubly-anchored pattern,
which includes both the class name and a class
member, would achieve high accuracy because of
its specificity. To address concerns about coverage,
we embedded the search in a bootstrapping process.
This method produced many correct instances, but
despite the highly restrictive nature of the pattern,
still produced many incorrect instances. This re-
sult led us to explore new ways to improve the ac-
curacy of hyponym patterns without requiring addi-
tional training resources.
The main contribution of this work is a novel
method for combining hyponym patterns with graph
structures that capture two properties associated
with pattern extraction: popularity and productivity.
Intuitively, a candidate word (or phrase) is popular
if it was discovered many times by other words (or
1048
phrases) in a hyponym pattern. A candidate word is
productive if it frequently leads to the discovery of
other words. Together, these two measures capture
not only frequency of occurrence, but also cross-
checking that the word occurs both near the class
name and near other class members.
We present two algorithms that use hyponym pat-
tern linkage graphs (HPLGs) to represent popularity
and productivity information. The first method uses
a dynamically constructed HPLG to assess the pop-
ularity of each candidate and steer the bootstrapping
process. This approach produces an efficient boot-
strapping process that performs reasonably well, but
it cannot take advantage of productivity information
because of the dynamic nature of the process.
The second method is a two-step procedure that
begins with an exhaustive pattern search that ac-
quires popularity and productivity information about
candidate instances. The candidates are then ranked
based on properties of the HPLG. We conducted ex-
periments with four semantic classes, achieving high
accuracies and outperforming the results reported by
others who have worked on the same classes.
2 Related Work
A substantial amount of research has been done in
the area of semantic class learning, under a variety
of different names and with a variety of different
goals. Given the great deal of similar work in infor-
mation extraction and ontology learning, we focus
here only on techniques for weakly supervised or
unsupervised semantic class (i.e., supertype-based)
learning, since that is most related to the work in
this paper.
Fully unsupervised semantic clustering (e.g.,
(Lin, 1998; Lin and Pantel, 2002; Davidov and Rap-
poport, 2006)) has the disadvantage that it may or
may not produce the types and granularities of se-
mantic classes desired by a user. Another related
line of work is automated ontology construction,
which aims to create lexical hierarchies based on se-
mantic classes (e.g., (Caraballo, 1999; Cimiano and
Volker, 2005; Mann, 2002)), and learning semantic
relations such as meronymy (Berland and Charniak,
1999; Girju et al, 2003).
Our research focuses on semantic lexicon induc-
tion, which aims to generate lists of words that be-
long to a given semantic class (e.g., lists of FISH
or VEHICLE words). Weakly supervised learning
methods for semantic lexicon generation have uti-
lized co-occurrence statistics (Riloff and Shepherd,
1997; Roark and Charniak, 1998), syntactic in-
formation (Tanev and Magnini, 2006; Pantel and
Ravichandran, 2004; Phillips and Riloff, 2002),
lexico-syntactic contextual patterns (e.g., ?resides
in <location>? or ?moved to <location>?) (Riloff
and Jones, 1999; Thelen and Riloff, 2002), and
local and global contexts (Fleischman and Hovy,
2002). These methods have been evaluated only on
fixed corpora1, although (Pantel et al, 2004) demon-
strated how to scale up their algorithms for the web.
Several techniques for semantic class induction
have also been developed specifically for learning
from the web. (Pas?ca, 2004) uses Hearst?s pat-
terns (Hearst, 1992) to learn semantic class instances
and class groups by acquiring contexts around the
pattern. Pasca also developed a second technique
(Pas?ca, 2007b) that creates context vectors for a
group of seed instances by searching web query
logs, and uses them to learn similar instances.
The work most closely related to ours is Hearst?s
early work on hyponym learning (Hearst, 1992)
and more recent work that has followed up on her
idea. Hearst?s system exploited patterns that explic-
itly identify a hyponym relation between a seman-
tic class and a word (e.g., ?such authors as Shake-
speare?). We will refer to these as hyponym pat-
terns. Pasca?s previously mentioned system (Pas?ca,
2004) applies hyponym patterns to the web and ac-
quires contexts around them. The KnowItAll system
(Etzioni et al, 2005) also uses hyponym patterns to
extract class instances from the web and then evalu-
ates them further by computing mutual information
scores based on web queries.
The work by (Widdows and Dorow, 2002) on lex-
ical acquisition is similar to ours because they also
use graph structures to learn semantic classes. How-
ever, their graph is based entirely on syntactic rela-
tions between words, while our graph captures the
ability of instances to find each other in a hyponym
pattern based on web querying, without any part-of-
speech tagging or parsing.
1Meta-bootstrapping (Riloff and Jones, 1999) was evaluated
on web pages, but used a precompiled corpus of downloaded
web pages.
1049
3 Semantic Class Learning with Hyponym
Pattern Linkage Graphs
3.1 A Doubly-Anchored Hyponym Pattern
Our work was motivated by early research on hy-
ponym learning (Hearst, 1992), which applied pat-
terns to a corpus to associate words with semantic
classes. Hearst?s system exploited patterns that ex-
plicitly link a class name with a class member, such
as ?X and other Ys? and ?Ys such as X?. Relying
on surface-level patterns, however, is risky because
incorrect items are frequently extracted due to poly-
semy, idiomatic expressions, parsing errors, etc.
Our work began with the simple idea of using an
extremely specific pattern to extract semantic class
members with high accuracy. Our expectation was
that a very specific pattern would virtually eliminate
the most common types of false hits that are caused
by phenomena such as polysemy and idiomatic ex-
pressions. A concern, however, was that an ex-
tremely specific pattern would suffer from sparse
data and not extract many new instances. By using
the web as a corpus, we hoped that the pattern could
extract at least a few instances for virtually any class,
and then we could gain additional traction by boot-
strapping these instances.
All of the work presented in this paper uses just
one doubly-anchored pattern to identify candidate
instances for a semantic class:
<class name> such as <class member> and *
This pattern has two variables: the name of the se-
mantic class to be learned (class name) and a mem-
ber of the semantic class (class member). The aster-
isk (*) indicates the location of the extracted words.
We describe this pattern as being doubly-anchored
because it is instantiated with both the name of the
semantic class as well as a class member.
For example, the pattern ?CARS such as FORD
and *? will extract automobiles, and the pattern
?PRESIDENTS such as FORD and *? will extract
presidents. The doubly-anchored nature of the pat-
tern serves two purposes. First, it increases the like-
lihood of finding a true list construction for the class.
Our system does not use part-of-speech tagging or
parsing, so the pattern itself is the only guide for
finding an appropriate linguistic context.
Second, the doubly-anchored pattern virtually
Members = {Seed};
P0= ?Class such as Seed and *?;
P = {P0};
iter = 0;
While ((iter < Max Iters) and (P 6= {}))
iter++;
For each Pi ? P
Snippets = web query(Pi);
Candidates = extract words(Snippets,Pi);
Pnew = {};
For each Candidatek ? Candidates
If (Candidatek /? Members);
Members = Members ? {Candidatek};
Pk= ?Class such as Candidatek and *?;
Pnew = Pnew ? { Pk };
P = Pnew;
Figure 1: Reckless Bootstrapping
eliminates ambiguity because the class name and
class member mutually disambiguate each other.
For example, the word FORD could refer to an auto-
mobile or a person, but in the pattern ?CARS such as
FORD and *? it will almost certainly refer to an au-
tomobile. Similarly, the class ?PRESIDENT? could
refer to country presidents or corporate presidents,
and ?BUSH? could refer to a plant or a person. But
in the pattern ?PRESIDENTS such as BUSH?, both
words will surely refer to country presidents.
Another advantage of the doubly-anchored pat-
tern is that an ambiguous or underspecified class
name will be constrained by the presence of the class
member. For example, to generate a list of com-
pany presidents, someone might naively define the
class name as PRESIDENTS. A singly-anchored pat-
tern (e.g., ?PRESIDENTS such as *?) might gener-
ate lists of other types of presidents (e.g., country
presidents, university presidents, etc.). Because the
doubly-anchored pattern also requires a class mem-
ber (e.g., ?PRESIDENTS such as BILL GATES and
*?), it is likely to generate only the desired types of
instances.
3.2 Reckless Bootstrapping
To evaluate the performance of the doubly-anchored
pattern, we began by using the pattern to search the
web and embedded this process in a simple boot-
strapping loop, which is presented in Figure 1. As
input, the user must provide the name of the desired
1050
semantic class (Class) and a seed example (Seed),
which are used to instantiate the pattern. On the
first iteration, the pattern is given to Google as a
web query, and new class members are extracted
from the retrieved text snippets. We wanted the
system to be as language-independent as possible,
so we refrained from using any taggers or parsing
tools. As a result, instances are extracted using only
word boundaries and orthographic information. For
proper name classes, we extract all capitalized words
that immediately follow the pattern. For common
noun classes, we extract just one word, if it is not
capitalized. Examples are shown below, with the ex-
tracted items underlined:
countries such as China and Sri Lanka are ...
fishes such as trout and bass can ...
One limitation is that our system cannot learn
multi-word instances of common noun categories,
or proper names that include uncapitalized words
(e.g., ?United States of America?). These limita-
tions could be easily overcome by incorporating a
noun phrase (NP) chunker and extracting NPs.
Each new class member is then used as a seed in-
stance in the bootstrapping loop. We implemented
this process as breadth-first search, where each ?ply?
of the search process is the result of bootstrapping
the class members learned during the previous it-
eration as seed instances for the next one. During
each iteration, we issue a new web query and add
the newly extracted class members to the queue for
the next cycle. We run this bootstrapping process for
a fixed number of iterations (search ply), or until no
new class members are produced. We will refer to
this process as reckless bootstrapping because there
are no checks of any kind. Every term extracted by
the pattern is assumed to be a class member.
3.2.1 Results
Table 1 shows the results for 4 iterations of reck-
less bootstrapping for four semantic categories: U.S.
states, countries, singers, and fish. The first two
categories are relatively small, closed sets (our gold
standard contains 50 U.S. states and 194 countries).
The singers and fish categories are much larger, open
sets (see Section 4 for details).
Table 1 reveals that the doubly-anchored pattern
achieves high accuracy during the first iteration, but
Iter. countries states singers fish
1 .80 .79 .91 .76
2 .57 .21 .87 .64
3 .21 .18 .86 .54
4 .16 ? .83 .54
Table 1: Reckless Bootstrapping Accuracies
quality deteriorates rapidly as bootstrapping pro-
gresses. Figure 2 shows the recall and precision
curves for countries and states. High precision is
achieved only with low levels of recall for countries.
Our initial hypothesis was that such a specific pat-
tern would be able to maintain high precision be-
cause non-class members would be unlikely to co-
occur with the pattern. But we were surprised to find
that many incorrect entries were generated for rea-
sons such as broken expressions like ?Merce -dez?,
misidentified list constructions (e.g., ?In countries
such as China U.S. Policy is failing...?), and incom-
plete proper names due to insufficient length of the
retrieved text snippet.
Incorporating a noun phrase chunker would elim-
inate some of these cases, but far from all of them.
We concluded that even such a restrictive pattern is
not sufficient for semantic class learning on its own.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Pr
ec
is
io
n
Recall
Country/State
Country
State
Figure 2: Recall/precision for reckless bootstrapping
In the next section, we present a new approach
that creates a Hyponym Pattern Linkage Graph to
steer bootstrapping and improve accuracy.
3.3 Using Dynamic Graphs to Steer
Bootstrapping
Intuitively, we expect true class members to occur
frequently in pattern contexts with other class mem-
1051
bers. To operationalize this intuition, we create a hy-
ponym pattern linkage graph, which represents the
frequencies with which candidate instances generate
each other in the pattern contexts.
We define a hyponym pattern linkage graph
(HPLG) as a G = (V,E), where each vertex v ? V
is a candidate instance and each edge (u, v) ? E
means that instance v was generated by instance u.
The weight w of an edge is the frequency with which
u generated v. For example, consider the following
sentence, where the pattern is italicized and the ex-
tracted instance is underlined:
Countries such as China and Laos have been...
In the HPLG, an edge e = (China, Laos) would
be created because the pattern anchored by China
extracted Laos as a new candidate instance. If this
pattern extracted Laos from 15 different snippets,
then the edge?s weight would be 15. The in-degree
of a node represents its popularity, i.e., the number
of instance occurrences that generated it.
The graph is constructed dynamically as boot-
strapping progresses. Initially, the seed is the only
trusted class member and the only vertex in the
graph. The bootstrapping process begins by instan-
tiating the doubly-anchored pattern with the seed
class member, issuing a web query to generate new
candidate instances, and adding these new instances
to the graph. A score is then assigned to every node
in the graph, using one of several different metrics
defined below. The highest-scoring unexplored node
is then added to the set of trusted class members, and
used as the seed for the next bootstrapping iteration.
We experimented with three scoring functions for
selecting nodes. The In-Degree (inD) score for ver-
tex v is the sum of the weights of all incoming edges
(u, v), where u is a trusted class member. Intuitively,
this captures the popularity of v among instances
that have already been identified as good instances.
The Best Edge (BE) score for vertex v is the maxi-
mum edge weight among the incoming edges (u, v),
where u is a trusted class member.
The Key Player Problem (KPP) measure is used in
social network analysis (Borgatti and Everett, 2006)
to identify nodes whose removal would result in a
residual network of minimum cohesion. A node re-
ceives a high value if it is highly connected and rel-
atively close to most other nodes in the graph. The
KPP score for vertex v is computed as:
KPP (v) =
?
u?V
1
d(u, v)
|V |?1
where d(u, v) is the shortest path between two ver-
tices, where u is a trusted node. For tie-breaking, the
distances are multiplied by the weight of the edge.
Note that all of these measures rely only on in-
coming edges because a node does not acquire out-
going edges until it has already been selected as a
trusted class member and used to acquire new in-
stances. In the next section, we describe a two-step
process for creating graphs that can take advantage
of both incoming and outgoing edges.
3.4 Re-Ranking with Precompiled Graphs
One way to try to confirm (or disconfirm) whether
a candidate instance is a true class member is to see
whether it can produce new candidate instances. If
we instantiate our pattern with the candidate (i.e.,
?CLASS NAME such as CANDIDATE and *?) and
successfully extract many new instances, then this
is evidence that the candidate frequently occurs with
the CLASS NAME in list constructions. We will re-
fer to the ability of a candidate to generate new in-
stances as its productivity.
The previous bootstrapping algorithm uses a dy-
namically constructed graph that is constantly evolv-
ing as new nodes are selected and explored. Each
node is scored based only on the set of instances
that have been generated and identified as ?trusted?
at that point in the bootstrapping process. To use
productivity information, we must adopt a different
procedure because we need to know not only who
generated each candidate, but also the complete set
of instances that the candidate itself can generate.
We adopted a two-step process that can use both
popularity and productivity information in a hy-
ponym pattern linkage graph to assess the quality of
candidate instances. First, we perform reckless boot-
strapping for a class name and seed until no new
instances are generated. Second, we assign a score
to each node in the graph using a scoring function
that takes into account both the in-degree (popular-
ity) and out-degree (productivity) of each node. We
experimented with four different scoring functions,
some of which were motivated by work on word
1052
sense disambiguation to identify the most ?impor-
tant? node in a graph containing its possible senses
(Navigli and Lapata, 2007).
The Out-degree (outD) score for vertex v is the
weighted sum of v?s outgoing edges, normalized by
the number of other nodes in the graph.
outD(v) =
?
?(v,p)?E
w(v, p)
|V |?1
This measure captures only productivity, while the
next three measures consider both productivity and
popularity. The Total-degree (totD) score for ver-
tex v is the weighted sum of both incoming and
outgoing edges, normalized by the number of other
nodes in the graph. The Betweenness (BT) score
(Freeman, 1979) considers a vertex to be important
if it occurs on many shortest paths between other
vertices.
BT (v) =
?
s,t?V :s 6=v 6=t
?st(v)
?st
where ?st is the number of shortest paths from s to t,
and ?st(v) is the number of shortest paths from s to
t that pass through vertex v. PageRank (Page et al,
1998) establishes the relative importance of a ver-
tex v through an iterative Markov chain model. The
PageRank (PR) score of a vertex v is determined
on the basis of the nodes it is connected to.
PR(v) = (1??)|V | + ?
?
u,v?E
PR(u)
outdegree(u)
? is a damping factor that we set to 0.85. We dis-
carded all instances that produced zero productivity
links, meaning that they did not generate any other
candidates when used in web queries.
4 Experimental evaluation
4.1 Data
We evaluated our algorithms on four semantic cat-
egories: U.S. states, countries, singers, and fish.
The states and countries categories are relatively
small, closed sets: our gold standards consist of 50
U.S. states and 194 countries (based on a list found
on Wikipedia). The singers and fish categories are
much larger, open classes. As our gold standard for
fish, we used a list of common fish names found on
Wikipedia.2 All the singer names generated by our
2We also counted as correct plural versions of items found
on the list. The total size of our fish list is 1102.
States
Popularity Prd Pop&Prd
N BE KPP inD outD totD BT PR
25 1.0 1.0 1.0 1.0 1.0 .88 .88
50 .96 .98 .98 1.0 1.0 .86 .82
64 .77 .78 .77 .78 .78 .77 .67
Countries
Popularity Prd Pop&Prd
N BE KPP inD outD totD BT PR
50 .98 .97 .98 1.0 1.0 .98 .97
100 .96 .97 .94 1.0 .99 .97 .95
150 .90 .92 .91 1.0 .95 .94 .92
200 .83 .81 .83 .90 .87 .82 .80
300 .60 .59 .61 .61 .62 .56 .60
323 .57 .55 .57 .57 .58 .52 .57
Singers
Popularity Prd Pop&Prd
N BE KPP inD outD totD BT PR
10 .92 .96 .92 1.0 1.0 1.0 1.0
25 .89 .90 .91 1.0 1.0 1.0 .99
50 .92 .85 .92 .97 .98 .95 .97
75 .89 .83 .91 .96 .95 .93 .95
100 .86 .81 .89 .96 .93 .94 .94
150 .86 .79 .88 .95 .92 .93 .87
180 .86 .80 .87 .91 .91 .91 .88
Fish
Popularity Prd Pop&Prd
N BE KPP inD outD totD BT PR
10 .90 .90 .90 1.0 1.0 .90 .70
25 .80 .88 .76 1.0 .96 .96 .72
50 .82 .80 .78 1.0 .94 .88 .66
75 .72 .69 .72 .93 .87 .79 .64
100 .63 .68 .66 .84 .80 .74 .62
116 .60 .65 .66 .80 .78 .71 .59
Table 2: Accuracies for each semantic class
algorithms were manually reviewed for correctness.
We evaluated performance in terms of accuracy (the
percentage of instances that were correct).3
4.2 Performance
Table 2 shows the accuracy results of the two al-
gorithms that use hyponym pattern linkage graphs.
We display results for the top-ranked N candidates,
for all instances that have a productivity value >
zero.4 The Popularity columns show results for the
3We never generated duplicates so the instances are distinct.
4Obviously, this cutoff is not available to the popularity-
based bootstrapping algorithm, but here we are just comparing
the top N results for both algorithms.
1053
bootstrapping algorithm described in Section 3.3,
using three different scoring functions. The re-
sults for the ranking algorithm described in Sec-
tion 3.4 are shown in the Productivity (Prd) and
Popularity&Productivity (Pop&Prd) columns. For
the states, countries, and singers categories, we ran-
domly selected 5 different initial seeds and then av-
eraged the results. For the fish category we ran each
algorithm using just the seed ?salmon?.
The popularity-based metrics produced good ac-
curacies on the states, countries, and singers cate-
gories under all 3 scoring functions. For fish, KPP
performed better than the others.
The Out-degree (outD) scoring function, which
uses only Productivity information, obtained the
best results across all 4 categories. OutD achieved
100% accuracy for the first 50 states and fish, 100%
accuracy for the top 150 countries, and 97% accu-
racy for the top 50 singers. The three scoring met-
rics that use both popularity and productivity also
performed well, but productivity information by it-
self seems to perform better in some cases.
It can be difficult to compare the results of differ-
ent semantic class learners because there is no stan-
dard set of benchmark categories, so researchers re-
port results for different classes. For the state and
country categories, however, we can compare our
results with that of other web-based semantic class
learners such as Pasca (Pas?ca, 2007a) and the Know-
ItAll system (Etzioni et al, 2005). For the U.S.
states category, our system achieved 100% recall
and 100% precision for the first 50 items generated,
and KnowItAll performed similarly achieving 98%
recall with 100% precision. Pasca did not evaluate
his system on states.
For the countries category, our system achieved
100% precision for the first 150 generated instances
(77% recall). (Pas?ca, 2007a) reports results of 100%
precision for the first 25 instances generated, and
82% precision for the first 150 instances gener-
ated. The KnowItAll system (Etzioni et al, 2005)
achieved 97% precision with 58% recall, and 79%
precision with 87% recall.5 To the best of our
knowledge, other researchers have not reported re-
sults for the singer and fish categories.
5(Etzioni et al, 2005) do not report exactly how many coun-
tries were in their gold standard.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  50  100  150  200  250  300  350  400
Ac
cu
ra
cy
Iterations
outD
inD
cutoff, t
Figure 3: Learning curve for Placido Domingo
Figure 3 shows the learning curve for both al-
gorithms using their best scoring functions on the
singer category with Placido Domingo as the initial
seed. In total, 400 candidate words were generated.
The Out-degree scoring function ranked the candi-
dates well. Figure 3 also includes a vertical line
indicating where the candidate list was cut (at 180
instances) based on the zero productivity cutoff.
One observation is that the rankings do a good
job of identifying borderline cases, which typically
are ranked just below most correct instances but just
above the obviously bad entries. For example, for
states, the 50 U.S. states are ranked first, followed
by 14 more entries (in order):
Russia, Ukraine, Uzbekistan, Azerbaijan,
Moldova, Tajikistan, Armenia, Chicago,
Boston, Atlanta, Detroit, Philadelphia, Tampa,
Moldavia
The first 7 entries are all former states of the So-
viet Union. In retrospect, we realized that we
should have searched for ?U.S. states? instead of just
?states?. This example illustrates the power of the
doubly-anchored hyponym pattern to correctly iden-
tify our intended semantic class by disambiguating
our class name based on the seed class member.
The algorithms also seem to be robust with re-
spect to initial seed choice. For the states, coun-
tries, and singers categories, we ran experiments
with 5 different initial seeds, which were randomly
selected. The 5 country seeds represented a diverse
set of nations, some of which are rarely mentioned in
the news: Brazil, France, Guinea-Bissau, Uganda,
1054
and Zimbabwe. All of these seeds obtained ? 92%
recall with ? 90% precision.
4.3 Error Analysis
We examined the incorrect instances produced by
our algorithms and found that most of them fell into
five categories.
Type 1 errors were caused by incorrect proper
name extraction. For example, in the sentence
?states such as Georgia and English speaking coun-
tries like Canada...?, ?English? was extracted as
a state. These errors resulted from complex noun
phrases and conjunctions, as well as unusual syn-
tactic constructions. An NP chunker might prevent
some of these cases, but we suspect that many of
them would have been misparsed regardless.
Type 2 errors were caused by instances that for-
merly belonged to the semantic class (e.g., Serbia-
Montenegro and Czechoslovakia are no longer coun-
tries). In this error type, we also include border-
line cases that could arguably belong to the semantic
class (e.g., Wales as a country).
Type 3 errors were spelling variants (e.g., Kyrgys-
tan vs. Kyrgyzhstan) and name variants (e.g., Bey-
once vs. Beyonce Knowles). Officially, every entity
has one official spelling and one complete name, but
in practice there are often variations that may occur
nearly as frequently as the official name. For exam-
ple, it is most common to refer to the singer Beyonce
by just her first name.
Type 4 errors were caused by sentences that were
just flat out wrong in their factual assertions. For ex-
ample, some sentences referred to ?North America?
as a country.
Type 5 errors were caused by broken expressions
found in the retrieved snippets (e.g. Michi -gan).
These errors may be fixable by cleaning up the web
pages or applying heuristics to prevent or recognize
partial words.
It is worth noting that incorrect instances of Types
2 and 3 may not be problematic to encounter in a
dictionary or ontology. Name variants and former
class members may in fact be useful to have.
5 Conclusions
Combining hyponym patterns with pattern linkage
graphs is an effective way to produce a highly ac-
curate semantic class learner that requires truly min-
imal supervision: just the class name and one class
member as a seed. Our results consistently produced
high accuracy and for the states and countries cate-
gories produced very high recall.
The singers and fish categories, which are much
larger open classes, also achieved high accuracy and
generated many instances, but the resulting lists are
far from complete. Even on the web, the doubly-
anchored hyponym pattern eventually ran out of
steam and could not produce more instances. How-
ever, all of our experiments were conducted using
just a single hyponym pattern. Other researchers
have successfully used sets of hyponym patterns
(e.g., (Hearst, 1992; Etzioni et al, 2005; Pas?ca,
2004)), and multiple patterns could be used with
our algorithms as well. Incorporating additional hy-
ponym patterns will almost certainly improve cover-
age, and could potentially improve the quality of the
graphs as well.
Our popularity-based algorithm was very effec-
tive and is practical to use. Our best-performing al-
gorithm, however, was the 2-step process that be-
gins with an exhaustive search (reckless bootstrap-
ping) and then ranks the candidates using the Out-
degree scoring function, which represents produc-
tivity. The first step is expensive, however, because
it exhaustively applies the pattern to the web until
no more extractions are found. In our evaluation, we
ran this process on a single PC and it usually finished
overnight, and we were able to learn a substantial
number of new class instances. If more hyponym
patterns are used, then this could get considerably
more expensive, but the process could be easily par-
allelized to perform queries across a cluster of ma-
chines. With access to a cluster of ordinary PCs,
this technique could be used to automatically create
extremely large, high-quality semantic lexicons, for
virtually any categories, without external training re-
sources.
Acknowledgments
This research was supported in part by the Department
of Homeland Security under ONR Grants N00014-07-1-014
and N0014-07-1-0152, the European Union Sixth Framework
project QALLME FP6 IST-033860, and the Spanish Ministry
of Science and Technology TEXT-MESS TIN2006-15265-C06-
01.
1055
References
M. Berland and E. Charniak. 1999. Finding Parts in Very
Large Corpora. In Proc. of the 37th Annual Meeting of
the Association for Computational Linguistics.
S. Borgatti and M. Everett. 2006. A graph-theoretic per-
spective on centrality. Social Networks, 28(4).
S. Caraballo. 1999. Automatic Acquisition of a
Hypernym-Labeled Noun Hierarchy from Text. In
Proc. of the 37th Annual Meeting of the Association
for Computational Linguistics, pages 120?126.
P. Cimiano and J. Volker. 2005. Towards large-scale,
open-domain and ontology-based named entity classi-
fication. In Proc. of Recent Advances in Natural Lan-
guage Processing, pages 166?172.
D. Davidov and A. Rappoport. 2006. Efficient unsu-
pervised discovery of word categories using symmet-
ric patterns and high frequency words. In Proc. of the
21st International Conference on Computational Lin-
guistics and the 44th annual meeting of the ACL.
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from the
web: an experimental study. Artificial Intelligence,
165(1):91?134, June.
M.B. Fleischman and E.H. Hovy. 2002. Fine grained
classification of named entities. In Proc. of the 19th
International Conference on Computational Linguis-
tics, pages 1?7.
C. Freeman. 1979. Centrality in social networks: Con-
ceptual clarification. Social Networks, 1:215?239.
R. Girju, A. Badulescu, and D. Moldovan. 2003. Learn-
ing semantic constraints for the automatic discovery of
part-whole relations. In Proc. of Conference of HLT /
North American Chapter of the Association for Com-
putational Linguistics.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proc. of the 14th confer-
ence on Computational linguistics, pages 539?545.
D. Lin and P. Pantel. 2002. Concept discovery from text.
In Proc. of the 19th International Conference on Com-
putational linguistics, pages 1?7.
D. Lin. 1998. Automatic retrieval and clustering of sim-
ilar words. In Proc. of the 17th international confer-
ence on Computational linguistics, pages 768?774.
G. Mann. 2002. Fine-grained proper noun ontologies for
question answering. In Proc. of the 19th International
Conference on Computational Linguistics, pages 1?7.
G. Miller. 1990. Wordnet: An On-line Lexical Database.
International Journal of Lexicography, 3(4).
R. Navigli and M. Lapata. 2007. Graph connectiv-
ity measures for unsupervised word sense disambigua-
tion. In Proc. of the 20th International Joint Confer-
ence on Artificial Intelligence, pages 1683?1688.
M. Pas?ca. 2004. Acquisition of categorized named en-
tities for web search. In Proc. of the Thirteenth ACM
International Conference on Information and Knowl-
edge Management, pages 137?145.
M. Pas?ca. 2007a. Organizing and searching the world
wide web of facts ? step two: harnessing the wisdom
of the crowds. In Proc. of the 16th International Con-
ference on World Wide Web, pages 101?110.
M. Pas?ca. 2007b. Weakly-supervised discovery of
named entities using web search queries. In Proc. of
the sixteenth ACM conference on Conference on infor-
mation and knowledge management, pages 683?690.
L. Page, S. Brin, R. Motwani, and T. Winograd. 1998.
The pagerank citation ranking: Bringing order to the
web. Technical report, Stanford Digital Library Tech-
nologies Project.
P. Pantel and D. Ravichandran. 2004. Automatically
labeling semantic classes. In Proc. of Conference of
HLT / North American Chapter of the Association for
Computational Linguistics, pages 321?328.
P. Pantel, D. Ravichandran, and E. Hovy. 2004. To-
wards terascale knowledge acquisition. In Proc. of the
20th international conference on Computational Lin-
guistics, page 771.
W. Phillips and E. Riloff. 2002. Exploiting Strong Syn-
tactic Heuristics and Co-Training to Learn Semantic
Lexicons. In Proc. of the 2002 Conference on Empiri-
cal Methods in Natural Language Processing.
E. Riloff and R. Jones. 1999. Learning Dictionaries for
Information Extraction by Multi-Level Bootstrapping.
In Proc. of the Sixteenth National Conference on Arti-
ficial Intelligence.
E. Riloff and J. Shepherd. 1997. A Corpus-Based Ap-
proach for Building Semantic Lexicons. In Proc. of
the Second Conference on Empirical Methods in Nat-
ural Language Processing, pages 117?124.
B. Roark and E. Charniak. 1998. Noun-phrase Co-
occurrence Statistics for Semi-automatic Semantic
Lexicon Construction. In Proc. of the 36th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1110?1116.
H. Tanev and B. Magnini. 2006. Weakly supervised ap-
proaches for ontology population. In Proc. of 11st
Conference of the European Chapter of the Associa-
tion for Computational Linguistics.
M. Thelen and E. Riloff. 2002. A Bootstrapping Method
for Learning Semantic Lexicons Using Extraction Pat-
tern Contexts. In Proc. of the 2002 Conference on Em-
pirical Methods in Natural Language Processing.
D. Widdows and B. Dorow. 2002. A graph model for
unsupervised lexical acquisition. In Proc. of the 19th
International Conference on Computational Linguis-
tics, pages 1?7.
1056
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 656?664,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Conundrums in Noun Phrase Coreference Resolution:
Making Sense of the State-of-the-Art
Veselin Stoyanov
Cornell University
Ithaca, NY
ves@cs.cornell.edu
Nathan Gilbert
University of Utah
Salt Lake City, UT
ngilbert@cs.utah.edu
Claire Cardie
Cornell University
Ithaca, NY
cardie@cs.cornell.edu
Ellen Riloff
University of Utah
Salt Lake City, UT
riloff@cs.utah.edu
Abstract
We aim to shed light on the state-of-the-art in NP
coreference resolution by teasing apart the differ-
ences in the MUC and ACE task definitions, the as-
sumptions made in evaluation methodologies, and
inherent differences in text corpora. First, we exam-
ine three subproblems that play a role in coreference
resolution: named entity recognition, anaphoric-
ity determination, and coreference element detec-
tion. We measure the impact of each subproblem on
coreference resolution and confirm that certain as-
sumptions regarding these subproblems in the eval-
uation methodology can dramatically simplify the
overall task. Second, we measure the performance
of a state-of-the-art coreference resolver on several
classes of anaphora and use these results to develop
a quantitative measure for estimating coreference
resolution performance on new data sets.
1 Introduction
As is common for many natural language process-
ing problems, the state-of-the-art in noun phrase
(NP) coreference resolution is typically quantified
based on system performance on manually anno-
tated text corpora. In spite of the availability of
several benchmark data sets (e.g. MUC-6 (1995),
ACE NIST (2004)) and their use in many formal
evaluations, as a field we can make surprisingly
few conclusive statements about the state-of-the-
art in NP coreference resolution.
In particular, it remains difficult to assess the ef-
fectiveness of different coreference resolution ap-
proaches, even in relative terms. For example, the
91.5 F-measure reported by McCallum and Well-
ner (2004) was produced by a system using perfect
information for several linguistic subproblems. In
contrast, the 71.3 F-measure reported by Yang et
al. (2003) represents a fully automatic end-to-end
resolver. It is impossible to assess which approach
truly performs best because of the dramatically
different assumptions of each evaluation.
Results vary widely across data sets. Corefer-
ence resolution scores range from 85-90% on the
ACE 2004 and 2005 data sets to a much lower 60-
70% on the MUC 6 and 7 data sets (e.g. Soon et al
(2001) and Yang et al (2003)). What accounts for
these differences? Are they due to properties of
the documents or domains? Or do differences in
the coreference task definitions account for the dif-
ferences in performance? Given a new text collec-
tion and domain, what level of performance should
we expect?
We have little understanding of which aspects
of the coreference resolution problem are handled
well or poorly by state-of-the-art systems. Ex-
cept for some fairly general statements, for exam-
ple that proper names are easier to resolve than
pronouns, which are easier than common nouns,
there has been little analysis of which aspects of
the problem have achieved success and which re-
main elusive.
The goal of this paper is to take initial steps to-
ward making sense of the disparate performance
results reported for NP coreference resolution. For
our investigations, we employ a state-of-the-art
classification-based NP coreference resolver and
focus on the widely used MUC and ACE corefer-
ence resolution data sets.
We hypothesize that performance variation
within and across coreference resolvers is, at least
in part, a function of (1) the (sometimes unstated)
assumptions in evaluation methodologies, and (2)
the relative difficulty of the benchmark text cor-
pora. With these in mind, Section 3 first examines
three subproblems that play an important role in
coreference resolution: named entity recognition,
anaphoricity determination, and coreference ele-
ment detection. We quantitatively measure the im-
pact of each of these subproblems on coreference
resolution performance as a whole. Our results
suggest that the availability of accurate detectors
for anaphoricity or coreference elements could
substantially improve the performance of state-of-
the-art resolvers, while improvements to named
entity recognition likely offer little gains. Our re-
sults also confirm that the assumptions adopted in
656
MUC ACE
Relative Pronouns no yes
Gerunds no yes
Nested non-NP nouns yes no
Nested NEs no GPE & LOC premod
Semantic Types all 7 classes only
Singletons no yes
Table 1: Coreference Definition Differences for MUC and
ACE. (GPE refers to geo-political entities.)
some evaluations dramatically simplify the resolu-
tion task, rendering it an unrealistic surrogate for
the original problem.
In Section 4, we quantify the difficulty of a
text corpus with respect to coreference resolution
by analyzing performance on different resolution
classes. Our goals are twofold: to measure the
level of performance of state-of-the-art corefer-
ence resolvers on different types of anaphora, and
to develop a quantitative measure for estimating
coreference resolution performance on new data
sets. We introduce a coreference performance pre-
diction (CPP) measure and show that it accurately
predicts the performance of our coreference re-
solver. As a side effect of our research, we pro-
vide a new set of much-needed benchmark results
for coreference resolution under common sets of
fully-specified evaluation assumptions.
2 Coreference Task Definitions
This paper studies the six most commonly used
coreference resolution data sets. Two of those are
from the MUC conferences (MUC-6, 1995; MUC-
7, 1997) and four are from the Automatic Con-
tent Evaluation (ACE) Program (NIST, 2004). In
this section, we outline the differences between the
MUC and ACE coreference resolution tasks, and
define terminology for the rest of the paper.
Noun phrase coreference resolution is the pro-
cess of determining whether two noun phrases
(NPs) refer to the same real-world entity or con-
cept. It is related to anaphora resolution: a NP is
said to be anaphoric if it depends on another NP
for interpretation. Consider the following:
John Hall is the new CEO. He starts on Monday.
Here, he is anaphoric because it depends on its an-
tecedent, John Hall, for interpretation. The two
NPs also corefer because each refers to the same
person, JOHN HALL.
As discussed in depth elsewhere (e.g. van
Deemter and Kibble (2000)), the notions of coref-
erence and anaphora are difficult to define pre-
cisely and to operationalize consistently. Further-
more, the connections between them are extremely
complex and go beyond the scope of this paper.
Given these complexities, it is not surprising that
the annotation instructions for the MUC and ACE
data sets reflect different interpretations and sim-
plifications of the general coreference relation. We
outline some of these differences below.
Syntactic Types. To avoid ambiguity, we will
use the term coreference element (CE) to refer
to the set of linguistic expressions that participate
in the coreference relation, as defined for each of
the MUC and ACE tasks.1 At times, it will be im-
portant to distinguish between the CEs that are in-
cluded in the gold standard ? the annotated CEs
? from those that are generated by the corefer-
ence resolution system ? the extracted CEs.
At a high level, both the MUC and ACE eval-
uations define CEs as nouns, pronouns, and noun
phrases. However, the MUC definition excludes
(1) ?nested? named entities (NEs) (e.g. ?Amer-
ica? in ?Bank of America?), (2) relative pronouns,
and (3) gerunds, but allows (4) nested nouns (e.g.
?union? in ?union members?). The ACE defini-
tion, on the other hand, includes relative pronouns
and gerunds, excludes all nested nouns that are not
themselves NPs, and allows premodifier NE men-
tions of geo-political entities and locations, such
as ?Russian? in ?Russian politicians?.
Semantic Types. ACE restricts CEs to entities
that belong to one of seven semantic classes: per-
son, organization, geo-political entity, location, fa-
cility, vehicle, and weapon. MUC has no semantic
restrictions.
Singletons. The MUC data sets include annota-
tions only for CEs that are coreferent with at least
one other CE. ACE, on the other hand, permits
?singleton? CEs, which are not coreferent with
any other CE in the document.
These substantial differences in the task defini-
tions (summarized in Table 1) make it extremely
difficult to compare performance across the MUC
and ACE data sets. In the next section, we take a
closer look at the coreference resolution task, ana-
lyzing the impact of various subtasks irrespective
of the data set differences.
1We define the term CE to be roughly equivalent to (a)
the notion of markable in the MUC coreference resolution
definition and (b) the structures that can be mentions in the
descriptions of ACE.
657
3 Coreference Subtask Analysis
Coreference resolution is a complex task that
requires solving numerous non-trivial subtasks
such as syntactic analysis, semantic class tagging,
pleonastic pronoun identification and antecedent
identification to name a few. This section exam-
ines the role of three such subtasks ? named en-
tity recognition, anaphoricity determination, and
coreference element detection ? in the perfor-
mance of an end-to-end coreference resolution
system. First, however, we describe the corefer-
ence resolver that we use for our study.
3.1 The RECONCILEACL09 Coreference
Resolver
We use the RECONCILE coreference resolution
platform (Stoyanov et al, 2009) to configure a
coreference resolver that performs comparably to
state-of-the-art systems (when evaluated on the
MUC and ACE data sets under comparable as-
sumptions). This system is a classification-based
coreference resolver, modeled after the systems of
Ng and Cardie (2002b) and Bengtson and Roth
(2008). First it classifies pairs of CEs as coreferent
or not coreferent, pairing each identified CE with
all preceding CEs. The CEs are then clustered
into coreference chains2 based on the pairwise de-
cisions. RECONCILE has a pipeline architecture
with four main steps: preprocessing, feature ex-
traction, classification, and clustering. We will
refer to the specific configuration of RECONCILE
used for this paper as RECONCILEACL09.
Preprocessing. The RECONCILEACL09 prepro-
cessor applies a series of language analysis tools
(mostly publicly available software packages) to
the source texts. The OpenNLP toolkit (Baldridge,
J., 2005) performs tokenization, sentence splitting,
and part-of-speech tagging. The Berkeley parser
(Petrov and Klein, 2007) generates phrase struc-
ture parse trees, and the de Marneffe et al (2006)
system produces dependency relations. We em-
ploy the Stanford CRF-based Named Entity Rec-
ognizer (Finkel et al, 2004) for named entity
tagging. With these preprocessing components,
RECONCILEACL09 uses heuristics to correctly ex-
tract approximately 90% of the annotated CEs for
the MUC and ACE data sets.
Feature Set. To achieve roughly state-of-the-
art performance, RECONCILEACL09 employs a
2A coreference chain refers to the set of CEs that refer to
a particular entity.
dataset docs CEs chains CEs/ch tr/tst split
MUC6 60 4232 960 4.4 30/30 (st)
MUC7 50 4297 1081 3.9 30/20 (st)
ACE-2 159 2630 1148 2.3 130/29 (st)
ACE03 105 3106 1340 2.3 74/31
ACE04 128 3037 1332 2.3 90/38
ACE05 81 1991 775 2.6 57/24
Table 2: Dataset characteristics including the number of
documents, annotated CEs, coreference chains, annotated
CEs per chain (average), and number of documents in the
train/test split. We use st to indicate a standard train/test split.
fairly comprehensive set of 61 features introduced
in previous coreference resolution systems (see
Bengtson and Roth (2008)). We briefly summarize
the features here and refer the reader to Stoyanov
et al (2009) for more details.
Lexical (9): String-based comparisons of the two
CEs, such as exact string matching and head noun
matching.
Proximity (5): Sentence and paragraph-based
measures of the distance between two CEs.
Grammatical (28): A wide variety of syntactic
properties of the CEs, either individually or as a
pair. These features are based on part-of-speech
tags, parse trees, or dependency relations. For ex-
ample: one feature indicates whether both CEs are
syntactic subjects; another indicates whether the
CEs are in an appositive construction.
Semantic (19): Capture semantic information
about one or both NPs such as tests for gender and
animacy, semantic compatibility based on Word-
Net, and semantic comparisons of NE types.
Classification and Clustering. We configure
RECONCILEACL09 to use the Averaged Percep-
tron learning algorithm (Freund and Schapire,
1999) and to employ single-link clustering (i.e.
transitive closure) to generate the final partition-
ing.3
3.2 Baseline System Results
Our experiments rely on the MUC and ACE cor-
pora. For ACE, we use only the newswire portion
because it is closest in composition to the MUC
corpora. Statistics for each of the data sets are
shown in Table 2. When available, we use the
standard test/train split. Otherwise, we randomly
split the data into a training and test set following
a 70/30 ratio.
3In trial runs, we investigated alternative classification
and clustering models (e.g. C4.5 decision trees and SVMs;
best-first clustering). The results were comparable.
658
Scoring Algorithms. We evaluate using two
common scoring algorithms4 ? MUC and B3.
The MUC scoring algorithm (Vilain et al, 1995)
computes the F1 score (harmonic mean) of preci-
sion and recall based on the identifcation of unique
coreference links. We use the official MUC scorer
implementation for the two MUC corpora and an
equivalent implementation for ACE.
The B3 algorithm (Bagga and Baldwin, 1998)
computes a precision and recall score for each CE:
precision(ce) = |Rce ?Kce|/|Rce|
recall(ce) = |Rce ?Kce|/|Kce|,
where Rce is the coreference chain to which ce is
assigned in the response (i.e. the system-generated
output) and Kce is the coreference chain that con-
tains ce in the key (i.e. the gold standard). Pre-
cision and recall for a set of documents are com-
puted as the mean over all CEs in the documents
and the F1 score of precision and recall is reported.
B3 Complications. Unlike the MUC score,
which counts links between CEs, B3 presumes
that the gold standard and the system response are
clusterings over the same set of CEs. This, of
course, is not the case when the system automat-
ically identifies the CEs, so the scoring algorithm
requires a mapping between extracted and anno-
tated CEs. We will use the term twin(ce) to refer
to the unique annotated/extracted CE to which the
extracted/annotated CE is matched. We say that
a CE is twinless (has no twin) if no corresponding
CE is identified. A twinless extracted CE signals
that the resolver extracted a spurious CE, while an
annotated CE is twinless when the resolver fails to
extract it.
Unfortunately, it is unclear how the B3 score
should be computed for twinless CEs. Bengtson
and Roth (2008) simply discard twinless CEs, but
this solution is likely too lenient ? it doles no pun-
ishment for mistakes on twinless annotated or ex-
tracted CEs and it would be tricked, for example,
by a system that extracts only the CEs about which
it is most confident.
We propose two different ways to deal with
twinless CEs for B3. One option, B3all, retains
all twinless extracted CEs. It computes the preci-
4We also experimented with the CEAF score (Luo, 2005),
but excluded it due to difficulties dealing with the extracted,
rather than annotated, CEs. CEAF assigns a zero score to
each twinless extracted CE and weights all coreference chains
equally, irrespective of their size. As a result, runs with ex-
tracted CEs exhibit very low CEAF precision, leading to un-
reliable scores.
sion as above when ce has a twin, and computes
the precision as 1/|Rce| if ce is twinless. (Simi-
larly, recall(ce) = 1/|Kce| if ce is twinless.)
The second option, B30, discards twinless
extracted CEs, but penalizes recall by setting
recall(ce) = 0 for all twinless annotated CEs.
Thus, B30 presumes that all twinless extracted
CEs are spurious.
Results. Table 3, box 1 shows the performance
of RECONCILEACL09 using a default (0.5) coref-
erence classifier threshold. The MUC score is
highest for the MUC6 data set, while the four ACE
data sets show much higher B3 scores as com-
pared to the two MUC data sets. The latter occurs
because the ACE data sets include singletons.
The classification threshold, however, can be
gainfully employed to control the trade-off be-
tween precision and recall. This has not tradi-
tionally been done in learning-based coreference
resolution research ? possibly because there is
not much training data available to sacrifice as a
validation set. Nonetheless, we hypothesized that
estimating a threshold from just the training data
might be effective. Our results (BASELINE box
in Table 3) indicate that this indeed works well.5
With the exception of MUC6, results on all data
sets and for all scoring algorithms improve; more-
over, the scores approach those for runs using an
optimal threshold (box 3) for the experiment as de-
termined by using the test set. In all remaining ex-
periments, we learn the threshold from the training
set as in the BASELINE system.
Below, we resume our investigation of the role
of three coreference resolution subtasks and mea-
sure the impact of each on overall performance.
3.3 Named Entities
Previous work has shown that resolving corefer-
ence between proper names is relatively easy (e.g.
Kameyama (1997)) because string matching func-
tions specialized to the type of proper name (e.g.
person vs. location) are quite accurate. Thus, we
would expect a coreference resolution system to
depend critically on its Named Entity (NE) extrac-
tor. On the other hand, state-of-the-art NE taggers
are already quite good, so improving this compo-
nent may not provide much additional gain.
To study the influence of NE recognition,
we replace the system-generated NEs of
5All experiments sample uniformly from 1000 threshold
values.
659
ReconcileACL09 MUC6 MUC7 ACE-2 ACE03 ACE04 ACE05
1. DEFAULT THRESHOLD (0.5)
MUC 70.40 58.20 65.76 66.73 56.75 64.30
B3all 69.91 62.88 77.25 77.56 73.03 72.82
B30 68.55 62.80 76.59 77.27 72.99 72.43
2. BASELINE
MUC 68.50 62.80 65.99 67.87 62.03 67.41
= THRESHOLD ESTIMATION
B3all 70.88 65.86 78.29 79.39 76.50 73.71
B30 68.43 64.57 76.63 77.88 75.41 72.47
3. OPTIMAL THRESHOLD
MUC 71.20 62.90 66.83 68.35 62.11 67.41
B3all 72.31 66.52 78.50 79.41 76.53 74.25
B30 69.49 64.64 76.83 78.27 75.51 72.94
4. BASELINE with
MUC 69.90 - 66.37 70.35 62.88 67.72
perfect NEs
B3all 72.31 - 78.06 80.22 77.01 73.92
B30 67.91 - 76.55 78.35 75.22 72.90
5. BASELINE with
MUC 85.80* 81.10* 76.39 79.68 76.18 79.42
perfect CEs
B3all 76.14 75.88 78.65 80.58 77.79 76.49
B30 76.14 75.88 78.65 80.58 77.79 76.49
6. BASELINE with
MUC 82.20* 71.90* 86.63 85.58 83.33 82.84
anaphoric CEs
B3all 72.52 69.26 80.29 79.71 76.05 74.33
B30 72.52 69.26 80.29 79.71 76.05 74.33
Table 3: Impact of Three Subtasks on Coreference Resolution Performance. A score marked with a * indicates that a 0.5
threshold was used because threshold selection from the training data resulted in an extreme version of the system, i.e. one that
places all CEs into a single coreference chain.
RECONCILEACL09 with gold-standard NEs
and retrain the coreference classifier. Results
for each of the data sets are shown in box 4 of
Table 3. (No gold standard NEs are available for
MUC7.) Comparison to the BASELINE system
(box 2) shows that using gold standard NEs
leads to improvements on all data sets with the
exception of ACE2 and ACE05, on which perfor-
mance is virtually unchanged. The improvements
tend to be small, however, between 0.5 to 3
performance points. We attribute this to two
factors. First, as noted above, although far from
perfect, NE taggers generally perform reasonably
well. Second, only 20 to 25% of the coreference
element resolutions required for these data sets
involve a proper name (see Section 4).
Conclusion #1: Improving the performance of NE tag-
gers is not likely to have a large impact on the performance
of state-of-the-art coreference resolution systems.
3.4 Coreference Element Detection
We expect CE detection to be an important sub-
problem for an end-to-end coreference system.
Results for a system that assumes perfect CEs
are shown in box 5 of Table 3. For these runs,
RECONCILEACL09 uses only the annotated CEs
for both training and testing. Using perfect CEs
solves a large part of the coreference resolution
task: the annotated CEs divulge anaphoricity in-
formation, perfect NP boundaries, and perfect in-
formation regarding the coreference relation de-
fined for the data set.
We see that focusing attention on all and only
the annotated CEs leads to (often substantial) im-
provements in performance on all metrics over
all data sets, especially when measured using the
MUC score.
Conclusion #2: Improving the ability of coreference re-
solvers to identify coreference elements would likely improve
the state-of-the-art immensely ? by 10-20 points in MUC F1
score and from 2-12 F1 points for B3.
This finding explains previously published re-
sults that exhibit striking variability when run with
annotated CEs vs. system-extracted CEs. On the
MUC6 data set, for example, the best published
MUC score using extracted CEs is approximately
71 (Yang et al, 2003), while multiple systems
have produced MUC scores of approximately 85
when using annotated CEs (e.g. Luo et al (2004),
McCallum and Wellner (2004)).
We argue that providing a resolver with the an-
notated CEs is a rather unrealistic evaluation: de-
termining whether an NP is part of an annotated
coreference chain is precisely the job of a corefer-
ence resolver!
Conclusion #3: Assuming the availability of CEs unre-
alistically simplifies the coreference resolution task.
3.5 Anaphoricity Determination
Finally, several coreference systems have suc-
cessfully incorporated anaphoricity determination
660
modules (e.g. Ng and Cardie (2002a) and Bean
and Riloff (2004)). The goal of the module is to
determine whether or not an NP is anaphoric. For
example, pleonastic pronouns (e.g. it is raining)
are special cases that do not require coreference
resolution.
Unfortunately, neither the MUC nor the ACE
data sets include anaphoricity information for all
NPs. Rather, they encode anaphoricity informa-
tion implicitly for annotated CEs: a CE is consid-
ered anaphoric if is not a singleton.6
To study the utility of anaphoricity informa-
tion, we train and test only on the ?anaphoric? ex-
tracted CEs, i.e. the extracted CEs that have an
annotated twin that is not a singleton. Note that
for the MUC datasets all extracted CEs that have
twins are considered anaphoric.
Results for this experiment (box 6 in Table 3)
are similar to the previous experiment using per-
fect CEs: we observe big improvements across the
board. This should not be surprising since the ex-
perimental setting is quite close to that for perfect
CEs: this experiment also presumes knowledge
of when a CE is part of an annotated coreference
chain. Nevertheless, we see that anaphoricity info-
mation is important. First, good anaphoricity iden-
tification should reduce the set of extracted CEs
making it closer to the set of annotated CEs. Sec-
ond, further improvements in MUC score for the
ACE data sets over the runs using perfect CEs (box
5) reveal that accurately determining anaphoric-
ity can lead to substantial improvements in MUC
score. ACE data includes annotations for single-
ton CEs, so knowling whether an annotated CE is
anaphoric divulges additional information.
Conclusion #4: An accurate anaphoricity determina-
tion component can lead to substantial improvement in coref-
erence resolution performance.
4 Resolution Complexity
Different types of anaphora that have to be han-
dled by coreference resolution systems exhibit dif-
ferent properties. In linguistic theory, binding
mechanisms vary for different kinds of syntactic
constituents and structures. And in practice, em-
pirical results have confirmed intuitions that differ-
ent types of anaphora benefit from different clas-
sifier features and exhibit varying degrees of diffi-
culty (Kameyama, 1997). However, performance
6Also, the first element of a coreference chain is usually
non-anaphoric, but we do not consider that issue here.
evaluations rarely include analysis of where state-
of-the-art coreference resolvers perform best and
worst, aside from general conclusions.
In this section, we analyze the behavior of
our coreference resolver on different types of
anaphoric expressions with two goals in mind.
First, we want to deduce the strengths and weak-
nesses of state-of-the-art systems to help direct
future research. Second, we aim to understand
why current coreference resolvers behave so in-
consistently across data sets. Our hypothesis is
that the distribution of different types of anaphoric
expressions in a corpus is a major factor for coref-
erence resolution performance. Our experiments
confirm this hypothesis and we use our empirical
results to create a coreference performance predic-
tion (CPP) measure that successfully estimates the
expected level of performance on novel data sets.
4.1 Resolution Classes
We study the resolution complexity of a text cor-
pus by defining resolution classes. Resolution
classes partition the set of anaphoric CEs accord-
ing to properties of the anaphor and (in some
cases) the antecedent. Previous work has stud-
ied performance differences between pronominal
anaphora, proper names, and common nouns, but
we aim to dig deeper into subclasses of each of
these groups. In particular, we distinguish be-
tween proper and common nouns that can be re-
solved via string matching, versus those that have
no antecedent with a matching string. Intuitively,
we expect that it is easier to resolve the cases
that involve string matching. Similarly, we par-
tition pronominal anaphora into several subcate-
gories that we expect may behave differently. We
define the following nine resolution classes:
Proper Names: Three resolution classes cover
CEs that are named entities (e.g. the PER-
SON, LOCATION, ORGANIZATION and DATE
classes for MUC and ACE) and have a prior ref-
erent7 in the text. These three classes are distin-
guished by the type of antecedent that can be re-
solved against the proper name.
(1) PN-e: a proper name is assigned to this exact string match
class if there is at least one preceding CE in its gold standard
coreference chain that exactly matches it.
(2) PN-p: a proper name is assigned to this partial string
match class if there is at least one preceding CE in its gold
standard chain that has some content words in common.
(3) PN-n: a proper name is assigned to this no string match
7We make a rough, but rarely inaccurate, assumption that
there are no cataphoric expressions in the data.
661
MUC6 MUC7 ACE2 ACE03 ACE04 ACE05 Avg
# % scr # % scr # % scr # % scr # % scr # % scr % scr
PN-e 273 17 .87 249 19 .79 346 24 .94 435 25 .93 267 16 .88 373 31 .92 22 .89
PN-p 157 10 .68 79 6 .59 116 8 .86 178 10 .87 194 11 .71 125 10 .71 9 .74
PN-n 18 1 .18 18 1 .28 85 6 .19 79 4 .15 66 4 .21 89 7 .27 4 .21
CN-e 292 18 .82 276 21 .65 84 6 .40 186 11 .68 165 10 .68 134 11 .79 13 .67
CN-p 229 14 .53 239 18 .49 147 10 .26 168 10 .24 147 9 .40 147 12 .43 12 .39
CN-n 194 12 .27 148 11 .15 152 10 .50 148 8 .90 266 16 .32 121 10 .20 11 .18
1+2Pr 48 3 .70 65 5 .66 122 8 .73 76 4 .73 158 9 .77 51 4 .61 6 .70
G3Pr 160 10 .73 50 4 .79 181 12 .83 237 13 .82 246 14 .84 69 60 .81 10 .80
U3Pr 175 11 .49 142 11 .49 163 11 .45 122 7 .48 153 9 .49 91 7 .49 9 .48
Table 4: Frequencies and scores for each resolution class.
class if no preceding CE in its gold standard chain has any
content words in common with it.
Common NPs: Three analogous string match
classes cover CEs that have a common noun as a
head: (4) CN-e (5) CN-p (6) CN-n.
Pronouns: Three classes cover pronouns:
(7) 1+2Pr: The anaphor is a 1st or 2nd person pronoun.
(8) G3Pr: The anaphor is a gendered 3rd person pronoun
(e.g. ?she?, ?him?).
(9) U3Pr: The anaphor is an ungendered 3rd person pro-
noun.
As noted above, resolution classes are defined for
annotated CEs. We use the twin relationship to
match extracted CEs to annotated CEs and to eval-
uate performance on each resolution class.
4.2 Scoring Resolution Classes
To score each resolution class separately, we de-
fine a new variant of the MUC scorer. We compute
a MUC-RC score (for MUC Resolution Class) for
class C as follows: we assume that all CEs that do
not belong to class C are resolved correctly by tak-
ing the correct clustering for them from the gold
standard. Starting with this correct partial cluster-
ing, we run our classifier on all ordered pairs of
CEs for which the second CE is of class C, es-
sentially asking our coreference resolver to deter-
mine whether each member of class C is corefer-
ent with each of its preceding CEs. We then count
the number of unique correct/incorrect links that
the system introduced on top of the correct par-
tial clustering and compute precision, recall, and
F1 score. This scoring function directly measures
the impact of each resolution class on the overall
MUC score.
4.3 Results
Table 4 shows the results of our resolution class
analysis on the test portions of the six data sets.
The # columns show the frequency counts for each
resolution class, and the % columns show the dis-
tributions of the classes in each corpus (i.e. 17%
MUC6 MUC7 ACE2 ACE03 ACE04 ACE05
0.92 0.95 0.91 0.98 0.97 0.96
Table 5: Correlations of resolution class scores with respect
to the average.
of all resolutions in the MUC6 corpus were in the
PN-e class). The scr columns show the MUC-
RC score for each resolution class. The right-hand
side of Table 4 shows the average distribution and
scores across all data sets.
These scores confirm our expectations about the
relative difficulty of different types of resolutions.
For example, it appears that proper names are eas-
ier to resolve than common nouns; gendered pro-
nouns are easier than 1st and 2nd person pronouns,
which, in turn, are easier than ungendered 3rd per-
son pronouns. Similarly, our intuition is confirmed
that many CEs can be accurately resolved based on
exact string matching, whereas resolving against
antecedents that do not have overlapping strings is
much more difficult. The average scores in Table 4
show that performance varies dramatically across
the resolution classes, but, on the surface, appears
to be relatively consistent across data sets.
None of the data sets performs exactly the same,
of course, so we statistically analyze whether the
behavior of each resolution class is similar across
the data sets. For each data set, we compute the
correlation between the vector of MUC-RC scores
over the resolution classes and the average vec-
tor of MUC-RC scores for the remaining five data
sets. Table 5 contains the results, which show high
correlations (over .90) for all six data sets. These
results indicate that the relative performance of the
resolution classes is consistent across corpora.
4.4 Coreference Performance Prediction
Next, we hypothesize that the distribution of res-
olution classes in a corpus explains (at least par-
tially) why performance varies so much from cor-
662
MUC6 MUC7 ACE2 ACE03 ACE04 ACE05
P 0.59 0.59 0.62 0.65 0.59 0.62
O 0.67 0.61 0.66 0.68 0.62 0.67
Table 6: Predicted (P) vs Observed (O) scores.
pus to corpus. To explore this issue, we create a
Coreference Performance Prediction (CPP) mea-
sure to predict the performance on new data sets.
The CPP measure uses the empirical performance
of each resolution class observed on previous data
sets and forms a predicton based on the make-up
of resolution classes in a new corpus. The distribu-
tion of resolution classes for a new corpus can be
easily determined because the classes can be rec-
ognized superficially by looking only at the strings
that represent each NP.
We compute the CPP score for each of our six
data sets based on the average resolution class per-
formance measured on the other five data sets.
The predicted score for each class is computed as
a weighted sum of the observed scores for each
resolution class (i.e. the mean for the class mea-
sured on the other five data sets) weighted by the
proportion of CEs that belong to the class. The
predicted scores are shown in Table 6 and com-
pared with the MUC scores that are produced by
RECONCILEACL09.8
Our results show that the CPP measure is a
good predictor of coreference resolution perfor-
mance on unseen data sets, with the exception
of one outlier ? the MUC6 data set. In fact,
the correlation between predicted and observed
scores is 0.731 for all data sets and 0.913 exclud-
ing MUC6. RECONCILEACL09?s performance on
MUC6 is better than predicted due to the higher
than average scores for the common noun classes.
We attribute this to the fact that MUC6 includes
annotations for nested nouns, which almost al-
ways fall in the CN-e and CN-p classes. In ad-
dition, many of the features were first created for
the MUC6 data set, so the feature extractors are
likely more accurate than for other data sets.
Overall, results indicate that coreference perfor-
mance is substantially influenced by the mix of
resolution classes found in the data set. Our CPP
measure can be used to produce a good estimate
of the level of performance on a new corpus.
8Observed scores for MUC6 and 7 differ slightly from Ta-
ble 3 because this part of the work did not use the OPTIONAL
field of the key, employed by the official MUC scorer.
5 Related Work
The bulk of the relevant related work is described
in earlier sections, as appropriate. This paper stud-
ies complexity issues for NP coreference resolu-
tion using a ?good?, i.e. near state-of-the-art, sys-
tem. For state-of-the-art performance on the MUC
data sets see, e.g. Yang et al (2003); for state-of-
the-art performance on the ACE data sets see, e.g.
Bengtson and Roth (2008) and Luo (2007). While
other researchers have evaluated NP coreference
resolvers with respect to pronouns vs. proper
nouns vs. common nouns (Ng and Cardie, 2002b),
our analysis focuses on measuring the complexity
of data sets, predicting the performance of coref-
erence systems on new data sets, and quantify-
ing the effect of coreference system subcompo-
nents on overall performance. In the related area
of anaphora resolution, researchers have studied
the influence of subsystems on the overall per-
formance (Mitkov, 2002) as well as defined and
evaluated performance on different classes of pro-
nouns (e.g. Mitkov (2002) and Byron (2001)).
However, due to the significant differences in task
definition, available datasets, and evaluation met-
rics, their conclusions are not directly applicable
to the full coreference task.
Previous work has developed methods to predict
system performance on NLP tasks given data set
characteristics, e.g. Birch et al (2008) does this for
machine translation. Our work looks for the first
time at predicting the performance of NP corefer-
ence resolvers.
6 Conclusions
We examine the state-of-the-art in NP coreference
resolution. We show the relative impact of perfect
NE recognition, perfect anaphoricity information
for coreference elements, and knowledge of all
and only the annotated CEs. We also measure the
performance of state-of-the-art resolvers on sev-
eral classes of anaphora and use these results to
develop a measure that can accurately estimate a
resolver?s performance on new data sets.
Acknowledgments. We gratefully acknowledge
technical contributions from David Buttler and
David Hysom in creating the Reconcile corefer-
ence resolution platform. This research was sup-
ported in part by the Department of Homeland
Security under ONR Grant N0014-07-1-0152 and
Lawrence Livermore National Laboratory subcon-
tract B573245.
663
References
A. Bagga and B. Baldwin. 1998. Algorithms for Scor-
ing Coreference Chains. In In Linguistic Corefer-
ence Workshop at LREC 1998.
Baldridge, J. 2005. The OpenNLP project.
http://opennlp.sourceforge.net/.
D. Bean and E. Riloff. 2004. Unsupervised Learn-
ing of Contextual Role Knowledge for Coreference
Resolution. In Proceedings of the Annual Meeting
of the North American Chapter of the Association
for Computational Linguistics (HLT/NAACL 2004).
Eric Bengtson and Dan Roth. 2008. Understanding
the Value of Features for Coreference Resolution.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
294?303. Association for Computational Linguis-
tics.
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2008. Predicting Success in Machine Translation.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
745?754. Association for Computational Linguis-
tics.
Donna Byron. 2001. The Uncommon Denomina-
tor: A Proposal for Consistent Reporting of Pro-
noun Resolution Results. Computational Linguis-
tics, 27(4):569?578.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
LREC.
J. Finkel, S. Dingare, H. Nguyen, M. Nissim, and
C. Manning. 2004. Exploiting Context for Biomed-
ical Entity Recognition: From Syntax to the Web. In
Joint Workshop on Natural Language Processing in
Biomedicine and its Applications at COLING 2004.
Yoav Freund and Robert E. Schapire. 1999. Large
Margin Classification Using the Perceptron Algo-
rithm. In Machine Learning, pages 277?296.
Megumi Kameyama. 1997. Recognizing Referential
Links: An Information Extraction Perspective. In
Workshop On Operational Factors In Practical Ro-
bust Anaphora Resolution For Unrestricted Texts.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A
Mention-Synchronous Coreference Resolution Al-
gorithm Based on the Bell Tree. In Proceedings
of the 42nd Annual Meeting of the Association for
Computational Linguistics.
X. Luo. 2005. On Coreference Resolution Perfor-
mance Metrics. In Proceedings of the 2005 Human
Language Technology Conference / Conference on
Empirical Methods in Natural Language Process-
ing.
Xiaoqiang Luo. 2007. Coreference or Not: A Twin
Model for Coreference Resolution. In Proceedings
of the Annual Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(HLT/NAACL 2007).
A. McCallum and B. Wellner. 2004. Conditional Mod-
els of Identity Uncertainty with Application to Noun
Coreference. In 18th Annual Conference on Neural
Information Processing Systems.
Ruslan Mitkov. 2002. Anaphora Resolution. Long-
man, London.
MUC-6. 1995. Coreference Task Definition. In Pro-
ceedings of the Sixth Message Understanding Con-
ference (MUC-6), pages 335?344.
MUC-7. 1997. Coreference Task Definition. In
Proceedings of the Seventh Message Understanding
Conference (MUC-7).
V. Ng and C. Cardie. 2002a. Identifying Anaphoric
and Non-Anaphoric Noun Phrases to Improve
Coreference Resolution. In Proceedings of the 19th
International Conference on Computational Lin-
guistics (COLING 2002).
V. Ng and C. Cardie. 2002b. Improving Machine
Learning Approaches to Coreference Resolution. In
Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics.
NIST. 2004. The ACE Evaluation Plan.
S. Petrov and D. Klein. 2007. Improved Inference for
Unlexicalized Parsing. In Proceedings of the Annual
Meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics (HLT/NAACL
2007).
W. Soon, H. Ng, and D. Lim. 2001. A Machine
Learning Approach to Coreference of Noun Phrases.
Computational Linguistics, 27(4):521?541.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, Ellen
Riloff, David Buttler, and David Hysom. 2009.
Reconcile: A Coreference Resolution Research Plat-
form. Computer Science Technical Report, Cornell
University, Ithaca, NY.
Kees van Deemter and Rodger Kibble. 2000. On
Coreferring: Coreference in MUC and Related
Annotation Schemes. Computational Linguistics,
26(4):629?637.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A Model-Theoretic Corefer-
ence Scoring Theme. In Proceedings of the Sixth
Message Understanding Conference (MUC-6).
Xiaofeng Yang, Guodong Zhou, Jian Su, and
Chew Lim Tan. 2003. Coreference Resolution Us-
ing Competition Learning Approach. In ACL ?03:
Proceedings of the 41st Annual Meeting on Associa-
tion for Computational Linguistics, pages 176?183.
664
A Rule-based Question Answering System for Reading 
Comprehension Tests 
E l len  R i lo f f  and Michae l  The len  
Department  of Computer  Science 
University of Utah 
Salt Lake City, Utah 84112 
{riloff, thelenm}@cs.utah.edu 
Abst rac t  
We have developed a rule-based system, Quarc, 
that can reada short story and find the sentence 
in the story that best answers a given question. 
Quarc uses heuristic rules that look for lexical 
and semantic lues in the question and the story. 
We have tested Quarc on reading comprehen- 
sion tests typically given to children in grades 
3-6. Overall, Quarc found the correct sentence 
40% of the time, which is encouraging given the 
simplicity of its rules. 
1 In t roduct ion  
In the United States, we evaluate the reading 
ability of children by giving them reading com- 
prehension tests. These test typically consist of 
a short story followed by questions. Presum- 
ably, the tests are designed so that the reader 
must understand important aspects of the story 
to answer the questions correctly. For this 
reason, we believe that reading comprehension 
tests can be a valuable tool to assess the state 
of the art in natural anguage understanding. 
These tests are especially challenging because 
they can discuss virtually any topic. Conse- 
quently, broad-coverage natural language pro- 
cessing (NLP) techniques must be used. But the 
reading comprehension tests also require seman- 
tic understanding, which is difficult to achieve 
with broad-coverage t chniques. 
We have developed a system called Quarc 
that "takes" reading comprehension tests. 
Given a story and a question, Quarc finds the 
sentence in the story that best answers the 
question. Quarc does not use deep language 
understanding or sophisticated techniques, yet 
it achieved 40% accuracy in our experiments. 
Quarc uses hand-crafted heuristic rules that 
look for lexical and semantic lues in the ques- 
tion and the story. In the next section, we de- 
scribe the reading comprehension tests. In the 
following sections, we describe the rules used by 
Quarc and present experimental results. 
2 Read ing  Comprehens ion  Tests  
Figure 1 shows an example of a reading compre- 
hension test from Remedia Publications. Each 
test is followed by five "WH" questions: WHO, 
WHAT, WHEN, WHERE, and WHY. 1 The answers 
to the questions typically refer to a string in 
the text, such as a name or description, which 
can range in length from a single noun phrase 
to an entire clause or sentence. The answers to 
WHEN and WHERE questions are also sometimes 
inferred from the dateline of the story. For ex- 
ample, (EGYPT, 1951) contains the answer to 
the WHEN question in Figure 1. 
Ideally, a natural anguage processing system 
would produce the exact answer to a question. 
Identifying the precise boundaries of the answer 
can be tricky, however. We will focus on the 
somewhat easier task of identifying the sentence 
that contains the answer to a question. 
3 A Ru le -based  System for  Quest ion  
Answer ing  
Quarc (QUestion Answering for Reading Com- 
prehension) is a rule-based system that uses lex- 
ical and semantic heuristics to look for evidence 
that a sentence contains the answer to a ques- 
tion. Each type of WH question looks for differ- 
ent types of answers, so Quarc uses a separate 
set of rules for each question type (WHO, WHAT, 
WHEN, WHERE, WHY). 
Given a question and a story, Quarc parses 
the question and all of the sentences in the story 
using our partial parser Sundance. Much of 
1There is also a lone HOW question in the data set, 
but we ignored it. 
13 
Tomb Keeps  I ts  Secrets  
(EGYPT, 1951) - A tomb was tound this year. It was a tomb built for a king. The king lived more 
than 4,000 years ago. His home was in Egypt. 
For years, no one saw the tomb. It was carved deep in rock. The wind blew sand over the top and 
hid it. Then a team of diggers came along. Their job was to search for hidden treasures. 
What they found thrilled them. Jewels and gold were found in the tomb. The king's treasures 
were buried inside 132 rooms. 
The men opened a 10-foot-thick door. It was 130 feet below the earth. Using torches, they saw a 
case. "It must contain the king's mummy!" they said. A mummy is a body wrapped in sheets. 
With great care, the case was removed. It was taken to a safe place to be opened. For two hours, 
workers tried to lift the lid. At last, they got it off. 
Inside they saw ... nothing! The case was empty. No one knows where the body is hidden. A new 
mystery has begun. 
1. Who was supposed to be buried in the tomb? 
2. Wha_t_is a mummy? 
3. When did this .story happen? 
4. Where was the 10-foot-thick door found? 
5. Why was the body gone? 
Figure 1: Sample Reading Comprehension Test 
the syntactic analysis is not used, but Quarc 
does use the morphological analysis, part-of- 
speech tagging, semantic lass tagging, and en- 
tity recognition. The rules are applied to each 
sentence in the story, as well as the title of the 
story, with the exception that the title is not 
considered for WHY questions. The dateline is 
also a possible answer for WHEN and WHERE 
questions and is evaluated using a special set of 
dateline rules. 
Each rule awards a certain number of points 
to a sentence. After all of the rules have been 
applied, the sentence (or dateline) that obtains 
the highest score is returned as the answer. 
All of the question types share a common 
WordMatch function, which counts the number 
of words that appear in both the question and 
the sentence being considered. The WordMatch 
function first strips a sentence of stopwords 2,
and then matches the remaining words against 
the words in the question. Two words match if 
they share the same morphological root. Verbs 
seem to be especially important for recogniz- 
ing that a question and sentence are related, so 
verb matches are weighted more heavily than 
non-verb matches. Matching verbs are awarded 
6 points each and other matching words are 
awarded 3 points each. 
The other rules used by Quarc look for a vari- 
~We used a stopword list containing 41 words, mostly 
prepositions, pronouns, and auxiliary verbs. 
ety of clues. Lexical clues look for specific words 
or phrases. Unless a rule indicates otherwise, 
words are compared using their morphological 
roots. Some rules can be satisfied by any of sev- 
eral lexical items; these rules are written using 
set notation (e.g., { yesterday, today, tomorrow}). 
Some rules also look for semantic lasses, which 
we will write in upper case (e.g., HUMAN). Our 
parser uses a dictionary and a semantic hierar- 
chy, so that words can be defined with semantic 
classes. The semantic lasses used by Quarc are 
shown below, along with a description of the 
words assigned to each class. 
HUMAN:  2608 words, 3 including common 
first names, last names, titles such as 
"Dr." and "Mrs.", and about 600 occupa- 
tion words acquired from WordNet (Miller, 
1990). 
LOCATION:  344 words, including 204 coun- 
try names and the 50 United States. 
MONTH:  the 12 months of the year. 
TIME:  667 words, 600 of which are enumer- 
ated years from 1400-1999. The others are 
general time expressions, including the 12 
months of the year. 
3About 2000 words came from the Social Security Ad- 
ministration's li t of the top 1000 names for each gender 
in 1998: www.ssa.gov/OACT/NOTES/note139/1998/ 
top1000in98.html. 
14 
Our parser also recognizes two types of se- 
mantic entities: proper nouns and names. A 
PROPER__NOUN is defined as a noun phrase 
in which all words are capitalized. A NAME is 
defined as a PROPER_NOUN that contains at 
least one HUMAN word. 
Each rule awards a specific number of points 
to a sentence, depending on how strongly the 
rule believes that it found the answer. A rule 
can assign four possible point values: c lue 
(+3), good_clue (+4), conf ident  (+6), and 
s lam_dunk  (+20). These point values were 
based on our intuitions and worked well empir- 
ically, but they are not well justified. The main 
purpose of these values is to assess the relative 
importance of each clue. 
Figure 2 shows the WHO rules, which use three 
fairly general heuristics as well as the Word- 
Match function (rule #1). If the question (Q) 
does not contain any names, then rules #2 and 
#3 assume that the question is looking for a 
name. Rule #2 rewards sentences that contain 
a recognized NAME, and rule #3 rewards sen- 
tences that contain the word "name". Rule #4 
awards points to all sentences that contain ei- 
ther a name or a reference to a human (often an 
occupation, such as "writer"). Note that more 
than one rule can apply to a sentence, in which 
case the sentence is awarded points by all of the 
rules that applied. 
1. Score(S) += WordMatch(Q,S) 
2. If-~ contains(Q,NAME) and 
contains(S,NAME) 
Then Score(S) += confident 
3. If ~ contains(Q,NAME) and 
contains(S,name) 
Then Score(S) += good_clue 
4. If contains(S,{NAME,HUMAN}) 
Then Score(S) += good_clue 
Figure 2: WHO Rules 
The WHAT questions were the most difficult 
to handle because they sought an amazing va- 
riety of answers. But Figure 3 shows a few spe- 
cific rules that worked reasonably well. Rule #1 
is the generic word matching function shared 
by all question types. Rule #2 rewards sen- 
tences that contain a date expression if the ques- 
tion contains a month of the year. This rule 
handles questions that ask what occurred on 
a specific date. We also noticed several "what 
kind?" questions, which looked for a description 
of an object. Rule #3 addresses these questions 
by rewarding sentences that contain the word 
"call" or "from" (e.g., "It is called..." or "It is 
made from ..."). Rule #4 looks for words asso- 
ciated with names in both the question and sen- 
tence. Rule #5 is very specific and recognizes 
questions that contain phrases such as "name 
of <x>" or "name for <x>".  Any sentence 
that contains a proper noun whose head noun 
matches <x> will be highly rewarded. For ex- 
ample, the question "What is the name of the 
creek?" is answered by a sentence that contains 
the noun phrase "Pigeon Creek". 
1. Score(S) += WordMatch(Q,S) 
2. If contains(Q,MONTH) and 
contains(S,{ today, yesterday, 
tomorrow, last night}) 
Then Score(S) += clue 
3. If contains(Q,kind) and 
contains (S, { call,from}) 
Then Score(S) += good_due 
4. If contains(Q,narne) and 
eontains( S, { name, call, known} ) 
Then Score += slam_dunk 
5. If contains(Q,name+PP) and 
contains(S,PROPER_NOUN) and 
contains(PROPER_NOUN,head(PP)) 
Then Score(S) += slam_dunk 
Figure 3: WHAT Rules 
The rule set for WHEN questions, shown in 
Figure 4, is the only rule set that does not ap- 
ply the WordMatch function to every sentence 
in the story. WHEN questions almost always re- 
quire a TIME expression, so sentences that do 
not contain a TIME expression are only con- 
sidered in special cases. Rule #1 rewards all 
sentences that contain a TIME expression with 
good_c lue points as well as WordMatch points. 
The remaining rules look-for specific words that 
suggest a duration of time. Rule #3 is inter- 
esting because it recognizes that certain verbs 
("begin", "start") can be indicative of time even 
when no specific time is mentioned. 
The WHERE questions almost always look for 
specific locations, so the WHERE rules are very 
focused. Rule #1 applies the general word 
matching function and Rule #2 looks for sen- 
15 
:'." 
1. If contains(S,TIME) 
Then Score(S) += good_clue 
Score(S) += WordMatch(Q,S) 
2. If contains(Q,the last) and 
contains(S,{first, last, since, ago}) 
Then Score(S) += slam_dunk 
? 3. If contains( Q, { start, begin}) and 
contains(S,{ start, begin,since,year}) 
Then Score(S) += slam_dunk 
Figure 4: WHEN Rules 
1. Score(S) += WordMatch(Q,S) 
2. If contains(S,LocationPrep) 
Then Score(S) += good_clue 
3. If contains(S, LOCATION) 
Then Score(S) += confident 
Figure 5: WHERE Rules 
tences with a location preposition. Quarc rec- 
ognizes 21 prepositions as being associated with 
locations, such as "in", "at", "near", and "in- 
side". Rule #3 looks for sentences that contain 
a word belonging to the LOCATION semantic 
class. 
WHY questions are handled differently than 
other questions. The WHY rules are based on 
the observation that the answer to a WHY ques- 
tion often appears immediately before or im- 
mediately after the sentence that most closely 
matches the question. We believe that this is 
due to the causal nature of WHY questions. 
First, all sentences are assigned a score using 
the WordMatch function. Then the sentences 
with the top score are isolated. We will refer to 
these sentences as BEST. Every sentence score 
is then reinitialized to zero and the WHY rules, 
shown in Figure 6, are applied to every sentence 
in the story. 
Rule #1 rewards all sentences that produced 
the best WordMatch score because they are 
plausible candidates. Rule #2 rewards sen- 
tences that immediately precede a best Word- 
Match sentence, and Rule #3 rewards sentences 
that immediately follow a best WordMatch sen- 
tence. Rule #3 gives a higher score than Rules 
#1 and #2 because we observed that WHY an- 
swers are somewhat more likely to follow the 
best WordMatch sentence. Finally, Rule #4 re- 
wards sentences that contain the word "want" 
1. I fSeBEST 
Then Score(S) +-- clue 
2. If S immed, precedes member of BEST 
Then Score(S) += clue 
3. If S immed, follows member of BEST 
Then Score(S) += good_clue 
4. If contains(S,want) 
Then Score(S) += good_clue 
5. If contains(S, { so, because} ) 
Then Score(S) += good_clue 
Figure 6: WHY Rules 
and Rule #5 rewards sentences that contain the 
word "so" or "because". These words are in- 
dicative of intentions, explanations, and justifi- 
cations. 
The answers to WHEN and WHERE questions 
are frequently found in the dateline rather than 
the story itself, so Quarc also considers the date- 
line as a possible answer. Figure 7 shows the 
dateline rules, which are used for both WHEN 
and WHERE questions. The words "happen" 
and "take place" suggest hat the dateline may 
be the best answer (rules #1 and #2).  We also 
found that that the words "this" and "story" 
were strong indicators that the dateline is the 
best answer (rules #3 and #4) .  We found sev- 
eral sentences of the form "When did this hap- 
pen?" or "When did this take place?". The 
verbs alone are not sufficient to be slam dunks 
because they often have a specific subject (e.g., 
"When did the surprise happen?") that refers 
back to a sentence in the story. But when the 
words "story" or "this" appear, the question 
seems to be referring to the story in its entirety 
and the dateline is the best answer. 
1. If contains(Q,happen) 
Then Score(DATELINE) -b= good_clue 
2. If contains(Q,take) and 
contains(Q,place) 
Then Score(DATELINE) +---- good_clue 
3. If contains(Q,this) 
Then Score(DATELINE) --b: slam_dunk 
4. If contains(Q,story) 
Then Score(DATELINE) +---- slam_dunk 
Figure 7: Dateline Rules 
After all the rules have been applied to every 
16 
sentence in the story, the sentence (or dateline) 
with the highest score is returned as the best 
answer. In the event of a tie, a WHY question 
chooses the sentence that appears latest in the 
story, and all other question types choose the 
sentence that appears earliest in the story. If 
no sentence r ceives apositive score, then WHEN 
and WHERE questions return the dateline as a 
default, WHY questions return the last sentence 
in the story, and all other questions return the 
first sentence in the story. 
4 Exper imenta l  Resu l ts  
We evaluates Quarc on the same data set that 
was used to evaluate the DeepRead reading 
comprehension system (Hirschman et al, 1999). 
This data set contains 115 reading comprehen- 
sion tests, 55 of which were used for develop- 
ment and 60 of which were reserved for testing 
purposes. We also used the answer keys created 
by the DeepRead evelopers (Hirschman et al, 
1999). The HumSent  answers are sentences 
that a human judged to be the best answer for 
each question. The AutSent  answers are gen- 
erated automatically by determining which sen- 
tence contains the highest percentage of words 
in the published answer key, excluding stop- 
words. We focused on obtaining the best pos- 
sible HumSent  score because we believed that 
humans were more reliable than the automatic 
word-counting routine. 
Table 1 shows Quarc's results for each type 
of question as well as its overall results. Quarc 
achieved 40% HumSent  accuracy overall, but 
the accuracy varied substantially across ques- 
tion types. Quarc performed the best on WHEN 
questions, achieving 55% accuracy, and per- 
formed the worst on WHAT and WHY questions, 
reaching only 28% accuracy. 
Quarc's rules use a variety of knowledge 
sources, so we ran a set of experiments oevalu- 
ate the contribution of each type of knowledge. 
Figure 8 shows the results of these experiments, 
based on the HumSent  answer keys. First, 
we evaluated the performance of Quarc's Word- 
Match function all by itself, giving equal weight 
to verbs and non-verbs. The WordMatch func- 
tion alone, shown as Word on the graph, pro- 
duced 27% accuracy. When we gave verbs twice 
as much weight as non-verbs (? Verb), overall 
accuracy improved to 28%. Interestingly, giv- 
WHO 
HumSent: 0.41 (24/59) 
AutSent: 0.49 (29/59) 
WHAT 
HumSent: 0.28 (17/61) 
AutSent: 0.31 (19/61) 
WHEN 
HumSent: 0.55 (33/60) 
AutSent: 0.28 (17/60) 
WHERE 
HumSent: 0.47 (28/60) 
AutSent: 0.48 (29/60) 
WHY 
HumSent: 0.28 (17/60) 
AutSent: 0.27 (16/60) 
OVERALL 
HumSent: 0.40 (119/300) 
AutSent: 0.37 (110/300) 
Table 1: Overall Results 
ing extra weight to verbs improved the WHO 
and WHAT questions, but hurt the WHEN and 
WHERE questions. These results suggest hat 
verbs should be weighted more heavily only for 
certain question types, even though we 
Next, we wanted to see how much effect the 
semantic classes had on performance, so we 
added the rules that use semantic lasses. Only 
the WHO, WHEN, and WHAT question types had 
such rules, and performance improved on those 
question types (+Sem). We then added the 
dateline rules for the WHEN and WHERE ques- 
tions, and added the WHY rules that reward the 
sentences immediately preceding and following 
the best WordMatch sentence (rules #1-3 in 
Figure 6). Figure 8 shows that these additions 
(+Why/Dateline) also improved results for all 
three question types. 
Finally, we added the remaining rules that 
look for specific words and phrases. The final 
version of Quarc achieved 40% HumSent  ac- 
curacy, which compares favorably with Deep- 
Read's results (36% HumSent  accuracy). Fur- 
thermore, DeepRead's best results used hand- 
tagged named entity recognition and hand- 
tagged coreference r solution. Quarc did not 
rely on any hand-tagging and did not perform 
any coreference r slution. 
We also ran an experiment to evaluate the 
quality of Quarc's tie-breaking procedure, which 
was described at the end of Section 3. When 
17 
HumSent  Score  
0 .55  - -  
0 .50  --- 
0 .45  ~ ~ ~ ~ ~ ~ . .  ~ 
r 
ps  ~ . .  
0.40  - -  - " . . . . . . . . . . . . . . . . . . .  2 
035 . . . . .  :>-  
0.30  ,-- "~ 
I I Overa l l  
s ~  ~ - -  Who 
~ ~ - What  
" Where  
s 
t" ~-"  
0:25  - -  / ~ . 
0 .20  :- / 
/ 
/ 
/ 
0.15  : / 
I I t 
Word + Verb  + Sem + Why/Date l ine  + Qtype  Ru les  
Figure 8: Experimental Results 
more than one sentence is tied with the best 
score, Quarc selects the sentence that appears 
earliest in the story, except for WHY questions 
when Quarc chooses the sentence appearing lat- 
est in the story. Table 2 shows the results of 
removing this tie-breaking procedure, so that 
Quarc is allowed to output all sentences that 
received the top score. These results represent 
an upper bound on performance if Quarc had a 
perfect ie-breaking mechanism. 
Table 2 shows that Quarc's performance on 
WHAT, WHEN, and WHY questions improved by 
several percentage points, but performance on 
WHO and WHERE questions was basically the 
same. Overall, Quarc was able to identify 46% 
of the correct sentences by generating 1.75 hy- 
potheses per question on average. These re- 
sults suggest hat a better tie-breaking proce- 
dure could substantially improve Quarc's per- 
formance by choosing between the top two or 
three candidates more intelligently. 
5 Lessons  Learned  
Quarc's rules were devised by hand after ex- 
perimenting with the 55 reading comprehension 
tests in the development set. These simple rules 
are probably not adequate to handle other types 
of question-answering tasks, but this exercise 
gave us some insights into the problem. 
First, semantic lasses were extremely useful 
fo r  WHO,  WHEN,  and W H E R E  questions because 
they look for descriptions of people, dates, and 
locations. Second, WHY questions are concerned 
with causal information, and we discovered sev- 
eral keywords that were useful for identifying 
intentions, explanations, and justifications. A 
better understanding of causal relationships and 
discourse structure would undoubtedly be very 
helpful. Finally, WHAT questions were the most 
difficult because they sought a staggering vari- 
ety of answers. The only general pattern that 
we discovered was that WHAT questions often 
look for a description of an event or an object. 
Reading comprehension tests are a wonderful 
testbed for research in natural anguage process- 
ing because they require broad-coverage t ch- 
niques and semantic knowledge. In the future, 
we plan to incorporate coreference resolution, 
which seems to be very important for this task. 
We also plan to experiment with techniques that 
acquire semantic knowledge automatically (e.g., 
(Riloff and Shepherd, 1997; Roark and Char- 
niak, 1998)) to generate bigger and better se- 
mantic lexicons. 
18 
WHO 
HumSent: 0.42 (25/59) 
AutSent: 0.53 (31/59) 
Avg # answers: 1.27 
WHAT 
HumSent: 0.44 (27/61) 
AutSent: 0.49 (30/61) 
Avg # answers: 2.84 
WHEN 
HumSent: 0.62 (37/60) 
AutSent: 0.32 (19/60) 
Avg # answers: 1.45 
WHERE 
HumSent: 0.48 (29/60) 
AutSent: 0.48 (29/60) 
Avg #-answers: 1.33 
WHY 
HumSent: 0.33 (20/60) 
AutSent: 0.30 (18/60) 
Avg # answers: 1.82 
OVERALL 
HumSent: 0.46 (138/300) 
AutSent: 0.42 (127/300) 
Avg # answers: 1.75 
Table 2: Generating multiple answers 
6 Acknowledgments  
This research is supported in part by the Na- 
tional Science Foundation under grant IRI- 
9704240. 
Re ferences  
L. Hirschman, M. Light, E. Breck, and 
J. Burger. 1999. Deep Read: A Reading 
Comprehension System. In Proceedings of the 
37th Annual Meeting of the Association for 
Computational Linguistics. 
G. Miller. 1990. Wordnet: An On-line Lexical 
Database. International Journal of Lexicog- 
raphy, 3(4). 
E. Riloff and J. Shepherd. 1997. A Corpus- 
Based Approach for Building Semantic Lex- 
icons. In Proceedings of the Second Confer- 
ence on Empirical Methods in Natural Lan- 
guage Processing, pages 117-124. 
B. Roark and E. Charniak. 1998. Noun-phrase 
Co-occurrence Statistics for Semi-automatic 
Semantic Lexicon Construction. In Proceed- 
ings off the 36th Annual Meeting of the Asso- 
ciation for Computational Linguistics, pages 
1110-1116. 
19 
Looking Under the Hood: Tools for Diagnosing Your Question
Answering Engine
Eric Breck?, Marc Light?, Gideon S. Mann?, Ellen Riloff?,
Brianne Brown?, Pranav Anand?, Mats Rooth?, Michael Thelen?
? The MITRE Corporation, 202 Burlington Rd.,Bedford, MA 01730, {ebreck,light}@mitre.org
? Department of Computer Science, Johns Hopkins University, Baltimore, MD 21218, gsm@cs.jhu.edu
? School of Computing, University of Utah, Salt Lake City, UT 84112, {riloff,thelenm}@cs.utah.edu
? Bryn Mawr College, Bryn Mawr, PA 19010, bbrown@brynmawr.edu
? Department of Mathematics, Harvard University, Cambridge, MA 02138, anand@fas.harvard.edu
? Department of Linguistics, Cornell University, Ithaca, NY 14853, mr249@cornell.edu
Abstract
In this paper we analyze two question
answering tasks : the TREC-8 ques-
tion answering task and a set of reading
comprehension exams. First, we show
that Q/A systems perform better when
there are multiple answer opportunities
per question. Next, we analyze com-
mon approaches to two subproblems:
term overlap for answer sentence iden-
tification, and answer typing for short
answer extraction. We present general
tools for analyzing the strengths and
limitations of techniques for these sub-
problems. Our results quantify the limi-
tations of both term overlap and answer
typing to distinguish between compet-
ing answer candidates.
1 Introduction
When building a system to perform a task, the
most important statistic is the performance on
an end-to-end evaluation. For the task of open-
domain question answering against text collec-
tions, there have been two large-scale end-to-
end evaluations: (TREC-8 Proceedings, 1999)
and (TREC-9 Proceedings, 2000). In addition, a
number of researchers have built systems to take
reading comprehension examinations designed to
evaluate children?s reading levels (Charniak et al,
2000; Hirschman et al, 1999; Ng et al, 2000;
Riloff and Thelen, 2000; Wang et al, 2000). The
performance statistics have been useful for deter-
mining how well techniques work.
However, raw performance statistics are not
enough. If the score is low, we need to under-
stand what went wrong and how to fix it. If the
score is high, it is important to understand why.
For example, performance may be dependent on
characteristics of the current test set and would
not carry over to a new domain. It would also be
useful to know if there is a particular character-
istic of the system that is central. If so, then the
system can be streamlined and simplified.
In this paper, we explore ways of gaining
insight into question answering system perfor-
mance. First, we analyze the impact of having
multiple answer opportunities for a question. We
found that TREC-8 Q/A systems performed bet-
ter on questions that had multiple answer oppor-
tunities in the document collection. Second, we
present a variety of graphs to visualize and ana-
lyze functions for ranking sentences. The graphs
revealed that relative score instead of absolute
score is paramount. Third, we introduce bounds
on functions that use term overlap1 to rank sen-
tences. Fourth, we compute the expected score of
a hypothetical Q/A system that correctly identifies
the answer type for a question and correctly iden-
tifies all entities of that type in answer sentences.
We found that a surprising amount of ambiguity
remains because sentences often contain multiple
entities of the same type.
1Throughout the text, we use ?overlap? to refer to the
intersection of sets of words, most often the words in the
question and the words in a sentence.
2 The data
The experiments in Sections 3, 4, and 5 were per-
formed on two question answering data sets: (1)
the TREC-8 Question Answering Track data set
and (2) the CBC reading comprehension data set.
We will briefly describe each of these data sets
and their corresponding tasks.
The task of the TREC-8 Question Answering
track was to find the answer to 198 questions us-
ing a document collection consisting of roughly
500,000 newswire documents. For each question,
systems were allowed to return a ranked list of
5 short (either 50-character or 250-character) re-
sponses. As a service to track participants, AT&T
provided top documents returned by their retrieval
engine for each of the TREC questions. Sec-
tions 4 and 5 present analyses that use all sen-
tences in the top 10 of these documents. Each
sentence is classified as correct or incorrect auto-
matically. This automatic classification judges a
sentence to be correct if it contains at least half
of the stemmed, content-words in the answer key.
We have compared this automatic evaluation to
the TREC-8 QA track assessors and found it to
agree 93-95% of the time (Breck et al, 2000).
The CBC data set was created for the Johns
Hopkins Summer 2000 Workshop on Reading
Comprehension. Texts were collected from the
Canadian Broadcasting Corporation web page for
kids (http://cbc4kids.ca/). They are an average
of 24 sentences long. The stories were adapted
from newswire texts to be appropriate for ado-
lescent children, and most fall into the follow-
ing domains: politics, health, education, science,
human interest, disaster, sports, business, crime,
war, entertainment, and environment. For each
CBC story, 8-12 questions and an answer key
were generated.2 We used a 650 question sub-
set of the data and their corresponding 75 stories.
The answer candidates for each question in this
data set were all sentences in the document. The
sentences were scored against the answer key by
the automatic method described previously.
2This work was performed by Lisa Ferro and Tim Bevins
of the MITRE Corporation. Dr. Ferro has professional expe-
rience writing questions for reading comprehension exams
and led the question writing effort.
3 Analyzing the number of answer
opportunities per question
In this section we explore the impact of multiple
answer opportunities on end-to-end system per-
formance. A question may have multiple answers
for two reasons: (1) there is more than one differ-
ent answer to the question, and (2) there may be
multiple instances of each answer. For example,
?What does the Peugeot company manufacture??
can be answered by trucks, cars, or motors and
each of these answers may occur in many sen-
tences that provide enough context to answer the
question. The table insert in Figure 1 shows that,
on average, there are 7 answer occurrences per
question in the TREC-8 collection.3 In contrast,
there are only 1.25 answer occurrences in a CBC
document. The number of answer occurrences
varies widely, as illustrated by the standard devia-
tions. The median shows an answer frequency of
3 for TREC and 1 for CBC, which perhaps gives
a more realistic sense of the degree of answer fre-
quency for most questions.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1 2 3 4 5 6 7 9 1 2 1 4 1 8 2 7 2 8 6 1 6 7
# Answers
%
 Q
ue
st
io
ns
TREC-8
5 0
3 5 2
7.04
3
12.94
CBC
2 1 9
2 7 4
1.25
1
0.61
# Questions
# Answers
Mean
Median
Standard Dev.
Figure 1: Frequency of answers in the TREC-8
(black bars) and CBC (white bars) data sets
To gather this data we manually reviewed 50
randomly chosen TREC-8 questions and identi-
fied all answers to these questions in our text col-
lection. We defined an ?answer? as a text frag-
ment that contains the answer string in a context
sufficient to answer the question. Figure 1 shows
the resulting graph. The x-axis displays the num-
ber of answer occurrences found in the text col-
lection per question and the y-axis shows the per-
3We would like to thank John Burger and John Aberdeen
for help preparing Figure 1.
centage of questions that had x answers. For ex-
ample, 26% of the TREC-8 questions had only
1 answer occurrence, and 20% of the TREC-8
questions had exactly 2 answer occurrences (the
black bars). The most prolific question had 67
answer occurrences (the Peugeot example men-
tioned above). Figure 1 also shows the analysis
of 219 CBC questions. In contrast, 80% of the
CBC questions had only 1 answer occurrence in
the targeted document, and 16% had exactly 2 an-
swer occurrences.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0 1 0 2 0 3 0 4 0 5 0 6 0 7 0
# answers occurences per question
%
 o
f s
ys
te
m
s 
w
ith
 a
t l
ea
st
 o
ne
 c
or
re
ct
 re
sp
on
se
Point per question
Mean correct per occurrence #
Figure 2: Answer repetition vs. system response
correctness for TREC-8
Figure 2 shows the effect that multiple answer
opportunities had on the performance of TREC-8
systems. Each solid dot in the scatter plot repre-
sents one of the 50 questions we examined.4 The
x-axis shows the number of answer opportunities
for the question, and the y-axis represents the per-
centage of systems that generated a correct an-
swer5 for the question. E.g., for the question with
67 answer occurrences, 80% of the systems pro-
duced a correct answer. In contrast, many ques-
tions had a single answer occurrence and the per-
centage of systems that got those correct varied
from about 2% to 60%.
The circles in Figure 2 represent the average
percentage of systems that answered questions
correctly for all questions with the same number
of answer occurrences. For example, on average
about 27% of the systems produced a correct an-
swer for questions that had exactly one answer oc-
4We would like to thank Lynette Hirschman for suggest-
ing the analysis behind Figure 2 and John Burger for help
with the analysis and presentation.
5For this analysis, we say that a system generated a cor-
rect answer if a correct answer was in its response set.
currence, but about 50% of the systems produced
a correct answer for questions with 7 answer op-
portunities. Overall, a clear pattern emerges: the
performance of TREC-8 systems was strongly
correlated with the number of answer opportuni-
ties present in the document collection.
4 Graphs for analyzing scoring
functions of answer candidates
Most question answering systems generate sev-
eral answer candidates and rank them by defin-
ing a scoring function that maps answer candi-
dates to a range of numbers. In this section,
we analyze one particular scoring function: term
overlap between the question and answer can-
didate. The techniques we use can be easily
applied to other scoring functions as well (e.g.,
weighted term overlap, partial unification of sen-
tence parses, weighted abduction score, etc.). The
answer candidates we consider are the sentences
from the documents.
The expected performance of a system that
ranks all sentences using term overlap is 35% for
the TREC-8 data. This number is an expected
score because of ties: correct and incorrect can-
didates may have the same term overlap score. If
ties are broken optimally, the best possible score
(maximum) would be 54%. If ties are broken
maximally suboptimally, the worst possible score
(minimum) would be 24%. The corresponding
scores on the CBC data are 58% expected, 69%
maximum, and 51% minimum. We would like to
understand why the term overlap scoring function
works as well as it does and what can be done to
improve it.
Figures 3 and 4 compare correct candidates and
incorrect candidates with respect to the scoring
function. The x-axis plots the range of the scor-
ing function, i.e., the amount of overlap. The
y-axis represents Pr(overlap=x | correct) and
Pr(overlap=x | incorrect), where separate curves
are plotted for correct and incorrect candidates.
The probabilities are generated by normalizing
the number of correct/incorrect answer candidates
with a particular overlap score by the total number
of correct/incorrect candidates, respectively.
Figure 3 illustrates that the correct candidates
for TREC-8 have term overlap scores distributed
between 0 and 10 with a peak of 24% at an over-
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0 2 4 6 8 10 12 14 16 18 20N
or
m
al
iz
ed
 (+
/30
87
,-/5
70
73
) C
ou
nt
overlap
incorrect
correct
Figure 3: Pr(overlap=x|[in]correct) for TREC-8
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0 5 10 15 20 25 30No
rm
al
iz
ed
 (+
/13
11
,-/1
46
10
) C
ou
nt
overlap
incorrect
correct
Figure 4: Pr(overlap=x|[in]correct) for CBC
lap of 2. However, the incorrect candidates have
a similar distribution between 0 and 8 with a peak
of 32% at an overlap of 0. The similarity of the
curves illustrates that it is unclear how to use the
score to decide if a candidate is correct or not.
Certainly no static threshold above which a can-
didate is deemed correct will work. Yet the ex-
pected score of our TREC term overlap system
was 35%, which is much higher than a random
baseline which would get an expected score of
less than 3% because there are over 40 sentences
on average in newswire documents.6
After inspecting some of the data directly, we
posited that it was not the absolute term overlap
that was important for judging candidate but how
the overlap score compares to the scores of other
candidates. To visualize this, we generated new
graphs by plotting the rank of a candidate?s score
6We also tried dividing the term overlap score by the
length of the question to normalize for query length but did
not find that the graph was any more helpful.
on the x-axis. For example, the candidate with
the highest score would be ranked first, the can-
didate with the second highest score would be
ranked second, etc. Figures 5 and 6 show these
graphs, which display Pr(rank=x | correct) and
Pr(rank=x | incorrect) on the y-axis. The top-
ranked candidate has rank=0.
0
0.002
0.004
0.006
0.008
0.01
0.012
0.014
0.016
0.018
0.02
-
10
00
-
90
0
-
80
0
-
70
0
-
60
0
-
50
0
-
40
0
-
30
0
-
20
0
-
10
0 0
N
or
m
al
iz
ed
 (+
/30
87
,-/5
70
73
) C
ou
nt
ranked overlap
incorrect
correct
Figure 5: Pr(rank=x | [in]correct) for TREC-8
0
0.05
0.1
0.15
0.2
0.25
0.3
-45 -40 -35 -30 -25 -20 -15 -10 -5 0No
rm
al
iz
ed
 (+
/13
11
,-/1
46
10
) C
ou
nt
ranked overlap
incorrect
correct
Figure 6: Pr(rank=x | [in]correct) for CBC
The ranked graphs are more revealing than the
graphs of absolute scores: the probability of a
high rank is greater for correct answers than in-
correct ones. Now we can begin to understand
why the term overlap scoring function worked as
well as it did. We see that, unlike classification
tasks, there is no good threshold for our scor-
ing function. Instead relative score is paramount.
Systems such as (Ng et al, 2000) make explicit
use of relative rank in their algorithms and now
we understand why this is effective.
Before we leave the topic of graphing scoring
functions, we want to introduce one other view of
the data. Figure 7 plots term overlap scores on
-4
-3.5
-3
-2.5
-2
-1.5
-1
-0.5
0
0.5
1
0 2 4 6 8 10 12 14
0
2000
4000
6000
8000
10000
12000
14000
16000
18000
20000
lo
g-
od
ds
 o
f c
or
re
ct
ne
ss
m
a
ss
overlap
log-odds
mass curve
Figure 7: TREC-8 log odds correct given overlap
the x-axis and the log odds of being correct given
a score on the y-axis. The log odds formula is:
log Pr(correct|overlap)Pr(incorrect|overlap)
Intuitively, this graph shows how much more
likely a sentence is to be correct versus incorrect
given a particular score. A second curve, labeled
?mass,? plots the number of answer candidates
with each score. Figure 7 shows that the odds of
being correct are negative until an overlap of 10,
but the mass curve reveals that few answer candi-
dates have an overlap score greater than 6.
5 Bounds on scoring functions that use
term overlap
The scoring function used in the previous sec-
tion simply counts the number of terms shared
by a question and a sentence. One obvious mod-
ification is to weight some terms more heavily
than others. We tried using inverse document fre-
quence based (IDF) term weighting on the CBC
data but found that it did not improve perfor-
mance. The graph analogous to Figure 6 but with
IDF term weighting was virtually identical.
Could another weighting scheme perform bet-
ter? How well could an optimal weighting
scheme do? How poorly would the maximally
suboptimal scheme do? The analysis in this sec-
tion addresses these questions. In essence the an-
swer is the following: the question and the can-
didate answers are typically short and thus the
number of overlapping terms is small ? conse-
quently, many candidate answers have exactly the
same overlapping terms and no weighting scheme
could differentiate them. In addition, subset rela-
tions often hold between overlaps. A candidate
whose overlap is a subset of a second candidate
cannot score higher regardless of the weighting
scheme.7 We formalize these overlap set relations
and then calculate statistics based on them for the
CBC and TREC data.
Question: How much was Babe Belanger paid to play
amateur basketball?
S1: She was a member of the winningest
basketball team Canada ever had.
S2: Babe Belanger never made a cent for her
skills.
S3: They were just a group of young women
from the same school who liked to
play amateur basketball.
S4: Babe Belanger played with the Grads from
1929 to 1937.
S5: Babe never talked about her fabulous career.
MaxOsets : ( {S2, S4}, {S3} )
Figure 8: Example of Overlap Sets from CBC
Figure 8 presents an example from the CBC
data. The four overlap sets are (i) Babe Belanger,
(ii) basketball, (iii) play amateur basketball, and
(iv) Babe. In any term-weighting scheme with
positive weights, a sentence containing the words
Babe Belanger will have a higher score than sen-
tences containing just Babe, and sentences with
play amateur basketball will have a higher score
than those with just basketball. However, we can-
not generalize with respect to the relative scores
of sentences containing Babe Belanger and those
containing play amateur basketball because some
terms may have higher weights than others.
The most we can say is that the highest scor-
ing candidate must be a member of {S2, S4} or
{S3}. S5 and S1 cannot be ranked highest be-
cause their overlap sets are a proper subset of
competing overlap sets. The correct answer is
S2 so an optimal weighting scheme would have
a 50% chance of ranking S2 first, assuming that
it identified the correct overlap set {S2, S4} and
then randomly chose between S2 and S4. A max-
imally suboptimal weighting scheme could rank
S2 no lower than third.
We will formalize these concepts using the fol-
lowing variables:
7Assuming that all term weights are positive.
q: a question (a set of words)
s: a sentence (a set of words)
w,v: sets of intersecting words
We define an overlap set (ow,q) to be a set of
sentences (answer candidates) that have the same
words overlapping with the question. We define a
maximal overlap set (Mq) as an overlap set that is
not a subset of any other overlap set for the ques-
tion. For simplicity, we will refer to a maximal
overlap set as a MaxOset.
ow,q = {s|s ? q = w}
?q = all unique overlap sets for q
maximal(ow,q) if ?ov,q ? ?q, w 6? v
Mq = {ow,q ? ?q | maximal(ow,q)}
Cq = {s|s correctly answers q}
We can use these definitions to give upper
and lower bounds on the performance of term-
weighting functions on our two data sets. Table 1
shows the results. The max statistic is the per-
centage of questions for which at least one mem-
ber of its MaxOsets is correct. The min statis-
tic is the percentage of questions for which all
candidates of all of its MaxOsets are correct (i.e.,
there is no way to pick a wrong answer). Finally
the expectedmax is a slightly more realistic up-
per bound. It is equivalent to randomly choosing
among members of the ?best? maximal overlap
set, i.e., the MaxOset that has the highest percent-
age of correct members. Formally, the statistics
for a set of questions Q are computed as:
max = |{q|?o ? Mq,?s ? o s.t. s ? Cq}||Q|
min = |{q|?o ? Mq,?s ? o s ? Cq}||Q|
exp. max = 1|Q| ?
?
q?Q
max
o?Mq
|{s ? o and s ? Cq}|
|o|
The results for the TREC data are considerably
lower than the results for the CBC data. One ex-
planation may be that in the CBC data, only sen-
tences from one document containing the answer
are considered. In the TREC data, as in the TREC
task, it is not known beforehand which docu-
ments contain answers, so irrelevant documents
exp. max max min
CBC training 72.7% 79.0% 24.4%
TREC-8 48.8% 64.7% 10.1%
Table 1: Maximum overlap analysis of scores
may contain high-scoring sentences that distract
from the correct sentences.
In Table 2, we present a detailed breakdown
of the MaxOset results for the CBC data. (Note
that the classifications overlap, e.g., questions that
are in ?there is always a chance to get it right?
are also in the class ?there may be a chance to
get it right.?) 21% of the questions are literally
impossible to get right using only term weight-
ing because none of the correct sentences are in
the MaxOsets. This result illustrates that maxi-
mal overlap sets can identify the limitations of a
scoring function by recognizing that some candi-
dates will always be ranked higher than others.
Although our analysis only considered term over-
lap as a scoring function, maximal overlap sets
could be used to evaluate other scoring functions
as well, for example overlap sets based on seman-
tic classes rather than lexical items.
In sum, the upper bound for term weighting
schemes is quite low and the lower bound is
quite high. These results suggest that methods
such as query expansion are essential to increase
the feature sets used to score answer candidates.
Richer feature sets could distinguish candidates
that would otherwise be represented by the same
features and therefore would inevitably receive
the same score.
6 Analyzing the effect of multiple
answer type occurrences in a sentence
In this section, we analyze the problem of extract-
ing short answers from a sentence. Many Q/A
systems first decide what answer type a question
expects and then identify instances of that type in
sentences. A scoring function ranks the possible
answers using additional criteria, which may in-
clude features of the surrounding sentence such
as term overlap with the question.
For our analysis, we will assume that two short
answers that have the same answer type and come
from the same sentence are indistinguishable to
the system. This assumption is made by many
number of percentage
questions of questions
Impossible to get it wrong 159 24%
(?ow ? Mq, ?s ? ow, s ? Cq)
There is always a chance to get it right 45 7%
(?ow ? Mq, ?s ? ow s.t. s ? Cq)
There may be a chance to get it right 310 48%
(?ow ? Mq s.t. ?s ? ow s.t. s ? Cq)
The wrong answers will always be weighted too highly 137 21%
(?ow ? Mq, ?s ? ow, s 6? Cq)
There are no correct answers with any overlap with Q 66 10%
(?s ? d, s is incorrect or s has 0 overlap)
There are no correct answers (auto scoring error) 12 2%
(?s ? d, s is incorrect)
Table 2: Maximal Overlap Set Analysis for CBC data
Q/A systems: they do not have features that can
prefer one entity over another of the same type in
the same sentence.
We manually annotated data for 165 TREC-
9 questions and 186 CBC questions to indicate
perfect question typing, perfect answer sentence
identification, and perfect semantic tagging. Us-
ing these annotations, we measured how much
?answer confusion? remains if an oracle gives you
the correct question type, a sentence containing
the answer, and correctly tags all entities in the
sentence that match the question type. For exam-
ple, the oracle tells you that the question expects
a person, gives you a sentence containing the cor-
rect person, and tags all person entities in that sen-
tence. The one thing the oracle does not tell you
is which person is the correct one.
Table 3 shows the answer types that we used.
Most of the types are fairly standard, except for
the Defaultnp and Defaultvp which are default
tags for questions that desire a noun phrase or
verb phrase but cannot be more precisely typed.
We computed an expected score for this hy-
pothetical system as follows: for each question,
we divided the number of correct candidates (usu-
ally one) by the total number of candidates of the
same answer type in the sentence. For example,
if a question expects a Location as an answer and
the sentence contains three locations, then the ex-
pected accuracy of the system would be 1/3 be-
cause the system must choose among the loca-
tions randomly. When multiple sentences contain
a correct answer, we aggregated the sentences. Fi-
nally, we averaged this expected accuracy across
all questions for each answer type.
TREC CBC
Answer Type Score Freq Score Freq
defaultnp .33 47 .25 28
organization .50 1 .72 3
length .50 1 .75 2
thingname .58 14 .50 1
quantity .58 13 .77 14
agent .63 19 .40 23
location .70 24 .68 29
personname .72 11 .83 13
city .73 3 n/a 0
defaultvp .75 2 .42 15
temporal .78 16 .75 26
personnoun .79 7 .53 5
duration 1.0 3 .67 4
province 1.0 2 1.0 2
area 1.0 1 n/a 0
day 1.0 1 n/a 0
title n/a 0 .50 1
person n/a 0 .67 3
money n/a 0 .88 8
ambigbig n/a 0 .88 4
age n/a 0 1.0 2
comparison n/a 0 1.0 1
mass n/a 0 1.0 1
measure n/a 0 1.0 1
Overall .59 165 .61 186
Overall-dflts .69 116 .70 143
Table 3: Expected scores and frequencies for each
answer type
Table 3 shows that a system with perfect ques-
tion typing, perfect answer sentence identifica-
tion, and perfect semantic tagging would still
achieve only 59% accuracy on the TREC-9 data.
These results reveal that there are often multi-
ple candidates of the same type in a sentence.
For example, Temporal questions received an ex-
pected score of 78% because there was usually
only one date expression per sentence (the correct
one), while Default NP questions yielded an ex-
pected score of 25% because there were four noun
phrases per question on average. Some common
types were particularly problematic. Agent ques-
tions (most Who questions) had an answer con-
fusability of 0.63, while Quantity questions had a
confusability of 0.58.
The CBC data showed a similar level of an-
swer confusion, with an expected score of 61%,
although the confusability of individual answer
types varied from TREC. For example, Agent
questions were even more difficult, receiving a
score of 40%, but Quantity questions were easier
receiving a score of 77%.
Perhaps a better question analyzer could assign
more specific types to the Default NP and De-
fault VP questions, which skew the results. The
Overall-dflts row of Table 3 shows the expected
scores without these types, which is still about
70% so a great deal of answer confusion remains
even without those questions. The confusability
analysis provides insight into the limitations of
the answer type set, and may be useful for com-
paring the effectiveness of different answer type
sets (somewhat analogous to the use of grammar
perplexity in speech research).
Q1: What city is Massachusetts General Hospital located
in?
A1: It was conducted by a cooperative group of on-
cologists from Hoag, Massachusetts General Hospital
in Boston, Dartmouth College in New Hampshire, UC
San Diego Medical Center, McGill University in Montreal
and the University of Missouri in Columbia.
Q2: When was Nostradamus born?
A2: Mosley said followers of Nostradamus, who lived
from 1503 to 1566, have claimed ...
Figure 9: Sentences with Multiple Items of the
Same Type
However, Figure 9 shows the fundamental
problem behind answer confusability. Many sen-
tences contain multiple instances of the same
type, such as lists and ranges. In Q1, recognizing
that the question expects a city rather than a gen-
eral location is still not enough because several
cities are in the answer sentence. To achieve bet-
ter performance, Q/A systems need use features
that can more precisely target an answer.
7 Conclusion
In this paper we have presented four analyses of
question answering system performance involv-
ing: multiple answer occurence, relative score for
candidate ranking, bounds on term overlap perfor-
mance, and limitations of answer typing for short
answer extraction. We hope that both the results
and the tools we describe will be useful to others.
In general, we feel that analysis of good perfor-
mance is nearly as important as the performance
itself and that the analysis of bad performance can
be equally important.
References
E.J. Breck, J.D. Burger, L. Ferro, L. Hirschman, D. House,
M. Light, and I. Mani. 2000. How to Evaluate your
Question Answering System Every Day and Still Get
Real Work Done. In Proceedings of the Second Con-
ference on Language Resources and Evaluation (LREC-
2000).
E. Charniak, Y. Altun, R. de Salvo Braz, B. Garrett, M. Kos-
mala, T. Moscovich, L. Pang, C. Pyo, Y. Sun, W. Wy,
Z. Yang, S. Zeller, and L. Zorn. 2000. Reading Compre-
hension Programs in a Statistical-Language-Processing
Class. In ANLP/NAACL Workshop on Reading Com-
prehension Tests as Evaluation for Computer-Based Lan-
guage Understanding Systems.
L. Hirschman, M. Light, E. Breck, and J. Burger. 1999.
Deep Read: A Reading Comprehension System. In Pro-
ceedings of the 37th Annual Meeting of the Association
for Computational Linguistics.
H.T. Ng, L.H. Teo, and J.L.P. Kwan. 2000. A Machine
Learning Approach to Answering Questions for Reading
Comprehension Tests. In Proceedings of EMNLP/VLC-
2000 at ACL-2000.
E. Riloff and M. Thelen. 2000. A Rule-based Question
Answering System for Reading Comprehension Tests.
In ANLP/NAACL Workshop on Reading Comprehension
Tests as Evaluation for Computer-Based Language Un-
derstanding Systems.
TREC-8 Proceedings. 1999. Proceedings of the Eighth
Text Retrieval Conference (TREC8). National Institute of
Standards and Technology, Special Publication 500-246,
Gaithersburg, MD.
TREC-9 Proceedings. 2000. Proceedings of the Ninth Text
Retrieval Conference (forthcoming). National Institute
of Standards and Technology, Special Publication 500-
XXX, Gaithersburg, MD.
W. Wang, Auer J., R. Parasuraman, I. Zubarev, D. Brandy-
berry, and M.P. Harper. 2000. A Question Answering
System Developed as a Project in a Natural Language
Processing Course. In ANLP/NAACL Workshop on Read-
ing Comprehension Tests as Evaluation for Computer-
Based Language Understanding Systems.
Exploiting Strong Syntactic Heuristics and Co-Training to Learn Semantic
Lexicons
William Phillips and Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT 84112 USA
fphillips,riloffg@cs.utah.edu
Abstract
We present a bootstrapping method that
uses strong syntactic heuristics to learn
semantic lexicons. The three sources
of information are appositives, compound
nouns, and ISA clauses. We apply heuris-
tics to these syntactic structures, embed
them in a bootstrapping architecture, and
combine them with co-training. Results
on WSJ articles and a pharmaceutical cor-
pus show that this method obtains high
precision and finds a large number of
terms.
1 Introduction
Syntactic structure helps us understand the seman-
tic relationships between words. Given a text cor-
pus, we can use knowledge about syntactic struc-
tures to obtain semantic knowledge. For example,
Hearst (Hearst, 1992) learned hyponymy relation-
ships by collecting words in lexico-syntactic expres-
sions, such as ?NP, NP, and other NPs?, and Roark
and Charniak (Roark and Charniak, 1998) gener-
ated semantically related words by applying statisti-
cal measures to syntactic contexts involving apposi-
tives, lists, and conjunctions.
Exploiting syntactic structures to learn semantic
knowledge holds great promise, but can run into
problems. First, lexico-syntactic expressions that
explicitly indicate semantic relationships (e.g., ?NP,
NP, and other NPs?) are reliable but a lot of semantic
information occurs outside these expressions. Sec-
ond, general syntactic structures (e.g., lists and con-
junctions) capture a wide range of semantic rela-
tionships. For example, conjunctions frequently
join items of the same semantic class (e.g., ?cats
and dogs?), but they can also join different seman-
tic classes (e.g., ?fire and ice?). Some researchers
(Roark and Charniak, 1998; Riloff and Shepherd,
1997) have applied statistical methods to identify
the strongest semantic associations. This approach
has produced reasonable results, but the accuracy of
these techniques still leaves much room for improve-
ment.
We adopt an intermediate approach that learns
semantic lexicons using strong syntactic heuristics,
which are both common and reliable. We have
identified certain types of appositives, compound
nouns, and identity (ISA) clauses that indicate spe-
cific semantic associations between words. We em-
bed syntactic heuristics in a bootstrapping process
and present empirical results demonstrating that this
bootstrapping process produces high-quality seman-
tic lexicons. In another set of experiments, we in-
corporate a co-training (Blum and Mitchell, 1998)
mechanism to combine the hypotheses generated by
different types of syntactic structures. Co-training
produces a synergistic effect across different heuris-
tics, substantially increasing the coverage of the lex-
icons while maintaining nearly the same level of ac-
curacy.
2 Semantic Lexicon Learning
The goal of our research is to automatically gener-
ate a semantic lexicon. For our purposes, we de-
fine a semantic lexicon to be a list of words with
semantic category labels. For example, the word
?bird? might be labeled as an ANIMAL and the word
?car? might be labeled as a VEHICLE. Semantic
lexicons have proven to be useful for many lan-
                                            Association for Computational Linguistics.
                    Language Processing (EMNLP), Philadelphia, July 2002, pp. 125-132.
                         Proceedings of the Conference on Empirical Methods in Natural
guage processing tasks, including anaphora resolu-
tion (Aone and Bennett, 1996; McCarthy and Lehn-
ert, 1995), prepositional phrase attachment (Brill
and Resnik, 1994), information extraction (Soder-
land et al, 1995; Riloff and Schmelzenbach, 1998),
and question answering (Harabagiu et al, 2000;
Hirschman et al, 1999).
Some general-purposes semantic dictionaries al-
ready exist, such as WordNet (Miller, 1990). Word-
Net has been used for many applications, but it may
not contain the vocabulary and jargon needed for
specialized domains. For example, WordNet does
not contain much of the vocabulary found in medical
texts. In previous research on semantic lexicon in-
duction, Roark and Charniak (Roark and Charniak,
1998) showed that 3 of every 5 words learned by
their system were not present in WordNet. Further-
more, they used relatively unspecialized text cor-
pora: Wall Street Journal articles and terrorism news
stories. Our goal is to develop techniques for seman-
tic lexicon induction that could be used to enhance
existing resources such as WordNet, or to create dic-
tionaries for specialized domains.
Several techniques have been developed to gen-
erate semantic knowledge using weakly supervised
learning techniques. Hearst (Hearst, 1992) ex-
tracted information from lexico-syntactic expres-
sions that explicitly indicate hyponymic relation-
ships. Hearst?s work is similar in spirit to our
work in that her system identified reliable syntac-
tic structures that explicitly reveal semantic associa-
tions. Meta-bootstrapping (Riloff and Jones, 1999)
is a semantic lexicon learning technique very differ-
ent from ours which utilizes information extraction
patterns to identify semantically related contexts.
Named entity recognizers (e.g., (Bikel et al, 1997;
Collins and Singer, 1999; Cucerzan and Yarowsky,
1999)) can be trained to recognize proper names
associated with semantic categories such as PER-
SON or ORGANIZATION, but they typically are not
aimed at learning common nouns such as ?surgeon?
or ?drugmaker?.
Several researchers have used some of the same
syntactic structures that we exploit in our research,
namely appositives and compound nouns. For ex-
ample, Riloff and Shepherd (Riloff and Shepherd,
1997) developed a statistical co-occurrence model
for semantic lexicon induction that was designed
with these structures in mind. Roark and Char-
niak (Roark and Charniak, 1998) followed up on
this work by using a parser to explicitly capture
these structures. Caraballo (Caraballo, 1999) also
exploited these syntactic structures and applied a co-
sine vector model to produce semantic groupings. In
our view, these previous systems used ?weak? syn-
tactic models because the syntactic structures some-
times identified desirable semantic associations and
sometimes did not. To compensate, statistical mod-
els were used to separate the meaningful semantic
associations from the spurious ones. In contrast, our
work aims to identify ?strong? syntactic heuristics
that can isolate instances of general structures that
reliably identify the desired semantic relations.
3 A Bootstrapping Model that Exploits
Strong Syntactic Heuristics
For the purposes of this research, we will define two
distinct types of lexicons. One lexicon will con-
sist of proper noun phrases, such as ?Federal Avi-
ation Administration?. We will call this the PNP
(proper noun phrase) lexicon. The second lexicon
will consist of common (non-proper) nouns, such as
?airplane?. We will call this the GN (general noun)
lexicon. The reason for creating these distinct lexi-
cons is that our algorithm takes advantage of syntac-
tic relationships between proper nouns and general
nouns.
3.1 Syntactic Heuristics
Our goal is to build a semantic lexicon of words that
belong to the same semantic class. More specifi-
cally, we aim to find words that have the same hyper-
nym, for example ?dog? and ?frog? would both have
the hypernym ANIMAL.1 We will refer to words that
have the same immediate hypernym as semantic sib-
lings.
We hypothesize that some syntactic structures can
be used to reliably identify semantic siblings. We
have identified three candidates: appositives, com-
pound nouns, and identity clauses whose main verb
is a form of ?to be? (we will call these ISA clauses).
1The appropriate granularity of a set of semantic classes,
or the organization of a semantic hierarchy, is always open to
debate. We chose categories that seem to represent important
and relatively general semantic distinctions.
While these structures often do capture semantic sib-
lings, they frequently capture other types of seman-
tic relationships as well. Therefore we use heuristics
to isolate subsets of these syntactic structures that
consistently contain semantic siblings. Our heuris-
tics are based on the observation that many of these
structures contain both a proper noun phrase and a
general noun phrase which are co-referent and usu-
ally belong to the same semantic class. In the fol-
lowing sections, we explain the heuristics that we
use for each syntactic structure, and how those struc-
tures are used to learn new lexicon entries.
3.1.1 Appositives
Appositives are commonly occurring syntactic
structures that contain pairs of semantically related
noun phrases. A simple appositive structure con-
sists of a noun phrase (NP), followed by a comma,
followed by another NP, where the two NPs are co-
referent. However, appositives often signify hyper-
nym relationships (e.g., ?the dog, a carnivorous ani-
mal?).
To identify semantic siblings, we only use appos-
itives that contain one proper noun phrase and one
general noun phrase. For example, ?George Bush,
the president? or ?the president, George Bush?. The-
oretically, such appositives could also indicate a hy-
pernym relationship (e.g., ?George Bush, a mam-
mal?), but we have found that this rarely happens
in practice.
3.1.2 Compound Nouns
Compound nouns are extremely common but they
can represent a staggering variety of semantic rela-
tionships. We have found one type of compound
noun that can be reliably used to harvest seman-
tic siblings. We loosely define these compounds
as ?GN+ PNP? noun phrases, where the compound
noun ends with a proper name but is modified with
one or more general nouns. Examples of such com-
pounds are ?violinist James Braum? or ?software
maker Microsoft?. One of the difficulties with rec-
ognizing these constructs, however, is resolving the
ambiguity between adjectives and nouns among the
modifiers (e.g., ?violinist? is a noun). We only use
constructs in which the GN modifier is unambigu-
ously a noun.
3.1.3 ISA Clauses
Certain ?to be? clauses can also be harvested to
extract semantic siblings. We define an ISA clause
as an NP followed by a VP that is a form of ?to be?,
followed by another NP. These identity clauses also
exhibit a wide range of semantic relationships, but
harvesting clauses which contain one proper NP and
one general NP can reliably identify noun phrases
of the same semantic class. We found that this struc-
ture yields semantic siblings when the subject NP is
constrained to be a proper NP and the object NP is
constrained to be a general NP (e.g., ?Jing Lee is the
president of the company?).
3.2 The Bootstrapping Model
Figure 1 illustrates the bootstrapping model for each
of the three syntactic structures. Initially, the lex-
icons contain only a few manually defined seed
words: some proper noun phrases and some general
nouns. The syntactic heuristics are then applied to
the text corpus to collect potentially ?harvestable?
structures. Each heuristic identifies structures with
one proper NP and one general NP, where one of
them is already present in the lexicon as a member
of a desired semantic class. The other NP is then as-
sumed to belong to the same semantic class and is
added to a prospective word list. Finally, statistical
filtering is used to divide the prospective word lists
into exclusive and non-exclusive subsets. We will
describe the motivation for this in Section 3.2.2. The
exclusive words are added to the lexicon, and the
bootstrapping process repeats. In the remainder of
this section, we explain how the bootstrapping pro-
cess works in more detail.
3.2.1 Bootstrapping Procedure
The input to our system is a small set of seed
words for the semantic categories of interest. To
identify good seed words, we sorted all nouns in
the corpus by frequency and manually identified the
most frequent nouns that belong to each targeted se-
mantic category.
Each bootstrapping iteration alternates between
using either the PNP lexicon or the GN lexicon to
grow the lexicons. As a motivating example, as-
sume that (1) appositives are the targeted syntactic
structure (2) bootstrapping begins by using the PNP
lexicon, and (3) PEOPLE is the semantic category of
Proper NP
Lexicon
Syntactic Heuristics Text Corpus
Prospective
Proper NPs
Seed Words
Exclusive
N
on?Exclusive
N
on?Exclusive
Exclusive
Lexicon
General Noun
Prospective
General Nouns
Figure 1: Bootstrapping Model
interest. The system will then collect all appositives
that contain a proper noun phrase known to be a per-
son. So if ?Mary Smith? belongs to the PNP lexicon
and the appositive ?Mary Smith, the analyst? is en-
countered, the head noun ?analyst? will be learned
as a person.
The next bootstrapping iteration uses the GN lex-
icon, so the system will collect all appositives that
contain a general noun phrase known to be a person.
If the appositive ?John Seng, the financial analyst?
is encountered, then ?John Seng? will be learned as
a person because the word ?analyst? is known to
be a person from the previous iteration. The boot-
strapping process will continue, alternately using the
PNP lexicon and the GN lexicon, until no new words
can be learned.
We treat proper noun phrases and general noun
phrases differently during learning. When a proper
noun phrase is learned, the full noun phrase is added
to the lexicon. But when a general noun phrase is
learned, only the head noun is added to the lexi-
con. This approach gives us generality because head
nouns are usually (though not always) sufficient to
associate a common noun phrase with a semantic
class. Proper names, however, often do not exhibit
this generality (e.g., ?Saint Louis? is a location but
?Louis? is not).
However, using full proper noun phrases can limit
the ability of the bootstrapping process to acquire
new terms because exact matches are relatively rare.
To compensate, head nouns and modifying nouns of
proper NPs are used as predictor terms to recognize
new proper NPs that belong to the same semantic
class. We identify reliable predictor terms using the
evidence and exclusivity measures that we will de-
fine in the next section. For example, the word ?Mr.?
is learned as a good predictor term for the person cat-
egory. These predictor terms are only used to clas-
sify noun phrases during bootstrapping and are not
themselves added to the lexicon.
3.2.2 Exclusivity Filtering
Our syntactic heuristics were designed to reliably
identify words belonging to the same semantic class,
but some erroneous terms still slip through for vari-
ous reasons, such as parser errors and idiomatic ex-
pressions. Perhaps the biggest problem comes from
ambiguous terms that can belong to several seman-
tic classes. For instance, in the financial domain
?leader? can refer to both people and corporations.
If ?leader? is added to the person lexicon, then it
will pull corporation terms into the lexicon during
subsequent bootstrapping iterations and the person
lexicon will be compromised.
To address this problem, we classify all candidate
words as being exclusive to the semantic category or
non-exclusive. For example, the word ?president?
nearly always refers to a person so it is exclusive to
the person category, but the word ?leader? is non-
exclusive. Only the exclusive terms are added to
the semantic lexicon during bootstrapping to keep
the lexicon as pure (unambiguous) as possible. The
non-exclusive terms can be added to the final lexicon
when bootstrapping is finished if polysemous terms
are acceptable to have in the dictionary.
Exclusivity filtering is the only step that uses
statistics. Two measures determine whether a word
is exclusive to a semantic category. First, we use an
evidence measure:
Evidence(w; c) =
S
w;c
S
w
where S
w
is the number of times word w was found
in the syntactic structure, and S
w;c
is the number of
times word w was found in the syntactic structure
collocated with a member of category c. The evi-
dence measure is the maximum likelihood estimate
that a word belongs to a semantic category given that
it appears in the targeted syntactic structure (a word
is assumed to belong to the category if it is collo-
cated with another category member). Since few
words are known category members initially, we use
a low threshold value (.25) which simply ensures
that a non-trivial proportion of instances are collo-
cated with category members.
The second measure that we use, exclusivity, is
the number of occurrences found in the given cate-
gory?s prospective list divided by the number of oc-
currences found in all other categories? prospective
lists.
Exclusivity(w; c) =
S
w;c
S
w;:c
where S
w;c
is the number of times word w was found
in the syntactic structure collocated with a member
of category c, and S
w;:c
is the number of times word
w was found in the syntactic structure collocated
with a member of a different semantic class. We
apply a threshold to this ratio to ensure that the term
is exclusive to the targeted semantic category.
3.3 Experimental Results
We evaluated our system on several semantic cate-
gories in two domains. In one set of experiments,
we generated lexicons for PEOPLE and ORGANIZA-
TIONS using 2500 Wall Street Journal articles from
the Penn Treebank (Marcus et al, 1993). In the sec-
ond set of experiments, we generated lexicons for
PEOPLE, ORGANIZATIONS, and PRODUCTS using
approximately 1350 press releases from pharmaceu-
tical companies.2
Our seeding consisted of 5 proper nouns and 5
general nouns for each semantic category. We used a
threshold of 25% for the evidence measure and 5 for
the exclusivity ratio. We ran the bootstrapping pro-
cess until no new words were learned, which ranged
from 6-14 iterations depending on the category and
syntactic structure.
Table 1 shows 10 examples of words learned for
each semantic category in each domain. The people
and organization lists illustrate (1) how dramatically
the vocabulary can differ across domains, and (2)
that the lexicons may include domain-specific word
meanings that are not the most common meaning
2We found these texts using Yahoo?s financial industry pages
at http://biz.yahoo.com/news/medical.html.
People (WSJ): adman, co-chairman, head,
economist, shareholder, AMR Chairman Robert
Crandall, Assistant Secretary David Mullins,
Deng Xiaoping, Abby Joseph Cohen, C. Everett
Koop
Organization (WSJ): parent, subsidiary, dis-
tiller, arm, suitor, AMR Corp., ABB ASEA
Brown Boveri, J.P. Morgan, James River, Federal
Reserve Board
People (Pharm): surgeon, executive, recipient,
co-author, pioneer, Amgen Chief Executive Of-
ficer, Barbara Ryan, Chief Scientific Officer Nor-
bert Riedel, Dr. Cole, Analyst Mark Augustine
Organization (Pharm): device-maker, drug-
maker, licensee, organization, venture, ALR
Technologies, Aventis Pharmaceuticals, Bayer
AG, FDA Advisory Panel, Hadassah University
Hospital
Product (Pharm): compound, stent, platform,
blocker, antibiotic, Bexxar, Viratrol, MBX-102,
Apothesys Decision Support System, AERx Pain
Management System
Table 1: Examples of Learned Words
of a word in general. For example, the word ?par-
ent? generally refers to a person, but in a financial
domain it nearly always refers to an organization.
The pharmaceutical product category contains many
nouns (e.g., drug names) that may not be in a general
purpose lexicon such as WordNet.
Tables 2 and 3 show the results of our evaluation.
We ran the bootstrapping algorithm on each type of
syntactic structure independently. The Total column
shows the total number of lexicon entries generated
by each syntactic structure. The Correct column
contains two accuracy numbers: X/Y. The first value
(X) is the percentage of entries that were judged to
be correct, and the second value (Y) is the accuracy
after removing entries resulting from parser errors.3
The PNP lexicons were substantially larger than
the GN lexicons, in part because we saved full noun
3For example, our parser frequently mistags adjectives as
nouns, so many adjectives were hypothesized to be people. If
the parser had tagged them correctly, they would not have been
allowed in the lexicon.
Appositives Compounds ISA Union
Category Total Correct Total Correct Total Correct Total Correct
People (WSJ) 1826 .97/.97 2026 .99/.99 113 .94/.94 3543 1.0/1.0
Orgs (WSJ) 674 .87/.94 3770 .77/.78 54 .93/.96 4191 .79/.79
People (Pharm) 280 .86/.87 1723 .87/.88 39 1.0/1.0 1872 .85/.91
Orgs (Pharm) 205 .85/.88 1128 .85/.92 248 .85/.91 1399 .78/.84
Products (Pharm) 64 .94/.95 223 .77/.79 64 .84/.84 330 .83/.85
Table 2: Proper Noun Phrase Lexicon Results
Appositives Compounds ISA Union
Category Total Correct Total Correct Total Correct Total Correct
People (WSJ) 159 .91/.95 60 .30/.56 41 .85/.97 229 .73/.88
Orgs (WSJ) 84 .69/.75 54 .26/.47 6 1.0/1.0 134 .52/.66
People (Pharm) 34 .91/.91 32 .66/.75 18 1.0/1.0 66 .79/.84
Orgs (Pharm) 36 .58/.60 29 .35/.46 41 .51/.66 95 .45/.54
Products (Pharm) 8 .75/1.0 11 .09/.33 13 .54/1.0 32 .50/.89
Table 3: General Noun Lexicon Results
phrases in the PNP lexicon but only head nouns in
the GN lexicon. Probably the main reason, however,
is that there are many more proper names associated
with most semantic categories than there are general
nouns. Consequently, we evaluated the PNP and GN
lexicons differently. For the GN lexicons, a volun-
teer (not one of the authors) labeled every word as
correct or incorrect. Due to the large size of the PNP
lexicons, we randomly sampled 100 words for each
syntactic structure and semantic category and asked
volunteers to label these samples. Consequently, the
PNP evaluation numbers are estimates of the true ac-
curacy.
The Union column tabulates the results obtained
from unioning the lexicons produced by the three
syntactic structures independently.4 Although there
is some overlap in their lexicons, we found that
many different words are being learned. This indi-
cates that the three syntactic structures are tapping
into different parts of the search space, which sug-
gests that combining them in a co-training model
could be beneficial.
4Since the number of words contributed by each syntac-
tic structure varied greatly, we evaluated the Union results for
the PNP lexicon by randomly sampling 100 words from the
unioned lexicons regardless of which structure generated them.
This maintained the same distribution in our evaluation set as
exists in the lexicon as a whole. However, this sampling strat-
egy means that the evaluation results in the Union column are
not simply the sum of the results in the preceding columns.
Seed Words
Syntactic Structures
Lexicons for All 3
Appositive
Bootstrapping
Process
Compound Noun
Bootstrapping 
Process
ISA Clause
Bootstrapping
Process
Figure 2: Co-Training Model
3.4 Co-Training
Co-training (Blum and Mitchell, 1998) is a learn-
ing technique which combines classifiers that sup-
port different views of the data in a single learning
mechanism. The co-training model allows examples
learned by one classifier to be used by the other clas-
sifiers, producing a synergistic effect. The three syn-
tactic structures that we have discussed provide three
different ways to harvest semantically related noun
phrases.
Figure 2 shows our co-training model, with each
syntactic structure serving as an independent classi-
fier. The words hypothesized by each classifier are
put into a single PNP lexicon and a single GN lex-
icon, which are shared by all three classifiers. We
used an aggressive form of co-training, where all
terms hypothesized by a syntactic structure with fre-
quency   are added to the shared lexicon. The
threshold ensures some confidence in a term before
it is allowed to be used by the other learners. We
used a threshold of =3 for the WSJ corpus and
=2 for the pharmaceutical corpus since it is sub-
stantially smaller. We ran the bootstrapping process
until no new words were learned, which was 12 it-
erations for the WSJ corpus and 10 iterations for the
pharmaceutical corpus.5
PNP PNP GN GN
Category cotrn w/o cotrn w/o
People (WSJ) 5414 3543 347 229
Orgs (WSJ) 4227 4191 213 134
People (Pharm) 2217 1872 84 66
Orgs (Pharm) 4068 1399 196 95
Products (Pharm) 309 330 38 32
Table 4: Lexicon sizes with and w/o co-training
Table 4 shows the size of the learned lexicons with
co-training and without co-training (i.e., running the
classifiers separately). In almost all cases, many ad-
ditional words were learned using the co-training
model. Tables 5 and 6 show the evaluation results
for the lexicons produced by co-training. The co-
training model produced substantially better cover-
age, while achieving nearly the same accuracy. One
exception was organizations in the pharmaceutical
domain, which suffered a sizeable loss in precision.
This is most likely due to the co-training loop be-
ing too aggressive. If one classifier produces a lot
of mistakes (in this case, the compound noun classi-
fier), then those mistakes can drag down the overall
accuracy of the lexicon.
4 Conclusions
We have presented a method for learning seman-
tic lexicons that uses strong syntactic heuristics in
a bootstrapping algorithm. We exploited three types
of syntactic structures (appositives, compound NPs,
5After co-training finished, we also added terms to the lexi-
con that were hypothesized by an individual classifier with fre-
quency <  if they had not previously been labeled.
and ISA clauses) in combination with heuristics to
identify instances of these structures that contain
both a proper and general noun phrase. Each syntac-
tic structure generated many lexicon entries, in most
cases with high accuracy. We also combined the
three classifiers using co-training. The co-training
model increased the number of learned lexicon en-
tries, while maintaining nearly the same level of ac-
curacy. One limitation of this work is that it can only
learn semantic categories that are commonly found
as proper nouns and general nouns.
This research illustrates that common syntactic
structures can be combined with heuristics to iden-
tify specific semantic relationships. So far we have
experimented with three structures and one type of
heuristic (proper NP/general NP collocations), but
we believe that this approach holds promise for other
semantic learning tasks as well. In future work, we
hope to investigate other types of syntactic struc-
tures that may be used to identify semantically re-
lated terms, and other types of heuristics that can
reveal specific semantic relationships.
5 Acknowledgements
This research was supported by the National Science
Foundation under award IRI-9704240. Thanks to
Erin Davies, Brijesh Garabadu, Dominic Jones, and
Henry Longmore for labeling data.
References
C. Aone and S. W. Bennett. 1996. Applying machine learn-
ing to anaphora resolution. In Stefan Wermter, Ellen Riloff,
and Gabriele Scheler, editors, Connectionist, Statistical, and
Symbolic Approaches to Learning for Natural Language
Processing, pages 302?314. Springer-Verlag, Berlin.
Daniel M. Bikel, Scott Miller, Richard Schwartz, and Ralph
Weischedel. 1997. Nymble: a high-performance learning
name-finder. In Proceedings of ANLP-97, pages 194?201.
A. Blum and T. Mitchell. 1998. Combining Labeled and Unla-
beled Data with Co-Training. In Proceedings of the 11th An-
nual Conference on Computational Learning Theory (COLT-
98).
E. Brill and P. Resnik. 1994. A Transformation-based Ap-
proach to Prepositional Phrase Attachment Disambiguation.
In Proceedings of the Fifteenth International Conference on
Computational Linguistics (COLING-94).
S. Caraballo. 1999. Automatic Acquisition of a Hypernym-
Labeled Noun Hierarchy from Text. In Proceedings of the
37th Annual Meeting of the Association for Computational
Linguistics, pages 120?126.
Appositives Compounds ISA Union
Category Total Correct Total Correct Total Correct Total Correct
People (WSJ) 1890 .98/.98 4979 .99/.99 143 .90/.90 5414 .99/.99
Orgs (WSJ) 744 .83/.88 3791 .77/.77 115 .76/.78 4227 .78/.78
People (Pharm) 292 .87/.88 2132 .82/.90 56 .80/.82 2217 .81/.90
Orgs (Pharm) 281 .79/.80 3872 .53/.59 305 .77/.82 4068 .49/.50
Products (Pharm) 65 .94/.95 225 .78/.80 69 .84/.84 309 .83/.86
Table 5: Proper Noun Phrase Lexicon Results after Co-Training
Appositives Compounds ISA Union
Category Total Correct Total Correct Total Correct Total Correct
People (WSJ) 200 .89/.93 160 .58/.78 73 .69/.81 347 .69/.83
Orgs (WSJ) 125 .66/.71 66 .26/.46 55 .46/.60 213 .47/.60
People (Pharm) 44 .86/.86 38 .66/.74 30 .90/.93 84 .75/.80
Orgs (Pharm) 73 .56/.59 90 .23/.35 70 .49/.61 196 .36/.47
Products (Pharm) 9 .78/1.0 17 .24/.67 17 .65/1.0 38 .50/.91
Table 6: General Noun Lexicon Results after Co-Training
M. Collins and Y. Singer. 1999. Unsupervised Models for
Named Entity Classification. In Proceedings of the Joint
SIGDAT Conference on Empirical Methods in Natural Lan-
guage Processing and Very Large Corpora (EMNLP/VLC-
99).
S. Cucerzan and D. Yarowsky. 1999. Language Independent
Named Entity Recognition Combining Morphologi cal and
Contextual Evidence. In Proceedings of the Joint SIGDAT
Conference on Empirical Methods in Natural Language Pro-
cessing and Very Large Corpora (EMNLP/VLC-99).
S. Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea, Surdeanu
M., R. Bunescu, R. Girju, V. Rus, and P. Morarescu. 2000.
FALCON: Boosting Knowledge for Answer Engines. In
Proceedings of the Ninth Text Retrieval Conference (TREC-
9).
M. Hearst. 1992. Automatic Acquisition of Hyponyms
from Large Text Corpora. In Proceedings of the Four-
teenth International Conference on Computational Linguis-
tics (COLING-92).
Lynette Hirschman, Marc Light, Eric Breck, and John D.
Burger. 1999. Deep Read: A reading comprehension sys-
tem. In Proceedings of the 37th Annual Meeting of the As-
sociation for Computational Linguistics.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Build-
ing a Large Annotated Corpus of English: The Penn Tree-
bank. Computational Linguistics, 19(2):313?330.
Joseph F. McCarthy and Wendy G. Lehnert. 1995. Using De-
cision Trees for Coreference Resolution. In Proceedings of
the Fourteenth International Joint Conference on Artificial
Intelligence, pages 1050?1055.
G. Miller. 1990. Wordnet: An On-line Lexical Database. In-
ternational Journal of Lexicography, 3(4).
E. Riloff and R. Jones. 1999. Learning Dictionaries for In-
formation Extraction by Multi-Level Bootstrapping. In Pro-
ceedings of the Sixteenth National Conference on Artificial
Intelligence.
E. Riloff and M. Schmelzenbach. 1998. An Empirical Ap-
proach to Conceptual Case Frame Acquisition. In Proceed-
ings of the Sixth Workshop on Very Large Corpora, pages
49?56.
E. Riloff and J. Shepherd. 1997. A Corpus-Based Approach
for Building Semantic Lexicons. In Proceedings of the Sec-
ond Conference on Empirical Methods in Natural Language
Processing, pages 117?124.
B. Roark and E. Charniak. 1998. Noun-phrase Co-occurrence
Statistics for Semi-automatic Semantic Lexicon Construc-
tion. In Proceedings of the 36th Annual Meeting of the As-
sociation for Computational Linguistics, pages 1110?1116.
S. Soderland, D. Fisher, J. Aseltine, and W. Lehnert. 1995.
CRYSTAL: Inducing a conceptual dictionary. In Proceed-
ings of the Fourteenth International Joint Conference on Ar-
tificial Intelligence, pages 1314?1319.
A Bootstrapping Method for Learning Semantic Lexicons using
Extraction Pattern Contexts
Michael Thelen and Ellen Rilo
School of Computing
University of Utah
Salt Lake City, UT 84112 USA
fthelenm,rilog@cs.utah.edu
Abstract
This paper describes a bootstrapping al-
gorithm called Basilisk that learns high-
quality semantic lexicons for multiple cate-
gories. Basilisk begins with an unannotated
corpus and seed words for each semantic
category, which are then bootstrapped to
learn new words for each category. Basilisk
hypothesizes the semantic class of a word
based on collective information over a large
body of extraction pattern contexts. We
evaluate Basilisk on six semantic categories.
The semantic lexicons produced by Basilisk
have higher precision than those produced
by previous techniques, with several cate-
gories showing substantial improvement.
1 Introduction
In recent years, several algorithms have been devel-
oped to acquire semantic lexicons automatically or
semi-automatically using corpus-based techniques.
For our purposes, the term semantic lexicon will refer
to a dictionary of words labeled with semantic classes
(e.g., \bird" is an animal and \truck" is a vehicle).
Semantic class information has proven to be useful
for many natural language processing tasks, includ-
ing information extraction (Rilo and Schmelzen-
bach, 1998; Soderland et al, 1995), anaphora resolu-
tion (Aone and Bennett, 1996), question answering
(Moldovan et al, 1999; Hirschman et al, 1999), and
prepositional phrase attachment (Brill and Resnik,
1994). Although some semantic dictionaries do exist
(e.g., WordNet (Miller, 1990)), these resources often
do not contain the specialized vocabulary and jargon
that is needed for specic domains. Even for rela-
tively general texts, such as the Wall Street Journal
(Marcus et al, 1993) or terrorism articles (MUC-
4 Proceedings, 1992), Roark and Charniak (Roark
and Charniak, 1998) reported that 3 of every 5 terms
generated by their semantic lexicon learner were not
present in WordNet. These results suggest that auto-
matic semantic lexicon acquisition could be used to
enhance existing resources such as WordNet, or to
produce semantic lexicons for specialized domains.
We have developed a weakly supervised bootstrap-
ping algorithm called Basilisk that automatically
generates semantic lexicons. Basilisk hypothesizes
the semantic class of a word by gathering collective
evidence about semantic associations from extraction
pattern contexts. Basilisk also learns multiple se-
mantic classes simultaneously, which helps constrain
the bootstrapping process.
First, we present Basilisk?s bootstrapping algo-
rithm and explain how it diers from previous work
on semantic lexicon induction. Second, we present
empirical results showing that Basilisk outperforms
a previous algorithm. Third, we explore the idea of
learning multiple semantic categories simultaneously
by adding this capability to Basilisk as well as an-
other bootstrapping algorithm. Finally, we present
results showing that learning multiple semantic cat-
egories simultaneously improves performance.
2 Bootstrapping using Collective
Evidence from Extraction Patterns
Basilisk (Bootstrapping Approach to SemantIc
Lexicon Induction using Semantic Knowledge) is a
weakly supervised bootstrapping algorithm that au-
tomatically generates semantic lexicons. Figure 1
shows the high-level view of Basilisk?s bootstrapping
process. The input to Basilisk is an unannotated
text corpus and a few manually dened seed words
for each semantic category. Before bootstrapping
begins, we run an extraction pattern learner over
the corpus which generates patterns to extract ev-
ery noun phrase in the corpus.
The bootstrapping process begins by selecting a
subset of the extraction patterns that tend to ex-
tract the seed words. We call this the pattern pool.
                                            Association for Computational Linguistics.
                    Language Processing (EMNLP), Philadelphia, July 2002, pp. 214-221.
                         Proceedings of the Conference on Empirical Methods in Natural
The nouns extracted by these patterns become can-
didates for the lexicon and are placed in a candidate
word pool. Basilisk scores each candidate word by
gathering all patterns that extract it and measur-
ing how strongly those contexts are associated with
words that belong to the semantic category. The
ve best candidate words are added to the lexicon,
and the process starts over again. In this section, we
describe Basilisk?s bootstrapping algorithm in more
detail and discuss related work.
extraction patterns and
      their extractions
add extractions of
     best patternsbest patterns
     select
pattern pool
 semantic
  lexicon
words
 seed
candidate
word pool
BASILISK
initialize
add 5 best candidate words
Figure 1: Basilisk Algorithm
2.1 Basilisk
The input to Basilisk is a text corpus and a set of seed
words. We generated seed words by sorting the words
in the corpus by frequency and manually identifying
the 10 most frequent nouns that belong to each cat-
egory. These seed words form the initial semantic
lexicon. In this section we describe the learning pro-
cess for a single semantic category. In Section 3 we
will explain how the process is adapted to handle
multiple categories simultaneously.
To identify new lexicon entries, Basilisk relies
on extraction patterns to provide contextual evi-
dence that a word belongs to a semantic class. As
our representation for extraction patterns, we used
the AutoSlog system (Rilo, 1996). AutoSlog?s
extraction patterns represent linguistic expressions
that extract a noun phrase in one of three syntac-
tic roles: subject, direct object, or prepositional
phrase object. For example, three patterns that
would extract people are: \<subject> was arrested",
\murdered <direct object>", and \collaborated with
<pp object>". Extraction patterns represent linguis-
tic contexts that often reveal the meaning of a word
by virtue of syntax and lexical semantics. Extraction
patterns are typically designed to capture role rela-
tionships. For example, consider the verb \robbed"
when it occurs in the active voice. The subject of
\robbed" identies the perpetrator, while the direct
object of \robbed" identies the victim or target.
Before bootstrapping begins, we run AutoSlog ex-
haustively over the corpus to generate an extraction
Generate all extraction patterns in the corpus
and record their extractions.
lexicon = fseed wordsg
i := 0
BOOTSTRAPPING
1. Score all extraction patterns
2. pattern pool = top ranked 20+i patterns
3. candidate word pool = extractions
of patterns in pattern pool
4. Score candidate words in candidate word pool
5. Add top 5 candidate words to lexicon
6. i := i + 1
7. Go to Step 1.
Figure 2: Basilisk?s bootstrapping algorithm
pattern for every noun phrase that appears. The
patterns are then applied to the corpus and all of
their extracted noun phrases are recorded. Figure 2
shows the bootstrapping process that follows, which
we explain in the following sections.
2.1.1 The Pattern Pool and Candidate Pool
The rst step in the bootstrapping process is to
score the extraction patterns based on their tendency
to extract known category members. All words that
are currently dened in the semantic lexicon are con-
sidered to be category members. Basilisk scores each
pattern using the RlogF metric that has been used
for extraction pattern learning (Rilo, 1996). The
score for each pattern is computed as:
RlogF (pattern
i
) =
F
i
N
i
 log
2
(F
i
) (1)
where F
i
is the number of category members ex-
tracted by pattern
i
and N
i
is the total number of
nouns extracted by pattern
i
. Intuitively, the RlogF
metric is a weighted conditional probability; a pat-
tern receives a high score if a high percentage of its
extractions are category members, or if a moderate
percentage of its extractions are category members
and it extracts a lot of them.
The top N extraction patterns are put into a pat-
tern pool. Basilisk uses a value of N=20 for the rst
iteration, which allows a variety of patterns to be
considered, yet is small enough that all of the pat-
terns are strongly associated with the category.1
1\Depleted" patterns are not included in this set. A
pattern is depleted if all of its extracted nouns are already
dened in the lexicon, in which case it has no unclassied
words to contribute.
The purpose of the pattern pool is to narrow down
the eld of candidates for the lexicon. Basilisk col-
lects all noun phrases (NPs) extracted by patterns in
the pattern pool and puts the head noun of each NP
into the candidate word pool. Only these nouns are
considered for addition to the lexicon.
As the bootstrapping progresses, using the same
value N=20 causes the candidate pool to become
stagnant. For example, let?s assume that Basilisk
performs perfectly, adding only valid category words
to the lexicon. After some number of iterations, all
of the valid category members extracted by the top
20 patterns will have been added to the lexicon, leav-
ing only non-category words left to consider. For this
reason, the pattern pool needs to be infused with new
patterns so that more nouns (extractions) become
available for consideration. To achieve this eect,
we increment the value of N by one after each boot-
strapping iteration. This ensures that there is always
at least one new pattern contributing words to the
candidate word pool on each successive iteration.
2.1.2 Selecting Words for the Lexicon
The next step is to score the candidate words. For
each word, Basilisk collects every pattern that ex-
tracted the word. All extraction patterns are used
during this step, not just the patterns in the pat-
tern pool. Initially, we used a scoring function that
computes the average number of category members
extracted by the patterns. The formula is:
score(word
i
) =
P
i
X
j=1
F
j
P
i
(2)
where P
i
is the number of patterns that extract
word
i
, and F
j
is the number of distinct category
members extracted by pattern j. A word receives
a high score if it is extracted by patterns that also
have a tendency to extract known category members.
As an example, suppose the word \Peru" is in the
candidate word pool as a possible location. Basilisk
nds all patterns that extract \Peru" and computes
the average number of known locations extracted by
those patterns. Let?s assume that the three patterns
shown below extract \Peru" and that the underlined
words are known locations. \Peru" would receive a
score of (2+3+2)/3 = 2.3. Intuitively, this means
that patterns that extract \Peru" also extract, on
average, 2.3 known location words.
\was killed in <np>"
Extractions: Peru, clashes, a shootout, El Salvador,
Colombia
\<np> was divided"
Extractions: the country, the Medellin cartel, Colombia,
Peru, the army, Nicaragua
\ambassador to <np>"
Extractions: Nicaragua, Peru, the UN, Panama
Unfortunately, this scoring function has a problem.
The average can be heavily skewed by one pattern
that extracts a large number of category members.
For example, suppose word w is extracted by 10 pat-
terns, 9 which do not extract any category members
but the tenth extracts 50 category members. The
average number of category members extracted by
these patterns will be 5. This is misleading because
the only evidence linking word w with the semantic
category is a single, high-frequency extraction pat-
tern (which may extract words that belong to other
categories as well).
To alleviate this problem, we modied the scor-
ing function to compute the average logarithm of the
number of category members extracted by each pat-
tern. The logarithm reduces the influence of any sin-
gle pattern. We will refer to this scoring metric as
the AvgLog function, which is dened below. Since
log
2
(1) = 0, we add one to each frequency count so
that patterns which extract a single category mem-
ber contribute a positive value.
AvgLog(word
i
) =
P
i
X
j=1
log
2
(F
j
+ 1)
P
i
(3)
Using this scoring metric, all words in the candi-
date word pool are scored and the top ve words are
added to the semantic lexicon. The pattern pool and
the candidate word pool are then emptied, and the
bootstrapping process starts over again.
2.1.3 Related Work
Several weakly supervised learning algorithms
have previously been developed to generate seman-
tic lexicons from text corpora. Rilo and Shepherd
(Rilo and Shepherd, 1997) developed a bootstrap-
ping algorithm that exploits lexical co-occurrence
statistics, and Roark and Charniak (Roark and
Charniak, 1998) rened this algorithm to focus more
explicitly on certain syntactic structures. Hale, Ge,
and Charniak (Ge et al, 1998) devised a technique
to learn the gender of words. Caraballo (Caraballo,
1999) and Hearst (Hearst, 1992) created techniques
to learn hypernym/hyponym relationships. None of
these previous algorithms used extraction patterns or
similar contexts to infer semantic class associations.
Several learning algorithms have also been de-
veloped for named entity recognition (e.g., (Collins
and Singer, 1999; Cucerzan and Yarowsky, 1999)).
(Collins and Singer, 1999) used contextual informa-
tion of a dierent sort than we do. Furthermore, our
research aims to learn general nouns (e.g., \artist")
rather than proper nouns, so many of the features
commonly used to great advantage for named entity
recognition (e.g., capitalization and title words) are
not applicable to our task.
The algorithm most closely related to Basilisk is
meta-bootstrapping (Rilo and Jones, 1999), which
also uses extraction pattern contexts for semantic
lexicon induction. Meta-bootstrapping identies a
single extraction pattern that is highly correlated
with a semantic category and then assumes that all of
its extracted noun phrases belong to the same cat-
egory. However, this assumption is often violated,
which allows incorrect terms to enter the lexicon.
Rilo and Jones acknowledged this issue and used
a second level of bootstrapping (the \Meta" boot-
strapping level) to alleviate this problem. While
meta-bootstrapping trusts individual extraction pat-
terns to make unilateral decisions, Basilisk gath-
ers collective evidence from a large set of extrac-
tion patterns. As we will demonstrate in Sec-
tion 2.2, Basilisk?s approach produces better re-
sults than meta-bootstrapping and is also consid-
erably more ecient because it uses only a single
bootstrapping loop (meta-bootstrapping uses nested
bootstrapping). However, meta-bootstrapping pro-
duces category-specic extraction patterns in addi-
tion to a semantic lexicon, while Basilisk focuses ex-
clusively on semantic lexicon induction.
2.2 Single Category Results
To evaluate Basilisk?s performance, we ran experi-
ments with the MUC-4 corpus (MUC-4 Proceedings,
1992), which contains 1700 texts associated with ter-
rorism. We used Basilisk to learn semantic lexicons
for six semantic categories: building, event, hu-
man, location, time, and weapon. Before we ran
these experiments, one of the authors manually la-
beled every head noun in the corpus that was found
by an extraction pattern. These manual annota-
tions were the gold standard. Table 1 shows the
breakdown of semantic categories for the head nouns.
These numbers represent a baseline: an algorithm
that randomly selects words would be expected to
get accuracies consistent with these numbers.
Three semantic lexicon learners have previously
been evaluated on the MUC-4 corpus (Rilo and
Shepherd, 1997; Roark and Charniak, 1998; Rilo
and Jones, 1999), and of these meta-bootstrapping
achieved the best results. So we implemented the
meta-bootstrapping algorithm ourselves to directly
Category Total Percentage
building 188 2.2%
event 501 5.9%
human 1856 21.9%
location 1018 12.0%
time 112 1.3%
weapon 147 1.7%
other 4638 54.8%
Table 1: Breakdown of semantic categories
compare its performance with that of Basilisk. A
dierence between the original implementation and
ours is that our version learns individual nouns (as
does Basilisk) instead of noun phrases. We believe
that learning individual nouns is a more conservative
approach because noun phrases often overlap (e.g.,
\high-power bombs" and \incendiary bombs" would
count as two dierent lexicon entries in the origi-
nal meta-bootstrapping algorithm). Consequently,
our meta-bootstrapping results dier from those re-
ported in (Rilo and Jones, 1999).
Figure 3 shows the results for Basilisk (ba-1) and
meta-bootstrapping (mb-1). We ran both algorithms
for 200 iterations, so that 1000 words were added to
the lexicon (5 words per iteration). The X axis shows
the number of words learned, and the Y axis shows
how many were correct. The Y axes have dierent
ranges because some categories are more prolic than
others. Basilisk outperforms meta-bootstrapping for
every category, often substantially. For the human
and location categories, Basilisk learned hundreds of
words, with accuracies in the 80-89% range through
much of the bootstrapping. It is worth noting that
Basilisk?s performance held up well on the human
and location categories even at the end, achieving
79.5% (795/1000) accuracy for humans and 53.2%
(532/1000) accuracy for locations.
3 Learning Multiple Semantic
Categories Simultaneously
We also explored the idea of bootstrapping multiple
semantic classes simultaneously. Our hypothesis was
that errors of confusion2 between semantic categories
can be lessened by using information about multi-
ple categories. This hypothesis makes sense only if a
word cannot belong to more than one semantic class.
In general, this is not true because words are often
polysemous. But within a limited domain, a word
usually has a dominant word sense. Therefore we
make a \one sense per domain" assumption (similar
2We use the term confusion to refer to errors where a
word is labeled as category X when it really belongs to
category Y .
0
10
20
30
40
50
60
70
80
90
100
0 200 400 600 800 1000
Co
rre
ct
 L
ex
ico
n 
En
tri
es
Total Lexicon Entries
Building
ba-1
mb-1
0
50
100
150
200
250
300
0 200 400 600 800 1000
Co
rre
ct
 L
ex
ico
n 
En
tri
es
Total Lexicon Entries
Event
ba-1
mb-1
0
100
200
300
400
500
600
700
800
0 200 400 600 800 1000
Co
rre
ct
 L
ex
ico
n 
En
tri
es
Total Lexicon Entries
Human
ba-1
mb-1
0
50
100
150
200
250
300
350
400
450
500
0 200 400 600 800 1000
Co
rre
ct
 L
ex
ico
n 
En
tri
es
Total Lexicon Entries
Location
ba-1
mb-1
0
5
10
15
20
25
30
35
40
0 200 400 600 800 1000
Co
rre
ct
 L
ex
ico
n 
En
tri
es
Total Lexicon Entries
Time
ba-1
mb-1
0
10
20
30
40
50
60
70
80
0 200 400 600 800 1000
Co
rre
ct
 L
ex
ico
n 
En
tri
es
Total Lexicon Entries
Weapon
ba-1
mb-1
Figure 3: Basilisk and Meta-Bootstrapping Results,
Single Category
to the \one sense per discourse" observation (Gale et
al., 1992)) that a word belongs to a single semantic
category within a limited domain. All of our ex-
periments involve the MUC-4 terrorism domain and
corpus, for which this assumption seems appropriate.
3.1 Motivation
Figure 4 shows one way of viewing the task of se-
mantic lexicon induction. The set of all words in the
corpus is visualized as a search space. Each cate-
gory owns a certain territory within the space (de-
marcated with a dashed line), representing the words
that are true members of that category. Not all ter-
ritories are the same size, since some categories have
more members than others.
A
B
C
D
E
F
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                








































Figure 4: Bootstrapping a Single Category
Figure 4 illustrates what happens when a semantic
lexicon is generated for a single category. The seed
words for the category (in this case, category C) are
represented by the solid black area in category C?s
territory. The hypothesized words in the growing
lexicon are represented by a shaded area. The goal
of the bootstrapping algorithm is to expand the area
of hypothesized words so that it exactly matches the
category?s true territory. If the shaded area expands
beyond the category?s true territory, then incorrect
words have been added to the lexicon. In Figure 4,
category C has claimed a signicant number of words
that belong to categories B and E. When generating
a lexicon for one category at a time, these confusion
errors are impossible to detect because the learner
has no knowledge of the other categories.
A
C
D E
F
B
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          

























             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             





















           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           

















        
     
     
     
     
     
     
     
     
     
        
        
        
           
           















                 
                 
                 
                 
                 
                 
                 
                 
                 









   
   
   
   
   
      
      
      
                
                
                
                
                
                
                
                
                
                
                
                
                
                





 
 
 














      
      
      
      
      
      






Figure 5: Bootstrapping Multiple Categories
Figure 5 shows the same search space when lexi-
cons are generated for six categories simultaneously.
If the lexicons cannot overlap, then we constrain the
ability of a category to overstep its bounds. Cate-
gory C is stopped when it begins to encroach upon
the territories of categories B and E because words
in those areas have already been claimed.
3.2 Simple Conflict Resolution
The easiest way to take advantage of multiple cate-
gories is to add simple conflict resolution that en-
forces the \one sense per domain" constraint. If
more than one category tries to claim a word, then
we use conflict resolution to decide which category
should win. We incorporated a simple conflict reso-
lution procedure into Basilisk, as well as the meta-
bootstrapping algorithm. For both algorithms, the
conflict resolution procedure works as follows. (1) If
a word is hypothesized for category A but has already
been assigned to category B during a previous iter-
ation, then the category A hypothesis is discarded.
(2) If a word is hypothesized for both category A
and category B during the same iteration, then it
020
40
60
80
100
120
0 200 400 600 800 1000
Co
rre
ct
 L
ex
ico
n 
En
tri
es
Total Lexicon Entries
Building
ba-M
ba-1
0
50
100
150
200
250
300
0 200 400 600 800 1000
Co
rre
ct
 L
ex
ico
n 
En
tri
es
Total Lexicon Entries
Event
ba-M
ba-1
0
100
200
300
400
500
600
700
800
0 200 400 600 800 1000
Co
rre
ct
 L
ex
ico
n 
En
tri
es
Total Lexicon Entries
Human
ba-M
ba-1
0
100
200
300
400
500
600
0 200 400 600 800 1000
Co
rre
ct
 L
ex
ico
n 
En
tri
es
Total Lexicon Entries
Location
ba-M
ba-1
0
5
10
15
20
25
30
35
40
0 200 400 600 800 1000
Co
rre
ct
 L
ex
ico
n 
En
tri
es
Total Lexicon Entries
Time
ba-M
ba-1
0
10
20
30
40
50
60
70
80
90
0 200 400 600 800 1000
Co
rre
ct
 L
ex
ico
n 
En
tri
es
Total Lexicon Entries
Weapon
ba-M
ba-1
Figure 6: Basilisk, MCAT vs. 1CAT
is assigned to the category for which it receives the
highest score. In Section 3.4, we will present empiri-
cal results showing how this simple conflict resolution
scheme aects performance.
3.3 A Smarter Scoring Function for
Multiple Categories
Simple conflict resolution helps the algorithm
recognize when it has encroached on another cate-
gory?s territory, but it does not actively steer the
bootstrapping in a more promising direction. A
more intelligent way to handle multiple categories
is to incorporate knowledge about other categories
directly into the scoring function. We modied
Basilisk?s scoring function to prefer words that have
strong evidence for one category but little or no
evidence for competing categories. Each word w
i
in
the candidate word pool receives a score for category
c
a
based on the following formula:
di(w
i
,c
a
) = AvgLog(w
i
,c
a
) - max
b6=a
(AvgLog(w
i
,c
b
))
where AvgLog is the candidate scoring function used
previously by Basilisk (see Equation 3) and the max
function returns the maximum AvgLog value over
all competing categories. For example, the score for
each candidate location word will be its AvgLog
score for the location category minus its maxi-
mum AvgLog score for all other categories. A word
is ranked highly only if it has a high score for the
0
10
20
30
40
50
60
70
80
90
100
0 200 400 600 800 1000
Co
rre
ct
 L
ex
ico
n 
En
tri
es
Total Lexicon Entries
Building
mb-M
mb-1
0
50
100
150
200
250
0 200 400 600 800 1000
Co
rre
ct
 L
ex
ico
n 
En
tri
es
Total Lexicon Entries
Event
mb-M
mb-1
0
100
200
300
400
500
600
700
0 200 400 600 800 1000
Co
rre
ct
 L
ex
ico
n 
En
tri
es
Total Lexicon Entries
Human
mb-M
mb-1
0
100
200
300
400
500
600
0 200 400 600 800 1000
Co
rre
ct
 L
ex
ico
n 
En
tri
es
Total Lexicon Entries
Location
mb-M
mb-1
0
5
10
15
20
25
30
0 200 400 600 800 1000
Co
rre
ct
 L
ex
ico
n 
En
tri
es
Total Lexicon Entries
Time
mb-M
mb-1
0
10
20
30
40
50
60
70
80
90
100
0 200 400 600 800 1000
Co
rre
ct
 L
ex
ico
n 
En
tri
es
Total Lexicon Entries
Weapon
mb-M
mb-1
Figure 7: Meta-Bootstrapping, MCAT vs. 1CAT
targeted category and there is little evidence that it
belongs to a dierent category. This has the eect
of steering the bootstrapping process away from am-
biguous parts of the search space.
3.4 Multiple Category Results
We will use the abbreviation 1CAT to indicate that
only one semantic category was bootstrapped, and
MCAT to indicate that multiple semantic categories
were simultaneously bootstrapped. Figure 6 com-
pares the performance of Basilisk-MCAT with con-
flict resolution (ba-M) against Basilisk-1CAT (ba-1).
Most categories show small performance gains, with
the building, location, and weapon categories
benetting the most. However, the improvement
usually doesn?t kick in until many bootstrapping it-
erations have passed. This phenomenon is consistent
with the visualization of the search space in Figure 5.
Since the seed words for each category are not gener-
ally located near each other in the search space, the
bootstrapping process is unaected by conflict reso-
lution until the categories begin to encroach on each
other?s territories.
Figure 7 compares the performance of Meta-
Bootstrapping-MCAT with conflict resolution
(mb-M) against Meta-Bootstrapping-1CAT (mb-
1). Learning multiple categories improves the
performance of meta-bootstrapping dramatically
for most categories. We were surprised that the
improvement for meta-bootstrapping was much
020
40
60
80
100
120
0 200 400 600 800 1000
Co
rre
ct
 L
ex
ico
n 
En
tri
es
Total Lexicon Entries
Building
ba-M+
ba-M
mb-M
0
50
100
150
200
250
300
0 200 400 600 800 1000
Co
rre
ct
 L
ex
ico
n 
En
tri
es
Total Lexicon Entries
Event
ba-M+
ba-M
mb-M
0
100
200
300
400
500
600
700
800
900
0 200 400 600 800 1000
Co
rre
ct
 L
ex
ico
n 
En
tri
es
Total Lexicon Entries
Human
ba-M+
ba-M
mb-M
0
100
200
300
400
500
600
0 200 400 600 800 1000
Co
rre
ct
 L
ex
ico
n 
En
tri
es
Total Lexicon Entries
Location
ba-M+
ba-M
mb-M
0
5
10
15
20
25
30
35
40
45
0 200 400 600 800 1000
Co
rre
ct
 L
ex
ico
n 
En
tri
es
Total Lexicon Entries
Time
ba-M+
ba-M
mb-M
0
10
20
30
40
50
60
70
80
90
100
0 200 400 600 800 1000
Co
rre
ct
 L
ex
ico
n 
En
tri
es
Total Lexicon Entries
Weapon
ba-M+
ba-M
mb-M
Figure 8: MetaBoot-MCAT vs. Basilisk-MCAT vs.
Basilisk-MCAT+
more pronounced than for Basilisk. It seems that
Basilisk was already doing a better job with errors
of confusion, so meta-bootstrapping had more room
for improvement.
Finally, we evaluated Basilisk using the di scoring
function to handle multiple categories. Figure 8 com-
pares all three MCAT algorithms, with the smarter
di version of Basilisk labeled as ba-M+. Over-
all, this version of Basilisk performs best, showing
a small improvement over the version with simple
conflict resolution. Both multiple category versions
of Basilisk also consistently outperform the multiple
category version of meta-bootstrapping.
Table 2 summarizes the improvement of the
best version of Basilisk over the original meta-
bootstrapping algorithm. The left-hand column rep-
resents the number of words learned and each cell in-
dicates how many of those words were correct. These
results show that Basilisk produces substantially bet-
ter accuracy and coverage than meta-bootstrapping.
Figure 9 shows examples of words learned by
Basilisk. Inspection of the lexicons reveals many un-
usual words that could be easily overlooked by some-
one building a dictionary by hand. For example, the
words \deserter" and \narcoterrorists" appear in a
variety of terrorism articles but they are not com-
monly used words in general.
We also measured the recall of Basilisk?s lexicons
after 1000 words had been learned, based on the gold
Total MetaBoot Basilisk
Words 1CAT MCAT+
building
100 21 (21.0%) 39 (39.0%)
200 28 (14.0%) 72 (36.0%)
500 33 (6.6%) 100 (20.0%)
800 39 (4.9%) 109 (13.6%)
1000 43 (4.3%) n/a
event
100 61 (61.0%) 61 (61.0%)
200 89 (44.5%) 114 (57.0%)
500 146 (29.2%) 186 (37.2%)
800 172 (21.5%) 240 (30.0%)
1000 190 (19.0%) 266 (26.6%)
human
100 36 (36.0%) 84 (84.0%)
200 53 (26.5%) 173 (86.5%)
500 143 (28.6%) 431 (86.2%)
800 224 (28.0%) 681 (85.1%)
1000 278 (27.8%) 829 (82.9%)
location
100 54 (54.0%) 84 (84.0%)
200 99 (49.5%) 175 (87.5%)
500 237 (47.4%) 371 (74.2%)
800 302 (37.8%) 509 (63.6%)
1000 310 (31.0%) n/a
time
100 9 (9.0%) 30 (30.0%)
200 13 (6.5%) 33 (16.5%)
500 21 (4.2%) 37 (7.4%)
800 25 (3.1%) 43 (5.4%)
1000 26 (2.6%) 45 (4.5%)
weapon
100 23 (23.0%) 42 (42.0%)
200 24 (12.0%) 62 (31.0%)
500 29 (5.8%) 85 (17.0%)
800 33 (4.1%) 88 (11.0%)
1000 33 (3.3%) n/a
Table 2: Lexicon Results
standard data shown in Table 1. The recall results
range from 40-60%, which indicates that a good per-
centage of the category words are being found, al-
though there are clearly more category words lurking
in the corpus.
4 Conclusions
Basilisk?s bootstrapping algorithm exploits two
ideas: (1) collective evidence from extraction pat-
terns can be used to infer semantic category associ-
ations, and (2) learning multiple semantic categories
simultaneously can help constrain the bootstrapping
process. The accuracy achieved by Basilisk is sub-
stantially higher than that of previous techniques for
semantic lexicon induction on the MUC-4 corpus,
and empirical results show that both of Basilisk?s
ideas contribute to its performance. We also demon-
Building: theatre store cathedral temple palace
penitentiary academy houses school mansions
Event: ambush assassination uprisings sabotage
takeover incursion kidnappings clash shoot-out
Human: boys snipers detainees commandoes
extremists deserter narcoterrorists demonstrators
cronies missionaries
Location: suburb Soyapango capital Oslo regions
cities neighborhoods Quito corregimiento
Time: afternoon evening decade hour March
weeks Saturday eve anniversary Wednesday
Weapon: cannon grenade launchers rebomb
car-bomb rifle pistol machineguns rearms
Figure 9: Example Semantic Lexicon Entries
strated that learning multiple semantic categories si-
multaneously improves the meta-bootstrapping algo-
rithm, which suggests that this is a general observa-
tion which may improve other bootstrapping algo-
rithms as well.
5 Acknowledgments
This research was supported by the National Science
Foundation under award IRI-9704240.
References
Chinatsu Aone and Scott William Bennett. 1996. Ap-
plying machine learning to anaphora resolution. In
Connectionist, Statistical, and Symbolic Approaches to
Learning for Natural Language Understanding, pages
302{314. Springer-Verlag, Berlin.
E. Brill and P. Resnik. 1994. A Transformation-
based Approach to Prepositional Phrase Attachment
Disambiguation. In Proceedings of the Fifteenth In-
ternational Conference on Computational Linguistics
(COLING-94).
S. Caraballo. 1999. Automatic Acquisition of a
Hypernym-Labeled Noun Hierarchy from Text. In
Proceedings of the 37th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 120{126.
M. Collins and Y. Singer. 1999. Unsupervised Models
for Named Entity Classication. In Proceedings of the
Joint SIGDAT Conference on Empirical Me thods in
Natural Language Processing and Very Large Corpora
(EMNLP/VLC-99).
S. Cucerzan and D. Yarowsky. 1999. Language Inde-
pendent Named Entity Recognition Combining Morph
ological and Contextual Evidence. In Proceedings of
the Joint SIGDAT Conference on Empirical Me thods
in Natural Language Processing and Very Large Cor-
pora (EMNLP/VLC-99).
W. Gale, K. Church, and David Yarowsky. 1992. A
method for disambiguating word senses in a large cor-
pus. Computers and the Humanities, 26:415{439.
Niyu Ge, John Hale, and Eugene Charniak. 1998. A sta-
tistical approach to anaphora resolution. In Proceed-
ings of the Sixth Workshop on Very Large Corpora.
M. Hearst. 1992. Automatic Acquisition of Hy-
ponyms from Large Text Corpora. In Proceedings of
the Fourteenth International Conference on Computa-
tional Linguistics (COLING-92).
Lynette Hirschman, Marc Light, Eric Breck, and John D.
Burger. 1999. Deep Read: A reading comprehension
system. In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a Large Annotated Corpus of English:
The Penn Treebank. Computational Linguistics,
19(2):313{330.
George Miller. 1990. Wordnet: An on-line lexical
database. In International Journal of Lexicography.
Dan Moldovan, Sanda Harabagiu, Marius Pasca, Rada
Mihalcea, Richard Goodrum, Roxana G^irju, and
Vasile Rus. 1999. Lasso: A tool for surng the an-
swer net. In Proceedings of the Eighth Text REtrieval
Conference (TREC-8).
MUC-4 Proceedings. 1992. Proceedings of the Fourth
Message Understanding Conference (MUC-4). Mor-
gan Kaufmann, San Mateo, CA.
E. Rilo and R. Jones. 1999. Learning Dictionaries for
Information Extraction by Multi-Level Bootstrapping.
In Proceedings of the Sixteenth National Conference on
Articial Intelligence.
E. Rilo and M. Schmelzenbach. 1998. An Empirical
Approach to Conceptual Case Frame Acquisition. In
Proceedings of the Sixth Workshop on Very Large Cor-
pora, pages 49{56.
E. Rilo and J. Shepherd. 1997. A Corpus-Based Ap-
proach for Building Semantic Lexicons. In Proceed-
ings of the Second Conference on Empirical Methods
in Natural Language Processing, pages 117{124.
E. Rilo. 1996. Automatically Generating Extraction
Patterns from Untagged Text. In Proceedings of the
Thirteenth National Conference on Articial Intelli-
gence, pages 1044{1049. The AAAI Press/MIT Press.
B. Roark and E. Charniak. 1998. Noun-phrase Co-
occurrence Statistics for Semi-automatic Semantic
Lexicon Construction. In Proceedings of the 36th An-
nual Meeting of the Association for Computational
Linguistics, pages 1110{1116.
Stephen Soderland, David Fisher, Jonathan Aseltine,
and Wendy Lehnert. 1995. CRYSTAL: Inducing a
conceptual dictionary. In Proceedings of the Four-
teenth International Joint Conference on Articial In-
telligence, pages 1314{1319.
Learning Subjective Nouns using Extraction Pattern Bootstrapping?
Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT 84112
riloff@cs.utah.edu
Janyce Wiebe
Department of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260
wiebe@cs.pitt.edu
Theresa Wilson
Intelligent Systems Program
University of Pittsburgh
Pittsburgh, PA 15260
twilson@cs.pitt.edu
Abstract
We explore the idea of creating a subjectiv-
ity classifier that uses lists of subjective nouns
learned by bootstrapping algorithms. The goal
of our research is to develop a system that
can distinguish subjective sentences from ob-
jective sentences. First, we use two bootstrap-
ping algorithms that exploit extraction patterns
to learn sets of subjective nouns. Then we
train a Naive Bayes classifier using the subjec-
tive nouns, discourse features, and subjectivity
clues identified in prior research. The boot-
strapping algorithms learned over 1000 subjec-
tive nouns, and the subjectivity classifier per-
formed well, achieving 77% recall with 81%
precision.
1 Introduction
Many natural language processing applications could
benefit from being able to distinguish between factual
and subjective information. Subjective remarks come
in a variety of forms, including opinions, rants, allega-
tions, accusations, suspicions, and speculation. Ideally,
information extraction systems should be able to distin-
guish between factual information (which should be ex-
tracted) and non-factual information (which should be
discarded or labeled as uncertain). Question answering
systems should distinguish between factual and specula-
tive answers. Multi-perspective question answering aims
to present multiple answers to the user based upon specu-
lation or opinions derived from different sources. Multi-
? This work was supported in part by the National Sci-
ence Foundation under grants IIS-0208798 and IRI-9704240.
The data preparation was performed in support of the North-
east Regional Reseach Center (NRRC) which is sponsored by
the Advanced Research and Development Activity (ARDA), a
U.S. Government entity which sponsors and promotes research
of import to the Intelligence Community which includes but is
not limited to the CIA, DIA, NSA, NIMA, and NRO.
document summarization systems need to summarize dif-
ferent opinions and perspectives. Spam filtering systems
must recognize rants and emotional tirades, among other
things. In general, nearly any system that seeks to iden-
tify information could benefit from being able to separate
factual and subjective information.
Subjective language has been previously studied in
fields such as linguistics, literary theory, psychology, and
content analysis. Some manually-developed knowledge
resources exist, but there is no comprehensive dictionary
of subjective language.
Meta-Bootstrapping (Riloff and Jones, 1999) and
Basilisk (Thelen and Riloff, 2002) are bootstrapping al-
gorithms that use automatically generated extraction pat-
terns to identify words belonging to a semantic cate-
gory. We hypothesized that extraction patterns could
also identify subjective words. For example, the pat-
tern ?expressed <direct object>? often extracts subjec-
tive nouns, such as ?concern?, ?hope?, and ?support?.
Furthermore, these bootstrapping algorithms require only
a handful of seed words and unannotated texts for train-
ing; no annotated data is needed at all.
In this paper, we use the Meta-Bootstrapping and
Basilisk algorithms to learn lists of subjective nouns from
a large collection of unannotated texts. Then we train
a subjectivity classifier on a small set of annotated data,
using the subjective nouns as features along with some
other previously identified subjectivity features. Our ex-
perimental results show that the subjectivity classifier
performs well (77% recall with 81% precision) and that
the learned nouns improve upon previous state-of-the-art
subjectivity results (Wiebe et al, 1999).
2 Subjectivity Data
2.1 The Annotation Scheme
In 2002, an annotation scheme was developed
for a U.S. government-sponsored project with a
team of 10 researchers (the annotation instruc-
tions and project reports are available on the Web
at http://www.cs.pitt.edu/?wiebe/pubs/ardasummer02/).
                                                               Edmonton, May-June 2003
                                                    held at HLT-NAACL 2003 , pp. 25-32
                                            Proceeings of the Seventh CoNLL conference
The scheme was inspired by work in linguistics and
literary theory on subjectivity, which focuses on how
opinions, emotions, etc. are expressed linguistically in
context (Banfield, 1982). The scheme is more detailed
and comprehensive than previous ones. We mention only
those aspects of the annotation scheme relevant to this
paper.
The goal of the annotation scheme is to identify and
characterize expressions of private states in a sentence.
Private state is a general covering term for opinions, eval-
uations, emotions, and speculations (Quirk et al, 1985).
For example, in sentence (1) the writer is expressing a
negative evaluation.
(1) ?The time has come, gentlemen, for Sharon, the as-
sassin, to realize that injustice cannot last long.?
Sentence (2) reflects the private state of Western coun-
tries. Mugabe?s use of ?overwhelmingly? also reflects a
private state, his positive reaction to and characterization
of his victory.
(2) ?Western countries were left frustrated and impotent
after Robert Mugabe formally declared that he had over-
whelmingly won Zimbabwe?s presidential election.?
Annotators are also asked to judge the strength of each
private state. A private state can have low, medium, high
or extreme strength.
2.2 Corpus and Agreement Results
Our data consists of English-language versions of foreign
news documents from FBIS, the U.S. Foreign Broadcast
Information Service. The data is from a variety of publi-
cations and countries. The annotated corpus used to train
and test our subjectivity classifiers (the experiment cor-
pus) consists of 109 documents with a total of 2197 sen-
tences. We used a separate, annotated tuning corpus of
33 documents with a total of 698 sentences to establish
some experimental parameters.1
Each document was annotated by one or both of two
annotators, A and T. To allow us to measure interanno-
tator agreement, the annotators independently annotated
the same 12 documents with a total of 178 sentences. We
began with a strict measure of agreement at the sentence
level by first considering whether the annotator marked
any private-state expression, of any strength, anywhere
in the sentence. If so, the sentence should be subjective.
Otherwise, it is objective. Table 1 shows the contingency
table. The percentage agreement is 88%, and the ? value
is 0.71.
1The annotated data will be available to U.S. government
contractors this summer. We are working to resolve copyright
issues to make it available to the wider research community.
Tagger T
Subj Obj
Tagger A Subj nyy = 112 nyn = 16
Obj nny = 6 nnn = 44
Table 1: Agreement for sentence-level annotations
Tagger T
Subj Obj
Tagger A Subj nyy = 106 nyn = 9
Obj nny = 0 nnn = 44
Table 2: Agreement for sentence-level annotations, low-
strength cases removed
One would expect that there are clear cases of objec-
tive sentences, clear cases of subjective sentences, and
borderline sentences in between. The agreement study
supports this. In terms of our annotations, we define
a sentence as borderline if it has at least one private-
state expression identified by at least one annotator, and
all strength ratings of private-state expressions are low.
Table 2 shows the agreement results when such border-
line sentences are removed (19 sentences, or 11% of the
agreement test corpus). The percentage agreement in-
creases to 94% and the ? value increases to 0.87.
As expected, the majority of disagreement cases in-
volve low-strength subjectivity. The annotators consis-
tently agree about which are the clear cases of subjective
sentences. This leads us to define the gold-standard that
we use in our experiments. A sentence is subjective if it
contains at least one private-state expression of medium
or higher strength. The second class, which we call ob-
jective, consists of everything else. Thus, sentences with
only mild traces of subjectivity are tossed into the objec-
tive category, making the system?s goal to find the clearly
subjective sentences.
3 Using Extraction Patterns to Learn
Subjective Nouns
In the last few years, two bootstrapping algorithms have
been developed to create semantic dictionaries by ex-
ploiting extraction patterns: Meta-Bootstrapping (Riloff
and Jones, 1999) and Basilisk (Thelen and Riloff, 2002).
Extraction patterns were originally developed for infor-
mation extraction tasks (Cardie, 1997). They represent
lexico-syntactic expressions that typically rely on shal-
low parsing and syntactic role assignment. For example,
the pattern ?<subject> was hired? would apply to sen-
tences that contain the verb ?hired? in the passive voice.
The subject would be extracted as the hiree.
Meta-Bootstrapping and Basilisk were designed to
learn words that belong to a semantic category (e.g.,
?truck? is a VEHICLE and ?seashore? is a LOCATION).
Both algorithms begin with unannotated texts and seed
words that represent a semantic category. A bootstrap-
ping process looks for words that appear in the same ex-
traction patterns as the seeds and hypothesizes that those
words belong to the same semantic class. The principle
behind this approach is that words of the same semantic
class appear in similar pattern contexts. For example, the
phrases ?lived in? and ?traveled to? will co-occur with
many noun phrases that represent LOCATIONS.
In our research, we want to automatically identify
words that are subjective. Subjective terms have many
different semantic meanings, but we believe that the same
contextual principle applies to subjectivity. In this sec-
tion, we briefly overview these bootstrapping algorithms
and explain how we used them to generate lists of subjec-
tive nouns.
3.1 Meta-Bootstrapping
The Meta-Bootstrapping (?MetaBoot?) process (Riloff
and Jones, 1999) begins with a small set of seed words
that represent a targeted semantic category (e.g., 10
words that represent LOCATIONS) and an unannotated
corpus. First, MetaBoot automatically creates a set of ex-
traction patterns for the corpus by applying and instanti-
ating syntactic templates. This process literally produces
thousands of extraction patterns that, collectively, will ex-
tract every noun phrase in the corpus. Next, MetaBoot
computes a score for each pattern based upon the num-
ber of seed words among its extractions. The best pat-
tern is saved and all of its extracted noun phrases are
automatically labeled as the targeted semantic category.2
MetaBoot then re-scores the extraction patterns, using the
original seed words as well as the newly labeled words,
and the process repeats. This procedure is called mutual
bootstrapping.
A second level of bootstrapping (the ?meta-? boot-
strapping part) makes the algorithm more robust. When
the mutual bootstrapping process is finished, all nouns
that were put into the semantic dictionary are re-
evaluated. Each noun is assigned a score based on how
many different patterns extracted it. Only the five best
nouns are allowed to remain in the dictionary. The other
entries are discarded, and the mutual bootstrapping pro-
cess starts over again using the revised semantic dictio-
nary.
3.2 Basilisk
Basilisk (Thelen and Riloff, 2002) is a more recent boot-
strapping algorithm that also utilizes extraction patterns
to create a semantic dictionary. Similarly, Basilisk be-
gins with an unannotated text corpus and a small set of
2Our implementation of Meta-Bootstrapping learns individ-
ual nouns (vs. noun phrases) and discards capitalized words.
seed words for a semantic category. The bootstrapping
process involves three steps. (1) Basilisk automatically
generates a set of extraction patterns for the corpus and
scores each pattern based upon the number of seed words
among its extractions. This step is identical to the first
step of Meta-Bootstrapping. Basilisk then puts the best
patterns into a Pattern Pool. (2) All nouns3 extracted by a
pattern in the Pattern Pool are put into a Candidate Word
Pool. Basilisk scores each noun based upon the set of
patterns that extracted it and their collective association
with the seed words. (3) The top 10 nouns are labeled as
the targeted semantic class and are added to the dictio-
nary. The bootstrapping process then repeats, using the
original seeds and the newly labeled words.
The main difference between Basilisk and Meta-
Bootstrapping is that Basilisk scores each noun based
on collective information gathered from all patterns that
extracted it. In contrast, Meta-Bootstrapping identifies
a single best pattern and assumes that everything it ex-
tracted belongs to the same semantic class. The second
level of bootstrapping smoothes over some of the prob-
lems caused by this assumption. In comparative experi-
ments (Thelen and Riloff, 2002), Basilisk outperformed
Meta-Bootstrapping. But since our goal of learning sub-
jective nouns is different from the original intent of the
algorithms, we tried them both. We also suspected they
might learn different words, in which case using both al-
gorithms could be worthwhile.
3.3 Experimental Results
The Meta-Bootstrapping and Basilisk algorithms need
seed words and an unannotated text corpus as input.
Since we did not need annotated texts, we created a much
larger training corpus, the bootstrapping corpus, by gath-
ering 950 new texts from the FBIS source mentioned
in Section 2.2. To find candidate seed words, we auto-
matically identified 850 nouns that were positively corre-
lated with subjective sentences in another data set. How-
ever, it is crucial that the seed words occur frequently
in our FBIS texts or the bootstrapping process will not
get off the ground. So we searched for each of the 850
nouns in the bootstrapping corpus, sorted them by fre-
quency, and manually selected 20 high-frequency words
that we judged to be strongly subjective. Table 3 shows
the 20 seed words used for both Meta-Bootstrapping and
Basilisk.
We ran each bootstrapping algorithm for 400 itera-
tions, generating 5 words per iteration. Basilisk gener-
ated 2000 nouns and Meta-Bootstrapping generated 1996
nouns.4 Table 4 shows some examples of extraction pat-
3Technically, each head noun of an extracted noun phrase.
4Meta-Bootstrapping will sometimes produce fewer than 5
words per iteration if it has low confidence in its judgements.
cowardice embarrassment hatred outrage
crap fool hell slander
delight gloom hypocrisy sigh
disdain grievance love twit
dismay happiness nonsense virtue
Table 3: Subjective Seed Words
Extraction Patterns Examples of Extracted Nouns
expressed <dobj> condolences, hope, grief,
views, worries, recognition
indicative of <np> compromise, desire, thinking
inject <dobj> vitality, hatred
reaffirmed <dobj> resolve, position, commitment
voiced <dobj> outrage, support, skepticism,
disagreement, opposition,
concerns, gratitude, indignation
show of <np> support, strength, goodwill,
solidarity, feeling
<subject> was shared anxiety, view, niceties, feeling
Table 4: Extraction Pattern Examples
terns that were discovered to be associated with subjec-
tive nouns.
Meta-Bootstrapping and Basilisk are semi-automatic
lexicon generation tools because, although the bootstrap-
ping process is 100% automatic, the resulting lexicons
need to be reviewed by a human.5 So we manually re-
viewed the 3996 words proposed by the algorithms. This
process is very fast; it takes only a few seconds to classify
each word. The entire review process took approximately
3-4 hours. One author did this labeling; this person did
not look at or run tests on the experiment corpus.
Strong Subjective Weak Subjective
tyranny scum aberration plague
smokescreen bully allusion risk
apologist devil apprehensions drama
barbarian liar beneficiary trick
belligerence pariah resistant promise
condemnation venom credence intrigue
sanctimonious diatribe distortion unity
exaggeration mockery eyebrows failures
repudiation anguish inclination tolerance
insinuation fallacies liability persistent
antagonism evil assault trust
atrocities genius benefit success
denunciation goodwill blood spirit
exploitation injustice controversy slump
humiliation innuendo likelihood sincerity
ill-treatment revenge peaceful eternity
sympathy rogue pressure rejection
Table 5: Examples of Learned Subjective Nouns
5This is because NLP systems expect dictionaries to have
high integrity. Even if the algorithms could achieve 90% ac-
curacy, a dictionary in which 1 of every 10 words is defined
incorrectly would probably not be desirable.
B M B ? M B ? M
StrongSubj 372 192 110 454
WeakSubj 453 330 185 598
Total 825 522 295 1052
Table 6: Subjective Word Lexicons after Manual Review
(B=Basilisk, M=MetaBootstrapping)
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 200 400 600 800 1000 1200 1400 1600 1800 2000
%
 o
f W
or
ds
 S
ub
jec
tiv
e
Number of Words Generated
?Basilisk?
?MetaBoot?
Figure 1: Accuracy during Bootstrapping
We classified the words as StrongSubjective, WeakSub-
jective, or Objective. Objective terms are not subjective at
all (e.g., ?chair? or ?city?). StrongSubjective terms have
strong, unambiguously subjective connotations, such as
?bully? or ?barbarian?. WeakSubjective was used for
three situations: (1) words that have weak subjective con-
notations, such as ?aberration? which implies something
out of the ordinary but does not evoke a strong sense of
judgement, (2) words that have multiple senses or uses,
where one is subjective but the other is not. For example,
the word ?plague? can refer to a disease (objective) or an
onslaught of something negative (subjective), (3) words
that are objective by themselves but appear in idiomatic
expressions that are subjective. For example, the word
?eyebrows? was labeled WeakSubjective because the ex-
pression ?raised eyebrows? probably occurs more often
in our corpus than literal references to ?eyebrows?. Ta-
ble 5 shows examples of learned words that were classi-
fied as StrongSubjective or WeakSubjective.
Once the words had been manually classified, we could
go back and measure the effectiveness of the algorithms.
The graph in Figure 1 tracks their accuracy as the boot-
strapping progressed. The X-axis shows the number of
words generated so far. The Y-axis shows the percent-
age of those words that were manually classified as sub-
jective. As is typical of bootstrapping algorithms, ac-
curacy was high during the initial iterations but tapered
off as the bootstrapping continued. After 20 words,
both algorithms were 95% accurate. After 100 words
Basilisk was 75% accurate and MetaBoot was 81% accu-
rate. After 1000 words, accuracy dropped to about 28%
for MetaBoot, but Basilisk was still performing reason-
ably well at 53%. Although 53% accuracy is not high for
a fully automatic process, Basilisk depends on a human
to review the words so 53% accuracy means that the hu-
man is accepting every other word, on average. Thus, the
reviewer?s time was still being spent productively even
after 1000 words had been hypothesized.
Table 6 shows the size of the final lexicons created
by the bootstrapping algorithms. The first two columns
show the number of subjective terms learned by Basilisk
and Meta-Bootstrapping. Basilisk was more prolific, gen-
erating 825 subjective terms compared to 522 for Meta-
Bootstrapping. The third column shows the intersection
between their word lists. There was substantial overlap,
but both algorithms produced many words that the other
did not. The last column shows the results of merging
their lists. In total, the bootstrapping algorithms produced
1052 subjective nouns.
4 Creating Subjectivity Classifiers
To evaluate the subjective nouns, we trained a Naive
Bayes classifier using the nouns as features. We also in-
corporated previously established subjectivity clues, and
added some new discourse features. In this section, we
describe all the feature sets and present performance re-
sults for subjectivity classifiers trained on different com-
binations of these features. The threshold values and fea-
ture representations used in this section are the ones that
produced the best results on our separate tuning corpus.
4.1 Subjective Noun Features
We defined four features to represent the sets of subjec-
tive nouns produced by the bootstrapping algorithms.
BA-Strong: the set of StrongSubjective nouns generated
by Basilisk
BA-Weak: the set of WeakSubjective nouns generated
by Basilisk
MB-Strong: the set of StrongSubjective nouns generated
by Meta-Bootstrapping
MB-Weak: the set of WeakSubjective nouns generated
by Meta-Bootstrapping
For each set, we created a three-valued feature based on
the presence of 0, 1, or ? 2 words from that set. We used
the nouns as feature sets, rather than define a separate
feature for each word, so the classifier could generalize
over the set to minimize sparse data problems. We will
refer to these as the SubjNoun features.
4.2 Previously Established Features
Wiebe, Bruce, & O?Hara (1999) developed a machine
learning system to classify subjective sentences. We ex-
perimented with the features that they used, both to com-
pare their results to ours and to see if we could benefit
from their features. We will refer to these as the WBO
features.
WBO includes a set of stems positively correlated with
the subjective training examples (subjStems) and a set
of stems positively correlated with the objective training
examples (objStems). We defined a three-valued feature
for the presence of 0, 1, or ? 2 members of subjStems
in a sentence, and likewise for objStems. For our exper-
iments, subjStems includes stems that appear ? 7 times
in the training set, and for which the precision is 1.25
times the baseline word precision for that training set.
objStems contains the stems that appear ? 7 times and
for which at least 50% of their occurrences in the training
set are in objective sentences. WBO also includes a bi-
nary feature for each of the following: the presence in the
sentence of a pronoun, an adjective, a cardinal number, a
modal other than will, and an adverb other than not.
We also added manually-developed features found by
other researchers. We created 14 feature sets represent-
ing some classes from (Levin, 1993; Ballmer and Bren-
nenstuhl, 1981), some Framenet lemmas with frame ele-
ment experiencer (Baker et al, 1998), adjectives manu-
ally annotated for polarity (Hatzivassiloglou and McKe-
own, 1997), and some subjectivity clues listed in (Wiebe,
1990). We represented each set as a three-valued feature
based on the presence of 0, 1, or ? 2 members of the set.
We will refer to these as the manual features.
4.3 Discourse Features
We created discourse features to capture the density of
clues in the text surrounding a sentence. First, we com-
puted the average number of subjective clues and objec-
tive clues per sentence, normalized by sentence length.
The subjective clues, subjClues, are all sets for which
3-valued features were defined above (except objStems).
The objective clues consist only of objStems. For sen-
tence S, let ClueRatesubj(S) = |subjClues in S||S| and
ClueRateobj(S) = |objStems in S||S| . Then we define
AvgClueRatesubj to be the average of ClueRate(S)
over all sentences S and similarly for AvgClueRateobj.
Next, we characterize the number of subjective and
objective clues in the previous and next sentences as:
higher-than-expected (high), lower-than-expected (low),
or expected (medium). The value for ClueRatesubj(S)
is high if ClueRatesubj(S) ? AvgClueRatesubj ? 1.3;
low if ClueRatesubj(S) ? AvgClueRatesubj/1.3; oth-
erwise it is medium. The values for ClueRateobj(S) are
defined similarly.
Using these definitions we created four features:
ClueRatesubj for the previous and following sen-
tences, and ClueRateobj for the previous and follow-
ing sentences. We also defined a feature for sentence
length. Let AvgSentLen be the average sentence length.
SentLen(S) is high if length(S) ? AvgSentLen?1.3;
low if length(S) ? AvgSentLen/1.3; and medium oth-
erwise.
4.4 Classification Results
We conducted experiments to evaluate the performance
of the feature sets, both individually and in various com-
binations. Unless otherwise noted, all experiments in-
volved training a Naive Bayes classifier using a particu-
lar set of features. We evaluated each classifier using 25-
fold cross validation on the experiment corpus and used
paired t-tests to measure significance at the 95% confi-
dence level. As our evaluation metrics, we computed ac-
curacy (Acc) as the percentage of the system?s classifica-
tions that match the gold-standard, and precision (Prec)
and recall (Rec) with respect to subjective sentences.
Acc Prec Rec
(1) Bag-Of-Words 73.3 81.7 70.9
(2) WBO 72.1 76.0 77.4
(3) Most-Frequent 59.0 59.0 100.0
Table 7: Baselines for Comparison
Table 7 shows three baseline experiments. Row (3)
represents the common baseline of assigning every sen-
tence to the most frequent class. The Most-Frequent
baseline achieves 59% accuracy because 59% of the sen-
tences in the gold-standard are subjective. Row (2) is
a Naive Bayes classifier that uses the WBO features,
which performed well in prior research on sentence-level
subjectivity classification (Wiebe et al, 1999). Row (1)
shows a Naive Bayes classifier that uses unigram bag-of-
words features, with one binary feature for the absence
or presence in the sentence of each word that appeared
during training. Pang et al (2002) reported that a similar
experiment produced their best results on a related clas-
sification task. The difference in accuracy between Rows
(1) and (2) is not statistically significant (Bag-of-Word?s
higher precision is balanced by WBO?s higher recall).
Next, we trained a Naive Bayes classifier using only
the SubjNoun features. This classifier achieved good
precision (77%) but only moderate recall (64%). Upon
further inspection, we discovered that the subjective
nouns are good subjectivity indicators when they appear,
but not every subjective sentence contains one of them.
And, relatively few sentences contain more than one,
making it difficult to recognize contextual effects (i.e.,
multiple clues in a region). We concluded that the ap-
propriate way to benefit from the subjective nouns is to
use them in tandem with other subjectivity clues.
Acc Prec Rec
(1) 76.1 81.3 77.4 WBO+SubjNoun+
manual+discourse
(2) 74.3 78.6 77.8 WBO+SubjNoun
(3) 72.1 76.0 77.4 WBO
Table 8: Results with New Features
Table 8 shows the results of Naive Bayes classifiers
trained with different combinations of features. The ac-
curacy differences between all pairs of experiments in
Table 8 are statistically significant. Row (3) uses only
the WBO features (also shown in Table 7 as a baseline).
Row (2) uses the WBO features as well as the SubjNoun
features. There is a synergy between these feature sets:
using both types of features achieves better performance
than either one alone. The difference is mainly precision,
presumably because the classifier found more and better
combinations of features. In Row (1), we also added the
manual and discourse features. The discourse features
explicitly identify contexts in which multiple clues are
found. This classifier produced even better performance,
achieving 81.3% precision with 77.4% recall. The 76.1%
accuracy result is significantly higher than the accuracy
results for all of the other classifiers (in both Table 8 and
Table 7).
Finally, higher precision classification can be obtained
by simply classifying a sentence as subjective if it con-
tains any of the StrongSubjective nouns. On our data, this
method produces 87% precision with 26% recall. This
approach could support applications for which precision
is paramount.
5 Related Work
Several types of research have involved document-level
subjectivity classification. Some work identifies inflam-
matory texts (e.g., (Spertus, 1997)) or classifies reviews
as positive or negative ((Turney, 2002; Pang et al, 2002)).
Tong?s system (Tong, 2001) generates sentiment time-
lines, tracking online discussions and creating graphs of
positive and negative opinion messages over time. Re-
search in genre classification may include recognition of
subjective genres such as editorials (e.g., (Karlgren and
Cutting, 1994; Kessler et al, 1997; Wiebe et al, 2001)).
In contrast, our work classifies individual sentences, as
does the research in (Wiebe et al, 1999). Sentence-level
subjectivity classification is useful because most docu-
ments contain a mix of subjective and objective sen-
tences. For example, newspaper articles are typically
thought to be relatively objective, but (Wiebe et al, 2001)
reported that, in their corpus, 44% of sentences (in arti-
cles that are not editorials or reviews) were subjective.
Some previous work has focused explicitly on learn-
ing subjective words and phrases. (Hatzivassiloglou and
McKeown, 1997) describes a method for identifying the
semantic orientation of words, for example that beauti-
ful expresses positive sentiments. Researchers have fo-
cused on learning adjectives or adjectival phrases (Tur-
ney, 2002; Hatzivassiloglou and McKeown, 1997; Wiebe,
2000) and verbs (Wiebe et al, 2001), but no previous
work has focused on learning nouns. A unique aspect
of our work is the use of bootstrapping methods that ex-
ploit extraction patterns. (Turney, 2002) used patterns
representing part-of-speech sequences, (Hatzivassiloglou
and McKeown, 1997) recognized adjectival phrases, and
(Wiebe et al, 2001) learned N-grams. The extraction
patterns used in our research are linguistically richer pat-
terns, requiring shallow parsing and syntactic role assign-
ment.
In recent years several techniques have been developed
for semantic lexicon creation (e.g., (Hearst, 1992; Riloff
and Shepherd, 1997; Roark and Charniak, 1998; Cara-
ballo, 1999)). Semantic word learning is different from
subjective word learning, but we have shown that Meta-
Bootstrapping and Basilisk could be successfully applied
to subjectivity learning. Perhaps some of these other
methods could also be used to learn subjective words.
6 Conclusions
This research produced interesting insights as well as per-
formance results. First, we demonstrated that weakly
supervised bootstrapping techniques can learn subjec-
tive terms from unannotated texts. Subjective features
learned from unannotated documents can augment or en-
hance features learned from annotated training data us-
ing more traditional supervised learning techniques. Sec-
ond, Basilisk and Meta-Bootstrapping proved to be use-
ful for a different task than they were originally intended.
By seeding the algorithms with subjective words, the ex-
traction patterns identified expressions that are associated
with subjective nouns. This suggests that the bootstrap-
ping algorithms should be able to learn not only general
semantic categories, but any category for which words
appear in similar linguistic phrases. Third, our best sub-
jectivity classifier used a wide variety of features. Sub-
jectivity is a complex linguistic phenomenon and our evi-
dence suggests that reliable subjectivity classification re-
quires a broad array of features.
References
C. Baker, C. Fillmore, and J. Lowe. 1998. The berkeley
framenet project. In Proceedings of the COLING-ACL.
T. Ballmer and W. Brennenstuhl. 1981. Speech Act Clas-
sification: A Study in the Lexical Analysis of English
Speech Activity Verbs. Springer-Verlag.
A. Banfield. 1982. Unspeakable Sentences. Routledge
and Kegan Paul, Boston.
S. Caraballo. 1999. Automatic Acquisition of a
Hypernym-Labeled Noun Hierarchy from Text. In
Proceedings of the 37th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 120?126.
C. Cardie. 1997. Empirical Methods in Information Ex-
traction. AI Magazine, 18(4):65?79.
V. Hatzivassiloglou and K. McKeown. 1997. Predicting
the semantic orientation of adjectives. In ACL-EACL
1997.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proc. of the 14th Interna-
tional Conference on Computational Linguistics.
J. Karlgren and D. Cutting. 1994. Recognizing text gen-
res with simple metrics using discriminant analysis. In
COLING-94.
B. Kessler, G. Nunberg, and H. Schutze. 1997. Auto-
matic detection of text genre. In Proc. ACL-EACL-97.
Beth Levin. 1993. English Verb Classes and Alter-
nations: A Preliminary Investigation. University of
Chicago Press, Chicago.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? sentiment classification using machine learning
techniques. In Proceedings of the 2002 Conference on
Empirical Methods in Natural Language Processing.
R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik. 1985.
A Comprehensive Grammar of the English Language.
Longman, New York.
E. Riloff and R. Jones. 1999. Learning Dictionaries for
Information Extraction by Multi-Level Bootstrapping.
In Proceedings of the 16th National Conference on Ar-
tificial Intelligence.
E. Riloff and J. Shepherd. 1997. A Corpus-Based Ap-
proach for Building Semantic Lexicons. In Proceed-
ings of the Second Conference on Empirical Methods
in Natural Language Processing, pages 117?124.
B. Roark and E. Charniak. 1998. Noun-phrase
Co-occurrence Statistics for Semi-automatic Seman-
tic Lexicon Construction. In Proceedings of the 36th
Annual Meeting of the Association for Computational
Linguistics, pages 1110?1116.
E. Spertus. 1997. Smokey: Automatic recognition of
hostile messages. In Proc. IAAI.
M. Thelen and E. Riloff. 2002. A Bootstrapping Method
for Learning Semantic Lexicons Using Extraction Pa
ttern Contexts. In Proceedings of the 2002 Conference
on Empirical Methods in Natural Language Process-
ing.
R. Tong. 2001. An operational system for detecting and
tracking opinions in on-line discussion. In SIGIR 2001
Workshop on Operational Text Classification.
P. Turney. 2002. Thumbs Up or Thumbs Down? Seman-
tic Orientation Applied to Unsupervised Classification
of Reviews. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics.
J. Wiebe, R. Bruce, and T. O?Hara. 1999. Development
and use of a gold standard data set for subjectivity clas-
sifications. In Proc. 37th Annual Meeting of the Assoc.
for Computational Linguistics (ACL-99).
J. Wiebe, T. Wilson, and M. Bell. 2001. Identifying col-
locations for recognizing opinions. In Proc. ACL-01
Workshop on Collocation: Computational Extraction,
Analysis, and Exploitation, July.
J. Wiebe. 1990. Recognizing Subjective Sentences: A
Computational Investigation of Narrative Text. Ph.D.
thesis, State University of New York at Buffalo.
J. Wiebe. 2000. Learning subjective adjectives from cor-
pora. In 17th National Conference on Artificial Intelli-
gence.
Learning Extraction Patterns for Subjective Expressions?
Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT 84112
riloff@cs.utah.edu
Janyce Wiebe
Department of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260
wiebe@cs.pitt.edu
Abstract
This paper presents a bootstrapping process
that learns linguistically rich extraction pat-
terns for subjective (opinionated) expressions.
High-precision classifiers label unannotated
data to automatically create a large training set,
which is then given to an extraction pattern
learning algorithm. The learned patterns are
then used to identify more subjective sentences.
The bootstrapping process learns many subjec-
tive patterns and increases recall while main-
taining high precision.
1 Introduction
Many natural language processing applications could
benefit from being able to distinguish between factual
and subjective information. Subjective remarks come
in a variety of forms, including opinions, rants, allega-
tions, accusations, suspicions, and speculations. Ideally,
information extraction systems should be able to distin-
guish between factual information (which should be ex-
tracted) and non-factual information (which should be
discarded or labeled as uncertain). Question answering
systems should distinguish between factual and specula-
tive answers. Multi-perspective question answering aims
to present multiple answers to the user based upon specu-
lation or opinions derived from different sources. Multi-
document summarization systems need to summarize dif-
ferent opinions and perspectives. Spam filtering systems
?This work was supported by the National Science Founda-
tion under grants IIS-0208798, IIS-0208985, and IRI-9704240.
The data preparation was performed in support of the North-
east Regional Research Center (NRRC) which is sponsored by
the Advanced Research and Development Activity (ARDA), a
U.S. Government entity which sponsors and promotes research
of import to the Intelligence Community which includes but is
not limited to the CIA, DIA, NSA, NIMA, and NRO.
must recognize rants and emotional tirades, among other
things. In general, nearly any system that seeks to iden-
tify information could benefit from being able to separate
factual and subjective information.
Some existing resources contain lists of subjective
words (e.g., Levin?s desire verbs (1993)), and some em-
pirical methods in NLP have automatically identified ad-
jectives, verbs, and N-grams that are statistically associ-
ated with subjective language (e.g., (Turney, 2002; Hatzi-
vassiloglou and McKeown, 1997; Wiebe, 2000; Wiebe
et al, 2001)). However, subjective language can be ex-
hibited by a staggering variety of words and phrases. In
addition, many subjective terms occur infrequently, such
as strongly subjective adjectives (e.g., preposterous, un-
seemly) and metaphorical or idiomatic phrases (e.g., dealt
a blow, swept off one?s feet). Consequently, we believe
that subjectivity learning systems must be trained on ex-
tremely large text collections before they will acquire a
subjective vocabulary that is truly broad and comprehen-
sive in scope.
To address this issue, we have been exploring the use
of bootstrapping methods to allow subjectivity classifiers
to learn from a collection of unannotated texts. Our re-
search uses high-precision subjectivity classifiers to au-
tomatically identify subjective and objective sentences in
unannotated texts. This process allows us to generate a
large set of labeled sentences automatically. The sec-
ond emphasis of our research is using extraction patterns
to represent subjective expressions. These patterns are
linguistically richer and more flexible than single words
or N-grams. Using the (automatically) labeled sentences
as training data, we apply an extraction pattern learning
algorithm to automatically generate patterns represent-
ing subjective expressions. The learned patterns can be
used to automatically identify more subjective sentences,
which grows the training set, and the entire process can
then be bootstrapped. Our experimental results show that
this bootstrapping process increases the recall of the high-
precision subjective sentence classifier with little loss in
precision. We also find that the learned extraction pat-
terns capture subtle connotations that are more expressive
than the individual words by themselves.
This paper is organized as follows. Section 2 discusses
previous work on subjectivity analysis and extraction pat-
tern learning. Section 3 overviews our general approach,
describes the high-precision subjectivity classifiers, and
explains the algorithm for learning extraction patterns as-
sociated with subjectivity. Section 4 describes the data
that we use, presents our experimental results, and shows
examples of patterns that are learned. Finally, Section 5
summarizes our findings and conclusions.
2 Background
2.1 Subjectivity Analysis
Much previous work on subjectivity recognition has fo-
cused on document-level classification. For example,
(Spertus, 1997) developed a system to identify inflamma-
tory texts and (Turney, 2002; Pang et al, 2002) developed
methods for classifying reviews as positive or negative.
Some research in genre classification has included the
recognition of subjective genres such as editorials (e.g.,
(Karlgren and Cutting, 1994; Kessler et al, 1997; Wiebe
et al, 2001)).
In contrast, the goal of our work is to classify individ-
ual sentences as subjective or objective. Document-level
classification can distinguish between ?subjective texts?,
such as editorials and reviews, and ?objective texts,? such
as newspaper articles. But in reality, most documents
contain a mix of both subjective and objective sentences.
Subjective texts often include some factual information.
For example, editorial articles frequently contain factual
information to back up the arguments being made, and
movie reviews often mention the actors and plot of a
movie as well as the theatres where it?s currently playing.
Even if one is willing to discard subjective texts in their
entirety, the objective texts usually contain a great deal of
subjective information in addition to facts. For example,
newspaper articles are generally considered to be rela-
tively objective documents, but in a recent study (Wiebe
et al, 2001) 44% of sentences in a news collection were
found to be subjective (after editorial and review articles
were removed).
One of the main obstacles to producing a sentence-
level subjectivity classifier is a lack of training data. To
train a document-level classifier, one can easily find col-
lections of subjective texts, such as editorials and reviews.
For example, (Pang et al, 2002) collected reviews from
a movie database and rated them as positive, negative, or
neutral based on the rating (e.g., number of stars) given
by the reviewer. It is much harder to obtain collections of
individual sentences that can be easily identified as sub-
jective or objective. Previous work on sentence-level sub-
jectivity classification (Wiebe et al, 1999) used training
corpora that had been manually annotated for subjectiv-
ity. Manually producing annotations is time consuming,
so the amount of available annotated sentence data is rel-
atively small.
The goal of our research is to use high-precision sub-
jectivity classifiers to automatically identify subjective
and objective sentences in unannotated text corpora. The
high-precision classifiers label a sentence as subjective or
objective when they are confident about the classification,
and they leave a sentence unlabeled otherwise. Unanno-
tated texts are easy to come by, so even if the classifiers
can label only 30% of the sentences as subjective or ob-
jective, they will still produce a large collection of labeled
sentences. Most importantly, the high-precision classi-
fiers can generate a much larger set of labeled sentences
than are currently available in manually created data sets.
2.2 Extraction Patterns
Information extraction (IE) systems typically use lexico-
syntactic patterns to identify relevant information. The
specific representation of these patterns varies across sys-
tems, but most patterns represent role relationships sur-
rounding noun and verb phrases. For example, an IE
system designed to extract information about hijackings
might use the pattern hijacking of <x>, which looks for
the noun hijacking and extracts the object of the prepo-
sition of as the hijacked vehicle. The pattern <x> was
hijacked would extract the hijacked vehicle when it finds
the verb hijacked in the passive voice, and the pattern
<x> hijacked would extract the hijacker when it finds
the verb hijacked in the active voice.
One of our hypotheses was that extraction patterns
would be able to represent subjective expressions that
have noncompositional meanings. For example, consider
the common expression drives (someone) up the wall,
which expresses the feeling of being annoyed with some-
thing. The meaning of this expression is quite different
from the meanings of its individual words (drives, up,
wall). Furthermore, this expression is not a fixed word
sequence that could easily be captured by N-grams. It is
a relatively flexible construction that may be more gener-
ally represented as <x> drives <y> up the wall, where x
and y may be arbitrary noun phrases. This pattern would
match many different sentences, such as ?George drives
me up the wall,? ?She drives the mayor up the wall,?
or ?The nosy old man drives his quiet neighbors up the
wall.?
We also wondered whether the extraction pattern rep-
resentation might reveal slight variations of the same verb
or noun phrase that have different connotations. For ex-
ample, you can say that a comedian bombed last night,
which is a subjective statement, but you can?t express
this sentiment with the passive voice of bombed. In Sec-
tion 3.2, we will show examples of extraction patterns
representing subjective expressions which do in fact ex-
hibit both of these phenomena.
A variety of algorithms have been developed to au-
tomatically learn extraction patterns. Most of these
algorithms require special training resources, such as
texts annotated with domain-specific tags (e.g., Au-
toSlog (Riloff, 1993), CRYSTAL (Soderland et al,
1995), RAPIER (Califf, 1998), SRV (Freitag, 1998),
WHISK (Soderland, 1999)) or manually defined key-
words, frames, or object recognizers (e.g., PALKA (Kim
and Moldovan, 1993) and LIEP (Huffman, 1996)).
AutoSlog-TS (Riloff, 1996) takes a different approach,
requiring only a corpus of unannotated texts that have
been separated into those that are related to the target do-
main (the ?relevant? texts) and those that are not (the ?ir-
relevant? texts). Most recently, two bootstrapping algo-
rithms have been used to learn extraction patterns. Meta-
bootstrapping (Riloff and Jones, 1999) learns both extrac-
tion patterns and a semantic lexicon using unannotated
texts and seed words as input. ExDisco (Yangarber et al,
2000) uses a bootstrapping mechanism to find new ex-
traction patterns using unannotated texts and some seed
patterns as the initial input.
For our research, we adopted a learning process very
similar to that used by AutoSlog-TS, which requires only
relevant texts and irrelevant texts as its input. We describe
this learning process in more detail in the next section.
3 Learning and Bootstrapping Extraction
Patterns for Subjectivity
We have developed a bootstrapping process for subjec-
tivity classification that explores three ideas: (1) high-
precision classifiers can be used to automatically iden-
tify subjective and objective sentences from unannotated
texts, (2) this data can be used as a training set to auto-
matically learn extraction patterns associated with sub-
jectivity, and (3) the learned patterns can be used to grow
the training set, allowing this entire process to be boot-
strapped.
Figure 1 shows the components and layout of the boot-
strapping process. The process begins with a large collec-
tion of unannotated text and two high precision subjec-
tivity classifiers. One classifier searches the unannotated
corpus for sentences that can be labeled as subjective
with high confidence, and the other classifier searches
for sentences that can be labeled as objective with high
confidence. All other sentences in the corpus are left
unlabeled. The labeled sentences are then fed to an ex-
traction pattern learner, which produces a set of extrac-
tion patterns that are statistically correlated with the sub-
jective sentences (we will call these the subjective pat-
terns). These patterns are then used to identify more sen-
tences within the unannotated texts that can be classified
as subjective. The extraction pattern learner can then re-
train using the larger training set and the process repeats.
The subjective patterns can also be added to the high-
precision subjective sentence classifier as new features to
improve its performance. The dashed lines in Figure 1
represent the parts of the process that are bootstrapped.
In this section, we will describe the high-precision sen-
tence classifiers, the extraction pattern learning process,
and the details of the bootstrapping process.
3.1 High-Precision Subjectivity Classifiers
The high-precision classifiers (HP-Subj and HP-Obj) use
lists of lexical items that have been shown in previous
work to be good subjectivity clues. Most of the items are
single words, some are N-grams, but none involve syntac-
tic generalizations as in the extraction patterns. Any data
used to develop this vocabulary does not overlap with the
test sets or the unannotated data used in this paper.
Many of the subjective clues are from manually de-
veloped resources, including entries from (Levin, 1993;
Ballmer and Brennenstuhl, 1981), Framenet lemmas with
frame element experiencer (Baker et al, 1998), adjec-
tives manually annotated for polarity (Hatzivassiloglou
and McKeown, 1997), and subjectivity clues listed in
(Wiebe, 1990). Others were derived from corpora, in-
cluding subjective nouns learned from unannotated data
using bootstrapping (Riloff et al, 2003).
The subjectivity clues are divided into those that are
strongly subjective and those that are weakly subjective,
using a combination of manual review and empirical re-
sults on a small training set of manually annotated data.
As the terms are used here, a strongly subjective clue is
one that is seldom used without a subjective meaning,
whereas a weakly subjective clue is one that commonly
has both subjective and objective uses.
The high-precision subjective classifier classifies a sen-
tence as subjective if it contains two or more of the
strongly subjective clues. On a manually annotated test
set, this classifier achieves 91.5% precision and 31.9%
recall (that is, 91.5% of the sentences that it selected are
subjective, and it found 31.9% of the subjective sentences
in the test set). This test set consists of 2197 sentences,
59% of which are subjective.
The high-precision objective classifier takes a different
approach. Rather than looking for the presence of lexical
items, it looks for their absence. It classifies a sentence as
objective if there are no strongly subjective clues and at
most one weakly subjective clue in the current, previous,
and next sentence combined. Why doesn?t the objective
classifier mirror the subjective classifier, and consult its
own list of strongly objective clues? There are certainly
lexical items that are statistically correlated with the ob-
Known Subjective
Vocabulary
High?Precision Objective
Sentence Classifier (HP?Obj)
High?Precision Subjective
Sentence Classifier (HP?Subj)
Unannotated Text Collection
unlabeled sentences
unlabeled sentences
unlabeled sentences
Pattern?based Subjective
Sentence Classifier
Extraction Pattern
Learner
subjective
sentences
subjective sentences
objective sentences
subjective patterns
subjective patterns
Figure 1: Bootstrapping Process
jective class (examples are cardinal numbers (Wiebe et
al., 1999), and words such as per, case, market, and to-
tal), but the presence of such clues does not readily lead
to high precision objective classification. Add sarcasm
or a negative evaluation to a sentence about a dry topic
such as stock prices, and the sentence becomes subjec-
tive. Conversely, add objective topics to a sentence con-
taining two strongly subjective words such as odious and
scumbag, and the sentence remains subjective.
The performance of the high-precision objective classi-
fier is a bit lower than the subjective classifier: 82.6% pre-
cision and 16.4% recall on the test set mentioned above
(that is, 82.6% of the sentences selected by the objective
classifier are objective, and the objective classifier found
16.4% of the objective sentences in the test set). Al-
though there is room for improvement, the performance
proved to be good enough for our purposes.
3.2 Learning Subjective Extraction Patterns
To automatically learn extraction patterns that are associ-
ated with subjectivity, we use a learning algorithm similar
to AutoSlog-TS (Riloff, 1996). For training, AutoSlog-
TS uses a text corpus consisting of two distinct sets of
texts: ?relevant? texts (in our case, subjective sentences)
and ?irrelevant? texts (in our case, objective sentences).
A set of syntactic templates represents the space of pos-
sible extraction patterns.
The learning process has two steps. First, the syntac-
tic templates are applied to the training corpus in an ex-
haustive fashion, so that extraction patterns are generated
for (literally) every possible instantiation of the templates
that appears in the corpus. The left column of Figure 2
shows the syntactic templates used by AutoSlog-TS. The
right column shows a specific extraction pattern that was
learned during our subjectivity experiments as an instan-
tiation of the syntactic form on the left. For example, the
pattern <subj> was satisfied1 will match any sentence
where the verb satisfied appears in the passive voice. The
pattern <subj> dealt blow represents a more complex ex-
pression that will match any sentence that contains a verb
phrase with head=dealt followed by a direct object with
head=blow. This would match sentences such as ?The
experience dealt a stiff blow to his pride.? It is important
to recognize that these patterns look for specific syntactic
constructions produced by a (shallow) parser, rather than
exact word sequences.
SYNTACTIC FORM EXAMPLE PATTERN
<subj> passive-verb <subj> was satisfied
<subj> active-verb <subj> complained
<subj> active-verb dobj <subj> dealt blow
<subj> verb infinitive <subj> appear to be
<subj> aux noun <subj> has position
active-verb <dobj> endorsed <dobj>
infinitive <dobj> to condemn <dobj>
verb infinitive <dobj> get to know <dobj>
noun aux <dobj> fact is <dobj>
noun prep <np> opinion on <np>
active-verb prep <np> agrees with <np>
passive-verb prep <np> was worried about <np>
infinitive prep <np> to resort to <np>
Figure 2: Syntactic Templates and Examples of Patterns
that were Learned
1This is a shorthand notation for the internal representation.
PATTERN FREQ %SUBJ
<subj> was asked 11 100%
<subj> asked 128 63%
<subj> is talk 5 100%
talk of <np> 10 90%
<subj> will talk 28 71%
<subj> put an end 10 90%
<subj> put 187 67%
<subj> is going to be 11 82%
<subj> is going 182 67%
was expected from <np> 5 100%
<subj> was expected 45 42%
<subj> is fact 38 100%
fact is <dobj> 12 100%
Figure 3: Patterns with Interesting Behavior
The second step of AutoSlog-TS?s learning process ap-
plies all of the learned extraction patterns to the train-
ing corpus and gathers statistics for how often each
pattern occurs in subjective versus objective sentences.
AutoSlog-TS then ranks the extraction patterns using a
metric called RlogF (Riloff, 1996) and asks a human to
review the ranked list and make the final decision about
which patterns to keep.
In contrast, for this work we wanted a fully automatic
process that does not depend on a human reviewer, and
we were most interested in finding patterns that can iden-
tify subjective expressions with high precision. So we
ranked the extraction patterns using a conditional proba-
bility measure: the probability that a sentence is subjec-
tive given that a specific extraction pattern appears in it.
The exact formula is:
Pr(subjective | patterni) = subjfreq(patterni)freq(patterni)
where subjfreq(patterni) is the frequency of patterni
in subjective training sentences, and freq(patterni) is
the frequency of patterni in all training sentences. (This
may also be viewed as the precision of the pattern on the
training data.) Finally, we use two thresholds to select ex-
traction patterns that are strongly associated with subjec-
tivity in the training data. We choose extraction patterns
for which freq(patterni) ? ?1 and Pr(subjective |
patterni) ? ?2.
Figure 3 shows some patterns learned by our system,
the frequency with which they occur in the training data
(FREQ) and the percentage of times they occur in sub-
jective sentences (%SUBJ). For example, the first two
rows show the behavior of two similar expressions us-
ing the verb asked. 100% of the sentences that contain
asked in the passive voice are subjective, but only 63%
of the sentences that contain asked in the active voice are
subjective. A human would probably not expect the ac-
tive and passive voices to behave so differently. To un-
derstand why this is so, we looked in the training data
and found that the passive voice is often used to query
someone about a specific opinion. For example, here is
one such sentence from our training set: ?Ernest Bai Ko-
roma of RITCORP was asked to address his supporters on
his views relating to ?full blooded Temne to head APC?.?
In contrast, many of the sentences containing asked in
the active voice are more general in nature, such as ?The
mayor asked a newly formed JR about his petition.?
Figure 3 also shows that expressions using talk as a
noun (e.g., ?Fred is the talk of the town?) are highly cor-
related with subjective sentences, while talk as a verb
(e.g., ?The mayor will talk about...?) are found in a mix
of subjective and objective sentences. Not surprisingly,
longer expressions tend to be more idiomatic (and sub-
jective) than shorter expressions (e.g., put an end (to) vs.
put; is going to be vs. is going; was expected from vs. was
expected). Finally, the last two rows of Figure 3 show that
expressions involving the noun fact are highly correlated
with subjective expressions! These patterns match sen-
tences such as The fact is... and ... is a fact, which appar-
ently are often used in subjective contexts. This example
illustrates that the corpus-based learning method can find
phrases that might not seem subjective to a person intu-
itively, but that are reliable indicators of subjectivity.
4 Experimental Results
4.1 Subjectivity Data
The text collection that we used consists of English-
language versions of foreign news documents from FBIS,
the U.S. Foreign Broadcast Information Service. The
data is from a variety of countries. Our system takes
unannotated data as input, but we needed annotated data
to evaluate its performance. We briefly describe the man-
ual annotation scheme used to create the gold-standard,
and give interannotator agreement results.
In 2002, a detailed annotation scheme (Wilson and
Wiebe, 2003) was developed for a government-sponsored
project. We only mention aspects of the annotation
scheme relevant to this paper. The scheme was inspired
by work in linguistics and literary theory on subjectiv-
ity, which focuses on how opinions, emotions, etc. are
expressed linguistically in context (Banfield, 1982). The
goal is to identify and characterize expressions of private
states in a sentence. Private state is a general covering
term for opinions, evaluations, emotions, and specula-
tions (Quirk et al, 1985). For example, in sentence (1)
the writer is expressing a negative evaluation.
(1) ?The time has come, gentlemen, for Sharon, the as-
sassin, to realize that injustice cannot last long.?
Sentence (2) reflects the private state of Western coun-
tries. Mugabe?s use of overwhelmingly also reflects a pri-
vate state, his positive reaction to and characterization of
his victory.
(2) ?Western countries were left frustrated and impotent
after Robert Mugabe formally declared that he had over-
whelmingly won Zimbabwe?s presidential election.?
Annotators are also asked to judge the strength of each
private state. A private state may have low, medium, high
or extreme strength.
To allow us to measure interannotator agreement, three
annotators (who are not authors of this paper) indepen-
dently annotated the same 13 documents with a total of
210 sentences. We begin with a strict measure of agree-
ment at the sentence level by first considering whether
the annotator marked any private-state expression, of any
strength, anywhere in the sentence. If so, the sentence is
subjective. Otherwise, it is objective. The average pair-
wise percentage agreement is 90% and the average pair-
wise ? value is 0.77.
One would expect that there are clear cases of objec-
tive sentences, clear cases of subjective sentences, and
borderline sentences in between. The agreement study
supports this. In terms of our annotations, we define a
sentence as borderline if it has at least one private-state
expression identified by at least one annotator, and all
strength ratings of private-state expressions are low. On
average, 11% of the corpus is borderline under this def-
inition. When those sentences are removed, the average
pairwise percentage agreement increases to 95% and the
average pairwise ? value increases to 0.89.
As expected, the majority of disagreement cases in-
volve low-strength subjectivity. The annotators consis-
tently agree about which are the clear cases of subjective
sentences. This leads us to define the gold-standard that
we use when evaluating our results. A sentence is subjec-
tive if it contains at least one private-state expression of
medium or higher strength. The second class, which we
call objective, consists of everything else.
4.2 Evaluation of the Learned Patterns
Our pool of unannotated texts consists of 302,163 indi-
vidual sentences. The HP-Subj classifier initially labeled
roughly 44,300 of these sentences as subjective, and the
HP-Obj classifier initially labeled roughly 17,000 sen-
tences as objective. In order to keep the training set rel-
atively balanced, we used all 17,000 objective sentences
and 17,000 of the subjective sentences as training data for
the extraction pattern learner.
17,073 extraction patterns were learned that have
frequency ? 2 and Pr(subjective | patterni) ? .60 on
the training data. We then wanted to determine whether
the extraction patterns are, in fact, good indicators of sub-
jectivity. To evaluate the patterns, we applied different
subsets of them to a test set to see if they consistently oc-
cur in subjective sentences. This test set consists of 3947
Figure 4: Evaluating the Learned Patterns on Test Data
sentences, 54% of which are subjective.
Figure 4 shows sentence recall and pattern (instance-
level) precision for the learned extraction patterns on the
test set. In this figure, precision is the proportion of pat-
tern instances found in the test set that are in subjective
sentences, and recall is the proportion of subjective sen-
tences that contain at least one pattern instance.
We evaluated 18 different subsets of the patterns, by
selecting the patterns that pass certain thresholds in the
training data. We tried all combinations of ?1 = {2,10}
and ?2 = {.60,.65,.70,.75,.80,.85,.90,.95,1.0}. The data
points corresponding to ?1=2 are shown on the upper line
in Figure 4, and those corresponding to ?1=10 are shown
on the lower line. For example, the data point correspond-
ing to ?1=10 and ?2=.90 evaluates only the extraction pat-
terns that occur at least 10 times in the training data and
with a probability ? .90 (i.e., at least 90% of its occur-
rences are in subjective training sentences).
Overall, the extraction patterns perform quite well.
The precision ranges from 71% to 85%, with the expected
tradeoff between precision and recall. This experiment
confirms that the extraction patterns are effective at rec-
ognizing subjective expressions.
4.3 Evaluation of the Bootstrapping Process
In our second experiment, we used the learned extrac-
tion patterns to classify previously unlabeled sentences
from the unannotated text collection. The new subjec-
tive sentences were then fed back into the Extraction Pat-
tern Learner to complete the bootstrapping cycle depicted
by the rightmost dashed line in Figure 1. The Pattern-
based Subjective Sentence Classifier classifies a sentence
as subjective if it contains at least one extraction pattern
with ?1?5 and ?2?1.0 on the training data. This process
produced approximately 9,500 new subjective sentences
that were previously unlabeled.
Since our bootstrapping process does not learn new ob-
jective sentences, we did not want to simply add the new
subjective sentences to the training set, or it would be-
come increasingly skewed toward subjective sentences.
Since HP-Obj had produced roughly 17,000 objective
sentences used for training, we used the 9,500 new sub-
jective sentences along with 7,500 of the previously iden-
tified subjective sentences as our new training set. In
other words, the training set that we used during the sec-
ond bootstrapping cycle contained exactly the same ob-
jective sentences as the first cycle, half of the same sub-
jective sentences as the first cycle, and 9,500 brand new
subjective sentences.
On this second cycle of bootstrapping, the extraction
pattern learner generated many new patterns that were not
discovered during the first cycle. 4,248 new patterns were
found that have ?1?2 and ?2?.60. If we consider only the
strongest (most subjective) extraction patterns, 308 new
patterns were found that had ?1?10 and ?2?1.0. This is
a substantial set of new extraction patterns that seem to
be very highly correlated with subjectivity.
An open question was whether the new patterns pro-
vide additional coverage. To assess this, we did a sim-
ple test: we added the 4,248 new patterns to the origi-
nal set of patterns learned during the first bootstrapping
cycle. Then we repeated the same analysis that we de-
pict in Figure 4. In general, the recall numbers increased
by about 2-4% while the precision numbers decreased by
less, from 0.5-2%.
In our third experiment, we evaluated whether the
learned patterns can improve the coverage of the high-
precision subjectivity classifier (HP-Subj), to complete
the bootstrapping loop depicted in the top-most dashed
line of Figure 1. Our hope was that the patterns would al-
low more sentences from the unannotated text collection
to be labeled as subjective, without a substantial drop in
precision. For this experiment, we selected the learned
extraction patterns that had ?1? 10 and ?2? 1.0 on the
training set, since these seemed likely to be the most reli-
able (high precision) indicators of subjectivity.
We modified the HP-Subj classifier to use extraction
patterns as follows. All sentences labeled as subjective
by the original HP-Subj classifier are also labeled as sub-
jective by the new version. For previously unlabeled sen-
tences, the new version classifies a sentence as subjective
if (1) it contains two or more of the learned patterns, or
(2) it contains one of the clues used by the original HP-
Subj classifier and at least one learned pattern. Table 1
shows the performance results on the test set mentioned
in Section 3.1 (2197 sentences) for both the original HP-
Subj classifier and the new version that uses the learned
extraction patterns. The extraction patterns produce a 7.2
percentage point gain in coverage, and only a 1.1 percent-
age point drop in precision. This result shows that the
learned extraction patterns do improve the performance
of the high-precision subjective sentence classifier, allow-
ing it to classify more sentences as subjective with nearly
the same high reliability.
HP-Subj HP-Subj w/Patterns
Recall Precision Recall Precision
32.9 91.3 40.1 90.2
Table 1: Bootstrapping the Learned Patterns into the
High-Precision Sentence Classifier
Table 2 gives examples of patterns used to augment the
HP-Subj classifier which do not overlap in non-function
words with any of the clues already known by the original
system. For each pattern, we show an example sentence
from our corpus that matches the pattern.
5 Conclusions
This research explored several avenues for improving the
state-of-the-art in subjectivity analysis. First, we demon-
strated that high-precision subjectivity classification can
be used to generate a large amount of labeled training data
for subsequent learning algorithms to exploit. Second, we
showed that an extraction pattern learning technique can
learn subjective expressions that are linguistically richer
than individual words or fixed phrases. We found that
similar expressions may behave very differently, so that
one expression may be strongly indicative of subjectivity
but the other may not. Third, we augmented our origi-
nal high-precision subjective classifier with these newly
learned extraction patterns. This bootstrapping process
resulted in substantially higher recall with a minimal loss
in precision. In future work, we plan to experiment with
different configurations of these classifiers, add new sub-
jective language learners in the bootstrapping process,
and address the problem of how to identify new objec-
tive sentences during bootstrapping.
6 Acknowledgments
We are very grateful to Theresa Wilson for her invaluable
programming support and help with data preparation.
References
C. Baker, C. Fillmore, and J. Lowe. 1998. The Berkeley
FrameNet Project. In Proceedings of the COLING-ACL-98.
T. Ballmer and W. Brennenstuhl. 1981. Speech Act Classifi-
cation: A Study in the Lexical Analysis of English Speech
Activity Verbs. Springer-Verlag.
A. Banfield. 1982. Unspeakable Sentences. Routledge and
Kegan Paul, Boston.
seems to be <dobj> I am pleased that there now seems to be broad political consensus . . .
underlined <dobj> Jiang?s subdued tone . . . underlined his desire to avoid disputes . . .
pretext of <np> On the pretext of the US opposition . . .
atmosphere of <np> Terrorism thrives in an atmosphere of hate . . .
<subj> reflect These are fine words, but they do not reflect the reality . . .
to satisfy <dobj> The pictures resemble an attempt to satisfy a primitive thirst for revenge . . .
way with <np> . . . to ever let China use force to have its way with . . .
bring about <np> ?Everything must be done by everyone to bring about de-escalation,? Mr Chirac added.
expense of <np> at the expense of the world?s security and stability
voiced <dobj> Khatami . . . voiced Iran?s displeasure.
turn into <np> . . . the surging epidemic could turn into ?a national security threat,? he said.
Table 2: Examples of Learned Patterns Used by HP-Subj and Sample Matching Sentences
M. E. Califf. 1998. Relational Learning Techniques for Natural
Language Information Extraction. Ph.D. thesis, Tech. Rept.
AI98-276, Artificial Intelligence Laboratory, The University
of Texas at Austin.
Dayne Freitag. 1998. Toward General-Purpose Learning for
Information Extraction. In Proceedings of the ACL-98.
V. Hatzivassiloglou and K. McKeown. 1997. Predicting the
Semantic Orientation of Adjectives. In Proceedings of the
ACL-EACL-97.
S. Huffman. 1996. Learning information extraction pat-
terns from examples. In Stefan Wermter, Ellen Riloff,
and Gabriele Scheler, editors, Connectionist, Statistical, and
Symbolic Approaches to Learning for Natural Language
Processing, pages 246?260. Springer-Verlag, Berlin.
J. Karlgren and D. Cutting. 1994. Recognizing Text Genres
with Simple Metrics Using Discriminant Analysis. In Pro-
ceedings of the COLING-94.
B. Kessler, G. Nunberg, and H. Schu?tze. 1997. Automatic De-
tection of Text Genre. In Proceedings of the ACL-EACL-97.
J. Kim and D. Moldovan. 1993. Acquisition of Semantic Pat-
terns for Information Extraction from Corpora. In Proceed-
ings of the Ninth IEEE Conference on Artificial Intelligence
for Applications.
Beth Levin. 1993. English Verb Classes and Alternations: A
Preliminary Investigation. University of Chicago Press.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs up? Sen-
timent Classification Using Machine Learning Techniques.
In Proceedings of the EMNLP-02.
R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik. 1985. A
Comprehensive Grammar of the English Language. Long-
man, New York.
E. Riloff and R. Jones. 1999. Learning Dictionaries for In-
formation Extraction by Multi-Level Bootstrapping. In Pro-
ceedings of the AAAI-99.
E. Riloff, J. Wiebe, and T. Wilson. 2003. Learning Subjective
Nouns using Extraction Pattern Bootstrapping. In Proceed-
ings of the Seventh Conference on Computational Natural
Language Learning (CoNLL-03).
E. Riloff. 1993. Automatically Constructing a Dictionary for
Information Extraction Tasks. In Proceedings of the AAAI-
93.
E. Riloff. 1996. Automatically Generating Extraction Patterns
from Untagged Text. In Proceedings of the AAAI-96.
S. Soderland, D. Fisher, J. Aseltine, and W. Lehnert. 1995.
CRYSTAL: Inducing a Conceptual Dictionary. In Proceed-
ings of the IJCAI-95.
S. Soderland. 1999. Learning Information Extraction Rules for
Semi-Structured and Free Text. Machine Learning, 34(1-
3):233?272.
E. Spertus. 1997. Smokey: Automatic Recognition of Hostile
Messages. In Proceedings of the IAAI-97.
P. Turney. 2002. Thumbs Up or Thumbs Down? Semantic Ori-
entation Applied to Unsupervised Classification of Reviews.
In Proceedings of the ACL-02.
J. Wiebe, R. Bruce, and T. O?Hara. 1999. Development and
Use of a Gold Standard Data Set for Subjectivity Classifica-
tions. In Proceedings of the ACL-99.
J. Wiebe, T. Wilson, and M. Bell. 2001. Identifying Collo-
cations for Recognizing Opinions. In Proceedings of the
ACL-01 Workshop on Collocation: Computational Extrac-
tion, Analysis, and Exploitation.
J. Wiebe. 1990. Recognizing Subjective Sentences: A Compu-
tational Investigation of Narrative Text. Ph.D. thesis, State
University of New York at Buffalo.
J. Wiebe. 2000. Learning Subjective Adjectives from Corpora.
In Proceedings of the AAAI-00.
T. Wilson and J. Wiebe. 2003. Annotating Opinions in the
World Press. In Proceedings of the ACL SIGDIAL-03.
R. Yangarber, R. Grishman, P. Tapanainen, and S. Huttunen.
2000. Automatic Acquisiton of Domain Knowledge for In-
formation Extraction. In Proceedings of COLING 2000.
Proceedings of the Workshop on Information Extraction Beyond The Document, pages 66?73,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Learning Domain-Specific Information Extraction Patterns from the Web
Siddharth Patwardhan and Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT 84112
{sidd,riloff}@cs.utah.edu
Abstract
Many information extraction (IE) systems
rely on manually annotated training data
to learn patterns or rules for extracting in-
formation about events. Manually anno-
tating data is expensive, however, and a
new data set must be annotated for each
domain. So most IE training sets are rel-
atively small. Consequently, IE patterns
learned from annotated training sets of-
ten have limited coverage. In this paper,
we explore the idea of using the Web to
automatically identify domain-specific IE
patterns that were not seen in the training
data. We use IE patterns learned from the
MUC-4 training set as anchors to identify
domain-specific web pages and then learn
new IE patterns from them. We compute
the semantic affinity of each new pattern
to automatically infer the type of informa-
tion that it will extract. Experiments on
the MUC-4 test set show that these new IE
patterns improved recall with only a small
precision loss.
1 Introduction
Information Extraction (IE) is the task of identi-
fying event descriptions in natural language text
and extracting information related to those events.
Many IE systems use extraction patterns or rules
to identify the relevant information (Soderland et
al., 1995; Riloff, 1996; Califf and Mooney, 1999;
Soderland, 1999; Yangarber et al, 2000). Most of
these systems use annotated training data to learn
pattern matching rules based on lexical, syntactic,
and/or semantic information. The learned patterns
are then used to locate relevant information in new
texts.
IE systems typically focus on information about
events that are relevant to a specific domain, such
as terrorism (Sundheim, 1992; Soderland et al,
1995; Riloff, 1996; Chieu et al, 2003), man-
agement succession (Sundheim, 1995; Yangarber
et al, 2000), or job announcements (Califf and
Mooney, 1999; Freitag and McCallum, 2000).
Supervised learning systems for IE depend on
domain-specific training data, which consists of
texts associated with the domain that have been
manually annotated with event information.
The need for domain-specific training data has
several disadvantages. Because of the manual la-
bor involved in annotating a corpus, and because a
new corpus must be annotated for each domain,
most annotated IE corpora are relatively small.
Language is so expressive that it is practically
impossible for the patterns learned from a rela-
tively small training set to cover all the different
ways of describing events. Consequently, the IE
patterns learned from manually annotated train-
ing sets typically represent only a subset of the IE
patterns that could be useful for the task. Many
recent approaches in natural language processing
(Yarowsky, 1995; Collins and Singer, 1999; Riloff
and Jones, 1999; Nigam et al, 2000; Wiebe and
Riloff, 2005) have recognized the need to use
unannotated data to improve performance.
While the Web provides a vast repository of
unannotated texts, it is non-trivial to identify texts
that belong to a particular domain. The difficulty
is that web pages are not specifically annotated
with tags categorizing their content. Nevertheless,
in this paper we look to the Web as a vast dynamic
resource for domain-specific IE learning. Our ap-
proach exploits an existing set of IE patterns that
were learned from annotated training data to auto-
matically identify new, domain-specific texts from
66
the Web. These web pages are then used for ad-
ditional IE training, yielding a new set of domain-
specific IE patterns. Experiments on the MUC-4
test set show that the new IE patterns improve cov-
erage for the domain.
This paper is organized as follows. Section 2
presents the MUC-4 IE task and data that we use in
our experiments. Section 3 describes how we cre-
ate a baseline IE system from the MUC-4 training
data. Section 4 describes the collection and pre-
processing of potentially relevant web pages. Sec-
tion 5 then explains how we use the IE patterns
learned from the MUC-4 training set as anchors to
learn new IE patterns from the web pages. We also
compute the semantic affinity of each new pattern
to automatically infer the type of information that
it will extract. Section 6 shows experimental re-
sults for two types of extractions, victims and tar-
gets, on the MUC-4 test set. Finally, Section 7
compares our approach to related research, and
Section 8 concludes with ideas for future work.
2 The MUC-4 IE Task and Data
The focus of our research is on the MUC-4 infor-
mation extraction task (Sundheim, 1992), which is
to extract information about terrorist events. The
MUC-4 corpus contains 1700 stories, mainly news
articles related to Latin American terrorism, and
associated answer key templates containing the in-
formation that should be extracted from each story.
We focused our efforts on two of the MUC-4
string slots, which require textual extractions: hu-
man targets (victims) and physical targets. The
MUC-4 data has proven to be an especially dif-
ficult IE task for a variety of reasons, including
the fact that the texts are entirely in upper case,
roughly 50% of the texts are irrelevant (i.e., they
do not describe a relevant terrorist event), and
many of the stories that are relevant describe mul-
tiple terrorist events that need to be teased apart.
The best results reported across all string slots
in MUC-4 were in the 50%-70% range for re-
call and precision (Sundheim, 1992), with most
of the MUC-4 systems relying on heavily hand-
engineered components. Chieu et al (2003) re-
cently developed a fully automatic template gen-
erator for the MUC-4 IE task. Their best system
produced recall scores of 41%-44% with precision
scores of 49%-51% on the TST3 and TST4 test
sets.
3 Learning IE Patterns from a Fixed
Training Set
As our baseline system, we created an IE
system for the MUC-4 terrorism domain us-
ing the AutoSlog-TS extraction pattern learn-
ing system (Riloff, 1996; Riloff and Phillips,
2004), which is freely available for research use.
AutoSlog-TS is a weakly supervised learner that
requires two sets of texts for training: texts that
are relevant to the domain and texts that are irrel-
evant to the domain. The MUC-4 data includes
relevance judgments (implicit in the answer keys),
which we used to partition our training set into rel-
evant and irrelevant subsets.
AutoSlog-TS? learning process has two phases.
In the first phase, syntactic patterns are applied
to the training corpus in an exhaustive fashion,
so that extraction patterns are generated for (lit-
erally) every lexical instantiation of the patterns
that appears in the corpus. For example, the syn-
tactic pattern ?<subj> PassVP? would generate
extraction patterns for all verbs that appear in the
corpus in a passive voice construction. The sub-
ject of the verb will be extracted. In the terrorism
domain, some of these extraction patterns might
be: ?<subj> PassVP(murdered)? and ?<subj>
PassVP(bombed).? These would match sentences
such as: ?the mayor was murdered?, and ?the em-
bassy and hotel were bombed?. Figure 1 shows
the 17 types of extraction patterns that AutoSlog-
TS currently generates. PassVP refers to passive
voice verb phrases (VPs), ActVP refers to active
voice VPs, InfVP refers to infinitive VPs, and
AuxVP refers to VPs where the main verb is a
form of ?to be? or ?to have?. Subjects (subj), di-
rect objects (dobj), PP objects (np), and posses-
sives can be extracted by the patterns.
In the second phase, AutoSlog-TS applies all
of the generated extraction patterns to the training
corpus and gathers statistics for how often each
pattern occurs in relevant versus irrelevant texts.
The extraction patterns are subsequently ranked
based on their association with the domain, and
then a person manually reviews the patterns, de-
ciding which ones to keep1 and assigning thematic
roles to them. We manually defined selectional
restrictions for each slot type (victim and target)
1Typically, many patterns are strongly associated with the
domain but will not extract information that is relevant to the
IE task. For example, in this work we only care about patterns
that will extract victims and targets. Patterns that extract other
types of information are not of interest.
67
Pattern Type Example Pattern
<subj> PassVP <victim> was murdered
<subj> ActVP <perp> murdered
<subj> ActVP Dobj <weapon> caused damage
<subj> ActInfVP <perp> tried to kill
<subj> PassInfVP <weapon> was intended to kill
<subj> AuxVP Dobj <victim> was casualty
<subj> AuxVP Adj <victim> is dead
ActVP <dobj> bombed <target>
InfVP <dobj> to kill <victim>
ActInfVP <dobj> planned to bomb <target>
PassInfVP <dobj> was planned to kill <victim>
Subj AuxVP <dobj> fatality is <victim>
NP Prep <np> attack against <target>
ActVP Prep <np> killed with <weapon>
PassVP Prep <np> was killed with <weapon>
InfVP Prep <np> to destroy with <weapon>
<possessive> NP <victim>?s murder
Figure 1: AutoSlog-TS? pattern types and sample
IE patterns
and then automatically added these to each pattern
when the role was assigned.
On our training set, AutoSlog-TS generated
40,553 distinct extraction patterns. A person man-
ually reviewed all of the extraction patterns that
had a score ? 0.951 and frequency ? 3. This
score corresponds to AutoSlog-TS? RlogF metric,
described in (Riloff, 1996). The lowest ranked pat-
terns that passed our thresholds had at least 3 rel-
evant extractions out of 5 total extractions. In all,
2,808 patterns passed the thresholds. The reviewer
ultimately decided that 396 of the patterns were
useful for the MUC-4 IE task, of which 291 were
useful for extracting victims and targets.
4 Data Collection
In this research, our goal is to automatically learn
IE patterns from a large, domain-independent text
collection, such as the Web. The billions of freely
available documents on the World Wide Web and
its ever-growing size make the Web a potential
source of data for many corpus-based natural lan-
guage processing tasks. Indeed, many researchers
have recently tapped the Web as a data-source
for improving performance on NLP tasks (e.g.,
Resnik (1999), Ravichandran and Hovy (2002),
Keller and Lapata (2003)). Despite these suc-
cesses, numerous problems exist with collecting
data from the Web, such as web pages contain-
ing information that is not free text, including ad-
vertisements, embedded scripts, tables, captions,
etc. Also, the documents cover many genres, and
it is not easy to identify documents of a particular
genre or domain. Additionally, most of the doc-
uments are in HTML, and some amount of pro-
cessing is required to extract the free text. In the
following subsections we describe the process of
collecting a corpus of terrorism-related CNN news
articles from the Web.
4.1 Collecting Domain-Specific Texts
Our goal was to automatically identify and collect
a set of documents that are similar in domain to the
MUC-4 terrorism text collection. To create such
a corpus, we used hand-crafted queries given to
a search engine. The queries to the search engine
were manually created to try to ensure that the ma-
jority of the documents returned by the search en-
gine would be terrorism-related. Each query con-
sisted of two parts: (1) the name of a terrorist or-
ganization, and (2) a word or phrase describing a
terrorist action (such as bombed, kidnapped, etc.).
The following lists of 5 terrorist organizations and
16 terrorist actions were used to create search en-
gine queries:
Terrorist organizations: Al Qaeda,
ELN, FARC, HAMAS, IRA
Terrorist actions: assassinated, assas-
sination, blew up, bombed, bombing,
bombs, explosion, hijacked, hijacking,
injured, kidnapped, kidnapping, killed,
murder, suicide bomber, wounded.
We created a total of 80 different queries repre-
senting each possible combination of a terrorist or-
ganization and a terrorist action.
We used the Google2 search engine with the
help of the freely available Google API3 to lo-
cate the texts on the Web. To ensure that we re-
trieved only CNN news articles, we restricted the
search to the domain ?cnn.com? by adding the
?site:? option to each of the queries. We also
restricted the search to English language docu-
ments by initializing the API with the lang en
option. We deleted documents whose URLs con-
tained the word ?transcript? because most of these
were transcriptions of CNN?s TV shows and were
stylistically very different from written text. We
ran the 80 queries twice, once in December 2005
and once in April 2005, which produced 3,496
documents and 3,309 documents, respectively.
After removing duplicate articles, we were left
2http://www.google.com
3http://www.google.com/apis
68
with a total of 6,182 potentially relevant terrorism
articles.
4.2 Processing the Texts
The downloaded documents were all HTML doc-
uments containing HTML tags and JavaScript in-
termingled with the news text. The CNN web-
pages typically also contained advertisements, text
for navigating the website, headlines and links to
other stories. All of these things could be problem-
atic for our information extraction system, which
was designed to process narrative text using a shal-
low parser. Thus, simply deleting all HTML tags
on the page would not have given us natural lan-
guage sentences. Instead, we took advantage of
the uniformity of the CNN web pages to ?clean?
them and extract just the sentences corresponding
to the news story.
We used a tool called HTMLParser4 to parse
the HTML code, and then deleted all nodes in the
HTML parse trees corresponding to tables, com-
ments, and embedded scripts (such as JavaScript
or VBScript). The system automatically extracted
news text starting from the headline (embedded
in an H1 HTML element) and inferred the end of
the article text using a set of textual clues such as
?Feedback:?, ?Copyright 2005?, ?contributed to
this report?, etc. In case of any ambiguity, all of
the text on the web page was extracted.
The size of the text documents ranged from 0
bytes to 255 kilobytes. The empty documents
were due to dead links that the search engine had
indexed at an earlier time, but which no longer ex-
isted. Some extremely small documents also re-
sulted from web pages that had virtually no free
text on them, so only a few words remained af-
ter the HTML had been stripped. Consequently,
we removed all documents less than 10 bytes in
size. Upon inspection, we found that many of the
largest documents were political articles, such as
political party platforms and transcriptions of po-
litical speeches, which contained only brief refer-
ences to terrorist events. To prevent the large doc-
uments from skewing the corpus, we also deleted
all documents over 10 kilobytes in size. At the end
of this process we were left with a CNN terrorism
news corpus of 5,618 documents, each with an av-
erage size of about 648 words. In the rest of the
paper we will refer to these texts as ?the CNN ter-
rorism web pages?.
4http://htmlparser.sourceforge.net
5 Learning Domain-Specific IE Patterns
from Web Pages
Having created a large domain-specific corpus
from the Web, we are faced with the problem
of identifying the useful extraction patterns from
these new texts. Our basic approach is to use the
patterns learned from the fixed training set as seed
patterns to identify sentences in the CNN terror-
ism web pages that describe a terrorist event. We
hypothesized that extraction patterns occurring in
the same sentence as a seed pattern are likely to be
associated with terrorism.
Our process for learning new domain-specific
IE patterns has two phases, which are described in
the following sections. Section 5.1 describes how
we produce a ranked list of candidate extraction
patterns from the CNN terrorism web pages. Sec-
tion 5.2 explains how we filter these patterns based
on the semantic affinity of their extractions, which
is a measure of the tendency of the pattern to ex-
tract entities of a desired semantic category.
5.1 Identifying Candidate Patterns
The first goal was to identify extraction patterns
that were relevant to our domain: terrorist events.
We began by exhaustively generating every pos-
sible extraction pattern that occurred in our CNN
terrorism web pages. We applied the AutoSlog-TS
system (Riloff, 1996) to the web pages to automat-
ically generate all lexical instantiations of patterns
in the corpus. Collectively, the resulting patterns
were capable of extracting every noun phrase in
the CNN collection. In all, 147,712 unique extrac-
tion patterns were created as a result of this pro-
cess.
Next, we computed the statistical correlation
of each extraction pattern with the seed patterns
based on the frequency of their occurrence in the
same sentence. IE patterns that never occurred
in the same sentence as a seed pattern were dis-
carded. We used Pointwise Mutual Information
(PMI) (Manning and Schu?tze, 1999; Banerjee and
Pedersen, 2003) as the measure of statistical corre-
lation. Intuitively, an extraction pattern that occurs
more often than chance in the same sentence as a
seed pattern will have a high PMI score.
The 147,712 extraction patterns acquired from
the CNN terrorism web pages were then ranked
by their PMI correlation to the seed patterns. Ta-
ble 1 lists the most highly ranked patterns. Many
of these patterns do seem to be related to terrorism,
69
<subj> killed sgt <subj> destroyed factories
<subj> burned flag explode after <np>
sympathizers of <np> <subj> killed heir
<subj> kills bystanders <subj> shattered roof
rescued within <np> fled behind <np>
Table 1: Examples of candidate patterns that are
highly correlated with the terrorism seed patterns
but many of them are not useful to our IE task (for
this paper, identifying the victims and physical tar-
gets of a terrorist attack). For example, the pattern
?explode after <np>? will not extract victims or
physical targets, while the pattern ?sympathizers
of <np>? may extract people but they would not
be the victims of an attack. In the next section, we
explain how we filter and re-rank these candidate
patterns to identify the ones that are directly useful
to our IE task.
5.2 Filtering Patterns based upon their
Semantic Affinity
Our next goal is to filter out the patterns that are
not useful for our IE task, and to automatically
assign the correct slot type (victim or target) to
the ones that are relevant. To automatically deter-
mine the mapping between extractions and slots,
we define a measure called semantic affinity. The
semantic affinity of an extraction pattern to a se-
mantic category is a measure of its tendency to
extract NPs belonging to that semantic category.
This measure serves two purposes:
(a) It allows us to filter out candidate patterns
that do not have a strong semantic affinity to
our categories of interest.
(b) It allows us to define a mapping between the
extractions of the candidate patterns and the
desired slot types.
We computed the semantic affinity of each can-
didate extraction pattern with respect to six seman-
tic categories: target, victim, perpetrator, organi-
zation, weapon and other. Targets and victims are
our categories of interest. Perpetrators, organiza-
tions, and weapons are common semantic classes
in this domain which could be ?distractors?. The
other category is a catch-all to represent all other
semantic classes. To identify the semantic class of
each noun phrase, we used the Sundance package
(Riloff and Phillips, 2004), which is a freely avail-
able shallow parser that uses dictionaries to assign
semantic classes to words and phrases.
We counted the frequencies of the semantic cat-
egories extracted by each candidate pattern and
applied the RLogF measure used by AutoSlog-TS
(Riloff, 1996) to rank the patterns based on their
affinity for the target and victim semantic classes.
For example, the semantic affinity of an extraction
pattern for the target semantic class would be cal-
culated as:
affinitypattern =
ftarget
fall
? log2ftarget (1)
where ftarget is the number of target semantic
class extractions and fall = ftarget + fvictim +
fperp +forg +fweapon +fother. This is essentially
a probability P (target) weighted by the log of the
frequency.
We then used two criteria to remove patterns
that are not strongly associated with a desired se-
mantic category. If the semantic affinity of a pat-
tern for category C was (1) greater than a thresh-
old, and (2) greater than its affinity for the other
category, then the pattern was deemed to have a
semantic affinity for category C. Note that we
intentionally allow for a pattern to have an affin-
ity for more than one semantic category (except
for the catch-all other class) because this is fairly
common in practice. For example, the pattern ?at-
tack on <np>? frequently extracts both targets
(e.g., ?an attack on the U.S. embassy?) and vic-
tims (e.g., ?an attack on the mayor of Bogota?).
Our hope is that such a pattern would receive a
high semantic affinity ranking for both categories.
Table 2 shows the top 10 high frequency
(freq ? 50) patterns that were judged to have a
strong semantic affinity for the target and victim
categories. There are clearly some incorrect en-
tries (e.g., ?<subj> fired missiles? is more likely
to identify perpetrators than targets), but most of
the patterns are indeed good extractors for the de-
sired categories. For example, ?fired into <np>?,
?went off in <np>?, and ?car bomb near <np>?
are all good patterns for identifying targets of a
terrorist attack. In general, the semantic affinity
measure seemed to do a reasonably good job of
filtering patterns that are not relevant to our task,
and identifying patterns that are useful for extract-
ing victims and targets.
6 Experiments and Results
Our goal has been to use IE patterns learned from
a fixed, domain-specific training set to automat-
ically learn additional IE patterns from a large,
70
Target Patterns Victim Patterns
<subj> fired missiles wounded in <np>
missiles at <np> <subj> was identified
bomb near <np> wounding <dobj>
fired into <np> <subj> wounding
died on <np> identified <dobj>
went off in <np> <subj> identified
car bomb near <np> including <dobj>
exploded outside <np> <subj> ahmed
gunmen on <np> <subj> lying
killed near <np> <subj> including
Table 2: Top 10 high-frequency target and victim
patterns learned from the Web
domain-independent text collection, such as the
Web. Although many of the patterns learned
from the CNN terrorism web pages look like good
extractors, an open question was whether they
would actually be useful for the original IE task.
For example, some of the patterns learned from
the CNN web pages have to do with behead-
ings (e.g., ?beheading of <np>? and ?beheaded
<np>?), which are undeniably good victim ex-
tractors. But the MUC-4 corpus primarily con-
cerns Latin American terrorism that does not in-
volve beheading incidents. In general, the ques-
tion is whether IE patterns learned from a large, di-
verse text collection can be valuable for a specific
IE task above and beyond the patterns that were
learned from the domain-specific training set, or
whether the newly learned patterns will simply not
be applicable. To answer this question, we evalu-
ated the newly learned IE patterns on the MUC-4
test set.
The MUC-4 data set is divided into 1300 devel-
opment (DEV) texts, and four test sets of 100 texts
each (TST1, TST2, TST3, and TST4).5 All of
these texts have associated answer key templates.
We used 1500 texts (DEV+TST1+TST2) as our
training set, and 200 texts (TST3+TST4) as our
test set.
The IE process typically involves extracting
information from individual sentences and then
mapping that information into answer key tem-
plates, one template for each terrorist event de-
scribed in the story. The process of template gen-
eration requires discourse processing to determine
how many events took place and which facts cor-
respond to which event. Discourse processing and
5The DEV texts were used for development in MUC-3
and MUC-4. The TST1 and TST2 texts were used as test sets
for MUC-3 and then as development texts for MUC-4. The
TST3 and TST4 texts were used as the test sets for MUC-4.
template generation are not the focus of this paper.
Our research aims to produce a larger set of extrac-
tion patterns so that more information will be ex-
tracted from the sentences, before discourse anal-
ysis would begin. Consequently, we evaluate the
performance of our IE system at that stage: after
extracting information from sentences, but before
template generation takes place. This approach di-
rectly measures how well we are able to improve
the coverage of our extraction patterns for the do-
main.
6.1 Baseline Results on the MUC-4 IE Task
The AutoSlog-TS system described in Section 3
used the MUC-4 training set to learn 291 target
and victim IE patterns. These patterns produced
64% recall with 43% precision on the targets, and
50% recall with 52% precision on the victims.6
These numbers are not directly comparable to
the official MUC-4 scores, which evaluate tem-
plate generation, but our recall is in the same ball-
park. Our precision is lower, but this is to be ex-
pected because we do not perform discourse anal-
ysis.7 These 291 IE patterns represent our base-
line IE system that was created from the MUC-4
training data.
6.2 Evaluating the Newly Learned Patterns
We used all 396 terrorism extraction patterns
learned from the MUC-4 training set8 as seeds to
identify relevant text regions in the CNN terrorism
web pages. We then produced a ranked list of new
terrorism IE patterns using a semantic affinity cut-
off of 3.0. We selected the top N patterns from the
ranked list, with N ranging from 50 to 300, and
added these N patterns to the baseline system.
Table 3 lists the recall, precision and F-measure
for the increasingly larger pattern sets. For the tar-
6We used a head noun scoring scheme, where we scored
an extraction as correct if its head noun matched the head
noun in the answer key. This approach allows for different
leading modifiers in an NP as long as the head noun is the
same. For example, ?armed men? will successfully match
?5 armed men?. We also discarded pronouns (they were not
scored at all) because our system does not perform corefer-
ence resolution.
7Among other things, discourse processing merges seem-
ingly disparate extractions based on coreference resolution
(e.g., ?the guerrillas? may refer to the same people as ?the
armed men?) and applies task-specific constraints (e.g., the
MUC-4 task definition has detailed rules about exactly what
types of people are considered to be terrorists).
8This included not only the 291 target and victim patterns,
but also 105 patterns associated with other types of terrorism
information.
71
Targets Victims
Precision Recall F-measure Precision Recall F-measure
baseline 0.425 0.642 0.511 0.498 0.517 0.507
50+baseline 0.420 0.642 0.508 0.498 0.517 0.507
100+baseline 0.419 0.650 0.510 0.496 0.521 0.508
150+baseline 0.415 0.650 0.507 0.480 0.521 0.500
200+baseline 0.412 0.667 0.509 0.478 0.521 0.499
250+baseline 0.401 0.691 0.507 0.478 0.521 0.499
300+baseline 0.394 0.691 0.502 0.471 0.542 0.504
Table 3: Performance of new IE patterns on MUC-4 test set
get slot, the recall increases from 64.2% to 69.1%
with a small drop in precision. The F-measure
drops by about 1% because recall and precision
are less balanced. But we gain more in recall
(+5%) than we lose in precision (-3%). For the
victim patterns, the recall increases from 51.7% to
54.2% with a similar small drop in precision. The
overall drop in the F-measure in this case is neg-
ligible. These results show that our approach for
learning IE patterns from a large, diverse text col-
lection (the Web) can indeed improve coverage on
a domain-specific IE task, with a small decrease in
precision.
7 Related Work
Unannotated texts have been used successfully for
a variety of NLP tasks, including named entity
recognition (Collins and Singer, 1999), subjectiv-
ity classification (Wiebe and Riloff, 2005), text
classification (Nigam et al, 2000), and word sense
disambiguation (Yarowsky, 1995). The Web has
become a popular choice as a resource for large
quantities of unannotated data. Many research
ideas have exploited the Web in unsupervised or
weakly supervised algorithms for natural language
processing (e.g., Resnik (1999), Ravichandran and
Hovy (2002), Keller and Lapata (2003)).
The use of unannotated data to improve in-
formation extraction is not new. Unannotated
texts have been used for weakly supervised train-
ing of IE systems (Riloff, 1996) and in boot-
strapping methods that begin with seed words
or patterns (Riloff and Jones, 1999; Yangarber
et al, 2000). However, those previous sys-
tems rely on pre-existing domain-specific cor-
pora. For example, EXDISCO (Yangarber et
al., 2000) used Wall Street Journal articles for
training. AutoSlog-TS (Riloff, 1996) and Meta-
bootstrapping (Riloff and Jones, 1999) used the
MUC-4 training texts. Meta-bootstrapping was
also trained on web pages, but the ?domain? was
corporate relationships so domain-specific web
pages were easily identified simply by gathering
corporate web pages.
The KNOWITALL system (Popescu et al, 2004)
also uses unannotated web pages for information
extraction. However, this work is quite differ-
ent from ours because KNOWITALL focuses on
extracting domain-independent relationships with
the aim of extending an ontology. In contrast,
our work focuses on using the Web to augment
a domain-specific, event-oriented IE system with
new, automatically generated domain-specific IE
patterns acquired from the Web.
8 Conclusions and Future Work
We have shown that it is possible to learn new
extraction patterns for a domain-specific IE task
by automatically identifying domain-specific web
pages using seed patterns. Our approach produced
a 5% increase in recall for extracting targets and a
3% increase in recall for extracting victims of ter-
rorist events. Both increases in recall were at the
cost of a small loss in precision.
In future work, we plan to develop improved
ranking methods and more sophisticated seman-
tic affinity measures to further improve coverage
and minimize precision loss. Another possible av-
enue for future work is to embed this approach in a
bootstrapping mechanism so that the most reliable
new IE patterns can be used to collect additional
web pages, which can then be used to learn more
IE patterns in an iterative fashion. Also, while
most of this process is automated, some human in-
tervention is required to create the search queries
for the document collection process, and to gener-
ate the seed patterns. We plan to look into tech-
niques to automate these manual tasks as well.
72
Acknowledgments
This research was supported by NSF Grant IIS-
0208985 and the Institute for Scientific Comput-
ing Research and the Center for Applied Scientific
Computing within Lawrence Livermore National
Laboratory.
References
S. Banerjee and T. Pedersen. 2003. The Design, Im-
plementation, and Use of the Ngram Statistics Pack-
age. In Proceedings of the Fourth International
Conference on Intelligent Text Processing and Com-
putational Linguistics, pages 370?381, Mexico City,
Mexico, February.
M. Califf and R. Mooney. 1999. Relational Learning
of Pattern-matching Rules for Information Extrac-
tion. In Proceedings of the Sixteenth National Con-
ference on Artificial Intelligence, pages 328?334,
Orlando, FL, July.
H. Chieu, H. Ng, and Y. Lee. 2003. Closing the
Gap: Learning-Based Information Extraction Rival-
ing Knowledge-Engineering Methods. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 216?223, Sap-
poro, Japan, July.
M. Collins and Y. Singer. 1999. Unsupervised Models
for Named Entity Classification. In Proceedings of
Joint SIGDAT Conference on Empirical Methods in
Natural Language Processing and Very Large Cor-
pora, pages 100?110, College Park, MD, June.
D. Freitag and A. McCallum. 2000. Informa-
tion Extraction with HMM Structures Learned by
Stochastic Optimization. In Proceedings of the Sev-
enteenth National Conference on Artificial Intelli-
gence, pages 584?589, Austin, TX, August.
F. Keller and M. Lapata. 2003. Using the Web to
Obtain Frequencies for Unseen Bigrams. Compu-
tational Linguistics, 29(3):459?484, September.
C. Manning and H. Schu?tze. 1999. Foundations of
Statistical Natural Language Processing. The MIT
Press, Cambridge, MA.
K. Nigam, A. McCallum, S. Thrun, and T. Mitchell.
2000. Text Classification from Labeled and Un-
labeled Documents using EM. Machine Learning,
39(2-3):103?134, May.
A. Popescu, A. Yates, and O. Etzioni. 2004. Class Ex-
traction from the World Wide Web. In Ion Muslea,
editor, Adaptive Text Extraction and Mining: Papers
from the 2004 AAAI Workshop, pages 68?73, San
Jose, CA, July.
D. Ravichandran and E. Hovy. 2002. Learning Surface
Text Patterns for a Question Answering System. In
Proceedings of the 40th Annual Meeting on Associ-
ation for Computational Linguistics, pages 41?47,
Philadelphia, PA, July.
P. Resnik. 1999. Mining the Web for Bilingual Text.
In Proceedings of the 37th meeting of the Associa-
tion for Computational Linguistics, pages 527?534,
College Park, MD, June.
E. Riloff and R. Jones. 1999. Learning Dictionar-
ies for Information Extraction by Multi-Level Boot-
strapping. In Proceedings of the Sixteenth National
Conference on Artificial Intelligence, pages 474?
479, Orlando, FL, July.
E. Riloff and W. Phillips. 2004. An Introduction to the
Sundance and AutoSlog Systems. Technical Report
UUCS-04-015, School of Computing, University of
Utah.
E. Riloff. 1996. Automatically Generating Extraction
Patterns from Untagged Text. In Proceedings of the
Thirteenth National Conference on Articial Intelli-
gence, pages 1044?1049, Portland, OR, August.
S. Soderland, D. Fisher, J. Aseltine, and W. Lehnert.
1995. CRYSTAL: Inducing a Conceptual Dictio-
nary. In Proceedings of the Fourteenth International
Joint Conference on Artificial Intelligence, pages
1314?1319, Montreal, Canada, August.
S. Soderland. 1999. Learning Information Extraction
Rules for Semi-Structured and Free Text. Machine
Learning, 34(1-3):233?272, February.
B. Sundheim. 1992. Overview of the Fourth Message
Understanding Evaluation and Conference. In Pro-
ceedings of the Fourth Message Understanding Con-
ference (MUC-4), pages 3?21, McLean, VA, June.
B. Sundheim. 1995. Overview of the Results of
the MUC-6 Evaluation. In Proceedings of the
Sixth Message Understanding Conference (MUC-6),
pages 13?31, Columbia, MD, November.
J. Wiebe and E. Riloff. 2005. Creating Subjective
and Objective Sentence Classifiers from Unanno-
tated Texts. In Proceedings of the 6th International
Conference on Computational Linguistics and Intel-
ligent Text Processing, pages 486?497, Mexico City,
Mexico, February.
R. Yangarber, R. Grishman, P. Tapanainen, and S. Hut-
tunen. 2000. Automatic Acquisition of Domain
Knowledge for Information Extraction. In Proceed-
ings of the 18th International Conference on Com-
putational Linguistics, pages 940?946, Saarbru?cken,
Germany, August.
D. Yarowsky. 1995. Unsupervised Word Sense Dis-
ambiguation Rivaling Supervised Methods. In Pro-
ceedings of the 33rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 189?196,
Cambridge, MA, June.
73
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 440?448,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Feature Subsumption for Opinion Analysis
Ellen Riloff and Siddharth Patwardhan
School of Computing
University of Utah
Salt Lake City, UT 84112
{riloff,sidd}@cs.utah.edu
Janyce Wiebe
Department of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260
wiebe@cs.pitt.edu
Abstract
Lexical features are key to many ap-
proaches to sentiment analysis and opin-
ion detection. A variety of representations
have been used, including single words,
multi-word Ngrams, phrases, and lexico-
syntactic patterns. In this paper, we use a
subsumption hierarchy to formally define
different types of lexical features and their
relationship to one another, both in terms
of representational coverage and perfor-
mance. We use the subsumption hierar-
chy in two ways: (1) as an analytic tool
to automatically identify complex features
that outperform simpler features, and (2)
to reduce a feature set by removing un-
necessary features. We show that reduc-
ing the feature set improves performance
on three opinion classification tasks, espe-
cially when combined with traditional fea-
ture selection.
1 Introduction
Sentiment analysis and opinion recognition are ac-
tive research areas that have many potential ap-
plications, including review mining, product rep-
utation analysis, multi-document summarization,
and multi-perspective question answering. Lexi-
cal features are key to many approaches, and a va-
riety of representations have been used, including
single words, multi-word Ngrams, phrases, and
lexico-syntactic patterns. It is common for dif-
ferent features to overlap representationally. For
example, the unigram ?happy? will match all of
the texts that the bigram ?very happy? matches.
Since both features represent a positive sentiment
and the bigram matches fewer contexts than the
unigram, it is probably sufficient just to have the
unigram. However, there are many cases where
a feature captures a subtlety or non-compositional
meaning that a simpler feature does not. For exam-
ple, ?basket case? is a highly opinionated phrase,
but the words ?basket? and ?case? individually
are not. An open question in opinion analysis is
how often more complex feature representations
are needed, and which types of features are most
valuable. Our first goal is to devise a method to
automatically identify features that are represen-
tationally subsumed by a simpler feature but that
are better opinion indicators. These subjective ex-
pressions could then be added to a subjectivity lex-
icon (Esuli and Sebastiani, 2005), and used to gain
understanding about which types of complex fea-
tures capture meaningful expressions that are im-
portant for opinion recognition.
Many opinion classifiers are created by adopt-
ing a ?kitchen sink? approach that throws together
a variety of features. But in many cases adding
new types of features does not improve perfor-
mance. For example, Pang et al (2002) found that
unigrams outperformed bigrams, and unigrams
outperformed the combination of unigrams plus
bigrams. Our second goal is to automatically iden-
tify features that are unnecessary because similar
features provide equal or better coverage and dis-
criminatory value. Our hypothesis is that a re-
duced feature set, which selectively combines un-
igrams with only the most valuable complex fea-
tures, will perform better than a larger feature set
that includes the entire ?kitchen sink? of features.
In this paper, we explore the use of a subsump-
tion hierarchy to formally define the subsump-
tion relationships between different types of tex-
tual features. We use the subsumption hierarchy
in two ways. First, we use subsumption as an an-
440
alytic tool to compare features of different com-
plexities and automatically identify complex fea-
tures that substantially outperform their simpler
counterparts. Second, we use the subsumption hi-
erarchy to reduce a feature set based on represen-
tational overlap and on performance. We conduct
experiments with three opinion data sets and show
that the reduced feature sets can improve classifi-
cation performance.
2 The Subsumption Hierarchy
2.1 Text Representations
We analyze two feature representations that have
been used for opinion analysis: Ngrams and Ex-
traction Patterns. Information extraction (IE)
patterns are lexico-syntactic patterns that rep-
resent expressions which identify role relation-
ships. For example, the pattern ?<subj>
ActVP(recommended)? extracts the subject of
active-voice instances of the verb ?recommended?
as the recommender. The pattern ?<subj>
PassVP(recommended)? extracts the subject of
passive-voice instances of ?recommended? as the
object being recommended.
(Riloff and Wiebe, 2003) explored the idea
of using extraction patterns to represent more
complex subjective expressions that have non-
compositional meanings. For example, the expres-
sion ?drive (someone) up the wall? expresses the
feeling of being annoyed, but the meanings of the
words ?drive?, ?up?, and ?wall? have no emotional
connotations individually. Furthermore, this ex-
pression is not a fixed word sequence that can be
adequately modeled by Ngrams. Any noun phrase
can appear between the words ?drive? and ?up?, so
a flexible representation is needed to capture the
general pattern ?drives <NP> up the wall?.
This example represents a general phenomenon:
many expressions allow intervening noun phrases
and/or modifying terms. For example:
?stepped on <mods> toes?
Ex: stepped on the boss? toes
?dealt <np> <mods> blow?
Ex: dealt the company a decisive blow
?brought <np> to <mods> knees?
Ex: brought the man to his knees
(Riloff and Wiebe, 2003) also showed that syn-
tactic variations of the same verb phrase can be-
have very differently. For example, they found that
passive-voice constructions of the verb ?ask? had
a 100% correlation with opinion sentences, but
active-voice constructions had only a 63% corre-
lation with opinions.
Pattern Type Example Pattern
<subj> PassVP <subj> is satisfied
<subj> ActVP <subj> complained
<subj> ActVP Dobj <subj> dealt blow
<subj> ActInfVP <subj> appear to be
<subj> PassInfVP <subj> is meant to be
<subj> AuxVP Dobj <subj> has position
<subj> AuxVP Adj <subj> is happy
ActVP <dobj> endorsed <dobj>
InfVP <dobj> to condemn <dobj>
ActInfVP <dobj> get to know <dobj>
PassInfVP <dobj> is meant to be <dobj>
Subj AuxVP <dobj> fact is <dobj>
NP Prep <np> opinion on <np>
ActVP Prep <np> agrees with <np>
PassVP Prep <np> is worried about <np>
InfVP Prep <np> to resort to <np>
<possessive> NP <noun>?s speech
Figure 1: Extraction Pattern Types
Our goal is to use the subsumption hierarchy
to identify Ngram and extraction pattern features
that are more strongly associated with opinions
than simpler features. We used three types of fea-
tures in our research: unigrams, bigrams, and IE
patterns. The Ngram features were generated us-
ing the Ngram Statistics Package (NSP) (Baner-
jee and Pedersen, 2003).1 The extraction pat-
terns (EPs) were automatically generated using
the Sundance/AutoSlog software package (Riloff
and Phillips, 2004). AutoSlog relies on the Sun-
dance shallow parser and can be applied exhaus-
tively to a text corpus to generate IE patterns that
can extract every noun phrase in the corpus. Au-
toSlog has been used to learn IE patterns for the
domains of terrorism, joint ventures, and micro-
electronics (Riloff, 1996), as well as for opinion
analysis (Riloff and Wiebe, 2003). Figure 1 shows
the 17 types of extraction patterns that AutoSlog
generates. PassVP refers to passive-voice verb
phrases (VPs), ActVP refers to active-voice VPs,
InfVP refers to infinitive VPs, and AuxVP refers
1NSP is freely available for use under the GPL from
http://search.cpan.org/dist/Text-NSP. We discarded Ngrams
that consisted entirely of stopwords. We used a list of 281
stopwords.
441
to VPs where the main verb is a form of ?to be?
or ?to have?. Subjects (subj), direct objects (dobj),
PP objects (np), and possessives can be extracted
by the patterns.2
2.2 The Subsumption Hierarchy
We created a subsumption hierarchy that defines
the representational scope of different types of fea-
tures. We will say that feature A representation-
ally subsumes feature B if the set of text spans
that match feature A is a superset of the set of text
spans that match feature B. For example, the uni-
gram ?happy? subsumes the bigram ?very happy?
because the set of text spans that match ?happy?
includes the text spans that match ?very happy?.
First, we define a hierarchy of valid subsump-
tion relationships, shown in Figure 2. The 2Gram
node, for example, is a child of the 1Gram node
because a 1Gram can subsume a 2Gram. Ngrams
may subsume extraction patterns as well. Ev-
ery extraction pattern has at least one correspond-
ing 1Gram that will subsume it.3. For example,
the 1Gram ?recommended? subsumes the pattern
?<subj> ActVP(recommended)? because the pat-
tern only matches active-voice instances of ?rec-
ommended?. An extraction pattern may also
subsume another extraction pattern. For exam-
ple, ?<subj> ActVP(recommended)? subsumes
?<subj> ActVP(recommended) Dobj(movie)?.
To compare specific features we need to for-
mally define the representation of each type of
feature in the hierarchy. For example, the hierar-
chy dictates that a 2Gram can subsume the pattern
?ActInfVP <dobj>?, but this should hold only if
the words in the bigram correspond to adjacent
words in the pattern. For example, the 2Gram ?to
fish? subsumes the pattern ?ActInfVP(like to fish)
<dobj>?. But the 2Gram ?like fish? should not
subsume it. Similarly, consider the pattern ?In-
fVP(plan) <dobj>?, which represents the infini-
tive ?to plan?. This pattern subsumes the pattern
?ActInfVP(want to plan) <dobj>?, but it should
not subsume the pattern ?ActInfVP(plan to start)?.
To ensure that different features truly subsume
each other representationally, we formally define
each type of feature based on words, sequential
2However, the items extracted by the patterns are not ac-
tually used by our opinion classifiers; only the patterns them-
selves are matched against the text.
3Because every type of extraction pattern shown in Fig-
ure 1 contains at least one word (not including the extracted
phrases, which are not used as part of our feature representa-
tion).
dependencies, and syntactic dependencies. A se-
quential dependency between words wi and wi+1
means that wi and wi+1 must be adjacent, and that
wi must precede wi+1. Figure 3 shows the formal
definition of a bigram (2Gram) node. The bigram
is defined as two words with a sequential depen-
dency indicating that they must be adjacent.
Name = 2Gram
Constituent[0] = WORD1
Constituent[1] = WORD2
Dependency = Sequential(0, 1)
Figure 3: 2Gram Definition
A syntactic dependency between words wi and
wi+1 means that wi has a specific syntactic rela-
tionship to wi+1, and wi must precede wi+1. For
example, consider the extraction pattern ?NP Prep
<np>?, in which the object of the preposition at-
taches to the NP. Figure 4 shows the definition of
this extraction pattern in the hierarchy. The pat-
tern itself contains three components: the NP, the
attaching preposition, and the object of the prepo-
sition (which is the NP that the pattern extracts).
The definition also includes two syntactic depen-
dencies: the first dependency is between the NP
and the preposition (meaning that the preposition
syntactically attaches to the NP), while the second
dependency is between the preposition and the ex-
traction (meaning that the extracted NP is the syn-
tactic object of the preposition).
Name = NP Prep <np>
Constituent[0] = NP
Constituent[1] = PREP
Constituent[2] = NP EXTRACTION
Dependency = Syntactic(0, 1)
Dependency = Syntactic(1, 2)
Figure 4: ?NP Prep <np>? Pattern Definition
Consequently, the bigram ?affair with? will not
subsume the extraction pattern ?affair with <np>?
because the bigram requires the noun and preposi-
tion to be adjacent but the pattern does not. For ex-
ample, the extraction pattern matches the text ?an
affair in his mind with Countess Olenska? but the
bigram does not. Conversely, the extraction pat-
tern does not subsume the bigram either because
the pattern requires syntactic attachment but the
bigram does not. For example, the bigram matches
442
<subj> ActVP
<subj> ActInfVP
<subj> ActVP Dobj
<subj> PassVP
<subj> PassInfVP
InfVP <dobj>
ActInfVP <dobj>
PassInfVP <dobj>
1Gram
2Gram
<possessive> NP
<subj> AuxVP AdjP
<subj> AuxVP Dobj
ActVP <dobj>
ActVP Prep <np>
NP Prep <np>
PassVP Prep <np>
Subj AuxVP <dobj>
3Gram
ActVP Prep:OF <np>
InfVP Prep <np>
NP Prep:OF <np>
PassVP Prep:OF <np>
4Gram
InfVP Prep:OF <np>
Figure 2: The Subsumption Hierarchy
the sentence ?He ended the affair with a sense of
relief?, but the extraction pattern does not.
Figure 5 shows the definition of another ex-
traction pattern, ?InfVP <dobj>?, which includes
both syntactic and sequential dependencies. This
pattern would match the text ?to protest high
taxes?. The pattern definition has three compo-
nents: the infinitive ?to?, a verb, and the direct ob-
ject of the verb (which is the NP that the pattern
extracts). The definition also shows two syntac-
tic dependencies. The first dependency indicates
that the verb syntactically attaches to the infinitive
?to?. The second dependency indicates that the ex-
tracted NP syntactically attaches to the verb (i.e.,
it is the direct object of that particular verb).
The pattern definition also includes a sequen-
tial dependency, which specifies that ?to? must be
adjacent to the verb. Strictly speaking, our parser
does not require them to be adjacent. For exam-
ple, the parser allows intervening adverbs to split
infinitives (e.g., ?to strongly protest high taxes?),
and this does happen occasionally. But split in-
finitives are relatively rare, so in the vast major-
ity of cases the infinitive ?to? will be adjacent to
the verb. Consequently, we decided that a bigram
(e.g., ?to protest?) should representationally sub-
sume this extraction pattern because the syntac-
tic flexibility afforded by the pattern is negligi-
ble. The sequential dependency link represents
this judgment call that the infinitive ?to? and the
verb are adjacent in most cases.
For all of the node definitions, we used our best
judgment to make decisions of this kind. We tried
to represent major distinctions between features,
without getting caught up in minor differences that
were likely to be negligible in practice.
Name = InfVP <dobj>
Constituent[0] = INFINITIVE TO
Constituent[1] = VERB
Constituent[2] = DOBJ EXTRACTION
Dependency = Syntactic(0, 1)
Dependency = Syntactic(1, 2)
Dependency = Sequential(0, 1)
Figure 5: ?InfVP <dobj>? Pattern Definition
To use the subsumption hierarchy, we assign
each feature to its appropriate node in the hierar-
chy based on its type. Then we perform a top-
down breadth-first traversal. Each feature is com-
pared with the features at its ancestor nodes. If
a feature?s words and dependencies are a superset
of an ancestor?s words and dependencies, then it
is subsumed by the (more general) ancestor and
discarded.4 When the subsumption process is fin-
ished, a feature remains in the hierarchy only if
4The words that they have in common must also be in the
same relative order.
443
there are no features above it that subsume it.
2.3 Performance-based Subsumption
Representational subsumption is concerned with
whether one feature is more general than another.
But the purpose of using the subsumption hier-
archy is to identify more complex features that
outperform simpler ones. Applying the subsump-
tion hierarchy to features without regard to per-
formance would simply eliminate all features that
have a more general counterpart in the feature set.
For example, all bigrams would be discarded if
their component unigrams were also present in the
hierarchy.
To estimate the quality of a feature, we use In-
formation Gain (IG) because that has been shown
to work well as a metric for feature selection (For-
man, 2003). We will say that feature A be-
haviorally subsumes feature B if two criteria are
met: (1) A representationally subsumes B, and (2)
IG(A) ? IG(B) - ?, where ? is a parameter repre-
senting an acceptable margin of performance dif-
ference. For example, if ?=0 then condition (2)
means that feature A is just as valuable as fea-
ture B because its information gain is the same or
higher. If ?>0 then feature A is allowed to be a lit-
tle worse than feature B, but within an acceptable
margin. For example, ?=.0001 means that A?s in-
formation gain may be up to .0001 lower than B?s
information gain, and that is considered to be an
acceptable performance difference (i.e., A is good
enough that we are comfortable discarding B in
favor of the more general feature A).
Note that based on the subsumption hierarchy
shown in Figure 2, all 1Grams will always sur-
vive the subsumption process because they cannot
be subsumed by any other types of features. Our
goal is to identify complex features that are worth
adding to a set of unigram features.
3 Data Sets
We used three opinion-related data sets for our
analyses and experiments: the OP data set created
by (Wiebe et al, 2004), the Polarity data set5 cre-
ated by (Pang and Lee, 2004), and the MPQA data
set created by (Wiebe et al, 2005).6 The OP and
Polarity data sets involve document-level opinion
classification, while the MPQA data set involves
5Version v2.0, which is available at:
http://www.cs.cornell.edu/people/pabo/movie-review-data/
6Available at http://www.cs.pitt.edu/mpqa/databaserelease/
sentence-level classification.
The OP data consists of 2,452 documents from
the Penn Treebank (Marcus et al, 1993). Metadata
tags assigned by the Wall Street Journal define the
opinion/non-opinion classes: the class of any doc-
ument labeled Editorial, Letter to the Editor, Arts
& Leisure Review, or Viewpoint by the Wall Street
Journal is opinion, and the class of documents in
all other categories (such as Business and News)
is non-opinion. This data set is highly skewed,
with only 9% of the documents belonging to the
opinion class. Consequently, a trivial (but useless)
opinion classifier that labels all documents as non-
opinion articles would achieve 91% accuracy.
The Polarity data consists of 700 positive and
700 negative reviews from the Internet Movie
Database (IMDb) archive. The positive and neg-
ative classes were derived from author ratings ex-
pressed in stars or numerical values. The MPQA
data consists of English language versions of ar-
ticles from the world press. It contains 9,732
sentences that have been manually annotated for
subjective expressions. The opinion/non-opinion
classes are derived from the lower-level annota-
tions: a sentence is an opinion if it contains a sub-
jective expression of medium or higher intensity;
otherwise, it is a non-opinion sentence. 55% of the
sentences belong to the opinion class.
4 Using the Subsumption Hierarchy for
Analysis
In this section, we illustrate how the subsump-
tion hierarchy can be used as an analytic tool to
automatically identify features that substantially
outperform simpler counterparts. These features
represent specialized usages and expressions that
would be good candidates for addition to a sub-
jectivity lexicon. Figure 6 shows pairs of features,
where the first is more general and the second is
more specific. These feature pairs were identified
by the subsumption hierarchy as being representa-
tionally similar but behaviorally different (so the
more specific feature was retained). The IGain
column shows the information gain values pro-
duced from the training set of one cross-validation
fold. The Class column shows the class that the
more specific feature is correlated with (the more
general feature is usually not strongly correlated
with either class).
The top table in Figure 6 contains examples for
the opinion/non-opinion classification task from
444
Opinion/Non-Opinion Classification
ID Feature IGain Class Example
A1 line .0016 - . . . issue consists of notes backed by credit line receivables
A2 the line .0075 opin ...lays it on the line; ...steps across the line
B1 nation .0046 - . . . has 750,000 cable-tv subscribers around the nation
B2 a nation .0080 opin It?s not that we are spawning a nation of ascetics . . .
C1 begin .0006 - Campeau buyers will begin writing orders...
C2 begin with .0036 opin To begin with, we should note that in contrast...
D1 benefits .0040 - . . . earlier period included $235,000 in tax benefits.
DEP NP Prep(benefits to) .0090 opin . . . boon to the rich with no proven benefits to the economy
E1 due .0001 - . . . an estimated $ 1.23 billion in debt due next spring
EEP ActVP Prep(due to) .0038 opin It?s all due to the intense scrutiny...
Positive/Negative Sentiment Classification
ID Feature IGain Class Example
F1 short .0014 - to make a long story short...
F2 nothing short .0039 pos nothing short of spectacular
G1 ugly .0008 - ...an ugly monster on a cruise liner
G2 and ugly .0054 neg it?s a disappointment to see something this dumb and ugly
H1 disaster .0010 - ...rated pg-13 for disaster related elements
HEP AuxVP Dobj(be disaster) .0048 neg . . . this is such a confused disaster of a film
I1 work .0002 - the next day during the drive to work...
IEP ActVP(work) .0062 pos the film will work just as well...
J1 manages .0003 - he still manages to find time for his wife
JEP ActInfVP(manages to keep) .0054 pos this film manages to keep up a rapid pace
Figure 6: Sample features that behave differently, as revealed by the subsumption hierarchy.
(1 ? unigram; 2 ? bigram; EP ? extraction pattern)
the OP data. The more specific features are more
strongly correlated with opinion articles. Surpris-
ingly, simply adding a determiner can dramatically
change behavior. Consider A2. There are many
subjective idioms involving ?the line? (two are
shown in the table; others include ?toe the line?
and ?draw the line?), while objective language
about credit lines, phone lines, etc. uses the deter-
miner less often. Similarly, consider B2. Adding
?a? to ?nation? often corresponds to an abstract
reference used when making an argument (e.g.,
?a nation of ascetics?), whereas other instances
of ?nation? are used more literally (e.g., ?the 6th
largest in the nation?). 21% of feature B1?s in-
stances appear in opinion articles, while 70% of
feature B2?s instances are in opinion articles.
?Begin with? (C2) captures an adverbial phrase
used in argumentation (?To begin with...?) but
does not match objective usages such as ?will
begin? an action. The word ?benets? alone
(D1) matches phrases like ?tax benets? and ?em-
ployee benets? that are not opinion expressions,
while DEP typically matches positive senses of
the word ?benets?. Interestingly, the bigram
?benets to? is not highly correlated with opin-
ions because it matches infinitive phrases such
as ?tax benets to provide? and ?health benets
to cut?. In this case, the extraction pattern ?NP
Prep(benefits to)? is more discriminating than the
bigram for opinion classification. The extraction
pattern EEP is also highly correlated with opin-
ions, while the unigram ?due? and the bigram
?due to? are not.
The bottom table in Figure 6 shows feature
pairs identified for their behavioral differences on
the Polarity data set, where the task is to distin-
guish positive reviews from negative reviews. F2
and G2 are bigrams that behave differently from
their component unigrams. The expression ?noth-
ing short (of)? is typically used to express posi-
tive sentiments, while ?nothing? and ?short? by
themselves are not. The word ?ugly? is often used
as a descriptive modifier that is not expressing
a sentiment per se, while ?and ugly? appears in
predicate adjective constructions that are express-
ing a negative sentiment. The extraction pattern
HEP is more discriminatory than H1 because it
distinguishes negative sentiments (?the lm is a
disaster!?) from plot descriptions (?the disaster
movie...?). IEP shows that active-voice usages of
?work? are strong positive indicators, while the
unigram ?work? appears in a variety of both pos-
itive and negative contexts. Finally, JEP shows
that the expression ?manages to keep? is a strong
positive indicator, while ?manages? by itelf is
much less discriminating.
445
These examples illustrate that the subsumption
hierarchy can be a powerful tool to better under-
stand the behaviors of different kinds of features,
and to identify specific features that may be desir-
able for inclusion in specialized lexical resources.
5 Using the Subsumption Hierarchy to
Reduce Feature Sets
When creating opinion classifiers, people often
throw in a variety of features and trust the ma-
chine learning algorithm to figure out how to make
the best use of them. However, we hypothesized
that classifiers may perform better if we can proac-
tively eliminate features that are not necesary be-
cause they are subsumed by other features. In this
section, we present a series of experiments to ex-
plore this hypothesis. First, we present the results
for an SVM classifier trained using different sets
of unigram, bigram, and extraction pattern fea-
tures, both before and after subsumption. Next, we
evaluate a standard feature selection approach as
an alternative to subsumption and then show that
combining subsumption with standard feature se-
lection produces the best results of all.
5.1 Classification Experiments
To see whether feature subsumption can improve
classification performance, we trained an SVM
classifier for each of the three opinion data sets.
We used the SVMlight (Joachims, 1998) package
with a linear kernel. For the Polarity and OP data
we discarded all features that have frequency < 5,
and for the MPQA data we discarded features that
have frequency < 2 because this data set is sub-
stantially smaller. All of our experimental results
are averages over 3-fold cross-validation.
First, we created 4 baseline classifiers: a 1Gram
classifier that uses only the unigram features; a
1+2Gram classifier that uses unigram and bigram
features; a 1+EP classifier that uses unigram and
extraction pattern features, and a 1+2+EP classi-
fier that uses all three types of features. Next, we
created analogous 1+2Gram, 1+EP, and 1+2+EP
classifiers but applied the subsumption hierar-
chy first to eliminate unnecessary features be-
fore training the classifier. We experimented with
three delta values for the subsumption process:
?=.0005, .001, and .002.
Figures 7, 8, and 9 show the results. The sub-
sumption process produced small but consistent
improvements on all 3 data sets. For example, Fig-
ure 8 shows the results on the OP data, where all
of the accuracy values produced after subsumption
(the rightmost 3 columns) are higher than the ac-
curacy values produced without subsumption (the
Base[line] column). For all three data sets, the best
overall accuracy (shown in boldface) was always
achieved after subsumption.
Features Base ?=.0005 ?=.001 ?=.002
1Gram 79.8
1+2Gram 81.2 81.0 81.3 81.0
1+EP 81.7 81.4 81.4 82.0
1+2+EP 81.7 82.3 82.3 82.7
Figure 7: Accuracies on Polarity Data
Features Base ?=.0005 ?=.001 ?=.002
1Gram 97.5 - - -
1+2Gram 98.0 98.7 98.6 98.7
1+EP 97.2 97.8 97.9 97.9
1+2+EP 97.8 98.6 98.7 98.7
Figure 8: Accuracies on OP Data
Features Base ?=.0005 ?=.001 ?=.002
1Gram 74.8
1+2Gram 74.3 74.9 74.6 74.8
1+EP 74.4 74.6 74.6 74.6
1+2+EP 74.4 74.9 74.7 74.6
Figure 9: Accuracies on MPQA Data
We also observed that subsumption had a dra-
matic effect on the F-measure scores on the OP
data, which are shown in Figure 10. The OP data
set is fundamentally different from the other data
sets because it is so highly skewed, with 91% of
the documents belonging to the non-opinion class.
Without subsumption, the classifier was conser-
vative about assigning documents to the opinion
class, achieving F-measure scores in the 82-88
range. After subsumption, the overall accuracy
improved but the F-measure scores increased more
dramatically. These numbers show that the sub-
sumption process produced not only a more ac-
curate classifier, but a more useful classifier that
identifies more documents as being opinion arti-
cles.
For the MPQA data, we get a very small im-
provement of 0.1% (74.8% ? 74.9%) using sub-
sumption. But note that without subsumption the
performance actually decreased when bigrams and
446
Features Base ?=.0005 ?=.001 ?=.002
1Gram 84.5
1+2Gram 88.0 92.5 92.0 92.3
1+EP 82.4 86.9 87.4 87.4
1+2+EP 86.7 91.8 92.5 92.3
Figure 10: F-measures on OP Data
 97.6
 97.8
 98
 98.2
 98.4
 98.6
 98.8
 99
 1000  2000  3000  4000  5000  6000  7000  8000  9000  10000
Ac
cu
ra
cy
 (%
)
Top N
Baseline
Subsumption ?=0.002
Feature Selection
Subsumption ?=0.002 + Feature Selection
Figure 11: Feature Selection on OP Data
extraction patterns were added! The subsumption
process counteracted the negative effect of adding
the more complex features.
5.2 Feature Selection Experiments
We conducted a second series of experiments to
determine whether a traditional feature selection
approach would produce the same, or better, im-
provements as subsumption. For each feature, we
computed its information gain (IG) and then se-
lected the N features with the highest scores.7 We
experimented with values of N ranging from 1,000
to 10,000 in increments of 1,000.
We hypothesized that applying subsumption be-
fore traditional feature selection might also help to
identify a more diverse set of high-performing fea-
tures. In a parallel set of experiments, we explored
this hypothesis by first applying subsumption to
reduce the size of the feature set, and then select-
ing the best N features using information gain.
Figures 11, 12, and 13 show the results of these
experiments for the 1+2+EP classifiers. Each
graph shows four lines. One line corresponds to
the baseline classifier with no subsumption, and
another line corresponds to the baseline classifier
with subsumption using the best ? value for that
data set. Each of these two lines corresponds to
7In the case of ties, we included all features with the same
score as the Nth-best as well.
 78
 78.5
 79
 79.5
 80
 80.5
 81
 81.5
 82
 82.5
 83
 83.5
 1000  2000  3000  4000  5000  6000  7000  8000  9000  10000
Ac
cu
ra
cy
 (%
)
Top N
Baseline
Subsumption ?=0.002
Feature Selection
Subsumption ?=0.002 + Feature Selection
Figure 12: Feature Selection on Polarity Data
 72
 72.5
 73
 73.5
 74
 74.5
 75
 75.5
 1000  2000  3000  4000  5000  6000  7000  8000  9000  10000
Ac
cu
ra
cy
 (%
)
Top N
Baseline
Subsumption ?=0.0005
Feature Selection
Subsumption ?=0.0005 + Feature Selection
Figure 13: Feature Selection on MPQA Data
just a single data point (accuracy value), but we
drew that value as a line across the graph for the
sake of comparison. The other two lines on the
graph correspond to (a) feature selection for dif-
ferent values of N (shown on the x-axis), and (b)
subsumption followed by feature selection for dif-
ferent values of N.
On all 3 data sets, traditional feature selection
performs worse than the baseline in some cases,
and it virtually never outperforms the best classi-
fier trained after subsumption (but without feature
selection). Furthermore, the combination of sub-
sumption plus feature selection generally performs
best of all, and nearly always outperforms feature
selection alone. For all 3 data sets, our best ac-
curacy results were achieved by performing sub-
sumption prior to feature selection. The best accu-
racy results are 99.0% on the OP data, 83.1% on
the Polarity data, and 75.4% on the MPQA data.
For the OP data, the improvement over baseline
for both accuracy and F-measure are statistically
significant at the p < 0.05 level (paired t-test). For
the MPQA data, the improvement over baseline is
447
statistically significant at the p < 0.10 level.
6 Related Work
Many features and classification algorithms have
been explored in sentiment analysis and opinion
recognition. Lexical cues of differing complexi-
ties have been used, including single words and
Ngrams (e.g., (Mullen and Collier, 2004; Pang et
al., 2002; Turney, 2002; Yu and Hatzivassiloglou,
2003; Wiebe et al, 2004)), as well as phrases
and lexico-syntactic patterns (e.g, (Kim and Hovy,
2004; Hu and Liu, 2004; Popescu and Etzioni,
2005; Riloff and Wiebe, 2003; Whitelaw et al,
2005)). While many of these studies investigate
combinations of features and feature selection,
this is the first work that uses the notion of sub-
sumption to compare Ngrams and lexico-syntactic
patterns to identify complex features that outper-
form simpler counterparts and to reduce a com-
bined feature set to improve opinion classification.
7 Conclusions
This paper uses a subsumption hierarchy of
feature representations as (1) an analytic tool
to compare features of different complexities,
and (2) an automatic tool to remove unneces-
sary features to improve opinion classification
performance. Experiments with three opinion
data sets showed that subsumption can improve
classification accuracy, especially when combined
with feature selection.
Acknowledgments
This research was supported by NSF Grants IIS-
0208798 and IIS-0208985, the ARDA AQUAINT
Program, and the Institute for Scientific Comput-
ing Research and the Center for Applied Scientific
Computing within Lawrence Livermore National
Laboratory.
References
S. Banerjee and T. Pedersen. 2003. The Design, Imple-
mentation, and Use of the Ngram Statistics Package.
In Proc. Fourth Int?l Conference on Intelligent Text
Processing and Computational Linguistics.
A. Esuli and F. Sebastiani. 2005. Determining the se-
mantic orientation of terms through gloss analysis.
In Proc. CIKM-05.
G. Forman. 2003. An Extensive Empirical Study of
Feature Selection Metrics for Text Classification. J.
Mach. Learn. Res., 3:1289?1305.
M. Hu and B. Liu. 2004. Mining and summarizing
customer reviews. In Proc. KDD-04.
T. Joachims. 1998. Making Large-Scale Support
Vector Machine Learning Practical. In A. Smola
B. Scho?lkopf, C. Burges, editor, Advances in Ker-
nel Methods: Support Vector Machines. MIT Press,
Cambridge, MA.
S-M. Kim and E. Hovy. 2004. Determining the senti-
ment of opinions. In Proc. COLING-04.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a Large Annotated Corpus of English:
The Penn Treebank. Computational Linguistics,
19(2):313?330.
T. Mullen and N. Collier. 2004. Sentiment Analysis
Using Support Vector Machines with Diverse Infor-
mation Sources. In Proc. EMNLP-04.
B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proc. ACL-04.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? Sentiment Classification using Machine Learn-
ing Techniques. In Proc. EMNLP-02.
A-M. Popescu and O. Etzioni. 2005. Extracting prod-
uct features and opinions from reviews. In Proc.
HLT-EMNLP-05.
E. Riloff and W. Phillips. 2004. An Introduction to the
Sundance and AutoSlog Systems. Technical Report
UUCS-04-015, School of Computing, University of
Utah.
E. Riloff and J. Wiebe. 2003. Learning Extraction Pat-
terns for Subjective Expressions. In Proc. EMNLP-
03.
E. Riloff. 1996. An Empirical Study of Automated
Dictionary Construction for Information Extraction
in Three Domains. Artificial Intelligence, 85:101?
134.
P. Turney. 2002. Thumbs up or thumbs down? Seman-
tic orientation applied to unsupervised classification
of reviews. In Proc. ACL-02.
C. Whitelaw, N. Garg, and S. Argamon. 2005. Us-
ing appraisal groups for sentiment analysis. In Proc.
CIKM-05.
J. Wiebe, T. Wilson, R. Bruce, M. Bell, and M. Mar-
tin. 2004. Learning subjective language. Computa-
tional Linguistics, 30(3):277?308.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating
expressions of opinions and emotions in language.
Language Resources and Evaluation, 39(2/3).
H. Yu and V. Hatzivassiloglou. 2003. Towards an-
swering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Proc. EMNLP-03.
448
Proceedings of the NAACL HLT Workshop on Unsupervised and Minimally Supervised Learning of Lexical Semantics, pages 18?26,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Corpus-based Semantic Lexicon Induction with Web-based Corroboration
Sean P. Igo
Center for High Performance Computing
University of Utah
Salt Lake City, UT 84112 USA
Sean.Igo@utah.edu
Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT 84112 USA
riloff@cs.utah.edu
Abstract
Various techniques have been developed to au-
tomatically induce semantic dictionaries from
text corpora and from the Web. Our research
combines corpus-based semantic lexicon in-
duction with statistics acquired from the Web
to improve the accuracy of automatically ac-
quired domain-specific dictionaries. We use
a weakly supervised bootstrapping algorithm
to induce a semantic lexicon from a text cor-
pus, and then issue Web queries to generate
co-occurrence statistics between each lexicon
entry and semantically related terms. The Web
statistics provide a source of independent ev-
idence to confirm, or disconfirm, that a word
belongs to the intended semantic category. We
evaluate this approach on 7 semantic cate-
gories representing two domains. Our results
show that the Web statistics dramatically im-
prove the ranking of lexicon entries, and can
also be used to filter incorrect entries.
1 Introduction
Semantic resources are extremely valuable for many
natural language processing (NLP) tasks, as evi-
denced by the wide popularity of WordNet (Miller,
1990) and a multitude of efforts to create similar
?WordNets? for additional languages (e.g. (Atserias
et al, 1997; Vossen, 1998; Stamou et al, 2002)).
Semantic resources can take many forms, but one of
the most basic types is a dictionary that associates
a word (or word sense) with one or more semantic
categories (hypernyms). For example, truck might
be identified as a VEHICLE, and dog might be identi-
fied as an ANIMAL. Automated methods for generat-
ing such dictionaries have been developed under the
rubrics of lexical acquisition, hyponym learning, se-
mantic class induction, and Web-based information
extraction. These techniques can be used to rapidly
create semantic lexicons for new domains and lan-
guages, and to automatically increase the coverage
of existing resources.
Techniques for semantic lexicon induction can be
subdivided into two groups: corpus-based methods
and Web-based methods. Although the Web can be
viewed as a (gigantic) corpus, these two approaches
tend to have different goals. Corpus-based methods
are typically designed to induce domain-specific se-
mantic lexicons from a collection of domain-specific
texts. In contrast, Web-based methods are typically
designed to induce broad-coverage resources, simi-
lar to WordNet. Ideally, one would hope that broad-
coverage resources would be sufficient for any do-
main, but this is often not the case. Many domains
use specialized vocabularies and jargon that are not
adequately represented in broad-coverage resources
(e.g., medicine, genomics, etc.). Furthermore, even
relatively general text genres, such as news, con-
tain subdomains that require extensive knowledge
of specific semantic categories. For example, our
work uses a corpus of news articles about terror-
ism that includes many arcane weapon terms (e.g.,
M-79, AR-15, an-fo, and gelignite). Similarly, our
disease-related documents mention obscure diseases
(e.g., psittacosis) and contain many informal terms,
abbreviations, and spelling variants that do not even
occur in most medical dictionaries. For example, yf
refers to yellow fever, tularaemia is an alternative
spelling for tularemia, and nv-cjd is frequently used
18
to refer to new variant Creutzfeldt Jacob Disease.
The Web is such a vast repository of knowledge
that specialized terminology for nearly any domain
probably exists in some niche or cranny, but find-
ing the appropriate corner of the Web to tap into is a
challenge. You have to know where to look to find
specialized knowledge. In contrast, corpus-based
methods can learn specialized terminology directly
from a domain-specific corpus, but accuracy can be
a problem because most corpora are relatively small.
In this paper, we seek to exploit the best of both
worlds by combining a weakly supervised corpus-
based method for semantic lexicon induction with
statistics obtained from the Web. First, we use
a bootstrapping algorithm, Basilisk (Thelen and
Riloff, 2002), to automatically induce a semantic
lexicon from a domain-specific corpus. This pro-
duces a set of words that are hypothesized to be-
long to the targeted semantic category. Second, we
use the Web as a source of corroborating evidence
to confirm, or disconfirm, whether each term truly
belongs to the semantic category. For each candi-
date word, we search the Web for pages that con-
tain both the word and a semantically related term.
We expect that true semantic category members will
co-occur with semantically similar words more of-
ten than non-members.
This paper is organized as follows. Section 2 dis-
cusses prior work on weakly supervised methods for
semantic lexicon induction. Section 3 overviews
our approach: we briefly describe the weakly su-
pervised bootstrapping algorithm that we use for
corpus-based semantic lexicon induction, and then
present our procedure for gathering corroborating
evidence from the Web. Section 4 presents exper-
imental results on seven semantic categories repre-
senting two domains: Latin American terrorism and
disease-related documents. Section 5 summarizes
our results and discusses future work.
2 Related Work
Our research focuses on semantic lexicon induc-
tion, where the goal is to create a list of words
that belong to a desired semantic class. A sub-
stantial amount of previous work has been done on
weakly supervised and unsupervised creation of se-
mantic lexicons. Weakly supervised corpus-based
methods have utilized noun co-occurrence statis-
tics (Riloff and Shepherd, 1997; Roark and Char-
niak, 1998), syntactic information (Widdows and
Dorow, 2002; Phillips and Riloff, 2002; Pantel and
Ravichandran, 2004; Tanev and Magnini, 2006),
and lexico-syntactic contextual patterns (e.g., ?re-
sides in <location>? or ?moved to <location>?)
(Riloff and Jones, 1999; Thelen and Riloff, 2002).
Due to the need for POS tagging and/or parsing,
these types of methods have been evaluated only
on fixed corpora1, although (Pantel et al, 2004)
demonstrated how to scale up their algorithms for
the Web. The goal of our work is to improve upon
corpus-based bootstrapping algorithms by using co-
occurrence statistics obtained from the Web to re-
rank and filter the hypothesized category members.
Techniques for semantic class learning have also
been developed specifically for the Web. Sev-
eral Web-based semantic class learners build upon
Hearst?s early work (Hearst, 1992) with hyponym
patterns. Hearst exploited patterns that explicitly
identify a hyponym relation between a semantic
class and a word (e.g., ?such authors as <X>?) to
automatically acquire new hyponyms. (Pas?ca, 2004)
applied hyponym patterns to the Web and learned se-
mantic class instances and groups by acquiring con-
texts around the patterns. Later, (Pasca, 2007) cre-
ated context vectors for a group of seed instances by
searching Web query logs, and used them to learn
similar instances. The KnowItAll system (Etzioni
et al, 2005) also uses hyponym patterns to extract
class instances from the Web and evaluates them fur-
ther by computing mutual information scores based
on Web queries. (Kozareva et al, 2008) proposed
the use of a doubly-anchored hyponym pattern and
a graph to represent the links between hyponym oc-
currences in these patterns.
Our work builds upon Turney?s work on seman-
tic orientation (Turney, 2002) and synonym learning
(Turney, 2001), in which he used a PMI-IR algo-
rithm to measure the similarity of words and phrases
based on Web queries. We use a similar PMI (point-
wise mutual information) metric for the purposes of
semantic class verification.
There has also been work on fully unsupervised
1Meta-bootstrapping (Riloff and Jones, 1999) was evaluated
on Web pages, but used a precompiled corpus of downloaded
Web pages.
19
semantic clustering (e.g., (Lin, 1998; Lin and Pan-
tel, 2002; Davidov and Rappoport, 2006; Davidov et
al., 2007)), however clustering methods may or may
not produce the types and granularities of seman-
tic classes desired by a user. Another related line
of work is automated ontology construction, which
aims to create lexical hierarchies based on semantic
classes (e.g., (Caraballo, 1999; Cimiano and Volker,
2005; Mann, 2002)).
3 Semantic Lexicon Induction with
Web-based Corroboration
Our approach combines a weakly supervised learn-
ing algorithm for corpus-based semantic lexicon in-
duction with a follow-on procedure that gathers cor-
roborating statistical evidence from the Web. In
this section, we describe both of these components.
First, we give a brief overview of the Basilisk boot-
strapping algorithm that we use for corpus-based se-
mantic lexicon induction. Second, we present our
new strategies for acquiring and utilizing corrobo-
rating statistical evidence from the Web.
3.1 Corpus-based Semantic Lexicon Induction
via Bootstrapping
For corpus-based semantic lexicon induction, we
use a weakly supervised bootstrapping algorithm
called Basilisk (Thelen and Riloff, 2002). As in-
put, Basilisk requires a small set of seed words for
each semantic category, and a collection of (unanno-
tated) texts. Basilisk iteratively generates new words
that are hypothesized to belong to the same seman-
tic class as the seeds. Here we give an overview of
Basilisk?s algorithm and refer the reader to (Thelen
and Riloff, 2002) for more details.
The key idea behind Basilisk is to use pattern con-
texts around a word to identify its semantic class.
Basilisk?s bootstrapping process has two main steps:
Pattern Pool Creation and Candidate Word Selec-
tion. First, Basilisk applies the AutoSlog pattern
generator (Riloff, 1996) to create a set of lexico-
syntactic patterns that, collectively, can extract every
noun phrase in the corpus. Basilisk then ranks the
patterns according to how often they extract the seed
words, under the assumption that patterns which ex-
tract known category members are likely to extract
other category members as well. The highest-ranked
patterns are placed in a pattern pool.
Second, Basilisk gathers every noun phrase that is
extracted by at least one pattern in the pattern pool,
and designates each head noun as a candidate for the
semantic category. The candidates are then scored
and ranked. For each candidate, Basilisk collects all
of the patterns that extracted that word, computes the
logarithm of the number of seeds extracted by each
of those patterns, and finally computes the average
of these log values as the score for the candidate.
Intuitively, a candidate word receives a high score
if it was extracted by patterns that, on average, also
extract many known category members.
The N highest ranked candidates are automati-
cally added to the list of seed words, taking a leap
of faith that they are true members of the semantic
category. The bootstrapping process then repeats,
using the larger set of seed words as known category
members in the next iteration.
Basilisk learns many good category members,
but its accuracy varies a lot across semantic cate-
gories (Thelen and Riloff, 2002). One problem with
Basilisk, and bootstrapping algorithms in general, is
that accuracy tends to deteriorate as bootstrapping
progresses. Basilisk generates candidates by iden-
tifying the contexts in which they occur and words
unrelated to the desired category can sometimes also
occur in those contexts. Some patterns consistently
extract members of several semantic classes; for ex-
ample, ?attack on <NP>? will extract both people
(?attack on the president?) and buildings (?attack
on the U.S. embassy?). Idiomatic expressions and
parsing errors can also lead to undesirable words be-
ing learned. Incorrect words tend to accumulate as
bootstrapping progresses, which can lead to gradu-
ally deteriorating performance.
(Thelen and Riloff, 2002) tried to address this
problem by learning multiple semantic categories si-
multaneously. This helps to keep the bootstrapping
focused by flagging words that are potentially prob-
lematic because they are strongly associated with a
competing category. This improved Basilisk?s accu-
racy, but by a relatively small amount, and this ap-
proach depends on the often unrealistic assumption
that a word cannot belong to more than one seman-
tic category. In our work, we use the single-category
version of Basilisk that learns each semantic cate-
gory independently so that we do not need to make
20
this assumption.
3.2 Web-based Semantic Class Corroboration
The novel aspect of our work is that we introduce a
new mechanism to independently verify each candi-
date word?s category membership using the Web as
an external knowledge source. We gather statistics
from the Web to provide evidence for (or against)
the semantic class of a word in a manner completely
independent of Basilisk?s criteria. Our approach
is based on the distributional hypothesis (Harris,
1954), which says that words that occur in the same
contexts tend to have similar meanings. We seek to
corroborate a word?s semantic class through statis-
tics that measure how often the word co-occurs with
semantically related words.
For each candidate word produced by Basilisk, we
construct a Web query that pairs the word with a se-
mantically related word. Our goal is not just to find
Web pages that contain both terms, but to find Web
pages that contain both terms in close proximity to
one another. We consider two terms to be collo-
cated if they occur within ten words of each other
on the same Web page, which corresponds to the
functionality of the NEAR operator used by the Al-
taVista search engine2. Turney (Turney, 2001; Tur-
ney, 2002) reported that the NEAR operator outper-
formed simple page co-occurrence for his purposes;
our early experiments informally showed the same
for this work.
We want our technique to remain weakly super-
vised, so we do not want to require additional hu-
man input or effort beyond what is already required
for Basilisk. With this in mind, we investigated two
types of collocation relations as possible indicators
of semantic class membership:
Hypernym Collocation: We compute co-
occurrence statistics between the candidate word
and the name of the targeted semantic class (i.e.,
the word?s hypothesized hypernym). For example,
given the candidate word jeep and the semantic
category VEHICLE, we would issue the Web query
?jeep NEAR vehicle?. Our intuition is that such
queries would identify definition-type Web hits.
For example, the query ?cow NEAR animal? might
retrieve snippets such as ?A cow is an animal found
2http://www.altavista.com
on dairy farms? or ?An animal such as a cow
has...?.
Seed Collocation: We compute co-occurrence
statistics between the candidate word and each seed
word that was given to Basilisk as input. For ex-
ample, given the candidate word jeep and the seed
word truck, we would issue the Web query ?jeep
NEAR truck?. Here the intuition is that members of
the same semantic category tend to occur near one
another - in lists, for example.
As a statistical measure of co-occurrence, we
compute a variation of Pointwise Mutual Informa-
tion (PMI), which is defined as:
PMI(x, y) = log( p(x,y)p(x)?p(y) )
where p(x, y) is the probability that x and y are col-
located (near each other) on a Web page, p(x) is the
probability that x occurs on a Web page, and p(y) is
the probability that y occurs on a Web page.
p(x) is calculated as count(x)N , where count(x) is
the number of hits returned by AltaVista, searching
for x by itself, and N is the total number of docu-
ments on the World Wide Web at the time the query
is made. Similarly, p(x, y) is count(x NEAR y)N .
Given this, the PMI equation can be rewritten as:
log(N) + log( count(x NEAR y)count(x)?count(y) )
N is not known, but it is the same for every
query (assuming the queries were made at roughly
the same time). We will use these scores solely to
compare the relative goodness of candidates, so we
can omit N from the equation because it will not
change the relative ordering of the scores. Thus, our
PMI score3 for a candidate word and related term
(hypernym or seed) is:
log( count(x NEAR y)count(x)?count(y) )
Finally, we created three different scoring func-
tions that use PMI values in different ways to cap-
ture different types of co-occurrence information:
Hypernym Score: PMI based on collocation be-
tween the hypernym term and candidate word.
3In the rare cases when a term had a zero hit count, we as-
signed -99999 as the PMI score, which effectively ranks it at the
bottom.
21
Average of Seeds Score: The mean of the PMI
scores computed for the candidate and each
seed word:
1
|seeds|
|seeds|?
i=1
PMI(candidate, seedi)
Max of Seeds Score: The maximum (highest) of
the PMI scores computed for the candidate and
each seed word.
The rationale for the Average of Seeds Score is
that the seeds are all members of the semantic cat-
egory, so we might expect other members to occur
near many of them. Averaging over all of the seeds
can diffuse unusually high or low collocation counts
that might result from an anomalous seed. The ra-
tionale for the Max of Seeds Score is that a word
may naturally co-occur with some category mem-
bers more often than others. For example, one would
expect dog to co-occur with cat much more fre-
quently than with frog. A high Max of Seeds Score
indicates that there is at least one seed word that fre-
quently co-occurs with the candidate.
Since Web queries are relatively expensive, it is
worth taking stock of how many queries are nec-
essary. Let N be the number of candidate words
produced by Basilisk, and S be the number of
seed words given to Basilisk as input. To com-
pute the Hypernym Score for a candidate, we need
3 queries: count(hypernym), count(candidate),
and count(hypernym NEAR candidate). The
first query is the same for all candidates, so for N
candidate words we need 2N + 1 queries in total.
To compute the Average or Max of Seeds Score for
a candidate, we need S queries for count(seedi), S
queries for count(seedi NEAR candidate), and 1
query for count(candidate). So for N candidate
words we need N ? (2S + 1) queries. S is typically
small for weakly supervised algorithms (S=10 in our
experiments), which means that this Web-based cor-
roboration process requires O(N) queries to process
a semantic lexicon of size N .
4 Evaluation
4.1 Data Sets
We ran experiments on two corpora: 1700 MUC-4
terrorism articles (MUC-4 Proceedings, 1992) and
a combination of 6000 disease-related documents,
consisting of 2000 ProMed disease outbreak re-
ports (ProMed-mail, 2006) and 4000 disease-related
PubMed abstracts (PubMed, 2009). For the terror-
ism domain, we created lexicons for four semantic
categories: BUILDING, HUMAN, LOCATION, and
WEAPON. For the disease domain, we created lexi-
cons for three semantic categories: ANIMAL4, DIS-
EASE, and SYMPTOM. For each category, we gave
Basilisk 10 seed words as input. The seeds were
chosen by applying a shallow parser to each corpus,
extracting the head nouns of all the NPs, and sort-
ing the nouns by frequency. A human then walked
down the sorted list and identified the 10 most fre-
quent nouns that belong to each semantic category5.
This strategy ensures that the bootstrapping process
is given seed words that occur in the corpus with
high frequency. The seed words are shown in Ta-
ble 1.
BUILDING: embassy office headquarters church
offices house home residence hospital airport
HUMAN: people guerrillas members troops
Cristiani rebels president terrorists soldiers leaders
LOCATION: country El Salvador Salvador
United States area Colombia city countries
department Nicaragua
WEAPON: weapons bomb bombs explosives arms
missiles dynamite rifles materiel bullets
ANIMAL: bird mosquito cow horse pig chicken
sheep dog deer fish
DISEASE: SARS BSE anthrax influenza WNV
FMD encephalitis malaria pneumonia flu
SYMPTOM: fever diarrhea vomiting rash paralysis
weakness necrosis chills headaches hemorrhage
Table 1: Seed Words
To evaluate our results, we used the gold standard
answer key that Thelen & Riloff created to evaluate
Basilisk on the MUC4 corpus (Thelen and Riloff,
2002); they manually labeled every head noun in the
corpus with its semantic class. For the ProMed /
PubMed disease corpus, we created our own answer
key. For all of the lexicon entries hypothesized by
Basilisk, a human annotator (not any of the authors)
4ANIMAL was chosen because many of the ProMed disease
outbreak stories concerned outbreaks among animal popula-
tions.
5The disease domain seed words were chosen from a larger
set of ProMed documents, which included the 2000 used for
lexicon induction.
22
BUILDING HUMAN LOCATION WEAPON
N Ba Hy Av Mx Ba Hy Av Mx Ba Hy Av Mx Ba Hy Av Mx
25 .40 .56 .52 .56 .40 .72 .80 .84 .68 .88 .88 1.0 .56 .84 1.0 1.0
50 .44 .56 .46 .40 .56 .80 .88 .86 .80 .86 .84 .98 .52 .74 .76 .90
75 .44 .45 .41 .39 .65 .84 .85 .85 .80 .88 .80 .99 .52 .63 .65 .79
100 .42 .41 .38 .36 .69 .81 .80 .87 .81 .85 .78 .95 .55 .55 .56 .63
300 .22 .82 .75 .26
ANIMAL DISEASE SYMPTOM
N Ba Hy Av Mx Ba Hy Av Mx Ba Hy Av Mx
25 .48 .88 .92 .92 .64 .84 .80 .84 .64 .84 .92 .80
50 .58 .82 .84 .80 .72 .84 .60 .82 .62 .76 .90 .74
75 .55 .68 .67 .69 .69 .83 .59 .81 .61 .68 .79 .71
100 .45 .55 .54 .57 .69 .78 .58 .80 .59 .71 .77 .64
300 .20 .62 .38
Table 2: Ranking results for 7 semantic categories, showing accuracies for the top-ranked N words.
(Ba=Basilisk, Hy=Hypernym Re-ranking, Av=Average of Seeds Re-ranking, Mx=Max of Seeds Re-ranking
labeled each word as either correct or incorrect for
the hypothesized semantic class. A word is consid-
ered to be correct if any sense of the word is seman-
tically correct.
4.2 Ranking Results
We ran Basilisk for 60 iterations, learning 5 new
words in each bootstrapping cycle, which produced
a lexicon of 300 words for each semantic category.
The columns labeled Ba in Table 2 show the accu-
racy results for Basilisk.6 As we explained in Sec-
tion 3.1, accuracy tends to decrease as bootstrapping
progresses, so we computed accuracy scores for the
top-ranked 100 words, in increments of 25, and also
for the entire lexicon of 300 words.
Overall, we see that Basilisk learns many cor-
rect words for each semantic category, and the top-
ranked terms are generally more accurate than the
lower-ranked terms. For the top 100 words, accu-
racies are generally in the 50-70% range, except for
LOCATION which achieves about 80% accuracy. For
the HUMAN category, Basilisk obtained 82% accu-
racy over all 300 words, but the top-ranked words
actually produced lower accuracy.
Basilisk?s ranking is clearly not as good as it could
be because there are correct terms co-mingled with
incorrect terms throughout the ranked lists. This has
6These results are not comparable to the Basilisk results re-
ported by (Thelen and Riloff, 2002) because our implementa-
tion only does single-category learning while the results in that
paper are based on simultaneously learning multiple categories.
two ramifications. First, if we want a human to man-
ually review each lexicon before adding the words
to an external resource, then the rankings may not
be very helpful (i.e., the human will need to review
all of the words), and (2) incorrect terms generated
during the early stages of bootstrapping may be hin-
dering the learning process because they introduce
noise during bootstrapping. The HUMAN category
seems to have recovered from early mistakes, but
the lower accuracies for some other categories may
be the result of this problem. The purpose of our
Web-based corroboration process is to automatically
re-evaluate the lexicons produced by Basilisk, using
Web-based statistics to create more separation be-
tween the good entries and the bad ones.
Our first set of experiments uses the Web-based
co-occurrence statistics to re-rank the lexicon en-
tries. The Hy, Av, and Mx columns in Ta-
ble 2 show the re-ranking results using each of the
Hypernym, Average of Seeds, and Maximum of
Seeds scoring functions. In all cases, Web-based
re-ranking outperforms Basilisk?s original rank-
ings. Every semantic category except for BUILDING
yielded accuracies of 80-100% among the top can-
didates. For each row, the highest accuracy for each
semantic category is shown in boldface (as are any
tied for highest).
Overall, the Max of Seeds Scores were best, per-
forming better than or as well as the other scoring
functions on 5 of the 7 categories. It was only out-
23
BUILDING HUMAN LOCATION WEAPON ANIMAL DISEASE SYMPTOM
consulate guerrilla San Salvador shotguns bird-to-bird meningo-encephalitis nausea
pharmacies extremists Las Hojas carbines cervids bse).austria diarrhoea
aiport sympathizers Tejutepeque armaments goats inhalational myalgias
zacamil assassins Ayutuxtepeque revolvers ewes anthrax disease chlorosis
airports patrols Copinol detonators ruminants otitis media myalgia
parishes militiamen Cuscatancingo pistols swine airport malaria salivation
Masariegos battalion Jiboa car bombs calf taeniorhynchus dysentery
chancery Ellacuria Chinameca calibers lambs hyopneumonia cramping
residences rebel Zacamil M-16 wolsington monkeypox dizziness
police station policemen Chalantenango grenades piglets kala-azar inappetance
Table 3: Top 10 words ranked by Max of Seeds Scores.
performed once by the Hypernym Scores (BUILD-
ING) and once by the Average of Seeds Scores
(SYMPTOM).
The strong performance of the Max of Seeds
scores suggests that one seed is often an especially
good collocation indicator for category membership
? though it may not be the same seed word for all of
the lexicon words. The relatively poor performance
of the Average of Seeds scores may be attributable
to the same principle; perhaps even if one seed is
especially strong, averaging over the less-effective
seeds? scores dilutes the results. Averaging is also
susceptible to damage from words that receive the
special-case score of -99999 when a hit count is zero
(see Section 3.2).
Table 3 shows the 10 top-ranked candidates for
each semantic category based on the Max of Seeds
scores. The table illustrates that this scoring func-
tion does a good job of identifying semantically cor-
rect words, although of course there are some mis-
takes. Mistakes can happen due to parsing errors
(e.g., bird-to-bird is an adjective and not a noun, as
in bird-to-bird transmission), and some are due to
issues associated with Web querying. For exam-
ple, the nonsense term ?bse).austria? was ranked
highly because Altavista split this term into 2 sep-
arate words because of the punctuation, and bse by
itself is indeed a disease term (bovine spongiform
encephalitis).
4.3 Filtering Results
Table 2 revealed that the 300-word lexicons pro-
duced by Basilisk vary widely in the number of true
category words that they contain. The least dense
category is ANIMAL, with only 61 correct words,
and the most dense is HUMAN with 247 correct
words. Interestingly, the densest categories are not
always the easiest to rank. For example, the HU-
MAN category is the densest category but Basilisk?s
ranking of the human terms was poor.
? Category Acc Cor/Tot
-22
WEAPON .88 46/52
LOCATION .98 59/60
HUMAN .80 8/10
BUILDING .83 5/6
ANIMAL .91 30/33
DISEASE .82 64/78
SYMPTOM .65 64/99
-23
WEAPON .79 59/75
LOCATION .96 82/85
HUMAN .85 23/27
BUILDING .71 12/17
ANIMAL .87 40/46
DISEASE .78 82/105
SYMPTOM .62 86/139
-24
WEAPON .63 63/100
LOCATION .93 111/120
HUMAN .87 54/62
BUILDING .45 17/38
ANIMAL .75 47/63
DISEASE .74 94/127
SYMPTOM .60 100/166
Table 4: Filtering results using the Max of Seeds Scores.
The ultimate goal behind a better ranking mech-
anism is to completely automate the process of se-
mantic lexicon induction. If we can produce high-
quality rankings, then we can discard the lower
ranked words and keep only the highest ranked
words for our semantic dictionary. However, this
24
presupposes that we know where to draw the line be-
tween the good and bad entries, and Table 2 shows
that this boundary varies across categories. For HU-
MANS, the top 100 words are 87% accurate, and in
fact we get 82% accuracy over all 300 words. But
for ANIMALS we achieve 80% accuracy only for the
top 50 words. It is paramount for semantic dictio-
naries to have high integrity, so accuracy must be
high if we want to use the resulting lexicons without
manual review.
As an alternative to ranking, another way that we
could use the Web-based corroboration statistics is
to automatically filter words that do not receive a
high score. The key question is whether the values
of the scores are consistent enough across categories
to set a single threshold that will work well across
the different categories.
Table 4 shows the results of using the Max of
Seeds Scores as a filtering mechanism: given a
threshold ?, all words that have a score < ? are dis-
carded. For each threshold value ? and semantic cat-
egory, we computed the accuracy (Acc) of the lex-
icon after all words with a score < ? have been re-
moved. The Cor/Tot column shows the number of
correct category members and the number of total
words that passed the threshold.
We experimented with a variety of threshold val-
ues and found that ?=-22 performed best. Table 4
shows that this threshold produces a relatively high-
precision filtering mechanism, with 6 of the 7 cat-
egories achieving lexicon accuracies ? 80%. As
expected, the Cor/Tot column shows that the num-
ber of words varies widely across categories. Au-
tomatic filtering represents a trade-off: a relatively
high-precision lexicon can be created, but some cor-
rect words will be lost. The threshold can be ad-
justed to increase the number of learned words, but
with a corresponding drop in precision. Depending
upon a user?s needs, a high threshold may be desir-
able to identify only the most confident lexicon en-
tries, or a lower threshold may be desirable to retain
most of the correct entries while reliably removing
some of the incorrect ones.
5 Conclusions
We have demonstrated that co-occurrence statis-
tics gathered from the Web can dramatically im-
prove the ranking of lexicon entries produced by
a weakly-supervised corpus-based bootstrapping al-
gorithm, without requiring any additional supervi-
sion. We found that computing Web-based co-
occurrence statistics across a set of seed words and
then using the highest score was the most success-
ful approach. Co-occurrence with a hypernym term
also performed well for some categories, and could
be easily combined with the Max of Seeds approach
by choosing the highest value among the seeds as
well as the hypernym.
In future work, we would like to incorporate this
Web-based re-ranking procedure into the bootstrap-
ping algorithm itself to dynamically ?clean up? the
learned words before they are cycled back into the
bootstrapping process. Basilisk could consult the
Web-based statistics to select the best 5 words to
generate before the next bootstrapping cycle begins.
This integrated approach has the potential to sub-
stantially improve Basilisk?s performance because
it would improve the precision of the induced lex-
icon entries during the earliest stages of bootstrap-
ping when the learning process is most fragile.
Acknowledgments
Many thanks to Julia James for annotating the gold
standards for the disease domain. This research was
supported in part by Department of Homeland Secu-
rity Grant N0014-07-1-0152.
References
J. Atserias, S. Climent, X. Farreres, G. Rigau, and H. Ro-
driguez. 1997. Combining Multiple Methods for the
Automatic Construction of Multilingual WordNets. In
Proceedings of the International Conference on Recent
Advances in Natural Language Processing.
S. Caraballo. 1999. Automatic Acquisition of a
Hypernym-Labeled Noun Hierarchy from Text. In
Proc. of the 37th Annual Meeting of the Association
for Computational Linguistics, pages 120?126.
P. Cimiano and J. Volker. 2005. Towards large-scale,
open-domain and ontology-based named entity classi-
fication. In Proc. of Recent Advances in Natural Lan-
guage Processing, pages 166?172.
D. Davidov and A. Rappoport. 2006. Efficient unsu-
pervised discovery of word categories using symmet-
ric patterns and high frequency words. In Proc. of the
21st International Conference on Computational Lin-
guistics and the 44th annual meeting of the ACL.
25
D. Davidov, A. Rappoport, and M. Koppel. 2007. Fully
unsupervised discovery of concept-specific relation-
ships by web mining. In Proc. of the 45th Annual
Meeting of the Association of Computational Linguis-
tics, pages 232?239, June.
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from the
web: an experimental study. Artificial Intelligence,
165(1):91?134, June.
Z. Harris. 1954. Distributional Structure. In J. A. Fodor
and J. J. Katz, editor, The Structure of Language, pages
33?49. Prentice-Hall.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proc. of the 14th In-
ternational Conference on Computational Linguistics
(COLING-92).
Z. Kozareva, E. Riloff, and E. Hovy. 2008. Semantic
Class Learning from the Web with Hyponym Pattern
Linkage Graphs. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies (ACL-08).
D. Lin and P. Pantel. 2002. Concept discovery from text.
In Proc. of the 19th International Conference on Com-
putational linguistics, pages 1?7.
D. Lin. 1998. Dependency-based Evaluation of MINI-
PAR. In Workshop on the Evaluation of Parsing Sys-
tems, Granada, Spain.
G. Mann. 2002. Fine-grained proper noun ontologies for
question answering. In Proc. of the 19th International
Conference on Computational Linguistics, pages 1?7.
G. Miller. 1990. Wordnet: An On-line Lexical Database.
International Journal of Lexicography, 3(4).
MUC-4 Proceedings. 1992. Proceedings of the Fourth
Message Understanding Conference (MUC-4). Mor-
gan Kaufmann.
M. Pas?ca. 2004. Acquisition of categorized named en-
tities for web search. In Proc. of the Thirteenth ACM
International Conference on Information and Knowl-
edge Management, pages 137?145.
P. Pantel and D. Ravichandran. 2004. Automatically
labeling semantic classes. In Proc. of Conference of
HLT / North American Chapter of the Association for
Computational Linguistics, pages 321?328.
P. Pantel, D. Ravichandran, and E. Hovy. 2004. To-
wards terascale knowledge acquisition. In Proc. of the
20th international conference on Computational Lin-
guistics, page 771.
M. Pasca. 2007. weakly-supervised Discovery of Named
Entities using Web Search Queries. In CIKM, pages
683?690.
W. Phillips and E. Riloff. 2002. Exploiting Strong Syn-
tactic Heuristics and Co-Training to Learn Semantic
Lexicons. In Proceedings of the 2002 Conference on
Empirical Methods in Natural Language Processing,
pages 125?132.
ProMed-mail. 2006. http://www.promedmail.org/.
PubMed. 2009. http://www.ncbi.nlm.nih.gov/sites/entrez.
E. Riloff and R. Jones. 1999. Learning Dictionaries for
Information Extraction by Multi-Level Bootstrapping.
In Proceedings of the Sixteenth National Conference
on Artificial Intelligence.
E. Riloff and J. Shepherd. 1997. A Corpus-Based Ap-
proach for Building Semantic Lexicons. In Proceed-
ings of the Second Conference on Empirical Methods
in Natural Language Processing, pages 117?124.
E. Riloff. 1996. Automatically Generating Extraction
Patterns from Untagged Text. In Proceedings of the
Thirteenth National Conference on Artificial Intelli-
gence, pages 1044?1049. The AAAI Press/MIT Press.
B. Roark and E. Charniak. 1998. Noun-phrase Co-
occurrence Statistics for Semi-automatic Semantic
Lexicon Construction. In Proceedings of the 36th
Annual Meeting of the Association for Computational
Linguistics, pages 1110?1116.
Sofia Stamou, Kemal Oflazer, Karel Pala, Dimitris Chris-
toudoulakis, Dan Cristea, Dan Tufis, Svetla Koeva,
George Totkov, Dominique Dutoit, and Maria Grigo-
riadou. 2002. Balkanet: A multilingual semantic net-
work for the balkan languages. In Proceedings of the
1st Global WordNet Association conference.
H. Tanev and B. Magnini. 2006. Weakly supervised ap-
proaches for ontology population. In Proc. of 11st
Conference of the European Chapter of the Associa-
tion for Computational Linguistics.
M. Thelen and E. Riloff. 2002. A Bootstrapping Method
for Learning Semantic Lexicons Using Extraction Pa
ttern Contexts. In Proceedings of the 2002 Conference
on Empirical Methods in Natural Language Process-
ing, pages 214?221.
Peter D. Turney. 2001. Mining the Web for Syn-
onyms: PMI-IR versus LSA on TOEFL. In EMCL
?01: Proceedings of the 12th European Conference
on Machine Learning, pages 491?502, London, UK.
Springer-Verlag.
P. D. Turney. 2002. Thumbs up or thumbs down? se-
mantic orientation applied to unsupervised classifica-
tion of reviews. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics (ACL?02), pages 417?424.
Piek Vossen, editor. 1998. EuroWordNet: A Multilingual
Database with Lexical Semantic Networks. Kluwer
Academic Publishers, Norwell, MA, USA.
D. Widdows and B. Dorow. 2002. A graph model for
unsupervised lexical acquisition. In Proc. of the 19th
International Conference on Computational Linguis-
tics, pages 1?7.
26
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 77?86,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Automatically Producing Plot Unit Representations for Narrative Text
Amit Goyal
Dept. of Computer Science
University of Maryland
College Park, MD 20742
amit@umiacs.umd.edu
Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT 84112
riloff@cs.utah.edu
Hal Daume? III
Dept. of Computer Science
University of Maryland
College Park, MD 20742
hal@umiacs.umd.edu
Abstract
In the 1980s, plot units were proposed as a
conceptual knowledge structure for represent-
ing and summarizing narrative stories. Our
research explores whether current NLP tech-
nology can be used to automatically produce
plot unit representations for narrative text. We
create a system called AESOP that exploits
a variety of existing resources to identify af-
fect states and applies ?projection rules? to
map the affect states onto the characters in a
story. We also use corpus-based techniques
to generate a new type of affect knowledge
base: verbs that impart positive or negative
states onto their patients (e.g., being eaten is
an undesirable state, but being fed is a desir-
able state). We harvest these ?patient polar-
ity verbs? from a Web corpus using two tech-
niques: co-occurrence with Evil/Kind Agent
patterns, and bootstrapping over conjunctions
of verbs. We evaluate the plot unit representa-
tions produced by our system on a small col-
lection of Aesop?s fables.
1 Introduction
In the 1980s, plot units (Lehnert, 1981) were pro-
posed as a knowledge structure for representing nar-
rative stories and generating summaries. Plot units
are fundamentally different from the story represen-
tations that preceded them because they focus on the
affect states of characters and the tensions between
them as the driving force behind interesting and co-
hesive stories. Plot units were used in narrative sum-
marization studies, both in computer science and
psychology (Lehnert et al, 1981), but previous com-
putational models of plot units relied on tremendous
amounts of manual knowledge engineering.
The last few decades have seen tremendous ad-
vances in NLP and the emergence of many resources
that could be useful for plot unit analysis. So we em-
barked on a project to see whether plot unit repre-
sentations can be generated automatically using cur-
rent NLP technology. We created a system called
AESOP that uses a variety of resources to iden-
tify words that correspond to positive, negative, and
mental affect states. AESOP uses affect projection
rules to map the affect states onto the characters in
the story based on verb argument structure. Addi-
tionally, affect states are inferred based on syntactic
properties, and causal and cross-character links are
created using simple heuristics.
Affect states often arise from actions that produce
good or bad states for the character that is acted
upon. For example, ?the cat ate the mouse? pro-
duces a negative state for the mouse because being
eaten is bad. Similarly, ?the man fed the dog? pro-
duces a positive state for the dog because being fed
is generally good. Knowledge about the effects of
actions (i.e., state changes) on patients is not readily
available in existing semantic resources. We create
a new type of lexicon consisting of patient polarity
verbs (PPVs) that impart positive or negative states
on their patients. These verbs reflect world knowl-
edge about desirable/undesirable states for animate
beings; for example, being fed, paid or adopted are
generally desirable states, while being eaten, chased
or hospitalized are generally undesirable states.
We automatically generate a lexicon of ?patient
polarity verbs? from a Web corpus using two tech-
77
The Father and His Sons
(s1) A father had a family of sons who were perpetually
quarreling among themselves. (s2) When he failed to
heal their disputes by his exhortations, he determined
to give them a practical illustration of the evils of dis-
union; and for this purpose he one day told them to
bring him a bundle of sticks. (s3) When they had done
so, he placed the faggot into the hands of each of them
in succession, and ordered them to break it in pieces.
(s4) They tried with all their strength, and were not
able to do it. (s5) He next opened the faggot, took the
sticks separately, one by one, and again put them into
his sons? hands, upon which they broke them easily.
(s6) He then addressed them in these words: ?My sons,
if you are of one mind, and unite to assist each other,
you will be as this faggot, uninjured by all the attempts
of your enemies; but if you are divided among your-
selves, you will be broken as easily as these sticks.?
(a) ?Father and Sons? Fable
Father Sons
(quarreling)a1
(stop quarreling)a3
(annoyed)a2
(exhortations)a4
(exhortations fail)a5
m
m
a
(teach lesson)a6
m
(get sticks & break)a7m (get sticks & break)a8
(cannot break sticks)a9
a
(cannot break sticks)a10
a
(bundle & break)a11 (bundle & break)a12
(break sticks)a13
a
(break sticks)a14
a
m
a
shared
request
request
mixed
shared
s2
s2
s2
s2
s2
s2
s4
s5
s5
s1
s2
s4
s5
s5
(lesson succeeds)a15s5
(b) Plot Unit Analysis for ?Father and Sons? Fable
Figure 1: Sample Fable and Plot Unit Representation
niques: patterns that identify co-occurrence with
stereotypically evil or kind agents, and a bootstrap-
ping algorithm that learns from conjunctions of
verbs. We evaluate the plot unit representations pro-
duced by our system on a small collection of fables.
2 Overview of Plot Units
Plot unit structures consist of affect states for each
character, and links defining the relationships be-
tween them. Plot units include three types of affect
states: positive (+), negative (-), and mental (M).
Affect states can be connected by causal links and
cross-character links, which explain how the nar-
rative hangs together. Causal links exist between
affect states for the same character and have four
types: motivation (m), actualization (a), termination
(t) and equivalence (e). Cross-character links indi-
cate that a single event affects multiple characters.
For instance, if one character requests something of
another, then each character is assigned an M state
and a cross-character link connects the states.
To see a concrete example of a plot unit represen-
tation, a short fable, ?The Father and His Sons,? is
shown in Figure 1(a) and our annotation of its plot
unit structure is shown in Figure 1(b). In this fable,
there are two characters, the ?Father? and (collec-
tively) the ?Sons?, who go through a series of affect
states depicted chronologically in the two columns.
The first affect state (a1) is produced from sen-
tence #1 (s1) and is a negative state for the sons be-
cause they are quarreling. This state is shared by the
father (via a cross-character link) who has a nega-
tive annoyance state (a2). The father decides that
he wants to stop the sons from quarreling, which
is a mental event (a3). The causal link from a2 to
a3 with an m label indicates that his annoyed state
?motivated? this decision. His first attempt is by ex-
hortations (a4). The first M (a3) is connected to the
second M (a4) with an m (motivation) link, which
represents subgoaling. The father?s overall goal is
to stop the quarreling (a3), and to do so he creates a
subgoal of exhorting the sons to stop (a4). The ex-
hortations fail, which produces a negative state (a5)
for the father. The a causal link indicates an ?actu-
alization?, representing the failure of his plan (a4).
This failure motivates a new subgoal: teach the
sons a lesson (a6). At a high level, this subgoal
has two parts, indicated by the two gray regions
(a7 ? a10 and a11 ? a14). The first gray region
begins with a cross-character link (M to M), which
indicates a request (in this case, to break a bundle
of sticks). The sons fail at this, which upsets them
(a9) but pleases the father (a10). The second gray
region depicts the second part of the father?s sub-
goal; he makes a second request (a11 to a12) to sep-
arate the bundle and break the sticks, which the sons
successfully do, making them happy (a13) and the
father happy (a14) as well. This latter structure (the
second gray region) is an HONORED REQUEST plot
unit structure. At the end, the father?s plan succeeds
(a15) which is an actualization (a link) of his goal
to teach the sons a lesson (a6).
78
3 Where Do Affect States Come From?
We briefly overview the variety of situations that can
be represented by affect states in plot units.
Direct Expressions of Emotion: Affect states can
correspond to positive/negative emotional states, as
have been studied in the realm of sentiment anal-
ysis. For example, ?Max was disappointed? pro-
duces a negative affect state for Max, and ?Max was
pleased? produces a positive affect state for Max.
Situational Affect States: Positive and negative af-
fect states can represent good and bad situational
states that characters find themselves in. These
states do not represent emotion, but indicate whether
a situation (state) is good or bad for a character
based on world knowledge. e.g., ?The wolf had a
bone stuck in his throat.? produces a negative affect
state for the wolf. Similarly, ?The woman recovered
her sight.? produces a positive affect state for the
woman.
Plans and Goals: The existence of a plan or goal is
represented as a mental state (M). Plans and goals
can be difficult to detect automatically and can be
revealed in many ways, such as:
? Direct expressions of plans/goals: a plan/goal
may be explicitly stated (e.g., ?John wants food?).
? Speech acts: a plan or goal may be revealed
through a speech act. For example, ?the wolf asked
an eagle to extract the bone? is a directive speech
act that indicates the wolf?s plan to resolve its
negative state (having a bone stuck). This example
illustrates how a negative state (bone stuck) can
motivate a mental state (plan). When a speech act
involves multiple characters, it produces multiple
mental states.
? Inferred plans/goals: plans and goals are some-
times inferred from actions. e.g., ?the lion hunted
deer? implies that the lion has a plan to obtain food.
Similarly, ?the serpent spat poison at John? implies
that the serpent wants to kill John.
? Plan/Goal completion: Plans and goals produce
+/- affect states when they succeed or fail. For
example, if the eagle successfully extracts the bone
from the wolf?s throat, then both the wolf and the
eagle will have positive affect states because both
were successful in their respective goals.
We observed that situational and plan/goal states
often originate from an action. When a character is
acted upon (the patient of a verb), then the charac-
ter may be in a positive or negative state depend-
ing upon whether the action was good or bad for
them based on world knowledge. For example, be-
ing fed, paid or adopted is generally desirable, but
being chased, eaten, or hospitalized is usually unde-
sirable. Consequently, we decided to create a lex-
icon of patient polarity verbs that produce positive
or negative states for their patients. In Section 4.2,
we present two methods for automatically harvest-
ing these verbs from a Web corpus.
4 AESOP: Automatically Generating Plot
Unit Representations
Our system, AESOP, automatically creates plot unit
representations for narrative text. AESOP has four
main steps: affect state recognition, character iden-
tification, affect state projection, and link creation.
During affect state recognition, AESOP identifies
words that may be associated with positive, nega-
tive, and mental states. AESOP then identifies the
main characters in the story and applies affect pro-
jection rules to map the affect states onto these char-
acters. During this process, some additional affect
states are inferred based on verb argument structure.
Finally, AESOP creates cross-character links and
causal links between affect states. We also present
two corpus-based methods to automatically produce
a new resource for affect state recognition: a patient
polarity verb lexicon.
4.1 Plot Unit Creation
4.1.1 Recognizing Affect States
The basic building blocks of plot units are af-
fect states which come in three flavors: positive,
negative, and mental. In recent years, many pub-
licly available resources have been created for sen-
timent analysis and other types of semantic knowl-
edge. We considered a wide variety of resources and
ultimately decided to experiment with five resources
that most closely matched our needs:
? FrameNet (Baker et al, 1998): We manually
identified 87 frame classes that seem to be associ-
ated with affect: 43 mental classes (e.g., COMMU-
NICATION and NEEDING), 22 positive classes (e.g.,
ACCOMPLISHMENT and SUPPORTING), and 22 neg-
ative classes (e.g., CAUSE HARM and PROHIBIT-
79
ING). We use the verbs listed for these classes to
produce M, +, and - affect states.
?MPQA Lexicon (Wilson et al, 2005b): We used
the words listed as having positive or negative senti-
ment polarity to produce +/- states, when they occur
with the designated part-of-speech.
? OpinionFinder (Wilson et al, 2005a) (Version
1.4) : We used the +/- labels assigned by its con-
textual polarity classifier (Wilson et al, 2005b) to
create +/- states and the MPQASD tags produced
by its Direct Subjective and Speech Event Identifier
(Choi et al, 2006) to produce mental (M) states.
? Semantic Orientation Lexicon (Takamura et al,
2005): We used the words listed as having posi-
tive or negative polarity to produce +/- affect states,
when they occur with the designated part-of-speech.
? Speech Act Verbs: We used 228 speech act
verbs from (Wierzbicka, 1987) to produce M states.
4.1.2 Identifying the Characters
For the purposes of this work, we made two sim-
plifying assumptions: (1) There are only two char-
acters per fable1, and (2) Both characters are men-
tioned in the fable?s title. The problem of corefer-
ence resolution for fables is somewhat different than
for other genres, primarily because the characters
are often animals (e.g., he=owl). So we hand-crafted
a simple rule-based coreference system. First, we
apply heuristics to determine number and gender
based on word lists, WordNet (Miller, 1990) and
part-of-speech tags. If no determination of a char-
acter?s gender or number can be made, we employ a
process of elimination. Given the two character as-
sumption, if one character is known to be male, but
there are female pronouns in the fable, then the other
character is assumed to be female. The same is done
for number agreement. Finally, if there is only one
character between a pronoun and the beginning of
a document, then we resolve the pronoun with that
character and the character assumes the gender and
number of the pronoun. Lastly, WordNet provides
some additional resolutions by exploiting hypernym
relations, for instance, linking peasant with man.
4.1.3 Mapping Affect States onto Characters
Plot unit representations are not just a set of af-
fect states, but they are structures that capture the
1We only selected fables that had two main characters.
chronological ordering of states for each character
as the narrative progresses. Consequently, every af-
fect state needs to be attributed to a character. Since
most plots revolve around events, we use verb argu-
ment structure as the primary means for projecting
affect states onto characters.
We developed four affect projection rules that or-
chestrate how affect states are assigned to the char-
acters. We used the Sundance parser (Riloff and
Phillips, 2004) to produce a shallow parse of each
sentence, which includes syntactic chunking, clause
segmentation, and active/passive voice recognition.
We normalized the verb phrases with respect to ac-
tive/passive voice to simplify the rules. We made the
assumption that the Subject of the VP is its AGENT
and the Direct Object of the VP is its PATIENT.2
The rules only project affect states onto AGENTS
and PATIENTS that refer to a character in the story.
The four projection rules are presented below.
1. AGENT VP : This rule applies when the VP
has no PATIENT or the PATIENT corefers with the
AGENT. All affect tags assigned to the VP are pro-
jected onto the AGENT. Example: ?Mary laughed
(+)? projects a + affect state onto Mary.
2. VP PATIENT : This rule applies when the VP
has no agent, which is common in passive voice con-
structions. All affect tags assigned to the VP are
projected onto the PATIENT. Example: ?John was
rewarded (+), projects a + affect state onto John.
3. AGENT VP PATIENT : This rules applies
when both an AGENT and PATIENT are present, do
not corefer, and at least one of them is a character. If
the PATIENT is a character, then all affect tags asso-
ciated with the VP are projected onto the PATIENT.
If the AGENT is a character and the VP has an M
tag, then we also project an M tag onto the AGENT
(representing a shared, cross-character mental state).
4. AGENT VERB1 to VERB2 PATIENT : This
rule has two cases: (a) If the AGENT and PATIENT
refer to the same character, then we apply Rule #1.
Example: ?Bo decided to teach himself...? (b) If the
AGENT and PATIENT are different, then we apply
Rule #1 to VERB1 and Rule #2 to VERB2.
Finally, if an adverb or adjectival phrase has af-
fect, then that affect is mapped onto the preceding
VP and the rules above are applied. For all of the
2This is not always correct, but worked ok in our fables.
80
rules, if a clause contains a negation word, then we
flip the polarity of all words in that clause.
4.1.4 Inferring Affect States
Recognizing plans and goals depends on world
knowledge and inference, and is beyond the scope
of this paper. However, we identified two cases
where affect states often can be inferred based on
syntactic properties. The first case involves verb
phrases (VPs) that have both an AGENT and PA-
TIENT, which corresponds to projection rule #3. If
the VP has polarity, then rule #3 assigns that po-
larity to the PATIENT, not the AGENT. For exam-
ple, ?John killed Paul? imparts negative polarity on
Paul, but not necessarily on John. Unless we are
told otherwise, one assumes that John intentionally
killed Paul, and so in a sense, John accomplished
his goal. Consequently, this action should produce a
positive affect state for John. We capture this notion
of accomplishment as a side effect of projection rule
#3: if the VP has +/- polarity, then we produce an
inferred positive state for the AGENT.
The second case involves infinitive verb phrases
of the form: ?AGENT VERB1 TO VERB2 PA-
TIENT? (e.g., ?Susan tried to warn Mary?). The
infinitive VP construction suggests that the AGENT
has a goal or plan that is being put into motion (e.g.,
tried to, wanted to, attempted to, hoped to, etc.). To
capture this intuition, in rule #4 if VERB1 does not
already have an affect state assigned to it then we
produce an inferred mental state for the AGENT.
4.1.5 Causal and Cross-Character Links
Our research is focused primarily on creating af-
fect states for characters, but plot unit structures
also include cross-character links to connect states
that are shared across characters and causal links
between states for a single character. As an ini-
tial attempt to create complete plot units, AESOP
produces links using simple heuristics. A cross-
character link is created when two characters in a
clause have affect states that originated from the
same word. A causal link is created between each
pair of (chronologically) consecutive affect states
for the same character. Currently, AESOP only pro-
duces forward causal links (motivation (m), actual-
ization (a)) and does not produce backward causal
links (equivalence (e), termination (t)). For forward
links, the causal syntax only allows for five cases:
M m? M , + m? M , ? m? M , M a? +, M a? ?.
So when AESOP produces a causal link between
two affect states, the order and types of the two states
uniquely determine which label it gets (m or a).
4.2 Generating PPV Lexicons
During the course of this research, we identified a
gap in currently available knowledge: we are not
aware of existing resources that identify verbs which
produce a desirable/undesirable state for their pa-
tients even though the verb itself does not carry po-
larity. For example, the verb eat describes an action
that is generally neutral, but being eaten is clearly
an undesirable state. Similarly, the verb fed does not
have polarity, but being fed is a desirable state for the
patient. In the following sections, we try to fill this
gap by using corpus-based techniques to automati-
cally acquire a Patient Polarity Verb (PPV) Lexicon.
4.2.1 PPV Harvesting with Evil/Kind Agents
The key idea behind our first approach is to iden-
tify verbs that frequently occur with evil or kind
agents. Our intuition was that an ?evil? agent will
typically perform actions that are bad for the patient,
while a ?kind? agent will typically perform actions
that are good for the patient.
We manually identified 40 stereotypically evil
agent words, such as monster, villain, terrorist, and
murderer, and 40 stereotypically kind agent words,
such as hero, angel, benefactor, and rescuer. We
searched the Google Web 1T N-gram corpus to
identify verbs that co-occur with these words as
probable agents. For each agent term, we applied
the pattern ?* by [a,an,the] AGENT? and extracted
the matching N-grams. Then we applied a part-of-
speech tagger to each N-gram and saved the words
that were tagged as verbs (i.e., the words in the *
position).3 This process produced 811 negative (evil
agent) PPVs and 1362 positive (kind agent) PPVs.
4.2.2 PPV Bootstrapping over Conjunctions
Our second approach for acquiring PPVs is based
on an observation from sentiment analysis research
that conjoined adjectives typically have the same po-
larity (e.g. (Hatzivassiloglou and McKeown, 1997)).
3The POS tagging quality is undoubtedly lower than if tag-
ging complete sentences but it seemed reasonable.
81
Our hypothesis was that conjoined verbs often share
the same polarity as well (e.g., ?abducted and
killed? or ?rescued and rehabilitated?). We exploit
this idea inside a bootstrapping algorithm to itera-
tively learn verbs that co-occur in conjunctions.
Bootstrapping begins with 10 negative and 10
positive PPV seeds. First, we extracted triples of
the form ?w1 and w2? from the Google Web 1T
N -gram corpus that had frequency ? 100 and were
lower case. We separated each conjunction into
two parts: a primary VERB (?w1?) and a CONTEXT
(?and w2?), and created a copy of the conjunction
with the roles of w1 and w2 reversed. For example,
?rescued and adopted? produces:
VERB=?rescued? CONTEXT=?and adopted?
VERB=?adopted? CONTEXT=?and rescued?
Next, we applied the Basilisk bootstrapping al-
gorithm (Thelen and Riloff, 2002) to learn PPVs.
Basilisk identifies semantically similar words based
on their co-occurrence with seeds in contextual pat-
terns. Basilisk was originally designed for semantic
class induction using lexico-syntactic patterns, but
has also been used to learn subjective and objective
nouns (Riloff et al, 2003).
Basilisk first identifies the pattern contexts that
are most strongly associated with the seed words.
Words that occur in those contexts are labeled as
candidates and scored based on the strength of their
contexts. The top 5 candidates are selected and the
bootstrapping process repeats. Basilisk produces a
lexicon of learned words as well as a ranked list of
pattern contexts. Since we bootstrapped over verb
conjunctions, we also extracted new PPVs from the
contexts. We ran the bootstrapping process to create
a lexicon of 500 words, and we collected verbs from
the top 500 contexts as well.
5 Evaluation
Plot unit analysis of narrative text is enormously
complex ? the idea of creating gold standard plot
unit annotations seemed like a monumental task.
So we began with relatively simple and constrained
texts that seemed appropriate: fables. Fables have
two desirable attributes: (1) they have a small cast
of characters, and (2) they typically revolve around
a moral, which is exemplified by a short and concise
plot. Even so, fables are challenging for NLP due to
anthropomorphic characters, flowery language, and
sometimes archaic vocabulary.
We collected 34 Aesop?s fables from a web site4,
choosing fables that have a true plot (some only con-
tain quotes) and exactly two characters. We divided
them into a development set of 11 stories, a tuning
set of 8 stories, and a test set of 15 stories.
Creating a gold standard was itself a substantial
undertaking, and training non-experts to produce
them did not seem feasible in the short term. So
the authors discussed and iteratively refined manual
annotations for the development and tuning sets un-
til we produced similar results and had a common
understanding of the task. Then two authors inde-
pendently created annotations for the test set, and a
third author adjudicated the differences.
5.1 Evaluation Procedure
For evaluation, we used recall (R), precision (P),
and F-measure (F). In our gold standard, each af-
fect state is annotated with the set of clauses that
could legitimately produce it. In most cases (75%),
we were able to ascribe the existence of a state to
precisely one clause. During evaluation, the system-
produced affect states must be generated from the
correct clause. However, for affect states that could
be ascribed to multiple clauses in a sentence, the
evaluation was done at the sentence level. In this
case, the system-produced affect state must come
from the sentence that contains one of those clauses.
Coreference resolution is far from perfect, so we
created gold standard coreference annotations for
our fables and used them for most of our experi-
ments. This allowed us to evaluate our approach
without coreference mistakes factoring in. In Sec-
tion 5.5, we re-evaluate our final results using auto-
matic coreference resolution.
5.2 Evaluation of Affect States using External
Resources
Our first set of experiments evaluates the quality of
the affect states produced by AESOP using only the
external resources. The top half of Table 1 shows the
results for each resource independently. FrameNet
produced the best results, yielding much higher re-
call than any other resource. The bottom half of Ta-
4www.pacificnet.net/?johnr/aesop/
82
Affect State M (59) + (47) - (37) All (143)
Resource(s) R P F R P F R P F R P F
FrameNet .49 .51 .50 .17 .57 .26 .14 .42 .21 .29 .51 .37
MPQA Lexicon .07 .50 .12 .21 .24 .22 .22 .38 .28 .15 .31 .20
OpinionFinder .42 .40 .41 .00 .00 .00 .03 .17 .05 .18 .35 .24
Semantic Orientation Lexicon .07 .44 .12 .17 .40 .24 .08 .38 .13 .10 .41 .16
Speech Act Verbs .36 .53 .43 .00 .00 .00 .00 .00 .00 .15 .53 .23
FrameNet+MPQA Lexicon .44 .52 .48 .30 .28 .29 .27 .38 .32 .35 .40 .37
FrameNet+OpinionFinder .53 .39 .45 .17 .38 .23 .16 .33 .22 .31 .38 .34
FrameNet+Semantic Orientation Lexicon .49 .51 .50 .26 .36 .30 .22 .42 .29 .34 .45 .39
FrameNet+Speech Act Verbs .51 .48 .49 .17 .57 .26 .14 .42 .21 .30 .49 .37
Table 1: Evaluation results for AESOP using external resources. The # in parentheses is the # of gold affect states.
Affect State M (59) + (47) - (37) All (143)
Resource(s) R P F R P F R P F R P F
- Evil Agent PPVs .07 .50 .12 .21 .40 .28 .46 .46 .46 .22 .44 .29
- Neg Basilisk PPVs .07 .44 .12 .11 .45 .18 .24 .45 .31 .13 .45 .20
- Evil Agent and Neg Basilisk PPVs .05 .43 .09 .21 .38 .27 .46 .40 .43 .21 .39 .27
+ Kind Agent PPVs (?>1) .03 .33 .06 .28 .17 .21 .00 .00 .00 .10 .19 .13
+ Pos Basilisk PPVs .08 .56 .14 .02 .12 .03 .03 1.00 .06 .05 .39 .09
FrameNet+SOLex+EvilAgentPPVs .49 .54 .51 .30 .38 .34 .46 .42 .44 .42 .46 .44
FrameNet+EvilAgentPPVs .49 .54 .51 .28 .45 .35 .46 .46 .46 .41 .49 .45
FrameNet+EvilAgentPPVs+PosBasiliskPPVs .49 .53 .51 .30 .41 .35 .49 .49 .49 .43 .48 .45
Table 2: Evaluation results for AESOP with PPVs. The # in parentheses is the # of gold affect states.
ble 1 shows the results when combining FrameNet
with other resources. In terms of F score, the only
additive benefit came from the Semantic Orientation
Lexicon, which produced a better balance of recall
and precision and an F score gain of +2.
5.3 Evaluation of Affect States using PPVs
Our second set of experiments evaluates the quality
of the automatically generated PPV lexicons. The
top portion of Table 2 shows the results for the neg-
ative PPVs. The PPVs harvested by the Evil Agent
patterns produced the best results, yielding recall
and precision of .46 for negative states. Note that
M and + states are also generated from the negative
PPVs because they are inferred during affect projec-
tion (Section 4.1.4). The polarity of a negative PPV
can also be flipped by negation to produce a + state.
Basilisk?s negative PPVs achieved similar preci-
sion but lower recall. We see no additional recall
and some precision loss when the Evil Agent and
Basilisk PPV lists are combined. The precision drop
is likely due to redundancy, which creates spurious
affect states. If two different words have negative
polarity but refer to the same event, then only one
negative affect state should be generated. But AE-
SOP will generate two affect states, so one will be
spurious.
The middle section of Table 2 shows the results
for the positive PPVs. Both positive PPV lexicons
were of dubious quality, so we tried to extract a high-
quality subset of each list. For the Kind Agent PPVs,
we computed the ratio of the frequency of the verb
with Evil Agents versus Kind Agents and only saved
verbs with an Evil:Kind ratio (?) > 1, which yielded
1203 PPVs. For the positive Basilisk PPVs, we used
only the top 100 lexicon and top 100 context verbs,
which yielded 164 unique verbs. The positive PPVs
did generate several correct affect states (including
a - state when a positive PPV was negated), but also
many spurious states.
The bottom section of Table 2 shows the impact
of the learned PPVs when combined with FrameNet
and the Semantic Orientation Lexicon (SOLex).
Adding the Evil Agent PPVs improved AESOP?s F
score from 39% to 44%, mainly due to a +8 recall
gain. The recall of the - states increased from 22%
to 46% with no loss of precision. Interestingly, if
we remove SOLex and use only FrameNet with our
PPVs, precision increases from 46% to 49% and re-
call only drops by -1. Finally, the last row of Table
83
2 shows that adding Basilisk?s positive PPVs pro-
duces a small recall boost (+2) with a slight drop in
precision (-1).
Evaluating the impact of PPVs on plot unit struc-
tures is an indirect way of assessing their quality be-
cause creating plot units involves many steps. Also,
our test set is small so many verbs will never appear.
To directly measure the quality of our PPVs, we re-
cruited 3 people to manually review them. We devel-
oped annotation guidelines that instructed each an-
notator to judge whether a verb is generally good or
bad for its patient, assuming the patient is animate.
They assigned each verb to one of 6 categories: ?
(not a verb), 2 (always good), 1 (usually good), 0
(neutral, mixed, or requires inanimate patient), -1
(usually bad), -2 (always bad). Each annotator la-
beled 250 words: 50 words randomly sampled from
each of our 4 PPV lexicons5 (Evil Agent PPVs, Kind
Agent PPVs, Positive Basilisk PPVs, and Negative
Basilisk PPVs) plus 50 verbs labeled as neutral in
the MPQA lexicon.
First, we measured agreement based on three
groupings: negative (-2 and -1), neutral (0), or pos-
itive (1 and 2). We computed ? scores to measure
inter-annotator agreement for each pair of annota-
tors.6, but the ? scores were relatively low because
the annotators had trouble distinguishing the posi-
tive cases from the neutral ones. So we re-computed
agreement using two groupings: negative (-2 and -
1) and not-negative (0 through 2), and obtained ?
scores of .69, .71, and .74. We concluded that peo-
ple largely agree on whether a verb is bad for the
patient, but they do not necessarily agree if a verb is
good for the patient. One possible explanation is that
many ?bad? verbs represent physical harm or dan-
ger: these verbs are both plentiful and easy to rec-
ognize. In contrast, ?good? verbs are often more ab-
stract and open to interpretation (e.g., is being ?en-
vied? or ?feared? a good thing?).
We used the labels produced by the two an-
notators with the highest ? score to measure the
accuracy of our PPVs. Both the Evil Agent and
Negative Basilisk PPVs were judged to be 72.5%
accurate, averaged over the judges. The Kind Agent
5The top-ranked Evil/Kind Agent PPV lists (? > 1) which
yields 1203 kind PPVs, and 477 evil PPVs, the top 164 positive
Basilisk verbs, and the 678 (unique) negative Basilisk verbs.
6We discarded words labeled as not a verb.
PPVs were only about 39% accurate, while the
Positive Basilisk PPVs were nearly 50% accurate.
These results are consistent with our impressions
that the negative PPVs are of relatively high quality,
while the positive PPVs are mixed. Some examples
of learned PPVs that were not present in our other
resources are:
- : censor, chase, fire, orphan, paralyze, scare, sue
+ : accommodate, harbor, nurse, obey, respect, value
5.4 Evaluation of Links
We represented each link as a 5-tuple
?src-clause, src-state, tgt-clause, tgt-state, link-type?,
where source/target denotes the direction of the
link, the source/target-states are the affect state type
(+,-,M) and link-type is one of 3 types: actualization
(a), motivation (m), or cross-character (xchar). A
system-produced link is considered correct if all 5
elements of the tuple match the human annotation.
Gold Aff States System Aff States
Links R P F R P F
xchar (56) .79 .85 .82 .18 .43 .25
a (51) .90 .94 .92 .04 .07 .05
m (26) 1.0 .57 .72 .15 .10 .12
Table 3: Link results; parentheses show # of gold links.
The second column of Table 3 shows the perfor-
mance of AESOP when using gold standard affect
states. Our simple heuristics for creating links work
surprisingly well for xchar and a links when given
perfect affect states. However, these heuristics pro-
duce relatively low precision for m links, albeit with
100% recall. This reveals that m links primarily do
connect adjacent states, but we need to be more dis-
criminating when connecting them. The third col-
umn of Table 3 shows the results when using system-
generated affect states. We see that performance is
much lower. This is not particularly surprising, since
AESOP?s F-score is 45%, so over half of the indi-
vidual states are wrong, which means that less than
a quarter of the pairs are correct. From that perspec-
tive, the xchar link performance is reasonable, but
the causal a and m links need improvement.
5.5 Analysis
We performed additional experiments to evaluate
some assumptions and components. First, we cre-
ated a Baseline system that is identical to AESOP
84
except that it does not use the affect projection rules.
Instead, it naively projects every affect state in a
clause onto every character in that clause. The first
two rows of the table below show that AESOP?s pre-
cision is double the Baseline, with nearly the same
recall. This illustrates the importance of the projec-
tion rules for mapping affect states onto characters.
R P F
Baseline .44 .24 .31
AESOP, gold coref .43 .48 .45
AESOP, gold coref, infstates .39 .48 .43
AESOP, auto coref, infstates .24 .56 .34
Our gold standard includes pure inference affect
states that are critical to the plot unit structure but
come from world knowledge outside the story itself.
Of 157 affect states in our test set, 14 were pure in-
ference states. We ignored these states in our previ-
ous experiments because our system has no way to
generate them. The third row of the table shows that
including them lowers recall by -4. Generating pure
inferences is an interesting challenge, but they seem
to be a relatively small part of the problem.
The last row of the table shows AESOP?s perfor-
mance when we use our automated coreference re-
solver (Section 4.1.2) instead of gold standard coref-
erence annotations. We see a -15 recall drop coupled
with a +8 precision gain. We were initially puz-
zled by the precision gain but believe that it is pri-
marily due to the handling of quotations. Our gold
standard includes annotations for characters men-
tioned in quotations, but our automated coreference
resolver ignores quotations. Most fables end with
a moral, which is often a quote that may not men-
tion the plot. Consequently, AESOP generates more
spurious affect states from the quotations when us-
ing the gold standard annotations.
6 Related Work and Conclusions
Our research is the first effort to fully automate
the creation of plot unit structures. Other prelimi-
nary work has begun to look at plot unit modelling
for single character stories (Appling and Riedl,
2009). More generally, our work is related to re-
search in narrative story understanding (e.g., (El-
son and McKeown, 2009)), automatic affect state
analysis (Alm, 2009), and automated learning of
scripts (Schank and Abelson, 1977) and other con-
ceptual knowledge structures (e.g., (Mooney and
DeJong, 1985; Fujiki et al, 2003; Chambers and Ju-
rafsky, 2008; Chambers and Jurafsky, 2009; Kasch
and Oates, 2010)). Our work benefitted from prior
research in creating semantic resources such as
FrameNet (Baker et al, 1998) and sentiment lex-
icons and classifiers (e.g., (Takamura et al, 2005;
Wilson et al, 2005b; Choi et al, 2006)). We showed
that affect projection rules can effectively assign af-
fect states to characters. This task is similar to, but
not the same as, associating opinion words with their
targets or topics (Kim and Hovy, 2006; Stoyanov
and Cardie, 2008). Some aspects of affect state iden-
tification are closely related to Hopper and Thomp-
son?s (1980) theory of transitivity. In particular, their
notions of aspect (has an action completed?), benefit
and harm (how much does an object gain/lose from
an action?) and volition (did the subject make a con-
scious choice to act?).
AESOP produces affect states with an F score of
45%. Identifying positive states appears to be more
difficult than negative or mental states. Our sys-
tem?s biggest shortcoming currently seems to hinge
around identifying plans and goals. This includes
the M affect states that initiate plans, the +/- com-
pletion states, as well as their corresponding links.
We suspect that the relatively low recall on positive
affect states is due to our inability to accurately iden-
tify successful plan completions. Finally, these re-
sults are based on fables; plot unit analysis of other
types of texts will pose additional challenges.
Acknowledgments
The authors gratefully acknowledge the support of
Department of Homeland Security Grant N0014-07-
1-0152, NSF grant IIS-0712764, and the Defense
Advanced Research Projects Agency (DARPA) Ma-
chine Reading Program under Air Force Research
Laboratory (AFRL) prime contract no. FA8750-09-
C-0172. Any opinions, findings, and conclusion
or recommendations expressed in this material are
those of the author(s) and do not necessarily reflect
the view of the DARPA, AFRL, or the U.S. gov-
ernment. Thanks to Peter Jensen, Emily Schlichter,
and Clay Templeton for PPV annotations, Nathan
Gilbert for help with the coreference resolver, and
the anonymous reviewers for many helpful com-
ments.
85
References
Cecilia Ovesdotter Alm. 2009. Affect in Text and Speech.
VDM Verlag Dr. Mller.
D. Scott Appling and Mark O. Riedl. 2009. Representa-
tions for learning to summarize plots. In Proceedings
of the AAAI Spring Symposium on Intelligent Narra-
tive Technologies II.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet Project. In In Proceed-
ings of COLING/ACL, pages 86?90.
Nathanael Chambers and Dan Jurafsky. 2008. Unsuper-
vised learning of narrative event chains. In Proceed-
ings of the Association for Computational Linguistics.
Nathanael Chambers and Dan Jurafsky. 2009. Unsuper-
vised learning of narrative schemas and their partici-
pants. In Proceedings of the Association for Compu-
tational Linguistics.
Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint
extraction of entities and relations for opinion recogni-
tion. In EMNLP ?06: Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 431?439, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
David Elson and Kathleen McKeown. 2009. Extending
and evaluating a platform for story understanding. In
Proceedings of the AAAI 2009 Spring Symposium on
Intelligent Narrative Technologies II.
Toshiaki Fujiki, Hidetsugu Nanba, and Manabu Oku-
mura. 2003. Automatic acquisition of script knowl-
edge from a text collection. In Proceedings of the Eu-
ropean Association for Computational Linguistics.
Vasileios Hatzivassiloglou and Kathy McKeown. 1997.
Predicting the semantic orientation of adjectives. In
Proceedings of the 35th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 174?181,
Madrid, Spain.
Paul J. Hopper and Sandra A. Thompson. 1980.
Transitivity in grammar and discourse. Language,
56:251299.
Niels Kasch and Tim Oates. 2010. Mining script-like
structures from the web. In NAACL-10 Workshop on
Formalisms and Methodology for Learning by Read-
ing (FAM-LbR).
S. Kim and E. Hovy. 2006. Extracting Opinions, Opin-
ion Holders, and Topics Expressed in Online News
Media Text. In Proceedings of ACL/COLING Work-
shop on Sentiment and Subjectivity in Text.
W. Lehnert, J. Black, and B. Reiser. 1981. Summariz-
ing Narratives. In Proceedings of the Seventh Interna-
tional Joint Conference on Artificial Intelligence.
W. G. Lehnert. 1981. Plot Units and Narrative Summa-
rization. Cognitive Science, 5(4):293?331.
G. Miller. 1990. Wordnet: An On-line Lexical Database.
International Journal of Lexicography, 3(4).
Raymond Mooney and Gerald DeJong. 1985. Learning
Schemata for Natural Language Processing. In Pro-
ceedings of the Ninth International Joint Conference
on Artificial Intelligence, pages 681?687.
E. Riloff and W. Phillips. 2004. An Introduction to the
Sundance and AutoSlog Systems. Technical Report
UUCS-04-015, School of Computing, University of
Utah.
E. Riloff, J. Wiebe, and T. Wilson. 2003. Learning Sub-
jective Nouns using Extraction Pattern Bootstrapping.
In Proceedings of the Seventh Conference on Natural
Language Learning (CoNLL-2003), pages 25?32.
Roger C. Schank and Robert P. Abelson. 1977. Scripts,
plans, goals and understanding. Lawrence Erlbaum.
V. Stoyanov and C. Cardie. 2008. Topic Identification
for Fine-Grained Opinion Analysis. In Conference on
Computational Linguistics (COLING 2008).
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words using
spin model. In ACL ?05: Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics.
M. Thelen and E. Riloff. 2002. A Bootstrapping Method
for Learning Semantic Lexicons Using Extraction Pa
ttern Contexts. In Proceedings of the 2002 Conference
on Empirical Methods in Natural Language Process-
ing, pages 214?221.
A. Wierzbicka. 1987. English speech act verbs: a se-
mantic dictionary. Academic Press, Sydney, Orlando.
T. Wilson, P. Hoffmann, S. Somasundaran, J. Kessler,
J. Wiebe, Y. Choi, C. Cardie, E. Riloff, and S. Patward-
han. 2005a. OpinionFinder: A system for subjectivity
analysis. In Proceedings of HLT/EMNLP 2005 Inter-
active Demonstrations.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005b. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of Human
Language Technology Conference and Conference on
Empirical Methods in Natural Language Processing,
pages 347?354. Association for Computational Lin-
guistics.
86
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 748?758,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Classifying Sentences as Speech Acts in Message Board Posts
Ashequl Qadir and Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT 84112
{asheq,riloff}@cs.utah.edu
Abstract
This research studies the text genre of mes-
sage board forums, which contain a mix-
ture of expository sentences that present fac-
tual information and conversational sentences
that include communicative acts between the
writer and readers. Our goal is to create
sentence classifiers that can identify whether
a sentence contains a speech act, and can
recognize sentences containing four different
speech act classes: Commissives, Directives,
Expressives, and Representatives. We con-
duct experiments using a wide variety of fea-
tures, including lexical and syntactic features,
speech act word lists from external resources,
and domain-specific semantic class features.
We evaluate our results on a collection of mes-
sage board posts in the domain of veterinary
medicine.
1 Introduction
In the 1990?s, the natural language processing com-
munity shifted much of its attention to corpus-based
learning techniques. Since then, most of the text cor-
pora that have been annotated and studied are collec-
tions of expository text (e.g., news articles, scientific
literature, etc.). The intent of expository text is to
present or explain information to the reader. In re-
cent years, there has been a growing interest in text
genres that originate from Web sources, such as we-
blogs and social media sites (e.g., tweets). These
text genres offer new challenges for NLP, such as
the need to handle informal and loosely grammatical
text, but they also pose new opportunities to study
discourse and pragmatic phenomena that are funda-
mentally different in these genres.
Message boards are common on the WWW as a
forum where people ask questions and post com-
ments to members of a community. They are typ-
ically devoted to a specific topic or domain, such as
finance, genealogy, or Alzheimer?s disease. Some
message boards offer the opportunity to pose ques-
tions to domain experts, while other communities
are open to anyone who has an interest in the topic.
From a natural language processing perspective,
message board posts are an interesting hybrid text
genre because they consist of both expository text
and conversational text. Most obviously, the conver-
sations appear as a thread, where different people
respond to each other?s questions in a sequence of
posts. Studying the conversational threads, however,
is not the focus of this paper. Our research addresses
the issue of conversational pragmatics within indi-
vidual message board posts.
Most message board posts contain both exposi-
tory sentences as well as speech acts. The person
posting a message (the ?writer?) often engages in
speech acts with the readers. The writer may explic-
itly greet the readers (?Hi everyone!?), request help
from the readers (?Anyone have a suggestion??), or
commit to a future action (?I promise I will report
back soon.?). But most posts contain factual infor-
mation as well, such as general knowledge or per-
sonal history describing a situation, experience, or
predicament.
Our research goals are twofold: (1) to distin-
guish between expository sentences and speech act
sentences in message board posts, and (2) to clas-
748
sify speech act sentences into four types: Com-
missives, Directives, Expressives, and Representa-
tives, following Searle?s original taxonomy (Searle,
1976). Speech act classification could be useful
for many applications. Information extraction sys-
tems could benefit from filtering speech act sen-
tences (e.g., promises and questions) so that facts are
only extracted from the expository text. Identifying
Directive sentences could be used to summarize the
questions being asked in a forum over a period of
time. Representative sentences could be extracted
to highlight the conclusions and beliefs of domain
experts in response to a question.
In this paper, we present sentence classifiers that
can identify speech act sentences and classify them
as Commissive, Directive, Expressive, and Repre-
sentative. First, we explain how each speech act
class is manifested in message board posts, which
can be different from how they occur in spoken dia-
logue. Second, we train classifiers to identify speech
act sentences using a variety of lexical, syntactic,
and semantic features. Finally, we evaluate our sys-
tem on a collection of message board posts in the
domain of veterinary medicine.
2 Related Work
There has been relatively little work on applying
speech act theory to written text genres, and most
of the previous work has focused on email classi-
fication. Cohen et al (2004) introduced the notion
of ?email speech acts? defined as specific verb-noun
pairs following a pre-designed ontology. They ap-
proached the problem as a document classification
task. Goldstein and Sabin (2006) adopted this no-
tion of email acts (Cohen et al, 2004) but focused
on verb lexicons to classify them. Carvalho and
Cohen (2005) presented a classification scheme us-
ing a dependency network, capturing the sequential
correlations with the context emails using transition
probabilities from or to a target email. Carvalho and
Cohen (2006) later employed N-gram sequence fea-
tures to determine which N-grams are meaningfully
related to different email speech acts with a goal
towards improving their earlier email classification
based on the writer?s intention.
Lampert et al (2006) performed speech act clas-
sification in email messages following a verbal re-
sponse modes (VRM) speech act taxonomy. They
also provided a comparison of VRM taxonomy with
Searle?s taxonomy (Searle, 1976) of speech act
classes. They evaluated several machine learning al-
gorithms using syntactic, morphological, and lexi-
cal features. Mildinhall and Noyes (2008) presented
a stochastic speech act model based on verbal re-
sponse modes (VRM) to classify email intentions.
Some research has considered speech act classes
in other means of online conversations. Twitchell
and Jr. (2004) and Twitchell et al (2004) employed
speech act profiling by plotting potential dialogue
categories in a radar graph to classify conversa-
tions in instant messages and chat rooms. Nas-
tri et al (2006) performed an empirical analysis of
speech acts in the away messages of instant mes-
senger services to achieve a better understanding of
the communication goals of such services. Ravi
and Kim (2007) employed speech act profiling in
online threaded discussions to determine message
roles and to identify threads with questions, answers,
and unanswered questions. They designed their own
speech act categories based on their analysis of stu-
dent interactions in discussion threads.
The work most closely related to ours is the re-
search of Jeong et al (2009) on semi-supervised
speech act recognition in both emails and forums.
Like our work, their research also classifies indi-
vidual sentences, as opposed to entire documents.
However, they trained their classifier on spoken
telephone (SWBD-DAMSL corpus) and meeting
(MRDA corpus) conversations and mapped the la-
belled dialog act classes of these corpora to 12 di-
alog act classes that they found suitable for email
and forum text genres. These dialog act classes (ad-
dressed as speech acts by them) are somewhat differ-
ent from Searle?s original speech act classes. They
also used substantially different types of features
than we do, focusing primarily on syntactic subtree
structures.
3 Classifying Speech Acts in Message
Board Posts
3.1 Speech Act Class Definitions
Searle?s (Searle, 1976) early research on speech acts
was seminal work in natural language processing
that opened up a new way of thinking about con-
749
versational dialogue and communication. Our goal
was to try and use Searle?s original speech act def-
initions and categories as the basis for our work to
the greatest extent possible, allowing for some inter-
pretation as warranted by the WWW message board
text genre.
For the purposes of defining and evaluating our
work, we created detailed annotation guidelines for
four of Searle?s speech act classes that commonly
occur in message board posts: Commissives, Direc-
tives, Expressives, and Representatives. We omitted
the fifth of Searle?s original speech act classes, Dec-
larations, because we virtually never saw declara-
tive speech acts in our data set.1 The data set used in
our study is a collection of message board posts in
the domain of veterinary medicine. We designed our
definitions and guidelines to reflect language use in
the text genre of message board posts, trying to be as
domain-independent as possible so that these defini-
tions should also apply to message board texts rep-
resenting other topics. However, we give examples
from the veterinary domain to illustrate how these
speech act classes are manifested in our data set.
Commissives: A Commissive speech act oc-
curs when the speaker commits to a future course
of action. In conversation, common Commissive
speech acts are promises and threats. In message
boards, these types of Commissives are relatively
rare. However, we found many statements where the
main purpose was to confirm to the readers that the
writer would perform some action in the future. For
example, a doctor may write ?I plan to do surgery on
this patient tomorrow? or ?I will post the test results
when I get them later today?. We viewed such state-
ments as implicit commitments to the reader about
intended actions. We also considered decisions not
to take an action as Commissive speech acts (e.g., ?I
will not do surgery on this cat because it would be
too risky.?). However, statements indicating that an
action will not occur because of circumstances be-
yond the writer?s control were considered to be fac-
tual statements and not speech acts (e.g., ?I cannot
do an ultrasound because my machine is broken.?).
Directives: A Directive speech act occurs when
1Searle defines Declarative speech acts as statements that
bring about a change in status or condition to an object by virtue
of the statement itself. For example, a statement declaring war
or a statement that someone is fired.
the speaker expects the listener to do something as
a response. For example, the speaker may ask a
question, make a request, or issue an invitation. Di-
rective speech acts are common in message board
posts, especially in the initial post of each thread
when the writer explicitly requests help or advice re-
garding a specific topic. Many Directive sentences
are posed as questions, so they are easy to identify
by the presence of a question mark. However, the
language in message board forums is informal and
often ungrammatical, so many Directives are posed
as a question but do not end in a question mark (e.g.,
?What do you think.?). Furthermore, many Direc-
tive speech acts are not stated as a question but as
a request for assistance. For example, a doctor may
write ?I need your opinion on what drug to give this
patient.? Finally, some sentences that end in ques-
tion marks are rhetorical in nature and do not repre-
sent a Directive speech act, such as ?Can you believe
that??.
Expressives: An Expressive speech act occurs in
conversation when a speaker expresses his or her
psychological state to the listener. Typical cases are
when the speaker thanks, apologizes, or welcomes
the listener. Expressive speech acts are common in
message boards because writers often greet readers
at the beginning of a post (?Hi everyone!?) or ex-
press gratitude for help from the readers (?I really
appreciate the suggestions.?). We also found Ex-
pressive speech acts in a variety of other contexts,
such as apologies.
Representatives: According to Searle, a Rep-
resentative speech act commits the speaker to the
truth of an expressed proposition. It represents the
speaker?s belief of something that can be evaluated
to be true or false. These types of speech acts were
less common in our data set, but some cases did ex-
ist. In the veterinary domain, we considered sen-
tences to be a Representative speech act when a
doctor explicitly confirmed a diagnosis or expressed
their suspicion or hypothesis about the presence (or
absence) of a disease or symptom. For example, if a
doctor writes that ?I suspect the patient has pancre-
atitis.? then this represents the doctor?s own propo-
sition/belief about what the disease might be.
Many sentences in our data set are stated as fact
but could be reasonably inferred to be speech acts.
For example, suppose a doctor writes ?The cat has
750
pancreatitis.?. It would be reasonable to infer that
the doctor writing the post diagnosed the cat with
pancreatitis. And in many cases, that is true. How-
ever, we saw many posts where that inference would
have been wrong. For example, the following sen-
tence might say ?The cat was diagnosed by a pre-
vious vet but brought to me due to new complica-
tions? or ?The cat was diagnosed with it 8 years
ago as a kitten in the animal shelter?. Consequently,
we were very conservative in labelling sentences as
Representative speech acts. Any sentence presented
as fact was not considered to be a speech act. A sen-
tence was only labelled as a Representative speech
act if the writer explicitly expressed his belief.
3.2 Features for Speech Act Classification
To create speech act classifiers, we designed a vari-
ety of lexical, syntactic, and semantic features. We
tried to capture linguistic properties associated with
speech act expressions as well as discourse prop-
erties associated with individual sentences and the
message board post as a whole. We also incorpo-
rated speech act word lists that were acquired from
external resources, and used two types of seman-
tic features to represent semantic entities associated
with the veterinary domain. Except for the semantic
features, all of our features are domain-independent
so should be able to recognize speech act sentences
across different domains. We experimented with
domain-specific semantic features to test our hy-
pothesis that Commissive speech acts can be asso-
ciated with domain-specific semantic entities.
For the purposes of analysis, we partition the fea-
ture set into three groups: Lexical and Syntactic
(LexSyn) Features, Speech Act Clue Features, and
Semantic Features. Unless otherwise noted, all of
the features had binary values indicating the pres-
ence or absence of that feature.
3.2.1 Lexical and Syntactic Features
We designed a variety of features to capture lexical
and syntactic properties of words and sentences. We
described the feature set below, with the features cat-
egorized based on the type of information that they
capture.
Unigrams: We created bag-of-word features rep-
resenting each unigram in the training set. Numbers
were replaced with a special # token.
Personal Pronouns: We defined three features to
look for the presence of a 1st person pronoun, 2nd
person pronoun, and 3rd person pronoun. We in-
cluded the subjective, objective, and possessive form
of each pronoun (e.g., he, him, and his).
Tense: Speech acts such as Commissives can be
related to tense. We created three features to iden-
tify verb phrases that occur in the past, present, or
future tense. To recognize tense, we followed the
rules defined by Allen (1995).
Tense + Person: We created four features that re-
quire the presence of a first person subjective pro-
noun (I, we) within a two word window on the left of
a verb phrase matching one of four tense representa-
tions: past, present, future, and present progressive
(a subset of the more general present tense represen-
tation).
Modals: One feature indicates whether the sen-
tence contains a modal (may, must, shall, will,
might, should, would, could).
Infinitive VP: One feature looks for an infinitive
verb phrase (?to? followed by a verb) that is preceded
by a first person pronoun (I, we) within a three word
window on the left. This feature tries to capture
common Commissive expressions (e.g., ?I definitely
plan to do the test tomorrow.?).
Plan Phrases: Commissives are often expressed
as a plan, so we created a feature that recognizes
four types of plan expressions: ?I am going to?, ?I
am planning to?, ?I plan to?, and ?My plan is to?.
Sentence contains Early Punctuation: One fea-
ture checks for the following punctuation marks
within the first three tokens of the sentence: , : ! This
feature was designed to recognize greetings, such as:
?Hi,? , or ?Hiya everyone !?.
Sentence begins with Modal/Verb: One feature
checks if a sentence begins with a modal or verb.
The intuition is to capture interrogative and impera-
tive sentences, since they are likely to be Directives.
Sentence begins with WH Question: One fea-
ture checks if a sentence begins with a WH question
word (Who, When, Where, What, Which, What,
How).
Neighboring Question: One feature checks
whether the following sentence contains a question
mark ???. We observed that in message boards, Di-
rectives often occur in clusters.
751
Sentence Position: Four binary features repre-
sent the relative position of the sentence in the post.
One feature indicates whether it is the first sentence,
one feature indicates whether it is the last sentence,
one feature indicates whether it is the second to last
sentence, and one feature indicates whether the sen-
tence occurs in the bottom 25% of the message. The
motivation for these features is that Expressives of-
ten occur at the beginning and end of the post, and
Directives tend to occur toward the end.
Number of Verbs: One feature represents the
number of verbs in the sentence using four possible
values: 0, 1, 2, >2. Some speech acts classes (e.g.,
Expressives) may occur with no verbs, and rarely
occur in long, complex sentences.
3.2.2 Speech Act Word Clues
We collected speech act word lists (mostly verbs)
from two external sources. In Searle?s original pa-
per (Searle, 1976), he listed words that he consid-
ered to be indicative of speech acts. We discarded
a few that we considered to be overly general, and
we added a few additional words. We also collected
a list of speech act verbs published in (Wierzbicka,
1987). The details for these speech act clue lists are
given below. Our system recognized all derivations
of these words.
Searle Keywords: We created one feature for
each speech act class. The Representative keywords
were: (hypothesize, insist, boast, complain, con-
clude, deduce, diagnose, and claim). We discarded 3
words from Searle?s list (suggest, call, believe) and
added 2 new words, assume and suspect. The Direc-
tive keywords were: (ask, order, command, request,
beg, plead, pray, entreat, invite, permit, advise,
dare, defy, challenge). We added the word please.
The Expressives keywords were: (thank, apolo-
gize, congratulate, condole, deplore, welcome). We
added the words appreciate and sorry. Searle did
not provide any hint on possible indicator words for
Commissives, so we manually defined five likely
Commissive keywords: (plan, commit, promise, to-
morrow, later).
Wierzbicka Verbs: We created one feature that
included 228 speech act verbs listed in the book
?English speech act verbs: a semantic dictionary?
(Wierzbicka, 1987)2.
3.2.3 Semantic Features
All of the previous features are domain-
independent and should be useful for identifying
speech acts sentences across many domains. How-
ever, we hypothesized that semantic entities may
correlate with speech acts within a particular do-
main. For example, consider medical domains. Rep-
resentative speech acts may involve diagnoses and
hypotheses regarding diseases and symptoms. Sim-
ilarly, Commissive speech acts may reveal a doc-
tor?s plan or intention regarding the administration
of drugs or tests. Thus, it may be beneficial for a
classifier to know whether a sentence contains cer-
tain semantic entities. We experimented with two
different sources of semantic information.
Semantic Lexicon: Basilisk (Thelen and Riloff,
2002) is a bootstrapping algorithm that has been
used to induce semantic lexicons for terrorist events
(Thelen and Riloff, 2002), biomedical concepts
(McIntosh, 2010), and subjective/objective nouns
for opinion analysis (Riloff et al, 2003). We
ran Basilisk over our collection of 15,383 veteri-
nary message board posts to create a semantic lex-
icon for veterinary medicine. As input, Basilisk
requires seed words for each semantic category.
To obtain seeds, we parsed the corpus using a
noun phrase chunker, sorted the head nouns by fre-
quency, and manually identified the 20 most fre-
quent nouns belonging to four semantic categories:
DISEASE/SYMPTOM, DRUG, TEST, and TREAT-
MENT.
However, the induced TREATMENT lexicon was
of relatively poor quality so we did not use it. The
DISEASE/SYMPTOM lexicon appeared to be of good
quality, but it did not improve the performance of
our speech act classifiers. We suspect that this is due
to the fact that diseases were not distinguised from
symptoms in our lexicon.3 Representative speech
acts are typically associated with disease diagnoses
2openlibrary.org/b/OL2413134M/English_
speech_act_verbs
3We induced a single lexicon for diseases and symptoms be-
cause it is difficult to draw a clear line between them seman-
tically. A veterinary consultant explained to us that the same
term (e.g., diabetes) may be considered a symptom in one con-
text if it is secondary to another condition (e.g., pancreatitis) but
a disease in a different context if it is the primary diagnosis.
752
and hypotheses, rather than individual symptoms.
In the end, we only used the DRUG and TEST se-
mantic lexicon in our classifiers. We used all 1000
terms in the DRUG lexicon, but only used the top
200 TEST words because the quality of the lexicon
seemed questionable after that point.
Semantic Tags: We also used bootstrapped con-
textual semantic taggers (Huang and Riloff, 2010)
that had been previously trained for the domain of
veterinary medicine. These taggers assign seman-
tic class labels to noun phrase instances based on
the surrounding context in a sentence. The tag-
gers were trained on 4,629 veterinary message board
posts using 10 seed words for each semantic cate-
gory (see (Huang and Riloff, 2010) for details). To
ensure good precision, only tags that have a confi-
dence value ? 1.0 were used. Our speech act classi-
fiers used the tags associated with two semantic cat-
egories: DRUG and TEST.
3.3 Classification
To create our classifiers, we used the Weka (Hall et
al., 2009) machine learning toolkit. We used Sup-
port Vector Machines (SVMs) with a polynomial
kernel and the default settings supplied by Weka.
Because a sentence can include multiple speech acts,
we created a set of binary classifiers, one for each of
the four speech act classes. All four classifiers were
applied to each sentence, so a sentence could be as-
signed multiple speech act classes.
4 Evaluation
4.1 Data Set
Our data set consists of message board posts from
the Veterinary Information Network (VIN), which is
a web site (www.vin.com) for professionals in vet-
erinary medicine. Among other things, VIN hosts
message board forums where veterinarians and other
veterinary professionals can discuss issues and pose
questions to each other. Over half of the small an-
imal veterinarians in the U.S. and Canada use the
VIN web site.
We obtained 15,383 VIN message board threads
representing three topics: cardiology, endocrinol-
ogy, and feline internal medicine. We did basic
cleaning, removing html tags and tokenizing num-
bers. We then applied the Stanford part-of-speech
tagger (Toutanova et al, 2003) to each sentence to
obtain part-of-speech tags for the words. For our ex-
periments, we randomly selected 150 message board
threads from this collection. Since the goal of our
work was to study speech acts in sentences, and not
the conversational dialogue between different writ-
ers, we used only the initial post of each thread.
These 150 message board posts contained a total of
1,956 sentences, with an average of 13.04 sentences
per post. In the next section, we explain how we
manually annotated each sentence in our data set to
create gold standard speech act labels.
4.2 Gold Standard Annotations
To create training and evaluation data for our re-
search, we asked two human annotators to manually
label sentences in our message board posts. Iden-
tifying speech acts is not always obvious, even to
people, so we gave them detailed annotation guide-
lines describing the four speech act classes discussed
in Section 3.1. Then we gave them the same set of
50 message board posts from our collection to an-
notate independently. Each annotator was told to
assign one or more speech act classes to each sen-
tence (COM, DIR, EXP, REP), or to label the sen-
tence as having no speech acts (NONE). The vast
majority of sentences had either no speech acts or
at most one speech act, but a small number of sen-
tences contained multiple types of speech acts.
We measured the inter-annotator agreement of the
two human judges using the kappa (?) score (Car-
letta, 1996). However, kappa agreement scores are
only applicable to labelling schemes where each in-
stance receives a single label. Therefore we com-
puted kappa agreement in two different ways to look
at the results from two different perspectives. In the
first scheme, we discarded the small number of sen-
tences that had multiple speech act labels and com-
puted kappa on the rest.4 This produced a kappa
score of .95, suggesting extremely high agreement.
However, over 70% of the sentences in our data set
have no speech act at all, so NONE was by far the
most common label. Consequently, this agreement
score does not necessarily reflect how consistently
the judges agreed on the four speech act classes.
4Of the 594 sentences in these 50 posts, only 22 sentences
contained multiple speech act classes.
753
In the second scheme, we computed kappa for
each speech act category independently. For each
category C, the judges were considered to be in
agreement if both of them assigned category C to
the sentence or if neither of the judges assigned cat-
egory C to the sentence. Table 1 shows the ? agree-
ment scores using this approach.
Speech Act Kappa (?) score
Expressive .97
Directive .94
Commissive .81
Representative .77
Table 1: Inter-annotator (?) agreement
Inter-annotator agreement was very high for both
the Expressive and Directive classes. Agreement
was lower for the Commissive and Representative
classes, but still relatively good so we felt comfort-
able that we had high-quality annotations.
To create our final data set, the two judges adjudi-
cated their disagreements on this set of 50 posts. We
then asked each annotator to label an additional (dif-
ferent) set of 50 posts each. All together, this gave
us a gold standard data set consisting of 150 anno-
tated message board posts. Table 2 shows the distri-
bution of speech act labels in our data set. 71% of
the sentences did not include any speech acts. These
were usually expository sentences containing factual
information. 29% of the sentences included one or
more speech acts, so nearly 13 of the sentences wereconversational in nature. Directive and Expressive
speech acts are by far the most common, with nearly
26% of all sentences containing one of these speech
acts. Commissive and Representative speech acts
are less common, each occurring in less than 3% of
the sentences.5
4.3 Experimental Results
4.3.1 Speech Act Filtering
For our first experiment, we created a speech act
filtering classifier to distinguish sentences that con-
tain one or more speech acts from sentences that do
not contain any speech acts. Sentences labelled as
5These numbers do not add up to 100% because some sen-
tences contain multiple speech acts.
Speech Act # sentences distribution
None 1397 71.42%
Directive 311 15.90%
Expressive 194 9.92%
Representative 57 2.91%
Commissive 51 2.61%
Table 2: Speech act class distribution in our data set.
having one or more speech acts were positive in-
stances, and sentences labelled as NONE were neg-
ative instances. Speech act filtering could be useful
for many applications, such as information extrac-
tion systems that only seek to extract facts. For ex-
ample, information may be posed as a question (in
a Directive) rather than a fact, information may be
mentioned as part of a future plan (in a Commis-
sive) that has not actually happened yet, or informa-
tion may be stated as a hypothesis or suspicion (in a
Representative) rather than as a fact.
We performed 10-fold cross validation on our set
of 150 annotated message board posts. Initially, we
used all of the features defined in Section 3.2. How-
ever, during the course of our research we discov-
ered that only a small subset of the lexical and syn-
tactic features seemed to be useful, and that remov-
ing the unnecessary features improved performance.
So we created a subset of minimal lexsyn features,
which will be described in Section 4.3.2. For speech
act filtering, we used the minimal lexsyn features
plus the speech act clues and semantic features.6
Class P R F
Speech Act .86 .83 .84
No Speech Act .93 .95 .94
Table 3: Precision, Recall, F-measure for speech act fil-
tering.
Table 3 shows the performance for speech act
filtering with respect to Precision (P), Recall (R),
and F-measure score (F).7 The classifier performed
well, recognizing 83% of the speech act sentences
with 86% precision, and 95% of the expository (no
6This is the same feature set used to produce the results for
row E of Table 4.
7We computed an F1 score with equal weighting of preci-
sion and recall.
754
Commissives Directives Expressives Representatives
Features P R F P R F P R F P R F
Baselines
Com baseline .45 .08 .14 - - - - - - - - -
Dir baseline - - - .97 .73 .83 - - - - - -
Exp baseline 1 - - - - - - .58 .18 .28 - - -
Exp baseline 2 - - - - - - .97 .86 .91 - - -
Rep baseline - - - - - - - - - 1.0 .05 .10
Classifiers
U Unigram .45 .20 .27 .87 .84 .85 .97 .88 .92 .32 .12 .18
A U+all lexsyn .52 .33 .40 .87 .84 .86 .98 .88 .92 .30 .14 .19
B U+minimal lexsyn .59 .33 .42 .87 .85 .86 .98 .88 .92 .32 .14 .20
C B+speechActClues .57 .31 .41 .86 .84 .85 .97 .91 .94 .33 .16 .21
D C+semTest .64 .35 .46 .87 .84 .85 .97 .91 .94 .33 .16 .21
E D+semDrug .63 .39 .48 .86 .84 .85 .97 .91 .94 .32 .16 .21
Table 4: Precision, Recall, F-measure for four speech act classes. The highest F score for each category appears in
boldface.
speech act) sentences with 93% precision.
4.3.2 Speech Act Categorization
BASELINES
Our next set of experiments focused on labelling
sentences with the four specific speech act classes:
Commissive, Directive. Expressive, and Represen-
tative. To assess the difficulty of identifying each
speech act category, we created several simple base-
lines using our intuitions about each category.
For Commissives, we created a heuristic to cap-
ture the most obvious cases of future tense (because
Commissive speech acts represent a writer?s com-
mitment toward a future course of action). For ex-
ample, the presence of the phrases ?I will? and ?I
shall? were hypothesized by Cohen et al (2004) to
be useful bigram clues for Commissives. This base-
line looks for future tense verb phrases with a 1st
person pronoun within one or two words preceding
the verb phrase. The Com baseline row of Table 4
shows the results for this heuristic, which obtained
8% recall with 45% precision. The heuristic applied
to only 9 sentences in our test set, 4 of which con-
tained a Commissive speech act.
Directive speech acts are often questions, so we
created a baseline system that labels all sentences
containing a question mark as a Directive. The Dir
baseline row of Table 4 shows that 97% of sentences
with a question mark were indeed Directives.8 But
only 73% of the Directive sentences contained a
question mark. The remaining 27% of Directives
did not contain a question mark and generally fell
into two categories. Some sentences asked a ques-
tion but the writer ended the sentence with a period
(e.g., ?Has anyone seen this before.?). And many di-
rectives were expressed as requests rather than ques-
tions (e.g., ?Let me know if anyone has a sugges-
tion.?).
For Expressives, we implemented two baselines.
Exp baseline 1 simply looks for an exclamation
mark, but this heuristic did not work well (18% re-
call with 58% precision) because exclamation marks
were often used for general emphasis (e.g., ?The
owner is frustrated with cleaning up urine!?). Exp
baseline 2 looks for the presence of four common
expressive words (appreciate, hi, hello, thank), in-
cluding morphological variations of appreciate and
thank. This baseline produced very good results,
86% recall with 97% precision. Obviously a small
set of common expressions account for most of the
Expressive speech acts in our corpus. However, the
word ?hi? did produce some false hits because it was
used as a shorthand for ?high?, usually when report-
ing test results (e.g., ?hi calcium?).
8235 sentences contained a question mark, and 227 of them
were Directives.
755
Finally, as a baseline for the Representative class
we simply looked for the words diagnose(d) and sus-
pect(ed). The Rep baseline row of Table 4 shows
that this heuristic was 100% accurate, but only pro-
duced 5% recall (matching 3 of the 57 Representa-
tive sentences in our test set).
CLASSIFIER RESULTS
The bottom portion of Table 4 shows the results
for our classifiers. As we explained in Section 3.3,
we created one classifier for each speech act cate-
gory, and all four classifiers were applied to each
sentence. So a sentence could receive anywhere
from 0-4 speech act labels indicating how many dif-
ferent types of speech acts appeared in the sentence.
We trained and evaluated each classifier using 10-
fold cross-validation on our gold standard data set.
The Unigram (U) row shows the performance of
classifiers that use only unigram features. For Di-
rectives, we see a 2% F-score improvement over the
baseline, which reflects a recall gain of 11% but
a corresponding precision loss of 10%. The uni-
grams are clearly helpful in identifying many Direc-
tive sentences that do not end in a question mark,
but at some cost to accuracy. For Expressives, the
unigram classifier achieves an F score of 92%, iden-
tifying slightly more Expressive sentences than the
baseline with the same level of precision. For Com-
missives and Representatives, the unigram classi-
fiers performed susbtantially better than their corre-
sponding baseline systems, but performance is still
relatively weak.
Row A (U+ all lexsyn) in Table 4 shows the re-
sults using unigram features plus all of the lexical
and syntactic features described in Section 3.2.1.
The lexical and syntactic features dramatically im-
prove performance on Commissives, increasing F
score from 27% to 40%, and they produce a 2% re-
call gain for Representatives but with a correspond-
ing loss of precision.
However, we observed that only a few of the lex-
ical and syntactic features had much impact on per-
formance. We experimented with different subsets
of the features and obtained even better performance
when using just 10 of them, which we will refer to as
the minimal lexsyn features. The minimal lexsyn fea-
ture set consists of the 4 Tense+Person features, the
Early Punctuation feature, the Sentence begins with
Modal/Verb feature, and the 4 Sentence Position fea-
tures. Row B shows the results using unigram fea-
tures plus only these minimal lexsyn features. Preci-
sion improves for Commissives by an additional 7%
and Representatives by 2% when using only these
lexical and syntactic features. Consequently, we use
the minimal lexsyn features for the rest of our exper-
iments.
Row C shows the results of adding the speech act
clue words (see Section 3.2.2) to the feature set used
in Row B. The speech act clue words produced an
additional recall gain of 3% for Expressives and 2%
for Representatives, although performance on Com-
missives dropped 2% in both recall and precision.
Rows D and E show the results of adding the se-
mantic features. We added one semantic category
at a time to measure the impact of them separately.
Row D adds two semantic features for the TEST cat-
egory, one from the Basilisk lexicon and one from
the semantic tagger. The TEST semantic features
produced an F-score gain of 5% for Commissives,
improving recall by 4% and precision by 7%. Row
E adds two semantic features for the DRUG category.
The DRUG features produced an additional F-score
gain of 2% for Commissives, improving recall by
4% with a slight drop in precision.
4.4 Analysis
Together, the TEST and DRUG semantic features dra-
matically improved the classifier?s ability to recog-
nize Commissive speech acts, increasing its F score
from 41% ? 48%. This result demonstrates that
in the domain of veterinary medicine, some types
of semantic entities are associated with speech acts.
Our intuition behind this result is that commitments
are usually related to future actions. In veterinary
medicine, TESTS and DRUGS are associated with ac-
tions performed by doctors. Doctors help their pa-
tients by prescribing or administering drugs and by
conducting tests. So these semantic entities may
serve as a proxy to implicitly represent actions that
the doctor has done or may do. In future work, ex-
plicitly recognizing actions and events many be a
worthwhile avenue to further improve results.
We achieved good success at identifying both Di-
rectives and Expressives, although simple heuristics
also perform well on these categories. We showed
that training a Directive classifier can help to iden-
756
tify Directive sentences that do not end with a ques-
tion mark, although at the cost of some precision.
The Commissive speech act class benefitted the
most from the rich feature set. Unigrams are clearly
not sufficient to identify Commissive sentences.
Many different types of clues seem to be important
for recognizing these sentences. The improvements
obtained from adding semantic features also sug-
gests that domain-specific semantics can be useful
for recognizing some speech acts. However, there is
still ample room for improvement, illustrating that
speech act classification is a challenging problem.
Representative speech acts were by far the most
difficult to recognize. We believe that there are
several reasons for their low performance. First,
Representatives were sparse in the data set, occur-
ring in only 2.91% of the sentences. Consequently,
the classifier had relatively few positive training
instances. Second, Representatives had the low-
est inter-annotator agreement, indicating that human
judges had difficulty recognizing these speech acts
too. The judges often disagreed about whether a
hypothesis or suspicion was the writer?s own belief
or whether it was stated as a fact reflecting general
medical knowledge. The message board text genre
is especially challenging in this regard because the
writer is often presumed to be expressing his/her be-
liefs even when the writer does not explicitly say so.
Finally, our semantic features could not distinguish
between diseases and symptoms. Access to a re-
source that can reliably identify disease terms could
potentially improve performance in this domain.
5 Conclusions
Our goal was to identify speech act sentences in
message board posts and to classify the sentences
with respect to four categories in Searle?s (1976)
speech act taxonomy. We achieved good results for
speech act filtering and the identification of Direc-
tive and Expressive speech act sentences. We found
that Representative and Commissive speech acts are
much more difficult to identify, although the per-
formance of our Commissive classifier substantially
improved with the addition of lexical, syntactic, and
semantic features. Except for the semantic class
information, our feature set is domain-independent
and could be used to recognize speech act sentences
in message boards for any domain. Furthermore, our
features only rely on part-of-speech tags and do not
require parsing, which is of practical importance for
text genres such as message boards that are littered
with ungrammatical text, typos, and shorthand nota-
tions.
In future work, we believe that segmenting sen-
tences into clauses may help to train classifiers more
precisely. Ultimately, we would like to identify
the speech act expressions themselves because some
sentences contain speech acts as well as factual in-
formation. Extracting the speech act expressions
and clauses from message boards and similar text
genres could provide better tracking of questions
and answers in web forums and be used for sum-
marization.
6 Acknowledgments
We gratefully acknowledge that this research was
supported in part by the National Science Founda-
tion under grant IIS-1018314. Any opinions, find-
ings, and conclusion or recommendations expressed
in this material are those of the authors and do not
necessarily reflect the view of the U.S. government.
References
James Allen. 1995. Natural language understanding
(2nd ed.). Benjamin-Cummings Publishing Co., Inc.,
Redwood City, CA, USA.
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: the kappa statistic. Comput. Linguist.,
22:249?254, June.
Vitor R. Carvalho and William W. Cohen. 2005. On the
collective classification of email ?speech acts?. In SI-
GIR ?05: Proceedings of the 28th annual international
ACM SIGIR conference on Research and development
in information retrieval, pages 345?352, New York,
NY, USA. ACM Press.
Vitor R. Carvalho and William W. Cohen. 2006. Improv-
ing ?email speech acts? analysis via n-gram selection.
In Proceedings of the HLT-NAACL 2006 Workshop on
Analyzing Conversations in Text and Speech, ACTS
?09, pages 35?41, Stroudsburg, PA, USA. Association
for Computational Linguistics.
William W. Cohen, Vitor R. Carvalho, and Tom M.
Mitchell. 2004. Learning to classify email into
?speech acts?. In EMNLP, pages 309?316. ACL.
Jade Goldstein and Roberta Evans Sabin. 2006. Using
speech acts to categorize email and identify email gen-
757
res. In Proceedings of the 39th Annual Hawaii Inter-
national Conference on System Sciences - Volume 03,
pages 50.2?, Washington, DC, USA. IEEE Computer
Society.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11:10?18, November.
Ruihong Huang and Ellen Riloff. 2010. Inducing
domain-specific semantic class taggers from (almost)
nothing. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics, ACL
?10, pages 275?285, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Minwoo Jeong, Chin-Yew Lin, and Gary Geunbae Lee.
2009. Semi-supervised speech act recognition in
emails and forums. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 3 - Volume 3, EMNLP ?09, pages
1250?1259, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Andrew Lampert, Robert Dale, and Cecile Paris. 2006.
Classifying speech acts using verbal response modes.
In Proceedings of the 2006 Australasian Language
Technology Workshop (ALTW2006), pages 34?41.
Sydney Australia : ALTA.
Tara McIntosh. 2010. Unsupervised discovery of neg-
ative categories in lexicon bootstrapping. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?10,
pages 356?365, Stroudsburg, PA, USA. Association
for Computational Linguistics.
John Mildinhall and Jan Noyes. 2008. Toward a stochas-
tic speech act model of email behavior. In CEAS.
Jacqueline Nastri, Jorge Pena, and Jeffrey T. Hancock.
2006. The construction of away messages: A speech
act analysis. J. Computer-Mediated Communication,
pages 1025?1045.
Sujith Ravi and Jihie Kim. 2007. Profiling student inter-
actions in threaded discussions with speech act classi-
fiers. In Proceeding of the 2007 conference on Arti-
ficial Intelligence in Education: Building Technology
Rich Learning Contexts That Work, pages 357?364,
Amsterdam, The Netherlands, The Netherlands. IOS
Press.
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003.
Learning subjective nouns using extraction pattern
bootstrapping. In Proceedings of the seventh confer-
ence on Natural language learning at HLT-NAACL
2003 - Volume 4, CONLL ?03, pages 25?32, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
John R. Searle. 1976. A classification of illocutionary
acts. Language in Society, 5(1):pp. 1?23.
Michael Thelen and Ellen Riloff. 2002. A bootstrapping
method for learning semantic lexicons using extrac-
tion pattern contexts. In Proceedings of the ACL-02
conference on Empirical methods in natural language
processing - Volume 10, EMNLP ?02, pages 214?221,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ?03, pages 173?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Douglas P. Twitchell and Jay F. Nunamaker Jr. 2004.
Speech act profiling: a probabilistic method for ana-
lyzing persistent conversations and their participants.
In System Sciences, 2004. Proceedings of the 37th An-
nual Hawaii International Conference on, pages 1?10,
January.
Douglas P. Twitchell, Mark Adkins, Jay F. Nunamaker
Jr., and Judee K. Burgoon. 2004. Using speech act
theory to model conversations for automated classi-
fication and retrieval. In Proceedings of the Inter-
national Working Conference Language Action Per-
spective Communication Modelling (LAP 2004), pages
121?130.
A. Wierzbicka. 1987. English speech act verbs: a se-
mantic dictionary. Academic Press, Sydney, Orlando.
758
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 704?714,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Sarcasm as Contrast between a Positive Sentiment and Negative Situation
Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalindra De Silva,
Nathan Gilbert, Ruihong Huang
School Of Computing
University of Utah
Salt Lake City, UT 84112
{riloff,asheq,alnds,ngilbert,huangrh}@cs.utah.edu, prafulla.surve@gmail.com
Abstract
A common form of sarcasm on Twitter con-
sists of a positive sentiment contrasted with a
negative situation. For example, many sarcas-
tic tweets include a positive sentiment, such as
?love? or ?enjoy?, followed by an expression
that describes an undesirable activity or state
(e.g., ?taking exams? or ?being ignored?). We
have developed a sarcasm recognizer to iden-
tify this type of sarcasm in tweets. We present
a novel bootstrapping algorithm that automati-
cally learns lists of positive sentiment phrases
and negative situation phrases from sarcastic
tweets. We show that identifying contrast-
ing contexts using the phrases learned through
bootstrapping yields improved recall for sar-
casm recognition.
1 Introduction
Sarcasm is generally characterized as ironic or satir-
ical wit that is intended to insult, mock, or amuse.
Sarcasm can be manifested in many different ways,
but recognizing sarcasm is important for natural lan-
guage processing to avoid misinterpreting sarcastic
statements as literal. For example, sentiment anal-
ysis can be easily misled by the presence of words
that have a strong polarity but are used sarcastically,
which means that the opposite polarity was intended.
Consider the following tweet on Twitter, which in-
cludes the words ?yay? and ?thrilled? but actually
expresses a negative sentiment: ?yay! it?s a holi-
day weekend and i?m on call for work! couldn?t be
more thrilled! #sarcasm.? In this case, the hashtag
#sarcasm reveals the intended sarcasm, but we don?t
always have the benefit of an explicit sarcasm label.
In the realm of Twitter, we observed that many
sarcastic tweets have a common structure that
creates a positive/negative contrast between a senti-
ment and a situation. Specifically, sarcastic tweets
often express a positive sentiment in reference to a
negative activity or state. For example, consider the
tweets below, where the positive sentiment terms
are underlined and the negative activity/state terms
are italicized.
(a) Oh how I love being ignored. #sarcasm
(b) Thoroughly enjoyed shoveling the driveway
today! :) #sarcasm
(c) Absolutely adore it when my bus is late
#sarcasm
(d) I?m so pleased mom woke me up with
vacuuming my room this morning. :) #sarcasm
The sarcasm in these tweets arises from the jux-
taposition of a positive sentiment word (e.g., love,
enjoyed, adore, pleased) with a negative activity or
state (e.g., being ignored, bus is late, shoveling, and
being woken up).
The goal of our research is to identify sarcasm
that arises from the contrast between a positive sen-
timent referring to a negative situation. A key chal-
lenge is to automatically recognize the stereotypi-
cally negative ?situations?, which are activities and
states that most people consider to be unenjoyable or
undesirable. For example, stereotypically unenjoy-
able activities include going to the dentist, taking an
exam, and having to work on holidays. Stereotypi-
cally undesirable states include being ignored, hav-
ing no friends, and feeling sick. People recognize
704
these situations as being negative through cultural
norms and stereotypes, so they are rarely accompa-
nied by an explicit negative sentiment. For example,
?I feel sick? is universally understood to be a nega-
tive situation, even without an explicit expression of
negative sentiment. Consequently, we must learn to
recognize phrases that correspond to stereotypically
negative situations.
We present a bootstrapping algorithm that auto-
matically learns phrases corresponding to positive
sentiments and phrases corresponding to negative
situations. We use tweets that contain a sarcasm
hashtag as positive instances for the learning pro-
cess. The bootstrapping algorithm begins with a sin-
gle seed word, ?love?, and a large set of sarcastic
tweets. First, we learn negative situation phrases
that follow a positive sentiment (initially, the seed
word ?love?). Second, we learn positive sentiment
phrases that occur near a negative situation phrase.
The bootstrapping process iterates, alternately learn-
ing new negative situations and new positive sen-
timent phrases. Finally, we use the learned lists
of sentiment and situation phrases to recognize sar-
casm in new tweets by identifying contexts that con-
tain a positive sentiment in close proximity to a neg-
ative situation phrase.
2 Related Work
Researchers have investigated the use of lexical
and syntactic features to recognize sarcasm in text.
Kreuz and Caucci (2007) studied the role that dif-
ferent lexical factors play, such as interjections (e.g.,
?gee? or ?gosh?) and punctuation symbols (e.g., ???)
in recognizing sarcasm in narratives. Lukin and
Walker (2013) explored the potential of a bootstrap-
ping method for sarcasm classification in social di-
alogue to learn lexical N-gram cues associated with
sarcasm (e.g., ?oh really?, ?I get it?, ?no way?, etc.)
as well as lexico-syntactic patterns.
In opinionated user posts, Carvalho et al (2009)
found oral or gestural expressions, represented us-
ing punctuation and other keyboard characters, to
be more predictive of irony1 in contrast to features
representing structured linguistic knowledge in Por-
1They adopted the term ?irony? instead of ?sarcasm? to re-
fer to the case when a word or expression with prior positive
polarity is figuratively used to express a negative opinion.
tuguese. Filatova (2012) presented a detailed de-
scription of sarcasm corpus creation with sarcasm
annotations of Amazon product reviews. Their an-
notations capture sarcasm both at the document level
and the text utterance level. Tsur et al (2010) pre-
sented a semi-supervised learning framework that
exploits syntactic and pattern based features in sar-
castic sentences of Amazon product reviews. They
observed correlated sentiment words such as ?yay!?
or ?great!? often occurring in their most useful pat-
terns.
Davidov et al (2010) used sarcastic tweets and
sarcastic Amazon product reviews to train a sarcasm
classifier with syntactic and pattern-based features.
They examined whether tweets with a sarcasm hash-
tag are reliable enough indicators of sarcasm to be
used as a gold standard for evaluation, but found that
sarcasm hashtags are noisy and possibly biased to-
wards the hardest form of sarcasm (where even hu-
mans have difficulty). Gonza?lez-Iba?n?ez et al (2011)
explored the usefulness of lexical and pragmatic fea-
tures for sarcasm detection in tweets. They used sar-
casm hashtags as gold labels. They found positive
and negative emotions in tweets, determined through
fixed word dictionaries, to have a strong correlation
with sarcasm. Liebrecht et al (2013) explored N-
gram features from 1 to 3-grams to build a classifier
to recognize sarcasm in Dutch tweets. They made an
interesting observation from their most effective N-
gram features that people tend to be more sarcastic
towards specific topics such as school, homework,
weather, returning from vacation, public transport,
the church, the dentist, etc. This observation has
some overlap with our observation that stereotypi-
cally negative situations often occur in sarcasm.
The cues for recognizing sarcasm may come from
a variety of sources. There exists a line of work
that tries to identify facial and vocal cues in speech
(e.g., (Gina M. Caucci, 2012; Rankin et al, 2009)).
Cheang and Pell (2009) and Cheang and Pell (2008)
performed studies to identify acoustic cues in sarcas-
tic utterances by analyzing speech features such as
speech rate, mean amplitude, amplitude range, etc.
Tepperman et al (2006) worked on sarcasm recog-
nition in spoken dialogue using prosodic and spec-
tral cues (e.g., average pitch, pitch slope, etc.) as
well as contextual cues (e.g., laughter or response to
questions) as features.
705
While some of the previous work has identi-
fied specific expressions that correlate with sarcasm,
none has tried to identify contrast between positive
sentiments and negative situations. The novel con-
tributions of our work include explicitly recogniz-
ing contexts that contrast a positive sentiment with a
negative activity or state, as well as a bootstrapped
learning framework to automatically acquire posi-
tive sentiment and negative situation phrases.
3 Bootstrapped Learning of Positive
Sentiments and Negative Situations
Sarcasm is often defined in terms of contrast or ?say-
ing the opposite of what you mean?. Our work fo-
cuses on one specific type of contrast that is common
on Twitter: the expression of a positive sentiment
(e.g., ?love? or ?enjoy?) in reference to a negative
activity or state (e.g., ?taking an exam? or ?being ig-
nored?). Our goal is to create a sarcasm classifier for
tweets that explicitly recognizes contexts that con-
tain a positive sentiment contrasted with a negative
situation.
Our approach learns rich phrasal lexicons of pos-
itive sentiments and negative situations using only
the seed word ?love? and a collection of sarcastic
tweets as input. A key factor that makes the algo-
rithm work is the presumption that if you find a pos-
itive sentiment or a negative situation in a sarcastic
tweet, then you have found the source of the sar-
casm. We further assume that the sarcasm probably
arises from positive/negative contrast and we exploit
syntactic structure to extract phrases that are likely
to have contrasting polarity. Another key factor is
that we focus specifically on tweets. The short na-
ture of tweets limits the search space for the source
of the sarcasm. The brevity of tweets also probably
contributes to the prevalence of this relatively com-
pact form of sarcasm.
3.1 Overview of the Learning Process
Our bootstrapping algorithm operates on the as-
sumption that many sarcastic tweets contain both a
positive sentiment and a negative situation in close
proximity, which is the source of the sarcasm.2 Al-
though sentiments and situations can be expressed
2Sarcasm can arise from a negative sentiment contrasted
with a positive situation too, but our observation is that this is
much less common, at least on Twitter.
Positive
Sentiment
Phrases
Negative
Situation
Phrases
Seed Word
"love"
Sarcastic Tweets
1 2
34
Figure 1: Bootstrapped Learning of Positive Sentiment
and Negative Situation Phrases
in numerous ways, we focus on positive sentiments
that are expressed as a verb phrase or as a predicative
expression (predicate adjective or predicate nomi-
nal), and negative activities or states that can be a
complement to a verb phrase. Ideally, we would
like to parse the text and extract verb complement
phrase structures, but tweets are often informally
written and ungrammatical. Therefore we try to rec-
ognize these syntactic structures heuristically using
only part-of-speech tags and proximity.
The learning process relies on an assumption that
a positive sentiment verb phrase usually appears to
the left of a negative situation phrase and in close
proximity (usually, but not always, adjacent). Picto-
rially, we assume that many sarcastic tweets contain
this structure:
[+ VERB PHRASE] [? SITUATION PHRASE]
This structural assumption drives our bootstrap-
ping algorithm, which is illustrated in Figure 1.
The bootstrapping process begins with a single seed
word, ?love?, which seems to be the most common
positive sentiment term in sarcastic tweets. Given
a sarcastic tweet containing the word ?love?, our
structural assumption infers that ?love? is probably
followed by an expression that refers to a negative
situation. So we harvest the n-grams that follow the
word ?love? as negative situation candidates. We se-
lect the best candidates using a scoring metric, and
add them to a list of negative situation phrases.
Next, we exploit the structural assumption in the
opposite direction. Given a sarcastic tweet that con-
tains a negative situation phrase, we infer that the
negative situation phrase is preceded by a positive
sentiment. We harvest the n-grams that precede the
negative situation phrases as positive sentiment can-
didates, score and select the best candidates, and
706
add them to a list of positive sentiment phrases.
The bootstrapping process then iterates, alternately
learning more positive sentiment phrases and more
negative situation phrases.
We also observed that positive sentiments are fre-
quently expressed as predicative phrases (i.e., pred-
icate adjectives and predicate nominals). For ex-
ample: ?I?m taking calculus. It is awesome. #sar-
casm?. Wiegand et al (2013) offered a related ob-
servation that adjectives occurring in predicate ad-
jective constructions are more likely to convey sub-
jectivity than adjectives occurring in non-predicative
structures. Therefore we also include a step in
the learning process to harvest predicative phrases
that occur in close proximity to a negative situation
phrase. In the following sections, we explain each
step of the bootstrapping process in more detail.
3.2 Bootstrapping Data
For the learning process, we used Twitter?s stream-
ing API to obtain a large set of tweets. We col-
lected 35,000 tweets that contain the hashtag #sar-
casm or #sarcastic to use as positive instances of sar-
casm. We also collected 140,000 additional tweets
from Twitter?s random daily stream. We removed
the tweets that contain a sarcasm hashtag, and con-
sidered the rest to be negative instances of sarcasm.
Of course, there will be some sarcastic tweets that do
not have a sarcasm hashtag, so the negative instances
will contain some noise. But we expect that a very
small percentage of these tweets will be sarcastic, so
the noise should not be a major issue. There will also
be noise in the positive instances because a sarcasm
hashtag does not guarantee that there is sarcasm in
the body of the tweet (e.g., the sarcastic content may
be in a linked url, or in a prior tweet). But again, we
expect the amount of noise to be relatively small.
Our tweet collection therefore contains a total of
175,000 tweets: 20% are labeled as sarcastic and
80% are labeled as not sarcastic. We applied CMU?s
part-of-speech tagger designed for tweets (Owoputi
et al, 2013) to this data set.
3.3 Seeding
The bootstrapping process begins by initializing the
positive sentiment lexicon with one seed word: love.
We chose this seed because it seems to be the most
common positive sentiment word in sarcastic tweets.
3.4 Learning Negative Situation Phrases
The first stage of bootstrapping learns new phrases
that correspond to negative situations. The learning
process consists of two steps: (1) harvesting candi-
date phrases, and (2) scoring and selecting the best
candidates.
To collect candidate phrases for negative situa-
tions, we extract n-grams that follow a positive senti-
ment phrase in a sarcastic tweet. We extract every 1-
gram, 2-gram, and 3-gram that occurs immediately
after (on the right-hand side) of a positive sentiment
phrase. As an example, consider the tweet in Figure
2, where ?love? is the positive sentiment:
I love waiting forever for the doctor #sarcasm
Figure 2: Example Sarcastic Tweet
We extract three n-grams as candidate negative situ-
ation phrases:
waiting, waiting forever, waiting forever for
Next, we apply the part-of-speech (POS) tagger
and filter the candidate list based on POS patterns so
we only keep n-grams that have a desired syntactic
structure. For negative situation phrases, our goal
is to learn possible verb phrase (VP) complements
that are themselves verb phrases because they should
represent activities and states. So we require a can-
didate phrase to be either a unigram tagged as a verb
(V) or the phrase must match one of 7 POS-based
bigram patterns or 20 POS-based trigram patterns
that we created to try to approximate the recogni-
tion of verbal complement structures. The 7 POS bi-
gram patterns are: V+V, V+ADV, ADV+V, ?to?+V,
V+NOUN, V+PRO, V+ADJ. Note that we used
a POS tagger designed for Twitter, which has a
smaller set of POS tags than more traditional POS
taggers. For example there is just a single V tag
that covers all types of verbs. The V+V pattern will
therefore capture negative situation phrases that con-
sist of a present participle verb followed by a past
participle verb, such as ?being ignored? or ?getting
hit?.3 We also allow verb particles to match a V tag
in our patterns. The remaining bigram patterns cap-
ture verb phrases that include a verb and adverb, an
3In some cases it may be more appropriate to consider the
second verb to be an adjective, but in practice they were usually
tagged as verbs.
707
infinitive form (e.g., ?to clean?), a verb and noun
phrase (e.g., ?shoveling snow?), or a verb and ad-
jective (e.g., ?being alone?). We use some simple
heuristics to try to ensure that we are at the end of an
adjective or noun phrase (e.g., if the following word
is tagged as an adjective or noun, then we assume
we are not at the end).
The 20 POS trigram patterns are similar in nature
and are designed to capture seven general types of
verb phrases: verb and adverb mixtures, an infini-
tive VP that includes an adverb, a verb phrase fol-
lowed by a noun phrase, a verb phrase followed by a
prepositional phrase, a verb followed by an adjective
phrase, or an infinitive VP followed by an adjective,
noun, or pronoun.
Returning to Figure 2, only two of the n-grams
match our POS patterns, so we are left with two can-
didate phrases for negative situations:
waiting, waiting forever
Next, we score each negative situation candidate
by estimating the probability that a tweet is sarcastic
given that it contains the candidate phrase following
a positive sentiment phrase:
| follows(?candidate, +sentiment) & sarcastic |
| follows(?candidate, +sentiment) |
We compute the number of times that the negative
situation candidate immediately follows a positive
sentiment in sarcastic tweets divided by the number
of times that the candidate immediately follows a
positive sentiment in all tweets. We discard phrases
that have a frequency < 3 in the tweet collection
since they are too sparse.
Finally, we rank the candidate phrases based on
this probability, using their frequency as a secondary
key in case of ties. The top 20 phrases with a prob-
ability ? .80 are added to the negative situation
phrase list.4 When we add a phrase to the nega-
tive situation list, we immediately remove all other
candidates that are subsumed by the selected phrase.
For example, if we add the phrase ?waiting?, then
the phrase ?waiting forever? would be removed from
the candidate list because it is subsumed by ?wait-
ing?. This process reduces redundancy in the set of
4Fewer than 20 phrases will be learned if < 20 phrases pass
this threshold.
phrases that we add during each bootstrapping itera-
tion. The bootstrapping process stops when no more
candidate phrases pass the probability threshold.
3.5 Learning Positive Verb Phrases
The procedure for learning positive sentiment
phrases is analogous. First, we collect phrases that
potentially convey a positive sentiment by extract-
ing n-grams that precede a negative situation phrase
in a sarcastic tweet. To learn positive sentiment verb
phrases, we extract every 1-gram and 2-gram that
occurs immediately before (on the left-hand side of)
a negative situation phrase.
Next, we apply the POS tagger and filter the n-
grams using POS tag patterns so that we only keep
n-grams that have a desired syntactic structure. Here
our goal is to learn simple verb phrases (VPs) so we
only retain n-grams that contain at least one verb and
consist only of verbs and (optionally) adverbs. Fi-
nally, we score each candidate sentiment verb phrase
by estimating the probability that a tweet is sarcastic
given that it contains the candidate phrase preceding
a negative situation phrase:
| precedes(+candidateVP,?situation) & sarcastic |
| precedes(+candidateVP,?situation) |
3.6 Learning Positive Predicative Phrases
We also use the negative situation phrases to harvest
predicative expressions (predicate adjective or pred-
icate nominal structures) that occur nearby. Based
on the same assumption that sarcasm often arises
from the contrast between a positive sentiment and
a negative situation, we identify tweets that contain
a negative situation and a predicative expression in
close proximity. We then assume that the predicative
expression is likely to convey a positive sentiment.
To learn predicative expressions, we use 24 copu-
lar verbs from Wikipedia5 and their inflections. We
extract positive sentiment candidates by extracting
1-grams, 2-grams, and 3-grams that appear immedi-
ately after a copular verb and occur within 5 words
of the negative situation phrase, on either side. This
constraint only enforces proximity because predica-
tive expressions often appear in a separate clause or
sentence (e.g., ?It is just great that my iphone was
stolen? or ?My iphone was stolen. This is great.?)
5http://en.wikipedia.org/wiki/List of English copulae
708
We then apply POS patterns to identify n-grams
that correspond to predicate adjective and predicate
nominal phrases. For predicate adjectives, we re-
tain ADJ and ADV+ADJ n-grams. We use a few
heuristics to check that the adjective is not part of a
noun phrase (e.g., we check that the following word
is not a noun). For predicate nominals, we retain
ADV+ADJ+N, DET+ADJ+N and ADJ+N n-grams.
We excluded noun phrases consisting only of nouns
because they rarely seemed to represent a sentiment.
The sentiment in predicate nominals was usually
conveyed by the adjective. We discard all candidates
with frequency < 3 as being too sparse. Finally,
we score each remaining candidate by estimating the
probability that a tweet is sarcastic given that it con-
tains the predicative expression near (within 5 words
of) a negative situation phrase:
| near(+candidatePRED,?situation) & sarcastic |
| near(+candidatePRED,?situation) |
We found that the diversity of positive senti-
ment verb phrases and predicative expressions is
much lower than the diversity of negative situation
phrases. As a result, we sort the candidates by their
probability and conservatively add only the top 5
positive verb phrases and top 5 positive predicative
expressions in each bootstrapping iteration. Both
types of sentiment phrases must pass a probability
threshold of ? .70.
3.7 The Learned Phrase Lists
The bootstrapping process alternately learns pos-
itive sentiments and negative situations until no
more phrases can be learned. In our experiments,
we learned 26 positive sentiment verb phrases, 20
predicative expressions and 239 negative situation
phrases.
Table 1 shows the first 15 positive verb phrases,
the first 15 positive predicative expressions, and the
first 40 negative situation phrases learned by the
bootstrapping algorithm. Some of the negative sit-
uation phrases are not complete expressions, but it
is clear that they will often match negative activities
and states. For example, ?getting yelled? was gener-
ated from sarcastic comments such as ?I love getting
yelled at?, ?being home? occurred in tweets about
?being home alone?, and ?being told? is often be-
ing told what to do. Shorter phrases often outranked
longer phrases because they are more general, and
will therefore match more contexts. But an avenue
for future work is to learn linguistic expressions that
more precisely characterize specific negative situa-
tions.
Positive Verb Phrases (26): missed, loves,
enjoy, cant wait, excited, wanted, can?t wait,
get, appreciate, decided, loving, really like,
looooove, just keeps, loveee, ...
Positive Predicative Expressions (20): great,
so much fun, good, so happy, better, my
favorite thing, cool, funny, nice, always fun,
fun, awesome, the best feeling, amazing,
happy, ...
Negative Situations (239): being ignored, be-
ing sick, waiting, feeling, waking up early, be-
ing woken, fighting, staying, writing, being
home, cleaning, not getting, crying, sitting at
home, being stuck, starting, being told, be-
ing left, getting ignored, being treated, doing
homework, learning, getting up early, going to
bed, getting sick, riding, being ditched, get-
ting ditched, missing, not sleeping, not talking,
trying, falling, walking home, getting yelled,
being awake, being talked, taking care, doing
nothing, wasting, ...
Table 1: Examples of Learned Phrases
4 Evaluation
4.1 Data
For evaluation purposes, we created a gold stan-
dard data set of manually annotated tweets. Even
for people, it is not always easy to identify sarcasm
in tweets because sarcasm often depends on con-
versational context that spans more than a single
tweet. Extracting conversational threads from Twit-
ter, and analyzing conversational exchanges, has its
own challenges and is beyond the scope of this re-
search. We focus on identifying sarcasm that is self-
contained in one tweet and does not depend on prior
conversational context.
We defined annotation guidelines that instructed
human annotators to read isolated tweets and label
709
a tweet as sarcastic if it contains comments judged
to be sarcastic based solely on the content of that
tweet. Tweets that do not contain sarcasm, or where
potential sarcasm is unclear without seeing the prior
conversational context, were labeled as not sarcas-
tic. For example, a tweet such as ?Yes, I meant that
sarcastically.? should be labeled as not sarcastic be-
cause the sarcastic content was (presumably) in a
previous tweet. The guidelines did not contain any
instructions that required positive/negative contrast
to be present in the tweet, so all forms of sarcasm
were considered to be positive examples.
To ensure that our evaluation data had a healthy
mix of both sarcastic and non-sarcastic tweets, we
collected 1,600 tweets with a sarcasm hashtag (#sar-
casm or #sarcastic), and 1,600 tweets without these
sarcasm hashtags from Twitter?s random streaming
API. When presenting the tweets to the annotators,
the sarcasm hashtags were removed so the annota-
tors had to judge whether a tweet was sarcastic or
not without seeing those hashtags.
To ensure that we had high-quality annotations,
three annotators were asked to annotate the same set
of 200 tweets (100 sarcastic + 100 not sarcastic).
We computed inter-annotator agreement (IAA) be-
tween each pair of annotators using Cohen?s kappa
(?). The pairwise IAA scores were ?=0.80, ?=0.81,
and ?=0.82. We then gave each annotator an addi-
tional 1,000 tweets to annotate, yielding a total of
3,200 annotated tweets. We used the first 200 tweets
as our Tuning Set, and the remaining 3000 tweets as
our Test Set.
Our annotators judged 742 of the 3,200 tweets
(23%) to be sarcastic. Only 713 of the 1,600 tweets
with sarcasm hashtags (45%) were judged to be sar-
castic based on our annotation guidelines. There are
several reasons why a tweet with a sarcasm hash-
tag might not have been judged to be sarcastic. Sar-
casm may not be apparent without prior conversa-
tional context (i.e., multiple tweets), or the sarcastic
content may be in a URL and not the tweet itself, or
the tweet?s content may not obviously be sarcastic
without seeing the sarcasm hashtag (e.g., ?The most
boring hockey game ever #sarcasm?).
Of the 1,600 tweets in our data set that were ob-
tained from the random stream and did not have a
sarcasm hashtag, 29 (1.8%) were judged to be sar-
castic based on our annotation guidelines.
4.2 Baselines
Overall, 693 of the 3,000 tweets in our Test Set
were annotated as sarcastic, so a system that classi-
fies every tweet as sarcastic will have 23% precision.
To assess the difficulty of recognizing the sarcastic
tweets in our data set, we evaluated a variety of base-
line systems.
We created two baseline systems that use n-gram
features with supervised machine learning to create
a sarcasm classifier. We used the LIBSVM (Chang
and Lin, 2011) library to train two support vector
machine (SVM) classifiers: one with just unigram
features and one with both unigrams and bigrams.
The features had binary values indicating the pres-
ence or absence of each n-gram in a tweet. The clas-
sifiers were evaluated using 10-fold cross-validation.
We used the RBF kernel, and the cost and gamma
parameters were optimized for accuracy using un-
igram features and 10-fold cross-validation on our
Tuning Set. The first two rows of Table 2 show the
results for these SVM classifiers, which achieved F
scores of 46-48%.
We also conducted experiments with existing sen-
timent and subjectivity lexicons to see whether they
could be leveraged to recognize sarcasm. We exper-
imented with three resources:
Liu05 : A positive and negative opinion lexicon
from (Liu et al, 2005). This lexicon contains
2,007 positive sentiment words and 4,783 neg-
ative sentiment words.
MPQA05 : The MPQA Subjectivity Lexicon that
is part of the OpinionFinder system (Wilson et
al., 2005a; Wilson et al, 2005b). This lexicon
contains 2,718 subjective words with positive
polarity and 4,910 subjective words with nega-
tive polarity.
AFINN11 The AFINN sentiment lexicon designed
for microblogs (Nielsen, 2011; Hansen et al,
2011) contains 2,477 manually labeled words
and phrases with integer values ranging from -5
(negativity) to 5 (positivity). We considered all
words with negative values to have negative po-
larity (1598 words), and all words with positive
values to have positive polarity (879 words).
We performed four sets of experiments with each
resource to see how beneficial existing sentiment
710
System Recall Precision F score
Supervised SVM Classifiers
1grams .35 .64 .46
1+2grams .39 .64 .48
Positive Sentiment Only
Liu05 .77 .34 .47
MPQA05 .78 .30 .43
AFINN11 .75 .32 .44
Negative Sentiment Only
Liu05 .26 .23 .24
MPQA05 .34 .24 .28
AFINN11 .24 .22 .23
Positive and Negative Sentiment, Unordered
Liu05 .19 .37 .25
MPQA05 .27 .30 .29
AFINN11 .17 .30 .22
Positive and Negative Sentiment, Ordered
Liu05 .09 .40 .14
MPQA05 .13 .30 .18
AFINN11 .09 .35 .14
Our Bootstrapped Lexicons
Positive VPs .28 .45 .35
Negative Situations .29 .38 .33
Contrast(+VPs, ?Situations), Unordered .11 .56 .18
Contrast(+VPs, ?Situations), Ordered .09 .70 .15
& Contrast(+Preds, ?Situations) .13 .63 .22
Our Bootstrapped Lexicons ? SVM Classifier
Contrast(+VPs, ?Situations), Ordered .42 .63 .50
& Contrast(+Preds, ?Situations) .44 .62 .51
Table 2: Experimental results on the test set
lexicons could be for sarcasm recognition in tweets.
Since our hypothesis is that sarcasm often arises
from the contrast between something positive and
something negative, we systematically evaluated the
positive and negative phrases individually, jointly,
and jointly in a specific order (a positive phrase fol-
lowed by a negative phrase).
First, we labeled a tweet as sarcastic if it con-
tains any positive term in each resource. The Pos-
itive Sentiment Only section of Table 2 shows that
all three sentiment lexicons achieved high recall (75-
78%) but low precision (30-34%). Second, we la-
beled a tweet as sarcastic if it contains any negative
term from each resource. The Negative Sentiment
Only section of Table 2 shows that this approach
yields much lower recall and also lower precision
of 22-24%, which is what would be expected of a
random classifier since 23% of the tweets are sar-
castic. These results suggest that explicit negative
sentiments are not generally indicative of sarcasm.
Third, we labeled a tweet as sarcastic if it contains
both a positive sentiment term and a negative senti-
ment term, in any order. The Positive and Negative
Sentiment, Unordered section of Table 2 shows that
this approach yields low recall, indicating that rela-
tively few sarcastic tweets contain both positive and
negative sentiments, and low precision as well.
Fourth, we required the contrasting sentiments to
occur in a specific order (the positive term must pre-
cede the negative term) and near each other (no more
than 5 words apart). This criteria reflects our obser-
vation that positive sentiments often closely precede
negative situations in sarcastic tweets, so we wanted
to see if the same ordering tendency holds for neg-
ative sentiments. The Positive and Negative Senti-
ment, Ordered section of Table 2 shows that this or-
dering constraint further decreases recall and only
slightly improves precision, if at all. Our hypothe-
711
sis is that when positive and negative sentiments are
expressed in the same tweet, they are referring to
different things (e.g., different aspects of a product).
Expressing positive and negative sentiments about
the same thing would usually sound contradictory
rather than sarcastic.
4.3 Evaluation of Bootstrapped Phrase Lists
The next set of experiments evaluates the effective-
ness of the positive sentiment and negative situa-
tion phrases learned by our bootstrapping algorithm.
The results are shown in the Our Bootstrapped Lex-
icons section of Table 2. For the sake of compar-
ison with other sentiment resources, we first eval-
uated our positive sentiment verb phrases and neg-
ative situation phrases independently. Our positive
verb phrases achieved much lower recall than the
positive sentiment phrases in the other resources, but
they had higher precision (45%). The low recall
is undoubtedly because our bootstrapped lexicon is
small and contains only verb phrases, while the other
resources are much larger and contain terms with
additional parts-of-speech, such as adjectives and
nouns.
Despite its relatively small size, our list of neg-
ative situation phrases achieved 29% recall, which
is comparable to the negative sentiments, but higher
precision (38%).
Next, we classified a tweet as sarcastic if it con-
tains both a positive verb phrase and a negative sit-
uation phrase from our bootstrapped lists, in any
order. This approach produced low recall (11%)
but higher precision (56%) than the sentiment lex-
icons. Finally, we enforced an ordering constraint
so a tweet is labeled as sarcastic only if it contains
a positive verb phrase that precedes a negative situa-
tion in close proximity (no more than 5 words apart).
This ordering constraint further increased precision
from 56% to 70%, with a decrease of only 2 points
in recall. This precision gain supports our claim that
this particular structure (positive verb phrase fol-
lowed by a negative situation) is strongly indicative
of sarcasm. Note that the same ordering constraint
applied to a positive verb phrase followed by a neg-
ative sentiment produced much lower precision (at
best 40% precision using the Liu05 lexicon). Con-
trasting a positive sentiment with a negative situa-
tion seems to be a key element of sarcasm.
In the last experiment, we added the positive pred-
icative expressions and also labeled a tweet as sar-
castic if a positive predicative appeared in close
proximity to (within 5 words of) a negative situa-
tion. The positive predicatives improved recall to
13%, but decreased precision to 63%, which is com-
parable to the SVM classifiers.
4.4 A Hybrid Approach
Thus far, we have used the bootstrapped lexicons
to recognize sarcasm by looking for phrases in our
lists. We will refer to our approach as the Contrast
method, which labels a tweet as sarcastic if it con-
tains a positive sentiment phrase in close proximity
to a negative situation phrase.
The Contrast method achieved 63% precision but
with low recall (13%). The SVM classifier with un-
igram and bigram features achieved 64% precision
with 39% recall. Since neither approach has high
recall, we decided to see whether they are comple-
mentary and the Contrast method is finding sarcastic
tweets that the SVM classifier overlooks.
In this hybrid approach, a tweet is labeled as sar-
castic if either the SVM classifier or the Contrast
method identifies it as sarcastic. This approach im-
proves recall from 39% to 42% using the Contrast
method with only positive verb phrases. Recall im-
proves to 44% using the Contrast method with both
positive verb phrases and predicative phrases. This
hybrid approach has only a slight drop in precision,
yielding an F score of 51%. This result shows that
our bootstrapped phrase lists are recognizing sarcas-
tic tweets that the SVM classifier misses.
Finally, we ran tests to see if the performance of
the hybrid approach (Contrast ? SVM) is statisti-
cally significantly better than the performance of the
SVM classifier alone. We used paired bootstrap sig-
nificance testing as described in Berg-Kirkpatrick
et al (2012) by drawing 106 samples with repeti-
tion from the test set. These results showed that the
Contrast ? SVM system is statistically significantly
better than the SVM classifier at the p < .01 level
(i.e., the null hypothesis was rejected with 99% con-
fidence).
4.5 Analysis
To get a better sense of the strength and limitations
of our approach, we manually inspected some of the
712
tweets that were labeled as sarcastic using our boot-
strapped phrase lists. Table 3 shows some of the sar-
castic tweets found by the Contrast method but not
by the SVM classifier.
i love fighting with the one i love
love working on my last day of summer
i enjoy tweeting [user] and not getting a reply
working during vacation is awesome .
can?t wait to wake up early to babysit !
Table 3: Five sarcastic tweets found by the Contrast
method but not the SVM
These tweets are good examples of a positive sen-
timent (love, enjoy, awesome, can?t wait) contrast-
ing with a negative situation. However, the negative
situation phrases are not always as specific as they
should be. For example, ?working? was learned as
a negative situation phrase because it is often neg-
ative when it follows a positive sentiment (?I love
working...?). But the attached prepositional phrases
(?on my last day of summer? and ?during vacation?)
should ideally have been captured as well.
We also examined tweets that were incorrectly la-
beled as sarcastic by the Contrast method. Some
false hits come from situations that are frequently
negative but not always negative (e.g., some peo-
ple genuinely like waking up early). However, most
false hits were due to overly general negative situa-
tion phrases (e.g., ?I love working there? was labeled
as sarcastic). We believe that an important direction
for future work will be to learn longer phrases that
represent more specific situations.
5 Conclusions
Sarcasm is a complex and rich linguistic phe-
nomenon. Our work identifies just one type of sar-
casm that is common in tweets: contrast between a
positive sentiment and negative situation. We pre-
sented a bootstrapped learning method to acquire
lists of positive sentiment phrases and negative ac-
tivities and states, and show that these lists can be
used to recognize sarcastic tweets.
This work has only scratched the surface of pos-
sibilities for identifying sarcasm arising from posi-
tive/negative contrast. The phrases that we learned
were limited to specific syntactic structures and we
required the contrasting phrases to appear in a highly
constrained context. We plan to explore methods for
allowing more flexibility and for learning additional
types of phrases and contrasting structures.
We also would like to explore new ways to iden-
tify stereotypically negative activities and states be-
cause we believe this type of world knowledge is
essential to recognize many instances of sarcasm.
For example, sarcasm often arises from a descrip-
tion of a negative event followed by a positive emo-
tion but in a separate clause or sentence, such as:
?Going to the dentist for a root canal this after-
noon. Yay, I can?t wait.? Recognizing the intensity
of the negativity may also be useful to distinguish
strong contrast from weak contrast. Having knowl-
edge about stereotypically undesirable activities and
states could also be important for other natural lan-
guage understanding tasks, such as text summariza-
tion and narrative plot analysis.
6 Acknowledgments
This work was supported by the Intelligence Ad-
vanced Research Projects Activity (IARPA) via De-
partment of Interior National Business Center (DoI
/ NBC) contract number D12PC00285. The U.S.
Government is authorized to reproduce and dis-
tribute reprints for Governmental purposes notwith-
standing any copyright annotation thereon. The
views and conclusions contained herein are those
of the authors and should not be interpreted as
necessarily representing the official policies or en-
dorsements, either expressed or implied, of IARPA,
DoI/NBE, or the U.S. Government.
References
Taylor Berg-Kirkpatrick, David Burkett, and Dan Klein.
2012. An empirical investigation of statistical signifi-
cance in nlp. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing, EMNLP-CoNLL ?12, pages 995?1005.
Paula Carvalho, Lu??s Sarmento, Ma?rio J. Silva, and
Euge?nio de Oliveira. 2009. Clues for detecting irony
in user-generated contents: oh...!! it?s ?so easy? ;-). In
Proceedings of the 1st international CIKM workshop
on Topic-sentiment analysis for mass opinion, TSA
2009.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
713
tions on Intelligent Systems and Technology, 2:27:1?
27:27.
Henry S. Cheang and Marc D. Pell. 2008. The sound of
sarcasm. Speech Commun., 50(5):366?381, May.
Henry S. Cheang and Marc D. Pell. 2009. Acous-
tic markers of sarcasm in cantonese and english.
The Journal of the Acoustical Society of America,
126(3):1394?1405.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised recognition of sarcastic sentences in
twitter and amazon. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, CoNLL 2010.
Elena Filatova. 2012. Irony and sarcasm: Corpus gener-
ation and analysis using crowdsourcing. In Proceed-
ings of the Eight International Conference on Lan-
guage Resources and Evaluation (LREC?12).
Roger J. Kreuz Gina M. Caucci. 2012. Social and par-
alinguistic cues to sarcasm. online 08/02/2012, 25:1?
22, February.
Roberto Gonza?lez-Iba?n?ez, Smaranda Muresan, and Nina
Wacholder. 2011. Identifying sarcasm in twitter: A
closer look. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies.
Lars Kai Hansen, Adam Arvidsson, Finn Arup Nielsen,
Elanor Colleoni, and Michael Etter. 2011. Good
friends, bad news - affect and virality in twitter. In
The 2011 International Workshop on Social Comput-
ing, Network, and Services (SocialComNet 2011).
Roger Kreuz and Gina Caucci. 2007. Lexical influences
on the perception of sarcasm. In Proceedings of the
Workshop on Computational Approaches to Figurative
Language.
Christine Liebrecht, Florian Kunneman, and Antal
Van den Bosch. 2013. The perfect solution for detect-
ing sarcasm in tweets #not. In Proceedings of the 4th
Workshop on Computational Approaches to Subjec-
tivity, Sentiment and Social Media Analysis, WASSA
2013.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: Analyzing and comparing opinions
on the web. In Proceedings of the 14th International
World Wide Web conference (WWW-2005).
Stephanie Lukin and Marilyn Walker. 2013. Really?
well. apparently bootstrapping improves the perfor-
mance of sarcasm and nastiness classifiers for online
dialogue. In Proceedings of the Workshop on Lan-
guage Analysis in Social Media.
Finn Arup Nielsen. 2011. A new anew: Evaluation of
a word list for sentiment analysis in microblogs. In
Proceedings of the ESWC2011 Workshop on ?Making
Sense of Microposts?: Big things come in small pack-
ages (http://arxiv.org/abs/1103.2903).
Olutobi Owoputi, Brendan O?Connor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and Noah A. Smith. 2013.
Improved part-of-speech tagging for online conversa-
tional text with word clusters. In The 2013 Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language
Technologies (NAACL 2013).
Katherine P. Rankin, Andrea Salazar, Maria Luisa Gorno-
Tempini, Marc Sollberger, Stephen M. Wilson, Dani-
jela Pavlic, Christine M. Stanley, Shenly Glenn,
Michael W. Weiner, and Bruce L. Miller. 2009. De-
tecting sarcasm from paralinguistic cues: Anatomic
and cognitive correlates in neurodegenerative disease.
Neuroimage, 47:2005?2015.
Joseph Tepperman, David Traum, and Shrikanth
Narayanan. 2006. ?Yeah right?: Sarcasm recogni-
tion for spoken dialogue systems. In Proceedings of
the INTERSPEECH 2006 - ICSLP, Ninth International
Conference on Spoken Language Processing.
Oren Tsur, Dmitry Davidov, and Ari Rappoport. 2010.
ICWSM - A Great Catchy Name: Semi-Supervised
Recognition of Sarcastic Sentences in Online Product
Reviews. In Proceedings of the Fourth International
Conference on Weblogs and Social Media (ICWSM-
2010), ICWSM 2010.
Michael Wiegand, Josef Ruppenhofer, and Dietrich
Klakow. 2013. Predicative adjectives: An unsuper-
vised criterion to extract subjective adjectives. In Pro-
ceedings of the 2013 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 534?
539, Atlanta, Georgia, June. Association for Compu-
tational Linguistics.
T. Wilson, P. Hoffmann, S. Somasundaran, J. Kessler,
J. Wiebe, Y. Choi, C. Cardie, E. Riloff, and S. Patward-
han. 2005a. OpinionFinder: A System for Subjec-
tivity Analysis. In Proceedings of HLT/EMNLP 2005
Interactive Demonstrations, pages 34?35, Vancouver,
Canada, October.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005b. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the 2005
Human Language Technology Conference / Confer-
ence on Empirical Methods in Natural Language Pro-
cessing.
714
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1557?1562,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Classifying Message Board Posts with an Extracted Lexicon of Patient
Attributes
Ruihong Huang and Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT 84112
{huangrh, riloff}@cs.utah.edu
Abstract
The goal of our research is to distinguish vet-
erinary message board posts that describe a
case involving a specific patient from posts
that ask a general question. We create a text
classifier that incorporates automatically gen-
erated attribute lists for veterinary patients to
tackle this problem. Using a small amount of
annotated data, we train an information extrac-
tion (IE) system to identify veterinary patient
attributes. We then apply the IE system to a
large collection of unannotated texts to pro-
duce a lexicon of veterinary patient attribute
terms. Our experimental results show that us-
ing the learned attribute lists to encode pa-
tient information in the text classifier yields
improved performance on this task.
1 Introduction
Our research focuses on the problem of classify-
ing message board posts in the domain of veterinary
medicine. Most of the posts in our corpus discuss a
case involving a specific patient, which we will call
patient-specific posts. But there are also posts that
ask a general question, for example to seek advice
about different medications, information about new
procedures, or how to perform a test. Our goal is
to distinguish the patient-specific posts from general
posts so that they can be automatically routed to dif-
ferent message board folders.
Distinguishing patient-specific posts from general
posts is a challenging problem for two reasons. First,
virtually any medical topic can appear in either type
of post, so the vocabulary is very similar. Second,
a highly skewed distribution exists between patient-
specific posts and general posts. Almost 90% of the
posts in our data are about specific patients.
With such a highly skewed distribution, it would
seem logical to focus on recognizing instances of the
minority class. But the distinguishing characteristic
of a general post is the absence of a patient. Two
nearly identical posts belong in different categories
if one mentions a patient and the other does not.
Consequently, our aim is to create features that iden-
tify references to a specific patient and use these to
more accurately distinguish the two types of posts.
Our research explores the use of information ex-
traction (IE) techniques to automatically identify
common attributes of veterinary patients, which we
use to encode patient information in a text classifier.
Our approach involves three phases. First, we train
a conditional random fields (CRF) tagger to iden-
tify seven common types of attributes that are of-
ten ascribed to veterinary patients: SPECIES/BREED,
NAME, AGE, GENDER, WEIGHT, POSSESSOR, and
DISEASE/SYMPTOM. Second, we apply the CRF
tagger to a large set of unannotated message board
posts, collect its extractions, and harvest the most
frequently extracted terms to create a Veterinary Pa-
tient Attribute (VPA) Lexicon.
Finally, we define three types of features that ex-
ploit the harvested VPA lexicon. These features rep-
resent the patient attribute terms, types, and com-
binations of them to help the classifier determine
whether a post is discussing a specific patient. We
conduct experiments which show that the extracted
patient attribute information improves text classifi-
cation performance on this task.
1557
2 Related Work
Our work demonstrates the use of information ex-
traction techniques to benefit a text classification ap-
plication. There has been a great deal of research on
text classification (e.g., (Borko and Bernick, 1963;
Hoyle, 1973; Joachims, 1998; Nigam et al, 2000;
Sebastiani, 2002)), which most commonly has used
bag-of-word features. Researchers have also inves-
tigated clustering (Baker and McCallum, 1998), La-
tent Semantic Indexing (LSI) (Zelikovitz and Hirsh,
2001), Latent Dirichlet Allocation (LDA) (Br et al,
2008) and string kernels (Lodhi et al, 2001). Infor-
mation extraction techniques have been used previ-
ously to create richer features for event-based text
classification (Riloff and Lehnert, 1994) and web
page classification (Furnkranz et al, 1998). Se-
mantic information has also been incorporated for
text classification. However, most previous work re-
lies on existing semantic resources, such as Wordnet
(Scott and Stan, 1998; Bloehdorn and Hotho, 2006)
or Wikipedia (Wang et al, 2009).
There is also a rich history of automatic lexicon
induction from text corpora (e.g., (Roark and Char-
niak, 1998; Riloff and Jones, 1999; McIntosh and
Curran, 2009)), Wikipedia (e.g., (Vyas and Pantel,
2009)), and the Web (e.g., (Etzioni et al, 2005;
Kozareva et al, 2008; Carlson et al, 2010)). The
novel aspects of our work are in using an IE tagger
to harvest a domain-specific lexicon from unanno-
tated texts, and using the induced lexicon to encode
domain-specific features for text classification.
3 Text Classification with Extracted
Patient Attributes
This resesarch studies message board posts from the
Veterinary Information Network (VIN), which is a
web site (www.vin.com) for professionals in veteri-
nary medicine. VIN hosts forums where veterinar-
ians discuss medical issues, challenging cases, etc.
We observed that patient-specific veterinary posts
almost always include some basic facts about the
patient, such as the animal?s breed, age, or gender.
It is also common to mention the patient?s owner
(e.g., ?a new client?s cat?) or a disease or symptom
that the patient has (e.g., ?a diabetic cat?). General
posts almost never contain this information.
Although some of these terms can be found in
existing resources such as Wordnet (Miller, 1990),
our veterinary message board posts are filled with
informal and unconventional vocabulary. For ex-
ample, one might naively assume that ?male? and
?female? are sufficient to identify gender. But the
gender of animals is often revealed by describing
their spayed/neutered status, often indicated with
shorthand notations. For example, ?m/n? means
male and neutered, ?fs? means female spayed, ?cas-
trated? means neutered and implies male. Short-
hand terms and informal jargon are also frequently
used for breeds (e.g., ?doxy? for dachsund, ?labx?
for labrador cross, ?gshep? for German Shepherd)
and ages (e.g., ?3-yr-old?, ?3yo?, ?3mo?). A par-
ticularly creative age expression describes an animal
as (say) ?a 1999 model? (i.e., born in 1999). To rec-
ognize the idiosyncratic vocabulary in these texts,
we use information extraction techniques to identify
terms corresponding to seven attributes of veterinary
patients: SPECIES/BREED, NAME, AGE, WEIGHT,
GENDER, POSSESSOR, and DISEASE/SYMPTOM.
Figure 1 illustrates our overall approach, which
consists of three steps. First, we train a sequential
IE tagger to label veterinary patient attributes using
supervised learning. Second, we apply the tagger
to 10,000 unannotated message board posts to auto-
matically create a Veterinary Patient Attribute (VPA)
Lexicon. Third, we use the VPA Lexicon to encode
patient attribute features in a document classifier.
Unannotated
Texts
PI Sentence 
Classifier
VPA Tagger
(CRF)
VPA
Lexicon
Step 2
PI Sentence 
Classifier
VPA Tagger
(CRF)
Annotated
Texts
Step 1
Annotated
Texts
VPA
Lexicon
Document
Classifier
Step 3
Figure 1: Flowchart for Creating a Patient-Specific vs.
General Document Classifier
3.1 Patient Attribute Tagger
The first component of our system is a tagger that
labels veterinary patient attributes. To train the tag-
ger, we need texts labeled with patient attributes.
1558
The message board posts can be long and tedious
to read (i.e., they are often filled with medical his-
tory and test results), so manually annotating every
word would be arduous. However, the patient is usu-
ally described at the beginning of a post, most com-
monly in 1-2 ?introductory? sentences. Therefore
we adopted a two stage process, both for manual and
automatic tagging of patient attributes.
First, we created annotation guidelines to iden-
tify ?patient introductory? (PI) sentences, which we
defined as sentences that introduce a patient to the
reader by providing a general (non-medical) descrip-
tion of the animal (e.g., ?I was presented with a m/n
Siamese cat that is lethargic.?) We randomly se-
lected 300 posts from our text collection and asked
two human annotators to manually identify the PI
sentences. We measured their inter-annotator agree-
ment using Cohen?s kappa (?) and their agreement
was ?=.93. The two annotators then adjudicated
their differences to create our gold standard set of PI
sentence annotations. 269 of the 300 posts contained
at least one PI sentence , indicating that 89.7% of the
posts mention a specific patient. The remaining 31
posts (10.3%) are general in nature.
Second, the annotators manually labeled the
words in these PI sentences with respect to the 7 vet-
erinary patient attributes. On 50 randomly selected
texts, the annotators achieved an inter-annotator
agreement of ? = .89. The remaining 250 posts were
then annotated with patient attributes (in the PI sen-
tences), providing us with gold standard attribute an-
notations for all 300 posts. To illustrate, the sentence
below would have the following labels:
Daisyname is a 10yrage oldage labspecies
We used these 300 annotated posts to train both
a PI sentence classifier and a patient attribute tag-
ger. The PI sentence classifier is a support vector
machine (SVM) with a linear kernel (Keerthi and
DeCoste, 2005), unigram and bigram features, and
binary feature values. The PI sentences are the posi-
tive training instances, and the sentences in the gen-
eral posts are negative training instances.
For the tagger, we trained a single conditional ran-
dom fields (CRF) model to label all 7 types of pa-
tient attributes using the CRF++ package (Lafferty
et al, 2001). We defined features for the word string
and the part-of-speech tags of the targeted word, two
words on its left, and two words on its right.
Given new texts to process, we first apply the PI
sentence classifier to identify sentences that intro-
duce a patient. These sentences are given to the pa-
tient attribute tagger, which labels the words in those
sentences for the 7 patient attribute categories.
To evaluate the performance of the patient at-
tribute tagger, we randomly sampled 200 of the 300
annotated documents to use as training data and used
the remaining 100 documents for testing. For this
experiment, we only applied the CRF tagger to the
gold standard PI sentences, to eliminate any con-
founding factors from the PI sentence classifier. Ta-
ble 1 shows the performance of the CRF tagger in
terms of Recall (%), Precision (%), and F Score (%).
Its precision is consistently high, averaging 91%
across all seven attributes. But the average recall is
only 47%, with only one attribute (AGE) achieving
recall ? 80%. Nevertheless, the CRF?s high preci-
sion justifies our plan to use the CRF tagger to har-
vest additional attribute terms from a large collection
of unannotated texts. As we will see in Section 4,
the additional terms harvested from the unannotated
texts provide substantially more attribute informa-
tion for the document classifier to use.
Attribute Rec Prec F
SPECIES/BREED 59 93 72
NAME 62 100 76
POSSESSOR 12 100 21
AGE 80 91 85
GENDER 59 81 68
WEIGHT 19 100 32
DISEASE/SYMPTOM 35 73 47
Average 47 91 62
Table 1: Patient Attribute Tagger Evaluation
3.2 Creating a Veterinary Patient Attribute
(VPA) Lexicon
The patient attribute tagger was trained with super-
vised learning, so its ability to recognize important
words is limited by the scope of its training set.
Since we had an additional 10,000 unannotated vet-
erinary message board posts, we used the tagger to
acquire a large lexicon of patient attribute terms.
We applied the PI sentence classifier to all 10,000
texts and then applied the patient attribute tagger to
each PI sentence. The patient attribute tagger is not
1559
perfect, so we assumed that words tagged with the
same attribute value at least five times1 are most
likely to be correct and harvested them to create a
veterinary patient attribute (VPA) lexicon. This pro-
duced a VPA lexicon of 592 words. Table 2 shows
examples of learned terms for each attribute, with
the total number of learned words in parentheses.
Species/Breed (177): DSH, Schnauzer, kitty, Bengal,
pug, Labrador, siamese, Shep, miniature, golden, lab,
Spaniel, Westie, springer, Chow, cat, Beagle, Mix, ...
Name (53): Lucky, Shadow, Toby, Ginger, Boo, Max,
Baby, Buddy, Tucker, Gracie, Maggie, Willie, Tiger,
Sasha, Rusty, Beau, Kiki, Oscar, Harley, Scooter, ...
Age (59): #-year, adult, young, YO, y/o, model, wk,
y.o., yr-old, yrs, y, #-yr, #-month, #m, mo, mth, ...
Gender (39): F/s, speyed, neutered, spayed, N/M,
FN, CM, F, mc, mn, SF, male, fs, M/N, Female,
S, S/F, m/n, m/c, intact, M, NM, castrated, ...
Weight (5): lb, lbs, pound, pounds, kg
Possessor (7): my, owner, client, technician, ...
Disease/Symptom (252): abscess, fever, edema,
hepatic, inappetance, sneezing, blindness, pain,
persistent, mass, insufficiency, acute, poor, ...
Table 2: Examples from the Induced VPA Lexicon
3.3 Text Classification with Patient Attributes
Our ultimate goal is to incorporate patient attribute
information into a text classifier to help it distinguish
between patient-specific posts and general posts. We
designed three sets of features:
Attribute Types: We create one feature for each
attribute type, indicating whether a word of that at-
tribute type appeared or not.
Attribute Types with Neighbor: For each word la-
beled as a patient attribute, we create two features
by pairing its Attribute Type with a preceding or fol-
lowing word. For example, given the sentence: ?The
tiny Siamese kitten was lethargic.?, if ?Siamese? has
attribute type SPECIES then we create two features:
<tiny, SPECIES> and <SPECIES, kitten>.
Attribute Pairs: We create features for all pairs of
patient attribute words that occur in the same sen-
tence. For each pair, we create one feature repre-
1After our text classification experiments were done, we re-
ran the experiments with the unigrams+lexicon classifier using
thresholds ranging from 1 to 10 for lexicon creation, just to see
how much difference this threshold made. We found that values
? 5 produced nearly identical classification results.
senting the words themselves and one feature repre-
senting the attribute types of the words.
4 Evaluation
To create a blind test set for evaluation, our anno-
tators labeled an additional 500 posts as patient-
specific or general. Specifically, they labeled those
500 posts with PI sentences. The absence of a PI
sentence meant that the post was general. Of the 500
texts, 48 (9.6%) were labeled as general posts. We
evaluated the performance of the PI sentence classi-
fier on this test set and found that it achieved 88% ac-
curacy at identifying patient introductory sentences.
We then conducted a series of experiments for the
document classification task: distinguishing patient-
specific message board posts from general posts.
All of our experiments used support vector machine
(SVM) classifiers with a linear kernel, and ran 10-
fold cross validation on our blind test set of 500
posts. We report Recall (%), Precision (%), and F
score (%) results for the patient-specific posts and
general posts separately, and for the macro-averaged
score across both classes. For the sake of complete-
ness, we also show overall Accuracy (%) results.
However, we will focus attention on the results for
the general posts, since our main goal is to improve
performance at recognizing this minority class.
As a baseline, we created SVM classifiers using
unigram features.2 We tried binary, frequency, and
tf-idf feature values. The first three rows of Table 3
show that binary feature values performed the best,
yielding a macro-averaged F score of 81% but iden-
tifying only 54% of the general posts.
The middle section of Table 3 shows the perfor-
mance of SVM classifiers using our patient attribute
features. We conducted three experiments: apply-
ing the CRF tagger to PI sentences (per its design),
and labeling words with the VPA lexicon either on
all sentences or only on PI sentences (as identi-
fied by the PI sentence classifier). The CRF fea-
tures produced extremely low recall and precision
on the general posts. The VPA lexicon performed
best when applied only to PI sentences and pro-
duced much higher recall than all of the other clas-
sifiers, although with lower precision than the two
2We also tried unigrams + bigrams, but they did not perform
better.
1560
Patient-Specific Posts General Posts Macro Avg
Method Rec Prec F Rec Prec F Rec Prec F Acc
Unigram Features
Unigrams (freq) 96 96 96 58 60 59 77 76 77 92
Unigrams (tf-idf) 99 93 96 33 84 48 66 89 76 93
Unigrams (binary) 98 95 97 54 79 64 76 87 81 94
Patient Attribute Features
CRF Features (PI Sents) 99 91 95 02 25 04 51 58 54 90
VPA Lexicon Features (All Sents) 96 96 96 60 63 62 78 79 79 93
VPA Lexicon Features (PI Sents) 96 98 97 81 66 73 88 82 85 94
Unigram & Patient Attribute Features
CRF Features (PI Sents) 97 96 97 60 71 65 79 83 81 94
VPA Lexicon Features (PI Sents) 98 98 98 79 78 78 88 88 88 96
Table 3: Experimental Results
best unigram-based SVMs.
The bottom section of Table 3 shows results for
classifiers with both unigrams (binary) and patient
attribute features. Using the CRF features increases
recall on the general posts from 54 ? 60, but de-
creases precision from 79 ? 71. Using the patient
attribute features from the VPA lexicon yields a sub-
stantial improvement. Recall improves from 54 ?
79 and precision is just one point lower. Overall, the
macro-averaged F score across the two categories
jumps from 81% to 88%.
We performed paired bootstrap testing (Berg-
Kirkpatrick et al, 2012)) to determine whether the
SVM with unigrams and VPA lexicon features is
statistically significantly better than the best SVM
with only unigram features (binary). The SVM with
unigrams and VPA lexicon features produces sig-
nificantly better F scores at the p < 0.05 level for
general post classification as well as the macro av-
erage. The F score for patient-specific classification
and overall accuracy are statistically significant at
the p < 0.10 level.
Attribute CRF VPA
Tagger Lexicon
SPECIES/BREED 270 1045
NAME 36 43
POSSESSOR 12 233
AGE 545 1773
GENDER 153 338
WEIGHT 27 83
DISEASE/SYMPTOM 220 2673
Table 4: Number of Attributes Labeled in Test Set
Finally, we did an analysis to understand why the
VPA lexicon was so much more effective than the
CRF tagger when used to create features for text
classification. Table 4 shows the number of words
in PI sentences (identified by the classifier) of the
test set that were labeled as patient attributes by the
CRF tagger or the VPA lexicon. The VPA lexicon
clearly labeled many more terms, and the additional
coverage made a big difference for the text classifier.
5 Conclusions
This work demonstrated how annotated data can be
leveraged to automatically harvest a domain-specific
lexicon from a large collection of unannotated texts.
Our induced VPA lexicon was then used to create
patient attribute features that improved the ability of
a document classifier to distinguish between patient-
specific message board posts and general posts. We
believe that this approach could also be used to cre-
ate specialized lexicons for many other domains and
applications. A key benefit of inducing lexicons
from unannotated texts is that they provide addi-
tional vocabulary coverage beyond the terms found
in annotated data sets, which are usually small.
6 Acknowledgements
This material is based upon work supported by
the National Science Foundation under grant IIS-
1018314. We are very grateful to the Veterinary In-
formation Network for providing us with samples of
their data.
1561
References
D. Baker and A. McCallum. 1998. Distributional cluster-
ing of words for text classification. In Proceedings of
the 21st annual international ACM SIGIR conference
on Research and development in information retrieval.
T. Berg-Kirkpatrick, D. Burkett, and D. Klein. 2012. An
Empirical Investigation of Statistical Significance in
NLP. In Proceedings of the 2012 Conference on Em-
pirical Methods in Natural Language Processing.
S. Bloehdorn and A. Hotho. 2006. Boosting for text
classification with semantic features. In Advances in
Web mining and Web usage Analysis.
H. Borko and M. Bernick. 1963. Automatic Document
Classification. J. ACM, 10(2):151?162.
I. Br, J. Szab, and A. Benczr. 2008. Latent dirichlet alo-
cation in web spam filtering. In Proceedings of the 4th
international workshop on Adversarial information re-
trieval on the web.
A. Carlson, J. Betteridge, B. Kisiel, B. Settles, R. Es-
tevam, J. Hruschka, and T. Mitchell. 2010. Toward
an Architecture for Never-Ending Language Learning.
In Proceedings of the Twenty-Fourth National Confer-
ence on Artificial Intelligence.
O. Etzioni, M. Cafarella, A. Popescu, T. Shaked,
S. Soderland, D. Weld, and A. Yates. 2005. Unsuper-
vised Named-Entity Extraction from the Web: An Ex-
perimental Study. Artificial Intelligence, 165(1):91?
134.
J. Furnkranz, T. Mitchell, and E. Riloff. 1998. A Case
Study in Using Linguistic Phrases for Text Catego-
rization from the WWW. In Working Notes of the
AAAI/ICML Workshop on Learning for Text Catego-
rization.
W. Hoyle. 1973. Automatic Indexing and Generation
of Classification Systems by Algorithm. Information
Storage and Retrieval, 9(4):233?242.
T. Joachims. 1998. Text categorization with support vec-
tor machines: Learning with many relevant features.
In Proceedings of the European Conference on Ma-
chine Learning (ECML).
S. Keerthi and D. DeCoste. 2005. A Modified Finite
Newton Method for Fast Solution of Large Scale Lin-
ear SVMs. Journal of Machine Learning Research.
Z. Kozareva, E. Riloff, and E. Hovy. 2008. Semantic
Class Learning from the Web with Hyponym Pattern
Linkage Graphs. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies (ACL-08).
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proceedings
of the Eighteenth International Conference on Ma-
chine Learning.
H. Lodhi, J. Shawe-Taylor, N. Christianini, and
C. Watkins. 2001. Text classification using string ker-
nels. In Advances in Neural Information Processing
Systems (NIPS).
T. McIntosh and J. Curran. 2009. Reducing Semantic
Drift with Bagging and Distributional Similarity. In
Proceedings of the 47th Annual Meeting of the Associ-
ation for Computational Linguistics.
G. Miller. 1990. Wordnet: An On-line Lexical Database.
International Journal of Lexicography, 3(4).
K. Nigam, A. McCallum, S. Thrun, and T. Mitchell.
2000. Text Classification from Labeled and Unla-
beled Documents using EM. Machine Learning, 39(2-
3):103?134, May.
E. Riloff and R. Jones. 1999. Learning Dictionaries for
Information Extraction by Multi-Level Bootstrapping.
In Proceedings of the Sixteenth National Conference
on Artificial Intelligence.
E. Riloff and W. Lehnert. 1994. Information Ex-
traction as a Basis for High-Precision Text Classifi-
cation. ACM Transactions on Information Systems,
12(3):296?333, July.
B. Roark and E. Charniak. 1998. Noun-phrase Co-
occurrence Statistics for Semi-automatic Semantic
Lexicon Construction. In Proceedings of the 36th
Annual Meeting of the Association for Computational
Linguistics, pages 1110?1116.
S. Scott and M. Stan. 1998. Text classification using
WordNet hypernyms. In In Use of WordNet in Natu-
ral Language Processing Systems: Proceedings of the
Conference.
F. Sebastiani. 2002. Machine learning in automated text
categorization. In ACM computing surveys (CSUR).
V. Vyas and P. Pantel. 2009. Semi-automatic entity set
refinement. In Proceedings of North American Asso-
ciation for Computational Linguistics / Human Lan-
guage Technology (NAACL/HLT-09).
P. Wang, J. Hu, H. Zeng, and Z. Chen. 2009. Using
Wikipedia knowledge to improve text classification.
In Knowledge and Information Systems.
S. Zelikovitz and H. Hirsh. 2001. Using LSI for text
classication in the presence of background text. In
Proceedings of the 10th International Conference on
Information and Knowledge Management (CIKM).
1562
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1203?1209,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Learning Emotion Indicators from Tweets: Hashtags, Hashtag Patterns,
and Phrases
Ashequl Qadir
School of Computing
University of Utah
Salt Lake City, UT 84112, USA
asheq@cs.utah.edu
Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT 84112, USA
riloff@cs.utah.edu
Abstract
We present a weakly supervised approach
for learning hashtags, hashtag patterns, and
phrases associated with five emotions: AFFEC-
TION, ANGER/RAGE, FEAR/ANXIETY, JOY,
and SADNESS/DISAPPOINTMENT. Starting
with seed hashtags to label an initial set of
tweets, we train emotion classifiers and use
them to learn new emotion hashtags and hash-
tag patterns. This process then repeats in a
bootstrapping framework. Emotion phrases
are also extracted from the learned hashtags
and used to create phrase-based emotion clas-
sifiers. We show that the learned set of emo-
tion indicators yields a substantial improve-
ment in F-scores, ranging from +%5 to +%18
over baseline classifiers.
1 Introduction
Identifying emotions in social media text can be benefi-
cial for many applications, for example to help compa-
nies understand how people feel about their products,
to assist governments in recognizing growing anger or
fear associated with an event, or to help media outlets
understand people?s emotional response toward contro-
versial issues or international affairs. On the Twitter
micro-blogging platform, people often use hashtags to
express an emotional state (e.g., #happyasalways, #an-
gryattheworld). While some hashtags consist of a sin-
gle word (e.g., #angry), many hashtags include multi-
ple words and creative spellings (e.g., #cantwait4tmrw,
#Youredabest), which can not be easily recognized us-
ing sentiment or emotion lexicons.
Our research learns three types of emotion in-
dicators for tweets: hashtags, hashtag patterns,
and phrases for one of five emotions: AFFEC-
TION, ANGER/RAGE, FEAR/ANXIETY, JOY, or SAD-
NESS/DISAPPOINTMENT. We present a bootstrapping
framework for learning emotion hashtags and extend
the framework to also learn more general hashtag pat-
terns. We then harvest emotion phrases from the hash-
tags and hashtag patterns for contextual emotion clas-
sification.
First, we make the observation that emotion hashtags
often share a common prefix. For example, #angry-
attheworld and #angryatlife both have the prefix ?an-
gry at?, which suggests the emotion ANGER. Conse-
quently, we generalize beyond specific hashtags to cre-
ate hashtag patterns that will match all hashtags with
the same prefix, such as the pattern #angryat* which
will match both #angryattheworld and #angryatlife.
A key challenge is that a seemingly strong emotion
word or phrase can have a different meaning depending
upon the following words. For example, #angry* may
seem like an obvious pattern to identify ANGER tweets.
But #angrybirds is a popular hashtag that refers to a
game, not the writer?s emotion. Similarly, ?love you?
usually expresses AFFECTION when it is followed by
a person (e.g., #loveyoumom). But it may express JOY
in other contexts (e.g., #loveyoulife). We use probabil-
ity estimates to determine which hashtag patterns are
reliable indicators for an emotion.
Our second observation is that hashtags can also be
used to harvest emotion phrases. For example, if we
learn that the hashtag #lovelife is associated with JOY,
then we can extract the phrase ?love life? from the
hashtag and use it to recognize emotion in the body
of tweets. However, unlike hashtags, which are self-
contained, the words surrounding a phrase in a tweet
must also be considered. For example, negation can
toggle polarity (?don?t love life? may suggest SAD-
NESS, not JOY) and the aspectual context may indicate
that no emotion is being expressed (e.g., ?I would love
life if ...?). Consequently, we train classifiers to deter-
mine if a tweet contains an emotion based on both an
emotion phrase and its context.
2 Related Work
In addition to sentiment analysis, which has been
widely studied (e.g., (Barbosa and Feng, 2010; Brody
and Diakopoulos, 2011; Kouloumpis et al., 2011;
Mitchell et al., 2013)), recognizing emotions in social
media text has also become a popular research topic in
recent years. Researchers have studied feature sets and
linguistic styles (Roberts et al., 2012), emotion influ-
encing behaviors (Kim et al., 2012), sentence contexts
(Yang et al., 2007b), hierarchical emotion classifica-
tion (Ghazi et al., 2010; Esmin et al., 2012) and emo-
tion lexicon creation (Yang et al., 2007a; Mohammad,
2012a; Staiano and Guerini, 2014). Researchers have
also started to utilize the hashtags of tweets, but pri-
marily to collect labeled data (e.g., for sarcasm (Davi-
1203
Figure 1: Bootstrapped Learning. (HT = hashtag; HP = hashtag pattern)
dov et al., 2010; Riloff et al., 2013) and for senti-
ment/emotion data (Wang et al., 2012; Mohammad et
al., 2013; Choudhury et al., 2012; Purver and Bat-
tersby, 2012; Mohammad, 2012a)).
Wang et al. (2011) investigated several graph based
algorithms to collectively classify hashtag sentiments,
but their work is focused on positive versus nega-
tive polarity classification. Our research extends the
preliminary work on bootstrapped learning of emo-
tion hashtags (Qadir and Riloff, 2013) to additionally
learn patterns corresponding to hashtag prefix expres-
sions and to extract emotion phrases from the hashtags,
which are used to train phrase-based emotion classi-
fiers.
3 Learning Emotion Hashtags, Hashtag
Patterns and Phrases
For our research, we collapsed Parrot?s emo-
tion taxonomy (Parrott, 2001)
1
into 5 emotion
classes that frequently occur in tweets and min-
imally overlap with each other: AFFECTION,
ANGER/RAGE, FEAR/ANXIETY, JOY, and SAD-
NESS/DISAPPOINTMENT. We also used a NONE OF
THE ABOVE class for tweets that do not express any
emotion or express an emotion different from our five
classes. For each of these categories, we identified 5
common hashtags that are strongly associated with the
emotion and used them as seeds. Table 1 shows the
seed hashtags.
Compared to the Ekman emotion classes (Ekman,
1992), one of the emotion taxonomies frequently used
in NLP research (Strapparava and Mihalcea, 2007; Mo-
hammad, 2012b), JOY, ANGER, SADNESS and FEAR
are comparable to 4 of our 5 emotion classes. We do
not study Ekman?s SURPRISE and DISGUST classes,
but include AFFECTION.
3.1 Learning Hashtags
Figure 1 presents the framework of the bootstrapping
algorithm for hashtag learning. The process begins by
1
There were other emotions in Parrott?s taxonomy such
as SURPRISE, NEGLECT, etc. that we did not use for this
research.
Emotion Classes Seed Hashtags
AFFECTION #loveyou, #sweetheart, #bff
#romantic, #soulmate
ANGER & RAGE #angry, #mad, #hateyou
#pissedoff, #furious
FEAR & ANXIETY #afraid, #petrified, #scared
#anxious, #worried
JOY #happy, #excited, #yay
#blessed, #thrilled
SADNESS & #sad, #depressed
DISAPPOINTMENT #disappointed, #unhappy
#foreveralone
Table 1: Emotion Classes and Seed Hashtags
collecting tweets that contain the seed hashtags and la-
beling them with the corresponding emotion. For this
purpose, we collected 323,000 tweets in total that con-
tain at least one of our seed hashtags. We also exploit a
large pool of unlabeled tweets to use during bootstrap-
ping, consisting of 2.3 million tweets with at least one
hashtag per tweet (because we want to learn hashtags),
collected using Twitter?s streaming API. We did not in-
clude retweets or tweets with URLs, to reduce duplica-
tion and focus on tweets with original content. The un-
labeled tweets dataset had 1.29 average hashtags-per-
tweet and 3.95 average tweets-per-hashtag. We prepro-
cessed the tweets with CMU?s tokenizer (Owoputi et
al., 2013) and normalized with respect to case.
The labeled tweets are then used to train a set of
emotion classifiers. We trained one logistic regression
classifier for each emotion class using the LIBLINEAR
package (Fan et al., 2008). We chose logistic regression
because it produces probabilities with its predictions,
which are used to assign scores to hashtags. As fea-
tures, we used unigrams and bigrams with frequency>
1. We removed the seed hashtags from the tweets so
the classifiers could not use them as features.
For each emotion class e ? E, the tweets contain-
ing a seed hashtag for e were used as positive training
instances. The negative training instances consisted of
the tweets containing seed hashtags for the competing
emotions as well as 100,000 randomly selected tweets
1204
Affection Anger & Fear & Joy Sadness &
Rage Anxiety Disappointment
#yourthebest #godie #hatespiders #tripleblessed #leftout
#myotherhalf #donttalktome #haunted #tgfad #foreverugly
#bestfriendforever #pieceofshit #shittingmyself #greatmood #singleprobs
#loveyoulots #irritated #worstfear #thankful #lonerlyfe
#flyhigh #fuming #scaresme #atlast #teamlonely
#comehomesoon #hateliars #nightmares #feelinggood #unloved
#wuvyou #heated #paranoid #happygirl #friendless
#alwaysandforever #getoutofmylife #hateneedles #godisgreat #heartbroken
#missyousomuch #angrytweet #frightened #superhappy #needalife
#loveyougirl #dontbothermewhen #freakedout #ecstatic #letdown
Table 2: Examples of Learned Hashtags
from our unlabeled tweets. Although some of the unla-
beled tweets may correspond to emotion e, we expect
that most will have no emotion or an emotion different
from e, giving us a slightly noisy but large, diverse set
of negative instances.
We then apply each emotion classifier to the un-
labeled tweets. For each emotion e, we collect the
tweets classified as e and extract the hashtags from
those tweets to create a candidate pool H
e
of hashtags
for emotion e. To limit the number of candidates, we
discard hashtags that occur < 10 times, have just one
character, or have> 20 characters. Next, we score each
candidate hashtag h by computing the average proba-
bility assigned by the logistic regression classifier for
emotion e over all of the tweets containing hashtag h.
For each emotion class, we select the 10 hashtags with
the highest scores. From the unlabeled tweets, we then
add all tweets with one of the learned hashtags to the
training instances, and the bootstrapping process con-
tinues. Table 2 shows examples of the learned hashtags.
3.2 Learning Hashtag Patterns
We learn hashtag patterns in a similar but separate boot-
strapping process. We first expand each hashtag into a
sequence of words using an N-gram based word seg-
mentation algorithm
2
supplied with corpus statistics
from our tweet collection. For example, #angryatlife
expands
3
to the phrase ?angry at life?. We use a Prefix
Tree (Trie) data structure to represent all possible pre-
fixes of the expanded hashtag phrases, but the prefixes
consist of words instead of characters.
Next, we traverse the tries and consider all possi-
ble prefix paths as candidate hashtag patterns. We
only consider prefixes that have occurred with at least
one following word. For example, #angryashell, #an-
gryasalways, #angrybird, #angryatlife, #angryatyou
would produce patterns: #angry*, #angryas*, #an-
gryat* as shown in Figure 2.
We score each pattern by applying the classifier for
2
http://norvig.com/ngrams/
3
On a random sample of 100 hashtags, we found expan-
sion accuracy to be 76% (+8% partially correct expansions).
Figure 2: Trie of example hashtags with prefix angry.
Dotted lines lead to non-terminal nodes where patterns
are extracted.
emotion e (trained in the same way as hashtag learn-
ing) to all tweets having hashtags that match the pat-
tern. We compute the average probability produced by
the classifier, and for each emotion class, we select the
10 hashtag patterns with the highest scores. From the
unlabeled tweets, we then add all tweets with hashtags
that match one of the learned hashtag patterns to the
training instances, and the bootstrapping process con-
tinues. Table 3 shows examples of learned hashtag pat-
terns and matched hashtags.
3.3 Creating Phrase-based Classifiers
The third type of emotion indicator that we acquire are
emotion phrases. At the end of the bootstrapping pro-
cess, we apply the word segmentation algorithm to all
of the learned hashtags and hashtag patterns to expand
them into phrases (e.g., #lovemylife ? ?love my life?).
Each phrase is assumed to express the same emotion as
the original hashtag. However, as we will see in Sec-
tion 4, just the presence of a phrase yields low preci-
sion, and surrounding context must also be taken into
account.
Consequently, we train a logistic regression classi-
fier for each emotion e, which classifies a tweet with
respect to emotion e based on the presence of a learned
phrase for e as well as a context window of size 6
around the phrase (set of 3 words on its left and set of 3
1205
Emotion Hashtag Pattern Examples of Matching Hashtags
AFFECTION #bestie* #bestiefolyfe, #bestienight, #bestielove
#missedyou* #missedyoutoomuch, #missedyouguys, #missedyoubabies
ANGER & RAGE #godie* #godieoldman, #godieyou, #godieinahole
#pissedoff* #pissedofffather, #pissedoffnow, #pissedoffmood
FEAR & ANXIETY #tooscared* #tooscaredtogoalone, #tooscaredformama, #tooscaredtomove
#nightmares* #nightmaresfordays, #nightmaresforlife, #nightmarestonight
JOY #feelinggood* #feelinggoodnow, #feelinggoodforme, #feelinggoodabout
#goodmood* #goodmooditsgameday, #goodmoodmode, #goodmoodnight
SADNESS & #bummed* #bummedout, #bummedaf, #bummednow
DISAPPOINTMENT #singlelife* #singlelifeblows, #singlelifeforme, #singlelifesucks
Table 3: Examples of Learned Hashtag Patterns and Matching Hashtags
words on its right). Tweets containing a learned phrase
for e and a seed hashtag for e are the positive training
instances. Tweets containing a learned phrase for e and
a seed hashtag for a different emotion are used as the
negative training instances. For example, when ?love
my life? is learned as an emotion phrase for JOY, the
tweet, ?how can I love my life when everybody leaves
me! #sad? will have one feature each for the left words
?how?, ?can?, and ?I?, one feature each for the right
words ?when?, ?everybody? and ?leaves?, and one
feature for the phrase ?love my life?. The tweet will
then be considered a negative instance for JOY because
?#sad? indicates a different emotion.
4 Experimental Results
To evaluate our learned emotion indicators, we manu-
ally selected 25 topic keywords/phrases
4
that we con-
sidered to be strongly associated with emotions, but
not necessarily with any specific emotions of our study.
We then searched in Twitter using Twitter Search API
for any of these topic phrases and their correspond-
ing hashtags. These 25 topic phrases are: Prom,
Exam, Graduation, Marriage, Divorce, Husband, Wife,
Boyfriend, Girlfriend, Job, Hire, Laid Off, Retirement,
Win, Lose, Accident, Failure, Success, Spider, Loud
Noise, Chest Pain, Storm, Home Alone, No Sleep and
Interview. Since the purpose is to evaluate the qual-
ity and coverage of the emotion hashtags that we learn,
we filtered out any tweet that did not have at least one
hashtag.
Two annotators were given annotation guidelines
and were instructed to label each tweet with up to
two emotions. The instructions specified that the emo-
tion must be felt by the writer. The annotators an-
notated 500 tweets with an inter-annotator agreement
level of 0.79 Kappa (?) (Carletta, 1996). The an-
notation disagreements in these 500 tweets were then
adjudicated, and each annotator labeled an additional
2,500 tweets. Altogether this gave us an emotion an-
notated dataset of 5,500 tweets. We randomly sepa-
rated out 1,000 tweets from this collection as a tuning
4
This data collection process is similar to the emotion
tweet dataset creation by Roberts et al. (2012)
set, and used the remaining 4,500 tweets as evaluation
data. The distribution of emotions in the evaluation
data was 6% for AFFECTION, 9% for ANGER/RAGE,
13% for FEAR/ANXIETY, 22% for JOY, and 12% for
SADNESS/DISAPPOINTMENT. 42% of the tweets had
none of the 5 emotions and 4% of the tweets had more
than one emotions in the same tweet.
We created two baseline systems to assess the diffi-
culty of the emotion classification task. First, we cre-
ated SVM classifiers for each emotion using N-gram
features and performed 10-fold cross-validation on the
test data. We used LIBSVM (Chang and Lin, 2011)
and set the cost and gamma parameters based on the
tuning data. Second, we acquired the NRC Emotional
Tweets Lexicon (Mohammad, 2012a), which contains
emotion unigrams and bigrams for 8 emotions, 4 that
are comparable to ours: ANGER, FEAR, JOY and SAD-
NESS. We created a hashtag from each term in the lexi-
con by appending a # symbol on the front and removing
whitespace. For each term, we chose the emotion with
the highest score in the lexicon.
Table 4 shows our experimental results. The baseline
classifiers (SVM
1
uses unigrams, SVM
1+2
uses uni-
grams and bigrams) have low recall but 63-78% pre-
cision. The hashtags created from the NRC Lexicon
have low precision. This could be due to possible en-
tries (e.g., ?candy? or ?idea?), which without context
are not much indicative of any specific emotion.
The second section of Table 4 shows the results when
we label a tweet based on the presence of a hash-
tag or hashtag pattern. First, we use just the 5 seed
hashtags to assess their coverage (as expected, high
precision but low recall). Next, we add the hashtags
learned during bootstrapping. For most emotions, the
hashtags achieve performance similar to the supervised
SVMs. The following row shows results for our learned
hashtag patterns. Recall improves by +14% for AF-
FECTION, which illustrates the benefit of more general
hashtag patterns, and at least maintains similar level of
precision for other emotions. When the hashtags and
hashtag patterns are combined (HTs+HPs), we see the
best of both worlds with improved recall as high as
+17% in AFFECTION and +10% in FEAR/ANXIETY
1206
AFFECTION ANGER & FEAR & JOY SADNESS &
Method RAGE ANXIETY DISAPPOINT.
P R F P R F P R F P R F P R F
Baselines
SVM
1
78 40 53 66 17 27 68 33 44 66 47 55 63 26 37
SVM
1+2
78 35 48 67 10 17 68 29 41 65 43 52 63 21 32
NRC Lexicon HTs n/a 26 16 20 39 12 18 36 13 19 28 18 22
Learned Hashtags (HTs) and Hashtag Patterns (HPs)
Seed HTs 94 06 11 75 01 03 100 06 11 93 04 08 81 02 05
All HTs 82 34 48 63 23 34 60 37 46 81 13 22 72 28 40
All HPs 76 48 59 60 22 32 57 42 48 84 09 16 73 16 26
All HTs+HPs 74 51 60 56 27 36 55 47 51 80 15 25 70 29 41
Learned Emotion Phrases
Emotion Phrases 32 28 30 17 46 25 28 45 35 50 23 32 26 30 28
Phrase-based Classifier (PC) 54 07 12 48 05 09 63 17 27 69 12 20 50 06 11
SVM
1
+PC 79 42 55 63 18 28 70 35 47 68 48 56 62 27 38
Hybrid Approach
SVM
1
+PC ? HTs+HPs 69 64 66 55 38 45 54 61 57 68 54 60 62 44 51
Table 4: Emotion Classification Results (P = Precision, R = Recall, F = F-score)
compared to All HTs, as well as improved F-scores
across the board.
The third section of Table 4 presents the results for
the emotion phrases. The first row (Emotion Phrases)
shows that labeling a tweet based solely on the pres-
ence of a phrase is not very accurate. Next, we applied
the trained models of the phrase-based classifiers (de-
scribed in Section 3.3) to each tweet of the evaluation
data. This provided us with probability of an emotion
for each of the 5 emotions. The phrase-based classifiers
(PC) yield higher precision, albeit with low recall. Fi-
nally, we use these probabilities as 5 additional features
to SVM
1
. The corresponding SVM
1
+PC row shows
a consistent 1-2 point F score gain over the original
SVM
1
baseline.
The last section of Table 4 shows the best results with
a hybrid system, which labels a tweet with emotion e if
EITHER the enhanced SVM labels it as e OR the tweet
contains a hashtag or hashtag pattern associated with e.
This combined approach achieves substantially higher
performance than any individual method across all 5
emotion classes, with improved F-scores ranging from
+%5 to +%18 over the baseline classifiers, demonstrat-
ing that the different types of emotion indicators are
complementary.
5 Conclusions
We have shown that three types of emotion indicators
can be learned from tweets with weakly supervised
bootstrapping: hashtags, hashtag patterns, and phrases.
Our findings suggest that emotion hashtags are strong
indicators for recognizing writer?s emotion in tweets,
and can be further generalized into hashtag patterns by
learning prefix expressions corresponding to an emo-
tion. Phrases learned from the hashtags and patterns
are not always reliable by themselves, but training ad-
ditional classifiers with the emotion phrases and their
surrounding context provides added benefits to emotion
classification in tweets. Our results showed that com-
bining the learned emotion indicators with an N-gram
classifier in a hybrid approach substantially improves
performance across 5 emotion classes.
Acknowledgments
This work was supported by the Intelligence Advanced
Research Projects Activity (IARPA) via Department of
Interior National Business Center (DoI/NBC) contract
number D12PC00285. The U.S. Government is autho-
rized to reproduce and distribute reprints for Govern-
mental purposes notwithstanding any copyright anno-
tation thereon. The views and conclusions contained
herein are those of the authors and should not be in-
terpreted as necessarily representing the official poli-
cies or endorsements, either expressed or implied, of
IARPA, DoI/NBC, or the U.S. Government.
References
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on twitter from biased and noisy data.
In Proceedings of the 23rd International Conference
on Computational Linguistics: Posters, COLING
?10.
Samuel Brody and Nicholas Diakopoulos. 2011.
Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!: using
word lengthening to detect sentiment in microblogs.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
?11.
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: the kappa statistic. Comput. Linguist.,
22:249?254, June.
1207
Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:
A library for support vector machines. ACM Trans.
Intell. Syst. Technol., 2(3):27:1?27:27, May.
Munmun De Choudhury, Michael Gamon, and Scott
Counts. 2012. Happy, nervous or surprised? clas-
sification of human affective states in social media.
In Proceedings of the Sixth International Conference
on Weblogs and Social Media.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised recognition of sarcastic sentences
in twitter and amazon. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, CoNLL ?10.
Paul Ekman. 1992. An argument for basic emotions.
Cognition and Emotion, 6(3):169200.
Ahmed Ali Abdalla Esmin, Roberto L. De Oliveira Jr.,
and Stan Matwin. 2012. Hierarchical classifica-
tion approach to emotion recognition in twitter. In
Proceedings of the 11th International Conference on
Machine Learning and Applications, ICMLA, Boca
Raton, FL, USA, December 12-15, 2012. Volume 2,
pages 381?385. IEEE.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. J. Mach. Learn.
Res., 9:1871?1874, June.
Diman Ghazi, Diana Inkpen, and Stan Szpakowicz.
2010. Hierarchical versus flat classification of
emotions in text. In Proceedings of the NAACL
HLT 2010 Workshop on Computational Approaches
to Analysis and Generation of Emotion in Text,
CAAGET ?10.
Suin Kim, JinYeong Bak, and Alice Oh. 2012.
Discovering emotion influence patterns in online
social network conversations. SIGWEB Newsl.,
(Autumn):3:1?3:6, September.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the omg! In Proceedings of the Fifth In-
ternational Conference on Weblogs and Social Me-
dia.
Margaret Mitchell, Jacqui Aguilar, Theresa Wilson,
and Benjamin Van Durme. 2013. Open domain tar-
geted sentiment. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. Nrc-canada: Building the state-of-the-
art in sentiment analysis of tweets. In Proceedings
of the seventh international workshop on Semantic
Evaluation Exercises (SemEval-2013).
Saif Mohammad. 2012a. #emotional tweets. In *SEM
2012: The First Joint Conference on Lexical and
Computational Semantics.
Saif Mohammad. 2012b. Portable features for clas-
sifying emotional text. In Proceedings of the 2012
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies.
Olutobi Owoputi, Brendan OConnor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of the North American Chapter of the
Association for Computational Linguistics (NAACL-
2013).
W. Gerrod Parrott, editor. 2001. Emotions in Social
Psychology. Psychology Press.
Matthew Purver and Stuart Battersby. 2012. Experi-
menting with distant supervision for emotion classi-
fication. In Proceedings of the 13th Conference of
the European Chapter of the Association for Com-
putational Linguistics, EACL ?12, pages 482?491.
Ashequl Qadir and Ellen Riloff. 2013. Bootstrapped
learning of emotion hashtags #hashtags4you. In
Proceedings of the 4th Workshop on Computational
Approaches to Subjectivity, Sentiment and Social
Media Analysis.
Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalin-
dra De Silva, Nathan Gilbert, and Ruihong Huang.
2013. Sarcasm as contrast between a positive senti-
ment and negative situation. In Proceedings of the
2013 Conference on Empirical Methods in Natural
Language Processing, EMNLP ?13.
Kirk Roberts, Michael A. Roach, Joseph Johnson, Josh
Guthrie, and Sanda M. Harabagiu. 2012. Em-
patweet: Annotating and detecting emotions on twit-
ter. In Proceedings of the Eighth International
Conference on Language Resources and Evaluation
(LREC-2012). ACL Anthology Identifier: L12-
1059.
Jacopo Staiano and Marco Guerini. 2014. De-
pechemood: a lexicon for emotion analysis from
crowd-annotated news. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers).
Carlo Strapparava and Rada Mihalcea. 2007.
SemEval-2007 Task 14: Affective Text. In Proceed-
ings of the Fourth International Workshop on Se-
mantic Evaluations (SemEval-2007).
Xiaolong Wang, Furu Wei, Xiaohua Liu, Ming Zhou,
and Ming Zhang. 2011. Topic sentiment analysis
in twitter: a graph-based hashtag sentiment classifi-
cation approach. In Proceedings of the 20th ACM
international conference on Information and knowl-
edge management, CIKM ?11.
Wenbo Wang, Lu Chen, Krishnaprasad Thirunarayan,
and Amit P. Sheth. 2012. Harnessing twitter ?big
data? for automatic emotion identification. In Pro-
ceedings of the 2012 ASE/IEEE International Con-
ference on Social Computing and 2012 ASE/IEEE
1208
International Conference on Privacy, Security, Risk
and Trust, SOCIALCOM-PASSAT ?12.
Changhua Yang, Kevin Hsin-Yih Lin, and Hsin-Hsi
Chen. 2007a. Building emotion lexicon from
weblog corpora. In Proceedings of the 45th An-
nual Meeting of the ACL on Interactive Poster and
Demonstration Sessions, ACL ?07.
Changhua Yang, Kevin Hsin-Yih Lin, and Hsin-Hsi
Chen. 2007b. Emotion classification using web blog
corpora. In Proceedings of the IEEE/WIC/ACM In-
ternational Conference on Web Intelligence, WI ?07,
pages 275?278.
1209
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 286?295,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Bootstrapped Training of Event Extraction Classifiers
Ruihong Huang and Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT 84112
{huangrh,riloff}@cs.utah.edu
Abstract
Most event extraction systems are trained
with supervised learning and rely on a col-
lection of annotated documents. Due to
the domain-specificity of this task, event
extraction systems must be retrained with
new annotated data for each domain. In
this paper, we propose a bootstrapping so-
lution for event role filler extraction that re-
quires minimal human supervision. We aim
to rapidly train a state-of-the-art event ex-
traction system using a small set of ?seed
nouns? for each event role, a collection
of relevant (in-domain) and irrelevant (out-
of-domain) texts, and a semantic dictio-
nary. The experimental results show that
the bootstrapped system outperforms previ-
ous weakly supervised event extraction sys-
tems on the MUC-4 data set, and achieves
performance levels comparable to super-
vised training with 700 manually annotated
documents.
1 Introduction
Event extraction systems process stories about
domain-relevant events and identify the role fillers
of each event. A key challenge for event extrac-
tion is that recognizing role fillers is inherently
contextual. For example, a PERSON can be a
perpetrator or a victim in different contexts (e.g.,
?John Smith assassinated the mayor? vs. ?John
Smith was assassinated?). Similarly, any COM-
PANY can be an acquirer or an acquiree depending
on the context.
Many supervised learning techniques have
been used to create event extraction systems us-
ing gold standard ?answer key? event templates
for training (e.g., (Freitag, 1998a; Chieu and Ng,
2002; Maslennikov and Chua, 2007)). How-
ever, manually generating answer keys for event
extraction is time-consuming and tedious. And
more importantly, event extraction annotations
are highly domain-specific, so new annotations
must be obtained for each domain.
The goal of our research is to use bootstrap-
ping techniques to automatically train a state-of-
the-art event extraction system without human-
generated answer key templates. The focus of our
work is the TIER event extraction model, which
is a multi-layered architecture for event extrac-
tion (Huang and Riloff, 2011). TIER?s innova-
tion over previous techniques is the use of four
different classifiers that analyze a document at in-
creasing levels of granularity. TIER progressively
zooms in on event information using a pipeline
of classifiers that perform document-level classi-
fication, sentence classification, and noun phrase
classification. TIER outperformed previous event
extraction systems on the MUC-4 data set, but re-
lied heavily on a large collection of 1,300 docu-
ments coupled with answer key templates to train
its four classifiers.
In this paper, we present a bootstrapping solu-
tion that exploits a large unannotated corpus for
training by using role-identifying nouns (Phillips
and Riloff, 2007) as seed terms. Phillips and
Riloff observed that some nouns, by definition,
refer to entities or objects that play a specific role
in an event. For example, ?assassin?, ?sniper?,
and ?hitman? refer to people who play the role
of PERPETRATOR in a criminal event. Similarly,
?victim?, ?casualty?, and ?fatality? refer to peo-
ple who play the role of VICTIM, by virtue of
their lexical semantics. Phillips and Riloff called
these words role-identifying nouns and used them
286
to learn extraction patterns. Our research also
uses role-identifying nouns to learn extraction pat-
terns, but the role-identifying nouns and patterns
are then used to create training data for event ex-
traction classifiers. Each classifier is then self-
trained in a bootstrapping loop.
Our weakly supervised training procedure re-
quires a small set of ?seed nouns? for each event
role, and a collection of relevant (in-domain) and
irrelevant (out-of-domain) texts. No answer key
templates or annotated texts are needed. The seed
nouns are used to automatically generate a set
of role-identifying patterns, and then the nouns,
patterns, and a semantic dictionary are used to
label training instances. We also propagate the
event role labels across coreferent noun phrases
within a document to produce additional train-
ing instances. The automatically labeled texts are
used to train three components of TIER: its two
types of sentence classifiers and its noun phrase
classifiers. To create TIER?s fourth component,
its document genre classifier, we apply heuristics
to the output of the sentence classifiers.
We present experimental results on the MUC-
4 data set, which is a standard benchmark for
event extraction research. Our results show that
the bootstrapped system, TIERlite, outperforms
previous weakly supervised event extraction sys-
tems and achieves performance levels comparable
to supervised training with 700 manually anno-
tated documents.
2 Related Work
Event extraction techniques have largely focused
on detecting event ?triggers? with their arguments
for extracting role fillers. Classical methods are
either pattern-based (Kim and Moldovan, 1993;
Riloff, 1993; Soderland et al 1995; Huffman,
1996; Freitag, 1998b; Ciravegna, 2001; Califf and
Mooney, 2003; Riloff, 1996; Riloff and Jones,
1999; Yangarber et al 2000; Sudo et al 2003;
Stevenson and Greenwood, 2005) or classifier-
based (e.g., (Freitag, 1998a; Chieu and Ng, 2002;
Finn and Kushmerick, 2004; Li et al 2005; Yu et
al., 2005)).
Recently, several approaches have been pro-
posed to address the insufficiency of using only
local context to identify role fillers. Some ap-
proaches look at the broader sentential context
around a potential role filler when making a de-
cision (e.g., (Gu and Cercone, 2006; Patwardhan
and Riloff, 2009)). Other systems take a more
global view and consider discourse properties of
the document as a whole to improve performance
(e.g., (Maslennikov and Chua, 2007; Ji and Gr-
ishman, 2008; Liao and Grishman, 2010; Huang
and Riloff, 2011)). Currently, the learning-based
event extraction systems that perform best all use
supervised learning techniques that require a large
number of texts coupled with manually-generated
annotations or answer key templates.
A variety of techniques have been explored
for weakly supervised training of event extrac-
tion systems, primarily in the realm of pattern or
rule-based approaches (e.g., (Riloff, 1996; Riloff
and Jones, 1999; Yangarber et al 2000; Sudo et
al., 2003; Stevenson and Greenwood, 2005)). In
some of these approaches, a human must man-
ually review and ?clean? the learned patterns to
obtain good performance. Research has also been
done to learn extraction patterns in an unsuper-
vised way (e.g., (Shinyama and Sekine, 2006;
Sekine, 2006)). But these efforts target open do-
main information extraction. To extract domain-
specific event information, domain experts are
needed to select the pattern subsets to use.
There have also been weakly supervised ap-
proaches that use more than just local context.
(Patwardhan and Riloff, 2007) uses a semantic
affinity measure to learn primary and secondary
patterns, and the secondary patterns are applied
only to event sentences. The event sentence clas-
sifier is self-trained using seed patterns. Most
recently, (Chambers and Jurafsky, 2011) acquire
event words from an external resource, group the
event words to form event scenarios, and group
extraction patterns for different event roles. How-
ever, these weakly supervised systems produce
substantially lower performance than the best su-
pervised systems.
3 Overview of TIER
The goal of our research is to develop a weakly
supervised training process that can successfully
train a state-of-the-art event extraction system for
a new domain with minimal human input. We de-
cided to focus our efforts on the TIER event ex-
traction model because it recently produced bet-
ter performance on the MUC-4 data set than prior
learning-based event extraction systems (Huang
and Riloff, 2011). In this section, we briefly give
an overview of TIER?s architecture and its com-
287
Figure 1: TIER Overview
ponents.
TIER is a multi-layered architecture for event
extraction, as shown in Figure 1. Documents pass
through a pipeline where they are analyzed at dif-
ferent levels of granularity, which enables the sys-
tem to gradually ?zoom in? on relevant facts. The
pipeline consists of a document genre classifier,
two types of sentence classifiers, and a set of noun
phrase (role filler) classifiers.
The lower pathway in Figure 1 shows that all
documents pass through an event sentence clas-
sifier. Sentences labeled as event descriptions
then proceed to the noun phrase classifiers, which
are responsible for identifying the role fillers in
each sentence. The upper pathway in Figure 1 in-
volves a document genre classifier to determine
whether a document is an ?event narrative? story
(i.e., an article that primarily discusses the details
of a domain-relevant event). Documents that are
classified as event narratives warrant additional
scrutiny because they most likely contain a lot of
event information. Event narrative stories are pro-
cessed by an additional set of role-specific sen-
tence classifiers that look for role-specific con-
texts that will not necessarily mention the event.
For example, a victim may be mentioned in a sen-
tence that describes the aftermath of a crime, such
as transportation to a hospital or the identifica-
tion of a body. Sentences that are determined to
have ?role-specific? contexts are passed along to
the noun phrase classifiers for role filler extrac-
tion. Consequently, event narrative documents
pass through both the lower pathway and the up-
per pathway. This approach creates an event ex-
traction system that can discover role fillers in a
variety of different contexts by considering the
type of document being processed.
TIER was originally trained with supervised
learning using 1,300 texts and their corresponding
answer key templates from the MUC-4 data set
(MUC-4 Proceedings, 1992). Human-generated
answer key templates are expensive to produce
because the annotation process is both difficult
and time-consuming. Furthermore, answer key
templates for one domain are virtually never
reusable for different domains, so a new set of
answer keys must be produced from scratch for
each domain. In the next section, we present our
weakly supervised approach for training TIER?s
event extraction classifiers.
4 Bootstrapped Training of Event
Extraction Classifiers
We adopt a two-phase approach to train TIER?s
event extraction modules using minimal human-
generated resources. The goal of the first phase
is to automatically generate positive training ex-
amples using role-identifying seed nouns as input.
The seed nouns are used to automatically gener-
ate a set of role-identifying patterns for each event
role. Each set of patterns is then assigned a set
of semantic constraints (selectional restrictions)
that are appropriate for that event role. The se-
mantic constraints consist of the role-identifying
seed nouns as well as general semantic classes
that constrain the event role (e.g., a victim must
be a HUMAN). A noun phrase will satisfy the se-
mantic constraints if its head noun is in the seed
noun list or if it has the appropriate semantic type
(based on dictionary lookup). Each pattern is then
matched against the unannotated texts, and if the
extracted noun phrase satisfies its semantic con-
straints, then the noun phrase is automatically la-
beled as a role filler.
The second phase involves bootstrapped train-
ing of TIER?s classifiers. Using the labeled in-
stances generated in the first phase, we iteratively
train three of TIER?s components: the two types
of sentential classifiers and the noun phrase clas-
sifiers. For the fourth component, the document
classifier, we apply heuristics to the output of the
sentence classifiers to assess the density of rel-
evant sentences in a document and label high-
density stories as event narratives. In the fol-
lowing sections, we present the details of each of
these steps.
4.1 Automatically Labeling Training Data
Finding seeding instances of high precision and
reasonable coverage is important in bootstrap-
ping. However, this is especially challenging
for event extraction task because identifying role
fillers is inherently contextual. Furthermore, role
288
Figure 2: Using Basilisk to Induce Role-Identifying
Patterns
fillers occur sparsely in text and in diverse con-
texts.
In this section, we explain how we gener-
ate role-identifying patterns automatically using
seed nouns, and we discuss why we add seman-
tic constraints to the patterns when producing la-
beled instances for training. Then, we discuss the
coreference-based label propagation that we used
to obtain additional training instances. Finally, we
give examples to illustrate how we create training
instances.
4.1.1 Inducing Role-Identifying Patterns
The input to our system is a small set of
manually-defined seed nouns for each event role.
Specifically, the user is required to provide
10 role-identifying nouns for each event role.
(Phillips and Riloff, 2007) defined a noun as be-
ing ?role-identifying? if its lexical semantics re-
veal the role of the entity/object in an event. For
example, the words ?assassin? and ?sniper? are
people who participate in a violent event as a PER-
PETRATOR. Therefore, the entities referred to by
role-identifying nouns are probable role fillers.
However, treating every context surrounding a
role-identifying noun as a role-identifying pattern
is risky. The reason is that many instances of role-
identifying nouns appear in contexts that do not
describe the event. But, if one pattern has been
seen to extract many role-identifying nouns and
seldomly seen to extract other nouns, then the pat-
tern likely represents an event context.
As (Phillips and Riloff, 2007) did, we use
Basilisk to learn patterns for each event role.
Basilisk was originally designed for semantic
class learning (e.g., to learn nouns belonging to
semantic categories, such as building or human).
As shown in Figure 2, beginning with a small set
of seed nouns for each semantic class, Basilisk
learns additional nouns belonging to the same se-
mantic class. Internally, Basilisk uses extraction
patterns automatically generated from unanno-
tated texts to assess the similarity of nouns. First,
Basilisk assigns a score to each pattern based on
the number of seed words that co-occur with it.
Basilisk then collects the noun phrases extracted
by the highest-scoring patterns. Next, the head
noun of each noun phrase is assigned a score
based on the set of patterns that it co-occurred
with. Finally, Basilisk selects the highest-scoring
nouns, automatically labels them with the seman-
tic class of the seeds, adds these nouns to the lex-
icon, and restarts the learning process in a boot-
strapping fashion.
For our work, we give Basilisk role-identifying
seed nouns for each event role. We run the boot-
strapping process for 20 iterations and then har-
vest the 40 best patterns that Basilisk identifies
for each event role. We also tried using the addi-
tional role-identifying nouns learned by Basilisk,
but found that these nouns were too noisy.
4.1.2 Using the Patterns to Label NPs
The induced role-identifying patterns can be
matched against the unannotated texts to produce
labeled instances. However, relying solely on the
pattern contexts can be misleading. For example,
the pattern context <subject> caused damage
will extract some noun phrases that are weapons
(e.g., the bomb) but some noun phrases that are
not (e.g., the tsunami).
Based on this observation, we add selectional
restrictions to each pattern that requires a noun
phrase to satisfy certain semantic constraints in
order to be extracted and labeled as a positive
instances for an event role. The selectional re-
strictions are satisfied if the head noun is among
the role-identifying seed nouns or if the semantic
class of the head noun is compatible with the cor-
responding event role. In the previous example,
tsunami will not be extracted as a weapon because
it has an incompatible semantic class (EVENT),
but bomb will be extracted because it has a com-
patible semantic class (WEAPON).
We use the semantic class labels assigned by
the Sundance parser (Riloff and Phillips, 2004) in
our experiments. Sundance looks up each noun
in a semantic dictionary to assign the semantic
class labels. As an alternative, general resources
(e.g., WordNet (Miller, 1990)) or a semantic tag-
ger (e.g., (Huang and Riloff, 2010)) could be
used.
289
John Smith was killed by 
. . . . . .
 
was killed by <np>
Role?Identifying
Patterns
two armed men
1
an hour later.
Police arrested the unidentified men  
3
 
in broad daylight this morning.  
left his house to go to work about 8:00 am.
The assassins
2
attacked the mayor as he 
<subject> fired shots
men = Human
Role?Identifying  Semantic
 Dictionary 
terrorists
snipers
assassins
. . .
building = Object <subject> attacked
Noun
Constraints Constraints
Figure 3: Automatic Training Data Creation
4.1.3 Propagating Labels with Coreference
To enrich the automatically labeled training in-
stances, we also propagate the event role labels
across coreferent noun phrases within a docu-
ment. The observation is that once a noun phrase
has been identified as a role filler, its corefer-
ent mentions in the same document likely fill the
same event role since they are referring to the
same real world entity.
To leverage these coreferential contexts, we
employ a simple head noun matching heuristic to
identify coreferent noun phrases. This heuristic
assumes that two noun phrases that have the same
head noun are coreferential. We considered us-
ing an off-the-shelf coreference resolver, but de-
cided that the head noun matching heuristic would
likely produce higher precision results, which is
important to produce high-quality labeled data.
4.1.4 Examples of Training Instance
Creation
Figure 3 illustrates how we label training in-
stances automatically. The text example shows
three noun phrases that are automatically labeled
as perpetrators. Noun phrases #1 and #2 oc-
cur in role-identifying pattern contexts (was killed
by <np> and <subject> attacked) and satisfy
the semantic constraints for perpetrators because
?men? has a compatible semantic type and ?assas-
sins? is a role-identifying noun for perpetrators.
Noun phrase #3 (?the unidentified men?) does
not occur in a pattern context, but it is deemed
to be coreferent with ?two armed men? because
they have the same head noun. Consequently, we
propagate the perpetrator label from noun phrase
#1 to noun phrase #3.
4.2 Creating TIERlite with Bootstrapping
In this section, we explain how the labeled in-
stances are used to train TIER?s classifiers with
bootstrapping. In addition to the automatically
labeled instances, the training process depends
on a text corpus that consists of both relevant
(in-domain) and irrelevant (out-of-domain) doc-
uments. Positive instances are generated from
the relevant documents and negative instances are
generated by randomly sampling from the irrele-
vant documents.
The classifiers are all support vector machines
(SVMs), implemented using the SVMlin software
(Keerthi and DeCoste, 2005). When applying the
classifiers during bootstrapping, we use a sliding
confidence threshold to determine which labels
are reliable based on the values produced by the
SVM. Initially, we set the threshold to be 2.0 to
identify highly confident predictions. But if fewer
than k instances pass the threshold, then we slide
the threshold down in decrements of 0.1 until we
obtain at least k labeled instances or the thresh-
old drops below 0, in which case bootstrapping
ends. We used k=10 for both sentence classifiers
and k=30 for the noun phrase classifiers.
The following sections present the details of the
bootstrapped training process for each of TIER?s
components.
Figure 4: The Bootstrapping Process
4.2.1 Noun Phrase Classifiers
The mission of the noun phrase classifiers is to
determine whether a noun phrase is a plausible
event role filler based on the local features sur-
rounding the noun phrase (NP). A set of classifiers
is needed, one for each event role.
As shown in Figure 4, to seed the classifier
training, the positive noun phrase instances are
290
generated from the relevant documents follow-
ing Section 4.1. The negative noun phrase in-
stances are drawn randomly from the irrelevant
documents. Considering the sparsity of role fillers
in texts, we set the negative:positive ratio to be
10:1. Once the classifier is trained, it is applied to
the unlabeled noun phrases in the relevant docu-
ments. Noun phrases that are assigned role filler
labels by the classifier with high confidence (us-
ing the sliding threshold) are added to the set of
positive instances. New negative instances are
drawn randomly from the irrelevant documents to
maintain the 10:1 (negative:positive) ratio.
We extract features from each noun phrase
(NP) and its surrounding context. The features
include the NP head noun and its premodifiers.
We also use the Stanford NER tagger (Finkel et
al., 2005) to identify Named Entities within the
NP. The context features include four words to the
left of the NP, four words to the right of the NP,
and the lexico-syntactic patterns generated by Au-
toSlog to capture expressions around the NP (see
(Riloff, 1993) for details).
4.2.2 Event Sentence Classifier
The event sentence classifier is responsible
for identifying sentences that describe a relevant
event. Similar to the noun phrase classifier train-
ing, positive training instances are selected from
the relevant documents and negative instances are
drawn from the irrelevant documents. All sen-
tences in the relevant documents that contain one
or more labeled noun phrases (belonging to any
event role) are labeled as positive training in-
stances. We randomly sample sentences from the
irrelevant documents to obtain a negative:positive
training instance ratio of 10:1. The bootstrapping
process is then identical to that of the noun phrase
classifiers. The feature set for this classifier con-
sists of unigrams, bigrams and AutoSlog?s lexico-
syntactic patterns surrounding all noun phrases in
the sentence.
4.2.3 Role-Specific Sentence Classifiers
The role-specific sentence classifiers are
trained to identify the contexts specific to each
event role. All sentences in the relevant doc-
uments that contain at least one labeled noun
phrase for the appropriate event role are used
as positive instances. Negative instances are
randomly sampled from the irrelevant documents
to maintain the negative:positive ratio of 10:1.
The bootstrapping process and feature set are the
same as for the event sentence classifier.
The difference between the two types of sen-
tence classifiers is that the event sentence classi-
fier uses positive instances from all event roles,
while each role-specific sentence classifiers only
uses the positive instances for one particular event
role. The rationale is similar as in the super-
vised setting (Huang and Riloff, 2011); the event
sentence classifier is expected to generalize over
all event roles to identify event mention contexts,
while the role-specific sentence classifiers are ex-
pected to learn to identify contexts specific to in-
dividual roles.
4.2.4 Event Narrative Document Classifier
TIER also uses an event narrative document
classifier and only extracts information from role-
specific sentences within event narrative docu-
ments. In the supervised setting, TIER uses
heuristic rules derived from answer key templates
to identify the event narrative documents in the
training set, which are used to train an event nar-
rative document classifier. The heuristic rules re-
quire that an event narrative should have a high
density of relevant information and tend to men-
tion the relevant information within the first sev-
eral sentences.
In our weakly supervised setting, we use the
information density heuristic directly instead of
training an event narrative classifier. We approxi-
mate the relevant information density heuristic by
computing the ratio of relevant sentences (both
event sentences and role-specific sentences) out of
all the sentences in a document. Thus, the event
narrative labeller only relies on the output of the
two sentence classifiers. Specifically, we label a
document as an event narrative if ? 50% of the
sentences in the document are relevant (i.e., la-
beled positively by either sentence classifier).
5 Evaluation
In this section, we evaluate our bootstrapped sys-
tem, TIERlite, on the MUC-4 event extraction
data set. First, we describe the IE task, the data
set, and the weakly supervised baseline systems
that we use for comparison. Then we present the
results of our fully bootstrapped system TIERlite,
the weakly supervised baseline systems, and two
fully supervised event extraction systems, TIER
291
and GLACIER. In addition, we analyze the per-
formance of TIERlite using different configura-
tions to assess the impact of its components.
5.1 IE Task and Data
We evaluated the performance of our systems on
the MUC-4 terrorism IE task (MUC-4 Proceed-
ings, 1992) about Latin American terrorist events.
We used 1,300 texts (DEV) as our training set and
200 texts (TST3+TST4) as the test set. All the
documents have answer key templates. For the
training set, we used the answer keys to separate
the documents into relevant and irrelevant sub-
sets. Any document containing at least one rel-
evant event was considered to be relevant.
PerpInd PerpOrg Target Victim Weapon
129 74 126 201 58
Table 1: # of Role Fillers in the MUC-4 Test Set
Following previous studies, we evaluate our
system on five MUC-4 string event roles: perpe-
trator individuals (PerpInd), perpetrator organi-
zations (PerpOrg), physical targets, victims, and
weapons. Table 1 shows the distribution of role
fillers in the MUC-4 test set. The complete IE task
involves the creation of answer key templates, one
template per event1. Our work focuses on extract-
ing individual role fillers and not template genera-
tion, so we evaluate the accuracy of the role fillers
irrespective of which template they occur in.
We used the same head noun scoring scheme
as previous systems, where an extraction is cor-
rect if its head noun matches the head noun in the
answer key2. Pronouns were discarded from both
the system responses and the answer keys since
no coreference resolution is done. Duplicate ex-
tractions were conflated before being scored, so
they count as just one hit or one miss.
5.2 Weakly Supervised Baselines
We compared the performance of our system with
three previous weakly supervised event extraction
systems.
AutoSlog-TS (Riloff, 1996) generates lexico-
syntactic patterns exhaustively from unannotated
texts and ranks them based on their frequency and
probability of occurring in relevant documents.
A human expert then examines the patterns and
1Documents may contain multiple events per article.
2For example, ?armed men? will match ?5 armed men?.
manually selects the best patterns for each event
role. During testing, the patterns are matched
against unseen texts to extract event role fillers.
PIPER (Patwardhan and Riloff, 2007; Patward-
han, 2010) learns extraction patterns using a se-
mantic affinity measure, and it distinguishes be-
tween primary and secondary patterns and ap-
plies them selectively. (Chambers and Jurafsky,
2011) (C+J) created an event extraction system
by acquiring event words from WordNet (Miller,
1990), clustering the event words into different
event scenarios, and grouping extraction patterns
for different event roles.
5.3 Performance of TIERlite
Table 2 shows the seed nouns that we used in our
experiments, which were generated by sorting the
nouns in the corpus by frequency and manually
identifying the first 10 role-identifying nouns for
each event role.3 Table 3 shows the number of
training instances (noun phrases) that were auto-
matically labeled for each event role using our
training data creation approach (Section 4.1).
Event Role Seed Nouns
Perpetrator terrorists assassins criminals rebels
Individual murderers death squads guerrillas
member members individuals
Perpetrator FMLN ELN FARC MRTA M-19 Front
Organization Shining Path Medellin Cartel
The Extraditables
Army of National Liberation
Target houses residence building home homes
offices pipeline hotel car vehicles
Victim victims civilians children jesuits Galan
priests students women peasants Romero
Weapon weapons bomb bombs explosives rifles
dynamite grenades device car bomb
Table 2: Role-Identifying Seed Nouns
PerpInd PerpOrg Target Victim Weapon
296 157 522 798 248
Table 3: # of Automatically Labeled NPs
Table 4 shows how our bootstrapped system
TIERlite compares with previous weakly super-
vised systems and two supervised systems, its su-
pervised counterpart TIER (Huang and Riloff,
2011) and a model that jointly considers local
and sentential contexts, GLACIER (Patwardhan
3We only found 9 weapon terms among the high-
frequency terms.
292
Weakly Supervised Baselines
PerpInd PerpOrg Target Victim Weapon Average
AUTOSLOG-TS (1996) 33/49/40 52/33/41 54/59/56 49/54/51 38/44/41 45/48/46
PIPERBest (2007) 39/48/43 55/31/40 37/60/46 44/46/45 47/47/47 44/46/45
C+J (2011) - - - - - 44/36/40
Supervised Models
GLACIER (2009) 51/58/54 34/45/38 43/72/53 55/58/56 57/53/55 48/57/52
TIER (2011) 48/57/52 46/53/50 51/73/60 56/60/58 53/64/58 51/62/56
Weakly Supervised Models
TIERlite 47/51/49 60/39/47 37/65/47 39/53/45 53/55/54 47/53/50
Table 4: Performance of the Bootstrapped Event Extraction System (Precision/Recall/F-score)
0 200 400 600 800 1000 1200 1400
30
35
40
45
50
55
60
# of training documents
IE
 pe
rfo
rm
an
ce
(F1
)
Figure 5: The Learning Curve of Supervised TIER
and Riloff, 2009). We see that TIERlite outper-
forms all three weakly supervised systems, with
slightly higher precision and substantially more
recall. When compared to the supervised sys-
tems, the performance of TIERlite is similar to
GLACIER, with comparable precision but slightly
lower recall. But the supervised TIER system,
which was trained with 1,300 annotated docu-
ments, is still superior, especially in recall.
Figure 5 shows the learning curve for TIER
when it is trained with fewer documents, rang-
ing from 100 to 1,300 in increments of 100. Each
data point represents five experiments where we
randomly selected k documents from the train-
ing set and averaged the results. The bars show
the range of results across the five runs. Figure 5
shows that TIER?s performance increases from an
F score of 34 when trained on just 100 documents
up to an F score of 56 when training on 1,300 doc-
uments. The circle shows the performance of our
bootstrapped system, TIERlite, which achieves an
F score comparable to supervised training with
about 700 manually annotated documents.
5.4 Analysis
Table 6 shows the effect of the coreference prop-
agation step described in Section 4.1.3 as part of
training data creation. Without this step, the per-
formance of the bootstrapped system yields an F
score of 41. With the benefit of the additional
training instances produced by coreference prop-
agation, the system yields an F score of 53. The
new instances produced by coreference propaga-
tion seem to substantially enrich the diversity of
the set of labeled instances.
Seeding P/R/F
wo/Coref 45/38/41
w/Coref 47/53/50
Table 6: Effects of Coreference Propagation
In the evaluation section, we saw that the su-
pervised event extraction systems achieve higher
recall than the weakly supervised systems. Al-
though our bootstrapped event extraction sys-
tem TIERlite produces higher recall than previ-
ous weakly supervised systems, a substantial re-
call gap still exists.
Considering the pipeline structure of the event
extraction system, as shown in Figure 1, the noun
phrase extractors are responsible for identifying
all candidate role fillers. The sentential classifiers
and the document classifier effectively serve as
filters to rule out candidates from irrelevant con-
texts. Consequently, there is no way to recover
missing recall (role fillers) if the noun phrase ex-
tractors fail to identify them.
Since the noun phrase classifiers are so central
to the performance of the system, we compared
the performance of the bootstrapped noun phrase
classifiers directly with their supervised conter-
parts. The results are shown in Table 5. Both sets
of classifiers produce low precision when used in
isolation, but their precision levels are compara-
293
PerpInd PerpOrg Target Victim Weapon Average
Supervised Classifier 25/67/36 26/78/39 34/83/49 32/72/45 30/75/43 30/75/42
Bootstrapped Classifier 30/54/39 37/53/44 30/71/42 28/63/39 36/57/44 32/60/42
Table 5: Evaluation of Bootstrapped Noun Phrase Classifiers (Precision/Recall/F-score)
ble. The TIER pipeline architecture is successful
at eliminating many of the false hits. However,
the recall of the bootstrapped classifiers is consis-
tently lower than the recall of the supervised clas-
sifiers. Specifically, the recall is about 10 points
lower for three event roles (PerpInd, Target and
Victim) and 20 points lower for the other two event
roles (PerpOrg and Weapon). These results sug-
gest that our bootstrapping approach to training
instance creation does not fully capture the diver-
sity of role filler contexts that are available in the
supervised training set of 1,300 documents. This
issue is an interesting direction for future work.
6 Conclusions
We have presented a bootstrapping approach for
training a multi-layered event extraction model
using a small set of ?seed nouns? for each event
role, a collection of relevant (in-domain) and ir-
relevant (out-of-domain) texts and a semantic dic-
tionary. The experimental results show that the
bootstrapped system, TIERlite, outperforms pre-
vious weakly supervised event extraction sys-
tems on a standard event extraction data set, and
achieves performance levels comparable to super-
vised training with 700 manually annotated docu-
ments. The minimal supervision required to train
such a model increases the portability of event ex-
traction systems.
7 Acknowledgments
We gratefully acknowledge the support of the
National Science Foundation under grant IIS-
1018314 and the Defense Advanced Research
Projects Agency (DARPA) Machine Reading
Program under Air Force Research Laboratory
(AFRL) prime contract no. FA8750-09-C-0172.
Any opinions, findings, and conclusions or rec-
ommendations expressed in this material are those
of the authors and do not necessarily reflect the
view of the DARPA, AFRL, or the U.S. govern-
ment.
References
M.E. Califf and R. Mooney. 2003. Bottom-up Re-
lational Learning of Pattern Matching rules for In-
formation Extraction. Journal of Machine Learning
Research, 4:177?210.
Nathanael Chambers and Dan Jurafsky. 2011.
Template-Based Information Extraction without the
Templates. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies (ACL-11).
H.L. Chieu and H.T. Ng. 2002. A Maximum Entropy
Approach to Information Extraction from Semi-
Structured and Free Text. In Proceedings of the
18th National Conference on Artificial Intelligence.
F. Ciravegna. 2001. Adaptive Information Extraction
from Text by Rule Induction and Generalisation. In
Proceedings of the 17th International Joint Confer-
ence on Artificial Intelligence.
J. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating Non-local Information into Information
Extraction Systems by Gibbs Sampling. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 363?370,
Ann Arbor, MI, June.
A. Finn and N. Kushmerick. 2004. Multi-level
Boundary Classification for Information Extraction.
In In Proceedings of the 15th European Conference
on Machine Learning, pages 111?122, Pisa, Italy,
September.
Dayne Freitag. 1998a. Multistrategy Learning for
Information Extraction. In Proceedings of the Fif-
teenth International Conference on Machine Learn-
ing. Morgan Kaufmann Publishers.
Dayne Freitag. 1998b. Toward General-Purpose
Learning for Information Extraction. In Proceed-
ings of the 36th Annual Meeting of the Association
for Computational Linguistics.
Z. Gu and N. Cercone. 2006. Segment-Based Hidden
Markov Models for Information Extraction. In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 481?488, Sydney, Australia, July.
Ruihong Huang and Ellen Riloff. 2010. Inducing
Domain-specific Semantic Class Taggers from (Al-
most) Nothing. In Proceedings of The 48th Annual
Meeting of the Association for Computational Lin-
guistics (ACL 2010).
Ruihong Huang and Ellen Riloff. 2011. Peeling Back
the Layers: Detecting Event Role Fillers in Sec-
ondary Contexts. In Proceedings of the 49th Annual
294
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies (ACL-11).
S. Huffman. 1996. Learning Information Extraction
Patterns from Examples. In Stefan Wermter, Ellen
Riloff, and Gabriele Scheler, editors, Connectionist,
Statistical, and Symbolic Approaches to Learning
for Natural Language Processing, pages 246?260.
Springer-Verlag, Berlin.
H. Ji and R. Grishman. 2008. Refining Event Extrac-
tion through Cross-Document Inference. In Pro-
ceedings of ACL-08: HLT, pages 254?262, Colum-
bus, OH, June.
S. Keerthi and D. DeCoste. 2005. A Modified Finite
Newton Method for Fast Solution of Large Scale
Linear SVMs. Journal of Machine Learning Re-
search.
J. Kim and D. Moldovan. 1993. Acquisition of
Semantic Patterns for Information Extraction from
Corpora. In Proceedings of the Ninth IEEE Con-
ference on Artificial Intelligence for Applications,
pages 171?176, Los Alamitos, CA. IEEE Computer
Society Press.
Y. Li, K. Bontcheva, and H. Cunningham. 2005. Us-
ing Uneven Margins SVM and Perceptron for Infor-
mation Extraction. In Proceedings of Ninth Confer-
ence on Computational Natural Language Learn-
ing, pages 72?79, Ann Arbor, MI, June.
Shasha Liao and Ralph Grishman. 2010. Using Docu-
ment Level Cross-Event Inference to Improve Event
Extraction. In Proceedings of the 48st Annual
Meeting on Association for Computational Linguis-
tics (ACL-10).
M. Maslennikov and T. Chua. 2007. A Multi-
Resolution Framework for Information Extraction
from Free Text. In Proceedings of the 45th Annual
Meeting of the Association for Computational Lin-
guistics.
G. Miller. 1990. Wordnet: An On-line Lexical
Database. International Journal of Lexicography,
3(4).
MUC-4 Proceedings. 1992. Proceedings of the
Fourth Message Understanding Conference (MUC-
4). Morgan Kaufmann.
S. Patwardhan and E. Riloff. 2007. Effective Informa-
tion Extraction with Semantic Affinity Patterns and
Relevant Regions. In Proceedings of 2007 the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-2007).
S. Patwardhan and E. Riloff. 2009. A Unified Model
of Phrasal and Sentential Evidence for Information
Extraction. In Proceedings of 2009 the Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP-2009).
S. Patwardhan. 2010. Widening the Field of View
of Information Extraction through Sentential Event
Recognition. Ph.D. thesis, University of Utah.
W. Phillips and E. Riloff. 2007. Exploiting Role-
Identifying Nouns and Expressions for Information
Extraction. In Proceedings of the 2007 Interna-
tional Conference on Recent Advances in Natural
Language Processing (RANLP-07), pages 468?473.
E. Riloff and R. Jones. 1999. Learning Dictionar-
ies for Information Extraction by Multi-Level Boot-
strapping. In Proceedings of the Sixteenth National
Conference on Artificial Intelligence.
E. Riloff and W. Phillips. 2004. An Introduction to the
Sundance and AutoSlog Systems. Technical Report
UUCS-04-015, School of Computing, University of
Utah.
E. Riloff. 1993. Automatically Constructing a Dictio-
nary for Information Extraction Tasks. In Proceed-
ings of the 11th National Conference on Artificial
Intelligence.
E. Riloff. 1996. Automatically Generating Extraction
Patterns from Untagged Text. In Proceedings of the
Thirteenth National Conference on Artificial Intel-
ligence, pages 1044?1049.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In Proceedings of Joint Conference of the In-
ternational Committee on Computational Linguis-
tics and the Association for Computational Linguis-
tics (COLING/ACL-06.
Y. Shinyama and S. Sekine. 2006. Preemptive In-
formation Extraction using Unrestricted Relation
Discovery. In Proceedings of the Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 304?311, New York City, NY,
June.
S. Soderland, D. Fisher, J. Aseltine, and W. Lehnert.
1995. CRYSTAL: Inducing a conceptual dictio-
nary. In Proc. of the Fourteenth International Joint
Conference on Artificial Intelligence, pages 1314?
1319.
M. Stevenson and M. Greenwood. 2005. A Seman-
tic Approach to IE Pattern Induction. In Proceed-
ings of the 43rd Annual Meeting of the Association
for Computational Linguistics, pages 379?386, Ann
Arbor, MI, June.
K. Sudo, S. Sekine, and R. Grishman. 2003. An Im-
proved Extraction Pattern Representation Model for
Automatic IE Pattern Acquisition. In Proceedings
of the 41st Annual Meeting of the Association for
Computational Linguistics (ACL-03).
R. Yangarber, R. Grishman, P. Tapanainen, and S. Hut-
tunen. 2000. Automatic Acquisition of Domain
Knowledge for Information Extraction. In Proceed-
ings of the Eighteenth International Conference on
Computational Linguistics (COLING 2000).
K. Yu, G. Guan, and M. Zhou. 2005. Resume? In-
formation Extraction with Cascaded Hybrid Model.
In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics, pages
499?506, Ann Arbor, MI, June.
295
Proceedings of NAACL-HLT 2013, pages 41?51,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Multi-faceted Event Recognition with Bootstrapped Dictionaries
Ruihong Huang and Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT 84112
{huangrh, riloff}@cs.utah.edu
Abstract
Identifying documents that describe a specific
type of event is challenging due to the high
complexity and variety of event descriptions.
We propose a multi-faceted event recognition
approach, which identifies documents about
an event using event phrases as well as defin-
ing characteristics of the event. Our research
focuses on civil unrest events and learns civil
unrest expressions as well as phrases cor-
responding to potential agents and reasons
for civil unrest. We present a bootstrapping
algorithm that automatically acquires event
phrases, agent terms, and purpose (reason)
phrases from unannotated texts. We use the
bootstrapped dictionaries to identify civil un-
rest documents and show that multi-faceted
event recognition can yield high accuracy.
1 Introduction
Many people are interested in following news re-
ports about events. Government agencies are keenly
interested in news about civil unrest, acts of terror-
ism, and disease outbreaks. Companies want to stay
on top of news about corporate acquisitions, high-
level management changes, and new joint ventures.
The general public is interested in articles about
crime, natural disasters, and plane crashes. We will
refer to the task of identifying documents that de-
scribe a specific type of event as event recognition.
It is tempting to assume that event keywords
are sufficient to identify documents that discuss in-
stances of an event. But event words are rarely reli-
able on their own. For example, consider the chal-
lenge of finding documents about civil unrest. The
words ?strike?, ?rally?, and ?riot? refer to com-
mon types of civil unrest, but they frequently refer to
other things as well. A strike can refer to a military
event or a sporting event (e.g., ?air strike?, ?bowl-
ing strike?), a rally can be a race or a spirited ex-
change (e.g.,?car rally?, ?tennis rally?), and a riot
can refer to something funny (e.g., ?she?s a riot?).
Event keywords also appear in general discussions
that do not mention a specific event (e.g., ?37 states
prohibit teacher strikes? or ?The fine for inciting a
riot is $1,000?). Furthermore, many relevant docu-
ments are not easy to recognize because events can
be described with complex expressions that do not
include event keywords. For example, ?took to the
streets?, ?walked off their jobs? and ?stormed par-
liament? often describe civil unrest.
The goal of our research is to recognize event de-
scriptions in text by identifying event expressions as
well as defining characteristics of the event. We pro-
pose that agents and purpose are characteristics of
an event that are essential to distinguish one type of
event from another. The agent responsible for an ac-
tion often determines how we categorize the action.
For example, natural disasters, military operations,
and terrorist attacks can all produce human casual-
ties and physical destruction. But the agent of a nat-
ural disaster must be a natural force, the agent of
a military incident must be military personnel, and
the agent of a terrorist attack is never a natural force
and rarely military personnel. There may be other
important factors as well, but the agent is often an
essential part of an event definition.
The purpose of an event is also a crucial factor
in distinguishing between event types. For exam-
41
ple, civil unrest events and sporting events both in-
volve large groups of people amassing at a specific
site. But the purpose of civil unrest gatherings is to
protest against socio-political problems, while sport-
ing events are intended as entertainment. As another
example, terrorist events and military incidents can
both cause casualties, but the purpose of terrorism is
to cause widespread fear, while the purpose of mili-
tary actions is to protect national security interests.
Our research explores the idea of multi-faceted
event recognition: using event expressions as well
as facets of the event (agents and purpose) to iden-
tify documents about a specific type of event. We
present a bootstrapping framework to automatically
create event phrase, agent, and purpose dictionaries.
The learning process uses unannotated texts, a few
event keywords, and seed terms for common agents
and purpose phrases associated with the event type.
Our bootstrapping algorithm exploits the obser-
vation that event expressions, agents, and purpose
phrases often appear together in sentences that in-
troduce an event. In the first step, we extract event
expressions based on dependency relations with an
agent and purpose phrase. The harvested event ex-
pressions are added to an event phrase dictionary. In
the second step, new agent terms are extracted from
sentences containing an event phrase and a purpose
phrase, and new purpose phrases are harvested from
sentences containing an event phrase and an agent.
These harvested terms are added to agent and pur-
pose dictionaries. The bootstrapping algorithm rico-
chets back and forth, alternately learning new event
phrases and learning new agent/purpose phrases, in
an iterative process.
We explore several ways of using these boot-
strapped dictionaries. We conclude that finding at
least two different types of event information pro-
duces high accuracy (88% precision) with good re-
call (71%) on documents that contain an event key-
word. We also present experiments with documents
that do not contain event keywords, and obtain 74%
accuracy when matching all three types of event in-
formation.
2 Related Work
Event recognition has been studied in several dif-
ferent contexts. There has been a lot of research
on event extraction, where the goal is to extract
facts about events from text (e.g., (ACE Evaluations,
2006; Appelt et al, 1993; Riloff, 1996; Yangar-
ber et al, 2000; Chieu and Ng, 2002; Califf and
Mooney, 2003; Sudo et al, 2003; Stevenson and
Greenwood, 2005; Sekine, 2006)). Although our re-
search does not involve extracting facts, event ex-
traction systems can also be used to identify sto-
ries about a specific type of event. For example, the
MUC-4 evaluation (MUC-4 Proceedings, 1992) in-
cluded ?text filtering? results that measured the per-
formance of event extraction systems at identifying
event-relevant documents. The best text filtering re-
sults were high (about 90% F score), but relied on
hand-built event extraction systems. More recently,
some research has incorporated event region detec-
tors into event extraction systems to improve extrac-
tion performance (Gu and Cercone, 2006; Patward-
han and Riloff, 2007; Huang and Riloff, 2011).
There has been recent work on event detection
from social media sources (Becker et al, 2011;
Popescu et al, 2011). Some research identifies spe-
cific types of events in tweets, such as earthquakes
(Sakaki et al, 2010) and entertainment events (Ben-
son et al, 2011). There has also been work on event
trend detection (Lampos et al, 2010; Mathioudakis
and Koudas, 2010) and event prediction through so-
cial media, such as predicting elections (Tumasjan
et al, 2010; Conover et al, 2011) or stock mar-
ket indicators (Zhang et al, 2010). (Ritter et al,
2012) generated a calendar of events mentioned on
twitter. (Metzler et al, 2012) proposed structured
retrieval of historical event information over mi-
croblog archives by distilling high quality event rep-
resentations using a novel temporal query expansion
technique.
Some text classification research has focused on
event categories. (Riloff and Lehnert, 1994) used
an information extraction system to generate rele-
vancy signatures that were indicative of different
event types. This work originally relied on man-
ually labeled patterns and a hand-crafted semantic
dictionary. Later work (Riloff and Lorenzen, 1999)
eliminated the need for the dictionary and labeled
patterns, but still assumed the availability of rele-
vant/irrelevant training texts.
Event recognition is also related to Topic Detec-
tion and Tracking (TDT) (Allan et al, 1998; Allan,
42
Figure 1: Bootstrapped Learning of Event Dictionaries
2002) which addresses event-based organization of a
stream of news stories. Event recognition is similar
to New Event Detection, also called First Story De-
tection, which is considered the most difficult TDT
task (Allan et al, 2000a). Typical approaches re-
duce documents to a set of features, either as a word
vector (Allan et al, 2000b) or a probability distri-
bution (Jin et al, 1999), and compare the incoming
stories to stories that appeared in the past by com-
puting similarities between their feature representa-
tions. Recently, event paraphrases (Petrovic et al,
2012) have been explored to deal with the diversity
of event descriptions. However, the New Event De-
tection task differs from our event recognition task
because we want to find all stories describing a cer-
tain type of event, not just new events.
3 Bootstrapped Learning of Event
Dictionaries
Our bootstrapping approach consists of two stages
of learning as shown in Figure 1. The process be-
gins with a few agent seeds, purpose phrase patterns,
and unannotated articles selected from a broad-
coverage corpus using event keywords. In the first
stage, event expressions are harvested from the sen-
tences that have both an agent and a purpose phrase
in specific syntactic positions. In the second stage,
new purpose phrases are harvested from sentences
that contain both an event phrase and an agent, while
new agent terms are harvested from sentences that
contain both an event phrase and a purpose phrase.
The new terms are added to growing event dictionar-
ies, and the bootstrapping process repeats. Our work
focuses on civil unrest events.
3.1 Stage 1: Event Phrase Learning
We first extract potential civil unrest stories from the
English Gigaword corpus (Parker et al, 2011) using
six civil unrest keywords. As explained in Section 1,
event keywords are not sufficient to obtain relevant
documents with high precision, so the extracted sto-
ries are a mix of relevant and irrelevant articles. Our
algorithm first selects sentences to use for learning,
and then harvests event expressions from them.
3.1.1 Event Sentence Identification
The input in stage 1 consists of a few agent terms
and purpose patterns for seeding. The agent seeds
are single nouns, while the purpose patterns are
verbs in infinitive or present participle forms. Table
1 shows the agent terms and purpose phrases used in
our experiments. The agent terms were manually se-
lected by inspecting the most frequent nouns in the
documents with civil unrest keywords. The purpose
patterns are the most common verbs that describe the
reason for a civil unrest event. We identify probable
event sentences by extracting all sentences that con-
tain at least one agent term and one purpose phrase.
Agents protesters, activists, demonstrators,
students, groups, crowd, workers,
palestinians, supporters, women
Purpose demanding, to demand,
Phrases protesting, to protest
Table 1: Agent and Purpose Phrases Used for Seeding
43
3.1.2 Harvesting Event Expressions
To constrain the learning process, we require
event expressions and purpose phrases to match cer-
tain syntactic structures. We apply the Stanford de-
pendency parser (Marneffe et al, 2006) to the prob-
able event sentences to identify verb phrase candi-
dates and to enforce syntactic constraints between
the different types of event information.
Figure 2: Phrasal Structure of Event & Purpose Phrases
Figure 2 shows the two types of verb phrases
that we learn. One type consists of a verb paired
with the head noun of its direct object. For exam-
ple, event phrases can be ?stopped work? or ?oc-
cupied offices?, and purpose phrases can be ?show
support? or ?condemn war?. The second type con-
sists of a verb and an attached prepositional phrase,
retaining only the head noun of the embedded noun
phrase. For example, ?took to street? and ?scuffled
with police? can be event phrases, while ?call for
resignation? and ?press for wages? can be purpose
phrases. In both types of verb phrases, a particle can
optionally follow the verb.
Event expressions, agents, and purpose phrases
must appear in specific dependency relations, as il-
lustrated in Figure 3. An agent must be the syn-
tactic subject of the event phrase. A purpose phrase
must be a complement of the event phrase, specif-
ically, we require a particular dependency relation,
?xcomp?1, between the two verb phrases. For ex-
ample, in the sentence ?Leftist activists took to
the streets in the Nepali capital Wednesday protest-
ing higher fuel prices.?, the dependency relation
1In the dependency parser, ?xcomp? denotes a general rela-
tion between a VP or an ADJP and its open clausal complement.
For example, in the sentence ?He says that you like to swim.?,
the ?xcomp? relation will link ?like? (head) and ?swim? (de-
pendent). With our constraints on the verb phrase forms, the
dependent verb phrase in this construction tends to describe the
purpose of the verb phrase.
?xcomp? links ?took to the streets? with ?protest-
ing higher fuel prices?.
Figure 3: Syntactic Dependencies between Agents, Event
Phrases, and Purpose Phrases
Given the syntactic construction shown in Figure
3, with a known agent and purpose phrase, we ex-
tract the head verb phrase of the ?xcomp? depen-
dency relation as an event phrase candidate. The
event phrases that co-occur with at least two unique
agent terms and two unique purposes phrases are
saved in our event phrase dictionary.
3.2 Stage 2: Learning Agent and Purpose
Phrases
In the second stage of bootstrapping, we learn new
agent terms and purpose phrases. Our rationale is
that if a sentence contains an event phrase and one
other important facet of the event (agent or pur-
pose), then the sentence probably describes a rele-
vant event. We can then look for additional facets
of the event in the same sentence. We learn both
agent and purpose phrases simultaneously in paral-
lel learning processes. As before, we first identify
probable event sentences and then harvest agent and
purpose phrases from these sentences.
3.2.1 Event Sentence Identification
We identify probable event sentences by extract-
ing sentences that contain at least one event phrase
(based on the dictionary produced in the first stage
of bootstrapping) and an agent term or a purpose
phrase. As before, the event information must oc-
cur in the sentential dependency structures shown in
Figure 3.
3.2.2 Harvesting Agent and Purpose Phrases
The sentences that contain an event phrase and
an agent are used to harvest more purpose phrases,
while the sentences that contain an event phrase
and a purpose phrase are used to harvest more
agent terms. Purpose phrases are extracted from the
phrasal structures shown in Figure 2. In the learn-
ing process for agents, if a sentence has an event
44
phrase as the head of the ?xcomp? dependency re-
lation and a purpose phrase as the dependent clause
of the ?xcomp? dependency relation, then the head
noun of the syntactic subject of the event phrase is
harvested as a candidate agent term. We also record
the modifiers appearing in all of the noun phrases
headed by an agent term. Agent candidates that co-
occur with at least two unique event phrases and at
least two different modifiers of known agent terms
are selected as new agent terms.
The learning process for purpose phrases is anal-
ogous. If the syntactic subject of an event phrase
is an agent and the event phrase is the head of
the ?xcomp? dependency relation, then the depen-
dent clause of the ?xcomp? dependency relation is
harvested as a candidate purpose phrase. Purpose
phrase candidates that co-occur with at least two dif-
ferent event phrases are selected as purpose phrases.
The bootstrapping process then repeats, ricochet-
ing back and forth between learning event phrases
and learning agent and purpose phrases.
3.3 Domain Relevance Criteria
To avoid domain drift during bootstrapping, we use
two additional criteria to discard phrases that are not
necessarily associated with the domain.
For each event phrase and purpose phrase, we es-
timate its domain-specificity as the ratio of its preva-
lence in domain-specific texts compared to broad-
coverage texts. The goal is to discard phrases that
are common across many types of documents, and
therefore not specific to the domain. We define the
domain-specificity of phrase p as:
domain-specificity(p) = frequency of p in domain-specific corpusfrequency of p in broad-coverage corpus
We randomly sampled 10% of the Gigaword texts
that contain a civil unrest event keyword to create
the ?domain-specific? corpus, and randomly sam-
pled 10% of the remaining Gigaword texts to cre-
ate the ?broad-coverage? corpus.2 Keyword-based
sampling is an approximation to domain-relevance,
but gives us a general idea about the prevalance of a
phrase in different types of texts.
For agent terms, our goal is to identify people who
participate as agents of civil unrest events. Other
types of people may be commonly mentioned in
civil unrest stories too, as peripheral characters. For
2The random sampling was simply for efficiency reasons.
example, police may provide security and reporters
may provide media coverage of an event, but they
are not the agents of the event. We estimate the
event-specificity of each agent term as the ratio of
the phrase?s prevalence in event sentences compared
to all the sentences in the domain-specific corpus.
We define an event sentence as one that contains
both a learned event phrase and a purpose phrase,
based on the dictionaries at that point in time. There-
fore, the number of event sentences increases as the
bootstrapped dictionaries grow. We define the event-
specificity of phrase p as:
event-specificity(p) = frequency of p in event sentencesfrequency of p in all sentences
In our experiments we required event and purpose
phrases to have domain-specificity ? .33 and agent
terms to have event-specificity ? .01.3
4 Evaluation
4.1 Data
We conducted experiments to evaluate the perfor-
mance of our bootstrapped event dictionaries for rec-
ognizing civil unrest events. Civil unrest is a broad
term typically used by the media or law enforce-
ment to describe a form of public disturbance that
involves a group of people, usually to protest or pro-
mote a cause. Civil unrest events include strikes,
protests, occupations, rallies, and similar forms of
obstructions or riots. We chose six event keywords to
identify potential civil unrest documents: ?protest?,
?strike?, ?march?, ?rally?, ?riot? and ?occupy?. We
extracted documents from the English Gigaword
corpus (Parker et al, 2011) that contain at least one
of these event keywords, or a morphological variant
of a keyword.4 This process extracted nearly one
million documents, which we will refer to as our
event-keyword corpus.
We randomly sampled 400 documents5 from the
event-keyword corpus and asked two annotators to
determine whether each document mentioned a civil
3This value is so small because we simply want to filter
phrases that virtually never occur in the event sentences, and
we can recognize very few event sentences in the early stages
of bootstrapping.
4We used ?marched? and ?marching? as keywords but did
not use ?march? because it often refers to a month.
5These 400 documents were excluded from the unannotated
data used for dictionary learning.
45
unrest event. We defined annotation guidelines and
conducted an inter-annotator agreement study on
100 of these documents. The annotators achieved a
? score of .82. We used these 100 documents as our
tuning set. Then each annotator annotated 150 more
documents to create our test set of 300 documents.
4.2 Baselines
The first row of Table 2 shows event recognition ac-
curacy when only the event keywords are used. All
of our documents were obtained by searching for a
keyword, but only 101 of the 300 documents in our
test set were labeled as relevant by the annotators
(i.e., 101 describe a civil unrest event). This means
that using only the event keywords to identify civil
unrest documents yields about 34% precision. In a
second experiment, KeywordTitle, we required the
event keyword to be in the title (headline) of the doc-
ument. The KeywordTitle approach produced better
precision (66%), but only 33% of the relevant docu-
ments had a keyword in the title.
Method Recall Precision F
Keyword Accuracy
Keyword - 34 -
KeywordTitle 33 66 44
Supervised Learning
Unigrams 62 66 64
Unigrams+Bigrams 55 71 62
Bootstrapped Dictionary Lookup
Event Phrases (EV) 60 79 69
Agent Phrases (AG) 98 42 59
Purpose Phrases (PU) 59 67 63
All Pairs 71 88 79
Table 2: Experimental Results
The second section of Table 2 shows the re-
sults of two supervised classifiers based on 10-fold
cross validation with our test set. Both classifiers
were trained using support vector machines (SVMs)
(Joachims, 1999) with a linear kernel (Keerthi and
DeCoste, 2005). The first classifier used unigrams
as features, while the second classifier used both un-
igrams and bigrams. All the features are binary. The
evaluation results show that the unigram classifier
has an F-score of .64. Using both unigram and bi-
gram features increased precision to 71% but recall
fell by 7%, yielding a slightly lower F-score of .62.
4.3 Event Recognition with Bootstrapped
Dictionaries
Next, we used our bootstrapped dictionaries for
event recognition. The bootstrapping process ran
for 8 iterations and then stopped because no more
phrases could be learned. The quality of boot-
strapped data often degrades as bootstrapping pro-
gresses, so we used the tuning set to evaluate the
dictionaries after each iteration. The best perfor-
mance6 on the tuning set resulted from the dictionar-
ies produced after four iterations, so we used these
dictionaries for our experiments. Table 3 shows the
Event Agent Purpose
Phrases Terms Phrases
Iter #1 145 67 124
Iter #2 410 106 356
Iter #3 504 130 402
Iter #4 623 139 569
Table 3: Dictionary Sizes after Several Iterations
number of event phrases, agents and purpose phrases
learned after each iteration. All three lexicons were
significantly enriched after each iteration. The final
bootstrapped dictionaries contain 623 event phrases,
569 purpose phrases and 139 agent terms. Table 4
shows samples from each event dictionary.
Event Phrases: went on strike, took to street,
chanted slogans, gathered in capital, formed chain,
clashed with police, staged rally, held protest,
walked off job, burned flags, set fire, hit streets,
marched in city, blocked roads, carried placards
Agent Terms: employees, miners, muslims, unions,
protestors, journalists, refugees, prisoners, immigrants,
inmates, pilots, farmers, followers, teachers, drivers
Purpose Phrases: accusing government, voice anger,
press for wages, oppose plans, urging end, defying ban,
show solidarity, mark anniversary, calling for right,
condemning act, pressure government, mark death,
push for hike, call attention, celebrating withdrawal
Table 4: Examples of Dictionary Entries
The third section of Table 2 shows the results
when using the bootstrapped dictionaries for event
recognition. We used a simple dictionary look-up
approach that searched for dictionary entries in each
document. Our phrases were generated based on
6Based on the performance for the All Pairs approach.
46
syntactic analysis and only head words were re-
tained for generality. But we wanted to match dic-
tionary entries without requiring syntactic analysis
of new documents. So we used an approximate
matching scheme that required each word to appear
within 5 words of the previous word. For example,
?held protest? would match ?held a large protest?
and ?held a very large political protest?. In this way,
we avoid the need for syntactic analysis when using
the dictionaries for event recognition.
First, we labeled a document as relevant if it con-
tained any Event Phrase (EV) in our dictionary. The
event phrases achieved better performance than all
of the baselines, yielding an F-score of 69%. The
best baseline was the unigram classifier, which was
trained with supervised learning. The bootstrapped
event phrase dictionary produced much higher pre-
cision (79% vs. 66%) with only slightly lower recall
(60% vs. 62%), and did not require annotated texts
for training. Statistical significance testing shows
that the Event Phrase lookup approach works signif-
icantly better than the unigram classifier (p < 0.05,
paired bootstrap (Berg-Kirkpatrick et al, 2012)).
For the sake of completeness, we also evaluated
the performance of dictionary look-up using our
bootstrapped Agent (AG) and Purpose (PU) dictio-
naries, individually. The agents terms produced 42%
precision with 98% recall, demonstrating that the
learned agent list has extremely high coverage but
(unsurprisingly) does not achieve high precision on
its own. The purpose phrases achieved a better bal-
ance of recall and precision, producing an F-score
of 63%, which is nearly the same as the supervised
unigram classifier.
Our original hypothesis was that a single type of
event information is not sufficient to accurately iden-
tify event descriptions. Our goal was high-accuracy
event recognition by requiring that a document con-
tain multiple clues pertaining to different facets of an
event (multi-faceted event recognition). The last row
of Table 2 (All Pairs) shows the results when requir-
ing matches from at least two different bootstrapped
dictionaries. Specifically, we labeled a document
as relevant if it contained at least one phrase from
each of two different dictionaries and these phrases
occurred in the same sentence. Table 2 shows that
multi-faceted event recognition achieves 88% preci-
sion with reasonably good recall of 71%, yielding an
F-score of 79%. This multi-faceted approach with
simple dictionary look-up outperformed all of the
baselines, and each dictionary used by itself. Sta-
tistical significance testing shows that the All Pairs
approach works significantly better than the unigram
classifier (p < 0.001, paired bootstrap). The All
Pairs approach is significantly better than the Event
Phrase (EV) lookup approach at the p < 0.1 level.
Method Recall Precision F-score
EV + PU 14 100 24
EV + AG 47 94 62
AG + PU 50 85 63
All Pairs 71 88 79
Table 5: Analysis of Dictionary Combinations
Table 5 takes a closer look at how each pair of
dictionaries performed. The first row shows that re-
quiring a document to have an event phrase and a
purpose phrase produces the best precision (100%)
but with low recall (14%). The second row reveals
that requiring a document to have an event phrase
and an agent term yields better recall (47%) and high
precision (94%). The third row shows that requiring
a document to have a purpose phrase and an agent
term produces the best recall (50%) but with slightly
lower precision (85%). Finally, the last row of Ta-
ble 5 shows that taking the union of these results
(i.e., any combination of dictionary pairs is suffi-
cient) yields the best recall (71%) with high preci-
sion (88%), demonstrating that we get the best cov-
erage by recognizing multiple combinations of event
information.
Lexicon Recall Precision F-score
Seeds 13 87 22
Iter #1 50 88 63
Iter #2 63 89 74
Iter #3 68 88 77
Iter #4 71 88 79
Table 6: All Pairs Lookup Results using only Seeds and
the Lexicons Learned after each Iteration, on the Test Set
Table 6 shows the performance of the lexicon
lookup approach using the All Pairs criteria dur-
ing the bootstrapping process. The first row shows
the results using only 10 agent seeds and 4 purpose
seeds as shown in Table 1. The following four rows
in the table show the performance of All Pairs using
47
the lexicons learned after each bootstrapping itera-
tion. We can see that the recall increases steadily and
that precision is maintained at a high level through-
out the bootstrapping process.
Event recognition can be formulated as an infor-
mation retrieval (IR) problem. As another point of
comparison, we ran an existing IR system, Terrier
(Ounis et al, 2007), on our test set. We used Ter-
rier to rank these 300 documents given our set of
event keywords as the query 7, and then generated a
recall/precision curve (Figure 4) by computing the
precisions at different levels of recall, ranging from
0 to 1 in increments of .10. Terrier was run with the
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Pr
ec
is
io
n
Figure 4: Comparison with the Terrier IR system
parameter PL2 which refers to an advanced Diver-
gence From Randomness weighting model (Amati
and Van Rijsbergen, 2002). In addition, Terrier used
automatic query expansion. We can see that Terrier
identified the first 60 documents (20% recall) with
100% precision. But precision dropped sharply after
that. The circle in Figure 4 shows the performance
of our bootstrapped dictionaries using the All Pairs
approach. At comparable level of precision (88%),
Terrier achieved about 45% recall versus 71% recall
produced with the bootstrapped dictionaries.
4.4 Supervised Classifiers with Bootstrapped
Dictionaries
We also explored the idea of using the bootstrapped
dictionaries as features for a classifier to see if a su-
pervised learner could make better use of the dic-
7We gave Terrier one query with all of the event keywords.
tionaries. We created five SVM classifiers and per-
formed 10-fold cross validation on the test set.
Method Recall Precision F-score
TermLex 66 85 74
PairLex 10 91 18
TermSets 59 83 69
PairSets 68 84 75
AllSets 70 84 76
Table 7: Supervised classifiers using the dictionaries
Table 7 shows the results for the five classifiers.
TermLex encodes a binary feature for every phrase
in any of our dictionaries. PairLex encodes a binary
feature for each pair of phrases from two different
dictionaries and requires them to occur in the same
sentence. The TermLex classifier achieves good per-
formance (74% F-score), but is not as effective as
our All Pairs dictionary look-up approach (79% F-
score). The PairLex classifier yield higher precision
but very low recall, undoubtedly due to sparsity is-
sues in matching specific pairs of phrases.
One of the strengths of our bootstrapping method
is that it creates dictionaries from large volumes of
unannotated documents. A limitation of supervised
learning with lexical features is that the classifier can
not benefit from terms in the bootstrapped dictionar-
ies that do not appear in its training documents. To
address this issue, we also tried encoding the dic-
tionaries as set-based features. The TermSets clas-
sifier encodes three binary features, one for each
dictionary. A feature gets a value of 1 if a docu-
ment contains any word in the corresponding dictio-
nary. The PairSets classifier also encodes three bi-
nary features, but each feature represents a different
pair of dictionaries (EV+AG, EV+PU, or AG+PU).
A feature gets a value of 1 if a document contains at
least one term from each of the two dictionaries in
the same sentence. The AllSets classifier encodes 7
set-based features: the previous six features and one
additional feature that requires a sentence to contain
at least one entry from all three dictionaries.
The All Sets classifier yields the best performance
with an F-score of 76%. However, our straightfor-
ward dictionary look-up approach still performs bet-
ter (79% F-score), and does not require annotated
documents for training.
48
4.5 Finding Articles with no Event Keyword
The learned event dictionaries have the potential to
recognize event-relevant documents that do not con-
tain any human-selected event keywords. This can
happen in two ways. First, 378 of the 623 learned
event phrases do not contain any of the original event
keywords. Second, we expect that some event de-
scriptions will contain a known agent and purpose
phrase, even if the event phrase is unfamiliar.
We performed an additional set of experiments
with documents in the Gigaword corpus that contain
no human-selected civil unrest keyword. Following
our multi-faceted approach to event recognition, we
collected all documents that contain a sentence that
matches phrases in at least two of our bootstrapped
event dictionaries. This process retrieved 178,197
documents. The first column of Table 8 shows the
number of documents that had phrases found in two
different dictionaries (EV+AG, EV+PU, AG+PU) or
in all three dictionaries (EV+AG+PU).
Total Samples Accuracy
EV+AG 67,796 50 44%
EV+PU 2,375 50 54%
AG+PU 101,173 50 18%
EV+AG+PU 6,853 50 74%
Table 8: Evaluation of articles with no event keyword
We randomly sampled 50 documents from each
category and had them annotated. The accura-
cies are shown in the third column. Finding all
three types of phrases produced the best accuracy,
74%. Furthermore, we found over 6,800 documents
that had all three types of event information us-
ing our learned dictionaries. This result demon-
strates that the bootstrapped dictionaries can recog-
nize many event descriptions that would have been
missed by searching only with manually selected
keywords. This experiment also confirms that multi-
facted event recognition using all three learned dic-
tionaries achieves good accuracy even for docu-
ments that do not contain the civil unrest keywords.
5 Conclusions
We proposed a multi-faceted approach to event
recognition and presented a bootstrapping technique
to learn event phrases as well as agent terms and
purpose phrases associated with civil unrest events.
Our results showed that multi-faceted event recog-
nition using the learned dictionaries achieved high
accuracy and performed better than several other
methods. The bootstrapping approach can be eas-
ily trained for new domains since it requires only
a large collection of unannotated texts and a few
event keywords, agent terms, and purpose phrases
for the events of interest. Furthermore, although the
training phase requires syntactic parsing to learn the
event dictionaries, the dictionaries can then be used
for event recognition without needing to parse the
documents.
An open question for future work is to investigate
whether the same multi-faceted approach to event
recognition will work well for other types of events.
Our belief is that many different types of events have
characteristic agent terms, but additional types of
facets will need to be defined to cover a broad array
of event types. The syntactic constructions used to
harvest dictionary items may also vary depending on
the types of event information that must be learned.
In future research, we plan to explore these issues in
more depth to design a more general multi-faceted
event recognition system, and we plan to investigate
new ways to use these event dictionaries for event
extraction as well.
6 Acknowledgments
This research was supported by the Intelligence Ad-
vanced Research Projects Activity (IARPA) via De-
partment of Interior National Business Center (DoI /
NBC) contract number D12PC00285 and by the Na-
tional Science Foundation under grant IIS-1018314.
The U.S. Government is authorized to reproduce
and distribute reprints for Governmental purposes
notwithstanding any copyright annotation thereon.
The views and conclusions contained herein are
those of the authors and should not be interpreted as
necessarily representing the official policies or en-
dorsements, either expressed or implied, of IARPA,
DoI/NBE, NSF, or the U.S. Government.
49
References
ACE Evaluations. 2006.
http://www.itl.nist.gov/iad/mig/tests/ace/.
J. Allan, J. Carbonell, G. Doddington, J. Yamron, and
Y. Yang. 1998. Topic Detection and Tracking Pilot
Study: Final Report. In Proceedings of DARPA Broad-
cast News Transcription and Understanding Work-
shop.
J. Allan, V. Lavrenko, and H. Jin. 2000a. First Story
Detection in TDT is Hard. In Proceedings of the 2000
ACM CIKM International Conference on Information
and Knowledge Management.
J. Allan, Victor Lavrenko, Daniella Malin, and Russell
Swan. 2000b. Detections, Bounds, and Timelines:
UMass and TDT-3. In Proceedings of Topic Detection
and Tracking Workshop.
J. Allan, 2002. Topic Detection and Tracking: Event
Based Information Organization. Kluwer Academic
Publishers.
G. Amati and C. J. Van Rijsbergen. 2002. Probabilistic
Models of Information Retrieval based on Measuring
Divergence from Randomness. ACM Transactions on
Information Systems, 20(4):357?389.
D. Appelt, J. Hobbs, J. Bear, D. Israel, and M. Tyson.
1993. FASTUS: a finite-state processor for informa-
tion extraction from real-world text. In Proceedings of
the Thirteenth International Joint Conference on Arti-
ficial Intelligence.
H. Becker, M. Naaman, and L. Gravano. 2011. Be-
yond trending topics: Real-world event identification
on twitter. In Proceedings of the Fifth International
AAAI Conference on Weblogs and Social Media.
E. Benson, A. Haghighi, and R. Barzilay. 2011. Event
discovery in social media feeds.
T. Berg-Kirkpatrick, D. Burkett, and D. Klein. 2012. An
Empirical Investigation of Statistical Significance in
NLP. In Proceedings of the 2012 Conference on Em-
pirical Methods in Natural Language Processing.
M.E. Califf and R. Mooney. 2003. Bottom-up Relational
Learning of Pattern Matching rules for Information
Extraction. Journal of Machine Learning Research,
4:177?210.
H.L. Chieu and H.T. Ng. 2002. A Maximum En-
tropy Approach to Information Extraction from Semi-
Structured and Free Text. In Proceedings of the 18th
National Conference on Artificial Intelligence.
M. D. Conover, J. Ratkiewicz, M. Francisco,
B. Goncalves, A. Flammini, and F. Menczer. 2011.
Political Polarization on Twitter. In Proceedings of
the Fifth International AAAI Conference on Weblogs
and Social Media.
Z. Gu and N. Cercone. 2006. Segment-Based Hidden
Markov Models for Information Extraction. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics, pages
481?488, Sydney, Australia, July.
R. Huang and E. Riloff. 2011. Peeling Back the Layers:
Detecting Event Role Fillers in Secondary Contexts.
H. Jin, R. Schwartz, S. Sista, and F. Walls. 1999. Topic
Tracking for Radio, TV broadcast, and Newswire. In
EUROSPEECH.
T. Joachims. 1999. Making Large-Scale Support
Vector Machine Learning Practical. In A. Smola
B. Scho?lkopf, C. Burges, editor, Advances in Kernel
Methods: Support Vector Machines. MIT Press, Cam-
bridge, MA.
S. Keerthi and D. DeCoste. 2005. A Modified Finite
Newton Method for Fast Solution of Large Scale Lin-
ear SVMs. Journal of Machine Learning Research.
V. Lampos, T. D. Bie, and N. Cristianini. 2010. Flu
Detector - Tracking Epidemics on Twitter. In ECML
PKDD.
M. d. Marneffe, B. MacCartney, and C. D. Manning.
2006. Generating Typed Dependency Parses from
Phrase Structure Parses. In Proceedings of the Fifth
Conference on Language Resources and Evaluation
(LREC-2006).
M. Mathioudakis and N. Koudas. 2010. TwitterMonitor:
trend detection over the twitter stream. In Proceedings
of the 2010 international conference on Management
of data, page 11551158. ACM.
D. Metzler, C. Cai, and E. Hovy. 2012. Structured Event
Retrieval over Microblog Archives. In Proceedings of
The 2012 Conference of the North American Chapter
of the Association for Computational Linguistics: Hu-
man Language Technologies.
MUC-4 Proceedings. 1992. Proceedings of the Fourth
Message Understanding Conference (MUC-4). Mor-
gan Kaufmann.
I. Ounis, C. Lioma, C. Macdonald, and V. Plachouras.
2007. Research Directions in Terrier. Novat-
ica/UPGRADE Special Issue on Web Information Ac-
cess, Ricardo Baeza-Yates et al (Eds), Invited Paper.
R. Parker, D. Graff, J. Kong, K. Chen, and Kazuaki M.
2011. English Gigaword. In Linguistic Data Consor-
tium.
S. Patwardhan and E. Riloff. 2007. Effective Information
Extraction with Semantic Affinity Patterns and Rele-
vant Regions. In Proceedings of 2007 the Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP-2007).
S. Petrovic, M. Osborne, and V. Lavrenko. 2012. Us-
ing Paraphrases for Improving First Story Detection in
50
News and Twitter. In Proceedings of The 2012 Con-
ference of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language
Technologies.
A.-M. Popescu, M. Pennacchiotti, and D. A. Paranjpe.
2011. Extracting events and event descriptions from
twitter.
E. Riloff and W. Lehnert. 1994. Information Ex-
traction as a Basis for High-Precision Text Classifi-
cation. ACM Transactions on Information Systems,
12(3):296?333, July.
E. Riloff and J. Lorenzen. 1999. Extraction-based text
categorization: Generating domain-specific role rela-
tionships automatically. In Tomek Strzalkowski, edi-
tor, Natural Language Information Retrieval. Kluwer
Academic Publishers.
E. Riloff. 1996. Automatically Generating Extraction
Patterns from Untagged Text. In Proceedings of the
Thirteenth National Conference on Artificial Intelli-
gence, pages 1044?1049. The AAAI Press/MIT Press.
A. Ritter, Mausam, O. Etzioni, and S. Clark. 2012. Open
domain event extraction from twitter. In The Proceed-
ings of The 18th ACM SIGKDD Conference on Knowl-
edge Discovery and Data Mining.
T. Sakaki, M. Okazaki, and Y. Matsuo. 2010. Earth-
quake shakes twitter users: real-time event detection
by social sensors.
S. Sekine. 2006. On-demand Information Extrac-
tion. In Proceedings of Joint Conference of the In-
ternational Committee on Computational Linguistics
and the Association for Computational Linguistics
(COLING/ACL-06).
M. Stevenson and M. Greenwood. 2005. A Seman-
tic Approach to IE Pattern Induction. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics, pages 379?386, Ann Ar-
bor, MI, June.
K. Sudo, S. Sekine, and R. Grishman. 2003. An Im-
proved Extraction Pattern Representation Model for
Automatic IE Pattern Acquisition. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics (ACL-03).
A. Tumasjan, T. O. Sprenger, P. G. Sandner, and I. M.
Welpe. 2010. Predicting Elections with Twitter: What
140 Characters Reveal about Political Sentiment. In
Proceedings of the 4th International AAAI Conference
on Weblogs and Social Media.
R. Yangarber, R. Grishman, P. Tapanainen, and S. Hut-
tunen. 2000. Automatic Acquisition of Domain
Knowledge for Information Extraction. In Proceed-
ings of the Eighteenth International Conference on
Computational Linguistics (COLING 2000).
X. Zhang, H. Fuehres, and P. A. Gloor. 2010. Predicting
Stock Market Indicators Through Twitter ?I hope it is
not as bad as I fear?. In COINs.
51
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 275?285,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Inducing Domain-specific Semantic Class Taggers from (Almost) Nothing
Ruihong Huang and Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT 84112
{huangrh,riloff}@cs.utah.edu
Abstract
This research explores the idea of inducing
domain-specific semantic class taggers us-
ing only a domain-specific text collection
and seed words. The learning process be-
gins by inducing a classifier that only has
access to contextual features, forcing it to
generalize beyond the seeds. The contex-
tual classifier then labels new instances,
to expand and diversify the training set.
Next, a cross-category bootstrapping pro-
cess simultaneously trains a suite of clas-
sifiers for multiple semantic classes. The
positive instances for one class are used as
negative instances for the others in an it-
erative bootstrapping cycle. We also ex-
plore a one-semantic-class-per-discourse
heuristic, and use the classifiers to dynam-
ically create semantic features. We eval-
uate our approach by inducing six seman-
tic taggers from a collection of veterinary
medicine message board posts.
1 Introduction
The goal of our research is to create semantic class
taggers that can assign a semantic class label to ev-
ery noun phrase in a sentence. For example, con-
sider the sentence: ?The lab mix was diagnosed
with parvo and given abx?. A semantic tagger
should identify the ?the lab mix? as an ANIMAL,
?parvo? as a DISEASE, and ?abx? (antibiotics)
as a DRUG. Accurate semantic tagging could be
beneficial for many NLP tasks, including coref-
erence resolution and word sense disambiguation,
and many NLP applications, such as event extrac-
tion systems and question answering technology.
Semantic class tagging has been the subject of
previous research, primarily under the guises of
named entity recognition (NER) and mention de-
tection. Named entity recognizers perform se-
mantic tagging on proper name noun phrases, and
sometimes temporal and numeric expressions as
well. The mention detection task was introduced
in recent ACE evaluations (e.g., (ACE, 2007;
ACE, 2008)) and requires systems to identify all
noun phrases (proper names, nominals, and pro-
nouns) that correspond to 5-7 semantic classes.
Despite widespread interest in semantic tag-
ging, nearly all semantic taggers for comprehen-
sive NP tagging still rely on supervised learn-
ing, which requires annotated data for training.
A few annotated corpora exist, but they are rela-
tively small and most were developed for broad-
coverage NLP. Many domains, however, are re-
plete with specialized terminology and jargon that
cannot be adequately handled by general-purpose
systems. Domains such as biology, medicine, and
law are teeming with specialized vocabulary that
necessitates training on domain-specific corpora.
Our research explores the idea of inducing
domain-specific semantic taggers using a small
set of seed words as the only form of human su-
pervision. Given an (unannotated) collection of
domain-specific text, we automatically generate
training instances by labelling every instance of a
seed word with its designated semantic class. We
then train a classifier to do semantic tagging using
these seed-based annotations, using bootstrapping
to iteratively improve performance.
On the surface, this approach appears to be a
contradiction. The classifier must learn how to as-
sign different semantic tags to different instances
of the same word based on context (e.g., ?lab?
may refer to an animal in one context but a labora-
tory in another). And yet, we plan to train the clas-
sifier using stand-alone seed words, making the as-
sumption that every instance of the seed belongs to
the same semantic class. We resolve this apparent
contradiction by using semantically unambiguous
seeds and by introducing an initial context-only
training phase before bootstrapping begins. First,
we train a strictly contextual classifier that only
275
has access to contextual features and cannot see
the seed. Then we apply the classifier to the corpus
to automatically label new instances, and combine
these new instances with the seed-based instances.
This process expands and diversifies the training
set to fuel subsequent bootstrapping.
Another challenge is that we want to use a small
set of seeds to minimize the amount of human ef-
fort, and then use bootstrapping to fully exploit
the domain-specific corpus. Iterative self-training,
however, often has difficulty sustaining momen-
tum or it succumbs to semantic drift (Komachi
et al, 2008; McIntosh and Curran, 2009). To
address these issues, we simultaneously induce
a suite of classifiers for multiple semantic cat-
egories, using the positive instances of one se-
mantic category as negative instances for the oth-
ers. As bootstrapping progresses, the classifiers
gradually improve themselves, and each other,
over many iterations. We also explore a one-
semantic-class-per-discourse (OSCPD) heuristic
that infuses the learning process with fresh train-
ing instances, which may be substantially differ-
ent from the ones seen previously, and we use the
labels produced by the classifiers to dynamically
create semantic features.
We evaluate our approach by creating six se-
mantic taggers using a collection of message board
posts in the domain of veterinary medicine. Our
results show this approach produces high-quality
semantic taggers after a sustained bootstrapping
cycle that maintains good precision while steadily
increasing recall over many iterations.
2 Related Work
Semantic class tagging is most closely related to
named entity recognition (NER), mention detec-
tion, and semantic lexicon induction. NER sys-
tems (e.g., (Bikel et al, 1997; Collins and Singer,
1999; Cucerzan and Yarowsky, 1999; Fleischman
and Hovy, 2002) identify proper named entities,
such as people, organizations, and locations. Sev-
eral bootstrapping methods for NER have been
previously developed (e.g., (Collins and Singer,
1999; Niu et al, 2003)). NER systems, how-
ever, do not identify nominal NP instances (e.g.,
?a software manufacturer? or ?the beach?), or han-
dle semantic classes that are not associated with
proper named entities (e.g., symptoms).1 ACE
1Some NER systems also handle specialized constructs
such as dates and monetary amounts.
mention detection systems (e.g., see (ACE, 2005;
ACE, 2007; ACE, 2008)) require tagging of NPs
that correspond to 5-7 general semantic classes.
These systems are typically trained with super-
vised learning using annotated corpora, although
techniques have been developed to use resources
for one language to train systems for different lan-
guages (e.g., (Zitouni and Florian, 2009)).
Another line of relevant work is semantic class
induction (e.g., (Riloff and Shepherd, 1997; Roark
and Charniak, 1998; Thelen and Riloff, 2002; Ng,
2007; McIntosh and Curran, 2009), where the goal
is to induce a stand-alone dictionary of words with
semantic class labels. These techniques are of-
ten designed to learn specialized terminology from
unannotated domain-specific texts via bootstrap-
ping. Our work, however, focuses on classifica-
tion of NP instances in context, so the same phrase
may be assigned to different semantic classes in
different contexts. Consequently, our classifier
can also assign semantic class labels to pronouns.
There has also been work on extracting seman-
tically related terms or category members from
the Web (e.g., (Pas?ca, 2004; Etzioni et al, 2005;
Kozareva et al, 2008; Carlson et al, 2009)). These
techniques harvest broad-coverage semantic infor-
mation from the Web using patterns and statistics,
typically for the purpose of knowledge acquisi-
tion. Importantly, our goal is to classify instances
in context, rather than generate lists of terms. In
addition, the goal of our research is to learn spe-
cialized terms and jargon that may not be common
on the Web, as well as domain-specific usages that
may differ from the norm (e.g., ?mix? and ?lab?
are usually ANIMALS in our domain).
The idea of simulataneously learning multiple
semantic categories to prevent semantic drift has
been explored for other tasks, such as semantic
lexicon induction (Thelen and Riloff, 2002; McIn-
tosh and Curran, 2009) and pattern learning (Yan-
garber, 2003). Our bootstrapping model can be
viewed as a form of self-training (e.g., (Ng and
Cardie, 2003; Mihalcea, 2004; McClosky et al,
2006)), and cross-category training is similar in
spirit to co-training (e.g., (Blum and Mitchell,
1998; Collins and Singer, 1999; Riloff and Jones,
1999; Mueller et al, 2002; Phillips and Riloff,
2002)). But, importantly, our classifiers all use the
same feature set so they do not represent indepen-
dent views of the data. They do, however, offer
slightly different perspectives because each is at-
276
tempting to recognize a different semantic class.
3 Bootstrapping an Instance-based
Semantic Class Tagger from Seeds
3.1 Motivation
Our goal is to create a bootstrapping model that
can rapidly create semantic class taggers using
just a small set of seed words and an unanno-
tated domain-specific corpus. Our motivation
comes from specialized domains that cannot be
adequately handled by general-purpose NLP sys-
tems. As an example of such a domain, we have
been working with a collection of message board
posts in the field of veterinary medicine. Given a
document, we want a semantic class tagger to label
every NP with a semantic category, for example:
[A 14yo doxy]ANIMAL owned by
[a reputable breeder]HUMAN is be-
ing treated for [IBD]DISEASE with
[pred]DRUG.
When we began working with these texts, we
were immediately confronted by a dizzying array
of non-standard words and word uses. In addition
to formal veterinary vocabulary (e.g., animal dis-
eases), veterinarians often use informal, shorthand
terms when posting on-line. For example, they
frequently refer to breeds using ?nicknames? or
shortened terms (e.g., gshep for German shepherd,
doxy for dachsund, bxr for boxer, labx for labrador
cross). Often, they refer to animals based solely on
their physical characteristics, for example ?a dlh?
(domestic long hair), ?a m/n? (male, neutered), or
?a 2yo? (2 year old). This phenomenon occurs
with other semantic categories as well, such as
drugs and medical tests (e.g., pred for prednisone,
and rads for radiographs).
Nearly all semantic class taggers are trained us-
ing supervised learning with manually annotated
data. However, annotated data is rarely available
for specialized domains, and it is expensive to ob-
tain because domain experts must do the annota-
tion work. So we set out to create a bootstrapping
model that can rapidly create domain-specific se-
mantic taggers using just a few seed words and a
domain-specific text collection.
Our bootstrapping model consists of two dis-
tinct phases. First, we train strictly contextual
classifiers from the seed annotations. We then ap-
ply the classifiers to the unlabeled data to gener-
ate new annotated instances that are added to the
training set. Second, we employ a cross-category
bootstrapping process that simultaneously trains
a suite of classifiers for multiple semantic cate-
gories, using the positive instances for one se-
mantic class as negative instances for the oth-
ers. This cross-category training process gives
the learner sustained momentum over many boot-
strapping iterations. Finally, we explore two ad-
ditional enhancements: (1) a one-semantic-class-
per-discourse heuristic to automatically generate
new training instances, and (2) dynamically cre-
ated semantic features produced by the classifiers
themselves. In the following sections, we explain
each of these steps in detail.
3.2 Phase 1: Inducing a Contextual Classifier
The main challenge that we faced was how to train
an instance-based classifier using seed words as
the only form of human supervision. First, the user
must provide a small set of seed words that are
relatively unambiguous (e.g., ?dog? will nearly
always refer to an animal in our domain). But
even so, training a traditional classifier from seed-
based instances would likely produce a classifier
that learns to recognize the seeds but is unable to
classify new examples. We needed to force the
classifier to generalize beyond the seed words.
Our solution was to introduce an initial train-
ing step that induces a strictly contextual classifier.
First, we generate training instances by automati-
cally labeling each instance of a seed word with
its designated semantic class. However, when we
create feature vectors for the classifier, the seeds
themselves are hidden and only contextual fea-
tures are used to represent each training instance.
By essentially ?masking? the seed words so the
classifier can only see the contexts around them,
we force the classifier to generalize.
We create a suite of strictly contextual classi-
fiers, one for each semantic category. Each classi-
fier makes a binary decision as to whether a noun
phrase belongs to its semantic category. We use
the seed words for category Ck to generate posi-
tive training instances for the Ck classifier, and the
seed words for all other categories to generate the
negative training instances for Ck.
We use an in-house sentence segmenter and NP
chunker to identify the base NPs in each sentence
and create feature vectors that represent each con-
stituent in the sentence as either an NP or an in-
dividual word. For each seed word, the feature
277
vector captures a context window of 3 constituents
(word or NP) to its left and 3 constituents (word
or NP) to its right. Each constituent is represented
with a lexical feature: for NPs, we use its head
noun; for individual words, we use the word itself.
The seed word, however, is discarded so that the
classifier is essentially blind-folded and cannot see
the seed that produced the training instance. We
also create a feature for every modifier that pre-
cedes the head noun in the target NP, except for
articles which are discarded. As an example, con-
sider the following sentence:
Fluffy was diagnosed with FELV after a
blood test showed that he tested positive.
Suppose that ?FELV? is a seed for the DISEASE
category and ?test? is a seed for the TEST cate-
gory. Two training instances would be created,
with feature vectors that look like this, where M
represents a modifier inside the target NP:
was?3 diagnosed?2 with?1 after1 test2
showed3 ? DISEASE
with?3 FELV?2 after?1 bloodM showed1
that2 he3 ? TEST
The contextual classifiers are then applied to the
corpus to automatically label new instances. We
use a confidence score to label only the instances
that the classifiers are most certain about. We com-
pute a confidence score for instance i with respect
to semantic class Ck by considering both the score
of the Ck classifier as well as the scores of the
competing classifiers. Intuitively, we have confi-
dence in labeling an instance as category Ck if the
Ck classifier gave it a positive score, and its score
is much higher than the score of any other classi-
fier. We use the following scoring function:
Confidence(i,Ck) =
score(i,Ck) - max(?j 6=k score(i,Cj ))
We employ support vector machines (SVMs)
(Joachims, 1999) with a linear kernel as our classi-
fiers, using the SVMlin software (Keerthi and De-
Coste, 2005). We use the value produced by the
decision function (essentially the distance from
the hyperplane) as the score for a classifier. We
specify a threshold ?cf and only assign a semantic
tag Ck to an instance i if Confidence(i,Ck) ? ?cf .
All instances that pass the confidence thresh-
old are labeled and added to the training set.
This process greatly enhances the diversity of
the training data. In this initial learning step,
the strictly contextual classifiers substantially in-
crease the number of training instances for each
semantic category, producing a more diverse mix
of seed-generated instances and context-generated
instances.
3.3 Phase 2: Cross-Category Bootstrapping
The next phase of the learning process is an iter-
ative bootstrapping procedure. The key challenge
was to design a bootstrapping model that would
not succumb to semantic drift and would have sus-
tained momentum to continue learning over many
iterations.
Figure 1 shows the design of our cross-category
bootstrapping model.2 We simultaneously train a
suite of binary classifiers, one for each semantic
category, C1 . . . Cn. After each training cycle,
all of the classifiers are applied to the remaining
unlabeled instances and each classifier labels the
(positive) instances that it is most confident about
(i.e., the instances that it classifies with a confi-
dence score ? ?cf ). The set of instances positively
labeled by classifier Ck are shown as C+k in Figure
1. All of the new instances produced by classifier
Ck are then added to the set of positive training
instances for Ck and to the set of negative training
instances for all of the other classifiers.
One potential problem with this scheme is that
some categories are more prolific than others, plus
we are collecting negative instances from a set
of competing classifiers. Consequently, this ap-
proach can produce highly imbalanced training
sets. Therefore we enforced a 3:1 ratio of nega-
tives to positives by randomly selecting a subset
of the possible negative instances. We discuss this
issue further in Section 4.4.
labeled
C+1
C2
C+2 C
+
n
C
n
unlabeled
seeds
C1
i=1C
+
_( )
C+1
(+) (+)
C+2 i=2C
+
_( ) _( )
i=nC
+C+
n
(+)
Figure 1: Cross-Category Bootstrapping
2For simplicity, this picture does not depict the initial con-
textual training step, but that can be viewed as the first itera-
tion in this general framework.
278
Cross-category training has two advantages
over independent self-training. First, as oth-
ers have shown for pattern learning and lexicon
induction (Thelen and Riloff, 2002; Yangarber,
2003; McIntosh and Curran, 2009), simultane-
ously training classifiers for multiple categories
reduces semantic drift because each classifier is
deterred from encroaching on another one?s terri-
tory (i.e., claiming the instances from a compet-
ing class as its own). Second, similar in spirit to
co-training3 , this approach allows each classifier
to obtain new training instances from an outside
source that has a slightly different perspective.
While independent self-training can quickly run
out of steam, cross-category training supplies each
classifier with a constant stream of new (negative)
instances produced by competing classifiers. In
Section 4, we will show that cross-category boot-
strapping performs substantially better than an in-
dependent self-training model, where each classi-
fier is bootstrapped separately.
The feature set for these classifiers is exactly the
same as described in Section 3.2, except that we
add a new lexical feature that represents the head
noun of the target NP (i.e., the NP that needs to be
tagged). This allows the classifiers to consider the
local context as well as the target word itself when
making decisions.
3.4 One Semantic Class Per Discourse
We also explored the idea of using a one semantic
class per discourse (OSCPD) heuristic to gener-
ate additional training instances during bootstrap-
ping. Inspired by Yarowsky?s one sense per dis-
course heuristic for word sense disambiguation
(Yarowsky, 1995), we make the assumption that
multiple instances of a word in the same discourse
will nearly always correspond to the same seman-
tic class. Since our data set consists of message
board posts organized as threads, we consider all
posts in the same thread to be a single discourse.
After each training step, we apply the classi-
fiers to the unlabeled data to label some new in-
stances. For each newly labeled instance, the OS-
CPD heuristic collects all instances with the same
head noun in the same discourse (thread) and uni-
laterally labels them with the same semantic class.
This heuristic serves as meta-knowledge to label
instances that (potentially) occur in very different
3But technically this is not co-training because our feature
sets are all the same.
contexts, thereby infusing the bootstrapping pro-
cess with ?fresh? training examples.
In early experiments, we found that OSCPD can
be aggressive, pulling in many new instances. If
the classifier labels a word incorrectly, however,
then the OSCPD heuristic will compound the er-
ror and mislabel even more instances incorrectly.
Therefore we only apply this heuristic to instances
that are labeled with extremely high confidence
(?cf ? 2.5) and that pass a global sanity check,
gsc(w) ? 0.2, which ensures that a relatively high
proportion of labeled instances with the same head
noun have been assigned to the same semantic
class. Specifically, gsc(w) = 0.1?wl/cwl +0.9?
wu/c
wu
where wl and wu are the # of labeled and unla-
beled instances, respectively, wl/c is the # of in-
stances labeled as c, and wu/c is the # of unlabeled
instances that receive a positive confidence score
for c when given to the classifier. The intuition
behind the second term is that most instances are
initially unlabeled and we want to make sure that
many of the unlabeled instances are likely to be-
long to the same semantic class (even though the
classifier isn?t ready to commit to them yet).
3.5 Dynamic Semantic Features
For many NLP tasks, classifiers use semantic fea-
tures to represent the semantic class of words.
These features are typically obtained from exter-
nal resources such as Wordnet (Miller, 1990). Our
bootstrapping model incrementally trains seman-
tic class taggers, so we explored the idea of using
the labels assigned by the classifiers to create en-
hanced feature vectors by dynamically adding se-
mantic features. This process allows later stages
of bootstrapping to directly benefit from earlier
stages. For example, consider the sentence:
He started the doxy on Vetsulin today.
If ?Vetsulin? was labeled as a DRUG in a previ-
ous bootstrapping iteration, then the feature vector
representing the context around ?doxy? can be en-
hanced to include an additional semantic feature
identifying Vetsulin as a DRUG, which would look
like this:
He?2 started?1 on1 V etsulin2 DRUG2 today3
Intuitively, the semantic features should help the
classifier identify more general contextual pat-
terns, such as ?started <X> on DRUG?. To create
semantic features, we use the semantic tags that
279
have been assigned to the current set of labeled in-
stances. When a feature vector is created for a tar-
get NP, we check every noun instance in its context
window to see if it has been assigned a semantic
tag, and if so, then we add a semantic feature. In
the early stages of bootstrapping, however, rela-
tively few nouns will be assigned semantic tags,
so these features are often missing.
3.6 Thresholds and Stopping Criterion
When new instances are automatically labeled
during bootstrapping, it is critically important that
most of the labels are correct or performance
rapidly deteriorates. This suggests that we should
only label instances in which the classifier has
high confidence. On the other hand, a high thresh-
old often yields few new instances, which can
cause the bootstrapping process to sputter and halt.
To balance these competing demands, we used
a sliding threshold that begins conservatively but
gradually loosens the reins as bootstrapping pro-
gresses. Initially, we set ?cf = 2.0, which only
labels instances that the classifier is highly confi-
dent about. When fewer than min new instances
can be labeled, we automatically decrease ?cf by
0.2, allowing another batch of new instances to be
labeled, albeit with slightly less confidence. We
continue decreasing the threshold, as needed, un-
til ?cf < 1.0, when we end the bootstrapping
process. In Section 4, we show that this sliding
threshold outperforms fixed threshold values.
4 Evaluation
4.1 Data
Our data set consists of message board posts from
the Veterinary Information Network (VIN), which
is a web site (www.vin.com) for professionals in
veterinary medicine. Among other things, VIN
hosts forums where veterinarians engage in dis-
cussions about medical issues, cases in their prac-
tices, etc. Over half of the small animal veterinar-
ians in the U.S. and Canada use VIN. Analysis of
veterinary data could not only improve pet health
care, but also provide early warning signs of in-
fectious disease outbreaks, emerging zoonotic dis-
eases, exposures to environmental toxins, and con-
tamination in the food chain.
We obtained over 15,000 VIN message board
threads representing three topics: cardiology, en-
docrinology, and feline internal medicine. We did
basic cleaning, removing html tags and tokeniz-
ing numbers. For training, we used 4,629 threads,
consisting of 25,944 individual posts. We devel-
oped classifiers to identify six semantic categories:
ANIMAL, DISEASE/SYMPTOM4 , DRUG, HUMAN,
TEST, and OTHER.
The message board posts contain an abundance
of veterinary terminology and jargon, so two do-
main experts5 from VIN created a test set (answer
key) for our evaluation. We defined annotation
guidelines6 for each semantic category and con-
ducted an inter-annotator agreement study to mea-
sure the consistency of the two domain experts on
30 message board posts, which contained 1,473
noun phrases. The annotators achieved a relatively
high ? score of .80. Each annotator then labeled an
additional 35 documents, which gave us a test set
containing 100 manually annotated message board
posts. The table below shows the distribution of
semantic classes in the test set.
Animal Dis/Sym Drug Test Human Other
612 900 369 404 818 1723
To select seed words, we used the procedure
proposed by Roark and Charniak (1998), ranking
all of the head nouns in the training corpus by fre-
quency and manually selecting the first 10 nouns
that unambiguously belong to each category.7 This
process is fast, relatively objective, and guaranteed
to yield high-frequency terms, which is important
for bootstrapping. We used the Stanford part-of-
speech tagger (Toutanova et al, 2003) to identify
nouns, and our own simple rule-based NP chunker.
4.2 Baselines
To assess the difficulty of our data set and task,
we evaluated several baselines. The first baseline
searches for each head noun in WordNet and la-
bels the noun as category Ck if it has a hypernym
synset corresponding to that category. We manu-
ally identified the WordNet synsets that, to the best
of our ability, seem to most closely correspond
4We used a single category for diseases and symptoms
because our domain experts had difficulty distinguishing be-
tween them. A veterinary consultant explained that the same
term (e.g., diabetes) may be considered a symptom in one
context if it is secondary to another condition (e.g., pancre-
atitis) and a disease in a different context if it is the primary
diagnosis.
5One annotator is a veterinarian and the other is a veteri-
nary technician.
6The annotators were also allowed to label an NP as
POS Error if it was clearly misparsed. These cases were not
used in the evaluation.
7We used 20 seeds for DIS/SYM because we merged two
categories and for OTHER because it is a broad catch-all class.
280
Method Animal Dis/Sym Drug Test Human Other Avg
BASELINES
WordNet 32/80/46 21/81/34 25/35/29 NA 62/66/64 NA 35/66/45.8
Seeds 38/100/55 14/99/25 21/97/35 29/94/45 80/99/88 18/93/30 37/98/53.1
Supervised 67/94/78 20/88/33 24/96/39 34/90/49 79/99/88 31/91/46 45/94/60.7
Ind. Self-Train I.13 61/84/71 39/80/52 53/77/62 55/70/61 81/96/88 30/82/44 58/81/67.4
CROSS-CATEGORY BOOTSTRAPPED CLASSIFIERS
Contextual I.1 59/77/67 33/84/47 42/80/55 49/77/59 82/93/87 33/80/47 53/82/64.3
XCategory I.45 86/71/78 57/82/67 70/78/74 73/65/69 85/92/89 46/82/59 75/78/76.1
XCat+OSCPD I.40 86/69/77 59/81/68 72/70/71 72/69/71 86/92/89 50/81/62 75/76/75.6
XCat+OSCPD+SF I.39 86/70/77 60/81/69 69/81/75 73/69/71 86/91/89 50/81/62 75/78/76.6
Table 1: Experimental results, reported as Recall/Precision/F score
to each semantic class. We do not report Word-
Net results for TEST because there did not seem
be an appropriate synset, or for the OTHER cate-
gory because that is a catch-all class. The first row
of Table 1 shows the results, which are reported
as Recall/Precision/F score8. The WordNet base-
line yields low recall (21-32%) for every category
except HUMAN, which confirms that many veteri-
nary terms are not present in WordNet. The sur-
prisingly low precision for some categories is due
to atypical word uses (e.g., patient, boy, and girl
are HUMAN in WordNet but nearly always ANI-
MALS in our domain), and overgeneralities (e.g.,
WordNet lists calcium as a DRUG).
The second baseline simply labels every in-
stance of a seed with its designated semantic class.
All non-seed instances remain unlabeled. As ex-
pected, the seeds produce high precision but low
recall. The exception is HUMAN, where 80% of
the instances match a seed word, undoubtedly be-
cause five of the ten HUMAN seeds are 1st and 2nd
person pronouns, which are extremely common.
A third baseline trains semantic classifiers using
supervised learning by performing 10-fold cross-
validation on the test set. The feature set and
classifier settings are exactly the same as with
our bootstrapped classifiers.9 Supervised learning
achieves good precision but low recall for all cate-
gories except ANIMAL and HUMAN. In the next
section, we present the experimental results for
our bootstrapped classifiers.
4.3 Results for Bootstrapped Classifiers
The bottom section of Table 1 displays the results
for our bootstrapped classifiers. The Contextual
I.1 row shows results after just the first iteration,
8We use an F(1) score, where recall and precision are
equally weighted.
9For all of our classifiers, supervised and bootstrapped,
we label all instances of the seed words first and then apply
the classifiers to the unlabeled (non-seed) instances.
which trains only the strictly contextual classi-
fiers. The average F score improved from 53.1 for
the seeds alone to 64.3 with the contextual classi-
fiers. The next row, XCategory I.45, shows the
results after cross-category bootstrapping, which
ended after 45 iterations. (We indicate the num-
ber of iterations until bootstrapping ended using
the notation I.#.) With cross-category bootstrap-
ping, the average F score increased from 64.3 to
76.1. A closer inspection reveals that all of the se-
mantic categories except HUMAN achieved large
recall gains. And importantly, these recall gains
were obtained with relatively little loss of preci-
sion, with the exception of TEST.
Next, we measured the impact of the one-
semantic-class-per-discourse heuristic, shown as
XCat+OSCPD I.40. From Table 1, it appears that
OSCPD produced mixed results: recall increased
by 1-4 points for DIS/SYM, DRUG, HUMAN, and
OTHER, but precision was inconsistent, improv-
ing by +4 for TEST but dropping by -8 for DRUG.
However, this single snapshot in time does not tell
the full story. Figure 2 shows the performance
of the classifiers during the course of bootstrap-
ping. The OSCPD heuristic produced a steeper
learning curve, and consistently improved perfor-
mance until the last few iterations when its perfor-
mance dipped. This is probably due to the fact that
noise gradually increases during bootstrapping, so
incorrect labels are more likely and OSCPD will
compound any mistakes by the classifier. A good
future strategy might be to use the OSCPD heuris-
tic only during the early stages of bootstrapping
when the classifier?s decisions are most reliable.
We also evaluated the effect of dynamically cre-
ated semantic features. When added to the ba-
sic XCategory system, they had almost no ef-
fect. We suspect this is because the semantic fea-
tures are sparse during most of the bootstrapping
process. However, the semantic features did im-
281
0 5 10 15 20 25 30 35 40 4564
66
68
70
72
74
76
78
F
 
m
e
a
s
u
re
 
(%
)
# of iterations
 
 
independent self?training
cross?category bootstrapping
+OSCPD
+OSCPD+SemFeat
Figure 2: Average F scores after each iteration
prove performance when coupled with the OSCPD
heuristic, presumably because the OSCPD heuris-
tic aggressively labels more instances in the earlier
stages of bootstrapping, increasing the prevalence
of semantic class tags. The XCat+OSCPD+SF
I.39 row in Table 1 shows that the semantic fea-
tures coupled with OSCPD dramatically increased
the precision for DRUG, yielding the best overall F
score of 76.6.
We conducted one additional experiment to as-
sess the benefits of cross-category bootstrapping.
We created an analogous suite of classifiers using
self-training, where each classifier independently
labels the instances that it is most confident about,
adds them only to its own training set, and then
retrains itself. The Ind. Self-Train I.13 row in
Table 1 shows that these classifiers achieved only
58% recall (compared to 75% for XCategory) and
an average F score of 67.4 (compared to 76.1 for
XCategory). One reason for the disparity is that
the self-training model ended after just 13 boot-
strapping cycles (I.13), given the same threshold
values. To see if we could push it further, we low-
ered the confidence threshold to 0 and it continued
learning through 35 iterations. Even so, its final
score was 65% recall with 79% precision, which is
still well below the 75% recall with 78% precision
produced by the XCategory model. These results
support our claim that cross-category bootstrap-
ping is more effective than independently self-
trained models.
Figure 3 tracks the recall and precision scores
of the XCat+OSCPD+SF system as bootstrap-
ping progresses. This graph shows the sustained
momentum of cross-category bootstrapping: re-
0 5 10 15 20 25 30 35 40
50
55
60
65
70
75
80
85
# of iterations
 
 
Precision
Recall
Figure 3: Recall and Precision scores during
cross-category bootstrapping
call steadily improves while precision stays con-
sistently high with only a slight dropoff at the end.
4.4 Analysis
To assess the impact of corpus size, we generated
a learning curve with randomly selected subsets
of the training texts. Figure 4 shows the average F
score of our best system using 116 ,
1
8 ,
1
4 ,
1
2 ,
3
4 , and
all of the data. With just 116 th of the training set,
the system has about 1,600 message board posts
to use for training, which yields a similar F score
(roughly 61%) as the supervised baseline that used
100 manually annotated posts via 10-fold cross-
validation. So with 16 times more text, seed-based
bootstrapping achieves roughly the same results as
supervised learning. This result reflects the natural
trade-off between supervised learning and seed-
based bootstrapping. Supervised learning exploits
manually annotated data, but must make do with
a relatively small amount of training text because
manual annotations are expensive. In contrast,
seed-based bootstrapping exploits a small number
of human-provided seeds, but needs a larger set of
(unannotated) texts for training because the seeds
produce relatively sparse annotations of the texts.
An additional advantage of seed-based boot-
strapping methods is that they can easily exploit
unlimited amounts of training text. For many do-
mains, large text collections are readily available.
Figure 4 shows a steady improvement in perfor-
mance as the amount of training text grows. Over-
all, the F score improves from roughly 61% to
nearly 77% simply by giving the system access to
more unannotated text during bootstrapping.
We also evaluated the effectiveness of our slid-
ing confidence threshold (Section 3.6). The ta-
ble below shows the results using fixed thresholds
282
0 1/16 1/8 1/4 1/2 3/4 10
20
40
60
65
70
75
80
ration of data
F 
m
e
a
su
re
 
(%
)
Figure 4: Learning Curve
of 1.0, 1.5, 2.0, as well as the sliding threshold
(which begins at 2.0 and ends at 1.0 decreasing by
0.2 when the number of newly labeled instances
falls below 3000 (i.e., < 500 per category, on av-
erage). This table depicts the expected trade-off
between recall and precision for the fixed thresh-
olds, with higher thresholds producing higher pre-
cision but lower recall. The sliding threshold pro-
duces the best F score, achieving the best balance
of high recall and precision.
?cf R/P/F
1.0 71/77/74.1
1.5 69/81/74.7
2.0 65/82/72.4
Sliding 75/78/76.6
As mentioned in Section 3.3, we used 3 times
as many negative instances as positive instances
for every semantic category during bootstrap-
ping. This ratio was based on early experiments
where we needed to limit the number of neg-
ative instances per category because the cross-
category framework naturally produces an ex-
tremely skewed negative/positive training set. We
revisited this issue to empirically assess the impact
of the negative/positive ratio on performance. The
table below shows recall, precision, and F score
results when we vary the ratio from 1:1 to 5:1. A
1:1 ratio seems to be too conservative, improving
precision a bit but lowering recall. However the
difference in performance between the other ra-
tios is small. Our conclusion is that a 1:1 ratio is
too restrictive but, in general, the cross-category
bootstrapping process is relatively insensitive to
the specific negative/positive ratio used. Our ob-
servation from preliminary experiments, however,
is that the negative/positive ratio does need to be
controlled, or else the dominant categories over-
whelm the less frequent categories with negative
instances.
Neg:Pos R/P/F
1:1 72/79/75.2
2:1 74/78/76.1
3:1 75/78/76.6
4:1 75/77/76.0
5:1 76/77/76.4
Finally, we examined performance on gendered
pronouns (he/she/him/her), which can refer to ei-
ther animals or people in the veterinary domain.
84% (220/261) of the gendered pronouns were an-
notated as ANIMAL in the test set. Our classi-
fier achieved 95% recall (209/220) and 90% preci-
sion (209/232) for ANIMAL and 15% recall (6/41)
and 100% precision (6/6) for HUMAN. So while
it failed to recognize most of the (relatively few)
gendered pronouns that refer to a person, it was
highly effective at identifying the ANIMAL refer-
ences and it was always correct when it did assign
a HUMAN tag to a pronoun.
5 Conclusions
We presented a novel technique for inducing
domain-specific semantic class taggers from a
handful of seed words and an unannotated text
collection. Our results showed that the induced
taggers achieve good performance on six seman-
tic categories associated with the domain of vet-
erinary medicine. Our technique allows seman-
tic class taggers to be rapidly created for special-
ized domains with minimal human effort. In future
work, we plan to investigate whether these seman-
tic taggers can be used to improve other tasks.
Acknowledgments
We are very grateful to the people at the Veterinary
Information Network for providing us access to
their resources. Special thanks to Paul Pion, DVM
and Nicky Mastin, DVM for making their data
available to us, and to Sherri Lofing and Becky
Lundgren, DVM for their time and expertise in
creating the gold standard annotations. This re-
search was supported in part by Department of
Homeland Security Grant N0014-07-1-0152 and
Air Force Contract FA8750-09-C-0172 under the
DARPA Machine Reading Program.
References
ACE. 2005. NIST ACE evaluation website. In
http://www.nist.gov/speech/tests/ace/2005.
283
ACE. 2007. NIST ACE evaluation website. In
http://www.nist.gov/speech/tests/ace/2007.
ACE. 2008. NIST ACE evaluation website. In
http://www.nist.gov/speech/tests/ace/2008.
Daniel M. Bikel, Scott Miller, Richard Schwartz,
and Ralph Weischedel. 1997. Nymble: a high-
performance learning name-finder. In Proceedings
of ANLP-97, pages 194?201.
A. Blum and T. Mitchell. 1998. Combining Labeled
and Unlabeled Data with Co-Training. In Proceed-
ings of the 11th Annual Conference on Computa-
tional Learning Theory (COLT-98).
Andrew Carlson, Justin Betteridge, Estevam R. Hr-
uschka Jr., and Tom M. Mitchell. 2009. Coupling
semi-supervised learning of categories and relations.
In HLT-NAACL 2009 Workshop on Semi-Supervised
Learning for NLP.
M. Collins and Y. Singer. 1999. Unsupervised Mod-
els for Named Entity Classification. In Proceedings
of the Joint SIGDAT Conference on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora (EMNLP/VLC-99).
S. Cucerzan and D. Yarowsky. 1999. Language In-
dependent Named Entity Recognition Combining
Morphologi cal and Contextual Evidence. In Pro-
ceedings of the Joint SIGDAT Conference on Empir-
ical Methods in Natural Language Processing and
Very Large Corpora (EMNLP/VLC-99).
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from
the web: an experimental study. Artificial Intelli-
gence, 165(1):91?134, June.
M.B. Fleischman and E.H. Hovy. 2002. Fine grained
classification of named entities. In Proceedings of
the COLING conference, August.
T. Joachims. 1999. Making Large-Scale Support
Vector Machine Learning Practical. In A. Smola
B. Scho?lkopf, C. Burges, editor, Advances in Ker-
nel Methods: Support Vector Machines. MIT Press,
Cambridge, MA.
S. Keerthi and D. DeCoste. 2005. A Modified Finite
Newton Method for Fast Solution of Large Scale
Linear SVMs. Journal of Machine Learning Re-
search.
Mamoru Komachi, Taku Kudo, Masashi Shimbo, and
Yuji Matsumoto. 2008. Graph-based analysis of
semantic drift in espresso-like bootstrapping algo-
rithms. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing.
Z. Kozareva, E. Riloff, and E. Hovy. 2008. Semantic
Class Learning from the Web with Hyponym Pattern
Linkage Graphs. In Proceedings of the 46th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies (ACL-08).
D. McClosky, E. Charniak, and M Johnson. 2006. Ef-
fective self-training for parsing. In HLT-NAACL-
2006.
T. McIntosh and J. Curran. 2009. Reducing Semantic
Drift with Bagging and Distributional Similarity. In
Proceedings of the 47th Annual Meeting of the As-
sociation for Computational Linguistics.
R. Mihalcea. 2004. Co-training and Self-training for
Word Sense Disambiguation. In CoNLL-2004.
G. Miller. 1990. Wordnet: An On-line Lexical
Database. International Journal of Lexicography,
3(4).
C. Mueller, S. Rapp, and M. Strube. 2002. Applying
co-training to reference resolution. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics.
V. Ng and C. Cardie. 2003. Weakly supervised natural
language learning without redundant views. In HLT-
NAACL-2003.
V. Ng. 2007. Semantic Class Induction and Corefer-
ence Resolution. In Proceedings of the 45th Annual
Meeting of the Association for Computational Lin-
guistics.
Cheng Niu, Wei Li, Jihong Ding, and Rohini K. Sri-
hari. 2003. A bootstrapping approach to named
entity classification using successive learners. In
Proceedings of the 41st Annual Meeting on Associa-
tion for Computational Linguistics (ACL-03), pages
335?342.
M. Pas?ca. 2004. Acquisition of categorized named
entities for web search. In Proc. of the Thirteenth
ACM International Conference on Information and
Knowledge Management, pages 137?145.
W. Phillips and E. Riloff. 2002. Exploiting Strong
Syntactic Heuristics and Co-Training to Learn Se-
mantic Lexicons. In Proceedings of the 2002 Con-
ference on Empirical Methods in Natural Language
Processing, pages 125?132.
E. Riloff and R. Jones. 1999. Learning Dictionar-
ies for Information Extraction by Multi-Level Boot-
strapping. In Proceedings of the Sixteenth National
Conference on Artificial Intelligence.
E. Riloff and J. Shepherd. 1997. A Corpus-Based Ap-
proach for Building Semantic Lexicons. In Proceed-
ings of the Second Conference on Empirical Meth-
ods in Natural Language Processing, pages 117?
124.
B. Roark and E. Charniak. 1998. Noun-phrase Co-
occurrence Statistics for Semi-automatic Semantic
Lexicon Construction. In Proceedings of the 36th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 1110?1116.
284
M. Thelen and E. Riloff. 2002. A Bootstrapping
Method for Learning Semantic Lexicons Using Ex-
traction Pa ttern Contexts. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing, pages 214?221.
K. Toutanova, D. Klein, C. Manning, and Y. Singer.
2003. Feature-Rich Part-of-Speech Tagging with
a Cyclic Dependency Network. In Proceedings of
HLT-NAACL 2003.
R. Yangarber. 2003. Counter-training in the discovery
of semantic patterns. In Proceedings of the 41th An-
nual Meeting of the Association for Computational
Linguistics.
D. Yarowsky. 1995. Unsupervised Word Sense Dis-
ambiguation Rivaling Supervised Methods. In Pro-
ceedings of the 33rd Annual Meeting of the Associa-
tion for Computational Linguistics.
Imed Zitouni and Radu Florian. 2009. Cross-language
information propagation for arabic mention detec-
tion. ACM Transactions on Asian Language Infor-
mation Processing (TALIP), 8(4):1?21.
285
Proceedings of the ACL 2010 Conference Short Papers, pages 156?161,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Coreference Resolution with Reconcile
Veselin Stoyanov
Center for Language
and Speech Processing
Johns Hopkins Univ.
Baltimore, MD
ves@cs.jhu.edu
Claire Cardie
Department of
Computer Science
Cornell University
Ithaca, NY
cardie@cs.cornell.edu
Nathan Gilbert
Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT
ngilbert@cs.utah.edu
riloff@cs.utah.edu
David Buttler
David Hysom
Lawrence Livermore
National Laboratory
Livermore, CA
buttler1@llnl.gov
hysom1@llnl.gov
Abstract
Despite the existence of several noun phrase coref-
erence resolution data sets as well as several for-
mal evaluations on the task, it remains frustratingly
difficult to compare results across different corefer-
ence resolution systems. This is due to the high cost
of implementing a complete end-to-end coreference
resolution system, which often forces researchers
to substitute available gold-standard information in
lieu of implementing a module that would compute
that information. Unfortunately, this leads to incon-
sistent and often unrealistic evaluation scenarios.
With the aim to facilitate consistent and realis-
tic experimental evaluations in coreference resolu-
tion, we present Reconcile, an infrastructure for the
development of learning-based noun phrase (NP)
coreference resolution systems. Reconcile is de-
signed to facilitate the rapid creation of corefer-
ence resolution systems, easy implementation of
new feature sets and approaches to coreference res-
olution, and empirical evaluation of coreference re-
solvers across a variety of benchmark data sets and
standard scoring metrics. We describe Reconcile
and present experimental results showing that Rec-
oncile can be used to create a coreference resolver
that achieves performance comparable to state-of-
the-art systems on six benchmark data sets.
1 Introduction
Noun phrase coreference resolution (or simply
coreference resolution) is the problem of identi-
fying all noun phrases (NPs) that refer to the same
entity in a text. The problem of coreference res-
olution is fundamental in the field of natural lan-
guage processing (NLP) because of its usefulness
for other NLP tasks, as well as the theoretical in-
terest in understanding the computational mech-
anisms involved in government, binding and lin-
guistic reference.
Several formal evaluations have been conducted
for the coreference resolution task (e.g., MUC-6
(1995), ACE NIST (2004)), and the data sets cre-
ated for these evaluations have become standard
benchmarks in the field (e.g., MUC and ACE data
sets). However, it is still frustratingly difficult to
compare results across different coreference res-
olution systems. Reported coreference resolu-
tion scores vary wildly across data sets, evaluation
metrics, and system configurations.
We believe that one root cause of these dispar-
ities is the high cost of implementing an end-to-
end coreference resolution system. Coreference
resolution is a complex problem, and successful
systems must tackle a variety of non-trivial sub-
problems that are central to the coreference task ?
e.g., mention/markable detection, anaphor identi-
fication ? and that require substantial implemen-
tation efforts. As a result, many researchers ex-
ploit gold-standard annotations, when available, as
a substitute for component technologies to solve
these subproblems. For example, many published
research results use gold standard annotations to
identify NPs (substituting for mention/markable
detection), to distinguish anaphoric NPs from non-
anaphoric NPs (substituting for anaphoricity de-
termination), to identify named entities (substitut-
ing for named entity recognition), and to identify
the semantic types of NPs (substituting for seman-
tic class identification). Unfortunately, the use of
gold standard annotations for key/critical compo-
nent technologies leads to an unrealistic evalua-
tion setting, and makes it impossible to directly
compare results against coreference resolvers that
solve all of these subproblems from scratch.
Comparison of coreference resolvers is further
hindered by the use of several competing (and
non-trivial) evaluation measures, and data sets that
have substantially different task definitions and
annotation formats. Additionally, coreference res-
olution is a pervasive problem in NLP and many
NLP applications could benefit from an effective
coreference resolver that can be easily configured
and customized.
To address these issues, we have created a plat-
form for coreference resolution, called Reconcile,
that can serve as a software infrastructure to sup-
port the creation of, experimentation with, and
evaluation of coreference resolvers. Reconcile
was designed with the following seven desiderata
in mind:
? implement the basic underlying software ar-
156
chitecture of contemporary state-of-the-art
learning-based coreference resolution sys-
tems;
? support experimentation on most of the stan-
dard coreference resolution data sets;
? implement most popular coreference resolu-
tion scoring metrics;
? exhibit state-of-the-art coreference resolution
performance (i.e., it can be configured to cre-
ate a resolver that achieves performance close
to the best reported results);
? can be easily extended with new methods and
features;
? is relatively fast and easy to configure and
run;
? has a set of pre-built resolvers that can be
used as black-box coreference resolution sys-
tems.
While several other coreference resolution sys-
tems are publicly available (e.g., Poesio and
Kabadjov (2004), Qiu et al (2004) and Versley et
al. (2008)), none meets all seven of these desider-
ata (see Related Work). Reconcile is a modular
software platform that abstracts the basic archi-
tecture of most contemporary supervised learning-
based coreference resolution systems (e.g., Soon
et al (2001), Ng and Cardie (2002), Bengtson and
Roth (2008)) and achieves performance compara-
ble to the state-of-the-art on several benchmark
data sets. Additionally, Reconcile can be eas-
ily reconfigured to use different algorithms, fea-
tures, preprocessing elements, evaluation settings
and metrics.
In the rest of this paper, we review related work
(Section 2), describe Reconcile?s organization and
components (Section 3) and show experimental re-
sults for Reconcile on six data sets and two evalu-
ation metrics (Section 4).
2 Related Work
Several coreference resolution systems are cur-
rently publicly available. JavaRap (Qiu et al,
2004) is an implementation of the Lappin and
Leass? (1994) Resolution of Anaphora Procedure
(RAP). JavaRap resolves only pronouns and, thus,
it is not directly comparable to Reconcile. GuiTaR
(Poesio and Kabadjov, 2004) and BART (Versley
et al, 2008) (which can be considered a succes-
sor of GuiTaR) are both modular systems that tar-
get the full coreference resolution task. As such,
both systems come close to meeting the majority
of the desiderata set forth in Section 1. BART,
in particular, can be considered an alternative to
Reconcile, although we believe that Reconcile?s
approach is more flexible than BART?s. In addi-
tion, the architecture and system components of
Reconcile (including a comprehensive set of fea-
tures that draw on the expertise of state-of-the-art
supervised learning approaches, such as Bengtson
and Roth (2008)) result in performance closer to
the state-of-the-art.
Coreference resolution has received much re-
search attention, resulting in an array of ap-
proaches, algorithms and features. Reconcile
is modeled after typical supervised learning ap-
proaches to coreference resolution (e.g. the archi-
tecture introduced by Soon et al (2001)) because
of the popularity and relatively good performance
of these systems.
However, there have been other approaches
to coreference resolution, including unsupervised
and semi-supervised approaches (e.g. Haghighi
and Klein (2007)), structured approaches (e.g.
McCallum and Wellner (2004) and Finley and
Joachims (2005)), competition approaches (e.g.
Yang et al (2003)) and a bell-tree search approach
(Luo et al (2004)). Most of these approaches rely
on some notion of pairwise feature-based similar-
ity and can be directly implemented in Reconcile.
3 System Description
Reconcile was designed to be a research testbed
capable of implementing most current approaches
to coreference resolution. Reconcile is written in
Java, to be portable across platforms, and was de-
signed to be easily reconfigurable with respect to
subcomponents, feature sets, parameter settings,
etc.
Reconcile?s architecture is illustrated in Figure
1. For simplicity, Figure 1 shows Reconcile?s op-
eration during the classification phase (i.e., assum-
ing that a trained classifier is present).
The basic architecture of the system includes
five major steps. Starting with a corpus of docu-
ments together with a manually annotated corefer-
ence resolution answer key1, Reconcile performs
1Only required during training.
157
Figure 1: The Reconcile classification architecture.
the following steps, in order:
1. Preprocessing. All documents are passed
through a series of (external) linguistic pro-
cessors such as tokenizers, part-of-speech
taggers, syntactic parsers, etc. These com-
ponents produce annotations of the text. Ta-
ble 1 lists the preprocessors currently inter-
faced in Reconcile. Note that Reconcile in-
cludes several in-house NP detectors, that
conform to the different data sets? defini-
tions of what constitutes a NP (e.g., MUC
vs. ACE). All of the extractors utilize a syn-
tactic parse of the text and the output of a
Named Entity (NE) extractor, but extract dif-
ferent constructs as specialized in the corre-
sponding definition. The NP extractors suc-
cessfully recognize about 95% of the NPs in
the MUC and ACE gold standards.
2. Feature generation. Using annotations pro-
duced during preprocessing, Reconcile pro-
duces feature vectors for pairs of NPs. For
example, a feature might denote whether the
two NPs agree in number, or whether they
have any words in common. Reconcile in-
cludes over 80 features, inspired by other suc-
cessful coreference resolution systems such
as Soon et al (2001) and Ng and Cardie
(2002).
3. Classification. Reconcile learns a classifier
that operates on feature vectors representing
Task Systems
Sentence UIUC (CC Group, 2009)
splitter OpenNLP (Baldridge, J., 2005)
Tokenizer OpenNLP (Baldridge, J., 2005)
POS OpenNLP (Baldridge, J., 2005)
Tagger + the two parsers below
Parser Stanford (Klein and Manning, 2003)
Berkeley (Petrov and Klein, 2007)
Dep. parser Stanford (Klein and Manning, 2003)
NE OpenNLP (Baldridge, J., 2005)
Recognizer Stanford (Finkel et al, 2005)
NP Detector In-house
Table 1: Preprocessing components available in
Reconcile.
pairs of NPs and it is trained to assign a score
indicating the likelihood that the NPs in the
pair are coreferent.
4. Clustering. A clustering algorithm consoli-
dates the predictions output by the classifier
and forms the final set of coreference clusters
(chains).2
5. Scoring. Finally, during testing Reconcile
runs scoring algorithms that compare the
chains produced by the system to the gold-
standard chains in the answer key.
Each of the five steps above can invoke differ-
ent components. Reconcile?s modularity makes it
2Some structured coreference resolution algorithms (e.g.,
McCallum and Wellner (2004) and Finley and Joachims
(2005)) combine the classification and clustering steps above.
Reconcile can easily accommodate this modification.
158
Step Available modules
Classification various learners in the Weka toolkit
libSVM (Chang and Lin, 2001)
SVMlight (Joachims, 2002)
Clustering Single-link
Best-First
Most Recent First
Scoring MUC score (Vilain et al, 1995)
B3 score (Bagga and Baldwin, 1998)
CEAF score (Luo, 2005)
Table 2: Available implementations for different
modules available in Reconcile.
easy for new components to be implemented and
existing ones to be removed or replaced. Recon-
cile?s standard distribution comes with a compre-
hensive set of implemented components ? those
available for steps 2?5 are shown in Table 2. Rec-
oncile contains over 38,000 lines of original Java
code. Only about 15% of the code is concerned
with running existing components in the prepro-
cessing step, while the rest deals with NP extrac-
tion, implementations of features, clustering algo-
rithms and scorers. More details about Recon-
cile?s architecture and available components and
features can be found in Stoyanov et al (2010).
4 Evaluation
4.1 Data Sets
Reconcile incorporates the six most commonly
used coreference resolution data sets, two from the
MUC conferences (MUC-6, 1995; MUC-7, 1997)
and four from the ACE Program (NIST, 2004).
For ACE, we incorporate only the newswire por-
tion. When available, Reconcile employs the stan-
dard test/train split. Otherwise, we randomly split
the data into a training and test set following a
70/30 ratio. Performance is evaluated according
to the B3 and MUC scoring metrics.
4.2 The Reconcile2010 Configuration
Reconcile can be easily configured with differ-
ent algorithms for markable detection, anaphoric-
ity determination, feature extraction, etc., and run
against several scoring metrics. For the purpose of
this sample evaluation, we create only one partic-
ular instantiation of Reconcile, which we will call
Reconcile2010 to differentiate it from the general
platform. Reconcile2010 is configured using the
following components:
1. Preprocessing
(a) Sentence Splitter: OpenNLP
(b) Tokenizer: OpenNLP
(c) POS Tagger: OpenNLP
(d) Parser: Berkeley
(e) Named Entity Recognizer: Stanford
2. Feature Set - A hand-selected subset of 60 out of the
more than 80 features available. The features were se-
lected to include most of the features from Soon et al
Soon et al (2001), Ng and Cardie (2002) and Bengtson
and Roth (2008).
3. Classifier - Averaged Perceptron
4. Clustering - Single-link - Positive decision threshold
was tuned by cross validation of the training set.
4.3 Experimental Results
The first two rows of Table 3 show the perfor-
mance of Reconcile2010. For all data sets, B3
scores are higher than MUC scores. The MUC
score is highest for the MUC6 data set, while B3
scores are higher for the ACE data sets as com-
pared to the MUC data sets.
Due to the difficulties outlined in Section 1,
results for Reconcile presented here are directly
comparable only to a limited number of scores
reported in the literature. The bottom three
rows of Table 3 list these comparable scores,
which show that Reconcile2010 exhibits state-of-
the-art performance for supervised learning-based
coreference resolvers. A more detailed study of
Reconcile-based coreference resolution systems
in different evaluation scenarios can be found in
Stoyanov et al (2009).
5 Conclusions
Reconcile is a general architecture for coreference
resolution that can be used to easily create various
coreference resolvers. Reconcile provides broad
support for experimentation in coreference reso-
lution, including implementation of the basic ar-
chitecture of contemporary state-of-the-art coref-
erence systems and a variety of individual mod-
ules employed in these systems. Additionally,
Reconcile handles all of the formatting and scor-
ing peculiarities of the most widely used coref-
erence resolution data sets (those created as part
of the MUC and ACE conferences) and, thus,
allows for easy implementation and evaluation
across these data sets. We hope that Reconcile
will support experimental research in coreference
resolution and provide a state-of-the-art corefer-
ence resolver for both researchers and application
developers. We believe that in this way Recon-
cile will facilitate meaningful and consistent com-
parisons of coreference resolution systems. The
full Reconcile release is available for download at
http://www.cs.utah.edu/nlp/reconcile/.
159
System Score Data sets
MUC6 MUC7 ACE-2 ACE03 ACE04 ACE05
Reconcile2010
MUC 68.50 62.80 65.99 67.87 62.03 67.41
B3 70.88 65.86 78.29 79.39 76.50 73.71
Soon et al (2001) MUC 62.6 60.4 ? ? ? ?
Ng and Cardie (2002) MUC 70.4 63.4 ? ? ? ?
Yang et al (2003) MUC 71.3 60.2 ? ? ? ?
Table 3: Scores for Reconcile on six data sets and scores for comparable coreference systems.
Acknowledgments
This research was supported in part by the Na-
tional Science Foundation under Grant # 0937060
to the Computing Research Association for the
CIFellows Project, Lawrence Livermore National
Laboratory subcontract B573245, Department of
Homeland Security Grant N0014-07-1-0152, and
Air Force Contract FA8750-09-C-0172 under the
DARPA Machine Reading Program.
The authors would like to thank the anonymous
reviewers for their useful comments.
References
A. Bagga and B. Baldwin. 1998. Algorithms for scoring
coreference chains. In Linguistic Coreference Workshop
at the Language Resources and Evaluation Conference.
Baldridge, J. 2005. The OpenNLP project.
http://opennlp.sourceforge.net/.
E. Bengtson and D. Roth. 2008. Understanding the value of
features for coreference resolution. In Proceedings of the
2008 Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).
CC Group. 2009. Sentence Segmentation Tool.
http://l2r.cs.uiuc.edu/ cogcomp/atool.php?tkey=SS.
C. Chang and C. Lin. 2001. LIBSVM: a Li-
brary for Support Vector Machines. Available at
http://www.csie.ntu.edu.tw/cjlin/libsvm.
J. Finkel, T. Grenager, and C. Manning. 2005. Incorporating
Non-local Information into Information Extraction Sys-
tems by Gibbs Sampling. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics and
44th Annual Meeting of the ACL.
T. Finley and T. Joachims. 2005. Supervised clustering with
support vector machines. In Proceedings of the Twenty-
second International Conference on Machine Learning
(ICML 2005).
A. Haghighi and D. Klein. 2007. Unsupervised Coreference
Resolution in a Nonparametric Bayesian Model. In Pro-
ceedings of the 45th Annual Meeting of the ACL.
T. Joachims. 2002. SVMLight, http://svmlight.joachims.org.
D. Klein and C. Manning. 2003. Fast Exact Inference with
a Factored Model for Natural Language Parsing. In Ad-
vances in Neural Information Processing (NIPS 2003).
S. Lappin and H. Leass. 1994. An algorithm for pronom-
inal anaphora resolution. Computational Linguistics,
20(4):535?561.
X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and
S. Roukos. 2004. A mention-synchronous coreference
resolution algorithm based on the bell tree. In Proceed-
ings of the 42nd Annual Meeting of the ACL.
X. Luo. 2005. On Coreference Resolution Performance
Metrics. In Proceedings of Human Language Technology
Conference and Conference on Empirical Methods in Nat-
ural Language Processing (HLT/EMNLP).
A. McCallum and B. Wellner. 2004. Conditional Models
of Identity Uncertainty with Application to Noun Coref-
erence. In Advances in Neural Information Processing
(NIPS 2004).
MUC-6. 1995. Coreference Task Definition. In Proceedings
of the Sixth Message Understanding Conference (MUC-
6).
MUC-7. 1997. Coreference Task Definition. In Proceed-
ings of the Seventh Message Understanding Conference
(MUC-7).
V. Ng and C. Cardie. 2002. Improving Machine Learning
Approaches to Coreference Resolution. In Proceedings of
the 40th Annual Meeting of the ACL.
NIST. 2004. The ACE Evaluation Plan. NIST.
S. Petrov and D. Klein. 2007. Improved Inference for Un-
lexicalized Parsing. In Proceedings of the Joint Meeting
of the Human Language Technology Conference and the
North American Chapter of the Association for Computa-
tional Linguistics (HLT-NAACL 2007).
M. Poesio and M. Kabadjov. 2004. A general-purpose,
off-the-shelf anaphora resolution module: implementation
and preliminary evaluation. In Proceedings of the Lan-
guage Resources and Evaluation Conference.
L. Qiu, M.-Y. Kan, and T.-S. Chua. 2004. A public reference
implementation of the rap anaphora resolution algorithm.
In Proceedings of the Language Resources and Evaluation
Conference.
W. Soon, H. Ng, and D. Lim. 2001. A Machine Learning Ap-
proach to Coreference of Noun Phrases. Computational
Linguistics, 27(4):521?541.
V. Stoyanov, N. Gilbert, C. Cardie, and E. Riloff. 2009. Co-
nundrums in noun phrase coreference resolution: Mak-
ing sense of the state-of-the-art. In Proceedings of
ACL/IJCNLP.
160
V. Stoyanov, C. Cardie, N. Gilbert, E. Riloff, D. Buttler, and
D. Hysom. 2010. Reconcile: A coreference resolution
research platform. Technical report, Cornell University.
Y. Versley, S. Ponzetto, M. Poesio, V. Eidelman, A. Jern,
J. Smith, X. Yang, and A. Moschitti. 2008. BART: A
modular toolkit for coreference resolution. In Proceed-
ings of the Language Resources and Evaluation Confer-
ence.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A Model-Theoretic Coreference
Scoring Theme. In Proceedings of the Sixth Message Un-
derstanding Conference (MUC-6).
X. Yang, G. Zhou, J. Su, and C. Tan. 2003. Coreference
resolution using competition learning approach. In Pro-
ceedings of the 41st Annual Meeting of the ACL.
161
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1137?1147,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Peeling Back the Layers: Detecting Event Role Fillers in Secondary Contexts
Ruihong Huang and Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT 84112
{huangrh,riloff}@cs.utah.edu
Abstract
The goal of our research is to improve
event extraction by learning to identify sec-
ondary role filler contexts in the absence
of event keywords. We propose a multi-
layered event extraction architecture that pro-
gressively ?zooms in? on relevant informa-
tion. Our extraction model includes a docu-
ment genre classifier to recognize event nar-
ratives, two types of sentence classifiers, and
noun phrase classifiers to extract role fillers.
These modules are organized as a pipeline to
gradually zero in on event-related information.
We present results on the MUC-4 event ex-
traction data set and show that this model per-
forms better than previous systems.
1 Introduction
Event extraction is an information extraction (IE)
task that involves identifying the role fillers for
events in a particular domain. For example, the
Message Understanding Conferences (MUCs) chal-
lenged NLP researchers to create event extraction
systems for domains such as terrorism (e.g., to iden-
tify the perpetrators, victims, and targets of terrorism
events) and management succession (e.g., to iden-
tify the people and companies involved in corporate
management changes).
Most event extraction systems use either a
learning-based classifier to label words as role
fillers, or lexico-syntactic patterns to extract role
fillers from pattern contexts. Both approaches, how-
ever, generally tackle event recognition and role
filler extraction at the same time. In other words,
most event extraction systems primarily recognize
contexts that explicitly refer to a relevant event. For
example, a system that extracts information about
murders will recognize expressions associated with
murder (e.g., ?killed?, ?assassinated?, or ?shot to
death?) and extract role fillers from the surround-
ing context. But many role fillers occur in contexts
that do not explicitly mention the event, and those
fillers are often overlooked. For example, the per-
petrator of a murder may be mentioned in the con-
text of an arrest, an eyewitness report, or specula-
tion about possible suspects. Victims may be named
in sentences that discuss the aftermath of the event,
such as the identification of bodies, transportation
of the injured to a hospital, or conclusions drawn
from an investigation. We will refer to these types of
sentences as ?secondary contexts? because they are
generally not part of the main event description. Dis-
course analysis is one option to explicitly link these
secondary contexts to the event, but discourse mod-
elling is itself a difficult problem.
The goal of our research is to improve event ex-
traction by learning to identify secondary role filler
contexts in the absence of event keywords. We cre-
ate a set of classifiers to recognize role-specific con-
texts that suggest the presence of a likely role filler
regardless of whether a relevant event is mentioned
or not. For example, our model should recognize
that a sentence describing an arrest probably in-
cludes a reference to a perpetrator, even though the
crime itself is reported elsewhere.
Extracting information from these secondary con-
texts can be risky, however, unless we know that
the larger context is discussing a relevant event. To
1137
address this, we adopt a two-pronged strategy for
event extraction that handles event narrative docu-
ments differently from other documents. We define
an event narrative as an article whose main purpose
is to report the details of an event. We apply the role-
specific sentence classifiers only to event narratives
to aggressively search for role fillers in these sto-
ries. However, other types of documents can men-
tion relevant events too. The MUC-4 corpus, for ex-
ample, includes interviews, speeches, and terrorist
propaganda that contain information about terrorist
events. We will refer to these documents as fleet-
ing reference texts because they mention a relevant
event somewhere in the document, albeit briefly. To
ensure that relevant information is extracted from all
documents, we also apply a conservative extraction
process to every document to extract facts from ex-
plicit event sentences.
Our complete event extraction model, called
TIER, incorporates both document genre and role-
specific context recognition into 3 layers of analy-
sis: document analysis, sentence analysis, and noun
phrase (NP) analysis. At the top level, we train a
text genre classifier to identify event narrative doc-
uments. At the middle level, we create two types
of sentence classifiers. Event sentence classifiers
identify sentences that are associated with relevant
events, and role-specific context classifiers identify
sentences that contain possible role fillers irrespec-
tive of whether an event is mentioned. At the low-
est level, we use role filler extractors to label indi-
vidual noun phrases as role fillers. As documents
pass through the pipeline, they are analyzed at dif-
ferent levels of granularity. All documents pass
through the event sentence classifier, and event sen-
tences are given to the role filler extractors. Docu-
ments identified as event narratives additionally pass
through role-specific sentence classifiers, and the
role-specific sentences are also given to the role filler
extractors. This multi-layered approach creates an
event extraction system that can discover role fillers
in a variety of different contexts, while maintaining
good precision.
In the following sections, we position our research
with respect to related work, present the details of
our multi-layered event extraction model, and show
experimental results for five event roles using the
MUC-4 data set.
2 Related Work
Some event extraction data sets only include doc-
uments that describe relevant events (e.g., well-
known data sets for the domains of corporate ac-
quisitions (Freitag, 1998b; Freitag and McCallum,
2000; Finn and Kushmerick, 2004), job postings
(Califf and Mooney, 2003; Freitag and McCallum,
2000), and seminar announcements (Freitag, 1998b;
Ciravegna, 2001; Chieu and Ng, 2002; Finn and
Kushmerick, 2004; Gu and Cercone, 2006). But
many IE data sets present a more realistic task where
the IE system must determine whether a relevant
event is present in the document, and if so, extract
its role fillers. Most of the Message Understand-
ing Conference data sets represent this type of event
extraction task, containing (roughly) a 50/50 mix
of relevant and irrelevant documents (e.g., MUC-3,
MUC-4, MUC-6, and MUC-7 (Hirschman, 1998)).
Our research focuses on this setting where the event
extraction system is not assured of getting only rele-
vant documents to process.
Most event extraction models can be character-
ized as either pattern-based or classifier-based ap-
proaches. Early event extraction systems used hand-
crafted patterns (e.g., (Appelt et al, 1993; Lehn-
ert et al, 1991)), but more recent systems gener-
ate patterns or rules automatically using supervised
learning (e.g., (Kim and Moldovan, 1993; Riloff,
1993; Soderland et al, 1995; Huffman, 1996; Fre-
itag, 1998b; Ciravegna, 2001; Califf and Mooney,
2003)), weakly supervised learning (e.g., (Riloff,
1996; Riloff and Jones, 1999; Yangarber et al,
2000; Sudo et al, 2003; Stevenson and Greenwood,
2005)), or unsupervised learning (e.g., (Shinyama
and Sekine, 2006; Sekine, 2006)). In addition, many
classifiers have been created to sequentially label
event role fillers in a sentence (e.g., (Freitag, 1998a;
Chieu and Ng, 2002; Finn and Kushmerick, 2004;
Li et al, 2005; Yu et al, 2005)). Research has
also been done on relation extraction (e.g., (Roth
and Yih, 2001; Zelenko et al, 2003; Bunescu and
Mooney, 2007)), but that task is different from event
extraction because it focuses on isolated relations
rather than template-based event analysis.
Most event extraction systems scan a text and
search small context windows using patterns or a
classifier. However, recent work has begun to ex-
1138
Figure 1: TIER: A Multi-Layered Architecture for Event Extraction
plore more global approaches. (Maslennikov and
Chua, 2007) use discourse trees and local syntactic
dependencies in a pattern-based framework to incor-
porate wider context. Ji and Grishman (2008) en-
force event role consistency across different docu-
ments. (Liao and Grishman, 2010) use cross-event
inference to help with the extraction of role fillers
shared across events. And there have been several
recent IE models that explore the idea of identify-
ing relevant sentences to gain a wider contextual
view and then extracting role fillers. (Gu and Cer-
cone, 2006) created HMMs to first identify relevant
sentences, but their research focused on eliminating
redundant extractions and worked with seminar an-
nouncements, where the system was only given rel-
evant documents. (Patwardhan and Riloff, 2007) de-
veloped a system that learns to recognize event sen-
tences and uses patterns that have a semantic affinity
for an event role to extract role fillers. GLACIER
(Patwardhan and Riloff, 2009) jointly considers sen-
tential evidence and phrasal evidence in a unified
probabilistic framework. Our research follows in
the same spirit as these approaches by performing
multiple levels of text analysis. But our event ex-
traction model includes two novel contributions: (1)
we develop a set of role-specific sentence classifiers
to learn to recognize secondary contexts associated
with each type of event role , and (2) we exploit text
genre to incorporate a third level of analysis that en-
ables the system to aggressively hunt for role fillers
in documents that are event narratives. In Section 5,
we compare the performance of our model with both
the GLACIER system and Patwardhan & Riloff?s
semantic affinity model.
3 A Multi-Layered Approach to Event
Extraction
The main idea behind our approach is to analyze
documents at multiple levels of granularity in order
to identify role fillers that occur in different types of
contexts. Our event extraction model progressively
?zooms in? on relevant information by first identi-
fying the document type, then identifying sentences
that are likely to contain relevant information, and
finally analyzing individual noun phrases to identify
role fillers. The key advantage of this architecture is
that it allows us to search for information using two
different principles: (1) we look for contexts that di-
rectly refer to the event, as per most traditional event
extraction systems, and (2) we look for secondary
contexts that are often associated with a specific type
of role filler. Identifying these role-specific contexts
can root out important facts would have been oth-
erwise missed. Figure 1 shows the multi-layered
pipeline of our event extraction system.
An important aspect of our model is that two dif-
ferent strategies are employed to handle documents
of different types. The event extraction task is to
find any description of a relevant event, even if the
event is not the topic of the article.1 Consequently,
all documents are given to the event sentence recog-
nizers and their mission is to identify any sentence
that mentions a relevant event. This path through the
pipeline is conservative because information is ex-
tracted only from event sentences, but all documents
are processed, including stories that contain only a
fleeting reference to a relevant event.
1Per the MUC-4 task definition (MUC-4 Proceedings,
1992).
1139
The second path through the pipeline performs
additional processing for documents that belong to
the event narrative text genre. For event narratives,
we assume that most of the document discusses a
relevant event so we can more aggressively hunt for
event-related information in secondary contexts.
In this section, we explain how we create the two
types of sentence classifiers and the role filler extrac-
tors. We will return to the issue of document genre
and the event narrative classifier in Section 4.
3.1 Sentence Classification
We have argued that event role fillers commonly oc-
cur in two types of contexts: event contexts and
role-specific secondary contexts. For the purposes
of this research, we use sentences as our definition
of a ?context?, although there are obviously many
other possible definitions. An event context is a sen-
tence that describes the actual event. A secondary
context is a sentence that provides information re-
lated to an event but in the context of other activities
that precede or follow the event.
For both types of classifiers, we use exactly the
same feature set, but we train them in different ways.
The MUC-4 corpus used in our experiments in-
cludes a training set consisting of documents and an-
swer keys. Each document that describes a relevant
event has answer key templates with the role fillers
(answer key strings) for each event. To train the
event sentence recognizer, we consider a sentence
to be a positive training instance if it contains one or
more answer key strings from any of the event roles.
This produced 3,092 positive training sentences. All
remaining sentences that do not contain any answer
key strings are used as negative instances. This pro-
duced 19,313 negative training sentences, yielding a
roughly 6:1 ratio of negative to positive instances.
There is no guarantee that a classifier trained in
this way will identify event sentences, but our hy-
pothesis was that training across all of the event
roles together would produce a classifier that learns
to recognize general event contexts. This approach
was also used to train GLACIER?s sentential event
recognizer (Patwardhan and Riloff, 2009), and they
demonstrated that this approach worked reasonably
well when compared to training with event sentences
labelled by human judges.
The main contribution of our work is introducing
additional role-specific sentence classifiers to seek
out role fillers that appear in less obvious secondary
contexts. We train a set of role-specific sentence
classifiers, one for each type of event role. Every
sentence that contains a role filler of the appropri-
ate type is used as a positive training instance. Sen-
tences that do not contain any answer key strings are
negative instances.2 In this way, we force each clas-
sifier to focus on the contexts specific to its particu-
lar event role. We expect the role-specific sentence
classifiers to find some secondary contexts that the
event sentence classifier will miss, although some
sentences may be classified as both.
Using all possible negative instances would pro-
duce an extremely skewed ratio of negative to pos-
itive instances. To control the skew and keep the
training set-up consistent with the event sentence
classifier, we randomly choose from the negative in-
stances to produce a 6:1 ratio of negative to positive
instances.
Both types of classifiers use an SVM model cre-
ated with SVMlin (Keerthi and DeCoste, 2005), and
exactly the same features. The feature set consists
of the unigrams and bigrams that appear in the train-
ing texts, the semantic class of each noun phrase3,
plus a few additional features to represent the tense
of the main verb phrase in the sentence and whether
the document is long (> 35 words) or short (< 5
words). All of the feature values are binary.
3.2 Role Filler Extractors
Our extraction model also includes a set of role filler
extractors, one per event role. Each extractor re-
ceives a sentence as input and determines which
noun phrases (NPs) in the sentence are fillers for the
event role. To train an SVM classifier, noun phrases
corresponding to answer key strings for the event
role are positive instances. We randomly choose
among all noun phrases that are not in the answer
keys to create a 10:1 ratio of negative to positive in-
stances.
2We intentionally do not use sentences that contain fillers
for competing event roles as negative instances because sen-
tences often contain multiple role fillers of different types (e.g.,
a weapon may be found near a body). Sentences without any
role fillers are certain to be irrelevant contexts.
3We used the Sundance parser (Riloff and Phillips, 2004) to
identify noun phrases and assign semantic class labels.
1140
The feature set for the role filler extractors is
much richer than that of the sentence classifiers be-
cause they must carefully consider the local context
surrounding a noun phrase. We will refer to the noun
phrase being labelled as the targeted NP. The role
filler extractors use three types of features:
Lexical features: we represent four words to the
left and four words to the right of the targeted NP, as
well as the head noun and modifiers (adjectives and
noun modifiers) of the targeted NP itself.
Lexico-syntactic patterns: we use the AutoSlog
pattern generator (Riloff, 1993) to automatically
create lexico-syntactic patterns around each noun
phrase in the sentence. These patterns are similar
to dependency relations in that they typically repre-
sent the syntactic role of the NP with respect to other
constituents (e.g., subject-of, object-of, and noun ar-
guments).
Semantic features: we use the Stanford NER tag-
ger (Finkel et al, 2005) to determine if the targeted
NP is a named entity, and we use the Sundance
parser (Riloff and Phillips, 2004) to assign seman-
tic class labels to each NP?s head noun.
4 Event Narrative Document Classification
One of our goals was to explore the use of document
genre to permit more aggressive strategies for ex-
tracting role fillers. In this section, we first present
an analysis of the MUC-4 data set which reveals the
distribution of event narratives in the corpus, and
then explain how we train a classifier to automati-
cally identify event narrative stories.
4.1 Manual Analysis
We define an event narrative as an article whose
main focus is on reporting the details of an event.
For the purposes of this research, we are only con-
cerned with events that are relevant to the event ex-
traction task (i.e., terrorism). An irrelevant docu-
ment is an article that does not mention any rele-
vant events. In between these extremes is another
category of documents that briefly mention a rele-
vant event, but the event is not the focus of the ar-
ticle. We will refer to these documents as fleeting
reference documents. Many of the fleeting reference
documents in the MUC-4 corpus are transcripts of
interviews, speeches, or terrorist propaganda com-
muniques that refer to a terrorist event and mention
at least one role filler, but within a discussion about
a different topic (e.g., the political ramifications of a
terrorist incident).
To gain a better understanding of how we might
create a system to automatically distinguish event
narrative documents from fleeting reference docu-
ments, we manually labelled the 116 relevant docu-
ments in our tuning set. This was an informal study
solely to help us understand the nature of these texts.
# of Event # of Fleeting
Narratives Ref. Docs Acc
Gold Standard 54 62
Heuristics 40 55 .82
Table 1: Manual Analysis of Document Types
The first row of Table 1 shows the distribution of
event narratives and fleeting references based on our
?gold standard? manual annotations. We see that
more than half of the relevant documents (62/116)
are not focused on reporting a terrorist event, even
though they contain information about a terrorist
event somewhere in the document.
4.2 Heuristics for Event Narrative
Identification
Our goal is to train a document classifier to automat-
ically identify event narratives. The MUC-4 answer
keys reveal which documents are relevant and irrel-
evant with respect to the terrorism domain, but they
do not tell us which relevant documents are event
narratives and which are fleeting reference stories.
Based on our manual analysis of the tuning set, we
developed several heuristics to help separate them.
We observed two types of clues: the location of
the relevant information, and the density of rele-
vant information. First, we noticed that event nar-
ratives tend to mention relevant information within
the first several sentences, whereas fleeting refer-
ence texts usually mention relevant information only
in the middle or end of the document. Therefore our
first heuristic requires that an event narrative men-
tion a role filler within the first 7 sentences.
Second, event narratives generally have a higher
density of relevant information. We use several cri-
teria to estimate information density because a sin-
gle criterion was inadequate to cover different sce-
1141
narios. For example, some documents mention role
fillers throughout the document. Other documents
contain a high concentration of role fillers in some
parts of the document but no role fillers in other
parts. We developed three density heuristics to ac-
count for different situations. All of these heuristics
count distinct role fillers. The first density heuristic
requires that more than 50% of the sentences contain
at least one role filler ( |RelSents||AllSents| > 0.5) . Figure 2
shows histograms for different values of this ratio in
the event narrative (a) vs. the fleeting reference doc-
uments (b). The histograms clearly show that docu-
ments with a high (> 50%) ratio are almost always
event narratives.
0 .1 .2 .3 .4 .5 .6 .7 .8 .9 10
5
10
15
Ratio of Relevant Sentences
# 
of
 D
oc
um
en
ts
(a)
0 .1 .2 .3 .4 .5 .6 .7 .8 .9 10
5
10
15
Ratio of Relevant Sentences
# 
of
 D
oc
um
en
ts
(b)
Figure 2: Histograms of Density Heuristic #1 in Event
Narratives (a) vs. Fleeting References (b).
A second density heuristic requires that the ratio
of different types of roles filled to sentences be >
50% ( |Roles||AllSents| > 0.5). A third density heuristic
requires that the ratio of distinct role fillers to sen-
tences be > 70% ( |RoleF illers||AllSents| > 0.7). If any of
these three criteria are satisfied, then the document
is considered to have a high density of relevant in-
formation.4
We use these heuristics to label a document as an
event narrative if: (1) it has a high density of relevant
information, and (2) it mentions a role filler within
the first 7 sentences.
The second row of Table 1 shows the performance
of these heuristics on the tuning set. The heuristics
correctly identify 4054 event narratives and
55
62 fleeting
reference stories, to achieve an overall accuracy of
82%. These results are undoubtedly optimistic be-
cause the heuristics were derived from analysis of
the tuning set. But we felt confident enough to move
forward with using these heuristics to generate train-
4Heuristic #1 covers most of the event narratives.
ing data for an event narrative classifier.
4.3 Event Narrative Classifier
The heuristics above use the answer keys to help de-
termine whether a story belongs to the event narra-
tive genre, but our goal is to create a classifier that
can identify event narrative documents without the
benefit of answer keys. So we used the heuristics
to automatically create training data for a classifier
by labelling each relevant document in the training
set as an event narrative or a fleeting reference doc-
ument. Of the 700 relevant documents, 292 were
labeled as event narratives. We then trained a doc-
ument classifier using the 292 event narrative docu-
ments as positive instances and all irrelevent training
documents as negative instances. The 308 relevant
documents that were not identified as event narra-
tives were discarded to minimize noise (i.e., we es-
timate that our heuristics fail to identify 25% of the
event narratives). We then trained an SVM classifier
using bag-of-words (unigram) features.
Table 2 shows the performance of the event nar-
rative classifier on the manually labeled tuning set.
The classifier identified 69% of the event narratives
with 63% precision. Overall accuracy was 81%.
Recall Precision Accuracy
.69 .63 .81
Table 2: Event Narrative Classifier Results
At first glance, the performance of this classifier
is mediocre. However, these results should be inter-
preted loosely because there is not always a clear di-
viding line between event narratives and other doc-
uments. For example, some documents begin with
a specific event description in the first few para-
graphs but then digress to discuss other topics. For-
tunately, it is not essential for TIER to have a per-
fect event narrative classifier since all documents
will be processed by the event sentence recognizer
anyway. The recall of the event narrative classifier
means that nearly 70% of the event narratives will
get additional scrutiny, which should help to find ad-
ditional role fillers. Its precision of 63% means that
some documents that are not event narratives will
also get additional scrutiny, but information will be
extracted only if both the role-specific sentence rec-
ognizer and NP extractors believe they have found
1142
Method PerpInd PerpOrg Target Victim Weapon Average
Baselines
AutoSlog-TS 33/49/40 52/33/41 54/59/56 49/54/51 38/44/41 45/48/46
Semantic Affinity 48/39/43 36/58/45 56/46/50 46/44/45 53/46/50 48/47/47
GLACIER 51/58/54 34/45/38 43/72/53 55/58/56 57/53/55 48/57/52
New Results without document classification
AllSent 25/67/36 26/78/39 34/83/49 32/72/45 30/75/43 30/75/42
EventSent 52/54/53 50/44/47 52/67/59 55/51/53 56/57/56 53/54/54
RoleSent 37/54/44 37/58/45 49/75/59 52/60/55 38/66/48 43/63/51
EventSent+RoleSent 38/60/46 36/63/46 47/78/59 52/64/57 36/66/47 42/66/51
New Results with document classification
DomDoc/EventSent+DomDoc/RoleSent 45/54/49 42/51/46 51/68/58 54/56/55 46/63/53 48/58/52
EventSent+DomDoc/RoleSent 43/59/50 45/61/52 51/77/61 52/61/56 44/66/53 47/65/54
EventSent+ENarrDoc/RoleSent 48/57/52 46/53/50 51/73/60 56/60/58 53/64/58 51/62/56
Table 3: Experimental results, reported as Precision/Recall/F-score
something relevant.
4.4 Domain-relevant Document Classifier
For comparison?s sake, we also created a docu-
ment classifier to identify domain-relevant docu-
ments. That is, we trained a classifier to determine
whether a document is relevant to the domain of
terrorism, irrespective of the style of the document.
We trained an SVM classifier with the same bag-of-
words feature set, using all relevant documents in the
training set as positive instances and all irrelevant
documents as negative instances. We use this clas-
sifier for several experiments described in the next
section.
5 Evaluation
5.1 Data Set and Metrics
We evaluated our approach on a standard benchmark
collection for event extraction systems, the MUC-4
data set (MUC-4 Proceedings, 1992). The MUC-4
corpus consists of 1700 documents with associated
answer key templates. To be consistent with previ-
ously reported results on this data set, we use the
1300 DEV documents for training, 200 documents
(TST1+TST2) as a tuning set and 200 documents
(TST3+TST4) as the test set. Roughly half of the
documents are relevant (i.e., they mention at least 1
terrorist event) and the rest are irrelevant.
We evaluate our system on the five MUC-4
?string-fill? event roles: perpetrator individuals,
perpetrator organizations, physical targets, victims
and weapons. The complete IE task involves tem-
plate generation, which is complex because many
documents have multiple templates (i.e., they dis-
cuss multiple events). Our work focuses on extract-
ing individual facts and not on template generation
per se (e.g., we do not perform coreference resolu-
tion or event tracking). Consequently, our evalua-
tion follows that of other recent work and evaluates
the accuracy of the extractions themselves by match-
ing the head nouns of extracted NPs with the head
nouns of answer key strings (e.g., ?armed guerril-
las? is considered to match ?guerrillas?)5 . Our re-
sults are reported as Precision/Recall/F(1)-score for
each event role separately. We also show an overall
average for all event roles combined.6
5.2 Baselines
As baselines, we compare the performance of our
IE system with three other event extraction sys-
tems. The first baseline is AutoSlog-TS (Riloff,
1996), which uses domain-specific extraction pat-
terns. AutoSlog-TS applies its patterns to every sen-
tence in every document, so does not attempt to
explicitly identify relevant sentences or documents.
The next two baselines are more recent systems:
the (Patwardhan and Riloff, 2007) semantic affin-
ity model and the (Patwardhan and Riloff, 2009)
GLACIER system. The semantic affinity approach
5Pronouns were discarded since we do not perform corefer-
ence resolution. Duplicate extractions with the same head noun
were counted as one hit or one miss.
6We generated the Average scores ourselves by macro-
averaging over the scores reported for the individual event roles.
1143
explicitly identifies event sentences and uses pat-
terns that have a semantic affinity for an event role
to extract role fillers. GLACIER is a probabilistic
model that incorporates both phrasal and sentential
evidence jointly to label role fillers.
The first 3 rows in Table 3 show the results for
each of these systems on the MUC-4 data set. They
all used the same evaluation criteria as our results.
5.3 Experimental Results
The lower portion of Table 3 shows the results of
a variety of event extraction models that we cre-
ated using different components of our system. The
AllSent row shows the performance of our Role
Filler Extractors when applied to every sentence in
every document. This system produced high recall,
but precision was consistently low.
The EventSent row shows the performance of
our Role Filler Extractors applied only to the event
sentences identified by our event sentence classi-
fier. This boosts precision across all event roles, but
with a sharp reduction in recall. We see a roughly
20 point swing from recall to precision. These re-
sults are similar to GLACIER?s results on most event
roles, which isn?t surprising because GLACIER also
incorporates event sentence identification.
The RoleSent row shows the results of our Role
Filler Extractors applied only to the role-specific
sentences identified by our classifiers. We see a 12-
13 point swing from recall to precision compared
to the AllSent row. This result is consistent with
our hypothesis that many role fillers exist in role-
specific contexts that are not event sentences. As ex-
pected, extracting facts from role-specific contexts
that do not necessarily refer to an event is less reli-
able. The EventSent+RoleSent row shows the re-
sults when information is extracted from both types
of sentences. We see slightly higher recall, which
confirms that one set of extractions is not a strict
subset of the other, but precision is still relatively
low.
The next set of experiments incorporates docu-
ment classification as the third layer of text analy-
sis. The DomDoc/EventSent+DomDoc/RoleSent
row shows the results of applying both types of
sentence classifiers only to documents identified as
domain-relevant by the Domain-relevant Document
(DomDoc) Classifier described in Section 4.4. Ex-
tracting information only from domain-relevant doc-
uments improves precision by +6, but also sacrifices
8 points of recall.
The EventSent row reveals that information
found in event sentences has the highest precision,
even without relying on document classification. We
concluded that evidence of an event sentence is
probably sufficient to warrant role filler extraction
irrespective of the style of the document. As we dis-
cussed in Section 4, many documents contain only
a fleeting reference to an event, so it is important
to be able to extract information from those isolated
event descriptions as well. Consequently, we cre-
ated a system, EventSent+DomDoc/RoleSent, that
extracts information from event sentences in all doc-
uments, but extracts information from role-specific
sentences only if they appear in a domain-relevant
document. This architecture captured the best of
both worlds: recall improved from 58% to 65% with
only a one point drop in precision.
Finally, we evaluated the idea of using document
genre as a filter instead of domain relevance. The
last row, EventSent+ENarrDoc/RoleSent, shows
the results of our final architecture which extracts
information from event sentences in all documents,
but extracts information from role-specific sentences
only in Event Narrative documents. This architec-
ture produced the best F1 score of 56. This model in-
creases precision by an additional 4 points and pro-
duces the best balance of recall and precision.
Overall, TIER?s multi-layered extraction architec-
ture produced higher F1 scores than previous sys-
tems on four of the five event roles. The improved
recall is due to the additional extractions from sec-
ondary contexts. The improved precision comes
from our two-pronged strategy of treating event nar-
ratives differently from other documents. TIER ag-
gressively searches for extractions in event narrative
stories but is conservative and extracts information
only from event sentences in all other documents.
5.4 Analysis
We looked through some examples of TIER?s output
to try to gain insight about its strengths and limita-
tions. TIER?s role-specific sentence classifiers did
correctly identify some sentences containing role
fillers that were not classified as event sentences.
Several examples are shown below, with the role
1144
fillers in italics:
(1) ?The victims were identified as David Lecky, director
of the Columbus school, and James Arthur Donnelly.?
(2) ?There were seven children, including four of the
Vice President?s children, in the home at the time.?
(3) ?The woman fled and sought refuge inside the
facilities of the Salvadoran Alberto Masferrer University,
where she took a group of students as hostages, threaten-
ing them with hand grenades.?
(4) ?The FMLN stated that several homes were damaged
and that animals were killed in the surrounding hamlets
and villages.?
The first two sentences identify victims, but the
terrorist event itself was mentioned earlier in the
document. The third sentence contains a perpetrator
(the woman), victims (students), and weapons (hand
grenades) in the context of a hostage situation after
the main event (a bus attack), when the perpetrator
escaped. The fourth sentence describes incidental
damage to civilian homes following clashes between
government forces and guerrillas.
However there is substantial room for improve-
ment in each of TIER?s subcomponents, and many
role fillers are still overlooked. One reason is that it
can be difficult to recognize acts of terrorism. Many
sentences refer to a potentially relevant subevent
(e.g., injury or physical damage) but recognizing
that the event is part of a terrorist incident depends
on the larger discourse. For example, consider the
examples below that TIER did not recognize as
relevant sentences:
(5) ?Later, two individuals in a Chevrolet Opala automo-
bile pointed AK rifles at the students, fired some shots,
and quickly drove away.?
(6) ?Meanwhile, national police members who were
dressed in civilian clothes seized university students
Hugo Martinez and Raul Ramirez, who are still missing.?
(7) ?All labor union offices in San Salvador were looted.?
In the first sentence, the event is described as
someone pointing rifles at people and the perpetra-
tors are referred to simply as individuals. There are
no strong keywords in this sentence that reveal this
is a terrorist attack. In the second sentence, police
are being accused of state-sponsored terrorism when
they seize civilians. The verb ?seize? is common
in this corpus, but usually refers to the seizing of
weapons or drug stashes, not people. The third sen-
tence describes a looting subevent. Acts of looting
and vandalism are not usually considered to be ter-
rorism, but in this article it is in the context of accu-
sations of terrorist acts by government officials.
6 Conclusions
We have presented a new approach to event extrac-
tion that uses three levels of analysis: document
genre classification to identify event narrative sto-
ries, two types of sentence classifiers, and noun
phrase classifiers. A key contribution of our work is
the creation of role-specific sentence classifiers that
can detect role fillers in secondary contexts that do
not directly refer to the event. Another important as-
pect of our approach is a two-pronged strategy that
handles event narratives differently from other doc-
uments. TIER aggressively hunts for role fillers in
event narratives, but is conservative about extract-
ing information from other documents. This strategy
produced improvements in both recall and precision
over previous state-of-the-art systems.
This work just scratches the surface of using doc-
ument genre identification to improve information
extraction accuracy. In future work, we hope to
identify additional types of document genre styles
and incorporate genre directly into the extraction
model. Coreference resolution and discourse anal-
ysis will also be important to further improve event
extraction performance.
7 Acknowledgments
We gratefully acknowledge the support of the Na-
tional Science Foundation under grant IIS-1018314
and the Defense Advanced Research Projects
Agency (DARPA) Machine Reading Program under
Air Force Research Laboratory (AFRL) prime con-
tract no. FA8750-09-C-0172. Any opinions, find-
ings, and conclusion or recommendations expressed
in this material are those of the authors and do not
necessarily reflect the view of the DARPA, AFRL,
or the U.S. government.
1145
References
D. Appelt, J. Hobbs, J. Bear, D. Israel, and M. Tyson.
1993. FASTUS: a finite-state processor for informa-
tion extraction from real-world text. In Proceedings of
the Thirteenth International Joint Conference on Arti-
ficial Intelligence.
R. Bunescu and R. Mooney. 2007. Learning to Extract
Relations from the Web using Minimal Supervision.
In Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics.
M.E. Califf and R. Mooney. 2003. Bottom-up Relational
Learning of Pattern Matching rules for Information
Extraction. Journal of Machine Learning Research,
4:177?210.
H.L. Chieu and H.T. Ng. 2002. A Maximum En-
tropy Approach to Information Extraction from Semi-
Structured and Free Text. In Proceedings of the 18th
National Conference on Artificial Intelligence.
F. Ciravegna. 2001. Adaptive Information Extraction
from Text by Rule Induction and Generalisation. In
Proceedings of the 17th International Joint Confer-
ence on Artificial Intelligence.
J. Finkel, T. Grenager, and C. Manning. 2005. Incor-
porating Non-local Information into Information Ex-
traction Systems by Gibbs Sampling. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics, pages 363?370, Ann Ar-
bor, MI, June.
A. Finn and N. Kushmerick. 2004. Multi-level Boundary
Classification for Information Extraction. In In Pro-
ceedings of the 15th European Conference on Machine
Learning, pages 111?122, Pisa, Italy, September.
D. Freitag and A. McCallum. 2000. Information Ex-
traction with HMM Structures Learned by Stochas-
tic Optimization. In Proceedings of the Seventeenth
National Conference on Artificial Intelligence, pages
584?589, Austin, TX, August.
Dayne Freitag. 1998a. Multistrategy Learning for In-
formation Extraction. In Proceedings of the Fifteenth
International Conference on Machine Learning. Mor-
gan Kaufmann Publishers.
Dayne Freitag. 1998b. Toward General-Purpose Learn-
ing for Information Extraction. In Proceedings of the
36th Annual Meeting of the Association for Computa-
tional Linguistics.
Z. Gu and N. Cercone. 2006. Segment-Based Hidden
Markov Models for Information Extraction. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics, pages
481?488, Sydney, Australia, July.
L. Hirschman. 1998. ?The Evolution of Evaluation:
Lessons from the Message Understanding Confer-
ences. Computer Speech and Language, 12.
S. Huffman. 1996. Learning Information Extraction Pat-
terns from Examples. In Stefan Wermter, Ellen Riloff,
and Gabriele Scheler, editors, Connectionist, Statisti-
cal, and Symbolic Approaches to Learning for Nat-
ural Language Processing, pages 246?260. Springer-
Verlag, Berlin.
H. Ji and R. Grishman. 2008. Refining Event Extraction
through Cross-Document Inference. In Proceedings of
ACL-08: HLT, pages 254?262, Columbus, OH, June.
S. Keerthi and D. DeCoste. 2005. A Modified Finite
Newton Method for Fast Solution of Large Scale Lin-
ear SVMs. Journal of Machine Learning Research.
J. Kim and D. Moldovan. 1993. Acquisition of Semantic
Patterns for Information Extraction from Corpora. In
Proceedings of the Ninth IEEE Conference on Artifi-
cial Intelligence for Applications, pages 171?176, Los
Alamitos, CA. IEEE Computer Society Press.
W. Lehnert, C. Cardie, D. Fisher, E. Riloff, and
R. Williams. 1991. University of Massachusetts: De-
scription of the CIRCUS System as Used for MUC-
3. In Proceedings of the Third Message Understand-
ing Conference (MUC-3), pages 223?233, San Mateo,
CA. Morgan Kaufmann.
Y. Li, K. Bontcheva, and H. Cunningham. 2005. Us-
ing Uneven Margins SVM and Perceptron for Infor-
mation Extraction. In Proceedings of Ninth Confer-
ence on Computational Natural Language Learning,
pages 72?79, Ann Arbor, MI, June.
Shasha Liao and Ralph Grishman. 2010. Using docu-
ment level cross-event inference to improve event ex-
traction. In Proceedings of the 48st Annual Meeting on
Association for Computational Linguistics (ACL-10).
M. Maslennikov and T. Chua. 2007. A Multi-Resolution
Framework for Information Extraction from Free Text.
In Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics.
MUC-4 Proceedings. 1992. Proceedings of the Fourth
Message Understanding Conference (MUC-4). Mor-
gan Kaufmann.
S. Patwardhan and E. Riloff. 2007. Effective Information
Extraction with Semantic Affinity Patterns and Rele-
vant Regions. In Proceedings of 2007 the Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP-2007).
S. Patwardhan and E. Riloff. 2009. A Unified Model of
Phrasal and Sentential Evidence for Information Ex-
traction. In Proceedings of 2009 the Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2009).
E. Riloff and R. Jones. 1999. Learning Dictionaries for
Information Extraction by Multi-Level Bootstrapping.
In Proceedings of the Sixteenth National Conference
on Artificial Intelligence.
1146
E. Riloff and W. Phillips. 2004. An Introduction to the
Sundance and AutoSlog Systems. Technical Report
UUCS-04-015, School of Computing, University of
Utah.
E. Riloff. 1993. Automatically Constructing a Dictio-
nary for Information Extraction Tasks. In Proceedings
of the 11th National Conference on Artificial Intelli-
gence.
E. Riloff. 1996. Automatically Generating Extraction
Patterns from Untagged Text. In Proceedings of the
Thirteenth National Conference on Artificial Intelli-
gence, pages 1044?1049. The AAAI Press/MIT Press.
D. Roth and W. Yih. 2001. Relational Learning via
Propositional Algorithms: An Information Extraction
Case Study. In Proceedings of the Seventeenth In-
ternational Joint Conference on Artificial Intelligence,
pages 1257?1263, Seattle, WA, August.
Satoshi Sekine. 2006. On-demand information ex-
traction. In Proceedings of Joint Conference of the
International Committee on Computational Linguis-
tics and the Association for Computational Linguistics
(COLING/ACL-06.
Y. Shinyama and S. Sekine. 2006. Preemptive Informa-
tion Extraction using Unrestricted Relation Discovery.
In Proceedings of the Human Language Technology
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 304?
311, New York City, NY, June.
S. Soderland, D. Fisher, J. Aseltine, and W. Lehnert.
1995. CRYSTAL: Inducing a conceptual dictionary.
In Proc. of the Fourteenth International Joint Confer-
ence on Artificial Intelligence, pages 1314?1319.
M. Stevenson and M. Greenwood. 2005. A Seman-
tic Approach to IE Pattern Induction. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics, pages 379?386, Ann Ar-
bor, MI, June.
K. Sudo, S. Sekine, and R. Grishman. 2003. An Im-
proved Extraction Pattern Representation Model for
Automatic IE Pattern Acquisition. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics (ACL-03).
R. Yangarber, R. Grishman, P. Tapanainen, and S. Hut-
tunen. 2000. Automatic Acquisition of Domain
Knowledge for Information Extraction. In Proceed-
ings of the Eighteenth International Conference on
Computational Linguistics (COLING 2000).
K. Yu, G. Guan, and M. Zhou. 2005. Resume? Infor-
mation Extraction with Cascaded Hybrid Model. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 499?506,
Ann Arbor, MI, June.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel Methods for Relation
Extraction. Journal of Machine Learning Research, 3.
1147
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 311?316,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Improving Classification of Medical Assertions in Clinical Notes 
  Youngjun Kim Ellen Riloff St?phane M. Meystre School of Computing School of Computing Department of Biomedical Informatics University of Utah University of Utah University of Utah Salt Lake City, UT Salt Lake City, UT Salt Lake City, UT youngjun@cs.utah.edu riloff@cs.utah.edu stephane.meystre@hsc.utah.edu       Abstract 
We present an NLP system that classifies the assertion type of medical problems in clinical notes used for the Fourth i2b2/VA Challenge. Our classifier uses a variety of linguistic fea-tures, including lexical, syntactic, lexico-syntactic, and contextual features. To overcome an extremely unbalanced distribution of asser-tion types in the data set, we focused our efforts on adding features specifically to improve the performance of minority classes. As a result, our system reached 94.17% micro-averaged and 79.76% macro-averaged F1-measures, and showed substantial recall gains on the minority classes.  
1 Introduction Since the beginning of the new millennium, there has been a growing need in the medical community for Natural Language Processing (NLP) technolo-gy to provide computable information from narra-tive text and enable improved data quality and de-cision-making. Many NLP researchers working with clinical text (i.e. documents in the electronic health record) are also realizing that the transition to machine learning techniques from traditional rule-based methods can lead to more efficient ways to process increasingly large collections of clinical narratives. As evidence of this transition, nearly all of the best-performing systems in the Fourth i2b2/VA Challenge (Uzuner and DuVall, 2010) used machine learning methods.  
In this paper, we focus on the medical assertions classification task. Given a medical problem men-tioned in a clinical text, an assertion classifier must look at the context and choose the status of how the medical problem pertains to the patient by as-signing one of six labels: present, absent, hypothet-ical, possible, conditional, or not associated with the patient. The corpus for this task consists of dis-charge summaries from Partners HealthCare (Bos-ton, MA) and Beth Israel Deaconess Medical Cen-ter, as well as discharge summaries and progress notes from the University of Pittsburgh Medical Center (Pittsburgh, PA). Our system performed well in the i2b2/VA Challenge, achieving a micro-averaged F1-measure of 93.01%. However, two of the assertion catego-ries (present and absent) accounted for nearly 90% of the instances in the data set, while the other four classes were relatively infrequent. When we ana-lyzed our results, we saw that our performance on the four minority classes was weak (e.g., recall on the conditional class was 22.22%). Even though the minority classes are not common, they are ex-tremely important to identify accurately (e.g., a medical problem not associated with the patient should not be assigned to the patient).  In this paper, we present our efforts to reduce the performance gap between the dominant asser-tion classes and the minority classes. We made three types of changes to address this issue: we changed the multi-class learning strategy, filtered the training data to remove redundancy, and added new features specifically designed to increase re-call on the minority classes. We compare the per-formance of our new classifier with our original 
311
i2b2/VA Challenge classifier and show that it per-forms substantially better on the minority classes, while increasing overall performance as well. 2 Related Work During the Fourth i2b2/VA Challenge, the asser-tion classification task was tackled by participating researchers. The best performing system (Berry de Bruijn et al, 2011) reached a micro-averaged F1-measure of 93.62%. Their breakdown of F1 scores on the individual classes was: present 95.94%, ab-sent 94.23%, possible 64.33%, conditional 26.26%, hypothetical 88.40%, and not associated with the patient 82.35%. Our system had the 6th best score out of 21 teams, with a micro-averaged F1-measure of 93.01%.     Previously, some researchers had developed sys-tems to recognize specific assertion categories. Chapman et al (2001) created the NegEx algo-rithm, a simple rule-based system that uses regular expressions with trigger terms to determine wheth-er a medical term is absent in a patient. They re-ported 77.8% recall and 84.5% precision for 1,235 medical problems in discharge summaries. Chap-man et al (2007) also introduced the ConText al-gorithm, which extended the NegEx algorithm to detect four assertion categories: absent, hypothet-ical, historical, and not associated with the patient.   Uzuner et al (2009) developed the Statistical As-sertion Classifier (StAC) and showed that a ma-chine learning approach for assertion classification could achieve results competitive with their own implementation of Extended NegEx algorithm (ENegEx). They used four assertion classes: pre-sent, absent, uncertain in the patient, or not asso-ciated with the patient. 3 The Assertion Classifier We approach the assertion classification task as a supervised learning problem. The classifier is giv-en a medical term within a sentence as input and must assign one of the six assertion categories to the medical term based on its surrounding context.    3.1 Pipeline Architecture We built a UIMA (Ferrucci and Lally, 2004; Apache, 2008) based pipeline with multiple com-ponents, as depicted in Figure 1. The architecture includes a section detector (adapted from earlier 
work by Meystre and Haug (2005)), a tokenizer (based on regular expressions to split text on white space characters), a part-of-speech (POS) tagger (OpenNLP (Baldridge et al, 2005) module with trained model from cTAKES (Savova et al, 2010)), a context analyzer (local implementation of the ConText algorithm (Chapman et al, 2001)), and a normalizer based on the LVG (Lexical Vari-ants Generation) (LVG, 2010) annotator from cTAKES to retrieve normalized word forms. 
Figure 1: System Pipeline  The assertion classifier uses features extracted by the subcomponents to represent training and test instances. We used LIBSVM, a library for support vector machines (SVM), (Chang and Lin, 2001) for multi-class classification with the RBF (Radial Basis Function) kernel. 3.2 Original i2b2 Feature Set The assertion classifier that we created for the i2b2/VA Challenge used the features listed below, which we developed by manually examining the training data: Lexical Features: The medical term itself, the three words preceding it, and the three words fol-lowing it. We used the LVG annotator in Lexical Tools (McCray et al, 1994) to normalize each word (e.g., with respect to case and tense). Syntactic Features: Part-of-speech tags of the three words preceding the medical term and the three words following it.  
312
Lexico-Syntactic Features: We also defined features representing words corresponding to sev-eral parts-of-speech in the same sentence as the medical term. The value for each feature is the normalized word string. To mitigate the limited window size of lexical features, we defined one feature each for the nearest preceding and follow-ing adjective, adverb, preposition, and verb, and one additional preceding adjective and preposition and one additional following verb and preposition. Contextual Features: We incorporated the ConText algorithm (Chapman et al, 2001) to de-tect four contextual properties in the sentence: ab-sent (negation), hypothetical, historical, and not associated with the patient. The algorithm assigns one of three values to each feature: true, false, or possible. We also created one feature to represent the Section Header with a string value normalized using (Meystre and Haug, 2005). The system only using contextual features gave reasonable results: F1-measure overall 89.96%, present 91.39%, ab-sent 86.58%, and hypothetical 72.13%.  Feature Pruning: We created an UNKNOWN feature value to cover rarely seen feature values. Lexical feature values that had frequency < 4 and other feature values that had frequency < 2 were all encoded as UNKNOWNs.  3.3 New Features for Improvements After the i2b2/VA Challenge submission, we add-ed the following new features, specifically to try to improve performance on the minority classes: Lexical Features: We created a second set of lexical features that were case-insensitive. We also created three additional binary features for each lexical feature. We computed the average tf-idf score for the words comprising the medical term itself, the average tf-idf score for the three words to its left, and the average tf-idf score for the three words to its right. Each binary feature has a value of true if the average tf-idf score is smaller than a threshold (e.g. 0.5 for the medical term itself), or false otherwise. Finally, we created another binary feature that is true if the medical term contains a word with a negative prefix.1 Lexico-Syntactic Features:  We defined two binary features that check for the presence of a                                                             1 Negative prefixes: ab, de, di, il, im, in, ir, re, un, no, mel, mal, mis. In retrospect, some of these are too general and should be tightened up in the future. 
comma or question mark adjacent to the medical term. We also defined features for the nearest pre-ceding and following modal verb and wh-adverb (e.g., where and when). Finally, we reduced the scope of these features from the entire sentence to a context window of size eight around the medical term.  Sentence Features: We created two binary fea-tures to represent whether a sentence is long (> 50 words) or short (<= 50 words), and whether the sentence contains more than 5 punctuation marks, primarily to identify sentences containing lists. 2 Context Features: We created a second set of ConText algorithm properties for negation restrict-ed to the six word context window around the medical term. According to the assertion annota-tion guidelines, problems associated with allergies were defined as conditional. So we added one bi-nary feature that is true if the section headers con-tain terms related to allergies (e.g., ?Medication allergies?). Feature Pruning: We changed the pruning strategy to use document frequency values instead of corpus frequency for the lexical features, and used document frequency > 1 for normalized words and > 2 for case-insensitive words as thresholds. We also removed 57 redundant in-stances from the training set. Finally, when a med-ical term co-exists with other medical terms (prob-lem concepts) in the same sentence, the others are excluded from the lexical and lexico-syntactic fea-tures. 3.4 Multi-class Learning Strategies Our original i2b2 system used a 1-vs-1 classifica-tion strategy. This approach creates one classifier for each possible pair of labels (e.g., one classifier decides whether an instance is present vs. absent, another decides whether it is present vs. condition-al, etc.). All of the classifiers are applied to a new instance and the label for the instance is deter-mined by summing the votes of the classifiers. However, Huang et al (2001) reported that this approach did not work well for data sets that had highly unbalanced class probabilities.     Therefore we experimented with an alternative 1-vs-all classification strategy. In this approach, we                                                             2 We hoped to help the classifier recognize lists for nega-tion scoping, although no scoping features were added per se. 
313
create one classifier for each type of label using instances with that label as positive instances and instances with any other label as negative instanc-es. The final class label is assigned by choosing the class that was assigned with the highest confidence value (i.e., the classifier?s score). 4 Evaluation After changing to the 1-vs-all multi-class strategy and adding the new feature set, we evaluated our improved system on the test data and compared its performance with our original system. 4.1 Data  The training set includes 349 clinical notes, with 11,967 assertions of medical problems. The test set includes 477 texts with 18,550 assertions. These assertions were distributed as follows (Table 1):    Training (%) Testing (%) Present 67.28    70.22    Absent  21.18    19.46    Hypothetical            5.44    3.87    Possible  4.47    4.76    Conditional 0.86    0.92    Not Patient 0.77    0.78     Table 1: Assertions Distribution 4.2 Results For the i2b2/VA Challenge submission, our system showed good performance, with 93.01% micro-averaged F1-measure. However, the macro F1-measure was much lower because our recall on the minority classes was weak. For example, most of 
the conditional test cases were misclassified as present. Table 2 shows the comparative results of the two systems (named ?i2b2? for the i2b2/VA Challenge system, and ?new? for our improved sys-tem).   Recall Precision F1-measure  i2b2 New i2b2 New i2b2 New Present 97.89 98.07 93.11 94.46 95.44 96.23 Absent 92.99 94.71 94.30 96.31 93.64 95.50 Possible 45.30 54.36 80.00 78.30 57.85 64.17 Conditional 22.22 30.41 90.48 81.25 35.68 44.26 Hypothetical 82.98 87.45 92.82 92.07 87.63 89.70 Not patient  78.62 81.38 100.0 97.52 88.03 88.72 Micro Avg.    93.01 94.17 93.01 94.17 93.01 94.17 Macro Avg. 70.00 74.39 91.79 89.99 76.38 79.76  Table 2: Result Comparison of Test Data  The micro-averaged F1-measure of our new system is 94.17%, which now outperforms the best official score reported for the 2010 i2b2 challenge (which was 93.62%). The macro-averaged F1-measure increased from 76.38% to 79.76% because perfor-mance on the minority classes improved. The F1-measure improved in all classes, but we saw espe-cially large improvements with the possible class (+6.32%) and the conditional class (+8.58%). Alt-hough the improvement on the dominant classes was limited in absolute terms (+.79% F1-measure for present and +1.86% for absent), the relative reduction in error rate was greater than for the mi-nority classes: -29.25% reduction in error rate for absent assertions, -17.32% for present assertions, and -13.3% for conditional assertions.  
 Present Absent Possible Conditional Hypothetical Not patient  R P R P R P R P R P R P i2b2 98.36 93.18 94.52 95.31 48.22 84.59 9.71 100.0 86.18 95.57 55.43 98.08 + 1-vs-all 97.28 94.56 95.07 94.88 57.38 75.25 27.18 77.78 90.32 93.33 72.83 95.71 + Pruning 97.45 94.63 94.91 94.75 60.34 79.26 33.01 70.83 89.40 94.48 69.57 95.52 +Lex+LS+Sen 97.51 94.82 95.11 95.50 63.35 78.74 33.98 71.43 88.63 93.52 70.65 97.01 + Context 97.60 94.94 95.39 95.97 63.72 78.11 35.92 71.15 88.63 93.52 69.57 96.97  Table 3: Cross Validation on Training Data: Results from Applying New Features Cumulatively (Lex=Lexical features; LS=Lexico-Syntactic features; Sen=Sentence features)  
314
4.3 Analysis We performed five-fold cross validation on the training data to measure the impact of each of the four subsets of features explained in Section 3. Ta-ble 3 shows the cross validation results when cu-mulatively adding each set of features. Applying the 1-vs-all strategy showed interesting results: recall went up and precision went down for all classes except present. Although the overall F1-measure remained almost same, it helped to in-crease the recall on the minority classes, and we were able to gain most of the precision back (with-out sacrificing this recall) by adding the new fea-tures.  The new lexical features including negative pre-fixes and binary tf-idf features primarily increased performance on the absent class. Using document frequency to prune lexical features showed small gains in all classes except absent. Sentence fea-tures helped recognize hypothetical assertions, which often occur in relatively long sentences. The possible class benefitted the most from the new lexico-syntactic features, with a 3.38% recall gain. We observed that many possible concepts were preceded by a question mark ('?') in the train-ing corpus. The new contextual features helped detect more conditional cases. Five allergy-related section headers (i.e. ?Allergies?, ?Allergies and Medicine Reactions?, ?Allergies/Sensitivities?, ?Allergy?, and ?Medication Allergies?) were asso-ciated with conditional assertions. Together, all the new features increased recall by 26.21% on the conditional class, 15.5% on possible, and 14.14% on not associated with the patient. 5.   Conclusions  We created a more accurate assertion classifier that now achieves state-of-the-art performance on as-sertion labeling for clinical texts. We showed that it is possible to improve performance on recogniz-ing minority classes by 1-vs-all strategy and richer features designed with the minority classes in mind. However, performance on the minority clas-ses still lags behind the dominant classes, so more work is needed in this area. Acknowledgments We thank the i2b2/VA challenge organizers for their efforts, and gratefully acknowledge the sup-
port and resources of the VA Consortium for Healthcare Informatics Research (CHIR), VA HSR HIR 08-374 Translational Use Case Projects; Utah CDC Center of Excellence in Public Health Infor-matics (Grant 1 P01HK000069-01), the National Science Foundation under grant IIS-1018314, and the University of Utah Department of Biomedical Informatics. We also wish to thank our other i2b2 team members: Guy Divita, Qing Z. Treitler, Doug Redd, Adi Gundlapalli, and Sasikiran Kandula. Finally, we truly appreciate Berry de Bruijn and Colin Cherry for the prompt responses to our in-quiry.  References  Apache UIMA 2008. Available at http://uima.apache.org. Jason Baldridge, Tom Morton, and Gann Bierner. 2005. OpenNLP Maxent Package in Java, Available at: http://incubator.apache.org/opennlp/. Berry de Bruijn, Colin Cherry, Svetlana Kiritchenko, Joel Martin, and Xiaodan Zhu. 2011. Machine-Learned Solutions for Three Stages of Clinical In-formation Extraction: the State of the Art at i2b2 2010. J Am Med Inform Assoc. Chih-Chung Chang and Chih-Jen Lin, LIBSVM: a Li-brary for Support Vector Machines, 2001. Available at http://www.csie.ntu.edu.tw/~cjlin/libsvm. Wendy W. Chapman, Will Bridewell, Paul Hanbury, Gregory F. Cooper, and Bruce G. Buchanan. 2001.   A Simple Algorithm for Identifying Negated Find-ings and Diseases in Discharge Summaries. Journal of Biomedical Informatics, 34:301-310. Wendy W. Chapman, David Chu, and John N. Dowling. 2007. ConText: An Algorithm for Identifying Con-textual Features from Clinical Text. BioNLP 2007: Biological, translational, and clinical language pro-cessing, Prague, CZ. David Ferrucci and Adam Lally. 2004. UIMA: An Ar-chitectural Approach to Unstructured Information Processing in the Corporate Research Environment. Journal of Natural Language Engineering, 10(3-4): 327-348. Tzu-Kuo Huang, Ruby C. Weng, and Chih-Jen Lin. 2006. Generalized Bradley-Terry Models and Mul-ticlass Probability Estimates. Journal of Machine Learning Research, 7:85-115. i2b2/VA 2010 Challenge Assertion Annotation Guidelines. https://www.i2b2.org/NLP/Relations/assets/Assertion%20Annotation%20Guideline.pdf. 
315
LVG (Lexical Variants Generation). 2010. Available at: http://lexsrv2.nlm.nih.gov/LexSysGroup/Projects/lvg. Alexa T. McCray, Suresh Srinivasan, and Allen C. Browne. 1994. Lexical Methods for Managing Varia-tion in Biomedical Terminologies. Proc Annu Symp Comput Appl Med Care.:235?239. St?phane M. Meystre and Peter J. Haug. 2005. Automa-tion of a Problem List Using Natural Language Pro-cessing. BMC Med Inform Decis Mak, 5:30. Guergana K. Savova, James J. Masanz, Philip V. Ogren, Jiaping Zheng, Sunghwan Sohn, Karin C. Kipper- Schuler, and Christopher G. Chute. 2010. Mayo clin-ical Text Analysis and Knowledge Extraction System (cTAKES): architecture, component evaluation and applications. J Am Med Inform Assoc., 17(5):507-513. ?zlem Uzuner and Scott DuVall. 2010. Fourth i2b2/VA Challenge. In http://www.i2b2.org/NLP/Relations/. ?zlem Uzuner, Xiaoran Zhang, and Sibanda Tawanda. 2009. Machine Learning and Rule-based Approaches to Assertion Classification. J Am Med Inform Assoc., 16:109-115.
316
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 81?86,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Domain-Specific Coreference Resolution with Lexicalized Features
Nathan Gilbert and Ellen Riloff
School of Computing
University of Utah
50 S. Central Campus Dr.
Salt Lake City, UT 84112
USA
{ngilbert,riloff}@cs.utah.edu
Abstract
Most coreference resolvers rely heavily on
string matching, syntactic properties, and
semantic attributes of words, but they lack
the ability to make decisions based on in-
dividual words. In this paper, we ex-
plore the benefits of lexicalized features
in the setting of domain-specific corefer-
ence resolution. We show that adding
lexicalized features to off-the-shelf coref-
erence resolvers yields significant perfor-
mance gains on four domain-specific data
sets and with two types of coreference res-
olution architectures.
1 Introduction
Coreference resolvers are typically evaluated on
collections of news articles that cover a wide range
of topics, such as the ACE (ACE03, 2003; ACE04,
2004; ACE05, 2005) and OntoNotes (Pradhan
et al, 2007) data sets. Many NLP applica-
tions, however, involve text analysis for special-
ized domains, such as clinical medicine (Gooch
and Roudsari, 2012; Glinos, 2011), legal text anal-
ysis (Bouayad-Agha et al, 2009), and biological
literature (Batista-Navarro and Ananiadou, 2011;
Castan?o et al, 2002). Learning-based corefer-
ence resolvers can be easily retrained for a spe-
cialized domain given annotated training texts for
that domain. However, we found that retraining
an off-the-shelf coreference resolver with domain-
specific texts showed little benefit.
This surprising result led us to question the na-
ture of the feature sets used by noun phrase (NP)
coreference resolvers. Nearly all of the features
employed by recent systems fall into three cate-
gories: string match and word overlap, syntactic
properties (e.g., appositives, predicate nominals,
parse features, etc.), and semantic matching (e.g.,
gender agreement, WordNet similarity, named en-
tity classes, etc.). Conspicuously absent from most
systems are lexical features that allow the classi-
fier to consider the specific words when making a
coreference decision. A few researchers have ex-
perimented with lexical features, but they achieved
mixed results in evaluations on broad-coverage
corpora (Bengston and Roth, 2008; Bjo?rkelund
and Nugues, 2011; Rahman and Ng, 2011a).
We hypothesized that lexicalized features can
have a more substantial impact in domain-specific
settings. Lexical features can capture domain-
specific knowledge and subtle semantic distinc-
tions that may be important within a domain.
For example, based on the resolutions found in
domain-specific training sets, our lexicalized fea-
tures captured the knowledge that ?tomcat? can
be coreferent with ?plane?, ?UAW? can be coref-
erent with ?union?, and ?anthrax? can be coref-
erent with ?diagnosis?. Capturing these types of
domain-specific information is often impossible
using only general-purpose resources. For exam-
ple, WordNet defines ?tomcat? only as an animal,
does not contain an entry for ?UAW?, and catego-
rizes ?anthrax? and ?diagnosis? very differently.1
In this paper, we evaluate the impact of lexi-
calized features on 4 domains: management suc-
cession (MUC-6 data), vehicle launches (MUC-7
data), disease outbreaks (ProMed texts), and ter-
rorism (MUC-4 data). We incorporate lexical-
ized feature sets into two different coreference ar-
chitectures: Reconcile (Stoyanov et al, 2010), a
pairwise coreference classifier, and Sieve (Raghu-
nathan et al, 2010), a rule-based system. Our re-
sults show that lexicalized features significantly
improve performance in all four domains and in
both types of coreference architectures.
2 Related Work
We are not the first researchers to use lexicalized
features for coreference resolution. However, pre-
1WordNet defines ?anthrax? as a disease (condition/state)
and ?diagnosis? as an identification (discovery event).
81
PPPPPPTrain
Test MUC-6 MUC-7 Promed MUC-4
P R F P R F P R F P R F
MUC-6 80.79 62.71 70.61 84.33 61.74 71.29 83.54 70.34 76.37 80.22 60.81 69.18
MUC-7 74.78 65.59 69.88 82.73 64.09 72.23 85.29 71.82 77.98 77.35 64.19 70.16
Promed 73.60 64.20 68.60 82.88 63.37 71.82 80.31 72.66 76.29 74.52 65.65 69.80
MUC-4 69.27 65.66 67.42 71.49 67.22 69.29 76.92 74.25 75.56 71.76 67.37 69.50
Table 1: Cross-domain B3 (Bagga and Baldwin, 1998) results for Reconcile with its general feature set.
The Paired Permutation test (Pesarin, 2001) was used for statistical significance testing and gray cells
represent results that are not significantly different from the best result.
vious work has evaluated the benefit of lexical fea-
tures only for broad-coverage data sets.
Bengston and Roth (2008) incorporated a mem-
orization feature to learn which entities can re-
fer to one another. They created a binary fea-
ture for every pair of head nouns, including pro-
nouns. They reported no significant improvement
from these features on the ACE 2004 data.
Rahman and Ng (2011a) also utilized lexical
features, going beyond strict memorization with
methods to combat data sparseness and incorpo-
rating semantic information. They created a fea-
ture for every ordered pair of head nouns (for
pronouns and nominals) or full NPs (for proper
nouns). Semi-lexical features were also used when
one NP was a Named Entity, and unseen features
were used when the NPs were not in the training
set. Their features did yield improvements on both
the ACE 2005 and OntoNotes-2 data, but the semi-
lexical features included Named Entity classes as
well as word-based features.
Rahman and Ng (2011b) explored the use of
lexical features in greater detail and showed their
benefit on the ACE05 corpus independent of, and
combined with, a conventional set of coreference
features. The ACE05 corpus is drawn from six
sources (Newswire, Broadcast News, Broadcast
Conversations, Conversational Telephone Speech,
Webblogs, and Usenet). The authors experi-
mented with utilizing lexical information drawn
from different sources. The results showed that
the best performance came from training and test-
ing with lexical knowledge drawn from the same
source. Although our approach is similar, this pa-
per focuses on learning lexical information from
different domains as opposed to the different gen-
res found in the six sources of the ACE05 corpus.
Bjo?rkelund and Nugues (2011) used lexical
word pairs for the 2011 CoNLL Shared Task,
showing significant positive impact on perfor-
mance. They used over 2000 annotated docu-
ments from the broad-coverage OntoNotes corpus
for training. Our work aims to show the benefit of
lexical features using much smaller training sets
(< 50 documents) focused on specific domains.
Lexical features have also been used for slightly
different purposes. Florian et al (2004) utilized
lexical information such as mention spelling and
context for entity tracking in ACE. Ng (2007) used
lexical information to assess the likelihood of a
noun phrase being anaphoric, but this did not show
clear improvements on ACE data.
There has been previous work on domain-
specific coreference resolution for several do-
mains, including biological literature (Castan?o et
al., 2002; Liang and Lin, 2005; Gasperin and
Briscoe, 2008; Kim et al, 2011; Batista-Navarro
and Ananiadou, 2011), clinical medicine (He,
2007; Zheng et al, 2011; Glinos, 2011; Gooch and
Roudsari, 2012) and legal documents (Bouayad-
Agha et al, 2009). In addition, BABAR (Bean and
Riloff, 2004) used contextual role knowledge for
coreference resolution in the domains of terrorism
and natural disasters. But BABAR acquired and
used lexical information to match the compatibil-
ity of contexts surrounding NPs, not the NPs them-
selves. To the best of our knowledge, our work is
the first to examine the impact of lexicalized fea-
tures for domain-specific coreference resolution.
3 Exploiting Lexicalized Features
Table 1 shows the performance of a learning-based
coreference resolver, Reconcile (Stoyanov et al,
2010), with its default feature set using different
combinations of training and testing data. Recon-
cile does not include any lexical features, but does
contain over 60 general features covering seman-
tic agreement, syntactic constraints, string match
and recency.
Each row represents a training set, each column
represents a test set, and each cell shows precision
(P), recall (R), and F score results under the B3
metric when using the corresponding training and
test data. The best results for each test set appear
82
MUC-6 MUC-7 ProMED MUC-4
P R F P R F P R F P R F
Reconcile 80.79 62.71 70.61 82.73 64.09 72.23 80.31 72.66 76.29 71.76 67.37 69.50
+LexLookup 87.01 63.40 73.35 87.39 62.86 73.12 86.66 70.95 78.02 82.89 67.53 74.42
+LexSets 86.50 63.76 73.41 85.86 64.35 73.56 86.19 72.14 78.54 81.98 67.73 74.18
Sieve 92.20 61.70 73.90 91.46 59.59 72.16 94.43 67.25 78.55 91.30 59.84 72.30
+LexBegin 91.22 62.97 74.51 91.24 60.28 72.59 93.51 69.15 79.51 89.01 62.84 73.67
+LexEnd 90.59 63.47 74.64 91.17 60.56 72.78 93.99 68.87 79.49 89.04 64.03 74.47
Table 2: B3 results for baselines and lexicalized feature sets across four domains.
in boldface.
We performed statistical significance testing us-
ing the Paired Permutation test (Pesarin, 2001) and
the gray cells represent results where there was
not significant difference from the best results in
the same column. If just one cell is gray in a col-
umn, that indicates the result was significantly bet-
ter than the other results in the same column with
p ? 0.05.
Table 1 does not show much benefit from train-
ing on the same domain as the test set. Three
different training sets produce F scores that are
not significantly different for both the MUC-6
and MUC-4 test data. For ProMed, training on
the MUC-7 data yields significantly better results
than training on all the other data sets, includ-
ing ProMed texts! Based on these results, it
would seem that training on the MUC-7 texts is
likely to yield the best results no matter what do-
main you plan to use the coreference resolver for.
The goal of our work is to investigate whether
lexical features can extract additional knowledge
from domain-specific training texts to help tailor
a coreference resolver to perform better for a spe-
cific domain.
3.1 Extracting Coreferent Training Pairs
We adopt the terminology introduced by Stoyanov
et al (2009) to define a coreference element (CE)
as a noun phrase that can participate in a corefer-
ence relation based on the task definition.
Each training document has manually annotated
gold coreference chains corresponding to the sets
of CEs that are coreferent. For each CE in a gold
chain, we pair that CE with all of the other CEs in
the same chain. We consider the coreference rela-
tion to be bi-directional, so we don?t retain infor-
mation about which CE was the antecedent. We
do not extract CE pairs that share the same head
noun because they are better handled with string
match. For nominal NPs, we retain only the head
noun, but we use the entire NP for proper names.
We discard pairs that include a pronoun, and nor-
malize strings to lower case for consistency.
3.2 Lexicalized Feature Sets
We explore two ways to capture lexicalized infor-
mation as features. The first approach indicates
whether two CEs have ever been coreferent in the
training data. We create a single feature called
LEXLOOKUP(X,Y) that receives a value of 1 when
x and y have been coreferent at least twice, or
a value of 0 otherwise.2 LEXLOOKUP(X,Y) is a
single feature that captures all CE pairs that were
coreferent in the training data.
We also created set-based features that capture
the set of terms that have been coreferent with a
particular CE. The CorefSet(x) is the set of CEs
that have appeared in the same coreference chain
as mention x at least twice.
We create a set of binary-valued features
LEXSET(X,Y), one for each CE x in the training
data. Given a pair of CEs, x and y, LEXSET(X,Y)
= 1 if y ? CorefSet(x), or 0 otherwise. The ben-
efit of the set-based features over a single mono-
lithic feature is that the classifier has one set-based
feature for each mention found in the training data,
so it can learn to handle individual terms differ-
ently.
We also tried encoding a separate feature for
each distinct pair of words, analogous to the mem-
orization feature in Bengston and Roth (2008).
This did not improve performance as much as the
other feature representations presented here.
4 Evaluation
4.1 Data Sets
We evaluated the performance of lexicalized fea-
tures on 4 domain-specific corpora including two
standard coreference benchmarks, the MUC-6 and
MUC-7 data sets. The MUC-6 domain is manage-
ment succession and consists of 30 training texts
and 30 test texts. The MUC-7 domain is vehicle
2We require a frequency ? 2 to minimize overfitting be-
cause many cases occur only once in the training data.
83
launches and consists of 30 training texts and 20
test texts. We used these standard train/test splits
to be consistent with previous work.
We also created 2 new coreference data sets
which we will make freely available. We
manually annotated 45 ProMed-mail articles
(www.promedmail.org) about disease outbreaks
and 45 MUC-4 texts about terrorism, following
the MUC guidelines (Hirschman, 1997). Inter-
annotator agreement between two annotators was
.77 (?) on ProMed and .84 (MUC F Score)(Villain
et al, 1995) on both ProMed and MUC-4.3 We
performed 5-fold cross-validation on both data
sets and report the micro-averaged results.
Gold CE spans were used in all experiments to
factor out issues with markable identification and
anaphoricity across the different domains.
4.2 Coreference Resolution Models
We conducted experiments using two coreference
resolution architectures. Reconcile4 (Stoyanov et
al., 2010) is a freely available pairwise mention
classifier. For classification, we chose Weka?s
(Witten and Frank, 2005) Decision Tree learner
inside Reconcile. Reconcile contains roughly 60
features (none lexical), largely modeled after Ng
and Cardie (2002). We modified Reconcile?s Sin-
gle Link clustering scheme to enforce an addi-
tional rule that non-overlapping proper names can-
not be merged into the same chain.
We also conducted experiments with the Sieve
coreference resolver, which applies high precision
heuristic rules to incrementally build coreference
chains. We implemented the LEXLOOKUP(X,Y)
feature as an additional heuristic rule. We tried
inserting this heuristic before Sieve?s other rules
(LexBegin), and also after Sieve?s other rules
(LexEnd).
4.3 Experimental Results
Table 2 presents results for Reconcile trained with
and without lexical features and when adding
a lexical heuristic with data drawn from same-
domain texts to Sieve.
The first row shows the results without the lex-
icalized features (from Table 1). All F scores
for Reconcile with lexicalized features are signifi-
cantly better than without these features based on
the Paired Permutation test (Pesarin, 2001) with
3We also computed ? on MUC-4, but unfortunately the
score and original data were lost.
4http://www.cs.utah.edu/nlp/reconcile/
p ? 0.05. MUC-4 showed the largest gain for
Reconcile, with the F score increasing from 69.5
to over 74. For most domains, adding the lexical
features to Reconcile substantially increased pre-
cision with comparable levels of recall.
The bottom half of Table 2 contains the results
of adding a lexical heuristic to Sieve. The first
row shows the default system with no lexical in-
formation. All F scores with the lexical heuristic
are significantly better than without it. In Sieve?s
high-precision coreference architecture, the lexi-
cal heuristic yields additional recall gains without
sacrificing much precision.
ACE 2004
P R F
Reconcile 70.59 83.09 76.33
+LexLookup 71.32 82.93 76.69
+LexSets 71.44 83.45 76.98
Sieve 90.09 74.23 81.39
+LexBegin 86.54 75.43 80.61
+LexEnd 87.00 75.45 80.82
Table 3: B3 results for baselines and lexicalized
feature sets on the broad-coverage ACE 2004 data
set.
Table 3 shows the results for Reconcile and
Sieve when training and testing on the ACE 2004
data. Here, we see little improvement from adding
lexical information. For Reconcile, the small dif-
ferences in F scores are not statistically significant.
For Sieve, the unlexicalized system yields a signif-
icantly higher F score than when adding the lexi-
cal heuristic. These results support our hypothesis
that lexicalized information can be beneficial for
capturing domain-specific word associations, but
may not be as helpful in a broad-coverage setting
where the language covers a diverse set of topics.
Table 4 shows a re-evaluation of the cross-
domain experiments from Table 1 for Reconcile
with the LexSet features added. The bottom half
of the table shows cross-domain experiments for
Sieve using the lexical heuristic at the end of its
rule set (LexEnd). Results are presented using
both the B3 metric and the MUC Score (Villain
et al, 1995).
Training and testing on the same domain al-
ways produced the highest recall scores for MUC-
7, ProMed, and MUC-4 when utilizing lexical
features. In all cases, lexical features acquired
from same-domain texts yield results that are ei-
ther clearly the best or not significantly different
from the best.
84
PPPPPPTrain
Test MUC-6 MUC-7 Promed MUC-4
P R F P R F P R F P R F
Reconcile (B3 Score)
MUC-6 86.50 63.76 73.41 90.44 60.75 72.68 89.28 68.14 77.29 84.05 60.61 70.44
MUC-7 80.65 63.42 71.01 85.86 64.46 73.56 89.41 70.05 78.55 80.61 63.26 70.89
Promed 81.69 62.73 70.96 88.32 62.79 73.40 86.19 72.14 78.54 84.81 62.58 72.02
MUC-4 81.20 62.34 70.53 87.23 63.13 73.25 87.52 71.11 78.46 81.98 67.73 74.18
Reconcile (MUC Score)
MUC-6 89.56 71.17 79.32 90.85 67.43 77.41 89.61 65.67 75.79 88.27 66.98 76.16
MUC-7 86.14 72.22 78.57 89.56 72.01 79.83 89.34 68.08 77.27 87.30 70.22 77.83
Promed 86.92 70.68 77.97 90.93 70.33 79.31 88.54 69.55 77.90 88.83 68.89 78.23
MUC-4 85.72 70.50 77.37 88.78 71.24 79.05 88.24 68.18 77.55 87.89 74.18 80.45
Sieve (B3 Score)
MUC-6 90.59 63.47 74.64 91.20 59.91 72.32 94.30 67.25 78.51 91.30 59.90 72.34
MUC-7 91.62 63.67 75.13 91.17 60.56 72.78 94.43 67.35 78.62 91.14 60.44 72.68
Promed 92.14 61.70 73.90 91.46 59.93 72.41 93.99 68.87 79.49 91.27 60.76 72.96
MUC-4 91.76 61.88 73.91 91.26 59.93 72.34 94.30 67.35 78.58 89.04 64.03 74.47
Sieve (MUC Score)
MUC-6 91.80 70.87 79.99 91.38 65.52 76.32 92.08 64.71 76.01 90.38 66.98 77.10
MUC-7 91.82 69.70 79.25 91.68 66.36 76.99 92.20 64.86 76.15 90.71 67.09 77.13
Promed 91.99 69.15 78.95 91.68 65.52 76.42 91.70 66.33 76.98 90.85 67.09 77.18
MUC-4 91.79 69.39 79.03 91.48 65.52 76.36 92.00 64.86 76.08 90.31 69.62 78.62
Table 4: Cross-domain B3 and MUC results for Reconcile and Sieve with lexical features. Gray cells
represent results that are not significantly different from the best results in the column at the 0.05 p-level.
For MUC-6 and MUC-7, the highest F score re-
sults almost always come from training on same-
domain texts, although in some cases these re-
sults are not significantly different from training
on other domains. Lexical features can yield im-
provements when training on a different domain if
there is overlap in the vocabulary across the do-
mains. For the ProMed domain, the Sieve system
performs significantly better, under both metrics,
with same-domain lexical features than with lexi-
cal features acquired from a different domain. For
Reconcile, there is not a significant difference in
the F score for ProMed when training on ProMed,
MUC-4, or MUC-7. In the MUC-4 domain, using
same-domain lexical information always produces
the best F score, under both metrics and in both
coreference systems.
5 Conclusions
We explored the use of lexical information for
domain-specific coreference resolution using 4
domain-specific data sets and 2 coreference re-
solvers. Lexicalized features consistently im-
proved performance for all of the domains and in
both coreference architectures. We see benefits
from lexicalized features in cross-domain training,
but the gains are often more substantial when uti-
lizing same-domain lexical knowledge.
In the future, we plan to explore additional types
of lexical information to benefit domain-specific
coreference resolution.
Acknowledgments
This material is based upon work supported by
the National Science Foundation under Grant
No. IIS-1018314 and the Defense Advanced Re-
search Projects Agency (DARPA) Machine Read-
ing Program under Air Force Research Laboratory
(AFRL) prime contract no. FA8750-09-C-0172.
Any opinions, findings, and conclusion or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the view
of the DARPA, AFRL, or the U.S. government.
References
ACE03. 2003. NIST ACE evaluation website. In
http://www.nist.gov/speech/tests/ace/2003.
ACE04. 2004. NIST ACE evaluation website. In
http://www.nist.gov/speech/tests/ace/2004.
ACE05. 2005. NIST ACE evaluation website. In
http://www.nist.gov/speech/tests/ace/2005.
Amit Bagga and Breck Baldwin. 1998. Entity-based
cross-document coreference using the Vector Space
Model. Proceedings of the 17th international con-
ference on Computational Linguistics (COLING).
Riza Theresa Batista-Navarro and Sophia Ananiadou.
2011. Building a coreference-annotated corpus
from the domain of biochemistry. In Proceedings of
BioNLP 2011 Workshop, BioNLP ?11, pages 83?91.
David Bean and Ellen Riloff. 2004. Unsupervised
learning of Contextual Role Knowledge for coref-
erence resolution. Proceedings of the HLT/NAACL
2004.
85
Eric Bengston and Dan Roth. 2008. Understanding the
value of features for coreference resolution. Empir-
ical Methods in Natural Language Processing.
Anders Bjo?rkelund and Pierre Nugues. 2011. Explor-
ing lexicalized features for coreference resolution.
Proceedings of the Fifteenth Conference on Compu-
tational Natural Language Learning: Shared Task,
pages 45?50.
Nadjet Bouayad-Agha, Gerard Casamayor, Gabriela
Ferraro, Simon Mille, Vanesa Vidal, and Leo Wan-
ner. 2009. Improving the comprehension of legal
documentation: the case of patent claims. In Pro-
ceedings of the 12th International Conference on Ar-
tificial Intelligence and Law, pages 78?87.
Jose? Castan?o, Jason Zhang, and James Pustejovsky.
2002. Anaphora resolution in biomedical literature.
International Symposium on Reference Resolution.
Radu Florian, Hany Hassan, Abraham Ittycheriah,
Hongyan Jing, Nanda Kambhatla, Xiaoqiang Luo,
Nicolas Nicolov, Salim Roukos, and T Zhang. 2004.
A statistical model for multilingual entity detection
and tracking. HLT-NAACL.
Caroline Gasperin and Ted Briscoe. 2008. Statistical
anaphora resolution in biomedical texts. Proceed-
ings of the 22nd Annual Conference on Computa-
tional Linguistics, pages 257?264.
Demetrios G. Glinos. 2011. A search based method for
clinical text coreference resolution. In Proceedings
of the Fifth i2b2/VA Track on Challenges in Natural
Language Processing for Clinical Data (i2b2 2011).
Phil Gooch and Abdul Roudsari. 2012. Lexical pat-
terns, features and knowledge resources for corefer-
ence resolution in clinical notes. Journal of Biomed-
ical Informatics, 45.
Tian Ye He. 2007. Coreference resolution on entities
and events for hospital discharge summaries. Ph.D.
thesis, Massachusetts Institute of Technology.
Lynette Hirschman. 1997. MUC-7 task definition.
Proceedings of MUC-7.
Youngjun Kim, Ellen Riloff, and Nathan Gilbert. 2011.
The taming of Reconcile as a Biomedical corefer-
ence resolver. ACL/HLT 2011 Workshop on Biomed-
ical Natural Language Processing (BioNLP 2011)
Shared Task Paper.
Tyne Liang and Yu-Hsiang Lin. 2005. Anaphora
resolution for biomedical literature by exploiting
multiple resources. Natural Language Processing?
IJCNLP 2005, pages 742?753.
Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
Proceedings of the 40th Annual Meeting of the ACL,
pages 104?111.
Vincent Ng. 2007. Shallow semantics for corefer-
ence resolution. Proceedings of the Twentieth Inter-
national Joint Conference on Artificial Intelligence
(IJCAI-07), pages 1689?1694.
Fortunato Pesarin. 2001. Multivariate permutation
tests: with applications in biostatistics, volume 240.
Wiley Chichester.
Sameer S. Pradhan, Lance Ramshaw, Ralph
Weischedel, Jessice MacBride, and Linnea Micci-
ulla. 2007. Unrestricted coreference: Identifying
entities and events in ontonotes. In Proceedings
of the International Conference on Semantic
Computing.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A Multi-
Pass Sieve for coreference resolution. Empirical
Methods in Natural Langugage Processing 2010.
Altaf Rahman and Vincent Ng. 2011a. Coreference
resolution with world knowledge. Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics and Human Language Tech-
nologies (ACL-HLT), pages 814?824.
Altaf Rahman and Vincent Ng. 2011b. Narrowing the
modelling gap: A cluster-ranking approach to coref-
erence resolution. Journal of Artificial Intelligence
Research.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase
coreference resolution: Making sense of the State-
of-the-Art. Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th IJC-
NLP (ACL-IJCNLP 2009).
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2010. Coreference resolution with
Reconcile. Proceedings of the Joint Conference of
the 48th Annual Meeting of the Association for Com-
putational Linguistics (ACL 2010).
Marc Villain, John Aberdeen, John Berger, Dennis
Connolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. Proceedings
of the 6th conference on Message understanding.
Ian H. Witten and Eibe Frank. 2005. Data Mining:
Practical machine learning tools and techniques.
Morgan Kaufmann, 2nd edition.
Jiaping Zheng, Wendy Chapman, Rebecca Crowley,
and Guergana Savova. 2011. Coreference resolu-
tion: A review of general methodologies and appli-
cations in the clinical domain. Journal of Biomedi-
cal Informatics, 44:1113?1122.
86
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 199?208,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Ensemble-based Semantic Lexicon Induction for Semantic Tagging
Ashequl Qadir
University of Utah
School of Computing
Salt Lake City, UT 84112, USA
asheq@cs.utah.edu
Ellen Riloff
University of Utah
School of Computing
Salt Lake City, UT 84112, USA
riloff@cs.utah.edu
Abstract
We present an ensemble-based framework
for semantic lexicon induction that incorpo-
rates three diverse approaches for semantic
class identification. Our architecture brings
together previous bootstrapping methods for
pattern-based semantic lexicon induction and
contextual semantic tagging, and incorpo-
rates a novel approach for inducing semantic
classes from coreference chains. The three
methods are embedded in a bootstrapping ar-
chitecture where they produce independent
hypotheses, consensus words are added to the
lexicon, and the process repeats. Our results
show that the ensemble outperforms individ-
ual methods in terms of both lexicon quality
and instance-based semantic tagging.
1 Introduction
One of the most fundamental aspects of meaning is
the association between words and semantic cate-
gories, which allows us to understand that a ?cow?
is an animal and a ?house? is a structure. We will
use the term semantic lexicon to refer to a dictionary
that associates words with semantic classes. Se-
mantic dictionaries are useful for many NLP tasks,
as evidenced by the widespread use of WordNet
(Miller, 1990). However, off-the-shelf resources are
not always sufficient for specialized domains, such
as medicine, chemistry, or microelectronics. Fur-
thermore, in virtually every domain, texts contain
lexical variations that are often missing from dic-
tionaries, such as acronyms, abbreviations, spelling
variants, informal shorthand terms (e.g., ?abx? for
?antibiotics?), and composite terms (e.g., ?may-
december? or ?virus/worm?). To address this prob-
lem, techniques have been developed to automate
the construction of semantic lexicons from text cor-
pora using bootstrapping methods (Riloff and Shep-
herd, 1997; Roark and Charniak, 1998; Phillips and
Riloff, 2002; Thelen and Riloff, 2002; Ng, 2007;
McIntosh and Curran, 2009; McIntosh, 2010), but
accuracy is still far from perfect.
Our research explores the use of ensemble meth-
ods to improve the accuracy of semantic lexicon in-
duction. Our observation is that semantic class as-
sociations can be learned using several fundamen-
tally different types of corpus analysis. Bootstrap-
ping methods for semantic lexicon induction (e.g.,
(Riloff and Jones, 1999; Thelen and Riloff, 2002;
McIntosh and Curran, 2009)) collect corpus-wide
statistics for individual words based on shared con-
textual patterns. In contrast, classifiers for semantic
tagging (e.g., (Collins and Singer, 1999; Niu et al,
2003; Huang and Riloff, 2010)) label word instances
and focus on the local context surrounding each in-
stance. The difference between these approaches is
that semantic taggers make decisions based on a sin-
gle context and can assign different labels to differ-
ent instances, whereas lexicon induction algorithms
compile corpus statistics from multiple instances of
a word and typically assign each word to a single
semantic category.1 We also hypothesize that coref-
erence resolution can be exploited to infer semantic
1This approach would be untenable for broad-coverage se-
mantic knowledge acquisition, but within a specialized domain
most words have a dominant word sense. Our experimental re-
sults support this assumption.
199
class labels. Intuitively, if we know that two noun
phrases are coreferent, then they probably belong to
the same high-level semantic category (e.g., ?dog?
and ?terrier? are both animals).
In this paper, we present an ensemble-based
framework for semantic lexicon induction. We in-
corporate a pattern-based bootstrapping method for
lexicon induction, a contextual semantic tagger, and
a new coreference-based method for lexicon induc-
tion. Our results show that coalescing the decisions
produced by diverse methods produces a better dic-
tionary than any individual method alone.
A second contribution of this paper is an analysis
of the effectiveness of dictionaries for semantic tag-
ging. In principle, an NLP system should be able to
assign different semantic labels to different senses
of a word. But within a specialized domain, most
words have a dominant sense and we argue that us-
ing domain-specific dictionaries for tagging may be
equally, if not more, effective. We analyze the trade-
offs between using an instance-based semantic tag-
ger versus dictionary lookup on a collection of dis-
ease outbreak articles. Our results show that the in-
duced dictionaries yield better performance than an
instance-based semantic tagger, achieving higher ac-
curacy with comparable levels of recall.
2 Related Work
Several techniques have been developed for seman-
tic class induction (also called set expansion) using
bootstrapping methods that consider co-occurrence
statistics based on nouns (Riloff and Shepherd,
1997), syntactic structures (Roark and Charniak,
1998; Phillips and Riloff, 2002), and contextual pat-
terns (Riloff and Jones, 1999; Thelen and Riloff,
2002; McIntosh and Curran, 2008; McIntosh and
Curran, 2009). To improve the accuracy of in-
duced lexicons, some research has incorporated neg-
ative information from human judgements (Vyas
and Pantel, 2009), automatically discovered neg-
ative classes (McIntosh, 2010), and distributional
similarity metrics to recognize concept drift (McIn-
tosh and Curran, 2009). Phillips and Riloff (2002)
used co-training (Blum and Mitchell, 1998) to ex-
ploit three simple classifiers that each recognized a
different type of syntactic structure. The research
most closely related to ours is an ensemble-based
method for automatic thesaurus construction (Cur-
ran, 2002). However, that goal was to acquire fine-
grained semantic information that is more akin to
synonymy (e.g., words similar to ?house?), whereas
we associate words with high-level semantic classes
(e.g., a ?house? is a transient structure).
Semantic class tagging is closely related to named
entity recognition (NER) (e.g., (Bikel et al, 1997;
Collins and Singer, 1999; Cucerzan and Yarowsky,
1999; Fleischman and Hovy, 2002)). Some boot-
strapping methods have been used for NER (e.g.,
(Collins and Singer, 1999; Niu et al, 2003) to
learn from unannotated texts. However, most NER
systems will not label nominal noun phrases (e.g.,
they will not identify ?the dentist? as a person)
or recognize semantic classes that are not associ-
ated with proper named entities (e.g., symptoms).2
ACE mention detection systems (e.g., (ACE, 2007;
ACE, 2008)) can label noun phrases that are asso-
ciated with 5-7 semantic classes and are typically
trained with supervised learning. Recently, (Huang
and Riloff, 2010) developed a bootstrapping tech-
nique that induces a semantic tagger from unanno-
tated texts. We use their system in our ensemble.
There has also been work on extracting semantic
class members from the Web (e.g., (Pas?ca, 2004; Et-
zioni et al, 2005; Kozareva et al, 2008; Carlson et
al., 2009)). This line of research is fundamentally
different from ours because these techniques benefit
from the vast repository of information available on
the Web and are therefore designed to harvest a wide
swath of general-purpose semantic information. Our
research is aimed at acquiring domain-specific se-
mantic dictionaries using a collection of documents
representing a specialized domain.
3 Ensemble-based Semantic Lexicon
Induction
3.1 Motivation
Our research combines three fundamentally differ-
ent techniques into an ensemble-based bootstrap-
ping framework for semantic lexicon induction:
pattern-based dictionary induction, contextual se-
mantic tagging, and coreference resolution. Our
motivation for using an ensemble of different tech-
2Some NER systems will handle special constructions such
as dates and monetary amounts.
200
niques is driven by the observation that these meth-
ods exploit different types of information to infer se-
mantic class knowledge. The coreference resolver
uses features associated with coreference, such as
syntactic constructions (e.g., appositives, predicate
nominals), word overlap, semantic similarity, prox-
imity, etc. The pattern-based lexicon induction al-
gorithm uses corpus-wide statistics gathered from
the contexts of all instances of a word and compares
them with the contexts of known category members.
The contextual semantic tagger uses local context
windows around words and classifies each word in-
stance independently from the others.
Since each technique draws its conclusions from
different types of information, they represent inde-
pendent sources of evidence to confirm whether a
word belongs to a semantic class. Our hypothe-
sis is that, combining these different sources of ev-
idence in an ensemble-based learning framework
should produce better accuracy than using any one
method alone. Based on this intuition, we create
an ensemble-based bootstrapping framework that it-
eratively collects the hypotheses produced by each
individual learner and selects the words that were
hypothesized by at least 2 of the 3 learners. This
approach produces a bootstrapping process with
improved precision, both at the critical beginning
stages of the bootstrapping process and during sub-
sequent bootstrapping iterations.
3.2 Component Systems in the Ensemble
In the following sections, we describe each of the
component systems used in our ensemble.
3.2.1 Pattern-based Lexicon Induction
The first component of our ensemble is Basilisk
(Thelen and Riloff, 2002), which identifies nouns
belonging to a semantic class based on collec-
tive information over lexico-syntactic pattern con-
texts. The patterns are automatically generated us-
ing AutoSlog-TS (Riloff, 1996). Basilisk begins
with a small set of seed words for each seman-
tic class and a collection of unannotated documents
for the domain. In an iterative bootstrapping pro-
cess, Basilisk identifies candidate nouns, ranks them
based on its scoring criteria, selects the 5 most confi-
dent words for inclusion in the lexicon, and this pro-
cess repeats using the new words as additional seeds
in subsequent iterations.
3.2.2 Lexicon Induction with a Contextual
Semantic Tagger
The second component in our ensemble is a con-
textual semantic tagger (Huang and Riloff, 2010).
Like Basilisk, the semantic tagger also begins with
seed nouns, trains itself on a large collection of
unannotated documents using bootstrapping, and it-
eratively labels new instances. This tagger labels
noun instances and does not produce a dictionary.
To adapt it for our purposes, we ran the bootstrap-
ping process over the training texts to induce a se-
mantic classifier. We then applied the classifier to
the same set of training documents and compiled a
lexicon by collecting the set of nouns that were as-
signed to each semantic class. We ignored words
that were assigned different labels in different con-
texts to avoid conflicts in the lexicons. We used
the identical configuration described by (Huang and
Riloff, 2010) that applies a 1.0 confidence threshold
for semantic class assignment.
3.2.3 Coreference-Based Lexicon Construction
The third component of our ensemble is a new
method for semantic lexicon induction that exploits
coreference resolution. Members of a coreference
chain represent the same entity, so all references to
the entity should belong to the same semantic class.
For example, suppose ?Paris? and ?the city? are in
the same coreference chain. If we know that city is
a Fixed Location, then we can infer that Paris is also
a Fixed Location.
We induced lexicons from coreference chains us-
ing a similar bootstrapping framework that begins
with seed nouns and unannotated texts. Let S de-
note a set of semantic classes and W denote a set of
unknown words. For any s ? S and w ? W , let
Ns,w denote the number of instances of s in the cur-
rent lexicon3 that are coreferent with w in the text
corpus. Then we estimate the probability that word
w belongs to semantic class s as:
P (s|w) = Ns,w?
s??S Ns?,w
We hypothesize the semantic class of w,
SemClass(w) by:
SemClass(w) = argmaxs P (s|w)
3In the first iteration, the lexicon is initialized with the seeds.
201
To ensure high precision for the induced lexicons,
we use a threshold of 0.5. All words with a prob-
ability above this thresold are added to the lexicon,
and the bootstrapping process repeats. Although the
coreference chains remain the same throughout the
process, the lexicon grows so more words in the
chains have semantic class labels as bootstrapping
progresses. Bootstrapping ends when fewer than 5
words are learned for each of the semantic classes.
Many noun phrases are singletons (i.e., they are
not coreferent with any other NPs), which limits the
set of words that can be learned using coreference
chains. Furthermore, coreference resolvers make
mistakes, so the accuracy of the induced lexicons
depends on the quality of the chains. For our experi-
ments, we used Reconcile (Stoyanov et al, 2010), a
freely available supervised coreference resolver.
3.3 Ensemble-based Bootstrapping
Framework
Figure 1 shows the architecture of our ensemble-
based bootstrapping framework. Initially, each lexi-
con only contains the seed nouns. Each component
hypothesizes a set of candidate words for each se-
mantic class, based on its own criteria. The word
lists produced by the three systems are then com-
pared, and we retain only the words that were hy-
pothesized with the same class label by at least two
of the three systems. The remaining words are dis-
carded. The consenus words are added to the lexi-
con, and the bootstrapping process repeats. As soon
as fewer than 5 words are learned for each of the
semantic classes, bootstrapping stops.
Figure 1: Ensemble-based bootstrapping framework
We ran each individual system with the same seed
words. Since bootstrapping typically yields the best
precision during the earliest stages, we used the se-
mantic tagger?s trained model immediately after its
first bootstrapping iteration. Basilisk generates 5
words per cycle, so we report results for lexicons
generated after 20 bootstrapping cycles (100 words)
and after 80 bootstrapping cycles (400 words).
3.4 Co-Training Framework
The three components in our ensemble use different
types of features (views) to identify semantic class
members, so we also experimented with co-training.
Our co-training model uses an identical framework,
but the hypotheses produced by the different meth-
ods are all added to the lexicon, so each method can
benefit from the hypotheses produced by the others.
To be conservative, each time we added only the 10
most confident words hypothesized by each method.
In contrast, the ensemble approach only adds
words to the lexicon if they are hypothesized by two
different methods. As we will see in Section 4.4,
the ensemble performs much better than co-training.
The reason is that the individual methods do not con-
sistently achieve high precision on their own. Con-
sequently, many mistakes are added to the lexicon,
which is used as training data for subsequent boot-
strapping. The benefit of the ensemble is that con-
sensus is required across two methods, which serves
as a form of cross-checking to boost precision and
maintain a high-quality lexicon.
4 Evaluation
4.1 Semantic Class Definitions
We evaluated our approach on nine semantic cate-
gories associated with disease outbreaks. The se-
mantic classes are defined below.
Animal: Mammals, birds, fish, insects and other
animal groups. (e.g., cow, crow, mosquito, herd)
4http://www.nlm.nih.gov/research/umls/
5http://www.maxmind.com/app/worldcities
6http://www.listofcountriesoftheworld.
com/
7http://names.mongabay.com/most_common_
surnames.htm
8http://www.sec.gov/rules/other/
4-460list.htm
9http://www.utexas.edu/world/univ/state/
10http://www.uta.fi/FAST/GC/usabacro.
html/
202
Semantic External Word List Sources
Class
Animal WordNet: [animal], [mammal family], [animal group]
Body Part WordNet: [body part], [body substance], [body covering], [body waste]
DisSym WordNet: [symptom], [physical condition], [infectious agent]; Wikipedia: common and infectious
diseases, symptoms, disease acronyms; UMLS Thesaurus4: diseases, abnormalities, microorganisms
(Archaea, Bacteria, Fungus, Virus)
Fixed Loc. WordNet: [geographic area], [land], [district, territory], [region]; Wiki:US-states; Other:cities5, countries6
Human WordNet: [person], [people], [personnel]; Wikipedia: people names, office holder titles, nationalities,
occupations, medical personnels & acronyms, players; Other: common people names & surnames7
Org WordNet: [organization], [assembly]; Wikipedia: acronyms in healthcare, medical organization acronyms,
news agencies, pharmaceutical companies; Other: companies8, US-universities9, organizations10
Plant & Food WordNet: [food], [plant, flora], [plant part]
Temp. Ref. WordNet: [time], [time interval], [time unit],[time period]
TimeBank: TimeBank1.2 (Pustejovsky et al, 2003) TIMEX3 expressions
Trans. Struct. WordNet: [structure, construction], [road, route], [facility, installation], [work place]
Table 1: External Word List Sources
Body Part: A part of a human or animal body, in-
cluding organs, bodily fluids, and microscopic parts.
(e.g., hand, heart, blood, DNA)
Diseases and Symptoms (DisSym): Diseases
and symptoms. We also include fungi and disease
carriers because, in this domain, they almost always
refer to the disease that they carry. (e.g. FMD, An-
thrax, fever, virus)
Fixed Location (Fixed Loc.): Named locations,
including countries, cities, states, etc. We also in-
clude directions and well-defined geographic areas
or geo-political entities. (e.g., Brazil, north, valley)
Human: All references to people, including
names, titles, professions, and groups. (e.g., John,
farmer, traders)
Organization (Org.): An entity that represents a
group of people acting as a single recognized body,
including named organizations, departments, gov-
ernments, and their acronyms. (e.g., department,
WHO, commission, council)
Temporal Reference (Temp. Ref.): Any refer-
ence to a time or duration, including months, days,
seasons, etc. (e.g., night, May, summer, week)
Plants & Food11: plants, plant parts, or any type
of food. (e.g., seed, mango, beef, milk)
Transient Structures (Trans. Struct.): Transient
physical structures. (e.g., hospital, building, home)
Additionally, we defined a Miscellaneous class
for words that do not belong to any of the other cat-
11We merged plants and food into a single category as it is
difficult to separate them because many food items are plants.
egories. (e.g., output, information, media, point).
4.2 Data Set
We ran our experiments on ProMED-mail12 articles.
ProMED-mail is an internet based reporting system
for infectious disease outbreaks, which can involve
people, animals, and plants grown for food. Our
ProMED corpus contains 5004 documents. We used
4959 documents as (unannotated) training data for
bootstrapping. For the remaining 45 documents,
we used 22 documents to train the coreference re-
solver (Reconcile) and 23 documents as our test set.
The coreference training set contains MUC-7 style
(Hirschman, 1997) coreference annotations13. Once
trained, Reconcile was applied to the 4959 unanno-
tated documents to produce coreference chains.
4.3 Gold Standard Semantic Class Annotations
To obtain gold standard annotations for the test set,
two annotators assigned one of the 9 semantic class
labels, or Miscellaneous, to each head noun based on
its surrounding context. A noun with multiple senses
could get assigned different semantic class labels in
different contexts. The annotators first annotated 13
of the 23 documents, and discussed the cases where
they disagreed. Then they independelty annotated
12http://www.promedmail.org/
13We omit the details of the coreference annotations since
it is not the focus of this research. However, the annotators
measured their agreement on 10 documents and achieved MUC
scores of Precision = .82, Recall = .86, F-measure = .84.
203
the remaining 10 documents and measured inter-
annotator agreement with Cohen?s Kappa (?) (Car-
letta, 1996). The ? score for these 10 documents was
0.91, indicating a high level of agreement. The an-
notators then adjudicated their disagreements on all
23 documents to create the gold standard.
4.4 Dictionary Evaluation
To assess the quality of the lexicons, we estimated
their accuracy by compiling external word lists
from freely available sources such as Wikipedia14
and WordNet (Miller, 1990). Table 1 shows the
sources that we used, where the bracketed items re-
fer to WordNet hypernym categories. We searched
each WordNet hypernym tree (also, instance-
relationship) for all senses of the word. Addition-
ally, we collected the manually labeled words in our
test set and included them in our gold standard lists.
Since the induced lexicons contain individual
nouns, we extracted only the head nouns of multi-
word phrases in the external resources. This
can produce incorrect entries for non-compositional
phrases, but we found this issue to be relatively rare
and we manually removed obviously wrong entries.
We adopted a conservative strategy and assumed that
any lexicon entries not present in our gold standard
lists are incorrect. But we observed many correct en-
tries that were missing from the external resources,
so our results should be interpreted as a lower bound
on the true accuracy of the induced lexicons.
We generated lexicons for each method sepa-
rately, and also for the ensemble and co-training
models. We ran Basilisk for 100 iterations (500
words). We refer to a Basilisk lexicon of size N
using the notation B[N ]. For example, B400 refers
to a lexicon containing 400 words, which was gen-
erated from 80 bootstrapping cycles. We refer to the
lexicon obtained from the semantic tagger as ST Lex.
Figure 2 shows the dictionary evaluation results.
We plotted Basilisk?s accuracy after every 5 boot-
strapping cycles (25 words). For ST Lex, we sorted
the words by their confidence scores and plotted the
accuracy of the top-ranked words in increments of
50. The plots for Coref, Co-Training, and Ensemble
B[N] are based on the lexicons produced after each
bootstrapping cycle.
14www.wikipedia.org/
The ensemble-based framework yields consis-
tently better accuracy than the individual methods
for Animal, Body Part, Human and Temporal Refer-
ence, and similar if not better for Disease & Symp-
tom, Fixed Location, Organization, Plant & Food.
However, relying on consensus from multiple mod-
els produce smaller dictionaries. Big dictionaries are
not always better than small dictionaries in practice,
though. We believe, it matters more whether a dic-
tionary contains the most frequent words for a do-
main, because they account for a disproportionate
number of instances. Basilisk, for example, often
learns infrequent words, so its dictionaries may have
high accuracy but often fail to recognize common
words. We investigate this issue in the next section.
4.5 Instance-based Tagging Evaluation
We also evaluated the effectiveness of the induced
lexicons with respect to instance-based semantic
tagging. Our goal was to determine how useful the
dictionaries are in two respects: (1) do the lexicons
contain words that appear frequently in the domain,
and (2) is dictionary look-up sufficient for instance-
based labeling? Our bootstrapping processes en-
force a constraint that a word can only belong to one
semantic class, so if polysemy is common, then dic-
tionary look-up will be problematic.15
The instance-based evaluation assigns a semantic
label to each instance of a head noun. When using a
lexicon, all instances of the same noun are assigned
the same semantic class via dictionary look-up. The
semantic tagger (SemTag), however, is applied di-
rectly since it was designed to label instances.
Table 2 presents the results. As a baseline, the
W.Net row shows the performance of WordNet for
instance tagging. For words with multiple senses,
we only used the first sense listed in WordNet.
The Seeds row shows the results when perform-
ing dictionary look-up using only the seed words.
The remaining rows show the results for Basilisk
(B100 and B400), coreference-based lexicon induc-
tion (Coref), lexicon induction using the semantic
tagger (ST Lex), and the original instance-based tag-
ger (SemTag). The following rows show the results
for co-training (after 4 iterations and 20 iterations)
15Only coarse polysemy across semantic classes is an issue
(e.g., ?plant? as a living thing vs. a factory).
204
Figure 2: Dictionary Evaluation Results
and for the ensemble (using Basilisk size 100 and
size 400). Table 3 shows the micro & macro average
results across all semantic categories.
Table 3 shows that the dictionaries produced by
the Ensemble w/B100 achieved better results than
the individual methods and co-training with an F
score of 80%. Table 2 shows that the ensemble
achieved better performance than the other methods
for 4 of the 9 classes, and was usually competitive
on the remaining 5 classes. WordNet (W.Net) con-
sistently produced high precision, but with compar-
atively lower recall, indicating that WordNet does
not have sufficient coverage for this domain.
4.6 Analysis
Table 4 shows the performance of our ensemble
when using only 2 of the 3 component methods.
Removing any one method decreases the average
F-measure by at least 3-5%. Component pairs
that include induced lexicons from coreference (ST
Lex+Coref and B100+Coref) yield high precision
but low recall. The component pair ST Lex+B100
produces higher recall but with slightly lower accu-
racy. The ensemble framework boosted recall even
more, while maintaining the same precision.
We observe that some of the smallest lexicons
produced the best results for instance-based seman-
tic tagging (e.g., Organization). Our hypothesis is
that consensus decisions across different methods
helps to promote the acquisition of high frequency
domain words, which are crucial to have in the dic-
tionary. The fact that dictionary look-up performed
better than an instance-based semantic tagger also
suggests that coarse polysemy (different senses that
205
Method Animal Body DisSym Fixed Human Org. Plant & Temp. Trans.
Part Loc. Food Ref. Struct.
P R F P R F P R F P R F P R F P R F P R F P R F P R F
Individual Methods
W.Net 92 88 90 93 59 72 99 77 87 86 58 69 83 55 66 86 44 59 65 79 71 93 85 89 85 64 73
Seeds 100 54 70 92 55 69 100 59 74 95 10 18 100 22 36 100 41 58 100 61 76 100 52 69 100 09 17
B100 99 77 86 94 73 82 100 66 80 96 23 37 96 31 47 91 58 71 82 64 72 68 83 75 67 22 33
B400 94 90 92 51 86 64 100 69 81 97 35 51 91 51 65 79 77 78 46 82 59 49 94 64 83 78 80
Coref 90 67 77 92 55 69 66 83 73 65 46 54 57 50 53 54 68 60 81 61 69 60 74 67 45 09 15
ST Lex 94 89 91 68 77 72 80 91 85 91 74 82 79 43 55 84 62 71 51 68 58 73 91 81 82 49 61
SemTag 91 90 90 52 68 59 77 90 83 91 78 84 81 48 60 80 63 70 43 82 56 77 93 84 83 53 64
Co-Training
pass4 64 76 70 67 73 70 91 79 85 91 39 54 98 44 61 83 69 76 43 68 53 73 94 82 49 36 42
pass20 60 89 71 56 91 69 88 91 90 83 64 72 92 54 68 72 77 74 28 71 40 65 98 78 46 40 43
Ensembles
w/B100 93 94 94 74 77 76 93 81 86 92 73 81 94 55 70 90 78 84 56 89 68 55 94 70 79 75 77
w/B400 94 93 93 65 91 75 96 87 91 89 75 81 92 56 70 79 79 79 47 86 61 53 94 68 63 55 58
Table 2: Instance-based Semantic Tagging Results (P = Precision, R = Recall, F = F-measure)
Method Micro Average Macro Average
P R F P R F
Individual Systems
W.Net 88 66 75 87 68 76
Seeds 99 35 52 99 40 57
B100 89 50 64 88 55 68
B400 77 66 71 77 74 75
Coref 65 59 62 68 57 62
ST Lex 82 72 77 78 72 75
SemTag 80 74 77 75 74 74
Co-Training
pass4 77 61 68 73 64 68
pass20 69 74 71 65 75 70
Ensembles
w/B100 83 77 80 81 80 80
w/B400 79 78 78 75 79 77
Table 3: Micro & Macro Average for Semantic Tagging
cut across semantic classes) is a relatively minor is-
sue within a specialized domain.
5 Conclusions
Our research combined three diverse methods
for semantic lexicon induction in a bootstrapped
ensemble-based framework, including a novel ap-
proach for lexicon induction based on coreference
chains. Our ensemble-based approach performed
better than the individual methods, in terms of
both dictionary accuracy and instance-based seman-
tic tagging. In future work, we believe this ap-
proach could be enhanced further by adding new
types of techniques to the ensemble and by investi-
Method Micro Average Macro Average
P R F P R F
Ensemble with component pairs
ST Lex+Coref 92 59 72 92 57 70
B100+Coref 92 40 56 94 44 60
ST Lex+B100 82 69 75 81 75 77
Ensemble with all components
ST Lex+B100+Coref 83 77 80 81 80 80
Table 4: Ablation Study of the Ensemble Framework for
Semantic Tagging
gating better methods for estimating the confidence
scores from the individual components.
Acknowledgments
We are grateful to Lalindra de Silva for manually
annotating data, Nathan Gilbert for help with Rec-
oncile, and Ruihong Huang for help with the se-
mantic tagger. We gratefully acknowledge the sup-
port of the National Science Foundation under grant
IIS-1018314 and the Defense Advanced Research
Projects Agency (DARPA) Machine Reading Pro-
gram under Air Force Research Laboratory (AFRL)
prime contract no. FA8750-09-C-0172. Any opin-
ions, findings, and conclusions or recommenda-
tions expressed in this material are those of the au-
thors and do not necessarily reflect the view of the
DARPA, AFRL, or the U.S. government.
206
References
ACE. 2007. NIST ACE evaluation website. In
http://www.nist.gov/speech/tests/ace/2007.
ACE. 2008. NIST ACE evaluation website. In
http://www.nist.gov/speech/tests/ace/2008.
Daniel M. Bikel, Scott Miller, Richard Schwartz, and
Ralph Weischedel. 1997. Nymble: a high-
performance learning name-finder. In Proceedings of
ANLP-97, pages 194?201.
A. Blum and T. Mitchell. 1998. Combining Labeled and
Unlabeled Data with Co-Training. In Proceedings of
the 11th Annual Conference on Computational Learn-
ing Theory (COLT-98).
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: the kappa statistic. Comput. Linguist.,
22:249?254, June.
Andrew Carlson, Justin Betteridge, Estevam R. Hr-
uschka Jr., and Tom M. Mitchell. 2009. Coupling
semi-supervised learning of categories and relations.
In HLT-NAACL 2009 Workshop on Semi-Supervised
Learning for NLP.
M. Collins and Y. Singer. 1999. Unsupervised Mod-
els for Named Entity Classification. In Proceedings
of the Joint SIGDAT Conference on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora (EMNLP/VLC-99).
S. Cucerzan and D. Yarowsky. 1999. Language Inde-
pendent Named Entity Recognition Combining Mor-
phologi cal and Contextual Evidence. In Proceedings
of the Joint SIGDAT Conference on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora (EMNLP/VLC-99).
J. Curran. 2002. Ensemble Methods for Automatic The-
saurus Extraction. In Proceedings of the 2002 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing.
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from the
web: an experimental study. Artificial Intelligence,
165(1):91?134, June.
M.B. Fleischman and E.H. Hovy. 2002. Fine grained
classification of named entities. In Proceedings of the
COLING conference, August.
L. Hirschman. 1997. MUC-7 Coreference Task Defini-
tion.
Ruihong Huang and Ellen Riloff. 2010. Inducing
domain-specific semantic class taggers from (almost)
nothing. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics.
Z. Kozareva, E. Riloff, and E. Hovy. 2008. Semantic
Class Learning from the Web with Hyponym Pattern
Linkage Graphs. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies (ACL-08).
T. McIntosh and J. Curran. 2008. Weighted mutual
exclusion bootstrapping for domain independent lex-
icon and template acquisition. In Proceedings of the
Australasian Language Technology Association Work-
shop.
T. McIntosh and J. Curran. 2009. Reducing Semantic
Drift with Bagging and Distributional Similarity. In
Proceedings of the 47th Annual Meeting of the Associ-
ation for Computational Linguistics.
T. McIntosh. 2010. Unsupervised Discovery of Negative
Categories in Lexicon Bootstrapping. In Proceedings
of the 2010 Conference on Empirical Methods in Nat-
ural Language Processing.
G. Miller. 1990. Wordnet: An On-line Lexical Database.
International Journal of Lexicography, 3(4).
V. Ng. 2007. Semantic Class Induction and Coreference
Resolution. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics.
Cheng Niu, Wei Li, Jihong Ding, and Rohini K. Srihari.
2003. A bootstrapping approach to named entity clas-
sification using successive learners. In Proceedings of
the 41st Annual Meeting on Association for Computa-
tional Linguistics (ACL-03), pages 335?342.
M. Pas?ca. 2004. Acquisition of categorized named en-
tities for web search. In Proc. of the Thirteenth ACM
International Conference on Information and Knowl-
edge Management, pages 137?145.
W. Phillips and E. Riloff. 2002. Exploiting Strong Syn-
tactic Heuristics and Co-Training to Learn Semantic
Lexicons. In Proceedings of the 2002 Conference on
Empirical Methods in Natural Language Processing,
pages 125?132.
J. Pustejovsky, P. Hanks, R. Saur??, A. See, R. Gaizauskas,
A. Setzer, D. Radev, B. Sundheim, D. Day, L. Ferro,
and M. Lazo. 2003. The TIMEBANK Corpus. In
Proceedings of Corpus Linguistics 2003, pages 647?
656.
E. Riloff and R. Jones. 1999. Learning Dictionaries for
Information Extraction by Multi-Level Bootstrapping.
In Proceedings of the Sixteenth National Conference
on Artificial Intelligence.
E. Riloff and J. Shepherd. 1997. A Corpus-Based Ap-
proach for Building Semantic Lexicons. In Proceed-
ings of the Second Conference on Empirical Methods
in Natural Language Processing, pages 117?124.
E. Riloff. 1996. Automatically Generating Extraction
Patterns from Untagged Text. In Proceedings of the
Thirteenth National Conference on Artificial Intelli-
gence, pages 1044?1049. The AAAI Press/MIT Press.
B. Roark and E. Charniak. 1998. Noun-phrase Co-
occurrence Statistics for Semi-automatic Semantic
207
Lexicon Construction. In Proceedings of the 36th
Annual Meeting of the Association for Computational
Linguistics, pages 1110?1116.
Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen
Riloff, David Buttler, and David Hysom. 2010. Coref-
erence resolution with Reconcile. In Proceedings of
the ACL 2010 Conference Short Papers, pages 156?
161.
M. Thelen and E. Riloff. 2002. A Bootstrapping Method
for Learning Semantic Lexicons Using Extraction Pa
ttern Contexts. In Proceedings of the 2002 Conference
on Empirical Methods in Natural Language Process-
ing, pages 214?221.
V. Vyas and P. Pantel. 2009. Semi-automatic entity set
refinement. In Proceedings of North American Asso-
ciation for Computational Linguistics / Human Lan-
guage Technology (NAACL/HLT-09).
208
Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 17?25,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Toward Plot Units: Automatic Affect State Analysis
Amit Goyal and Ellen Riloff and Hal Daume III and Nathan Gilbert
School of Computing
University of Utah
Salt Lake City, UT 84112
{amitg,riloff,hal,ngilbert}@cs.utah.edu
Abstract
We present a system called AESOP that au-
tomatically produces affect states associated
with characters in a story. This research repre-
sents a first step toward the automatic genera-
tion of plot unit structures from text. AESOP
incorporates several existing sentiment analy-
sis tools and lexicons to evaluate the effective-
ness of current sentiment technology on this
task. AESOP also includes two novel compo-
nents: a method for acquiring patient polar-
ity verbs, which impart negative affect on their
patients, and affect projection rules to propa-
gate affect tags from surrounding words onto
the characters in the story. We evaluate AE-
SOP on a small collection of fables.
1 Introduction
In the 1980s, plot units (Lehnert, 1981) were pro-
posed as a knowledge structure for representing nar-
rative stories and generating summaries. Plot units
are fundamentally different from the story represen-
tations that preceded them because they focus on the
emotional states and tensions between characters as
the driving force behind interesting plots and cohe-
sive stories. Plot units were used in narrative sum-
marization studies, both in computer science and
psychology (Lehnert et al, 1981), but the compu-
tational models of plot units relied on tremendous
amounts of manual knowledge engineering.
Given the recent swell of activity in automated
methods for sentiment analysis, we embarked on a
project to see whether current techniques could auto-
matically detect the affect states needed for plot unit
analysis. Plot units are complex structures that in-
clude affect states, causal links, and cross-character
links, and generating complete plot unit structures is
beyond the scope of this work. As an initial step to-
ward the long-term goal of automatically generating
plot units, we began by creating a system to automat-
ically identify the affect states associated with char-
acters. An affect state represents the emotional state
of a character, based on their perspective of events
in the story. Plots units include three types of af-
fect states: positive (+) states, negative (-) states, and
mental (M) states that have neutral emotion (these
are often associated with plans and goals).
Our system, called AESOP, pulls together a va-
riety of existing technologies in sentiment analy-
sis to automatically identify words and phrases that
have positive/negative polarity or that correspond
to speech acts (for mental states). However, we
needed to develop a method to automatically map
these affect tags onto characters in the story.1 To
address this issue, we created affect projection rules
that propagate affect tags from words and phrases to
characters in the story via syntactic relations.
During the course of our research, we came to ap-
preciate that affect states, of the type required for
plot units, can represent much more than just di-
rect expressions of emotion. A common phenom-
ena are affect states that result from a character be-
ing acted upon in a positive or negative way. For
example, ?the cat ate the mouse? produces a pos-
itive affect state for the cat and a negative affect
1This is somewhat analogous to, but not exactly the same as,
associating opinion words with their targets or topics (Kim and
Hovy, 2006; Stoyanov and Cardie, 2008).
17
The Father and His Sons
(s1) A father had a family of sons who were perpetually
quarreling among themselves. (s2) When he failed to
heal their disputes by his exhortations, he determined to
give them a practical illustration of the evils of disunion;
and for this purpose he one day told them to bring him a
bundle of sticks. (s3) When they had done so, he placed
the faggot into the hands of each of them in succession,
and ordered them to break it in pieces. (s4) They tried
with all their strength, and were not able to do it. (s5) He
next opened the faggot, took the sticks separately, one by
one, and again put them into his sons? hands, upon which
they broke them easily. (s6) He then addressed them in
these words: ?My sons, if you are of one mind, and unite
to assist each other, you will be as this faggot, uninjured
by all the attempts of your enemies; but if you are divided
among yourselves, you will be broken as easily as these
sticks.?
(a) ?Father and Sons? Fable
Father Sons
(quarreling)a1
(stop quarreling)a3
(annoyed)a2
(exhortations)a4
(exhortations fail)a5
m
m
a
(teach lesson)a6
m
(get sticks & break)a7
m
(get sticks & break)a8
(cannot break sticks)a9
a
(cannot break sticks)a10
a
(bundle & break)a11
(bundle & break)a12
(break sticks)a13
a
(break sticks)a14
a
m
a
shared
request
request
shared
shared
s2
s2
s2
s2
s2
s2
s4
s5
s5
s1
s2
s4
s5
s5
(lesson succeeds)a15s5
(b) Plot Unit Analysis for ?Father and Sons? Fable
state for the mouse because obtaining food is good
but being eaten is bad. This type of world knowl-
edge is difficult to obtain, yet essential for plot unit
analysis. In AESOP, we use corpus statistics to au-
tomatically learn a set of negative patient polarity
verbs which impart a negative polarity on their pa-
tient (e.g., eaten, killed, injured, fired). To acquire
these verbs, we queried a large corpus with patterns
to identify verbs that frequently occur with agents
who stereotypically have evil intent.
We evaulate our complete system on a set of AE-
SOP?s fables. In this paper, we also explain and cat-
egorize different types of situations that can produce
affect states, several of which cannot be automati-
cally recognized by existing sentiment analysis tech-
nology. We hope that one contribution of our work
will be to create a better awareness of, and apprecia-
tion for, the different types of language understand-
ing mechanisms that will ultimately be necessary for
comprehensive affect state analysis.
2 Overview of Plot Units
Narratives can often be understood in terms of the
emotional reactions and affect states of the char-
acters therein. The plot unit formalism (Lehnert,
1981) provides a representational mechanism for af-
fect states and the relationships between them. Plot
unit structures can be used for tasks such as narrative
summarization and question answering.
Plot unit structures consist of affect states for each
character in a narrative, and links explaining the re-
lationships between these affect states. The affect
states themselves each have a type: (+) for positive
states, (-) for negative states, and (M) for mental
states (with neutral affect). Although affect states
are not events per se, events often trigger affect
states. If an event affects multiple characters, it can
trigger multiple affect states, one for each character.
Affect states are further connected by causal links,
which explain how the narrative hangs together.
These include motivations (m), actualizations (a),
terminations (t) and equivalences (e). Causal links
exist between affect states for the same character.
Cross-character links explain how single events af-
fect two characters. For instance, if one character
requests something of the other, this is an M-to-M
link, since it spans a shared mental affect for both
characters. Other speech acts can be represented as
M to + (promise) or M to - (threat).
To get a better feeling of the plot unit represen-
tation, a short fable, ?The Father and His Sons,? is
shown in Figure 1(a) and our annotation of its plot
unit structure is shown in Figure 1(b). In this fa-
ble, there are two characters (the ?Father? and the
?Sons?) who go through a series of affect states, de-
picted chronologically in the two columns.
In this example, the first affect state is a negative
state for the sons, who are quarreling (a1). This state
is shared by the father (via a cross-character link)
who has a negative annoyance state (a2). The fa-
ther then decides that he wants to stop the sons from
quarreling, which is a mental event (a3). The causal
link from a2 to a3 with an m label indicates a ?mo-
tivation.? His first attempt is by exhortations (a4).
18
This produces an M (a3) linked to an M (a4) with
a m (motivation) link, which represents subgoaling.
The father?s overall goal is to stop the quarreling
(a3) and in order to do so, he creates a subgoal of
exhorting the sons to stop (a4). The exhortations
fail, which produces a negative state (a5) for the fa-
ther. The a causal link indicates an ?actualization?,
representing the failure of the plan (a4).
The failure of the father?s exhortations leads to a
new subgoal: to teach the sons a lesson (a6). The m
link from a5 to a6 is an example of ?enablement.?
At a high level, this subgoal has two parts, indicated
by the two gray regions (a7 ? a10 and a11 ? a14).
The first gray region begins with a cross-character
link (M to M), which indicates a request (in this case,
to break a bundle of sticks). The sons fail at this,
which upsets them (a9) but pleases the father (a10).
The second gray region depicts the second part of
the father?s subgoal; he makes a second request (a11
to a12) to separate the bundle and break the sticks,
which the sons successfully do, making them happy
(a13) and the father happy (a14). This latter struc-
ture (the second gray region) is an HONORED RE-
QUEST plot unit. At the end, the father?s plan suc-
ceeds (a15) which is an actualization (a link) of his
goal to teach the sons a lesson (a6).
In this example, as well as the others that we an-
notated in our gold standard, (see Section 5.1), we
annotated conservatively. In particular, in reading
the story, we may assume that the father?s origi-
nal plan of stopping the son?s quarrelling also suc-
ceeded. However, this is not mentioned in the story
and therefore we chose not to represent it. It is also
important to note that plot unit representations can
have t (termination) and e (equivalence) links that
point backwards in time, but they do not occur in
the Father and Sons fable.
3 Where Do Affect States Come From?
We began this research with the hope that recent re-
search in sentiment analysis would supply us with
effective tools to recognize affect states. However,
we soon realized that affect states, as required for
plot unit analysis, go well beyond the notions of pos-
itive/negative polarity and private states that have
been studied in recent sentiment analysis work. In
this section, we explain the wide variety of situa-
tions that can produce an affect state, based on our
observations in working with fables. Most likely, an
even wider variety of situations could produce affect
states in other text genres.
3.1 Direct Expressions of Emotion
Plot units can include affect states that correspond to
explicit expressions of positive/negative emotional
states, as has been studied in the realm of sentiment
analysis. For example, ?Max was disappointed?
produces a negative affect state for Max, and ?Max
was pleased? produces a positive affect state for
Max. However, the affect must relate to an event that
occurs in the story?s plot. For example, a hypotheti-
cal expression of emotion would not yield an affect
state (e.g., ?if the rain stops, she will be pleased?).
3.2 Situational Affect States
Positive and negative affect states also frequently
represent good and bad situational states that char-
acters find themselves in. These states do not rep-
resent emotion, but indicate whether a situation is
good or bad for a character based on world knowl-
edge. For example, ?Wolf, who had a bone stuck
in his throat, ...? produces a negative affect state
for the wolf. Similarly, ?The Old Woman recovered
her sight...? produces a positive affect state. Senti-
ment analysis is not sufficient to generate these af-
fect states. Sometimes, however, a direct expression
of emotion will also be present (e.g., ?Wolf was un-
happy because he had a bone stuck...?), providing
redundancy and multiple opportunities to recognize
the correct affect state for a character.
Situational affect states are common and often
motivate plans and goals that are central to the plot.
3.3 Plans and Goals
Plans and goals are another common reason for
affect states. The existence of a plan or goal is
usually represented as a mental state (M). Plans and
goals can be difficult to detect automatically. A
story may reveal that a character has a plan or goal
in a variety of ways, such as:
Direct expressions of plans/goals: a plan or goal
may be explicitly stated (e.g., ?the lion wanted to
find food?). In this case, a mental state (M) should
19
be generated.
Speech acts: a plan or goal may be revealed
through a speech act between characters. For
example, ?the wolf asked an eagle to extract the
bone? is a directive speech act that indicates the
wolf?s plan to resolve its negative state (having a
bone stuck). This example illustrates how a negative
state (bone stuck) can motivate a mental state (plan).
When a speech act involves multiple characters, it
produces multiple mental states. For example, a
mental state should also be produced for the eagle,
because it now has a plan to help the wolf (by virtue
of being asked).
Inferred plans/goals: plans and goals sometimes
must be inferred from actions. For example, ?the
lion hunted deer? reveals the lion?s plan to obtain
food. Similarly, the serpent spat poison into the
man?s water? implies that the serpent had a plan to
kill the man.
Plans and goals also produce positive/negative af-
fect states when they succeed/fail. For example, if
the eagle successfully extracts the bone from the
wolf?s throat, then both the wolf and the eagle will
have positive affect states, because both were suc-
cessful in their respective goals. A directive speech
act between two characters coupled with positive af-
fect states for both characters is a common plot unit
structure called an HONORED REQUEST, depicted
by the second gray block shown in Fig.1(b).
The affect state for a character is always with
respect to its view of the situation. For example,
consider: ?The owl besought a grasshopper to
stop chirping. The grasshopper refused to desist,
and chirped louder and louder.? Both the owl and
the grasshopper have M affect states representing
the request from the owl to the grasshopper (i.e.,
the owl?s plan to stop the chirping is to ask the
grasshopper to knock it off). The grasshopper
refuses the request, so a negative affect state is
produced for the owl, indicating that its plan failed.
However, a positive affect state is produced for
the grasshopper, because its goal was to continue
chirping which was accomplished by refusing the
request. This scenario is also a common plot unit
structure called a DENIED REQUEST.
3.4 Patient Role Affect States
Many affect states come directly from events. In
particular, when a character is acted upon (the theme
or patient of an event), a positive or negative affect
state often results for the character. These affect
states reflect world knowledge about what situations
are good and bad. For example:
Negative patient roles: killed X, ate X, chased X,
captured X, fired X, tortured X
Positive patient roles: rescued X, fed X, adopted X,
housed X, protected X, rewarded X
For example, ?a man captured a bear? indicates a
negative state for the bear. Overall, this sentence
would generate a SUCCESS plot unit consisting of
an M state and a + state for the man (with an actual-
ization a causal link between them representing the
plan?s success) and a - state for the bear (as a cross-
character link indicating that what was good for the
man was bad for the bear). A tremendous amount of
world knowledge is needed to generate these states
from such a seemingly simple sentence. Similarly,
if a character is rescued, fed, or adopted, then a + af-
fect state should be produced for the character based
on knowledge that these events are desirable. We
are not aware of existing resources that can automat-
ically identify affect polarity with respect to event
roles. In Section 4.1.2, we explain how we automat-
ically acquire Patient Polarity Verbs from a corpus
to identify some of these affect states.
4 AESOP: Automatic Affect State Analysis
We created a system, called AESOP, to try to auto-
matically identify the types of affect states that are
required for plot unit analysis. AESOP incorporates
existing resources for sentiment analysis and speech
act recognition, and includes two novel components:
patient polarity verbs, which we automatically gen-
erate using corpus statistics, and affect projection
rules, which automatically project and infer affect
labels via syntactic relations.
AESOP produces affect states in a 3-step process.
First, AESOP labels individual words and phrases
with an M, +, or - affect tag. Second, it identi-
fies all references to the two main characters of the
20
story. Third, AESOP applies affect projection rules
to propagate affect states onto the characters, and in
some cases, to infer new affect states.
4.1 Step 1: Assigning Affect Tags to Words
4.1.1 Sentiment Analysis Resources
AESOP incorporates several existing sentiment
analysis resources to recognize affect states associ-
ated with emotions and speech acts.
? OpinionFinder2 (Wilson et al, 2005) (Version
1.4) is used to identify all three types of states. We
use the +/- labels assigned by its contextual polar-
ity classifier (Wilson, 2005) to create +/- affect tags.
The MPQASD tags produced by its Direct Subjective
and Speech Event Identifier (Choi et al, 2006) are
used as M affect tags.
? Subjectivity Lexicon3 (Wilson, 2005): The pos-
itive/negative words in this list are assigned +/- af-
fect tags, when they occur with the designated part-
of-speech (POS).
? Semantic Orientation Lexicon4 (Takamura et
al., 2005): The positive/negative words in this list
are assigned +/- affect tags, when they occur with
the designated part-of-speech.
? A list of 228 speech act verbs compiled from
(Wierzbicka, 1987)5, which are used for M states.
4.1.2 Patient Polarity Verbs
As we discussed in Section 3.4, existing resources
are not sufficient to identify affect states that arise
from a character being acted upon. Sentiment lexi-
cons, for example, assign polarity to verbs irrespec-
tive of their agents or patients. To fill this gap,
we tried to automatically acquire verbs that have a
strong patient polarity (i.e., the patient will be in a
good or bad state by virtue of being acted upon).
We used corpus statistics to identify verbs that
frequently occur with agents who typically have
evil (negative) or charitable (positive) intent. First,
we identified 40 words that are stereotypically evil
agents, such as monster, villain, terrorist, and mur-
derer, and 40 words that are stereotypically charita-
ble agents, such as hero, angel, benefactor, and res-
cuer. Next, we searched the google Web 1T 5-gram
2http://www.cs.pitt.edu/mpqa/opinionfinderrelease/
3http://www.cs.pitt.edu/mpqa/lexiconrelease/collectinfo1.html
4http://www.lr.pi.titech.ac.jp/?takamura/pndic en.html
5http://openlibrary.org/b/OL2413134M/English speech act verbs
corpus6 using patterns designed to identify verbs
that co-occur with these words as agents. For each
agent term, we applied the pattern ?*ed by [a,an,the]
AGENT? and extracted the list of matching verbs.7
Next, we rank the extracted verbs by computing
the ratio between the frequency of the verb with a
negative agent versus a positive agent. If this ratio
is > 1, then we save the verb as a negative patient
polarity verb (i.e., it imparts negative polarity to its
patient). This process produced 408 negative patient
polarity verbs, most of which seemed clearly neg-
ative for the patient. Table 1 shows the top 20 ex-
tracted verbs. We also tried to identify positive pa-
tient polarity verbs using a positive-to-negative ra-
tio, but the extracted verbs were often neutral for the
patient, so we did not use them.
scammed damaged disrupted ripped
raided corrupted hindered crippled
slammed chased undermined possesed
dogged tainted grounded levied
patched victimized posessed bothered
Table 1: Top 20 negative patient polarity verbs
4.2 Step 2: Identifying the Characters
The problem of coreference resolution in fables
is somewhat different than for other genres, pri-
marily because characters are often animals (e.g.,
?he?=?owl?). So we hand-crafted a simple rule-
based coreference system. For the sake of this task,
we made two assumptions: (1) There are only two
characters per fable, and (2) Both characters are
mentioned in the fable?s title.
We then apply heuristics to determine number and
gender for the characters based on word lists, Word-
Net (Miller, 1990) and POS tags. If no determina-
tion of a character?s gender or number can be made
from these resources, a process of elimination is em-
ployed. Given the two character assumption, if one
character is known to be male, but there are female
pronouns in the fable, then the other character is as-
sumed to be female. The same is done for number
agreement. Finally, if there is only one character be-
tween a pronoun and the beginning of a document,
6http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2006T13
7The corpus is not POS tagged so there is no guarantee these
will be verbs, but they usually are in this construction.
21
the pronoun is assumed to corefer with that char-
acter. The character then assumes the gender and
number of that pronoun. Lastly, WordNet is used
to obtain a small set of non-pronominal, non-string-
match resolutions by exploiting hypernym relations,
for instance, linking Peasant with the man.
4.3 Step 3: Affect Projection
Our goal is to produce affect states for each char-
acter in the story. Therefore every affect tag needs
to be attributed to a character, or discarded. Since
plots typically revolve around actions, we used the
verbs as the basis for projecting affect tags onto the
characters. In some cases, we also spawn new affect
tags associated with mental states to indicate that an
action is likely the manifestation of a plan.
We developed 6 types of affect projection rules
that orchestrate how affect tags are assigned to the
characters based on verb argument structure. We
use the Sundance shallow parsing toolkit (Riloff and
Phillips, 2004) to generate a syntactic analysis of
each sentence, including syntactic chunking, clause
segmentation, and active/passive voice recognition.
We normalize the verb phrases (VPs) with respect to
voice (i.e., we transform the passive voice construc-
tions into an active voice equivalent) to simplify our
rules. We then make the assumption that the Subject
of the VP is its AGENT and the Direct Object of the
VP is its PATIENT.8 The affect projection rules only
project affect states onto AGENTS and PATIENTS
that correspond to a character in the story. The five
types of rules are described below.
1. AGENT VP : This case applies when the VP
has no PATIENT, or a PATIENT that is not a char-
acter in the story, or the PATIENT corefers with
the AGENT. All affect tags associated with the VP
are projected onto the AGENT. For example, ?Mary
laughed (+)? projects a positive affect state onto
Mary.
2. VP PATIENT9: All affect tags associated with
the VP are projected onto the PATIENT, unless both
M and +/- tags exist, in which case only the +/- tags
are projected. For example, ?loved (+) the cat?,
projects a positive affect state onto the cat.
8We are not actually doing thematic role recognition, so this
will not always be correct, but it is a reasonable approximation.
9Agent is missing or not a character.
3. AGENT VP PATIENT: This case applies when
the AGENT and PATIENT refer to different char-
acters. All affect tags associated with the VP are
projected onto the PATIENT, unless both M and +/-
tags exist, in which case only the +/- tags are pro-
jected (as in Rule #2). If the VP has an M tag, then
we also project an M tag onto the AGENT (repre-
senting a shared, cross-character mental state). If
the VP has a +/- tag, then we project a + tag onto
the agent (as an inference that the AGENT accom-
plished some action).
4. AGENT VERB1 to VERB2 PATIENT. We di-
vide this into two cases: (a) If the agent and patient
refer to the same character, then Rule #1 is applied
(e.g., ?Bo decided to teach himself...?). (b) If the
agent and patient are different, we apply Rule #1 to
VERB1 to agent and Rule #2 to VERB2. If no af-
fect tags are assigned to either verb, then we create
an M affect state for the agent (assuming that the VP
represents some sort of plan).
5. If a noun phrase refers to a character and in-
cludes a modifying adjective with an affect tag, then
the affect is mapped onto the character. For exam-
ple, ?the happy (+) fox?.
Finally, if an adverb or adjectival phrase (e.g.,
predicate adjective) has an affect tag, then that affect
tag is mapped onto the preceding VP and the projec-
tion rules above are applied. For all of the rules, if
a clause contains a negation word, then we flip the
polarity of all words in that clause. Our negation list
contains: no, not, never, fail, failed, fails, don?t, and
didn?t.
5 Evaluation
5.1 Data Set
Plot unit analysis of ordinary text is enormously
complex ? even the idea of manually creating gold
standard annotations seemed like a monumental
task. So we began our exploration with simpler and
more constrained texts that seemed particularly ap-
propriate for plot unit analysis: fables. Fables have
two desirable attributes: (1) they have a small cast
of characters, and (2) they typically revolve around
a moral, which is exemplified by a short and concise
plot. Even so, fables are challenging for NLP due to
anthropomorphic characters, flowery language, and
sometimes archaic vocabulary.
22
State M (66) + (52) - (39) All (157)
System R P F R P F R P F R P F
Bsent baseline .65 .10 .17 .52 .08 .14 .74 .06 .11 .63 .08 .14
Bclause baseline .48 .28 .35 .44 .22 .29 .69 .17 .27 .52 .22 .31
All 4 resources (w/proj. rules) .48 .43 .45 .23 .39 .29 .23 .41 .29 .34 .41 .37
OpinionFinder .36 .42 .39 .00 .00 .00 .00 .00 .00 .15 .35 .21
Subjectivity Lexicon .45 .43 .44 .23 .35 .28 .21 .44 .28 .32 .41 .36
Semantic Dictionary .42 .45 .43 .00 .00 .00 .00 .00 .00 .18 .45 .26
Semantic Orientation Lexicon .41 .43 .42 .17 .53 .26 .08 .43 .13 .25 .45 .32
PPV Lexicon .41 .42 .41 .02 .17 .04 .21 .73 .33 .23 .44 .30
AESOP (All 4 + PPV) .48 .40 .44 .25 .36 .30 .33 .46 .38 .37 .40 .38
Table 2: Evaluation results for 2 baselines, 4 sentiment analysis resources with projection rules, and our PPV lexicon
with projection rules. (The # in parentheses is the number of occurrences of that state in the gold standard).
We collected 34 fables from an Aesop?s Fables
web site10, choosing fables that have a true plot
(some only contain quotes) and exactly two charac-
ters. We divided them into a development set of 11
stories, a tuning set of 8 stories, and a test set of 15
stories. The Father and Sons story from Figure 1(a)
is an example from our set.
Creating a gold standard was itself a substantial
undertaking. Plot units are complex structures, and
training non-experts to produce them did not seem
feasible in the short term. So three of the authors
discussed and iteratively refined manual annotations
for the development and tuning set stories until we
became comfortable that we had a common under-
standing for the annotation task. Then to create our
gold standard test set, two authors independently
created annotations for the test set, and a third au-
thor adjudicated the differences. The gold standard
contains complete plot unit annotations, including
affect states, causal links, and cross-character links.
For the experiments in this paper, however, only the
affect state annotations were used.
5.2 Baselines
We created two baselines to measure what would
happen if we use all 4 sentiment analysis resources
without any projection rules. The first one (Bsent)
operates at the sentence level. It naively projects ev-
ery affect tag that occurs in a sentence onto every
character in the same sentence. The second base-
line (Bclause) operates identically, but at the clause
level.
10http://www.pacificnet.net/?johnr/aesop/
5.3 Evaluation
As our evaluation metrics we used recall (R), preci-
sion (P), and F-measure (F). We evaluate each sys-
tem on individual affect states (+, - and M) as well
as across all affect states. The evaluation is done at
the sentence level. Meaning, if a system produces
the same affect state as present in the gold standard
for a sentence, we count it as a correct affect state.
Our main evaluation also requires each affect state
to be associated with the correct character.
Table 2 shows the coverage of our two baseline
systems as well as the four Sentiment Analysis
Resources used with our projection rules. We can
make several observations:
? As expected, the baselines achieve relatively high
recall, but low precision.
? Each of the sentiment analysis resources alone
is useful, and using them with the projection rules
leads to improved performance over the baselines
(10 points in F score for M and 6 points overall).
This shows that the projection rules are helpful
in identifying the characters associated with each
affect state.
? The PPV Lexicon, alone, is quite good at cap-
turing negative affect states. Together with the
projection rules, this leads to good performance on
identifying mental states as well.
To better assess our projection rules, we evaluated
the systems both with respect to characters and with-
out respect to characters. In this evaluation, system-
produced states are correct even if they are assigned
to the wrong character. Table 3 reveals several re-
sults: (1) For the baseline: there is a large drop when
23
State M (66) + (52) - (39) All (157)
System R P F R P F R P F R P F
Bclause w/o char .65 .37 .47 .50 .25 .33 .77 .19 .30 .63 .26 .37
AESOP w/o char .55 .44 .49 .33 .47 .39 .36 .50 .42 .43 .46 .44
Bclause w/ char .48 .28 .35 .44 .22 .29 .69 .17 .27 .52 .22 .31
AESOP w/ char .48 .40 .44 .25 .36 .30 .33 .46 .38 .37 .40 .38
Table 3: Evaluating affect states with and without respect to character.
State M (66) + (52) - (39) All (157)
System R P F R P F R P F R P F
Bclause PCoref .48 .28 .35 .44 .22 .29 .69 .17 .27 .52 .22 .31
AESOP PCoref .48 .40 .44 .25 .36 .30 .33 .46 .38 .37 .40 .38
Bclause ACoref .42 .45 .43 .25 .34 .29 .54 .24 .33 .39 .33 .36
AESOP ACoref .41 .54 .47 .12 .40 .18 .26 .45 .33 .27 .49 .35
Table 4: Final results of Bclause and AESOP systems with perfect and automated coreference
evaluated with respect to the correct character. (2)
For AESOP: there is a smaller drop in both preci-
sion and recall for M and -, suggesting that our pro-
jection rules are doing well for these affect states.
(3) For AESOP: there is a large drop in both preci-
sion and recall for +, suggesting that there is room
for improvement of our projection rules for positive
affect.
Finally, we wish to understand the role that coref-
erence plays. Table 4 summarizes the results with
perfect coreference and with automated coreference.
AESOP is better than both baselines when we use
perfect coreference (PCoref), which indicates that
the affect projection rules are useful. However,
when we use automated coreference (ACoref), re-
call goes down and precision goes up. Recall goes
down because our automated coreference system is
precision oriented: it only says ?coreferent? if it is
sure.
The increase in precision when moving to auto-
mated coreference is bizarre. We suspect it is pri-
marily due to the handling of quotations. Our perfect
coreference system resolves first and second person
pronouns in quotations, but the automated system
does not. Thus, with automated coreference, we al-
most never produce affect states from quotations.
This is a double-edged sword: sometimes quotes
contain important affect states, sometimes they do
not. For example, from the Father and Sons fable,
?if you are divided among yourselves, you will be
broken as easily as these sticks.? Automated coref-
erence does not produce any character resolutions
and therefore AESOP produces no affect states. In
this case this is the right thing to do. However, in
another well-known fable, a tortoise says to a hare:
?although you be as swift as the wind, I have beaten
you in the race.? Here, perfect coreference produces
multiple affect states, which are related to the plot:
the hare recieves a negative affect state for having
been beaten in the race.
6 Conclusions
AESOP demonstrates that sentiment analysis tools
can successfully recognize many affect states when
coupled with syntax-based projection rules to map
the affect states onto characters. We also showed
that negative patient polarity verbs can be harvested
from a corpus to identify characters that are in a neg-
ative state due to an action. However, performance is
still modest, revealing that much work remains to be
done. In future work, new methods will be needed
to represent affect states associated with plans/goals,
events, and inferences.
7 Acknowledgments
The authors thank the anonymous reviewers for
many helpful comments. This work was sup-
ported in part by the Department of Homeland Se-
curity Grant N0014-07-1-0152, the DARPA Ma-
chine Reading program under contract FA8750-09-
C-0172, and the NSF grant IIS-0712764.
24
References
Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint
extraction of entities and relations for opinion recogni-
tion. In EMNLP ?06: Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 431?439, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
S. Kim and E. Hovy. 2006. Extracting Opinions, Opin-
ion Holders, and Topics Expressed in Online News
Media Text. In Proceedings of ACL/COLING Work-
shop on Sentiment and Subjectivity in Text.
W. Lehnert, J. Black, and B. Reiser. 1981. Summariz-
ing Narratives. In Proceedings of the Seventh Interna-
tional Joint Conference on Artificial Intelligence.
W. G. Lehnert. 1981. Plot Units and Narrative Summa-
rization. Cognitive Science, 5(4):293?331.
G. Miller. 1990. Wordnet: An On-line Lexical Database.
International Journal of Lexicography, 3(4).
E. Riloff and W. Phillips. 2004. An Introduction to the
Sundance and AutoSlog Systems. Technical Report
UUCS-04-015, School of Computing, University of
Utah.
V. Stoyanov and C. Cardie. 2008. Topic Identification
for Fine-Grained Opinion Analysis. In Conference on
Computational Linguistics (COLING 2008).
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words using
spin model. In ACL ?05: Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics.
A. Wierzbicka. 1987. English speech act verbs: a se-
mantic dictionary. Academic Press, Sydney, Orlando.
T. Wilson, P. Hoffmann, S. Somasundaran, J. Kessler,
J. Wiebe, Y. Choi, C. Cardie, E. Riloff, and S. Pat-
wardhan. 2005. OpinionFinder: A system for subjec-
tivity analysis. In Proceedings of HLT/EMNLP 2005
Interactive Demonstrations.
Theresa Wilson. 2005. Recognizing contextual polarity
in phrase-level sentiment analysis. In In Proceedings
of HLT-EMNLP.
25
Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 46?55,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
The Role of Information Extraction in the Design of a Document Triage
Application for Biocuration
Sandeep Pokkunuri
School of Computing
University of Utah
Salt Lake City, UT
sandeepp@cs.utah.edu
Cartic Ramakrishnan
Information Sciences Institute
Univ. of Southern California
Marina del Rey, CA
cartic@isi.edu
Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT
riloff@cs.utah.edu
Eduard Hovy
Information Sciences Institute
Univ. of Southern California
Marina del Rey, CA
hovy@isi.edu
Gully APC Burns
Information Sciences Institute
Univ. of Southern California
Marina del Rey, CA
burns@isi.edu
Abstract
Traditionally, automated triage of papers is
performed using lexical (unigram, bigram,
and sometimes trigram) features. This pa-
per explores the use of information extrac-
tion (IE) techniques to create richer linguistic
features than traditional bag-of-words models.
Our classifier includes lexico-syntactic pat-
terns and more-complex features that repre-
sent a pattern coupled with its extracted noun,
represented both as a lexical term and as a
semantic category. Our experimental results
show that the IE-based features can improve
performance over unigram and bigram fea-
tures alone. We present intrinsic evaluation
results of full-text document classification ex-
periments to determine automatically whether
a paper should be considered of interest to
biologists at the Mouse Genome Informatics
(MGI) system at the Jackson Laboratories. We
also further discuss issues relating to design
and deployment of our classifiers as an ap-
plication to support scientific knowledge cu-
ration at MGI.
1 Introduction
A long-standing promise of Biomedical Natural
Language Processing is to accelerate the process of
literature-based ?biocuration?, where published in-
formation must be carefully and appropriately trans-
lated into the knowledge architecture of a biomed-
ical database. Typically, biocuration is a manual
activity, performed by specialists with expertise in
both biomedicine and the computational represen-
tation of the target database. It is widely acknowl-
edged as a vital lynch-pin of biomedical informatics
(Bourne and McEntyre, 2006).
A key step in biocuration is the initial triage of
documents in order to direct to specialists only the
documents appropriate for them. This classifica-
tion (Cohen and Hersh, 2006)(Hersh W, 2005) can
be followed by a step in which desired information
is extracted and appropriately standardized and for-
malized for entry into the database. Both these steps
can be enhanced by suitably powerful Natural Lan-
guage Processing (NLP) technology. In this paper,
we address text mining as a step within the broader
context of developing both infrastructure and tools
for biocuration support within the Mouse Genome
Informatics (MGI) system at the Jackson Labora-
tories. We previously identified ?document triage?
as a crucial bottleneck (Ramakrishnan et al, 2010)
within MGI?s biocuration workflow.
Our research explores the use of information ex-
traction (IE) techniques to create richer linguis-
tic features than traditional bag-of-words models.
These features are employed by a classifier to per-
form the triage step. The features include lexico-
syntactic patterns as well as more-complex features,
such as a pattern coupled with its extracted noun,
where the noun is represented both as a lexical term
and by its semantic category. Our experimental re-
sults show that the IE-based enhanced features can
improve performance over unigram and bigram fea-
tures alone.
46
Evaluating the performance of BioNLP tools is
not trivial. So-called intrinsic metrics measure the
performance of a tool against some gold standard of
performance, while extrinsic ones (Alex et al, 2008)
measure how much the overall biocuration process
is benefited. Such metrics necessarily involve the
deployment of the software in-house for testing
by biocurators, and require a large-scale software-
engineering infrastructure effort. In this paper, we
present intrinsic evaluation results of full-text doc-
ument classification experiments to determine auto-
matically whether a paper should be considered of
interest to MGI curators. We plan in-house deploy-
ment and extrinsic evaluation in near-term work.
Our work should be considered as the first step of
a broader process within which (a) the features used
in this particular classification approach will be re-
engineered so that they may be dynamically recre-
ated in any new domain by a reusable component,
(b) this component is deployed into reusable infras-
tructure that also includes document-, annotation-
and feature-storage capabilities that support scaling
and reuse, and (c) the overall functionality can then
be delivered as a software application to biocurators
themselves for extrinsic evaluation in any domain
they choose. Within the ?SciKnowMine? project, we
are constructing such a framework (Ramakrishnan et
al., 2010), and this work reported here forms a pro-
totype component that we plan to incorporate into
a live application. We describe the underlying NLP
research here, and provide context for the work by
describing the overall design and implementation of
the SciKnowMine infrastructure.
1.1 Motivation
MGI?s biocurators use very specific guidelines for
triage that continuously evolve. These guidelines
are tailored to specific subcategories within MGI?s
triage task (phenotype, Gene Ontology1 (GO) term,
gene expression, tumor biology and chromosomal
location mapping). They help biocurators decide
whether a paper is relevant to one or more subcat-
egories. As an example, consider the guideline for
the phenotype category shown in Table 1.
This example makes clear that it is not sufficient
to match on relevant words like ?transgene? alone.
1http://www.geneontology.org/
?Select paper
If: it is about transgenes where a gene from any
species is inserted in mice and this results in
a phenotype.
Except: if the paper uses transgenes to
examine promoter function?.
Table 1: Sample triage guideline used by MGI biocura-
tors
To identify a paper as being ?within-scope? or ?out-
of-scope? requires that a biocurator understand the
context of the experiment described in the paper.
To check this we examined two sample papers; one
that matches the precondition of the above rule and
another that matches its exception. The first paper
(Sjo?gren et al, 2009) is about a transgene inser-
tion causing a pheotype and is a positive example
of the category phenotype, while the second paper
(Bouatia-Naji et al, 2010) is about the use of trans-
genes to study promoter function and is a negative
example for the same category.
Inspection of the negative-example paper illus-
trates the following issues concerning the language
used: (1) This paper is about transgene-use in study-
ing promoter function. Understanding this requires
the following background knowledge: (a) the two
genes mentioned in the title are transgenes; (b) the
phrase ?elevation of fasting glucose levels? in the ti-
tle represents an up-regulation phenotype event. (2)
Note that the word ?transgene? never occurs in the
entire negative-example paper. This suggests that
recognizing that a paper involves the use of trans-
genes requires annotation of domain-specific enti-
ties and a richer representation than that offered by
a simple bag-of-words model.
Similar inspection of the positive-example paper
reveals that (3) the paper contains experimental ev-
idence showing the phenotype resulting from the
transgene insertion. (4) The ?Materials and Meth-
ods? section of the positive-example paper clearly
identifies the construction of the transgene and the
?Results? section describes the development of the
transgenic mouse model used in the study. (3)
and (4) above suggest that domain knowledge about
complex biological phenomena (events) such as
phenotype and experimental protocol may be help-
ful for the triage task.
47
Together, points (1)?(4) suggest that different
sections of a paper contain additional important
context-specific clues. The example highlights the
complex nature of the triage task facing the MGI
biocurators. At present, this level of nuanced ?un-
derstanding? of content semantics is extremely hard
for machines to replicate. Nonetheless, merely treat-
ing the papers as a bag-of-words is unlikely to make
nuanced distinctions between positive and negative
examples with the level of precision and recall re-
quired in MGI?s triage task.
In this paper we therefore describe: (1) the design
and performance of a classifier that is enriched with
three types of features, all derived from informa-
tion extraction: (a) lexico-syntactic patterns, (b) pat-
terns coupled with lexical extractions, and (c) pat-
terns coupled with semantic extractions. We com-
pare the enriched classifier against classifiers that
use only unigram and bigram features; (2) the de-
sign of a biocuration application for MGI along with
the first prototype system where we emphasize the
infrastructure necessary to support the engineering
of domain-specific features of the kind described
in the examples above. Our application is based
on Unstructured Information Management Architec-
ture (UIMA) (Ferrucci and Lally, 2004), which is
a pipeline-based framework for the development of
software systems that analyze large volumes of un-
structured information.
2 Information Extraction for Triage
Classification
In this section, we present the information extraction
techniques that we used as the basis for our IE-based
features, and we describe the three types of IE fea-
tures that we incorporated into the triage classifier.
2.1 Information Extraction Techniques
Information extraction (IE) includes a variety of
techniques for extracting factual information from
text. We focus on pattern-based IE methods
that were originally designed for event extrac-
tion. Event extraction systems identify the role
fillers associated with events. For example, con-
sider the task of extracting information from dis-
ease outbreak reports, such as ProMed-mail arti-
cles (http://www.promedmail.org/). In contrast to a
named entity recognizer, which should identify all
mentions of diseases and people, an event extraction
system should only extract the diseases involved in
an outbreak incident and the people who were the
victims. Other mentions of diseases (e.g., in histori-
cal discussions) or people (e.g., doctors or scientists)
should be discarded.
We utilized the Sundance/AutoSlog software
package (Riloff and Phillips, 2004), which is freely
available for research. Sundance is an information
extraction engine that applies lexico-syntactic pat-
terns to extract noun phrases from specific linguistic
contexts. Sundance performs its own syntactic anal-
ysis, which includes morphological analysis, shal-
low parsing, clause segmentation, and syntactic role
assignment (i.e., identifying subjects and direct ob-
jects of verb phrases). Sundance labels verb phrases
with respect to active/passive voice, which is im-
portant for event role labelling. For example, ?Tom
Smith was diagnosed with bird flu? means that Tom
Smith is a victim, but ?Tom Smith diagnosed the el-
derly man with bird flu? means that the elderly man
is the victim.
Sundance?s information extraction engine can ap-
ply lexico-syntactic patterns to extract noun phrases
that participate in syntactic relations. Each pat-
tern represents a linguistic expression, and extracts
a noun phrase (NP) argument from one of three syn-
tactic positions: Subject, Direct Object, or Prepo-
sitional Phrase. Patterns may be defined manu-
ally, or they can be generated by the AutoSlog pat-
tern generator (Riloff, 1993), which automatically
generates patterns from a domain-specific text cor-
pus. AutoSlog uses 17 syntactic ?templates? that are
matched against the text. Lexico-syntactic patterns
are generated by instantiating the matching words in
the text with the syntactic template. For example,
five of AutoSlog?s syntactic templates are shown in
Table 2:
(a) <SUBJ> PassVP
(b) PassVP Prep <NP>
(c) <SUBJ> ActVP
(d) ActVP Prep <NP>
(e) Subject PassVP Prep <NP>
Table 2: Five example syntactic templates (PassVP
means passive voice verb phrase, ActVP means active
voice verb phrase)
48
Pattern (a) matches any verb phrase (VP) in a pas-
sive voice construction and extracts the Subject of
the VP. Pattern (b) matches passive voice VPs that
are followed by a prepositional phrase. The NP
in the prepositional phrase is extracted. Pattern (c)
matches any active voice VP and extracts its Subject,
while Pattern (d) matches active voice VPs followed
by a prepositional phrase. Pattern (e) is a more com-
plex pattern that requires a specific Subject2, passive
voice VP, and a prepositional phrase. We applied the
AutoSlog pattern generator to our corpus (described
in Section 3.1) to exhaustively generate every pat-
tern that occurs in the corpus.
As an example, consider the following sentence,
taken from an article in PLoS Genetics:
USP14 is endogenously expressed in
HEK293 cells and in kidney tissue derived
from wt mice.
<SUBJ> PassVP(expressed)
<SUBJ> ActVP(derived)
PassVP(expressed) Prep(in) <NP>
ActVP(derived) Prep(from) <NP>
Subject(USP14) PassVP(expressed) Prep(in) <NP>
Table 3: Lexico-syntactic patterns for the PLoS Genetics
sentence shown above.
AutoSlog generates five patterns from this sen-
tence, which are shown in Table 3:
The first pattern matches passive voice instances
of the verb ?expressed?, and the second pattern
matches active voice instances of the verb ?de-
rived?.3 These patterns rely on syntactic analysis,
so they will match any syntactically appropriate con-
struction. For example, the first pattern would match
?was expressed?, ?were expressed?, ?have been ex-
pressed? and ?was very clearly expressed?. The third
and fourth patterns represent the same two VPs but
also require the presence of a specific prepositional
phrase. The prepositional phrase does not need to
be adjacent to the VP, so long as it is attached to
the VP syntactically. The last pattern is very spe-
cific and will only match passive voice instances of
2Only the head nouns must match.
3Actually, the second clause is in reduced passive voice (i.e.,
tissue that was derived from mice), but the parser misidentifies
it as an active voice construction.
?expressed? that also have a Subject with a particular
head noun (?USP14?) and an attached prepositional
phrase with the preposition ?in?.
The example sentence contains four noun phrases,
which are underlined. When the patterns generated
by AutoSlog are applied to the sentence, they pro-
duce the following NP extractions (shown in bold-
face in Table 4):
<USP14> PassVP(expressed)
<kidney tissue> ActVP(derived)
PassVP(expressed) Prep(in) <HEK293 cells>
ActVP(derived) Prep(from) <wt mice>
Subject(USP14) PassVP(expressed) Prep(in) <HEK293
cells>
Table 4: Noun phrase extractions produced by Sundance
for the sample sentence.
In the next section, we explain how we use the in-
formation extraction system to produce rich linguis-
tic features for our triage classifier.
2.2 IE Pattern Features
For the triage classification task, we experimented
with four types of IE-based features: Patterns, Lexi-
cal Extractions, and Semantic Extractions.
The Pattern features are the lexico-syntactic IE
patterns. Intuitively, each pattern represents a phrase
or expression that could potentially capture contexts
associated with mouse genomics better than isolated
words (unigrams). We ran the AutoSlog pattern gen-
erator over the training set to exhaustively generate
every pattern that appeared in the corpus. We then
defined one feature for each pattern and gave it a
binary feature value (i.e., 1 if the pattern occurred
anywhere in the document, 0 otherwise).
We also created features that capture not just the
pattern expression, but also its argument. The Lex-
ical Extraction features represent a pattern paired
with the head noun of its extracted noun phrase.
Table 5 shows the Lexical Extraction features that
would be generated for the sample sentence shown
earlier. Our hypothesis was that these features could
help to distinguish between contexts where an activ-
ity is relevant (or irrelevant) to MGI because of the
combination of an activity and its argument.
The Lexical Extraction features are very specific,
requiring the presence of multiple terms. So we
49
PassVP(expressed), USP14
ActVP(derived), tissue
PassVP(expressed) Prep(in), cells
ActVP(derived) Prep(from), mice
Subject(USP14) PassVP(expressed) Prep(in), cells
Table 5: Lexical Extraction features
also experimented with generalizing the extracted
nouns by replacing them with a semantic category.
To generate a semantic dictionary for the mouse ge-
nomics domain, we used the Basilisk bootstrapping
algorithm (Thelen and Riloff, 2002). Basilisk has
been used previously to create semantic lexicons for
terrorist events (Thelen and Riloff, 2002) and senti-
ment analysis (Riloff et al, 2003), and recent work
has shown good results for bioNLP domains using
similar bootstrapping algorithms (McIntosh, 2010;
McIntosh and Curran, 2009).
As input, Basilisk requires a domain-specific text
corpus (unannotated) and a handful of seed nouns
for each semantic category to be learned. A boot-
strapping algorithm then iteratively hypothesizes ad-
ditional words that belong to each semantic cat-
egory based on their association with the seed
words in pattern contexts. The output is a lexicon
of nouns paired with their corresponding semantic
class. (e.g., liver : BODY PART).
We used Basilisk to create a lexicon for eight se-
mantic categories associated with mouse genomics:
BIOLOGICAL PROCESS, BODY PART, CELL TYPE,
CELLULAR LOCATION, BIOLOGICAL SUBSTANCE,
EXPERIMENTAL REAGENT, RESEARCH SUBJECT,
TUMOR. To choose the seed nouns, we parsed
the training corpus, ranked all of the nouns by fre-
quency4, and selected the 10 most frequent, unam-
biguous nouns belonging to each semantic category.
The seed words that we used for each semantic cat-
egory are shown in Table 6.
Finally, we defined Semantic Extraction features
as a pair consisting of a pattern coupled with the
semantic category of the noun that it extracted. If
the noun was not present in the semantic lexicons,
then no feature was created. The Basilisk-generated
lexicons are not perfect, so some entries will be in-
correct. But our hope was that replacing the lexical
terms with semantic categories might help the clas-
4We only used nouns that occurred as the head of a NP.
BIOLOGICAL PROCESS: expression, ac-
tivity, activation, development, function,
production, differentiation, regulation, re-
duction, proliferation
BODY PART: brain, muscle, thymus, cor-
tex, retina, skin, spleen, heart, lung, pan-
creas
CELL TYPE: neurons, macrophages, thy-
mocytes, splenocytes, fibroblasts, lym-
phocytes, oocytes, monocytes, hepato-
cytes, spermatocytes
CELLULAR LOCATION: receptor, nu-
clei, axons, chromosome, membrane, nu-
cleus, chromatin, peroxisome, mitochon-
dria, cilia
BIOLOGICAL SUBSTANCE: antibody,
lysates, kinase, cytokines, peptide, anti-
gen, insulin, ligands, peptides, enzyme
EXPERIMENTAL REAGENT: buffer,
primers, glucose, acid, nacl, water, saline,
ethanol, reagents, paraffin
RESEARCH SUBJECT: mice, embryos,
animals, mouse, mutants, patients, litter-
mates, females, males, individuals
TUMOR: tumors, tumor, lymphomas,
tumours, carcinomas, malignancies,
melanoma, adenocarcinomas, gliomas,
sarcoma
Table 6: Seed words given to Basilisk
sifier learn more general associations. For exam-
ple, ?PassVP(expressed) Prep(in), CELLULAR LO-
CATION? will apply much more broadly than the
corresponding lexical extraction with just one spe-
cific cellular location (e.g., ?mitochondria?).
Information extraction patterns and their argu-
ments have been used for text classification in pre-
vious work (Riloff and Lehnert, 1994; Riloff and
Lorenzen, 1999), but the patterns and arguments
were represented separately and the semantic fea-
tures came from a hand-crafted dictionary. In con-
trast, our work couples each pattern with its ex-
tracted argument as a single feature, uses an auto-
matically generated semantic lexicon, and is the first
application of these techniques to the biocuration
triage task.
50
3 Results
3.1 Data Set
For our experiments in this paper we use articles
within the PubMed Central (PMC) Open Access
Subset5. From this subset we select all articles that
are published in journals of interest to biocurators
at MGI. This results in a total of 14,827 documents
out of which 981 have been selected manually by
MGI biocurators as relevant (referred to as IN docu-
ments). This leaves 13,846 that are presumably out
of scope (referred to as OUT documents), although
it was not guaranteed that all of them had been man-
ually reviewed so some relevant documents could be
included as well. (We plan eventually to present to
the biocurators those papers not included by them
but nonetheless selected by our tools as IN with
high confidence, for possible reclassification. Such
changes will improve the system?s evaluated score.)
As preprocessing for the NLP tools, we split
the input text into sentences using the Lin-
gua::EN::Sentence perl package. We trimmed non-
alpha-numerics attached before and after words.
We also removed stop words using the Lin-
gua::EN::StopWords package.
3.2 Classifier
We used SVM Light6(Joachims, 1999) for all of our
experiments. We used a linear kernel and a tol-
erance value of 0.1 for QP solver termination. In
preliminary experiments, we observed that the cost
factor (C value) made a big difference in perfor-
mance. In SVMs, the cost factor represents the
importance of penalizing errors on the training in-
stances in comparison to the complexity (general-
ization) of the model. We observed that higher val-
ues of C produced increased recall, though at the ex-
pense of some precision. We used a tuning set to
experiment with different values of C, trying a wide
range of powers of 2. We found that C=1024 gen-
erally produced the best balance of recall and preci-
sion, so we used that value throughout our experi-
ments.
5http://www.ncbi.nlm.nih.gov/pmc/about/
openftlist.html
6http://svmlight.joachims.org/
3.3 Experiments
We randomly partitioned our text corpus into 5 sub-
sets of 2,965 documents each.7 We used the first 4
subsets as the training set, and reserved the fifth sub-
set as a blind test set.
In preliminary experiments, we found that the
classifiers consistently benefitted from feature se-
lection when we discarded low-frequency features.
This helps to keep the classifier from overfitting to
the training data. For each type of feature, we set
a frequency threshold ? and discarded any features
that occurred fewer than ? times in the training set.
We chose these ? values empirically by performing
4-fold cross-validation on the training set. We eval-
uated ? values ranging from 1 to 50, and chose the
value that produced the highest F score. The ? val-
ues that were selected are: 7 for unigrams, 50 for
bigrams, 35 for patterns, 50 for lexical extractions,
and 5 for semantic extractions.
Finally, we trained an SVM classifier on the en-
tire training set and evaluated the classifier on the
test set. We computed Precision (P), Recall (R), and
the F score, which is the harmonic mean of preci-
sion and recall. Precision and recall were equally
weighted, so this is sometimes called an F1 score.
Table 7 shows the results obtained by using each
of the features in isolation. The lexical extraction
features are shown as ?lexExts? and the semantic ex-
traction features are shown as ?semExts?. We also
experimented with using a hybrid extraction fea-
ture, ?hybridExts?, which replaced a lexical extrac-
tion noun with its semantic category when one was
available but left the noun as the extraction term
when no semantic category was known.
Table 7 shows that the bigram features produced
the best Recall (65.87%) and F-Score (74.05%),
while the hybrid extraction features produced the
best Precision (85.52%) but could not match the bi-
grams in terms of recall. This is not surprising be-
cause the extraction features on their own are quite
specific, often requiring 3-4 words to match.
Next, we experimented with adding the IE-based
features to the bigram features to allow the classifier
to choose among both feature sets and get the best
of both worlds. Combining bigrams with IE-based
7Our 5-way random split left 2 documents aside, which we
ignored for our experiments.
51
Feature P R F
unigrams 79.75 60.58 68.85
bigrams 84.57 65.87 74.05
patterns 78.98 59.62 67.95
lexExts 76.54 59.62 67.03
semExts 72.39 46.63 56.73
hybridExts 85.52 59.62 70.25
bigrams + patterns 84.87 62.02 71.67
bigrams + lexExts 85.28 66.83 74.93
bigrams + semExts 85.43 62.02 71.87
bigrams + hybridExts 87.10 64.90 74.38
Table 7: Triage classifier performance using different sets
of features.
features did in fact yield the best results. Using bi-
grams and lexical extraction features achieved both
the highest recall (66.83%) and the highest F score
(74.93%). In terms of overall F score, we see a rela-
tively modest gain of about 1% by adding the lexical
extraction features to the bigram features, which is
primarily due to the 1% gain in recall.
However, precision is of paramount importance
for many applications because users don?t want to
wade through incorrect predictions. So it is worth
noting that adding the hybrid extraction features to
the bigram features produced a 2.5% increase in pre-
cision (84.57% ? 87.10%) with just a 1% drop in
recall. This recall/precision trade-off is likely to be
worthwhile for many real-world application settings,
including biocuration.
4 Biocuration Application for MGI
Developing an application that supports MGI biocu-
rators necessitates an application design that mini-
mally alters existing curation workflows while main-
taining high classification F-scores (intrinsic mea-
sures) and speeding up the curation process (extrin-
sic measures). We seek improvements with respect
to intrinsic measures by engineering context-specific
features and seek extrinsic evaluations by instru-
menting the deployed triage application to record us-
age statistics that serve as input to extrinsic evalua-
tion measures.
4.1 Software Architecture
As stated earlier, one of our major goals is to build,
deploy, and extrinsically evaluate an NLP-assisted
curation application (Alex et al, 2008) for triage at
MGI. By definition, an extrinsic evaluation of our
triage application requires its deployment and sub-
sequent tuning to obtain optimal performance with
respect to extrinsic evaluation criteria. We antici-
pate that features, learning parameters, and training
data distributions may all need to be adjusted during
a tuning process. Cognizant of these future needs,
we have designed the SciKnowMine system so as
to integrate the various components and algorithms
using the UIMA infrastructure. Figure 1 shows a
schematic of SciKnowMine?s overall architecture.
4.1.1 Building configurable & reusable UIMA
pipelines
The experiments we have presented in this paper
have been conducted using third party implementa-
tions of a variety of algorithms implemented on a
wide variety of platforms. We use SVMLight to
train a triage classifier on features that were pro-
duced by AutoSlog and Sundance on sentences iden-
tified by the perl package Lingua::EN::Sentence.
Each of these types of components has either been
reimplemented or wrapped as a component reusable
in UIMA pipelines within the SciKnowMine in-
frastructure. We hope that building such a li-
brary of reusable components will help galvanize the
BioNLP community towards standardization of an
interoperable and open-access set of NLP compo-
nents. Such a standardization effort is likely to lower
the barrier-of-entry for NLP researchers interested in
applying their algorithms to knowledge engineering
problems in Biology (such as biocuration).
4.1.2 Storage infrastructure for annotations &
features
As we develop richer section-specific and
context-specific features we anticipate the need for
provenance pertaining to classification decisions for
a given paper. We have therefore built an Annotation
Store and a Feature Store collectively referred to as
the Classification Metadata Store8 in Figure 1. Fig-
ure 1 also shows parallel pre-processing populating
the annotation store. We are working on develop-
ing parallel UIMA pipelines that extract expensive
(resource & time intensive) features (such as depen-
8Our classification metadata store has been implemented us-
ing Solr http://lucene.apache.org/solr/
52
dency parses).The annotation store holds features
produced by pre-processing pipelines. The annota-
tion store has been designed to support query-based
composition of feature sets specific to a classifica-
tion run. These feature sets can be asserted to the
feature store and reused later by any pipeline. This
design provides us with the flexibility necessary to
experiment with a wide variety of features and tune
our classifiers in response to feedback from biocura-
tors.
5 Discussions & Conclusions
In this paper we have argued the need for richer se-
mantic features for the MGI biocuration task. Our
results show that simple lexical and semantic fea-
tures used to augment bigram features can yield
higher classification performance with respect to in-
trinsic metrics (such as F-Score). It is noteworthy
that using a hybrid of lexical and semantic features
results in the highest precision of 87%.
In our motivating example, we have proposed
the need for sectional-zoning of articles and have
demonstrated that certain zones like the ?Materi-
als and Methods? section can contain contextual
features that might increase classification perfor-
mance. It is clear from the samples of MGI man-
ual classification guidelines that biocurators do, in
fact, use zone-specific features in triage. It there-
fore seems likely that section specific feature ex-
traction might result in better classification perfor-
mance in the triage task. Our preliminary analysis of
the MGI biocuration guidelines suggests that exper-
imental procedures described in the ?Materials and
Methods? seem to be a good source of triage clues.
We therefore propose to investigate zone and context
specific features and the explicit use of domain mod-
els of experimental procedure as features for docu-
ment triage.
We have also identified infrastructure needs aris-
ing within the construction of a biocuration applica-
tion. In response we have constructed preliminary
versions of metadata stores and UIMA pipelines to
support MGI?s biocuration. Our next step is to de-
ploy a prototype assisted-curation application that
uses a classifier trained on the best performing fea-
tures discussed in this paper. This application will
be instrumented to record usage statistics for use in
extrinsic evaluations (Alex et al, 2008). We hope
that construction on such an application will also
engender the creation of an open environment for
NLP scientists to apply their algorithms to biomedi-
cal corpora in addressing biomedical knowledge en-
gineering challenges.
6 Acknowledgements
This research is funded by the U.S. National Sci-
ence Foundation under grant #0849977 for the
SciKnowMine project (http://sciknowmine.
isi.edu/). We wish to acknowledge Kevin Co-
hen for helping us collect the seed terms for Basilisk
and Karin Verspoor for discussions regarding feature
engineering.
References
[Alex et al2008] Beatrice Alex, Claire Grover, Barry
Haddow, Mijail Kabadjov, Ewan Klein, Michael
Matthews, Stuart Roebuck, Richard Tobin, and Xin-
glong Wang. 2008. Assisted curation: does text min-
ing really help? Pacific Symposium On Biocomputing,
567:556?567.
[Bouatia-Naji et al2010] Nabila Bouatia-Naji, Ame?lie
Bonnefond, Devin A Baerenwald, Marion Marchand,
Marco Bugliani, Piero Marchetti, Franc?ois Pattou,
Richard L Printz, Brian P Flemming, Obi C Umu-
nakwe, Nicholas L Conley, Martine Vaxillaire, Olivier
Lantieri, Beverley Balkau, Michel Marre, Claire Le?vy-
Marchal, Paul Elliott, Marjo-Riitta Jarvelin, David
Meyre, Christian Dina, James K Oeser, Philippe
Froguel, and Richard M O?Brien. 2010. Genetic and
functional assessment of the role of the rs13431652-
A and rs573225-A alleles in the G6PC2 promoter that
are strongly associated with elevated fasting glucose
levels. Diabetes, 59(10):2662?2671.
[Bourne and McEntyre2006] Philip E Bourne and Jo-
hanna McEntyre. 2006. Biocurators: Contributors to
the World of Science. PLoS Computational Biology,
2(10):1.
[Cohen and Hersh2006] Aaron M Cohen and William R
Hersh. 2006. The TREC 2004 genomics track cate-
gorization task: classifying full text biomedical docu-
ments. Journal of Biomedical Discovery and Collab-
oration, 1:4.
[Ferrucci and Lally2004] D Ferrucci and A Lally. 2004.
Building an example application with the Unstructured
Information Management Architecture. IBM Systems
Journal, 43(3):455?475.
53
Citation
Store
Feature
Store 
(token trigrams, 
stemmed bigrams)
Document
Store
MGI
training
Corpus
Digital Library
Parallel
pre-processing pipelines
Classification Metadata Store
Annotation Store 
(token, Sundance 
patterns, parse trees,)
Classifier training
pipeline
trained triage 
classification 
model
Ranked Triage ResultsDigital Library
MGI Biocuration Application
Newly 
published
papers
Figure 1: Design schematic of the MGI biocuration application. The components of the application are: (A) Digital
Library composed of a citation store and document store. (B) Pre-processing UIMA pipelines which are a mecha-
nism to pre-extract standard features such as parse trees, tokenizations etc. (C) Classification Metadata Store which
is composed of an Annotation Store for the pre-extracted standard features from (B), and a Feature Store to hold de-
rived features constructed from the standard ones in the Annotation Store. (D) Classifier training pipeline. (E) MGI
Biocuration Application.
[Hersh W2005] Yang J Bhupatiraju RT Roberts P M.
Hearst M Hersh W, Cohen AM. 2005. TREC 2005
genomics track overview. In The Fourteenth Text Re-
trieval Conference.
[Joachims1999] Thorsten Joachims. 1999. Making
Large-Scale SVM Learning Practical. Advances in
Kernel Methods Support Vector Learning, pages 169?
184.
[McIntosh and Curran2009] T. McIntosh and J. Curran.
2009. Reducing Semantic Drift with Bagging and
Distributional Similarity. In Proceedings of the 47th
Annual Meeting of the Association for Computational
Linguistics.
[McIntosh2010] Tara McIntosh. 2010. Unsupervised dis-
covery of negative categories in lexicon bootstrapping.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, number Oc-
tober, pages 356?365. Association for Computational
Linguistics.
[Ramakrishnan et al2010] Cartic Ramakrishnan, William
A Baumgartner Jr, Judith A Blake, Gully A P C
Burns, K Bretonnel Cohen, Harold Drabkin, Janan
Eppig, Eduard Hovy, Chun-Nan Hsu, Lawrence E
Hunter, Tommy Ingulfsen, Hiroaki Rocky Onda,
Sandeep Pokkunuri, Ellen Riloff, and Karin Verspoor.
2010. Building the Scientific Knowledge Mine ( Sci-
KnowMine 1 ): a community- driven framework for
text mining tools in direct service to biocuration. In
proceeding of Workshop ?New Challenges for NLP
Frameworks? collocated with The seventh interna-
tional conference on Language Resources and Eval-
uation (LREC) 2010.
[Riloff and Lehnert1994] E. Riloff and W. Lehnert. 1994.
Information Extraction as a Basis for High-Precision
Text Classification. ACM Transactions on Information
54
Systems, 12(3):296?333, July.
[Riloff and Lorenzen1999] E. Riloff and J. Lorenzen.
1999. Extraction-based text categorization: Generat-
ing domain-specific role relationships automatically.
In Tomek Strzalkowski, editor, Natural Language In-
formation Retrieval. Kluwer Academic Publishers.
[Riloff and Phillips2004] E. Riloff and W. Phillips. 2004.
An Introduction to the Sundance and AutoSlog Sys-
tems. Technical Report UUCS-04-015, School of
Computing, University of Utah.
[Riloff et al2003] E. Riloff, J. Wiebe, and T. Wilson.
2003. Learning Subjective Nouns using Extraction
Pattern Bootstrapping. In Proceedings of the Seventh
Conference on Natural Language Learning (CoNLL-
2003), pages 25?32.
[Riloff1993] E. Riloff. 1993. Automatically Construct-
ing a Dictionary for Information Extraction Tasks. In
Proceedings of the 11th National Conference on Arti-
ficial Intelligence.
[Sjo?gren et al2009] Klara Sjo?gren, Marie Lagerquist,
Sofia Moverare-Skrtic, Niklas Andersson, Sara H
Windahl, Charlotte Swanson, Subburaman Mohan,
Matti Poutanen, and Claes Ohlsson. 2009. Elevated
aromatase expression in osteoblasts leads to increased
bone mass without systemic adverse effects. Journal
of bone and mineral research the official journal of
the American Society for Bone and Mineral Research,
24(7):1263?1270.
[Thelen and Riloff2002] M. Thelen and E. Riloff. 2002.
A Bootstrapping Method for Learning Semantic Lexi-
cons Using Extraction Pa ttern Contexts. In Proceed-
ings of the 2002 Conference on Empirical Methods in
Natural Language Processing, pages 214?221.
55
Proceedings of BioNLP Shared Task 2011 Workshop, pages 89?93,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
The Taming of Reconcile as a Biomedical Coreference Resolver 
  Youngjun Kim,    Ellen Riloff,    Nathan Gilbert School of Computing University of Utah Salt Lake City, UT {youngjun, riloff, ngilbert} @cs.utah.edu       Abstract 
To participate in the Protein Coreference section of the BioNLP 2011 Shared Task, we use Reconcile, a coreference resolution engine, by replacing some pre-processing components and adding a new mention detector. We got some improvement from training two separate classifiers for detecting anaphora and antecedent mentions. Our system yielded the highest score in the task, F-score 34.05% in partial mention, protein links, and system recall mode. We witnessed that specialized mention detection is crucial for coreference resolution in the biomedical domain.  
1 Introduction Coreference resolution is a mechanism that groups entity mentions in a text into coreference chains based on whether they refer to the same real-world entity or concept. Like other NLP applications, which must meet the need for aggressive and sophisticated methods of detecting valuable information in emerging domains, numerous coreference resolvers have been developed, including JavaRap (Qiu et al, 2004), GuiTaR (Poesio and Kabadjov, 2004) and BART (Versley et al, 2008). Our research uses a recently released system, Reconcile (Stoyanov et al 2009; 2010a; 2010b), which was designed as a general architecture for coreference resolution that can be used to easily create learning-based coreference resolvers. Reconcile is based on supervised learning approaches to coreference resolution and 
has showed relatively good performance compared with similar types of systems.  As a first step to adapting Reconcile for the biomedical domain, specifically the BioNLP Shared Task 2011 (Kim et al, 2011), we modified several subcomponents in Reconcile and revised the feature set for this task. Most importantly, we created a specialized mention detector trained for biomedical text. We trained separate classifiers for detecting anaphor and antecedent mentions, and experimented with several clustering techniques to discover the most suitable algorithm for producing coreference chains in this domain. 2 BioNLP 2011 Shared Task  Our system was developed to participate in a Protein Coreference (COREF) task (Nguyen et al, 2011), one of the supporting tasks in the BioNLP Shared Task 2011. The COREF task is to find all mentions participating in the coreference relation and to connect the anaphora-antecedent pairs. The corpus is based on the Genia-Medco coreference corpus. The Genia-Medco corpus was produced for the biomedical domain, and some comparative analysis with this corpus and other newswire domain data have been performed (Yang et al, 2004a; 2004b; Nguyen and Kim, 2008; Nguyen et al, 2008).  The COREF corpus consists of 800 text files for training, 150 for development, and 260 for testing, which all have gene/protein coreference annotations. The training set has 2,313 pairs of coreference links with 4,367 mentions. 2,117 mentions are antecedents, with an average of 4.21 tokens each (delimited by white space), and 2,301 
89
mentions are anaphora, with an average of 1.28 tokens each. The anaphora are much shorter because many of them are pronouns. The five most frequent anaphora are that (686 times), which (526), its (270), their (130), and it (100).  3 Our Coreference Resolver Reconcile was designed to be a research testbed capable of implementing the most current approaches to coreference resolution. Reconcile is written in Java, to be portable across platforms, and was designed to be easily reconfigurable with respect to sub-components, feature sets, parameter settings, etc. A mention detector and an anaphora-antecedent pairs generator are added for the COREF task. 3.1 Preprocessing For pre-processing, we used the Genia Tagger (Tsuruoka and Tsujii. 2005) for sentence splitting, tokenizing, and part-of-speech (POS) tagging. For parsing, we used the Enju parser (Miyao and Tsujii, 2008). We replaced Reconcile?s mention detection module with new classifiers because of poor performance on the biomedical domain with the provided classifiers. We reformatted the training data with IOB tags and trained a sequential classifier using CRF++ (Kudoh, 2007). For this sequence tagging, we borrowed the features generally used for named entity recognition in the biomedical literature (Finkel et al, 2005; Zhou et al, 2005; McDonald and Pereira, 2005), including word, POS, affix, orthographic features and combinations of these features. We extracted features from the target word, as well as two words to its left and two words to its right. Two versions of mention detectors were developed. The first (MD-I) trained one model without differentiating between anaphora and antecedents. For this method, we chose the longest mentions when multiple mentions overlapped. The other detector (MD-II) used two different models for the antecedent and anaphor, classifying them separately. MD-II?s classification result was used when generating the anaphora-antecedent pairs. Table 1 shows the performance of exact matching by these detectors compared with the performance of the Genia Noun Phrase (NP) chunker. Our classifiers did much better, 81.31% precision and 
64.78% recall (MD-II), than the Genia chunker, 6.58% precision and 72.67% recall. Only an average of six mentions occurred in each text, while the Genia chunker detected 66.27 noun phrases on average. The Genia annotation scheme was not limited to specific types of concepts, so the Genia NP chunker identifies every possible concept. In contrast, the COREF shared task only involves a subset of the concepts. Mention boundaries were also frequently mismatched. For example, ?its? was annotated as a mention in the COREF task when it appears as a possessive inside a noun phrase (e.g., ?its activity?), but the Genia NP Chunker tags the entire noun phrase as a mention.   Prec Rec F Genia NP Chunker   6.58 72.67 12.07 Mention Detector-I 80.85 63.33 71.03 Mention Detector-II 81.31 64.78 72.11     Antecedent 65.48 41.35 50.69     Anaphor 91.72 85.07 88.27 Table 1:Mention Detection Results on Dev. Set 3.2 Feature Generation We used the following four types of features: Lexical: String-based comparisons of the two mentions, such as exact string matching and head noun matching.  Proximity: Sentence measures of the distance between two mentions.   Grammatical: A wide variety of syntactic properties of the mentions, either individually or in pairs. These features are based on part-of-speech tags, or parse trees.  Semantic: Semantic information about one or both mentions, such as tests for gender and animacy.  Due to the unavailability of paragraph information in our training data, we excluded Reconcile?s paragraph features. Also, named entity and dependency parsing features were not used for training. Table 2 shows the complete feature set used for this task. In total, we excluded nine existing Reconcile features, mostly semantic features: WordNetClass, WordNetDist, WordNetSense, Subclass, ParNum, SameParagraph, IAntes, Prednom, WordOverlap. Full descriptions of these features can be found in Stoyanov (2010a).  
90
Lexical HeadMatch, PNStr, PNSubstr, ProStr, SoonStr, WordsStr, WordsSubstr Proximity ConsecutiveSentences, SentNum, SameSentence  Syntactic Binding, BothEmbedded, BothInQuotes, BothPronouns, BothProperNouns, BothSubjects, ContainsPN, Contraindices, Definite1, Definite2, Demonstrative2, Embedded1, Embedded2, Indefinite, Indefinite1, InQuote1, InQuote2, MaximalNP, Modifier, PairType, Pronoun, Pronoun1, Pronoun2, ProperNoun, ProResolve, RuleResolve, Span, Subject1, Subject2, Syntax Semantic Agreement, Alias, AlwaysCompatible, Animacy, Appositive, ClosestComp, Constraints, Gender, instClass, Number, ProComp, ProperName, Quantity, WNSynonyms  Table 2: Feature Set for Coreference Resolution 3.3 Clustering After Reconcile makes pairwise decisions linking each anaphor and antecedent, it produces a clustering of the mentions in a document to create coreference chains. Because the format of the COREF task submission was not chains but anaphora-antecedent pairs, it would have been possible to submit the direct results of Reconcile?s pairwise decisions. However, it was easier to use Reconcile as a black-box and post-process the chains to reverse-engineer coreferent pairs from them. Reconcile supports three clustering algorithms: Single-link Clustering (SL) (Transitive Closure) groups together all mentions that are connected by a path of coreferent links.  Best-first (BF) clustering uses the classifier?s confidence value to cluster each noun phrase with its most confident antecedent. Most Recent First (MRF) pairs each noun phrase with the single most recent antecedent that is labeled as coreferent.  Table 3 shows the MUC scores of each clustering method with gold standard mentions and with the mentions automatically detected by each of our two mention detectors. Not surprisingly, using gold mentions produced the highest score of 87.32%. Automatically detected mentions yielded much lower performance. MD-I performed best, in this evaluation, achieving 49.65%. The most recent 
first clustering algorithm produced the best results for both gold mentions and MD-I. The single link clustering algorithm, which is the default method used by Reconcile, produced the lowest results for both gold mentions and MD-I.   SL BF MRF Gold Mention 85.34 86.87 87.32 Mention Detector-I 48.64 48.82 49.65 Mention Detector-II 48.31 48.62 48.07 Table 3: MUC Scores of Dev. Set by Three Different Clustering Methods (SL: Single-link, BF: Best-first, MRF: Most recent first) 3.4 Pair Generation from Chains Reconcile generates coreference chains, but the output for the shared task required anaphora-antecedent pairs. Therefore, we needed to extract individual pairs from the chains. We used the chains produced by the most recent first clustering algorithm for pair generation. When using MD-I output, we took the earliest mention (i.e., the one occurring first in the source document) in the chain and paired it with each of the subsequent mentions in the same chain. Thus, each chain of size N produced N-1 pairs. When using the MD-II predictions, the classifiers gave us two separate lists of antecedent and anaphora mentions. In this case, we paired each anaphor in the chain with every antecedent in the same chain that preceded it in the source document.  3.5 Evaluation and Analysis The mention linking can be evaluated using three different scores: atom coreference links, protein coreference links, and surface coreference links. In the atom link option, only links containing given gene/protein annotations are considered while in the surface link option, every link is a target for the evaluation. Protein links are similar to atom links but loosen the boundary of gene/protein annotations. There were 202 protein links out of 469 surface links in development set.  For mention detection, exact match and partial match are supported in the task evaluation. Recall is measured in two modes. In system mode, every link is calculated for the linking evaluation. In algorithm mode, only links with correctly detected mentions are considered for evaluation. For 
91
detailed information refer to Nguyen et al (2011) or the task web site.1 Table 4 shows the mention linking results (F-score) for the COREF task evaluation using partial match and system recall. The surface link score on gold mentions reached 90.06%. For automatic mention detection, MD-I achieved a score of 45.38% score, but MD-II produced a substantially better score of 50.41%. MD-II, which was trained separately for antecedent and anaphora detection, performed about 5% higher than MD-I in every link mode.    Atom Protein Surface Gold Mention 84.09 84.09 90.06 Mention Detector-I 28.67 34.41 45.38 Mention Detector-II 33.45 39.27 50.41 Table 4: Dev. Set Results by Three Different Evaluation Options Table 5 shows the recall and precision breakdown for the protein evaluation results. Looking behind the composite F-score reveals that our system produced higher precision than recall. Looking back at Table 1, we saw that our anaphor detector performed much better than our antecedent detector. Since every coreference link requires one of each, the relatively poor performance of antecedent detection (especially in terms of recall) is a substantial bottleneck.     Prec Rec F Gold Mention 98.67 73.27 84.09 Mention Detector-I 62.34 23.76 34.41 Mention Detector-II 73.97 26.73 39.27 Table 5: Precision and Recall Breakdown for Protein Evaluation Coreference Links 3.6 Results: Submission for COREF Task We merged the training and development sets to use as training data for Reconcile. We used MD-II for mention detection and the most recent first algorithm for clustering to submit the final output on the test data. Table 6 shows the results of our final submission along with the five other participating teams for the protein evaluation coreference links (Nguyen et al, 2011). Our                                                             1 http://sites.google.com/site/bionlpst/home/protein-gene-coreference-task 
system produced a 34.05% F-score (73.26% precision and 22.18% recall) in protein coreference links and 25.41% F-score in atom links.   Team  Prec  Rec F University of Utah 73.26 22.18 34.05 University of Zurich 55.45 21.48 30.96 Concordia University 63.22 19.37 29.65 University of Turku 67.21 14.44 23.77 University of Szeged   3.47   3.17   3.31 University College Dublin   0.25   0.70   0.37 Table 6: Evaluation Results of Final Submissions (Protein Coreference Links) 4 Conclusions The effort to tame Reconcile as a coreference engine for the biomedical domain was successful and our team?s submission obtained satisfactory results. However, there is ample room for improvement in coreference resolution. We observed that mention detection is crucial - the MUC score reached 87.32% with gold mentions on the development set but only 49.65% with automatically detected mentions (Table 3). One possible avenue for future work is to develop domain-specific features to better identify mentions in biomedical domains.  Acknowledgments  We thank the BioNLP Shared Task 2011 organizers for their efforts, and gratefully acknowledge the support of the National Science Foundation under grants IIS-1018314 and DBI-0849977 and the Defense Advanced Research Projects Agency (DARPA) Machine Reading Program under Air Force Research Laboratory (AFRL) prime contract no. FA8750-09-C-0172. Any opinions, findings, and conclusion or recommendations expressed in this material are those of the authors and do not necessarily express the view of the DARPA, AFRL, NSF, or the U.S. government. References Jenny Finkel, Shipra Dingare, Christopher D Manning, Malvina Nissim, Beatrice Alex, and Claire Grover. 2005. Exploring the Boundaries: Gene and Protein Identifica-tion in Biomedical Text. BMC Bioinformatics. 6:S5. 
92
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert Bossy, and Jun?ichi Tsujii. 2011. Overview of BioNLP Shared Task 2011. Proceedings of the BioNLP 2011 Workshop Companion Volume for Shared Task, Portland, Oregon, June. ACL 2011. Taku Kudoh. 2007. CRF++. http://crfpp.sourceforge.net/. Ryan McDonald and Fernando Pereira. 2005. Identifying Gene and Protein Mentions in Text Using Condi-tional Random Fields. BMC Bioinformatics. 6:S6. Yusuke Miyao and Jun'ichi Tsujii. 2008. Feature Forest Models for Probabilistic HPSG Parsing. Computational Linguistics, 34(1):35?80. Ngan L. T. Nguyen and Jin-Dong Kim. 2008. Exploring Domain Differences for the Design of Pronoun Resolution Systems for Biomedical Text. Proceedings of COLING 2008:625-632 Ngan L. T. Nguyen, Jin-Dong Kim, and Jun?ichi Tsujii. 2008. Challenges in Pronoun Resolution System for Biomedical Text. Proceedings of LREC 2008.  Ngan L. T. Nguyen, Jin-Dong Kim, and Jun?ichi Tsujii. 2011. Overview of the Protein Coreference task in BioNLP Shared Task 2011. Proceedings of the BioNLP 2011 Workshop Companion Volume for Shared Task, Portland, Oregon, June. ACL 2011. Massimo Poesio and Mijail A. Kabadjov. 2004. A general-purpose, off-the-shelf anaphora resolution module: implementation and preliminary evaluation. Proceedings of LREC 2004. Long Qiu, Min-Yen Kan, and Tat-Seng Chua. 2004. A Public Reference Implementation of the Rap Anaphora Resolution Algorithm. Proceedings of LREC 2004. Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen Riloff, David Buttler, and David Hysom. 2010a. Reconcile: A Coreference Resolution Platform. Tech Report. Cornell University. Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen Riloff, David Buttler, and David Hysom. 2010b. Coreference Resolution with Reconcile. Proceedings of ACL 2010. Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and Ellen Riloff. 2009. Conundrums in Noun Phrase Coreference Resolution: Making Sense of the State-of-the-Art. Proceedings of ACL-IJCNLP 2009. Yoshimasa Tsuruoka and Jun'ichi Tsujii. 2005. Bidirectional Inference with the Easiest-First Strategy for Tagging Sequence Data. Proceedings of HLT/EMNLP 2005:467-474. Yannick Versley,  Simone P. Ponzetto, Massimo Poesio, Vladimir Eidelman, Alan Jern, Jason Smith, Xiaofeng 
Yang, and Alessandro Moschitti. 2008. BART: A modular toolkit for coreference resolution. Proceedings of LREC 2008. Xiaofeng Yang, Jian Su, Guodong Zhou, and Chew Lim Tan. 2004a. A NP-Cluster Based Approach to Coreference Resolution. Proceedings of COLING 2004:226-232. XiaoFeng Yang, GuoDong Zhou, Jian Su, and Chew Lim Tan. 2004b. Improving Noun Phrase Coreference Resolution by Matching Strings. Proceedings of IJCNLP 2004:226-333. GuoDong Zhou, Dan Shen, Jie Zhang, Jian Su, and SoonHeng Tan. 2005. Recognition of Protein/Gene Names from Text Using an Ensemble of Classifiers. BMC Bioinformatics. 6:S7. 
93
Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 2?11,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
Bootstrapped Learning of Emotion Hashtags #hashtags4you
Ashequl Qadir
School of Computing
University of Utah
Salt Lake City, UT 84112, USA
asheq@cs.utah.edu
Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT 84112, USA
riloff@cs.utah.edu
Abstract
We present a bootstrapping algorithm to au-
tomatically learn hashtags that convey emo-
tion. Using the bootstrapping framework, we
learn lists of emotion hashtags from unlabeled
tweets. Our approach starts with a small num-
ber of seed hashtags for each emotion, which
we use to automatically label tweets as initial
training data. We then train emotion classi-
fiers and use them to identify and score candi-
date emotion hashtags. We select the hashtags
with the highest scores, use them to automat-
ically harvest new tweets from Twitter, and
repeat the bootstrapping process. We show
that the learned hashtag lists help to improve
emotion classification performance compared
to an N-gram classifier, obtaining 8% micro-
average and 9% macro-average improvements
in F-measure.
1 Introduction
The increasing popularity of social media has given
birth to new genres of text that have been the
focus of NLP research for applications such as
event discovery (Benson et al, 2011), election out-
come prediction (Tumasjan et al, 2011; Berming-
ham and Smeaton, 2011), user profile classification
(De Choudhury et al, 2012), conversation model-
ing (Ritter et al, 2010), consumer insight discovery
(Chamlertwat et al, 2012), etc. A hallmark of so-
cial media is that people tend to share their personal
feelings, often in publicly visible forums. As a re-
sult, social media has also been the focus of NLP
research on sentiment analysis (Kouloumpis et al,
2011), emotion classification and lexicon generation
(Mohammad, 2012), and sarcasm detection (Davi-
dov et al, 2010). Identifying emotion in social me-
dia text could be beneficial for many application ar-
eas, for example to help companies understand how
people feel about their products, to assist govern-
ments in recognizing growing anger or fear associ-
ated with an event, and to help media outlets under-
stand the public?s emotional response toward con-
troversial issues or international affairs.
Twitter, a micro-blogging platform, is particularly
well-known for its use by people who like to in-
stantly express thoughts within a limited length of
140 characters. These status updates, known as
tweets, are often emotional. Hashtags are a distinc-
tive characteristic of tweets, which are a community-
created convention for providing meta-information
about a tweet. Hashtags are created by adding the ?#?
symbol as a prefix to a word or a multi-word phrase
that consists of concatenated words without whites-
pace (e.g., #welovehashtags). People use hashtags
in many ways, for example to represent the topic of
a tweet (e.g., #graduation), to convey additional in-
formation (e.g., #mybirthdaytoday), or to express an
emotion (e.g., #pissedoff).
The usage of hashtags in tweets is common, as
reflected in the study of a sample of 0.6 million
tweets by Wang et al (2011) which found that
14.6% of tweets in their sample had at least one
hashtag. In tweets that express emotion, it is com-
mon to find hashtags representing the emotion felt
by the tweeter, such as ?the new iphone is a waste
of money! nothing new! #angry? denoting anger or
?buying a new sweater for my mom for her birthday!
#loveyoumom? denoting affection.
2
Identifying the emotion conveyed by a hashtag
has not yet been studied by the natural language pro-
cessing community. The goal of our research is to
automatically identify hashtags that express one of
five emotions: affection, anger/rage, fear/anxiety,
joy, or sadness/disappointment. The learned hash-
tags are then used to recognize tweets that express
one of these emotions. We use a bootstrapping ap-
proach that begins with 5 seed hashtags for each
emotion class and iteratively learns more hashtags
from unlabeled tweets. We show that the learned
hashtags can accurately identify tweets that convey
emotion and yield additional coverage beyond the
recall of an N-gram classifier.
The rest of the paper is divided into the following
sections. In Section 2, we present a brief overview
of previous research related to emotion classification
in social media and the use of hashtags. In Sec-
tion 3, we describe our bootstrapping approach for
learning lists of emotion hashtags. In Section 4 we
discuss the data collection process and our experi-
mental design. In Section 5, we present the results
of our experiments. Finally, we conclude by sum-
marizing our findings and presenting directions for
future work.
2 Related Work
Recognizing emotions in social media texts has
grown popular among researchers in recent years.
Roberts et al (2012) investigated feature sets to clas-
sify emotions in Twitter and presented an analysis
of different linguistic styles people use to express
emotions. The research of Kim et al (2012a) is fo-
cused on discovering emotion influencing patterns
to classify emotions in social network conversations.
Esmin et al (2012) presented a 3-level hierarchi-
cal emotion classification approach by differentiat-
ing between emotion vs. non-emotion text, positive
vs. negative emotion, and then classified different
emotions. Yang et al (2007b) investigated sentence
contexts to classify emotions in blogs at the doc-
ument level. Some researchers have also worked
on analyzing the correlation of emotions with topics
and trends. Kim et al (2012b) analyzed correlations
between topics and emotions in Twitter using topic
modeling. Gilbert and Karahalios (2010) analyzed
correlation of anxiety, worry and fear with down-
ward trends in the stock market. Bollen et al (2011)
modeled public mood and emotion by creating six-
dimensional mood vectors to correlate with popular
events that happened in the timeframe of the dataset.
On the other hand, researchers have recently
started to pay attention to the hashtags of tweets, but
mostly to use them to collect labeled data. Davi-
dov et al (2010) used #sarcasm to collect sarcastic
tweets from twitter. Choudhury et al (2012) used
hashtags of 172 mood words to collect training data
to find associations between mood and human af-
fective states, and trained classifiers with unigram
and bigram features to classify these states. Purver
and Battersby (2012) used emotion class name hash-
tags and emoticons as distant supervision in emotion
classification. Mohammad (2012) also used emotion
class names as hashtags to collect labeled data from
Twitter, and used these tweets to generate emotion
lexicons. Wang et al (2012) used a selection of emo-
tion hashtags as the means to acquire labeled data
from twitter, and found that a combination of uni-
grams, bigrams, sentiment/emotion-bearing words,
and parts-of-speech information to be the most ef-
fective in classifying emotions. A study by Wang
et al (2012) also shows that hashtags can be used
to create a high quality emotion dataset. They found
about 93.16% of the tweets having emotion hashtags
were relevant to the corresponding emotion.
However, none of this work investigated the use
of emotion hashtag lists to help classify emotions in
tweets. In cases where hashtags were used to collect
training data, the hashtags were manually selected
for each emotion class. In many cases, only the
name of the emotion classes were used for this pur-
pose. The work most closely related to our research
focus is the work of Wang et al (2011) where they
investigated several graph based algorithms to col-
lectively classify hashtag sentiments. However, their
work is focused on classifying hashtags of positive
and negative sentiment polarities, and they made use
of sentiment polarity of the individual tweets to clas-
sify hashtag sentiments. On the contrary, we learn
emotion hashtags and use the learned hashtag lists
to classify emotion tweets. To the best of our knowl-
edge, we are the first to present a bootstrapped learn-
ing framework to automatically learn emotion hash-
tags from unlabeled data.
3
3 Learning Emotion Hashtags via
Bootstrapping
3.1 Motivation
The hashtags that people use in tweets are often very
creative. While it is common to use just single word
hashtags (e.g., #angry), many hashtags are multi-
word phrases (e.g., #LoveHimSoMuch). People also
use elongated1 forms of words (e.g., #yaaaaay,
#goawaaay) to put emphasis on their emotional
state. In addition, words are often spelled creatively
by replacing a word with a number or replacing
some characters with phonetically similar characters
(e.g., #only4you, #YoureDaBest). While many of
these hashtags convey emotions, these stylistic vari-
ations in the use of hashtags make it very difficult
to create a repository of emotion hashtags manu-
ally. While emotion word lexicons exist (Yang et al,
2007a; Mohammad, 2012), and adding a ?#? symbol
as a prefix to these lexicon entries could potentially
give us lists of emotion hashtags, it would be un-
likely to find multi-word phrases or stylistic varia-
tions frequently used in tweets. This drives our mo-
tivation to automatically learn hashtags that are com-
monly used to express emotion in tweets.
3.2 Emotion Classes
For this research, we selected 5 prominent emo-
tion classes that are frequent in tweets: Af-
fection, Anger/Rage, Fear/Anxiety, Joy and Sad-
ness/Disappointment. We started by analyzing Par-
rott?s (Parrott, 2001) emotion taxonomy and how
these emotions are expressed in tweets. We also
wanted to ensure that the selected emotion classes
would have minimal overlap with each other. We
took Parrott?s primary emotion Joy and Fear2 di-
rectly. We merged Parrott?s secondary emotion Af-
fection and Lust into our Affection class and merged
Parrott?s secondary emotion Sadness and Disap-
pointment into our Sadness/Disappointment class,
since these emotions are often difficult to distinguish
from each other. Lastly, we mapped Parrott?s sec-
ondary emotion Rage to our Anger/Rage class di-
rectly. There were other emotions in Parrott?s tax-
onomy such as Surprise, Neglect, etc. that we did
1This feature has also been found to have a strong associa-
tion with sentiment polarities (Brody and Diakopoulos, 2011)
2we renamed the Fear class as Fear/Anxiety
not use for this research. In addition to the five emo-
tion classes, we used a None of the Above class for
tweets that do not carry any emotion or that carry an
emotion other than one of our five emotion classes.
3.3 Overview of Bootstrapping Framework
Figure 1: Bootstrapping Architecture
Figure 1 presents the framework of our bootstrap-
ping algorithm for learning emotion hashtags. The
algorithm runs in two steps. In the first step, the
bootstrapping process begins with five manually de-
fined ?seed? hashtags for each emotion class. For
each seed hashtag, we search Twitter for tweets that
contain the hashtag and label these tweets with the
emotion class associated with the hashtag. We use
these labeled tweets to train a supervised N-gram
classifier for every emotion e ? E, where E is the
set of emotion classes we are classifying.
In the next step, the emotion classifiers are applied
to a large pool of unlabeled tweets and we collect
the tweets that are labeled by the classifiers. From
these labeled tweets, we extract the hashtags found
in these tweets to create a candidate pool of emo-
tion hashtags. The hashtags in the candidate pool
are then scored and ranked and we select the most
highly ranked hashtags to add to a hashtag reposi-
tory for each emotion class.
Finally, we then search for tweets that contain
the learned hashtags in a pool of unlabeled tweets
and label each of these with the appropriate emotion
class. These newly labeled tweets are added to the
4
set of training instances. The emotion classifiers are
retrained using the larger set of training instances,
and the bootstrapping process continues.
3.4 Seeding
For each of the 5 emotion classes, we manually
selected 5 seed hashtags that we determined to be
strongly representative of the emotion. Before col-
lecting the initial training tweets containing the seed
hashtags, we manually searched in Twitter to en-
sure that these seed hashtags are frequently used by
tweeters. Table 1 presents our seed hashtags.
Emotion Classes Seed Hashtags
AFFECTION #loveyou, #sweetheart, #bff
#romantic, #soulmate
ANGER & #angry, #mad, #hateyou
RAGE #pissedoff, #furious
FEAR & #afraid, #petrified, #scared
ANXIETY #anxious, #worried
JOY #happy, #excited, #yay
#blessed, #thrilled
SADNESS & #sad, #depressed
DISAPPOINT- #disappointed, #unhappy
MENT #foreveralone
Table 1: Seed Emotion Hashtags
3.5 N-gram Tweet Classifier
The tweets acquired using the seed hashtags are used
as training instances to create emotion classifiers
with supervised learning. We first pre-process the
training instances by tokenizing the tweets with a
freely available tokenizer for Twitter (Owoputi et
al., 2013). Although it is not uncommon to express
emotion states in tweets with capitalized characters
inside words, the unique writing styles of the tweet-
ers often create many variations of the same words
and hashtags. We, therefore, normalized case to en-
sure generalization.
We trained one logistic regression classifier for
each emotion class. We chose logistic regression
as the classification algorithm because it produces
probabilities along with each prediction that we later
use to assign scores to candidate emotion hashtags.
As features, we used unigrams to represent all of
the words and hashtags in a tweet, but we removed
the seed hashtags that were used to select the tweets
(or the classifier would simply learn to recognize the
seed hashtags). Our hypothesis is that the seed hash-
tag will not be the only emotion indicator in a tweet,
most of the time. The goal is for the classifier to
learn to recognize words and/or additional hashtags
that are also indicative of the emotion. Additionally,
we removed from the feature set any user mentions
(by looking for words with ?@? prefix). We also re-
moved any word or hashtag from the feature set that
appeared only once in the training data.
For emotion e, we used the tweets containing
seed hashtags for e as the positive training instances
and the tweets containing hashtags for the other
emotions as negative instances. However, we also
needed to provide negative training instances that
do not belong to any of the 5 emotion classes. For
this purpose, we added 100,000 randomly collected
tweets to the training data. While it is possible that
some of these tweets are actually positive instances
for e, our hope is that the vast majority of them will
not belong to emotion e.
We experimented with feature options such as bi-
grams, unigrams with the ?#? symbol stripped off
from hashtags, etc., but the combination of unigrams
and hashtags as features worked the best. We used
the freely available java version of the LIBLINEAR
(Fan et al, 2008) package with its default parameter
settings for logistic regression.
3.6 Learning Emotion Hashtags
The next step is to learn emotion hashtags. We apply
the emotion classifiers to a pool of unlabeled tweets
and collect all of the tweets that the classifier can
label. For each emotion e ? E, we first create a can-
didate pool of emotion hashtags He, by collecting
all of the hashtags in the labeled tweets for emotion
e. To limit the size of the candidate pool, we dis-
carded hashtags with just one character or more than
20 characters, and imposed a frequency threshold of
10. We then score these hashtags to select the top N
emotion hashtags we feel most confident about.
To score each candidate hashtag h ? He, we com-
pute the average of the probabilities assigned by the
logistic regression classifier to all the tweets con-
taining hashtag h. We expect the classifier to as-
sign higher probabilities only to tweets it feels confi-
dent about. Therefore, if h conveys e, we expect that
5
the average probability of all the tweets containing
h will also be high. We select the top 10 emotion
hashtags for each emotion class e, and add them to
our list of learned hashtags for e.
3.7 Adding New Training Instances for
Bootstrapping
To facilitate the next stage of bootstrapping, we col-
lect all tweets from the unlabeled data that contain
hashtag h and label them with the emotion associ-
ated with h. By adding more training instances, we
expect to provide the classifiers with new tweets that
will contain a potentially more diverse set of words
that the classifiers can consider in the next stage of
the bootstrapping.
When the new tweets are added to the training set,
we remove the hashtags from them that we used for
labelling to avoid bias, and the bootstrapping pro-
cess continues. We ran the bootstrapped learning for
100 iterations. Since we learned 10 hashtags during
each iteration, we ended up with emotion hashtag
lists consisting of 1000 hashtags for each emotion.
4 Experimental Setup
4.1 Data Collection
To collect our initial training data, we searched Twit-
ter for the seed hashtags mentioned in Section 3.4
using Twitter?s Search API3 over a period of time.
To ensure that the collected tweets are written in En-
glish, we used a freely available language recognizer
trained for tweets (Carter et al, 2013). We filtered
out tweets that were marked as re-tweets using #rt or
beginning with ?rt?4 because re-tweets are in many
cases exactly the same or very similar to the origi-
nal. We also filtered out any tweet containing a URL
because if such a tweet contains emotion, it is pos-
sible that the emotion indicator may be present only
on the linked website (e.g., a link to a comic strip
followed by an emotion hashtag). After these filter-
ing steps, we ended up with a seed labeled training
dataset of 325,343 tweets.
In addition to the seed labeled data, we collected
random tweets using Twitter?s Streaming API5 over
a period of time to use as our pool of unlabeled
3https://dev.twitter.com/docs/api/1/get/search
4a typical convention to mark a tweet as a re-tweet
5https://dev.twitter.com/docs/streaming-apis
tweets. Like the training data, we filtered out re-
tweets and tweets containing a URL as well as
tweets containing any of the seed hashtags. Since
our research focus is on learning emotion hashtags,
we also filtered out any tweet that did not have at
least one hashtag. After filtering, we ended up with
roughly 2.3 million unlabeled tweets.
4.2 Test Data
Since manual annotation is time consuming, to en-
sure that many tweets in our test data have at least
one of our 5 emotions, we manually selected 25
topic keywords/phrases6 that we considered to be
strongly associated with emotions, but not neces-
sarily any specific emotion. We then searched in
Twitter for any of these topic phrases and their cor-
responding hashtags. These 25 topic phrases are:
Prom, Exam, Graduation, Marriage, Divorce, Hus-
band, Wife, Boyfriend, Girlfriend, Job, Hire, Laid
Off, Retirement, Win, Lose, Accident, Failure, Suc-
cess, Spider, Loud Noise, Chest Pain, Storm, Home
Alone, No Sleep and Interview. Since the purpose of
collecting these tweets is to evaluate the quality and
coverage of the emotion hashtags that we learn, we
filtered out any tweet that did not have at least one
hashtag (other than the topic hashtag).
To annotate tweets with respect to emotion, two
annotators were given definitions of the 5 emotion
classes from Collins English Dictionary7, Parrott?s
(Parrott, 2001) emotion taxonomy of these 5 emo-
tions and additional annotation guidelines. The an-
notators were instructed to label each tweet with up
to two emotions. The instructions specified that the
emotion must be felt by the tweeter at the time the
tweet was written. After several trials and discus-
sions, the annotators reached a satisfactory agree-
ment level of 0.79 Kappa (?) (Carletta, 1996). The
annotation disagreements in these 500 tweets were
then adjudicated, and each annotator labeled an ad-
ditional 2,500 tweets. Altogether this gave us an
emotion annotated dataset of 5,500 tweets. We ran-
domly separated out 1,000 tweets from this collec-
tion as a tuning set, and used the remaining 4,500
tweets as evaluation data.
In Table 2, we present the emotion distribution in
6This data collection process is similar to the emotion tweet
dataset creation by Roberts et al (2012)
7http://www.collinsdictionary.com/
6
tweets that were labeled using the seed hashtags in
the second column. In the next column, we present
the emotion distribution in the tweets that were an-
notated for evaluation by the human annotators.
Emotion Tweets with Evaluation
Seed Hashtags Tweets
AFFECTION 14.38% 6.42%
ANGER/RAGE 14.01% 8.91%
FEAR/ANXIETY 11.42% 13.16%
JOY 37.47% 22.33%
SADNESS/ 23.69% 12.45%
DISAPPOINTMENT
NONE OF THE ABOVE - 42.38%
Table 2: Distribution of emotions in tweets with seed
hashtags and evaluation tweets
4.3 Evaluating Emotion Hashtags
For comparison, we trained logistic regression clas-
sifiers with word unigrams and hashtags as features
for each emotion class, and performed 10-fold cross-
validation on the evaluation data. As a second base-
line for comparison, we added bigrams to the feature
set of the classifiers.
To decide on the optimum size of the lists for
each emotion class, we performed list lookup on the
tuning data that we had set aside before evaluation.
For any hashtag in a tweet in the tuning dataset, we
looked up that hashtag in our learned lists, and if
found, assigned the corresponding emotion as the
label for that tweet. We did this experiment starting
with only seeds in our lists, and incrementally in-
creased the sizes of the lists by 50 hashtags at each
experiment. We decided on the optimum size based
on the best F-measure obtained for each emotion
class. In Table 3, we show the list sizes we found to
achieve the best F-measure for each emotion class in
the tuning dataset.
Emotion List Sizes
AFFECTION 500
ANGER/RAGE 1000
FEAR/ANXIETY 850
JOY 1000
SADNESS/DISAPPOINTMENT 400
Table 3: Optimum list sizes decided from tuning dataset
To use the learned lists of emotion hashtags for
classifying emotions in tweets, we first used them as
features for the logistic regression classifiers. We
created 5 list features with binary values, one for
each emotion class. Whenever a tweet in the evalua-
tion data contained a hashtag from one of the learned
emotion hashtags lists, we set the value of that list
feature to be 1, and 0 otherwise. We used these
5 new features in addition to the word unigrams
and hashtag features, and evaluated the classification
performance of the logistic regression classifiers in
a 10-fold cross-validation setup by calculating pre-
cision, recall and F-measure.
Since the more confident hashtags are added to
the lists at the beginning stages of bootstrapping, we
also tried creating subsets from each list by group-
ing hashtags together that were learned after each 5
iterations of bootstrapping (50 hashtags in each sub-
set). We then created 20 list subset features for each
emotion with binary values, yielding 100 additional
features in total. We also evaluated this feature rep-
resentation of the hashtag lists in a 10-fold cross-
validation setup.
As a different approach, we also used the lists in-
dependently from the logistic regression classifiers.
For any hashtag in the evaluation tweets, we looked
up the hashtag in our learned lists. If the hashtag was
found, we assigned the corresponding emotion class
label to the tweet containing the hashtag. Lastly,
we combined the list lookup decisions with the de-
cisions of the baseline logistic regression classifiers
by taking a union of the decisions, i.e., if either as-
signed an emotion to a tweet, we assigned that emo-
tion as the label for the tweet. We present the results
of these different approaches in Section 5.
5 Results and Analysis
Table 4 shows the precision, recall and F-measure
of the N-gram classifier as well as several different
utilizations of the learned hashtag lists. The first and
the second row in Table 4 correspond to the results
for the baseline unigram classifier (UC) alone and
when bigrams are added to the feature set. These
baseline classifiers had low recall for most emotion
classes, suggesting that the N-grams and hashtags
are not adequate as features to recognize the emotion
classes.
Results of using the hashtag lists as 5 additional
features for the classifier are shown in the third row
7
Affection Anger Fear Joy Sadness
Evaluation Rage Anxiety Disappointment
P R F P R F P R F P R F P R F
Baseline Classifiers
Unigram Classifier (UC) 67 43 52 51 19 28 63 33 43 65 48 55 57 29 39
UC + Bigram Features 70 38 50 52 15 23 64 29 40 65 45 53 57 25 34
Baseline Classifier with List Features
UC + List Features 71 49 58 56 28 37 67 41 51 66 50 57 61 34 44
UC + List Subset Features 73 45 56 58 23 33 69 38 49 66 48 55 61 32 42
List Lookup
Seed Lookup 94 06 11 75 01 03 100 06 11 93 04 08 81 02 05
List Lookup 73 40 52 59 25 35 61 36 45 70 16 26 80 17 28
Baseline Classifier with List Lookup
UC ? Seed Lookup 68 45 54 52 21 30 63 33 44 66 49 56 58 31 40
UC ? List Lookup 63 60 61 52 38 44 56 53 54 64 54 59 59 38 46
Table 4: Emotion classification result (P = Precision, R = Recall, F = F-measure)
of Table 4. The hashtag lists consistently improve
precision and recall across all five emotions. Com-
pared to the unigram classifier, F-measure improved
by 6% for AFFECTION, by 9% for ANGER/RAGE,
by 8% for FEAR/ANXIETY, by 2% for JOY, and by
5% for SADNESS/DISAPPOINTMENT. The next
row presents the results when the list subset fea-
tures were used. Using this feature representation
as opposed to using each list as a whole shows pre-
cision recall tradeoff as the classifier learns to rely
on the subsets of hashtags that are good, resulting in
improved precision for several emotion classes, but
recognizes emotions in fewer tweets, which resulted
in less recall.
The fifth and the sixth rows of Table 4 show re-
sults of list lookup only. As expected, seed lookup
recognizes emotions in tweets with high precision,
but does not recognize the emotions in many tweets
because the seed lists have only 5 hashtags per emo-
tion class. Comparatively, using learned hashtag
lists shows substantial improvement in recall as the
learned lists contain a lot more emotion hashtags
than the initial seeds.
Finally, the last two rows of Table 4 show classi-
fication performance of taking the union of the de-
cisions made by the unigram classifier and the deci-
sions made by matching against just the seed hash-
tags or the lists of learned hashtags. The union with
the seed hashtags lookup shows consistent improve-
ment across all emotion classes compared to the un-
igram baseline but the improvements are small. The
Evaluation Micro Macro
Average Average
P R F P R F
Baseline Classifiers
Unigram Classifier (UC) 62 37 46 61 34 44
UC + Bigram Features 63 33 43 62 30 41
Baseline Classifier with List Features
UC + List Features 65 42 51 64 40 49
UC + List Subset Features 66 39 49 65 37 48
List Lookup
Seed Lookup 93 04 08 89 04 08
List Lookup 67 24 35 68 27 38
Baseline Classifier with List Lookup
UC ? Seed Lookup 63 38 47 61 36 45
UC ? List Lookup 60 49 54 59 49 53
Table 5: Micro and Macro averages
union with the lookup in the learned lists of emo-
tion hashtags shows substantial recall gains. This
approach improves recall over the unigram baseline
by 17% for AFFECTION, 19% for ANGER/RAGE,
20% for FEAR/ANXIETY, 6% for JOY, and 9% for
SADNESS/DISAPPOINTMENT. At the same time,
we observe that despite this large recall gain, preci-
sion is about the same or just a little lower. As a re-
sult, we observe an overall F-measure improvement
of 9% for AFFECTION, 16% for ANGER/RAGE,
11% for FEAR/ANXIETY, 4% for JOY, and 7% for
SADNESS/DISAPPOINTMENT.
Table 5 shows the overall performance improve-
ment of the classifiers, averaged across all five emo-
tion classes, measured as micro and macro aver-
8
AFFECTION ANGER FEAR JOY SADNESS
RAGE ANXIETY DISAPPOINT-
MENT
#youthebest #godie #hatespiders #thankinggod #catlady
#yourthebest #donttalktome #freakedout #thankyoulord #buttrue
#hyc #fuckyourself #creepedout #thankful #singleprobs
#yourethebest #getoutofmylife #sinister #superexcited #singleproblems
#alwaysandforever #irritated #wimp #tripleblessed #lonelytweet
#missyou #pieceofshit #shittingmyself #24hours #lonely
#loveyoumore #ruinedmyday #frightened #ecstatic #crushed
#loveyoulots #notfriends #paranoid #happyme #lonerproblems
#thanksforeverything #yourgross #haunted #lifesgood #unloved
#flyhigh #madtweet #phobia #can?twait #friendless
#comehomesoon #stupidbitch #shittingbricks #grateful #singlepringle
#yougotthis #sofuckingannoying #hateneedles #goodmood #brokenheart
#missyoutoo #annoyed #biggestfear #superhappy #singleforever
#youdabest #fuming #worstfear #missedthem #nosociallife
#otherhalf #wankers #concerned #greatmood #teamnofriends
#youramazing #asshole #waitinggame #studio #foreverugly
#cutiepie #dontbothermewhen #mama #tgfl #nofriends
#bestfriendforever #fu #prayforme #exicted #leftout
#alwayshereforyou #fuckyou #nightmares #smiles #singleforlife
#howimetmybestfriend #yousuck #baddriver #liein #:?(
Table 6: Top 20 hashtags learned for each emotion class
age precision, recall and F-measure scores. We see
both types of feature representations of the hashtag
lists improve precision and recall across all emo-
tion classes over the N-gram classifier baselines.
Using the union of the classifier and list lookup,
we see a 12% recall gain with only 2% precision
drop in micro-average over the unigram baseline,
and 15% recall gain with only 2% precision drop
in macro-average. As a result, we see an overall
8% micro-average F-measure improvement and 9%
macro-average F-measure improvement.
In Table 6, we show the top 20 hashtags learned
in each emotion class by our bootstrapped learning.
While many of these hashtags express emotion, we
also notice a few hashtags representing reasons (e.g.,
#baddriver in FEAR/ANXIETY) that are strongly
associated with the corresponding emotion, as well
as common misspellings (e.g., #exicted in JOY).
6 Conclusions
In this research we have presented a bootstrapped
learning framework to automatically learn emotion
hashtags. Our approach makes use of supervi-
sion from seed hashtag labeled tweets, and through
a bootstrapping process, iteratively learns emotion
hashtags. We have experimented with several ap-
proaches to use the lists of emotion hashtags for
emotion classification and have found that the hash-
tag lists consistently improve emotion classification
performance in tweets. In future research, since our
bootstrapped learning approach does not rely on any
language specific techniques, we plan to learn emo-
tion hashtags in other prominent languages such as
Spanish, Portuguese, etc.
7 Acknowledgments
This work was supported by the Intelligence Ad-
vanced Research Projects Activity (IARPA) via De-
partment of Interior National Business Center (DoI
/ NBC) contract number D12PC00285. The U.S.
Government is authorized to reproduce and dis-
tribute reprints for Governmental purposes notwith-
standing any copyright annotation thereon. The
views and conclusions contained herein are those
of the authors and should not be interpreted as
necessarily representing the official policies or en-
dorsements, either expressed or implied, of IARPA,
DoI/NBE, or the U.S. Government.
9
References
Edward Benson, Aria Haghighi, and Regina Barzilay.
2011. Event discovery in social media feeds. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies - Volume 1, HLT ?11, pages 389?398.
Adam Bermingham and Alan Smeaton. 2011. On using
twitter to monitor political sentiment and predict elec-
tion results. In Proceedings of the Workshop on Sen-
timent Analysis where AI meets Psychology (SAAIP
2011), pages 2?10.
Johan Bollen, Huina Mao, and Alberto Pepe. 2011.
Modeling public mood and emotion: Twitter sentiment
and socio-economic phenomena. In Proceedings of
the Fifth International Conference on Weblogs and So-
cial Media.
Samuel Brody and Nicholas Diakopoulos. 2011.
Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!: using
word lengthening to detect sentiment in microblogs.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
?11, pages 562?570.
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: the kappa statistic. Comput. Linguist.,
22:249?254, June.
S. Carter, W. Weerkamp, and E. Tsagkias. 2013. Mi-
croblog language identification: Overcoming the limi-
tations of short, unedited and idiomatic text. Language
Resources and Evaluation Journal, 47(1).
Wilas Chamlertwat, Pattarasinee Bhattarakosol, Tip-
pakorn Rungkasiri, and Choochart Haruechaiyasak.
2012. Discovering consumer insight from twitter via
sentiment analysis. Journal of Universal Computer
Science, 18(8):973?992, apr.
Munmun De Choudhury, Michael Gamon, and Scott
Counts. 2012. Happy, nervous or surprised? classi-
fication of human affective states in social media. In
Proceedings of the Sixth International Conference on
Weblogs and Social Media.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised recognition of sarcastic sentences in
twitter and amazon. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, CoNLL ?10, pages 107?116.
Munmun De Choudhury, Nicholas Diakopoulos, and Mor
Naaman. 2012. Unfolding the event landscape on
twitter: classification and exploration of user cate-
gories. In Proceedings of the ACM 2012 conference on
Computer Supported Cooperative Work, CSCW ?12,
pages 241?244.
Ahmed Ali Abdalla Esmin, Roberto L. De Oliveira Jr.,
and Stan Matwin. 2012. Hierarchical classification
approach to emotion recognition in twitter. In Pro-
ceedings of the 11th International Conference on Ma-
chine Learning and Applications, ICMLA, Boca Ra-
ton, FL, USA, December 12-15, 2012. Volume 2, pages
381?385. IEEE.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. Liblinear: A library
for large linear classification. J. Mach. Learn. Res.,
9:1871?1874, June.
Eric Gilbert and Karrie Karahalios. 2010. Widespread
worry and the stock market. In Proceedings of the In-
ternational AAAI Conference on Weblogs and Social
Media.
Suin Kim, JinYeong Bak, and Alice Oh. 2012a. Discov-
ering emotion influence patterns in online social net-
work conversations. SIGWEB Newsl., (Autumn):3:1?
3:6, September.
Suin Kim, JinYeong Bak, and Alice Oh. 2012b. Do you
feel what i feel? social aspects of emotions in twitter
conversations. In International AAAI Conference on
Weblogs and Social Media.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the omg! In Proceedings of the Fifth In-
ternational Conference on Weblogs and Social Media.
Saif Mohammad. 2012. #emotional tweets. In *SEM
2012: The First Joint Conference on Lexical and Com-
putational Semantics, pages 246?255.
Olutobi Owoputi, Brendan OConnor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and Noah A. Smith. 2013.
Improved part-of-speech tagging for online conversa-
tional text with word clusters. In Proceedings of the
North American Chapter of the Association for Com-
putational Linguistics (NAACL-2013).
W. Gerrod Parrott, editor. 2001. Emotions in Social Psy-
chology. Psychology Press.
Matthew Purver and Stuart Battersby. 2012. Experi-
menting with distant supervision for emotion classi-
fication. In Proceedings of the 13th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, EACL ?12, pages 482?491.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of twitter conversations. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, HLT ?10, pages 172?180.
Kirk Roberts, Michael A. Roach, Joseph Johnson, Josh
Guthrie, and Sanda M. Harabagiu. 2012. Empatweet:
Annotating and detecting emotions on twitter. In Pro-
ceedings of the Eighth International Conference on
Language Resources and Evaluation (LREC-2012),
pages 3806?3813. ACL Anthology Identifier: L12-
1059.
10
Andranik Tumasjan, Timm O. Sprenger, Philipp G. Sand-
ner, and Isabell M. Welpe. 2011. Election fore-
casts with twitter: How 140 characters reflect the po-
litical landscape. Social Science Computer Review,
29(4):402?418, November.
Xiaolong Wang, Furu Wei, Xiaohua Liu, Ming Zhou,
and Ming Zhang. 2011. Topic sentiment analysis in
twitter: a graph-based hashtag sentiment classification
approach. In Proceedings of the 20th ACM interna-
tional conference on Information and knowledge man-
agement, CIKM ?11, pages 1031?1040.
Wenbo Wang, Lu Chen, Krishnaprasad Thirunarayan,
and Amit P. Sheth. 2012. Harnessing twitter ?big
data? for automatic emotion identification. In Pro-
ceedings of the 2012 ASE/IEEE International Confer-
ence on Social Computing and 2012 ASE/IEEE In-
ternational Conference on Privacy, Security, Risk and
Trust, SOCIALCOM-PASSAT ?12, pages 587?592.
Changhua Yang, Kevin Hsin-Yih Lin, and Hsin-Hsi
Chen. 2007a. Building emotion lexicon from weblog
corpora. In Proceedings of the 45th Annual Meeting
of the ACL on Interactive Poster and Demonstration
Sessions, ACL ?07, pages 133?136.
Changhua Yang, Kevin Hsin-Yih Lin, and Hsin-Hsi
Chen. 2007b. Emotion classification using web blog
corpora. In Proceedings of the IEEE/WIC/ACM In-
ternational Conference on Web Intelligence, WI ?07,
pages 275?278.
11
Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 98?108,
Baltimore, Maryland USA, 27 June 2014.
c
?2014 Association for Computational Linguistics
User Type Classification of Tweets with Implications for Event
Recognition
Lalindra De Silva and Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT 84112
{alnds,riloff}@cs.utah.edu
Abstract
Twitter has become one of the foremost
platforms for information sharing. Conse-
quently, it is beneficial for the consumers
of Twitter to know the origin of a tweet,
as it affects how they view and inter-
pret this information. In this paper, we
classify tweets based on their origin, ex-
ploiting only the textual content of tweets.
Specifically, using a rich, linguistic fea-
ture set and a supervised classifier frame-
work, we classify tweets into two user
types - organizations and individual per-
sons. Our user type classifier achieves an
89% F
1
-score for identifying tweets that
originate from organizations in English
and an 87% F
1
-score for Spanish. We
also demonstrate that classifying the user
type of a tweet can improve downstream
event recognition tasks. We analyze sev-
eral schemes that exploit user type infor-
mation to enhance Twitter event recogni-
tion and show that substantial improve-
ments can be achieved by training separate
models for different user types.
1 Introduction
Twitter has become one of the most widely used
social media platforms, with users (as of March
2013) posting approximately 400 million tweets
per day (Wickre, 2013). This public data serves
as a potential source for a multitude of informa-
tion needs, but the sheer volume of tweets is a bot-
tleneck in identifying relevant content (Becker et
al., 2011). De Choudhury et al. (2012) showed
that the user type of a Twitter account is an impor-
tant indicator in sifting through Twitter data. The
knowledge of a tweet?s origin has potential impli-
cations on the nature of the content to an end user
(e.g., credibility, location, etc). Also, certain types
of events are more likely to be reported by individ-
ual persons (e.g., local events) whereas organiza-
tions generally report events that are of interest to
a wider audience.
The first part of our research focuses on user
type classification in Twitter. De Choudhury et
al. (2012) addressed this problem by examining
meta-information derived from the Twitter API.
In contrast, the goal of our work is to classify
tweets, based solely on their textual content. We
highlight several reasons why this can be advanta-
geous. One reason is that people frequently share
content from other sources, but the shared con-
tent often appears in their Twitter timeline as if
it was their own. Consequently, a tweet that was
posted by an individual may have originated from
an organization. Moreover, meta-information can
sometimes be infeasible to obtain given the rate
limits
1
and there are times when profile informa-
tion for a user account is unavailable or ambigu-
ous (e.g., users often leave their profile informa-
tion blank or write vague entries). Therefore, we
believe there is value in being able to infer the
type of user who authored a tweet based solely on
its textual content. Potentially, our methods for
user type classification based on textual content
can also be combined with methods that examine
user profile data or other meta-data, since they are
complementary sources of information.
In this paper, we present a classifier that tries to
determine whether a tweet originated from an or-
ganization or a person using a rich, linguistically-
motivated feature set. We design features to rec-
ognize linguistic characteristics, including senti-
ment and emotion expressions, informal language
use, tweet style, and similarity with news head-
lines. We evaluate our classifier on both English
and Spanish Twitter data and find that the classifier
achieves an 89% F
1
-score for identifying tweets
that originate from organizations in English and a
1
https://dev.twitter.com/docs/rate-limiting/1.1/limits
98
87% F
1
-score for Spanish.
The second contribution of this paper is to
demonstrate that user type classification can im-
prove event recognition in Twitter. We conduct a
study of event recognition for civil unrest events
and disease outbreak events. Based on statistics
from manually annotated tweets, we found that
organization-tweets are much more likely to men-
tion these events than person-tweets. We then in-
vestigate several approaches to incorporate user
type information into event recognition models.
Our best results are produced by training sepa-
rate event classifiers for tweets from different user
types. We show that user type information con-
sistently improves event recognition performance
for both civil unrest events and disease outbreak
events and for both English and Spanish tweets.
2 Related Work
Our work is most closely related to that of De
Choudhury et al. (2012), which proposed methods
to classify Twitter users into three categories: 1)
Journalists/media bloggers, 2) Organizations and
3) Ordinary Individuals. They employed features
derived from social network structure, user ac-
tivity and users? social interaction behaviors, and
named entities and historical topic distributions in
tweets. In contrast, our work classifies isolated
tweets into two different user types, based on their
textual content. Consequently, our work can pro-
duce different user type labels for different tweets
by the same user, which can help identify shared
content not authored by the user.
Another body of related work tries to classify
Twitter users along other dimensions such as eth-
nicity and political orientation (Pennacchiotti and
Popescu, 2011; Cohen and Ruths, 2013). Gender
inference in Twitter has also garnered interest in
the recent past (Ciot et al., 2013; Liu and Ruths,
2013; Fink et al., 2012). Researchers have also fo-
cused on user behaviors showcased in Twitter in-
cluding the types of messages posted (Naaman et
al., 2010), social connections (Wu et al., 2011),
user responses to events (Popescu and Pennac-
chiotti, 2011) and behaviors related to demograph-
ics (Volkova et al., 2013; Mislove et al., 2011; Rao
et al., 2010).
Event recognition is another area that continues
to attract a lot of interest in social media. Previ-
ous work has investigated event identification and
extraction (Jackoway et al., 2011; Becker et al.,
2009; Becker et al., 2010; Ritter et al., 2012),
event discovery (Benson et al., 2011; Sakaki et al.,
2010; Petrovi?c et al., 2010), tracking events over
time (Kim et al., 2012; Sayyadi et al., 2009) and
event retrieval over archived Twitter data (Metzler
et al., 2012). While our work focuses on user type
classification, we show that the user type of a tweet
is an important piece of information that can be
beneficial in event recognition models.
3 Twitter User Types
Twitter user types can be analyzed in different
granularities and across different dimensions. We
follow a high-level categorization of user types
into organizations and individual persons. While
we acknowledge the existence of other user types,
such as automated bots, we focus only on the orga-
nization and individual person user types for our
research.
? Banking Commission Split Over EU Bonus
Cap http://t.co/GSSbmHAWsQ
? Apple likely to introduce smaller, cheaper
iPad mini today http://t.co/TuKBHZ3z
? Diet Coke may be the new #2, but U.S. soda
market is shrinking http://ow.ly/1bSNnh
Sample Tweets from Organizations
? @john It?s a stress fracture. Nah, no dancing
was involved!
? My gawd feels like my head?s gonna explode
? Watching The Rainmaker. It has totally
sucked me in :D #notsomuch lol
Sample Tweets from Individual Persons
Figure 1: Sample tweets from individual persons
and organizations
From a linguistic point of view, we can ob-
serve several distinguishing characteristics be-
tween organization- and person-tweets. As shown
in Figure 1, organization-tweets are often char-
acterized by headline-like language usage, struc-
tured style, a lack of conversation with the au-
dience (i.e., few reply-tweets), and hyperlinks to
original articles. In contrast, person-tweets show
significant language variability including short-
hand terms, conversational behavior, slang and
profanity, expressions of emotion, and an overall
relaxed usage of language.
99
3.1 Data Acquisition for User Types
To create our data sets, we archived tweets (us-
ing Twitter Streaming API) for six months, be-
ginning February 1
st
, 2013. We then used a lan-
guage filter (Lui and Baldwin, 2012) to separate
out the English and Spanish tweets. Also, in the
data sets we created (see below), we removed du-
plicates, retweets and any tweet with less than 5
words. Given that large-scale human annotation
is expensive, we explored several heuristics to re-
liably compile a large gold standard collection of
person- and organization-tweets.
3.1.1 Acquiring Person-tweets
To acquire person-tweets, we devised a person
heuristic, focusing on the name and the profile de-
scription fields in each user account correspond-
ing to a tweet. We first gathered lists of person
names (first names and surnames), for both En-
glish and Spanish, using census data
2
and online
resources
3
. We also compiled a list of common
organization terms (e.g., agency, institute, com-
pany, etc) in both English and Spanish.
The person heuristic labels a tweet as a person-
tweet if
[
no organization term is in the name or
the profile description fields
]
AND
[
all the words
in the name field are person names OR the profile
description field starts with either ?I?m? or ?I am?
]
4
. To assess the accuracy of the person heuris-
tic, we also performed a manual annotation task.
We employed two annotators and provided them
with guidelines of what constitutes an individual
person?s Twitter account. We defined an individ-
ual person as someone who uses Twitter in their
day-to-day life to post information about his/her
daily activities, update personal status messages,
comment about societal issues and/or interact with
close social circles. The annotators were able to
see the name, profile description, location and url
fields of the Twitter user account and were asked to
label each account as individual, not individual or
undetermined. To calculate annotator agreement
between the two annotators, we gave them 100
Twitter accounts, corresponding to English tweets
collected using the person heuristic. The inter-
annotator agreement (IAA) was .98 (raw agree-
ment) and .97 (G-Index score). We did not use
2
http://www.census.gov/genealogy/www/
data/1990surnames/names_files.html
3
http://genealogy.familyeducation.com/
browse/origin/spanish
4
Corresponding terms were used for Spanish
Cohen?s Kappa (?) as it is known to underestimate
agreement (known as Kappa Paradox) when one
category dominates. We then released another 250
accounts to each of the annotators, giving us a total
of 600 manually labeled accounts
5
.
In the distribution of labels assigned by the hu-
man annotators for these 600 accounts, 91.5% was
confirmed as belonging to individual persons. 5%
was identified as not individual whereas 3.5% was
labeled as undetermined. These numbers corrob-
orate our claim that the person heuristic is a valid
approximation for acquiring person-tweets.
However, limiting our person-tweets to those
from accounts identified with the person heuris-
tic could introduce bias (i.e., it may consider only
the people who provided more complete profile
information). To address this issue, we looked
into additional heuristics that are representative
of individual persons? Twitter accounts. We ob-
served that applications designed specifically for
hand-held devices (e.g., twitter for iphone) are fre-
quently used to author tweets and used by individ-
ual persons. Organizations, on the other hand, pri-
marily use the Twitter web tool and content man-
agement software applications to create, manage
and post content to Twitter.
To further investigate our observation, we ex-
tracted the source information (i.e., the software
applications used to author tweets) for a collection
of 1.2 million English tweets from our tweet pool,
for a random day, and identified those that were
clearly hand-held device apps and covered at least
1% of the tweets. Table 1 shows the distribution
of these hand-held device apps, which together ac-
counted for approximately 66% of all tweets.
Hand-held Device App % of Tweets
twitter for iphone 37.11
twitter for android 16.50
twitter for blackberry 5.50
twitter for ipad 2.55
mobile web (m2) 1.46
ios 1.36
echofon 1.29
ALL 65.77
Table 1: Percentage of (English) tweets authored
from hand-held device apps
To evaluate our hypothesis that a high percent-
age of these tweets are person-tweets, we carried
out another manual annotation task. We selected
5
We adjudicated the disagreements in the initial 100 Twit-
ter accounts.
100
100 English Twitter accounts whose tweets were
generated using one of the above hand-held de-
vice apps and asked the two annotators to label
them using the same guidelines. For this task, the
IAA was .84 (raw agreement) and .76 (G-Index
score). As before, we released another 250 ac-
counts to each of the annotators. In these 600 user
accounts, 87.1% was confirmed to be individual
persons. Only 1% was judged to be clearly not
individual whereas 11.9% was labeled as unde-
termined.
3.1.2 Acquiring Organization-tweets
Designing similar heuristics to identify
organization-tweets proved to be difficult.
Organizations describe themselves in numerous
ways, making it difficult to automatically identify
their names in user profiles. Furthermore, organi-
zation names often appear in individual persons?
accounts because they mention their employers
(e.g., I?m a software engineer at Microsoft Corpo-
ration). Therefore, to acquire organization-tweets,
we relied on web-based directories of organiza-
tions (e.g., www.twellow.com) and gathered
their tweets using the Twitter API. We used 58
organization accounts for English tweets and 83
accounts for Spanish.
3.1.3 Complete Data Set
We created a data set of 200,000 tweets for each
language, consisting of 90% person-tweets and
10% organization-tweets. Among the 180,000
person-tweets, 132,000 (66% of 200,000) were
tweets whose source was a hand-held device
app. To collect the remaining 48,000 (24%
of 200,000) of the person-tweets, we relied
on the person heuristic. Finally, we gathered
20,000 organization-tweets using the web directo-
ries mentioned previously. In doing so, to ensure
that we had a balanced mix of organizations, each
organization contributed with a maximum of 500
tweets.
4 User Type Classification
To automatically distinguish person-tweets from
organization-tweets, we trained a supervised clas-
sifier using N-gram features, an organization
heuristic, and a linguistic feature set categorized
into six classes. For the classification algorithm,
we employed a Support Vector Machine (SVM)
with a linear kernel, using the LIBSVM package
(Chang and Lin, 2011). For the features that rely
on part-of-speech (POS) tags, we used the English
Twitter POS tagger by Gimpel et al. (2011) and
another tagger trained on the CoNLL 2002 shared
task data for Spanish (Tjong Kim Sang, 2002) us-
ing the OpenNLP toolkit (OpenSource, 2010).
4.1 N-gram Features
We started off by introducing N-gram features to
capture the words in a tweet. Specifically, we
trained a supervised classifier using unigram and
bigram features encoded with binary values. In se-
lecting the N-gram features, we discarded any N-
gram that appears less than five times in the train-
ing data.
4.2 Organization Heuristic
Following observations by Messner et al. (2011),
we combined two simple heuristic rules to flag
tweets that are likely to be from an organization.
The first observation is that ?replies? (i.e., @user
mentions at the beginning of a tweet) are uncom-
mon in organization-tweets. Hence, if a tweet is a
reply, it is likely to be a person-tweet. The second
observation is that organization-tweets frequently
include a web link to external content.
Our organization heuristic, therefore, com-
bined these two properties. If the tweet is not a
reply AND contains a web link, we labeled it as
an organization-tweet. Otherwise, we labeled it
as a person-tweet. In Section 5, we evaluate this
heuristic as a classification rule on its own, and
also incorporate its label as a feature in our classi-
fier.
4.3 Linguistic Features
In the following sections, we describe our linguis-
tic features and the intuitions in designing them.
4.3.1 Emotion and Sentiment
Twitter is a platform where individuals often ex-
press emotion. We detected emotions using four
feature types: 1) interjections, 2) profanity, 3)
emoticons and 4) overall sentiment of the tweet.
Interjections, profanity, and emoticons are
widely used by individuals to convey emotion,
such as anger, surprise, happiness, etc. To iden-
tify these three feature types, we used a combina-
tion of POS tags in the English tagger (which con-
tains tags for interjections, emoticons, etc), com-
piled lists of interjections and profanity from the
101
web for both English
6
and Spanish
7
and regular
expression patterns for emoticons.
We also included sentiment features using the
sentiment140 API
8
(Go et al., 2009). This API
provides a sentiment label (positive, negative or
neutral) for a tweet corresponding to its overall
sentiment. We expect person-tweets to show more
positive and negative sentiment and organization-
tweets to be more neutral.
4.3.2 Similarity to News Headlines
Earlier, we observed that organization-tweets are
often similar to news headlines. To exploit this ob-
servation, we introduced four features using lan-
guage models and verb categories.
First, we collected 3 million person-tweets, for
each language, using the person heuristic de-
scribed in Section 3.1. Second, we collected an-
other 3 million news headlines from each of the
English and Spanish Gigaword corpora (Parker
et al., 2009; Mendonca et al., 2009). Using
these two data sets, we built unigram and bigram
language models (with Laplace smoothing) for
person-tweets and for news headlines. Given a
new tweet, we calculated the probability of the
tweet using both the person-tweet and headline
language models. We defined a binary feature that
indicates which unigram language model (person-
tweet model vs. headline model) produced the
highest probability. A similar feature is defined
for the bigram language models.
We also observed that certain verbs are pre-
dominantly used in news headlines and are rarely
associated with colloquial language (therefore, in
person-tweets). Similarly, we observed verbs that
are much more likely to be used by individual per-
sons. To identify the most discriminating verbs,
we ranked verbs appearing more than 5 times in
the collected news headlines and person-tweets
based on the following probabilities:
p(h|verb) =
Frequency of verb in headlines
Frequency of verb in all instances
p(pt|verb) =
Frequency of verb in person-tweets
Frequency of verb in all instances
The verbs were sorted by probability and we re-
tained two disjoint sets of verbs, 1) the verbs most
6
http://www.noswearing.com/dictionary
7
http://nawcom.com/swearing/mexican_
spanish.htm
8
http://help.sentiment140.com/api
representative of headlines (i.e., headline verbs),
selected by applying a threshold of p(h|verb) >
0.8 and 2) verbs most representative of person-
tweets (i.e., personal verbs), with a similar thresh-
old of p(pt|verb) > 0.8. We introduced two bi-
nary features that look for verbs in the tweet from
these two learned verb lists. The top-ranked verbs
for each category are displayed in Table 2. The
learned headline verbs tend to be more formal
and are often used in business or government con-
texts (e.g., revoke, granting, etc) whereas the per-
sonal verbs tend to represent personal activities,
communications, and emotions (e.g., hate, sleep,
etc). In total, we learned 687 headline verbs and
2221 personal verbs for English, and 1924 head-
line verbs and 5719 personal verbs for Spanish.
Headline verbs: aided, revoke, issued, broaden, tes-
tify, leads, postponing, forged, deepen, hijacked, raises,
granting, honoring, pledged, departing, suspending, cit-
ing, compensate, preserved, weakening, differing
Personal verbs: raining, sleep, hanging, hate, march-
ing, teaching, sway, having, risk, lurk, screaming, tag-
ging, disturb, baking, exaggerate, pinch, enjoy, shred-
ding, force, hide, wreck, saved, cooking, blur, told
Table 2: Top-ranked representative verbs learned
from headlines and person-tweets
4.3.3 1
st
and 2
nd
Person Pronouns
Person-tweets often include self-references, in
the form of first-person pronouns and their vari-
ant forms (e.g., possessive, reflexive), while
organization-tweets rarely contain self-references.
Also, organizations often address their audience
using second-person pronouns in tweets (e.g., Will
you High Five the #Bruins or #Blackhawks? Sign
up for a chance to win a trip to the Cup Final:
http://t.co/XQP8ZDOINV). To capture these char-
acteristics, we included two binary features that
look for 1
st
and 2
nd
person pronouns in a tweet.
4.3.4 NER Features
We hypothesized that organization-tweets will
carry more named entities and proper nouns. For
English tweets, we identified Persons, Organiza-
tions and Locations using the Named Entity Rec-
ognizer (NER) from Ritter et al. (2011). For
Spanish tweets, we used NER models trained on
CoNLL 2002 shared task data for Spanish. The
features were encoded as three values, represent-
ing the frequency of each entity type in a tweet.
102
English Spanish
P R F
1
P R F
1
ULM: Unigram Language Model 71.63 63.18 67.14 66.14 60.43 63.16
BLM: Bigram Language Model 81.46 49.17 61.32 80.03 51.08 62.36
NGR: SVM with N-grams 86.02 62.76 72.57 85.76 66.56 74.95
OrgH: Organization Heuristic 66.87 91.08 77.12 65.32 81.44 72.49
NGR + OrgH 82.26 86.82 84.48 83.85 85.17 84.50
NGR + OrgH + Linguistic Features 89.01 89.40 89.20
?
87.59 85.47 86.52
?
Table 3: User type classification results with Precision (%), Recall (%) and F
1
-Score (%). ? denotes
statistical significance at p < 0.01 compared to NGR + OrgH
4.3.5 Informal Language Features
Person-tweets often showcase erratic and casual
use of language, whereas organization-tweets tend
to have (relatively) more grammatical language
usage. Hence, we introduced a feature to deter-
mine the informality of a tweet. Specifically, we
check if a tweet begins with an uppercase letter or
not, and whether sentences are properly separated
with punctuation. To accomplish this, we used
regular expression patterns that look for capital-
ized characters following punctuation and white-
space characters. We also added a feature to check
if all the letters in the tweet are lowercased. Use of
elongated words (e.g., cooooooool) for emphasis,
is another property of person-tweets and we cap-
tured this property by identifying words with three
or more repetitions of the same character.
To comply with the 140 character length restric-
tion of a tweet, person-tweets often employ ad-
hoc short-hand usage of words that omit or replace
characters with a phonetic substitute (e.g., 2mrw,
good n8). We used lists of common abbreviations
found in social media
9
collected from the web and
a binary feature was set if a tweet contained an in-
stance from these lists.
4.3.6 Twitter Stylistic Features
One can also notice structural properties that are
prevalent in either user type. News organiza-
tions often append a topic descriptor to the be-
ginning of a tweet (e.g., Petraeus affair: Woman
who complained of harassing emails identified
http://t.co/hpyLQYeL). To encode this behavior,
we employed a simple heuristic that looked for a
semicolon or a hyphen within the first three words
of a tweet. Also, person-tweets employ heavy use
of hashtags so we included the frequency of hash-
tags in a tweet as a single feature. We added two
more features in the form of the length of the tweet
9
http://www.noslang.com/dictionary/
full/
and the frequency of @user mentions in the tweet.
5 Evaluation of User Type Classification
In this section, we discuss and evaluate our user
type classifier. All of the experiments were carried
out using five-fold cross-validation, using data sets
described in Section 3.1. In these experiments, we
maintained the separation of organization-tweets
at a user-account level in order to avoid tweets
from one organization appearing in both train and
test sets.
5.1 User Type Classifier Results
We first evaluated several baseline systems to as-
sess the difficulty of the user type classification
task. We report precision, recall and F
1
-score with
organization-tweets as the positive class.
To evaluate our hypothesis that organization-
tweets are similar to news headlines, we first pre-
dicted user types using only the unigram and bi-
gram language models described in Section 4.3.2.
As shown in Table 3 (ULM & BLM), unigram
models were capable of discerning organization-
tweets with 71% and 66% precision on English
and Spanish tweets, respectively. This is sub-
stantial performance given that the random chance
of labeling an organization-tweet (i.e., precision)
is merely 10%. The bigram models show ?
80% precision whereas the unigram models show
higher recall.
As another baseline, we evaluated an SVM clas-
sifier that uses only N-gram features. As Table 3
shows, the N-gram classifier (NGR) achieved very
high precision (86%) for both English and Spanish
tweets. However, it yielded relatively moderate re-
call (63% for English and 67% for Spanish).
We then evaluated the organization heuris-
tic (OrgH) all by itself. The heuristic identi-
fies two common characteristics of organization-
tweets and as expected, it achieved substantial re-
call (91% for English and 81% for Spanish) but
103
English Spanish
P R F
1
P R F
1
NGR + OrgH 82.26 86.82 84.48 83.85 85.17 84.50
+ Emotion and Sentiment Features 86.58 86.41 86.50 85.91 84.19 85.05
+ Features Derived from News Headlines 87.83 87.10 87.46 86.68 84.05 85.35
+ 1
st
and 2
nd
Person Pronouns 87.88 88.53 88.20 86.61 84.38 85.48
+ NER Features 88.05 88.75 88.40 86.71 84.69 85.69
+ Informal Language Features 88.39 89.14 88.77 86.89 85.31 86.09
+ Twitter Stylistic Features 89.01 89.40 89.20 87.59 85.47 86.52
NGR + OrgH + Linguistic Features 89.01 89.40 89.20 87.59 85.47 86.52
Table 4: Linguistic feature ablation with Precision (%), Recall (%) and F
1
-Score (%)
with mediocre precision.
These results show that the N-gram classifier
achieved high precision whereas the organization
heuristic achieved high recall. To exploit the best
of both worlds, we evaluated another model (NGR
+ OrgH) that added the organization heuristic as
an additional feature for the N-gram classifier.
This system fares better than all the previous mod-
els, achieving 82% precision with 87% recall for
English and 84% precision with 85% recall for
Spanish.
Next, we show the benefits obtained from
adding the linguistic feature set. As the final row
in Table 3 shows, having incorporated all the lin-
guistic features, our final system showed an im-
provement of 7% precision and 3% recall on En-
glish tweets for an overall F
1
-score gain of approx-
imately 5%. On Spanish tweets, the same incre-
ments were 4%, 0.3% and 2%, respectively. This
final classifier is statistically significantly better
than the model without linguistic features (NGR +
OrgH) for both languages at the p < 0.01 level,
analyzed using a paired booststrap test drawing
10
6
samples with repetition from test data, as de-
scribed in Berg-Kirkpatrick et al. (2012).
5.2 Analysis of Linguistic Features
Having observed that linguistic features improved
user type classification, we evaluated the impact
of each type of linguistic feature using an ablation
study. Table 4 shows the classifier performance
when each of the features types was added cumu-
latively over the NGR + OrgH baseline.
We immediately see a 4% and 2% precision
gain by adding emotion and sentiment features,
for English and Spanish, respectively. Adding fea-
tures derived from news headlines, we observe
that the classifier fares better, improving precision
for both languages and improving recall for En-
glish. 1
st
and 2
nd
person pronouns improved re-
call on English data but had little impact on Span-
ish data. The NER features produced very small
gains in both languages. The informal language
features increased recall from 84.69% to 85.31%
on Spanish tweets. Finally, the Twitter stylistic
features gained 0.7% more precision for both lan-
guages. Overall, the feature types that contributed
the most were the emotion/sentiment features, the
news headline features, and the Twitter stylistic
features.
6 Twitter Event Recognition
Twitter provides a facility where users can search
for tweets using keywords. However, keyword-
based queries for events can often lead to myriad
irrelevant results due to different senses of key-
words (polysemy) and figurative or metaphorical
use of keywords. For instance, a Twitter search
for civil unrest events with a few representative
keywords (e.g., strike, rally, riot, etc.) can often
lead to results referring to sports events, such as a
bowling strike or a tennis rally or where the key-
words are used figuratively (e.g., She?s a riot!). In
this section, we investigate if the user type of a
tweet can help cut through such ambiguity. Specif-
ically, we hypothesize that event keywords may be
used more consistantly and with less ambiguity in
organization-tweets, and therefore user type infor-
mation may be helpful in improving event recog-
nition.
To explore our hypothesis that the user type can
influence the event relevance of a tweet, we con-
structed a set of experiments using two types of
events - civil unrest events and disease outbreaks.
Civil unrest refers to forms of public disturbance
that affect the order of a society (e.g., strikes,
protests, etc.) whereas a disease outbreak refers to
an unusual or widespread occurrence of a disease
(e.g., a measles outbreak).
104
English Spanish
Civil Unrest Disease Outbreaks Civil Unrest Disease Outbreaks
Person-tweets 5.27% 9.52% 9.32% 5.00%
Organization-tweets 36.54% 39.34% 51.66% 44.06%
All-tweets 12.50% 20.07% 14.72% 13.22%
Table 6: Percentage of event-relevant tweets in 4000 tweets with keywords for each category
English Civil Unrest: protest, protested, protesting,
riot, rioted, rioting, rally, rallied, rallying, marched,
marching, strike, striked, striking
English Disease Outbreaks: outbreak, epidemic, in-
fluenza, h1n1, h5n1, pandemic, quarantine, cholera,
ebola, flu, malaria, dengue, hepatitis, measles
Spanish Civil Unrest: protesta, protestar, amoti-
naron, protestaron, protestaban, protestado, amotinarse,
amotinaban, marcha, huelga, amotinando, protestando,
amotinado
Spanish Disease Outbreaks: brote, epidemia, in-
fluenza, h1n1, h5n1, pandemia, cuarantena, sarampi?on,
c?olera, ebola, malaria, dengue, hepatitis, gripe
Table 5: Keywords used to query Twitter for two
types of events in English and Spanish
6.1 Data Acquisition for Event Recognition
We began by collecting tweets that contained at
least one of the keywords listed in Table 5, using
the Twitter search API, and we set up an annota-
tion task using Amazon Mechanical Turk (AMT)
annotators. First, we created guidelines to distin-
guish event-relevant tweets from irrelevant tweets
and annotated 300 tweets for each of the four cat-
egories (i.e., English Civil Unrest, Spanish Civil
Unrest, English Disease Outbreaks and Spanish
Disease Outbreaks).
We released 200 tweets in each category for
annotation to three AMT annotators
10
. We used
these 200 tweets to calculate pair-wise IAA using
Cohen?s Kappa (?) which we report in Table 7.
The IAA scores were generally good, ranging
from 0.67 to 0.89. Each annotator subsequently la-
beled 2000 tweets, yielding a total of 6000 tweets
for each category. In each of these 6000 tweet sets,
we randomly separated 2000 tweets as tuning data
and 4000 as test data.
First, we applied our user type classifier to these
tweets and analyzed the number of true event
tweets for each user type. Table 6 shows the per-
centage of true event tweets in the entire test set,
as well as the percentage of event tweets for each
10
We first released 100 tweets in each category to AMT
and enlisted 10 annotators. After calculating IAA on these
100 tweets, we retained 3 annotators who had the highest
agreement with our annotations.
English Spanish
Civil Unrest .89, .88, .77 .74, .74, .67
Disease Outbreaks .82, .73, .68 .84, .83, .80
Table 7: Pair-wise inter-annotator agreement
(IAA) measured using Cohen?s Kappa (?) on 200
tweets among the three AMT annotators for each
event type in each language
user type. Overall, the percentage of true event
tweets in each test set is ? 20%. This means that
most of the tweets (> 80%) with event keywords
do not discuss an event, confirming the unreliabil-
ity of using event keywords alone.
However, there is a substantial difference in
the density of true event tweets between the two
user types. Across both civil unrest and dis-
ease outbreaks, and for both languages, we see
a much higher percentage of organization-tweets
with event keywords mentioning an event than
person-tweets with event keywords. Table 6 shows
that, in English civil unrest category, organization-
tweets are 7 times more likely (36.54% as opposed
to 5.27%) to report an actual event than person-
tweets with the same keywords. In the English dis-
ease outbreaks category, organization-tweets are
4 times more likely to report an event (39.34%
vs. 9.52%). We notice similar observations in the
Spanish tweets too.
6.2 Event Recognition Results
In this section, we evaluate the impact of user type
information by introducing a simple baseline ex-
periment for Twitter event recognition followed
by several schemes that we devised to incorporate
user type information in more principled ways.
First, we trained a supervised classifier to pre-
dict the probability of a tweet being event-relevant
using only unigrams and bigrams as features, en-
coded with binary values. This baseline system is
agnostic to the user type. We used the SVM Platt
method implementation of LIBSVM (Chang and
Lin, 2011) and carried out experiments using five-
fold cross-validation. As Table 8 shows, this ap-
105
English Spanish
P R F
1
P R F
1
Civil Unrest Events
User type-agnostic classifier 80.97 50.20 61.98 77.51 60.37 67.88
User type included as a feature 80.00 50.40 61.84 77.19 61.56 68.50
(?
p
, ?
o
) optimized for F
1
-score 60.50 72.60 66.00 64.97 78.57 71.13
User type-specific classifier 79.34 63.61 70.61
?
79.20 81.89 80.52
?
Disease Outbreak Events
User type-agnostic classifier 83.15 55.99 66.92 80.49 56.14 66.15
User type included as a feature 83.46 55.36 66.57 80.93 59.36 68.48
(?
p
, ?
o
) optimized for F
1
-score 75.10 66.58 70.58 68.94 72.58 70.71
User type-specific classifier 80.35 66.07 72.51
?
82.20 74.26 78.03
?
Table 8: Event recognition results showing Precision (%), Recall (%) and F
1
-Score (%), for the two
event types in English and Spanish. ? denotes statistical significance at p < 0.01 compared to the
baseline (User type-agnostic classifier)
proach achieved 62% F
1
-score in English and 68%
F
1
-score in Spanish, for civil unrest events. For
disease outbreak events, the corresponding values
were 67% and 66%.
As our first attempt to incorporate user type in-
formation, we added the user type label as one ad-
ditional feature. As shown in Table 8, the added
feature yielded small gains for Spanish but made
little difference for English.
Given our initial hypothesis (and evidence in
Table 6) about events and organization-tweets,
we would prefer to be aggressive in labeling
organization-tweets as event-relevant. One way to
accomplish this with a trained probabilistic classi-
fier is to assign different probability thresholds to
person- and organization-tweets. To acquire the
optimal threshold parameters for person-tweets
(?
p
) and organization-tweets (?
o
), we performed a
grid-based threshold sweep on tuning data and op-
timized with respect to F
1
-scores. Table 8 shows
that this approach yielded substantial recall gains
for all four categories and produced the best F
1
-
scores thus far.
A more principled approach is to create two
completely different classifiers, one for each user
type. Each classifier can then model the vocabu-
lary and word associations that are most likely to
occur in tweets of that type. Using five-fold cross-
validation, we train separate models for person-
and organization-tweets. During event recogni-
tion, we first apply our user type classifier to a
tweet and then apply the appropriate event recog-
nition model. As shown in the final rows in Ta-
ble 8, this method consistently outperforms the
other approaches. Compared to the best compet-
ing method, the user type-specific classifiers pro-
duced F
1
-score gains of 4.6% and 9.4% for En-
glish and Spanish civil unrest events, and F
1
-score
gains of 2% and 7.3% for English and Spanish dis-
ease outbreak events.
7 Conclusion
In this work, we tackled the problem of classify-
ing tweets into two user types, organizations and
individual persons, based on their textual content.
We designed a rich set of features that exploit
different linguistic aspects of tweet content, and
demonstrated that our classifier achieves F
1
-scores
of 89% for English and 87% for Spanish. We also
presented results showing that organization-tweets
with event keywords have a much higher den-
sity of event mentions than person-tweets with the
same keywords and showed the benefits of incor-
porating user type information into event recog-
nition models. Our results showed that creating
separate event recognition classifiers for different
user types yields substantially better performance
than using a single event recognition model on all
tweets.
8 Acknowledgments
This work was supported by the Intelligence Ad-
vanced Research Projects Activity (IARPA) via
Department of Interior National Business Cen-
ter (DoI/NBC) contract number D12PC00285.
The U.S. Government is authorized to reproduce
and distribute reprints for Governmental purposes
notwithstanding any copyright annotation thereon.
The views and conclusions contained herein are
those of the authors and should not be interpreted
as necessarily representing the official policies
or endorsements, either expressed or implied, of
IARPA, DoI/NBC, or the U.S. Government.
106
References
Hila Becker, Mor Naaman, and Luis Gravano. 2009.
Event identification in social media. In WebDB.
Hila Becker, Mor Naaman, and Luis Gravano. 2010.
Learning similarity metrics for event identification
in social media. In Proceedings of the Third ACM
International Conference on Web Search and Data
Mining, WSDM ?10, pages 291?300, New York,
NY, USA. ACM.
H. Becker, M. Naaman, and L. Gravano. 2011. Select-
ing quality twitter content for events. In Proceed-
ings of the Fifth International AAAI Conference on
Weblogs and Social Media (ICWSM11).
E. Benson, A. Haghighi, and R. Barzilay. 2011. Event
discovery in social media feeds. In The 49th Annual
Meeting of the Association for Computational Lin-
guistics, Portland, Oregon, USA. To appear.
Taylor Berg-Kirkpatrick, David Burkett, and Dan
Klein. 2012. An empirical investigation of statis-
tical significance in nlp. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 995?1005. Association
for Computational Linguistics.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technol-
ogy, 2:27:1?27:27. Software available at http://
www.csie.ntu.edu.tw/
?
cjlin/libsvm.
Morgane Ciot, Morgan Sonderegger, and Derek Ruths.
2013. Gender inference of twitter users in non-
english contexts. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, Seattle, Wash, pages 18?21.
Raviv Cohen and Derek Ruths. 2013. Classifying po-
litical orientation on twitter: Its not easy! In Seventh
International AAAI Conference on Weblogs and So-
cial Media.
M. De Choudhury, N. Diakopoulos, and M. Naaman.
2012. Unfolding the event landscape on twitter:
classification and exploration of user categories. In
Proceedings of the ACM 2012 conference on Com-
puter Supported Cooperative Work, pages 241?244.
ACM.
Clayton Fink, Jonathon Kopecky, and Maksym
Morawski. 2012. Inferring gender from the content
of tweets: A region specific example. In ICWSM.
K. Gimpel, N. Schneider, B. O?Connor, D. Das,
D. Mills, J. Eisenstein, M. Heilman, D. Yogatama,
J. Flanigan, and N.A. Smith. 2011. Part-of-speech
tagging for twitter: annotation, features, and exper-
iments. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies: short papers-
Volume 2, pages 42?47. Association for Computa-
tional Linguistics.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
CS224N Project Report, Stanford, pages 1?12.
A. Jackoway, H. Samet, and J. Sankaranarayanan.
2011. Identification of live news events using twit-
ter. In Proceedings of the 3rd ACM SIGSPATIAL
International Workshop on Location-Based Social
Networks, page 9. ACM.
M. Kim, L. Xie, and P. Christen. 2012. Event diffusion
patterns in social media. In Sixth International AAAI
Conference on Weblogs and Social Media.
Wendy Liu and Derek Ruths. 2013. Whats in a name?
using first names as features for gender inference in
twitter.
Marco Lui and Timothy Baldwin. 2012. langid. py:
An off-the-shelf language identification tool. In
Proceedings of the ACL 2012 System Demonstra-
tions, pages 25?30. Association for Computational
Linguistics.
Angelo Mendonca, David Andrew Graff, Denise
DiPersio, Linguistic Data Consortium, et al. 2009.
Spanish gigaword second edition. Linguistic Data
Consortium.
M. Messner, M. Linke, and A. Eford. 2011. Shov-
eling tweets: An analysis of the microblogging
engagement of traditional news organizations. In
International Symposium on Online Journalism,
UT Austin, available at: http://online. journalism.
utexas. edu/2011/papers/Messner2011. pdf (last ac-
cessed April 3, 2011).
D. Metzler, C. Cai, and E. Hovy. 2012. Structured
event retrieval over microblog archives. In Proceed-
ings of the 2012 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
646?655.
Alan Mislove, Sune Lehmann, Yong-Yeol Ahn, Jukka-
Pekka Onnela, and J Niels Rosenquist. 2011.
Understanding the demographics of twitter users.
ICWSM, 11:5th.
M. Naaman, J. Boase, and C.H. Lai. 2010. Is it re-
ally about me?: message content in social aware-
ness streams. In Proceedings of the 2010 ACM con-
ference on Computer supported cooperative work,
pages 189?192. ACM.
OpenSource. 2010. Opennlp: http :
//opennlp.sourceforge.net/.
Robert Parker, Linguistic Data Consortium, et al.
2009. English gigaword fourth edition. Linguistic
Data Consortium.
Marco Pennacchiotti and Ana-Maria Popescu. 2011.
A machine learning approach to twitter user classifi-
cation.
107
Sa?sa Petrovi?c, Miles Osborne, and Victor Lavrenko.
2010. Streaming first story detection with applica-
tion to twitter. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 181?189. Association for Computa-
tional Linguistics.
A.M. Popescu and M. Pennacchiotti. 2011. Dancing
with the stars, nba games, politics: An exploration of
twitter users response to events. In Proceedings of
the International AAAI Conference on Weblogs and
Social Media.
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in twitter. In Proceedings of the 2nd in-
ternational workshop on Search and mining user-
generated contents, pages 37?44. ACM.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An ex-
perimental study. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?11, pages 1524?1534, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
A. Ritter, O. Etzioni, S. Clark, et al. 2012. Open
domain event extraction from twitter. In Proceed-
ings of the 18th ACM SIGKDD international con-
ference on Knowledge discovery and data mining,
pages 1104?1112. ACM.
T. Sakaki, M. Okazaki, and Y. Matsuo. 2010. Earth-
quake shakes twitter users: real-time event detection
by social sensors. In Proceedings of the 19th inter-
national conference on World wide web, pages 851?
860. ACM.
H. Sayyadi, M. Hurst, and A. Maykov. 2009. Event
detection and tracking in social streams. In Proceed-
ings of International Conference on Weblogs and So-
cial Media (ICWSM).
Erik F. Tjong Kim Sang. 2002. Introduction to
the conll-2002 shared task: Language-independent
named entity recognition. In Proceedings of the 6th
Conference on Natural Language Learning - Volume
20, COLING-02, pages 1?4, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Svitlana Volkova, Theresa Wilson, and David
Yarowsky. 2013. Exploring demographic lan-
guage variations to improve multilingual sentiment
analysis in social media. In Proceedings of the
2013 Conference on Empirical Methods on Natural
Language Processing.
K Wickre. 2013. Celebrating #twitter7.
https://blog.twitter.com/2013/
celebrating-twitter7. Accessed:
03/20/2014.
S. Wu, J.M. Hofman, W.A. Mason, and D.J. Watts.
2011. Who says what to whom on twitter. In
Proceedings of the 20th international conference on
World wide web, pages 705?714. ACM.
108
