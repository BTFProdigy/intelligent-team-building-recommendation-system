Corpus-based Question Answering for why-Questions
Ryuichiro Higashinaka and Hideki Isozaki
NTT Communication Science Laboratories, NTT Corporation
2-4, Hikaridai, Seika-cho, Kyoto 619-0237, Japan
{rh,isozaki}@cslab.kecl.ntt.co.jp
Abstract
This paper proposes a corpus-based ap-
proach for answering why-questions. Con-
ventional systems use hand-crafted patterns
to extract and evaluate answer candidates.
However, such hand-crafted patterns are
likely to have low coverage of causal expres-
sions, and it is also difficult to assign suit-
able weights to the patterns by hand. In our
approach, causal expressions are automati-
cally collected from corpora tagged with se-
mantic relations. From the collected expres-
sions, features are created to train an an-
swer candidate ranker that maximizes the
QA performance with regards to the corpus
of why-questions and answers. NAZEQA, a
Japanese why-QA system based on our ap-
proach, clearly outperforms a baseline that
uses hand-crafted patterns with a Mean Re-
ciprocal Rank (top-5) of 0.305, making it
presumably the best-performing fully imple-
mented why-QA system.
1 Introduction
Following the trend of non-factoid QA, we are
seeing the emergence of work on why-QA; e.g.,
answering generic ?why X?? questions (Verberne,
2006). However, since why-QA is an inherently dif-
ficult problem, there have only been a small number
of fully implemented systems dedicated to solving
it. Recent systems at NTCIR-61 Question Answer-
ing Challenge (QAC-4) can handle why-questions
(Fukumoto et al, 2007). However, their perfor-
mance is much lower (Mori et al, 2007) than that
of factoid QA systems (Fukumoto et al, 2004;
Voorhees and Dang, 2005).
We consider that this low performance is due to
the great amount of hand-crafting involved in the
1http://research.nii.ac.jp/ntcir/ntcir-ws6/ws-en.html
systems. Currently, most of the systems rely on
hand-crafted patterns to extract and evaluate answer
candidates (Fukumoto et al, 2007). Such patterns
include typical cue phrases and POS-tag sequences
related to causality, such as ?because of? and ?by
reason of.? However, as noted in (Inui and Okumura,
2005), causes are expressed in various forms, and
it is difficult to cover all such expressions by hand.
Hand-crafting is also very costly. Some patterns
may be more indicative of causes than others. There-
fore, it may be useful to assign different weights to
the patterns for better answer candidate extraction,
but currently this must be done by hand (Mori et al,
2007). It is not clear whether theweights determined
by hand are suitable.
In this paper, we propose a corpus-based approach
for why-QA in order to reduce this hand-crafting
effort. We automatically collect causal expressions
from corpora to improve the coverage of causal ex-
pressions, and utilize a machine learning technique
to train a ranker of answer candidates on the ba-
sis of features created from the expressions together
with other possible features related to causality. The
ranker is trained to maximize the QA performance
with regards to a corpus of why-questions and an-
swers, automatically tuning the weights of the fea-
tures.
This paper is organized as follows: Section 2 de-
scribes previous work onwhy-QA, and Section 3 de-
scribes our approach. Section 4 describes the imple-
mentation of our approach, and Section 5 presents
the evaluation results. Section 6 summarizes and
mentions future work.
2 Previous Work
Although systems that can answer why-questions
are emerging, they tend to have limitations in that
they can answer questions only with causal verbs
(Girju, 2003), in specific domains (Khoo et al,
418
2000), or questions covered by a specific knowl-
edge base (Curtis et al, 2005). Recently, Verberne
(2006; 2007a) has been intensively working on why-
QA based on the Rhetorical Structure Theory (RST)
(Mann and Thompson, 1988). However, her ap-
proach requires manually annotated corpora with
RST relations.
When we look for fully implemented systems for
generic ?why X?? questions, we only find a small
number of such systems. Since why-QA would be
a challenging task when tackled straightforwardly,
requiring common-sense knowledge and semantic
interpretation of questions and answer candidates,
current systems place higher priority on achiev-
ability and therefore use hand-crafted patterns and
heuristics to extract causal expressions as answer
candidates and use conventional sentence similarity
metrics for answer candidate evaluation (Fukumoto,
2007; Mori et al, 2007). We argue, in this paper,
that this hand-crafting is the cause of the current
low performance levels. Recently, (Shima and Mi-
tamura, 2007) applied a machine learning approach
to why-QA, but they also rely on manually selected
cue words to create their features.
Semantic Role Labeling (SRL) techniques can be
used to automatically detect causal expressions. In
the CoNLL-2005 shared task (SRL for English), the
best system found causal adjuncts with a reasonable
accuracy of 65% (Ma`rquez et al, 2005). However,
when we analyzed the data, we found that more than
half of the causal adjuncts contain explicit cues such
as ?because.? Since causes are reported to be ex-
pressed by a wide variety of linguistic phenomena,
not just explicit cues (Inui and Okumura, 2005), fur-
ther verification is needed before SRL can be safely
used for why-QA.
Why-questions are a subset of non-factoid ques-
tions. Since non-factoid questions are observed
in many FAQ sites, such sites have been regarded
as valuable resources for the development of non-
factoid QA systems. Examples include Burke et al
(1997), who used FAQ corpora to analyze questions
to achieve accurate question-type matching; Soricut
and Brill (2006), who used them to train statistical
models for answer evaluation and formulation; and
Mizuno et al (2007), who used them to train clas-
sifiers of question and answer-types. However, they
do not focus on why-questions and do not use any
causal knowledge, which is considered to be useful
for explicit why-questions (Soricut and Brill, 2006).
3 Approach
In this paper, we propose a corpus-based approach
for why-QA in order to reduce the hand-crafting ef-
fort that is currently necessary. We first automat-
ically collect causal expressions from corpora and
use them to create features to represent an answer
candidate. The features are then used to train an an-
swer candidate ranker that maximizes the QA per-
formance with regards to a corpus of why-questions
and answers. We also enumerate possible features
that may be useful for why-QA to be incorporated
in the training to improve the QA performance.
Following the systems at QAC-4 (Fukumoto,
2007) and the answer analysis in (Verberne, 2007b;
Verberne et al, 2007), we consider the task of why-
QA to be a sentence/paragraph extraction task. We
also assume that a document retrieval module of a
system returns top-N documents for a question on
the basis of conventional IR-related metrics and all
sentences/paragraphs extracted from them are re-
garded as answer candidates. Hence, the task be-
comes the ranking of given sentences/paragraphs.
For an answer candidate (a sentence or a para-
graph) to be the correct answer, the candidate should
(1) have an expression indicating a cause and (2)
be similar to the question in content, and (3) some
causal relation should be observed between the can-
didate and the question. For example, an answer
candidate ?X was arrested for fraud.? is likely to
be a correct answer to the question ?Why was X
arrested?? because ?for fraud? expresses a cause,
the question and the answer are both about the same
event (X being arrested), and ?fraud? and ?arrest? in-
dicate a causal relation between the question and the
candidate. Condition (3) would be especially use-
ful when the candidates do not have obvious cues
or topically similar words/phrases to the question;
it may be worthwhile to rely on some prior causal
knowledge to select one over others. Although cur-
rent working systems (Fukumoto, 2007; Mori et al,
2007) do not explicitly state these conditions, they
can be regarded as using hand-crafted patterns for
(1) and (3).2 Lexical similarity metrics, such as co-
sine similarity and n-gram overlaps, are generally
used for (2).
We represent each answer candidate with causal
expression, content similarity, and causal relation
2(3) is dealt with in a manner similar to the treatment of
?cause of death? in (Smith et al, 2005).
419
features that encode how it complies with the three
conditions. Here, the causal expression features are
those based on the causal expressions we aim to col-
lect automatically. For the other two types of fea-
tures, we turn to the existing similarity metrics and
dictionaries to derive features that would be useful
for why-QA. To train a ranker, we create a corpus of
why-questions and answers and adopt one of thema-
chine learning algorithms for ranking. The follow-
ing sections describe the three types of features, the
corpus creation, and the ranker training. The actual
instances of the features, the corpus, and the ranker
will be presented in Section 4.
3.1 Causal Expression Features
With the increasing attention paid to SRL, we cur-
rently have a number of corpora, such as PropBank
(Palmer, 2005) and FrameNet (Baker et al, 1998),
that are tagged with semantic relations including a
causal relation. Since text spans for such relations
are annotated in the corpora, we can simply col-
lect the spans marked by a causal relation as causal
expressions. Since an answer candidate that has a
matching expression for one of the collected causal
expressions is likely to be expressing a cause as
well, we can make the existence of each expression
a feature. Although the collected causal expressions
without any modification might be used to create
features, for generality, it would be better to abstract
them into syntactic patterns. From m causal expres-
sions/patterns automatically extracted from corpora,
we can create m binary features.
In addition, some why-QA systems may already
possess some good hand-crafted patterns to detect
causal expressions. Since there is no reason not to
use them if we know they are useful for why-QA,
we can create a feature indicatingwhether an answer
candidate matches existing hand-crafted patterns.
3.2 Content Similarity Features
In general, if a question and an answer candidate
share many words, it is likely that they are about
the same content. From this assumption, we cre-
ate a feature that encodes the lexical similarity of an
answer candidate to the question. To calculate its
value, existing sentence similarity metrics, such as
cosine similarity or n-gram overlaps, can be used.
Even if a question and an answer candidate do not
share the same words, they may still be about the
same content. One such case is when they are about
the same topic. To express this case as a feature, we
can use the similarity of the question and the docu-
ment in which the answer candidate is found. Since
the documents from which we extract answer candi-
dates typically have scores output by an IR engine
that encode their relevance to the question, we can
use this score or simply the rank of the retrieved doc-
ument as a feature.
A question and an answer candidate may be se-
mantically expressing the same content with differ-
ent expressions. The simplest case is when syn-
onyms are used to describe the same content; e.g.,
when ?arrest? is used instead of ?apprehend.? For
such cases, we can exploit existing thesauri. We
can create a feature encoding whether synonyms of
words in the question are found in the answer can-
didate. We could also use the value of semantic
similarity and relatedness measures (Pedersen et al,
2004) or the existence of hypernym or hyponym re-
lations as features.
3.3 Causal Relation Features
There are semantic lexicons where a semantic re-
lation between concepts is indicated. For example,
the EDR dictionary3 shows whether a causal relation
holds between two concepts; e.g., between ?murder?
and ?arrest.? Using such dictionaries, we can create
pairs of expressions, one indicating a cause and the
other its effect. If we find an expression for a cause
in the answer candidate and that for an effect in the
question, it is likely that they hold a causal relation.
Therefore, we can create a feature encoding whether
this is the case. In cases where such semantic lex-
icons are not available, they may be automatically
constructed, although with noise, using causal min-
ing techniques such as (Marcu and Echihabi, 2002;
Girju, 2003; Chang and Choi, 2004).
3.4 Creating a QA Corpus
For ranker training, we need a corpus of why-
questions and answers. Because we regard the
task of why-QA as a ranking of given sen-
tences/paragraphs, it is best to prepare the corpus in
the same setting. Therefore, we use the following
procedure to create the corpus: (a) create a question,
(b) use an IR engine to retrieve documents for the
question, (c) select among all sentences/paragraphs
in the retrieved documents those that contain the an-
swer to the question, and (d) store the question and a
3http://www2.nict.go.jp/r/r312/EDR/index.html
420
set of selected sentences/paragraphs with their doc-
ument IDs as answers.
3.5 Training a Ranker
Having created the QA corpus, we can apply exist-
ing machine learning algorithms for ranking, such
as RankBoost (Freund et al, 2003) or Ranking
SVM (Joachims, 2002), so that the selected sen-
tences/paragraphs are preferred to non-selected ones
on the basis of their features. Good ranking would
result in goodMean Reciprocal Rank (MRR), which
is one of the most commonly used measures in QA.
4 Implementation
Using our approach, we implemented a Japanese
why-QA system, NAZEQA (?Naze? means ?why?
in Japanese). The system was built as an extension
to our factoid QA system, SAIQA (Isozaki, 2004;
Isozaki, 2005), and works as follows:
1. The question is analyzed by a rule-based ques-
tion analysis component to derive a question
type; ?REASON? for a why-question.
2. The document retrieval engine extracts n-best
documents from Mainichi newspaper articles
(1998?2001) using DIDF (Isozaki, 2005), a
variant of the IDF metric. We chose 20 as n.
All sentences/paragraphs in the n documents
are extracted as answer candidates. Whether
to use sentences or paragraphs as answer can-
didates is configurable.
3. The feature extraction component produces, for
each answer candidate, causal expression, con-
tent similarity, and causal relation features en-
coding how it satisfies conditions (1)?(3) de-
scribed in Section 3.
4. The SVM ranker trained by a QA corpus ranks
the answer candidates based on the features.
5. The top-N answer candidates are presented to
the user as answers.
In the following sections, we describe the features
(399 in all), the QA corpus, and the ranker.
4.1 Causal Expression Features
(F1?F394: AUTO-Causal Expression) We au-
tomatically extracted causal expressions from the
EDR dictionary. The EDR dictionary is a suite
of corpora and dictionaries and includes the EDR
corpus, the EDR concept dictionary (hierarchy of
word senses), and the EDR Japanese word dictio-
nary (sense to word mappings). The EDR corpus
is a collection of independent Japanese sentences
taken from various sources, such as newspaper ar-
ticles, magazines, and dictionary glosses. The cor-
pus is annotated with semantic relations including a
causal relation in a manner similar to PropBank and
FrameNet corpora. We extracted regions marked by
?cause? tags and abstracted them by leaving only
the functional words (auxiliary verbs and case, as-
pect, tense markers) and replacing others with wild-
cards ?*.? For example, a causal expression ?ar-
rested for fraud? would be abstracted to ?*-PASS
for *.? We used CaboCha4 as a morphological ana-
lyzer. From 8,747 regions annotated with ?cause,?
we obtained 394 causal expression patterns after fil-
tering out those that occurred only once. Finally, we
have 394 binary features representing the existence
of each abstracted causal expression pattern.
(F395: MAN-Causal Expression) We emulate the
manually created patterns described in (Fukumoto,
2007) and create a binary feature indicating whether
an answer candidate is matched by the patterns.
4.2 Content Similarity Features
(F396: Question-Candidate Cosine Similarity)
We use the cosine similarity between a question and
an answer candidate using the word frequency vec-
tors of the content words. We chose nouns, verbs,
and adjectives as content words.
(F397: Question-Document Relevance) We use,
as a feature, the inverse of the rank of the document
where the answer candidate is found.
(F398: Synonym Pair) This is a binary feature that
indicates whether a word and its synonym appear
in an answer candidate and a question, respectively.
We use the combination of the EDR concept dictio-
nary and the EDR Japanese word dictionary as a the-
saurus to collect synonym pairs. We have 133,486
synonym pairs.
4.3 Causal Relation Feature
(F399: Cause-Effect Pair) This is a binary fea-
ture that indicates whether a word representing a
cause and a word corresponding to its effect ap-
pear in an answer candidate and a question, respec-
tively. We used the EDR concept dictionary to find
pairs of word senses holding a causal relation and
4http://chasen.org/?taku/software/cabocha/
421
Q13: Why are pandas on the verge of extinction?
(000217262)
A:000217262,L2 Since pandas are not good at raising
their offspring, the Panda Preservation Center in
Sichuan Province is promoting artificial insemina-
tion as well as the training of mother pandas.
A:000217262,L3 A mother panda often gives birth to
two cubs, but when there are two cubs, one is dis-
carded, and young mothers sometimes crush their
babies to death.
A:000406060,L6 However, because of the recent devel-
opment in the midland, they are becoming extinct.
A:010219075,L122 The most common cause of the ex-
tinction for mammals, birds, and plants is degrada-
tion and destruction of habitat, followed by hunting
and poaching for mammals and the impact of alien
species for birds.
Figure 1: An excerpt from the WHYQA collection.
The number in parentheses is the ID of the docu-
ment used to come up with the question. The an-
swers were headed by the document ID and the line
number where the sentence is found in the docu-
ment. (N.B. The above sentences were translated by
the authors.)
expanded the senses to corresponding words using
the EDR Japanese word dictionary to create cause-
effect word pairs. We have 355,641 cause-effect
word pairs.
4.4 WHYQA Collection
Since QAC-4 does not provide official answer sets
and their questions include only a small number
of why-questions, we created a corpus of why-
questions and answers on our own.
An expert, who specializes in text analysis and
is not one of authors, created questions from arti-
cles randomly extracted from Mainichi newspaper
articles (1998?2001). Then, for each question, she
created sentence-level answers by selecting the sen-
tences that she considered to fully include the an-
swer from a list of sentences from top-20 documents
returned from the text retrieval engine with the ques-
tion as input. Paragraph-level answers were auto-
matically created from the sentence-level answers
by selecting the paragraphs containing the answer
sentences.
The analyst was instructed not to create ques-
tions by simply converting existing declarative sen-
tences into interrogatives. It took approximately five
months to create 1,000 question and answer sets
(called the WHYQA collection). All questions are
guaranteed to have answers. Figure 1 lists an exam-
ple question and answer sentences in the collection.
4.5 Training a Ranker by Ranking SVM
Using the WHYQA collection, we trained rank-
ing models using the ranking SVM (Joachims,
2002) (with a linear kernel) that minimizes the
pairwise ranking error among the answer candi-
dates. In the training data, the answers were la-
beled ?+1? and non-answers ??1.? When using sen-
tences as answers, there are 4,849 positive exam-
ples and 521,177 negative examples. In the case of
paragraphs, there are 4,371 positive examples and
261,215 negative examples.
5 Evaluation
For evaluation, we compared the proposed system
(NAZEQA) with two baselines. Baseline-1 (COS)
simply uses, for answer candidate evaluation, the co-
sine similarity between an answer candidate and a
question based on frequency vectors of their con-
tent words. The aim of having this baseline is to see
how the system performs without any use of causal
knowledge. Baseline-2 (FK) uses hand-crafted pat-
terns described in (Fukumoto, 2007) to narrow down
the answer candidates to those having explicit causal
expressions, which are then ranked by the cosine
similarity to the question. NAZEQA and the two
baselines used the same document retrieval engine
to obtain the top-20 documents and ranked the sen-
tences or paragraphs in these documents.
5.1 Results
We made each system output the top-1, 5, 10, and 20
answer sentences and paragraphs for all 1,000 ques-
tions in the WHYQA collection. We used the MRR
and coverage as the evaluation metrics. Coverage
means the rate of questions that can be answered
by the top-N answer candidates. Table 1 shows the
MRRs and coverage for the baselines and NAZEQA.
A 10-fold cross validation was used for the evalua-
tion of NAZEQA.
We can see from the table that NAZEQA is bet-
ter in all comparisons. A statistical test (a sign
test that compares the number of times one sys-
tem places the correct answer before the other)
showed that NAZEQA is significantly better than
FK for the top-5, 10, and 20 answers in the sen-
tence and paragraph-levels (p<0.01). Although the
sentence-level MRR for NAZEQA is rather low, the
paragraph-level MRR for the top-5 answers is 0.305,
which is reasonably high for a non-factoid QA sys-
tem (Mizuno et al, 2007). The coverage is also
422
MRR Coverage
top-N COS FK NZQ COS FK NZQ
Sentences as answer candidates:
top-1 0.036 0.091+ 0.113 3.6% 9.1% 11.3%
top-5 0.086 0.139+ 0.196* 19.1% 23.1% 35.4%
top-10 0.102 0.149+ 0.216* 31.3% 30.7% 50.4%
top-20 0.115 0.152 0.227* 51.4% 35.5% 66.6%
Paragraphs as answer candidates:
top-1 0.065 0.152+ 0.186 6.5% 15.2% 18.6%
top-5 0.140 0.245+ 0.305* 29.2% 41.6% 53.1%
top-10 0.166 0.257+ 0.328* 48.8% 50.5% 70.3%
top-20 0.181 0.262+ 0.339* 70.7% 56.4% 85.6%
Table 1: Mean Reciprocal Rank (MRR) and cov-
erage for the baselines (COS and FK) and the pro-
posed NAZEQA (NZQ in the table) system for the
entire WHYQA collection. The top-1, 5, 10, and
20 mean the numbers of topmost candidates used
to calculate MRR and coverage. Asterisks indicate
NAZEQA?s statistical significance (p<0.01) over
FK, and ?+? FK?s over COS.
Feature Set Sent. Para.
All features (NAZEQA) 0.181 0.287
w/o F1?F394 (AUTO-Causal Exp.) 0.138* 0.217*
w/o F395 (MAN-Causal Exp.) 0.179 0.286
w/o F396 (Q-Cand. Cosine Similarity) 0.131* 0.188*
w/o F397 (Doc.-Q Relevance ) 0.161 0.275
w/o F398 (Synonym Pair) 0.180 0.282
w/o F399 (Cause-Effect Pair) 0.184 0.287
Table 2: Performance changes in MRR (top-5) when
we exclude one of the feature sets. Asterisks indi-
cate a statistically significant drop in performance
from NAZEQA. In this experiment, we used a two-
fold cross validation to reduce computational cost.
high for NAZEQA, making it possible to find an-
swers within the top-10 sentences and top-5 para-
graphs for more than 50% of the questions. Because
there are no why-QA systems known to be better
than NAZEQA in MRR and coverage and because
NAZEQA clearly outperforms a competitive base-
line (FK), we conclude that NAZEQA has one of
the best performance levels for why-QA.
It is interesting to know how each of the feature
sets (e.g., AUTO-Causal Expression Features) con-
tributes to the QA performance. Table 2 shows how
the performance in MRR (top-5) changes when one
of the feature sets is excluded in the training. Al-
though the drop in performance by removing the
Question-Candidate Cosine Similarity feature is un-
derstandable, the performance also drops signifi-
cantly from NAZEQA when we exclude AUTO-
Causal Expression features, showing the effective-
ness of our automatically collected causal patterns.
Rank Feature Name Weight
1 Question-Candidate Cosine Similarity 4.66
2 Exp.[de (by) * wo (-ACC) * teshimai (-PERF)] 1.86
3 Exp.[no (of) * niyote wa (according to)] 1.44
4 Exp.[no (of) * na (AUX) * no (of) * de (by)] 1.42
5 Exp.[no (of) * ya (or) * niyotte (by)] 1.35
6 Exp.[no (of) * ya (or) * no (of) * de (by)] 1.30
7 Exp.[na (AUX) * niyotte (by)] 1.23
8 Exp.[koto niyotte (by the fact that)] 1.22
9 Exp.[to (and) * no (of) * niyotte (by)] 1.20
10 Document-Question Relevance 0.89
...
27 Synonym Pair 0.40
102 MAN-Causal Expression 0.16
127 Cause-Effect Pair 0.15
Table 3: Weights of features learned by the rank-
ing SVM. ?AUTO-Causal Expression? is denoted as
?Exp.? for lack of space. AUX means an auxiliary
verb. The abstracted causal expression patterns are
shown in square brackets with their English transla-
tions in parentheses.
The MAN-Causal Expression, Synonym Pair, and
Cause-Effect Pair features, do not seem to contribute
much to the performance. One of the reasons for
the small contribution of the MAN-Causal Expres-
sion feature may be that the manual patterns used to
create this feature overlap greatly with the automat-
ically collected causal expression patterns, lowering
the impact of the MAN-Causal Expression feature.
The small contribution of the Synonym Pair feature
is probably attributed to the way the answers were
created in the creation of the WHYQA Collection.
Since the answer candidates from which the expert
chose the answers were those retrieved by a text re-
trieval engine that uses lexical similarity to retrieve
relevant documents, it is possible that the answers
that contain synonyms had already been filtered out
in the beginning, making the Synonym Pair feature
less effective. Without the Cause-Effect Pair feature,
the performance does not change or even improves
a little when sentences are used as answers. The
reason for this may be that the syntactically well-
formed sentences of the newspaper articles might
have made causal cues and patterns more effective
than prior causal knowledge. We need to investigate
the difference between the manually created causal
patterns and the automatically collected ones. We
also need to investigate whether the Synonym Pair
and Cause-Effect Pair features could be useful in
other conditions; e.g., when answers are created in
different ways. We also need to examine the quality
of our synonym and cause-effect word pairs because
423
 0
 20
 40
 60
 80
 100
 120
 140
 160
 180
 200
 1  2  3  4  5  6  7  8  9  10
N
um
be
r o
f q
ue
sti
on
s
Rank of the first correct answer
Baseline-1 (COS)
Baseline-2 (FK)
NAZEQA
Figure 2: Distribution of the ranks of first correct
answers. Paragraphs were used as answers. A 10-
fold cross validationwas used to evaluate NAZEQA.
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 100  200  300  400  500  600  700  800  900
M
R
R
Number of training samples
top-1
top-5
top-10
top-20
Figure 3: Learning curve: Performance changes
when answering Q1?Q100 with different sizes of
training samples. Paragraphs are used as answer
candidates.
their quality itself may be to blame.
Furthermore, analyzing the trained ranking mod-
els allows us to calculate the weights given to the
features (Hirao et al, 2002). Table 3 shows the
weights of the top-10 features. We also include in
the table the weights of the Synonym Pair, MAN-
Causal Expression and Cause Effect Pair features so
that the role of all three types of features in our ap-
proach can be shown. The analyzed model was the
one trained with all 1,000 questions in the WHYQA
collection with paragraphs as answers. Just as sug-
gested by Table 2, the Question-Candidate Cosine
Similarity feature plays a key role, followed by au-
tomatically collected causal expression features.
Figure 2 shows the distribution of the ranks of
the first correct answers for all questions in the
WHYQA collection for COS, FK, and NAZEQA.
The distribution of COS is almost uniform, indicat-
ing that lexical similarity cannot be directly trans-
lated into causality. The figure also shows that
NAZEQA consistently outperforms FK.
It may be useful to know how much training data
is needed to train a ranker. We therefore fixed the
test set to Q1?Q100 in the WHYQA collection and
trained rankers with nine different sizes of train-
ing data (100?900) created from Q101?{Q200 ? ? ?
Q1000}. Figure 3 shows the learning curve. Natu-
rally, the performance improves as we increase the
data. However, the performance gains begin to de-
crease relatively early, possibly indicating the limi-
tation of our approach. Since our approach heavily
relies on surface patterns, the use of syntactic and
semantic features may be necessary.
6 Summary and Future Work
This paper proposed corpus-based QA for why-
questions. We automatically collected causal ex-
pressions from semantically tagged corpora and
used them to create features to train an answer can-
didate ranker that maximizes the QA performance
with regards to the corpus of why-questions and an-
swers. The implemented system NAZEQA outper-
formed baselines with an MRR (top-5) of 0.305 and
the coverage was also high, making NAZEQA pre-
sumably the best-performing system as a fully im-
plemented why-QA system.
As future work, we are planning to investigate
other features that may be useful for why-QA. We
also need to examine how QA performance and the
weights of the features differ when we use other
sources for answer retrieval. In this work, we fo-
cused only on the ?cause? relation in the EDR cor-
pus to obtain causal expressions. However, there are
other relations, such as ?purpose,? that may also
be related to causality (Verberne, 2006).
Although we believe our approach is language-
independent, it would be worth verifying it by creat-
ing an English version of NAZEQA based on causal
expressions that can be derived from PropBank and
FrameNet. Finally, we are planning to make public
some of the WHYQA collection at the authors? web-
page so that various why-QA systems can be com-
pared.
Acknowledgments
We thank Jun Suzuki, Kohji Dohsaka, Masaaki Na-
gata, and all members of the Knowledge Processing
424
Research Group for helpful discussions and com-
ments. We also thank the anonymous reviewers for
their valuable suggestions.
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998.
The Berkeley FrameNet Project. In Proc. COLING-ACL,
pages 86?90.
Robin Burke, Kristian Hammond, Vladimir Kulyukin, Steve
Lytinen, Noriko Tomuro, and Scott Schoenberg. 1997.
Question answering from frequently asked question files:
Experiences with the FAQFinder system. AI Magazine,
18(2):57?66.
Du-Seong Chang and Key-Sun Choi. 2004. Causal relation
extraction using cue phrase and lexical pair probabilities. In
Proc. IJCNLP, pages 61?70.
Jon Curtis, Gavin Matthews, and David Baxter. 2005. On the
effective use of Cyc in a question answering system. In Proc.
IJCAI Workshop on Knowledge and Reasoning for Answer-
ing Questions, pages 61?70.
Yoav Freund, Raj Iyer, Robert E. Schapire, and Yoram Singer.
2003. An efficient boosting algorithm for combining prefer-
ences. Journal of Machine Learning Research, 4:933?969.
Jun?ichi Fukumoto, Tsuneaki Kato, and Fumito Masui. 2004.
Question answering challenge for five ranked answers and
list answers ? overview of NTCIR4 QAC2 subtask 1 and 2
?. In Proc. NTCIR, pages 283?290.
Jun?ichi Fukumoto, Tsuneaki Kato, Fumito Masui, and
Tsunenori Mori. 2007. An overview of the 4th question an-
swering challenge (QAC-4) at NTCIR workshop 6. In Proc.
NTCIR, pages 483?440.
Jun?ichi Fukumoto. 2007. Question answering system for non-
factoid type questions and automatic evaluation based on BE
method. In Proc. NTCIR, pages 441?447.
Roxana Girju. 2003. Automatic detection of causal relations
for question answering. In Proc. ACL 2003 Workshop on
Multilingual Summarization and Question Answering, pages
76?83.
Tsutomu Hirao, Hideki Isozaki, Eisaku Maeda, and Yuji Mat-
sumoto. 2002. Extracting important sentences with support
vector machines. In Proc. 19th COLING, pages 342?348.
Takashi Inui and Manabu Okumura. 2005. Investigating the
characteristics of causal relations in Japanese text. In Proc.
ACL 2005 Workshop on Frontiers in Corpus Annotation II:
Pie in the Sky.
Hideki Isozaki. 2004. NTT?s question answering system for
NTCIR QAC2. In Proc. NTCIR, pages 326?332.
Hideki Isozaki. 2005. An analysis of a high-performance
Japanese question answering system. ACM Transactions on
Asian Language Information Processing (TALIP), 4(3):263?
279.
Thorsten Joachims. 2002. Optimizing search engines using
clickthrough data. In Proc. KDD, pages 133?142.
Christopher S. G. Khoo, Syin Chan, and Yun Niu. 2000. Ex-
tracting causal knowledge from a medical database using
graphical patterns. In Proc. 38th ACL, pages 336?343.
W. Mann and S. Thompson. 1988. Rhetorical structure theory:
Toward a functional theory of text organization. In Text, vol-
ume 8, pages 243?281.
Daniel Marcu and Abdessamad Echihabi. 2002. In Proc. 40th
ACL, pages 368?375.
Llu??s Ma`rquez, Pere Comas, Jesu?s Gime?nez, and Neus Catala`.
2005. Semantic role labeling as sequential tagging. In Proc.
CoNLL, pages 193?196.
Junta Mizuno, Tomoyosi Akiba, Atsushi Fujii, and Katunobu
Itou. 2007. Non-factoid question answering experiments
at NTCIR-6: Towards answer type detection for realworld
questions. In Proc. NTCIR, pages 487?492.
Tatsunori Mori, Mitsuru Sato, Madoka Ishioroshi, Yugo
Nishikawa, Shigenori Nakano, and Kei Kimura. 2007. A
monolithic approach and a type-by-type approach for non-
factoid question-answering ? YokohamaNational University
at NTCIR-6 QAC ?. In Proc. NTCIR, pages 469?476.
Martha Palmer. 2005. The proposition bank: An annotated
corpus of semantic roles. Comp. Ling., 31(1):71?106.
Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi.
2004. Wordnet::Similarity - Measuring the Relatedness of
Concepts. In Proc. HLT-NAACL (Demonstration Papers),
pages 38?41.
Hideki Shima and Teruko Mitamura. 2007. JAVELIN III: An-
swering non-factoid questions in Japanese. In Proc. NTCIR,
pages 464?468.
Troy Smith, Thomas M. Repede, and Steven L. Lytinen. 2005.
Determining the plausibility of answers to questions. In
Proc. AAAI Workshop on Inference for Textual Question An-
swering, pages 52?58.
Radu Soricut and Eric Brill. 2006. Automatic question answer-
ing using the web: Beyond the factoid. Journal of Informa-
tion Retrieval, 9:191?206.
Suzan Verberne, Lou Boves, Nelleke Oostdijk, and Peter-Arno
Coppen. 2007. Evaluating discourse-based answer extrac-
tion for why-question answering. In Proc. SIGIR (Posters
and Demonstrations), pages 735?736.
Suzan Verberne. 2006. Developing an approach for why-
question answering. In Proc. 11th European Chapter of
ACL, pages 39?46.
Suzan Verberne. 2007a. Evaluating answer extraction for why-
QA using RST-annotated Wikipedia texts. In Proc. 12th
ESSLLI Student Session, pages 255?266.
Suzan Verberne. 2007b. Paragraph retrieval for why-question
answering. In Proc. Doctoral Consortium Workshop at
SIGIR-2007, page 922.
Ellen M. Voorhees and Hoa Trang Dang. 2005. Overview of
the TREC 2005 question answering track. In Proc. TREC.
425
Corpus-based Discourse Understanding in Spoken Dialogue Systems
Ryuichiro Higashinaka and Mikio Nakano and Kiyoaki Aikawa?
NTT Communication Science Laboratories
Nippon Telegraph and Telephone Corporation
3-1 Morinosato Wakamiya
Atsugi, Kanagawa 243-0198, Japan
{rh,nakano}@atom.brl.ntt.co.jp, aik@idea.brl.ntt.co.jp
Abstract
This paper concerns the discourse under-
standing process in spoken dialogue sys-
tems. This process enables the system to
understand user utterances based on the
context of a dialogue. Since multiple can-
didates for the understanding result can
be obtained for a user utterance due to
the ambiguity of speech understanding, it
is not appropriate to decide on a single
understanding result after each user ut-
terance. By holding multiple candidates
for understanding results and resolving the
ambiguity as the dialogue progresses, the
discourse understanding accuracy can be
improved. This paper proposes a method
for resolving this ambiguity based on sta-
tistical information obtained from dia-
logue corpora. Unlike conventional meth-
ods that use hand-crafted rules, the pro-
posed method enables easy design of the
discourse understanding process. Experi-
ment results have shown that a system that
exploits the proposed method performs
sufficiently and that holding multiple can-
didates for understanding results is effec-
tive.
?Currently with the School of Media Science, Tokyo Uni-
versity of Technology, 1404-1 Katakuracho, Hachioji, Tokyo
192-0982, Japan.
1 Introduction
For spoken dialogue systems to correctly understand
user intentions to achieve certain tasks while con-
versing with users, the dialogue state has to be ap-
propriately updated (Zue and Glass, 2000) after each
user utterance. Here, a dialogue state means all
the information that the system possesses concern-
ing the dialogue. For example, a dialogue state in-
cludes intention recognition results after each user
utterance, the user utterance history, the system ut-
terance history, and so forth. Obtaining the user in-
tention and the content of an utterance using only the
single utterance is called speech understanding, and
updating the dialogue state based on both the previ-
ous utterance and the current dialogue state is called
discourse understanding. In general, the result of
speech understanding can be ambiguous, because it
is currently difficult to uniquely decide on a single
speech recognition result out of the many recogni-
tion candidates available, and because the syntac-
tic and semantic analysis process normally produce
multiple hypotheses. The system, however, has to be
able to uniquely determine the understanding result
after each user utterance in order to respond to the
user. The system therefore must be able to choose
the appropriate speech understanding result by re-
ferring to the dialogue state.
Most conventional systems uniquely determine
the result of the discourse understanding, i.e., the
dialogue state, after each user utterance. However,
multiple dialogue states are created from the current
dialogue state and the speech understanding results
corresponding to the user utterance, which leads to
ambiguity. When this ambiguity is ignored, the dis-
course understanding accuracy is likely to decrease.
Our idea for improving the discourse understanding
accuracy is to make the system hold multiple dia-
logue states after a user utterance and use succeed-
ing utterances to resolve the ambiguity among di-
alogue states. Although the concept of combining
multiple dialogue states and speech understanding
results has already been reported (Miyazaki et al,
2002), they use intuition-based hand-crafted rules
for the disambiguation of dialogue states, which are
costly and sometimes lead to inaccuracy. To resolve
the ambiguity of dialogue states and reduce the cost
of rule making, we propose using statistical infor-
mation obtained from dialogue corpora, which com-
prise dialogues conducted between the system and
users.
The next section briefly illustrates the basic ar-
chitecture of a spoken dialogue system. Section 3
describes the problem to be solved in detail. Then
after introducing related work, our approach is de-
scribed with an example dialogue. After that, we
describe the experiments we performed to verify our
approach, and discuss the results. The last section
summarizes the main points and mentions future
work.
2 Discourse Understanding
Here, we describe the basic architecture of a spoken
dialogue system (Figure 1). When receiving a user
utterance, the system behaves as follows.
1. The speech recognizer receives a user utterance
and outputs a speech recognition hypothesis.
2. The language understanding component re-
ceives the speech recognition hypothesis. The
syntactic and semantic analysis is performed
to convert it into a form called a dialogue
act. Table 1 shows an example of a dialogue
act. In the example, ?refer-start-and-end-time?
is called the dialogue act type, which briefly
describes the meaning of a dialogue act, and
?start=14:00? and ?end=15:00? are add-on in-
formation.1
1In general, a dialogue act corresponds to one sentence.
However, in dialogues where user utterances are unrestricted,
smaller units, such as phrases, can be regarded as dialogue acts.
Speech
Recognizer
Language
Understanding
Component
Discourse
Understanding
Component
Dialogue
State
Dialogue
Manager
Speech
Synthesizer
Update
Update
Refer
Refer
Speech Recognition
Hypothesis Dialogue Act
Figure 1: Architecture of a spoken dialogue system.
3. The discourse understanding component re-
ceives the dialogue act, refers to the current di-
alogue state, and updates the dialogue state.
4. The dialogue manager receives the current dia-
logue state, decides the next utterance, and out-
puts the next words to speak. The dialogue state
is updated at the same time so that it contains
the content of system utterances.
5. The speech synthesizer receives the output of
the dialogue manager and responds to the user
by speech.
This paper deals with the discourse understand-
ing component. Since we are resolving the ambi-
guity of speech understanding from the discourse
point of view and not within the speech understand-
ing candidates, we assume that a dialogue state is
uniquely determined given a dialogue state and the
next dialogue act, which means that a dialogue act
is a command to change a dialogue state. We also
assume that the relationship between the dialogue
act and the way to update the dialogue state can be
easily described without expertise in dialogue sys-
tem research. We found that these assumptions are
reasonable from our experience in system develop-
ment. Note also that this paper does not separately
deal with reference resolution; we assume that it is
performed by a command. A speech understanding
result is considered to be equal to a dialogue act in
this article.
In this paper, we consider frames as representa-
tions of dialogue states. To represent dialogue states,
plans have often been used (Allen and Perrault,
1980; Carberry, 1990). Traditionally, plan-based
discourse understanding methods have been imple-
mented mostly in keyboard-based dialogue systems,
User Utterance ?from two p.m. to three p.m.?
Dialogue Act [act-type=refer-start-and-end-
time, start=14:00, end=15:00]
Table 1: A user utterance and the corresponding di-
alogue act.
although there are some recent attempts to apply
them to spoken dialogue systems as well (Allen et
al., 2001; Rich et al, 2001); however, considering
the current performance of speech recognizers and
the limitations in task domains, we believe frame-
based discourse understanding and dialogue man-
agement are sufficient (Chu-Carroll, 2000; Seneff,
2002; Bobrow et al, 1977).
3 Problem
Most conventional spoken dialogue systems
uniquely determine the dialogue state after a user
utterance. Normally, however, there are multiple
candidates for the result of speech understanding,
which leads to the creation of multiple dialogue
state candidates. We believe that there are cases
where it is better to hold more than one dialogue
state and resolve the ambiguity as the dialogue
progresses rather than to decide on a single dialogue
state after each user utterance.
As an example, consider a piece of dialogue in
which the user utterance ?from two p.m.? has been
misrecognized as ?uh two p.m.? (Figure 2). Fig-
ure 3 shows the description of the example dia-
logue in detail including the system?s inner states,
such as dialogue acts corresponding to the speech
recognition hypotheses2 and the intention recogni-
tion results.3 After receiving the speech recogni-
tion hypothesis ?uh two p.m.,? the system cannot
tell whether the user utterance corresponds to a dia-
logue act specifying the start time or the end time
(da1,da2). Therefore, the system tries to obtain
further information about the time. In this case,
the system utters a backchannel to prompt the next
user utterance to resolve the ambiguity from the dis-
course.4 At this stage, the system holds two dialogue
2In this example, for convenience of explanation, the n-best
speech recognition input is not considered.
3An intention recognition result is one of the elements of a
dialogue state.
4A yes/no question may be an appropriate choice as well.
 
S1 : what time would you like to reserve a
meeting room?
U1 : from two p.m. [uh two p.m.]
S2 : uh-huh
U2 : to three p.m. [to three p.m.]
S3 : from two p.m. to three p.m.?
U3 : yes [yes]
 
Figure 2: Example dialogue.
(S means a system utterance and U a user utterance.
Recognition results are enclosed in square brackets.)
states having different intention recognition results
(ds1,ds2). The next utterance, ?to three p.m.,? is
one that uniquely corresponds to a dialogue act spec-
ifying the end time (da3), and thus updates the two
current dialogue states. As a result, two dialogue
states still remain (ds3,ds4). If the system can tell
that the previous dialogue act was about the start
time at this moment, it can understand the user in-
tention correctly. The correct understanding result,
ds3, is derived from the combination of ds1 and
da3, where ds1 is induced by ds0 and da1. As
shown here, holding multiple understanding results
can be better than just deciding on the best speech
understanding hypothesis and discarding other pos-
sibilities.
In this paper, we consider a discourse understand-
ing component that deals with multiple dialogue
states. Such a component must choose the best com-
bination of a dialogue state and a dialogue act out of
all possibilities. An appropriate scoring method for
the dialogue states is therefore required.
4 Related Work
Nakano et al (1999) proposed a method that holds
multiple dialogue states ordered by priority to deal
with the problem that some utterances convey mean-
ing over several speech intervals and that the under-
standing result cannot be determined at each inter-
val end. Miyazaki et al (2002) proposed a method
combining Nakano et al?s (1999) method and n-best
recognition hypotheses, and reported improvement
in discourse understanding accuracy. They used a
metric similar to the concept error rate for the evalu-
[System utterance (S1)]
?What time would you like to reserve a meeting
room??
[Dialogue act] [act-type=ask-time]
[Intention recognition result candidates]
1. [room=nil, start=nil, end=nil] (ds0)
?
[User utterance (U1)]
?From two p.m.?
[Speech recognition hypotheses]
1. ?uh two p.m.?
[Dialogue act candidates]
1. [act-type=refer-start-time,time=14:00] (da1)
2. [act-type=refer-end-time,time=15:00] (da2)
[Intention recognition result candidates]
1. [room=nil, start=14:00, end=nil]
(ds1, induced from ds0 and da1)
2. [room=nil, start=nil, end=14:00]
(ds2, induced from ds0 and da2)
?
[System utterance (S2)] ?uh-huh?
[Dialogue act] [act-type=backchannel]
?
[User utterance (U2)]
?To three p.m.?
[Speech recognition hypotheses]
1. ?to three p.m.?
[Dialogue act candidates]
1. [act-type=refer-end-time, time=15:00] (da3)
[Intention recognition result candidates]
1. [room=nil, start=14:00, end=15:00]
(ds3, induced from ds1 and da3)
2. [room=nil, start=nil, end=15:00]
(ds4, induced from ds2 and da3)
?
[System utterance (S3)]
?from two p.m. to three p.m.??
[Dialogue act]
[act-type=confirm-time,start=14:00, end=15:00]
?
[User utterance (U3)] ?yes?
[Speech recognition hypotheses]
1. ?yes?
[Dialogue act candidates]
1. [act-type=acknowledge]
[Intention recognition result candidates]
1. [room=nil, start=14:00, end=15:00]
2. [room=nil, start=nil, end=15:00]
Figure 3: Detailed description of the understanding
of the example dialogue.
ation of discourse accuracy, comparing reference di-
alogue states with hypothesis dialogue states. Both
these methods employ hand-crafted rules to score
the dialogue states to decide the best dialogue state.
Creating such rules requires expert knowledge, and
is also time consuming.
There are approaches that propose statistically es-
timating the dialogue act type from several previous
dialogue act types using N-gram probability (Nagata
and Morimoto, 1994; Reithinger and Maier, 1995).
Although their approaches can be used for disam-
biguating user utterance using discourse informa-
tion, they do not consider holding multiple dialogue
states.
In the context of plan-based utterance understand-
ing (Allen and Perrault, 1980; Carberry, 1990),
when there is ambiguity in the understanding re-
sult of a user utterance, an interpretation best suited
to the estimated plan should be selected. In ad-
dition, the system must choose the most plausible
plans from multiple possible candidates. Although
we do not adopt plan-based representation of dia-
logue states as noted before, this problem is close to
what we are dealing with. Unfortunately, however,
it seems that no systematic ways to score the candi-
dates for disambiguation have been proposed.
5 Approach
The discourse understanding method that we pro-
pose takes the same approach as Miyazaki et al
(2002). However, our method is different in that,
when ordering the multiple dialogue states, the sta-
tistical information derived from the dialogue cor-
pora is used. We propose using two kinds of statisti-
cal information:
1. the probability of a dialogue act type sequence,
and
2. the collocation probability of a dialogue state
and the next dialogue act.
5.1 Statistical Information
Probability of a dialogue act type sequence
Based on the same idea as Nagata and Morimoto
(1994) and Reithinger and Maier (1995), we use the
probability of a dialogue act type sequence, namely,
the N-gram probability of dialogue act types. Sys-
tem utterances and the transcription of user utter-
ances are both converted to dialogue acts using a di-
alogue act conversion parser, then the N-gram prob-
ability of the dialogue act types is calculated.
# explanation
1. whether slots asked previously by the system
are changed
2. whether slots being confirmed are changed
3. whether slots already confirmed are changed
4. whether the dialogue act fills slots that do not
have values
5. whether the dialogue act tries changing slots
that have values
6. when 5 is true, whether slot values are not
changed as a result
7. whether the dialogue act updates the initial
dialogue state 5
Table 2: Seven binary attributes to classify collo-
cation patterns of a dialogue state and the next dia-
logue act.
Collocation probability of a dialogue state and
the next dialogue act From the dialogue corpora,
dialogue states and the succeeding user utterances
are extracted. Then, pairs comprising a dialogue
state and a dialogue act are created after convert-
ing user utterances into dialogue acts. Contrary to
the probability of sequential patterns of dialogue act
types that represents a brief flow of a dialogue, this
collocation information expresses a local detailed
flow of a dialogue, such as dialogue state changes
caused by the dialogue act. The simple bigram of
dialogue states and dialogue acts is not sufficient
due to the complexity of the data that a dialogue
state possesses, which can cause data sparseness
problems. Therefore, we classify the ways that di-
alogue states are changed by dialogue acts into 64
classes characterized by seven binary attributes (Ta-
ble 2) and compute the occurrence probability of
each class in the corpora. We assume that the un-
derstanding result of the user intention contained in
a dialogue state is expressed as a frame, which is
common in many systems (Bobrow et al, 1977). A
frame is a bundle of slots that consist of attribute-
value pairs concerning a certain domain.
5The first user utterance should be treated separately, be-
cause the system?s initial utterance is an open question leading
to an unrestricted utterance of a user.
5.2 Scoring of Dialogue Acts
Each speech recognition hypothesis is converted to
a dialogue act or acts. When there are several di-
alogue acts corresponding to a speech recognition
hypothesis, all possible dialogue acts are created as
in Figure 3, where the utterance ?uh two p.m.? pro-
duces two dialogue act candidates. Each dialogue
act is given a score using its linguistic and acous-
tic scores. The linguistic score represents the gram-
matical adequacy of a speech recognition hypothe-
sis from which the dialogue act originates, and the
acoustic score the acoustic reliability of a dialogue
act. Sometimes, there is a case that a dialogue act
has such a low acoustic or linguistic score and that
it is better to ignore the act. We therefore create a
dialogue act called null act, and add this null act to
our list of dialogue acts. A null act is a dialogue act
that does not change the dialogue state at all.
5.3 Scoring of Dialogue States
Since the dialogue state is uniquely updated by a di-
alogue act, if there are l dialogue acts derived from
speech understanding and m dialogue states, m ? l
new dialogue states are created. In this case, we de-
fine the score of a dialogue state S
t+1
as
S
t+1
= S
t
+ ? ? s
act
+ ? ? s
ngram
+ ? ? s
col
where S
t
is the score of a dialogue state just before
the update, s
act
the score of a dialogue act, s
ngram
the score concerning the probability of a dialogue
act type sequence, s
col
the score concerning the col-
location probability of dialogue states and dialogue
acts, and ?, ?, and ? are the weighting factors.
5.4 Ordering of Dialogue States
The newly created dialogue states are ordered based
on the score. The dialogue state that has the best
score is regarded as the most probable one, and the
system responds to the user by referring to it. The
maximum number of dialogue states is needed in
order to drop low-score dialogue states and thereby
perform the operation in real time. This dropping
process can be considered as a beam search in view
of the entire discourse process, thus we name the
maximum number of dialogue states the dialogue
state beam width.
6 Experiment
6.1 Extracting Statistical Information from Di-
alogue Corpus
Dialogue Corpus We analyzed a corpus of dia-
logues between naive users and a Japanese spoken
dialogue system, which were collected in acousti-
cally insulated booths. The task domain was meet-
ing room reservation. Subjects were instructed to
reserve a meeting room on a certain date from a cer-
tain time to a certain time. As a speech recognition
engine, Julius3.1p1 (Lee et al, 2001) was used with
its attached acoustic model. For the language model,
we used a trigram trained from randomly generated
texts of acceptable phrases. For system response,
NTT?s speech synthesis engine FinalFluet (Takano
et al, 2001) was used. The system had a vocabulary
of 168 words, each registered with a category and
a semantic feature in its lexicon. The system used
hand-crafted rules for discourse understanding. The
corpus consists of 240 dialogues from 15 subjects
(10 males and 5 females), each one performing 16
dialogues. Dialogues that took more than three min-
utes were regarded as failures. The task completion
rate was 78.3% (188/240).
Extraction of Statistical Information From the
transcription, we created a trigram of dialogue act
types using the CMU-Cambridge Toolkit (Clarkson
and Rosenfeld, 1997). Figure 3 shows an example
of the trigram information starting from {refer-start-
time backchannel}. The bigram information used
for smoothing is also shown. The collocation proba-
bility was obtained from the recorded dialogue states
and the transcription following them. Out of 64 pos-
sible patterns, we found 17 in the corpus as shown in
Figure 4. Taking the case of the example dialogue in
Figure 3, it happened that the sequence {refer-start-
time backchannel refer-end-time} does not appear in
the corpus; thus, the probability is calculated based
on the bigram probability using the backoff weight,
which is 0.006. The trigram probability for {refer-
end-time backchannel refer-end-time} is 0.031.
The collocation probability of the sequence ds1
+ da3 ? ds3 fits collocation pattern 12, where a
slot having no value was changed. The sequence
ds2 + da3 ? ds4 fits collocation pattern 17, where
a slot having a value was changed to have a differ-
ent value. The probabilities were 0.155 and 0.009,
dialogue act type sequence (trigram) probability
score
refer-start-time backchannel backchannel -1.0852
refer-start-time backchannel ask-date -2.0445
refer-start-time backchannel ask-start-time -0.8633
refer-start-time backchannel request -2.0445
refer-start-time backchannel refer-day -1.7790
refer-start-time backchannel refer-month -0.4009
refer-start-time backchannel refer-room -0.8633
refer-start-time backchannel refer-start-time -0.7172
dialogue act type sequence
(bigram)
backoff
weight
probability
score
refer-start-time backchannel -1.1337 -0.7928
refer-end-time backchannel 0.4570 -0.6450
backchannel refer-end-time -0.5567 -1.0716
Table 3: An example of bigram and trigram of dia-
logue act types with their probability score in com-
mon logarithm.
collocation occurrence
# pattern probability
1. 0 1 1 1 0 0 1 0.001
2. 0 1 1 0 0 1 0 0.053
3. 0 0 0 0 0 0 0 0.273
4. 1 0 0 0 1 0 0 0.001
5. 1 0 1 1 0 0 0 0.005
6. 0 0 1 1 0 0 0 0.036
7. 0 0 0 0 1 0 0 0.047
8. 0 1 1 0 1 0 0 0.041
9. 0 0 1 1 0 0 1 0.010
10. 0 0 1 0 0 1 0 0.016
11. 0 0 0 0 0 0 1 0.064
12. 0 0 0 1 0 0 0 0.155
13. 1 0 0 1 0 0 0 0.043
14. 0 0 1 0 1 0 0 0.061
15. 1 0 0 1 0 0 1 0.001
16. 0 0 0 1 0 0 1 0.186
17. 0 0 0 0 0 1 0 0.009
Table 4: The 17 collocation patterns and their oc-
currence probabilities. See Figure 2 for the detail
of binary attributes. Attributes 1-7 are ordered from
left to right.
respectively. By the simple adding of the two proba-
bilities in common logarithms in each case, ds3 has
the probability score -3.015 and ds4 -3.549, sug-
gesting that the sequence ds3 is the most probable
discourse understanding result after U2.
6.2 Verification of our approach
To verify the effectiveness of the proposed ap-
proach, we built a Japanese spoken dialogue system
in the meeting reservation domain that employs the
proposed discourse understanding method and per-
formed dialogue experiments.
The speech recognition engine was Julius3.3p1
(Lee et al, 2001) with its attached acoustic models.
For the language model, we made a trigram from
the transcription obtained from the corpora. The
system had a vocabulary of 243. The recognition
engine outputs 5-best recognition hypotheses. This
time, values for s
act
, s
ngram
, s
col
are the logarithm
of the inverse number of n-best ranks,6 the log like-
lihood of dialogue act type trigram probability, and
the common logarithm of the collocation probabil-
ity, respectively. For the experiment, weighting fac-
tors are all set to one (? = ? = ? = 1). The di-
alogue state beam width was 15. We collected 256
dialogues from 16 subjects (7 males and 9 females).
The speech recognition accuracy (word error rate)
was 65.18%. Dialogues that took more than five
minutes were regarded as failures. The task com-
pletion rate was 88.3% (226/256).7
From all user speech intervals, the number of
times that dialogue states below second place be-
came first place was 120 (7.68%), showing a relative
frequency of shuffling within the dialogue states.
6.3 Effectiveness of Holding Multiple Dialogue
States
The main reason that we developed the proposed
corpus-based discourse understanding method was
that it is difficult to manually create rules to deal
with multiple dialogue states. It is yet to be exam-
ined, however, whether holding multiple dialogue
states is really effective for accurate discourse un-
derstanding.
To verify that holding multiple dialogue states is
effective, we fixed the speech recognizer?s output to
1-best, and studied the system performance changes
when the dialogue state beam width was changed
from 1 to 30. When the dialogue state beam width is
too large, the computational cost becomes high and
the system cannot respond in real time. We therefore
selected 30 for empirical reasons.
The task domain and other settings were the same
6In this experiment, only the acoustic score of a dialogue act
was considered.
7It should be noted that due to the creation of an enormous
number of dialogue states in discourse understanding, the pro-
posed system takes a few seconds to respond after the user in-
put.
as in the previous experiment except for the dialogue
state beam width changes. We collected 448 dia-
logues from 28 subjects (4 males and 24 females),
each one performing 16 dialogues. Each subject was
instructed to reserve the same meeting room twice,
once with the 1-beam-width system and again with
30-beam-width system. The order of what room to
reserve and what system to use was randomized.
The speech recognition accuracy was 69.17%. Di-
alogues that took more than five minutes were re-
garded as failures. The task completion rates for the
1-beam-width system and the 30-beam-width sys-
tem were 88.3% and 91.0%, and the average task
completion times were 107.66 seconds and 95.86
seconds, respectively. A statistical hypothesis test
showed that times taken to carry out a task with the
30-beam-width system are significantly shorter than
those with the 1-beam-width system (Z = ?2.01,
p < .05). In this test, we used a kind of censored
mean computed by taking the mean of the times
only for subjects that completed the tasks with both
systems. The population distribution was estimated
by the bootstrap method (Cohen, 1995). It may be
possible to evaluate the discourse understanding by
comparing the best dialogue state with the reference
dialogue state, and calculate a metric such as the
CER (concept error rate) as Miyazaki et al (2002)
do; however it is not clear whether the discourse
understanding can be evaluated this way, since it is
not certain whether the CER correlates closely with
the system?s performance (Higashinaka et al, 2002).
Therefore, this time, we used the task completion
time and the task completion rate for comparison.
7 Discussion
Cost of creating the discourse understanding
component The best task completion rate in the ex-
periments was 91.0% (the case of 1-best recognition
input and a 30 dialogue state beam width). This high
rate suggests that the proposed approach is effective
in reducing the cost of creating the discourse un-
derstanding component in that no hand-crafted rules
are necessary. For statistical discourse understand-
ing, an initial system, e.g., a system that employs
the proposed approach with only s
act
for scoring the
dialogue states, is needed in order to create the di-
alogue corpus; however, once it has been made, the
creation of the discourse understanding component
requires no expert knowledge.
Effectiveness of holding multiple dialogue states
The result of the examination of dialogue state beam
width changes suggests that holding multiple dia-
logue states shortens the task completion time. As
far as task-oriented spoken dialogue systems are
concerned, holding multiple dialogue states con-
tributes to the accuracy of discourse understanding.
8 Summary and Future Work
We proposed a new discourse understanding method
that orders multiple dialogue states created from
multiple dialogue states and the succeeding speech
understanding results based on statistical informa-
tion obtained from dialogue corpora. The results of
the experiments show that our approach is effective
in reducing the cost of creating the discourse under-
standing component, and the advantage of keeping
multiple dialogue states was also shown.
There still remain several issues that we need to
explore. These include the use of statistical informa-
tion other than the probability of a dialogue act type
sequence and the collocation probability of dialogue
states and dialogue acts, the optimization of weight-
ing factors ?, ?, ?, other default parameters that we
used in the experiments, and more experiments in
larger domains. Despite these issues, the present re-
sults have shown that our approach is promising.
Acknowledgements
We thank Dr. Hiroshi Murase and all members of the
Dialogue Understanding Research Group for useful
discussions. Thanks also go to the anonymous re-
viewers for their helpful comments.
References
James F. Allen and C. Raymond Perrault. 1980. Analyz-
ing intention in utterances. Artif. Intel., 15:143?178.
James Allen, George Ferguson, and Amanda Stent. 2001.
An architecture for more realistic conversational sys-
tems. In Proc. IUI, pages 1?8.
Daniel G. Bobrow, Ronald M. Kaplan, Martin Kay, Don-
ald A. Norman, Henry Thompson, and Terry Wino-
grad. 1977. GUS, a frame driven dialog system. Artif.
Intel., 8:155?173.
Sandra Carberry. 1990. Plan Recognition in Natural
Language Dialogue. MIT Press, Cambridge, Mass.
Junnifer Chu-Carroll. 2000. MIMIC: An adaptive
mixed initiative spoken dialogue system for informa-
tion queries. In Proc. 6th Applied NLP, pages 97?104.
P.R. Clarkson and R. Rosenfeld. 1997. Statistical lan-
guagemodeling using the CMU-Cambridge toolkit. In
Proc. Eurospeech, pages 2707?2710.
Paul R. Cohen. 1995. Empirical Methods for Artificial
Intelligence. MIT Press.
Ryuichiro Higashinaka, Noboru Miyazaki, Mikio
Nakano, and Kiyoaki Aikawa. 2002. A method
for evaluating incremental utterance understanding
in spoken dialogue systems. In Proc. ICSLP, pages
829?832.
Akinobu Lee, Tatsuya Kawahara, and Kiyohiro Shikano.
2001. Julius ? an open source real-time large vocab-
ulary recognition engine. In Proc. Eurospeech, pages
1691?1694.
Noboru Miyazaki, Mikio Nakano, and Kiyoaki Aikawa.
2002. Robust speech understanding using incremen-
tal understanding with n-best recognition hypothe-
ses. In SIG-SLP-40, Information Processing Society
of Japan., pages 121?126. (in Japanese).
Masaaki Nagata and Tsuyoshi Morimoto. 1994. First
steps toward statistical modeling of dialogue to predict
the speech act type of the next utterance. Speech Com-
munication, 15:193?203.
Mikio Nakano, Noboru Miyazaki, Jun-ichi Hirasawa,
Kohji Dohsaka, and Takeshi Kawabata. 1999. Un-
derstanding unsegmented user utterances in real-time
spoken dialogue systems. In Proc. 37th ACL, pages
200?207.
Norbert Reithinger and Elisabeth Maier. 1995. Utiliz-
ing statistical dialogue act processing in Verbmobil. In
Proc. 33th ACL, pages 116?121.
Charles Rich, Candace Sidner, and Neal Lesh. 2001.
COLLAGEN: Applying collaborative discourse the-
ory. AI Magazine, 22(4):15?25.
Stephanie Seneff. 2002. Response planning and genera-
tion in the MERCURY flight reservation system. Com-
puter Speech and Language, 16(3?4):283?312.
Satoshi Takano, Kimihito Tanaka, Hideyuki Mizuno,
Masanobu Abe, and ShiN?ya Nakajima. 2001. A
Japanese TTS system based on multi-form units and a
speech modification algorithm with harmonics recon-
struction. IEEE Transactions on Speech and Process-
ing, 9(1):3?10.
Victor W. Zue and James R. Glass. 2000. Conversational
interfaces: Advances and challenges. Proceedings of
IEEE, 88(8):1166?1180.
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 117?120,
Prague, June 2007. c?2007 Association for Computational Linguistics
Learning to Rank Definitions to Generate Quizzes for Interactive
Information Presentation
Ryuichiro Higashinaka and Kohji Dohsaka and Hideki Isozaki
NTT Communication Science Laboratories, NTT Corporation
2-4, Hikaridai, Seika-cho, Kyoto 619-0237, Japan
{rh,dohsaka,isozaki}@cslab.kecl.ntt.co.jp
Abstract
This paper proposes the idea of ranking def-
initions of a person (a set of biographi-
cal facts) to automatically generate ?Who
is this?? quizzes. The definitions are or-
dered according to how difficult they make
it to name the person. Such ranking would
enable users to interactively learn about a
person through dialogue with a system with
improved understanding and lasting motiva-
tion, which is useful for educational sys-
tems. In our approach, we train a ranker
that learns from data the appropriate ranking
of definitions based on features that encode
the importance of keywords in a definition
as well as its content. Experimental results
show that our approach is significantly better
in ranking definitions than baselines that use
conventional information retrieval measures
such as tf*idf and pointwisemutual informa-
tion (PMI).
1 Introduction
Appropriate ranking of sentences is important, as
noted in sentence ordering tasks (Lapata, 2003), in
effectively delivering content. Whether the task is
to convey news texts or definitions, the objective is
to make it easier for users to understand the content.
However, just conveying it in an encyclopedia-like
or temporal order may not be the best solution, con-
sidering that interaction between a system and a user
improves understanding (Sugiyama et al, 1999) and
that the cognitive load in receiving information is be-
lieved to correlate with memory fixation (Craik and
Lockhart, 1972).
In this paper, we discuss the idea of ranking defi-
nitions as a way to present people?s biographical in-
formation to users, and propose ranking definitions
to automatically generate a ?Who is this?? quiz.
Here, we use the term ?definitions of a person? to
mean a short series of biographical facts (See Fig. 1).
The definitions are ordered according to how diffi-
cult they make it to name the person. The ranking
also enables users to easily come up with answer
candidates. The definitions are presented to users
one by one as hints until users give the correct name
(See Fig. 2). Although the interaction would take
time, we could expect improved understanding of
people?s biographical information by users through
their deliberation and the long lasting motivation af-
forded by the entertaining nature of quizzes, which
is important in tutorial tasks (Baylor and Ryu, 2003).
Previous work on definition ranking has used
measures such as tf*idf (Xu et al, 2004) or ranking
models trained to encode the likelihood of a defini-
tion being good (Xu et al, 2005). However, such
measures/models may not be suitable for quiz-style
ranking. For example, a definition having a strong
co-occurrence with a person may not be an easy hint
when it is about a very minor detail. Certain de-
scriptions, such as a person?s birthplace, would have
to come early so that users can easily start guessing
who the person is. In our approach, we train a ranker
that learns from data the appropriate ranking of def-
initions. Note that we only focus on the ranking of
definitions and not on the interaction with users in
this paper. We also assume that the definitions to be
ranked are given.
Section 2 describes the task of ranking definitions,
and Section 3 describes our approach. Section 4 de-
scribes our collection of ranking data and the rank-
ing model training using the ranking support vector
machine (SVM), and Section 5 presents the evalu-
ation results. Section 6 summarizes and mentions
future work.
2 Ranking Definitions for Quizzes
Figure 1 shows a list of definitions of Natsume
Soseki, a famous Japanese novelist, in their original
ranking at the encyclopedic website goo (http://dic-
tionary.goo.ne.jp/) and in the quiz-style ranking we
aim to achieve. Such a ranking would realize a dia-
logue like that in Fig. 2. At the end of the dialogue,
the user would be able to associate the person and
the definitions better, and it is expected that some
new facts could be learned about that person.
117
Original Ranking:
1. Novelist and scholar of British literature.
2. Real name: Kinnosuke.
3. Born in Ushigome, Edo.
4. Graduated from the University of Tokyo.
5. Master of early-modern literature along with Mori Ogai.
6. After the success of ?I Am a Cat?, quit all teaching jobs and joined
Asahi Shimbun.
7. Published masterpieces in Asahi Shimbun.
8. Familiar with Haiku, Chinese poetry, and calligraphy.
9. Works include ?Botchan?, ?Sanshiro?, etc.
?
Quiz-style Ranking:
1. Graduated from the University of Tokyo.
2. Born in Ushigome, Edo.
3. Novelist and scholar of British literature.
4. Familiar with Haiku, Chinese poetry, and calligraphy.
5. Published masterpieces in Asahi Shimbun.
6. Real name: Kinnosuke.
7. Master of early-modern literature along with Mori Ogai.
8. After the success of ?I Am a Cat?, quit all teaching jobs and joined
Asahi Shimbun.
9. Works include ?Botchan?, ?Sanshiro?, etc.
Figure 1: List of definitions of Natsume Soseki, a
famous Japanese novelist, in their original ranking in
the encyclopedia and in the quiz-style ranking. The
definitions were translated by the authors.
Ranking definitions is closely related to defini-
tional question answering and sentence ordering
in multi-document summarization. In definitional
question answering, measures related to information
retrieval (IR), such as tf*idf or pointwise mutual in-
formation (PMI), have been used to rank sentences
or information nuggets (Xu et al, 2004; Sun et al,
2005). Such measures are used under the assump-
tion that outstanding/co-occurring keywords about a
definiendum characterize that definiendum. How-
ever, this assumptionmay not be appropriate in quiz-
style ranking; most content words in the definitions
are already important in the IR sense, and strong co-
occurrence may not guarantee high ranks for hints
to be presented later because the hint can be too spe-
cific. An approach to creating a ranking model of
definitions in a supervised manner using machine
learning techniques has been reported (Xu et al,
2005). However, the model is only used to distin-
guish definitions from non-definitions on the basis
of features related mainly to linguistic styles.
In multi-document summarization, the focus has
been mainly on creating cohesive texts. (Lapata,
2003) uses the probability of words in adjacent sen-
tences as constraints to maximize the coherence of
all sentence-pairs in texts. Although we acknowl-
edge that having cohesive definitions is important,
since we are not creating a single text and the dia-
logue that we aim to achieve would involve frequent
user/system interaction (Fig. 2), we do not deal with
the coherence of definitions in this paper.
 
S1 Who is this? First hint: Graduated from the
University of Tokyo.
U1 Yoshida Shigeru?
S2 No, not even close! Second hint: Born in
Ushigome, Edo.
U2 I don?t know.
S3 OK. Third hint: Novelist and scholar of
British literature.
U3 Murakami Haruki?
S4 Close! Fourth hint: Familiar with Haiku,
Chinese poetry, and calligraphy.
U4 Mori Ogai?
S5 Very close! Fifth hint: Published master-
pieces in Asahi Shimbun.
U5 Natsume Soseki?
S6 That?s right!
 
Figure 2: Example dialogue based on the quiz-style
ranking of definitions. S stands for a system utter-
ance and U for a user utterance.
3 Approach
Since it is difficult to know in advance what char-
acteristics are important for quiz-style ranking, we
learn the appropriate ranking of definitions from
data. The approach is the same as that of (Xu et al,
2005) in that we adopt a machine learning approach
for definition ranking, but is different in that what is
learned is a quiz-style ranking of sentences that are
already known to be good definitions.
First, we collect ranking data. For this purpose,
we turn to existing encyclopedias for concise biogra-
phies. Then, we annotate the ranking. Secondly, we
devise a set of features for a definition. Since the
existence of keywords that have high scores in IR-
related measures may suggest easy hints, we incor-
porate the scores of IR-related measures as features
(IR-related features).
Certain words tend to appear before or after oth-
ers in a biographical document to convey particular
information about people (e.g., words describing oc-
cupations at the beginning; those describing works
at the end, etc.) Therefore, we use word positions
within the biography of the person in question as
features (positional features). Biographies can be
found in online resources, such as biography.com
(http://www.biography.com/) and Wikipedia. In ad-
dition, to focus on the particular content of the def-
inition, we use bag-of-words (BOW) features, to-
gether with semantic features (e.g., semantic cate-
gories in Nihongo Goi-Taikei (Ikehara et al, 1997)
or word senses in WordNet) to complement the
sparseness of BOW features. We describe the fea-
tures we created in Section 4.2. Finally, we create
a ranking model using a preference learning algo-
118
rithm, such as the ranking SVM (Joachims, 2002),
which learns ranking by reducing the pairwise rank-
ing error.
4 Experiment
4.1 Data Collection
We collected biographies (in Japanese) from the goo
encyclopedia. We first mined Wikipedia to calcu-
late the PageRankTMof people using the hyper-link
structure. After sorting them in descending order by
the PageRank score, we extracted the top-150 peo-
ple for whom we could find an entry in the goo en-
cyclopedia. Then, 11 annotators annotated rankings
for each of the 150 people individually. The annota-
tors were instructed to rank the definitions assuming
that they were creating a ?who is this?? quiz; i.e.,
to place the definition that is the most characteris-
tic of the person in question at the end. The mean
of the Kendall?s coefficients of concordance for the
150 people was sufficiently high at 0.76 with a stan-
dard deviation of 0.13. Finally, taking the means of
ranks given to each definition, we merged the indi-
vidual rankings to create the reference rankings. An
example of a reference ranking is the bottom one in
Fig. 1. There are 958 definition sentences in all, with
each person having approximately 6?7 definitions.
4.2 Deriving Features
We derived our IR-related features based on
Mainichi newspaper articles (1991?2004) and
Wikipedia articles. We used these two different
sources to take into account the difference in the
importance of terms depending on the text. We
also used sentences, sections (for Wikipedia arti-
cles only) and documents as units to calculate doc-
ument frequency, which resulted in the creation of
five frequency tables: (i) Mainichi-Document, (ii)
Mainichi-Sentence, (iii) Wikipedia-Document, (iv)
Wikipedia-Section, and (v) Wikipedia-Sentence.
Using the five frequency tables, we calculated, for
each content word (nouns, verbs, adjectives, and un-
known words) in the definition, (1) frequency (the
number of documents where the word is found), (2)
relative frequency (frequency divided by the maxi-
mum number of documents), (3) co-occurrence fre-
quency (the number of documents where both the
word and the person?s name are found), (4) rela-
tive co-occurrence frequency, and (5) PMI. Then, we
took the minimum, maximum, and mean values of
(1)?(5) for all content words in the definition as fea-
tures, deriving 75 (5 ? 5 ? 3) features. Then, using
the Wikipedia article (called an entry) for the person
in question, we calculated (1)?(4) within the entry,
and calculated tf*idf scores of words in the defini-
tion using the term frequency in the entry. Again, by
taking the minimum, maximum, and mean values of
(1)?(4) and tf*idf, we yielded 15 (5 ? 3) features,
for a total of 90 (75 + 15) IR-related features.
Positional features were derived also using the
Wikipedia entry. For each word in the definition, we
calculated (a) the number of times the word appears
in the entry, (b) the minimum position of the word in
the entry, (c) its maximum position, (d) its mean po-
sition, and (e) the standard deviation of the positions.
Note that positions are either ordinal or relative; i.e.,
the relative position is calculated by dividing the or-
dinal position by the total number of words in the
entry. Then, we took the minimum, maximum, and
mean values of (a)?(e) for all content words in the
definition as features, deriving 30 (5 ? 2 (ordinal or
relative positions)? 3) features.
For the BOW features, we first parsed all our
definitions with CaboCha (a Japanese morphologi-
cal/dependency parser, http://chasen.org/?taku/soft-
ware/cabocha/) and extracted all content words to
make binary features representing the existence of
each content word. There are 2,156 BOW features
in our data.
As for the semantic features, we used the seman-
tic categories in NihongoGoi-Taikei. Since there are
2,715 semantic categories, we created 2,715 features
representing the existence of each semantic category
in the definition. Semantic categories were assigned
to words in the definition by a morphological ana-
lyzer that comes with ALT/J-E, a Japanese-English
machine translation system (Ikehara et al, 1991).
In total, we have 4,991 features to represent each
definition. We calculated all feature values for all
definitions in our data to be used for the learning.
4.3 Training Ranking Models
Using the reference ranking data, we trained a rank-
ing model using the ranking SVM (Joachims, 2002)
(with a linear kernel) that minimizes the pairwise
ranking error among the definitions of each person.
5 Evaluation
To evaluate the performance of the ranking model,
following (Xu et al, 2004; Sun et al, 2005), we
compared it with baselines that use only the scores
of IR-related and positional features for ranking, i.e.,
sorting. Table 1 shows the performance of the rank-
ing model (by the leave-one-out method, predicting
the ranking of definitions of a person by other peo-
119
Rank Description Ranking Error
1 Proposed ranking model 0.185
2 Wikipedia-Sentence-PMI-max 0.299
3 Wikipedia-Section-PMI-max 0.309
4 Wikipedia-Document-PMI-max 0.312
5 Mainichi-Sentence-PMI-max 0.318
6 Mainichi-Document-PMI-max 0.325
7 Mainichi-Sentence-relative-co-occurrence-max 0.338
8 Wikipedia-Entry-ordinal-Min-max 0.338
9 Wikipedia-Sentence-relative-co-occurrence-max 0.339
10 Wikipedia-Entry-relative-Min-max 0.340
11 Wikipedia-Entry-ordinal-Mean-mean 0.342
Table 1: Performance of the proposed rankingmodel
and that of 10 best-performing baselines.
ple?s rankings) and that of the 10 best-performing
baselines. The ranking error is pairwise ranking er-
ror; i.e., the rate of misordered pairs. A descrip-
tive name is given for each baseline. For example,
Wikipedia-Sentence-PMI-max means that we used
the maximum PMI values of content words in the
definition calculated from Wikipedia, with sentence
as the unit for obtaining frequencies.
Our ranking model outperforms all of the base-
lines. McNemar?s test showed that the difference be-
tween the proposed model and the best-performing
baseline is significant (p<0.00001). The results also
show that PMI is more effective in quiz-style rank-
ing than any other measure. The fact that max is im-
portant probably means that the mere existence of a
word that has a high PMI score is enough to raise the
ranking of a hint. It is also interesting thatWikipedia
gives better ranking, which is probably because peo-
ple?s names and related keywords are close to each
other in such descriptive texts.
Analyzing the ranking model trained by the rank-
ing SVM allows us to calculate the weights given to
the features (Hirao et al, 2002). Table 2 shows the
top-10 features in weights in absolute figures when
all samples were used for training. It can be seen
that high PMI values and words/semantic categories
related to government or creation lead to easy hints,
whereas semantic categories, such as birth and oth-
ers (corresponding to the person in ?a person from
Tokyo?), lead to early hints. This supports our in-
tuitive notion that birthplaces should be presented
early for users to start thinking about a person.
6 Summary and Future Work
This paper proposed ranking definitions of a person
to automatically generate a ?Who is this?? quiz.
Using reference ranking data that we created man-
ually, we trained a ranking model using a ranking
SVM based on features that encode the importance
of keywords in a definition as well as its content.
Rank Feature Name Weight
1 Wikipedia-Sentence-PMI-max 0.723
2 SemCat:33 (others/someone) -0.559
3 SemCat:186 (creator) 0.485
4 BOW:bakufu (feudal government) 0.451
5 SemCat:163 (sovereign/ruler/monarch) 0.422
6 Wikipedia-Document-PMI-max 0.409
7 SemCat:2391 (birth) -0.404
8 Wikipedia-Section-PMI-max 0.402
9 SemCat:2595 (unit; e.g., numeral classifier) 0.374
10 SemCat:2606 (plural; e.g., plural form) -0.368
Table 2: Weights of features learned for ranking def-
initions by the ranking SVM. SemCat denotes it is
a semantic-category feature with its semantic cate-
gory ID followed by the description of the category
in parentheses. BOW denotes a BOW feature.
Experimental results show that our ranking model
significantly outperforms baselines that use single
IR-related and positional measures for ranking. We
are currently in the process of building a dialogue
system that uses the quiz-style ranking for definition
presentation. We are planning to examine how the
different rankings affect the understanding and mo-
tivation of users.
References
Amy Baylor and Jeeheon Ryu. 2003. Does the presence of
image and animation enhance pedagogical agent persona?
Journal of Educational Computing Research, 28(4):373?
395.
Fergus I. M. Craik and Robert S. Lockhart. 1972. Levels of
processing: A framework for memory research. Journal of
Verbal Learning and Verbal Behavior, 11:671?684.
Tsutomu Hirao, Hideki Isozaki, Eisaku Maeda, and Yuji Mat-
sumoto. 2002. Extracting important sentences with support
vector machines. In Proc. 19th COLING, pages 342?348.
Satoru Ikehara, Satoshi Shirai, Akio Yokoo, and Hiromi
Nakaiwa. 1991. Toward an MT system without pre-editing
?Effects of new methods in ALT-J/E?. In Proc. Third Ma-
chine Translation Summit: MT Summit III, pages 101?106.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio
Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi
Ooyama, and Yoshihiko Hayashi. 1997. Goi-Taikei ? A
Japanese Lexicon. Iwanami Shoten.
Thorsten Joachims. 2002. Optimizing search engines using
clickthrough data. In Proc. KDD, pages 133?142.
Mirella Lapata. 2003. Probabilistic text structuring: Exper-
iments with sentence ordering. In Proc. 41st ACL, pages
545?552.
Akira Sugiyama, Kohji Dohsaka, and Takeshi Kawabata. 1999.
A method for conveying the contents of written texts by spo-
ken dialogue. In Proc. PACLING, pages 54?66.
Renxu Sun, Jing Jiang, Yee Fan Tan, Hang Cui, Tat-Seng Chua,
and Min-Yen Kan. 2005. Using syntactic and semantic rela-
tion analysis in question answering. In Proc. TREC.
Jinxi Xu, Ralph Weischedel, and Ana Licuanan. 2004. Eval-
uation of an extraction-based approach to answering defini-
tional questions. In Proc. SIGIR, pages 418?424.
Jun Xu, Yunbo Cao, Hang Li, and Min Zhao. 2005. Rank-
ing definitions with supervised learning methods. In Proc.
WWW, pages 811?819.
120
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 124?127,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Analysis of Listening-oriented Dialogue for Building Listening Agents
Toyomi Meguro, Ryuichiro Higashinaka, Kohji Dohsaka, Yasuhiro Minami, Hideki Isozaki
NTT Communication Science Laboratories, NTT Corporation
2-4, Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0237, Japan
{meguro,rh,dohsaka,minami,isozaki}@cslab.kecl.ntt.co.jp
Abstract
Our aim is to build listening agents that
can attentively listen to the user and sat-
isfy his/her desire to speak and have him-
self/herself heard. This paper investigates
the characteristics of such listening-oriented
dialogues so that such a listening process
can be achieved by automated dialogue sys-
tems. We collected both listening-oriented
dialogues and casual conversation, and ana-
lyzed them by comparing the frequency of
dialogue acts, as well as the dialogue flows
using HiddenMarkovModels (HMMs). The
analysis revealed that listening-oriented dia-
logues and casual conversation have charac-
teristically different dialogue flows and that
it is important for listening agents to self-
disclose before asking questions and to utter
more questions and acknowledgment than in
casual conversation to be good listeners.
1 Introduction
Although task-oriented dialogue systems have been
actively researched over the years (Walker et al,
2001), systems that perform more flexible (less task-
oriented) dialogues such as chats are beginning to be
actively investigated from their social and entertain-
ment aspects (Bickmore and Cassell, 2001; Higuchi
et al, 2008).
This paper deals with dialogues in which one con-
versational participant attentively listens to the other
(hereafter, listening-oriented dialogue). Our aim is
to build listening agents that can implement such a
listening process so that a user can satisfy his/her
desire to speak and have him/herself heard. Such
agents would lead the user?s state of mind for the
better as in a therapy session, although we want our
listening agents to help users mentally in everyday
conversation. It should also be noted that the pur-
pose of the listening-oriented dialogue is to simply
listen to users, not to elicit information as in inter-
views.
L: The topic is ?travel?, so did you
travel during summer vacation?
(QUESTION)
S: I like traveling. (SELF-DISCLOSURE)
L: Oh! I see! (SYMPATHY)
Why do you like to travel? (QUESTION)
S: This summer, I just went back
to my hometown.
(SELF-DISCLOSURE)
I was busy at work, but I?m
planning to go to Kawaguchi
Lake this weekend.
(SELF-DISCLOSURE)
I like traveling because it is
stimulating.
(SELF-DISCLOSURE)
L: Going to unusual places
changes one?s perspective,
doesn?t it?
(SYMPATHY)
You said you?re going to go to
Kawaguchi Lake this weekend.
Is this travel?
(QUESTION)
Will you go by car or train? (QUESTION)
Figure 1: Excerpt of a typical listening-oriented di-
alogue. Dialogue acts corresponding to utterances
are shown in parentheses (See Section 3.1 for their
meanings). The dialogue was originally in Japanese
and was translated by the authors.
There has been little research on listening agents.
One exception is (Maatman et al, 2005), which
showed that systems can make the user have the
sense of being heard by using gestures, such as nod-
ding and shaking of the head. Although our work is
similar to theirs, the difference is that we focus more
on verbal communication instead of non-verbal one.
For the purpose of gaining insight into how to
build our listening agents, we collected listening-
oriented dialogues as well as casual conversation,
and compared them in order to reveal the charac-
teristics of the listening-oriented dialogue. Figure 1
shows an example of a typical listening-oriented di-
alogue. In the figure, the conversational participants
talk about travel with the listener (L), repeatedly ask-
ing the speaker (S) to make self-disclosure.
2 Approach
We analyze the characteristics of listening-oriented
dialogues by comparing them with casual conversa-
tion. Here, casual conversation means a dialogue
where conversational participants have no prede-
fined roles (i.e., listeners and speakers). In this
124
study, we collect dialogues in texts because we want
to avoid the particular problems of voice, such as
filled pauses and interruptions, although we plan to
deal with speech input in the future.
As a procedure, we first collect listening-oriented
dialogues and casual conversation using human sub-
jects. Then, we label the collected dialogues with
dialogue act tags (see Section 3.1 for details of the
tags) to facilitate the analysis of the data. In the anal-
ysis, we examine the frequency of the tags in each
type of dialogue. We also look into the difference of
dialogue flows by modeling each type of dialogue by
Hidden Markov Models (HMMs) and comparing the
obtained models. We employ HMMs because they
are useful for modeling sequential data especially
when the number of states is unknown. We check
whether the HMMs for the listening-oriented dia-
logue and casual conversation can be successfully
distinguished from each other to see if the listen-
ing process can be successfully modeled. We also
analyze the transitions between states in the created
HMMs to examine the dialogue flows. We note that
HMMs have been used to model task-oriented dia-
logues (Shirai, 1996) and casual conversation (Iso-
mura et al, 2006). In this study, we use HMMs to
model and analyze listening-oriented dialogues.
3 Data collection
We recruited 16 participants. Eight participated as
listeners and the other eight as speakers. The male-
to-female ratio was even. The participants were 21
to 29 years old. Each participant engaged in four di-
alogues: two casual conversations followed by two
listening-oriented dialogues with a fixed role of lis-
tener/speaker. In listening-oriented dialogue, the lis-
teners were instructed to make it easy for the speak-
ers to say what they wanted to say. When col-
lecting the casual conversation, listeners were not
aware that they would be listeners afterwards. Lis-
teners had never met nor talked to the speakers prior
to the data collection. The listeners and speakers
talked over Microsoft Live MessengerTMin different
rooms; therefore, they could not see each other.
In each conversation, participants chatted for 30
minutes about their favorite topic that they selected
from the topic list we prepared. The topics were
food, travel, movies, music, entertainers, sports,
health, housework and childcare, personal comput-
ers and the Internet, animals, fashion and games. Ta-
ble 1 shows the number of collected dialogues, utter-
ances and words in each utterance of listeners and
Listening Casual
# dialogues 16 16
# utterances 850 720
# words Listener 20.60 17.92
per utt. Speaker 26.46 21.44
Table 1: Statistics of collected dialogues.
speakers. Generally, utterances in listening-oriented
dialogue were longer than those in casual conversa-
tion, probably because the subjects explained them-
selves in detail to make themselves better under-
stood.
At the end of each dialogue, the participants
filled out questionnaires that asked for their sat-
isfaction levels of dialogue, as well as how well
they could talk about themselves to their conver-
sational partners on the 10-point Likert scale. The
analysis of the questionnaire results showed that, in
listening-oriented dialogue, speakers were having a
better sense of making themselves heard than in ca-
sual conversation (Welch?s pairwise t-test; p=0.016)
without any degradation in the satisfaction level of
dialogue. This indicates that the subjects were suc-
cessfully performing attentive listening and that it is
meaningful to investigate the characteristics of the
collected listening-oriented dialogues.
3.1 Dialogue act
We labeled the collected dialogues using the dia-
logue act tag set: (1) SELF-DISCLOSURE (disclo-
sure of one?s preferences and feelings), (2) INFOR-
MATION (delivery of objective information), (3) AC-
KNOWLEDGMENT (encourages the conversational
partner to speak), (4) QUESTION (utterances that ex-
pect answers), (5) SYMPATHY (sympathetic utter-
ances and praises) and, (6) GREETING (social cues
to begin/end a dialogue).
We selected these tags from the DAMSL tag set
(Jurafsky et al, 1997) that deals with general con-
versation and also from those used to label therapy
conversation (Ivey and Ivey, 2002). Since our work
is still preliminary, we selected only a small num-
ber of labels that we thought were important for
modeling utterances in our collected dialogues, al-
though we plan to incorporate other tags in the fu-
ture. We expected that self-disclosure would occur
quite often in our data because the subjects were to
talk about their favorite topics and the participants
would be willing to communicate about their expe-
riences and feelings. We also expected that the lis-
teners would sympathize often to make others talk
with ease. Note that sympathy has been found useful
to increase closeness between conversational partic-
125
Listener Speaker
Casual Listening Casual Listening
DISC 66.6% 44.5% 53.3% 57.3%
INFO 6.5% 1.4% 5.6% 5.2%
ACK 8.0% 12.3% 6.6% 6.9%
QUES 4.1% 25.8% 21.3% 14.0%
SYM 2.6% 3.7% 3.2% 3.3%
GR 10.9% 9.8% 7.2% 9.6%
OTHER 1.3% 2.5% 2.9% 3.7%
Table 2: Rates of dialogue act tags.
DISC INFO ACK QUES SYM GR
Increase 0 0 8 8 5 4
Decrease 8 8 0 0 3 4
Table 3: Number of listeners whose tags in-
creased/decreased in listening-oriented dialogue.
ipants (Reis and Shaver, 1998).
A single annotator, who is not one of the authors,
labeled each utterance using the seven tags (six di-
alogue act tags plus OTHER). As a result, 1,177
tags were labeled to the utterances in the listening-
oriented dialogues and 1,312 tags to those in casual
conversation. The numbers of tags and utterances do
not match because, in text dialogue, an utterance can
be long and may be annotated with several tags.
4 Analysis
4.1 Comparing the frequency of dialogue acts
We compared the frequency of the dialogue act tags
in listening-oriented dialogues and casual conversa-
tion. Table 2 shows the rates of the tags in each type
of dialogue. In the table, OTHER means the expres-
sions that did not fall into any of our six dialogue
acts, such as facial expressions and mistypes. Table
3 shows the number of listeners whose rates of tags
increased or decreased from casual conversation to
listening-oriented dialogue.
Compared to casual conversation, the rates of
SELF-DISCLOSURE and INFORMATION decreased
in the listening-oriented dialogue. On the other
hand, the rates of ACKNOWLEDGMENT and QUES-
TION increased. This means that the listeners tended
to hold the transmission of information and focused
on letting speakers self-disclose or deliver informa-
tion. It can also be seen that the speakers decreased
QUESTION to increase self-disclosure.
4.2 Modeling dialogue act sequences by HMM
We analyzed the flow of listening-oriented dialogue
and casual conversation by modeling their dialogue
act sequences using HMMs. We defined 14 obser-
vation symbols, corresponding to the seven tags for
a listener and the same number of tags for a speaker.
L:Greeting:0.483S:Greeting:0.39
L:Self-disclosure:0.107L:Question:0.456S:Ack:0.224 S:Self-disclosure:0.828
L:Self-disclosure:0.579L:Ack:0.132
0.1
0.58
0.13
0.38
0.83
0.41
0.55
0.51
?
? ?
?
Figure 2: Ergodic HMM for listening-oriented dia-
logue. Circled numbers represent state IDs.
We trained the following two types of HMMs for
each type of dialogue.
Ergodic HMM: Each state emits all 14 observation
symbols. All states are connected to each other.
Speaker HMM: Half the states in this HMM only
emit one speaker?s dialogue acts and the other
half emit other speaker?s dialogue acts. All
states are connected to each other.
The EM algorithm was used to train the HMMs.
To find the best fitting HMM with minimal states,
we trained 1,000 HMMs for each type of HMM by
increasing the number of states from one to ten and
training 100 HMMs for each number of states. This
was necessary because the HMMs severely depend
on the initial probabilities. From the 1,000 HMMs,
we chose the most fitting model using the MDL
(Minimum Description Length) criterion.
4.2.1 Distinguishing Dialogue Types
We performed an experiment to examine whether
the trained HMMs can distinguish listening-oriented
dialogues and casual conversation. For this exper-
iment, we used eight listening-oriented dialogues
and eight casual conversations to train HMMs and
made them classify the remaining 16 dialogues. We
found that Ergodic HMM can distinguish the dia-
logues with an accuracy of 87.5%, and the Speaker
HMM achieved 100% accuracy. This indicates that
we can successfully train HMMs for each type of
dialogue and that investigating the trained HMMs
would show the characteristics of each type of di-
alogue. In the following sections, we analyze the
HMMs trained using all 16 dialogues of each type.
4.2.2 Analysis of Ergodic HMM
Figure 2 shows the Ergodic HMM for listening-
oriented dialogue. It can be seen that the major flow
126
L:Greeting:0.888
L:Self-disclosure:0.445L:Question:0.492 S:Self-disclosure:0.835
L:Self-disclosure:0.556L:Ack:0.27
S:Greeting:0.98
S:Self-disclosure:0.125S:Ack:0.661
0.42 0.370.43
0.38
0.11
0.56 0.51
0.18 0.92
0.47
0.63
0.25
? ?
? ?
? ?
Figure 3: Speaker HMM for listening-oriented dia-
logue.
S2:Greeting:0.775
S2:Self-disclosure:0.523S2:Question:0.414S1:Self-disclosure:0.644S1:Question:0.26
S2:Self-disclosure:0.629S2:Ack:0.12
S1:Greeting:0.848
S1:Self-disclosure:0.662S1:Ack:0.135
0.45 0.350.45
0.45 0.110.16
0.32 0.42
0.43
0.1
0.740.51
0.12
0.76 0.15
? ?
? ?
? ?
Figure 4: Speaker HMM for casual conversation.
of dialogue acts are: 2? L?s question ? 3? S?s self-
disclosure ? 4? L?s self-disclosure ? 2? L?s ques-
tion. This flow indicates that listeners tend to self-
disclose before the next question, showing the cycle
of reciprocal self-disclosure. This indicates that lis-
tening agents would need to have the capability of
self-disclosure in order to become human-like lis-
teners.
4.2.3 Analysis of Speaker HMM
Figures 3 and 4 show the Speaker HMMs for
listening-oriented dialogue and casual conversation,
respectively. Here, L and S correspond to S1 and
S2. It can be clearly seen that the two HMMs
have very similar structures. From the probabili-
ties, states with the same IDs seem to correspond to
each other. When we compare state IDs 3 and 5, it
can be seen that, when speakers take the role of lis-
teners, they reduce self-disclosure while increasing
questions and acknowledgment. Questions seem to
have more importance in listening-oriented dialogue
than in casual conversation, indicating that listening
agents need to have a good capability of generating
questions. The agents would also need to explicitly
increase acknowledgment in their utterances. Note
that, compared to spoken dialogue, acknowledgment
has to be performed consciously in text-based dia-
logue. When we compare state ID 4, we see that
the speaker starts questioning in casual conversation,
whereas the speaker only self-discloses in listening-
oriented dialogue. This shows that, in our data, the
speakers are successfully concentrating on making
self-disclosure in listening-oriented dialogue.
5 Conclusion and Future work
We collected listening-oriented dialogue and ca-
sual conversation, and compared them to find the
characteristics of listening-oriented dialogues that
are useful for building automated listening agents.
Our analysis found that it is important for listen-
ing agents to self-disclose before asking questions
and that it is necessary to utter more questions and
acknowledgment than in casual conversation to be
good listeners. As future work, we plan to use a
more elaborate tag set to further analyze the dia-
logue flows. We also plan to extend the HMMs
to Partially Observable Markov Decision Processes
(POMDPs) (Williams and Young, 2007) to achieve
dialogue management of listening agents from data.
References
Timothy Bickmore and Justine Cassell. 2001. Relational
agents: A model and implementation of building user trust.
In Proc. ACM CHI, pages 396?403.
Shinsuke Higuchi, Rafal Rzepka, and Kenji Araki. 2008. A
casual conversation system using modality and word associ-
ations retrieved from the web?. In EMNLP, pages 382?390.
Naoki Isomura, Fujio Toriumi, and Kenichiro Ishii. 2006.
Evaluation method of non-task-oriented dialogue system by
HMM. In Proc. the 4th Symposium on Intelligent Media In-
tegration for Social Information Infrastructure, pages 149?
152.
Allen E. Ivey and Mary Bradford Ivey. 2002. Intentional Inter-
viewing and Counseling: Facilitating Client Development in
a Multicultural Society. Brooks/Cole Publishing Company.
Dan Jurafsky, Liz Shriberg, and Debra Biasca, 1997. Switch-
board SWBD-DAMSL Shallow-Discourse-Function Annota-
tion Coders Manual.
Martijn Maatman, Jonathan Gratch, and Stacy Marsella. 2005.
Natural behavior of a listening agent. Lecture Notes in Com-
puter Science, 3661:25?36.
Harry T. Reis and Phillip Shaver. 1998. Intimacy as an inter-
personal process. In S. Duck, editor, Handbook of personal
relationships, pages 367?398. John Wiley & Sons Ltd.
Katsuhiko Shirai. 1996. Modeling of spoken dialogue with and
without visual information. In Proc. ICSLP, volume 1, pages
188?191.
Marilyn A. Walker, Rebecca Passonneau, and Julie E. Boland.
2001. Quantitative and qualitative evaluation of darpa com-
municator spoken dialogue systems. In Proc. ACL, pages
515?522.
Jason D. Williams and Steve Young. 2007. Partially observ-
able Markov decision processes for spoken dialog systems.
Computer Speech and Language, 21(2):393?422.
127
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 217?224,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Effects of Conversational Agents on Human Communication
in Thought-Evoking Multi-Party Dialogues
Kohji Dohsaka
NTT Communication Science Laboratories
NTT Corporation
2-4, Hikaridai, Seika-cho,
Kyoto 619-0237, Japan
Ryota Asai
Graduate School of
Information Science and Technology
Osaka University, 1-1 Yamadaoka,
Suita, Osaka 565-0871, Japan
Ryuichiro Higashinaka and Yasuhiro Minami and Eisaku Maeda
NTT Communication Science Laboratories, NTT Corporation
2-4, Hikaridai, Seika-cho, Kyoto 619-0237, Japan
{dohsaka,rh,minami,maeda}@cslab.kecl.ntt.co.jp
Abstract
This paper presents an experimental
study that analyzes how conversational
agents activate human communication in
thought-evoking multi-party dialogues be-
tween multi-users and multi-agents. A
thought-evoking dialogue, which is a kind
of interaction in which agents act on user
willingness to provoke user thinking, has
the potential to stimulate multi-party in-
teraction. In this paper, we focus on
quiz-style multi-party dialogues between
two users and two agents as an example
of a thought-evoking multi-party dialogue.
The experiment results showed that the
presence of a peer agent significantly im-
proved user satisfaction and increased the
number of user utterances. We also found
that agent empathic expressions signifi-
cantly improved user satisfaction, raised
user ratings of a peer agent, and increased
user utterances. Our findings will be use-
ful for stimulating multi-party communi-
cation in various applications such as ed-
ucational agents and community facilita-
tors.
1 Introduction
Conversational interfaces including dialogue sys-
tems and conversational agents have been typi-
cally used as a single interface to a single user (Zue
et al, 1994; Allen et al, 2001; Cassell et al,
2000). On the other hand, a new area of re-
search in conversational interfaces is dealing with
multi-party interaction (Traum and Rickel, 2002;
Liu and Chee, 2004; Zheng et al, 2005). Multi-
party conversational interfaces have been applied
to such tasks as training decision-making in team
activities (Traum and Rickel, 2002), collabora-
tive learning (Liu and Chee, 2004), and coordinat-
ing and facilitating interaction in a casual social
group (Zheng et al, 2005).
The advantage of such multi-party dialogues
over two-party cases is that the multi-party case
encourages group interaction and collaboration
among human users. This advantage can be ex-
ploited to foster such human activities as student
learning in more social settings and to build and
maintain social relationships among people. How-
ever, unless users actively engage in the interac-
tion, these multi-party dialogue qualities cannot
be adequately exploited. Our objective is to stim-
ulate human communication in multi-party dia-
logues between multi-users and multi-agents by
raising user willingness to engage in the interac-
tion and increasing the number of user utterances.
As the first step toward this objective, we ex-
ploit a new style of dialogue called thought-
evoking dialogue and experimentally investigate
the impact of a peer agent?s presence and agent
emotional expressions on communication activa-
tion in thought-evoking multi-party dialogues. A
thought-evoking dialogue, an interaction in which
agents act on the willingness of users to provoke
user thinking and encourage involvement in the
dialogue, has the potential to activate interaction
among participants in multi-party dialogues.
Previous work proposed a quiz-style informa-
tion presentation dialogue system (hereafter quiz-
style dialogue system) (Higashinaka et al, 2007a)
that is regarded as a kind of thought-evoking di-
alogue system. This system conveys contents as
biographical facts of famous people through quiz-
style interaction with users by creating a ?Who
is this?? quiz and individually presenting hints.
217
The hints are automatically created from the bi-
ographical facts of people and ordered based on
the difficulty naming the people experienced by
the users (Higashinaka et al, 2007b). Since the
user has to consider the hints to come up with rea-
sonable answers, the system stimulates user think-
ing. This previous work reported that, for in-
teraction between a single user and a computer,
a quiz-style dialogue improved user understand-
ing and willingness to engage in the interaction.
In this paper, we focus on a quiz-style informa-
tion presentation multi-party dialogue (hereafter
quiz-style multi-party dialogue) as an example of
a thought-evoking multi-party dialogue.
A peer agent acts as a peer of the users and par-
ticipates in the interactions in the same way that
the users do. We are interested in the peer agent?s
role in quiz-style multi-party dialogues since the
positive effects of a peer agent on users have been
shown in the educational domain (Chou et al,
2003; Maldonado et al, 2005), which is a promis-
ing application area for quiz-style dialogues. In
the educational domain, a user could benefit not
only from direct communication with a peer agent
but also from overhearing dialogues between a
peer agent and a tutor. Learning by observing oth-
ers who are learning is called vicarious learning
and positively affects user performance (Craig et
al., 2000; Stenning et al, 1999). To the best of our
knowledge, detailed experimental investigations
on the effect of a peer agent on communication
activation have not been reported in multi-party
dialogues between multi-users and multi-agents,
which are our main concern in this paper.
The topic of emotion has gained widespread
attention in human-computer interaction (Bates,
1994; Picard, 1997; Hudlicka, 2003; Prendinger
and Ishizuka, 2004). The impact of an agent?s
emotional behaviors on users has also recently
been studied (Brave et al, 2005; Maldonado et
al., 2005; Prendinger et al, 2005). However, these
previous studies addressed scenario-based interac-
tion in which a user and an agent acted with prede-
termined timing. In this paper, we investigate the
impact of agent emotional expressions on users in
multi-party dialogues in which multiple users and
agents can make utterances with more flexible tim-
ing.
Resembling work by Brave et al (2005), we
classify agent emotional expressions into em-
pathic and self-oriented ones and investigate their
impact on users in a thought-evoking multi-party
dialogue system. As stated above, Brave et
al. (2005) addressed scenario-based Black-jack in-
teraction, but we deal with multi-party dialogues
that enable more flexible turn-taking. Previous
studies (Bickmore and Picard, 2005; Higashinaka
et al, 2008) showed that agent empathic expres-
sions have a positive psychological impact upon
users, but they only examined two-party cases.
Although Traum et al (2002) and Gebhard et
al. (2004) exploited the role of agent emotion in
multi-party dialogues, they did not adequately ex-
amine the effects of agent emotion on communi-
cation activation by experiment.
In this work, we deal with disembodied agents
and focus on their linguistic behaviors. We believe
that our results are useful for designing embodied
conversational agents using other modalities.
This paper presents an experimental study that
analyzes how agents stimulate human communi-
cation in quiz-style multi-party dialogues between
two users and two agents. We are especially inter-
ested in how the presence of a peer agent and agent
emotional expressions improve user satisfaction,
enhance user opinions about the peer agent, and
increase the number of user utterances. Our find-
ings will be useful for stimulating human com-
munication in various applications such as educa-
tional agents and community facilitators.
In the following, Section 2 shows an overview
of our quiz-style multi-party dialogue system.
Section 3 explains the experiment design, and Sec-
tion 4 describes the results. Section 5 concludes
the paper.
2 Thought-Evoking Multi-Party
Dialogue System
We implemented a quiz-style multi-party dialogue
system between multi-users and multi-agents. The
system is a Japanese keyboard-based dialogue sys-
tem with a chat-like interface. The users can make
utterances any time they want. A user utterance
is completed and displayed on the chat window
when the Enter key is pressed.
Our experiment dealt with cases where two
users and two agents engaged in a dialogue. The
two agents are a quizmaster and a peer. The quiz-
master agent creates a ?Who is this?? quiz about
a famous person and presents hints one by one to
the users and the peer agent who guess the correct
answer.
218
1 Whowho Who is this? First hint: Graduated
from the University of Tokyo.
2 Mary I don?t know.
3 Kinoko Yoshida Shigeru.
4 Whowho No, not even close!
5 Jane That?s very difficult.
6 Kinoko Difficult for me, too.
7 Whowho Second hint: Born in Ushigome,
Edo.
...
8 Whowho Third hint: Novelist and scholar of
British literature.
9 Mary Murakami Haruki.
10 Whowho Close!
11 Kinoko You are close. Excellent.
12 Jane Well then, who is it?
13 Whowho Fourth hint: Familiar with Haiku,
Chinese poetry, and calligraphy.
14 Mary Natsume Soseki.
15 Whowho That?s right. Wonderful.
16 Kinoko Mary, excellent. I?m happy for
you.
17 Jane Mary, that?s the right answer. Good
job.
Figure 1: Sample dialogue
Figure 1 shows a sample dialogue. Mary and
Jane are human users. Whowho is the quizmaster
agent, and Kinoko is the peer agent. Quizmaster
agent Whowho presents hints in lines 1, 7, 8, and
13. Users Mary and Jane and peer agent Kinoko
give answers in lines 3, 9, and 14.
The hints were automatically created using
biographical facts (in Japanese) of people in
Wikipedia 1 based on a previously reported
method (Higashinaka et al, 2007b).
2.1 Dialogue acts
The users and the two agents perform several dia-
logue acts based on the dialogue context.
Present-hint: The quizmaster agent presents
hints one by one (lines 1, 7, 8, and 13) in the
sample dialogue shown in Figure 1.
Give-ans: Users and the peer agent give answers
(lines 3, 9, and 14).
Show-difficulty: Users and the peer agent offer
opinions about the quiz difficulty (lines 2, 5,
6, and 12).
1http://ja.wikipedia.org/
Evaluate-ans: When the answer is wrong, the
quizmaster agent evaluates the answer based
on the person-name similarity score (Hi-
gashinaka et al, 2007a) and utters ?very
close!,? ?close!,? ?a little close!,? ?a little far,?
?far,? or ?not even close!? (lines 4 and 10).
Complete-quiz-with-success: When the right
answer is given, the quizmaster agent in-
forms the dialogue participants that the
current quiz is completed (line 15).
Complete-quiz-with-failure: If all hints have
been generated and no right answer is given,
the quizmaster agent gives the right answer,
and the current quiz is completed.
Feedback-on-wrong-ans: Users and the peer
agent give feedback when their own or the
other?s answers are wrong during the current
quiz (line 11).
Feedback-on-success: Users and the peer agent
give feedback when their own or the other?s
answers are right and the current quiz session
is completed (lines 16 and 17).
Feedback-on-failure: Users and the peer agent
give feedback when the current quiz is com-
pleted without the right answer.
Address-hearer: Users and the two agents spec-
ify an intended addressee by uttering the
other?s name (lines 16 and 17).
When a user utterance is input, the system sep-
arates it into word tokens using a Japanese mor-
phological analyzer and converts it into dialogue
acts using hand-crafted grammar. The system can
recognize 120,000 proper names of persons.
2.2 Utterance generation
Surface realization forms were prepared for each
dialogue act by the agents. Agent utterances are
generated by randomly selecting one of the forms.
Some agent dialogue acts can be generated
with emotional expressions. Agent emotional ex-
pressions are categorized into empathic and self-
oriented ones (Brave et al, 2005). The agent
self-oriented emotional expressions (self-oriented
expressions) are oriented to their own state, and
the agent empathic expressions are oriented to the
other?s state and are congruent with the other?s
219
Dialog act Emotion Expressions
Show-
difficulty
EMP Difficult for me, too.
Show-
difficulty
SELF I don?t remember.
That?s so frustrating.
Show-
difficulty
NONE I don?t know.
Feedback-
on-success
EMP You?re right. I?m
happy for you.
Feedback-
on-success
SELF I?m really glad I got
the correct answer.
Feedback-
on-success
NONE You?re right / I?m
right.
Feedback-
on-failure
EMP Too bad you didn?t
know the right an-
swer.
Feedback-
on-failure
SELF I?m disappointed
that I didn?t know
the right answer.
Feedback-
on-failure
NONE I/You didn?t know
the right answer.
Table 1: Examples of agent expressions. EMP
shows empathic expressions, SELF shows self-
oriented expressions, and NONE shows neutral
expressions when neither emotion is present.
welfare. As explained in 3.1, we prepared differ-
ent experimental conditions to determine the pres-
ence/absence of agent empathic and self-oriented
expressions. Based on the conditions, we con-
trolled the agent emotional expressions. Table 1
shows examples of agent empathic, self-oriented,
and neutral expressions.
2.3 Dialogue management
The system maintains a dialogue state in which
the history of the participant?s dialogue acts is
recorded with the time of each act. We prepared
preconditions of each dialogue act by the agents.
For example, the quizmaster agent?s Evaluate-
ans can be executed after the users or the peer
agent provides a wrong answer. The peer agent?s
Feedback-on-success can be executed after the
quizmaster agent performs Complete-quiz-with-
success. We also used the following turn-taking
rules:
1. Either agent must talk when neither the users
nor the agents make utterances within a given
time (4 sec.).
Condition Peer
agent
Empathic Self-
oriented
(0) Absent Absent Absent
(1) Present Absent Absent
(2) Present Present Absent
(3) Present Absent Present
(4) Present Present Present
Table 2: Experimental conditions based on pres-
ence/absence of peer agent and agent empathic
and self-oriented expressions
2. Agents must not talk for a given time (0.5
sec.) after the others talk.
3. The quizmaster agent must move to the next
hint when neither the users nor the peer agent
give a correct answer within a given time (30
sec.).
Based on the dialogue state, the preconditions
of the dialogue acts and the turn-taking rules, the
system chooses the next speaker and its dialogue
act.
3 Experiment
3.1 Experimental conditions
To evaluate the effects of the presence of the peer
agent and the agent emotional expressions, we
prepared five systems under different experimen-
tal conditions, (0), (1), (2), (3), and (4), based on
the presence/absence of the peer agent and agent
empathic and self-oriented expressions. They are
shown in Table 2. In condition (0), the peer agent
was absent, and only the quizmaster agent was
present. In other conditions, both the quizmas-
ter and peer agents were present. In conditions
(0) and (1), neither empathic nor self-oriented ex-
pressions were exhibited. In condition (2), only
empathic expressions were exhibited. In condition
(3), only self-oriented expressions were exhibited.
In condition (4), both empathic and self-oriented
expressions were exhibited.
We evaluated the effects of the presence of the
peer agent by comparing conditions (0) and (1).
We evaluated the effects of agent empathic and
self-oriented expressions by comparing conditions
(1), (2), (3), and (4).
3.2 Measures
We used three measures: user satisfaction, user
opinions about the peer agent, and the number of
220
Questionnaire items
Q1 Did you want to converse with this sys-
tem again? (Willingness to engage in di-
alogue)
Q2 Was the dialogue enjoyable? (Pleasant-
ness of dialogue)
Q3 Did you feel satisfied using the dialogue
system? (Satisfaction of system usage)
Q4 Was the peer agent friendly? (Agent?s
closeness)
Q5 Did you feel that the peer agent cared
about you? (Agent?s caring)
Q6 Was the peer agent likable? (Agent?s lik-
ability)
Q7 Did the peer agent support you?
(Agent?s support)
Table 3: Questionnaire items to evaluate user sat-
isfaction (Q1, Q2, and Q3) and user opinions
about the peer agent (Q4, Q5, Q6, and Q7)
user utterances. Among these measures, we re-
garded the number of user utterances as an ob-
jective measure to evaluate communication activa-
tion. User satisfaction and opinions about the peer
agent are subjective measures based on the ques-
tionnaires (ten-point Likert scale). Table 3 shows
the questionnaires used in the experiment. We ex-
pected that a high level of user satisfaction and
positive opinions about the peer agent would lead
to a high level of user engagement, which would
promote user utterances.
User satisfaction was evaluated from different
perspectives with three questions: Q1, Q2, and
Q3. Q1 focused on user willingness to engage in
the dialogue; Q2 focused on the user experience
of the dialogue?s pleasantness; Q3 focused on user
satisfaction with the system. We evaluated user
satisfaction with averages of the ratings of Q1, Q2,
and Q3. Using the averaged ratings of Likert ques-
tions allows us to apply such parametric statistical
tests as a multi-factor ANOVA since the summed
or averaged responses to Likert questions tend to
follow a normal distribution.
User opinions about the peer agent were evalu-
ated in terms of how the user perceived the peer
agent?s closeness (Q4), its caring (Q5), its likabil-
ity (Q6), and its support (Q7). We evaluated user
opinions about the peer agent with the averaged
ratings of these items. Previous studies showed
that empathic behaviors exhibited by an agent im-
proved user opinions about the agent in a Black-
jack scenario (Brave et al, 2005) and in a social
dialogue between a single user and an agent (Hi-
gashinaka et al, 2008). We examined these items
in multi-party dialogues with flexible turn-taking.
3.3 Procedure
We recruited and paid 64 Japanese adults (32
males and 32 females) for their participation. The
mean ages of the male and female groups were
32.0 and 36.2, respectively (male group: SD=9.2
, min=22, max=59, female group: SD=9.6,
min=20, max=50). The participants were divided
into 32 pairs of the same gender: 16 pairs of males
and 16 pairs of females. The participants in each
pair were unacquainted.
The experiment had a within-participants de-
sign. Each pair of participants successively en-
gaged in dialogues using the five systems under
different experimental conditions. The order of
using the systems was counter-balanced to prevent
order effect.
Before starting the experiment, the participants
were informed that, after completing a dialogue
with each system, they would fill out question-
naires. The questionnaires on user opinions about
the peer agent were used only when it was present
(conditions (1), (2), (3), and (4)). The participants
were also told that the agents were computer pro-
grams and not human participants. During the ex-
periment, each pair of participants was seated in
separate rooms in front of a computer display, a
keyboard, and a mouse, and they could only com-
municate with each other through the system.
In the dialogue with each system, five ?Who
is this?? quizzes about famous people were pre-
sented. The quiz subjects were chosen so that
the difficulty level of the quizzes was approxi-
mately the same in all the systems. For this pur-
pose, we first sorted people in Wikipedia in de-
scending order by their PageRank TM score based
on Wikipedia?s hyper-link structure. We then ex-
tracted the top-50 people and divided them from
the top into five groups of 10. Next we randomly
selected five people from each group to make
five sets of five people of approximately identical
PageRank scores. Each set of five people was used
for quizzes in each system.
On average, a pair of participants took 18 min-
utes to complete a dialogue with each system. The
number of hints that were actually presented in a
221
Figure 2: User satisfaction
quiz averaged 7.5.
4 Results
4.1 User satisfaction
For questions Q1, Q2, and Q3, Cronbach?s alpha
was 0.83, which justified combining these items
into a single index. Therefore we evaluated user
satisfaction with averages of the ratings of these
items. Figure 2 shows user satisfaction under each
experimental condition.
To evaluate the effect of the peer agent?s pres-
ence on user satisfaction, we compared conditions
(0) and (1). The F-test results showed that vari-
ances were assumed to be equal across groups
(p > 0.2), and the Kolmogorov-Smirnov test re-
sults showed that the assumption of normality was
satisfied (p > 0.6). By applying the paired t-test
to both the male and female groups, we found that
the peer agent?s presence significantly improved
user satisfaction (male group: t(31) = 4.2, p <
0.001, female group: t(31) = 2.8, p < 0.008).
To evaluate the effect of the empathic and self-
oriented expressions exhibited by the agents on
user satisfaction, we compared conditions (1),
(2), (3), and (4). A three-factor ANOVA was
conducted with two within-participant factors of
empathic and self-oriented expressions and one
between-participant factor of gender. The F-test
for the homogeneity of variances (p > 0.1) and
the Kolmogorov-Smirnov normality test (p > 0.1)
showed that the data met the ANOVA assump-
tions. As a result of the ANOVA, a signifi-
cant main effect was found for empathic expres-
sions with respect to user satisfaction, F (1, 62) =
92.7, p < 0.001. No significant main effects were
found for either self-oriented expressions or gen-
der, and there were no significant interactions.
Figure 3: User ratings of peer agent
These results showed that the peer agent?s pres-
ence and the agent empathic expressions signif-
icantly improved user satisfaction in quiz-style
multi-party dialogues.
4.2 User opinions about the peer agent
For questions Q4, Q5, Q6, and Q7, Cronbach?s
alpha was 0.92, which justified combining these
items into a single index. Therefore we evaluated
user opinions about the peer agent with the aver-
aged ratings of these items under each experimen-
tal condition. Figure 3 shows the user ratings of
the peer agent under each condition.
To evaluate the effect of agent empathic and
self-oriented expressions on the user ratings of the
peer agent, we compared conditions (1), (2), (3)
and (4). A three-factor ANOVA was conducted
with two within-participant factors of empathic
and self-oriented expressions and one between-
participant factor of gender. The F-test for the
homogeneity of variances (p > 0.3) and the
Kolmogorov-Smirnov normality test (p > 0.2)
showed that the data met the ANOVA assump-
tions. As a result of the ANOVA, a significant
main effect was found for empathic expressions
with respect to the user ratings of the peer agent,
F (1, 62) = 77.4, p < 0.001. There was a
moderate main effect for self-oriented expressions
with respect to the user ratings of the peer agent,
F (1, 62) = 4.38, p < 0.04. There were no sig-
nificant main effects for gender, and there were no
significant interactions.
These results showed that agent empathic ex-
pressions significantly improved user ratings of
the peer agent in quiz-style multi-party dialogues.
222
Figure 4: User utterances per quiz hint
4.3 Number of user utterances
Figure 4 shows the number of user utterances per
quiz hint under each condition.
To evaluate the effect of the peer agent?s pres-
ence on the number of user utterances per quiz
hint, we compared conditions (0) and (1). Based
on the F-test and the Kolmogorov-Smirnov test,
the assumptions of variance homogeneity (p >
0.6) and normality (p > 0.5) were met. By apply-
ing the paired t-test to both the male and female
groups, we found that the presence of the peer
agent significantly increased the number of user
utterances per hint (male group: t(31) = 3.1, p <
0.004, female group: t(31) = 5.6, p < 0.001).
To evaluate the effect of empathic and self-
oriented expressions by agents on the number
of user utterances, we compared conditions (1),
(2), (3), and (4). A three-factor ANOVA was
conducted with two within-participant factors of
empathic and self-oriented expressions and one
between-participant factor of gender. The F-test
for the homogeneity of variances (p > 0.05) and
the Kolmogorov-Smirnov normality test (p > 0.6)
showed that the data met the ANOVA assump-
tions. As a result of the ANOVA, a significant
main effect was found for empathic expressions
with respect to the number of user utterances,
F (1, 62) = 18.9, p < 0.001. No significant main
effects were found for either self-oriented expres-
sions or gender, and there were no significant in-
teractions.
These results showed that the peer agent?s pres-
ence and agent empathic expressions increased
the number of user utterances and stimulated hu-
man communication in quiz-style multi-party dia-
logues.
5 Conclusion
This paper experimentally analyzed how conver-
sational agents stimulate human communication
in thought-evoking multi-party dialogues between
multi-users and multi-agents. As an example of
such multi-party dialogue, we focused on quiz-
style multi-party dialogues between two users and
two agents. We investigated how a peer agent?s
presence and agent emotional expressions influ-
enced user satisfaction, the user ratings of the peer
agent, and the number of user utterances. The
user ratings of the peer agent included user?s per-
ceived closeness, likability and caring from the
peer agent, and the user?s feeling of being sup-
ported by the peer agent.
The experiment results showed that the peer
agent?s presence significantly improved user sat-
isfaction and increased the number of user utter-
ances. We also found significant effects that agent
empathic expressions improved user satisfaction
and user positive ratings of the peer agent and that
they further increased the number of user utter-
ances. These results indicate that employing a peer
agent and agent empathic behaviors in thought-
evoking multi-party dialogues will stimulate inter-
action among people in computer-mediated com-
munication. Our findings will be useful for a
broader class of applications such as educational
agents and community facilitators.
Many directions for future work remain. First,
we plan to extend our work to deal with various
modalities such as speech, gestures, body posture,
facial expressions, and the direction of eye gazes
to investigate the effects of agent representation
(embodied or disembodied) and other modalities
in thought-evoking multi-party dialogues. Second,
we will analyze how agent behaviors influence
users and dialogues in more detail and develop a
more sophisticated dialogue management method
based on our detailed analysis. Learning optimal
dialogue management strategies in multi-party di-
alogues is a challenging research topic. Third, ex-
amining the relationship between user personality
traits and the impact of agents on users is valuable.
Previous work reported that the effect of embodi-
ment depended on user personalities (Lee et al,
2006). This direction is important to the stimula-
tion of multi-party interaction for therapeutic and
emotional support.
223
References
James Allen, Donna Byron, Myroslava Dzikovska,
George Ferguson, Lucian Galescu, and Amanda
Stent. 2001. Toward conversational human-
computer interaction. AI Magazine, 22(4):27?37.
Joseph Bates. 1994. The role of emotion in believable
agents. Communications of the ACM, 37(7):122?
125.
Timothy W. Bickmore and Rosalind W. Picard. 2005.
Establishing and maintaining long-term human-
computer relationships. ACM Transactions on
Computer-Human Interaction, 12(2):293?327.
Scott Brave, Clifford Nass, and Kevin Hutchinson.
2005. Computers that care: investigating the effects
of orientation of emotion exhibited by an embodied
computer agent. International Journal of Human-
Computer Studies, 62(2):161?178.
Justine Cassell, Joseph Sullivan, Scott Prevost, and
Elizabeth Churchill, editors. 2000. Embodied Con-
versational Agents. MIT Press, Cambridge, MA.
Chih-Yueh Chou, Tak-Wai Chan, and Chi-Jen Lin.
2003. Redefining the learning companion: the past,
present, and future of educational agents. Comput-
ers & Education, 40(3):255?269.
Scotty D. Craig, Barry Gholson, Matthew Ventura,
Arthur C. Graesser, and the Tutoring Research
Group. 2000. Overhearing dialogues and mono-
logues in virtual tutoring sessions: Effects on ques-
tioning and vicarious learning. International Jour-
nal of Artificial Intelligence in Education, 11:242?
253.
Patrick Gebhard, Martin Klesen, and Thomas Rist.
2004. Coloring multi-character conversations
through the expression of emotions. In Lecture
Notes in Computer Science (Tutorial and Research
Workshop on Affective Dialogue Systems), volume
3068, pages 128?141.
Ryuichiro Higashinaka, Kohji Dohsaka, Shigeaki
Amano, and Hideki Isozaki. 2007a. Effects of quiz-
style information presentation on user understand-
ing. In Proceedings of the 8th Annual Conference
of the International Speech Communication Associ-
ation, pages 2725?2728.
Ryuichiro Higashinaka, Kohji Dohsaka, and Hideki
Isozaki. 2007b. Learning to rank definitions to
generate quizzes for interactive information presen-
tation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
(Poster Presentation), pages 117?120.
Ryuichiro Higashinaka, Kohji Dohsaka, and Hideki
Isozaki. 2008. Effects of self-disclosure and em-
pathy in human-computer dialogue. In Proceedings
of 2008 IEEE Workshop on Spoken Language Tech-
nology, pages 109?112.
Eva Hudlicka. 2003. To feel or not to feel: The role of
affect in human-computer interaction. International
Journal of Human-Computer Studies, 59(1-2):1?32.
Kwan Min Lee, Younbo Jung, Jaywoo Kim, and
Sang Ryong Kim. 2006. Are physically em-
bodied social agents better than disembodied social
agents?: Effects of embodiment, tactile interaction,
and people?s loneliness in human-robot interaction.
International Journal of Human-Computer Studies,
64(10):962?973.
Yi Liu and Yam San Chee. 2004. Intelligent pedagog-
ical agents with multiparty interaction support. In
Proceedings of International Conference on Intelli-
gent Agent Technology, pages 134?140.
Heidy Maldonado, Jong-Eun Roselyn Lee, Scott
Brave, Cliff Nass, Hiroshi Nakajima, Ryota Ya-
mada, Kimihiko Iwamura, and Yasunori Morishima.
2005. We learn better together: enhancing elearn-
ing with emotional characters. In Proceedings of the
2005 Conference on Computer Support for Collab-
orative Learning, pages 408?417.
Rosalind W. Picard. 1997. Affective Computing. MIT
Press, Cambridge, MA.
Helmut Prendinger and Mitsuru Ishizuka, editors.
2004. Life-Like Characters: Tools, Affective Func-
tions, and Applications. Springer, Berlin.
Helmut Prendinger, Junichiro Mori, and Mitsuru
Ishizuka. 2005. Using human physiology to eval-
uate subtle expressivity of a virtual quizmaster in
a mathematical game. International Journal of
Human-Computer Studies, 62(2):231?245.
Keith Stenning, Jean McKendree, John Lee, Richard
Cox, Finbar Dineen, and Terry Mayes. 1999. Vi-
carious learning from educational dialogue. In Pro-
ceedings of the 1999 Conference on Computer Sup-
port for Collaborative Learning, pages 341?347.
David Traum and Jeff Rickel. 2002. Embodied agents
for multi-party dialogue in immersive virtual worlds.
In Proceedings of the 1st International Joint Confer-
ence on. Autonomous Agents and Multi-Agent Sys-
tems, pages 766?773.
Jun Zheng, Xiang Yuan, and Yam San Chee. 2005.
Designing multiparty interaction support in Elva, an
embodied tour guide. In Proceedings of the 4th In-
ternational Joint Conference on Autonomous Agents
and Multiagent Systems, pages 929?936.
Victor Zue, Stephanie Seneff, Joseph Polifroni,
Michael Phillips, Christine Pao, David Goodine,
David Goddeau, and James Glass. 1994. PEGA-
SUS: a spoken dialogue interface for on-line air
travel planning. Speech Communication, 15:331?
340.
224
Interactive Paraphrasing
Based on Linguistic Annotation
Ryuichiro Higashinaka
Keio Research Institute at SFC
5322 Endo, Fujisawa-shi,
Kanagawa 252-8520, Japan
rh@sfc.keio.ac.jp
Katashi Nagao
Dept. of Information Engineering
Nagoya University
Furo-cho, Chikusa-ku,
Nagoya 464-8603, Japan
nagao@nuie.nagoya-u.ac.jp
Abstract
We propose a method ?Interactive Paraphras-
ing? which enables users to interactively para-
phrase words in a document by their definitions,
making use of syntactic annotation and word
sense annotation. Syntactic annotation is used
for managing smooth integration of word sense
definitions into the original document, and word
sense annotation for retrieving the correct word
sense definition for a word in a document. In
this way, documents can be paraphrased so that
they fit into the original context, preserving the
semantics and improving the readability at the
same time. No extra layer (window) is necessary
for showing the word sense definition as in con-
ventional methods, and other natural language
processing techniques such as summarization,
translation, and voice synthesis can be easily
applied to the results.
1 Introduction
There is a large number of documents of great
diversity on the Web, which makes some of the
documents difficult to understand due to view-
ers? lack of background knowledge. In particu-
lar, if technical terms or jargon are contained in
the document, viewers who are unfamiliar with
them might not understand their correct mean-
ings.
When we encounter unknown words in a doc-
ument, for example scientific terms or proper
nouns, we usually look them up in dictionar-
ies or ask experts or friends for their mean-
ings. However, if there are lots of unfamiliar
words in a document or there are no experts
around, the work of looking the words up can
be very time consuming. To facilitate the effort,
we need (1) machine understandable online dic-
tionaries, (2) automated consultation of these
dictionaries, and (3) effective methods to show
the lookup results.
There is an application which consults online
dictionaries when the user clicks on a certain
word on a Web page, then shows the lookup re-
sults in a popped up window. In this case, the
application accesses its inner/online dictionaries
and the consultation process is automated using
the viewer?s mouse click as a cue. Popup win-
dows correspond to the display method. Other
related applications operate more or less in the
same way.
We encounter three big problems with the
conventional method.
First, due to the difficulty of word sense dis-
ambiguation, in the case of polysemic words, ap-
plications to date show all possible word sense
candidates for certain words, which forces the
viewer to choose the correct meaning.
Second, the popup window showing the
lookup results hides the area near the clicked
word, so that the user tends to lose the context
and has to reread the original document.
Third, since the document and the dictio-
nary lookup results are shown in different layers
(e.g., windows), other natural language process-
ing techniques such as summarization, transla-
tion, and voice synthesis cannot be easily ap-
plied to the results.
To cope with these problems, we realized a
systematic method to annotate words in a doc-
ument with word senses in such a way that
anyone (e.g., the author) can easily add word
sense information to a certain word using a user-
friendly annotating tool. This operation can be
considered as a creation of a link between a word
in the document and a node in a domain-specific
ontology.
The ?Interactive Paraphrasing? that we pro-
pose makes use of word sense annotation and
paraphrases words by embedding their word
sense definitions into the original document to
generate a new document.
Embedding occurs at the user?s initiative,
which means that the user decides when and
where to embed the definition. The generated
document can also be the target for another em-
bedding operation which can be iterated until
the document is understandable enough for the
user.
One of the examples of embedding a doc-
ument into another document is quotation.
Transcopyright (Nelson, 1997) proposes a way
for quoting hypertext documents.
However, quoting means importing other doc-
uments as they are. Our approach is to convert
other documents so that they fit into the orig-
inal context, preserving the semantics and im-
proving the readability at the same time.
As the result of embedding, there are no win-
dows hiding any part of the original text, which
makes the context easy to follow, and the new
document is ready to be used for further natural
language processing.
2 Example
In this section, we present how our system per-
forms using screenshots.
Figure 1 shows an example of a Web docu-
ment 1 after the automatic lookup of dictionary.
Words marked with a different remains back-
ground color have been successfully looked up.
Figure 1: Example of a web document showing
dictionary lookup results
The conventional method such as showing the
definition of a word in a popup window hides the
neighboring text. (Figure 2)
Figure 2: Example of a conventional method
popup window for showing the definition
1This text, slightly modified here, is from ?Internet
Agents: Spiders, Wanderers, Brokers, and Bots,? Fah-
Chun Cheong, New Riders Publishing, 1996.
Figure 3 shows the result of paraphrasing the
word ?agent.? It was successfully paraphrased
using its definition ?personal software assistants
with authority delegated from their users.? The
word ?deployed? was also paraphrased by the
definition ?to distribute systematically.? The
paraphrased area is marked by a different back-
ground color.
Figure 3: Example of the results after para-
phrasing ?agents? and ?deployed?
Figure 4 shows the result of paraphrasing the
word in the area already paraphrased. The word
?authority? was paraphrased by its definition
?power to make decisions.?
Figure 4: Example of incremental paraphrasing
3 Linguistic Annotation
Semantically embedding word sense definitions
into the original document without changing
the original context is much more difficult than
showing the definition in popup windows.
For example, replacing some word in a sen-
tence only with its word sense definition may
cause the original sentence to be grammatically
wrong or less cohesive.
This is due to the fact that the word sense def-
initions are usually incapable of simply replac-
ing original words because of their fixed forms.
For appropriately integrating the word sense
definition into the original context, we employ
syntactic annotation (described in the next sec-
tion) to both original documents and the word
sense definitions to let the machine know their
contexts.
Thus, we need two types of annotations for
Interactive Paraphrasing. One is the word sense
annotation to retrieve the correct word sense
definition for a particular word, and the other is
the syntactic annotation for managing smooth
integration of word sense definitions into the
original document.
In this paper, linguistic annotation covers
syntactic annotation and word sense annota-
tion.
3.1 Syntactic Annotation
Syntactic annotation is very useful to make on-
line documents more machine-understandable
on the basis of a new tag set, and to de-
velop content-based presentation, retrieval,
question-answering, summarization, and
translation systems with much higher qual-
ity than is currently available. The new
tag set was proposed by the GDA (Global
Document Annotation) project (Hasida,
http://www.etl.go.jp/etl/nl/gda/). It is based
on XML , and designed to be as compatible
as possible with TEI (The Text Encoding Ini-
tiative, http://www.uic.edu:80/orgs/tei/)
and CES (Corpus Encoding Standard,
http://www.cs.vassar.edu/CES/). It specifies
modifier-modifiee relations, anaphor-referent
relations, etc.
An example of a GDA-tagged sentence is as
follows:
 ?
<su><np rel="agt">Time</np>
<v>flies</v><adp rel="eg">
<ad>like</ad><np>an <n>arrow</n></np>
</adp>.</su>
? ?
The tag, <su>, refers to a sentential unit.
The other tags above, <n>, <np>, <v>, <ad> and
<adp> mean noun, noun phrase, verb, adnoun
or adverb (including preposition and postposi-
tion), and adnominal or adverbial phrase, re-
spectively.
Syntactic annotation is generated by auto-
matic morphological analysis and interactive
sentence parsing.
Some research issues concerning syntactic an-
notation are related to how the annotation cost
can be reduced within some feasible levels. We
have been developing some machine-guided an-
notation interfaces that conceal the complexity
of annotation. Machine learning mechanisms
also contribute to reducing the cost because
they can gradually increase the accuracy of au-
tomatic annotation.
3.2 Word Sense Annotation
In the computational linguistic field, word sense
disambiguation has been one of the biggest is-
sues. For example, to have a better translation
of documents, disambiguation of certain poly-
semic words is essential. Even if an estimation
of the word sense is achieved to some extent, in-
correct interpretation of certain words can lead
to irreparable misunderstanding.
To avoid this problem, we have been pro-
moting annotation of word sense for polysemic
words in the document, so that their word
senses can be machine-understandable.
For this purpose, we need a dictionary of con-
cepts, for which we use existing domain ontolo-
gies. An ontology is a set of descriptions of con-
cepts - such as things, events, and relations -
that are specified in some way (such as specific
natural language) in order to create an agreed-
upon vocabulary for exchanging information.
Annotating a word sense is therefore equal to
creating a link between a word in the document
and a concept in a certain domain ontology. We
have made a word sense annotating tool for this
purpose which has been integrated with the an-
notation editor described in the next section.
3.3 Annotation Editor
Our annotation editor, implemented as a Java
application, facilitates linguistic annotation of
the document. An example screen of our anno-
tation editor is shown in Figure 5.
Figure 5: Annotation editor
The left window of the editor shows the docu-
ment object structure of the HTML document.
The center window shows some text that was
selected on the Web browser as shown on the
right top of the figure. The selected area is auto-
matically assigned an XPointer (i.e., a location
identifier in the document) (World Wide Web
Consortium, http://www.w3.org/TR/xptr/).
The right bottom window shows the linguistic
structure of the sentence in the selected area. In
this window, the user can modify the results of
the automatically-analyzed sentence structure.
Using the editor, the user annotates text
with linguistic structure (syntactic and seman-
tic structure) and adds a comment to an ele-
ment in the document. The editor is capable of
basic natural language processing and interac-
tive disambiguation.
The tool also supports word sense annotation
as shown in Figure 6. The ontology viewer ap-
pears in the right middle of the figure. The user
can easily select a concept in the domain ontol-
ogy and assign a concept ID to a word in the
document as a word sense.
Figure 6: Annotation editor with ontology
viewer
4 Interactive Paraphrasing
Using the linguistic annotation (syntactic and
word sense annotation), Interactive Paraphras-
ing offers a way to paraphrase words in the doc-
ument on user demand.
4.1 Interactivity
One of the objectives of this research is to make
online documents more understandable by para-
phrasing unknown words using their word sense
definitions.
Users can interactively select words to para-
phrase by casual movements like mouse clicks.
The paraphrase history is stored for later use
such as profile-based paraphrasing (yet to be
developped) which automatically selects words
to paraphrase based on user?s knowledge.
The resulting sentence can also be a target
for the next paraphrase. By allowing incremen-
tal operation, users can interact with the doc-
ument until there are no paraphrasable words
in the document or the document has become
understandable enough.
Interactive Paraphrasing is divided into click
paraphrasing and region paraphrasing accord-
ing to user interaction type. The former para-
phrases a single word specified by mouse click,
and the latter, one or more paraphrasable words
in a specified region.
4.2 Paraphrasing Mechanism
As described in previous sections, the original
document and the word sense definitions are an-
notated with linguistic annotation, which means
they have graph structures. A word corresponds
to a node, a phrase or sentence to a subgraph.
Our paraphrasing is an operation that replaces
a node with a subgraph to create a new graph.
Linguistic operations are necessary for creating
a graph that correctly fits the original context.
We have made some simple rules (principles)
for replacing a node in the original document
with a node representing the word sense defini-
tion.
There are two types of rules for paraphrasing.
One is a ?global rule? which can be applied to
any pair of nodes, the other is a ?local rule?
which takes syntactic features into account.
Below is the description of paraphrasing rules
(principles) that we used this time. Org stands
for the node in the original document to be
paraphrased by Def which represents the word
sense definition node. Global rules are applied
first followed by local rules. Pairs to which rules
cannot be applied are left as they are.
- Global Rules -
1. If the word Org is included in Def , para-
phrasing is not performed to avoid the loop
of Org.
2. Ignore the area enclosed in parentheses in
Def . The area is usually used for making
Def an independent statement.
3. Avoid double negation, which increases the
complexity of the sentence.
4. To avoid redundancy, remove from Def the
same case-marked structure found both in
Org and Def .
5. Other phrases expressing contexts in Def
are ignored, since similar contexts are likely
to be in the original sentence already.
- Local Rules -
The left column shows the pair of linguistic
features 2 corresponding to Org and Def . (e.g.
N ? N signifies the rule to be applied between
nodes having noun features.)
2
N stands for the noun feature, V , AJ and AD for
verbal, adjective and adverbial features respectively.
N ?N Replace Org with Def agreeing in
number.
N ? V Nominalize Def and replace Org.
(e.g., explain ? the explanation of)
V ?N If there is a verbal phrase modify-
ing Def , conjugate Org using Def ?s
conjugation and replace Org.
V ? V Apply Org?s conjugation to Def
and replace Org.
AD ?N Replace Org with any adverbial
phrase modifying Def .
AJ ?N Replace Org with any adjective
phrase modifying Def .
4.3 Implementation
We have implemented a system to realize Inter-
active Paraphrasing. Figure 7 shows the basic
layout of the system. The proxy server in the
middle deals with user interactions, document
retrievals, and the consultation of online dictio-
naries.
Figure 7: System architecture
The paraphrasing process follows the steps
described below.
1. On a user?s request, the proxy server
retrieves a document through which it
searches for words with word sense anno-
tations. If found, the proxy server changes
their background color to notify the user of
the paraphrasable words.
2. The user specifies a word in the document
on the browser.
3. Receiving the word to be paraphrased, the
proxy server looks it up in online dictio-
naries using the concept ID assigned to the
word.
4. Using the retrieved word sense definition,
the proxy server attempts to integrate it
into the original document using linguistic
annotation attached to both the definition
and the original document.
5 Related Work
Recently there have been some activities to add
semantics to the Web (Nagao et al, 2001) (Se-
manticWeb.org, http://www.semanticweb.org/)
(Heflin and Hendler, 2000) enabling comput-
ers to better handle online documents. As
for paraphrasing rules concerning structured
data, Inui et al are developing Kura (Inui
et al, 2001) which is a Transfer-Based Lexico-
Structural Paraphrasing Engine.
6 Conclusion and Future Plans
We have described a method, ?Interactive Para-
phrasing?, which enables users to interactively
paraphrase words in a document by their defi-
nitions, making use of syntactic annotation and
word sense annotation.
By paraphrasing, no extra layer (window) is
necessary for showing the word sense definition
as in conventional methods, and other natural
language processing techniques such as summa-
rization, translation, and voice synthesis can be
easily applied to the results.
Our future plans include: reduction of
the annotation cost, realization of profile-based
paraphrasing using personal paraphrasing his-
tory, and retrieval of similar pages for semanti-
cally merging them using linguistic annotation.
References
Jeff Heflin and James Hendler. 2000. Semantic In-
teroperability on the Web. In Proceedings of Ex-
treme Markup Languages 2000. Graphic Commu-
nications Association, 2000. pp. 111-120.
Kentaro Inui, Tetsuro Takahashi, Tomoya Iwakura,
Ryu Iida, and Atsushi Fujita. 2001. KURA:
A Transfer-Based Lexico-Structural Paraphrasing
Engine. In Proceedings of the 6th Natural Lan-
guage Processing Pacific Rim Symposium, Work-
shop on Automatic Paraphrasing: Theories and
Applications.
Katashi Nagao, Yoshinari Shirai, and Kevin Squire.
2001. Semantic annotation and transcoding:
Making Web content more accessible. IEEE Mul-
tiMedia. Vol. 8, No. 2, pp. 69?81.
Theodor Holm Nelson. 1997. Transcopyright: Deal-
ing with the Dilemma of Digital Copyright.
Educom Review, Vol. 32, No. 1, pp. 32-35.
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 265?272,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Learning to Generate Naturalistic Utterances Using Reviews in Spoken
Dialogue Systems
Ryuichiro Higashinaka
NTT Corporation
rh@cslab.kecl.ntt.co.jp
Rashmi Prasad
University of Pennsylvania
rjprasad@linc.cis.upenn.edu
Marilyn A. Walker
University of Sheffield
walker@dcs.shef.ac.uk
Abstract
Spoken language generation for dialogue
systems requires a dictionary of mappings
between semantic representations of con-
cepts the system wants to express and re-
alizations of those concepts. Dictionary
creation is a costly process; it is currently
done by hand for each dialogue domain.
We propose a novel unsupervised method
for learning such mappings from user re-
views in the target domain, and test it on
restaurant reviews. We test the hypothesis
that user reviews that provide individual
ratings for distinguished attributes of the
domain entity make it possible to map re-
view sentences to their semantic represen-
tation with high precision. Experimental
analyses show that the mappings learned
cover most of the domain ontology, and
provide good linguistic variation. A sub-
jective user evaluation shows that the con-
sistency between the semantic representa-
tions and the learned realizations is high
and that the naturalness of the realizations
is higher than a hand-crafted baseline.
1 Introduction
One obstacle to the widespread deployment of
spoken dialogue systems is the cost involved
with hand-crafting the spoken language generation
module. Spoken language generation requires a
dictionary of mappings between semantic repre-
sentations of concepts the system wants to express
and realizations of those concepts. Dictionary cre-
ation is a costly process: an automatic method
for creating them would make dialogue technol-
ogy more scalable. A secondary benefit is that a
learned dictionary may produce more natural and
colloquial utterances.
We propose a novel method for mining user re-
views to automatically acquire a domain specific
generation dictionary for information presentation
in a dialogue system. Our hypothesis is that re-
views that provide individual ratings for various
distinguished attributes of review entities can be
used to map review sentences to a semantic rep-
An example user review (we8there.com)
Ratings Food=5, Service=5, Atmosphere=5,
Value=5, Overall=5
Review
comment
The best Spanish food in New York. I am
from Spain and I had my 28th birthday
there and we all had a great time. Salud!
?
Review comment after named entity recognition
The best {NE=foodtype, string=Spanish} {NE=food,
string=food, rating=5} in {NE=location, string=New
York}. . . .
?
Mapping between a semantic representation (a set of
relations) and a syntactic structure (DSyntS)
? Relations:
RESTAURANT has FOODTYPE
RESTAURANT has foodquality=5
RESTAURANT has LOCATION
([foodtype, food=5, location] for shorthand.)
? DSyntS:
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
lexeme : food
class : common noun
number : sg
article : def
ATTR
[
lexeme : best
class : adjective
]
ATTR
?
?
lexeme : FOODTYPE
class : common noun
number : sg
article : no-art
?
?
ATTR
?
?
?
?
?
lexeme : in
class : preposition
II
?
?
lexeme : LOCATION
class : proper noun
number : sg
article : no-art
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 1: Example of procedure for acquiring a
generation dictionary mapping.
resentation. Figure 1 shows a user review in the
restaurant domain, where we hypothesize that the
user rating food=5 indicates that the semantic rep-
resentation for the sentence ?The best Spanish
food in New York? includes the relation ?RESTAU-
RANT has foodquality=5.?
We apply the method to extract 451 mappings
from restaurant reviews. Experimental analyses
show that the mappings learned cover most of the
domain ontology, and provide good linguistic vari-
ation. A subjective user evaluation indicates that
the consistency between the semantic representa-
tions and the learned realizations is high and that
the naturalness of the realizations is significantly
higher than a hand-crafted baseline.
265
Section 2 provides a step-by-step description of
the method. Sections 3 and 4 present the evalua-
tion results. Section 5 covers related work. Sec-
tion 6 summarizes and discusses future work.
2 Learning a Generation Dictionary
Our automatically created generation dictionary
consists of triples (U ,R,S) representing a map-
ping between the original utterance U in the user
review, its semantic representation R(U), and its
syntactic structure S(U). Although templates are
widely used in many practical systems (Seneff and
Polifroni, 2000; Theune, 2003), we derive syn-
tactic structures to represent the potential realiza-
tions, in order to allow aggregation, and other
syntactic transformations of utterances, as well as
context specific prosody assignment (Walker et al,
2003; Moore et al, 2004).
The method is outlined briefly in Fig. 1 and de-
scribed below. It comprises the following steps:
1. Collect user reviews on the web to create a
population of utterances U .
2. To derive semantic representations R(U):
? Identify distinguished attributes and
construct a domain ontology;
? Specify lexicalizations of attributes;
? Scrape webpages? structured data for
named-entities;
? Tag named-entities.
3. Derive syntactic representations S(U).
4. Filter inappropriate mappings.
5. Add mappings (U ,R,S) to dictionary.
2.1 Creating the corpus
We created a corpus of restaurant reviews by
scraping 3,004 user reviews of 1,810 restau-
rants posted at we8there.com (http://www.we8-
there.com/), where each individual review in-
cludes a 1-to-5 Likert-scale rating of different
restaurant attributes. The corpus consists of
18,466 sentences.
2.2 Deriving semantic representations
The distinguished attributes are extracted from the
webpages for each restaurant entity. They in-
clude attributes that the users are asked to rate,
i.e. food, service, atmosphere, value, and over-
all, which have scalar values. In addition, other
attributes are extracted from the webpage, such
as the name, foodtype and location of the restau-
rant, which have categorical values. The name
attribute is assumed to correspond to the restau-
rant entity. Given the distinguished attributes, a
Dist. Attr. Lexicalization
food food, meal
service service, staff, waitstaff, wait staff, server,
waiter, waitress
atmosphere atmosphere, decor, ambience, decoration
value value, price, overprice, pricey, expensive,
inexpensive, cheap, affordable, afford
overall recommend, place, experience, establish-
ment
Table 1: Lexicalizations for distinguished at-
tributes.
simple domain ontology can be automatically de-
rived by assuming that a meronymy relation, rep-
resented by the predicate ?has?, holds between the
entity type (RESTAURANT) and the distinguished
attributes. Thus, the domain ontology consists of
the relations:
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
RESTAURANT has foodquality
RESTAURANT has servicequality
RESTAURANT has valuequality
RESTAURANT has atmospherequality
RESTAURANT has overallquality
RESTAURANT has foodtype
RESTAURANT has location
We assume that, although users may discuss
other attributes of the entity, at least some of the
utterances in the reviews realize the relations spec-
ified in the ontology. Our problem then is to iden-
tify these utterances. We test the hypothesis that,
if an utterance U contains named-entities corre-
sponding to the distinguished attributes, thatR for
that utterance includes the relation concerning that
attribute in the domain ontology.
We define named-entities for lexicalizations of
the distinguished attributes, starting with the seed
word for that attribute on the webpage (Table 1).1
For named-entity recognition, we use GATE (Cun-
ningham et al, 2002), augmented with named-
entity lists for locations, food types, restaurant
names, and food subtypes (e.g. pizza), scraped
from the we8there webpages.
We also hypothesize that the rating given for the
distinguished attribute specifies the scalar value
of the relation. For example, a sentence contain-
ing food or meal is assumed to realize the re-
lation ?RESTAURANT has foodquality.?, and the
value of the foodquality attribute is assumed to be
the value specified in the user rating for that at-
tribute, e.g. ?RESTAURANT has foodquality = 5? in
Fig. 1. Similarly, the other relations in Fig. 1 are
assumed to be realized by the utterance ?The best
Spanish food in New York? because it contains
1In future, we will investigate other techniques for boot-
strapping these lexicalizations from the seed word on the
webpage.
266
filter filtered retained
No Relations Filter 7,947 10,519
Other Relations Filter 5,351 5,168
Contextual Filter 2,973 2,195
Unknown Words Filter 1,467 728
Parsing Filter 216 512
Table 2: Filtering statistics: the number of sen-
tences filtered and retained by each filter.
one FOODTYPE named-entity and one LOCATION
named-entity. Values of categorical attributes are
replaced by variables representing their type be-
fore the learned mappings are added to the dictio-
nary, as shown in Fig. 1.
2.3 Parsing and DSyntS conversion
We adopt Deep Syntactic Structures (DSyntSs) as
a format for syntactic structures because they can
be realized by the fast portable realizer RealPro
(Lavoie and Rambow, 1997). Since DSyntSs are a
type of dependency structure, we first process the
sentences with Minipar (Lin, 1998), and then con-
vert Minipar?s representation into DSyntS. Since
user reviews are different from the newspaper ar-
ticles on which Minipar was trained, the output
of Minipar can be inaccurate, leading to failure in
conversion. We check whether conversion is suc-
cessful in the filtering stage.
2.4 Filtering
The goal of filtering is to identify U that realize
the distinguished attributes and to guarantee high
precision for the learned mappings. Recall is less
important since systems need to convey requested
information as accurately as possible. Our proce-
dure for deriving semantic representations is based
on the hypothesis that if U contains named-entities
that realize the distinguished attributes, thatRwill
include the relevant relation in the domain ontol-
ogy. We also assume that if U contains named-
entities that are not covered by the domain ontol-
ogy, or words indicating that the meaning of U de-
pends on the surrounding context, that R will not
completely characterizes the meaning of U , and so
U should be eliminated. We also require an accu-
rate S for U . Therefore, the filters described be-
low eliminate U that (1) realize semantic relations
not in the ontology; (2) contain words indicating
that its meaning depends on the context; (3) con-
tain unknown words; or (4) cannot be parsed ac-
curately.
No Relations Filter: The sentence does not con-
tain any named-entities for the distinguished
attributes.
Other Relations Filter: The sentence contains
named-entities for food subtypes, person
Rating
Dist.Attr.
1 2 3 4 5 Total
food 5 8 6 18 57 94
service 15 3 6 17 56 97
atmosphere 0 3 3 8 31 45
value 0 0 1 8 12 21
overall 3 2 5 15 45 70
Total 23 15 21 64 201 327
Table 3: Domain coverage of single scalar-valued
relation mappings.
names, country names, dates (e.g., today, to-
morrow, Aug. 26th) or prices (e.g., 12 dol-
lars), or POS tag CD for numerals. These in-
dicate relations not in the ontology.
Contextual Filter: The sentence contains index-
icals such as I, you, that or cohesive markers
of rhetorical relations that connect it to some
part of the preceding text, which means that
the sentence cannot be interpreted out of con-
text. These include discourse markers, such
as list item markers with LS as the POS tag,
that signal the organization structure of the
text (Hirschberg and Litman, 1987), as well
as discourse connectives that signal semantic
and pragmatic relations of the sentence with
other parts of the text (Knott, 1996), such as
coordinating conjunctions at the beginning of
the utterance like and and but etc., and con-
junct adverbs such as however, also, then.
Unknown Words Filter: The sentence contains
words not in WordNet (Fellbaum, 1998)
(which includes typographical errors), or
POS tags contain NN (Noun), which may in-
dicate an unknown named-entity, or the sen-
tence has more than a fixed length of words,2
indicating that its meaning may not be esti-
mated solely by named entities.
Parsing Filter: The sentence fails the parsing to
DSyntS conversion. Failures are automati-
cally detected by comparing the original sen-
tence with the one realized by RealPro taking
the converted DSyntS as an input.
We apply the filters, in a cascading manner, to the
18,466 sentences with semantic representations.
As a result, we obtain 512 (2.8%) mappings of
(U ,R,S). After removing 61 duplicates, 451 dis-
tinct (2.4%) mappings remain. Table 2 shows the
number of sentences eliminated by each filter.
3 Objective Evaluation
We evaluate the learned expressions with respect
to domain coverage, linguistic variation and gen-
erativity.
2We used 20 as a threshold.
267
# Combination of Dist. Attrs Count
1 food-service 39
2 food-value 21
3 atmosphere-food 14
4 atmosphere-service 10
5 atmosphere-food-service 7
6 food-foodtype 4
7 atmosphere-food-value 4
8 location-overall 3
9 food-foodtype-value 3
10 food-service-value 2
11 food-foodtype-location 2
12 food-overall 2
13 atmosphere-foodtype 2
14 atmosphere-overall 2
15 service-value 1
16 overall-service 1
17 overall-value 1
18 foodtype-overall 1
19 food-foodtype-location-overall 1
20 atmosphere-food-service-value 1
21 atmosphere-food-overall-
service-value
1
Total 122
Table 4: Counts for multi-relation mappings.
3.1 Domain Coverage
To be usable for a dialogue system, the mappings
must have good domain coverage. Table 3 shows
the distribution of the 327 mappings realizing a
single scalar-valued relation, categorized by the
associated rating score.3 For example, there are 57
mappings with R of ?RESTAURANT has foodqual-
ity=5,? and a large number of mappings for both
the foodquality and servicequality relations. Al-
though we could not obtain mappings for some re-
lations such as price={1,2}, coverage for express-
ing a single relation is fairly complete.
There are also mappings that express several re-
lations. Table 4 shows the counts of mappings
for multi-relation mappings, with those contain-
ing a food or service relation occurring more fre-
quently as in the single scalar-valued relation map-
pings. We found only 21 combinations of rela-
tions, which is surprising given the large poten-
tial number of combinations (There are 50 com-
binations if we treat relations with different scalar
values differently). We also find that most of the
mappings have two or three relations, perhaps sug-
gesting that system utterances should not express
too many relations in a single sentence.
3.2 Linguistic Variation
We also wish to assess whether the linguistic
variation of the learned mappings was greater
than what we could easily have generated with a
hand-crafted dictionary, or a hand-crafted dictio-
nary augmented with aggregation operators, as in
3There are two other single-relation but not scalar-valued
mappings that concern LOCATION in our mappings.
(Walker et al, 2003). Thus, we first categorized
the mappings by the patterns of the DSyntSs. Ta-
ble 5 shows the most common syntactic patterns
(more than 10 occurrences), indicating that 30%
of the learned patterns consist of the simple form
?X is ADJ? where ADJ is an adjective, or ?X is RB
ADJ,? where RB is a degree modifier. Furthermore,
up to 55% of the learned mappings could be gen-
erated from these basic patterns by the application
of a combination operator that coordinates mul-
tiple adjectives, or coordinates predications over
distinct attributes. However, there are 137 syntac-
tic patterns in all, 97 with unique syntactic struc-
tures and 21 with two occurrences, accounting for
45% of the learned mappings. Table 6 shows ex-
amples of learned mappings with distinct syntactic
structures. It would be surprising to see this type
of variety in a hand-crafted generation dictionary.
In addition, the learned mappings contain 275 dis-
tinct lexemes, with a minimum of 2, maximum of
15, and mean of 4.63 lexemes per DSyntS, indi-
cating that the method extracts a wide variety of
expressions of varying lengths.
Another interesting aspect of the learned map-
pings is the wide variety of adjectival phrases
(APs) in the common patterns. Tables 7 and 8
show the APs in single scalar-valued relation map-
pings for food and service categorized by the as-
sociated ratings. Tables for atmosphere, value and
overall can be found in the Appendix. Moreover,
the meanings for some of the learned APs are very
specific to the particular attribute, e.g. cold and
burnt associated with foodquality of 1, attentive
and prompt for servicequality of 5, silly and inat-
tentive for servicequality of 1. and mellow for at-
mosphere of 5. In addition, our method places the
adjectival phrases (APs) in the common patterns
on a more fine-grained scale of 1 to 5, similar to
the strength classifications in (Wilson et al, 2004),
in contrast to other automatic methods that clas-
sify expressions into a binary positive or negative
polarity (e.g. (Turney, 2002)).
3.3 Generativity
Our motivation for deriving syntactic representa-
tions for the learned expressions was the possibil-
ity of using an off-the-shelf sentence planner to
derive new combinations of relations, and apply
aggregation and other syntactic transformations.
We examined how many of the learned DSyntSs
can be combined with each other, by taking ev-
ery pair of DSyntSs in the mappings and apply-
ing the built-in merge operation in the SPaRKy
generator (Walker et al, 2003). We found that
only 306 combinations out of a potential 81,318
268
# syntactic pattern example utterance count ratio accum.
1 NN VB JJ The atmosphere is wonderful. 92 20.4% 20.4%
2 NN VB RB JJ The atmosphere was very nice. 52 11.5% 31.9%
3 JJ NN Bad service. 36 8.0% 39.9%
4 NN VB JJ CC JJ The food was flavorful but cold. 25 5.5% 45.5%
5 RB JJ NN Very trendy ambience. 22 4.9% 50.3%
6 NN VB JJ CC NN VB JJ The food is excellent and the atmosphere is great. 13 2.9% 53.2%
7 NN CC NN VB JJ The food and service were fantastic. 10 2.2% 55.4%
Table 5: Common syntactic patterns of DSyntSs, flattened to a POS sequence for readability. NN, VB,
JJ, RB, CC stand for noun, verb, adjective, adverb, and conjunction, respectively.
[overall=1, value=2] Very disappointing experience for
the money charged.
[food=5, value=5] The food is excellent and plentiful at a
reasonable price.
[food=5, service=5] The food is exquisite as well as the
service and setting.
[food=5, service=5] The food was spectacular and so was
the service.
[food=5, foodtype, value=5] Best FOODTYPE food with
a great value for money.
[food=5, foodtype, value=5] An absolutely outstanding
value with fantastic FOODTYPE food.
[food=5, foodtype, location, overall=5] This is the best
place to eat FOODTYPE food in LOCATION.
[food=5, foodtype] Simply amazing FOODTYPE food.
[food=5, foodtype] RESTAURANTNAME is the best of the
best for FOODTYPE food.
[food=5] The food is to die for.
[food=5] What incredible food.
[food=4] Very pleasantly surprised by the food.
[food=1] The food has gone downhill.
[atmosphere=5, overall=5] This is a quiet little place
with great atmosphere.
[atmosphere=5, food=5, overall=5, service=5, value=5]
The food, service and ambience of the place are all fabu-
lous and the prices are downright cheap.
Table 6: Acquired generation patterns (with short-
hand for relations in square brackets) whose syn-
tactic patterns occurred only once.
combinations (0.37%) were successful. This is
because the merge operation in SPaRKy requires
that the subjects and the verbs of the two DSyntSs
are identical, e.g. the subject is RESTAURANT and
verb is has, whereas the learned DSyntSs often
place the attribute in subject position as a definite
noun phrase. However, the learned DSyntS can
be incorporated into SPaRKy using the semantic
representations to substitute learned DSyntSs into
nodes in the sentence plan tree. Figure 2 shows
some example utterances generated by SPaRKy
with its original dictionary and example utterances
when the learned mappings are incorporated. The
resulting utterances seem more natural and collo-
quial; we examine whether this is true in the next
section.
4 Subjective Evaluation
We evaluate the obtained mappings in two re-
spects: the consistency between the automatically
derived semantic representation and the realiza-
food=1 awful, bad, burnt, cold, very ordinary
food=2 acceptable, bad, flavored, not enough, very
bland, very good
food=3 adequate, bland and mediocre, flavorful but
cold, pretty good, rather bland, very good
food=4 absolutely wonderful, awesome, decent, ex-
cellent, good, good and generous, great, out-
standing, rather good, really good, tradi-
tional, very fresh and tasty, very good, very
very good
food=5 absolutely delicious, absolutely fantastic, ab-
solutely great, absolutely terrific, ample, well
seasoned and hot, awesome, best, delectable
and plentiful, delicious, delicious but simple,
excellent, exquisite, fabulous, fancy but tasty,
fantastic, fresh, good, great, hot, incredible,
just fantastic, large and satisfying, outstand-
ing, plentiful and outstanding, plentiful and
tasty, quick and hot, simply great, so deli-
cious, so very tasty, superb, terrific, tremen-
dous, very good, wonderful
Table 7: Adjectival phrases (APs) in single scalar-
valued relation mappings for foodquality.
tion, and the naturalness of the realization.
For comparison, we used a baseline of hand-
crafted mappings from (Walker et al, 2003) ex-
cept that we changed the word decor to at-
mosphere and added five mappings for overall.
For scalar relations, this consists of the realiza-
tion ?RESTAURANT has ADJ LEX? where ADJ is
mediocre, decent, good, very good, or excellent for
rating values 1-5, and LEX is food quality, service,
atmosphere, value, or overall depending on the re-
lation. RESTAURANT is filled with the name of
a restaurant at runtime. For example, ?RESTAU-
RANT has foodquality=1? is realized as ?RESTAU-
RANT has mediocre food quality.? The location
and food type relations are mapped to ?RESTAU-
RANT is located in LOCATION? and ?RESTAU-
RANT is a FOODTYPE restaurant.?
The learned mappings include 23 distinct se-
mantic representations for a single-relation (22 for
scalar-valued relations and one for location) and
50 for multi-relations. Therefore, using the hand-
crafted mappings, we first created 23 utterances
for the single-relations. We then created three ut-
terances for each of 50multi-relations using differ-
ent clause-combining operations from (Walker et
al., 2003). This gave a total of 173 baseline utter-
ances, which together with 451 learned mappings,
269
service=1 awful, bad, great, horrendous, horrible,
inattentive, forgetful and slow, marginal,
really slow, silly and inattentive, still
marginal, terrible, young
service=2 overly slow, very slow and inattentive
service=3 bad, bland and mediocre, friendly and
knowledgeable, good, pleasant, prompt,
very friendly
service=4 all very warm and welcoming, attentive,
extremely friendly and good, extremely
pleasant, fantastic, friendly, friendly and
helpful, good, great, great and courteous,
prompt and friendly, really friendly, so
nice, swift and friendly, very friendly, very
friendly and accommodating
service=5 all courteous, excellent, excellent and
friendly, extremely friendly, fabulous,
fantastic, friendly, friendly and helpful,
friendly and very attentive, good, great,
great, prompt and courteous, happy and
friendly, impeccable, intrusive, legendary,
outstanding, pleasant, polite, attentive and
prompt, prompt and courteous, prompt
and pleasant, quick and cheerful, stupen-
dous, superb, the most attentive, unbeliev-
able, very attentive, very congenial, very
courteous, very friendly, very friendly and
helpful, very friendly and pleasant, very
friendly and totally personal, very friendly
and welcoming, very good, very helpful,
very timely, warm and friendly, wonderful
Table 8: Adjectival phrases (APs) in single scalar-
valued relation mappings for servicequality.
yielded 624 utterances for evaluation.
Ten subjects, all native English speakers, eval-
uated the mappings by reading them from a web-
page. For each system utterance, the subjects were
asked to express their degree of agreement, on a
scale of 1 (lowest) to 5 (highest), with the state-
ment (a) The meaning of the utterance is consis-
tent with the ratings expressing their semantics,
and with the statement (b) The style of the utter-
ance is very natural and colloquial. They were
asked not to correct their decisions and also to rate
each utterance on its own merit.
4.1 Results
Table 9 shows the means and standard deviations
of the scores for baseline vs. learned utterances for
consistency and naturalness. A t-test shows that
the consistency of the learned expression is signifi-
cantly lower than the baseline (df=4712, p < .001)
but that their naturalness is significantly higher
than the baseline (df=3107, p < .001). However,
consistency is still high. Only 14 of the learned
utterances (shown in Tab. 10) have a mean consis-
tency score lower than 3, which indicates that, by
and large, the human judges felt that the inferred
semantic representations were consistent with the
meaning of the learned expressions. The correla-
tion coefficient between consistency and natural-
ness scores is 0.42, which indicates that consis-
Original SPaRKy utterances
? Babbo has the best overall quality among the selected
restaurants with excellent decor, excellent service and
superb food quality.
? Babbo has excellent decor and superb food quality
with excellent service. It has the best overall quality
among the selected restaurants.
?
Combination of SPaRKy and learned DSyntS
? Because the food is excellent, the wait staff is pro-
fessional and the decor is beautiful and very com-
fortable, Babbo has the best overall quality among the
selected restaurants.
? Babbo has the best overall quality among the selected
restaurants because atmosphere is exceptionally nice,
food is excellent and the service is superb.
? Babbo has superb food quality, the service is excep-
tional and the atmosphere is very creative. It has the
best overall quality among the selected restaurants.
Figure 2: Utterances incorporating learned
DSyntSs (Bold font) in SPaRKy.
baseline learned stat.
mean sd. mean sd. sig.
Consistency 4.714 0.588 4.459 0.890 +
Naturalness 4.227 0.852 4.613 0.844 +
Table 9: Consistency and naturalness scores aver-
aged over 10 subjects.
tency does not greatly relate to naturalness.
We also performed an ANOVA (ANalysis Of
VAriance) of the effect of each relation in R on
naturalness and consistency. There were no sig-
nificant effects except that mappings combining
food, service, and atmosphere were significantly
worse (df=1, F=7.79, p=0.005). However, there
is a trend for mappings to be rated higher for
the food attribute (df=1, F=3.14, p=0.08) and the
value attribute (df=1, F=3.55, p=0.06) for consis-
tency, suggesting that perhaps it is easier to learn
some mappings than others.
5 Related Work
Automatically finding sentences with the same
meaning has been extensively studied in the field
of automatic paraphrasing using parallel corpora
and corpora with multiple descriptions of the same
events (Barzilay and McKeown, 2001; Barzilay
and Lee, 2003). Other work finds predicates of
similar meanings by using the similarity of con-
texts around the predicates (Lin and Pantel, 2001).
However, these studies find a set of sentences with
the same meaning, but do not associate a specific
meaning with the sentences. One exception is
(Barzilay and Lee, 2002), which derives mappings
between semantic representations and realizations
using a parallel (but unaligned) corpus consisting
of both complex semantic input and correspond-
ing natural language verbalizations for mathemat-
270
shorthand for relations and utterance score
[food=4] The food is delicious and beautifully
prepared.
2.9
[overall=4] A wonderful experience. 2.9
[service=3] The service is bland and mediocre. 2.8
[atmosphere=2] The atmosphere here is eclec-
tic.
2.6
[overall=3] Really fancy place. 2.6
[food=3, service=4] Wonderful service and
great food.
2.5
[service=4] The service is fantastic. 2.5
[overall=2] The RESTAURANTNAME is once a
great place to go and socialize.
2.2
[atmosphere=2] The atmosphere is unique and
pleasant.
2.0
[food=5, foodtype] FOODTYPE and FOODTYPE
food.
1.8
[service=3] Waitstaff is friendly and knowl-
edgeable.
1.7
[atmosphere=5, food=5, service=5] The atmo-
sphere, food and service.
1.6
[overall=3] Overall, a great experience. 1.4
[service=1] The waiter is great. 1.4
Table 10: The 14 utterances with consistency
scores below 3.
ical proofs. However, our technique does not re-
quire parallel corpora or previously existing se-
mantic transcripts or labeling, and user reviews are
widely available in many different domains (See
http://www.epinions.com/).
There is also significant previous work on min-
ing user reviews. For example, Hu and Liu (2005)
use reviews to find adjectives to describe products,
and Popescu and Etzioni (2005) automatically find
features of a product together with the polarity of
adjectives used to describe them. They both aim at
summarizing reviews so that users can make deci-
sions easily. Our method is also capable of finding
polarities of modifying expressions including ad-
jectives, but on a more fine-grained scale of 1 to
5. However, it might be possible to use their ap-
proach to create rating information for raw review
texts as in (Pang and Lee, 2005), so that we can
create mappings from reviews without ratings.
6 Summary and Future Work
We proposed automatically obtaining mappings
between semantic representations and realizations
from reviews with individual ratings. The results
show that: (1) the learned mappings provide good
coverage of the domain ontology and exhibit good
linguistic variation; (2) the consistency between
the semantic representations and realizations is
high; and (3) the naturalness of the realizations are
significantly higher than the baseline.
There are also limitations in our method. Even
though consistency is rated highly by human sub-
jects, this may actually be a judgement of whether
the polarity of the learned mapping is correctly
placed on the 1 to 5 rating scale. Thus, alter-
nate ways of expressing, for example foodqual-
ity=5, shown in Table 7, cannot be guaranteed to
be synonymous, which may be required for use in
spoken language generation. Rather, an examina-
tion of the adjectival phrases in Table 7 shows that
different aspects of the food are discussed. For
example ample and plentiful refer to the portion
size, fancy may refer to the presentation, and deli-
cious describes the flavors. This suggests that per-
haps the ontology would benefit from represent-
ing these sub-attributes of the food attribute, and
sub-attributes in general. Another problem with
consistency is that the same AP, e.g. very good
in Table 7 may appear with multiple ratings. For
example, very good is used for every foodquality
rating from 2 to 5. Thus some further automatic
or by-hand analysis is required to refine what is
learned before actual use in spoken language gen-
eration. Still, our method could reduce the amount
of time a system designer spends developing the
spoken language generator, and increase the natu-
ralness of spoken language generation.
Another issue is that the recall appears to be
quite low given that all of the sentences concern
the same domain: only 2.4% of the sentences
could be used to create the mappings. One way
to increase recall might be to automatically aug-
ment the list of distinguished attribute lexicaliza-
tions, using WordNet or work on automatic iden-
tification of synonyms, such as (Lin and Pantel,
2001). However, the method here has high pre-
cision, and automatic techniques may introduce
noise. A related issue is that the filters are in some
cases too strict. For example the contextual fil-
ter is based on POS-tags, so that sentences that do
not require the prior context for their interpreta-
tion are eliminated, such as sentences containing
subordinating conjunctions like because, when, if,
whose arguments are both given in the same sen-
tence (Prasad et al, 2005). In addition, recall is
affected by the domain ontology, and the automat-
ically constructed domain ontology from the re-
view webpages may not cover all of the domain.
In some review domains, the attributes that get
individual ratings are a limited subset of the do-
main ontology. Techniques for automatic feature
identification (Hu and Liu, 2005; Popescu and Et-
zioni, 2005) could possibly help here, although
these techniques currently have the limitation that
they do not automatically identify different lexi-
calizations of the same feature.
A different type of limitation is that dialogue
systems need to generate utterances for informa-
tion gathering whereas the mappings we obtained
271
can only be used for information presentation.
Thus these would have to be constructed by hand,
as in current practice, or perhaps other types of
corpora or resources could be utilized. In addi-
tion, the utility of syntactic structures in the map-
pings should be further examined, especially given
the failures in DSyntS conversion. An alternative
would be to leave some sentences unparsed and
use them as templates with hybrid generation tech-
niques (White and Caldwell, 1998). Finally, while
we believe that this technique will apply across do-
mains, it would be useful to test it on domains such
as movie reviews or product reviews, which have
more complex domain ontologies.
Acknowledgments
We thank the anonymous reviewers for their help-
ful comments. This work was supported by a
Royal Society Wolfson award to Marilyn Walker
and a research collaboration grant from NTT to
the Cognitive Systems Group at the University of
Sheffield.
References
Regina Barzilay and Lillian Lee. 2002. Bootstrapping lex-
ical choice via multiple-sequence alignment. In Proc.
EMNLP, pages 164?171.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proc. HLT/NAACL, pages 16?23.
Regina Barzilay and Kathleen McKeown. 2001. Extracting
paraphrases from a parallel corpus. In Proc. 39th ACL,
pages 50?57.
Hamish Cunningham, Diana Maynard, Kalina Bontcheva,
and Valentin Tablan. 2002. GATE: A framework and
graphical development environment for robust NLP tools
and applications. In Proc. 40th ACL.
Christiane Fellbaum. 1998. WordNet: An Electronic Lexical
Database (Language, Speech, and Communication). The
MIT Press.
Julia Hirschberg and Diane. J. Litman. 1987. Now let?s talk
about NOW: Identifying cue phrases intonationally. In
Proc. 25th ACL, pages 163?171.
Minqing Hu and Bing Liu. 2005. Mining and summarizing
customer reviews. In Proc. KDD, pages 168?177.
Alistair Knott. 1996. A Data-Driven Methodology for Moti-
vating a Set of Coherence Relations. Ph.D. thesis, Univer-
sity of Edinburgh, Edinburgh.
Benoit Lavoie and Owen Rambow. 1997. A fast and portable
realizer for text generation systems. In Proc. 5th Applied
NLP, pages 265?268.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language En-
gineering, 7(4):343?360.
Dekang Lin. 1998. Dependency-based evaluation of MINI-
PAR. In Workshop on the Evaluation of Parsing Systems.
Johanna D. Moore, Mary Ellen Foster, Oliver Lemon, and
Michael White. 2004. Generating tailored, comparative
descriptions in spoken dialogue. In Proc. 7th FLAIR.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with re-
spect to rating scales. In Proc. 43st ACL, pages 115?124.
Ana-Maria Popescu and Oren Etzioni. 2005. Extracting
product features and opinions from reviews. In Proc.
HLT/EMNLP, pages 339?346.
Rashmi Prasad, Aravind Joshi, Nikhil Dinesh, Alan Lee,
Eleni Miltsakaki, and Bonnie Webber. 2005. The Penn
Discourse TreeBank as a resource for natural language
generation. In Proc. Corpus Linguistics Workshop on Us-
ing Corpora for NLG.
Stephanie Seneff and Joseph Polifroni. 2000. Formal and
natural language generation in the mercury conversational
system. In Proc. ICSLP, volume 2, pages 767?770.
Marie?t Theune. 2003. From monologue to dialogue: natural
language generation in OVIS. In AAAI 2003 Spring Sym-
posium on Natural Language Generation in Written and
Spoken Dialogue, pages 141?150.
Peter D. Turney. 2002. Thumbs up or thumbs down? se-
mantic orientation applied to unsupervised classification
of reviews. In Proc. 40th ACL, pages 417?424.
Marilyn Walker, Rashmi Prasad, and Amanda Stent. 2003.
A trainable generator for recommendations in multimodal
dialog. In Proc. Eurospeech, pages 1697?1700.
Michael White and Ted Caldwell. 1998. EXEMPLARS: A
practical, extensible framework for dynamic text genera-
tion. In Proc. INLG, pages 266?275.
Theresa Wilson, Janyce Wiebe, and Rebecca Hwa. 2004.
Just how mad are you? finding strong and weak opinion
clauses. In Proc. AAAI, pages 761?769.
Appendix
Adjectival phrases (APs) in single scalar-valued
relation mappings for atmosphere, value, and
overall.
atmosphere=2 eclectic, unique and pleasant
atmosphere=3 busy, pleasant but extremely hot
atmosphere=4 fantastic, great, quite nice and simple,
typical, very casual, very trendy, wonder-
ful
atmosphere=5 beautiful, comfortable, excellent, great,
interior, lovely, mellow, nice, nice and
comfortable, phenomenal, pleasant, quite
pleasant, unbelievably beautiful, very
comfortable, very cozy, very friendly,
very intimate, very nice, very nice and
relaxing, very pleasant, very relaxing,
warm and contemporary, warm and very
comfortable, wonderful
value=3 very reasonable
value=4 great, pretty good, reasonable, very good
value=5 best, extremely reasonable, good, great,
reasonable, totally reasonable, very good,
very reasonable
overall=1 just bad, nice, thoroughly humiliating
overall=2 great, really bad
overall=3 bad, decent, great, interesting, really
fancy
overall=4 excellent, good, great, just great, never
busy, not very busy, outstanding, recom-
mended, wonderful
overall=5 amazing, awesome, capacious, delight-
ful, extremely pleasant, fantastic, good,
great, local, marvelous, neat, new, over-
all, overwhelmingly pleasant, pampering,
peaceful but idyllic, really cool, really
great, really neat, really nice, special,
tasty, truly great, ultimate, unique and en-
joyable, very enjoyable, very excellent,
very good, very nice, very wonderful,
warm and friendly, wonderful
272
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 761?769,
Beijing, August 2010
Controlling Listening-oriented Dialogue using Partially Observable
Markov Decision Processes
Toyomi Meguro?, Ryuichiro Higashinaka?, Yasuhiro Minami?, Kohji Dohsaka?
?NTT Communication Science Laboratories, NTT Corporation
?NTT Cyber Space Laboratories, NTT Corporation
meguro@cslab.kecl.ntt.co.jp
higashinaka.ryuichiro@lab.ntt.co.jp
{minami,dohsaka}@cslab.kecl.ntt.co.jp
Abstract
This paper investigates how to automat-
ically create a dialogue control compo-
nent of a listening agent to reduce the cur-
rent high cost of manually creating such
components. We collected a large number
of listening-oriented dialogues with their
user satisfaction ratings and used them to
create a dialogue control component using
partially observable Markov decision pro-
cesses (POMDPs), which can learn a pol-
icy to satisfy users by automatically find-
ing a reasonable reward function. A com-
parison between our POMDP-based com-
ponent and other similarly motivated sys-
tems using human subjects revealed that
POMDPs can satisfactorily produce a dia-
logue control component that can achieve
reasonable subjective assessment.
1 Introduction
Although task-oriented dialogue systems have
been actively researched (Hirshman, 1989; Fer-
guson et al, 1996; Nakano et al, 1999; Walker
et al, 2002), recently non-task-oriented functions
are starting to attract attention, and systems with-
out a specific task that deal with more casual di-
alogues, such as chats, are being actively investi-
gated from their social and entertainment aspects
(Bickmore and Cassell, 2001; Higashinaka et al,
2008; Higuchi et al, 2008).
In the same vein, we have been working on
listening-oriented dialogues in which one conver-
sational participant attentively listens to the other
(hereafter, listening-oriented dialogue). Our aim
is to build listening agents that can implement
such a listening process so that users can satisfy
their desire to speak and be heard. Figure 1 shows
an excerpt from a typical listening-oriented dia-
logue. In the literature, dialogue control compo-
nents for less (or non-) task-oriented dialogue sys-
tems, such as listening agents, have typically used
hand-crafted rules for dialogue control, which
can be problematic because completely covering
all dialogue states by hand-crafted rules is diffi-
cult when the dialogue has fewer task restrictions
(Wallace, 2004; Isomura et al, 2009).
To solve this problem, this paper aims to auto-
matically build a dialogue control component of a
listening agent using partially observable Markov
decision processes (POMDPs). POMDPs, which
make it possible to learn a policy that can max-
imize the averaged reward in partially observable
environments (Pineau et al, 2003), have been suc-
cessfully adopted in task-oriented dialogue sys-
tems for learning a dialogue control module from
data (Williams and Young, 2007). However, no
work has attempted to use POMDPs for less (or
non-) task-oriented dialogue systems, such as lis-
tening agents, because user goals are not as well-
defined as task-oriented ones, complicating the
finding of a reasonable reward function.
We apply POMDPs to listening-oriented dia-
logues by having the system learn a policy that si-
multaneously maximizes how well users feel that
they are being listened to (hereafter, user satis-
faction) and how smoothly the system generates
dialogues (hereafter, smoothness). This formu-
lation is new; no work has considered both user
satisfaction and smoothness using POMDPs. We
collected a large amount of listening-oriented di-
alogues and annotated them with dialogue acts
and also obtained subjective evaluation results for
them. From them, we calculated the rewards and
learned the POMDP policies. We evaluated the
dialogue-act tag sequences of our POMDPs using
human subjects.
761
Utterance Dialogue act
S: Good evening. GREETING
The topic is ?food,? nice to
meet you.
GREETING
L: Nice to meet you, too. GREETING
S: I had curry for dinner. S-DISC (sub: fact)
Do you like curry? QUESTION (sub: pref)
L: Yes, I do. SYMPATHY
S: Really? REPEAT
Me, too. SYMPATHY
L: Do you usually go out to eat? QUESTION (sub: habit)
S: No, I always cook at home. S-DISC (sub: habit)
I don?t use any special spices,
but I sometimes cook noodles
using soup and curry.
S-DISC (sub: habit)
L: That sounds good! S-DISC (sub: pref (pos-
itive))
Figure 1: Excerpt of a typical listening-oriented
dialogue. Dialogue topic is ?food.? Dialogue acts
corresponding to utterances are shown in paren-
theses (See Table 1 for meanings): S-DISC stands
for SELF-DISCLOSURE; PREF for PREFERENCE;
S for speaker; and L for listener. The dialogue
was translated from Japanese by the authors.
The next section introduces related work. Sec-
tion 3 describes our approach. Section 4 de-
scribes our collection of listening-oriented dia-
logues. This is followed in Section 5 by an evalua-
tion experiment that compared our POMDP-based
dialogue control with other similarly motivated
systems. The last section summarizes the main
points and mentions future work.
2 Related work
With increased attention on social dialogues and
senior peer counseling, work continues to emerge
on listening-oriented dialogues. One early work
is (Maatman et al, 2005), which showed that vir-
tual agents can give users the sense of being heard
using such gestures as nodding and head shak-
ing. Recently, Meguro et al (2009a) analyzed
the characteristics of listening-oriented dialogues.
They compared listening-oriented dialogues and
casual conversations between humans, revealing
that the two types of dialogues have significantly
different flows and that listeners actively ques-
tion with frequently inserted self-disclosures; the
speaker utterances were mostly concerned with
self-disclosure.
Shitaoka et al (2010) also investigated the
functions of listening agents, focusing on their
response generation components. Their system
takes the confidence score of speech recognition
into account and changes the system response ac-
cordingly; it repeats the user utterance or makes
an empathic utterance for high-confidence user ut-
terances and makes a backchannel when the con-
fidence is low. The system?s empathic utterances
can be ?I?m happy? or ?That?s too bad,? depend-
ing on whether a positive or negative expression
is included in the user utterances. Their system?s
response generation only uses the speech recogni-
tion confidence and the polarity of user utterances
as cues to choose its actions. Currently, it does
not consider the utterance content or the user in-
tention.
In order for listening agents to achieve high
smoothness, a switching mechanism between the
?active listening mode,? in which the system is
a listener, and the ?topic presenting mode,? in
which the system is a speaker, has been proposed
(Yokoyama et al, 2010; Kobayashi et al, 2010).
Here, the system uses a heuristic function to main-
tain a high user interest level and to keep the sys-
tem in an active listening mode. Dialogue con-
trol is done by hand-crafted rules. Our motivation
bears some similarity to theirs in that we want to
build a listening agent that gives users a sense of
being heard; however, we want to automatically
make such an agent from dialogue data.
POMDPs have been introduced for robot action
control (Pineau et al, 2003). Here, the system
learns to make suitable movements for complet-
ing a certain task. Over the years, POMDPs have
been actively studied for applications to spoken
dialogue systems. Williams et al (2007) suc-
cessfully used a POMDP for dialogue control in a
ticket-buying domain in which the objective was
to fix the departure and arrival places for tickets.
Recent work on POMDPs indicates that it is pos-
sible to train a dialogue control module in task-
oriented dialogues when the user goal is obvious.
In contrast, in this paper, we aim to verify whether
POMDPs can be applied to less task-oriented di-
alogues (i.e., listening-oriented dialogues) where
user goals are not as obvious.
In a recent study, Minami et al (2009) ap-
plied POMDPs to non-task-orientedman-machine
interaction. Their system learned suitable ac-
tion control of agents that can act smoothly by
obtaining rewards from the statistics of artifi-
cially generated data. Our work is different be-
cause we use real human-human dialogue data to
762
train POMDPs for dialogue control in listening-
oriented dialogues.
3 Approach
A typical dialogue system has utterance under-
standing, dialogue control, and utterance gen-
eration modules. The utterance understanding
module comprehends user natural-language utter-
ances, whose output (i.e., a user dialogue act) is
passed to the dialogue control module. The dia-
logue control module chooses the best system di-
alogue act at every dialogue point using the user
dialogue act as input. The utterance generation
module generates natural-language utterances and
says them to users by realizing the system dia-
logue acts as surface forms.
This paper focuses on the dialogue control
module of a listening agent. Since a listening-
oriented dialogue has a characteristic conversation
flow (Meguro et al, 2009a), focusing on this mod-
ule is crucial because it deals with the dialogue
flow. Our objective is to train from data a dialogue
control module that achieves a smooth dialogue
flow that makes users feel that they are being lis-
tened to attentively.
3.1 Dialogue control using POMDPs
The purpose of our dialogue control is to simulta-
neously create situations in which users feel lis-
tened to (i.e., user satisfaction) and to generate
smooth action sequences (i.e., smoothness). To
do this, we automatically and statistically train
the reward and the policy of the POMDP using a
large amount of listening-oriented dialogue data.
POMDP is a reinforcement learning framework
that can learn a policy to select an action sequence
that maximizes average future rewards. Setting a
reward is crucial in POMDPs.
For our purpose, we introduce two different re-
wards: one for user satisfaction and the other for
smoothness. Before creating a POMDP structure,
we used the dynamic Bayesian network (DBN)
structure (Fig. 2) to obtain the statistical structure
of the data and the two rewards.
The random values in the DBN are as follows:
so and sa are the dialogue state and action state,
o is a speaker observation, a is a listener action,
and d is a random variable for an evaluation score
that indicates the degree of the user being listened
to. This evaluation score can be obtained by ques-
'aa
o
s
r
o
s
'o
a
s
a
s
o
?
?
a
'a
o
s
d
o
s
'o
a
s
a
s
o
?
?
DBN structure
POMDP structure
Figure 2: DBN and POMDP structures employed
in this paper. Note that a in the POMDP is isolated
from other states because it is decided by a learned
policy.
tionnaires, and the variable is used for calculat-
ing a user satisfaction reward for the POMDP.
The DBN arcs in Fig. 2 define the emission and
transition probabilities. Pr(o?|s?o) is the emission
probability of o? given s?o. Pr(d|so) is the emis-
sion probability of d given so. Pr(s?o|so, a) is a
transition probability from so to s?o given a. The
DBN is trained using the EM algorithm. Using
the obtained variables, we calculate the two re-
ward functions as follows:
(1) Reward for user satisfaction This reward is
obtained from the d variable by
r1((so, ?), a) =
max?
d=min
d ? Pr(d|so, a),
where * is arbitrary sa and min and max are min-
imum and maximum evaluation scores.
(2) Reward for smoothness For smoothness,
we maximize the action predictive probability
given the history of actions and observations. The
probability is calculated from listening-oriented
dialogue data. sa is introduced for estimating the
predictive probability of action a and for selecting
a to maximize the predictive probability.
We set Pr(a|sa) = 1 when a = sa so that sa
corresponds one-on-one with a. Then, if at = sa
at time t is given, we obtain
Pr(at|o1, a1, . . . , at?1, ot)
=
?
s?a
Pr(at|s?a) Pr(s?a|o1, a1, . . . , at?1, ot)
= Pr(sa|o1, a1, . . . , ot?1, at?1, ot)
Consequently, maximizing the predictive proba-
bility of a equals maximizing that of sa. If we
763
set 1.0 to reward r2((?, sa), a) when sa = a, the
POMDP will generate actions that maximize their
predictive probabilities. We believe that this re-
ward should increase the smoothness of a system
action sequence since the sequence is generated
according to the statistics of human-human dia-
logues.
Converting a DBN into a POMDP The DBN
is converted into a POMDP (Fig. 2), while main-
taining the transition and output probabilities. We
convert d to r as described above.
The system is in a partially observed state.
Since the state is not known exactly, we use a dis-
tribution called ?belief state? bt with which we ob-
tain the average reward that will be gained in the
future at time t by:
Vt =
??
?=0
??
?
s
b?+t((so, sa))r((so, sa), a?+t),
where ? is a discount factor; namely, the future
reward is decreased by ? . A policy is learned by
value iteration so that the action that maximizes
Vt can be chosen. We define r((so, sa), a) as fol-
lows:
r((so, sa), a) = r1((so, ?), a) + r2((?, sa), a).
By balancing these two rewards, we can choose
an action that satisfies both user satisfaction and
smoothness.
4 Data collection
We collected listening-oriented dialogues using
human subjects who consisted of ten listeners
(five males and five females) and 37 speakers (18
males and 19 females). The listeners and speak-
ers ranged from 20 to 60 years old and were all
native Japanese speakers. Listeners and speakers
were matched to form a listener-speaker pair and
communicated over the Internet with our chat in-
terface. They used only text; they were not al-
lowed to use voice, video, or facial expressions.
The speakers chose their own listener and freely
participated in dialogues from 7:00 pm to mid-
night for a period of 15 days. One conversation
was restricted to about ten minutes. The subjects
talked about a topic chosen by the speaker. There
were 20 predefined topics: money, sports, TV and
radio, news, fashion, pets, movies, music, house-
work and childcare, family, health, work, hob-
bies, food, human relationships, reading, shop-
ping, beauty aids, travel, and miscellaneous. The
listeners were instructed to make it easy for the
speakers to say what the speakers wanted to say.
We collected 1260 listening-oriented dialogues.
4.1 Dialogue-act annotation
We labeled the collected dialogues using the
dialogue-act tag set shown in Table 1. We made
these tags by selecting, extending, and modifying
those from previous studies that concerned human
listening behaviors in some way (Meguro et al,
2009a; Jurafsky et al, 1997; Ivey and Ivey, 2002).
In our tag set, only question and self-disclosure
tags have sub-category tags. Two annotators (not
the authors) labeled each sentence of our collected
dialogues using these 32 tags. In dialogue-act an-
notation, since there can be several sentences in
one utterance, one annotator first split the utter-
ances into sentences, and then both annotators la-
beled each sentence with a single dialogue act.
4.2 Obtaining evaluation scores
POMDPs need evaluation scores (i.e., d) for dia-
logue acts (i.e., a) for training a reward function.
Therefore, we asked a third-party participant, who
was neither a listener nor a speaker in our dialogue
data collection, to evaluate the user satisfaction
levels of the collected dialogues. She rated each
dialogue in terms of how she would have felt ?be-
ing heard? after the dialogue if she had been the
speaker of the dialogue in question. She provided
ratings on the 7-point Likert scale for each dia-
logue. Since she rated the whole dialogue with a
single rating, we set the evaluation score of each
action within a dialogue using the evaluation score
for that dialogue.
We used a third-person?s evaluation and not the
original person?s to avoid the fact that the eval-
uative criterion is too different between humans;
identical evaluation scores from two people do
not necessarily reflect identical user satisfaction
levels. We highly valued the reliability and con-
sistency of the third-person scores. This way, at
least, we can train a policy that maximizes its av-
erage reward function for the rater, which we need
to verify first before considering adaptation to two
or more individuals.
5 Experiment
5.1 Experimental setup
The experiment followed three steps.
764
GREETING Greeting and confirmation of dialogue
theme. e.g., Hello. Let?s talk about
lunch.
INFORMATION Delivery of objective information. e.g.,
My friend recommended a restaurant.
SELF-
DISCLOSURE
Disclosure of preferences and feelings.
sub: fact e.g., I live in Tokyo.
sub: experience e.g., I had a hamburger for lunch.
sub: habit e.g., I always go out for dinner.
sub: preference e.g., I like hamburgers.
(positive)
sub: preference e.g., I don?t really like hamburgers.
(negative)
sub: preference e.g., Its taste is near my homemade
(neutral) taste.
sub: desire e.g., I want to try it.
sub: plan e.g., I?m going there next week.
sub: other
ACKNOWLEDGM-
ENT
Encourage the conversational partner to
speak. e.g., Well. Aha.
QUESTION Utterances that expect answers.
sub: information e.g., Please tell me how to cook it.
sub: fact e.g., What kind of curry?
sub: experience e.g., What did you have for dinner?
sub: habit e.g., Did you cook it yourself?
sub: preference e.g., Do you like it?
sub: desire e.g., Don?t you want to eat rice?
sub: plan e.g., What are you going to have for
dinner?
sub: other
SYMPATHY Sympathetic utterances and praises.
e.g., Me, too.
NON-SYMPATHY Negative utterances. e.g., Not really.
CONFIRMATION Confirm what the conversation partner
said. e.g., Really?
PROPOSAL Encourage the partner to act. e.g., Try
it.
REPEAT Repeat the partner?s utterance.
PARAPHRASE Paraphrase the partner?s utterance.
APPROVAL Broach or show goodwill toward the
partner. e.g., Absolutely!
THANKS Express thanks e.g., Thank you.
APOLOGY Express regret e.g., I?m sorry.
FILLER Filler between utterances. e.g., Uh. Let
me see.
ADMIRATION Express affection. e.g., Ha-ha.
OTHER Other utterances.
Table 1: Definition and example of dialogue acts
In the first step, we created our POMDP sys-
tem using our approach (See Section 3.1). We
also made five other systems for comparison that
we describe in Section 5.2. Each system outputs
dialogue-act tag sequences for evaluation. The
dialogue theme was ?food? because it was the
most frequent theme and accounted for 20% of
our data (See Table 2 for the statistics); we trained
our POMDP using the ?food? dialogues. We re-
stricted the dialogue topic to verify that our ap-
proach at least works with a small set. Since there
is no established measure for automatically eval-
uating a dialogue-act tag sequence, we evaluated
All Food (subset of All)
# dialogues 1260 250
# words 479881 94867
# utterances per dialogue 28.2 29.1
# dialogues per listener 126 25
# dialogues per speaker 34 6.8
# dialogue acts 67801 13376
inter-annotator agreement 0.57 0.55
Table 2: Statistics of collected dialogues and
dialogue-act annotation. Inter-annotator agree-
ment means agreement of dialogue-act annotation
using Cohen?s ?.
our dialogue control module using human subjec-
tive evaluations. However, this is very difficult to
do because dialogue control modules only output
dialogue acts, not natural language utterances.
In the second step, we recruited participants
who created natural language utterances from
dialogue-act tag sequences. In their creating dia-
logues, we provided them with situations to stim-
ulate their imaginations. Table 3 shows the situ-
ations, which were deemed common in everyday
Japanese life; we let the participants create utter-
ances that fit the situations. These situations were
necessary because, without restrictions, the evalu-
ation scores could be influenced by dialogue con-
tent rather than by dialogue flow.
For this dialogue-imagining exercise, we re-
cruited 16 participants (eight males and eight fe-
males) who ranged from 19 to 39 years old. Each
participant made twelve dialogues using two situ-
ations. For assigning the situations, we first cre-
ated four conditions: (1) a student and living with
family, (2) working and living with family, (3) a
student and living alone, and (4) working and liv-
ing alone. Then the participants were categorized
into one of these conditions on the basis of their
actual lifestyle and assigned two of the situations
matching the condition.
For each situation, each participant created six
imaginary dialogues from the six dialogue-act se-
quences output by the six systems: our POMDP
and the other five systems for comparison. This
process produced such dialogues as shown in
Figs. 5 and 6. The dialogue in Fig. 5 was made
from a dialogue-act tag sequence of a human-
human conversation using No. 1 of Table 3. The
dialogue in Fig. 6 was made from the sequence of
our POMDP using No. 2 of Table 3.
In the third step, we additionally recruited three
judges (one male and two females) to evalu-
765
ate the imagined 192 (16 ? 2 ? 6) dialogues.
The judges were neither the participants who
made dialogues nor those who rated the collected
listening-oriented dialogues. Six dialogues made
from one situation were randomly shown to the
judges one-by-one, who then filled out question-
naires to indicate their user satisfaction levels by
answering this question on a 7-point Likert scale:
?If you had been the speaker, would you have felt
that you were listened to??
5.2 Systems for comparison
We created our POMDP-based dialogue control
and five other systems for comparison.
POMDP We learned a policy based on our ap-
proach. We used ?food? dialogues (See Section
4), and the evaluation scores were those described
in Section 4.2. This system used the policy to
generate sequences of dialogue-act tags by sim-
ulation; user observations were generated based
on emission probability, and system actions were
generated based on the policy.
In this paper, the total number of observations
and actions was 33 because we have 32 dialogue-
act tags (See Table 1) plus a ?skip? tag. In learning
the policy, an observation and an actionmust indi-
vidually take turns, but our data can include mul-
tiple dialogue-act tags in one utterance. There-
fore, if there is more than one dialogue-act tag
in one utterance, a ?skip? is inserted between the
tags. The state numbers for So and Sa were 16
and 33, respectively. In this experiment, we set 10
to r2((?, sa), a).
EvenPOMDP We arranged a POMDP using
only the smoothness reward (hereafter, Even-
POMDP) by creating a POMDP system with a
fixed evaluation score; hence user satisfaction
is not incorporated in the reward. When using
fixed (even) evaluation scores for all dialogues,
the effect of the user satisfaction reward is de-
nied, and the system only generates highly fre-
quent sequences. We have EvenPOMDP to clarify
whether user satisfaction is necessary. The other
conditions are identical as in the POMDP system.
HMM We modeled our dialogue-act tag se-
quences using a Speaker HMM (SHMM) (Me-
guro et al, 2009a), which has been utilized to
model two-party listening-orienteddialogues. In a
SHMM, half the states emit listener dialogue acts,
Listener?
GREETING
Speaker?
GREETING
Listener?
QUESTION
Speaker?
S-DISC
Listener?
S-DISC
SYMPATHY
or
1
2
3
4
5
Figure 3: Structure of rule-based system
and the other half emit speaker dialogue acts. All
states are connected to each other. We modeled
the ?food? dialogues using an SHMM, and made
themodel generate themost probable dialogue-act
tag sequences. More specifically, first, a dialogue-
act tag was generated randomly based on the ini-
tial state. If the state was that of a listener, we
generated a maximum likelihood action and the
state was randomly transited based on the transi-
tion probability. If the state was that of a speaker,
we randomly generated an action based on the
emission probability and the state was transited
using the maximum likelihood transition proba-
bility.
Rule-based system This system creates
dialogue-act tag sequences using hand-crafted
rules that are based on the findings in (Meguro et
al., 2009a) and are realized as shown in Fig. 3.
A sequence begins at state 1? in Fig. 3, and one
dialogue act is generated at each state. At state
3?, a sub-category tag under QUESTION is chosen
randomly, and at state 4?, a matched sub-category
tag under SELF-DISCLOSURE is chosen. At
state 5?, the listener?s SELF-DISCLOSURE or
SYMPATHY is generated randomly.
Human dialogue sequence This system created
dialogue-act tag sequences by randomly choosing
dialogues between humans from the collected data
and used their annotated tag sequences.
Random This system simply created dialogue-
act tag sequences at random.
5.3 Experimental results
Figure 4 shows the average subjective evaluation
scores. Except between HMM and EvenPOMDP,
there was a significant difference (p<0.01) be-
tween all systems in a non-parametric multiple
comparison test (Steel-Dwass test). The dialogues
shown in Figs. 5 and 6 were generated by the sys-
tems. The dialogue in Fig. 5 was made from hu-
man dialogue sequences, and the one in Fig. 6 was
made from POMDP.
766
With whom What day What time What Where Who made
1 family weekday around 6:00 pm grilled salmon home mother
2 family weekend around 7:00 pm potato and meat home mother
3 co-workers weekday at noon boiled seaweed lunch box myself
... ... ... ... ... ... ...
32 friend weekday at noon hamburger school cafeteria N/A
Table 3: Dialogue situations relating to everyday Japanese life
We qualitatively analyzed the dialogues of each
system and observed the following characteristics:
POMDP At a dialogue?s beginning, the system
greets several times and shifts to a different phase
in which listeners ask questions and self-disclose
to encourage speakers to reciprocate.
Rule-based The output of this system seems
very natural and easy to read. The dialogue-act
tags followed reasonable rules, making it easier
for the participants to create natural utterances
from them.
Human conversation The dialogues between
humans were obviously natural before they were
changed to tags from the natural-language ut-
terances. However, human dialogues have ran-
domness, which makes it difficult for the partic-
ipants to create natural-language utterances from
the tags. Hence, the evaluation score for this sys-
tem was lower than the ?Rule-based.?
HMM, EvenPOMDP Since these systems con-
tinually output the same action tags, their output
was very unnatural. For example, greetings never
stopped because GREETING is most frequently
followed by GREETING in the data. These sys-
tems have no mechanism to stop this loop.
POMDP successfully avoided such continua-
tion because its actions have more varied rewards.
For example, GREETING is repeated in Even-
POMDP because its smoothness reward is high;
however, in POMDP, although the smoothness re-
ward remains high, its user satisfaction reward is
not that high. This is because greetings appear
in all dialogues and their user satisfaction reward
converges to the average. Therefore, such actions
as greetings do not get repeated in POMDP. In
POMDP, some states have high user satisfaction
rewards, and the POMDP policy generated actions
to move to such states.
Random Since this system has more variety
of tags than HMM, its evaluation scores out-
performed HMM, but were outperformed by
POMDP, which learned statistically from the data.
Rule-based
6.07
Human 
dialogue
5.22
POMDP
?Proposed?
3.76
Random
2.67
HMM
1.17
Even
POMDP 
1.16
0
1
2
3
4
5
6
7
A
v
e
r
a
g
e
d
 
E
v
a
l
u
a
t
i
o
n
 
S
c
o
r
e
s
Figure 4: System scores. Except between
POMDP and EvenPOMDP, significant differences
exist among all systems (p<0.01).
From our qualitative analysis, we found that
POMDP can generate more satisfying sequences
than HMM/EvenPOMDP because it does not fall
into the loop of frequent dialogue-act tag se-
quences. This suggests the usefulness of incor-
porating two kinds of rewards into the policy and
that our approach for setting a reward is promis-
ing.
However, with the proposed POMDP, unnatural
sequences remain; for example, the system sud-
denly output THANKS, as shown in Fig. 6. The
number of states may have been too small. We
plan to investigate what caused this in the future.
In our qualitative analysis, we observed that
randomness in dialogues might hold a clue for
improving evaluation scores. Therefore, we
measured the perplexity of each system output
using dialogue-act trigrams and obtained 72.8
for ?Random,? 27.4 for ?Human dialogue,? 7.4
for ?POMDP,? 3.2 for ?HMM,? 2.5 for ?Even-
POMDP,? and 1.7 for ?Rule-based.?
The perplexity of the human dialogues is less
than that of the random system, but humans also
exhibit a certain degree of freedom. On the other
hand, POMDP?s perplexity is less than the human
dialogues; they still have some freedom, which
probably led to their reasonable evaluation scores.
Considering that HMM and EvenPOMDP, which
continually output the same dialogue acts, had low
767
Utterance Dialogue act
S: Hello. GREETING
L: Nice to meet you GREETING
S: I had dinner at home today. S-DISC (sub: fact)
Do you like grilled salmon? QUESTION, PREF
L: Yes, I think so. SYMPATHY
I sometimes want to have a
fancy meal.
S-DISC (sub: de-
sire)
S: Deluxe. REPEAT
Me too. SYMPATHY
L: Do you usually do your own
cooking?
QUESTION (sub:
habit)
S: No, I don?t. S-DISC, HABIT
I always buy my meals at the
convenience store.
S-DISC (sub:
habit)
L: I like the lunch boxes of conve-
nience stores
S-DISC (sub: pref
(positive))
Figure 5: Excerpt of listening-oriented dialogue
that participant imagined from tag sequences of
human conversations. Dialogue was translated
from Japanese by the authors.
Utterance Dialogue act
L: Nice to meet you. GREETING
Where and who did you have
dinner with today?
QUESTION (sub:
fact)
S: I had ?niku-jaga? (meat and
beef) with my family at home.
S-DISC (sub: fact)
L: Oh. ADMIRATION
S: I think it is normal to eat with
your family at home.
S-DISC (sub: pref
(neutral))
L: Thanks. THANKS
Do you have any brothers or sis-
ters?
QUESTION (sub:
fact)
Soon, my brother and his wife
will visit my home.
S-DISC (sub: plan)
S: I see. SYMPATHY
L: I want to use expensive meat in
my ?niku-jaga.?
S-DISC (sub: de-
sire)
Oh. ADMIRATION
Please give me your recipe. QUESTION (sub:
information)
S: My friends claim that my
?niku-jaga? is as good as a
restaurant?s.
INFORMATION
L: I?d love to try it S-DISC (sub: de-
sire)
Figure 6: Excerpt of a listening-oriented dialogue
made from tag sequences of POMDP
evaluation scores, we conclude that randomness is
necessary in non-task-oriented dialogues and that
some randomness can be included with our ap-
proach. We do not discuss ?Rule-based? here be-
cause its tag sequence was meant to have small
perplexity.
6 Conclusion and Future work
This paper investigated the possibility of automat-
ically building a dialogue control module from di-
alogue data to create automated listening agents.
With a POMDP as a learning framework,
a dialogue control module was learned from
the listening-oriented dialogues we collected and
compared with five different systems. Our
POMDP system showed higher performance in
subjective evaluations than other statistically mo-
tivated systems, such as an HMM-based one, that
work by selecting the most likely subsequent ac-
tion in the dialogue data. When we investigated
the output sequences of our POMDP system, the
system frequently chose to self-disclose and ques-
tion, which corresponds to human listener be-
havior, as revealed in the literature (Meguro et
al., 2009a). This suggests that learning dialogue
control by POMDPs is achievable for listening-
oriented dialogues.
The main contribution of this paper is that
we successfully showed that POMDPs can be
used to train dialogue control policies for less
task-oriented dialogue systems, such as listening
agents, where the user goals are not as clear as
task-oriented ones. We also revealed that the re-
ward function can be learned effectively by our
formulation that simultaneously maximizes user
satisfaction and smoothness. Finding an appro-
priate reward function is a real challenge for less
task-oriented dialogue systems; this work has pre-
sented the first workable solution.
Much work still remains. Even though we
conducted an evaluation experiment by simula-
tion (i.e, offline evaluation), human dialogues ob-
viously do not necessarily proceed as in simula-
tions. Therefore, we plan to evaluate our sys-
tem using online evaluation, which also forces us
to implement utterance understanding and gener-
ation modules. We also want to incorporate the
idea of topic shift into our policy learning because
we observed in our data that listeners frequently
change topics to keep speakers motivated. We are
also considering adapting the system behavior to
users. Specifically, we want to investigate dia-
logue control that adapts to the personality traits
of users because it has been found that the flow
of listening-oriented dialogues differs depending
on the personality traits of users (Meguro et al,
2009b). Finally, although we only dealt with text,
we also want to extend our approach to speech and
other modalities, such as gestures and facial ex-
pressions.
768
References
Bickmore, Timothy and Justine Cassell. 2001. Rela-
tional agents: a model and implementation of build-
ing user trust. In Proc. SIGCHI conference on
human factors in computing systems (CHI), pages
396?403.
Ferguson, George, James F. Allen, and Brad Miller.
1996. TRAINS-95: towards a mixed-initiativeplan-
ning assistant. In Proc. Third Artificial Intelligence
Planning Systems Conference (AIPS), pages 70?77.
Higashinaka, Ryuichiro, Kohji Dohsaka, and Hideki
Isozaki. 2008. Effects of self-disclosure and em-
pathy in human-computer dialogue. In Proc. IEEE
Workshop on Spoken Language Technology (SLT),
pages 108?112.
Higuchi, Shinsuke, Rafal Rzepka, and Kenji Araki.
2008. A casual conversation system using modal-
ity and word associations retrieved from the web.
In Proc. 2008 conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
382?390.
Hirshman, Lynette. 1989. Overview of the DARPA
speech and natural language workshop. In Proc.
DARPA Speech and Natural Language Workshop
1989, pages 1?2.
Isomura, Naoki, Fujio Toriumi, and Kenichiro Ishii.
2009. Evaluation method of non-task-oriented di-
alogue system using HMM. IEICE Transactions on
Information and Systems, J92-D(4):542?551.
Ivey, Allen E. and Mary Bradford Ivey. 2002. In-
tentional Interviewing and Counseling: Facilitat-
ing Client Development in a Multicultural Society.
Brooks/Cole Publishing Company.
Jurafsky, Dan, Elizabeth Shriberg, and Debra Bi-
asca, 1997. Switchboard SWBD-DAMSL Shallow-
Discourse-Function Annotation Coders Manual.
Kobayashi, Yuka, Daisuke Yamamoto, Toshiyuki
Koga, Sachie Yokoyama, and Miwako Doi. 2010.
Design targeting voice interface robot capable of
active listening. In Proc. 5th ACM/IEEE inter-
national conference on Human-robot interaction
(HRI), pages 161?162,
Maatman, R. M., Jonathan Gratch, and Stacy Marsella.
2005. Natural behavior of a listening agent. Lecture
Notes in Computer Science, 3661:25?36.
Meguro, Toyomi, Ryuichiro Higashinaka, Kohji
Dohsaka, Yasuhiro Minami, and Hideki Isozaki.
2009a. Analysis of listening-oriented dialogue for
building listening agents. In Proc. 10th Annual SIG-
DIAL Meeting on Discourse and Dialogue (SIG-
DIAL), pages 124?127.
Meguro, Toyomi, Ryuichiro Higashinaka, Kohji
Dohsaka, Yasuhiro Minami, and Hideki Isozaki.
2009b. Effects of personality traits on listening-
oriented dialogue. In Proc. International Workshop
on Spoken Dialogue Systems Technology (IWSDS),
pages 104?107.
Minami, Yasuhiro, Akira Mori, Toyomi Meguro,
Ryuichiro Higashinaka, Kohji Dohsaka, and Eisaku
Maeda. 2009. Dialogue control algorithm for
ambient intelligence based on partially observable
markov decision processes. In Proc. International
Workshop on Spoken Dialogue Systems Technology
(IWSDS), pages 254?263.
Nakano, Mikio, Noboru Miyazaki, Jun ichi Hirasawa,
Kohji Dohsaka, and Takeshi Kawabata. 1999. Un-
derstanding unsegmented user utterances in real-
time spoken dialogue systems. In Proc. 37th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 200?207.
Pineau, Joelle., Geoff. Gordon, and Sebastian Thrun.
2003. Point-based value iteration: An anytime al-
gorithm for POMDPs. In Proc. International Joint
Conference on Artificial Intelligence (IJCAI), pages
1025?1032.
Shitaoka, Kazuya, Ryoko Tokuhisa, Takayoshi
Yoshimura, Hiroyuki Hoshino, and Narimasa
Watanabe. 2010. Active listening system for dia-
logue robot. In JSAI SIG-SLUD Technical Report,
volume 58, pages 61?66. (in Japanese).
Walker, Marilyn, Alex Rudnicky, John Aberdeen, Eliz-
abeth Owen Bratt, Rashmi Prasad, Salim Roukos,
Greg S, and Seneff Dave Stallard. 2002. DARPA
communicator evaluation: progress from 2000 to
2001. In Proc. International Conference on Spoken
Language Processing (ICSLP), pages 273?276.
Wallace, Richard S. 2004. The Anatomy of A.L.I.C.E.
A.L.I.C.E. Artificial Intelligence Foundation, Inc.
Williams, Jason D. and Steve Young. 2007. Par-
tially observable markov decision processes for spo-
ken dialog systems. Computer Speech & Language,
21(2):393?422.
Yokoyama, Sachie, Daisuke Yamamoto, Yuka
Kobayashi, and Miwako Doi. 2010. Development
of dialogue interface for elderly people ?switching
the topic presenting mode and the attentive listening
mode to keep chatting?. In IPSJ SIG Technical
Report, volume 2010-SLP-80, pages 1?6. (in
Japanese).
769
Coling 2010: Poster Volume, pages 400?408,
Beijing, August 2010
Learning to Model Domain-Specific Utterance Sequences for Extractive
Summarization of Contact Center Dialogues
Ryuichiro Higashinaka?, Yasuhiro Minami?, Hitoshi Nishikawa?,
Kohji Dohsaka?, Toyomi Meguro?, Satoshi Takahashi?, Genichiro Kikui?
? NTT Cyber Space Laboratories, NTT Corporation
? NTT Communication Science Laboratories, NTT Corporation
higashinaka.ryuichiro@lab.ntt.co.jp, minami@cslab.kecl.ntt.co.jp
nishikawa.hitoshi@lab.ntt.co.jp, {dohsaka,meguro}@cslab.kecl.ntt.co.jp
{takahashi.satoshi,kikui.genichiro}@lab.ntt.co.jp
Abstract
This paper proposes a novel extractive
summarization method for contact cen-
ter dialogues. We use a particular
type of hidden Markov model (HMM)
called Class Speaker HMM (CSHMM),
which processes operator/caller utterance
sequences of multiple domains simulta-
neously to model domain-specific utter-
ance sequences and common (domain-
wide) sequences at the same time. We
applied the CSHMM to call summariza-
tion of transcripts in six different con-
tact center domains and found that our
method significantly outperforms compet-
itive baselines based on the maximum
coverage of important words using integer
linear programming.
1 Introduction
In modern business, contact centers are becom-
ing more and more important for improving cus-
tomer satisfaction. Such contact centers typically
have quality analysts who mine calls to gain in-
sight into how to improve business productivity
(Takeuchi et al, 2007; Subramaniam et al, 2009).
To enable them to handle the massive number of
calls, automatic summarization has been utilized
and shown to successfully reduce costs (Byrd et
al., 2008). However, one of the problems in cur-
rent call summarization is that a domain ontology
is required for understanding operator/caller utter-
ances, which makes it difficult to port one summa-
rization system from domain to domain.
This paper describes a novel automatic sum-
marization method for contact center dialogues
without the costly process of creating domain on-
tologies. More specifically, given contact center
dialogues categorized into multiple domains, we
create a particular type of hidden Markov model
(HMM) called Class Speaker HMM (CSHMM)
to model operator/caller utterance sequences. The
CSHMM learns to distinguish sequences of indi-
vidual domains and common sequences in all do-
mains at the same time. This approach makes it
possible to accurately distinguish utterances spe-
cific to a certain domain and thereby has the po-
tential to generate accurate extractive summaries.
In Section 2, we review recent work on auto-
matic summarization, including its application to
contact center dialogues. In Section 3, we de-
scribe the CSHMM. In Section 4, we describe
our automatic summarization method in detail. In
Section 5, we describe the experiment we per-
formed to verify our method and present the re-
sults. In Section 6, we summarize and mention
future work.
2 Related Work
There is an abundance of research in automatic
summarization. It has been successfully applied to
single documents (Mani, 2001) as well as to mul-
tiple documents (Radev et al, 2004), and various
summarization methods, such as the conventional
LEAD method, machine-learning based sentence
selection (Kupiec et al, 1995; Osborne, 2002),
and integer linear programming (ILP) based sen-
tence extraction (Gillick and Favre, 2009), have
been proposed. Recent years have seen work on
summarizing broadcast news speech (Hori and
Furui, 2003), multi-party meetings (Murray et al,
2005), and contact center dialogues (Byrd et al,
2008). However, despite the large amount of pre-
vious work, little work has tackled the automatic
summarization of multi-domain data.
400
In the past few decades, contact center dia-
logues have been an active research focus (Gorin
et al, 1997; Chu-Carroll and Carpenter, 1999).
Initially, the primary aim of such research was
to transfer calls from answering agents to oper-
ators as quickly as possible in the case of prob-
lematic situations. However, real-time processing
of calls requires a tremendous engineering effort,
especially when customer satisfaction is at stake,
which led to recent work on the offline process-
ing of calls, such as call mining (Takeuchi et al,
2007) and call summarization (Byrd et al, 2008).
The work most related to ours is (Byrd et al,
2008), which maps operator/caller utterances to
an ontology in the automotive domain by using
support vector machines (SVMs) and creates a
structured summary by heuristic rules that assign
the mapped utterances to appropriate summary
sections. Our work shares the same motivation
as theirs in that we want to make it easier for
quality analysts to analyze the massive number of
calls. However, we tackle the problem differently
in that we propose a newmodeling of utterance se-
quences for extractive summarization that makes
it unnecessary to create heuristics rules by hand
and facilitates the porting of a summarization sys-
tem.
HMMs have been successfully applied to au-
tomatic summarization (Barzilay and Lee, 2004).
In their work, an HMM was used to model the
transition of content topics. The Viterbi decod-
ing (Rabiner, 1990) was performed to find con-
tent topics that should be incorporated into a sum-
mary. Their approach is similar to ours in that
HMMs are utilized to model topic sequences, but
they did not use data of multiple domains in creat-
ing their model. In addition, their method requires
training data (original articles with their reference
summaries) in order to find which content top-
ics should be included in a summary, whereas our
method requires only the raw sequences with their
domain labels.
3 Class Speaker HMM
A Class Speaker HMM (CSHMM) is an exten-
sion of Speaker HMM (SHMM), which has been
utilized to model two-party conversations (Me-
guro et al, 2009). In an SHMM, there are two
states, and each state emits utterances of one of
the two conversational participants. The states are
1:speaker1 2:speaker2
Speaker HMM for Class 1
3:speaker1 4:speaker2
Speaker HMM for Class 2
Figure 1: Topology of an ergodic CSHMM. Num-
bers before ?speaker1? and ?speaker2? denote state
IDs.
connected ergodically and the emission/transition
probabilities are learned from training data by
using the EM-algorithm. Although Meguro et
al., (2009) used SHMMs to analyze the flow of
listening-oriented dialogue, we extend their idea
to make it applicable to classification tasks, such
as dialogue segmentation.
A CSHMM is simply a concatenation of
SHMMs, each of which is trained by using ut-
terance sequences of a particular dialogue class.
After such SHMMs are concatenated, the Viterbi
algorithm is used to decode an input utterance
sequence into class labels by estimating from
which class each utterance has most likely to have
been generated. Figure 1 illustrates the basic
topology of a CSHMM where two SHMMs are
concatenated ergodically. When the most likely
state sequence for an input utterance sequence is
<1,3,4,2>, we can convert these state IDs into
their corresponding classes; that is, <1,2,2,1>,
which becomes the result of utterance classifica-
tion.
We have conceived three variations of CSHMM
as we describe below. They differ in how we treat
utterance sequences that appear commonly in all
classes and how we train the transition probabili-
ties between independently trained SHMMs.
3.1 Ergodic CSHMM
The most basic CSHMM is the ergodic CSHMM,
which is a simple concatenation of SHMMs in
an ergodic manner as shown in Fig. 1. For K
classes, K SHMMs are combined with the initial
and transition probabilities all set to equal. In this
CSHMM, the assignment of class labels solely de-
pends on the output distributions of each class.
3.2 Ergodic CSHMM with Common States
This type of CSHMM is the same as the ergodic
CSHMM except that it additionally has a SHMM
trained from all dialogues of all classes. There-
401
3:speaker1 4:speaker2
1:speaker1 2:speaker2
5:speaker1 6:speaker2
Speaker HMM for Class 1
Speaker HMM for Class 2
Speaker HMM for All Classes (Class 0)
Figure 2: CSHMM with common states.
Copy
Class
1
M1
M1M0
Retrain
Train
Class
k
M0
Mk
Mk
Retrain
Train
Class
K
M0
MK
MK
Retrain
Train
All 
Classes
M0
Train
+
M0
M1 Mk MK
AVG
Concatenate
M1+0 Mk+0 MK+0
M1M0 M0 Mk
M0 MK
M1+0 Mk+0 MK+0
Step 1
Step 2
Step 3
Step 2?
END
Mconcat
If the fitting has 
converged for 
all Mk+0
Split Mconcat into 
pairs again and 
retrain Mk+0
M1?MK become 
less likely to
output common 
sequences
Transition probabilities
of M0 are redistributed
between M0 and Mk
Figure 3: Three steps to create a CSHMM using
concatenated training.
fore, for K classes, this CSHMM has K + 1
SHMMs. Figure 2 shows the model topology.
This newly added SHMM works in a manner sim-
ilar to the background model (Reynolds et al,
2000) representing sequences that are common
to all classes. By having these common states,
common utterance sequences can be classified as
?common?, making it possible to avoid forcefully
classifying common utterance sequences into one
of the given classes.
Detecting common sequences is especially
helpfulwhen several classes overlap in nature. For
example, most dialogues commonly start and end
with greetings, and many calls at contact centers
commonly contain exchanges in which the opera-
tor requests personal information about the caller
for confirmation. Regarding the model topology
in Fig. 2, if the most likely state sequence by
the Viterbi decoding is <1,4,5,6,3,2>, we obtain
a class label sequence <1,2,0,0,2,1> where the
third and fourth utterances are classified as ?zero?,
meaning that they do not belong to any class.
3.3 CSHMM using Concatenated Training
The CSHMMs presented so far have two prob-
lems: one is that the order of utterances of differ-
ent classes cannot be taken into account because
of the equal transition probabilities. As a result,
the very merit of HMMs, their ability to model
time series data, is lost. The other is that the out-
put distributions of common states may be overly
broad because they are the averaged distributions
over all classes; that is, the best path determined
by the Viterbi decoding may not go through the
common states at all.
Our solution to these problems is to apply con-
catenated training (Lee, 1989), which has been
successfully used in speech recognition to model
phoneme sequences in an unsupervised manner.
The procedure for concatenated training is illus-
trated in Fig. 3 and has three steps.
step 1 Let Mk (Mk ? M, 1 ? k ? K) be the
SHMM trained using dialogues Dk where
Dk = {?dj|c(dj) = k}, and M0 be the
SHMM trained using all dialogues; i.e., D.
Here, K means the total number of classes
and c(dj) the class assigned to a dialogue dj .
step 2 Connect each Mk ? M with a copy of
M0 using equal initial and transition proba-
bilities (we call this connected model Mk+0)
and retrain Mk+0 with ?dj ? Dk where
c(dj) = k.
step 3 Merge all models Mk+0 (1 ? k ? K) to
produce one concatenated HMM (Mconcat).
Here, the output probabilities of the copies
of M0 are averaged over K when all models
are merged to create a combined model. If
the fitting of all Mk+0 models has converged
against the training data, exit this procedure;
otherwise, go to step 2 by connecting a copy
of M0 and Mk for all k. Here, the transi-
tion probabilities from M0 to Ml(l 6= k) are
summed and equally distributed between the
copied M0?s self-loop and transitions to the
states in Mk.
In concatenated training, the transition and output
probabilities can be optimized between M0 and
402
Contact
Center
Dialogues
Domain 1
Domain K
?
HMM for Domain 1 HMM for Domain K
HMM for All Domains
Model topic label 
sequences
INPUT: A dialogue in Domain k
Topic Model
Topic label sequence
L
S
A
/
L
D
A
A
s
s
i
g
n
 
t
o
p
i
c
l
a
b
e
l
s
Domain label sequence OUTPUT: summary
Viterbi decoding
Assign
topic labels
Select utterances labeled with Domain k
Class Speaker
HMM
?..
Utterance sequence
Feature sequence
Extract content words
as utterance features
Figure 4: Overview of our summarization
method.
Mk, meaning that the output probabilities of utter-
ance sequences that are common and also found
in Mk can be moved from Mk to M0. This makes
the distribution of Mk sharp (not broad/uniform),
making it likely to output only the utterances rep-
resentative of a class k. As regards M0, its distri-
bution of output probabilities can also be sharp-
ened for utterances that occur commonly in all
classes. This sharpening of distributions is likely
to be helpful for class discrimination.
4 Summarization Method
We apply CSHMMs to extractive summarization
of contact center dialogues because such dia-
logues are two-party, can be categorized into mul-
tiple classes by their call domains (e.g., inquiry
types), and are likely contain many overlapping
exchanges between an operator and a caller across
domains, such as greetings, the confirmation of
personal information, and other cliches in busi-
ness (e.g., name exchanges, thanking/apologizing
phrases, etc.), making them the ideal target for
CSHMMs.
In our method, summarization is performed by
decoding a sequence of utterances of a domain
DMk into domain labels and selecting those ut-
terances that have domain labels DMk. This
makes it possible to extract utterances that are
characteristic of DMk in relation to other do-
mains. Our assumption is that extracting charac-
teristic sequences of a given domain provides a
good summary for that domain because such se-
quences should contain important information ne-
cessitated by the domain.
Figure 4 outlines our extractive summarization
process. The process consists of a training phase
and a decoding phase as described below.
Training phase: Let D (d1 . . . dN ) be the entire
set of contact center dialogues, DMk (DMk ?
DM, 1 ? k ? K) the domain assigned to do-
main k, and Udi,1 . . .Udi,H the utterances in di.
Here, H is the number of utterances in di. From
D, we create two models: a topic model (TM )
and a CSHMM.
The topic model is used to assign a single topic
to each utterance so as to facilitate the training
of the CSHMM by reducing the dimensions of
the feature space. The same approach has been
taken in (Barzilay and Lee, 2004). The topic
model can be created by such techniques as prob-
abilistic latent semantic analysis (PLSA) (S?ingliar
and Hauskrecht, 2006) and latent Dirichlet alo-
cation (LDA) (Tam and Schultz, 2005). PLSA
models the latent topics of the documents and its
Baysian extension is LDA, which also models the
co-occurrence of topics using the Dirichlet prior.
We first derive features Fd1 . . . FdN for the dia-logues. Here, we assume a bag-of-words repre-
sentation for the features; therefore, Fdi is repre-sented as {< w1, c1 > . . . < wV , cV >}, where
V means the total number of content words in the
vocabulary and < wi, ci > denotes that a content
word wi appears ci times in a dialogue. Note that
we derive the features for dialogues, not for utter-
ances, because utterances in dialogue can be very
short, often consisting of only one or two words
and thus making it hard to calculate the word co-
occurrence required for creating a topic model.
From the features, we build a topic model that in-
cludes P(z|w), where w is a word and z is a topic.
Using the topic model, we can assign a single
topic label to every utterance in D by finding its
likely topic; i.e., argmax
z
?
w?words(Udi) P(z|w).
After labeling all utterances in D with topic la-
bels, we train a CSHMM that learns characteristic
topic label sequences in each domain as well as
common topic label sequences across domains.
Decoding phase: Let dj be the input dialogue,
DM(dj) (? DM ) the table for obtaining the do-
main label of dj , and Udj ,1 . . .Udj ,Hdj the utter-ances in dj, where Hdj is the number of the utter-ances. We use TM to map the utterances to topic
403
Domain # Tasks Sentences Characters
FIN 15 8.93 289.93
ISP 15 7.20 259.53
LGU 20 9.85 328.55
MO 15 10.07 326.20
PC 15 9.40 354.07
TEL 18 8.44 322.22
ALL 98 9.01 314.46
Table 1: Scenario statistics: the number of tasks
and averaged number of sentences/characters in a
task scenario in the six domains.
labels Tdj ,1 . . .Tdj ,Hdj and convert them into do-main label sequences DMdj ,1 . . .DMdj ,Hdj us-ing the trained CSHMM by the Viterbi decoding.
Then, we select Udj ,h (1 ? h ? Hdj ) whose cor-responding domain labelDMdj ,h equalsDM(dj)and output the selected utterances in the order of
appearance in the original dialogue as a summary.
5 Experiment
We performed an experiment to verify our sum-
marization method. We first collected simulated
contact center dialogues using human subjects.
Then, we compared our method with baseline sys-
tems. Finally, we analyzed the created summaries
to investigate what had been learned by our CSH-
MMs.
5.1 Dialogue Data
Since we do not have access to actual contact cen-
ter data, we recruited human subjects to collect
simulated contact center dialogues. A total of 90
participants (49 males and 41 females) took the
roles of operator or a caller and talked over tele-
phones in separate rooms. The callers were given
realistic scenarios that included their motivation
for a call as well as detailed instructions about
what to ask. The operators, who had experience
of working at contact centers, were given manuals
containing the knowledge of the domain and ex-
plaining how to answer questions in specific sce-
narios.
The dialogues took place in six different do-
mains: Finance (FIN), Internet Service Provider
(ISP), Local Government Unit (LGU), Mail Or-
der (MO), PC support (PC), and Telecommuni-
cation (TEL). In each domain, there were 15?20
tasks. Table 1 shows the statistics of the task sce-
narios used by the callers. We cannot describe the
details of each domain for lack of space, but ex-
MO task No. 3: It is becoming a good season for the
Japanese Nabe (pan) cuisine. You own a Nabe restau-
rant and it is going well. When you were searching on
the Internet, thinking of creating a new dish, you saw
that drop-shipped Shimonoseki puffer fish was on sale.
Since you thought the puffer fish cuisine would become
hot in the coming season, you decided to order it as a
trial. . . . You ordered a puffer fish set on the Internet,
but you have not received the confirmation email that
you were supposed to receive. . . . You decided to call
the contact center to make an inquiry, ask them whether
the order has been successful, and request them to send
you the confirmation email.
Figure 5: Task scenario in the MO domain. The
scenario was originally in Japanese and was trans-
lated by the authors.
amples of the tasks for FIN are inquiries about in-
surance, notifications of the loss of credit cards,
and applications for finance loans, and those for
ISP are inquiries about fees for Internet access, re-
quests to forward emails, and reissuance of pass-
words. Figure 5 shows one of the task scenarios
in the MO domain.
We collected data on two separate occasions us-
ing identical scenarios but different participants,
which gave us two sets of dialogue data. We used
the former for training our summarization sys-
tem and the latter for testing. We only use the
transcriptions in this paper so as to avoid partic-
ular problems of speech. All dialogues were in
Japanese. Tables 2 and 3 show the statistics of the
training data and the test data, respectively. As
can be seen from the tables, each dialogue is quite
long, which attests to the complexity of the tasks.
5.2 Training our Summarization System
For training our system, we first created a topic
model using LDA.We performed a morphological
analysis using ChaSen1 to extract content words
from each dialogue and made its bag-of-words
features. We defined content words as nouns,
verbs, adjectives, unknown words, and interjec-
tions (e.g., ?yes?, ?no?, ?thank you?, and ?sorry?).
We included interjections because they occur very
frequently in dialogues and often possess impor-
tant content, such as agreement and refusal, in
transactional communication. We use this defini-
tion of content words throughout the paper.
Then, using an LDA software package2, we
built a topic model. We tentatively set the number
1http://chasen-legacy.sourceforge.jp/
2http://chasen.org/?daiti-m/dist/lda/
404
Utterances/Dial. Characters/Utt.
Domain # dial. OPE CAL Both OPE CAL Both
FIN 59 75.73 72.69 148.42 17.44 7.54 12.59
ISP 64 55.09 53.17 108.27 20.11 8.03 14.18
LGU 76 58.28 50.55 108.83 12.83 8.55 10.84
MO 70 66.39 58.74 125.13 15.09 7.43 11.49
PC 56 89.34 77.80 167.14 15.48 6.53 11.31
TEL 66 75.58 63.97 139.55 12.74 8.24 10.67
ALL 391 69.21 61.96 131.17 15.40 7.69 11.76
Table 2: Training data statistics: Averaged num-
ber of utterances per dialogue and characters per
utterance for each domain. OPE and CAL denote
operator and caller, respectively. See Section 5.1
for the full domain names.
Utterances/Dial. Characters/Utt.
Domain # dial. OPE CAL Both OPE CAL Both
FIN 60 73.97 61.05 135.02 14.53 7.50 11.35
ISP 59 76.08 61.24 137.32 15.43 6.94 11.65
LGU 56 66.55 51.59 118.14 14.54 7.53 11.48
MO 47 75.53 64.87 140.40 10.53 6.79 8.80
PC 44 124.02 94.16 218.18 14.23 7.79 11.45
TEL 41 93.71 68.54 162.24 13.94 7.85 11.37
ALL 307 83.07 65.69 148.76 13.98 7.41 11.08
Table 3: Test data statistics.
of topics to 100. Using this topic model, we la-
beled all utterances in the training data using these
100 topic labels.
We trained seven different CSHMMs in all: one
ergodic CSHMM (ergodic0), three variants of er-
godic CSHMMs with common states (ergodic1,
ergodic2, ergodic3), and three variants of CSH-
MMs with concatenated training (concat1, con-
cat2, concat3). The difference within the variants
is in the number of common states. The numbers
0?3 after ?ergodic? and ?concat? indicate the num-
ber of SHMMs containing common states. For
example, ergodic3 has nine SHMMs (six SHMMs
for the six domains plus three SHMMs contain-
ing common states). Since more states would
enable more minute modeling of sequences, we
made such variants in the hope that common se-
quences could be more accurately modeled. We
also wanted to examine the possibility of creat-
ing sharp output distributions in common states
without the concatenated training by such minute
modeling. These seven CSHMMs make seven dif-
ferent summarization systems.
5.3 Baselines
Baseline-1: BL-TF We prepared two baseline
systems for comparison. One is a simple sum-
marizer based on the maximum coverage of high
term frequency (TF) content words. We call
this baseline BL-TF. This baseline summarizes a
dialogue by maximizing the following objective
function:
max
?
zi?Z
weight(wi) ? zi
where ?weight? returns the importance of a con-
tent word wi and zi is a binary value indicating
whether to include wi in the summary. Here,
?weight? returns the count of wi in a given dia-
logue. The maximization is done using ILP (we
used an off-the-shelf solver lp solve3) with the
following three constraints:
xi, zi ? {0, 1}
?
xi?X
lixi ? K
?
i
mijxi ? zj (?zj ? Z)
where xi is a binary value that indicates whether
to include the i-th utterance in the summary, li is
the length of the i-th utterance,K is the maximum
number of characters to include in a summary, and
mij is a binary value that indicates whether wi is
included in the j-th utterance. The last constraint
means that if a certain utterance is included in the
summary, all words in that utterance have to be
included in the summary.
Baseline-2: BL-DD Although BL-TF should be
a very competitive baseline because it uses the
state-of-the-art formulation as noted in (Gillick
and Favre, 2009), having only this baseline is
rather unfair because it does not make use of the
training data, whereas our proposed method uses
them. Therefore, we made another baseline that
learns domain-specific dictionaries (DDs) from
the training data and incorporates them into the
weights of content words of the objective function
of BL-TF. We call this baseline BL-DD. In this
baseline, the weight of a content word wi in a do-
main DMk is
weight(wi,DMk) =
log(P(wi|DMk))
log(P(wi|DM\DMk))
3http://lpsolve.sourceforge.net/5.5/
405
Metric ergodic0 ergodic1 ergodic2 ergodic3 concat1 concat2 concat3
PROPOSED
F 0.177 0.177 0.177 0.177 0.187?e0e1e2e3 0.198?+e0e1e2e3c1 0.199?+e0e1e2e3c1precision 0.145 0.145 0.145 0.145 0.161? 0.191?+ 0.195?+
recall 0.294 0.294 0.294 0.294 0.280? 0.259?+ 0.259?+
(Same-length) BL-TF
F 0.171 0.171 0.171 0.171 0.168 0.164 0.163
precision 0.132 0.132 0.132 0.132 0.135 0.140 0.140
recall 0.294 0.294 0.294 0.294 0.270 0.241 0.240
(Same-length) BL-DD
F 0.189 0.189 0.189 0.189 0.189 0.187 0.187
precision 0.155 0.155 0.155 0.155 0.162 0.170 0.172
recall 0.287 0.287 0.287 0.287 0.273 0.250 0.248
Compression Rate 0.42 0.42 0.42 0.42 0.37 0.30 0.30
Table 4: F-measure, precision, and recall averaged over all 307 dialogues (cf. Table 3) in the test
set for the proposed methods and baselines BL-TF and BL-DD configured to output the same-length
summaries as the proposed systems. The averaged compression rate for each proposed system is shown
at the bottom. The columns (ergodic0?concat3) indicate our methods as well as the character lengths
used by the baselines. Asterisks, ?+?, e0?e3, and c1?c3 indicate our systems? statistical significance by
the Wilcoxon signed-rank test (p<0.01) over BL-TF, BL-DD, ergodic0?3, and concat1?3, respectively.
Statistical tests for the precision and recall were only performed between the proposed systems and
their same-length baseline counterparts. Bold font indicates the best score in each row.
where P(wi|DMk) denotes the occurrence prob-
ability of wi in the dialogues of DMk , and
P(wi|DM\DMk) the occurrence probability of
wi in all domains except for DMk. This log like-
lihood ratio estimates how much a word is char-
acteristic of a given domain. Incorporating such
weights would make a very competitive baseline.
5.4 Evaluation Procedure
We made our seven proposed systems and two
baselines (BL-TF and BL-DD) output extractive
summaries for the test data. Since one of the
shortcomings of our proposedmethod is its inabil-
ity to set the compression rate, we made our sys-
tems output summaries first and made the baseline
systems output their summaries within the charac-
ter lengths of our systems? summaries.
We used scenario texts (See Fig. 5) as reference
data; that is, a dialogue dealing with a certain task
is evaluated using the scenario text for that task.
As an evaluation criterion, we used the F-measure
(F1) to evaluate the retrieval accuracy on the ba-
sis of the recall and precision of retrieved content
words. We used the scenarios as references be-
cause they contain the basic content exchanged
between an operator and a caller, the retrieval ac-
curacy of which should be important for quality
analysts.
We could have used ROUGE (Lin and Hovy,
2003), but we did not because ROUGE does not
correlate well with human judgments in conversa-
tional data (Liu and Liu, 2008). Another benefit of
using the F-measure is that summaries of varying
lengths can be compared.
5.5 Results
Table 4 shows the evaluation results for the pro-
posed systems and the baselines. It can be seen
that concat3 shows the best performance in F-
measure among all systems, having a statistically
better performance over all systems except for
concat2. The CSHMMs with concatenated train-
ing were all better than ergodic0?3. Here, the per-
formance (and output) of ergodic0?3 was exactly
the same. This happened because of the broad dis-
tributions in their common states; no paths went
through the common states and all paths went
through the SHMMs of the six domains instead.
The evaluation results in Table 4 may be rather
in favor of our systems because the summarization
lengths were set by the proposed systems. There-
fore, we performed another experiment to inves-
tigate the performance of the baselines with vary-
ing compression rates and compared their perfor-
mance with the proposed systems in F-measure.
We found that the best performance was achieved
by BL-DD when the compression rate was 0.4
with the F-measure of 0.191, which concat3 sig-
nificantly outperformed by the Wilcoxon signed-
rank test (p<0.01). Note that the performance
shown in Table 4 may seem low. However, we
found that the maximum recall is 0.355 (cal-
406
CAL1 When I order a product from you, I get a confir-
mation email
CAL2 Puffer fish
CAL3 Sets I have ordered, but I haven?t received
the confirmation email
OPE1 Order
OPE2 I will make a confirmation whether you have
ordered
OPE3 Ten sets of Shimonoseki puffer fish by drop-
ship
OPE4 ?Yoriai? (name of the product)
OPE5 Two kilos of bony parts of tiger puffer fish
OPE6 Baked fins for fin sake
OPE7 600 milliliter of puffer fish soy sauce
OPE8 And, grated radish and red pepper
OPE9 Your desired delivery date is the 13th of Febru-
ary
CAL4 Yes, all in small cases
CAL5 This is q in alphabet right?
CAL6 Hyphen g
CAL7 You mean that the order was successful
OPE10 Yes, it was Nomura at JDS call center
Figure 6: Example output of concat3 for MO task
No. 3 (cf Fig. 5). The utterances were translated
by the authors. The compression rate for this dia-
logue was 0.24.
culated by using summaries with no compres-
sion). This means that the maximum F-measure
we could attain is lower than 0.524 (when the pre-
cision is ideal with 1). This is because of the dif-
ferences between the scenarios and the actual di-
alogues. We want to pursue ways to improve our
evaluation methodology in the future.
Despite such issues in evaluation, from the re-
sults, we conclude that our extractive summa-
rization method is effective and that having the
common states and training CSHMMs with con-
catenated training are useful in modeling domain-
specific sequences of contact center dialogues.
5.6 Example of System Output
Figure 6 shows an example output of concat3 for
the scenario MO task No. 3 (cf. Fig. 5). Bold font
indicates utterances that were NOT included in the
summary of concat3?s same-length-BF-DD coun-
terpart. It is clear that sequences related to the
MO domain were successfully extracted. When
we look at the summary of BF-DD, we see such
utterances as ?Can I have your address from the
postcode? and ?Finally, can I have your email ad-
dress?, which are obvious cliches in contact center
dialogues. This indicates the usefulness of com-
mon states for ignoring such common exchanges.
6 Summary and Future Work
This paper proposed a novel extractive sum-
marization method for contact center dialogues.
We devised a particular type of HMM called
CSHMM, which processes operator/caller utter-
ance sequences of multiple domains simulta-
neously to model domain-specific utterance se-
quences and common sequences at the same time.
We trained a CSHMM using the transcripts of
simulated contact center dialogues and verified its
effectiveness for the summarization of calls.
There still remain several limitations in our ap-
proach. One is its inability to change the com-
pression rate, which we aim to solve in the next
step using the forward-backward algorithm (Ra-
biner and Juang, 1986). This algorithm can cal-
culate the posterior probability of each state at
each time frame given an input dialogue sequence,
enabling us to extract top-N domain-specific se-
quences. We also need to find the appropriate
topic number for the topic model. In our imple-
mentation, we used a tentative value of 100, which
may not be appropriate. In addition, we believe
the topic model and the CSHMM can be unified
because these models are fundamentally similar,
especially when LDA is employed. Model topolo-
gies may also have to be reconsidered. In our
CSHMM with concatenated training, the states in
domain-specific SHMMs are only connected to
the common states, which may be inappropriate
because there could be a case where a domain
changes from one to another without having a
common sequence. ApplyingCSHMMs to speech
and other NLP tasks is another challenge. As a
near-term goal, we aim to apply our method to the
summarization of meetings, where we will need to
extend our CSHMMs to deal with more than two
participants. Finally, we also want to build a con-
tact center dialogue agent by extending the CSH-
MMs to partially observableMarkov decision pro-
cesses (POMDPs) (Williams and Young, 2007) by
following the recent work on building POMDPs
from dialogue data in the dynamic Bayesian net-
work (DBN) framework (Minami et al, 2009).
Acknowledgments
We thank the members of the Spoken Dialog
System Group, especially Noboru Miyazaki and
Satoshi Kobashikawa, for their effort in dialogue
data collection.
407
References
Barzilay, Regina and Lillian Lee. 2004. Catching the drift:
Probabilistic content models, with applications to gener-
ation and summarization. In Proceedings of the Human
Language Technology Conference of the North American
Chapter of the Association for Computational Linguistics
(HLT-NAACL), pages 113?120.
Byrd, Roy J., Mary S. Neff, Wilfried Teiken, Youngja
Park, Keh-Shin F. Cheng, Stephen C. Gates, and Karthik
Visweswariah. 2008. Semi-automated logging of contact
center telephone calls. In Proceeding of the 17th ACM
conference on Information and knowledge management
(CIKM), pages 133?142.
Chu-Carroll, Jennifer and Bob Carpenter. 1999. Vector-
based natural language call routing. Computational Lin-
guistics, 25(3):361?388.
Gillick, Dan and Benoit Favre. 2009. A scalable global
model for summarization. In Proceedings of the Work-
shop on Integer Linear Programming for Natural Lan-
guage Processing, pages 10?18.
Gorin, Allen L., Giuseppe Riccardi, and Jerry H. Wright.
1997. How may I help you? Speech Communication,
23(1-2):113?127.
Hori, Chiori and Sadaoki Furui. 2003. A new approach to
automatic speech summarization. IEEE Transactions on
Multimedia, 5(3):368?378.
Kupiec, Julian, Jan Pedersen, and Francine Chen. 1995. A
trainable document summarizer. In Proceedings of the
18th annual international ACM SIGIR conference on Re-
search and development in information retrieval (SIGIR),
pages 68?73.
Lee, Kai-Fu. 1989. Automatic speech recognition: the de-
velopment of the SPHINX system. Kluwer Academic Pub-
lishers.
Lin, Chin-Yew and Eduard Hovy. 2003. Automatic evalua-
tion of summaries using n-gram co-occurrence statistics.
In Proceedings of the 2003Conference of the North Amer-
ican Chapter of the Association for Computational Lin-
guistics on Human Language Technology (NAACL-HLT),
pages 71?78.
Liu, Feifan and Yang Liu. 2008. Correlation between
ROUGE and human evaluation of extractive meeting sum-
maries. In Proceedings of the 46th Annual Meeting of
the Association for Computational Linguistics on Human
Language Technologies (HLT), pages 201?204.
Mani, Inderjeet. 2001. Automatic summarization. John
Benjamins Publishing Company.
Meguro, Toyomi, Ryuichiro Higashinaka, Kohji Dohsaka,
Yasuhiro Minami, and Hideki Isozaki. 2009. Analysis of
listening-oriented dialogue for building listening agents.
In Proceedings of the SIGDIAL 2009 conference, pages
124?127.
Minami, Yasuhiro, Akira Mori, Toyomi Meguro, Ryuichiro
Higashinaka, Kohji Dohsaka, and Eisaku Maeda. 2009.
Dialogue control algorithm for ambient intelligence based
on partially observable Markov decision processes. In
Proceedings of the 1st international workshop on spoken
dialogue systems technology (IWSDS), pages 254?263.
Murray, Gabriel, Steve Renals, and Jean Carletta. 2005. Ex-
tractive summarization of meeting recordings. In Pro-
ceedings of the 9th European Conference on Speech
Communication and Technology (EUROSPEECH), pages
593?596.
Osborne, Miles. 2002. Using maximum entropy for sen-
tence extraction. In Proceedings of the ACL-02 Workshop
on Automatic Summarization, pages 1?8.
Rabiner, Lawrence R. and Biing-Hwang Juang. 1986. An
introduction to hiddenMarkov models. IEEE ASSP Mag-
azine, 3(1):4?16.
Rabiner, Lawrence R. 1990. A tutorial on hidden Markov
models and selected applications in speech recognition.
Readings in speech recognition, 53(3):267?296.
Radev, Dragomir R., Hongyan Jing, Ma?gorzata Stys?, and
Daniel Tam. 2004. Centroid-based summarization of
multiple documents. Information Processing & Manage-
ment, 40(6):919?938.
Reynolds, Douglas A., Thomas F. Quatieri, and Robert B.
Dunn. 2000. Speaker verification using adaptedGaussian
mixture models. Digital Signal Processing, 10(1-3):19 ?
41.
Subramaniam, L. Venkata, Tanveer A. Faruquie, Shajith Ik-
bal, Shantanu Godbole, and Mukesh K. Mohania. 2009.
Business intelligence from voice of customer. In Pro-
ceedings of the 2009 IEEE International Conference on
Data Engineering (ICDE), pages 1391?1402.
Takeuchi, Hironori, L Venkata Subramaniam, Tetsuya Na-
sukawa, Shourya Roy, and Sreeram Balakrishnan. 2007.
A conversation-mining system for gathering insights to
improve agent productivity. In Proceedings of the IEEE
International Conference on E-Commerce Technology
and the IEEE International Conference on Enterprise
Computing, E-Commerce, and E-Services, pages 465?
468.
Tam, Yik-Cheung and Tanja Schultz. 2005. Dynamic
language model adaptation using variational Bayes in-
ference. In Proceedings of the 9th European Confer-
ence on Speech Communication and Technology (EU-
ROSPEECH), pages 5?8.
S?ingliar, Tomas and Milos Hauskrecht. 2006. Noisy-OR
component analysis and its application to link analy-
sis. The Journal of Machine Learning Research, 7:2189?
2213.
Williams, JasonD. and Steve Young. 2007. Partially observ-
able Markov decision processes for spoken dialog sys-
tems. Computer Speech & Language, 21(2):393?422.
408
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 806?815, Dublin, Ireland, August 23-29 2014.
Predicate-Argument Structure Analysis with Zero-Anaphora Resolution
for Dialogue Systems
Kenji Imamura, Ryuichiro Higashinaka, and Tomoko Izumi
NTT Media Intelligence Laboratories, NTT Corporation
1-1 Hikari-no-oka, Yokosuka, 239-0847, Japan
{imamura.kenji,higashinaka.ryuchiro,izumi.tomoko}@lab.ntt.co.jp
Abstract
This paper presents predicate-argument structure analysis (PASA) for dialogue systems in
Japanese. Conventional PASA and semantic role labeling have been applied to newspaper arti-
cles. Because pronominalization and ellipses frequently appear in dialogues, we base our PASA
on a strategy that simultaneously resolves zero-anaphora and adapt it to dialogues. By incor-
porating parameter adaptation and automatically acquiring knowledge from large text corpora,
we achieve a PASA specialized to dialogues that has higher accuracy than that for newspaper
articles.
1 Introduction
Semantic role labeling (SRL) and predicate-argument structure analysis (PASA) are important analysis
techniques for acquiring ?who did what to whom? from sentences
1
. These analyses have been applied to
written texts because most annotated corpora comprise newspaper articles (Carreras and M`arquez, 2004;
Carreras and M`arquez, 2005; Matsubayashi et al., 2014).
Recently, systems for speech dialogue between humans and computers (e.g., Siri of Apple Inc. and
Shabette Concier of NTT DoCoMo) have become familiar with the popularization of smart phones. A
man-machine dialogue system has to interpret human utterances to associate them with system utter-
ances. The predicate-argument structure could be an effective data structure for dialogue management.
However, it is unclear whether we can apply the SRL/PASA for newspaper articles to dialogues because
there are many differences between them, such as the number of speakers, written or spoken language,
and context processing. For example, the following dialogue naturally includes pronouns, and thus
anaphora resolution is necessary for semantic role labeling.
A: [I]
ARG0
want [an iPad Air]
ARG1
.
B: [When]
ARGM
will [you]
ARG0
buy [it(=an iPad Air)]
ARG1
?
Similar phenomena exist in Japanese dialogues. However, most pronouns are omitted (called zero-
pronouns), and zero-anaphora resolution is necessary for Japanese PASA.
A: [iPad Air]
NOM
-ga hoshii-na.
iPad Air NOM. want
?? want an iPad Air.?
B: itsu ?
NOM
?
ACC
kau-no?
when buy?
?When will ? buy ???
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
Recent SRL systems assign labels of predicates and their arguments as semantic roles. Consequently, SRL and PASA are
very similar tasks. We use the term predicate-argument structure analysis in this paper because most Japanese analyzers use
this term.
806
This paper presents predicate-argument structure analysis with zero-anaphora resolution for Japanese
chat dialogues. Here, we regard the task of constructing PASA for dialogues as a kind of domain adap-
tation from newspaper articles to dialogues. M`arquez et al. (2008) and Pradhan et al. (2008) indicated
that the tuning of parameter distribution and reducing the out-of-vocabulary are important for the do-
main adaptation of SRL. We also focus on parameter distribution and out-of-vocabulary to construct a
PASA adapted to dialogues. To the best of our knowledge, this is the first paper to describe a PASA for
dialogues that include many zero-pronouns.
The paper is organized as follows. Section 2 briefly reviews SRL/PASA in English and Japanese.
Section 3 discusses characteristics of chat dialogues by comparing two annotated corpora, newspaper
articles and dialogues. Section 4 describes the basic strategy of our PASA, and Section 5 shows how it
was adapted for dialogues. Experiments are presented in Section 6, and Section 7 concludes the paper.
2 Related Work
2.1 Semantic Role Labeling in English
The advent of the supervised method proposed by Gildea and Jurafsky (2002) has led to the creation of
annotated corpora for semantic role labeling. In the CoNLL-2004 and 2005 shared task (Carreras and
M`arquez, 2004; Carreras and M`arquez, 2005), evaluations were carried out using the Proposition Bank
(Palmer et al., 2005). Because the Proposition Bank was annotated to the Penn Treebank (i.e., the source
texts were from the Wall Street Journal), the shared tasks were evaluated on newspaper articles. M`arquez
et al. (2008) provides a review of SRL.
OntoNotes Corpus (Hovy et al., 2006) contains multiple genres such as newswire, broadcast news,
broadcast conversation. The annotation to OntoNotes includes semantic role labels compliant with the
Proposition Bank. It is currently used for coreference resolution (Pradhan et al., 2012), and is expected
to be applied to dialogue analysis.
A few SRL studies have focused on not only verbal predicates (e.g., ?decide?) but also nominal predi-
cates (e.g., ?decision?) (Jiang and Ng, 2006; Gerber and Chai, 2012; Laparra and Rigau, 2013). Because
the subject and object of nominal predicates are frequently omitted (e.g., the object in the phrase ?the
decision? is omitted), problems similar to the Japanese zero-pronouns have to be resolved in the SRL of
nominal predicates.
2.2 Predicate-Argument Structure Analyses in Japanese
Japanese material includes the NAIST Text Corpus (Iida et al., 2007)
2
, which is an annotated corpus
of predicate-argument structures and coreference information for newspaper articles. Argument noun
phrases of the nominative, accusative, and dative cases are assigned to each predicate. The predicate and
the noun phrases are not limited to the same sentence. If arguments of the predicate are represented as
zero-pronouns, the antecedent noun phrases in other sentences are assigned as the arguments.
Many PASA methods have been studied on the NAIST Text Corpus (Komachi et al., 2007; Taira et al.,
2008; Imamura et al., 2009; Yoshikawa et al., 2011). In Japanese, some of them simultaneously resolve
the zero-anaphora caused by zero-pronouns.
Most English SRL and Japanese PASA currently target newspaper articles, and it is unclear whether
the methods for newspapers can be applied to dialogue conversations.
3 Characteristics of Chat Dialogues
We first collected chat dialogues of two speakers and annotated them with the predicate-argument struc-
ture. The participants chatted via keyboard input. Therefore, fillers and repetitions, which are frequent
in speech dialogues, were rare. The theme was one of 20 topics, such as meals, travel, hobbies, and
TV/radio programs. Annotation of the predicate-argument structure complied with the NAIST Text Cor-
pus. Figure 1 shows a chat dialogue example and its predicate-argument structure annotation.
2
http://cl.naist.jp/nldata/corpus/. We use version 1.5 with our own preprocessing in this paper. NAIST is
an acronym of ?Nara Institute of Science and Technology.?
807
A: natsu-wa (exo2)
NOM
(exog)
DAT
dekake-tari-shimashi-ta-ka?
?Did (you)
NOM
go (anywhere)
DAT
in this summer??
B: 8-gatsu-wa Ito-no [hanabi-taikai]
DAT
-ni (exo1)
NOM
yuki-mashi-ta.
?(I)
NOM
went to
[
the fireworks
?1
]
DAT
at Ito in August.?
A:
[
hanabi
?2
]
ACC
,
[
watashi
?3
]
NOM
-mo mi-takatta-desu.
?
[
Fireworks
?2
]
ACC
,
[
I
?3
]
NOM
also wanted to see (it).?
A: demo, kotoshi-wa (exo1)
NOM
isogashiku-te (exo1)
NOM
(
*
2)
ACC
mi-ni (
*
2)
DAT
ike-masen-deshita.
?But (I)
NOM
couldn?t go (?2)
DAT
to see (it=*2)
ACC
this year because (I)
NOM
was busy.?
Figure 1: Chat Dialogue Example and Its Predicate-Argument Structure Annotation
Lower lines denote glosses of the upper lines. The bold words denote predicates, the square brack-
ets [] denote intra-sentential arguments, and the round brackets () denote inter-sentential or exophoric
arguments.
# of Articles # of Sentences # of Words # of Predicates
Corpus Set /Dialogues /Utterances (per Sentence) (per Sentence)
NAIST Text Corpus Training 1,751 24,283 664,898 (27.4) 68,602 (2.83)
Development 480 4,833 136,585 (28.3) 13,852 (2.87)
Test 696 9,284 255,624 (27.5) 26,309 (2.83)
Chat Dialog Corpus Training 184 6,960 61,872 (8.9) 7,470 (1.07)
Test 101 4,056 38,099 (9.4) 5,333 (1.31)
Table 1: Sizes of Corpora
Zero- Zero- Exophora
Case Corpus # of Arguments Dep Intra Inter exo1 exo2 exog
Nominative NAIST 68,598 54.5% 17.3% 11.4% 2.0% 0.0% 14.7%
Dialogue 7,467 31.8% 7.4% 12.6% 23.9% 5.6% 18.8%
Accusative NAIST 27,986 89.2% 6.9% 3.4% 0.0% 0.0% 0.4%
Dialogue 1,901 46.6% 12.8% 27.5% 0.8% 0.1% 12.2%
Datative NAIST 6,893 84.7% 10.2% 4.3% 0.0% 0.0% 0.8%
Dialogue 2,089 37.6% 7.8% 15.0% 2.5% 1.1% 36.1%
Table 2: Distribution of Arguments in Training Corpora
Table 1 shows the statistics of the NAIST Text Corpus and the Chat Dialogue Corpus we created
3
.
The size of the Dialogue Corpus is about 10% of the NAIST Corpus. The NAIST Corpus is divided into
three parts: training, development, and test. The Dialogue Corpus is divided into training and test.
Table 2 shows distributions of arguments in the training sets of the NAIST/Dialogue corpora. We clas-
sified the arguments into the following six categories because each argument presents different difficulties
for analysis by its position and syntactic relation. The first two categories (Dep and Zero-Intra) are
the ones that in which the predicate and the argument occupy the same sentence.
? Dep: The argument directly depends on the predicate and vice versa on the parse tree.
? Zero-Intra: Intra-sentential zero-pronoun. The predicate and the argument are in the same
sentence, but there is no direct dependency.
? Zero-Inter: Inter-sentential zero-pronoun. The predicate and the argument are in different
sentences.
? exo1/exo2/exog: These are exophoric and denote zero-pronouns of the first person, second per-
son, and the others (general), respectively.
By Table 2, we can see that the ratios of Dep in all cases decreased in the Dialogue Corpus. In the other
categories, the tendencies between the nominative case and the accusative/dative cases were different. In
the nominative case, the Zero-Intra also decreased in the Dialogue Corpus, and the declines were
3
We regard a dialogue and an utterance as an article and a sentence, respectively.
808
exo1 exo2 exogNULL Phrase 1 Phrase 2 Phrase 3 Phrase 4 ?
Special Noun Phrases Candidate Argumentsin Past  Sentences Candidate Argumentsin Current Sentence
Candidate Arguments
SelectorNominativeModel SelectorAccusativeModel SelectorDativeModel
exo1exophoric(first person) zero-anaphoric(inter-sentential)
Phrase 2 NULLno argument
Figure 2: Structure of Argument Identification and Classification
assigned to exo1 and exo2. Namely, the arguments in a sentence were reduced, and zero-pronouns
increased compared with the newspaper articles. Note that many antecedents were the first or second
person. On the other hand, in the accusative and dative cases, the declines of the Dep were assigned to
the Zero-Inter or the exog in the Dialogue Corpus. Namely, anaphora resolution across multiple
sentences is important to dialogue analysis. In contrast, most arguments and the predicate appear in the
same sentence in the accusative/dative cases of newspapers.
4 Basic Strategy for Predicate-Argument Structure Analysis and Zero-Anaphora
Resolution
4.1 Architecture
We use Imamura et al. (2009)?s method developed for newspaper articles as the base PASA in this paper.
It can simultaneously identify arguments of a predicate in the sentence, those in other sentences, and
exophoric arguments. The analyzer receives the entire article (dialogue) and performs the following
steps for each sentence (utterance).
1. The input sentences are tagged and parsed. During parsing, the base phrases and their headwords
are also identified. At this time, the part-of-speech tags and the parse trees of the Dialogue Corpus
are supplied by applying the morphological analyzer MeCab (Kudo et al., 2004) and the dependency
parser CaboCha (Kudo and Matsumoto, 2002). The NAIST Corpus version 1.5 already includes the
part-of-speech tags and the parse trees.
2. Predicate phrases are identified from the sentences. We use the correct predicates in the corpora
for the evaluation. When we build dialogue systems on PASA, predicate phrases will be identified
using part-of-speech patterns that include verbs, adjectives, and copular verbs.
3. For each predicate, candidate arguments are acquired from the sentence that includes the predicate
(called the current sentence) and the past sentences. Concretely, the following base phrases are
regarded as candidates.
? All noun phrases in the current sentence are extracted as intra-sentential candidates regardless
of syntactic relations.
? From the past sentences, noun phrases are contextually extracted as inter-sentential candidates.
Details are described in Section 4.4.
? Exophoric labels (exo1, exo2, and exog) and the NULL (the argument is not required) are
added as special noun phrases.
809
4. The features are generated from the predicate phrase, candidate arguments, and their relations. The
best candidate for each case is independently selected (Figure 2).
4.2 Models
The models for the selector are based on maximum entropy classification. The selector identifies the best
noun phrase n? that satisfies the following equations from the candidate argument set N.
n? = argmax
n
j
?N
P (d(n
j
) = 1|X
j
;M
c
) (1)
P (d(n
j
) = 1|X
j
;M
c
) =
1
Z
c
(X)
exp
?
k
{?
ck
f
k
(d(n
j
) = 1, X
j
)} (2)
Z
c
(X) =
?
n
j
?N
exp
?
k
{?
ck
f
k
(d(n
j
) = 1, X
j
)} (3)
X
j
= ?n
j
, v, A? (4)
where n denotes a candidate argument, N denotes a set of candidate arguments of predicate v, d(n) is
a function that returns 1 iff candidate n becomes the argument, and M
c
denotes the model of case c. In
addition, f
k
(d(n
j
) = 1, X
j
) is a feature function, ?
ck
denotes a weight parameter of the feature function,
and A denotes the article from which all sentences are parsed.
Training phase optimizes the weight parameters in order to maximize the difference in posterior prob-
abilities among the correct noun phrase and the other candidates. Specifically, the model of case M
c
is
learnt by minimizing the following loss function `
c
.
`
c
= ?
?
i
logP (d(n
i
) = 1|X
i
;M
c
) +
1
2C
?
k
||?
ck
||
2
(5)
where n
i
denotes the correct noun phrase of the i-th predicate in the training set, X
i
denotes the i-th
tuple of the correct noun phrase, the predicate, and the article ?n
i
, v
i
, A
i
?. Since the posterior probability
is normalized for each set of candidate arguments of a predicate by Equation (3), the probability of
the correct noun phrase approaches closer to 1.0, and the probabilities of the other candidates approach
closer to 0.0 in Equation (5).
4.3 Features
Similar to other studies (e.g., (Gildea and Jurafsky, 2002)), we use three types of features: 1) predicate
features, 2) noun phrase (NP) features, and 3) the relationship between predicates and noun phrases
(Table 3). We also introduce combined features of the ?Noun? with all other binary features because the
features aim to select the best noun phrase.
The special features in this paper are the dependency language models (three types) and the obligatory
case information (?Frame? feature), which are automatically acquired from large text corpora. We discuss
them in Section 5.2.
4.4 Context Processing
Contexts of dialogues and newspaper articles are different. We should employ context processing spe-
cialized for the dialogues. However, contexts, including system and user utterances, should be managed
collectively by the dialogue manager from the viewpoint of dialogue systems. Thus, this study uses the
same context processing for the newspaper articles and dialogues. Note that the method in this paper
controls the context by selecting the inter-sentential candidates. We can easily alter context management
by providing candidate arguments from an external manager.
Context processing in this paper is as follows.
? From the current sentence, trace back to the past, and find a sentence that contains the other pred-
icate (we call this the prior sentence). This process aims to ignore utterances that do not contain
predicates.
810
Type Name Value Remark
Predicate Pred Binary Lemma of the predicate.
PType Binary Type of predicate. One of ?verb?, ?adjective?, and ?copular verb?.
Voice Binary Declarative or not. If not, the passive/causative auxiliary verb is assigned.
Suffix Binary Sequence of the functional words of the main clause. This feature aims to reflect
the speech act of the utterance.
Frame Binary Obligatory case information. The case requires argument (1) or not (0).
Noun Phrase Noun Binary Headword of the NP
Particle Binary Case particle of the base phrase. If the NP is a special noun phrase, this is NULL.
NType Binary If the substance of the NP is in the article, this is ?NP?; otherwise the same value
of the ?Noun? feature.
Surround Binary POS tags of the surrounding words of the NP. The window size is ?2.
Relation
between
Predicate and
NP
PhPosit Binary Distance between the predicate and the NP. If they are in different sentences, or
the NP is an exophora, this is NULL.
Syn Binary Dependency path between the predicate and the NP. If they are in different sen-
tences, or the NP is an exophora, this is NULL.
Speaker Binary Whether the speakers of the predicate and the NP are the same (SAME) or not
(OTHER).
Dependency
Language
Models
log P (n|c, v) Real Generation probability of NP n given predicate v and case c.
log P (v|c, n) Real Generation probability of predicate v given NP n and case c.
log P (c|n) Real Generation probability of case c given NP n.
Table 3: List of Features
? All noun phrases that lie between the prior to the current sentence are added to the candidate argu-
ments. In addition, noun phrases that are used as arguments of any predicates are also added (called
argument recycling (Imamura et al., 2009)). Argument recycling covers wide contexts because it
can employ distant noun phrases if the past predicates have inter-sentential arguments.
5 Adaptation to Chat Dialogues
The method described in the previous section is common to dialogues and newspaper articles. This
section describes the adaptation made to target dialogues.
5.1 Adaptation of Model Parameters
In order to tune the difference in the argument distribution, model parameters of the selectors are adapted
to the dialogue domain. We use the feature augmentation method (Daum?e, 2007) as the domain adap-
tation technique; it has the same effect as regarding the source domain to be prior knowledge, and the
parameters are optimized to the target domain. Concretely, the models of the selectors are learnt and
applied as follows.
1. First, the feature space is segmented into three parts: common, source, and target.
2. The NAIST Corpus and the Dialogue Corpus are regarded as the source and the target domains,
respectively. The features from the NAIST Corpus are deployed to the common and the source
spaces, and those from the Dialogue Corpus are deployed to the common and the target spaces.
3. The parameters are estimated in the usual way on the above feature space. The weights of the
common features are emphasized if the features are consistent between the source and target. With
regard to domain-dependent features, the weights in the respective space, source or target, are em-
phasized.
4. When the argument is identified, the selectors use only the features in the common and target spaces.
The parameters in the spaces are optimized to the target domain, plus we can utilize the features
that appear only in the source domain data.
5.2 Weak Knowledge Acquisition from Very Large Resources
In this paper, we use two types of knowledge to reduce the harmful effect of out-of-vocabulary in the
training corpus. Both types are constructed by automatically analyzing, summing up, and filtering large
811
text corpora (Kawahara and Kurohashi, 2002; Sasano et al., 2008; Sasano et al., 2013). They provide
information about unknown words with some confidence but they do contain some errors. We use them
as the features of the models, and parameters are optimized by the discriminative learning of the selectors.
5.2.1 Obligatory Case Information (Frame Feature)
Case frames are important clues for SRL and PASA. The obligatory case information (OCI) comprises
subsets of the case frames that only clarify whether the cases of each predicate are necessary or not.
The OCI dictionary is automatically constructed from large text corpora as follows. The process
assumes that 1) most of the cases match the case markers if the noun phrase directly depends on the
predicate, and 2) if the case is obligatory, the occurrence rate on a specific predicate is higher than the
average rate of all predicates.
1. Similar to PASA in this paper (c.f., Section 4.1), predicates and base phrases are identified by
tagging and parsing raw texts.
2. Noun phrases that directly depend on the predicate and accompany a case marker are extracted. We
sum up the frequency of the predicate and cases.
3. Highly frequent predicates are selected according to the final dictionary size. Obligation of the cases
is determined so as to satisfy the following two conditions.
? Co-occurrence of the predicate and the case ?v, c? are higher than the significance level (p ?
0.001; LLR ? 10.83) by the log-likelihood-ratio test.
? The case of the predicate appears at least 10% more frequently than the average of all predi-
cates.
We constructed two OCI dictionaries. The Blog dictionary contains about 480k predicates from one
year of blogs (about 2.3G sentences,). The News dictionary contains about 200k predicates from 12
years of newspaper articles (about 7.7M sentences). The coverage of predicates in the training set of the
Dialogue Corpus was 98.5% by the Blog dictionary and 96.4% by the News dictionary.
5.2.2 Dependency Language Models
Dependency language models (LMs) represent semantic/pragmatic collocations among predicate v, case
c, and noun phrase n. The generation probabilities of v, c, and n are computed by n-gram models. More
concretely, the following real values are computed. The purpose of the biases (probabilities involved
<unk>) is to correct the values to be positive.
? logP (n|c, v) ? logP (<unk>|c, v)
? logP (v|c, n) ? logP (v|c,<unk>)
? logP (c|n) ? logP (c|<unk>)
Each dependency LM is constructed from the tuples of ?v, c, n? extracted in Section 5.2.1 using the
SRILM (Stolcke et al., 2011). Note that since the obligatory case information corresponds to the gener-
ation probability of the case (P (c|v)), we exclude it from the dependency LMs.
Similar to the OCI dictionaries, we constructed two sets of dependency language models from the Blog
and the News sentences. The coverage of triples ?v, c, n? appeared in the training set of the Dialogue
Corpus was 76.4% by the Blog LMs and 38.3% by the News LMs. The Blog LMs cover the Dialogue
Corpus more comprehensively than the News LMs.
6 Experiments
We evaluate the accuracies of the proposed PASA on the Dialogue Corpus (Table 1) from the perspectives
of parameter adaptation and the effect of the automatically acquired knowledge. The evaluation metric
is F-measure of each case (includes exophora identification).
812
a) Adaptation b) NAIST? c) Dialogue? d) Adaptation e) Adaptation
# of OCI:Blog OCI:Blog OCI:Blog OCI:News? OCI:Blog
Case Type Args. LMs:Blog LMs:Blog LMs:Blog LMs:Blog LMs:News?
Nominative Dep 1,811 83.3%?? 77.6% 82.7% 83.0% 82.7%
Zero-Intra 511 37.4% 43.7%? 36.6% 36.5% 38.1%
Zero-Inter 767 8.6% ? 9.1% 9.0% 8.3% 4.5%
exo1 1,193 70.2%? 13.5% 69.9% 70.1% 70.3%
exo2 281 46.8%?? 0.0% 43.1% 47.2% 46.8%
exog 767 46.8%? 32.5% 27.9% 47.2% 47.7%?
Total 5,330 61.5%? 44.4% 61.1% 61.4% 61.4%
Accusative Dep 614 84.2%?? ? 78.6% 81.5% 84.2% 82.4%
Zero-Intra 149 42.9%? ?? 27.1% 45.0% 38.9% 34.3%
Zero-Inter 399 30.4%? ? 0.5% 30.9% 29.4% 24.3%
exo1 19 0.0% 0.0% 0.0% 9.5% 10.0%
exo2 7 0.0% 0.0% 0.0% 0.0% 0.0%
exog 98 25.6%? 0.0% 27.9% 25.2% 25.6%
Total 1,286 59.0%? ? 51.6% 58.9% 58.4% 56.0%
Dative Dep 566 80.5%?? 54.0% 79.0% 80.1% 80.7%
Zero-Intra 70 20.7%? ? 0.0% 20.0% 20.7% 11.8%
Zero-Inter 169 14.6%? 0.0% 14.8% 14.4% 13.4%
exo1 32 0.0% 0.0% 0.0% 0.0% 0.0%
exo2 4 0.0% 0.0% 0.0% 0.0% 0.0%
exog 265 45.4%?? 0.0% 43.1% 44.0% 44.9%
Total 1,106 58.6%?? 32.2% 57.2% 58.2% 58.4%
Table 4: F-measures among Methods/OCI dictionary/Dependency LMs on Dialogue Test Set
The bold values denote the highest F-measures among all methods. The marks ?, ?, ?, ? denote sig-
nificantly better methods by comparing a) with b), c), d), and e), respectively. We used the bootstrap
resampling method (1,000 iterations) as the significance test, in which the significance level was 0.05.
6.1 Experiment 1: Effect of Parameter Adaptation
We compared three methods in order to evaluate parameter adaptation: a) The feature augmentation is
applied to the training (Adaptation). b) Only the NAIST Corpus is used for training (NAIST Training).
c) Only the Dialogue Corpus is used (Dialogue Training). The NAIST Training corresponds to a conven-
tional PASA for newspaper articles. The results on the Dialogue test set are shown in the 4th, 5th, and
6th columns in Table 4.
First, comparing methods a) Adaptation and b) NAIST training, Adaptation was better than the NAIST
training for most types (The ? mark denotes ?significantly better?). In particular, the total F-measures
of all cases were significantly better than NAIST training. Focusing on the types of arguments, the most
characteristic results were exophoras of the first/second persons (exo1 and exo2) of the nominative
case. These two types dominate of the nominative case (about 28%), and exo1 (70.2%) and exo2
(46.8%) became analyzable. Other types such as the Zero-Inter and the exog of the accusative and
dative cases, which could not be analyzed by NAIST training, became analyzable.
Comparing methods a) Adaptation and c) Dialogue training (c.f., ?), the F-measures of Dialogue
training approached those of Adaptation even though the size of the Dialogue Corpus was small. Only
the F-measure of the dative case of Adaptation was significantly better than Dialogue training in total.
This does not imply that the corpus size is sufficient. Rather, we suppose that the Adaptation strategy
could not adequately utilize the advantages of the NAIST Corpus. Adding more dialogue data would
further improve the accuracies on the Dialogue test set.
6.2 Experiment 2: Differences among Automatically Acquired Knowledge
The columns a), d), and e) in Table 4 show the results for the proposed method (Adaptation). Note that
the combination of the OCI dictionary and the dependency language models were changed to a) ?Blog,
Blog?, d) ?News, Blog?, and e) ?Blog, News?.
When the OCI dictionary was changed from a) Blog to d) News (c.f., ?), there were no significant
differences in almost all types except for the Zero-Intra of the accusative case. We suppose that this
813
is because the coverage of the Blog and News dictionaries were almost the same, and obligatory cases of
predicates are general information regardless of the domain.
On the contrary, when the dependency LMs were changed from a) Blog to e) News (c.f., ?), the F-
measures of some types significantly dropped, especially the Zero-Intra and Zero-Inter types,
which are strongly influenced by semantic relation. For example, the Zero-Inter type of the ac-
cusative case was changed from 30.4% to 24.3%, and the F-measure consequently decreased by 3.0
points in total in the accusative case. Zero-anaphora resolution cannot rely on syntax, and the dependency
LMs that measure semantic collocation become relatively important. The Blog LMs yielded greater cov-
erage than the News LMs in this experiment. We can conclude that high-coverage LMs are better for
improving the zero-anaphora resolution.
7 Conclusion
This paper presented predicate-argument structure analysis with zero-anaphora resolution for dialogues.
We regarded this task as a kind of domain adaptation from newspaper articles, which are conventionally
studied, to dialogues. The model parameters were adapted to the dialogues by using a domain adapta-
tion technique. In order to address the out-of-vocabulary issue, the obligatory case information and the
dependency language models were constructed from large text corpora and applied to the selectors.
As a result, arguments that could not be analyzed by PASA for newspaper articles (e.g., zero-pronouns
of the first and second persons in the nominative case) became analyzable by adding only a small number
of dialogues. The parameter adaptation achieved some improvement. Moreover, we confirmed that high-
coverage dependency LMs contribute to improving zero-anaphora resolution and the overall accuracy.
Although we focused on parameter distribution and out-of-vocabulary in this paper, there are the other
differences between dialogues and newspaper articles. For example, we did not discuss the exchange
of turns, which is a special phenomenon of dialogues. To consider further phenomena is our future
work. We are also evaluating the effectiveness of our PASA by incorporating it into a dialogue system
(Higashinaka et al., 2014).
References
Xavier Carreras and Llu??s M`arquez. 2004. Introduction to the CoNLL-2004 shared task: Semantic role labeling.
In Hwee Tou Ng and Ellen Riloff, editors, HLT-NAACL 2004 Workshop: Eighth Conference on Computational
Natural Language Learning (CoNLL-2004), pages 89?97, Boston, Massachusetts, USA, May.
Xavier Carreras and Llu??s M`arquez. 2005. Introduction to the CoNLL-2005 shared task: Semantic role labeling.
In Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL-2005), pages
152?164, Ann Arbor, Michigan, June.
Hal Daum?e, III. 2007. Frustratingly easy domain adaptation. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 256?263, Prague, Czech Republic, June.
Matthew Gerber and Joyce Y. Chai. 2012. Semantic role labeling of implicit arguments for nominal predicates.
Computational Linguistics, 38(4):755?798.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics,
28(3):245?288.
Ryuichiro Higashinaka, Kenji Imamura, Toyomi Meguro, Chiaki Miyazaki, Nozomi Kobayashi, Hiroaki
Sugiyama, Toru Hirano, Toshiro Makino, and Yoshihiro Matsuo. 2014. Towards an open domain conversa-
tional system fully based on natural language processing. In Proceedings of the 25th International Conference
on Computational Linguistics (COLING 2014), Dublin, Ireland, August.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. OntoNotes: The
90% solution. In Proceedings of the Human Language Technology Conference of the NAACL, Companion
Volume: Short Papers, pages 57?60, New York City, USA, June.
Ryu Iida, Mamoru Komachi, Kentaro Inui, and Yuji Matsumoto. 2007. Annotating a Japanese text corpus with
predicate-argument and coreference relations. In Proceedings of the Linguistic Annotation Workshop, pages
132?139, Prague, Czech Republic, June.
814
Kenji Imamura, Kuniko Saito, and Tomoko Izumi. 2009. Discriminative approach to predicate-argument structure
analysis with zero-anaphora resolution. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,
pages 85?88, Singapore, August.
Zheng Ping Jiang and Hwee Tou Ng. 2006. Semantic role labeling of NomBank: A maximum entropy approach.
In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 138?145,
Sydney, Australia, July.
Daisuke Kawahara and Sadao Kurohashi. 2002. Fertilization of case frame dictionary for robust Japanese case
analysis. In Proceedings of the 19th International Conference on Computational Linguistics (COLING-2002),
pages 425?431, Taipei, Taiwan, August.
Mamoru Komachi, Ryu Iida, Kentaro Inui, and Yuji Matsumoto. 2007. Learning-based argument structure analy-
sis of event-nouns in Japanese. In Proceedings of the Conference of the Pacific Association for Computational
Linguistics (PACLING), pages 208?215, Melbourne, Australia, September.
Taku Kudo and Yuji Matsumoto. 2002. Japanese dependency analysis using cascaded chunking. In CoNLL
2002: Proceedings of the 6th Conference on Natural Language Learning 2002 (COLING 2002 Post-Conference
Workshops), pages 63?69, Taipei, Taiwan.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto. 2004. Applying conditional random fields to Japanese
morphological analysis. In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 230?237,
Barcelona, Spain, July.
Egoitz Laparra and German Rigau. 2013. ImpAr: A deterministic algorithm for implicit semantic role labelling.
In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pages 1180?1189, Sofia, Bulgaria, August.
Llu??s M`arquez, Xavier Carreras, Kenneth C. Litkowski, and Suzanne Stevenson. 2008. Semantic role labeling:
An introduction to the special issue. Computational Linguistics, 34(2):145?159.
Yuichiro Matsubayashi, Ryu Iida, Ryohei Sasano, Hikaru Yokono, Suguru Matsuyoshi, Atsushi Fujita, Yusuke
Miyao, and Kentaro Inui. 2014. Issues on annotation guidelines for Japanese predicate-argument structures.
Journal of Natural Language Processing, 21(2):333?377, April. in Japanese.
Martha Palmer, Daniel Gildia, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?105.
Sameer S. Pradhan, Wayne Ward, and James H. Martin. 2008. Towards robust semantic role labeling. Computa-
tional Linguistics, 34(2):289?310.
Sameer Pradhan, Alessandro Moschitti, and Nianwen Xue, editors. 2012. Joint Conference on EMNLP and
CoNLL: Proceeding of the Shared Task: Modeling Multilingual Unrestricted Coreference in Onto Notes, Jeju,
Korea, July.
Ryohei Sasano, Daisuke Kawahara, and Sadao Kurohashi. 2008. A fully-lexicalized probabilistic model for
Japanese zero anaphora resolution. In Proceedings of the 22nd International Conference on Computational
Linguistics (Coling 2008), pages 769?776, Manchester, UK, August.
Ryohei Sasano, Daisuke Kawahara, Sadao Kurohashi, and Manabu Okumura. 2013. Automatic knowledge ac-
quisition for case alternation between the passive and active voices in Japanese. In Proceedings of the 2013
Conference on Empirical Methods in Natural Language Processing, pages 1213?1223, Seattle, Washington,
USA, October.
Andreas Stolcke, Jing Zheng, Wen Wang, and Victor Abrash. 2011. SRILM at sixteen: Update and outlook.
In Proceedings of IEEE Automatic Speech Recognition and Understanding Workshop (ASRU 2011), Waikoloa,
Hawaii, December.
Hirotoshi Taira, Sanae Fujita, and Masaaki Nagata. 2008. A Japanese predicate argument structure analysis using
decision lists. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,
pages 523?532, Honolulu, Hawaii, October.
Katsumasa Yoshikawa, Masayuki Asahara, and Yuji Matsumoto. 2011. Jointly extracting Japanese predicate-
argument relation with markov logic. In Proceedings of 5th International Joint Conference on Natural Lan-
guage Processing, pages 1125?1133, Chiang Mai, Thailand, November.
815
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 928?939, Dublin, Ireland, August 23-29 2014.
Towards an open-domain conversational system fully based on natural
language processing
Ryuichiro Higashinaka
1
, Kenji Imamura
1
, Toyomi Meguro
2
, Chiaki Miyazaki
1
Nozomi Kobayashi
1
, Hiroaki Sugiyama
2
, Toru Hirano
1
Toshiro Makino
1
, Yoshihiro Matsuo
1
1
NTT Media Intelligence Laboratories
2
NTT Communication Science Laboratories
{higashinaka.ryuichiro, imamura.kenji, meguro.toyomi, miyazaki.chiaki,
kobayashi.nozomi, sugiyama.hiroaki, hirano.tohru,
makino.toshiro, matsuo.yoshihiro}@lab.ntt.co.jp
Abstract
This paper proposes an architecture for an open-domain conversational system and evaluates an
implemented system. The proposed architecture is fully composed of modules based on natu-
ral language processing techniques. Experimental results using human subjects show that our
architecture achieves significantly better naturalness than a retrieval-based baseline and that its
naturalness is close to that of a rule-based system using 149K hand-crafted rules.
1 Introduction
Although task-oriented dialogue systems have been extensively researched over the decades (Walker
et al., 2001; Williams et al., 2013), it is only recently that non-task-oriented dialogue, open-domain
conversation, or chat has been attracting attention for its social and entertainment aspects (Bickmore
and Picard, 2005; Ritter et al., 2011; Bessho et al., 2012). Creating an open-domain conversational
system is a challenging problem. In task-oriented dialogue systems, it is possible to prepare knowledge
for a domain and create understanding and generation modules for that domain (Nakano et al., 2000).
However, for open-domain conversation, such preparation cannot be performed. Since it is difficult to
handle users? open-domain utterances, to create workable systems, conventional approaches have used
hand-crafted rules (Wallace, 2004). Although elaborate rules may work well, the problem with the rule-
based approach is the high cost and the dependence on individual skills of developers, which hinders
systematic development. Another problem with the rule-based approach is its low coverage; that is, the
inability to handle unexpected utterances.
The recent increase of web data has propelled the development of approaches that use data retrieved
from the web for open-domain conversation (Shibata et al., 2009; Ritter et al., 2011). The merit of such
retrieval-based approaches is that, owing to the diversity of the web, systems can retrieve at least some
responses for user input, which solves the coverage problem. However, this comes at the cost of utterance
quality. Since the web, especially Twitter, is inherently noisy, it is, in many cases, difficult to sift out
appropriate sentences from retrieval results.
In this paper, we propose an architecture for an open-domain conversational system. The proposed
architecture is fully composed of modules based on natural language processing (NLP) techniques. Our
stance is not just to hand-craft or to search the web for utterances, but to create a system that can fully
understand and generate utterances. We want to show that it is possible to build an open-domain conver-
sational system by combining NLP modules, which will open the way to a systematic development and
improvement. We describe our open-domain conversational system based on our architecture and present
results of an evaluation of its performance by human subjects. We compare our system with rule-based
and retrieval-based systems, and show that our architecture is a promising direction. In this work, we
regard the term open-domain conversation to be interchangeable with non-task-oriented dialogue, casual
conversation (Eggins and Slade, 2005), chat, or social dialogue (Bickmore and Cassell, 2000). We use
the term to denote that user input is not restricted in any way as in open-domain question answering
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
928
	
			
	

	
		
	

Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 18?27,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Modeling User Satisfaction Transitions in Dialogues from Overall Ratings
Ryuichiro Higashinaka?, Yasuhiro Minami?, Kohji Dohsaka?, and Toyomi Meguro?
? NTT Cyber Space Laboratories, NTT Corporation
? NTT Communication Science Laboratories, NTT Corporation
higashinaka.ryuichiro@lab.ntt.co.jp
{minami,dohsaka,meguro}@cslab.kecl.ntt.co.jp
Abstract
This paper proposes a novel approach
for predicting user satisfaction transitions
during a dialogue only from the ratings
given to entire dialogues, with the aim
of reducing the cost of creating refer-
ence ratings for utterances/dialogue-acts
that have been necessary in conventional
approaches. In our approach, we first
train hidden Markov models (HMMs) of
dialogue-act sequences associated with
each overall rating. Then, we combine
such rating-related HMMs into a single
HMM to decode a sequence of dialogue-
acts into state sequences representing to
which overall rating each dialogue-act is
most related, which leads to our rating pre-
dictions. Experimental results in two di-
alogue domains show that our approach
can make reasonable predictions; it signif-
icantly outperforms a baseline and nears
the upper bound of a supervised approach
in some evaluation criteria. We also
show that introducing states that represent
dialogue-act sequences that occur com-
monly in all ratings into an HMM signifi-
cantly improves prediction accuracy.
1 Introduction
In recent years, there has been intensive work
on the automatic evaluation of dialogues (Walker
et al, 1997; Mo?ller et al, 2008). Automatic
evaluation makes it possible to predict the per-
formance of dialogue systems without the costly
process of performing surveys with human sub-
jects, leading to a rapid improvement cycle for
dialogue systems. It is also useful for detect-
ing problematic situations in an ongoing dialogue
(Walker et al, 2002; Herm et al, 2008; Kim,
2007). In these studies, the typical approach is
to train a prediction model, such as a regression
or classification model, using features represent-
ing the whole or a part of a dialogue together with
human reference labels (e.g., reference ratings).
However, creating such reference labels by hand
can be extremely costly when we want to predict
user satisfaction transitions during a dialogue be-
cause we need to create reference labels after each
utterance/dialogue-act in the training data (Engel-
brecht et al, 2009).
This paper proposes a novel approach for pre-
dicting user satisfaction transitions during a dia-
logue only from the dialogues with overall rat-
ings. The approach makes it possible to avoid
creating reference labels for utterances/dialogue-
acts and only requires a single reference label for
each dialogue. More specifically, we predict the
user satisfaction rating after each dialogue-act in a
dialogue only by using dialogues with dialogue-
level (overall) user satisfaction ratings as train-
ing data. Our basic approach is to train hid-
den Markov models (HMMs) of dialogue-act se-
quences associated with each overall rating and
combine such rating-related HMMs into a single
HMM. We use this combined HMM to decode a
sequence of dialogue-acts by the Viterbi algorithm
(Rabiner, 1990) into state sequences that indicate
from which rating-related HMM each dialogue-act
is most likely to have been generated, leading to
our rating predictions for the dialogue-acts. This
paper experimentally examines the validity of our
approach and explores several model topologies
for possible improvement.
In Section 2, we review related work on auto-
matic evaluation of dialogues. In Section 3, we
describe our approach in detail. In Section 4, we
describe the experiment we performed to verify
our approach and present the results. In Section
5, we summarize and mention future work.
2 Related Work
Regression models are typically utilized for eval-
uating the quality of an entire dialogue. Most fa-
mously, the PARADISE framework (Walker et al,
1997) learns from data a linear regression model
that predicts dialogue-level user satisfaction from
various objective characteristics of a dialogue that
concern task success and dialogue costs. This
framework is widely used today and a number of
extensions have been proposed to improve the pre-
diction performance (Mo?ller et al, 2008); how-
18
ever, it is not aimed at predicting user satisfaction
transitions.
Classification models are widely employed to
detect problematic situations in an ongoing dia-
logue. Walker et al (2002) developed the Prob-
lematic Dialogue Predictor for the ?How May I
Help You? system (Gorin et al, 1997) to robustly
transfer problematic calls to human operators in
call routing tasks. They derive speech recogni-
tion, language understanding, and dialogue man-
agement features from the first few turns of a dia-
logue and apply a decision tree classifier to detect
problematic calls. For a similar task, Hirschberg
et al (2004) and Herm et al (2008) used prosodic
and emotional features. Kim (2007) recently pro-
posed an approach for online call quality monitor-
ing so that problematic calls can be transferred to
human operators as quickly as possible rather than
waiting for the first few turns.
N-grams andHMM-based approaches have also
been actively studied. Hara et al (2010) proposed
predicting the most likely user satisfaction level of
a dialogue by using N-grams of dialogues for each
satisfaction level in the music navigation domain.
Isomura et al (2009) used HMMs to evaluate the
naturalness of a dialogue in their interview system.
They trained HMMs that model dialogue-act se-
quences between human subjects and used them to
evaluate human-machine dialogues by the output
probabilities of the HMMs. Recently, there have
been approaches to predict user satisfaction tran-
sitions by evaluating the quality of individual ut-
terances in a dialogue. For example, Engelbrecht
et al (2009) predicted user satisfaction ratings af-
ter each user utterance by HMMs trained from
utterance-level features and utterance-level refer-
ence ratings.
The problem with these approaches is that they
require a lot of training data, especially when we
want to predict the quality of smaller units such
as utterances. Our aim is to reduce such cost.
Our work is similar to Engelbrecht?s work (Engel-
brecht et al, 2009) in that we useHMMs to predict
user satisfaction transitions during a dialogue but
different in that we only use dialogue-level ratings
to model dialogue-act-level user satisfaction tran-
sitions.
3 Approach
We aim to predict user satisfaction transitions only
from dialogues with overall ratings. More for-
mally, given a dialogue d
i
of a set of dialogues
D (= {d
1
. . .d
N
}), we want to predict the user
satisfaction rating after each dialogue-act in d
i
,
namely, r?(da(d
i
, 1)) . . .r?(da(d
i
, m
i
)), using D
with their dialogue-level ratings r(d
1
) . . . r(d
N
).
1:speaker1 2:speaker2
Speaker HMM for Rating 1
3:speaker1 4:speaker2
Speaker HMM for Rating 2
Figure 1: SHMMs connected ergodically. In the
figure, an oval marked with speaker1/speaker2
indicates a state that emits speaker1/speaker2?s
dialogue-acts. Arrows denote transitions and
numbers before speaker1/speaker2 are state IDs.
Boxes group together the states related to a partic-
ular overall rating.
Here, da(d
i
, l) denotes the l-th dialogue-act in d
i
,
N the total number of dialogues, and m
i
the total
number of dialogue-acts in d
i
.
Our basic idea is to train HMMs representing
dialogue-act sequences of dialogues for each over-
all rating and combine these rating-related HMMs
into a single HMM that can assign ratings for
dialogue-acts by estimating from which HMM
each dialogue-act has most likely to have been
generated by the Viterbi decoding. We use HMMs
because they can deal with sequences that evolve
over time and have been successfully utilized to
model and evaluate dialogue-act sequences (Shi-
rai, 1996; Isomura et al, 2009; Engelbrecht et
al., 2009). The generative feature of an HMM is
also useful when we want to build a probabilis-
tic dialogue manager that produces the most likely
dialogue-act sequences (Hori et al, 2008) or that
aims to maximize a reward function in partially
observable Markov decision processes (Williams
and Young, 2007; Minami et al, 2009).
When there are K levels of user satisfaction as
overall ratings, we create K HMMs each of which
is trained using the dialogue-act sequences in dia-
logues D
k
? D, where D
k
= {?d
i
, |r(d
i
) = k}.
We use the EM-algorithm to train HMMs. Here,
we assume that each HMM has two states, each
of which emits dialogue-acts of one of the con-
versational participants. This type of HMM is
called a speaker HMM (SHMM) and has been
successfully utilized to model two-party conversa-
tion (Meguro et al, 2009).
As an illustrative example, Fig. 1 shows two
SHMMs for ratings 1 and 2 that are connected
ergodically. We can simply use these connected
SHMMs (namely, states 1, 2, 3, and 4) to decode a
sequence of dialogue-acts into state sequences and
thereby obtain rating predictions. For example, if
the optimal state sequence obtained by the Viterbi
decoding is {4, 2, 1, 3, 2}, we can convert it into
ratings <2, 1, 1, 2, 1> using the ratings associated
with the states.
19
3:speaker1 4:speaker2
1:speaker1 2:speaker2
5:speaker1 6:speaker2
Speaker HMM for Rating 1
Speaker HMM for Rating 2
Speaker HMM for All Ratings
Figure 2: SHMMs with an additional SHMM
trained from all dialogues.
Introducing Common States: The simple er-
godic model may not be sufficient for appropri-
ately assigning ratings to input dialogue-act se-
quences because it is often the case that there
are dialogue-act sequences, such as greetings and
question-answer pairs, that commonly occur in ev-
ery dialogue. If we forcefully assign a rating for
such dialogue-act sequences, it may result in de-
grading the prediction accuracy. Therefore, in
addition to the simple ergodic model, we intro-
duce another SHMM that represents dialogue-act
sequences of dialogues for all ratings (see Fig.
2). This additional SHMM models dialogue-act
sequences that occur commonly in all dialogues
and it can simply be trained using all dialogues.
Hence, we call the states in this SHMM common
states. When this SHMM is added to the ergodic
model, it may be possible to reduce the possibil-
ity of our having to forcefully assign inappropriate
scores to common dialogue-act sequences. In this
model, when the optimal state sequence is {1, 4,
5, 6, 2}, the predicted ratings become <1, 2, 0, 0,
1>. Here, we assume that the SHMM for all rat-
ings corresponds to rating 0, which is reasonable
because common dialogue-acts should not affect
ratings. The obtained ratings can also be inter-
preted as <1, 2, 2, 2, 1> when we assume that
the rating of a dialogue-act is taken over from the
previous turn.
Using Concatenated Training: We have so far
presented two model topologies, one with K
SHMMs connected ergodically and the other with
K + 1 SHMMs having an additional SHMM rep-
resenting all ratings. However, we still have a
problem; that is, we need to find optimal transi-
tion probabilities between the SHMMs of different
ratings. Our solution is to use concatenated train-
ing (Lee, 1989). The procedure for concatenated
training is illustrated in Fig. 3 and has the follow-
ing three steps.
step 1 Train an SHMM M
k
(M
k
? M, 1 ?
k ? K) using dialogues D
k
, where D
k
=
Copy
Rating
1
M1
M1M0
Retrain
Train
Rating
k
M0
Mk
Mk
Retrain
Train
Rating
K
M0
MK
MK
Retrain
Train
All 
Ratings
M0
Train
+
M0
M1 Mk MK
AVG
Concatenate
M1+0 Mk+0 MK+0
M1M0 M0 Mk
M0 MK
M1+0 Mk+0 MK+0
Step 1
Step 2
Step 3
Step 2?
END
Mconcat
If the fitting has 
converged for 
all Mk+0
Split Mconcat into 
pairs again and 
retrain Mk+0
M1?MK become 
less likely to
output common 
sequences
Transition probabilities
of M0 are redistributed
between M0 and Mk
Figure 3: Three steps to combine SHMMs using
concatenated training.
{?d
i
|r(d
i
) = k}, and an SHMM M
0
using
all dialogues; i.e., D. Here, K means the
maximum level of user satisfaction and r(d
i
)
the rating assigned to d
i
.
step 2 Connect each M
k
? M with a copy of
M
0
using equal initial and transition proba-
bilities (we call this connected model M
k+0
)
and retrain M
k+0
with ?d
i
? D
k
, where
r(d
i
) = k.
step 3 Merge all models M
k+0
(1 ? k ? K) to
produce one concatenated HMM (M
concat
).
Here, the output probabilities of the copies
of M
0
are averaged over K when all models
are merged to create a combined model. If
the fitting of all M
k+0
models has converged
against the training data, exit this procedure;
otherwise, go to step 2 by connecting a copy
of M
0
and M
k
for all k. Here, the transi-
tion probabilities from M
0
to M
l
(l 6= k) are
summed and equally distributed between the
copied M
0
?s self-loop and transitions to the
states in M
k
.
In concatenated training, the transition and out-
put probabilities can be optimized between M
0
and M
k
, meaning that the output probabilities
of dialogue-act sequences that are common and
also found in M
k
can be moved from M
k
to
M
0
. This makes the distribution of M
k
sharp (not
broad/uniform), making it likely to output only
the dialogue-acts specific to a rating k. As re-
gards M
0
, its distribution of output probabilities
can also be sharpened for dialogue-acts that oc-
cur commonly in all ratings. This sharpening of
distributions is likely to be helpful in assigning
20
appropriate ratings to dialogue-act sequences. In
the next section, we experimentally examine how
these proposed HMMs perform in modeling and
predicting user satisfaction transitions in dialogue.
4 Experiment
To verify our approach, we first prepared dialogue
data. Then, we trained our HMMs and compared
them with a random baseline and an upper bound
that uses a supervised approach; that is, an HMM
is trained using reference labels on the dialogue-
act level.
4.1 Dialogue Data
We used dialogues in two domains; the animal
discussion (AD) domain and the attentive listen-
ing (AL) domain. All dialogues are in Japanese.
In both domains, the data we used were text dia-
logues. We did not use spoken dialogue data be-
cause we wanted to avoid particular problems of
voice, such as filled pauses and overlaps, although
we aim to deal with spoken dialogue in the future.
4.1.1 Animal Discussion
We used the dialogue data in the AD domain that
we previously collected (Higashinaka et al, 2008).
In this domain, the system and user talk about likes
and dislikes about animals via a text chat inter-
face. The data consist of 1000 dialogues between
a dialogue system and 50 human users. Each
user conversed with the system 20 times, includ-
ing two example dialogues at the beginning. All
user/system utterances have been annotated with
dialogue-acts. There are 29 dialogue-act types in-
cluding those related to self-disclosure, question,
response, and greetings. For example, a dialogue-
act DISC-P denotes one?s self-disclosure about a
proposition P. Here, P is either like(X,A) or
dislike(X,A) where X is a conversational par-
ticipant and A a certain animal. DISC-R denotes
one?s self-disclosure of a reason for a proposition.
See (Higashinaka et al, 2008) for the details of the
dialogue-acts.
For our experiment, we created two subsets of
the data. We first extracted 180 dialogues by
taking all 18 non-example dialogues for the ini-
tial ten users sorted by user ID (AD-SUB1; 4147
user dialogue-acts and 6628 system dialogue-
acts). Then, from AD-SUB1, we randomly ex-
tracted nine dialogues per user to form another
subset of 90 dialogues (AD-SUB2; 2050 user
dialogue-acts and 3290 system dialogue-acts). An
annotator, who was not one of the authors, la-
beled AD-SUB1 with dialogue-level user satis-
faction ratings and AD-SUB2 with utterance-level
ratings. More specifically, each dialogue/utterance
Utterance (dialogue-acts) Sm Cl Wi
SYS Do you like rabbits? (DA: Q-DISC-P) 6 6 6
USR I like rabbits. They are cute.
(DA: DISC-P, DISC-R)
SYS Indeed they are cute. (DA: REPEAT) 6 6 6
SYS Tell me why you like rabbits. 6 5 6
(DA: Q-DISC-R-OTHER)
USR I like them because they are small and
warm. (DA: DISC-P-R)
SYS You like them because they are warm. 7 5 7
(DA: REPEAT)
Overall rating for the dialogue 7 5 6
Figure 4: Excerpt of a dialogue with utterance-
level user satisfaction ratings for smoothness
(Sm), closeness (Cl), and willingness (Wi) in the
AD domain. SYS and USR denote system and
user, respectively. The dialogue was translated by
the authors.
was given three different user satisfaction rat-
ings related to ?Smoothness of the conversation?,
?Closeness perceived by the user towards the sys-
tem?, and ?Willingness to continue the conversa-
tion?. The ratings ranged from 1 to 7, where 1
is the worst and 7 the best (see Fig. 4 for exam-
ples of utterance-level and overall ratings given by
the annotator for an excerpt of a dialogue). In a
manner similar to (Evanini et al, 2008), we used a
third-person?s user satisfaction rating for the sake
of consistency.
For utterance-level ratings, the annotator care-
fully read each utterance and gave ratings after
each system utterance according to how she would
have felt after receiving each system utterance if
she had been the user in the dialogue. To make
the situation more realistic, she was not allowed
to look down at the dialogue after the current ut-
terance. At the beginning of a dialogue, the rat-
ings always started from four (neutral). When the
annotator gave dialogue-level ratings, she looked
through the entire dialogue and rated its quality
(smoothness, closeness, and willingness) accord-
ing to how she would have felt after having had
the dialogue in question.
4.1.2 Attentive Listening
We collected human-human listening-oriented di-
alogues in a manner similar to (Meguro et al,
2009). In this AL domain, a listener attentively
listens to the other in order to satisfy the speaker?s
desire to speak and to make himself/herself heard.
We collected such listening-oriented dialogues us-
ing a website where users taking the roles of lis-
teners and speakers were matched up to have con-
versations. There were ten listeners who always
stayed at the website and 37 speakers who could
talk to them anytime the listeners were available.
They were all paid for their participation. A con-
versation was done through a text-chat interface.
21
The use of facial and other non-linguistic expres-
sions were not allowed for analysis purposes. The
participants were instructed to end the conversa-
tion approximately after ten minutes. Within a
three-week period, each speaker was instructed to
have at least two conversations a day, resulting in
our collecting 1260 listening-oriented dialogues.
Two independent annotators labeled each utter-
ance with 40 dialogue-act types, including those
related to self-disclosure, question, internal argu-
ment, sympathy, and information giving. The
inter-annotator agreement was reasonable, with
0.57 in Cohen?s ?. Although we cannot describe
the full details of our dialogue-acts for lack of
space, we have dialogue-acts DISC-EVAL-POS for
one?s self-disclosure of his/her positive evalua-
tion towards a certain entity, DISC-EXP for one?s
self-disclosure of his/her experience, and SELF-Q-
DESIRE for one?s internal argument about his/her
desire (e.g., ?Have I ever wanted to go abroad??).
We used the dialogue-act annotation of one of the
annotators in this work.
An annotator gave dialogue-level user satis-
faction ratings to all 1260 dialogues (AL-ALL;
31779 speaker dialogue-acts and 28681 listener
dialogue-acts). Then, we made a subset of the
data by randomly selecting ten dialogues for
each of the ten listeners to obtain 100 dialogues
(AL-SUB1; 2453 speaker dialogue-acts and 2197
listener dialogue-acts). Finally, the annotator
gave utterance-level ratings to AL-SUB1. The
utterance-level ratings were given only after lis-
teners? utterances. The annotator gave three rat-
ings as in the AD domain; namely, smoothness,
closeness, and good listening. Instead of willing-
ness, we have a ?good listener? criterion asking
for how good the annotator thinks the listener is
from the viewpoint of attentive listening; for ex-
ample, how well the listener is making it easy for
the speaker to speak. All ratings ranged from 1 to
7. See Fig. 5 for a sample dialogue in the AL do-
main with utterance-level and overall ratings given
by the annotator.
4.2 Training HMMs
From the dialogue data and their dialogue-level
ratings, we created our proposed HMMs. We had
five topology variations:
ergodic0: The simple ergodic model with no ad-
ditional SHMM for all ratings. See Fig.
1 for the topology. This HMM has 7
SHMMs connected ergodically with equal
initial/transition probabilities.
ergodic1: The simple ergodic model with an ad-
ditional SHMM for all ratings. See Fig. 2
for the topology. This HMM has 8 (7 +
Utterance (dialogue-acts) Sm Cl GL
LIS You know, in spring, Japanese food tastes de-
licious. (DA: DISC-EVAL-POS)
5 5 5
SPK This time every year, I make a plan to go on
a healthy diet. But . . . (DA: DISC-HABIT)
LIS Uh-huh (DA: ACK) 6 5 6
SPK The temperature goes up suddenly!
(DA: INFO)
SPK It?s always too late! (DA: DISC-EVAL-NEG)
LIS Clothing worn gets less and less while not be-
ing able to lose weight. (DA: DISC-FACT)
6 6 6
SPK Well, people around me soon get used to my
body shape though. (DA: DISC-FACT)
Overall rating for the dialogue 7 7 7
Figure 5: Excerpt of a dialogue with utterance-
level user satisfaction ratings for smoothness
(Sm), closeness (Cl), and good listener (GL) in the
AL domain. SPK and LIS denote speaker and lis-
tener, respectively. Both the speaker and listener
are human.
1) SHMMs connected ergodically with equal
initial/transition probabilities.
ergodic2: Same as ergodic1 except that the num-
ber of common states is doubled so that com-
mon dialogue-act sequences can be more ac-
curately modeled. Note that without concate-
nated training, SHMMs for each rating may
also have sharp distributions for common se-
quences. One possible solution to avoid this
is to sharpen the distributions of common
states by increasing its number of states.
concat1: 8 (7 + 1) SHMMs combined using con-
catenated training. See Fig. 3 for the topol-
ogy.
concat2: Same as concat1 except that the number
of common states is doubled.
[See Appendices A and B for the actual examples
of the obtained models]
4.2.1 Baseline and Upper Bound
We created the following baseline (random) and
upper bound (supervised) models for comparison:
random: This outputs ratings 1?7 at random.
supervised: This is an HMM trained in a man-
ner similar to (Engelbrecht et al, 2009). This
model is the same as ergodic0 in topology but
different in that the initial, transition, and out-
put probabilities are trained in a supervised
manner using the dialogue-acts and dialogue-
act-level reference ratings in AD-SUB2 and
AL-SUB1. Since we only have ratings for
system/listener utterances in the corpora, in
order to make training data, we assumed that
the ratings for dialogue-acts corresponding
to user/speaker utterances were the same as
22
those after the previous system/listener utter-
ances. This model simulates the ideal situ-
ation where we possess user satisfaction rat-
ings for all dialogue-acts in the data.
4.3 Evaluation Procedure
We performed a ten-fold cross validation. We first
separated utterance-level labeled data (i.e., AD-
SUB2 or AL-SUB1) into 10 disjoint sets. Then,
for each set S, we used dialogue-level labeled
data (i.e., AD-SUB1 or AL-ALL) excluding S
for training HMMs. Here, ?supervised? only used
the utterance-level labeled data excluding S for
training. Then, we made the models (i.e., er-
godic0, ergodic1, ergodic2, concat1, concat2, ran-
dom and supervised) output rating sequences for
the dialogue-acts in S and evaluated them with the
reference ratings in S. We repeated this process
ten times to evaluate the overall performance.
Since utterance-level ratings are provided only
after system/listener utterances, we only evaluated
ratings after dialogue-acts corresponding to sys-
tem/listener utterances. When a system/listener
utterance contained multiple dialogue-acts, the
dialogue-acts were assumed to have the same rat-
ing as that utterance. When the output rating
sequences contain 0, which can be the case for
ergodic1?2 and concat1?2, the 0 is replaced by the
most previous non-zero rating. When 0 is found at
the beginning of a dialogue, it remained 0. Al-
though our reference ratings always started with
four (cf. Section 4.1.1), we did not use this in-
formation to fill initial zeros because we wanted
to evaluate the prediction accuracy when we do
not have any prior knowledge. Since some mod-
els may benefit from avoiding evaluating dialogue-
acts at the beginning because of these zeros, we
simply compared the rating sequences where all
models produced non-zero values. For exam-
ple, when we have three output rating sequences
<0,5,6,0,4>, <0,0,1,2,0>, and <1,2,3,4,5> for a
given dialogue-act sequence, the zeros that follow
non-zero values are first filled with their preceed-
ing values, and thereby we obtain <0,5,6,6,4>,
<0,0,1,2,2>, and <1,2,3,4,5>. Then, by cropping
the common non-zero span, we obtain <6,6,4>,
<1,2,2>, and <3,4,5>, and use these rating se-
quences for evaluation.
4.3.1 Evaluation Criteria
We used two kinds of evaluation criteria: one for
evaluating individual matches and the other for
evaluating distributions.
Evaluating Individual Matches: We used the
match rate and mean absolute error to evaluate the
matching of reference and hypothesis rating se-
quences. They are derived by the equations shown
below. In the equations, R (= {R
1
. . .R
L
}) and
H (= {H
1
. . .H
L
}) denote reference and hypoth-
esis rating sequences for a dialogue, respectively.
L is the length of R and H (Note that they have
the same length).
? Match Rate (MR)
MR(R, H) = 1L
L
?
i=1
match(R
i
, H
i
), (1)
where ?match? returns 1 or 0 depending on
whether a rating in R matches that in H .
? Mean Absolute Error (MAE)
MAE(R, H) = 1L
L
?
i=1
|R
i
? H
i
|. (2)
Evaluating Distributions: In generative mod-
els, it is important that the output distribution
matches that of the reference. Therefore, we ad-
ditionally use Kullback-Leibler divergence, match
rate per rating, and mean absolute error per rat-
ing. The Kullback-Leibler divergence evaluates
the shape of output distributions. The match rate
per rating and mean absolute error per rating eval-
uate how accurately each individual rating can
be predicted; namely, the accuracy for predict-
ing dialogue-acts with one rating is equally val-
ued with those for other ratings irrespective of the
distribution of ratings in the reference. It is im-
portant to use these metrics in the practical as well
as information theoretic sense because it is no use
predicting only easy-to-guess ratings; we should
be able to correctly predict rare but still important
cases. For example, rating 1 in human-human di-
alogue is quite rare; however, predicting it is very
important for detecting problematic situations in a
dialogue.
? Kullback-Leibler Divergence (KL)
KL(R,H) =
K
?
r=1
P(H, r) ? log(P(H, r)
P(R, r)), (3)
where K is the maximum user satisfaction rating
(i.e. 7 in this experiment),R andH denote the se-
quentially concatenated reference/hypothesis rat-
ing sequences of the entire dialogues, and P(?, r)
denotes the occurrence probability that a rating r
is found in an arbitrary rating sequence.
? Match Rate per rating (MR/r)
MR/r(R,H) = 1K
K
?
r=1
?
i?{i|R
i
=r}
match(R
i
,H
i
)
?
i?{i|R
i
=r}
1
,
(4)
23
Criterion random ergodic0 ergodic1 ergodic2 concat1 concat2 supervised
Smoothness
MR 0.142
e0e1
0.111 0.111 0.157
e0e1
0.153 0.199
e0e1r
0.275
c1e0e1e2r
MAE 1.988
e0e1
2.212 2.212 1.980 1.936
e0e1
1.870
e0e1
1.420
c1c2e0e1e2r
KL 0.287 0.699 0.699 0.562 0.280 0.369 0.162
MR/r 0.143 0.137 0.137 0.176 0.136 0.177 0.217
MAE/r 2.286 2.414 2.414 2.152 2.301 2.206 1.782
Closeness
MR 0.143 0.129 0.129 0.171
e0e1
0.174 0.189
e0e1
0.279
c1c2e0e1e2r
MAE 2.028 2.066 2.066 1.964 1.798
e0e1r
1.886 1.431
c1c2e0e1e2r
KL 0.195 0.449 0.449 0.261 0.138 0.263 0.092
MR/r 0.143 0.156 0.156 0.170 0.155 0.164 0.231
MAE/r 2.283 2.236 2.236 2.221 2.079 2.067 1.702
Willingness
MR 0.143
e0e1
0.112 0.112 0.180
e0e1
0.152 0.183
e0e1
0.283
c1c2e0e1e2r
MAE 2.005 2.133 2.133 1.962 1.801
e0e1r
1.882 1.403
c1c2e0e1e2r
KL 0.225 0.568 0.568 0.507 0.238 0.255 0.125
MR/r 0.143 0.152 0.152 0.192 0.181 0.167 0.224
MAE/r 2.286 2.258 2.258 2.107 1.958 2.164 1.705
Table 1: The match rate (MR), mean absolute error (MAE), Kullback-Leibler divergence (KL), match
rate per rating (MR/r) and mean absolute error per rating (MAE/r) for our proposed HMMs, the random
baseline, and the upper bound (supervised) for the AD domain. ?e0?e2?, ?c1?c2?, and ?r? indicate the sta-
tistical significance (p<0.01) over ergodic0?2, concat1?2, and random, respectively. Bold font indicates
the best value within each row (except for ?supervised?).
whereR
i
andH
i
denote ratings at i-th positions.
? Mean Absolute Error per rating (MAE/r)
MAE/r(R,H) = 1K
K
?
r=1
?
i?{i|R
i
=r}
|R
i
?H
i
|
?
i?{i|R
i
=r}
1
.
(5)
4.4 Evaluation Results
Tables 1 and 2 show the evaluation results for the
AD and AL domains, respectively. The MR and
MAE values are averaged over all dialogues. To
compare the means of the MR and MAE, we per-
formed a non-parametric multiple comparison test
[Steel-Dwass test (Dwass, 1960)]. We did not per-
form a statistical test for other criteria because it
was difficult to perform sample-wise comparison
for distributions. Naturally, ?supervised? is the
best performing model for all criteria in both do-
mains. Therefore, we focus on how much our pro-
posed models differ from the baseline (random)
and the upper bound (supervised).
In the AD domain, we find that ergodic0 and er-
godic1 performed rather poorly and concat1 and
concat2 performed fairly well, significantly out-
performing the random baseline. However, it is
also clear that we still need a great deal of im-
provement for our models to reach the level of
?supervised?. A promising sign is that concat2
is not significantly different from ?supervised? in
smoothness. Here, ergodic0 and ergodic1 re-
turned the exact same results. This means that the
state transition paths did not go through the com-
mon states at all in ergodic1, suggesting that the
common states in ergodic1 have very broad out-
put distributions and the optimal path could not
go through the common states, instead preferring
other states having sharper distributions. How-
ever, this phenomenon was rightly avoided by in-
troducing more common states as seen in the re-
sults for ergodic2; nonetheless, as the results for
concat1 and concat2 indicate, the transition prob-
abilities have to be trained appropriately to obtain
better results.
In the AL domain, although the tendency of
the evaluation results is the same as that for the
AD domain, concat2 is clearly the best perform-
ing model. It outperformed other models in al-
most all cases except for ?Good Listener? for
which concat1 performed better. In fact, the MR/r
and MAE/r of concat1 are quite close to those of
?supervised?, suggesting the potential of our ap-
proach.
Overall, although we still need further improve-
ment in order for our models to be closer to the
upper bound, we showed that we can, to some ex-
tent, predict user satisfaction transitions in a dia-
logue only from overall ratings of dialogues using
our proposed HMMs. We also showed that model
topologies and learning methods can make signif-
icant differences. Especially, we found the intro-
duction of common states to be crucial in making
appropriate models for prediction. Since our mod-
els, especially concat2, significantly outperformed
the baseline, we believe that our approach can be
one of the viable options for automatically predict-
ing user satisfaction transitions when there exist
only overall rating data.
5 Summary and Future Work
We presented a novel approach for modeling user
satisfaction transitions only from dialogues with
overall ratings. The experimental results show that
it is possible to predict user satisfaction transi-
24
Criterion random ergodic0 ergodic1 ergodic2 concat1 concat2 supervised
Smoothness
MR 0.143
e0e1e2
0.069 0.069 0.131
e0e1
0.173
e0e1
0.243
c1e0e1e2r
0.439
c1c2e0e1e2r
MAE 1.868
e0e1e2
2.519 2.519 2.433 1.687
e0e1e2r
1.594
e0e1e2r
0.802
c1c2e0e1e2r
KL 0.989 2.253 2.253 2.319 0.851 0.753 0.087
MR/r 0.141 0.118 0.118 0.156 0.161 0.167 0.231
MAE/r 2.289 2.500 2.500 2.492 2.093 2.077 1.868
Closeness
MR 0.143
e0e1
0.050 0.050 0.175
e0e1
0.158
e0e1
0.263
c1e0e1e2r
0.425
c1c2e0e1e2r
MAE 1.849
e0e1e2
2.357 2.357 2.316 1.778
e0e1e2
1.562
e0e1e2r
0.890
c1c2e0e1e2r
KL 1.022 2.137 2.137 2.220 1.155 0.909 0.109
MR/r 0.143 0.090 0.090 0.122 0.117 0.159 0.237
MAE/r 2.281 2.577 2.577 2.811 2.260 2.039 1.972
Good Listener
MR 0.143
e0e1
0.075 0.075 0.145
e0e1
0.199
e0e1
0.206
e0e1e2
0.422
c1c2e0e1e2r
MAE 1.890
e0e1e2
2.237 2.237 2.150 1.634
e0e1e2r
1.634
e0e1e2r
0.852
c1c2e0e1e2r
KL 0.945 1.738 1.738 1.782 0.924 0.824 0.087
MR/r 0.143 0.121 0.121 0.184 0.224 0.200 0.227
MAE/r 2.284 2.358 2.358 2.236 1.911 2.083 1.769
Table 2: Evaluation results for the AL domain. See Table 1 for the notations in the table.
tions to some extent by our approach and that in-
troducing common states and concatenated train-
ing can significantly improve prediction accuracy.
For improvement, we plan to explore new dialogic
features for emissions, different topologies, and
other optimization functions, such as discrimina-
tive ones. We also need to validate our approach
using dialogue-act recognition results instead of
hand-labeled dialogue-acts. We also want to ap-
ply our approach to sequence mining in dialogues
where we have categories instead of ratings for di-
alogues. It is also necessary to test whether our
HMMs can be generalized over different raters,
since user satisfaction ratings may differ greatly
among individuals. Although there remain such
issues, we believe we have presented a new di-
rection in automatic evaluation of dialogues and
the experimental results show that our approach is
promising.
References
Meyer Dwass. 1960. Some k-sample rank-order tests. In
Ingram Olkin et al, editor, Contributions to Probability
and Statistics, pages 198?202. Stanford University Press.
Klaus-Peter Engelbrecht, Florian Go?dde, Felix Hartard,
Hamed Ketabdar, and Sebastian Mo?ller. 2009. Model-
ing user satisfactionwith hidden Markov models. In Proc.
SIGDIAL, pages 170?177.
Keelan Evanini, Phillip Hunter, Jackson Liscombe, David
Suendermann, Krishna Dayanidhi, and Roberto Pierac-
cini. 2008. Caller experience: A method for evaluating
dialog systems and its automatic prediction. In Proc. SLT,
pages 129?132.
Allen L. Gorin, Giuseppe Riccardi, and Jerry H. Wright.
1997. How may I help you? Speech Communication,
23(1-2):113?127.
Sunao Hara, Norihide Kitaoka, and Kazuya Takeda. 2010.
Estimation method of user satisfaction using N-gram-
based dialog history model for spoken dialog system. In
Proc. LREC, pages 78?83.
Ota Herm, Alexander Schmitt, and Jackson Liscombe. 2008.
When calls go wrong: How to detect problematic calls
based on log-files and emotions? In Proc. INTER-
SPEECH, pages 463?466.
Ryuichiro Higashinaka, Kohji Dohsaka, and Hideki Isozaki.
2008. Effects of self-disclosure and empathy in human-
computer dialogue. In Proc. SLT, pages 109?112.
Julia Hirschberg, Diane Litman, and Marc Swerts. 2004.
Prosodic and other cues to speech recognition failures.
Speech Communication, 43:155?175.
Chiori Hori, Kiyonori Ohtake, Teruhisa Misu, Hideki Kash-
ioka, and Satoshi Nakamura. 2008. Dialog management
using weighted finite-state transducers. In Proc. INTER-
SPEECH, pages 211?214.
Naoki Isomura, Fujio Toriumi, and Kenichiro Ishii. 2009.
Evaluation method of non-task-oriented dialogue system
usingHMM. IEICE Transactions on Information and Sys-
tems, J92-D(4):542?551.
Woosung Kim. 2007. Online call quality monitoring for
automating agent-based call centers. In Proc. INTER-
SPEECH, pages 130?133.
Kai-Fu Lee. 1989. Automatic speech recognition: the de-
velopment of the SPHINX system. Kluwer Academic Pub-
lishers.
ToyomiMeguro, Ryuichiro Higashinaka,Kohji Dohsaka,Ya-
suhiro Minami, and Hideki Isozaki. 2009. Analysis of
listening-oriented dialogue for building listening agents.
In Proc. SIGDIAL, pages 124?127.
Yasuhiro Minami, Akira Mori, Toyomi Meguro, Ryuichiro
Higashinaka, Kohji Dohsaka, and Eisaku Maeda. 2009.
Dialogue control algorithm for ambient intelligence based
on partially observable Markov decision processes. In
Proc. IWSDS, pages 254?263.
Sebastian Mo?ller, Klaus-Peter Engelbrecht, and Robert
Schleicher. 2008. Predicting the quality and usability of
spoken dialogue services. Speech Communication, 50(8-
9):730?744.
Lawrence R. Rabiner. 1990. A tutorial on hidden Markov
models and selected applications in speech recognition.
Readings in speech recognition, 53(3):267?296.
Katsuhiko Shirai. 1996. Modeling of spoken dialogue with
and without visual information. In Proc. ICSLP, vol-
ume 1, pages 188?191.
Marilyn A. Walker, Diane Litman, Candace A. Kamm, and
Alicia Abella. 1997. PARADISE: A framework for evalu-
ating spoken dialogue agents. In Proc. EACL, pages 271?
280.
Marilyn A. Walker, Irene Langkilde-Geary, Helen Wright
Hastie, Jerry Wright, and Allen Gorin. 2002. Automat-
ically training a problematic dialogue predictor for a spo-
ken dialogue system. Journal of Artificial Intelligence Re-
search, 16(1):293?319.
Jason D. Williams and Steve Young. 2007. Partially ob-
servableMarkov decision processes for spokendialog sys-
tems. Computer Speech & Language, 21(2):393?422.
25
Appendix A. HMM obtained by concat2 for Willingness rating in the AD domain.
This HMM is the model obtained for one of the folds in the experiment. Square and oval states emit
a system?s dialogue-act and a user?s dialogue-act, respectively. Emissions (dialogue-acts) are shown in
each state as a table with their probabilities. Only the emissions and transitions over the probability of
0.1 are displayed for the sake of brevity. Here, ?pi? denotes initial probability.
ra
ti
ng
:0
ra
ti
ng
:1
ra
ti
ng
:2
ra
ti
ng
:3
ra
ti
ng
:4
ra
ti
ng
:5
ra
ti
ng
:6
ra
ti
ng
:7
SY
ST
EM
 (p
i: 
0.
00
)
A
CK
0.
35
D
IS
C-
A
G
RE
E-
P
0.
14
D
IS
C-
D
IS
A
G
RE
E-
P
0.
14
R
EP
EA
T
0.
10
SY
ST
EM
 (p
i: 
1.
00
)
D
IS
C
-R
-O
TH
ER
0.
17
Q-
DI
SC
-R
0.
23
Q-
DI
SC
-R
-O
TH
ER
0.
24
0.
35
U
SE
R
 (p
i: 
0.
00
)
A
CK
0.
14
D
IS
C
-R
-O
TH
ER
0.
300.
13
0.
37
U
SE
R
 (p
i: 
0.
00
)
D
IS
C
-P
0.
40
D
IS
C
-R
0.
26
0.
31
0.
61
0.
12
0.
49
0.
15
SY
ST
EM
 (p
i: 
0.
00
)
A
CK
0.
81
EM
P
0.
11
0.
27
0.
26
U
SE
R
 (p
i: 
0.
00
)
A
CK
0.
30
D
IS
C
-O
TH
ER
0.
14
O
TH
ER
0.
27
Q-
DI
SC
-P
0.
10
0.
33
0.
12
0.
26
0.
56
SY
ST
EM
 (p
i: 
0.
00
)
A
CK
0.
77
Q-
DI
SC
-P
0.
14
0.
21
0.
22
0.
22
0.
11
U
SE
R
 (p
i: 
0.
00
)
O
TH
ER
0.
29
Q-
DI
SC
-O
TH
ER
0.
20
Q-
DI
SC
-R
0.
23
0.
23
0.
23
0.
16
0.
59
SY
ST
EM
 (p
i: 
0.
00
)
D
IS
C-
A
G
RE
E-
P
0.
33
D
IS
C
-P
0.
13
G
O
O
D
BY
E
0.
11
Q-
DI
SC
-P
0.
22
Q-
DI
SC
-P
-O
PE
N
0.
18
0.
19
0.
22
U
SE
R
 (p
i: 
0.
00
)
D
IS
C
-P
0.
46
O
TH
ER
0.
26
Q-
DI
SC
-R
0.
18
0.
46
0.
62
0.
12
0.
21
SY
ST
EM
 (p
i: 
0.
00
)
D
IS
C-
D
IS
A
G
RE
E-
P
0.
46
D
IS
C
-P
0.
20
D
IS
C
-P
-R
0.
12
G
O
O
D
BY
E
0.
15
0.
54
0.
19
U
SE
R
 (p
i: 
0.
00
)
D
IS
C
-P
-R
0.
10
G
RE
ET
IN
G
0.
14
Q-
DI
SC
-O
TH
ER
0.
21
Q-
DI
SC
-R
0.
19
R
ES
0.
10
0.
22
0.
61
0.
200
.1
5
SY
ST
EM
 (p
i: 
0.
00
)
D
IS
C-
A
G
RE
E-
P
0.
28
D
IS
C
-R
0.
10
EM
P
0.
14
R
EP
EA
T
0.
17
0.
51
0.
15
0.
14
SY
ST
EM
 (p
i: 
0.
00
)
D
IS
C-
D
IS
A
G
RE
E-
P
0.
35
D
IS
C
-P
0.
13
D
IS
C
-R
0.
15
Q-
DI
SC
-P
-O
PE
N
0.
15
0.
25 0
.1
1
0.
32
0.
16
U
SE
R
 (p
i: 
0.
00
)
D
IS
C
-D
IS
A
G
R
EE
-O
TH
ER
0.
16
D
IS
C
-O
TH
ER
0.
27
D
IS
C
-P
-R
0.
14
EM
P
0.
18
R
EP
EA
T
0.
16
0.
17
0.
52
0.
17
0.
19
SY
ST
EM
 (p
i: 
0.
00
)
D
IS
C
-P
0.
14
D
IS
C
-R
0.
15
D
IS
C
-R
-O
TH
ER
0.
45
G
O
O
D
BY
E
0.
11
Q-
DI
SC
-P
0.
16
0.
16
0.
16
0.
18
U
SE
R
 (p
i: 
0.
00
)
A
CK
0.
31
D
IS
C
-O
TH
ER
0.
23
EM
P
0.
10
R
EP
EA
T
0.
10
0.
46
0.
51
0.
20
U
SE
R
 (p
i: 
0.
00
)
D
IS
C
-R
-O
TH
ER
0.
41
G
O
O
D
BY
E
0.
12
Q-
DI
SC
-O
TH
ER
0.
22
R
EP
EA
T
0.
12
0.
37
0.
16
0.
45
26
Appendix B. HMM obtained by concat1 for Good Listener rating in the AL domain.
This HMM is the model obtained for one of the folds in the experiment. Square and oval states emit a lis-
tener?s dialogue-act and a speaker?s dialogue-act, respectively. We find DICS-EVAL-NEG (self-disclosure
of one?s evaluation with a negative polarity) in the rating score 1 and DICS-EVAL-POS in the rating score
7, indicating that it may be better to make speakers talk about positive evaluations to be a good listener.
ra
ti
ng
:0
ra
ti
ng
:1
ra
ti
ng
:2
ra
ti
ng
:3
ra
ti
ng
:4
ra
ti
ng
:5
ra
ti
ng
:6
ra
ti
ng
:7
LI
ST
EN
ER
 (p
i: 
0.1
6)
G
RE
ET
IN
G
0.
13
D
IS
C-
FA
CT
0.
10
D
IS
C-
EV
A
L-
PO
S
0.
11
0.
22
SP
EA
K
ER
 (p
i: 
0.
41
)
G
RE
ET
IN
G
0.
13
D
IS
C-
FA
CT
0.
15
SY
N
PA
TH
Y
0.
15
0.
35
0.
32
0.
26
L
IS
TE
N
ER
 (p
i: 
0.
01
)
G
RE
ET
IN
G
0.
28
D
IS
C-
FA
CT
0.
19
IN
FO
0.
15
0.
12
0.
24
0.
32
SP
EA
K
ER
 (p
i: 
0.
14
)
G
RE
ET
IN
G
0.
23
Q-
FA
CT
0.
13
D
IS
C-
EV
A
L-
N
EG
0.
12
0.
33
0.
24
0.
12
0.
32
0.
33
L
IS
TE
N
ER
 (p
i: 
0.
00
)
D
IS
C-
FA
CT
0.
21
IN
FO
0.
18
0.
16
0.
25
0.
23
SP
EA
K
ER
 (p
i: 
0.
00
)
IN
FO
0.
22
Q-
FA
CT
0.
12
0.
36
0.
27
0.
14
0.
35
0.
24
L
IS
TE
N
ER
 (p
i: 
0.
00
)
D
IS
C-
FA
CT
0.
32
Q-
FA
CT
0.
24
TH
A
N
K
0.
10
Q-
IN
FO
0.
11
0.
15
0.
34
0.
18
SP
EA
K
ER
 (p
i: 
0.
00
)
D
IS
C-
FA
CT
0.
34
D
IS
C-
EV
A
L-
N
EG
0.
12
0.
33
0.
39
0.
13
0.
21
0.
27
L
IS
TE
N
ER
 (p
i: 
0.
04
)
G
RE
ET
IN
G
0.
23
SY
N
PA
TH
Y
0.
16
Q-
FA
CT
0.
14
0.
18
0.
38
0.
22
SP
EA
K
ER
 (p
i: 
0.
10
)
G
RE
ET
IN
G
0.
31
Q-
FA
CT
0.
10
D
IS
C-
EV
A
L-
N
EG
0.
15
0.
23
0.
24
0.
16
0.
41
0.
19
L
IS
TE
N
ER
 (p
i: 
0.
00
)
SY
N
PA
TH
Y
0.
17
D
IS
C-
EV
A
L-
PO
S
0.
20
0.
16
0.
23
0.
23
SP
EA
K
ER
 (p
i: 
0.
00
)
D
IS
C-
FA
CT
0.
27
SY
N
PA
TH
Y
0.
20
D
IS
C-
EV
A
L-
PO
S
0.
17
0.
39
0.
22
0.
15
0.
31
0.
32
L
IS
TE
N
ER
 (p
i: 
0.
00
)
SY
N
PA
TH
Y
0.
19
D
IS
C-
EV
A
L-
PO
S
0.
24
CO
N
FI
RM
0.
14
0.
21
0.
24
0.
22
SP
EA
K
ER
 (p
i: 
0.
00
)
SY
N
PA
TH
Y
0.
26
D
IS
C-
EV
A
L-
PO
S
0.
23
IN
FO
0.
25
D
IS
C-
EX
P
0.
11
0.
33
0.
21
0.
21
0.
29
0.
29
L
IS
TE
N
ER
 (p
i: 
0.
04
)
G
RE
ET
IN
G
0.
24
D
IS
C-
EV
A
L-
PO
S
0.
22
IN
FO
0.
11
CO
N
FI
RM
0.
13
0.
17
0.
24
0.
22
SP
EA
K
ER
 (p
i: 
0.
10
)
G
RE
ET
IN
G
0.
18
D
IS
C-
FA
CT
0.
26
D
IS
C-
EV
A
L-
PO
S
0.
16
IN
FO
0.
18
0.
38
0.
26
0.
12
0.
31
0.
30
27
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 314?321,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
User-adaptive Coordination of Agent Communicative Behavior
in Spoken Dialogue
Kohji Dohsaka
NTT Communication Science Laboratories
NTT Corporation
2-4, Hikaridai, Seika-cho,
Kyoto 619-0237, Japan
Atsushi Kanemoto
Graduate School of
Information Science and Technology
Osaka University, 1-1, Yamadaoka,
Suita, Osaka 565-0871, Japan
Ryuichiro Higashinaka
NTT Cyber Space Laboratories
NTT Corporation
1-1, Hikarinooka, Yokosuka,
Kanagawa 239-0847, Japan
Yasuhiro Minami and Eisaku Maeda
NTT Communication Science Laboratories
NTT Corporation
2-4, Hikaridai, Seika-cho,
Kyoto 619-0237, Japan
{dohsaka,minami,maeda}@cslab.kecl.ntt.co.jp
higashinaka.ryuichiro@lab.ntt.co.jp
Abstract
In this paper, which addresses smooth spo-
ken interaction between human users and
conversational agents, we present an ex-
perimental study that evaluates a method
for user-adaptive coordination of agent
communicative behavior. Our method
adapts the pause duration preceding agent
utterances and the agent gaze duration to
reduce the discomfort perceived by indi-
vidual users during interaction. The exper-
imental results showed a statistically sig-
nificant tendency: the duration of the agent
pause and the gaze converged during inter-
action with the method. The method also
significantly improved the perceived rele-
vance of the agent communicative behav-
ior.
1 Introduction
Conversational agents have been studied as an ef-
fective human-computer interface for such pur-
poses as training decision-making in team ac-
tivities (Traum and Rickel, 2002), learning sup-
port (Johnson et al, 2002), museum guides (Kopp
et al, 2005), and community facilitators (Zheng
et al, 2005; Fujie et al, 2009). They will play
a crucial role in establishing a society where hu-
mans and robots collaborate through natural in-
teraction. However, agents cannot produce their
intended effects when the smooth flow of interac-
tion is disturbed. To fully exploit the promise of
agents, we need to achieve smooth interaction be-
tween human users and agents.
Although various types of modalities have been
used in human-computer interfaces, speech has
drawn a great deal of interest because it is one of
the most pervasive communication methods in our
daily lives and we usually perform it without any
special effort (Nass and Brave, 2005). In this pa-
per, we are interested in smooth spoken dialogues
between users and agents.
A spoken dialogue is a joint activity among
participants (Clark, 1996). For such a joint ac-
tivity to be smooth and successful, participants
need to coordinate their communicative behav-
iors in various ways. In human dialogues, par-
ticipants agree on lexical choices to refer to ob-
jects (Brennan and Clark, 1996) and coordinate
eye gaze (Richardson and Dale, 2005) and whose
turn it is to speak (Sacks et al, 1974). They
become more similar to their partners as the di-
alogue proceeds in many aspects such as pitch,
speech rate, and pause structure (Burgoon et al,
1995; Hayashi et al, 2009). Such coordination
serves to make conversation flow easily and intel-
ligibly (Garrod and Pickering, 2004).
The coordination of communicative behaviors
also plays a crucial role in smooth human-agent
interaction. Previous work addressed human be-
havior adaptation to agents (Oviatt et al, 2004),
agent behavior adaptation to human partners (Mit-
sunaga et al, 2005; Tapus and Mataric?, 2007), and
the mutual adaptation of human and agent behav-
ior (Breazeal, 2003).
In this paper, which addresses smooth spoken
interaction between human users and agents, we
focus on the adaptation of agent communicative
behavior to individual users in spoken dialogues
314
with flexible turn-taking. We present a method
for user-adaptive coordination of agent commu-
nicative behavior to reduce the discomfort per-
ceived by individual users during the interaction
and show experimental results that evaluate how
the method influences agent communicative be-
havior and improves its relevance as perceived by
users. For evaluation purposes, we used a quiz-
style multi-party spoken dialogue system (Minami
et al, 2007; Dohsaka et al, 2009). A quiz-style
dialogue is a kind of thought-evoking dialogue
that can stir user thinking and activate communi-
cation (Higashinaka et al, 2007a; Dohsaka et al,
2009). This characteristic is expected to be ad-
vantageous for evaluation experiments since it en-
courages involvement in the dialogue.
Our method adapts agent communicative be-
havior based on policy gradient reinforcement
learning (Sutton et al, 2000; Kohl and Stone,
2004). The policy gradient method has been
used for robot communicative behavior adapta-
tion (Mitsunaga et al, 2005; Tapus and Mataric?,
2007). However, both studies dealt with scenario-
based interaction in which a user and a robot acted
with predetermined timing. In contrast, we focus
on spoken dialogues in which users and agents can
speak with more flexible timing. In addition, we
allow for two- and three-party interactions among
a user and two agents. It remains unclear whether
the policy gradient method can successfully adapt
agent communicative behavior to a user in two-
or three-party spoken dialogues with flexible turn-
taking. Although this paper focuses on agent be-
havior adaptation to human users, we believe that
our investigation of the agent behavior adaptation
mechanism in flexible spoken interaction will con-
tribute to conversational interfaces where human
users and agents can mutually adapt their commu-
nicative behaviors.
As agent communicative behavior to be
adapted, this paper focuses on the pause duration
preceding agent utterances and the agent gaze du-
ration. In conversation, the participant pause du-
ration is influenced by partners, and the coordina-
tion of pause structure leads to smooth communi-
cation (Burgoon et al, 1995; Hayashi et al, 2009).
Without pause structure coordination, undesired
speech overlaps or utterance collisions are likely
to occur between users and agents, which may dis-
turb smooth communication. Funakoshi et al pro-
posed a method to prevent undesired speech over-
laps in human-robot speech interactions by using
a robot?s subtle expressions produced by a blink-
ing LED attached to its chest (Funakoshi et al,
2008). In their method, a blinking light notifies
users about such internal states of the robot as pro-
cessing or busy and helps users identify the robot
pause structures; however we are concerned with
the adaptation of robot pause structures to users.
Gaze coordination is causally related to the
success of communication (Richardson and Dale,
2005), and the amount of gaze influences conver-
sational turn-taking (Vertegaal and Ding, 2002).
The relevant control of agent gaze duration is
thus essential to the smooth flow of conversation.
Moreover, since the amount of gaze is related to
specific interpersonal attitudes among participants
and is also subject to such individual differences as
personalities (Argyle and Cook, 1976), agent gaze
duration must be adapted to individual users.
In the following, Section 2 describes our quiz-
style multi-party spoken dialogue system. Sec-
tion 3 shows our method for the user-adaptive co-
ordination of agent communicative behavior. Sec-
tion 4 explains the experiment, and Section 5 de-
scribes its results. Section 6 concludes our paper.
2 Quiz-Style Spoken Dialogue System
To evaluate a method for agent communicative
behavior adaptation, we used a quiz-style multi-
party spoken dialogue system based on a quiz-
style two-party spoken dialogue system (Minami
et al, 2007) and extended it to performmulti-party
interaction (Dohsaka et al, 2009).
In this system, a human user and one or two
agents interact. The two agents include a quiz-
master and a peer. The quizmaster agent creates
a ?Who is this?? quiz about a famous person
and presents hints one by one to the user and the
peer agent, who participates in the interaction and
guesses the correct answer in the same way that
the user does.
The hints are automatically created from the
biographical facts of people in Wikipedia1 and
ranked based on the difficulty of solving the
quizzes experienced by users (Higashinaka et al,
2007b). Since users must consider the hints to of-
fer reasonable answers, the system can stimulate
their thinking and encourage them to engage in the
interaction (Higashinaka et al, 2007a). In addi-
tion, the peer agent?s presence and the agent?s em-
pathic expressions improve user satisfaction and
1http://ja.wikipedia.org/
315
Figure 1: User interacting with two agents using
the quiz-style spoken dialogue system
increase user utterances (Dohsaka et al, 2009).
Figure 1 shows a human user interacting with
the two agents, both of whom are physically em-
bodied robots. The system utilizes an extremely
large vocabulary with continuous speech recogni-
tion (Hori et al, 2007). Agent utterances are pro-
duced by speech synthesis. The agents can gaze at
other participants by directing their faces to them.
At each point of the dialogue, the system chooses
the next speaker and its utterance based on the
dialogue state that the system maintains, the pre-
conditions of the individual utterances, and a few
turn-taking rules (Dohsaka et al, 2009). The agent
pause and gaze durations are controlled based on
the adaptation method described in Section 3.
A sample dialogue among a user and two agents
is depicted in Figure 2. Master is the quizmaster
agent, and Peer is the peer agent. The agent ut-
terances are classified as either spontaneous or re-
sponsive. Spontaneous utterances are those made
after an agent takes his turn in an unforced man-
ner, and responsive utterances are responses to the
other?s utterances. In the sample dialogue, spon
identifies spontaneous and res identifies respon-
sive utterances.
Quizmaster agent Master makes spontaneous
utterances such as presenting hints (lines 1 and 5),
indicating the quiz difficulty, and addressing lis-
teners. It also makes such responsive utterances
as evaluating the other?s answers (lines 3, 9, and
11). Peer agent Peer makes such spontaneous ut-
terances as showing its own difficulty (line 4), giv-
ing an answer (line 8), giving feedback when its
own or the other?s answer is right (line 12), and
addressing listeners. It also makes such responsive
utterances as showing empathy to the user (line 7).
3 Method for Agent Communicative
Behavior Adaptation
We apply policy gradient reinforcement learn-
ing (Sutton et al, 2000; Kohl and Stone, 2004)
1 Master Who is this? First hint. He gradu-
ated from the University of Tokyo.
(hint/spon)
2 User Yoshida Shigeru? (answer/spon)
3 Master No, not even close! He?s not a
politician. (evaluation/res)
4 Peer I don?t know. Very difficult.
(show difficulty/spon)
5 Master It?s time for the second hint: He?s
a novelist and a scholar of British
literature. (hint/spon)
6 User Oh, I think I know it but I can?t re-
member his name. That?s so frus-
trating. (show difficulty/spon)
7 Peer Difficult for me, too.
(show empathy/res)
8 Peer Haruki Murakami? (answer/spon)
9 Master Close! You are half right, because
he is a novelist. (evaluation/res)
10 User Natsume Soseki? (answer/spon)
11 Master That?s right. Wonderful.
(evaluation/res)
12 Peer Good job. (feedback/spon)
Figure 2: Sample dialogue between user and two
agents: quizmaster Master and peer Peer. Spon
identifies spontaneous and res identifies respon-
sive utterances.
to the user-adaptive coordination of agent com-
municative behavior. A policy gradient method
is a reinforcement learning (RL) approach that di-
rectly optimizes a parameterized policy by gradi-
ent ascent based on the gradient of the expected
reward with respect to the policy parameters. Al-
though RL methods have recently been applied to
optimizing dialogue management in spoken dia-
logue systems (Williams and Young, 2007; Mi-
nami et al, 2009), these previous studies utilized
RL methods based on the value-function estima-
tion. The policy gradient method is an alterna-
tive approach to RL that has the following mer-
its. It can handle continuous and large action
spaces (Kimura and Kobayashi, 1998) and is usu-
ally assured to converge to a locally optimal pol-
icy in such action spaces (Sutton et al, 2000).
Moreover, it does not need to explicitly estimate
the value function, and it is incremental, requiring
only a constant amount of computation per learn-
ing step (Kimura and Kobayashi, 1998).
Due to these advantages, the policy gradient
method is suitable for adapting agent communica-
tive behavior to a user during interaction, because
316
(1) ? = [?j ]? initial policy (policy parameter
vector of size n)
(2) ? = [?j ]? step size vector of size n
(3) ? ? overall scalar step size
(4) maxA? 0 (greatest absolute value of
reward ever observed in adaptation process)
(5) while dialogue continues
(6) for i = 1 to T
(7) for j = 1 to n
(8) rj ? random choice from {?1, 0, 1}
(9) Rij ? ?j + ?j ? rj
(Ri is T random perturbations of?)
(10) for i = 1 to T
(11) Perform a hint dialogue based on
policyRi, and evaluate reward
(12) for j = 1 to n
(13) Avg+?,j ? average reward for all Ri
with positive perturbation in parameter ?j
(14) Avg0,j ? average reward for all Ri
with zero perturbation in parameter ?j
(15) Avg??,j ? average reward for all Ri
with negative perturbation in parameter ?j
(16) if (Avg0,j > Avg+?,j and
Avg0,j > Avg??,j)
(17) aj ? 0
(18) else
(19) aj ? Avg+?,j ?Avg??,j
(20) ?j(aj ? aj|A| ? ?j ? ?)
(21) maxC ? maximum of absolute value of
reward in current adaptation cycle
(22) if (maxC > maxA)
(23) maxA? maxC (update maxA)
(24) else
(25) A? A ? maxCmaxA
(26) ?? ?+A
Figure 3: Pseudocode for user-adaptive coordina-
tion of agent communicative behavior
it can naturally incorporate such continuous pa-
rameters as pause and gaze duration and incremen-
tally adapt agent behavior. In fact, the policy gra-
dient method has been successfully used for robot
behavior adaptation (Mitsunaga et al, 2005; Tapus
and Mataric?, 2007). In this paper, we apply this
method to agent communicative behavior adapta-
tion in spoken dialogues with flexible turn-taking.
Figure 3 shows our method for the user-adaptive
coordination of agent communicative behavior.
This method is a modification of an algorithm pre-
sented by Kohl and Stone (2004) in that the gra-
dient is adjusted based on the maximum absolute
value of the reward obtained during each adapta-
tion cycle.
The agent communicative behaviors are deter-
mined based on a policy that is represented as vec-
tor?(= [?j ]) of n policy parameters. In the quiz-
style dialogues, the behavior of both the quizmas-
ter and peer agents is controlled based on the same
policy parameters. The method adapts the behav-
ior of both agents to individual users by adapting
the policy parameters. In this experiment, we used
the following four parameters (n = 4):
? pre-spontaneous-utterance pause duration
?spon: duration of pauses preceding agent
spontaneous utterances
? pre-responsive-utterance pause duration
?res: duration of pauses preceding agent
responsive utterances
? gaze duration ?gaze: duration of agent?s di-
recting its face to the other while it is speak-
ing or listening
? hint interval ?hint: interval of presenting quiz
hints
As shown above, we used two types of pause
duration since the relevant pause duration can be
dependent on dialogue acts (Itoh et al, 2009). Al-
though our main concern is the pause and gaze du-
ration, we examined the hint interval as a parame-
ter particular to quiz-style dialogues.
To adapt the policy parameters to individual
users, we first generate T random perturbations
[R1, . . . ,RT ] of current policy ? by randomly
adding ?j , 0,??j to each parameter ?j of ? in
lines 6 to 9, where ?j is a step size set for each
parameter. In the experiment, we set T to 10. The
step sizes of the parameters used in the experiment
will be shown later in Table 1.
Dialogue per hint (a hint dialogue) is then per-
formed based on each perturbation policyRi, and
the reward for each hint dialogue is obtained in
lines 10 to 11. All agent behaviors in a hint di-
alogue are determined based on the same pertur-
bation policy. As we will explain in Section 4, in
the experiment, we regarded the magnitude of dis-
comfort perceived by users during a hint dialogue
as a negative reward. Users signified discomfort
by pressing buttons on the controller held in their
hands. After performing hint dialogues for all T
perturbationsRi, gradientA(= [aj ]) is computed
in lines 12 to 19. The gradient is normalized by
317
Parameters ?spon
(sec.)
?res
(sec.)
?gaze
(sec.)
?hint
(sec.)
Initial value 4.96 0.53 3.04 27.7
Step size 0.50 0.20 0.30 2.5
Table 1: Initial values and step sizes of policy pa-
rameters: ?spon (pre-spontaneous-utterance pause
duration), ?res (pre-responsive-utterance pause
duration), ?gaze (gaze duration), and ?hint (hint
interval)
overall scalar step size ? and individual step size
?j for each parameter in line 20. Overall scalar
step size ? is used to adjust the adaptation speed,
which we set to 1.0.
Next we get the maximum maxC of the abso-
lute value of the reward in the current adaptation
cycle. As in lines 21 to 25, the gradient is ad-
justed based on the ratio of maxC to the greatest
absolute value maxA of reward ever observed in
the overall adaptation process. Finally, the current
policy parameters are updated using the gradient
in line 26.
This is an adaptation cycle. By iterating it, the
agent communicative behavior is adapted to re-
duce the discomfort perceived by each user.
4 Experiment
We recruited and paid 32 Japanese adults (16
males and 16 females) for their participation. The
mean ages of the male and female groups were
33.2 and 36.8, respectively. They were divided
into two groups: two-party dialogues (user and
quizmaster) and three-party dialogues (user, quiz-
master, and peer). In each group, the numbers of
males and females were identical.
For this experiment, we used a quiz-style spo-
ken dialogue system. We chose the quiz sub-
jects in advance and divided them into sets of five
so that the difficulty level was approximately the
same in all sets. For this purpose, we made sev-
eral sets of five people of approximately identical
PageRank TM scores based on Wikipedia?s hyper-
link structure.
The users first rehearsed the dialogues for a set
of five quizzes to familiarize themselves with the
system. After practicing, they performed the dia-
logues to evaluate the adaptation method and took
a break per five-quiz set. The presentation order
of the quiz sets was permutated to prevent order
effect. For each user, the dialogues continued un-
til the user received 150 hints. The adaptation
method was applied during the interaction, and the
policy parameters were updated per 10 hint dia-
logues. As a result, the parameters were updated
15 times through the dialogues. It took about two
hours for each user to complete all dialogues.
The policy parameters were updated based on
the magnitude of discomfort perceived by users.
In this experiment, users were told to concentrate
on the discomfort caused by agent pause and gaze
duration and signified it by pressing buttons on
the controller held in their hands at three levels of
magnitude: ?3?, ?2?, and ?1?. The sum of discom-
fort obtained during a hint dialogue was normal-
ized with respect to the hint dialogue length, and
the normalized values were regarded as negative
rewards. Ideally we should estimate user discom-
fort from such user behaviors as pause structure
and eye gaze. However, as the first step toward that
goal, in this experiment we adopted this setting in
which users directly signified their discomfort by
pressing buttons.
Table 1 shows the initial values and the step
sizes of the policy parameters used in the exper-
iment. To obtain the relevant initial values, we
conducted a preparatory experiment in which ten
other participants performed quiz-style dialogues
under the same conditions as this experiment. The
initial values in this experiment were set to the
averaged final values of the policy parameters in
the preparatory experiment. The step sizes were
determined as approximately one-tenth of the ini-
tial values except for the pre-responsive-utterance
pause, for which the step size was set to 200 msec
based on the limits of human perception.
Before and after the adaptation, the users filled
out the following questionnaire items (7-point Lik-
ert scale) to evaluate the relevance of agent pause
and gaze duration:
? Did you feel that the pause duration preced-
ing the agent utterances was relevant?
? Did you feel that the agent gaze duration was
relevant while the agents were speaking or
listening to you?
5 Results
5.1 Convergence of policy parameters
The policy parameters were updated based on the
adaptation method during the user-agent interac-
tion. Figure 4 exemplifies how the policy param-
eter values changed during the adaptation cycles
with a user engaged in the two-party dialogue.
318
33.5
44.5
55.5
0 2 4 6 8 10 12 14(a) Pre-spontaneous-utt.  pause duration (sec.)
22.5
33.5
4
0 2 4 6 8 10 12 14(c) Gazeduration (sec.)
0.450.5
0.550.6
0.65
0 2 4 6 8 10 12 14
2022
2426
2830
0 2 4 6 8 10 12 14
(b) Pre-responsive-utt.  pause duration (sec.)
(d) Hint interval (sec.)
Figure 4: Change of policy parameter values dur-
ing adaptation cycles with a user engaged in two-
party dialogue. Horizontal axis shows adaptation
cycles and vertical axis shows parameter values.
0
0.04
0.08
0.12 First-phase RAC Last-phase RAC
Two-party dialogue
p=0.029 *
p=0.0071 **
p=0.041 * N.S.
?spon ?res ?gaze ?hint
0
0.04
0.08
0.12
First-phase RAC Last-phase RAC
Three-party dialogue
p=0.038 *
p<0.001 ***
p=0.016 *
?spon ?res ?gaze ?hint
p=0.011 *
Figure 5: For each policy parameter, average and
standard error of first- and last-phase RACs (rela-
tive amount of change in parameter values)
Table 2 shows the statistics of the final values
of the policy parameters at the end of the adapta-
tion process. Since the initial values were appro-
priately determined based on the preparatory ex-
periment, the final value averages were not greatly
different from the initial values. However, judging
from the maximum, minimum, and standard devi-
ations, the final values reflected individual users.
If the adaptation method works successfully, the
policy parameter values should converge during
the user-agent interaction. From this viewpoint,
we examined the relative amount of change in the
policy parameters (RAC). Given parameter value
pk?1 at (k ? 1)-th adaptation cycle and param-
eter value pk at k-th cycle, RAC is defined as
Two-party dialogues
Parameters ?spon
(sec.)
?res
(sec.)
?gaze
(sec.)
?hint
(sec.)
Average 5.04 0.62 3.10 25.8
Min 3.90 0.39 2.40 19.5
Max 6.17 1.18 3.69 31.2
Sd. 0.72 0.21 0.36 2.7
Three-party dialogues
Parameters ?spon
(sec.)
?res
(sec.)
?gaze
(sec.)
?hint
(sec.)
Average 4.86 0.62 3.15 27.4
Min 4.07 0.35 2.52 22.0
Max 5.54 0.90 3.58 32.7
Sd. 0.44 0.18 0.27 2.5
Table 2: Statistics of final values of policy param-
eters: ?spon (pre-spontaneous-utterance pause du-
ration), ?res (pre-responsive-utterance pause dura-
tion), ?gaze (gaze duration), and ?hint (hint inter-
val)
|pk?pk?1|
pk?1 .
For each policy parameter, we compared the
RAC averages in the first and in the last three adap-
tation cycles: the first-phase RAC and the last-
phase RAC. As shown in Figure 5, the last-phase
RAC tends to be smaller than the first-phase RAC.
The Kolmogorov-Smirnov test showed that the as-
sumption of normality (p > 0.2) was met for each
group. By applying the paired Welch?s t-test, as
shown in Figure 5, we found that the last-phase
RAC is significantly smaller than the first-phase
RAC except for the hint interval in the two-party
dialogues. This shows that the agent pause and
gaze duration converged during the interaction in
both the two- and three-party dialogues.
The hint interval is unlikely to converge, prob-
ably because it is a longer period than the pause
and gaze duration and is subject to various factors.
Moreover, it greatly depends on user interest.
5.2 User evaluations
Figure 6 shows the subjective user evaluations of
the relevance of agent pause and gaze duration be-
fore and after the adaptation. Each user evaluation
was measured by a Likert question. The rating of
a single Likert question is an ordinal measure, and
we generally cannot apply a parametric statistical
test to an ordinal measure. Therefore we used a
nonparametric test, the Wilcoxon signed-rank test,
to compare user evaluations before and after the
319
12
34
56
7
Pause Gaze Pause Gaze
Before Adaptation After Adaptation
Two-party dialogue Three-party dialogue
p=0.014 * p=0.0051 ** p=0.015 * p=0.021 *
Figure 6: Average and standard error of user eval-
uations of relevance of agent pause and gaze dura-
tion before and after adaptation
adaptation. The F-test for the homogeneity of vari-
ances (p > 0.1) showed that the data satisfied the
statistical test assumption.
We found that in both the two- and three-party
dialogues, the relevance of the agent pause and
gaze duration significantly improved during the
two-hour adaptation process (p < 0.01 for gaze
duration in the two-party dialogues, p < 0.05 for
other cases). The p-values are shown in Figure 6.
No significant differences between gender were
found.
These results on the convergence of policy
parameters and user evaluations show that the
policy-gradient-based method can adapt agent
communicative behavior to individual users in
spoken dialogues with flexible turn-taking.
6 Conclusion
In this paper, addressing smooth spoken inter-
action between human users and conversational
agents, we presented a method for user-adaptive
coordination of agent communicative behavior
and experimentally evaluated how it can adapt
agent behavior to individual users in spoken dia-
logues with flexible turn-taking. The method co-
ordinates agent pause and gaze duration based on
policy gradient reinforcement learning to reduce
the discomfort perceived by individual users dur-
ing interaction. We experimentally evaluated the
method in a setting where the users performed
two- and three-party quiz-style dialogues and sig-
nified their discomfort by pressing buttons held in
their hands. Our experimental results showed a
statistically significant tendency: the agent pause
and gaze duration converged during interaction
with the method in both two- or three-party dia-
logues. The method also significantly improved
the perceived relevance of the agent communica-
tive behavior in both two- and three-party di-
alogues. These results indicate that in spoken
dialogues with flexible turn-taking, the policy-
gradient-based method can adapt agent commu-
nicative behavior to individual users.
Many directions for future work remain. First,
we will analyze how users adapt their communica-
tive behaviors with our method. Second, we need
to automatically estimate user discomfort or sat-
isfaction based on such user behaviors as pause
structure, prosody, eye gaze, and body posture.
Third, we will extend the adaptation method to
regulate agent behavior based on dialogue states,
since one limitation of the current method is its
inability to recognize them. Fourth, we are inter-
ested in the adaptation of additional higher-level
actions like the relevant choice of dialogue topics
based on the level of user interest.
Acknowledgments
This work was partially supported by a Grant-in-
Aid for Scientific Research on Innovative Areas,
?Founding a creative society via collaboration be-
tween humans and robots? (21118004), from the
Ministry of Education, Culture, Sports, Science
and Technology (MEXT), Japan.
References
Michael Argyle and Mark Cook. 1976. Gaze and Mu-
tual Gaze. Cambridge University Press.
Cynthia Breazeal. 2003. Regulation and entrainment
for human-robot interaction. International Journal
of Experimental Robotics, 21(10-11):883?902.
Susan E. Brennan and Herbert H. Clark. 1996. Con-
ceptual pacts and lexical choice in conversation.
Journal of Experimental Psychology: Learning,
Memory, and Cognition, 22:1482?1493.
Judee K. Burgoon, Lesa A. Stern, and Leesa Dillman.
1995. Interpersonal Adaptation: Dyadic Interaction
Patterns. Cambridge University Press.
Herbert H. Clark. 1996. Using Language. Cambridge
University Press.
Kohji Dohsaka, Ryota Asai, Ryuichiro Higashinaka,
Yasuhiro Minami, and Eisaku Maeda. 2009. Effects
of conversational agents on human communication
in thought-evoking multi-party dialogues. In Proc.
of SIGDIAL 2009, pages 217?224.
Shinya Fujie, Yoichi Matsuyama, Hikaru Taniyama,
and Tetsunori Kobayashi. 2009. Conversation robot
participating in and activating a group communica-
tion. In Proc. of Interspeech 2009, pages 264?267.
320
Kotaro Funakoshi, Kazuki Kobayashi, Mikio Nakano,
Seiji Yamada, Yasuhiko Kitamura, and Hiroshi Tsu-
jino. 2008. Smoothing human-robot speech interac-
tions by using a blinking-light as subtle expression.
In Proc. of ICMI 2008, pages 293?296.
Simon Garrod and Martin J. Pickering. 2004. Why is
conversation so easy? Trends in Cognitive Sciences,
8:8?11.
Takanori Hayashi, Shohei Kato, and Hidenori Itoh.
2009. A synchronous model of mental rhythm using
paralanguage for communication robots. In Lecture
Notes in Computer Science (PRIMA 2009), volume
5925, pages 376?388.
Ryuichiro Higashinaka, Kohji Dohsaka, Shigeaki
Amano, and Hideki Isozaki. 2007a. Effects of quiz-
style information presentation on user understand-
ing. In Proc. of Interspeech 2007, pages 2725?2728.
Ryuichiro Higashinaka, Kohji Dohsaka, and Hideki
Isozaki. 2007b. Learning to rank definitions to gen-
erate quizzes for interactive information presenta-
tion. In Proc. of ACL 2007 (Poster Presentation),
pages 117?120.
Takaaki Hori, Chiori Hori, Yasuhiro Minami, and At-
sushi Nakamura. 2007. Efficient WFST-based one-
pass decoding with on-the-fly hypothesis rescoring
in extremely large vocabulary continuous speech
recognition. IEEE Transactions on Audio, Speech
and Language Processing, 15:1352?1365.
Toshihiko Itoh, Norihide Kitaoka, and Ryota
Nishimura. 2009. Subjective experiments on
influence of response timing in spoken dialogues.
In Proc. of Interspeech 2009, pages 1835?1838.
W. Lewis Johnson, Jeff W. Rickel, and James C. Lester.
2002. Animated pedagogical aqgents: face-to-face
interaction in interactive learning environments. In-
ternational Journal of Artificial Intelligence in Edu-
cation, 11:47?78.
Hajime Kimura and Shigenobu Kobayashi. 1998. Re-
inforcement learning for continuous action using
stochastic gradient ascent. In Proc. of the 5th Inter-
national Conference on Intelligent Autonomous Sys-
tems, pages 288?295.
Nate Kohl and Peter Stone. 2004. Policy gradient rein-
forcement learning for fast quadrupedal locomotion.
In Proc. of ICRA 2004, volume 3, pages 2619?2624.
Stefan Kopp, Lars Gesellensetter, Nicole C. Kra?mer,
and Ipke Wachsmuth. 2005. A conversational agent
as museum guide: design and evaluation of a real-
world application. In Lecture Notes in Computer
Science (IVA 2009), volume 3661, pages 329?343.
Yasuhiro Minami, Minako Sawaki, Kohji Dohsaka,
Ryuichiro Higashinaka, Kentaro Ishizuka, Hideki
Isozaki, Tatsushi Matsubayashi, Masato Miyoshi,
Atsushi Nakamura, Takanobu Oba, Hiroshi Sawada,
Takeshi Yamada, and Eisaku Maeda. 2007. The
World of Mushrooms: human-computer interaction
prototype systems for ambient intelligence. In Proc.
of ICMI 2007, pages 366?373.
Yasuhiro Minami, Akira Mori, Ryuichiro Higashinaka,
Kohji Dohsaka, and Eisaku Maeda. 2009. Dialogue
control algorithm for ambient intelligence based on
partially observable Markov decision processes. In
Proc. of IWSDS 2009.
Noriaki Mitsunaga, Christian Smith, Takayuki Kanda,
Hiroshi Isiguro, and Norihiro Hagita. 2005.
Human-robot interaction based on policy gradient
reinforcement learning. In Proc. of IROS 2005,
pages 1594?1601.
Clifford Nass and Scott Brave. 2005. Wired for
Speech: How Voice Activates and Advances the
Human-Computer Relationship. The MIT Press.
Sharon Oviatt, Courtney Darves, and Rachel Coulston.
2004. Toward adaptive conversational interfaces:
modeling speech convergence with animated per-
sonas. ACM Transactions on Computer-Human In-
teraction, 11(3):300?328.
Daniel C. Richardson and Rick Dale. 2005. Look-
ing to understand: the coupling between speakers?
and listeners? eye movements and its relationship
to discourse comprehension. Cognitive Science,
29:1045?1060.
Harvey Sacks, Emanuel A. Schegloff, and Gail Jeffer-
son. 1974. A simplest systematics for the orga-
nization of turn-taking in conversation. Language,
50:696?735.
Richard S. Sutton, David McAllester, Satinder Singh,
and Yishay Mansour. 2000. Policy gradient meth-
ods for reinforcement learning with function approx-
imation. In Advances in Neural Information Pro-
cessing Systems, volume 12, pages 1057?1063.
Adriana Tapus and Maja J. Mataric?. 2007. Hands-off
therapist robot behavior adaptation to user person-
ality for post-stroke rehabilitation therapy. In Proc.
of 2007 IEEE International Conference on Robotics
and Automation, pages 1547?1553.
David Traum and Jeff Rickel. 2002. Embodied agents
for multi-party dialogue in immersive virtual worlds.
In Proc. of AAMAS 2002, pages 766?773.
Roel Vertegaal and Yaping Ding. 2002. Explaining
effects of eye gaze on mediated group conversations:
amount or synchronization. In Proc. of CSCW 2002,
pages 41?48.
Jason D. Williams and Steve Young. 2007. Par-
tially observable Markov decision processes for spo-
ken dialog systems. Computer & Speech Language,
21(2):393?422.
Jun Zheng, Xiang Yuan, and Yam San Chee. 2005.
Designing multiparty interaction support in Elva, an
embodied tour guide. In Proc. of AAMAS 2005,
pages 929?936.
321
Proceedings of the SIGDIAL 2013 Conference, pages 334?338,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Open-domain Utterance Generation for Conversational Dialogue Systems
using Web-scale Dependency Structures
Hiroaki Sugiyama?, Toyomi Meguro?, Ryuichiro Higashinaka??, Yasuhiro Minami?
?NTT Communication Science Laboratories
2-4, Hikari-dai, Seika-cho, Souraku-gun, Kyoto, Japan
??NTT Media Intelligence Laboratories
1-1, Hikari-no-Oka, Yokosuka-shi, Kanagawa, Japan
{sugiyama.hiroaki,meguro.toyomi,higashinaka.ryuichiro,minami.yasuhiro}@lab.ntt.co.jp
Abstract
Even though open-domain conversational
dialogue systems are required in many
fields, their development is complicated
because of the flexibility and variety of
user utterances. To address this flexibil-
ity, previous research on conversational di-
alogue systems has selected system utter-
ances from web articles based on surface
cohesion and shallow semantic coherence;
however, the generated utterances some-
times contain irrelevant sentences with re-
spect to the input user utterance. We pro-
pose a template-based approach that fills
templates with the most salient words in
a user utterance and with related words
that are extracted using web-scale depen-
dency structures gathered from Twitter.
Our open-domain conversational dialogue
system outperforms retrieval-based con-
ventional systems in chat experiments.
1 Introduction
The need for open-domain conversational dia-
logue systems continues to grow. Such systems
are beginning to be actively investigated from their
social and entertainment aspects (Shibata et al,
2009; Ritter et al, 2011; Wong et al, 2012);
conversational dialogues also have potential for
therapy purposes and for evoking a user?s uncon-
scious requests in task-oriented dialogues (Bick-
more and Cassell, 2001). However, developing
open-domain conversational dialogue systems is
difficult, since the huge variety of user utterances
makes it harder to build knowledge resources for
generating appropriate system responses. To ad-
dress this issue, previous research has selected sys-
tem utterances from web articles or microblogs on
the basis of surface cohesion and shallow seman-
tic coherence (Shibata et al, 2009; Jafarpour and
Burges, 2010; Wong et al, 2012); however, the se-
lected utterances sometimes contain sentences ir-
relevant to the user utterance since they originally
appeared in a different context.
To satisfy both web-scale topic coverage and
suppression of irrelevant sentences, we propose a
template-based approach that fills templates with
words related to the topic of the user utterance and
with words related to the topic-words. This ap-
proach enables us to generate a wide range of sys-
tem responses when we properly extract related
words. To obtain words related to topic-words,
we analyzed the dependency structures of a huge
number of sentences posted to such microblogs as
Twitter, where a large number and variety of sen-
tences are posted daily. This way, we can generate
a variety of appropriate system responses despite
wide variation in user utterances.
We develop a conversational dialogue system
that generates system utterances with our pro-
posed utterance generation approach and exam-
ine its effectiveness by chat experiments with real
users.
2 Related Work
To generate system utterances for conversational
dialogue systems, Ritter et al (2011) proposed a
statistical machine translation-based approach that
considers source-reply tweet pairs as a bilingual
corpus. They compared the following three ap-
proaches: IR-status, which retrieves reply tweets
whose associated source tweets most resemble
the user utterance (Jafarpour and Burges, 2010);
IR-response, which retrieves reply tweets that
are the most similar to the user utterance; and
their proposed SMT-based approach, named MT-
chat. They reported that MT-chat outperformed
the other approaches and that IR-response was su-
perior to IR-status. However, these approaches
used only the words, and not the structures, of user
utterances to generate system utterances.
Yoshino et al (2011) proposed a QA system
that answers questions about current events by re-
trieving, from news articles, descriptions contain-
ing similar dependency structures as those of the
user?s questions. Although this retrieval-based ap-
proach is effective for answering the user?s fac-
tual questions, it is insufficient to generate sub-
jective utterances for conversational dialogue sys-
tems since such systems are required to introduce
334
new topics or to respond with opinions related to
user utterances.
3 Open-domain Utterance Generation
Open-domain conversational dialogue systems
should be able to respond to any user utterance on
any topic. To achieve this, we adopt a template-
based approach that estimates the topic of the
user utterance, extracts words related to the topic-
words, and fills templates with these words. The
template-based approach resembles previous rule-
based approaches, but these dialogue systems had
difficulty achieving coverage for template fillers.
In contrast, our approach utilizes the dependency
structures of sentences gathered from microblogs
that have a wide range of topics, in order to ex-
tract the related words used in template-filling.
The dependency parser we use is a state-of-the-art
Japanese dependency parser that uses Conditional
Random Fields trained on text and blog posts, and
performs cascaded chunking until all dependen-
cies are found. This parser achieved 84.59% de-
pendency accuracy on a corpus of Japanese blog
posts (Imamura et al, 2007).
Microblog posts do not typically contain formu-
laic utterances such as greetings or back-channels.
Therefore, in addition to the template-filling ap-
proach, we adopt dialogue act based utterance
generation for the formulaic uttenances. Figure 1
illustrates the whole architecture of our system.
3.1 Topic-word-driven Template-based
Utterance Generation
Our topic-word-driven template-based approach
consists of the following three steps: topic estima-
tion, related word extraction, and template-filling
utterance generation.
3.1.1 Topic Estimation
We identify three types of potential topic in an in-
put user utterance: proper nouns, common nouns,
and predicates (verbs, adjectives, adjectival verbs,
and verbal nouns).
Proper Nouns We take the last proper noun
that appears in the user?s utterance as a poten-
tial topic. Since general Japanese morphologi-
cal analyzers cannot capture recent proper nouns,
we complement the proper noun dictionary entries
with Wikipedia entries1.
Common Nouns To identify potential topics
from common nouns, we calculate the inverse doc-
ument frequency (IDF) of each common noun (all
nouns except for proper, time-related, and verbal
ones) in the user?s utterance. We use a corpus of
microblog posts and treat each post as a document.
We adopt the word with the highest IDF as a po-
tential topic.
1https://github.com/nabokov/mecab-dic-overdrive
Related word Extracon 
TopicEsmaon 
Template-filling based U"erance Generaon 
User U"erance 
System U"erance 
Topic-word-driven  Template-based U"erance 
Topic-word 
Topic-related words 
DialogueControl 
Dialogue act Esmaon 
U"erance Generaon with Predicted Dialogue act 
User?s Dialogue act 
System?s Dialogue act 
Dialogue act based U"erance (only for greengs and  back-channel feedback) 
Topic-word-driven  Template-based  Approach 
Dialogue actbased Approach (Secondarily) 
Figure 1: System Architecture
Predicates We take the predicate that composes
a dependency in the highest layer of the depen-
dency structure as a potential topic. For example,
we adopt ?ask?, but not ?walk? from the utterance
?I asked the man walking on the street?.
3.1.2 Related word Extraction
To obtain topic-related words, a thesaurus or topic
model such as Latent Dirichlet Allocation are the
most popular approaches (Blei et al, 2003). How-
ever, these approaches return semantically simi-
lar words to input query words, which do not ef-
fectively introduce new information into the sys-
tem utterances. Therefore, we count the depen-
dencies between words in a huge number of sen-
tences gathered from microblogs, and utilize the
most frequently dependent words. This approach
enables us to extract adjectives related to proper
noun topics; for example, the adjectives beautiful,
good, clear, white, and huge are extracted for Mt.
Fuji. Since microblogs contain a huge number of
subjective posts, we expect the extracted words to
be subjective and suitable for conversational dia-
logue systems. In this work, we extract adjectives
for proper and common nouns, and nouns and their
case frames for predicates. Examples of extracted
words are shown in Table 2.
3.1.3 Template-filling Utterance Generation
We generate two types of system utterances using
manually defined templates: subjective sentences
with proper nouns and common nouns; and ques-
tions with predicates and their case frames.
Noun-driven Subjective Sentence Generation
We generate system utterances using the proper
and common nouns and their related adjectives.
Here, we adopt different templates for each word
type; proper nouns have explicit meanings, so ad-
jectives related to them are easily suited for any di-
alogue context. By contrast, since common nouns
are used in various contexts in microblogs, ad-
jectives related to common nouns may not fit the
dialogue context. Thus, we use ?suki? (?like?
in English), or ?nigate? (?don?t like? in English)
in the templates based on the proportion of posi-
tive/negative adjectives in the set of related words
for a common noun topic. Table 3 shows represen-
tative examples for each type. If the system gener-
335
ates subjective utterances as the system?s own im-
pression of the dialogue topic, the user will expect
the system to justify or explain its opinion; how-
ever, our system cannot answer that kind of ques-
tion. Thus, we define the templates using hedges
such as ?I hear that...? to avoid such questions.
The number of templates for proper nouns is eight,
and for common nouns is four for each polarity.
Predicate-driven Question Sentence Genera-
tion We generate question sentences using pred-
icates and their related nouns and case frames. To
elicit user utterances on a particular topic, we gen-
erate How/What/Where/When types of questions
as shown in Table 3. To select a question word,
we use the predicate types and the classes of the
related nouns. If the predicate type is adjective or
adjectival noun, we select ?how? for the question
word. If the predicate type is verbal noun or verb
and location class words appear in the related noun
phrase, we select ?where? for the question word;
the time class induces the question word ?when?.
When no proper noun is found in the topic-word,
we select ?what?. The number of templates for
proper nouns is three for each interrogative type.
3.2 Dialogue act based Utterance Generation
Our approach has difficulty generating appropriate
responses to formulaic utterances such as greet-
ings and back-channels. To address this weakness,
we adopt dialogue act based utterance generation
for these types of utterance. A dialogue act is an
abstract expression of a speaker?s intention (Stol-
cke et al, 2000); we used the 33 dialogue acts de-
fined in Meguro et al (2010).
Our dialogue act based approach estimates the
next dialogue act that the system should output
based on the user?s utterance, and generates a sys-
tem utterance based on the system?s predicted di-
alogue act if the dialogue act is greetings, sympa-
thy, non-sympathy, filler, or confirmation.
3.2.1 User?s Dialogue act Estimation
We collected 1,259 conversational dialogues from
47 human subjects and labeled each sentence of
the collected data using the 33 dialogue acts.
67,801 dialogue acts are contained in the corpus.
We estimated the 33 dialogue acts from user
utterances using a logistic regression model and
adopted 1- and 2-gram words and 3- and 4-gram
characters as model features. We trained our
model using 1,000 dialogues and evaluated it us-
ing 259 dialogues. The estimation accuracy was
about 61%, whereas the human annotation agree-
ment rate was about 59%.
3.2.2 Dialogue control Model and Utterance
Generation with Predicted Dialogue act
We developed a dialogue control model that esti-
mates the system?s next dialogue act based on the
user?s dialogue act. The model features are the
user?s current dialogue act vector, the system?s last
dialogue act vector, and the user?s last dialogue act
vector. Each dialogue act vector consists of a 33-
dimensional binary vector space. We used the dia-
logue corpus described above to train and evaluate
our model, which we trained with 1,000 dialogues
and evaluated using 259 dialogues. The estimation
accuracy was 31%, whereas the dialogue act an-
notation agreement rate between humans is 60%.
We exploited the fact that formulaic utterances can
pre-define corresponding utterances regardless of
the context. Table 4 shows example generated sen-
tences for each dialogue act.
4 Experiment
4.1 Experiment Setting
We recruited ten native Japanese-speaking partic-
ipants in their 20?s and 30?s (two males and eight
females) from outside of the authors? organiza-
tion, who have experience using chat systems (not
bots). Each participant chatted with the following
systems, provided subjective evaluation scores for
each system for each of the eight criteria shown in
Table 1 (2)-(10) using 7-point Likert scales, and at
the end ranked all the systems. We examined the
effectiveness of our proposed approach by com-
parison with the following six systems.
We built the following proposed systems with
about 150 M posts gathered from Twitter (ex-
cluding posts that contain ?@?, ?RT?, ?http? and
brackets, and posts that don?t contain any depen-
dency pairs). At the beginning of a dialogue or
the end of a conversation topic when the topic-
based approach didn?t generate system utterances,
the proposed approaches generated questions such
as ?What is your favorite movie?? to introduce
the next conversation topic. These questions were
gathered from utterances in the self-introduction
phase (about the five initial utterances) of each di-
alogue in our dialogue corpus. We manually se-
lected 109 questions that have no context from
179 questions gathered from our corpus, and chose
a question at random to generate each topic-
inductive question.
Proposed-All This approach used all found top-
ics: proper and common nouns, and predicates.
This approach is expected to be well-balanced
since it generates both content-focused utterances
and general WH-type questions.
Proposed-Nouns This approach used only
proper and common nouns, not predicates.
Proposed-Predicates This approach used only
predicates, not proper nor common nouns.
Retrieval-Self This approach resembles the IR-
response method in Ritter et al (2011). This ap-
proach chose the most similar posts to the user ut-
336
Prop.-All Prop.-Noun Prop.-Pred. Ret.-self Ret.-reply Human
(1) Number of superior prefs. vs. Prop.-All - 4 3 0?? 2? 9??
(2) Naturalness of dialogue flow 4.0 3.1?? 3.5 2.2?? 3.5 6.5??(3) Grammatical correctness 4.0 3.7 4.4 4.1 3.9 6.4??(4) Dialogue usefulness 3.7 2.9?? 3.9 2.7?? 3.5 6.1??(5) Ease of considering next utterance 3.5 3.4 4.4?? 2.4?? 3.3 5.7??(6) Variety of system utterances 4.3 4.0 4.2 2.9?? 4.0 5.5(7) User motivation 4.5 4.0? 4.7 3.7? 4.6 5.6??(8) System motivation that the user feels 4.7 4.1? 4.3 3.5?? 4.5 5.7?(9) Desire to chat again 3.7 2.8?? 3.3 2.0?? 3.1 5.7??
(10) Averaged score of all evaluation items 4.05 3.50?? 4.08 2.93?? 3.8? 5.9??
Table 1: System preferences and evaluation scores on 7-point Likert scale (?: p<.1, ??: p<.05)
terance from source posts using the Lucene2 infor-
mation retrieval library, which is an IDF-weighted
vector-space similarity. We built about 55 M
source-reply post pairs from Twitter.
Retrieval-Reply This approach is the same as
the IR-status method in Ritter et al (2011). It
chooses a reply post whose associated source posts
most resemble the user?s utterance.
Human As an upper-bound of these systems,
the user chats with a human using the same chat
interface used by the other systems.
Each dialogue took place over four minutes and
was conducted through a text chat interface, and
the orders of presentation of systems to partici-
pants was randomized. Since the humans have
to type their utterances and the systems can gen-
erate utterances much faster than typing, we set
the transition of the system utterances to about ten
seconds to avoid different response intervals be-
tween the systems and the humans. Table 5 shows
a dialogue example.
4.2 Results and Discussion
Table 1 shows that Proposed-All is ranked the
highest of all the automatic systems (1), and
achieves the best average evaluation scores (2)-
(10). Statistical analyses were performed using
the Binomial test for (1) and Welch?s t test for (2)
to (10). Proposed-All was ranked higher than the
retrieval-based approaches (10 of 10 participants
ranked Proposed-All higher than Retrieval-Self,
and 8 participants ranked Proposed-All higher
than Retrieval-Reply), but none of our three pro-
posed approaches was ranked significantly higher
than the others.
The evaluation scores also demonstrate the
characteristics of each approach. Proposed-Nouns
shows significantly low scores in dialogue flow
(2), dialogue usefulness (4), and system motiva-
tion (9). Since this approach is overly affected by
the nouns in the user utterances, users didn?t feel
that the system was actually thinking. Proposed-
Predicates shows a high score in ease of thinking
about the next utterance (5) since it generates WH-
type questions for which users can easily produce
answer utterances.
2http://lucene.apache.org
For conventional retrieval-based approaches,
contrary to Ritter et al (2011), Retrieval-Self
shows significantly lower scores in almost all
the evaluation items, and Retrieval-Reply shows
scores close to Proposed-All. These results re-
flect the retrieved corpus size, which is 40 times
larger than that of Ritter et al (2011). When
the retrieval performance improves, Retrieval-Self
returns posts that are too similar to user utter-
ances, while Retrieval-Reply can find appropri-
ate source posts. Retrieval-Reply shows almost
the same scores as Proposed-All for each single
evaluation metric, but Retrieval-Reply is inferior
to Proposed-All in the averaged evaluation items
(10). This is a reason why Retrieval-Reply is also
inferior in (1).
None of the systems approached human per-
formance. The users thought that the systems
were not able to respond to user utterances that
referred to the system itself, like personal ques-
tions; and that the systems didn?t understand user
utterances since the systems sometimes generate
a question that contains different but semantically
similar words to those used by the user, due to the
lack of thesaurus knowledge.
5 Conclusions
We proposed a novel open-domain utterance gen-
eration approach for a conversational dialogue
system that generates system utterances using
templates populated with topics and related words
extracted from a huge number of dependency
structures. Our chat experiments demonstrated
that our template-based approach generated sys-
tem utterances preferred over those produced
with retrieval-based approaches, and that WH-
type questions make it easy for users to produce
their next utterance. Our work also indicated
that template-based utterance generation, which is
considered a legacy approach, has potential when
the template-filling resource is huge. Future work
includes improving the data-driven topic selec-
tion in the proposed approach, the aggregation of
words with web-scale class structures like Tama-
gawa et al (2012), response generation for utter-
ances that describe the systems themselves, and
exploitation of information about the user to gen-
erate system utterances.
337
References
Timothy Bickmore and Justine Cassell. 2001. Re-
lational Agents: A Model and Implementation of
Building User Trust. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Sys-
tems, pages 396?403.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Kenji Imamura, Genichiro Kikui, and Norihito Yasuda.
2007. Japanese Dependency Parsing Using Sequen-
tial Labeling for Semi-Spoken Language. In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 225?228.
Sina Jafarpour and Christopher J.C. Burges. 2010. Fil-
ter, Rank, and Transfer the Knowledge: Learning
to Chat. Technical Report MSR-TR-2010-93, Mi-
crosoft.
Toyomi Meguro, Ryuichiro Higashinaka, Yasuhiro Mi-
nami, and Kohji Dohsaka. 2010. Controlling
Listening-oriented Dialogue using Partially Observ-
able Markov Decision Processes. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics, pages 761?769.
Alan Ritter, Colin Cherry, and William.B. Dolan.
2011. Data-Driven Response Generation in Social
Media. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 583?593.
Masahiro Shibata, Tomomi Nishiguchi, and Yoichi
Tomiura. 2009. Dialog System for Open-Ended
Conversation Using Web Documents. Informatica,
33:277?284.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliz-
abeth Shriberg, Rebecca Bates, Daniel Jurafsky,
Paul Taylor, Rachel Martin, Carol Van Ess-Dykema,
and Marie Meteer. 2000. Dialogue Act Mod-
eling for Automatic Tagging and Recognition of
Conversational Speech. Computational Linguistics,
26(3):339?373.
Susumu Tamagawa, Takeshi Morita, and Takahira Ya-
maguchi. 2012. Extracting Property Semantics
from Japanese Wikipedia. In Proceedings of the 8th
international conference on Active Media Technol-
ogy, pages 357?368.
Wilson Wong, Lawrence Cavedon, John Thangara-
jah, and Lin Padgham. 2012. Strategies for
Mixed-Initiative Conversation Management using
Question-Answer Pairs. In Proceedings of the 24th
International Conference on Computational Lin-
guistics, pages 2821?2834.
Koichiro Yoshino, Shinsuke Mori, and Tatsuya Kawa-
hara. 2011. Spoken Dialogue System based on In-
formation Extraction using Similarity of Predicate
Argument Structures. In Proceedings of the 12th
Annual Meeting of the Special Interest Group on
Discourse and Dialogue, pages 59?66.
Appendix
Topic-word Extracted topic-related words
Mt.Fuji beautiful, good, clear, white, huge (adj)Bread delicious, good, like, cute (adj)Eat I (?ga?: subject-case), rice (?wo?: target-case)together (?ni?: ?with?,? to?)
Table 2: Extracted topic-related words (translated
by authors)
Word type Template example
Proper nouns I hear that [Noun] is [adj], isn?t it?[Noun] ha [adj] deshitakke? (JP)General nouns Actually, I am weak in [Noun]...[Noun] ha chotto nigate desu... (JP)Adjectives and How does it [Noun]?adjectival nouns douiu huuni [Noun] nndesuka? (JP)Verbs and Where do you [Verb]?verbal nouns doko de [Verb] nndesuka? (JP)
Table 3: Templates for topic types (translated by
authors)
Dialogue act Example
Greetings Hello.
Sympathy Good./ Nice.
Non-sympathy Not so good..
Filler Uh.
Confirmation Really?/Oh, [Noun].
Table 4: Generated utterances for dialogue acts
(translated by authors)
Speaker Utterance
User Hello.System Hello. Do you like movies?User I like movies.System Good. I think movie is fun, isn?t it?User I like reading more than going to the movies.System Reading is also good.User I borrow books from my local library.System Nice. I hear that libraries are quiet.User Yes, they usually are.I used to study, write reports, and read books.
Table 5: Dialogues between user and Proposed-
All (translated by authors)
338
