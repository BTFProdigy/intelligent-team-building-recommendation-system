Proceedings of NAACL HLT 2007, pages 97?104,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Improving Diversity in Ranking using Absorbing Random Walks
Xiaojin Zhu Andrew B. Goldberg Jurgen Van Gael David Andrzejewski
Department of Computer Sciences
University of Wisconsin, Madison
Madison, WI 53705
{jerryzhu, goldberg, jvangael, andrzeje}@cs.wisc.edu
Abstract
We introduce a novel ranking algorithm
called GRASSHOPPER, which ranks items
with an emphasis on diversity. That is, the
top items should be different from each
other in order to have a broad coverage
of the whole item set. Many natural lan-
guage processing tasks can benefit from
such diversity ranking. Our algorithm is
based on random walks in an absorbing
Markov chain. We turn ranked items into
absorbing states, which effectively pre-
vents redundant items from receiving a
high rank. We demonstrate GRASSHOP-
PER?s effectiveness on extractive text sum-
marization: our algorithm ranks between
the 1st and 2nd systems on DUC 2004
Task 2; and on a social network analy-
sis task that identifies movie stars of the
world.
1 Introduction
Many natural language processing tasks involve
ranking a set of items. Sometimes we want the top
items to be not only good individually but also di-
verse collectively. For example, extractive text sum-
marization generates a summary by selecting a few
good sentences from one or more articles on the
same topic (Goldstein et al, 2000). This can be for-
mulated as ranking all the sentences, and taking the
top ones. A good sentence is one that is represen-
tative, i.e., similar to many other sentences, so that
it likely conveys the central meaning of the articles.
On the other hand, we do not want multiple near-
identical sentences. The top sentences should be di-
verse.
As another example, in information retrieval on
news events, an article is often published by multi-
ple newspapers with only minor changes. It is unde-
sirable to rank all copies of the same article highly,
even though it may be the most relevant. Instead,
the top results should be different and complemen-
tary. In other words, one wants ?subtopic diversity?
in retrieval results (Zhai et al, 2003).
The need for diversity in ranking is not unique to
natural language processing. In social network anal-
ysis, people are connected by their interactions, e.g.,
phone calls. Active groups of people have strong in-
teractions among them, but many groups may exist
with fewer interactions. If we want a list of people
that represent various groups, it is important to con-
sider both activity and diversity, and not to fill the
list with people from the same active groups.
Given the importance of diversity in ranking,
there has been significant research in this area. Per-
haps the most well-known method is maximum
marginal relevance (MMR) (Carbonell and Gold-
stein, 1998), as well as cross-sentence informational
subsumption (Radev, 2000), mixture models (Zhang
et al, 2002), subtopic diversity (Zhai et al, 2003),
diversity penalty (Zhang et al, 2005), and others.
The basic idea is to penalize redundancy by lowering
an item?s rank if it is similar to items already ranked.
However, these methods often treat centrality rank-
ing and diversity ranking separately, sometimes with
heuristic procedures.
97
We propose GRASSHOPPER (Graph Random-walk
with Absorbing StateS that HOPs among PEaks
for Ranking), a novel ranking algorithm that en-
courages diversity. GRASSHOPPER is an alternative
to MMR and variants, with a principled mathemat-
ical model and strong empirical performance. It
ranks a set of items such that: 1. A highly ranked
item is representative of a local group in the set,
i.e., it is similar to many other items (centrality);
2. The top items cover as many distinct groups as
possible (diversity); 3. It incorporates an arbitrary
pre-specified ranking as prior knowledge (prior).
Importantly GRASSHOPPER achieves these in a uni-
fied framework of absorbing Markov chain random
walks. The key idea is the following: We define
a random walk on a graph over the items. Items
which have been ranked so far become absorbing
states. These absorbing states ?drag down? the im-
portance of similar unranked states, thus encourag-
ing diversity. Our model naturally balances central-
ity, diversity, and prior. We discuss the algorithm
in Section 2. We present GRASSHOPPER?s empiri-
cal results on text summarization and social network
analysis in Section 3.
2 The GRASSHOPPER Algorithm
2.1 The Input
GRASSHOPPER requires three inputs: a graph W , a
probability distribution r that encodes the prior rank-
ing, and a weight ? ? [0, 1] that balances the two.
The user needs to supply a graph with n nodes,
one for each item. The graph is represented by an
n? n weight matrix W , where wij is the weight on
the edge from i to j. It can be either directed or undi-
rected. W is symmetric for undirected graphs. The
weights are non-negative. The graph does not need
to be fully connected: if there is no edge from item
i to j, then wij = 0. Self-edges are allowed. For ex-
ample, in text summarization one can create an undi-
rected, fully connected graph on the sentences. The
edge between sentences i, j has weight wij , their co-
sine similarity. In social network analysis one can
create a directed graph with wij being the number
of phone calls i made to j. The graph should be
constructed carefully to reflect domain knowledge.
For examples, see (Erkan and Radev, 2004; Mihal-
cea and Tarau, 2004; Pang and Lee, 2004).
The user can optionally supply an arbitrary rank-
ing on the items as prior knowledge. In this
case GRASSHOPPER can be viewed as a re-ranking
method. For example, in information retrieval,
the prior ranking can be the ranking by relevance
scores. In text summarization, it can be the po-
sition of sentences in the original article. (There
is evidence that the first few sentences in an ar-
ticle are likely good summaries.) Somewhat un-
conventionally, the prior ranking is represented as
a probability distribution r = (r1, ? ? ? , rn)? such
that ri ? 0,
?n
i=1 ri = 1. The highest-ranked item
has the largest probability, the next item has smaller
probability, and so on. A distribution gives the user
more control. For example ra = (0.1, 0.7, 0.2)?
and rb = (0.3, 0.37, 0.33)? both represent the same
ranking of items 2, 3, 1, but with different strengths.
When there is no prior ranking, one can let r =
(1/n, ? ? ? , 1/n)?, the uniform distribution.
2.2 Finding the First Item
We find the first item in GRASSHOPPER ranking by
teleporting random walks. Imagine a random walker
on the graph. At each step, the walker may do one of
two things: with probability ?, she moves to a neigh-
bor state1 according to the edge weights; otherwise
she is teleported to a random state according to the
distribution r. Under mild conditions (which are sat-
isfied in our setting, see below), the stationary distri-
bution of the random walk defines the visiting prob-
abilities of the nodes. The states with large probabil-
ities can be regarded as central items, an idea used
in Google PageRank (Page et al, 1998) and other in-
formation retrieval systems (Kurland and Lee, 2005;
Zhang et al, 2005), text summarization (Erkan and
Radev, 2004), keyword extraction (Mihalcea and Ta-
rau, 2004) and so on. Depending on ?, items high on
the user-supplied prior ranking r may also have large
stationary probabilities, which is a way to incorpo-
rate the prior ranking.
As an example, we created a toy data set with 300
points in Figure 1(a). There are roughly three groups
with different densities. We created a fully con-
nected graph on the data, with larger edge weights
if points are closer2. Figure 1(b) shows the station-
ary distribution of the random walk on the graph.
1We use state, node and item interchangeably.
2We use wij = exp(??xi ? xj?2/0.16), ? = 1.
98
0 5 100
2
4
6
8
0 5
10
05
100
0.005
0.01
0.015
g1
0 5
10
05
100
2
4
6
g2
0 5
10
05
100
0.5
1
1.5
g3
(a) (b) (c) (d)
Figure 1: (a) A toy data set. (b) The stationary distribution pi reflects centrality. The item with the largest
probability is selected as the first item g1. (c) The expected number of visits v to each node after g1 becomes
an absorbing state. (d) After both g1 and g2 become absorbing states. Note the diversity in g1, g2, g3 as they
come from different groups.
Items at group centers have higher probabilities, and
tighter groups have overall higher probabilities.
However, the stationary distribution does not ad-
dress diversity at all. If we were to rank the items
by their stationary distribution, the top list would be
dominated by items from the center group in Fig-
ure 1(b). Therefore we only use the stationary dis-
tribution to find the first item, and use a method
described in the next section to rank the remaining
items.
Formally we first define an n ? n raw transition
matrix P? by normalizing the rows of W : P?ij =
wij/
?n
k=1 wik, so that P?ij is the probability that the
walker moves to j from i. We then make the walk
a teleporting random walk P by interpolating each
row with the user-supplied initial distribution r:
P = ?P? + (1 ? ?)1r?, (1)
where 1 is an all-1 vector, and 1r? is the outer prod-
uct. If ? < 1 and r does not have zero elements,
our teleporting random walk P is irreducible (possi-
ble to go to any state from any state by teleporting),
aperiodic (the walk can return to a state after any
number of steps), all states are positive recurrent (the
expected return time to any state is finite) and thus
ergodic (Grimmett and Stirzaker, 2001). Therefore
P has a unique stationary distribution pi = P?pi.
We take the state with the largest stationary proba-
bility to be the first item g1 in GRASSHOPPER rank-
ing: g1 = argmaxni=1 pii.
2.3 Ranking the Remaining Items
As mentioned early, the key idea of GRASSHOPPER
is to turn ranked items into absorbing states. We
first turn g1 into an absorbing state. Once the ran-
dom walk reaches an absorbing state, the walk is ab-
sorbed and stays there. It is no longer informative to
compute the stationary distribution of an absorbing
Markov chain, because the walk will eventually be
absorbed. Nonetheless, it is useful to compute the
expected number of visits to each node before ab-
sorption. Intuitively, those nodes strongly connected
to g1 will have many fewer visits by the random
walk, because the walk tends to be absorbed soon
after visiting them. In contrast, groups of nodes far
away from g1 still allow the random walk to linger
among them, and thus have more visits. In Fig-
ure 1(c), once g1 becomes an absorbing node (rep-
resented by a circle ?on the floor?), the center group
is no longer the most prominent: nodes in this group
have fewer visits than the left group. Note now the
y-axis is the number of visits instead of probability.
GRASSHOPPER selects the second item g2 with the
largest expected number of visits in this absorbing
Markov chain. This naturally inhibits items similar
to g1 and encourages diversity. In Figure 1(c), the
item near the center of the left group is selected as
g2. Once g2 is selected, it is converted into an ab-
sorbing state, too. This is shown in Figure 1(d). The
right group now becomes the most prominent, since
both the left and center groups contain an absorbing
state. The next item g3 in ranking will come from the
right group. Also note the range of y-axis is smaller:
99
with more absorbing states, the random walk will be
absorbed sooner. The procedure is repeated until all
items are ranked. The name GRASSHOPPER reflects
the ?hopping? behavior on the peaks.
It is therefore important to compute the expected
number of visits in an absorbing Markov chain. Let
G be the set of items ranked so far. We turn the states
g ? G into absorbing states by setting Pgg = 1 and
Pgi = 0,?i 6= g. If we arrange items so that ranked
ones are listed before unranked ones, we can write
P as
P =
[
IG 0
R Q
]
. (2)
Here IG is the identity matrix on G. Submatrices R
and Q correspond to rows of unranked items, those
from (1). It is known that the fundamental matrix
N = (I ?Q)?1 (3)
gives the expected number of visits in the absorbing
random walk (Doyle and Snell, 1984). In particular
Nij is the expected number of visits to state j be-
fore absorption, if the random walk started at state i.
We then average over all starting states to obtain vj ,
the expected number of visits to state j. In matrix
notation,
v = N
?1
n? |G| , (4)
where |G| is the size of G. We select the state with
the largest expected number of visits as the next item
g|G|+1 in GRASSHOPPER ranking:
g|G|+1 = argmaxni=|G|+1 vi. (5)
The complete GRASSHOPPER algorithm is summa-
rized in Figure 2.
2.4 Some Discussions
To see how ? controls the tradeoff, note when ? = 1
we ignore the user-supplied prior ranking r, while
when ? = 0 one can show that GRASSHOPPER re-
turns the ranking specified by r.
Our data in Figure 1(a) has a cluster struc-
ture. Many methods have exploited such structure,
e.g., (Hearst and Pedersen, 1996; Leuski, 2001; Liu
and Croft, 2004). In fact, a heuristic algorithm is
to first cluster the items, then pick the central items
from each cluster in turn. But it can be difficult to
Input: W , r, ?
1. Create the initial Markov chain P from
W, r, ? (1).
2. Compute P ?s stationary distribution pi. Pick the
first item g1 = argmaxi pii.
3. Repeat until all items are ranked:
(a) Turn ranked items into absorbing
states (2).
(b) Compute the expected number of visits v
for all remaining items (4). Pick the next
item g|G|+1 = argmaxi vi
Figure 2: The GRASSHOPPER algorithm
determine the appropriate number and control the
shape of clusters. In contrast, GRASSHOPPER does
not involve clustering. However it is still able to
automatically take advantage of cluster structures in
the data.
In each iteration we need to compute the fun-
damental matrix (3). This involves inverting an
(n ? |G|) ? (n ? |G|) matrix, which is expensive.
However the Q matrix is reduced by one row and
one column in every iteration, but is otherwise un-
changed. This allows us to apply the matrix in-
version lemma (Sherman-Morrison-Woodbury for-
mula) (Press et al, 1992). Then we only need to
invert the matrix once in the first iteration, but not in
subsequent iterations. Space precludes a full discus-
sion, but we point out that it presents a significant
speed up. A Matlab implementation can be found
at http://www.cs.wisc.edu/?jerryzhu/
pub/grasshopper.m.
3 Experiments
3.1 Text Summarization
Multi-document extractive text summarization is a
prime application for GRASSHOPPER. In this task, we
must select and rank sentences originating from a
set of documents about a particular topic or event.
The goal is to produce a summary that includes all
the relevant facts, yet avoids repetition that may
result from using similar sentences from multiple
documents. In this section, we demonstrate that
100
GRASSHOPPER?s balance of centrality and diversity
makes it successful at this task. We present em-
pirical evidence that GRASSHOPPER achieves results
competitive with the top text summarizers in the
2004 Document Understanding Conference (http:
//duc.nist.gov). DUC is a yearly text summa-
rization community evaluation, with several tasks in
recent years concentrating on multi-document sum-
marization (described in more detail below).
Many successful text summarization systems
achieve a balance between sentence centrality and
diversity in a two-step process. Here we review the
LexRank system (Erkan and Radev, 2004), which
is most similar to our current approach. LexRank
works by placing sentences in a graph, with edges
based on the lexical similarity between the sentences
(as determined by a cosine measure). Each sen-
tence is then assigned a centrality score by finding
its probability under the stationary distribution of
a random walk on this graph. Unlike the similar
PageRank algorithm (Page et al, 1998), LexRank
uses an undirected graph of sentences rather than
Web pages, and the edge weights are either cosine
values or 0/1 with thresholding. The LexRank cen-
trality can be combined with other centrality mea-
sures, as well as sentence position information. Af-
ter this first step of computing centrality, a sec-
ond step performs re-ranking to avoid redundancy
in the highly ranked sentences. LexRank uses cross-
sentence informational subsumption (Radev, 2000)
to this end, but MMR (Carbonell and Goldstein,
1998) has also been widely used in the text sum-
marization community. These methods essentially
disqualify sentences that are too lexically similar to
sentences ranked higher by centrality. In short, sim-
ilar graph-based approaches to text summarization
rely on two distinct processes to measure each sen-
tence?s importance and ensure some degree of diver-
sity. GRASSHOPPER, on the other hand, achieves the
same goal in a unified procedure.
We apply GRASSHOPPER to text summarization in
the following manner. Our graph contains nodes
for all the sentences in a document set. We
used the Clair Library (http://tangra.si.
umich.edu/clair/clairlib) to split docu-
ments into sentences, apply stemming, and create
a cosine matrix for the stemmed sentences. Cosine
values are computed using TF-IDF vectors. As in
LexRank, edges in the graph correspond to text sim-
ilarity. To create a sparse graph, we use the cosine
threshold value of 0.1 obtained in (Erkan and Radev,
2004). Specifically, the edge weight between sen-
tence vectors si and sj is defined as
wij =
{
1 if s
?
i sj
?si???sj? > 0.1
0 otherwise
. (6)
The second input for GRASSHOPPER is an initial
ranking distribution, which we derive from the po-
sition of each sentence in its originating document.
Position forms the basis for lead-based summaries
(i.e., using the first N sentences as the summary)
and leads to very competitive summaries (Brandow
et al, 1995). We form an initial ranking for each
sentence by computing p??, where p is the position
of the sentence in its document, and ? is a posi-
tive parameter trained on a development dataset. We
then normalize over all sentences in all documents
to form a valid distribution r ? p?? that gives high
probability to sentences closer to the beginning of
documents. With a larger ?, the probability assigned
to later sentences decays more rapidly.
To evaluate GRASSHOPPER, we experimented with
DUC datasets. We train our parameters (? and ?)
using the DUC 2003 Task 2 data. This dataset con-
tains 30 document sets, each with an average of 10
documents about a news event. We test GRASSHOP-
PER?s performance on the DUC 2004 Task 2, Tasks
4a and 4b data. DUC 2004 Task 2 has 50 document
sets of 10 documents each. Tasks 4a and 4b explored
cross-lingual summarization. These datasets consist
of Arabic-to-English translations of news stories.
The documents in Task 4a are machine-translated,
while Task 4b?s are manually-translated. Note that
we handle the translated documents in exactly the
same manner as the English documents.
We evaluate our results using the standard text
summarization metric ROUGE (http://www.
isi.edu/?cyl/ROUGE/). This is a recall-based
measure of text co-occurrence between a machine-
generated summary and model summaries manually
created by judges. ROUGE metrics exist based on
bigram, trigram, and 4-gram overlap, but ROUGE-1
(based on unigram matching) has been found to cor-
relate best with human judgments (Lin and Hovy,
2003).
101
Using the DUC 2003 training data, we tuned ?
and ? on a small grid (? ? {0.125, 0.25, 0.5, 1.0};
? ? {0.0, 0.0625, 0.125, 0.25, 0.5, 0.95}). Specifi-
cally, for each of the 30 DUC 2003 Task 2 document
sets, we computed ROUGE-1 scores comparing our
generated summary to 4 model summaries. We av-
eraged the resulting ROUGE-1 scores across all 30
sets to produce a single average ROUGE-1 score to
assess a particular parameter configuration. After
examining the results for all 24 configurations, we
selected the best one: ? = 0.25 and ? = 0.5.
Table 1 presents our results using these parame-
ter values to generate summaries for the three DUC
2004 datasets. Note that the averages listed are ac-
tually averages over 4 model summaries per set, and
over all the sets. Following the standard DUC pro-
tocol, we list the confidence intervals calculated by
ROUGE using a bootstrapping technique. The fi-
nal column compares our results to the official sys-
tems that participated in the DUC 2004 evaluation.
GRASSHOPPER is highly competitive in these text
summarization tasks: in particular it ranks between
the 1st and 2nd automatic systems on 2004 Task 2.
The lower performance in Task 4a is potentially due
to the documents being machine-translated. If they
contain poorly translated sentences, graph edges
based on cosine similarity could be less meaning-
ful. For such a task, more advanced text processing
is probably required.
3.2 Social Network Analysis
As another application of GRASSHOPPER, we iden-
tify the nodes in a social network that are the most
prominent, and at the same time maximally cover
the network. A node?s prominence comes from its
intrinsic stature, as well as the prominence of the
nodes it touches. However, to ensure that the top-
ranked nodes are representative of the larger graph
structure, it is important to make sure the results are
not dominated by a small group of highly prominent
nodes who are closely linked to one another. This re-
quirement makes GRASSHOPPER a useful algorithm
for this task.
We created a dataset from the Internet Movie
Database (IMDb) that consists of all comedy movies
produced between 2000 and 2006, and have received
more than 500 votes by IMDb users. This results in
1027 movies. We form a social network of actors by
co-star relationship. Not surprisingly, actors from
the United States dominate our dataset, although a
total of 30 distinct countries are represented. We
seek an actor ranking such that the top actors are
prominent. However, we also want the top actors to
be diverse, so they represent comedians from around
the world.
This problem is framed as a GRASSHOPPER rank-
ing problem. For each movie, we considered only
the main stars, i.e., the first five cast members, who
tend to be the most important. The resulting list con-
tains 3452 unique actors. We formed a social net-
work where the nodes are the actors, and undirected
weighted edges connect actors who have appeared in
a movie together. The edge weights are equal to the
number of movies from our dataset in which both
actors were main stars. Actors are also given a self-
edge with weight 1. The co-star graph is given to
GRASSHOPPER as an input. For the prior actor rank-
ing, we simply let r be proportional to the number
of movies in our dataset in which an actor has ap-
peared. We set the weight ? = 0.95. It is important
to note that no country information is ever given to
GRASSHOPPER.
We use two measurements, ?country coverage?
and ?movie coverage?, to study the diversity and
prominence of the ranking produced by GRASSHOP-
PER. We compare GRASSHOPPER to two baselines:
ranking based solely on the number of movies an ac-
tor has appeared in, MOVIECOUNT, and a randomly
generated ranking, RANDOM.
First, we calculate ?country coverage? as the num-
ber of different countries represented by the top k ac-
tors, for all k values. Each actor represents a single
country?the country that the actor has appeared in
the most. We hypothesize that actors are more likely
to have co-star connections to actors within the same
country, so our social network may have, to some
extent, a clustering structure by country. ?Country
coverage? approximates the number of clusters rep-
resented at different ranks.
Figure 3(a) shows that country coverage grows
much more rapidly for GRASSHOPPER than for
MOVIECOUNT. That is, we see more comedians from
around the world ranked highly by GRASSHOPPER.
In contrast, the top ranks of MOVIECOUNT are dom-
inated by US actors, due to the relative abundance
of US movies on IMDb. Many other countries are
102
Number of Average GRASSHOPPER
Dataset Doc. Sets ROUGE-1 95% C.I. Unofficial Rank
DUC 2004 Task 2 50 0.3755 [0.3622, 0.3888] Between 1 & 2 of 34
DUC 2004 Task 4a 24 0.3785 [0.3613, 0.3958] Between 5 & 6 of 11
DUC 2004 Task 4b 24 0.4067 [0.3883, 0.4251] Between 2 & 3 of 11
Table 1: Text summarization results on DUC 2004 datasets. GRASSHOPPER was configured using parameters
tuned on the DUC 2003 Task 2 dataset. The rightmost column lists what our rank would have been if we
had participated in the DUC 2004 evaluation.
not represented until further down in the ranked
list. This demonstrates that GRASSHOPPER ranking is
successful in returning a more diverse ranking. Be-
cause of the absorbing states in GRASSHOPPER, the
first few highly ranked US actors encourage the se-
lection of actors from other regions of the co-star
graph, which roughly correspond to different coun-
tries. RANDOM achieves even higher country cover-
age initially, but is quickly surpassed by GRASSHOP-
PER. The initial high coverage comes from the ran-
dom selection of actors. However these randomly
selected actors are often not prominent, as we show
next.
Second, we calculate ?movie coverage? as the to-
tal number of unique movies the top k actors are
in. We expect that actors who have been in more
movies are more prominent. This is reasonable be-
cause we count an actor in a movie only if the actor
is among the top five actors from that movie. Our
counts thus exclude actors who had only small roles
in numerous movies. Therefore high movie cov-
erage roughly corresponds to ranking more promi-
nent actors highly. It is worth noting that this mea-
sure also partially accounts for diversity, since an
actor whose movies completely overlap with those
of higher-ranked actors contributes nothing to movie
coverage (i.e., his/her movies are already covered by
higher-ranked actors).
Figure 3(b) shows that the movie cover-
age of GRASSHOPPER grows more rapidly than
MOVIECOUNT, and much more rapidly than RAN-
DOM. The results show that, while the RANDOM
ranking is diverse, it is not of high quality be-
cause it fails to include many prominent actors in
its high ranks. This is to be expected of a ran-
dom ranking. Since the vast majority of the ac-
tors appear in only one movie, the movie cover-
age curve is roughly linear in the number of ac-
tors. By ranking more prominent actors highly, the
GRASSHOPPER and MOVIECOUNT movie coverage
curves grow faster. Many of the US actors highly
ranked by MOVIECOUNT are co-stars of one an-
other, so GRASSHOPPER outperforms MOVIECOUNT
in terms of movie coverage too.
We inspect the GRASSHOPPER ranking, and find
the top 5 actors to be Ben Stiller, Anthony Anderson,
Johnny Knoxville, Eddie Murphy and Adam San-
dler. GRASSHOPPER also brings many countries, and
major stars from those countries, into the high ranks.
Examples include Mads Mikkelsen (?synonym to
the great success the Danish film industry has had?),
Cem Yilmaz (?famous Turkish comedy actor, cari-
caturist and scenarist?), Jun Ji-Hyun (?face of South
Korean cinema?), Tadanobu Asano (?Japan?s an-
swer to Johnny Depp?), Aamir Khan (?prominent
Bollywood film actor?), and so on3. These actors
are ranked significantly lower by MOVIECOUNT.
These results indicate that GRASSHOPPER
achieves both prominence and diversity in ranking
actors in the IMDb co-star graph.
4 Conclusions
GRASSHOPPER ranking provides a unified approach
for achieving both diversity and centrality. We have
shown its effectiveness in text summarization and
social network analysis. As future work, one direc-
tion is ?partial absorption,? where at each absorbing
state the random walk has an escape probability to
continue the random walk instead of being absorbed.
Tuning the escape probability creates a continuum
between PageRank (if the walk always escapes) and
GRASSHOPPER (if always absorbed). In addition, we
will explore the issue of parameter learning, and
3Quotes from IMDb and Wikipedia.
103
0 100 200 300 400 500
0
5
10
15
20
25
30
k (number of actors)
N
um
be
r o
f c
ou
nt
rie
s 
co
ve
re
d
 
 
GRASSHOPPER
MOVIECOUNT
RANDOM
0 100 200 300 400 500
0
100
200
300
400
500
600
700
800
900
1000
k (number of actors)
N
um
be
r o
f m
ov
ie
s 
co
ve
re
d
 
 
GRASSHOPPER
MOVIECOUNT
RANDOM
(a) Country coverage (b) Movie coverage
Figure 3: (a) Country coverage at ranks up to 500, showing that GRASSHOPPER and RANDOM rankings are
more diverse than MOVIECOUNT. (b) Movie coverage at ranks up to 500, showing that GRASSHOPPER and
MOVIECOUNT have more prominent actors than RANDOM. Overall, GRASSHOPPER is the best.
user feedback (e.g., ?This item should be ranked
higher.?). We also plan to apply GRASSHOPPER to a
variety of tasks, including information retrieval (for
example ranking news articles on the same event as
in Google News, where many newspapers might use
the same report and thus result in a lack of diversity),
image collection summarization, and social network
analysis for national security and business intelli-
gence.
Acknowledgment We thank Mark Craven and the anony-
mous reviewers for helpful comments. This work is supported
in part by Wisconsin Alumni Research Foundation (WARF) and
NLM training grant 5T15LM07359.
References
R. Brandow, K. Mitze, and Lisa F. Rau. 1995. Automatic con-
densation of electronic publications by sentence selection.
Inf. Process. Manage., 31(5):675?685.
Jaime Carbonell and Jade Goldstein. 1998. The use of MMR,
diversity-based reranking for reordering documents and pro-
ducing summaries. In SIGIR?98.
P.G. Doyle and J.L. Snell. 1984. Random Walks and Electric
Networks. Mathematical Assoc. of America.
Gu?nes? Erkan and Dragomir R. Radev. 2004. LexRank: Graph-
based centrality as salience in text summarization. Journal
of Artificial Intelligence Research.
Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and Mark
Kantrowitz. 2000. Multi-document summarization by sen-
tence extraction. In NAACL-ANLP 2000 Workshop on Auto-
matic summarization, pages 40?48.
Geoffrey R. Grimmett and David R. Stirzaker. 2001. Proba-
bility and Random Processes. Oxford Science Publications,
third edition.
Marti A. Hearst and Jan O. Pedersen. 1996. Reexamining
the cluster hypothesis: Scatter/gather on retrieval results. In
SIGIR-96.
Oren Kurland and Lillian Lee. 2005. PageRank without hyper-
links: Structural re-ranking using links induced by language
models. In SIGIR?05.
Anton Leuski. 2001. Evaluating document clustering for inter-
active information retrieval. In CIKM?01.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evalua-
tion of summaries using n-gram co-occurrence statistics. In
NAACL?03, pages 71?78.
Xiaoyong Liu and W. Bruce Croft. 2004. Cluster-based re-
trieval using language models. In SIGIR?04.
Rada Mihalcea and Paul Tarau. 2004. TextRank: Bringing
order into texts. In EMNLP?04.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Wino-
grad. 1998. The PageRank citation ranking: Bringing order
to the web. Technical report, Stanford Digital Library Tech-
nologies Project.
Bo Pang and Lillian Lee. 2004. A sentimental education: Sen-
timent analysis using subjectivity summarization based on
minimum cuts. In ACL, pages 271?278.
W.H. Press, S.A. Teukolsky, W.T. Vetterling, and B.P. Flannery.
1992. Numerical recipes in C: the art of scientific comput-
ing. Cambridge University Press New York, NY, USA.
Dragomir Radev. 2000. A common theory of information fu-
sion from multiple text sources, step one: Cross-document
structure. In Proceedings of the 1st ACL SIGDIAL Workshop
on Discourse and Dialogue.
ChengXiang Zhai, William W. Cohen, and John Lafferty. 2003.
Beyond independent relevance: Methods and evaluation
metrics for subtopic retrieval. In SIGIR?03.
Yi Zhang, Jamie Callan, and Thomas Minka. 2002. Novelty
and redundancy detection in adaptive filtering. In SIGIR?02.
Benyu Zhang, Hua Li, Yi Liu, Lei Ji, Wensi Xi, Weiguo Fan,
Zheng Chen, and Wei-Ying Ma. 2005. Improving web
search results using affinity graph. In SIGIR?05.
104
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 263?271,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
May All Your Wishes Come True:
A Study of Wishes and How to Recognize Them
Andrew B. Goldberg, Nathanael Fillmore, David Andrzejewski
Zhiting Xu, Bryan Gibson, Xiaojin Zhu
Computer Sciences Department, University of Wisconsin-Madison, Madison, WI 53706, USA
{goldberg, nathanae, andrzeje, zhiting, bgibson, jerryzhu}@cs.wisc.edu
Abstract
A wish is ?a desire or hope for something
to happen.? In December 2007, people from
around the world offered up their wishes to
be printed on confetti and dropped from the
sky during the famous New Year?s Eve ?ball
drop? in New York City?s Times Square. We
present an in-depth analysis of this collection
of wishes. We then leverage this unique re-
source to conduct the first study on building
general ?wish detectors? for natural language
text. Wish detection complements traditional
sentiment analysis and is valuable for collect-
ing business intelligence and insights into the
world?s wants and desires. We demonstrate
the wish detectors? effectiveness on domains
as diverse as consumer product reviews and
online political discussions.
1 Introduction
Each year, New York City rings in the New Year
with the famous ?ball drop? in Times Square. In
December 2007, the Times Square Alliance, co-
producer of the Times Square New Year?s Eve Cele-
bration, launched a Web site called the Virtual Wish-
ing Wall1 that allowed people around the world to
submit their New Year?s wishes. These wishes were
then printed on confetti and dropped from the sky
at midnight on December 31, 2007 in sync with the
ball drop.
We obtained access to this set of nearly 100,000
New Year?s wishes, which we call the ?WISH cor-
pus.? Table 1 shows a selected sample of the WISH
1http://www.timessquarenyc.org/nye/nye interactive.html
corpus. Some are far-reaching fantasies and aspi-
rations, while others deal with everyday concerns
like economic and medical distress. We analyze this
first-of-its-kind corpus in Section 2.
The New Oxford American Dictionary defines
?wish? as ?a desire or hope for something to hap-
pen.? How wishes are expressed, and how such
wishful expressions can be automatically recog-
nized, are open questions in natural language pro-
cessing. Leveraging the WISH corpus, we conduct
the first study on building general ?wish detectors?
for natural language text, and demonstrate their ef-
fectiveness on domains as diverse as consumer prod-
uct reviews and online political discussions. Such
wish detectors have tremendous value in collecting
business intelligence and public opinions. We dis-
cuss the wish detectors in Section 3, and experimen-
tal results in Section 4.
1.1 Relation to Prior Work
Studying wishes is valuable in at least two aspects:
1. Being a special genre of subjective expression,
wishes add a novel dimension to sentiment analy-
sis. Sentiment analysis is often used as an auto-
matic market research tool to collect valuable busi-
ness intelligence from online text (Pang and Lee,
2008; Shanahan et al, 2005; Koppel and Shtrim-
berg, 2004; Mullen and Malouf, 2008). Wishes
differ from the recent focus of sentiment analysis,
namely opinion mining, by revealing what people
explicitly want to happen, not just what they like or
dislike (Ding et al, 2008; Hu and Liu, 2004). For ex-
ample, wishes in product reviews could contain new
feature requests. Consider the following (real) prod-
263
514 peace on earth
351 peace
331 world peace
244 happy new year
112 love
76 health and happiness
75 to be happy
51 i wish for world peace
21 i wish for health and happiness for my family
21 let there be peace on earth
16 i wish u to call me if you read this 555-1234
16 to find my true love
8 i wish for a puppy
7 for the war in iraq to end
6 peace on earth please
5 a free democratic venezuela
5 may the best of 2007 be the worst of 2008
5 to be financially stable
1 a little goodness for everyone would be nice
1 i hope i get accepted into a college that i like
1 i wish to get more sex in 2008
1 please let name be healthy and live all year
1 to be emotionally stable and happy
1 to take over the world
Table 1: Example wishes and their frequencies in the
WISH corpus.
uct review excerpt: ?Great camera. Indoor shots
with a flash are not quite as good as 35mm. I wish
the camera had a higher optical zoom so that I could
take even better wildlife photos.? The first sentence
contains positive opinion, the second negative opin-
ion. However, wishful statements like the third sen-
tence are often annotated as non-opinion-bearing in
sentiment analysis corpora (Hu and Liu, 2004; Ding
et al, 2008), even though they clearly contain im-
portant information. An automatic ?wish detector?
text-processing tool can be useful for product manu-
facturers, advertisers, politicians, and others looking
to discover what people want.
2. Wishes can tell us a lot about people: their in-
nermost feelings, perceptions of what they?re lack-
ing, and what they desire (Speer, 1939). Many
psychology researchers have attempted to quantify
the contents of wishes and how they vary with
factors such as location, gender, age, and per-
sonality type (Speer, 1939; Milgram and Riedel,
1969; Ehrlichman and Eichenstein, 1992; King and
Broyles, 1997). These studies have been small scale
with only dozens or hundreds of participants. The
WISH corpus provides the first large-scale collec-
tion of wishes as a window into the world?s desires.
Beyond sentiment analysis, classifying sentences
as wishes is an instance of non-topical classifica-
tion. Tasks under this heading include compu-
tational humor (Mihalcea and Strapparava, 2005),
genre classification (Boese and Howe, 2005), au-
thorship attribution (Argamon and Shimoni, 2003),
and metaphor detection (Krishnakumaran and Zhu,
2007), among others (Mishne et al, 2007; Mihal-
cea and Liu, 2006). We share the common goal of
classifying text into a unique set of target categories
(in our case, wishful and non-wishful), but use dif-
ferent techniques catered to our specific task. Our
feature-generation technique for wish detection re-
sembles template-based methods for information ex-
traction (Brin, 1999; Agichtein and Gravano, 2000).
2 Analyzing the WISH Corpus
We analyze the WISH corpus with a variety of sta-
tistical methods. Our analyses not only reveal what
people wished for on New Year?s Eve, but also pro-
vide insight for the development of wish detectors in
Section 3.
The complete WISH corpus contains nearly
100,000 wishes collected over a period of 10 days
in December 2007, most written in English, with the
remainder in Portuguese, Spanish, Chinese, French,
and other languages. For this paper, we consider
only the 89,574 English wishes. Most of these En-
glish wishes contain optional geographic meta data
provided by the wisher, indicating a variety of coun-
tries (not limited to English-speaking) around the
world. We perform minimal preprocessing, includ-
ing TreeBank-style tokenization, downcasing, and
punctuation removal. Each wish is treated as a sin-
gle entity, regardless of whether it contains multiple
sentences. After preprocessing, the average length
of a wish is 8 tokens.
2.1 The Topic and Scope of Wishes
As a first step in understanding the content of the
wishes, we asked five annotators to manually an-
notate a random subsample of 5,000 wishes. Sec-
tions 2.1 and 2.2 report results on this subsample.
The wishes were annotated in terms of two at-
264
(a) Topic of Wishes
(b) Scope of Wishes
Figure 1: Topic and scope distributions based on manual
annotations of a random sample of 5,000 wishes in the
WISH corpus.
tributes: topic and scope. We used 11 pre-defined
topic categories, and their distribution in this sub-
sample of the WISH corpus is shown in Figure 1(a).
The most frequent topic is love, while health,
happiness, and peace are also common themes.
Many wishes also fell into an other category, in-
cluding specific individual requests (?i wish for a
new puppy?), solicitations or advertisements (?call
me 555-1234?, ?visit website.com?), or sinister
thoughts (?to take over the world?).
The 5,000 wishes were also manually assigned
a scope. The scope of a wish refers to the range
of people that are targeted by the wish. We used
6 pre-defined scope categories: self (?I want to be
happy?), family (?For a cure for my husband?), spe-
cific person by name (?Prayers for name?), country
(?Bring our troops home!?), world (?Peace to every-
one in the world?), and other. In cases where mul-
tiple scope labels applied, the broadest scope was
selected. Figure 1(b) shows the scope distribution.
It is bimodal: over one third of the wishes are nar-
rowly directed at one?s self, while broad wishes at
the world level are also frequent. The in-between
scopes are less frequent.
2.2 Wishes Differ by Geographic Location
As mentioned earlier, wishers had the option to enter
a city/country when submitting wishes. Of the man-
ually annotated wishes, about 4,000 included valid
location information, covering all 50 states in the
U.S., and all continents except Antarctica.
We noticed a statistically significant difference
between wishes submitted from the United States
(about 3600) versus non-U.S. (about 400), both in
terms of their topic and scope distributions. For each
comparison, we performed a Pearson ?2-test using
location as the explanatory variable and either topic
or scope as the response variable.2 The null hypoth-
esis is that the variables are independent. For both
tests we reject the null hypothesis, with p < 0.001
for topic, and p = 0.006 for scope. This indicates a
dependence between location and topic/scope. As-
terisks in Figure 2 denote the labels that differ sig-
nificantly between U.S. and non-U.S. wishes.3
In particular, we observed that there are signif-
icantly more wishes about love, peace, and travel
from non-U.S. locales, and more about religion from
the U.S. There are significantly more world-scoped
wishes from non-U.S. locales, and more country-
and family-scoped wishes from the U.S.
We also compared wishes from ?red states? ver-
sus ?blue states? (U.S. states that voted a majority
for the Republican and Democratic presidential can-
didates in 2008, respectively), but found no signifi-
cant differences.
2The topic test examined a 2 ? 11 contingency table, while
the scope test used a 2 ? 6 contingency table. In both tests, all
of the cells in the tables had an expected frequency of at least 5,
so the ?2 approximation is valid.
3To identify the labels that differ significantly by location,
we computed the standardized residuals for the cells in the two
contingency tables. Standardized residuals are approximately
N (0, 1)-distributed and can be used to locate the major con-
tributors to a significant ?2-test statistic (Agresti, 2002). The
asterisks in Figure 2 indicate the surprisingly large residuals,
i.e., the difference between observed and expected frequencies
is outside a 95% confidence interval.
265
(a) Wish topics differ by Location
(b) Wish scopes differ by Location
Figure 2: Geographical breakdown of topic and scope
distributions based on approximately 4,000 location-
tagged wishes. Asterisks indicate statistically significant
differences.
2.3 Wishes Follow Zipf?s Law
We now move beyond the annotated subsample and
examine the full set of 89,574 English wishes. We
noticed that a small fraction (4%) of unique wishes
account for a relatively large portion (16%) of wish
occurrences, while there are also many wishes that
only occur once. The question naturally arises: do
wishes obey Zipf?s Law (Zipf, 1932; Manning and
Schu?tze, 1999)? If so, we should expect the fre-
quency of a unique wish to be inversely proportional
to its rank, when sorted by frequency. Figure 3
plots rank versus frequency on a log-log scale and
reveals an approximately linear negative slope, thus
suggesting that wishes do follow Zipf?s law. It also
shows that low-occurrence wishes dominate, hence
learning might be hindered by data sparseness.
2.4 Latent Topic Modeling for Wishes
The 11 topics in Section 2.1 were manually pre-
defined based on domain knowledge. In contrast,
in this section we applied Latent Dirichlet Alloca-
tion (LDA) (Blei et al, 2003) to identify the latent
topics in the full set of 89,574 English wishes in an
100 101 102 103 104 10510
0
101
102
103 peace
to find my true love
to take overthe world
log(rank)
log(f
requ
ency
)
Figure 3: The rank vs. frequency plot of wishes, approx-
imately obeying Zipf?s law. Note the log-log scale.
unsupervised fashion. The goal is to validate and
complement the study in Section 2.1.
To apply LDA to the wishes, we treated each indi-
vidual wish as a short document. We used 12 topics,
Collapsed Gibbs Sampling (Griffiths and Steyvers,
2004) for inference, hyperparameters ? = 0.5 and
? = 0.1, and ran Markov Chain Monte Carlo for
2000 iterations.
The resulting 12 LDA topics are shown in Ta-
ble 2, in the form of the highest probability words
p(word|topic) in each topic. We manually added
summary descriptors for readability. With LDA, it is
also possible to observe which words were assigned
to which topics in each wish. For example, LDA as-
signed most words in the wish ?world(8) peace(8)
and my friends(4) in iraq(1) to come(1) home(1)?
to two topics: peace and troops (topic numbers in
parentheses). Interestingly, these LDA topics largely
agree with the pre-defined topics in Section 2.1.
3 Building Wish Detectors
We now study the novel NLP task of wish detection,
i.e., classifying individual sentences as being wishes
or not. Importantly, we want our approach to trans-
fer to domains other than New Year?s wishes, in-
cluding consumer product reviews and online politi-
cal discussions. It should be pointed out that wishes
are highly domain dependent. For example, ?I wish
for world peace? is a common wish on New Year?s
Eve, but is exceedingly rare in product reviews; and
vice versa: ?I want to have instant access to the vol-
ume? may occur in product reviews, but is an un-
266
Topic Summary Top words in the topic, sorted by p(word|topic)
0 New Year year, new, happy, 2008, best, everyone, great, years, wishing, prosperous, may, hope
1 Troops all, god, home, come, may, safe, s, us, bless, troops, bring, iraq, return, 2008, true, dreams
2 Election wish, end, no, more, 2008, war, stop, president, paul, not, ron, up, free, less, bush, vote
3 Life more, better, life, one, live, time, make, people, than, everyone, day, wish, every, each
4 Prosperity health, happiness, good, family, friends, all, love, prosperity, wealth, success, wish, peace
5 Love love, me, find, wish, true, life, meet, want, man, marry, call, someone, boyfriend, fall, him
6 Career get, wish, job, out, t, hope, school, better, house, well, want, back, don, college, married
7 Lottery wish, win, 2008, money, want, make, become, lottery, more, great, lots, see, big, times
8 Peace peace, world, all, love, earth, happiness, everyone, joy, may, 2008, prosperity, around
9 Religion love, forever, jesus, know, loves, together, u, always, 2, 3, 4, much, best, mom, christ
10 Family healthy, happy, wish, 2008, family, baby, life, children, long, safe, husband, stay, marriage
11 Health com, wish, s, me, lose, please, let, cancer, weight, cure, mom, www, mother, visit, dad
Table 2: Wish topics learned from Latent Dirichlet Allocation. Words are sorted by p(word|topic).
likely New Year?s wish. For this initial study, we do
assume that there are some labeled training data in
the target domains of interest.
To transfer the knowledge learned from the out-
of-domain WISH corpus to other domains, our key
insight is the following: while the content of wishes
(e.g., ?world peace?) may not transfer across do-
mains, the ways wishes are expressed (e.g., ?I wish
for ?) may. We call these expressions wish tem-
plates. Our novel contribution is an unsupervised
method for discovering candidate templates from the
WISH corpus which, when applied to other target
domains, improve wish detection in those domains.
3.1 Two Simple Wish Detectors
Before describing our template discovery method,
we first describe two simple wish detectors, which
serve as baselines.
1. [Manual]: It may seem easy to locate
wishes. Perhaps looking for sentences containing
the phrases ?i wish,? ?i hope,? or some other sim-
ple patterns is sufficient for identifying the vast ma-
jority of wishes in a domain. To test this hypothe-
sis, we asked two native English speakers (not the
annotators, nor affiliated with the project; no expo-
sure to any of the wish datasets) to come up with
text patterns that might be used to express wishes.
They were shown three dictionary definitions of ?to
wish (v)? and ?wish (n)?. They produced a ranked
list of 13 templates; see Table 3. The underscore
matches any string. These templates can be turned
into a simple rule-based classifier: If part of a sen-
tence matches one of the templates, the sentence is
i wish
i hope
i want
hopefully
if only
would be better if
would like if
should
would that
can?t believe didn?t
don?t believe didn?t
do want
i can has
Table 3: Manual templates for identifying wishes.
classified as a wish. By varying the depth of the list,
one can produce different precision/recall behaviors.
Overall, we expect [Manual] to have relatively high
precision but low recall.
2. [Words]: Another simple method for detecting
wishes is to train a standard word-based text clas-
sifier using the labeled training set in the target do-
main. Specifically, we represent each sentence as
a binary word-indicator vector, normalized to sum
to 1. We then train a linear Support Vector Ma-
chine (SVM). This method may have higher recall,
but precision may suffer. For instance, the sentence
?Her wish was carried out by her husband? is not a
wish, but could be misclassified as one because of
the word ?wish.?
Note that neither of the two baseline methods uses
the WISH corpus.
267
3.2 Automatically Discovering Wish Templates
We now present our method to automatically dis-
cover high quality wish templates using the WISH
corpus. The key idea is to exploit redundancy in
how the same wish content is expressed. For ex-
ample, as we see in Table 1, both ?world peace? and
?i wish for world peace? are common wishes. Sim-
ilarly, both ?health and happiness? and ?i wish for
health and happiness? appear in the WISH corpus.
It is thus reasonable to speculate that ?i wish for ?
is a good wish template. Less obvious templates can
be discovered in this way, too, such as ?let there be
? from ?peace on earth? and ?let there be peace
on earth.?
We formalize this intuition as a bipartite graph, il-
lustrated in Figure 4. Let W = {w1, . . . , wn} be the
set of unique wishes in the WISH corpus. The bi-
partite graph has two types of nodes: content nodes
C and template nodes T , and they are generated as
follows. If a wish wj (e.g., ?i wish for world peace?)
contains another wish wi (e.g., ?world peace?), we
create a content node c1 = wi and a template node
t1 =?i wish for ?. We denote this relationship by
wj = c1+ t1. Note the order of c1 and t1 is insignif-
icant, as how the two combine is determined by the
underscore in t1, and wj = t1 + c1 is just fine. In
addition, we place a directed edge from c1 to t1 with
edge weight count(wj), the frequency of wish wj in
the WISH corpus. Then, a template node appears to
be a good one if many heavy edges point to it.
On the other hand, a template is less desirable
if it is part of a content node. For example, when
wj =?health and happiness? and wi =?health?, we
create the template t2 =? and happiness? and the
content node c3 = wi. If there is another wish
wk =?i wish for health and happiness?, then there
will be a content node c2 = wj . The template t2
thus contains some content words (since it matches
c2), and may not generalize well in a new domain.
We capture this by backward edges: if ?c? ? C, and
? string s (s not necessarily in C or W ) such that
c? = s+ t, we add a backward edge from t to c? with
edge weight count(c?).
Based on such considerations, we devised the fol-
lowing scheme for scoring templates:
score(t) = in(t)? out(t), (1)
health and happiness
c1
c2
c3
t1
t2
i wish for ___
___ and happiness
world peace
health
count(c1+t1)
count(c2)
Figure 4: The bipartite graph to create templates.
where in(t) is the in-degree of node t, defined as the
sum of edge weights coming into t; out(t) is the out-
degree of node t, defined similarly. In other words, a
template receives a high score if it is ?used? by many
frequent wishes but does not match many frequent
content-only wishes. To create the final set of tem-
plate features, we apply the threshold score(t) ? 5.
This produces a final list of 811 templates. Table 4
lists some of the top templates ranked by score(t).
While some of these templates still contain time- or
scope-related words (?for my family?), they are de-
void of specific topical content. Notice that we have
automatically identified several of the manually de-
rived templates in Table 3, and introduce many new
variations that a learning algorithm can leverage.
Top 10 Others in Top 200
in 2008 i want to
i wish for for everyone
i wish i hope
i want my wish is
this year please
i wish in 2008 wishing for
i wish to may you
for my family i wish i had
i wish this year to finally
in the new year for my family to have
Table 4: Top templates according to Equation 1.
3.3 Learning with Wish Template Features
After discovering wish templates as described
above, we use them as features for learning in a new
domain (e.g., product reviews). For each sentence in
the new domain, we assign binary features indicat-
ing which templates match the sentence. Two types
of matching are possible. Strict matching requires
that the template must match an entire sentence from
beginning to end, with at least one word filling in for
the underscore. (All matching during the template
generation process was strict.) Non-strict matching
268
0 0.2 0.4 0.6 0.8 10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Prec
ision
 
 
ManualWordsTemplatesWords + Templates
Figure 5: Politics domain precision-recall curves.
requires only that template match somewhere within
a sentence. Rather than choose one type of match-
ing, we create both strict and non-strict template fea-
tures (1622 binary features total) and let the machine
learning algorithm decide what is most useful.
Our third wish detector, [Templates], is a linear
SVM with the 1622 binary wish template features.
Our fourth wish detector, [Words + Templates], is
a linear SVM with both template and word features.
4 Experimental Results
4.1 Target Domains and Experimental Setup
We experimented with two domains, manually la-
beled at the sentence-level as wishes or non-wishes.4
Example wishes are listed in Table 6.
Products. Consumer product reviews: 1,235 sen-
tences selected from a collection of amazon.com and
cnet.com reviews (Hu and Liu, 2004; Ding et al,
2008). 12% of the sentences are labeled as wishes.
Politics. Political discussion board postings:
6,379 sentences selected from politics.com (Mullen
and Malouf, 2008). 34% are labeled as wishes.
We automatically split the corpora into sen-
tences using MxTerminator (Reynar and Ratna-
parkhi, 1997). As preprocessing before learning, we
tokenized the text in the Penn TreeBank style, down-
4These wish-annotated corpora are available for download
at http://pages.cs.wisc.edu/?goldberg/wish data.
0 0.2 0.4 0.6 0.8 10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Prec
ision
 
 
ManualWordsTemplatesWords + Templates
Figure 6: Products domain precision-recall curves.
cased, and removed all punctuation.
For all four wish detectors, we performed 10-fold
cross validation. We used the default parameter in
SVMlight for all trials (Joachims, 1999). As the
data sets are skewed, we compare the detectors us-
ing precision-recall curves and the area under the
curve (AUC). For the manual baseline, we produce
the curve by varying the number of templates ap-
plied (in rank order), which gradually predicts more
sentences as wishes (increasing recall at the expense
of precision). A final point is added at recall 1.0,
corresponding to applying an empty template that
matches all sentences. For the SVM-based meth-
ods, we vary the threshold applied to the real-valued
margin prediction to produce the curves. All curves
are interpolated, and AUC measures are computed,
using the techniques of (Davis and Goadrich, 2006).
4.2 Results
Figure 5 shows the precision-recall curves for the
Politics corpus. All curves are averages over 10
folds (i.e., for each of 100 evenly spaced, interpo-
lated recall points, the 10 precision values are aver-
aged). As expected, [Manual] can be very precise
with low recall?only the very top few templates
achieve high precision and pick out a small num-
ber of wishes with ?i wish? and ?i hope.? As we
introduce more templates to cover more true wishes,
precision drops off quickly. [Templates] is similar,
269
Corpus [Manual] [Words] [Templates] [Words + Templates]
Politics 0.67? 0.03 0.77? 0.03 0.73? 0.03 0.80? 0.03
Products 0.49? 0.13 0.52? 0.16 0.47? 0.16 0.56? 0.16
Table 5: AUC results (10-fold averages ? one standard deviation).
Products:
the only area i wish apple had improved upon would be the screen
i just want music to eminate from it when i want how i want
the dial on the original zen was perfect and i wish it was on this model
i would like album order for my live albums and was just wondering
Politics:
all children should be allowed healthcare
please call on your representatives in dc and ask them to please stop the waste in iraq
i hope that this is a new beginning for the middle east
may god bless and protect the brave men and that we will face these dangers in the future
Table 6: Example target-domain wishes correctly identified by [Words + Templates].
with slightly better precision in low recall regions.
[Words] is the opposite: bad in high recall but good
in low recall regions. [Words + Templates] is the
best, taking the best from both kinds of features to
dominate other curves. Table 5 shows the average
AUC across 10 folds. [Words + Templates] is sig-
nificantly better than all other detectors under paired
t-tests (p = 1 ? 10?7 vs. [Manual], p = 0.01 vs.
[Words], and p = 4 ? 10?7 vs. [Templates]). All
other differences are statistically significant, too.
Figure 6 shows the precision-recall curves for
the Products corpus. Again, [Words + Templates]
mostly dominates other detectors. In terms of av-
erage AUC across folds (Table 5), [Words + Tem-
plates] is also the best. However, due to the small
size of this corpus, the AUC values have high vari-
ance, and the difference between [Words + Tem-
plates] and [Words] is not statistically significant un-
der a paired t-test (p = 0.16).
Finally, to understand what is being learned in
more detail, we take a closer look at the SVM mod-
els? weights for one fold of the Products corpus
(Table 7). The most positive and negative features
make intuitive sense. Note that [Words + Templates]
seems to rely on templates for selecting wishes and
words for excluding non-wishes. This partially ex-
plains the synergy of combining the feature types.
Sign [Words] [Templates] [Words +Templates]
+ wish i hope hoping
+ hope i wish i hope
+ hopefully hoping i just want
+ hoping i just want i wish
+ want i would like i would like
- money family micro
- find forever about
- digital let me fix
- again d digital
- you for my dad you
Table 7: Features with the largest magnitude weights in
the SVM models for one fold of the Products corpus.
5 Conclusions and Future Work
We have presented a novel study of wishes from
an NLP perspective. Using the first-of-its-kind
WISH corpus, we generated domain-independent
wish templates that improve wish detection perfor-
mance across product reviews and political discus-
sion posts. Much work remains in this new research
area, including the creation of more types of fea-
tures. Also, due to the difficulty in obtaining wish-
annotated training data, we plan to explore semi-
supervised learning for wish detection.
Acknowledgements We thank the Times Square Al-
liance for providing the WISH corpus, and the Wisconsin
Alumni Research Foundation. AG is supported in part by
a Yahoo! Key Technical Challenges Grant.
270
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
Extracting relations from large plain-text collections.
In In Proceedings of the 5th ACM International Con-
ference on Digital Libraries, pages 85?94.
Alan Agresti. 2002. Categorical Data Analysis. Wiley-
Interscience, second edition.
Shlomo Argamon and Anat Rachel Shimoni. 2003. Au-
tomatically categorizing written texts by author gen-
der. Literary and Linguistic Computing, 17:401?412.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Elizabeth Sugar Boese and Adele Howe. 2005. Genre
classification of web documents. In Proceedings of
the 20th National Conference on Artificial Intelligence
(AAAI-05), Poster paper.
Sergey Brin. 1999. Extracting patterns and relations
from the world wide web. In WebDB ?98: Selected
papers from the International Workshop on The World
Wide Web and Databases, pages 172?183. Springer-
Verlag.
Jesse Davis and Mark Goadrich. 2006. The relationship
between precision-recall and roc curves. In ICML ?06:
Proceedings of the 23rd international conference on
Machine learning, New York, NY, USA. ACM.
Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A
holistic lexicon-based approach to opinion mining. In
WSDM ?08: Proceedings of the international confer-
ence on Web search and web data mining, pages 231?
240. ACM.
Howard Ehrlichman and Rosalind Eichenstein. 1992.
Private wishes: Gender similarities and difference.
Sex Roles, 26(9):399?422.
Thomas Griffiths and Mark Steyvers. 2004. Finding sci-
entific topics. Proceedings of the National Academy of
Sciences, 101(suppl. 1):5228?5235.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of KDD ?04,
the ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 168?177. ACM
Press.
Thorsten Joachims. 1999. Making large-scale svm
learning practical. In B. Scho?lkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods - Sup-
port Vector Learning. MIT Press.
Laura A. King and Sheri J. Broyles. 1997. Wishes, gen-
der, personality, and well-being. Journal of Personal-
ity, 65(1):49?76.
Moshe Koppel and Itai Shtrimberg. 2004. Good news
or bad news? let the market decide. In AAAI Spring
Symposium on Exploring Attitude and Affect in Text,
pages 86?88.
Saisuresh Krishnakumaran and Xiaojin Zhu. 2007.
Hunting elusive metaphors using lexical resources.
In Proceedings of the Workshop on Computational
Approaches to Figurative Language, pages 13?20,
Rochester, New York, April. Association for Compu-
tational Linguistics.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language Process-
ing. The MIT Press, Cambridge, Massachusetts.
Rada Mihalcea and Hugo Liu. 2006. A corpus-based ap-
proach to finding happiness. In Proceedings of AAAI-
CAAW-06, the Spring Symposia on Computational Ap-
proaches to Analyzing Weblogs.
Rada Mihalcea and Carlo Strapparava. 2005. Making
computers laugh: Investigations in automatic humor
recognition. In Empirical Methods in Natural Lan-
guage Processing.
Norman A. Milgram and Wolfgang W. Riedel. 1969.
Developmental and experiential factors in making
wishes. Child Development, 40(3):763?771.
Gilad Mishne, Krisztian Balog, Maarten de Rijke, and
Breyten Ernsting. 2007. Moodviews: Tracking and
searching mood-annotated blog posts. In Proceed-
ings International Conf. on Weblogs and Social Media
(ICWSM-2007), pages 323?324.
Tony Mullen and Robert Malouf. 2008. Taking sides:
User classification for informal online political dis-
course. Internet Research, 18:177?190.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1?135.
Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A
maximum entropy approach to identifying sentence
boundaries. In Fifth Conference on Applied Natural
Language Processing.
James Shanahan, Yan Qu, and Janyce Wiebe, editors.
2005. Computing attitude and affect in text. Springer,
Dordrecht, The Netherlands.
George S. Speer. 1939. Oral and written wishes of
rural and city school children. Child Development,
10(3):151?155.
G. K. Zipf. 1932. Selected Studies of the Principle of
Relative Frequency in Language. Harvard University
Press.
271
Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 43?48,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Latent Dirichlet Allocation with Topic-in-Set Knowledge?
David Andrzejewski
Computer Sciences Department
University of Wisconsin-Madison
Madison, WI 53706, USA
andrzeje@cs.wisc.edu
Xiaojin Zhu
Computer Sciences Department
University of Wisconsin-Madison
Madison, WI 53706, USA
jerryzhu@cs.wisc.edu
Abstract
Latent Dirichlet Allocation is an unsupervised
graphical model which can discover latent top-
ics in unlabeled data. We propose a mech-
anism for adding partial supervision, called
topic-in-set knowledge, to latent topic mod-
eling. This type of supervision can be used
to encourage the recovery of topics which are
more relevant to user modeling goals than the
topics which would be recovered otherwise.
Preliminary experiments on text datasets are
presented to demonstrate the potential effec-
tiveness of this method.
1 Introduction
Latent topic models such as Latent Dirichlet Alloca-
tion (LDA) (Blei et al, 2003) have emerged as a use-
ful family of graphical models with many interesting
applications in natural language processing. One of
the key virtues of LDA is its status as a fully genera-
tive probabilistic model, allowing principled exten-
sions and variations capable of expressing rich prob-
lem domain structure (Newman et al, 2007; Rosen-
Zvi et al, 2004; Boyd-Graber et al, 2007; Griffiths
et al, 2005).
LDA is an unsupervised learning model. This
work aims to add supervised information in the form
of latent topic assignments to LDA. Traditionally,
topic assignments have been denoted by the variable
z in LDA, and we will call such supervised informa-
tion ?z-labels.? In particular, a z-label is the knowl-
? We would like to acknowledge the assistance of Brandi
Gancarz with the biological annotations. This work is supported
in part by the Wisconsin Alumni Research Foundation.
edge that the topic assignment for a given word po-
sition is within a subset of topics. As such, this work
is a combination of unsupervised model and super-
vised knowledge, and falls into the category simi-
lar to constrained clustering (Basu et al, 2008) and
semi-supervised dimensionality reduction (Yang et
al., 2006).
1.1 Related Work
A similar but simpler type of topic labeling infor-
mation has been applied to computer vision tasks.
Topic modeling approaches have been applied to
scene modeling (Sudderth et al, 2005), segmen-
tation, and classification or detection (Wang and
Grimson, 2008). In some of these vision applica-
tions, the latent topics themselves are assumed to
correspond to object labels. If labeled data is avail-
able, either all (Wang and Mori, 2009) or some (Cao
and Fei-Fei, 2007) of the z values can be treated as
observed, rather than latent, variables. Our model
extends z-labels from single values to subsets, thus
offer additional model expressiveness.
If the topic-based representations of documents
are to be used for document clustering or classi-
fication, providing z-labels for words can be seen
as similar to semi-supervised learning with labeled
features (Druck et al, 2008). Here the words are
features, and z-label guidance acts as a feature la-
bel. This differs from other supervised LDA vari-
ants (Blei and McAuliffe, 2008; Lacoste-Julien et
al., 2008) which use document label information.
The ?LDA model for statistical software debug-
ging (Andrzejewski et al, 2007) partitions the topics
into 2 sets: ?usage? topics which can appear in all
43
documents, and ?bug? topics which can only appear
in a special subset of documents. This effect was
achieved by using different ? hyperparameters for
the 2 subsets of documents. z-labels can achieve the
same effect by restricting the z?s in documents out-
side the special subset, so that the z?s cannot assume
the ?bug? topic values. Therefore, the present ap-
proach can be viewed as a generalization of ?LDA.
Another perspective is that our z-labels may
guide the topic model towards the discovery of sec-
ondary or non-dominant statistical patterns in the
data (Chechik and Tishby, 2002). These topics may
be more interesting or relevant to the goals of the
user, but standard LDA would ignore them in favor
of more prominent (and perhaps orthogonal) struc-
ture.
2 Our Model
2.1 Review of Latent Dirichlet Allocation
We briefly review LDA, following the notation
of (Griffiths and Steyvers, 2004) 1. Let there be
T topics. Let w = w1 . . . wn represent a cor-
pus of D documents, with a total of n words. We
use di to denote the document of word wi, and zi
the hidden topic from which wi is generated. Let
?(w)j = p(w|z = j), and ?(d)j = p(z = j) for
document d. LDA involves the following generative
model:
? ? Dirichlet(?) (1)
zi|?(di) ? Multinomial(?(di)) (2)
? ? Dirichlet(?) (3)
wi|zi, ? ? Multinomial(?zi), (4)
where ? and ? are hyperparameters for the
document-topic and topic-word Dirichlet distribu-
tions, respectively. Even though they can be vector
valued, for simplicity we assume ? and ? are scalars,
resulting in symmetric Dirichlet priors.
Given our observed words w, the key task is in-
ference of the hidden topics z. Unfortunately, this
posterior is intractable and we resort to a Markov
Chain Monte Carlo (MCMC) sampling scheme,
specifically Collapsed Gibbs Sampling (Griffiths
and Steyvers, 2004). The full conditional equation
1We enclose superscripts in parentheses in this paper.
used for sampling individual zi values from the pos-
terior is given by
P (zi = v|z?i,w, ?, ?) ?(
n(d)?i,v + ??T
u (n(d)?i,u + ?)
)(
n(wi)?i,v + ??W
w?(? + n(w
?)
?i,v)
)
(5)
where n(d)?i,v is the number of times topic v is used in
document d, and n(wi)?i,v is the number of times word
wi is generated by topic v. The ?i notation signifies
that the counts are taken omitting the value of zi.
2.2 Topic-in-Set Knowledge: z-labels
Let
qiv =
(
n(d)?i,v + ??T
u (n(d)?i,u + ?)
)(
n(wi)?i,v + ??W
w?(? + n(w
?)
?i,v)
)
.
We now define our z-labels. Let C(i) be the set of
possible z-labels for latent topic zi. We set a hard
constraint by modifying the Gibbs sampling equa-
tion with an indicator function ?(v ? C(i)), which
takes on value 1 if v ? C(i) and is 0 otherwise:
P (zi = v|z?i,w, ?, ?) ? qiv?(v ? C(i)) (6)
If we wish to restrict zi to a single value (e.g., zi =
5), this can now be accomplished by setting C(i) =
{5}. Likewise, we can restrict zi to a subset of val-
ues {1, 2, 3} by setting C(i) = {1, 2, 3}. Finally, for
unconstrained zi we simply set C(i) = {1, 2, ..., T},
in which case our modified sampling (6) reduces to
the standard Gibbs sampling (5).
This formulation gives us a flexible method for in-
serting prior domain knowledge into the inference of
latent topics. We can set C(i) independently for ev-
ery single word wi in the corpus. This allows us, for
example, to force two occurrences of the same word
(e.g., ?Apple pie? and ?Apple iPod?) to be explained
by different topics. This effect would be impossible
to achieve by using topic-specific asymmetric ? vec-
tors and setting some entries to zero.
This hard constraint model can be relaxed. Let
0 ? ? ? 1 be the strength of our constraint, where
? = 1 recovers the hard constraint (6) and ? = 0
recovers unconstrained sampling (5):
P (zi = v|z?i,w, ?, ?) ? qiv
(
??(v ? C(i)) + 1? ?
)
.
44
While we present the z-label constraints as a me-
chanical modification to the Gibbs sampling equa-
tions, it can be derived from an undirected extension
of LDA (omitted here) which encodes z-labels. The
soft constraint Gibbs sampling equation arises nat-
urally from this formulation, which is the basis for
the First-Order Logic constraints described later in
the future work section.
3 Experiments
We now present preliminary experimental results to
demonstrate some interesting applications for topic-
in-set knowledge. Unless otherwise specified, sym-
metric hyperparameters ? = .5 and ? = .1 were
used and all MCMC chains were run for 2000 sam-
ples before estimating ? and ? from the final sample,
as in (Griffiths and Steyvers, 2004).
3.1 Concept Expansion
We explore the use of topic-in-set for identifying
words related to a target concept, given a set of
seed words associated with that concept. For ex-
ample, a biological expert may be interested in the
concept ?translation?. The expert would then pro-
vide a set of seed words which are strongly related
to this concept, here we assume the seed word set
{translation,trna,anticodon,ribosome}. We add the
hard constraint that zi = 0 for all occurrences of
these four words in our corpus of approximately
9,000 yeast-related abstracts.
We ran LDA with the number of topics T = 100,
both with and without the z-label knowledge on the
seed words. Table 1 shows the most probable words
in selected topics from both runs. Table 1a shows
Topic 0 from the constrained run, while Table 1b
shows the topics which contained seed words among
the top 50 most probable words from the uncon-
strained run.
In order to better understand the results, these
top words were annotated for relevance to the tar-
get concept (translation) by an outside biological ex-
pert. The words in Table 1 were then colored blue
if they were one of the original seed words, red if
they were judged as relevant, and left black other-
wise. From a quick glance, we can see that Topic
0 from the constrained run contains more relevant
terms than Topic 43 from the standard LDA run.
Topic 31 has a similar number of relevant terms, but
taken together we can see that the emphasis of Topic
31 is slightly off-target, more focused on ?mRNA
turnover? than ?translation?. Likewise, Topic 73
seems more focused on the ribosome itself than the
process of translation. Overall, these results demon-
strate the potential effectiveness of z-label informa-
tion for guiding topic models towards a user-seeded
concept.
3.2 Concept Exploration
Suppose that a user has chosen a set of terms and
wishes to discover different topics related to these
terms. By constraining these terms to only appear
in a restricted set of topics, these terms will be con-
centrated in the set of topics. The split within those
set of topics may be different from what a standard
LDA will produce, thus revealing new information
within the data.
To make this concrete, say we are interested in
the location ?United Kingdom?. We seed this con-
cept with the following LOCATION-tagged terms
{britain, british, england, uk, u.k., wales, scotland,
london}. These terms are then restricted to ap-
pear only in the first 3 topics. Our corpus is an
entity-tagged Reuters newswire corpus used for the
CoNLL-2003 shared task (Tjong Kim Sang and
De Meulder, 2003). In order to focus on our tar-
get location, we also restrict all other LOCATION-
tagged tokens to not appear in the first 3 topics. For
this experiment we set T = 12, arrived at by trial-
and-error in the baseline (standard LDA) case.
The 50 most probable words for each topic are
shown in Figure 2, and tagged entities are prefixed
with their tags for easy identification. Table 2a
shows the top words for the first 3 topics of our z-
label run. These three topics are all related to the
target LOCATION United Kingdom, but they also
split nicely into business, cricket, and soccer. Words
which are highly relevant to each of these 3 concepts
are colored blue, red, and green, respectively.
In contrast, in Table 2b we show topics from stan-
dard LDA which contain any of the ?United King-
dom? LOCATION terms (which are underlined)
among the 50 most probable words for that topic.
We make several observations about these topics.
First, standard LDA Topic 0 is mostly concerned
with political unrest in Russia, which is not particu-
45
Topic 0
translation, ribosomal, trna, rrna, initiation, ribosome, protein, ribosomes, is, factor, processing, translational
nucleolar, pre-rrna, synthesis, small, 60s, eukaryotic, biogenesis, subunit, trnas, subunits, large, nucleolus
factors, 40, synthetase, free, modification, rna, depletion, eif-2, initiator, 40s, ef-3, anticodon, maturation
18s, eif2, mature, eif4e, associated, synthetases, aminoacylation, snornas, assembly, eif4g, elongation
(a) Topic 0 with z-label
Topic 31
mrna, translation, initiation, mrnas, rna, transcripts, 3, transcript, polya, factor, 5, translational, decay, codon
decapping, factors, degradation, end, termination, eukaryotic, polyadenylation, cap, required, efficiency
synthesis, show, codons, abundance, rnas, aug, nmd, messenger, turnover, rna-binding, processing, eif2, eif4e
eif4g, cf, occurs, pab1p, cleavage, eif5, cerevisiae, major, primary, rapid, tail, efficient, upf1p, eif-2
Topic 43
type, is, wild, yeast, trna, synthetase, both, methionine, synthetases, class, trnas, enzyme, whereas, cytoplasmic
because, direct, efficiency, presence, modification, aminoacylation, anticodon, either, eukaryotic, between
different, specific, discussed, results, similar, some, met, compared, aminoacyl-trna, able, initiator, sam
not, free, however, recognition, several, arc1p, fully, same, forms, leads, identical, responsible, found, only, well
Topic 73
ribosomal, rrna, protein, is, processing, ribosome, ribosomes, rna, nucleolar, pre-rrna, rnase, small, biogenesis
depletion, subunits, 60s, subunit, large, synthesis, maturation, nucleolus, associated, essential, assembly
components, translation, involved, rnas, found, component, mature, rp, 40s, accumulation, 18s, 40, particles
snornas, factors, precursor, during, primary, rrnas, 35s, has, 21s, specifically, results, ribonucleoprotein, early
(b) Standard LDA Topics
Figure 1: Concept seed words are colored blue, other words judged relevant to the target concept are colored
red.
larly related to the target location. Second, Topic 2
is similar to our previous business topic, but with
a more US-oriented slant. Note that ?dollar? ap-
pears with high probability in standard LDA Topic
2, but not in our z-label LDA Topic 0. Standard
LDA Topic 8 appears to be a mix of both soccer and
cricket words. Therefore, it seems that our topic-in-
set knowledge helps in distilling topics related to the
seed words.
Given this promising result, we attempted to
repeat this experiment with some other nations
(United States, Germany, China), but without much
success. When we tried to restrict these LOCATION
words to the first few topics, these topics tended to
be used to explain other concepts unrelated to the
target location (often other sports). We are investi-
gating the possible causes of this problem.
4 Conclusions and Future Work
We have defined Topic-in-Set knowledge and
demonstrated its use within LDA. As shown in the
experiments, the partial supervision provided by z-
labels can encourage LDA to recover topics rele-
vant to user interests. This approach combines the
pattern-discovery power of LDA with user-provided
guidance, which we believe will be very attractive to
practical users of topic modeling.
Future work will deal with at least two impor-
tant issues. First, when will this form of partial
supervision be most effective or appropriate? Our
experimental results suggest that this approach will
struggle if the user?s target concepts are simply not
prevalent in the text. Second, can we modify this
approach to express richer forms of partial super-
vision? More sophisticated forms of knowledge
may allow users to specify their preferences or prior
knowledge more effectively. Towards this end, we
are investigating the use of First-Order Logic in
specifying prior knowledge. Note that the set z-
labels presented here can be expressed as simple log-
ical formulas. Extending our model to general log-
ical formulas would allow the expression of more
powerful relational preferences.
References
David Andrzejewski, Anne Mulhern, Ben Liblit, and Xi-
aojin Zhu. 2007. Statistical debugging using latent
topic models. In Stan Matwin and Dunja Mladenic,
editors, 18th European Conference on Machine Learn-
ing, Warsaw, Poland.
46
Topic 0
million, company, ?s, year, shares, net, profit, half, group, [I-ORG]corp, market, sales, share, percent
expected, business, loss, stock, results, forecast, companies, deal, earnings, statement, price, [I-LOC]london
billion, [I-ORG]newsroom, industry, newsroom, pay, pct, analysts, issue, services, analyst, profits, sale
added, firm, [I-ORG]london, chief, quarter, investors, contract, note, tax, financial, months, costs
Topic 1
[I-LOC]england, [I-LOC]london, [I-LOC]britain, cricket, [I-PER]m., overs, test, wickets, scores, [I-PER]ahmed
[I-PER]paul, [I-PER]wasim, innings, [I-PER]a., [I-PER]akram, [I-PER]mushtaq, day, one-day, [I-PER]mark, final
[I-LOC]scotland, [I-PER]waqar, [I-MISC]series, [I-PER]croft, [I-PER]david, [I-PER]younis, match, [I-PER]ian
total, [I-MISC]english, [I-PER]khan, [I-PER]mullally, bat, declared, fall, [I-PER]d., [I-PER]g., [I-PER]j.
bowling, [I-PER]r., [I-PER]robert, [I-PER]s., [I-PER]steve, [I-PER]c. captain, golf, tour, [I-PER]sohail, extras
[I-ORG]surrey
Topic 2
soccer, division, results, played, standings, league, matches, halftime, goals, attendance, points, won, [I-ORG]st
drawn, saturday, [I-MISC]english, lost, premier, [I-MISC]french, result, scorers, [I-MISC]dutch, [I-ORG]united
[I-MISC]scottish, sunday, match, [I-LOC]london, [I-ORG]psv, tabulate, [I-ORG]hapoel, [I-ORG]sydney, friday
summary, [I-ORG]ajax, [I-ORG]manchester, tabulated, [I-MISC]german, [I-ORG]munich, [I-ORG]city
[I-MISC]european, [I-ORG]rangers, summaries, weekend, [I-ORG]fc, [I-ORG]sheffield, wednesday, [I-ORG]borussia
[I-ORG]fortuna, [I-ORG]paris, tuesday
(a) Topics with set z-labels
Topic 0
police, ?s, people, killed, [I-MISC]russian, friday, spokesman, [I-LOC]moscow, told, rebels, group, officials
[I-PER]yeltsin, arrested, found, miles, km, [I-PER]lebed, capital, thursday, tuesday, [I-LOC]chechnya, news
saturday, town, authorities, airport, man, government, state, agency, plane, reported, security, forces
city, monday, air, quoted, students, region, area, local, [I-LOC]russia, [I-ORG]reuters, military, [I-LOC]london
held, southern, died
Topic 2
percent, ?s, market, thursday, july, tonnes, week, year, lower, [I-LOC]u.s., rate, prices, billion, cents, dollar
friday, trade, bank, closed, trading, higher, close, oil, bond, fell, markets, index, points, rose
demand, june, rates, september, traders, [I-ORG]newsroom, day, bonds, million, price, shares, budget, government
growth, interest, monday, [I-LOC]london, economic, august, expected, rise
Topic 5
?s, match, team, win, play, season, [I-MISC]french, lead, home, year, players, [I-MISC]cup, back, minutes
champion, victory, time, n?t, game, saturday, title, side, set, made, wednesday, [I-LOC]england
league, run, club, top, good, final, scored, coach, shot, world, left, [I-MISC]american, captain
[I-MISC]world, goal, start, won, champions, round, winner, end, years, defeat, lost
Topic 8
division, [I-LOC]england, soccer, results, [I-LOC]london, [I-LOC]pakistan, [I-MISC]english, matches, played
standings, league, points, [I-ORG]st, cricket, saturday, [I-PER]ahmed, won, [I-ORG]united, goals
[I-PER]wasim, [I-PER]akram, [I-PER]m., [I-MISC]scottish, [I-PER]mushtaq, drawn, innings, premier, lost
[I-PER]waqar, test, [I-PER]croft, [I-PER]a., [I-PER]younis, declared, wickets, [I-ORG]hapoel, [I-PER]mullally
[I-ORG]sydney, day, [I-ORG]manchester, [I-PER]khan, final, scores, [I-PER]d., [I-MISC]german, [I-ORG]munich
[I-PER]sohail, friday, total, [I-LOC]oval
Topic 10
[I-LOC]germany, ?s, [I-LOC]italy, [I-LOC]u.s., metres, seconds, [I-LOC]france, [I-LOC]britain, [I-LOC]russia
world, race, leading, [I-LOC]sweden, [I-LOC]australia, [I-LOC]spain, women, [I-MISC]world, [I-LOC]belgium
[I-LOC]netherlands, [I-PER]paul, [I-LOC]japan, [I-MISC]olympic, [I-LOC]austria, [I-LOC]kenya, men, time
results, [I-LOC]brussels, [I-MISC]cup, [I-LOC]canada, final, minutes, record, [I-PER]michael, meeting, round
[I-LOC]norway, friday, scores, [I-PER]mark, [I-PER]van, [I-LOC]ireland, [I-PER]peter, [I-MISC]grand
[I-MISC]prix, points, saturday, [I-LOC]finland, cycling, [I-ORG]honda
(b) Standard LDA Topics
Figure 2: Topics containing ?United Kingdom? location words. Words related to business are colored blue,
cricket red, and soccer green.
Sugato Basu, Ian Davidson, and Kiri Wagstaff, edi-
tors. 2008. Constrained Clustering: Advances in
Algorithms, Theory, and Applications. Chapman &
Hall/CRC Press.
David Blei and Jon McAuliffe. 2008. Supervised topic
models. In J.C. Platt, D. Koller, Y. Singer, and
S. Roweis, editors, Advances in Neural Information
Processing Systems 20, pages 121?128. MIT Press,
47
Cambridge, MA.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Jordan Boyd-Graber, David Blei, and Xiaojin Zhu. 2007.
A topic model for word sense disambiguation. In
Proceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 1024?1033.
Liangliang Cao and Li Fei-Fei. 2007. Spatially coher-
ent latent topic model for concurrent segmentation and
classification of objects and scenes. In ICCV, pages 1?
8.
Gal Chechik and Naftali Tishby. 2002. Extracting rel-
evant structures with side information. In NIPS 15,
pages 857?864. MIT press.
Gregory Druck, Gideon Mann, and Andrew McCallum.
2008. Learning from labeled features using general-
ized expectation criteria. In SIGIR 2008, pages 595?
602, New York, NY, USA. ACM.
Thomas Griffiths and Mark Steyvers. 2004. Finding sci-
entific topics. Proceedings of the National Academy of
Sciences, 101(suppl. 1):5228?5235.
Thomas L. Griffiths, Mark Steyvers, David M. Blei, and
Joshua B. Tenenbaum. 2005. Integrating topics and
syntax. In NIPS 17.
S. Lacoste-Julien, F. Sha, and M. Jordan. 2008. Disclda:
Discriminative learning for dimensionality reduction
and classification. In Advances in Neural Information
Processing Systems 21 (NIPS08).
David Newman, Kat Hagedorn, Chaitanya
Chemudugunta, and Padhraic Smyth. 2007. Subject
metadata enrichment using statistical topic models.
In JCDL ?07: Proceedings of the 7th ACM/IEEE-CS
joint conference on Digital libraries, pages 366?375,
New York, NY, USA. ACM.
Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers, and
Padhraic Smyth. 2004. The author-topic model for au-
thors and documents. In Proceedings of the 20th con-
ference on Uncertainty in artificial intelligence (UAI),
pages 487?494, Arlington, Virginia, United States.
AUAI Press.
Erik B. Sudderth, Antonio B. Torralba, William T. Free-
man, and Alan S. Willsky. 2005. Learning hierar-
chical models of scenes, objects, and parts. In ICCV,
pages 1331?1338.
Erik Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the conll-2003 shared task: Language-
independent named entity recognition. In Proceedings
of CoNLL-2003, pages 142?147, Edmonton, Canada.
Xiaogang Wang and Eric Grimson. 2008. Spatial latent
dirichlet alocation. In J.C. Platt, D. Koller, Y. Singer,
and S. Roweis, editors, NIPS 20, pages 1577?1584.
MIT Press, Cambridge, MA.
Yang Wang and Greg Mori. 2009. Human action recog-
nition by semi-latent topic models. In IEEE Transac-
tions on Pattern Analysis and Machine Intelligence.
Xin Yang, Haoying Fu, Hongyuan Zha, and Jesse Barlow.
2006. Semi-supervised nonlinear dimensionality re-
duction. In ICML-06, 23nd International Conference
on Machine Learning.
48
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 952?961, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Exploring Topic Coherence over many models and many topics
Keith Stevens1,2 Philip Kegelmeyer3 David Andrzejewski2 David Buttler2
1University of California Los Angeles; Los Angeles , California, USA
2Lawrence Livermore National Lab; Livermore, California, USA
3Sandia National Lab; Livermore, California, USA
{stevens35,andrzejewski1,buttler1}@llnl.gov
wpk@sandia.gov
Abstract
We apply two new automated semantic eval-
uations to three distinct latent topic models.
Both metrics have been shown to align with
human evaluations and provide a balance be-
tween internal measures of information gain
and comparisons to human ratings of coher-
ent topics. We improve upon the measures
by introducing new aggregate measures that
allows for comparing complete topic models.
We further compare the automated measures
to other metrics for topic models, compar-
ison to manually crafted semantic tests and
document classification. Our experiments re-
veal that LDA and LSA each have different
strengths; LDA best learns descriptive topics
while LSA is best at creating a compact se-
mantic representation of documents and words
in a corpus.
1 Introduction
Topic models learn bags of related words from large
corpora without any supervision. Based on the
words used within a document, they mine topic level
relations by assuming that a single document cov-
ers a small set of concise topics. Once learned,
these topics often correlate well with human con-
cepts. For example, one model might produce topics
that cover ideas such as government affairs, sports,
and movies. With these unsupervised methods, we
can extract useful semantic information in a variety
of tasks that depend on identifying unique topics or
concepts, such as distributional semantics (Jurgens
and Stevens, 2010), word sense induction (Van de
Cruys and Apidianaki, 2011; Brody and Lapata,
2009), and information retrieval (Andrzejewski and
Buttler, 2011).
When using a topic model, we are primarily con-
cerned with the degree to which the learned top-
ics match human judgments and help us differen-
tiate between ideas. But until recently, the evalua-
tion of these models has been ad hoc and applica-
tion specific. Evaluations have ranged from fully
automated intrinsic evaluations to manually crafted
extrinsic evaluations. Previous extrinsic evaluations
have used the learned topics to compactly represent
a small fixed vocabulary and compared this distribu-
tional space to human judgments of similarity (Jur-
gens and Stevens, 2010). But these evaluations are
hand constructed and often costly to perform for
domain-specific topics. Conversely, intrinsic mea-
sures have evaluated the amount of information en-
coded by the topics, where perplexity is one com-
mon example(Wallach et al 2009), however, Chang
et al(2009) found that these intrinsic measures do
not always correlate with semantically interpretable
topics. Furthermore, few evaluations have used the
same metrics to compare distinct approaches such
as Latent Dirichlet Allocation (LDA) (Blei et al
2003), Latent Semantic Analysis (LSA) (Landauer
and Dutnais, 1997), and Non-negative Matrix Fac-
torization (NMF) (Lee and Seung, 2000). This has
made it difficult to know which method is most use-
ful for a given application, or in terms of extracting
useful topics.
We now provide a comprehensive and automated
evaluation of these three distinct models (LDA,
LSA, NMF), for automatically learning semantic
topics. While these models have seen significant im-
provements, they still represent the core differences
between each approach to modeling topics. For our
evaluation, we use two recent automated coherence
measures (Mimno et al 2011; Newman et al 2010)
952
originally designed for LDA that bridge the gap be-
tween comparisons to human judgments and intrin-
sic measures such as perplexity. We consider several
key questions:
1. How many topics should be learned?
2. How many learned topics are useful?
3. How do these topics relate to often used semantic tests?
4. How well do these topics identify similar documents?
We begin by summarizing the three topic mod-
els and highlighting their key differences. We then
describe the two metrics. Afterwards, we focus on
a series of experiments that address our four key
questions and finally conclude with some overall re-
marks.
2 Topic Models
We evaluate three latent factor models that have seen
widespread usage:
1. Latent Dirichlet Allocation
2. Latent Semantic Analysis with Singular Value De-
composition
3. Latent Semantic Analysis with Non-negative Ma-
trix Factorization
Each of these models were designed with differ-
ent goals and are supported by different statistical
theories. We consider both LSA models as topic
models as they have been used in a variety of sim-
ilar contexts such as distributional similarity (Jur-
gens and Stevens, 2010) and word sense induction
(Van de Cruys and Apidianaki, 2011; Brody and
Lapata, 2009). We evaluate these distinct models
on two shared tasks (1) grouping together similar
words while separating unrelated words and (2) dis-
tinguishing between documents focusing on differ-
ent concepts.
We distill the different models into a shared repre-
sentation consisting of two sets of learned relations:
how words interact with topics and how topics inter-
act with documents. For a corpus withD documents
and V words, we denote these relations in terms of
T topics as
(1) a V ? T matrix, W , that indicates the strength
each word has in each topic, and
(2) a T ? D matrix, H , that indicates the strength
each topic has in each document.
T serves as a common parameter to each model.
2.1 Latent Dirichlet Allocation
Latent Dirichlet Allocation (Blei et al 2003) learns
the relationships between words, topics, and docu-
ments by assuming documents are generated by a
particular probabilistic model. It first assumes that
there are a fixed set of topics, T used throughout the
corpus, and each topic z is associated with a multi-
nomial distribution over the vocabulary?z , which is
drawn from a Dirichlet prior Dir(?). A given docu-
ment Di is then generated by the following process
1. Choose ?i ? Dir(?), a topic distribution for Di
2. For each word wj ? Di:
(a) Select a topic zj ? ?i
(b) Select the word wj ? ?zj
In this model, the ? distributions represent the
probability of each topic appearing in each docu-
ment and the ? distributions represent the proba-
bility of words being used for each topic. These
two sets of distributions correspond to our H and W
matrices, respectively. The process above defines a
generative model; given the observed corpus, we use
collapsed Gibbs sampling implementation found in
Mallet1 to infer the values of the latent variables ?
and ? (Griffiths and Steyvers, 2004). The model re-
lies only on two additional hyper parameters, ? and
?, that guide the distributions.
2.2 Latent Semantic Analysis
Latent Semantic Analysis (Landauer and Dutnais,
1997; Landauer et al 1998) learns topics by first
forming a traditional term by document matrix used
in information retrieval and then smoothing the
counts to enhance the weight of informative words.
Based on the original LSA model, we use the Log-
Entropy transform. LSA then decomposes this
smoothed, term by document matrix in order to gen-
eralize observed relations between words and docu-
ments. For both LSA models, we used implementa-
tions found in the S-Space package.2
Traditionally, LSA has used the Singular Value
Decomposition, but we also consider Non-negative
Matrix Factorization as we?ve seen NMF applied
in similar situations (Pauca et al 2004) and others
1http://mallet.cs.umass.edu/
2https://github.com/fozziethebeat/S-Space
953
Model Label Top Words UMass UCI
High Quality Topics
LDA interview told asked wanted interview people made thought time called knew -2.52 1.29wine wine wines bottle grapes made winery cabernet grape pinot red -1.97 1.30
NMF grilling grilled sweet spicy fried pork dish shrimp menu dishes sauce -1.01 1.98cloning embryonic cloned embryo human research stem embryos cell cloning cells -1.84 1.46
SVD cooking sauce food restaurant water oil salt chicken pepper wine cup -1.87 -1.21stocks fund funds investors weapons stocks mutual stock movie film show -2.30 -1.88
Low Quality Topics
LDA rates 10-yr rate 3-month percent 6-month bds bd 30-yr funds robot -1.94 -12.32charity fund contributions .com family apartment charities rent 22d children assistance -2.43 -8.88
NMF plants stem fruitful stems trunk fruiting currants branches fence currant espalier -3.12 -12.59farming buzzards groundhog prune hoof pruned pruning vines wheelbarrow tree clematis -1.90 -12.56
SVD city building city area buildings p.m. floors house listed eat-in a.m. -2.70 -8.03time p.m. system study a.m. office political found school night yesterday -1.67 -7.02
Table 1: Top 10 words from several high and low quality topics when ordered by the UCI Coherence
Measure. Topic labels were chosen in an ad hoc manner only to briefly summarize the topic?s focus.
have found a connection between NMF and Proba-
bilistic Latent Semantic Analysis (Ding et al 2008),
an extension to LSA.We later refer to these two LSA
models simply as SVD and NMF to emphasize the
difference in factorization method.
Singular Value Decomposition decomposes M
into three smaller matrices
M = U?V T
and minimizes Frobenius norm of M ?s reconstruc-
tion error with the constraint that the rows of U and
V are orthonormal eigenvectors. Interestingly, the
decomposition is agnostic to the number of desired
dimensions. Instead, the rows and columns in U and
V T are ordered based on their descriptive power, i.e.
how well they remove noise, which is encoded by
the diagonal singular value matrix ?. As such, re-
duction is done by retaining the first T rows and
columns from U and V T . For our generalization,
we use W = U? and H = ?V T . We note that
values in U and V T can be both negative and pos-
itive, preventing a straightforward interpretation as
unnormalized probabilities
Non-negative Matrix Factorization also factor-
izes M by minimizing the reconstruction error, but
with only one constraint: the decomposed matrices
consist of only non-negative values. In this respect,
we can consider it to be learning an unnormalized
probability distributions over topics. We use the
original Euclidean least squares definition of NMF3.
Formally, NMF is defined as
M = WH
where H and W map directly onto our generaliza-
tion. As in the original NMF work, we learn these
unnormalized probabilities by initializing each set of
probabilities at random and update them according
to the following iterative update rules
W = W MHTWHHT H = H
WTM
WTWH
3 Coherence Measures
Topic Coherence measures score a single topic by
measuring the degree of semantic similarity between
high scoring words in the topic. These measure-
ments help distinguish between topics that are se-
mantically interpretable topics and topics that are ar-
tifacts of statistical inference, see Table 1 for exam-
ples ordered by the UCI measure. For our evalua-
tions, we consider two new coherence measures de-
signed for LDA, both of which have been shown to
match well with human judgements of topic quality:
(1) The UCI measure (Newman et al 2010) and (2)
The UMass measure (Mimno et al 2011).
Both measures compute the coherence of a topic
as the sum of pairwise distributional similarity
3We note that the alternative KL-Divergence form of NMF
has been directly linked to PLSA (Ding et al 2008)
954
scores over the set of topic words, V . We generalize
this as
coherence(V ) =
?
(vi,vj)?V
score(vi, vj , )
where V is a set of word describing the topic and 
indicates a smoothing factor which guarantees that
score returns real numbers. (We will be exploring
the effect of the choice of ; the original authors used
 = 1.)
The UCI metric defines a word pair?s score to
be the pointwise mutual information (PMI) between
two words, i.e.
score(vi, vj , ) = log
p(vi, vj) + 
p(vi)p(vj)
The word probabilities are computed by counting
word co-occurrence frequencies in a sliding window
over an external corpus, such as Wikipedia. To some
degree, this metric can be thought of as an external
comparison to known semantic evaluations.
The UMass metric defines the score to be based
on document co-occurrence:
score(vi, vj , ) = log
D(vi, vj) + 
D(vj)
whereD(x, y) counts the number of documents con-
taining words x and y and D(x) counts the num-
ber of documents containing x. Significantly, the
UMass metric computes these counts over the orig-
inal corpus used to train the topic models, rather
than an external corpus. This metric is more intrin-
sic in nature. It attempts to confirm that the models
learned data known to be in the corpus.
4 Evaluation
We evaluate the quality of our three topic models
(LDA, SVD, and NMF) with three experiments. We
focus first on evaluating aggregate coherence meth-
ods for a complete topic model and consider the
differences between each model as we learn an in-
creasing number of topics. Secondly, we compare
coherence scores to previous semantic evaluations.
Lastly, we use the learned topics in a classifica-
tion task and evaluate whether or not coherent top-
ics are equally informative when discriminating be-
tween documents.
For all our experiments, we trained our models on
92,600 New York Times articles from 2003 (Sand-
haus, 2008). For all articles, we removed stop words
and any words occurring less than 200 times in the
corpus, which left 35,836 unique tokens. All doc-
uments were tokenized with OpenNLP?s MaxEnt4
tokenizer. For the UCI measure, we compute the
PMI between words using a 20 word sliding win-
dow passed over the WaCkypedia corpus (Baroni et
al., 2009). In all experiments, we compute the co-
herence with the top 10 words from each topic that
had the highest weight, in terms of LDA and NMF
this corresponds with a high probability of the term
describing the topic but for SVD there is no clear
semantic interpretation.
4.1 Aggregate methods for topic coherence
Before we can compare topic models, we require an
aggregate measure that represents the quality of a
complete model, rather than individual topics. We
consider two aggregates methods: (1) the average
coherence of all topics and (2) the entropy of the co-
herence for all topics. The average coherence pro-
vides a quick summarization of a model?s quality
whereas the entropy provides an alternate summa-
rization that differentiates between two interesting
situations. Since entropy measures the complexity
of a probability distribution, it can easily differenti-
ate between uniform distributions and multimodal,
distributions. This distinction is relevant when users
prefer to have roughly uniform topic quality instead
of a wide gap between high- and low-quality topics,
or vice versa. We compute the entropy by dropping
the log and  factor from each scoring function.
Figure 1 shows the average coherence scores for
each model as we vary the number of topics. These
average scores indicate some simple relationships
between the models: LDA and NMF have approx-
imately the same performance and both models are
consistently better than SVD. All of the models
quickly reach a stable average score at around 100
topics. This initially suggests that learning more
4http://incubator.apache.org/opennlp/
955
Number of topics
Avera
ge To
pic Co
heren
ce
?5
?4
?3
?2
?1
0
100 200 300 400 500
(a) UMass
Number of topics
Avera
ge To
pic Co
heren
ce
?10
?8
?6
?4
?2
0
2
100 200 300 400 500
MethodLDANMFSVD
(b) UCI
Figure 1: Average Topic Coherence for each model
Number of topics
Cohe
rence
 Entro
py
0
1
2
3
4
5
6
7
100 200 300 400 500
(a) UMass
Number of topics
Cohe
rence
 Entro
py
0
1
2
3
4
5
6
7
100 200 300 400 500
MethodLDANMFSVD
(b) UCI
Figure 2: Entropy of the Topic Coherence for each model
topics neither increases or decreases the quality of
the model, but Figure 2 indicates otherwise. While
the entropy for the UMass score stays stable for all
models, NMF produces erratic entropy results under
the UCI score as we learn more topics. As entropy is
higher for even distributions and lower for all other
distributions, these results suggest that the NMF is
learning topics with drastically different levels of
quality, i.e. some with high quality and some with
very low quality, but the average coherence over all
topics do not account for this.
Low quality topics may be composed of highly
unrelated words that can?t be fit into another topic,
and in this case, our smoothing factor, , may be ar-
tificially increasing the score for unrelated words.
Following the practice of the original use of these
metrics, in Figures 1 and 2 we set  = 1. In Fig-
ure 3, we consider  = 10?12, which should sig-
nificantly reduce the score for completely unrelated
words. Here, we see a significant change in the per-
formance of NMF, the average coherence decreases
dramatically as we learn more topics. Similarly, per-
formance of SVD drops dramatically and well below
the other models. In figure 4 we lastly compute the
average coherence using only the top 10% most co-
herence topics with  = 10?12. Here, NMF again
performs on par with LDA. With the top 10% topics
still having a high average coherence but the full set
956
Number of topics
Avera
ge To
pic Co
heren
ce
?5
?4
?3
?2
?1
0
100 200 300 400 500
(a) UMass
Number of topics
Avera
ge To
pic Co
heren
ce
?10
?8
?6
?4
?2
0
2
100 200 300 400 500
MethodLDANMFSVD
(b) UCI
Figure 3: Average Topic Coherence with  = 10?12
Number of topics
Avera
ge Co
heren
ce of 
top 10
%
?5
?4
?3
?2
?1
0
100 200 300 400 500
(a) UMass
Number of topics
Avera
ge Co
heren
ce of 
top 10
%
?10
?8
?6
?4
?2
0
2
100 200 300 400 500
MethodLDANMFSVD
(b) UCI
Figure 4: Average Topic Coherence of the top 10% topics with  = 10?12
of topics having a low coherence, NMF appears to
be learning more low quality topics once it?s learned
the first 100 topics, whereas LDA learns fewer low
quality topics in general.
4.2 Word Similarity Tasks
The initial evaluations for each coherence mea-
sure asked human judges to directly evaluate top-
ics (Newman et al 2010; Mimno et al 2011). We
expand upon this comparison to human judgments
by considering word similarity tasks that have of-
ten been used to evaluate distributional semantic
spaces (Jurgens and Stevens, 2010). Here, we use
the learned topics as generalized semantics describ-
ing our knowledge about words. If a model?s topics
generalize the knowledge accurately, we would ex-
pect similar words, such as ?cat? and ?dog?, to be
represented with a similar set of topics. Rather than
evaluating individual topics, this similarity task con-
siders the knowledge within the entire set of topics,
the topics act as more compact representation for the
known words in a corpus.
We use the Rubenstein and Goodenough (1965)
and Finkelstein et al(2002) word similarity tasks.
In each task, human judges were asked to evaluate
the similarity or relatedness between different sets of
word pairs. Fifty-One Evaluators for the Rubenstein
and Goodenough (1965) dataset were given 65 pairs
957
Tscor
e
0.0
0.1
0.2
0.3
0.4
0.5
0.6
100 200 300 400 500
modelLDANMFSVD
(a) Rubenstein & Goodenough
T
scor
e
0.0
0.1
0.2
0.3
0.4
0.5
100 200 300 400 500
modelLDANMFSVD
(b) Wordsim 353/Finklestein et. al.
Figure 5: Word Similarity Evaluations for each model
Topics
Corre
lation
0.0
0.2
0.4
0.6
100 200 300 400 500
(a) UMass
Topics
Corre
lation
0.0
0.2
0.4
0.6
100 200 300 400 500
modelLDANMFSVD
(b) UCI
Figure 7: Correlation between topic coherence and topic ranking in classification
of words and asked to rate their similarity on a scale
from 0 to 4, where a higher score indicates a more
similar word pair. Finkelstein et al(2002) broadens
the word similarity evaluation and asked 13 to 16
different subjects to rate 353 word pairs on a scale
from 0 to 10 based on their relatedness, where relat-
edness includes similarity and other semantic rela-
tions. We can evaluate each topic model by comput-
ing the cosine similarity between each pair of words
in the evaluate set and then compare the model?s
ratings to the human ratings by ranked correlation.
A high correlation signifies that the topics closely
model human judgments.
Figure 5 displays the results. SVD and LDA
both surpass NMF on the Rubenstein & Goode-
nough test while SVD is clearly the best model on
the Finklestein et. al test. While our first experi-
ment showed that SVDwas the worst model in terms
of topic coherence scores, this experiment indicates
that SVD provides an accurate, stable, and reliable
approximation to human judgements of similarity
and relatedness between word pairs in comparison
to other topic models.
4.3 Coherence versus Classification
For our final experiment, we examine the relation-
ship between topic coherence and classification ac-
curacy for each topic model. We suspect that highly
958
score
Corre
lation
0.01
0.02
0.03
0.04
0.05
0.06
0.07
?25 ?20 ?15 ?10 ?5
(a) UMass
score
Corre
lation
0.01
0.02
0.03
0.04
0.05
0.06
0.07
?30 ?20 ?10 0
modelLDANMFSVD
(b) UCI
Figure 8: Comparison between topic coherence and topic rank with 500 topics
Topics
Accu
racy
20
30
40
50
60
70
80
100 200 300 400 500
ModelLDANMFSVD
Figure 6: Classification accuracy for each model
coherent topics, and coherent topic models, will per-
form better for classification. We address this ques-
tion by performing a document classification task
using the topic representations of documents as in-
put features and examine the relationship between
topic coherence and the usefulness of the corre-
sponding feature for classification.
We trained each topic model with all 92,600 New
York Times articles as before. We use the sec-
tion labels provided for each article as class labels,
where each label indicates the on-line section(s) un-
der which the article was published and should thus
be related to the topics contained in each article. To
reduce the noise in our data set we narrow down the
articles to those that have only one label and whose
label is applied to at least 2000 documents. This re-
sults in 57,696 articles with label distributions listed
in Table 2. We then represent each document using
columns in the topic by document matrix H learned
for each topic model.
Label Count Label Count
New York and Region 11219 U.S. 3675
Paid Death Notices 11152 Arts 3437
Opinion 8038 World 3330
Business 7494 Style 2137
Sports 7214
Table 2: Section label counts for New York Times
articles used for classification
For each topic model trained on N topics, we
performed stratified 10-fold cross-validation on the
57,696 labeled articles. In each fold, we build an
automatically-sized bagged ensemble of unpruned
CART-style decision trees(Banfield et al 2007) on
90% of the dataset5, use that ensemble to assign la-
bels to the other 10%, and measure the accuracy of
that assignment. Figure 6 shows the average classifi-
cation accuracy over all ten folds for each model. In-
terestingly, SVD has slightly, but statistically signif-
icantly, higher accuracy results than both NMF and
LDA. Furthermore, performance quickly increases
5The precise choice of the classifier scheme matters little, as
long as it is accurate, speedy, and robust to label noise; all of
which is true of the choice here.
959
and plateaus with well under 50 topics.
Our bagged decision trees can also determine the
importance of each feature during classification. We
evaluate the strength of each topic during classifi-
cation by tracking the number of times each node
in our decision trees observe each topic, please see
(Caruana et al 2006) for more details. Figure 8 plot
the relationship between this feature ranking and the
topic coherence for each topic when training LDA,
SVD, and NMF on 500 topics. Most topics for each
model provide little classification information, but
SVD shows a much higher rank for several topics
with a relatively higher coherence score. Interest-
ingly, for all models, the most coherent topics are not
the most informative. Figure 7 plots a more compact
view of this same relationship: the Spearman rank
correlation between classification feature rank and
topic coherence. NMF shows the highest correlation
between rank and coherence, but none of the mod-
els show a high correlation when using more than
100 topics. SVD has the lowest correlation, which
is probably due to the model?s overall low coherence
yet high classification accuracy.
5 Discussion and Conclusion
Through our experiments, we made several excit-
ing and interesting discoveries. First, we discov-
ered that the coherence metrics depend heavily on
the smoothing factor . The original value, 1.0 cre-
ated a positive bias towards NMF from both met-
rics even when NMF generated incoherent topics.
The high smoothing factor also gave a significant in-
crease to SVD scores. We suspect that this was not
an issue in previous studies with the coherence mea-
sures as LDA prefers to form topics from words that
co-occur frequently, whereas NMF and SVD have
no such preferences and often create low quality top-
ics from completely unrelated words. Therefore, we
suggest a smaller  value in general.
We also found that the UCI measure often agreed
with the UMass measure, but the UCI-entropy ag-
gregate method induced more separation between
LSA, SVD, and NMF in terms of topic coherence.
This measure also revealed the importance of the
smoothing factor for topic coherence measures.
With respects to human judgements, we found
that coherence scores do not always indicate a bet-
ter representation of distributional information. The
SVD model consistently out performed both LDA
and NMF models, which each had higher coherence
scores, when attempting to predict human judge-
ments of similarity.
Lastly, we found all models capable of producing
topics that improved document classification. At the
same time, SVD provided the most information dur-
ing classification and outperformed the other mod-
els, which again had more coherent topics. Our com-
parison between topic coherence scores and feature
importance in classification revealed that relatively
high quality topics, but not the most coherent topics,
drive most of the classification decisions, and most
topics do not affect the accuracy.
Overall, we see that each topic model paradigm
has it?s own strengths and weaknesses. Latent Se-
mantic Analysis with Singular Value Decomposition
fails to form individual topics that aggregate similar
words, but it does remarkably well when consider-
ing all the learned topics as similar words develop
a similar topic representation. These topics simi-
larly perform well during classification. Conversely,
both Non Negative Matrix factorization and Latent
Dirichlet Allocation learn concise and coherent top-
ics and achieved similar performance on our evalua-
tions. However, NMF learns more incoherent topics
than LDA and SVD. For applications in which a hu-
man end-user will interact with learned topics, the
flexibility of LDA and the coherence advantages of
LDA warrant strong consideration. All of code for
this work will be made available through an open
source project.6
6 Acknowledgments
This work was performed under the auspices of
the U.S. Department of Energy by Lawrence Liv-
ermore National Laboratory under Contract DE-
AC52-07NA27344 (LLNL-CONF-522871) and by
Sandia National Laboratory under Contract DE-
AC04-94AL85000.
References
David Andrzejewski and David Buttler. 2011. Latent
topic feedback for information retrieval. In Proceed-
6https://github.com/fozziethebeat/TopicModelComparison
960
ings of the 17th ACM SIGKDD international confer-
ence on Knowledge discovery and data mining, KDD
?11, pages 600?608, New York, NY, USA. ACM.
Robert E. Banfield, Lawrence O. Hall, Kevin W. Bowyer,
andW. Philip Kegelmeyer. 2007. A comparison of de-
cision tree ensemble creation techniques. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence,
29(1):173?180, January.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The WaCky wide web: A
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209?226.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alcation. J. Mach. Learn.
Res., 3:993?1022, March.
Samuel Brody and Mirella Lapata. 2009. Bayesian word
sense induction. In Proceedings of the 12th Con-
ference of the European Chapter of the Association
for Computational Linguistics, EACL ?09, pages 103?
111, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Rich Caruana, Mohamed Elhawary, Art Munson, Mirek
Riedewald, Daria Sorokina, Daniel Fink, Wesley M.
Hochachka, and Steve Kelling. 2006. Mining cit-
izen science data to predict orevalence of wild bird
species. In Proceedings of the 12th ACM SIGKDD
international conference on Knowledge discovery and
data mining, KDD ?06, pages 909?915, New York,
NY, USA. ACM.
Jonathan Chang, Sean Gerrish, Chong Wang, and
David M Blei. 2009. Reading tea leaves : How hu-
mans interpret topic models. New York, 31:1?9.
Chris Ding, Tao Li, and Wei Peng. 2008. On the equiv-
alence between non-negative matrix factorization and
probabilistic latent semantic indexing. Comput. Stat.
Data Anal., 52:3913?3927, April.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2002. Placing search in context: the concept
revisited. ACM Trans. Inf. Syst., 20:116?131, January.
T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the National Academy of
Sciences, 101(Suppl. 1):5228?5235, April.
David Jurgens and Keith Stevens. 2010. The s-space
package: an open source package for word space mod-
els. In Proceedings of the ACL 2010 System Demon-
strations, ACLDemos ?10, pages 30?35, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Thomas K Landauer and Susan T. Dutnais. 1997. A so-
lution to platos problem: The latent semantic analysis
theory of acquisition, induction, and representation of
knowledge. Psychological review, pages 211?240.
Thomas K. Landauer, Peter W. Foltz, and Darrell Laham.
1998. An Introduction to Latent Semantic Analysis.
Discourse Processes, (25):259?284.
Daniel D. Lee and H. Sebastian Seung. 2000. Algo-
rithms for non-negative matrix factorization. In In
NIPS, pages 556?562. MIT Press.
David Mimno, Hanna Wallach, Edmund Talley, Miriam
Leenders, and Andrew McCallum. 2011. Optimizing
semantic coherence in topic models. In Proceedings of
the 2011 Conference on Emperical Methods in Natu-
ral Language Processing, pages 262?272, Edinburgh,
Scotland, UK. Association of Computational Linguis-
tics.
David Newman, Youn Noh, Edmund Talley, Sarvnaz
Karimi, and Timothy Baldwin. 2010. Evaluating topic
models for digital libraries. In Proceedings of the 10th
annual joint conference on Digital libraries, JCDL
?10, pages 215?224, New York, NY, USA. ACM.
V Paul Pauca, Farial Shahnaz, Michael W Berry, and
Robert J Plemmons, 2004. Text mining using nonnega-
tive matrix factorizations, volume 54, pages 452?456.
SIAM.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Commun. ACM,
8:627?633, October.
Evan Sandhaus. 2008. The New York Times Annotated
Corpus.
Tim Van de Cruys and Marianna Apidianaki. 2011. La-
tent semantic word sense induction and disambigua-
tion. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies - Volume 1, HLT ?11,
pages 1476?1485, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Hanna Wallach, Iain Murray, Ruslan Salakhutdinov, and
David Mimno. 2009. Evaluation methods for topic
models. In Proceedings of the 26th International Con-
ference on Machine Learning (ICML). Omnipress.
961
