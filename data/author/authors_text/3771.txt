Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 393?400
Manchester, August 2008
The Effect of Syntactic Representation on Semantic Role Labeling
Richard Johansson and Pierre Nugues
Lund University, Sweden
{richard, pierre}@cs.lth.se
Abstract
Almost all automatic semantic role label-
ing (SRL) systems rely on a preliminary
parsing step that derives a syntactic struc-
ture from the sentence being analyzed.
This makes the choice of syntactic repre-
sentation an essential design decision. In
this paper, we study the influence of syn-
tactic representation on the performance
of SRL systems. Specifically, we com-
pare constituent-based and dependency-
based representations for SRL of English
in the FrameNet paradigm.
Contrary to previous claims, our results
demonstrate that the systems based on de-
pendencies perform roughly as well as
those based on constituents: For the ar-
gument classification task, dependency-
based systems perform slightly higher on
average, while the opposite holds for the
argument identification task. This is re-
markable because dependency parsers are
still in their infancy while constituent pars-
ing is more mature. Furthermore, the re-
sults show that dependency-based seman-
tic role classifiers rely less on lexicalized
features, which makes them more robust
to domain changes and makes them learn
more efficiently with respect to the amount
of training data.
1 Introduction
The role-semantic paradigm has a long and rich
history in linguistics, and the NLP community
has recently devoted much attention to develop-
ing accurate and robust methods for performing
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
role-semantic analysis automatically (Gildea and
Jurafsky, 2002; Litkowski, 2004; Carreras and
M?rquez, 2005; Baker et al, 2007). It is widely
conjectured that an increased SRL accuracy will
lead to improvements in certain NLP applica-
tions, especially template-filling systems. SRL has
also been used in prototypes of more advanced
semantics-based applications such as textual en-
tailment recognition.
It has previously been shown that SRL systems
need a syntactic structure as input (Gildea and
Palmer, 2002; Punyakanok et al, 2008). An im-
portant consideration is then what information this
input should represent. By habit, most systems for
automatic role-semantic analysis have used Penn-
style constituents (Marcus et al, 1993) produced
by Collins? (1997) or Charniak?s (2000) parsers.
The influence of the syntactic formalism on SRL
has only been considered in a few previous arti-
cles. For instance, Gildea and Hockenmaier (2003)
reported that a CCG-based parser gives improved
results over the Collins parser.
Dependency syntax has only received little at-
tention for the SRL task, despite a surge of inter-
est in dependency parsing during the last few years
(Buchholz and Marsi, 2006). Early examples of
dependency-based SRL systems, which used gold-
standard dependency treebanks, include ?abokrt-
sk? et al (2002) and Hacioglu (2004). Two stud-
ies that compared the respective performances of
constituent-based and dependency-based SRL sys-
tems (Pradhan et al, 2005; Swanson and Gor-
don, 2006), both using automatic parsers, reported
that the constituent-based systems outperformed
the dependency-based ones by a very wide mar-
gin. However, the figures reported in these studies
can be misleading since the comparison involved a
10-year-old rule-based dependency parser versus a
state-of-the-art statistical constituent parser. The
recent progress in statistical dependency parsing
gives grounds for a new evaluation.
393
In addition, there are a number of linguistic mo-
tivations why dependency syntax could be bene-
ficial in an SRL context. First, complex linguis-
tic phenomena such as wh-word extraction and
topicalization can be transparently represented by
allowing nonprojective dependency links. These
links also justify why dependency syntax is of-
ten considered superior for free-word-order lan-
guages; it is even very questionable whether the
traditional constituent-based SRL strategies are vi-
able for such languages. Second, grammatical
function such as subject and object is an integral
concept in dependency syntax. This concept is in-
tuitive when reasoning about the link between syn-
tax and semantics, and it has been used earlier in
semantic interpreters such as Absity (Hirst, 1983).
However, except from a few tentative experiments
(Toutanova et al, 2005), grammatical function is
not explicitly used by current automatic SRL sys-
tems, but instead emulated from constituent trees
by features like the constituent position and the
governing category. More generally, these lin-
guistic reasons have made a number of linguists
argue that dependency structures are more suit-
able for explaining the syntax-semantics interface
(Mel?c?uk, 1988; Hudson, 1984).
In this work, we provide a new evaluation of
the influence of the syntactic representation on se-
mantic role labeling in English. Contrary to previ-
ously reported results, we show that dependency-
based systems are on a par with constituent-based
systems or perform nearly as well. Furthermore,
we show that semantic role classifiers using a de-
pendency parser learn faster than their constituent-
based counterparts and therefore need less train-
ing data to achieve similar performances. Finally,
dependency-based role classifiers are more robust
to vocabulary change and outperform constituent-
based systems when using out-of-domain test sets.
2 Statistical Dependency Parsing for
English
Except for small-scale efforts, there is no depen-
dency treebank of significant size for English. Sta-
tistical dependency parsers of English must there-
fore rely on dependency structures automatically
converted from a constituent corpus such as the
Penn Treebank (Marcus et al, 1993).
Typical approaches to conversion of constituent
structures into dependencies are based on hand-
constructed head percolation rules, an idea that has
its roots in lexicalized constituent parsing (Mager-
man, 1994; Collins, 1997). The head rules cre-
ated by Yamada and Matsumoto (2003) have been
used in almost all recent work on statistical depen-
dency parsing of English (Nivre and Scholz, 2004;
McDonald et al, 2005).
Recently, Johansson and Nugues (2007) ex-
tended the head percolation strategy to incorporate
long-distance links such as wh-movement and top-
icalization, and used the full set of grammatical
function tags from Penn in addition to a number of
inferred tags (in total 57 function tags). A depen-
dency parser based on this syntax was used in the
best-performing system in the SemEval-2007 task
on Frame-semantic Structure Extraction (Baker et
al., 2007), and the conversion method (in two dif-
ferent forms) was used for the English data in the
CoNLL Shared Tasks of 2007 and 2008.
3 Automatic Semantic Role Labeling
with Constituents and Dependencies
To study the influence of syntactic representation
on SRL performance, we developed a framework
that could be easily parametrized to process either
constituent or dependency input1. This section de-
scribes its implementation. As the role-semantic
paradigm, we used FrameNet (Baker et al, 1998).
3.1 Systems
We built SRL systems based on six different
parsers. All parsers were trained on the Penn Tree-
bank, either directly for the constituent parsers or
through the LTH constituent-to-dependency con-
verter (Johansson and Nugues, 2007). Our systems
are identified as follows:
LTH. A dependency-based system using the LTH
parser (Johansson and Nugues, 2008).
Malt. A dependency-based system using
MaltParser (Nivre et al, 2007).
MST. A dependency-based system using
MSTParser (McDonald et al, 2005).
C&J. A constituent-based system using the
reranking parser (the May 2006 version) by
Charniak and Johnson (2005).
Charniak. A constituent-based system using
Charniak?s parser (Charniak, 2000).
Collins. A constituent-based system using
Collins? parser (Collins, 1997).
1Our implementation is available for download at
http://nlp.cs.lth.se/fnlabeler.
394
MaltParser is an incremental greedy classifier-
based parser based on SVMs, while the LTH parser
and MSTParser use exact edge-factored search
with a linear model trained using online margin-
based structure learning. MaltParser and MST-
Parser have achieved state-of-the-art results for a
wide range of languages in the 2006 and 2007
CoNLL Shared Tasks on dependency parsing, and
the LTH parser obtained the best result in the 2008
CoNLL Shared Task on joint syntactic and seman-
tic parsing. Charniak?s and Collins? parsers are
widely used constituent parsers for English, and
the C&J parser is the best-performing freely avail-
able constituent parser at the time of writing ac-
cording to published figures. Charniak?s parser
and the C&J parser come with a built-in part-of-
speech tagger; all other systems used the Stanford
tagger (Toutanova et al, 2003).
Following Gildea and Jurafsky (2002), the SRL
problem is traditionally divided into two subtasks:
identifying the arguments and labeling them with
semantic roles. Although state-of-the-art SRL sys-
tems use sophisticated statistical models to per-
form these two tasks jointly (e.g. Toutanova et
al., 2005, Johansson and Nugues, 2008), we im-
plemented them as two independent support vector
classifiers to be able to analyze the impact of syn-
tactic representation on each task separately. The
features used by the classifiers are traditional, al-
though the features for the dependency-based clas-
sifiers needed some adaptation. Table 1 enumer-
ates the features, which are described in more de-
tail in Appendix A. The differences in the fea-
ture sets reflect the structural differences between
constituent and dependency trees: The constituent-
only features are based on phrase tags and the
dependency-only features on grammatical func-
tions labels.
3.2 Dependency-based Argument
Identification
The argument identification step consists of find-
ing the arguments for a given predicate. For
constituent-based SRL, this problem is formulated
as selecting a subset of the constituents in a parse
tree. This is then implemented in practice as a
binary classifier that determines whether or not a
given constituent is an argument. We approached
the problem similarly in the dependency frame-
work, applying the classifier on dependency nodes
rather than constituents. In both cases, the identi-
Argument Argument
Features identification classification
TARGETLEMMA C,D C,D
FES C,D C,D
TARGETPOS C,D C,D
VOICE C,D C,D
POSITION C,D C,D
ARGWORD/POS C,D C,D
LEFTWORD/POS C,D C,D
RIGHTWORD/POS C,D C,D
PARENTWORD/POS C,D
C-SUBCAT C C
C-PATH C C
PHRASETYPE C C
GOVCAT C C
D-SUBCAT D D
D-PATH D D
CHILDDEPSET D D
PARENTHASOBJ D
RELTOPARENT D
FUNCTION D
Table 1: Classifier features. The features used by
the constituent-based and the dependency-based
systems are marked C and D, respectively.
fication step was preceded by a pruning stage that
heuristically removes parse tree nodes unlikely to
represent arguments (Xue and Palmer, 2004).
To score the performance of the argument iden-
tifier, traditional evaluation procedures treat the
identification as a bracketing problem, meaning
that the entities scored by the evaluation procedure
are labeled snippets of text; however, it is ques-
tionable whether this is the proper way to evalu-
ate a task whose purpose is to find semantic re-
lations between logical entities. We believe that
the same criticisms that have been leveled at the
PARSEVAL metric for constituent structures are
equally valid for the bracket-based evaluation of
SRL systems. The inappropriateness of the tra-
ditional metric has led to a number of alternative
metrics (Litkowski, 2004; Baker et al, 2007).
We have stuck to the traditional bracket-based
scoring metric for compatibility with previous re-
sults, but since it represents the arguments as la-
beled spans, a conversion step is needed when us-
ing dependencies. Algorithm 1 shows how the
spans are constructed from the argument depen-
dency nodes. For each argument node, the algo-
rithm computes the yield Y , the set of dependency
nodes to include in the bracketing. This set is then
partitioned into contiguous parts, which are then
converted into spans. In most cases, the yield is
just the subtree dominated by the argument node.
However, if the argument dominates the predi-
395
cate, then the branch containing the predicate is
removed. Also, FrameNet alows arguments to co-
incide with the predicate; in this case, the yield is
just the predicate node.
Algorithm 1 Span creation from argument depen-
dency nodes.
input Predicate node p, argument node a
if a does not dominate p
Y ? {n; a dominates n}
else if p = a
Y ? {p}
else
c? the child of a that dominates p
Y ? {n; a dominates n} \ {n; c dominates n}
end if
S ? partition of Y into contiguous subsets
return {(min-index s,max-index s); s ? S}
that we have been relying onthe ideas
ROOT?FRAG
VCSBJ VC CLR
PMOD
NMOD
NMOD
Figure 1: Example of a dependency tree containing
a predicate relyingwith three arguments: the ideas,
we, and on . . . that.
To illustrate Algorithm 1, consider Figure 1. In
this sentence, the predicate relying has three argu-
ments: the ideas, we, and on . . . that. The simplest
of them is we, which does not dominate its predi-
cate and which is not discontinuous. A more com-
plex case is the discontinuous argument headed by
on, where the yield {on, that} is partitioned into
two subsets that result in two separate spans. Fi-
nally, the dependency node ideas dominates the
predicate. In this case, the algorithm removes the
subtree headed by have, so the remaining yield is
{the, ideas}.
4 Experiments
We carried out a number of experiments to com-
pare the influence of the syntactic representation
on different aspects of SRL performance. We
used the FrameNet example corpus and running-
text corpus, from which we randomly sampled a
training and test set. The training set consisted
of 134,697 predicates and 271,560 arguments, and
the test set of 14,952 predicates and 30,173 argu-
ments. This does not include null-instantiated ar-
guments, which were removed from the training
and test sets.
4.1 Argument Identification
Before evaluating the full automatic argument
identification systems, we studied the effect of
the span creation from dependency nodes (Algo-
rithm 1). To do this, we measured the upper-bound
recall of argument identification using the con-
ventional span-based evaluation metric. We com-
pared the quality of pruned spans (Algorithm 1)
to unpruned spans (a baseline method that brack-
ets the full subtree). Table 2 shows the re-
sults of this experiment. The figures show that
proper span creation is essential when the tradi-
tional metrics are used: For all dependency-based
systems, the upper-bound recall increases signif-
icantly. However, the dependency-based systems
generally have lower figures for the upper-bound
recall than constituent-based ones.
System Pruned Unpruned
LTH 83.9 82.1
Malt 82.1 78.3
MST 80.4 77.1
C&J 85.3
Charniak 83.4
Collins 81.8
Table 2: Upper-bound recall for argument identifi-
cation.
Our first experiment investigated how the syn-
tactic representation influenced the performance
of the argument identification step. Table 3
shows the result of this evaluation. As can be
seen, the constituent-based systems outperform the
dependency-based systems on average. However,
the picture is not clear enough to draw any firm
conclusion about a fundamental structural differ-
ence. There are also a number of reasons to be cau-
tious: First, the dependency parsers were trained
on a treebanks that had been automatically cre-
ated from a constituent treebank, which probably
results in a slight decrease in annotation quality.
Second, dependency parsing is still a developing
field, while constituent parsing is more mature.
The best constituent parser (C&J) is a reranking
parser utilizing global features, while the depen-
dency parsers use local features only; we believe
396
that a reranker could be used to improve the de-
pendency parsers as well.
System P R F1
LTH 79.7 77.3 78.5
Malt 77.4 73.8 75.6
MST 73.9 71.9 72.9
C&J 81.4 77.3 79.2
Charniak 79.8 75.0 77.3
Collins 78.4 72.9 75.6
Table 3: Argument identification performance.
Differences between parsers using the same syn-
tactic formalism are also considerable, which sug-
gests that the attachment accuracy is probably the
most important parameter when choosing a parser
for this task.
4.2 Argument Classification
To evaluate the argument classification accuracies,
we provided the systems with gold-standard argu-
ments, which were then automatically classified.
Table 4 shows the results.
System Accuracy
LTH 89.6
Malt 88.5
MST 88.1
C&J 88.9
Charniak 88.5
Collins 88.3
Table 4: Semantic role classification accuracy.
Here, the situation is different: the best
dependency-based system make 6.3% fewer errors
than the best constituent-based one, a statistically
significant difference at the 99.9% level according
to a McNemar test. Again, there are no clear differ-
ences that can be attributed to syntactic formalism.
However, this result is positive, because it shows
clearly that SRL can be used in situations where
only dependency parsers are available.
On the other hand, it may seem paradoxical
that the rich set of grammatical functions used by
the dependency-based systems did not lead to a
clearer difference between the groups, despite the
linguistic intuition that this feature should be use-
ful for argument classification. Especially for for
the second- and third-best systems (Malt and MST
versus Charniak and Collins), the performance fig-
ures are almost identical. However, all systems
use lexical features of the argument, and given
enough training data, one may say that the gram-
matical function is implicitly encoded in these fea-
tures. This suggests that lexical features are more
important for constituent-based systems than for
dependency-based ones.
4.3 Robustness of SRL Classifiers
In this section, we test the hypothesis that the
SRL systems based on dependency syntax rely less
heavily on lexical features. We also investigate
two parameters that are influenced by lexicaliza-
tion: domain sensitivity and the amount of training
data required by classifiers.
Tests of Unlexicalized Models
To test the hypothesis about the reliance on lex-
icalization, we carried out a series of experiments
where we set aside the lexical features of the argu-
ment. Table 5 shows the results.
As expected, there is a sharp drop in perfor-
mance for all systems, but the results are very
clear: When no argument lexical features are avail-
able, the dependency-based systems have a supe-
rior performance. The difference between MST
and C&J constitutes an error reduction of 6.9% and
is statistically significant at the 99.9% level.
System Accuracy
LTH 83.0
Malt 81.9
MST 81.7
C&J 80.3
Charniak 80.0
Collins 79.8
Table 5: Accuracy for unlexicalized role classi-
fiers. Dependency-based systems make at least
6.9% fewer errors.
Training Set Size
Since the dependency-based systems rely less
on lexicalization, we can expect them to have a
steeper learning curve. To investigate this, we
trained semantic role classifiers using training sets
of varying sizes and compared the average clas-
sification accuracies of the two groups. Fig-
ure 2 shows the reduction in classification error
of the dependency-based group compared to the
constituent-based group (again, all systems were
lexicalized). For small training sets, the differ-
ences are large; the largest observed error reduc-
397
tion was 5.4% with a training set of 25,000 in-
stances. When the training set size increases, the
difference between the groups decreases. The plot
is consistent with our hypothesis that the gram-
matical function features used by the dependency-
based systems make generalization easier for sta-
tistical classifiers. We interpret the flatter learning
curves for constituent-based systems as a conse-
quence of lexicalization ? these systems need more
training data to use lexical information to capture
grammatical function information implicitly.
105
1
1.5
2
2.5
3
3.5
4
4.5
5
5.5
6
Training set size
Er
ro
r r
ed
uc
tio
n
Figure 2: Error reduction of average dependency-
based systems as a function of training set size.
Out-of-domain Test Sets
We finally conducted an evaluation of the se-
mantic role classification accuracies on an out-of-
domain test set: the FrameNet-annotated Nuclear
Threat Initiative texts from SemEval task (Baker
et al, 2007). Table 6 shows the results. This cor-
pus contained 9,039 predicates and 15,343 argu-
ments. The writing style is very different from
the FrameNet training data, and the annotated data
contain several instances of predicates and frames
unseen in the training set. We thus see that all sys-
tems suffer severely from domain sensitivity, but
we also see that the dependency-based systems are
more resilient ? the difference between MST and
C&J is statistically significant at the 97.5% level
and corresponds to an error reduction of 2%. The
experiment reconfirms previous results (Carreras
and M?rquez, 2005) that the argument classifica-
tion part of SRL systems is sensitive to domain
changes, and Pradhan et al (2008) argued that an
important reason for this is that the lexical fea-
tures are heavily domain-dependent. Our results
are consistent with this hypothesis, and suggest
that the inclusion of grammatical function features
is an effective way to mitigate this sensitivity.
System Accuracy
LTH 71.1
Malt 70.1
MST 70.1
C&J 69.5
Charniak 69.3
Collins 69.3
Table 6: Classification accuracy on the NTI texts.
Dependency-based systems make 2% fewer errors.
5 Discussion
We have described a set of experiments that in-
vestigate the relation between syntactic represen-
tation and semantic role labeling performance,
specifically focusing on a comparison between
constituent- and dependency-based SRL systems.
A first conclusion is that our dependency-based
systems perform more or less as well as the more
mature constituent-based systems: For the argu-
ment classification task, dependency-based sys-
tems are slightly better on average, while the
constituent-based systems perform slightly higher
in argument identification.
This result contrasts with previously published
comparisons, which used less accurate depen-
dency parsers, and shows that semantic analyz-
ers can be implemented for languages where con-
stituent parsers are not available. While traditional
constituent-based SRL techniques have so far been
applied to languages characterized by simple mor-
phology and rigid word order, such as English and
Chinese, we think that dependency-based SRL can
be particularly useful for languages with a free
word order.
For dependency-based systems, the conversion
from parse tree nodes to argument spans, which
are needed to use the traditional span-based evalu-
ation method, is less trivial than in the constituent
case. To make a comparison feasible, we imple-
mented an algorithm for span creation from ar-
gument nodes. However, the fundamental prob-
lem lies in evaluation ? the field needs to design
new evaluation procedures that use some sort of
link-based scoring method. The evaluation met-
rics used in the SemEval task on Frame-semantic
398
Structure Extraction and the 2008 CoNLL Shared
Task are examples of steps in the right direction.
Our second main result is that for argument
classification, dependency-based systems rely less
heavily on lexicalization, and we suggest that this
is because they use features based on grammatical
function labels. These features make the learning
curve steeper when training the classifier, and im-
prove robustness to domain changes.
A Features Used by the Classifiers
The following subsections itemize the features
used by the systems. All examples are given with
respect to the sentence she gave the horse an apple.
The constituent and dependency trees are shown
in Figure 3. For this sentence, the predicate is
gave, which has the FrameNet frame GIVING. It
has three arguments: she, which has the DONOR
role; the horse, the RECIPIENT; and an apple, the
THEME.
NP NP NP
VP
S
gave the horse an appleshe
gave the horse an appleshe
SBJ
ROOT?S
NMODNMOD
IOBJ
OBJ
Figure 3: Examples of parse trees.
A.1 Common Features
The following features are used by both the
constituent-based and the dependency-based se-
mantic analyzers. Head-finding rules (Johansson
and Nugues, 2007) were applied when heads of
constituents were needed.
TARGETLEMMA. The lemma of the target word
itself, e.g. give.
FES. For a given frame, the set of available frame
elements listed in FrameNet. For instance, for
give in the GIVING frame, we have 12 frame
elements: DONOR, RECIPIENT, THEME, . . .
TARGETPOS. Part-of-speech tag for the target
word.
VOICE. For verbs, this feature is Active or Pas-
sive. For other types of words, it is not de-
fined.
POSITION. Position of the head word of the argu-
ment with respect to the target word: Before,
After, or On.
ARGWORD and ARGPOS. Lexical form and
part-of-speech tag of the head word of the ar-
gument.
LEFTWORD and LEFTPOS. Form and part-of-
speech tag of the leftmost dependent of the
argument head.
RIGHTWORD and RIGHTPOS. Form and part-
of-speech tag of the rightmost dependent of
the argument head.
PARENTWORD and PARENTPOS. Form and
part-of-speech tag of the parent node of the
target.
A.2 Features Used by the Constituent-based
Analyzer Only
C-SUBCAT. Subcategorization frame: corre-
sponds to the phrase-structure rule used to ex-
pand the phrase around the target. For give in
the example, this feature is VP?VB NP NP.
C-PATH. A string representation of the path
through the constituent tree from the target
word to the argument constituent. For in-
stance, the path from gave to she is ?VP-?S-
?NP.
PHRASETYPE. Phrase type of the argument con-
stituent, e.g. NP for she.
GOVCAT. Governing category: this feature is ei-
ther S or VP, and is found by starting at the ar-
gument constituent and moving upwards until
either a VP or a sentence node (S, SINV, or
SQ) is found. For instance, for she, this fea-
ture is S, while for the horse, it is VP. This
can be thought of as a very primitive way of
distinguishing subjects and objects.
A.3 Features Used by the Dependency-based
Analyzer Only
D-SUBCAT. Subcategorization frame: the
grammatical functions of the dependents
concatenated. For gave, this feature is
SBJ+IOBJ+OBJ.
D-PATH. A string representation of the path
through the dependency tree from the target
node to the argument node. Moving upwards
through verb chains is not counted in this path
string. In the example, the path from gave to
she is ?SBJ.
399
CHILDDEPSET. The set of grammatical func-
tions of the direct dependents of the target
node. For instance, for give, this is { SBJ,
IOBJ, OBJ }.
PARENTHASOBJ. Binary feature that is set to
true if the parent of the target has an object.
RELTOPARENT. Dependency relation between
the target node and its parent.
FUNCTION. The grammatical function of the ar-
gument node. For direct dependents of the
target, this feature is identical to the D-PATH.
References
Baker, Collin F., Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet Project. In Proceedings
of COLING/ACL-1998.
Baker, Collin, Michael Ellsworth, and Katrin Erk. 2007. Se-
mEval task 19: Frame semantic structure extraction. In
Proceedings of SemEval-2007.
Buchholz, Sabine and Erwin Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proceedings
of the CoNLL-X.
Carreras, Xavier and Llu?s M?rquez. 2005. Introduction to
the CoNLL-2005 shared task: Semantic role labeling. In
Proceedings of CoNLL-2005.
Charniak, Eugene and Mark Johnson. 2005. Coarse-to-fine
n-best parsing and MaxEnt discriminative reranking. In
Proceedings of ACL.
Charniak, Eugene. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL-2000.
Collins, Michael. 1997. Three generative, lexicalised models
for statistical parsing. In Proceedings of ACL/EACL-1997.
Gildea, Daniel and Julia Hockenmaier. 2003. Identifying
semantic roles using combinatory categorial grammar. In
Proceedings of EMNLP-2003.
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguistics,
28(3):245?288.
Gildea, Daniel and Martha Palmer. 2002. The necessity of
syntactic parsing for predicate argument recognition. In
Proceedings of the ACL-2002.
Hacioglu, Kadri. 2004. Semantic role labeling using depen-
dency trees. In Proceedings of COLING-2004.
Hirst, Graeme. 1983. A foundation for semantic interpreta-
tion. In Proceedings of the ACL-1983.
Hudson, Richard. 1984. Word Grammar. Blackwell.
Johansson, Richard and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English. In Pro-
ceedings of NODALIDA 2007.
Johansson, Richard and Pierre Nugues. 2008. Dependency-
based syntactic?semantic analysis with PropBank and
NomBank. In Proceedings of CoNLL?2008.
Litkowski, Ken. 2004. Senseval-3 task: Automatic labeling
of semantic roles. In Proceedings of Senseval-3.
Magerman, David M. 1994. Natural language parsing as sta-
tistical pattern recognition. Ph.D. thesis, Stanford Univer-
sity.
Marcus, Mitchell P., Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of English: the Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
McDonald, Ryan, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency parsers.
In Proceedings of ACL-2005.
Mel?c?uk, Igor A. 1988. Dependency Syntax: Theory and
Practice. State University Press of New York.
Nivre, Joakim and Mario Scholz. 2004. Deterministic depen-
dency parsing of English text. In Proceedings of COLING-
2004.
Nivre, Joakim, Johan Hall, Jens Nilsson, Atanas Chanev,
G?ls?en Eryig?it, Sandra K?bler, Svetoslav Marinov, and Er-
win Marsi. 2007. MaltParser: A language-independent
system for data-driven dependency parsing. Natural Lan-
guage Engineering, 13(2):95?135.
Pradhan, Sameer, Wayne Ward, Kadri Hacioglu, James Mar-
tin, and Daniel Jurafsky. 2005. Semantic role labeling
using different syntactic views. In Proceedings of ACL-
2005.
Pradhan, Sameer, Wayne Ward, and James H. Martin. 2008.
Towards robust semantic role labeling. Computational
Linguistics, 34(2):289?310.
Punyakanok, Vasin, Dan Roth, and Wen-tau Yih. 2008. The
importance of syntactic parsing and inference in semantic
role labeling. Computational Linguistics, 34(2):257?287.
Swanson, Reid and Andrew S. Gordon. 2006. A comparison
of alternative parse tree paths for labeling semantic roles.
In Proceedings of COLING/ACL-2006.
Toutanova, Kristina, Dan Klein, Christopher Manning, and
Yoram Singer. 2003. Feature-rich part-of-speech tagging
with a cyclic dependency network. In Proceedings of HLT-
NAACL-2003.
Toutanova, Kristina, Aria Haghighi, and Christopher D. Man-
ning. 2005. Joint learning improves semantic role label-
ing. In Proceedings of ACL-2005.
?abokrtsk?, Zdene?k, Petr Sgall, and Sa?o D?eroski. 2002.
A machine learning approach to automatic functor assign-
ment in the Prague dependency treebank. In Proceedings
of LREC-2002.
Xue, Nianwen and Martha Palmer. 2004. Calibrating features
for semantic role labeling. In Proceedings of EMNLP-
2004.
Yamada, Hiroyasu and Yuji Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In Pro-
ceedings of IWPT-2003.
400
Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 1134?1138,
Prague, June 2007. c?2007 Association for Computational Linguistics
Incremental Dependency Parsing Using Online Learning
Richard Johansson and Pierre Nugues
Department of Computer Science, Lund University, Sweden
{richard, pierre}@cs.lth.se
Abstract
We describe an incremental parser that
was trained to minimize cost over sen-
tences rather than over individual parsing ac-
tions. This is an attempt to use the advan-
tages of the two top-scoring systems in the
CoNLL-X shared task.
In the evaluation, we present the perfor-
mance of the parser in the Multilingual task,
as well as an evaluation of the contribution
of bidirectional parsing and beam search to
the parsing performance.
1 Introduction
The two best-performing systems in the CoNLL-X
shared task (Buchholz and Marsi, 2006) can be clas-
sified along two lines depending on the method they
used to train the parsing models. Although the
parsers are quite different, their creators could re-
port near-tie scores. The approach of the top sys-
tem (McDonald et al, 2006) was to fit the model
to minimize cost over sentences, while the second-
best system (Nivre et al, 2006) trained the model to
maximize performance over individual decisions in
an incremental algorithm. This difference is a nat-
ural consequence of their respective parsing strate-
gies: CKY-style maximization of link score and in-
cremental parsing.
In this paper, we describe an attempt to unify the
two approaches: an incremental parsing strategy that
is trained to maximize performance over sentences
rather than over individual parsing actions.
2 Parsing Method
2.1 Nivre?s Parser
We used Nivre?s algorithm (Nivre et al, 2006),
which is a variant of the shift?reduce parser. Like
the regular shift?reduce, it uses a stack S and a list
of input words W , and builds the parse tree incre-
mentally using a set of parsing actions (see Table 1).
It can be shown that Nivre?s parser creates projec-
tive and acyclic graphs and that every projective de-
pendency graph can be produced by a sequence of
parser actions. In addition, the worst-case number of
actions is linear with respect to the number of words
in the sentence.
2.2 Handling Nonprojective Parse Trees
While the parsing algorithm produces projective
trees only, nonprojective arcs can be handled using
a preprocessing step before training the model and a
postprocessing step after parsing the sentences.
The projectivization algorithm (Nivre and Nils-
son, 2005) iteratively moves each nonprojective arc
upward in the tree until the whole tree is projective.
To be able to recover the nonprojective arcs after
parsing, the projectivization operation replaces the
labels of the arcs it modifies with traces indicating
which links should be moved and where attach to at-
tach them (the ?Head+Path? encoding). The model
is trained with these new labels that makes it pos-
sible to carry out the reverse operation and produce
nonprojective structures.
2.3 Bidirectional Parsing
Shift-reduce is by construction a directional parser,
typically applied from left to right. To make bet-
ter use of the training set, we applied the algorithm
in both directions as Johansson and Nugues (2006)
and Sagae and Lavie (2006) for all languages except
Catalan and Hungarian. This, we believe, also has
the advantage of making the parser less sensitive to
whether the language is head-initial or head-final.
We trained the model on projectivized graphs
from left to right and right to left and used a vot-
ing strategy based on link scores. Each link was as-
signed a score (simply by using the score of the la
or ra actions for each link). To resolve the conflicts
1134
Table 1: Nivre?s parser transitions where W is the initial word list; I , the current input word list; A, the
graph of dependencies; and S, the stack. (n?, n) denotes a dependency relations between n? and n, where n?
is the head and n the dependent.
Actions Parser actions Conditions
Initialize ?nil, W, ??
Terminate ?S, nil, A?
Left-arc ?n|S, n?|I,A? ? ?S, n?|I, A ? {(n?, n)}? ??n??(n??, n) ? A
Right-arc ?n|S, n?|I,A? ? ?n?|n|S, I,A ? {(n, n?)}? ??n??(n??, n?) ? A
Reduce ?n|S, I,A? ? ?S, I, A? ?n?(n?, n) ? A
Shift ?S, n|I, A? ? ?n|S, I,A?
between the two parses in a manner that makes the
tree projective, single-head, rooted, and cycle-free,
we applied the Eisner algorithm (Eisner, 1996).
2.4 Beam Search
As in our previous parser (Johansson and Nugues,
2006), we used a beam-search extension to Nivre?s
original algorithm (which is greedy in its original
formulation). Each parsing action was assigned a
score, and the beam search allows us to find a bet-
ter overall score of the sequence of actions. In
this work, we used a beam width of 8 for Catalan,
Chinese, Czech, and English and 16 for the other
languages.
3 Learning Method
3.1 Overview
We model the parsing problem for a sentence x as
finding the parse y? = arg maxy F (x, y) that max-
imizes a discriminant function F . In this work, we
consider linear discriminants of the following form:
F (x, y) = w ??(x, y)
where ?(x, y) is a numeric feature representation
of the pair (x, y) and w a vector of feature weights.
Learning F in this case comes down to assigning
good weights in the vector w.
Machine learning research for similar prob-
lems have generally used margin-based formula-
tions. These include global batch methods such
as SVMstruct (Tsochantaridis et al, 2005) as well
as online methods such as the Online Passive-
Aggressive Algorithm (OPA) (Crammer et al,
2006). Although the batch methods are formulated
very elegantly, they do not seem to scale well to
the large training sets prevalent in NLP contexts ?
we briefly considered using SVMstruct but train-
ing was too time-consuming. The online methods
on the other hand, although less theoretically ap-
pealing, can handle realistically sized data sets and
have successfully been applied in dependency pars-
ing (McDonald et al, 2006). Because of this, we
used the OPA algorithm throughout this work.
3.2 Implementation
In the online learning framework, the weight vector
is constructed incrementally. At each step, it com-
putes an update to the weight vector based on the
current example. The resulting weight vector is fre-
quently overfit to the last examples. One way to
reduce overfitting is to use the average of all suc-
cessive weight vectors as the result of the training
(Freund and Schapire, 1999).
Algorithm 1 shows the algorithm. It uses an
?aggressiveness? parameter C to reduce overfitting,
analogous to the C parameter in SVMs. The algo-
rithm also needs a cost function ?, which describes
how much a parse tree deviates from the gold stan-
dard. In this work, we defined ? as the sum of link
costs, where the link cost was 0 for a correct depen-
dency link with a correct label, 0.5 for a correct link
with an incorrect label, and 1 for an incorrect link.
The number of iterations was 5 for all languages.
For a sentence x and a parse tree y, we defined
the feature representation by finding the sequence
??S1, I1? , a1? , ??S2, I2? , a2? . . . of states and their
corresponding actions, and creating a feature vector
for each state/action pair. The discriminant function
was thus written
?(x, y) ?w =
?
i
?(?Si, Ii? , ai) ?w
where ? is a feature function that assigns a feature
1135
Algorithm 1 The Online PA Algorithm
input Training set T = {(xt, yt)}Tt=1
Number of iterations N
Regularization parameter C
Cost function ?
Initialize w to zeros
repeat N times
for (xt, yt) in T
let y?t = arg maxy F (xt, y) +
?
?(yt, y)
let ?t = min
(
C, F (xt,y?t)?F (xt,yt)+
?
?(yt,y?t)
??(x,yt)??(x,y?t)?2
)
w ? w + ?t(?(x, yt)??(x, y?t))
return waverage
vector to a state ?Si, Ii? and the action ai taken in
that state. Table 2 shows the feature sets used in
? for all languages. In principle, a kernel could
also be used, but that would degrade performance
severely. Instead, we formed a new vector by com-
bining features pairwisely ? this is equivalent to us-
ing a quadratic kernel.
Since the history-based feature set used in the
parsing algorithm makes it impossible to use inde-
pendence to factorize the scoring function, an ex-
act search to find the best-scoring action sequence
(arg maxy in Algorithm 1) is not possible. How-
ever, the beam search allows us to find a reasonable
approximation.
4 Results
Table 3 shows the results of our system in the Mul-
tilingual task.
4.1 Compared to SVM-based Local Classifiers
We compared the performance of the parser with
a parser based on local SVM classifiers (Johansson
and Nugues, 2006). Table 4 shows the performance
of both parsers on the Basque test set. We see that
what is gained by using a global method such as
OPA is lost by sacrificing the excellent classifica-
tion performance of the SVM. Possibly, better per-
formance could be achieved by using a large-margin
batch method such as SVMstruct.
Table 2: Feature sets.
ar ca cs el en eu hu it tr zh
Fine POS top ? ? ? ? ? ? ? ? ? ?
Fine POS top-1 ? ? ? ? ? ? ?
Fine POS list ? ? ? ? ? ? ? ? ? ?
Fine POS list-1 ? ? ? ? ? ? ? ? ? ?
Fine POS list+1 ? ? ? ? ? ? ? ? ? ?
Fine POS list+2 ? ? ? ? ? ? ? ? ? ?
Fine POS list+3 ? ? ? ? ? ?
POS top ? ? ? ? ? ? ? ? ? ?
POS top-1 ?
POS list ? ? ? ? ? ? ? ? ? ?
POS list-1 ? ? ? ? ? ?
POS list+1 ? ? ? ? ? ? ? ? ? ?
POS list+2 ? ? ? ? ? ? ? ?
POS list+3 ? ? ? ? ? ? ? ?
Features top ? ? ? ? ? ? ? ?
Features list ? ? ? ? ? ? ? ?
Features list-1 ? ? ? ? ?
Features list+1 ? ? ? ? ? ? ?
Features list+2 ? ? ? ? ?
Word top ? ? ? ? ? ? ? ? ?
Word top-1 ? ?
Word list ? ? ? ? ? ? ? ? ? ?
Word list-1 ? ? ? ? ? ?
Word list+1 ? ? ? ?
Lemma top ? ? ? ? ? ?
Lemma list ? ? ? ? ?
Lemma list-1 ? ?
Relation top ? ?
Relation top left ? ? ? ? ?
Relation top right ? ? ? ? ?
Relation list right ?
Word top left ?
Word top right ?
Word list left ?
POS top left ? ?
POS top right ? ? ?
POS list left ? ? ? ? ? ? ? ?
Features top right ?
Features first left ? ?
Table 3: Summary of results.
Languages Unlabeled Labeled
Arabic 80.91 71.76
Basque 80.41 75.08
Catalan 88.34 83.33
Chinese 81.30 76.30
Czech 77.39 70.98
English 81.43 80.29
Greek 79.58 72.77
Hungarian 75.53 71.31
Italian 81.55 77.55
Turkish 84.80 78.46
Average result 81.12 75.78
Table 4: Accuracy by learning method.
Learning Method Accuracy
OPA 75.08
SVM 75.53
1136
4.2 Beam Width
To investigate the influence of the beam width on the
performance, we measured the accuracy of a left-to-
right parser on a development set for Basque (15%
of the training data) as a function of the width. Ta-
ble 5 shows the result. We see clearly that widening
the beam considerably improves the figures, espe-
cially in the lower ranges.
Table 5: Accuracy by beam width.
Width Accuracy
2 72.01
4 74.18
6 75.05
8 75.30
12 75.49
4.3 Direction
We also investigated the contribution of the bidirec-
tional parsing. Table 6 shows the result of this exper-
iment on the Basque development set (the same 15%
as in 4.2). The beam width was 2 in this experiment.
Table 6: Accuracy by parsing direction.
Direction Accuracy
Left to right 72.01
Right to left 71.02
Bidirectional 74.48
Time did not allow a full-scale experiment, but
for all languages except Catalan and Hungarian, the
bidirectional parsing method outperformed the uni-
directional methods when trained on a 20,000-word
subset. However, the gain of using bidirectional
parsing may be more obvious when the treebank is
small. For all languages except Czech, left-to-right
outperformed right-to-left parsing.
5 Discussion
The paper describes an incremental parser that we
trained to minimize the cost over sentences, rather
than over parsing actions as is usually done. It
was trained using the Online Passive-Aggressive
method, a cost-sensitive online margin-based learn-
ing method, and shows reasonable performance and
received above-average scores for most languages.
The performance of the parser (relative the other
teams) was best for Basque and Turkish, which were
two of the smallest treebanks. Since we found that
the optimal number of iterations was 5 for Basque
(the smallest treebank), we used this number for all
languages since we did not have time to investigate
this parameter for the other languages. This may
have had a detrimental effect for some languages.
We think that some of the figures might be squeezed
slightly higher by optimizing learning parameters
and feature sets.
This work shows that it was possible to combine
approaches used by Nivre?s and McDonald?s parsers
in a single system. While the parser is outperformed
by a system based on local classifiers, we still hope
that the parsing and training combination described
here opens new ways in parser design and eventually
leads to the improvement of parsing performance.
Acknowledgements
This work was made possible because of the anno-
tated corpora that were kindly provided to us: (Hajic?
et al, 2004; Aduriz et al, 2003; Mart? et al, 2007;
Chen et al, 2003; B?hmov? et al, 2003; Marcus et
al., 1993; Johansson and Nugues, 2007; Prokopidis
et al, 2005; Csendes et al, 2005; Montemagni et al,
2003; Oflazer et al, 2003)
References
A. Abeill?, editor. 2003. Treebanks: Building and Using
Parsed Corpora. Kluwer.
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,
A. Diaz de Ilarraza, A. Garmendia, and M. Oronoz.
2003. Construction of a Basque dependency treebank.
In Proc. of the 2nd Workshop on Treebanks and Lin-
guistic Theories (TLT), pages 201?204.
A. B?hmov?, J. Hajic?, E. Hajic?ov?, and B. Hladk?. 2003.
The PDT: a 3-level annotation scenario. In Abeill?
(Abeill?, 2003), chapter 7, pages 103?127.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task
on multilingual dependency parsing. In CoNLL-X.
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang,
and Z. Gao. 2003. Sinica treebank: Design criteria,
representational issues and implementation. In Abeill?
(Abeill?, 2003), chapter 13, pages 231?248.
1137
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Schwartz,
and Y. Singer. 2006. Online passive-aggressive algo-
rithms. JMLR, 2006(7):551?585.
D. Csendes, J. Csirik, T. Gyim?thy, and A. Kocsor. 2005.
The Szeged Treebank. Springer.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proceedings of
ICCL.
Y. Freund and R. E. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning, 37(3):277?296.
J. Hajic?, O. Smr?, P. Zem?nek, J. ?naidauf, and E. Be?ka.
2004. Prague Arabic dependency treebank: Develop-
ment in data and tools. In Proc. of the NEMLAR In-
tern. Conf. on Arabic Language Resources and Tools,
pages 110?117.
R. Johansson and P. Nugues. 2006. Investigating multi-
lingual dependency parsing. In CoNLL-X.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proc. of the 16th Nordic Conference on Computational
Linguistics (NODALIDA).
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19(2):313?330.
M. A. Mart?, M. Taul?, L. M?rquez, and M. Bertran.
2007. CESS-ECE: A multilingual and multilevel
annotated corpus. Available for download from:
http://www.lsi.upc.edu/?mbertran/cess-ece/.
R. McDonald, K. Lerman, and F. Pereira. 2006. Multi-
lingual dependency parsing with a two-stage discrimi-
native parser. In CoNLL-X.
S. Montemagni, F. Barsotti, M. Battista, N. Calzolari,
O. Corazzari, A. Lenci, A. Zampolli, F. Fanciulli,
M. Massetani, R. Raffaelli, R. Basili, M. T. Pazienza,
D. Saracino, F. Zanzotto, N. Nana, F. Pianesi, and
R. Delmonte. 2003. Building the Italian Syntactic-
Semantic Treebank. In Abeill? (Abeill?, 2003), chap-
ter 11, pages 189?210.
J. Nivre and J. Nilsson. 2005. Pseudo-projective depen-
dency parsing. In Proceedings of ACL-05.
J. Nivre, J. Hall, J. Nilsson, G. Eryig?it, and S. Marinov.
2006. Labeled pseudo-projective dependency parsing
with support vector machines. In CoNLL-X.
K. Oflazer, B. Say, D. Zeynep Hakkani-T?r, and G. T?r.
2003. Building a Turkish treebank. In Abeill?
(Abeill?, 2003), chapter 15, pages 261?277.
P. Prokopidis, E. Desypri, M. Koutsombogera, H. Papa-
georgiou, and S. Piperidis. 2005. Theoretical and
practical issues in the construction of a Greek depen-
dency treebank. In Proc. of the 4th Workshop on Tree-
banks and Linguistic Theories (TLT), pages 149?160.
K. Sagae and A. Lavie. 2006. Parser combination by
reparsing. In Proceedings of the HLT-NAACL.
I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Al-
tun. 2005. Large margin methods for structured and
interdependent output variables. JMLR, 6:1453?1484.
1138
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 69?78,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Dependency-based Semantic Role Labeling of PropBank
Richard Johansson and Pierre Nugues
Lund University, Sweden
{richard, pierre}@cs.lth.se
Abstract
We present a PropBank semantic role label-
ing system for English that is integrated with
a dependency parser. To tackle the problem
of joint syntactic?semantic analysis, the sys-
tem relies on a syntactic and a semantic sub-
component. The syntactic model is a projec-
tive parser using pseudo-projective transfor-
mations, and the semantic model uses global
inference mechanisms on top of a pipeline of
classifiers. The complete syntactic?semantic
output is selected from a candidate pool gen-
erated by the subsystems.
We evaluate the system on the CoNLL-
2005 test sets using segment-based and
dependency-based metrics. Using the
segment-based CoNLL-2005 metric, our
system achieves a near state-of-the-art F1
figure of 77.97 on the WSJ+Brown test set,
or 78.84 if punctuation is treated consistently.
Using a dependency-based metric, the F1
figure of our system is 84.29 on the test
set from CoNLL-2008. Our system is the
first dependency-based semantic role labeler
for PropBank that rivals constituent-based
systems in terms of performance.
1 Introduction
Automatic semantic role labeling (SRL), the task
of determining who does what to whom, is a use-
ful intermediate step in NLP applications perform-
ing semantic analysis. It has obvious applications
for template-filling tasks such as information extrac-
tion and question answering (Surdeanu et al, 2003;
Moschitti et al, 2003). It has also been used in
prototypes of NLP systems that carry out complex
reasoning, such as entailment recognition systems
(Haghighi et al, 2005; Hickl et al, 2006). In addi-
tion, role-semantic features have recently been used
to extend vector-space representations in automatic
document categorization (Persson et al, 2008).
The NLP community has recently devoted much
attention to developing accurate and robust methods
for performing role-semantic analysis automatically,
and a number of multi-system evaluations have been
carried out (Litkowski, 2004; Carreras andM?rquez,
2005; Baker et al, 2007; Surdeanu et al, 2008).
Following the seminal work of Gildea and Juraf-
sky (2002), there have been many extensions in ma-
chine learning models, feature engineering (Xue and
Palmer, 2004), and inference procedures (Toutanova
et al, 2005; Surdeanu et al, 2007; Punyakanok et
al., 2008).
With very few exceptions (e.g. Collobert and
Weston, 2007), published SRL methods have used
some sort of syntactic structure as input (Gildea and
Palmer, 2002; Punyakanok et al, 2008). Most sys-
tems for automatic role-semantic analysis have used
constituent syntax as in the Penn Treebank (Marcus
et al, 1993), although there has also been much re-
search on the use of shallow syntax (Carreras and
M?rquez, 2004) in SRL.
In comparison, dependency syntax has received
relatively little attention for the SRL task, despite
the fact that dependency structures offer a more
transparent encoding of predicate?argument rela-
tions. Furthermore, the few systems based on de-
pendencies that have been presented have generally
performed much worse than their constituent-based
69
counterparts. For instance, Pradhan et al (2005) re-
ported that a system using a rule-based dependency
parser achieved much inferior results compared to a
system using a state-of-the-art statistical constituent
parser: The F-measure on WSJ section 23 dropped
from 78.8 to 47.2, or from 83.7 to 61.7 when using
a head-based evaluation. In a similar vein, Swanson
and Gordon (2006) reported that parse tree path fea-
tures extracted from a rule-based dependency parser
are much less reliable than those from a modern con-
stituent parser.
In contrast, we recently carried out a de-
tailed comparison (Johansson and Nugues, 2008b)
between constituent-based and dependency-based
SRL systems for FrameNet, in which the results of
the two types of systems where almost equivalent
when using modern statistical dependency parsers.
We suggested that the previous lack of progress in
dependency-based SRL was due to low parsing ac-
curacy. The experiments showed that the grammat-
ical function information available in dependency
representations results in a steeper learning curve
when training semantic role classifiers, and it also
seemed that the dependency-based role classifiers
were more resilient to lexical problems caused by
change of domain.
The recent CoNLL-2008 Shared Task (Surdeanu
et al, 2008) was an attempt to show that SRL can be
accurately carried out using only dependency syn-
tax. However, these results are not easy to compare
to previously published results since the task defini-
tions and evaluation metrics were different.
This paper compares the best-performing sys-
tem in the CoNLL-2008 Shared Task (Johans-
son and Nugues, 2008a) with previously published
constituent-based SRL systems. The system carries
out joint dependency-syntactic and semantic anal-
ysis. We first describe its implementation in Sec-
tion 2, and then compare the system with the best
system in the CoNLL-2005 Shared Task in Section
3. Since the outputs of the two systems are differ-
ent, we carry out two types of evaluations: first by
using the traditional segment-based metric used in
the CoNLL-2005 Shared Task, and then by using
the dependency-based metric from the CoNLL-2008
Shared Task. Both evaluations require a transforma-
tion of the output of one system: For the segment-
based metric, we have to convert the dependency-
based output to segments; and for the dependency-
based metric, a head-finding procedure is needed to
select heads in segments. For the first time for a sys-
tem using only dependency syntax, we report results
for PropBank-based semantic role labeling of En-
glish that are close to the state of the art, and for
some measures even superior.
2 Syntactic?Semantic Dependency
Analysis
The training corpus that we used is the dependency-
annotated Penn Treebank from the 2008 CoNLL
Shared Task on joint syntactic?semantic analysis
(Surdeanu et al, 2008). Figure 1 shows a sentence
annotated in this framework. The CoNLL task in-
volved semantic analysis of predicates from Prop-
Bank (for verbs, such as plan) and NomBank (for
nouns, such as investment); in this paper, we report
the performance on PropBank predicates only since
we compare our system with previously published
PropBank-based SRL systems.
Chrysler plans new investment in Latin America
plan.01
LOC PMODNMODNMODOBJ
A0
investment.01
A1A0 A2
SBJ
ROOT
Figure 1: An example sentence annotated with syntactic
and semantic dependency structures.
We model the problem of constructing a syntac-
tic and a semantic graph as a task to be solved
jointly. Intuitively, syntax and semantics are highly
interdependent and semantic interpretation should
help syntactic disambiguation, and joint syntactic?
semantic analysis has a long tradition in deep-
linguistic formalisms. Using a discriminative model,
we thus formulate the problem of finding a syntactic
tree y?syn and a semantic graph y?sem for a sentence
x as maximizing a function Fjoint that scores the
complete syntactic?semantic structure:
?y?syn, y?sem? = arg max
ysyn,ysem
Fjoint(x, ysyn, ysem)
The dependencies in the feature representation used
to compute Fjoint determine the tractability of the
70
rerankingPred?argLinguisticconstraintsSensedisambig. Argumentidentification Argumentlabeling
dependencySyntacticparsing
Global semantic model
Syntactic?semantic
reranking
Semantic pipeline
Figure 2: The architecture of the syntactic?semantic analyzer.
search procedure needed to perform the maximiza-
tion. To be able to use complex syntactic features
such as paths when predicting semantic structures,
exact search is clearly intractable. This is true even
with simpler feature representations ? the problem
is a special case of multi-headed dependency analy-
sis, which is NP-hard even if the number of heads is
bounded (Chickering et al, 1994).
This means that we must resort to a simplifica-
tion such as an incremental method or a rerank-
ing approach. We chose the latter option and thus
created syntactic and semantic submodels. The
joint syntactic?semantic prediction is selected from
a small list of candidates generated by the respective
subsystems. Figure 2 shows the architecture.
2.1 Syntactic Submodel
We model the process of syntactic parsing of a
sentence x as finding the parse tree y?syn =
argmaxysyn Fsyn(x, ysyn) that maximizes a scoring
function Fsyn. The learning problem consists of fit-
ting this function so that the cost of the predictions is
as low as possible according to a cost function ?syn.
In this work, we consider linear scoring functions of
the following form:
Fsyn(x, ysyn) = ?syn(x, ysyn) ?w
where ?syn(x, ysyn) is a numeric feature represen-
tation of the pair (x, ysyn) andw a vector of feature
weights. We defined the syntactic cost ?syn as the
sum of link costs, where the link cost was 0 for a
correct dependency link with a correct label, 0.5 for
a correct link with an incorrect label, and 1 for an
incorrect link.
A widely used discriminative framework for fit-
ting the weight vector is the max-margin model
(Taskar et al, 2003), which is a generalization of
the well-known support vector machines to gen-
eral cost-based prediction problems. Since the large
number of training examples and features in our
case make an exact solution of the max-margin op-
timization problem impractical, we used the on-
line passive?aggressive algorithm (Crammer et al,
2006), which approximates the optimization process
in two ways:
? The weight vector w is updated incrementally,
one example at a time.
? For each example, only the most violated con-
straint is considered.
The algorithm is a margin-based variant of the per-
ceptron (preliminary experiments show that it out-
performs the ordinary perceptron on this task). Al-
gorithm 1 shows pseudocode for the algorithm.
Algorithm 1 The Online PA Algorithm
input Training set T = {(xt, yt)}Tt=1
Number of iterations N
Regularization parameter C
Initialize w to zeros
repeat N times
for (xt, yt) in T
let y?t = argmaxy F (xt, y) + ?(yt, y)
let ?t = min
(
C, F (xt,y?t)?F (xt,yt)+?(yt,y?t)??(x,yt)??(x,y?t)?2
)
w ? w + ?t(?(x, yt)??(x, y?t))
return waverage
We used a C value of 0.01, and the number of
iterations was 6.
2.1.1 Features and Search
The feature function?syn is a factored represen-
tation, meaning that we compute the score of the
complete parse tree by summing the scores of its
parts, referred to as factors:
?(x, y) ?w =
?
f?y
?(x, f) ?w
71
We used a second-order factorization (McDonald
and Pereira, 2006; Carreras, 2007), meaning that
the factors are subtrees consisting of four links: the
governor?dependent link, its sibling link, and the
leftmost and rightmost dependent links of the depen-
dent.
This factorization allows us to express useful fea-
tures, but also forces us to adopt the expensive
search procedure by Carreras (2007), which ex-
tends Eisner?s span-based dynamic programming al-
gorithm (1996) to allow second-order feature depen-
dencies. This algorithm has a time complexity of
O(n4), where n is the number of words in the sen-
tence. The search was constrained to disallow mul-
tiple root links.
To evaluate the argmax in Algorithm 1 during
training, we need to handle the cost function ?syn in
addition to the factor scores. Since the cost function
?syn is based on the cost of single links, this can
easily be integrated into the factor-based search.
2.1.2 Handling Nonprojective Links
Although only 0.4% of the links in the training
set are nonprojective, 7.6% of the sentences con-
tain at least one nonprojective link. Many of these
links represent long-range dependencies ? such as
wh-movement ? that are valuable for semantic pro-
cessing. Nonprojectivity cannot be handled by
span-based dynamic programming algorithms. For
parsers that consider features of single links only, the
Chu-Liu/Edmonds algorithm can be used instead.
However, this algorithm cannot be generalized to the
second-order setting ?McDonald and Pereira (2006)
proved that this problem is NP-hard, and described
an approximate greedy search algorithm.
To simplify implementation, we instead opted for
the pseudo-projective approach (Nivre and Nilsson,
2005), in which nonprojective links are lifted up-
wards in the tree to achieve projectivity, and spe-
cial trace labels are used to enable recovery of the
nonprojective links at parse time. The use of trace
labels in the pseudo-projective transformation leads
to a proliferation of edge label types: from 69 to 234
in the training set, many of which occur only once.
Since the running time of our parser depends on the
number of labels, we used only the 20 most frequent
trace labels.
2.2 Semantic Submodel
Our semantic model consists of three parts:
? A SRL classifier pipeline that generates a list of
candidate predicate?argument structures.
? A constraint system that filters the candidate
list to enforce linguistic restrictions on the
global configuration of arguments.
? A global reranker that assigns scores to
predicate?argument structures in the filtered
candidate list.
Rather than training the models on gold-standard
syntactic input, we created an automatically parsed
training set by 5-fold cross-validation. Training
on automatic syntax makes the semantic classifiers
more resilient to parsing errors, in particular adjunct
labeling errors.
2.2.1 SRL Pipeline
The SRL pipeline consists of classifiers for pred-
icate disambiguation, argument identification, and
argument labeling. For the predicate disambigua-
tion classifiers, we trained one subclassifier for each
lemma. All classifiers in the pipeline were L2-
regularized linear logistic regression classifiers, im-
plemented using the efficient LIBLINEAR package
(Lin et al, 2008). For multiclass problems, we used
the one-vs-all binarization method, which makes it
easy to prevent outputs not allowed by the PropBank
frame.
Since our classifiers were logistic, their output
values could be meaningfully interpreted as prob-
abilities. This allowed us to combine the scores
from subclassifiers into a score for the complete
predicate?argument structure. To generate the can-
didate lists used by the global SRL models, we ap-
plied beam search based on these scores using a
beam width of 4.
The argument identification classifier was pre-
ceded by a pruning step similar to the constituent-
based pruning by Xue and Palmer (2004).
The features used by the classifiers are listed in
Table 1, and are described in Appendix A. We se-
lected the feature sets by greedy forward subset se-
lection.
72
Feature PredDis ArgId ArgLab
PREDWORD ?
PREDLEMMA ?
PREDPARENTWORD/POS ?
CHILDDEPSET ? ? ?
CHILDWORDSET ?
CHILDWORDDEPSET ?
CHILDPOSSET ?
CHILDPOSDEPSET ?
DEPSUBCAT ?
PREDRELTOPARENT ?
PREDPARENTWORD/POS ?
PREDLEMMASENSE ? ?
VOICE ? ?
POSITION ? ?
ARGWORD/POS ? ?
LEFTWORD/POS ?
RIGHTWORD/POS ? ?
LEFTSIBLINGWORD/POS ?
PREDPOS ? ?
RELPATH ? ?
VERBCHAINHASSUBJ ? ?
CONTROLLERHASOBJ ?
PREDRELTOPARENT ? ?
FUNCTION ?
Table 1: Classifier features in predicate disambiguation
(PredDis), argument identification (ArgId), and argument
labeling (ArgLab).
2.2.2 Linguistically Motivated Global
Constraints
The following three global constraints were used
to filter the candidates generated by the pipeline.
CORE ARGUMENT CONSISTENCY. Core argu-
ment labels must not appear more than once.
DISCONTINUITY CONSISTENCY. If there is a la-
bel C-X, it must be preceded by a label X.
REFERENCE CONSISTENCY. If there is a label R-
X and the label is inside an attributive relative
clause, it must be preceded by a label X.
2.2.3 Predicate?Argument Reranker
Toutanova et al (2005) have showed that a global
model that scores the complete predicate?argument
structure can lead to substantial performance gains.
We therefore created a global SRL classifier using
the following global features in addition to the fea-
tures from the pipeline:
CORE ARGUMENT LABEL SEQUENCE. The com-
plete sequence of core argument labels. The
sequence also includes the predicate and voice,
for instance A0+break.01/Active+A1.
MISSING CORE ARGUMENT LABELS. The set of
core argument labels declared in the PropBank
frame that are not present in the predicate?
argument structure.
Similarly to the syntactic submodel, we trained
the global SRL model using the online passive?
aggressive algorithm. The cost function ? was
defined as the number of incorrect links in the
predicate?argument structure. The number of iter-
ations was 20 and the regularization parameter C
was 0.01. Interestingly, we noted that the global
SRL model outperformed the pipeline even when
no global features were added. This shows that the
global learning model can correct label bias prob-
lems introduced by the pipeline architecture.
2.3 Syntactic?Semantic Reranking
As described previously, we carried out reranking
on the candidate set of complete syntactic?semantic
structures. To do this, we used the top 16 trees from
the syntactic module and applied a linear model:
Fjoint(x, ysyn, ysem) = ?joint(x, ysyn, ysem) ?w
Our baseline joint feature representation?joint con-
tained only three features: the log probability of the
syntactic tree and the log probability of the seman-
tic structure according to the pipeline and the global
model, respectively. This model was trained on the
complete training set using cross-validation. The
probabilities were obtained using the multinomial
logistic function (?softmax?).
We carried out an initial experiment with a more
complex joint feature representation, but failed to
improve over the baseline. Time prevented us from
exploring this direction conclusively.
3 Comparisons with Previous Results
To compare our results with previously published
results in SRL, we carried out an experiment com-
paring our system to the top system (Punyakanok et
al., 2008) in the CoNLL-2005 Shared Task. How-
ever, comparison is nontrivial since the output of
the CoNLL-2005 systems was a set of labeled seg-
ments, while the CoNLL-2008 systems (including
ours) produced labeled semantic dependency links.
To have a fair comparison of our link-based sys-
tem against previous segment-based systems, we
73
carried out a two-way evaluation: In the first eval-
uation, the dependency-based output was converted
to segments and evaluated using the segment scorer
from CoNLL-2005, and in the second evaluation, we
applied a head-finding procedure to the output of a
segment-based system and scored the result using
the link-based CoNLL-2008 scorer.
It can be discussed which of the two metrics is
most correlated with application performance. The
traditional metric used in the CoNLL-2005 task
treats SRL as a bracketing problem, meaning that
the entities scored by the evaluation procedure are
labeled snippets of text; however, it is questionable
whether this is the proper way to evaluate a task
whose purpose is to find semantic relations between
logical entities. We believe that the same criticisms
that have been leveled at the PARSEVAL metric
for constituent structures are equally valid for the
bracket-based evaluation of SRL systems. The in-
appropriateness of the traditional metric has led to
a number of alternative metrics (Litkowski, 2004;
Baker et al, 2007; Surdeanu et al, 2008).
3.1 Segment-based Evaluation
To be able to score the output of a dependency-based
SRL system using the segment scorer, a conversion
step is needed. Algorithm 2 shows how a set of seg-
ments is constructed from an argument dependency
node. For each argument node, the algorithm com-
putes the yield Y of the argument node, i.e. the set of
dependency nodes to include in the bracketing. This
set is then partitioned into contiguous parts, from
which the segments are computed. In most cases,
the yield is just the subtree dominated by the argu-
ment node. However, if the argument dominates the
predicate, then the branch containing the predicate
is removed.
Table 2 shows the performance figures of our
system on the WSJ and Brown corpora: preci-
sion, recall, F1-measure, and complete proposition
accuracy (PP). These figures are compared to the
best-performing system in the CoNLL-2005 Shared
Task (Punyakanok et al, 2008), referred to as Pun-
yakanok in the table, and the best result currently
published (Surdeanu et al, 2007), referred to as Sur-
deanu. To validate the sanity of the segment cre-
ation algorithm, the table also shows the result of ap-
plying segment creation to gold-standard syntactic?
Algorithm 2 Segment creation from an argument
dependency node.
input Predicate node p, argument node a
if a does not dominate p
Y ? {n : a dominates n}
else
c? the child of a that dominates p
Y ? {n : a dominates n} \ {n : c dominates n}
end if
S ? partition of Y into contiguous subsets
return {(min-index s,max-index s) : s ? S}
WSJ P R F1 PP
Our system 82.22 77.72 79.90 57.24
Punyakanok 82.28 76.78 79.44 53.79
Surdeanu 87.47 74.67 80.56 51.66
Gold standard 97.38 96.77 97.08 93.20
Brown P R F1 PP
Our system 68.79 61.87 65.15 32.34
Punyakanok 73.38 62.93 67.75 32.34
Surdeanu 81.75 61.32 70.08 34.33
Gold standard 97.22 96.55 96.89 92.79
WSJ+Brown P R F1 PP
Our system 80.50 75.59 77.97 53.94
Punyakanok 81.18 74.92 77.92 50.95
Surdeanu 86.78 72.88 79.22 49.36
Gold standard 97.36 96.75 97.05 93.15
Table 2: Evaluation with unnormalized segments.
semantic trees. We see that the two conversion pro-
cedures involved (constituent-to-dependency con-
version by the CoNLL-2008 Shared Task organizers,
and our dependency-to-segment conversion) work
satisfactorily although the process is not completely
lossless.
During inspection of the output, we noted that
many errors arise from inconsistent punctuation at-
tachment in PropBank/Treebank. We therefore nor-
malized the segments to exclude punctuation at the
beginning or end of a segment. The results of this
evaluation is shown in Table 3. This table does not
include the Surdeanu system since we did not have
74
access to its output.
WSJ P R F1 PP
Our system 82.95 78.40 80.61 58.65
Punyakanok 82.67 77.14 79.81 54.55
Gold standard 97.85 97.24 97.54 94.34
Brown P R F1 PP
Our system 70.84 63.71 67.09 36.94
Punyakanok 74.29 63.71 68.60 34.08
Gold standard 97.46 96.78 97.12 93.41
WSJ+Brown P R F1 PP
Our system 81.39 76.44 78.84 55.77
Punyakanok 81.63 75.34 78.36 51.84
Gold standard 97.80 97.18 97.48 94.22
Table 3: Evaluation with normalized segments.
The results on the WSJ test set clearly show
that dependency-based SRL systems can rival
constituent-based systems in terms of performance
? it clearly outperforms the Punyakanok system, and
has a higher recall and complete proposition accu-
racy than the Surdeanu system. We interpret the high
recall as a result of the dependency syntactic repre-
sentation, which makes the parse tree paths simpler
and thus the arguments easier to find.
For the Brown test set, on the other hand, the
dependency-based system suffers from a low pre-
cision compared to the constituent-based systems.
Our error analysis indicates that the domain change
caused problems with prepositional attachment for
the dependency parser ? it is well-known that prepo-
sitional attachment is a highly lexicalized problem,
and thus sensitive to domain changes. We believe
that the reason why the constituent-based systems
are more robust in this respect is that they utilize a
combination strategy, using inputs from two differ-
ent full constituent parsers, a clause bracketer, and
a chunker. However, caution is needed when draw-
ing conclusions from results on the Brown test set,
which is only 7,585 words, compared to the 59,100
words in the WSJ test set.
3.2 Dependency-based Evaluation
It has previously been noted (Pradhan et al, 2005)
that a segment-based evaluation may be unfavorable
to a dependency-based system, and that an evalua-
tion that scores argument heads may be more indica-
tive of its true performance. We thus carried out an
evaluation using the evaluation script of the CoNLL-
2008 Shared Task. In this evaluation method, an ar-
gument is counted as correctly identified if its head
and label are correct. Note that this is not equivalent
to the segment-based metric: In a perfectly identi-
fied segment, we may still pick out the wrong head,
and if the head is correct, we may infer an incorrect
segment. The evaluation script also scores predicate
disambiguation performance; we did not include this
score since the 2005 systems did not output predi-
cate sense identifiers.
Since CoNLL-2005-style segments have no in-
ternal tree structure, it is nontrivial to extract a
head. It is conceivable that the output of the parsers
used by the Punyakanok system could be used to
extract heads, but this is not recommendable be-
cause the Punyakanok system is an ensemble sys-
tem and a segment does not always exactly match
a constituent in a parse tree. Furthermore, the
CoNLL-2008 constituent-to-dependency conversion
method uses a richer structure than just the raw con-
stituents: empty categories, grammatical functions,
and named entities. To recreate this additional infor-
mation, we would have to apply automatic systems
and end up with unreliable results.
Instead, we thus chose to find an upper bound
on the performance of the segment-based system.
We applied a simple head-finding procedure (Algo-
rithm 3) to find a set of head nodes for each seg-
ment. Since the CoNLL-2005 output does not in-
clude dependency information, the algorithm uses
gold-standard dependencies and intersects segments
with the gold-standard segments. This will give us
an upper bound, since if the segment contains the
correct head, it will always be counted as correct.
The algorithm looks for dependencies leaving the
segment, and if multiple outgoing edges are found,
a couple of simple heuristics are applied. We found
that the best performance is achieved when selecting
only one outgoing edge. ?Small clauses,? which are
split into an object and a predicative complement in
the dependency framework, are the only cases where
we select two heads.
Table 4 shows the results of the dependency-
based evaluation. In the table, the output of the
75
Algorithm 3 Finding head nodes in a segment.
input Argument segment a
if a overlaps with a segment in the gold standard
a? intersection of a and gold standard
F ? {n : governor of n outside a}
if |F | = 1
return F
remove punctuation nodes from F
if |F | = 1
return F
if F = {n1, n2, . . .} where n1 is an object and n2 is
the predicative part of a small clause
return {n1, n2}
if F contains a node n that is a subject or an object
return {n}
else
return {n}, where n is the leftmost node in F
dependency-based system is compared to the seman-
tic dependency links automatically extracted from
the segments of the Punyakanok system.
WSJ P R F1 PP
Our system 88.46 83.55 85.93 61.97
Punyakanok 87.25 81.59 84.32 58.17
Brown P R F1 PP
Our system 77.67 69.63 73.43 41.32
Punyakanok 80.29 68.59 73.98 37.28
WSJ+Brown P R F1 PP
Our system 87.07 81.68 84.29 59.22
Punyakanok 86.94 80.21 83.45 55.39
Table 4: Dependency-based evaluation.
In this evaluation, the dependency-based system
has a higher F1-measure than the Punyakanok sys-
tem on both test sets. This suggests that the main ad-
vantage of using a dependency-based semantic role
labeler is that it is better at finding the heads of
semantic arguments, rather than finding segments.
The results are also interesting in comparison to
the multi-view system described by Pradhan et al
(2005), which has a reported head F1 measure of
85.2 on the WSJ test set. The figure is not exactly
compatible with ours, however, since that system
used a different head extraction mechanism.
4 Conclusion
We have described a dependency-based system1 for
semantic role labeling of English in the PropBank
framework. Our evaluations show that the perfor-
mance of our system is close to the state of the
art. This holds regardless of whether a segment-
based or a dependency-based metric is used. In-
terestingly, our system has a complete proposition
accuracy that surpasses other systems by nearly 3
percentage points. Our system is the first semantic
role labeler based only on syntactic dependency that
achieves a competitive performance.
Evaluation and comparison is a difficult issue
since the natural output of a dependency-based sys-
tem is a set of semantic links rather than segments,
as is normally the case for traditional systems. To
handle this situation fairly to both types of systems,
we carried out a two-way evaluation: conversion of
dependencies to segments for the dependency-based
system, and head-finding heuristics for segment-
based systems. However, the latter is difficult since
no structure is available inside segments, and we
had to resort to computing upper-bound results using
gold-standard input; despite this, the dependency-
based system clearly outperformed the upper bound
of the performance of the segment-based system.
The comparison can also be slightly misleading
since the dependency-based system was optimized
for the dependency metric and previous systems for
the segment metric.
Our evaluations suggest that the dependency-
based SRL system is biased to finding argument
heads, rather than argument text snippets, and this
is of course perfectly logical. Whether this is an ad-
vantage or a drawback will depend on the applica-
tion ? for instance, a template-filling system might
need complete segments, while an SRL-based vector
space representation for text categorization, or a rea-
soning application, might prefer using heads only.
In the future, we would like to further investigate
whether syntactic and semantic analysis could be in-
tegrated more tightly. In this work, we used a sim-
1Our system is freely available for download at
http://nlp.cs.lth.se/lth_srl.
76
plistic loose coupling by means of reranking a small
set of complete structures. The same criticisms that
are often leveled at reranking-based models clearly
apply here too: The set of tentative analyses from the
submodules is too small, and the correct analysis is
often pruned too early. An example of a method to
mitigate this shortcoming is the forest reranking by
Huang (2008), in which complex features are evalu-
ated as early as possible.
A Classifier Features
Features Used in Predicate Disambiguation
PREDWORD, PREDLEMMA. The lexical form and
lemma of the predicate.
PREDPARENTWORD and PREDPARENTPOS.
Form and part-of-speech tag of the parent node
of the predicate.
CHILDDEPSET, CHILDWORDSET, CHILD-
WORDDEPSET, CHILDPOSSET, CHILD-
POSDEPSET. These features represent the set
of dependents of the predicate using combina-
tions of dependency labels, words, and parts of
speech.
DEPSUBCAT. Subcategorization frame: the con-
catenation of the dependency labels of the pred-
icate dependents.
PREDRELTOPARENT. Dependency relation be-
tween the predicate and its parent.
Features Used in Argument Identification and
Labeling
PREDLEMMASENSE. The lemma and sense num-
ber of the predicate, e.g. give.01.
VOICE. For verbs, this feature is Active or Passive.
For nouns, it is not defined.
POSITION. Position of the argument with respect
to the predicate: Before, After, or On.
ARGWORD and ARGPOS. Lexical form and part-
of-speech tag of the argument node.
LEFTWORD, LEFTPOS, RIGHTWORD, RIGHT-
POS. Form/part-of-speech tag of the left-
most/rightmost dependent of the argument.
LEFTSIBLINGWORD, LEFTSIBLINGPOS.
Form/part-of-speech tag of the left sibling of
the argument.
PREDPOS. Part-of-speech tag of the predicate.
RELPATH. A representation of the complex gram-
matical relation between the predicate and the
argument. It consists of the sequence of de-
pendency relation labels and link directions in
the path between predicate and argument, e.g.
IM?OPRD?OBJ?.
VERBCHAINHASSUBJ. Binary feature that is set
to true if the predicate verb chain has a subject.
The purpose of this feature is to resolve verb
coordination ambiguity as in Figure 3.
CONTROLLERHASOBJ. Binary feature that is true
if the link between the predicate verb chain and
its parent is OPRD, and the parent has an ob-
ject. This feature is meant to resolve control
ambiguity as in Figure 4.
FUNCTION. The grammatical function of the argu-
ment node. For direct dependents of the predi-
cate, this is identical to the RELPATH.
I
SBJ
eat drinkyouand
COORD SBJCONJ
ROOT
SBJ COORD
ROOT
drinkandeatI
CONJ
Figure 3: Coordination ambiguity: The subject I is in an
ambiguous position with respect to drink.
I to
IMSBJ
want sleephim
OBJOPRD
ROOT
IM
sleepI
SBJ
want
ROOT
to
OPRD
Figure 4: Subject/object control ambiguity: I is in an am-
biguous position with respect to sleep.
77
References
Collin Baker, Michael Ellsworth, and Katrin Erk. 2007.
SemEval task 19: Frame semantic structure extraction.
In Proceedings of SemEval-2007.
Xavier Carreras and Llu?s M?rquez. 2004. Introduction
to the CoNLL-2004 shared task: Semantic role label-
ing. In Proceedings of CoNLL-2004.
Xavier Carreras and Llu?s M?rquez. 2005. Introduction
to the CoNLL-2005 shared task: Semantic role label-
ing. In Proceedings of CoNLL-2005.
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proceedings of
CoNLL-2007.
David M. Chickering, Dan Geiger, and David Hecker-
man. 1994. Learning Bayesian networks: The com-
bination of knowledge and statistical data. Technical
Report MSR-TR-94-09, Microsoft Research.
Ronan Collobert and Jason Weston. 2007. Fast semantic
extraction using a novel neural network architecture.
In Proceedings of ACL-2007.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Schwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. JMLR, 2006(7):551?585.
Jason M. Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Proceed-
ings of ICCL.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Daniel Gildea and Martha Palmer. 2002. The necessity
of syntactic parsing for predicate argument recogni-
tion. In Proceedings of the ACL-2002.
Aria Haghighi, Andrew Y. Ng, and Christopher D. Man-
ning. 2005. Robust textual inference via graph match-
ing. In Proceedings of EMNLP-2005.
Andrew Hickl, Jeremy Bensley, John Williams, Kirk
Roberts, Bryan Rink, and Ying Shi. 2006. Recogniz-
ing textual entailment with LCC?s GROUNDHOG sys-
tems. In Proceedings of the Second PASCAL Recog-
nizing Textual Entailment Challenge.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL-2008.
Richard Johansson and Pierre Nugues. 2008a.
Dependency-based syntactic?semantic analysis with
PropBank and NomBank. In Proceedings of the
Shared Task Session of CoNLL-2008.
Richard Johansson and Pierre Nugues. 2008b. The effect
of syntactic representation on semantic role labeling.
In Proceedings of COLING-2008.
Chih-Jen Lin, Ruby C. Weng, and S. Sathiya Keerthi.
2008. Trust region Newton method for large-scale lo-
gistic regression. JMLR, 2008(9):627?650.
Ken Litkowski. 2004. Senseval-3 task: Automatic label-
ing of semantic roles. In Proceedings of Senseval-3.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proceedings of EACL-2006.
Alessandro Moschitti, Paul Mora?rescu, and Sanda
Harabagiu. 2003. Open domain information extrac-
tion via automatic semantic labeling. In Proceedings
of FLAIRS.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In Proceedings of ACL-2005.
Jacob Persson, Richard Johansson, and Pierre Nugues.
2008. Text categorization using predicate?argument
structures. Submitted.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James
Martin, and Daniel Jurafsky. 2005. Semantic role la-
beling using different syntactic views. In Proceedings
of ACL-2005.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics,
34(2):257?287.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using predicate-argument struc-
tures for information extraction. In Proceedings of
ACL-2003.
Mihai Surdeanu, Llu?s M?rquez, Xavier Carreras, and
Pere R. Comas. 2007. Combination strategies for se-
mantic role labeling. Journal of Artificial Intelligence
Research, 29:105?151.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu?s M?rquez, and Joakim Nivre. 2008. The
CoNLL?2008 shared task on joint parsing of syntac-
tic and semantic dependencies. In Proceedings of
CoNLL?2008.
Reid Swanson and Andrew S. Gordon. 2006. A compari-
son of alternative parse tree paths for labeling semantic
roles. In Proceedings of COLING/ACL-2006.
Ben Taskar, Carlos Guestrin, and Daphne Koller. 2003.
Max-margin Markov networks. In Proceedings of
NIPS-2003.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2005. Joint learning improves semantic role
labeling. In Proceedings of ACL-2005.
Nianwen Xue and Martha Palmer. 2004. Calibrating
features for semantic role labeling. In Proceedings of
EMNLP-2004.
78
A Machine Learning Approach to Extract Temporal Information from
Texts in Swedish and Generate Animated 3D Scenes
Anders Berglund Richard Johansson Pierre Nugues
Department of Computer Science, LTH
Lund University
SE-221 00 Lund, Sweden
d98ab@efd.lth.se, {richard, pierre}@cs.lth.se
Abstract
Carsim is a program that automatically
converts narratives into 3D scenes. Carsim
considers authentic texts describing road
accidents, generally collected from web
sites of Swedish newspapers or transcribed
from hand-written accounts by victims of
accidents. One of the program?s key fea-
tures is that it animates the generated scene
to visualize events.
To create a consistent animation, Carsim
extracts the participants mentioned in a
text and identifies what they do. In this
paper, we focus on the extraction of tem-
poral relations between actions. We first
describe how we detect time expressions
and events. We then present a machine
learning technique to order the sequence
of events identified in the narratives. We
finally report the results we obtained.
1 Extraction of Temporal Information
and Scene Visualization
Carsim is a program that generates 3D scenes from
narratives describing road accidents (Johansson et
al., 2005; Dupuy et al, 2001). It considers au-
thentic texts, generally collected from web sites
of Swedish newspapers or transcribed from hand-
written accounts by victims of accidents.
One of Carsim?s key features is that it animates
the generated scene to visualize events described
in the narrative. The text below, a newspaper arti-
cle with its translation into English, illustrates the
goals and challenges of it. We bracketed the enti-
ties, time expressions, and events and we anno-
tated them with identifiers, denoted respectively
oi, tj , and ek:
En {bussolycka}e1 i s?dra Afghanistan
kr?vdee2 {p? torsdagen}t1 {20
d?dsoffer}o1 . Ytterligare {39
personer}o2 skadadese3 i olyckane4.
Busseno3 {var p? v?g}e5 fr?n Kanda-
har mot huvudstaden Kabul n?r deno4
under en omk?rninge6 k?rdee7
av v?gbanano5 och voltadee8,
meddeladee9 general Salim Khan,
bitr?dande polischef i Kandahar.
TT-AFP & Dagens Nyheter, July 8,
2004
{20 persons}o1 diede2 in a {bus
accident}e1 in southern Afghanistan
{on Thursday}t1. In addition, {39
persons}o2 {were injured}e3 in the
accidente4.
The buso3 {was on its way}e5 from
Kandahar to the capital Kabul when
ito4 {drove off}e7 the roado5 while
overtakinge6 and {flipped over}e8,
saide9 General Salim Khan, assistant
head of police in Kandahar.
The text above, our translation.
To create a consistent animation, the program
needs to extract and understand who the partici-
pants are and what they do. In the case of the ac-
cident above, it has to:
1. Detect the involved physical entities o3, o4,
and o5.
2. Understand that the pronoun o4 refers to o3.
3. Detect the events e6, e7, and e8.
385
4. Link the participants to the events using se-
mantic roles or grammatical functions and in-
fer the unmentioned vehicle that is overtaken.
5. Understand that the order of the events is e6-
e7-e8.
6. Detect the time expression t1 to anchor tem-
porally the animation.
In this paper, we describe how we address tasks
3, 5, and 6 within the Carsim program, i.e., how
we detect, interpret, and order events and how we
process time expressions.
2 Previous Work
Research on the representation of time, events,
and temporal relations dates back the beginning
of logic. It resulted in an impressive number of
formulations and models. In a review of contem-
porary theories and an attempt to unify them, Ben-
nett and Galton (2004) classified the most influen-
tial formalisms along three lines. A first approach
is to consider events as transitions between states
as in STRIPS (Fikes and Nilsson, 1971). A sec-
ond one is to map events on temporal intervals
and to define relations between pairs of intervals.
Allen?s (1984) 13 temporal relations are a widely
accepted example of this. A third approach is to
reify events, to quantify them existentially, and
to connect them to other objects using predicates
based on action verbs and their modifiers (David-
son, 1967). The sentence John saw Mary in Lon-
don on Tuesday is then translated into the logical
form: ?[Saw(, j,m)?Place(, l)?T ime(, t)].
Description of relations between time, events,
and verb tenses has also attracted a considerable
interest, especially in English. Modern work on
temporal event analysis probably started with Re-
ichenbach (1947), who proposed the distinction
between the point of speech, point of reference,
and point of event in utterances. This separation
allows for a systematic description of tenses and
proved to be very powerful.
Many authors proposed general principles to
extract automatically temporal relations between
events. A basic observation is that the tempo-
ral order of events is related to their narrative or-
der. Dowty (1986) investigated it and formulated a
Temporal Discourse Interpretation Principle to in-
terpret the advance of narrative time in a sequence
of sentences. Lascarides and Asher (1993) de-
scribed a complex logical framework to deal with
events in simple past and pluperfect sentences.
Hitzeman et al (1995) proposed a constraint-
based approach taking into account tense, aspect,
temporal adverbials, and rhetorical structure to an-
alyze a discourse.
Recently, groups have used machine learn-
ing techniques to determine temporal relations.
They trained automatically classifiers on hand-
annotated corpora. Mani et al (2003) achieved
the best results so far by using decision trees to
order partially events of successive clauses in En-
glish texts. Boguraev and Ando (2005) is another
example of it for English and Li et al (2004) for
Chinese.
3 Annotating Texts with Temporal
Information
Several schemes have been proposed to anno-
tate temporal information in texts, see Setzer and
Gaizauskas (2002), inter alia. Many of them were
incompatible or incomplete and in an effort to rec-
oncile and unify the field, Ingria and Pustejovsky
(2002) introduced the XML-based Time markup
language (TimeML).
TimeML is a specification language whose
goal is to capture most aspects of temporal rela-
tions between events in discourses. It is based
on Allen?s (1984) relations and a variation of
Vendler?s (1967) classification of verbs. It de-
fines XML elements to annotate time expressions,
events, and ?signals?. The SIGNAL tag marks sec-
tions of text indicating a temporal relation. It
includes function words such as later and not.
TimeML also features elements to connect entities
using different types of links, most notably tem-
poral links, TLINKs, that describe the temporal re-
lation holding between events or between an event
and a time.
4 A System to Convert Narratives of
Road Accidents into 3D Scenes
4.1 Carsim
Carsim is a text-to-scene converter. From a nar-
rative, it creates a complete and unambiguous 3D
geometric description, which it renders visually.
Carsim considers authentic texts describing road
accidents, generally collected from web sites of
Swedish newspapers or transcribed from hand-
written accounts by victims of accidents. One of
the program?s key features is that it animates the
generated scene to visualize events.
386
The Carsim architecture is divided into two
parts that communicate using a frame representa-
tion of the text. Carsim?s first part is a linguistic
module that extracts information from the report
and fills the frame slots. The second part is a vir-
tual scene generator that takes the structured rep-
resentation as input, creates the visual entities, and
animates them.
4.2 Knowledge Representation in Carsim
The Carsim language processing module reduces
the text content to a frame representation ? a tem-
plate ? that outlines what happened and enables a
conversion to a symbolic scene. It contains:
? Objects. They correspond to the physical en-
tities mentioned in the text. They also include
abstract symbols that show in the scene. Each
object has a type, that is selected from a pre-
defined, finite set. An object?s semantics is
a separate geometric entity, where its shape
(and possibly its movement) is determined by
its type.
? Events. They correspond intuitively to an ac-
tivity that goes on during a period in time
and here to the possible object behaviors. We
represent events as entities with a type taken
from a predefined set, where an event?s se-
mantics will be a proposition paired with a
point or interval in time during which the
proposition is true.
? Relations and Quantities. They describe spe-
cific features of objects and events and how
they are related to each other. The most obvi-
ous examples of such information are spatial
information about objects and temporal in-
formation about events. Other meaningful re-
lations and quantities include physical prop-
erties such as velocity, color, and shape.
5 Time and Event Processing
We designed and implemented a generic com-
ponent to extract temporal information from the
texts. It sits inside the natural language part of
Carsim and proceeds in two steps. The first step
uses a pipeline of finite-state machines and phrase-
structure rules that identifies time expressions, sig-
nals, and events. This step also generates a feature
vector for each element it identifies. Using the
vectors, the second step determines the temporal
relations between the extracted events and orders
them in time. The result is a text annotated using
the TimeML scheme.
We use a set of decision trees and a machine
learning approach to find the relations between
events. As input to the second step, the decision
trees take sequences of events extracted by the
first step and decide the temporal relation, possi-
bly none, between pairs of them. To run the learn-
ing algorithm, we manually annotated a small set
of texts on which we trained the trees.
5.1 Processing Structure
We use phrase-structure rules and finite state ma-
chines to mark up events and time expressions. In
addition to the identification of expressions, we of-
ten need to interpret them, for instance to com-
pute the absolute time an expression refers to. We
therefore augmented the rules with procedural at-
tachments.
We wrote a parser to control the processing flow
where the rules, possibly recursive, apply regular
expressions, call procedures, and create TimeML
entities.
5.2 Detection of Time Expressions
We detect and interpret time expressions with a
two-level structure. The first level processes in-
dividual tokens using a dictionary and regular ex-
pressions. The second level uses the results from
the token level to compute the meaning of multi-
word expressions.
Token-Level Rules. In Swedish, time expres-
sions such as en tisdagseftermiddag ?a Tuesday
afternoon? use nominal compounds. To decode
them, we automatically generate a comprehensive
dictionary with mappings from strings onto com-
pound time expressions. We decode other types
of expressions such as 2005-01-14 using regular
expressions
Multiword-Level Rules. We developed a
grammar to interpret the meaning of multiword
time expressions. It includes instructions on how
to combine the values of individual tokens for ex-
pressions such as {vid lunchtid}t1 {en tisdagefter-
middag}t2 ?{at noon}t1 {a Tuesday afternoon}t2?.
The most common case consists in merging the to-
kens? attributes to form a more specific expression.
However, relative time expressions such as i tors-
dags ?last Tuesday? are more complex. Our gram-
mar handles the most frequent ones, mainly those
387
that need the publishing date for their interpreta-
tion.
5.3 Detection of Signals
We detect signals using a lexicon and na?ve string
matching. We annotate each signal with a sense
where the possible values are: negation, before, af-
ter, later, when, and continuing. TimeML only de-
fines one attribute for the SIGNAL tag, an identifier,
and encodes the sense as an attribute of the LINKs
that refer to it. We found it more appropriate to
store the sense directly in the SIGNAL element, and
so we extended it with a second attribute.
We use the sense information in decision trees
as a feature to determine the order of events. Our
strategy based on string matching results in a lim-
ited overdetection. However, it does not break the
rest of the process.
5.4 Detection of Events
We detect the TimeML events using a part-of-
speech tagger and phrase-structure rules. We con-
sider that all verbs and verb groups are events. We
also included some nouns or compounds, which
are directly relevant to Carsim?s application do-
main, such as bilolycka ?car accident? or krock
?collision?. We detect these nouns through a set
of six morphemes.
TimeML annotates events with three features:
aspect, tense, and ?class?, where the class corre-
sponds to the type of the event. The TimeML spec-
ifications define seven classes. We kept only the
two most frequent ones: states and occurrences.
We determine the features using procedures at-
tached to each grammatical construct we extract.
The grammatical features aspect and tense are
straightforward and a direct output of the phrase-
structure rules. To infer the TimeML class, we use
heuristics such as these ones: predicative clauses
(copulas) are generally states and verbs in preterit
are generally occurrences.
The domain, reports of car accidents, makes
this approach viable. The texts describe sequences
of real events. They are generally simple, to the
point, and void of speculations and hypothetical
scenarios. This makes the task of feature identifi-
cation simpler than it is in more general cases.
In addition to the TimeML features, we extract
the grammatical properties of events. Our hypoth-
esis is that specific sequences of grammatical con-
structs are related to the temporal order of the de-
scribed events. The grammatical properties con-
sist of the part of speech, noun (NOUN) or verb
(VB). Verbs can be finite (FIN) or infinitive (INF).
They can be reduced to a single word or part of a
group (GR). They can be a copula (COP), a modal
(MOD), or a lexical verb. We combine these prop-
erties into eight categories that we use in the fea-
ture vectors of the decision trees (see ...EventStruc-
ture in Sect. 6.2).
6 Event Ordering
TimeML defines three different types of links:
subordinate (SLINK), temporal (TLINK), and aspec-
tual (ALINK). Aspectual links connect two event in-
stances, one being aspectual and the other the ar-
gument. As its significance was minor in the visu-
alization of car accidents, we set aside this type of
link.
Subordinate links generally connect signals to
events, for instance to mark polarity by linking a
not to its main verb. We identify these links simul-
taneously with the event detection. We augmented
the phrase-structure rules to handle subordination
cases at the same time they annotate an event. We
restricted the cases to modality and polarity and
we set aside the other ones.
6.1 Generating Temporal Links
To order the events in time and create the tempo-
ral links, we use a set of decision trees. We apply
each tree to sequences of events where it decides
the order between two of the events in each se-
quence. If e1, ..., en are the events in the sequence
they appear in the text, the trees correspond to the
following functions:
fdt1(ei, ei+1) ? trel(ei, ei+1)
fdt2(ei, ei+1, ei+2) ? trel(ei, ei+1)
fdt3(ei, ei+1, ei+2) ? trel(ei+1, ei+2)
fdt4(ei, ei+1, ei+2) ? trel(ei, ei+2)
fdt5(ei, ei+1, ei+2, ei+3) ? trel(ei, ei+3)
The possible output values of the trees are: si-
multaneous, after, before, is_included, includes,
and none. These values correspond to the relations
described by Setzer and Gaizauskas (2001).
The first decision tree should capture more gen-
eral relations between two adjacent events with-
out the need of a context. Decision trees dt2 and
dt3 extend the context by one event to the left re-
spectively one event to the right. They should cap-
ture more specific phenomena. However, they are
not always applicable as we never apply a decision
388
tree when there is a time expression between any
of the events involved. In effect, time expressions
?reanchor? the narrative temporally, and we no-
ticed that the decision trees performed very poorly
across time expressions.
We complemented the decision trees with a
small set of domain-independent heuristic rules
that encode common-sense knowledge. We as-
sume that events in the present tense occur after
events in the past tense and that all mentions of
events such as olycka ?accident? refer to the same
event. In addition, the Carsim event interpreter
recognizes some semantically motivated identity
relations.
6.2 Feature Vectors
The decision trees use a set of features correspond-
ing to certain attributes of the considered events,
temporal signals between them, and some other
parameters such as the number of tokens separat-
ing the pair of events to be linked. We list below
the features of fdt1 together with their values. The
first event in the pair is denoted by a mainEvent pre-
fix and the second one by relatedEvent:
? mainEventTense: none, past, present, future,
NOT_DETERMINED.
? mainEventAspect: progressive, perfective, per-
fective_progressive, none, NOT_DETERMINED.
? mainEventStructure: NOUN, VB_GR_COP_INF,
VB_GR_COP_FIN, VB_GR_MOD_INF,
VB_GR_MOD_FIN, VB_GR, VB_INF, VB_FIN,
UNKNOWN.
? relatedEventTense: (as mainEventTense)
? relatedEventAspect: (as mainEventAspect)
? relatedEventStructure: (as mainEventStructure)
? temporalSignalInbetween: none, before, after,
later, when, continuing, several.
? tokenDistance: 1, 2 to 3, 4 to 6, 7 to 10, greater
than 10.
? sentenceDistance: 0, 1, 2, 3, 4, greater than 4.
? punctuationSignDistance: 0, 1, 2, 3, 4, 5, greater
than 5.
The four other decision trees consider more
events but use similar features. The values for the
...Distance features are of course greater.
6.3 Temporal Loops
The process described above results in an overgen-
eration of temporal links. As some of them may be
conflicting, a post-processing module reorganizes
them and discards the temporal loops.
The initial step of the loop resolution assigns
each link with a score. This score is created by the
decision trees and is derived from the C4.5 metrics
(Quinlan, 1993). It reflects the accuracy of the leaf
as well as the overall accuracy of the decision tree
in question. The score for links generated from
heuristics is rule dependent.
The loop resolution algorithm begins with an
empty set of orderings. It adds the partial order-
ings to the set if their inclusion doesn?t introduce
a temporal conflict. It first adds the links with the
highest scores, and thus, in each temporal loop, the
ordering with the lowest score is discarded.
7 Experimental Setup and Evaluation
As far as we know, there is no available time-
annotated corpus in Swedish, which makes the
evaluation more difficult. As development and
test sets, we collected approximately 300 reports
of road accidents from various Swedish newspa-
pers. Each report is annotated with its publishing
date. Analyzing the reports is complex because
of their variability in style and length. Their size
ranges from a couple of sentences to more than a
page. The amount of details is overwhelming in
some reports, while in others most of the informa-
tion is implicit. The complexity of the accidents
described ranges from simple accidents with only
one vehicle to multiple collisions with several par-
ticipating vehicles and complex movements.
We manually annotated a subset of our corpus
consisting of 25 texts, 476 events and 1,162 tem-
poral links. We built the trees automatically from
this set using the C4.5 program (Quinlan, 1993).
Our training set is relatively small and the num-
ber of features we use relatively large for the set
size. This can produce a training overfit. However,
C4.5, to some extent, makes provision for this and
prunes the decision trees.
We evaluated three aspects of the temporal in-
formation extraction modules: the detection and
interpretation of time expressions, the detection
and interpretation of events, and the quality of the
final ordering. We report here the detection of
events and the final ordering.
389
Feature Ncorrect Nerroneous Correct
Tense 179 1 99.4%
Aspect 161 19 89.4%
Class 150 30 83.3%
Table 1: Feature detection for 180 events.
7.1 Event Detection
We evaluated the performance of the event detec-
tion on a test corpus of 40 previously unseen texts.
It should be noted that we used a simplified defi-
nition of what an event is, and that the manual an-
notation and evaluation were both done using the
same definition (i.e. all verbs, verb groups, and a
small number of nouns are events). The system
detected 584 events correctly, overdetected 3, and
missed 26. This gives a recall of 95.7%, a preci-
sion of 99.4%, and an F -measure of 97.5%.
The feature detection is more interesting and
Table 1 shows an evaluation of it. We carried out
this evaluation on the first 20 texts of the test cor-
pus.
7.2 Evaluation of Final Ordering
We evaluated the final ordering with the method
proposed by Setzer and Gaizauskas (2001). Their
scheme is comprehensive and enables to compare
the performance of different systems.
Description of the Evaluation Method. Set-
zer and Gaizauskas carried out an inter-annotator
agreement test for temporal relation markup.
When evaluating the final ordering of a text, they
defined the set E of all the events in the text and
the set T of all the time expressions. They com-
puted the set (E ? T )? (E ? T ) and they defined
the sets S`, I`, and B` as the transitive closures
for the relations simultaneous, includes, and be-
fore, respectively.
If S`k and S`r represent the set S` for the an-
swer key (?Gold Standard?) and system response,
respectively, the measures of precision and recall
for the simultaneous relation are:
R = |S
`
k ? S`r |
|S`k |
P = |S
`
k ? S`r |
|S`r |
For an overall measure of recall and precision,
Setzer and Gaizauskas proposed the following for-
mulas:
R = |S
`
k ? S`r | + |B`k ?B`r | + |I`k ? I`r |
|S`k | + |B`k | + |I`k |
P = |S
`
k ? S`r | + |B`k ?B`r | + |I`k ? I`r |
|S`r | + |B`r | + |I`r |
They used the classical definition of the F -
measure: the harmonic means of precision and re-
call. Note that the precision and recall are com-
puted per text, not for all relations in the test set
simultaneously.
Results. We evaluated the output of the Car-
sim system on 10 previously unseen texts against
our Gold Standard. As a baseline, we used a sim-
ple algorithm that assumes that all events occur in
the order they are introduced in the narrative. For
comparison, we also did an inter-annotator evalu-
ation on the same texts, where we compared the
Gold Standard, annotated by one of us, with the
annotation produced by another member in our
group.
As our system doesn?t support comparisons of
time expressions, we evaluated the relations con-
tained in the set E ? E. We only counted the
reflexive simultaneous relation once per tuples
(ex, ey) and (ey, ex) and we didn?t count relations
(ex, ex).
Table 2 shows our results averaged over the
10 texts. As a reference, we also included Set-
zer and Gaizauskas? averaged results for inter-
annotator agreement on temporal relations in six
texts. Their results are not directly comparable
however as they did the evaluation over the set
(E ? T ) ? (E ? T ) for English texts of another
type.
Comments. The computation of ratios on the
transitive closure makes Setzer and Gaizauskas?
evaluation method extremely sensitive. Missing a
single link often results in a loss of scores of gener-
ated transitive links and thus has a massive impact
on the final evaluation figures.
As an example, one of our texts contains six
events whose order is e4 < e5 < e6 < e1 < e2 <
e3. The event module automatically detects the
chains e4 < e5 < e6 and e1 < e2 < e3 correctly,
but misses the link e6 < e1. This gives a recall of
6/15 = 0.40. When considering evaluations per-
formed using the method above, it is meaningful
to have this in mind.
8 Carsim Integration
The visualization module considers a subset of the
detected events that it interprets graphically. We
390
Evaluation Average nwords Average nevents Pmean Rmean Fmean
Gold vs. Baseline 98.5 14.3 49.42 29.23 35.91
Gold vs. Automatic " " 54.85 37.72 43.97
Gold vs. Other Annotator " " 85.55 58.02 68.01
Setzer and Gaizauskas 312.2 26.7 67.72 40.07 49.13
Table 2: Evaluation results for final ordering averaged per text (with P , R, and F in %).
call this subset the Carsim events. Once the event
processing has been done, Carsim extracts these
specific events from the full set using a small do-
main ontology and inserts them into the template.
We use the event relations resulting from temporal
information extraction module to order them. For
all pairs of events in the template, Carsim queries
the temporal graph to determine their relation.
Figure 1 shows a part of the template represent-
ing the accident described in Section 1. It lists
the participants, with the unmentioned vehicle in-
ferred to be a car. It also shows the events and
their temporal order. Then, the visualization mod-
ule synthesizes a 3D scene and animates it. Fig-
ure 2 shows four screenshots picturing the events.
Figure 1: Representation of the accident in the ex-
ample text.
9 Conclusion and Perspectives
We have developed a method for detecting time
expressions, events, and for ordering these events
temporally. We have integrated it in a text-to-
scene converter enabling the animation of generic
actions.
The module to detect time expression and inter-
pret events performs significantly better than the
baseline technique used in previous versions of
Carsim. In addition, it should to be easy to sep-
arate it from the Carsim framework and reuse it in
other domains.
The central task, the ordering of all events,
leaves lots of room for improvement. The accu-
racy of the decision trees should improve with a
larger training set. It would result in a better over-
all performance. Switching from decision trees to
other training methods such as Support Vector Ma-
chines or using semantically motivated features, as
suggested by Mani (2003), could also be sources
of improvements.
More fundamentally, the decision tree method
we have presented is not able to take into account
long-distance links. Investigation into new strate-
gies to extract such links directly without the com-
putation of a transitive closure would improve re-
call and, given the evaluation procedure, increase
the performance.
References
James F. Allen. 1984. Towards a general theory of
action and time. Artificial Intelligence, 23(2):123?
154.
Brandon Bennett and Antony P. Galton. 2004. A uni-
fying semantics for time and events. Artificial Intel-
ligence, 153(1-2):13?48.
Branimir Boguraev and Rie Kubota Ando. 2005.
TimeML-compliant text analysis for temporal rea-
soning. In IJCAI-05, Proceedings of the Nineteenth
International Joint Conference on Artificial Intelli-
gence, pages 997?1003, Edinburgh, Scotland.
Donald Davidson. 1967. The logical form of action
sentences. In N. Rescher, editor, The Logic of Deci-
sion and Action. University of Pittsburgh Press.
David R. Dowty. 1986. The effects of aspectual class
on the temporal structure of discourse: Semantics or
pragmatics? Linguistics and Philosophy, 9:37?61.
391
Figure 2: Animation of the scene and event visualization.
Sylvain Dupuy, Arjan Egges, Vincent Legendre, and
Pierre Nugues. 2001. Generating a 3D simulation
of a car accident from a written description in nat-
ural language: The Carsim system. In ACL 2001,
Workshop on Temporal and Spatial Information Pro-
cessing, pages 1?8, Toulouse, France.
Richard Fikes and Nils J. Nilsson. 1971. Strips: A
new approach to the application of theorem proving
to problem solving. Artificial Intelligence, 2:189?
208.
Janet Hitzeman, Marc Noels Moens, and Clare Grover.
1995. Algorithms for analyzing the temporal struc-
ture of discourse. In Proceedings of the Annual
Meeting of the European Chapter of the Associa-
tion of Computational Linguistics, pages 253?260,
Dublin, Ireland.
Bob Ingria and James Pustejovsky. 2002. Specification
for TimeML 1.0.
Richard Johansson, Anders Berglund, Magnus
Danielsson, and Pierre Nugues. 2005. Automatic
text-to-scene conversion in the traffic accident
domain. In IJCAI-05, Proceedings of the Nineteenth
International Joint Conference on Artificial Intelli-
gence, pages 1073?1078, Edinburgh, Scotland.
Alex Lascarides and Nicholas Asher. 1993. Tem-
poral interpretation, discourse relations, and com-
mon sense entailment. Linguistics & Philosophy,
16(5):437?493.
Wenjie Li, Kam-Fai Wong, Guihong Cao, and Chunfa
Yuan. 2004. Applying machine learning to Chinese
temporal relation resolution. In Proceedings of the
42nd Meeting of the Association for Computational
Linguistics (ACL?04), pages 582?588, Barcelona.
Inderjeet Mani, Barry Schiffman, and Jianping Zhang.
2003. Inferring temporal ordering of events in
news. In Human Language Technology Conference
(HLT?03), Edmonton, Canada.
Inderjeet Mani. 2003. Recent developments in tempo-
ral information extraction. In Nicolas Nicolov and
Ruslan Mitkov, editors, Proceedings of RANLP?03.
John Benjamins.
John Ross Quinlan. 1993. C4.5: Programs for Ma-
chine Learning. Morgan Kauffman.
Hans Reichenbach. 1947. Elements of Symbolic Logic.
Academic Press, New York.
Andrea Setzer and Robert Gaizauskas. 2001. A pi-
lot study on annotating temporal relations in text. In
ACL 2001, Workshop on Temporal and Spatial Infor-
mation Processing, pages 73?80, Toulouse, France.
Andrea Setzer and Robert Gaizauskas. 2002. On the
importance of annotating temporal event-event rela-
tions in text. In LREC 2002, Workshop on Annota-
tion Standards for Temporal Information in Natural
Language.
Zeno Vendler. 1967. Linguistics in Philosophy. Cor-
nell University Press, Ithaca, New York.
392
Automatic Annotation for All Semantic Layers in FrameNet
Richard Johansson and Pierre Nugues
Department of Computer Science, Lund University
Box 118
SE-221 00 Lund, Sweden
{richard, pierre}@cs.lth.se
Abstract
We describe a system for automatic an-
notation of English text in the FrameNet
standard. In addition to the conventional
annotation of frame elements and their se-
mantic roles, we annotate additional se-
mantic information such as support verbs
and prepositions, aspectual markers, cop-
ular verbs, null arguments, and slot fillers.
As far as we are aware, this is the first sys-
tem that finds this information automati-
cally.
1 Introduction
Shallow semantic parsing has been an active area
of research during the last few years. Seman-
tic parsers, which are typically based on the
FrameNet (Baker et al, 1998) or PropBank for-
malisms, have proven useful in a number of NLP
projects, such as information extraction and ques-
tion answering. The main reason for their popular-
ity is that they can produce a flat layer of semantic
structure with a fair degree of robustness.
Building English semantic parsers for the
FrameNet standard has been studied widely
(Gildea and Jurafsky, 2002; Litkowski, 2004).
These systems typically address the task of identi-
fying and classifying Frame Elements (FEs), that
is semantic arguments of predicates, for a given
target word (predicate).
Although the FE layer is arguably the most cen-
tral, the FrameNet annotation standard defines a
number of additional semantic layers, which con-
tain information about support expressions (verbs
and prepositions), copulas, null arguments, slot-
fillers, and aspectual particles. This information
can for example be used in a semantic parser to
refine the meaning of a predicate, to link predi-
cates in a sentence together, or possibly to improve
detection and classification of FEs. The task of
automatic reconstruction of the additional seman-
tic layers has not been addressed by any previous
system. In this work, we describe a system that au-
tomatically identifies the entities in those layers.
2 Introduction to FrameNet
FrameNet (Baker et al, 1998; Johnson et al,
2003) is a comprehensive lexical database that
lists descriptions of words in the frame-semantic
paradigm (Fillmore, 1976). The core concept is
the frame, which is conceptual structure that rep-
resents a type of situation, object, or event, cou-
pled with a semantic valence description that de-
scribes what kinds of semantic arguments (frame
elements) are allowed or required for that partic-
ular frame. The frames are arranged in an ontol-
ogy using relations such as inheritance (such as the
relation between COMMUNICATION and COM-
MUNICATION_NOISE) and causative-of (such as
KILLING and DEATH).
For each frame, FrameNet lists a set of lemmas
or lexical units (mostly nouns, verbs, and adjec-
tives, but also a few prepositions and adverbs).
When such a word occurs in a sentence, it is called
a target word that evokes the frame. FrameNet
comes with a large set of manually annotated ex-
ample sentences, which is typically used by sta-
tistical systems for training and testing. Figure 1
shows an example of such a sentence. Here,
the target word eat evokes the INGESTION frame.
Three FEs are present: INGESTOR, INGESTIBLES,
and PLACE.
135
Often [an informal group]INGESTOR will eat
[lunch]INGESTIBLES [near a machine or other
work station]PLACE, even though a canteen is
available.
Figure 1: A sentence from the FrameNet example
corpus, with FEs bracketed and the target word in
italics.
3 Semantic Entities in FrameNet
The semantic annotation in FrameNet consists of
a set of layers. One of the layers defines the tar-
get, and the other layers provide additional infor-
mation with respect to the target. The following
layers are used:
? The FE layer, which defines the spans and se-
mantic roles of the arguments of the predi-
cate.
? A part-of-speech-specific layer, which con-
tains aspectual information for verbs; and
copulas, support expressions, and slot filling
information for nouns and adjectives.
? The ?Other? layer, containing special cases
such as null arguments.
The semantic entities that we consider in this
article are defined in the second and third of these
layers.
3.1 Support Expressions
Some noun targets, typically denoting events, are
often constructed using support verbs. In this case,
the noun carries most of the semantics (that is, it
evokes the frame), while the verb allows the slots
of the frame to be filled. Thus, the dependents
of a support verb are annotated as FEs, just like
for a verb target. Support verbs are annotated us-
ing the SUPP label on the Noun or Adjective layer.
In the following sentence, there is a support verb
(underwent) for the noun target (operation).
[Frances Patterson]PATIENT underwent an op-
eration at RMH today and is expected to be hos-
pitalized for a week or more.
The support verbs do not change the core se-
mantics of the noun target (that is, they bear no re-
lation to the frame). However, they may determine
the relation between the FEs and the target (?point-
of-view supports?, such as ?undergo an operation?
or ?perform an operation?) or provide aspectual
information (such as ?start an operation?).
The following sentence shows an example
where a governing verb is not a support verb of the
noun target. An automatic system must be able to
distinguish support verbs from other verbs.
A senior nurse observed the operation.
Although a large majority of the support expres-
sions are verbs, there are additionally some cases
of support prepositions, such as the following ex-
ample:
Secret agents of this ilk are at work all the time.
3.2 Copulas
Copular verbs, typically be, may be seen as a spe-
cial kind of support verb. They are marked us-
ing the COP label on the Noun or Adjective layer.
There are several uses of copulas:
? Class membership: John is a sailor.
? Qualities: Your literary masterpiece was delicious.
? Location: This was inside a desk drawer.
? Identity: Smithers is the vice-president of the arm-
chair division.
In FrameNet annotation, these uses of the cop-
ular verb are not distinguished.
3.3 Null Arguments
There are constructions that require special argu-
ments to be syntactically valid, but where these ar-
guments have no relation to the semantics of the
sentence. In the example below, it is an example
of this phenomenon.
I hate it when you do that.
Other common cases include existential con-
stuctions (?there are?) and subject requirement of
zero-place predicates (?it rains?). These null argu-
ments are tagged as NULL on the Other layer.
3.4 Aspectual Particles
Verb particles that indicate aspectual information
are marked using the ASPECT label. These parti-
cles must be distinguished from particles that are
parts of multiword units, such as carry out.
They just moan on and on about Fergie this and
Fergie that and I ?ve simply had enough.
136
3.5 Slot Fillers: GOV and X
FrameNet annotation contains some information
about the relation of predicates in the same sen-
tence when one predicate is a slot filler (that is,
an argument) of the other. This is most common
for noun target words, typically referring to natu-
ral kinds or artifacts.
In the following example, the target word
fingertips evokes the OBSERVABLE_BODYPARTS
frame, involving two FEs: POSSESSOR (?his?)
and BODY_PART (?fingertips?). This noun phrase
is also a slot filler (that is, an argument) of another
predicate in the sentence: cling on. In FrameNet,
such predicates are annotated using the GOV la-
bel. The constituent that contains the slot filler in
question is called (for lack of a better name) X.
Shares will boom and John Major will
[cling on]GOV [by [his]POSSESSOR
[fingertips]BODY_PART ]X.
If GOV and X are present, all FEs must be
contained in the span of the X node, such as
BODY_PART and POSSESSOR above. This may
be of use for automatic FE identifiers.
4 Identifying Semantic Entities
To find the semantic entities in the text, we used
the method that has previously been used for
FE detection: classification of nodes in a parse
tree. We divide the identification process into two
stages:
? The first stage finds SUPP, COP, and GOV.
? The second stage finds NULL, ASP, and X.
The reason for this division is that we expect
that the knowledge of the presence of SUPP, COP,
and GOV, which are almost always verbs, is use-
ful when detecting the other entities. The second
stage makes use of the information found in the
first stage. Above all, it is necessary to have infor-
mation about GOV to be able to detect X.
To train the classifiers, we selected the 150 most
common frames and divided the annotated exam-
ple sentences for those frames into a training set
of 100,000 sentences and a test set of 8,000 sen-
tences.
The classifiers used the Support Vector learning
method using the LIBSVM package (Chang and
Lin, 2001). The features used by the classifiers are
listed in Table 1. Apart from the features used by
Features for first and second stage
Target lemma
Target POS
Voice
Available semantic role labels
Position (before or after target)
Head word and POS
Phrase type
Parse tree path from target to node
Features for second stage only
Has SUPP
Has COP
Has GOV
Parse tree path from SUPP to node
Parse tree path from COP to node
Parse tree path from GOV to node
Table 1: Features used by the classifiers.
Stage 2, most of them are well-known from pre-
vious literature on FE identification and labeling
(Gildea and Jurafsky, 2002; Litkowski, 2004). For
all path features, we used both the traditional con-
stituent parse tree path (as by Gildea and Jurafsky
(2002)) and a dependency tree path (as by Ahn et
al. (2004)). We produced the parse trees using the
parser of Collins (1999).
5 Evaluation
We applied the system to a test set consisting of
approximately 8,000 sentences.
Because of inconsistent annotation, we did not
evaluate the performance of detection of the EX-
IST tag used in existential constructions. Prelim-
inary experiments indicated that the performance
was very poor.
The results, with confidence intervals at the
95% level, are shown in Table 2. They demon-
strate that the classical approach for FE identifica-
tion, that is classification of nodes in the parse tree,
is as well a viable method for detection of other
kinds of semantic information. The detection of
X shows the poorest performance. This is to be
expected, since it is very dependent on a GOV to
have been detected in the first stage.
The results for detection of aspectual particles
is not very reliable (the confidence interval was
?0.17 for precision and ?0.19 for recall), since
test corpus contained just 25 of these particles.
137
P R F?=1
SUPP 0.85 ? 0.046 0.64 ? 0.054 0.73
COP 0.90 ? 0.027 0.87 ? 0.030 0.88
NULL 0.76 ? 0.082 0.80 ? 0.080 0.78
ASP 0.83 ? 0.17 0.6 ? 0.19 0.70
GOV 0.79 ? 0.029 0.64 ? 0.030 0.71
X 0.59 ? 0.035 0.49 ? 0.032 0.54
Table 2: Results with 95% confidence intervals on
the test set.
6 Conclusion and Future Work
We have described a system that reconstructs all
semantic layers in FrameNet: in addition to the
traditional task of building the FE layer, it marks
up support expressions, aspectual particles, cop-
ulas, null arguments, and slot filling information
(GOV/X). As far as we know, no previous system
has addressed these tasks.
In the future, we would like to study how the
information provided by the additional layers in-
fluence the performance of the traditional task for
a semantic parser. FE identification, especially
for noun and adjective target words, may be made
easier by knowledge of the additional layers. As
mentioned above, if a support verb is present, its
dependents are arguments of the predicate. The
same holds for copular verbs. GOV/X nodes also
restrict where FEs may occur. In addition, support
verbs (such as ?perform? or ?undergo? an opera-
tion) may be beneficial when determining the re-
lationship between the FE and the predicate, that
is when assigning semantic roles.
References
David Ahn, Sisay Fissaha, Valentin Jijkoun, and
Maarten de Rijke. 2004. The university of Amster-
dam at Senseval-3: Semantic roles and logic forms.
In Proceedings of SENSEVAL-3.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet Project. In Proceed-
ings of COLING-ACL?98, pages 86?90, Montr?al,
Canada.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM:
a library for support vector machines.
Michael J. Collins. 1999. Head-driven statistical mod-
els for natural language parsing. Ph.D. thesis, Uni-
versity of Pennsylvania, Philadelphia.
Charles J. Fillmore. 1976. Frame semantics and
the nature of language. Annals of the New York
Academy of Sciences: Conference on the Origin and
Development of Language, 280:20?32.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
Christopher Johnson, Miriam Petruck, Collin Baker,
Michael Ellsworth, Josef Ruppenhofer, and Charles
Fillmore. 2003. FrameNet: Theory and Practice.
Ken Litkowski. 2004. Senseval-3 task: Automatic
labeling of semantic roles. In Rada Mihalcea and
Phil Edmonds, editors, Senseval-3: Third Interna-
tionalWorkshop on the Evaluation of Systems for the
Semantic Analysis of Text, pages 9?12, Barcelona,
Spain, July. Association for Computational Linguis-
tics.
138
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 436?443,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A FrameNet-based Semantic Role Labeler for Swedish
Richard Johansson and Pierre Nugues
Department of Computer Science, LTH
Lund University, Sweden
{richard, pierre}@cs.lth.se
Abstract
We present a FrameNet-based semantic
role labeling system for Swedish text. As
training data for the system, we used an
annotated corpus that we produced by
transferring FrameNet annotation from the
English side to the Swedish side in a par-
allel corpus. In addition, we describe two
frame element bracketing algorithms that
are suitable when no robust constituent
parsers are available.
We evaluated the system on a part of the
FrameNet example corpus that we trans-
lated manually, and obtained an accuracy
score of 0.75 on the classification of pre-
segmented frame elements, and precision
and recall scores of 0.67 and 0.47 for the
complete task.
1 Introduction
Semantic role labeling (SRL), the process of auto-
matically identifying arguments of a predicate in
a sentence and assigning them semantic roles, has
received much attention during the recent years.
SRL systems have been used in a number of
projects in Information Extraction and Question
Answering, and are believed to be applicable in
other domains as well.
Building SRL systems for English has been
studied widely (Gildea and Jurafsky, 2002;
Litkowski, 2004), inter alia. However, all these
works rely on corpora that have been produced at
the cost of a large effort by human annotators. For
instance, the current FrameNet corpus (Baker et
al., 1998) consists of 130,000 manually annotated
sentences. For smaller languages such as Swedish,
such corpora are not available.
In this work, we describe a FrameNet-based se-
mantic role labeler for Swedish text. Since there
was no existing training corpus available ? no
FrameNet-annotated Swedish corpus of substan-
tial size exists ? we used an English-Swedish
parallel corpus whose English part was annotated
with semantic roles using the FrameNet annota-
tion scheme. We then applied a cross-language
transfer to derive an annotated Swedish part. To
evaluate the performance of the Swedish SRL
system, we applied it to a small portion of the
FrameNet example corpus that we translated man-
ually.
1.1 FrameNet: an Introduction
FrameNet (Baker et al, 1998) is a lexical database
that describes English words using Frame Seman-
tics (Fillmore, 1976). In this framework, predi-
cates (or in FrameNet terminology, target words)
and their arguments are linked by means of seman-
tic frames. A frame can intuitively be thought of
as a template that defines a set of slots, frame ele-
ments (FEs), that represent parts of the conceptual
structure and typically correspond to prototypical
participants or properties.
Figure 1 shows an example sentence annotated
with FrameNet information. In this example, the
target word statements belongs to (?evokes?) the
frame STATEMENT. Two constituents that fill slots
of the frame (SPEAKER and TOPIC) are annotated
as well.
As usual in these cases, [both parties]SPEAKER
agreed to make no further statements [on the
matter]TOPIC.
Figure 1: A sentence from the FrameNet example
corpus.
436
The initial versions of FrameNet were focused
on describing situations and events, i.e. typically
verbs and their nominalizations. Currently, how-
ever, FrameNet defines frames for a wider range of
semantic relations that can be thought of as predi-
cate/argument structures, including descriptions of
events, states, properties, and objects.
FrameNet consists of the following main parts:
? An ontology consisting of a set of frames,
frame elements for each frame, and rela-
tions (such as inheritance and causative-of)
between frames.
? A list of lexical units, that is word forms
paired with their corresponding frames. The
frame is used to distinguish between differ-
ent senses of the word, although the treatment
of polysemy in FrameNet is relatively coarse-
grained.
? A collection of example sentences that pro-
vide lexical evidence for the frames and the
corresponding lexical units. Although this
corpus is not intended to be representative, it
is typically used as a training corpus when
contructing automatic FrameNet labelers.
1.2 Related Work
Since training data is often a scarce resource for
most languages other than English, a wide range
of methods have been proposed to reduce the need
for manual annotation. Many of these have relied
on existing resources for English and a transfer
method based on word alignment in a parallel cor-
pus to automatically create an annotated corpus in
a new language. Although these data are typically
quite noisy, they have been used to train automatic
systems.
For the particular case of transfer of FrameNet
annotation, there have been a few projects that
have studied transfer methods and evaluated the
quality of the automatically produced corpus. Jo-
hansson and Nugues (2005) applied the word-
based methods of Yarowsky et al (2001) and ob-
tained promising results. Another recent effort
(Pad? and Lapata, 2005) demonstrates that deeper
linguistic information, such as parse trees in the
source and target language, is very beneficial for
the process of FrameNet annotation transfer.
A rather different method to construct bilingual
semantic role annotation is the approach taken by
BiFrameNet (Fung and Chen, 2004). In that work,
annotated structures in a new language (in that
case Chinese) are produced by mining for similar
structures rather than projecting them via parallel
corpora.
2 Automatic Annotation of a Swedish
Training Corpus
2.1 Training an English Semantic Role
Labeler
We selected the 150 most frequent frames in
FrameNet and applied the Collins parser (Collins,
1999) to the example sentences for these frames.
We built a conventional FrameNet parser for En-
glish using 100,000 of these sentences as a train-
ing set and 8,000 as a development set. The classi-
fiers were based on Support Vector Machines that
we trained using LIBSVM (Chang and Lin, 2001)
with the Gaussian kernel. When testing the sys-
tem, we did not assume that the frame was known
a priori. We used the available semantic roles for
all senses of the target word as features for the
classifier.
On a test set from FrameNet, we estimated that
the system had a precision of 0.71 and a recall of
0.65 using a strict scoring method. The result is
slightly lower than the best systems at Senseval-
3 (Litkowski, 2004), possibly because we used a
larger set of frames, and we did not assume that
the frame was known a priori.
2.2 Transferring the Annotation
We produced a Swedish-language corpus anno-
tated with FrameNet information by applying
the SRL system to the English side of Europarl
(Koehn, 2005), which is a parallel corpus that is
derived from the proceedings of the European Par-
liament. We projected the bracketing of the target
words and the frame elements onto the Swedish
side of the corpus by using the Giza++ word
aligner (Och and Ney, 2003). Each word on the
English side was mapped by the aligner onto a
(possibly empty) set of words on the Swedish side.
We used the maximal span method to infer the
bracketing on the Swedish side, which means that
the span of a projected entity was set to the range
from the leftmost projected token to the rightmost.
Figure 2 shows an example of this process.
To make the brackets conform to the FrameNet
annotation practices, we applied a small set of
heuristics. The FrameNet conventions specify that
linking words such as prepositions and subordinat-
437
SPEAKER express MESSAGE[We]             wanted to               [our perplexity as regards these points]             [by abstaining in committee]MEANS
MEANS SPEAKER[Genom att avst? fr?n att r?sta i utskottet]           har [vi]            velat                [denna v?r tveksamhet]uttrycka MESSAGE
Figure 2: Example of projection of FrameNet annotation.
ing conjunctions should be included in the brack-
eting. However, since constructions are not iso-
morphic in the sentence pair, a linking word on
the target side may be missed by the projection
method since it is not present on the source side.
For example, the sentence the doctor was answer-
ing an emergency phone call is translated into
Swedish as doktorn svarade p? ett larmsamtal,
which uses a construction with a preposition p?
?to/at/on? that has no counterpart in the English
sentence. The heuristics that we used are spe-
cific for Swedish, although they would probably
be very similar for any other language that uses
a similar set of prepositions and connectives, i.e.
most European languages.
We used the following heuristics:
? When there was only a linking word (preposi-
tion, subordinating conjunction, or infinitive
marker) between the FE and the target word,
it was merged with the FE.
? When a Swedish FE was preceded by a link-
ing word, and the English FE starts with such
a word, it was merged with the FE.
? We used a chunker and adjusted the FE
brackets to include only complete chunks.
? When a Swedish FE crossed the target word,
we used only the part of the FE that was on
the right side of the target.
In addition, some bad annotation was discarded
because we obviously could not use sentences
where no counterpart for the target word could be
found. Additionally, we used only the sentences
where the target word was mapped to a noun, verb,
or an adjective on the Swedish side.
Because of homonymy and polysemy problems,
applying a SRL system without knowing target
words and frames a priori necessarily introduces
noise into the automatically created training cor-
pus. There are two kinds of word sense ambigu-
ity that are problematic in this case: the ?internal?
ambiguity, or the fact that there may be more than
one frame for a given target word; and the ?exter-
nal? ambiguity, where frequently occurring word
senses are not listed in FrameNet. To sidestep the
problem of internal ambiguity, we used the avail-
able semantic roles for all senses of the target word
as features for the classifier (as described above).
Solving the problem of external ambiguity was
outside the scope of this work.
Some potential target words had to be ignored
since their sense ambiguity was too difficult to
overcome. This category includes auxiliaries such
as be and have, as well as verbs such as take and
make, which frequently appear as support verbs
for nominal predicates.
2.3 Motivation
Although the meaning of the two sentences in
a sentence pair in a parallel corpus should be
roughly the same, a fundamental question is
whether it is meaningful to project semantic
markup of text across languages. Equivalent
words in two different languages sometimes ex-
hibit subtle but significant semantic differences.
However, we believe that a transfer makes sense,
since the nature of FrameNet is rather coarse-
grained. Even though the words that evoke a frame
may not have exact counterparts, it is probable that
the frame itself has.
For the projection method to be meaningful, we
must make the following assumptions:
? The complete frame ontology in the English
FrameNet is meaningful in Swedish as well,
and each frame has the same set of semantic
roles and the same relations to other frames.
? When a target word evokes a certain frame in
English, it has a counterpart in Swedish that
evokes the same frame.
? Some of the FEs on the English side have
counterparts with the same semantic roles on
the Swedish side.
438
In addition, we made the (obviously simplistic)
assumption that the contiguous entities we project
are also contiguous on the target side.
These assumptions may all be put into ques-
tion. Above all, the second assumption will fail
in many cases because the translations are not lit-
eral, which means that the sentences in the pair
may express slightly different information. The
third assumption may be invalid if the information
expressed is realized by radically different con-
structions, which means that an argument may be-
long to another predicate or change its semantic
role on the Swedish side. Pad? and Lapata (2005)
avoid this problem by using heuristics based on a
target-language FrameNet to select sentences that
are close in meaning. Since we have no such re-
source to rely on, we are forced to accept that this
problem introduces a certain amount of noise into
the automatically annotated corpus.
3 Training a Swedish SRL System
Using the transferred FrameNet annotation, we
trained a SRL system for Swedish text. Like most
previous systems, it consists of two parts: a FE
bracketer and a classifier that assigns semantic
roles to FEs. Both parts are implemented as SVM
classifiers trained using LIBSVM. The semantic
role classifier is rather conventional and is not de-
scribed in this paper.
To construct the features used by the classifiers,
we used the following tools:
? An HMM-based POS tagger,
? A rule-based chunker,
? A rule-based time expression detector,
? Two clause identifiers, of which one is rule-
based and one is statistical,
? The MALTPARSER dependency parser
(Nivre et al, 2004), trained on a 100,000-
word Swedish treebank.
We constructed shallow parse trees using the
clause trees and the chunks. Dependency and shal-
low parse trees for a fragment of a sentence from
our test corpus are shown in Figures 3 and 4, re-
spectively. This sentence, which was translated
from an English sentence that read the doctor was
answering an emergency phone call, comes from
the English FrameNet example corpus.
doktorn svarade p? ett larmsamtal
SUB ADV
PR
DET
Figure 3: Example dependency parse tree.
[ doktorn ] svarade[ ] larmsamtal[[ ett ]NG_nomPPp?]VG_finNG_nom Clause[ ]
Figure 4: Example shallow parse tree.
3.1 Frame Element Bracketing Methods
We created two redundancy-based FE bracket-
ing algorithms based on binary classification of
chunks as starting or ending the FE. This is some-
what similar to the chunk-based system described
by Pradhan et al (2005a), which uses a segmenta-
tion strategy based on IOB2 bracketing. However,
our system still exploits the dependency parse tree
during classification.
We first tried the conventional approach to the
problem of FE bracketing: applying a parser to the
sentence, and classifying each node in the parse
tree as being an FE or not. We used a dependency
parser since there is no constituent-based parser
available for Swedish. This proved unsuccessful
because the spans of the dependency subtrees fre-
quently were incompatible with the spans defined
by the FrameNet annotations. This was especially
the case for non-verbal target words and when the
head of the argument was above the target word in
the dependency tree. To be usable, this approach
would require some sort of transformation, possi-
bly a conversion into a phrase-structure tree, to be
applied to the dependency trees to align the spans
with the FEs. Preliminary investigations were un-
successful, and we left this to future work.
We believe that the methods we developed are
more suitable in our case, since they base their
decisions on several parse trees (in our case, two
clause-chunk trees and one dependency tree). This
redundancy is valuable because the dependency
parsing model was trained on a treebank of just
100,000 words, which makes it less robust than
Collins? or Charniak?s parsers for English. In ad-
dition, the methods do not implicitly rely on the
common assumption that every FE has a counter-
part in a parse tree. Recent work in semantic role
labeling, see for example Pradhan et al (2005b),
has focused on combining the results of SRL sys-
tems based on different types of syntax. Still, all
439
systems exploiting recursive parse trees are based
on binary classification of nodes as being an argu-
ment or not.
The training sets used to train the final classi-
fiers consisted of one million training instances for
the start classifier, 500,000 for the end classifier,
and 272,000 for the role classifier. The features
used by the classifiers are described in Subsec-
tion 3.2, and the performance of the two FE brack-
eting algorithms compared in Subsection 4.2.
3.1.1 Greedy start-end
The first FE bracketing algorithm, the greedy
start-end method, proceeds through the sequence
of chunks in one pass from left to right. For each
chunk opening bracket, a binary classifier decides
if an FE starts there or not. Similarly, another bi-
nary classifier tests chunk end brackets for ends
of FEs. To ensure compliance to the FrameNet
annotation standard (bracket matching, and no FE
crossing the target word), the algorithm inserts ad-
ditional end brackets where appropriate. Pseu-
docode is given in Algorithm 1.
Algorithm 1 Greedy Bracketing
Input: A list L of chunks and a target word t
Binary classifiers starts and ends
Output: The sets S and E of start and end brackets
Split L into the sublists Lbefore , Ltarget , and Lafter , which correspond
to the parts of the list that is before, at, and after the target word, respectively.
Initialize chunk-open to FALSE
for Lsub in {Lbefore, Ltarget, Lafter} do
for c in Lsub do
if starts(c) then
if chunk-open then
Add an end bracket before c to E
end if
chunk-open? TRUE
Add a start bracket before c to S
end if
if chunk-open ? (ends(c) ? c is final in Lsub) then
chunk-open? FALSE
Add an end bracket after c to E
end if
end for
end for
Figure 5 shows an example of this algorithm,
applied to the example fragment. The small brack-
ets correspond to chunk boundaries, and the large
brackets to FE boundaries that the algorithm in-
serts. In the example, the algorithm inserts an end
bracket after the word doktorn ?the doctor?, since
no end bracket was found before the target word
svarade ?was answering?.
3.1.2 Globally optimized start-end
The second algorithm, the globally optimized
start-end method, maximizes a global probability
score over each sentence. For each chunk open-
ing and closing bracket, probability models assign
START
[ ] svarade [...  [doktorn]                    [p?] [ett larmsamtal]   ...]
Additional END inserted END
START
Figure 5: Illustration of the greedy start-end
method.
the probability of an FE starting (or ending, re-
spectively) at that chunk. The probabilities are
estimated using the built-in sigmoid fitting meth-
ods of LIBSVM. Making the somewhat unrealis-
tic assumption of independence of the brackets,
the global probability score to maximize is de-
fined as the product of all start and end proba-
bilities. We added a set of constraints to ensure
that the segmentation conforms to the FrameNet
annotation standard. The constrained optimiza-
tion problem is then solved using the JACOP fi-
nite domain constraint solver (Kuchcinski, 2003).
We believe that an n-best beam search method
would produce similar results. The pseudocode
for the method can be seen in Algorithm 2. The
definitions of the predicates no-nesting and
no-crossing, which should be obvious, are
omitted.
Algorithm 2 Globally Optimized Bracketing
Input: A list L of chunks and a target word t
Probability models P?starts and P?ends
Output: The sets Smax and Emax of start and end brackets
legal(S, E) ? |S| = |E|
? max(E) > max(S) ?min(S) < min(E)
? no-nesting(S, E) ? no-crossing(t, S, E)
score(S, E) ?
?
c?S P?starts(c) ?
?
c?L\S(1? P?starts(c))
?
?
c?E P?ends(c) ?
?
c?L\E(1 ? P?ends(c))
(Smax, Emax)? argmax{legal(S,E)}score(S, E)
Figure 6 shows an example of the globally op-
timized start-end method. In the example, the
global probability score is maximized by a brack-
eting that is illegal because the FE starting at dok-
torn is not closed before the target (0.8 ? 0.6 ? 0.6 ?
0.7 ? 0.8 ? 0.7 = 0.11). The solution of the con-
strained problem is a bracketing that contains an
end bracket before the target (0.8 ? 0.4 ? 0.6 ? 0.7 ?
0.8 ? 0.7 = 0.075)
3.2 Features Used by the Classifiers
Table 1 summarizes the feature sets used by
the greedy start-end (GSE), optimized start-end
(OSE), and semantic role classification (SRC).
440
[ ] svarade [...  [doktorn]                    [p?] [ett larmsamtal]   ...]
P^starts1? P
^
starts1? =0.4
P^startsP
^
starts P
^
starts
P^starts1?
Pends
^
Pends
^ Pends
^
Pends
^
Pends
^
Pends
^
1? 1? 1?
=0.4
=0.6
=0.3
=0.7
=0.7
=0.3
=0.8
=0.2
=0.6 =0.2
=0.8
Figure 6: Illustration of the globally optimized
start-end method.
GSE OSE SRC
Target lemma + + +
Target POS + + +
Voice + + +
Allowed role labels + + +
Position + + +
Head word (HW) + + +
Head POS + + +
Phrase/chunk type (PT) + + +
HW/POS/PT,?2 chunk window + + -
Dep-tree & shallow path ?target + + +
Starting paths ?target + + -
Ending paths ?target + + -
Path?start + - -
Table 1: Features used by the classifiers.
3.2.1 Conventional Features
Most of the features that we use have been used
by almost every system since the first well-known
description (Gildea and Jurafsky, 2002). The fol-
lowing of them are used by all classifiers:
? Target word (predicate) lemma and POS
? Voice (when the target word is a verb)
? Position (before or after the target)
? Head word and POS
? Phrase or chunk type
In addition, all classifiers use the set of allowed
semantic role labels as a set of boolean features.
This is needed to constrain the output to a la-
bel that is allowed by FrameNet for the current
frame. In addition, this feature has proven use-
ful for the FE bracketing classifiers to distinguish
between event-type and object-type frames. For
event-type frames, dependencies are often long-
distance, while for object-type frames, they are
typically restricted to chunks very near the target
word. The part of speech of the target word alone
is not enough to distinguish these two classes,
since many nouns belong to event-type frames.
For the phrase/chunk type feature, we use
slightly different values for the bracketing case
and the role assignment case: for bracketing, the
value of this feature is simply the type of the cur-
rent chunk; for classification, it is the type of the
largest chunk or clause that starts at the leftmost
token of the FE. For prepositional phrases, the
preposition is attached to the phrase type (for ex-
ample, the second FE in the example fragment
starts with the preposition p? ?at/on?, which causes
the value of the phrase type feature to be PP-p?).
3.2.2 Chunk Context Features
Similarly to the chunk-based PropBank ar-
gument bracketer described by Pradhan et al
(2005a), the start-end methods use the head word,
head POS, and chunk type of chunks in a window
of size 2 on both sides of the current chunk to clas-
sify it as being the start or end of an FE.
3.2.3 Parse Tree Path Features
Parse tree path features have been shown to be
very important for argument bracketing in several
studies. All classifiers used here use a set of such
features:
? Dependency tree path from the head to the
target word. In the example text, the first
chunk (consisting of the word doktorn), has
the value SUB-? for this feature. This means
that to go from the head of the chunk to the
target in the dependency graph (Figure 3),
you traverse a SUB (subject) link upwards.
Similarly, the last chunk (ett larmsamtal) has
the value PR-?-ADV-?.
? Shallow path from the chunk containing the
head to the target word. For the same chunks
as above, these values are both NG_nom-?-
Clause-?-VG_fin, which means that to tra-
verse the shallow parse tree (Figure 4) from
the chunk to the target, you start with a
NG_nom node, go upwards to a Clause
node, and finally down to the VG_fin node.
The start-end classifiers additionally use the full
set of paths (dependency and shallow paths) to the
target word from each node starting (or ending, re-
spectively) at the current chunk, and the greedy
end classifier also uses the path from the current
chunk to the start chunk.
441
4 Evaluation of the System
4.1 Evaluation Corpus
To evaluate the system, we manually translated
150 sentences from the FrameNet example corpus.
These sentences were selected randomly from the
English development set. Some sentences were re-
moved, typically because we found the annotation
dubious or the meaning of the sentence difficult to
comprehend precisely. The translation was mostly
straightforward. Because of the extensive use of
compounding in Swedish, some frame elements
were merged with target words.
4.2 Comparison of FE Bracketing Methods
We compared the performance of the two methods
for FE bracketing on the test set. Because of lim-
ited time, we used smaller training sets than for the
full evaluation below (100,000 training instances
for all classifiers). Table 2 shows the result of this
comparison.
Greedy Optimized
Precision 0.70 0.76
Recall 0.50 0.44
F?=1 0.58 0.55
Table 2: Comparison of FE bracketing methods.
As we can see from the Table 2, the globally op-
timized start-end method increased the precision
somewhat, but decreased the recall and made the
overall F-measure lower. We therefore used the
greedy start-end method for our final evaluation
that is described in the next section.
4.3 Final System Performance
We applied the Swedish semantic role labeler to
the translated sentences and evaluated the result.
We used the conventional experimental setting
where the frame and the target word were given
in advance. The results, with approximate 95%
confidence intervals included, are presented in Ta-
ble 3. The figures are precision and recall for the
full task, classification accuracy of pre-segmented
arguments, precision and recall for the bracket-
ing task, full task precision and recall using the
Senseval-3 scoring metrics, and finally the propor-
tion of full sentences whose FEs were correctly
bracketed and classified. The Senseval-3 method
uses a more lenient scoring scheme that counts a
FE as correctly identified if it overlaps with the
gold standard FE and has the correct label. Al-
though the strict measures are more interesting,
we include these figures for comparison with the
systems participating in the Senseval-3 Restricted
task (Litkowski, 2004).
We include baseline scores for the argument
bracketing and classification tasks, respectively.
The bracketing baseline method considers non-
punctuation subtrees dependent of the target word.
When the target word is a verb, the baseline puts
FE brackets around the words included in each of
these subtrees1. When the target is a noun, we also
bracket the target word token itself, and when it is
an adjective, we additionally bracket its parent to-
ken. As a baseline for the argument classification
task, every argument is assigned the most frequent
semantic role in the frame. As can be seen from
the table, all scores except the argument bracket-
ing recall are well above the baselines.
Precision (Strict scoring method) 0.67 ? 0.064
Recall 0.47 ? 0.057
Argument Classification Accuracy 0.75 ? 0.050
Baseline 0.41 ? 0.056
Argument Bracketing Precision 0.80 ? 0.055
Baseline Precision 0.50 ? 0.055
Argument Bracketing Recall 0.57 ? 0.057
Baseline Recall 0.55 ? 0.057
Precision (Senseval-3 scoring method) 0.77 ? 0.057
Overlap 0.75 ? 0.039
Recall 0.55 ? 0.057
Complete Sentence Accuracy 0.29 ? 0.073
Table 3: Results on the Swedish test set with ap-
proximate 95% confidence intervals.
Although the performance figures are better
than the baselines, they are still lower than for
most English systems (although higher than some
of the systems at Senseval-3). We believe that
the main reason for the performance is the qual-
ity of the data that were used to train the system,
since the results are consistent with the hypoth-
esis that the quality of the transferred data was
roughly equal to the performance of the English
system multiplied by the figures for the transfer
method (Johansson and Nugues, 2005). In that
experiment, the transfer method had a precision
of 0.84, a recall of 0.81, and an F-measure of
0.82. If we assume that the transfer performance
is similar for Swedish, we arrive at a precision of
0.71 ? 0.84 = 0.60, a recall of 0.65 ? 0.81 = 0.53,
1This is possible because MALTPARSER produces projec-
tive trees, i.e. the words in each subtree form a contiguous
substring of the sentence.
442
and an F-measure of 0.56. For the F-measure,
0.55 for the system and 0.56 for the product, the
figures match closely. For the precision, the sys-
tem performance (0.67) is significantly higher than
the product (0.60), which suggests that the SVM
learning method handles the noisy training set
rather well for this task. The recall (0.47) is lower
than the corresponding product (0.53), but the dif-
ference is not statistically significant at the 95%
level. These figures suggest that the main effort
towards improving the system should be spent on
improving the training data.
5 Conclusion
We have described the design and implementa-
tion of a Swedish FrameNet-based SRL system
that was trained using a corpus that was anno-
tated using cross-language transfer from English
to Swedish. With no manual effort except for
translating sentences for evaluation, we were able
to reach promising results. To our knowledge, the
system is the first SRL system for Swedish in liter-
ature. We believe that the methods described could
be applied to any language, as long as there ex-
ists a parallel corpus where one of the languages
is English. However, the relatively close relation-
ship between English and Swedish probably made
the task comparatively easy in our case.
As we can see, the figures (especially the FE
bracketing recall) leave room for improvement for
the system to be useful in a fully automatic set-
ting. Apart from the noisy training set, proba-
ble reasons for this include the lower robustness
of the Swedish parsers compared to those avail-
able for English. In addition, we have noticed
that the European Parliament corpus is somewhat
biased. For instance, a very large proportion of
the target words evoke the STATEMENT or DIS-
CUSSION frames, but there are very few instances
of the BEING_WET and MAKING_FACES frames.
While training, we tried to balance the selection
somewhat, but applying the projection methods
on other types of parallel corpora (such as novels
available in both languages) may produce a better
training corpus.
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet Project. In Proceed-
ings of COLING-ACL?98, pages 86?90, Montr?al,
Canada.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM:
a library for support vector machines.
Michael J. Collins. 1999. Head-driven statistical mod-
els for natural language parsing. Ph.D. thesis, Uni-
versity of Pennsylvania, Philadelphia.
Charles J. Fillmore. 1976. Frame semantics and
the nature of language. Annals of the New York
Academy of Sciences: Conference on the Origin and
Development of Language, 280:20?32.
Pascale Fung and Benfeng Chen. 2004. BiFrameNet:
Bilingual frame semantics resource construction
by cross-lingual induction. In Proceedings of
COLING-2004.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
Richard Johansson and Pierre Nugues. 2005. Using
parallel corpora for automatic transfer of FrameNet
annotation. In Proceedings of the 1st ROMANCE
FrameNet Workshop, Cluj-Napoca, Romania, 26-28
July.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. InMT Summit 2005.
Krzysztof Kuchcinski. 2003. Constraints-driven
scheduling and resource assignment. ACM Transac-
tions on Design Automation of Electronic Systems,
8(3):355?383.
Ken Litkowski. 2004. Senseval-3 task: Automatic la-
beling of semantic roles. In Senseval-3: Third Inter-
national Workshop on the Evaluation of Systems for
the Semantic Analysis of Text, pages 9?12.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based dependency parsing. In Proceedings
of CoNLL-2004, pages 49?56.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Sebastian Pad? and Mirella Lapata. 2005. Cross-
lingual projection of role-semantic information. In
Proceedings of HLT/EMNLP 2005.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James Martin, and Dan Jurafsky.
2005a. Support vector learning for semantic argu-
ment classification. Machine Learning, 60(1):11?
39.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James
Martin, and Daniel Jurafsky. 2005b. Semantic role
labeling using different syntactic views. In Proceed-
ings of ACL-2005.
David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analysis
tools via robust projection across aligned corpora.
In Proceedings of HLT 2001.
443
Proceedings of the ACL 2007 Student Research Workshop, pages 49?54,
Prague, June 2007. c?2007 Association for Computational Linguistics
Logistic Online Learning Methods and Their Application to
Incremental Dependency Parsing
Richard Johansson
Department of Computer Science
Lund University
Lund, Sweden
richard@cs.lth.se
Abstract
We investigate a family of update methods
for online machine learning algorithms for
cost-sensitive multiclass and structured clas-
sification problems. The update rules are
based on multinomial logistic models. The
most interesting question for such an ap-
proach is how to integrate the cost function
into the learning paradigm. We propose a
number of solutions to this problem.
To demonstrate the applicability of the al-
gorithms, we evaluated them on a number
of classification tasks related to incremental
dependency parsing. These tasks were con-
ventional multiclass classification, hiearchi-
cal classification, and a structured classifica-
tion task: complete labeled dependency tree
prediction. The performance figures of the
logistic algorithms range from slightly lower
to slightly higher than margin-based online
algorithms.
1 Introduction
Natural language consists of complex structures,
such as sequences of phonemes, parse trees, and dis-
course or temporal graphs. Researchers in NLP have
started to realize that this complexity should be re-
flected in their statistical models. This intuition has
spurred a growing interest of related research in the
machine learning community, which in turn has led
to improved results in a wide range of applications
in NLP, including sequence labeling (Lafferty et al,
2001; Taskar et al, 2006), constituent and depen-
dency parsing (Collins and Duffy, 2002; McDon-
ald et al, 2005), and logical form extraction (Zettle-
moyer and Collins, 2005).
Machine learning research for structured prob-
lems have generally used margin-based formula-
tions. These include global batch methods such as
Max-margin Markov Networks (M3N) (Taskar et al,
2006) and SVMstruct (Tsochantaridis et al, 2005)
as well as online methods such as Margin Infused
Relaxed Algorithm (MIRA) (Crammer and Singer,
2003) and the Online Passive-Aggressive Algorithm
(OPA) (Crammer et al, 2006). Although the batch
methods are formulated very elegantly, they do not
seem to scale well to the large training sets prevalent
in NLP contexts. The online methods on the other
hand, although less theoretically appealing, can han-
dle realistically sized data sets.
In this work, we investigate whether logistic
online learning performs as well as margin-based
methods. Logistic models are easily extended to us-
ing kernels; that this is theoretically well-justified
was shown by Zhu and Hastie (2005), who also
made an elegant argument that margin-based meth-
ods are in fact related to regularized logistic models.
For batch learning, there exist several learning algo-
rithms in a logistic framework for conventional mul-
ticlass classification but few for structured problems.
Prediction of complex structures is conventionally
treated as a cost-sensitive multiclass classification
problem, although special care has to be taken to
handle the large space of possible outputs. The in-
tegration of the cost function into the logistic frame-
work leads to two distinct (although related) update
methods: the Scaled Prior Variance (SPV) and the
Minimum Expected Cost (MEC) updates.
Apart from its use in structured prediction, cost-
sensitive classification is useful for hierachical clas-
sification, which we briefly consider here in an ex-
periment. This type of classification has useful ap-
49
plications in NLP. Apart from the obvious use in
classification of concepts in an ontology, it is also
useful for prediction of complex morphological or
named-entity tags. Cost-sensitive learning is also
required in the SEARN algorithm (Daum? III et al,
2006), which is a method to decompose the predic-
tion problem of a complex structure into a sequence
of actions, and train the search in the space of action
sequences to maximize global performance.
2 Algorithm
We model the learning problem as finding a discrim-
inant function F that assigns a score to each possible
output y given an input x. Classification in this set-
ting is done by finding the y? that maximizes F (x, y).
In this work, we consider linear discriminants of the
following form:
F (x, y) = ?w,?(x, y)?
Here, ?(x, y) is a numeric feature representation of
the pair (x, y) and w a vector of feature weights.
Learning in this case is equivalent to assigning ap-
propriate weights in the vector w.
In the online learning framework, the weight vec-
tor is constructed incrementally. Algorithm 1 shows
the general form of the algorithm. It proceeds a
number of times through the training set. In each
step, it computes an update to the weight vector
based on the current example. The resulting weight
vector tends to be overfit to the last few examples;
one way to reduce overfitting is to use the average
of all successive weight vectors as the result of the
training (Freund and Schapire, 1999).
Algorithm 1 General form of online algorithms
input Training set T = {(xt, yt)}Tt=1
Number of iterations N
for n in 1..N
for (xt, yt) in T
Compute update vector ?w for (xt, yt)
w ? w + ?w
return waverage
Following earlier online learning methods such as
the Perceptron, we assume that in each update step,
we adjust the weight vector by incrementally adding
feature vectors. For stability, we impose the con-
straint that the sum of the updates in each step should
be zero. We assume that the possible output values
are {yi}mi=0 and, for convenience, that y0 is the cor-
rect value. This leads to the following ansatz:
?w =
m
?
j=1
?j(?(x, y0)??(x, yj))
Here, ?j defines how much F is shifted to favor y0
instead of yj . This is also the approach (implicitly)
used by other algorithms such as MIRA and OPA.
The following two subsections present two ways
of creating the weight update ?w, differing in how
the cost function is integrated into the model. Both
are based on a multinomial logistic framework,
where we model the probability of the class y being
assigned to an input x using a ?soft-max? function
as follows:
P (y|x) = e
F (x,y)
?m
j=0 eF (x,yj)
2.1 Scaled Prior Variance Approach
The first update method, Scaled Prior Variance
(SPV), directly uses the probability of the correct
output. It uses a maximum a posteriori approach,
where the cost function is used by the prior.
Na?vely, the update could be done by maximizing
the likelihood with respect to ? in each step. How-
ever, this would lead to overfitting ? in the case of
separability, a maximum does not even exist. We
thus introduce a regularizing prior that penalizes
large values of?. We introduce variance-controlling
hyperparameters sj for each ?j , and with a Gaussian
prior we obtain (disregarding constants) the follow-
ing log posterior:
L(?) =
m
?
j=1
?j(K00 ?Kj0)?
m
?
j=1
sj?2j
? log
m
?
k=0
efk+
Pm
j=1 ?j(K0k?Kjk)
where Kij = ??(x, yi),?(x, yj)? and fk =
F (x, yk) (i.e. the output before w is updated).
As usual, the feature vectors occur only in inner
products, allowing us to use kernels if appropriate.
50
We could have used any prior; however, in prac-
tice we will require it to be log-concave to avoid
suboptimal local maxima. A Laplacian prior (i.e.
??mj=1 sj|?j |) will also be considered in this work
? the discontinuity of its gradient at the origin seems
to pose no problem in practice.
Costs are incorporated into the model by as-
sociating them to the prior variances. We tried
two variants of variance scaling. In the first case,
we let the variance be directly proportional to the
cost (C-SPV):
sj =
?
c(yj)
where ? is a tradeoff parameter controlling the rel-
ative weight of the prior with respect to the likeli-
hood. Intuitively, this model allows the algorithm
more freedom to adjust an ?j associated with a yj
with a high cost.
In the second case, inspired by margin-based
learning we instead scaled the variance by the loss,
i.e. the scoring error plus the cost (L-SPV):
sj =
?
max(0, fj ? f0) + c(yj)
Here, the intuition is instead that the algorithm is
allowed more freedom for ?dangerous? outputs that
are ranked high but have high costs.
2.2 Minimum Expected Cost Approach
In the second approach to integrating the cost func-
tion, the Minimum Expected Cost (MEC) update,
the method seeks to minimize the expected cost in
each step. Once again using the soft-max probabil-
ity, we get the following expectation of the cost:
E(c(y)|x) =
m
?
k=0
c(yk)P (yk|x)
=
?m
k=0 c(yk)efk+
Pm
j=1 ?j(K0k?Kjk)
?m
k=0 e
fk+
Pm
j=1 ?j(K0k?Kjk)
This quantity is easily minimized in the same way
as the SPV posterior was maximized, although
we had to add a constant 1 to the expectation to
avoid numerical instability. To avoid overfitting, we
added a quadratic regularizer ?
?m
j=1 ?2j to log(1 +
E(c(y)|x)) just like the prior in the SPV method,
although this regularizer does not have an interpre-
tation as a prior.
The MEC update is closely related to SPV: for
cost-insensitive classification (i.e. the cost of every
misclassified instance is 1), the expectation is equal
to one minus the likelihood in the SPV model.
2.3 Handling Complex Prediction Problems
The algorithm can thus be used for any cost-
sensitive classification problem. This class of prob-
lems includes prediction of complex structures such
as trees or graphs. However, for those problems the
set of possible outputs is typically very large. Two
broad categories of solutions to this problem have
been common in literature, both of which rely on
the structure of the domain:
? Subset selection: instead of working with the
complete range of outputs, only an ?interest-
ing? subset is used, for instance by repeatedly
finding the most violated constraints (Tsochan-
taridis et al, 2005) or by using N -best search
(McDonald et al, 2005).
? Decomposition: the inherent structure of the
problem is used to factorize the optimiza-
tion problem. Examples include Markov de-
compositions in M3N (Taskar et al, 2006)
and dependency-based factorization for MIRA
(McDonald et al, 2005).
In principle, both methods could be used in our
framework. In this work, we use subset selec-
tion since it is easy to implement for many do-
mains (in the form of an N -best search) and al-
lows a looser coupling between the domain and the
learning algorithm.
2.4 Implementation Issues
Since we typically work with only a few variables in
each iteration, maximizing the log posterior or mini-
mizing the expectation is easy (assuming, of course,
that we chose a log-concave prior). We used gra-
dient ascent and did not try to use more sophisti-
cated optimization procedures like BFGS or New-
ton?s method. Typically, only a few iterations were
needed to reach the optimum. The running time of
the update step is almost identical to that of MIRA,
which solves a small quadratic program in each step,
but longer than for the Perceptron algorithm or OPA.
51
Actions Parser actions Conditions
Initialize (nil,W, ?)
Terminate (S, nil, A)
Left-arc (n|S, n?|I,A)? (S, n?|I,A ? {(n?, n)}) ??n??(n??, n) ? A
Right-arc (n|S, n?|I,A)? (n?|n|S, I,A ? {(n, n?)}) ??n??(n??, n?) ? A
Reduce (n|S, I,A)? (S, I,A) ?n?(n?, n) ? A
Shift (S, n|I,A)? (n|S, I,A)
Table 1: Nivre?s parser transitions where W is the initial word list; I , the current input word list; A, the
graph of dependencies; and S, the stack. (n?, n) denotes a dependency relations between n? and n, where n?
is the head and n the dependent.
3 Experiments
To compare the logistic online algorithms against
other learning algorithms, we performed a set of ex-
periments in incremental dependency parsing using
the Nivre algorithm (Nivre, 2003).
The algorithm is a variant of the shift?reduce al-
gorithm and creates a projective and acyclic graph.
As with the regular shift?reduce, it uses a stack S
and a list of input words W , and builds the parse
tree incrementally using a set of parsing actions (see
Table 1). However, instead of finding constituents,
it builds a set of arcs representing the graph of de-
pendencies. It can be shown that every projective
dependency graph can be produced by a sequence
of parser actions, and that the worst-case number of
actions is linear with respect to the number of words
in the sentence.
3.1 Multiclass Classification
In the first experiment, we trained multiclass clas-
sifiers to choose an action in a given parser state
(see (Nivre, 2003) for a description of the feature
set). We stress that this is true multiclass classifica-
tion rather than a decomposed method (such as one-
versus-all or pairwise binarization).
As a training set, we randomly selected 50,000
instances of state?action pairs generated for a
dependency-converted version of Penn Treebank.
This training set contained 22 types of actions (such
as SHIFT, REDUCE, LEFT-ARC(SUBJECT), and
RIGHT-ARC(OBJECT). The test set was also ran-
domly selected and contained 10,000 instances.
We trained classifiers using the logistic updates
(C-SPV, L-SPV, and MEC) with Gaussian and
Laplacian priors. Additionally, we trained OPA
and MIRA classifiers, as well as an Additive Ultra-
conservative (AU) classifier (Crammer and Singer,
2003), a variant of the Perceptron.
For all algorithms, we tried to find the best val-
ues of the respective regularization parameter using
cross-validation. All training algorithms iterated five
times through the training set and used an expanded
quadratic kernel.
Table 2 shows the classification error for all algo-
rithms. As can be seen, the performance was lower
for the logistic algorithms, although the difference
was slight. Both the logistic (MEC and SPV) and
the margin-based classifiers (OPA and MIRA) out-
performed the AU classifier.
Method Test error
MIRA 6.05%
OPA 6.17%
C-SPV, Laplace 6.20%
MEC, Laplace 6.21%
C-SPV, Gauss 6.22%
MEC, Gauss 6.23%
L-SPV, Laplace 6.25%
L-SPV, Gauss 6.26%
AU 6.39%
Table 2: Multiclass classification results.
3.2 Hierarchical Classification
In the second experiment, we used the same train-
ing and test set, but considered the selection of the
parsing action as a hierarchical classficiation task,
i.e. the predicted value has a main type (SHIFT,
REDUCE, LEFT-ARC, and RIGHT-ARC) and possi-
bly also a subtype (such as LEFT-ARC(SUBJECT) or
52
RIGHT-ARC(OBJECT)).
To predict the class in this experiment, we used
the same feature function but a new cost function:
the cost of misclassification was 1 for an incorrect
parsing action, and 0.5 if the action was correct but
the arc label incorrect.
We used the same experimental setup as in the
multiclass experiment. Table 3 shows the average
cost on the test set for all algorithms. Here, the
MEC update outperformed the margin-based ones
by a negligible difference. We did not use AU in
this experiment since it does not optimize for cost.
Method Average cost
MEC, Gauss 0.0573
MEC, Laplace 0.0576
OPA 0.0577
C-SPV, Gauss 0.0582
C-SPV, Laplace 0.0587
MIRA 0.0590
L-SPV, Gauss 0.0590
L-SPV, Laplace 0.0632
Table 3: Hierarchical classification results.
3.3 Prediction of Complex Structures
Finally, we made an experiment in prediction of de-
pendency trees. We created a global model where
the discriminant function was trained to assign high
scores to the correct parse tree. A similar model was
previously used by McDonald et al (2005), with the
difference that we here represent the parse tree as
a sequence of actions in the incremental algorithm
rather than using the dependency links directly.
For a sentence x and a parse tree y, we defined
the feature representation by finding the sequence
((S1, I1) , a1) , ((S2, I2) , a2) . . . of states and their
corresponding actions, and creating a feature vector
for each state/action pair. The discriminant function
was thus written
??(x, y),w? =
?
i
??((Si, Ii) , ai),w?
where ? is the feature function from the previous
two experiments, which assigns a feature vector to a
state (Si, Ii) and the action ai taken in that state.
The cost function was defined as the sum of link
costs, where the link cost was 0 for a correct depen-
dency link with a correct label, 0.5 for a correct link
with an incorrect label, and 1 for an incorrect link.
Since the history-based feature set used in the
parsing algorithm makes it impossible to use inde-
pendence to factorize the scoring function, an exact
search to find the best-scoring action sequence is not
possible. We used a beam search of width 2 in this
experiment.
We trained models on a 5000-word subset of the
Basque Treebank (Aduriz et al, 2003) and evalu-
ated them on a 8000-word subset of the same cor-
pus. As before, we used an expanded quadratic ker-
nel, and all algorithms iterated five times through the
training set.
Table 4 shows the results of this experiment. We
show labeled accuracy instead of cost for ease of in-
terpretation. Here, the loss-based SPV outperformed
Method Labeled Accuracy
L-SPV, Gauss 66.24
MIRA 66.19
MEC, Gauss 65.99
C-SPV, Gauss 65.84
OPA 65.45
MEC, Laplace 64.81
C-SPV, Laplace 64.73
L-SPV, Laplace 64.50
Table 4: Results for dependency tree prediction.
MIRA, and two other logistic updates also outper-
formed OPA. The differences between the first four
scores are however not statistically significant. In-
terestingly, all updates with Laplacian prior resulted
in low performance. The reason for this may be that
Laplacian priors tend to promote sparse solutions
(see Krishnapuram et al (2005), inter alia), and that
this sparsity is detrimental for this highly lexicalized
feature set.
4 Conclusion and Future Work
This paper presented new update methods for online
machine learning algorithms. The update methods
are based on a multinomial logistic model. Their
performance is on par with other state-of-the-art on-
line learning algorithms for cost-sensitive problems.
53
We investigated two main approaches to integrat-
ing the cost function into the logistic model. In the
first method, the cost was linked to the prior vari-
ances, while in the second method, the update rule
sets the weights to minimize the expected cost. We
tried a few different priors. Which update method
and which prior was the best varied between exper-
iments. For instance, the update where the prior
variances were scaled by the costs was the best-
performing in the multiclass experiment but the
worst-performing in the dependency tree prediction
experiment.
In the SPV update, the cost was incorporated into
the MAP model in a rather ad-hoc fashion. Al-
though this seems to work well, we would like to
investigate this further and possibly devise a cost-
based prior that is both theoretically well-grounded
and performs well in practice.
To achieve a good classification performance us-
ing the updates presented in this article, there is a
considerable need for cross-validation to find the
best value for the regularization parameter. This is
true for most other classification methods as well,
including SVM, MIRA, and OPA. There has been
some work on machine learning methods where this
parameter is tuned automatically (Tipping, 2001),
and a possible extension to our work could be to
adapt those models to the multinomial and cost-
sensitive setting.
We applied the learning models to three problems
in incremental dependency parsing, the last of which
being prediction of full labeled dependency trees.
Our system can be seen as a unification of the two
best-performing parsers presented at the CoNLL-X
Shared Task (Buchholz and Marsi, 2006).
References
Itzair Aduriz, Maria Jesus Aranzabe, Jose Mari Arriola,
Aitziber Atutxa, Arantza Diaz de Ilarraza, Aitzpea
Garmendia, and Maite Oronoz. 2003. Construction
of a Basque dependency treebank. In Proceedings of
the TLT, pages 201?204.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of the CoNLL-X.
Michael Collins and Nigel Duffy. 2002. New ranking
algorithms for parsing and tagging: Kernels over dis-
crete structures, and the voted perceptron. In Proceed-
ings of the ACL.
Koby Crammer and Yoram Singer. 2003. Ultraconserva-
tive online algorithms for multiclass problems. Jour-
nal of Machine Learning Research, 2003(3):951?991.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Schwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 2006(7):551?585.
Hal Daum? III, John Langford, and Daniel Marcu. 2006.
Search-based structured prediction. Submitted.
Yoav Freund and Robert E. Schapire. 1999. Large mar-
gin classification using the perceptron algorithm. Ma-
chine Learning, 37(3):277?296.
Balaji Krishnapuram, Lawrence Carin, M?rio A. T.
Figueiredo, and Alexander J. Hartemink. 2005.
Sparse multinomial logistic regression: Fast algo-
rithms and generalization bounds. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 27(6).
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of the 18th International Conference on Ma-
chine Learning.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of HLT-EMNLP-2005.
Joakim Nivre. 2003. An efficient algorithm for projec-
tive dependency parsing. In Proceedings of the 8th In-
ternational Workshop on Parsing Technologies (IWPT
03), pages 149?160, Nancy, France, 23-25 April.
Ben Taskar, Carlos Guestrin, Vassil Chatalbashev, and
Daphne Koller. 2006. Max-margin Markov networks.
Journal of Machine Learning Research, to appear.
Michael E. Tipping. 2001. Sparse Bayesian learning
and the relevance vector machine. Journal of Machine
Learning Research, 1:211 ? 244.
Iannis Tsochantaridis, Thorsten Joachims, Thomas Hof-
mann, and Yasemin Altun. 2005. Large margin meth-
ods for structured and interdependent output variables.
Journal of Machine Learning Research, 6(Sep):1453?
1484.
Luke S. Zettlemoyer and Michael Collins. 2005. Learn-
ing to map sentences to logical form: Structured clas-
sification with probabilistic categorial grammars. In
Proceedings of UAI 2005.
Ji Zhu and Trevor Hastie. 2005. Kernel logistic regres-
sion and the import vector machine. Journal of Com-
putational and Graphical Statistics, 14(1):185?205.
54
Carsim: A System to Visualize Written Road Accident Reports as Animated
3D Scenes
Richard Johansson David Williams Anders Berglund Pierre Nugues
LUCAS, Department of Computer Science, Lund University
Box 118
SE-221 00 Lund, Sweden
{richard, pierre}@cs.lth.se, {d98dw, d98ab}@efd.lth.se
Abstract
This paper describes a system to create animated
3D scenes of car accidents from reports written in
Swedish. The system has been developed using
news reports of varying size and complexity. The
text-to-scene conversion process consists of two
stages. An information extraction module creates
a structured representation of the accident and a vi-
sual simulator generates and animates the scene.
We first describe the overall structure of the text-
to-scene conversion and the structure of the repre-
sentation. We then explain the information extrac-
tion and visualization modules. We show snapshots
of the car animation output and we conclude with
the results we obtained.
1 Text-to-Scene Conversion
As noted by Michel Denis, language and images are
two different representation modes whose cooper-
ation is needed in many forms of cognitive opera-
tions. The description of physical events, mathe-
matical theorems, or structures of any kind using
language is sometimes difficult to understand. Im-
ages and graphics can then help understand ideas or
situations and realize their complexity. They have
an indisputable capacity to represent and to commu-
nicate knowledge and are an effective means to rep-
resent and explain things, see (Kosslyn, 1983; Tufte,
1997; Denis, 1991).
Narratives of a car accidents, for instance, often
make use of space descriptions, movements, and
directions that are sometimes difficult to grasp for
most readers. We believe that forming consistent
mental images are necessary to understand them
properly. However, some people have difficulties in
imagining situations and may need visual aids pre-
designed by professional analysts.
In this paper, we will describe Carsim, a text-to-
scene converter that automates the generation of im-
ages from texts.
2 Related Work
The conversion of natural language texts into graph-
ics has been investigated in a few projects. NALIG
(Adorni et al, 1984; Manzo et al, 1986) is an early
example of them that was aimed at recreating static
2D scenes. One of its major goals was to study rela-
tionships between space and prepositions. NALIG
considered simple phrases in Italian of the type sub-
ject, preposition, object that in spite of their simplic-
ity can have ambiguous interpretations. From what
is described in the papers, NALIG has not been ex-
tended to process sentences and even less to texts.
WordsEye (Coyne and Sproat, 2001) is an im-
pressive system that recreates 3D animated scenes
from short descriptions. The number of 3D objects
WordsEye uses ? 12,000 ? gives an idea of its am-
bition. WordsEye integrates resources such as the
Collins? dependency parser and the WordNet lexical
database. The narratives cited as examples resemble
imaginary fairy tales and WordsEye does not seem
to address real world stories.
CogViSys is a last example that started with the
idea of generating texts from a sequence of video
images. The authors found that it could also be
useful to reverse the process and generate synthetic
video sequences from texts. The logic engine be-
hind the text-to-scene converter (Arens et al, 2002)
is based on the Discourse Representation Theory.
The system is limited to the visualization of single
vehicle maneuvers at an intersection as the one de-
scribed in this two-sentence narrative: A car came
from Kriegstrasse. It turned left at the intersection.
The authors give no further details on the text cor-
pus and no precise description of the results.
3 Carsim
Carsim (Egges et al, 2001; Dupuy et al, 2001) is
a program that analyzes texts describing car acci-
dents and visualizes them in a 3D environment. It
has been developed using real-world texts.
The Carsim architecture is divided into two parts
that communicate using a formal representation of
Input Text
Linguistic
Component
Formal
Description
Visualizer
Component
Output
Animation
Figure 1: The Carsim architecture.
the accident. Carsim?s first part is a linguistic mod-
ule that extracts information from the report and fills
the frame slots. The second part is a virtual scene
generator that takes the structured representation as
input, creates the visual entities, and animates them
(Figure 1).
4 A Corpus of Traffic Accident
Descriptions
As development and test sets, we have collected ap-
proximately 200 reports of road accidents from vari-
ous Swedish newspapers. The task of analyzing the
news reports is made more complex by their vari-
ability in style and length. The size of the texts
ranges from a couple of sentences to more than a
page. The amount of details is overwhelming in
some reports, while in others most of the informa-
tion is implicit. The complexity of the accidents de-
scribed ranges from simple accidents with only one
vehicle to multiple collisions with several partici-
pating vehicles and complex movements.
Although our work has concentrated on the press
clippings, we also have access to accident reports
from the STRADA database (Swedish TRaffic Ac-
cident Data Acquisition) of Va?gverket, the Swedish
traffic authority. STRADA registers nearly all the
accidents that occur in Sweden (Karlberg, 2003).
(All the accidents where there are casualties.) Af-
ter an accident, the victims describe the location
and conditions of it in a standardized form col-
lected in hospitals. The corresponding reports are
transcribed in a computer-readable format in the
STRADA database. This source contains two kinds
of reports: the narratives written by the victims of
the accident and their transcriptions by traffic ex-
perts. The original texts contain spelling mistakes,
abbreviations, and grammatical errors. The tran-
scriptions often simplify, interpret the original texts,
and contain jargon.
The next text is an excerpt from our development
corpus. This report is an example of a press wire
describing an accident.
En do?dsolycka intra?ffade inatt so?der
om Vissefja?rda pa? riksva?g 28. Det var en
bil med tva? personer i som kom av va?gen i
en va?nsterkurva och ko?rde i ho?g hastighet
in i en gran. Passageraren, som var fo?dd
-84, dog. Fo?raren som var 21 a?r gam-
mal va?rdas pa? sjukhus med sva?ra skador.
Polisen missta?nker att bilen de fa?rdades i,
en ny Saab, var stulen i Emmaboda och
det ska under dagen underso?kas.
Sveriges Radio, November 9, 2002
A fatal accident took place tonight south
of Vissefja?rda on Road 28. A car carry-
ing two persons departed from the road
in a left-hand curve and crashed at a high
speed into a spruce. The passenger, who
was born in 1984, died. The driver, who
was 21 years old, is severely injured and
is taken care of in a hospital. The police
suspects that the car they were traveling
in, a new Saab, was stolen in Emmaboda
and will investigate it today.
The text above, our translation.
5 Knowledge Representation
The Carsim language processing module reduces
the text content to a formal representation that out-
lines what happened and enables a conversion to a
symbolic scene. It uses information extraction tech-
niques to map a text onto a structure that consists of
three main elements:
? A scene object, which describes the static pa-
rameters of the environment, such as weather,
light, and road configuration.
? A list of road objects, for example cars, trucks,
and trees, and their associated sequences of
movements.
? A list of collisions between road objects.
The structure of the formalism, which sets the
limit of what information can be expressed, was de-
signed with the help of traffic safety experts at the
Department of Traffic and Road at Lund University.
It contains the information necessary to reproduce
and animate the accident entities in our visualiza-
tion model. We used an iterative process to design
it. We started from a first incomplete model (Dupuy
et al, 2001) and we manually constructed the rep-
resentation of about 50 texts until we had reached a
sufficient degree of expressivity.
The representation we use is a typical example of
frames a` la Minsky, where the objects in the rep-
resentation consist of a number of attribute/values
slots which are to be filled by the information ex-
traction module. Each object in the representation
Figure 2: Representation of the accident in the ex-
ample above.
belongs to a concept in a domain ontology we have
developed. The concepts are ordered in an inheri-
tance hierarchy.
Figure 2 shows how Carsim?s graphical user in-
terface presents the representation of the accident
in the example above. The scene element contains
the location of the accident and the configuration of
roads, in this case a left-hand bend. The list of road
objects contains one car and one tree. The event
chain for the car describes the movements: the car
leaves the road. Finally, the collision list describes
one collision between the car and the tree.
6 The Information Extraction Module
The information extraction subsystem fills the frame
slots. Its processing flow consists in analyzing the
text linguistically using the word groups obtained
from the linguistic modules and a sequence of se-
mantic modules. The information extraction sub-
system uses the literal content of certain phrases it
finds in the text or infers the environment and the
actions.
We use a pipeline of modules in the first stages
of the natural language processing chain. The
tasks consists of tokenizing, part-of-speech tagging,
splitting into sentences, detecting the noun groups,
clause boundaries, and domain-specific multiwords.
We use the Granska part-of-speech tagger (Carl-
berger and Kann, 1999) and Ejerhed?s algorithm
(Ejerhed, 1996) to detect clause boundaries.
6.1 Named Entity Recognition
Carsim uses a domain-specific named entity recog-
nition module, which detects names of persons,
places, roads, and car makes (Persson and Daniels-
son, 2004).
The recognition is based on a small database of
2,500 entries containing person names, city and re-
gion names, and car names. It applies a cascade
of regular expressions that takes into account the
morphology of Swedish proper noun formation and
the road nomenclature. The recall/precision perfor-
mance of the detector is 0.89/0.97.
6.2 Finding the Participants
The system uses the detected noun groups to iden-
tify the physical objects, which are involved in the
accident. It extracts the headword of each group and
associates it to an entity in the ontology. We used
parts of the Swedish WordNet as a resource to de-
velop this dictionary (A?ke Viberg et al, 2002).
We track the entities along the text with a sim-
ple coreference resolution algorithm. It assumes
that each definite expression corefers with the last
sortally consistent (according to the ontology) en-
tity which was mentioned. Indefinite expressions
are assumed to be references to previously unmen-
tioned entities. This is similar to the algorithm men-
tioned in (Appelt and Israel, 1999). Although this
approach is relatively simple, we get reasonable re-
sults with it and could use it as a baseline when in-
vestigating other approaches.
Figure 3 shows an excerpt from a text with the
annotation of the participants as well as their coref-
erences.
Olyckan intra?ffade na?r [bilen]1 som de fem
fa?rdades i ko?rde om [en annan personbil]2 . Na?r
[den]1 sva?ngde tillbaka in framfo?r [den omko?rda
bilen]2 fick [den]1 sladd och for med sidan rakt
mot fronten pa? [den mo?tande lastbilen]3 .
The accident took place when [the car]1 where the
five people were traveling overtook [another car]2.
When [it]1 pulled in front of [the overtaken car]2,
[it]1 skidded and hit with its side the front of [the
facing truck]3.
Figure 3: A sentence where references to road ob-
jects have been marked.
6.3 Resolution of Metonymy
Use of metonymy, such as alternation between the
driver and his vehicle, is frequent in the Swedish
press clippings. An improper resolution of it intro-
duces errors in the templates and in the visualiza-
tion. It can create independently moving graphic
entities i.e. the vehicle and its driver, that should be
represented as one single object, a moving vehicle,
or stand together.
We detect the metonymic relations between
drivers and their vehicles. We use either cue phrases
like lastbilschauffo?ren (?the truck driver?) or the lo-
cation or instrument semantic roles in phrases like
Mannen som fa?rdades i lastbilen (?The man who
was traveling in the truck?). We then apply con-
straints on the detected events and directions to ex-
clude wrong candidates. For example, given the
phrase Mannen krockade med en traktor (?The man
collided with a tractor?), we know that the man can-
not be the driver of the tractor.
We do not yet handle the metonymic relations be-
tween parts of vehicles and the vehicles themselves.
They are less frequent in the texts we have exam-
ined.
6.4 Marking Up the Events
Events in car accident reports correspond to vehicle
motions and collisions. We detect them to be able
to visualize and animate the scene actions. To carry
out the detection, we created a dictionary of words
? nouns and verbs ? depicting vehicle activity and
maneuvers. We use these words to anchor the event
identification as well as the semantic roles of the
dependents to determine the event arguments.
6.4.1 Detecting the Semantic Roles
Figure 4 shows a sentence that we translated from
our corpus of news texts, where the groups have
been marked up and labeled with semantic roles.
[En personbil]Actor ko?rde [vid femtiden]T ime
[pa? torsdagseftermiddagen]T ime [in i ett rad-
hus]V ictim [i ett a?ldreboende]Loc [pa? Alva?gen]Loc
[i Enebyberg]Loc [norr om Stockholm]Loc .
[About five]T ime [on Thursday afternoon]T ime , [a
car]Actor crashed [into a row house]V ictim [in an
old people?s home]Loc [at Alva?gen street]Loc [in
Enebyberg]Loc [north of Stockholm]Loc.
Figure 4: A sentence tagged with semantic roles.
Gildea and Jurafsky (2002) describe an algorithm
to label automatically semantic roles in a general
context. They use the semantic frames and associ-
ated roles defined in FrameNet (Baker et al, 1998)
and train their classifier on the FrameNet corpus.
They report a performance of 82 percent.
Carsim uses a classification algorithm similar to
the one described in this paper. However, as there is
no lexical resource such as FrameNet for Swedish
and no widely available parser, we adapted it. Our
classifier uses a more local strategy as well as a dif-
ferent set of attributes.
The analysis starts from the words in our dictio-
nary for which we designed a specific set of frames
and associated roles. The classifier limits the scope
of each event to the clause where it appears. It iden-
tifies the verb and nouns dependents: noun groups,
prepositional groups, and adverbs that it classifies
according to semantic roles.
The attributes of the classifier are:
? Target word: the keyword denoting the event.
? Head word: the head word of the group to be
classified.
? Syntactic class of head word: noun group,
prepositional group, or adverb.
? Voice of the target word: active or passive.
? Domain-specific semantic type: Dynamic ob-
ject, static object, human, place, time, cause,
or speed.
The classifier chooses the role, which maximizes
the estimated probability of a role given the values
of the target, head, and semantic type attributes:
P? (r|t, head, sem) = C(r, t, head, sem)C(t, head, sem) .
If a particular combination of target, head, and
semantic type is not found in the training set, the
classifier uses a back-off strategy, taking the other
attributes into account.
We annotated manually a set of 819 examples on
which we trained and tested our classifier. We used
a random subset of 100 texts as a test set and the
rest as a learning set. On the test set, the classi-
fier achieved an accuracy of 90 percent. A classi-
fier based on decision trees built using the ID3 algo-
rithm with gain ratio measure yielded roughly the
same performance.
The value of the semantic type attribute is set us-
ing domain knowledge. Removing this attribute de-
graded the performance of the classifier to 80 per-
cent.
6.4.2 Interpreting the Events
When the events have been detected in the text, they
can be represented and interpreted in the formal de-
scription of the accidents.
We observed that event coreferences are very fre-
quent in longer texts: A same action like a colli-
sion is repeated in several places in the text. As
for metonymy, duplicated events in the template en-
tails a wrong visualization. We solve it through the
unification of as many events as possible, taking
metonymy relations into account, and we remove
the duplicates.
6.5 Time Processing and Event Ordering
In some texts, the order in which events are men-
tioned does not correspond to their chronological
order. To address this issue and order the events cor-
rectly, we developed a module based on the generic
TimeML framework (Pustejovsky et al, 2002). We
use a machine learning approach to annotate the
whole set of events contained in a text and from this
set, we extract events used specifically by the Car-
sim template ? the Carsim events.
TimeML has tags for time expressions (today),
?signals? indicating the polarity (not), the modal-
ity (could), temporal prepositions and connectives
such as for, during, before, after, events (crashed,
accident), and tags that indicate relations between
entities. Amongst the relations, the TLINKs are
the most interesting for our purposes. They ex-
press temporal relations between time expressions
and events as well as temporal relations between
pairs of events.
We developed a comprehensive phrase-structure
grammar to detect the time expressions, signals, and
TimeML events and to assign values to the enti-
ties? attributes. The string den tolfte maj (?May
12th?) is detected as a time expression with the
attribute value=?YYYY-05-12?. We extended the
TimeML attributes to store the events? syntactic fea-
tures. They include the part-of-speech annotation
and verb group structure, i.e. auxiliary + participle,
etc.
We first apply the PS rules to detect the time ex-
pressions, signals, and events. Let e1, e2, e3, ...,
en be the events in the order they are mentioned
in a text. We then generate TLINKs to relate these
events together using a set of decision trees.
We apply three decision trees on se-
quences of two to four consecutive events
(ei, ei+1, [, ei+2[, ei+3]]), with the constraint
that there is no time expression between them,
as they might change the temporal ordering sub-
stantially. The output of each tree is the temporal
relation holding between the first and last event
of the considered sequence, i.e. respectively:
adjacent pairs (ei, ei+1), pairs separated by one
event (ei, ei+2), and by two events (ei, ei+3). The
possible output values are simultaneous, after,
before, is included, includes, and none. As a result,
each event is linked by TLINKs to the three other
events immediately after and before it.
We built automatically the decision trees using
the ID3 algorithm (Quinlan, 1986). We trained them
on a set of hand-annotated examples, which consists
of 476 events and 1,162 TLINKs.
As a set of features, the decision trees use certain
attributes of the events considered, temporal signals
between them, and some other parameters such as
the number of tokens separating the pair of events
to be linked. The complete list of features with x
ranging from 0 to 1, 0 to 2, and 0 to 3 for each tree
respectively, and their possible values is:
? Eventi+xTense: none, past, present, future,
NOT DETERMINED.
? Eventi+xAspect: progressive, per-
fective, perfective progressive, none,
NOT DETERMINED.
? Eventi+xStructure: NOUN,
VB GR COP INF, VB GR COP FIN,
VB GR MOD INF, VB GR MOD FIN,
VB GR, VB INF, VB FIN.
? temporalSignalInbetween: none, before, after,
later, when, still, several.
? tokenDistance: 1, 2 to 3, 4 to 6, 7 to 10, greater
than 10.
? sentenceDistance: 0, 1, 2, 3, 4, greater than 4.
? punctuationSignDistance: 0, 1, 2, 3, 4, 5,
greater than 5.
The process results in an overgeneration of links.
The reason for doing this is to have a large set of
TLINKs to ensure a fine-grained ordering of the
events. As the generated TLINKs can be conflict-
ing, we assign each of them a score, which is de-
rived from the C4.5 metrics (Quinlan, 1993).
We complement the decision trees with heuris-
tics and hints from the event interpreter that events
are identical. Heuristics represent common-sense
knowledge and are encoded as nine production
rules. An example of them is that an event in the
present tense is after an event in the past tense.
Event identity and heuristics enable to connect
events across the time expressions. The TLINKs
generated by the rules also have a score that is rule
dependent.
When all TLINKs are generated, we resolve tem-
poral loops by removing the TLINK with the lowest
score within the loop. Finally, we extract the Carsim
events from the whole set of TimeML events and we
order them using the relevant TLINKs.
6.6 Detecting the Roads
The configuration of roads is inferred from the in-
formation in the detected events. When one of the
involved vehicles makes a turn, this indicates that
the configuration is probably a crossroads.
Additional information is provided using key-
word spotting in the text. Examples of relevant key-
words are korsning (?crossing?), ?rondell? (?round-
about?) and kurva (?bend?), which are very likely
indicators of the road configuration if seen in the
text.
These methods are very simple, but the cases
where they fail are quite rare. During the evalua-
tion described below, we found no text where the
road configuration was misclassified.
7 Evaluation of the Information
Extraction Module
To evaluate the performance of the information ex-
traction component, we applied it to 50 previously
unseen texts, which were collected from newspaper
sources on the web. The size of the texts ranged
from 31 to 459 words. We calculated precision and
recall measures for detection of road objects and for
detection of events. A road object was counted as
correctly detected if there was a corresponding ob-
ject in the text, and the type of the object was cor-
rect. The same criteria apply to the detection of
events, but here we also add the criterion that the
actor (and victim, where this applies) must be cor-
rect. The performance figures are shown in Tables 1
and 2.
Total number of objects in the texts 105
Number of detected objects 110
Number of correctly detected objects 94
Precision 0.85
Recall 0.90
F-measure (? = 1) 0.87
Table 1: Statistics for the detection of road objects
in the test set.
Total number of events in the texts 92
Number of detected events 91
Number of correctly detected events 71
Precision 0.78
Recall 0.77
F-measure (? = 1) 0.78
Table 2: Statistics for the detection of events in the
test set.
The system was able to extract or infer all rele-
vant information correctly in 23 of the 50 texts. In
order to find out the causes of the errors, we investi-
gated what simplifications of the texts needed to be
Figure 5: Planning the trajectories.
made to make the system produce a correct analysis.
The result of this investigation is shown in Table 3.
Object coreference 6
Role labeling 5
Metonymy 5
Clause segmentation 3
Representational expressivity 3
Unknown objects 2
Event detection 2
Unknown event 1
Tagger error 1
PP attachment 1
Table 3: Causes of errors.
8 Scene Synthesis and Visualization
The visualizer reads its input from the formal de-
scription. It synthesizes a symbolic 3D scene and
animates the vehicles. We designed the graphic el-
ements in the scene with the help of traffic safety
experts.
The scene generation algorithm positions the
static objects and plans the vehicle motions. It uses
rule-based modules to check the consistency of the
description and to estimate the 3D start and end co-
ordinates of the vehicles.
The visualizer uses a planner to generate the vehi-
cle trajectories. A first module determines the start
and end positions of the vehicles from the initial di-
rections, the configuration of the other objects in the
scene, and the chain of events as if they were no ac-
cident. Then, a second module alters these trajecto-
ries to insert the collisions according to the accident
slots in the accident representation (Figure 5).
This two-step procedure can be justified by the
descriptions found in most reports. The car drivers
generally start the description of their accident as if
it were a normal movement, which is subsequently
been modified by the abnormal conditions of the ac-
cident.
Finally, the temporal module of the planner as-
signs time intervals to all the segments of the trajec-
tories.
Figure 6 shows two screenshots that the Carsim
visualizer produces for the text above. It should be
noted that the graphic representation is intended to
be iconic in order not to convey any meaning which
is not present in the text.
9 Conclusion and Perspectives
We have presented an architecture and a strategy
based on information extraction and a symbolic vi-
sualization that enable to convert real texts into 3D
scenes. We have obtained promising results that val-
idate our approach. They show that the Carsim ar-
chitecture is applicable to Swedish and other lan-
guages. As far as we know, Carsim is the only
text-to-scene conversion system working on non-
invented narratives.
We are currently improving Carsim and we hope
in future work to obtain better results in the reso-
lution of coreferences. We are implementing and
adapting algorithms such as the one described in
(Soon et al, 2001) to handle this. We also intend
to improve the visualizer to handle more complex
scenes and animations.
The current aim of the Carsim project is to visu-
alize the content of a text as accurately as possible,
with no external knowledge. In the future, we would
like to integrate additional knowledge sources in or-
der to make the visualization more realistic and un-
derstandable. Geographical and meteorological in-
formation systems are good examples of this, which
could be helpful to improve the realism. Another
topic, which has been prominent in our discussions
with traffic safety experts, is how to reconcile dif-
ferent narratives that describe a same accident.
In our work on the information extraction mod-
ule, we have concentrated on the extraction of data
which are relevant for the visual reconstruction of
the scene. We believe that the information extrac-
tion component could be interesting in itself to ex-
tract other relevant data, for example casualty statis-
tics or traffic conditions.
Acknowledgements
We are very grateful to Karin Brundell-Freij, A?se
Svensson, and Andra?s Va?rhelyi, traffic safety ex-
perts at LTH, for helping us in the design the Carsim
template and advising us with the 3D graphic repre-
sentation.
This work is partly supported by grant num-
ber 2002-02380 from the Spra?kteknologi program
of Vinnova, the Swedish Agency of Innovation
Systems.
References
Giovanni Adorni, Mauro Di Manzo, and Fausto
Giunchiglia. 1984. Natural language driven im-
age generation. In Proceedings of COLING 84,
pages 495?500, Stanford, California.
Douglas E. Appelt and David Israel. 1999. In-
troduction to information extraction technology.
Tutorial Prepared for IJCAI-99. Artificial Intelli-
gence Center, SRI International.
Michael Arens, Artur Ottlik, and Hans-Hellmut
Nagel. 2002. Natural language texts for a cogni-
tive vision system. In Frank van Harmelen, edi-
tor, ECAI2002, Proceedings of the 15th European
Conference on Artificial Intelligence, Lyon, July
21-26.
Collin F. Baker, Charles J. Fillmore, and John B.
Lowe. 1998. The Berkeley FrameNet Project. In
Proceedings of COLING-ACL?98, pages 86?90,
Montre?al, Canada.
Johan Carlberger and Viggo Kann. 1999. Imple-
menting an efficient part-of-speech tagger. Soft-
ware Practice and Experience, 29:815?832.
Bob Coyne and Richard Sproat. 2001. Wordseye:
An automatic text-to-scene conversion system.
In Proceedings of the Siggraph Conference, Los
Angeles.
Michel Denis. 1991. Imagery and thinking. In Ce-
sare Cornoldi and Mark A. McDaniel, editors,
Imagery and Cognition, pages 103?132. Springer
Verlag.
Sylvain Dupuy, Arjan Egges, Vincent Legendre,
and Pierre Nugues. 2001. Generating a 3D simu-
lation of a car accident from a written descrip-
tion in natural language: The Carsim system.
In Proceedings of The Workshop on Temporal
and Spatial Information Processing, pages 1?8,
Toulouse, July 7. Association for Computational
Linguistics.
Arjan Egges, Anton Nijholt, and Pierre Nugues.
2001. Generating a 3D simulation of a car ac-
cident from a formal description. In Venetia Gi-
agourta and Michael G. Strintzis, editors, Pro-
ceedings of The International Conference on
Augmented, Virtual Environments and Three-
Dimensional Imaging (ICAV3D), pages 220?223,
Mykonos, Greece, May 30-June 01.
Eva Ejerhed. 1996. Finite state segmentation of
discourse into clauses. In Proceedings of the 12th
European Conference on Artificial Intelligence
(ECAI-96) Workshop on Extended Finite State
Models of Language, Budapest, Hungary.
Daniel Gildea and Daniel Jurafsky. 2002. Auto-
Figure 6: Screenshots from the animation of the text above.
matic labeling of semantic roles. Computational
Linguistics, 28(3):245?288.
Nils-Olof Karlberg. 2003. Field results from
STRADA ? a traffic accident data system telling
the truth. In ITS World Congress, Madrid, Spain,
November 16-20.
Stephen Michael Kosslyn. 1983. Ghosts in the
Mind?s Machine. Norton, New York.
Mauro Di Manzo, Giovanni Adorni, and Fausto
Giunchiglia. 1986. Reasoning about scene de-
scriptions. IEEE Proceedings ? Special Issue on
Natural Language, 74(7):1013?1025.
Lisa Persson and Magnus Danielsson. 2004. Name
extraction in car accident reports for Swedish.
Technical report, LTH, Department of Computer
science, Lund, January.
James Pustejovsky, Roser Saur??, Andrea Setzer, Rob
Gaizauskas, and Bob Ingria. 2002. TimeML An-
notation Guidelines. Technical report.
John Ross Quinlan. 1986. Induction of decision
trees. Machine Learning, 1(1):81?106.
John Ross Quinlan. 1993. C4.5: Programs for Ma-
chine Learning. Morgan Kauffman.
A?ke Viberg, Kerstin Lindmark, Ann Lindvall, and
Ingmarie Mellenius. 2002. The Swedish Word-
Net project. In Proceedings of Euralex 2002,
pages 407?412, Copenhagen.
Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
Edward R. Tufte. 1997. Visual Explanations: Im-
ages and Quantities, Evidence and Narrative.
Graphic Press.
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 177?180, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Sparse Bayesian Classification of Predicate Arguments
Richard Johansson and Pierre Nugues
LUCAS, Department of Computer Science, Lund University
Box 118
SE-221 00 Lund, Sweden
{richard, pierre}@cs.lth.se
Abstract
We present an application of Sparse
Bayesian Learning to the task of semantic
role labeling, and we demonstrate that this
method produces smaller classifiers than
the popular Support Vector approach.
We describe the classification strategy and
the features used by the classifier. In par-
ticular, the contribution of six parse tree
path features is investigated.
1 Introduction
Generalized linear classifiers, in particular Support
Vector Machines (SVMs), have recently been suc-
cessfully applied to the task of semantic role iden-
tification and classification (Pradhan et al, 2005),
inter alia.
Although the SVM approach has a number of
properties that make it attractive (above all, excel-
lent software packages exist), it also has drawbacks.
First, the resulting classifier is slow since it makes
heavy use of kernel function evaluations. This is
especially the case in the presence of noise (since
each misclassified example has to be stored as a
bound support vector). The number of support vec-
tors typically grows with the number of training ex-
amples. Although there exist optimization methods
that speed up the computations, the main drawback
of the SVM approach is still the classification speed.
Another point is that it is necessary to tune the
parameters (typically C and ?). This makes it nec-
essary to train repeatedly using cross-validation to
find the best combination of parameter values.
Also, the output of the decision function of the
SVM is not probabilistic. There are methods to map
the decision function onto a probability output using
the sigmoid function, but they are considered some-
what ad-hoc (see (Tipping, 2001) for a discussion).
In this paper, we apply a recent learning
paradigm, namely Sparse Bayesian learning, or
more specifically the Relevance Vector learning
method, to the problem of role classification. Its
principal advantages compared to the SVM ap-
proach are:
? It typically utilizes fewer examples compared
to the SVM, which makes the classifier faster.
? It uses no C parameter, which reduces the need
for cross-validation.
? The decision function is adapted for probabilis-
tic output.
? Arbitrary basis functions can be used.
Its significant drawback is that the training pro-
cedure relies heavily on dense linear algebra, and is
thus difficult to scale up to large training sets and
may be prone to numerical difficulties.
For a description of the task and the data, see (Car-
reras and M?rquez, 2005).
2 Sparse Bayesian Learning and the
Relevance Vector Machine
The Sparse Bayesian method is described in detail in
(Tipping, 2001). Like other generalized linear learn-
ing methods, the resulting binary classifier has the
form
signf(x) = sign
m
?
i=1
?
i
f
i
(x) + b
177
where the f
i
are basis functions. Training the
model then consists of finding a suitable ? =
(b, ?
1
, . . . , ?
m
) given a data set (X,Y ).
Analogous with the SVM approach, we can let
f
i
(x) = k(x, x
i
), where x
i
is an example from the
training set and k a function. We have then arrived
at the Relevance Vector Machine (RVM). There are
however no restrictions on the function k (such as
Mercer?s condition for SVM). We use the Gaussian
kernel k(x, y) = exp(???x ? y?2) throughout this
work.
We first model the probability of a positive ex-
ample as a sigmoid applied to f(x). This can be
used to write the likelihood function P (Y |X,?).
Instead of a conventional ML approach (maximiz-
ing the likelihood with respect to ?, which would
give an overfit model), we now adopt a Bayesian
approach and encode the model preferences using
priors on ?. For each ?
i
, we introduce a parame-
ter s
i
and assume that ?
i
? N(0, s?1
i
) (i.e. Gaus-
sian). This is in effect an ?Occam penalty? that en-
codes our preference for sparse models. We should
finally specify the distributions of the s
i
. However,
we make the simplifying assumption that their dis-
tribution is flat (noninformative).
We now find the maximum of the marginal likeli-
hood, or ?evidence?, with respect to s, that is
p(Y |X, s) =
?
P (Y |X,?)p(?|s)d?.
This integral is not tractable, hence we approximate
the integrand using a Gaussian centered at the mode
of the integrand (Laplace?s approximation). The
marginal likelihood can then be differentiated with
respect to s, and maximized using iterative methods
such as gradient descent.
The algorithm thus proceeds iteratively as fol-
lows: First maximize the penalized likelihood func-
tion P (Y |X,?)p(?|s) with respect to ? (for ex-
ample via the Newton-Raphson method), then up-
date the parameters s
i
. This goes on until a con-
vergence criterion is met, for example that the s
i
changes are small enough. During iteration, the s
i
parameters for redundant examples tend to infinity.
They (and the corresponding columns of the kernel
matrix) are then removed from the model. This is
necessary because of numerical stability and also re-
duces the training time considerably.
We implemented the RVM training method using
the ATLAS (Whaley et al, 2000) implementation
of the BLAS and LAPACK standard linear algebra
APIs. To make the algorithm scale up, we used a
working-set strategy that used the results of partial
solutions to train the final classifier. Our implemen-
tation is based on the original description of the al-
gorithm (Tipping, 2001) rather than the greedy opti-
mized version (Tipping and Faul, 2003), since pre-
liminary experiments suggested a decrease in clas-
sification accuracy. Our current implementation can
handle training sets up to about 30000 examples.
We used the conventional one-versus-one method
for multiclass classification. Although the Sparse
Bayesian paradigm is theoretically not limited to bi-
nary classifiers, this is of little use in practice, since
the size of the Hessian matrix (used while maximiz-
ing the likelihood and updating s) grows with the
number of classes.
3 System Description
Like previous systems for semantic role identifica-
tion and classification, we used an approach based
on classification of nodes in the constituent tree.
To simplify training, we used the soft-prune ap-
proach as described in (Pradhan et al, 2005), which
means that before classification, the nodes were fil-
tered through a binary classifier that classifies them
as having a semantic role or not (NON-NULL or
NULL). The NULL nodes missed by the filter were
included in the training set for the final classifier.
Since our current implementation of the RVM
training algorithm does not scale up to large training
sets, training on the whole PropBank was infeasible.
We instead trained the multiclass classifier on sec-
tions 15 ? 18, and used an SVM for the soft-pruning
classifier, which was then trained on the remaining
sections. The excellent LIBSVM (Chang and Lin,
2001) package was used to train the SVM.
The features used by the classifiers can be
grouped into predicate and node features. Of the
node features, we here pay most attention to the
parse tree path features.
3.1 Predicate Features
We used the following predicate features, all of
which first appeared in (Gildea and Jurafsky, 2002).
178
? Predicate lemma.
? Subcategorization frame.
? Voice.
3.2 Node Features
? Head word and head POS. Like most previous
work, we used the head rules of Collins to ex-
tract this feature.
? Position. A binary feature that describes if the
node is before or after the predicate token.
? Phrase type (PT), that is the label of the con-
stituent.
? Named entity. Type of the first contained NE.
? Governing category. As in (Gildea and Juraf-
sky, 2002), this was used to distinguish subjects
from objects. For an NP, this is either S or VP.
? Path features. (See next subsection.)
For prepositional phrases, we attached the prepo-
sition to the PT and replaced head word and head
POS with those of the first contained NP.
3.3 Parse Tree Path Features
Previous studies have shown that the parse tree path
feature, used by almost all systems since (Gildea and
Jurafsky, 2002), is salient for argument identifica-
tion. However, it is extremely sparse (which makes
the system learn slowly) and is dependent on the
quality of the parse tree. We therefore investigated
the contribution of the following features in order
to come up with a combination of path features that
leads to a robust system that generalizes well.
? Constituent tree path. As in (Gildea and Ju-
rafsky, 2002), this feature represents the path
(consisting of step directions and PTs of the
nodes traversed) from the node to the predicate,
for example NP?VP?VB for a typical object.
Removing the direction (as in (Pradhan et al,
2005)) improved neither precision nor recall.
? Partial path. To reduce sparsity, we introduced
a partial path feature (as in (Pradhan et al,
2005)), which consists of the path from the
node to the lowest common ancestor.
? Dependency tree path. We believe that la-
beled dependency paths provide more informa-
tion about grammatical functions (and, implic-
itly, semantic relationships) than the raw con-
stituent structure. Since the grammatical func-
tions are not directly available from the parse
trees, we investigated two approximations of
dependency arc labels: first, the POSs of the
head tokens; secondly, the PTs of the head node
and its immediate parent (such labels were used
in (Ahn et al, 2004)).
? Shallow path. Since the UPC shallow parsers
were expected to be more robust than the full
parsers, we used a shallow path feature. We
first built a parse tree using clause and chunk
bracketing, and the shallow path feature was
then constructed like the constituent tree path.
? Subpaths. All subpaths of the constituent path.
We used the parse trees from Charniak?s parser to
derive all paths except for the shallow path.
4 Results
4.1 Comparison with SVM
The binary classifiers that comprise the one-versus-
one multiclass classifier were 89% ? 98% smaller
when using RVM compared to SVM. However, the
performance dropped by about 2 percent. The rea-
son for the drop is possibly that the classifier uses a
number of features with extremely sparse distribu-
tions (two word features and three path features).
4.2 Path Feature Contributions
To estimate the contribution of each path feature, we
measured the difference in performance between a
system that used all six features and one where one
of the features had been removed. Table 2 shows
the results for each of the six features. For the final
system, we used the dependency tree path with PT
pairs, the shallow path, and the partial path.
4.3 Final System Results
The results of the complete system on the test sets
are shown in Table 1. The smaller training set (as
mentioned above, we used only sections 15 ? 18
179
Precision Recall F
?=1
Development 73.40% 70.85% 72.10
Test WSJ 75.46% 73.18% 74.30
Test Brown 65.17% 60.59% 62.79
Test WSJ+Brown 74.13% 71.50% 72.79
Test WSJ Precision Recall F
?=1
Overall 75.46% 73.18% 74.30
A0 84.56% 85.18% 84.87
A1 73.40% 73.35% 73.37
A2 61.99% 57.30% 59.55
A3 71.43% 46.24% 56.14
A4 72.53% 64.71% 68.39
A5 100.00% 40.00% 57.14
AM-ADV 58.13% 51.58% 54.66
AM-CAU 70.59% 49.32% 58.06
AM-DIR 59.62% 36.47% 45.26
AM-DIS 81.79% 71.56% 76.33
AM-EXT 72.22% 40.62% 52.00
AM-LOC 54.05% 55.10% 54.57
AM-MNR 54.33% 52.91% 53.61
AM-MOD 98.52% 96.73% 97.62
AM-NEG 96.96% 96.96% 96.96
AM-PNC 36.75% 37.39% 37.07
AM-PRD 0.00% 0.00% 0.00
AM-REC 0.00% 0.00% 0.00
AM-TMP 76.00% 70.19% 72.98
R-A0 83.33% 84.82% 84.07
R-A1 68.75% 70.51% 69.62
R-A2 57.14% 25.00% 34.78
R-A3 0.00% 0.00% 0.00
R-A4 0.00% 0.00% 0.00
R-AM-ADV 0.00% 0.00% 0.00
R-AM-CAU 100.00% 25.00% 40.00
R-AM-EXT 0.00% 0.00% 0.00
R-AM-LOC 92.31% 57.14% 70.59
R-AM-MNR 40.00% 33.33% 36.36
R-AM-TMP 75.00% 69.23% 72.00
V 98.82% 98.82% 98.82
Table 1: Overall results (top) and detailed results on
the WSJ test (bottom).
for the role classifier) causes the result to be signifi-
cantly lower than state of the art (F-measure of 79.4,
reported in (Pradhan et al, 2005)).
5 Conclusion and Future Work
We have provided an application of Relevance Vec-
tor Machines to a large-scale NLP task. The re-
sulting classifiers are drastically smaller that those
produced by the SV training methods. On the other
hand, the classification accuracy is lower, probably
because of the use of lexicalized features.
The results on the Brown test set shows that the
genre has a significant impact on the performance.
An evaluation of the contribution of six parse tree
P R F
?=1
Const. tree -0.2% -0.6% -0.4
Partial -0.4% +0.4% 0
Dep. w/ POSs -0.1% -0.4% -0.3
Dep. w/ PT pairs +0.4% +0.4% +0.4
Shallow -0.1% +0.4% +0.1
Const. subpaths -10.9% +2.5% -4.5
Table 2: Contribution of path features
path features suggests that dependency tree paths are
more useful for semantic role labeling than the tra-
ditional constituent tree path.
In the future, we will investigate if it is possible
to incorporate the ? parameter into the probability
model, thus eliminating the need for cross-validation
completely. In addition, the training algorithm will
need to be redesigned to scale up to larger training
sets. The learning paradigm is still young and op-
timized methods (such as for SVM) have yet to ap-
pear. One possible direction is the greedy method
described in (Tipping and Faul, 2003).
References
David Ahn, Sisay Fissaha, Valentin Jijkoun, and Maarten
de Rijke. 2004. The university of Amsterdam at
Senseval-3: Semantic roles and logic forms. In Pro-
ceedings of SENSEVAL-3.
Xavier Carreras and Llu?s M?rquez. 2005. Introduction
to the CoNLL-2005 Shared Task: Semantic Role La-
beling. In Proceedings of CoNLL-2005.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a
library for support vector machines.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James Martin, and Dan Jurafsky. 2005.
Support vector learning for semantic argument classi-
fication. Machine Learning. To appear.
Michael E. Tipping and Anita Faul. 2003. Fast marginal
likelihood maximisation for sparse bayesian models.
In 9th International Workshop on AI and Statistics.
Michael E. Tipping. 2001. Sparse bayesian learning
and the relevance vector machine. Journal of Machine
Learning Research, 1:211 ? 244.
R. Clint Whaley, Antoine Petitet, and Jack J. Dongarra.
2000. Automated empirical optimizations of software
and the ATLAS project.
180
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 206?210, New York City, June 2006. c?2006 Association for Computational Linguistics
Investigating Multilingual Dependency Parsing
Richard Johansson
Department of Computer Science
LTH, Lund University
221 00 Lund, Sweden
Richard.Johansson@cs.lth.se
Pierre Nugues
Department of Computer Science
LTH, Lund University
221 00 Lund, Sweden
Pierre.Nugues@cs.lth.se
Abstract
In this paper, we describe a system for
the CoNLL-X shared task of multilin-
gual dependency parsing. It uses a base-
line Nivre?s parser (Nivre, 2003) that first
identifies the parse actions and then la-
bels the dependency arcs. These two steps
are implemented as SVM classifiers using
LIBSVM. Features take into account the
static context as well as relations dynami-
cally built during parsing.
We experimented two main additions to
our implementation of Nivre?s parser: N -
best search and bidirectional parsing. We
trained the parser in both left-right and
right-left directions and we combined the
results. To construct a single-head, rooted,
and cycle-free tree, we applied the Chu-
Liu/Edmonds optimization algorithm. We
ran the same algorithm with the same pa-
rameters on all the languages.
1 Nivre?s Parser
Nivre (2003) proposed a dependency parser that cre-
ates a projective and acyclic graph. The parser is an
extension to the shift?reduce algorithm. As with the
regular shift?reduce, it uses a stack S and a list of
input words W . However, instead of finding con-
stituents, it builds a set of arcs G representing the
graph of dependencies.
Nivre?s parser uses two operations in addition to
shift and reduce: left-arc and right-arc. Given a se-
quence of words, possibly annotated with their part
of speech, parsing simply consists in applying a se-
quence of operations: left-arc (la), right-arc (ra),
reduce (re), and shift (sh) to the input sequence.
2 Parsing an Annotated Corpus
The algorithm to parse an annotated corpus is
straightforward from Nivre?s parser and enables us
to obtain, for any projective sentence, a sequence of
actions taken in the set {la,ra,re,sh} that parses
it. At a given step of the parsing process, let TOP
be the top of the stack and FIRST , the first token of
the input list, and arc, the relation holding between
a head and a dependent.
1. if arc(TOP,FIRST ) ? G, then ra;
2. else if arc(FIRST, TOP ) ? G, then la;
3. else if ?k ? Stack, arc(FIRST, k) ? G or
arc(k, FIRST ) ? G, then re;
4. else sh.
Using the first sentence of the Swedish corpus
as input (Table 1), this algorithm produces the se-
quence of 24 actions: sh, sh, la, ra, re, la, sh,
sh, sh, la, la, ra, ra, sh, la, re, ra, ra, ra,
re, re, re, re, and ra (Table 2).
3 Adapting Nivre?s Algorithm to
Machine?Learning
3.1 Overview
We used support vector machines to predict the
parse action sequence and a two step procedure to
206
Table 1: Dependency graph of the sentence ?kten-
skapet och familjen ?r en gammal institution, som
funnits sedan 1800-talet ?Marriage and family are
an old institution that has been around from the 19th
century?.
ID Form POS Head Rel.
1 ?ktenskapet NN 4 SS
2 och ++ 3 ++
3 familjen NN 1 CC
4 ?r AV 0 ROOT
5 en EN 7 DT
6 gammal AJ 7 AT
7 institution NN 4 SP
8 , IK 7 IK
9 som PO 10 SS
10 funnits VV 7 ET
11 sedan PR 10 TA
12 1800-talet NN 11 PA
13 . IP 4 IP
produce the graph. We first ran the classifier to se-
lect unlabeled actions, la, ra, sh, re. We then ran
a second classifier to assign a function to ra and la
parse actions.
We used the LIBSVM implementation of the
SVM learning algorithm (Chang and Lin, 2001). We
used the Gaussian kernel throughout. Optimal val-
ues for the parameters (C and ?) were found using a
grid search. The first predicted action is not always
possible, given the parser?s constraints. We trained
the model using probability estimates to select the
next possible action.
3.2 Feature Set
We used the following set of features for the classi-
fiers:
? Word and POS of TOP and FIRST
? Word and POS of the second node on the stack
? Word and POS of the second node in the input
list
? POS of the third and fourth nodes in the input
list
? The dependency type of TOP to its head, if any
? The word, POS, and dependency type of the
leftmost child of TOP to TOP, if any
? The word, POS, and dependency type of the
rightmost child of TOP to TOP, if any
? The word, POS, and dependency type of the
leftmost child of FIRST to FIRST, if any
For the POS, we used the Coarse POS, the Fine
POS, and all the features (encoded as boolean flags).
We did not use the lemma.
Table 2: Actions to parse the sentence ?ktenskapet
och familjen ?r en gammal institution, som funnits
sedan 1800-talet.
Ac. Top word First word Rel.
sh nil ?ktenskapet
sh ?ktenskapet och
la och familjen ++
ra ?ktenskapet familjen CC
re familjen ?r
la ?ktenskapet ?r SS
sh nil ?r
sh ?r en
sh en gammal
la gammal institution AT
la en institution DT
ra ?r institution SP
ra institution , IK
sh , som
la som funnits SS
re , funnits
ra institution funnits ET
ra funnits sedan TA
ra sedan 1800-talet PA
re 1800-talet .
re sedan .
re funnits .
re institution .
ra ?r . IP
4 Extensions to Nivre?s Algorithm
4.1 N -best Search
We extended Nivre?s original algorithm with a beam
search strategy. For each action, la, ra, sh and re,
207
we computed a probability score using LIBSVM.
These scores can then be used to carry out an N -
best search through the set of possible sequences of
actions.
We measured the improvement over a best-first
strategy incrementing values of N . We observed the
largest difference between N = 1 and N = 2, then
leveling off and we used the latter value.
4.2 Bidirectionality and Voting
Tesni?re (1966) classified languages as centrifuge
(head to the left) and centripetal (head to the right)
in a table (page 33 of his book) that nearly exactly
fits corpus evidence from the CONLL data. Nivre?s
parser is inherently left-right. This may not fit all
the languages. Some dependencies may be easier
to capture when proceeding from the reverse direc-
tion. Jin et al (2005) is an example of it for Chinese,
where the authors describe an adaptation of Nivre?s
parser to bidirectionality.
We trained the model and ran the algorithm in
both directions (left to right and right to left). We
used a voting strategy based on probability scores.
Each link was assigned a probability score (simply
by using the probability of the la or ra actions for
each link). We then summed the probability scores
of the links from all four trees. To construct a single-
head, rooted, and cycle-free tree, we finally applied
the Chu-Liu/Edmonds optimization algorithm (Chu
and Liu, 1965; Edmonds, 1967).
5 Analysis
5.1 Experimental Settings
We trained the models on ?projectivized? graphs fol-
lowing Nivre and Nilsson (2005) method. We used
the complete annotated data for nine langagues. Due
to time limitations, we could not complete the train-
ing for three languages, Chinese, Czech, and Ger-
man.
5.2 Overview of the Results
We parsed the 12 languages using exactly the same
algorithms and parameters. We obtained an average
score of 74.93 for the labeled arcs and of 80.39 for
the unlabeled ones (resp. 74.98 and 80.80 for the
languages where we could train the model using the
complete annotated data sets). Table 3 shows the
results per language. As a possible explanation of
the differences between languages, the three lowest
figures correspond to the three smallest corpora. It
is reasonable to assume that if corpora would have
been of equal sizes, results would have been more
similar. Czech is an exception to this rule that ap-
plies to all the participants. We have no explanation
for this. This language, or its annotation, seems to
be more complex than the others.
The percentage of nonprojective arcs also seems
to play a role. Due to time limitations, we trained
the Dutch and German models with approximately
the same quantity of data. While both languages
are closely related, the Dutch corpus shows twice
as much nonprojective arcs. The score for Dutch is
significantly lower than for German.
Our results across the languages are consistent
with the other participants? mean scores, where we
are above the average by a margin of 2 to 3% ex-
cept for Japanese and even more for Chinese where
we obtain results that are nearly 7% less than the av-
erage for labeled relations. Results are similar for
unlabeled data. We retrained the data with the com-
plete Chinese corpus and you obtained 74.41 for the
labeled arcs, still far from the average. We have no
explanation for this dip with Chinese.
5.3 Analysis of Swedish and Portuguese
Results
5.3.1 Swedish
We obtained a score of 78.13% for the labeled at-
tachments in Swedish. The error breakdown shows
significant differences between the parts of speech.
While we reach 89% of correct head and dependents
for the adjectives, we obtain 55% for the preposi-
tions. The same applies to dependency types, 84%
precision for subjects, and 46% for the OA type of
prepositional attachment.
There is no significant score differences for the
left and right dependencies, which could attributed
to the bidirectional parsing (Table 4). Distance plays
a dramatic role in the error score (Table 5). Preposi-
tions are the main source of errors (Table 6).
5.3.2 Portuguese
We obtained a score 84.57% for the labeled at-
tachments in Portuguese. As for Swedish, error
distribution shows significant variations across the
208
Table 3: Summary of results. We retrained the Chi-
nese* model after the deadline.
Languages Unlabeled Labeled
Completed training
Arabic 75.53 64.29
Chinese* 79.13 74.41
Danish 86.59 81.54
Dutch 76.01 72.67
Japanese 87.11 85.63
Portuguese 88.4 84.57
Slovene 74.36 66.43
Spanish 81.43 78.16
Swedish 84.17 78.13
Turkish 73.59 63.39
x 80.80 74.98
? 5.99 8.63
Noncompleted training
Chinese 77.04 72.49
Czech 77.4 71.46
German 83.09 80.43
x all languages 80.39 74.93
? all languages 5.36 7.65
parts of speech, with a score of 94% for adjectives
and only 67% for prepositions.
As for Swedish, there is no significant score dif-
ferences for the left and right dependencies (Ta-
ble 7). Distance also degrades results but the slope is
not as steep as with Swedish (Table 8). Prepositions
are also the main source of errors (Table 9).
5.4 Acknowledgments
This work was made possible because of the anno-
tated corpora that were kindly provided to us: Ara-
bic (Hajic? et al, 2004), Bulgarian (Simov et al,
2005; Simov and Osenova, 2003), Chinese (Chen
et al, 2003), Czech (B?hmov? et al, 2003), Danish
(Kromann, 2003), Dutch (van der Beek et al, 2002),
German (Brants et al, 2002), Japanese (Kawata and
Bartels, 2000), Portuguese (Afonso et al, 2002),
Slovene (D?eroski et al, 2006), Spanish (Civit Tor-
ruella and Mart? Anton?n, 2002), Swedish (Nilsson
et al, 2005), and Turkish (Oflazer et al, 2003; Ata-
lay et al, 2003).
Table 4: Precision and recall of binned HEAD direc-
tion. Swedish.
Dir. Gold Cor. Syst. R P
to_root 389 330 400 84.83 82.50
left 2745 2608 2759 95.01 94.53
right 1887 1739 1862 92.16 93.39
Table 5: Precision and recall of binned HEAD dis-
tance. Swedish.
Dist. Gold Cor. Syst. R P
to_root 389 330 400 84.83 82.50
1 2512 2262 2363 90.05 95.73
2 1107 989 1122 89.34 88.15
3-6 803 652 867 81.20 75.20
7-... 210 141 269 67.14 52.42
Table 6: Focus words where most of the errors occur.
Swedish.
Word POS Any Head Dep Both
till PR 48 20 45 17
i PR 42 25 34 17
p? PR 39 22 32 15
med PR 28 11 25 8
f?r PR 27 22 25 20
Table 7: Precision and recall of binned HEAD direc-
tion. Portuguese.
Dir. Gold Cor. Syst. R P
to_root 288 269 298 93.40 90.27
left 3006 2959 3020 98.44 97.98
right 1715 1649 1691 96.15 97.52
Table 8: Precision and recall of binned HEAD dis-
tance. Portuguese.
Dist. Gold Cor. Syst. R P
to_root 288 269 298 93.40 90.27
1 2658 2545 2612 95.75 97.43
2 1117 1013 1080 90.69 93.80
3-6 623 492 647 78.97 76.04
7-... 323 260 372 80.50 69.89
209
Table 9: Focus words where most of the errors occur.
Portuguese.
Word POS Any Head Dep Both
em prp 66 38 47 19
de prp 51 37 35 21
a prp 46 30 39 23
e conj 28 28 0 0
para prp 21 13 18 10
References
A. Abeill?, editor. 2003. Treebanks: Building and Us-
ing Parsed Corpora, volume 20 of Text, Speech and
Language Technology. Kluwer Academic Publishers,
Dordrecht.
S. Afonso, E. Bick, R. Haber, and D. Santos. 2002. ?Flo-
resta sint?(c)tica?: a treebank for Portuguese. In Proc.
of the Third Intern. Conf. on Language Resources and
Evaluation (LREC), pages 1698?1703.
N. B. Atalay, K. Oflazer, and B. Say. 2003. The annota-
tion process in the Turkish treebank. In Proc. of the 4th
Intern. Workshop on Linguistically Interpreteted Cor-
pora (LINC).
A. B?hmov?, J. Hajic?, E. Hajic?ov?, and B. Hladk?. 2003.
The PDT: a 3-level annotation scenario. In Abeill?
(Abeill?, 2003), chapter 7.
S. Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith.
2002. The TIGER treebank. In Proc. of the
First Workshop on Treebanks and Linguistic Theories
(TLT).
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a
library for support vector machines.
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang,
and Z. Gao. 2003. Sinica treebank: Design criteria,
representational issues and implementation. In Abeill?
(Abeill?, 2003), chapter 13, pages 231?248.
Y.J. Chu and T.H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396?
1400.
M. Civit Torruella and Ma A. Mart? Anton?n. 2002. De-
sign principles for a Spanish treebank. In Proc. of the
First Workshop on Treebanks and Linguistic Theories
(TLT).
S. D?eroski, T. Erjavec, N. Ledinek, P. Pajas, Z. ?abokrt-
sky, and A. ?ele. 2006. Towards a Slovene depen-
dency treebank. In Proc. of the Fifth Intern. Conf. on
Language Resources and Evaluation (LREC).
J. Edmonds. 1967. Optimum branchings. Journal of Re-
search of the National Bureau of Standards, 71B:233?
240.
J. Hajic?, O. Smr?, P. Zem?nek, J. ?naidauf, and E. Be?ka.
2004. Prague Arabic dependency treebank: Develop-
ment in data and tools. In Proc. of the NEMLAR In-
tern. Conf. on Arabic Language Resources and Tools,
pages 110?117.
Meixun Jin, Mi-Young Kim, and Jong-Hyeok Lee.
2005. Two-phase shift-reduce deterministic depen-
dency parser of Chinese. In Proceedings of the Second
International Joint Conference on Natural Language
Processing.
Y. Kawata and J. Bartels. 2000. Stylebook for the
Japanese treebank in VERBMOBIL. Verbmobil-
Report 240, Seminar f?r Sprachwissenschaft, Univer-
sit?t T?bingen.
M. T. Kromann. 2003. The Danish dependency treebank
and the underlying linguistic theory. In Proc. of the
Second Workshop on Treebanks and Linguistic Theo-
ries (TLT).
J. Nilsson, J. Hall, and J. Nivre. 2005. MAMBA meets
TIGER: Reconstructing a Swedish treebank from an-
tiquity. In Proc. of the NODALIDA Special Session on
Treebanks.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL?05), pages 99?106, Ann Arbor, June.
Joakim Nivre. 2003. An efficient algorithm for projec-
tive dependency parsing. In Proceedings of the 8th In-
ternational Workshop on Parsing Technologies (IWPT
03), pages 149?160, Nancy, 23-25 April.
K. Oflazer, B. Say, D. Zeynep Hakkani-T?r, and G. T?r.
2003. Building a Turkish treebank. In Abeill?
(Abeill?, 2003), chapter 15.
K. Simov and P. Osenova. 2003. Practical annotation
scheme for an HPSG treebank of Bulgarian. In Proc.
of the 4th Intern. Workshop on Linguistically Inter-
preteted Corpora (LINC), pages 17?24.
K. Simov, P. Osenova, A. Simov, and M. Kouylekov.
2005. Design and implementation of the Bulgarian
HPSG-based treebank. In Journal of Research on Lan-
guage and Computation ? Special Issue, pages 495?
522. Kluwer Academic Publishers.
Lucien Tesni?re. 1966. ?l?ments de syntaxe structurale.
Klincksieck, Paris, 2e edition.
L. van der Beek, G. Bouma, R. Malouf, and G. van No-
ord. 2002. The Alpino dependency treebank. In Com-
putational Linguistics in the Netherlands (CLIN).
210
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 227?230,
Prague, June 2007. c?2007 Association for Computational Linguistics
LTH: Semantic Structure Extraction using Nonprojective Dependency Trees
Richard Johansson and Pierre Nugues
Department of Computer Science, Lund University, Sweden
{richard, pierre}@cs.lth.se
Abstract
We describe our contribution to the SemEval
task on Frame-Semantic Structure Extrac-
tion. Unlike most previous systems de-
scribed in literature, ours is based on depen-
dency syntax. We also describe a fully auto-
matic method to add words to the FrameNet
lexical database, which gives an improve-
ment in the recall of frame detection.
1 Introduction
The existence of links between grammatical rela-
tions and various forms of semantic interpretation
has long been observed; grammatical relations play
a crucial role in theories of linking, i.e. the realiza-
tion of the semantic arguments of predicates as syn-
tactic units (Manning, 1994; Mel?c?uk, 1988). Gram-
matical relations may be covered by many defini-
tions but it is probably easier to use them as an exten-
sion of dependency grammars, where relations take
the form of arc labels. In addition, some linguistic
phenomena such as wh-movement and discontinu-
ous structures are conveniently described using de-
pendency syntax by allowing nonprojective depen-
dency arcs. It has also been claimed that dependency
syntax is easier to understand and to teach to people
without a linguistic background.
Despite these advantages, dependency syntax has
relatively rarely been used in semantic structure ex-
traction, with a few exceptions. Ahn et al (2004)
used a post-processing step to convert constituent
trees into labeled dependency trees that were then
used as input to a semantic role labeler. Pradhan et
al. (2005) used a rule-based dependency parser, but
the results were significantly worse than when using
a constituent parser.
This paper describes a system for frame-semantic
structure extraction that is based on a dependency
parser. The next section presents the dependency
grammar that we rely on. We then give the de-
tails on the frame detection and disambiguation, the
frame element (FE) identification and classification,
and dictionary extension, after which the results and
conclusions are given.
2 Dependency Parsing with the Penn
Treebank
The last few years have seen an increasing interest
in dependency parsing (Buchholz and Marsi, 2006)
with significant improvements of the state of the art,
and dependency treebanks are now available for a
wide range of languages. The parsing algorithms
are comparatively easy to implement and efficient:
some of the algorithms parse sentences in linear time
(Yamada and Matsumoto, 2003; Nivre et al, 2006).
In the semantic structure extraction system, we
used the Stanford part-of-speech tagger (Toutanova
et al, 2003) to tag the training and test sentences and
MaltParser, a statistical dependency parser (Nivre et
al., 2006), to parse them.
We trained the parser on the Penn Treebank (Mar-
cus et al, 1993). The dependency trees used to
train the parser were created from the constituent
trees using a conversion program (Johansson and
Nugues, 2007)1. The converter handles most of
the secondary edges in the Treebank and encodes
those edges as (generally) nonprojective dependency
arcs. Such information is available in the Penn Tree-
bank in the form of empty categories and secondary
edges, it is however not available in the output of
traditional constituent parsers, although there have
been some attempts to apply a post-processing step
to predict it, see Ahn et al (2004), inter alia.
Figures 1 and 2 show a constituent tree from the
Treebank and its corresponding dependency tree.
Note that the secondary edge from the wh-trace to
Why is converted into a nonprojective PRP link.
3 Semantic Structure Extraction
This section describes how the dependency trees are
used to create the semantic structure. The system
1Available at http://nlp.cs.lth.se/pennconverter
227
NPNP
ADVPWHADVP
PRPSBJ
VP
SQ
SBARQ
*T*
Why would intelligent beings kidnap seven Soviet mailmen *T* ?
Figure 1: A constituent tree from the Penn Treebank.
Why would intelligent beings kidnap seven Soviet mailmen ?
NMOD
NMOD
NMOD
OBJ
VC
PROOT?SBARQ
SBJ
PRP
Figure 2: Converted dependency tree.
is divided into two main components: frame detec-
tion and disambiguation, and frame element detec-
tion and classification.
3.1 Frame Detection and Disambiguation
3.1.1 Filtering Rules
Since many potential target words appear in
senses that should not be tagged with a frame, we
use a filtering component as a first step in the frame
detection. We also removed some words (espe-
cially prepositions) that caused significant perfor-
mance degradation because of lack of training data.
With the increasing availability of tagged running
text, we expect that we will be able to replace the
filtering rules with a classifier in the future.
? have was retained only if it had an object,
? be only if it was preceded by there,
? will was removed in its modal sense,
? of course and in particular were removed,
? the prepositions above, against, at, below, be-
side, by, in, on, over, and under were removed
unless their head was marked as locative,
? after and before were removed unless their
head was marked as temporal,
? into, to, and through were removed unless their
head was marked as direction,
? as, for, so, and with were always removed,
? since the only sense of of was PARTITIVE,
we removed it unless it was preceded by only,
member, one, most, many, some, few, part, ma-
jority, minority, proportion, half, third, quar-
ter, all, or none, or if it was followed by all,
group, them, or us.
We also removed all targets that had been tagged
as support verbs for some other target.
3.1.2 Sense Disambiguation
For the target words left after the filtering, we
used a classifier to assign a frame, following
Erk (2005). We trained a disambiguating SVM clas-
sifier on all ambiguous words listed in FrameNet. Its
accuracy was 84% on the ambiguous words, com-
pared to a first-sense baseline score of 74%.
The classifier used the following features: target
lemma, target word, subcategorization frame (for
verb targets only), the set of dependencies of the
target, the set of words of the child nodes, and the
parent word of the target.
The subcategorization frame feature was formed
by concatenating the dependency labels of the chil-
dren, excluding subject, parentheticals, punctuation
and coordinations. For instance, for kidnap in Fig-
ure 2, the feature is PRP+OBJ.
3.1.3 Extending the Lexical Database
Coverage is one of the main weaknesses of the
current FrameNet lexical database ? it lists only
10,197 lexical units, compared to 207,016 word?
sense pairs in WordNet 3.0 (Fellbaum, 1998). We
tried to remedy this problem by training classifiers
to find words that are related to the words in a frame.
We designed a feature representation for each
lemma in WordNet, which uses a sequence of iden-
tifiers for each synset in its hypernym tree. All
senses of the lemma were used, and the features
were weighted with respect to the relative frequency
of the sense. Using this feature representation, we
trained an SVM classifier for each frame that tells
whether a lemma belongs to that frame or not.
The FrameNet dictionary could thus be extended
by 18,372 lexical units. If we assume a Zipf distri-
bution and that the lexical units already in FrameNet
are the most common ones, this would increase the
228
coverage by up to 9%. In the test set, the new lexical
units account for 53 out of the 808 target words our
system detected (6.5%). We roughly estimated the
precision to 70% by manually inspecting 100 ran-
domly selected words in the extended dictionary.
This strategy is most successful when the frame
is equivalent to one or a few synsets (and their
subtrees). For instance, for the frame MEDI-
CAL_CONDITION, we can add the complete sub-
tree of the synset pathological state, resulting in
641 new lemmas referring to all sorts of diseases.
On the other hand, the strategy also works well for
motion verbs (which often exhibit complex patterns
of polysemy): 137 lemmas could be added to the
SELF_MOTION frame. Examples of frames with fre-
quent errors are LEADERSHIP, which includes many
insects (probably because the most frequent sense
of queen in SemCor is the queen bee), and FOOD,
which included many chemical substances as well
as inedible plants and animals.
3.2 Frame Element Extraction
Following convention, we divided the FE extraction
into two subtasks: argument identification and argu-
ment classification. We did not try to assign multiple
labels to arguments. Figure 3 shows an overview. In
addition to detecing the FEs, the argument identifi-
cation classifier detects the dependency nodes that
should be tagged on the layers other than the frame
element layer: SUPP, COP, NULL, EXIST, and ASP.
The ANT and REL labels could be inserted using
simple rules. Similarly to Xue and Palmer (2004),
Argument
identification
FE
Supp
Cop
Asp
Exist
Null
Argument
None
Self_mover
Path
etc
classification
Figure 3: FE extraction steps.
we could filter away many nodes before the argu-
ment identification step by assuming that the argu-
ments for a given predicate correspond to a subset of
the dependents of the target or of its transitive heads.
Both classifiers were implemented using SVMs
and use the following features: target lemma, voice
(for verb targets only), subcategorization frame (for
verb targets only), the set of dependencies of the tar-
get, part of speech of the target node, path through
the dependency tree from the target to the node, po-
sition (before, after, or on), word and part of speech
for the head, word and part of speech for leftmost
and rightmost descendent.
In the path feature, we removed steps through
verb chains and coordination. For instance, in the
sentece I have seen and heard it, the path from heard
to I is only SBJ? and to it OBJ?.
3.3 Named Entity Recognition
In addition to the frame-semantic information, the
SemEval task also scores named entities. We used
YamCha (Kudo and Matsumoto, 2003) to detect
named entities, and we trained it on the SemEval
full-text training sets. Apart from the word and part
of speech, we used suffixes up to length 5 as fea-
tures. We think that results could be improved fur-
ther by using an external NE tagger.
4 Results
The system was evaluated on three texts. Table 1
shows the results for frame detection averaged over
the test texts. In the Setting colums, the first shows
whether Exact or Partial frame matching was used
by the evaluation script, and the second whether La-
bels or Dependencies were used. Table 2 compares
the results of the system using the extended dictio-
nary with one using the orignal FrameNet dictio-
nary, using the Partial matching and Labels scoring.
The extended dictionary introduces some noise and
thus lowers the precision slightly, but the effects on
the recall are positive. Table 3 shows the aver-
Table 1: Results for frame detection.
Setting Recall Precision F1
E L 0.528 0.688 0.597
P L 0.581 0.758 0.657
E D 0.549 0.715 0.621
P D 0.601 0.784 0.681
Table 2: Comparison of dictionaries.
Dictionary Recall Precision F1
Original 0.550 0.767 0.634
Extended 0.581 0.758 0.657
229
aged precision, recall, and F1 measures for differ-
ent evaluation parameters. The third column shows
whether named entities were used (Y) or not (N).
Interestingly, the scores are higher for the seman-
tic dependency graphs than for flat labels, while the
two other teams generally had higher scores for flat
labels. We believe that the reason for this is that we
used a dependency parser, and that the rules that we
used to convert dependency nodes into spans may
have produced some errors. It is possible that the fig-
ures would have been slightly higher if our program
produced semantic dependency graphs directly.
Table 3: Results for frame and FE detection.
Setting Recall Precision F1
E L Y 0.372 0.532 0.438
P L Y 0.398 0.570 0.468
E D Y 0.389 0.557 0.458
P D Y 0.414 0.594 0.488
E L N 0.364 0.530 0.432
P L N 0.391 0.570 0.464
E D N 0.384 0.561 0.456
P D N 0.411 0.600 0.488
5 Conclusion and Future Work
We have presented a system for frame-semantic
structure extraction that achieves promising results.
While most previous systems have been based on
constituents, our system relies on a dependency
parser. We also described an automatic method to
add new units to the FrameNet lexical database.
To improve labeling quality, we would like to ap-
ply constraints to the semantic output so that se-
mantic type and coreness rules are obeyed. In ad-
dition, while the system described here is based on
pipelined classification, recent research on seman-
tic role labeling has shown that significant perfor-
mance improvements can be gained by exploiting
interdependencies between arguments (Toutanova et
al., 2005). With an increasing amount of running
text annotated with frame semantics, we believe that
this insight can be extended to model interdependen-
cies between frames as well.
Our motivation for using dependency grammar is
that we hope that it will eventually make semantic
structure extraction easier to implement and more
theoretically well-founded. How to best design the
dependency syntax is also still an open question.
Ideally, all arguments would be direct dependents of
the predicate node and we could get rid of the sparse
and brittle Path feature in the classifier.
References
David Ahn, Sisay Fissaha, Valentin Jijkoun, and Maarten
de Rijke. 2004. The university of Amsterdam at
Senseval-3: Semantic roles and logic forms. In Pro-
ceedings of SENSEVAL-3.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of the CoNLL-X.
Katrin Erk. 2005. Frame assignment as word sense dis-
ambiguation. In Proceedings of IWCS 6.
Christiane Fellbaum, editor. 1998. WordNet: An elec-
tronic lexical database. MIT Press.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proceedings of NODALIDA 2007. To appear.
Taku Kudo and Yuji Matsumoto. 2003. Fast methods for
kernel-based text analysis. In ACL-2003.
Chistopher Manning. 1994. Ergativity: Argument struc-
ture and grammatical relations.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
Igor A. Mel?c?uk. 1988. Dependency Syntax: Theory and
Practice. State University Press of New York, Albany.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
Parser: A data-driven parser generator for dependency
parsing. In Proceedings of LREC.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James
Martin, and Daniel Jurafsky. 2005. Semantic role la-
beling using different syntactic views. In ACL-2005.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of HLT-NAACL.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2005. Joint learning improves semantic role
labeling. In Proceedings of ACL 2005.
Nianwen Xue and Martha Palmer. 2004. Calibrating fea-
tures for semantic role labeling. In Proc. of EMNLP.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proceedings of IWPT-03.
230
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 561?569,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Statistical Bistratal Dependency Parsing
Richard Johansson
Department of Information Engineering and Computer Science
University of Trento
Trento, Italy
johansson@disi.unitn.it
Abstract
We present an inexact search algorithm for
the problem of predicting a two-layered
dependency graph. The algorithm is based
on a k-best version of the standard cubic-
time search algorithm for projective de-
pendency parsing, which is used as the
backbone of a beam search procedure.
This allows us to handle the complex non-
local feature dependencies occurring in
bistratal parsing if we model the interde-
pendency between the two layers.
We apply the algorithm to the syntactic?
semantic dependency parsing task of the
CoNLL-2008 Shared Task, and we obtain
a competitive result equal to the highest
published for a system that jointly learns
syntactic and semantic structure.
1 Introduction
Numerous linguistic theories assume a multistratal
model of linguistic structure, such as a layer of
surface syntax, deep syntax, and shallow seman-
tics. Examples include Meaning?Text Theory
(Mel?c?uk, 1988), Discontinuous Grammar (Buch-
Kromann, 2006), Extensible Dependency Gram-
mar (Debusmann et al, 2004), and the Functional
Generative Description (Sgall et al, 1986) which
forms the theoretical foundation of the Prague De-
pendency Treebank (Hajic?, 1998).
In the statistical NLP community, the most
widely used grammatical resource is the Penn
Treebank (Marcus et al, 1993). This is a purely
syntactic resource, but we can also include this
treebank in the category of multistratal resources
since the PropBank (Palmer et al, 2005) and
NomBank (Meyers et al, 2004) projects have an-
notated shallow semantic structures on top of it.
Dependency-converted versions of the Penn Tree-
bank, PropBank and NomBank were used in the
CoNLL-2008 Shared Task (Surdeanu et al, 2008),
in which the task of the participants was to pro-
duce a bistratal dependency structure consisting of
surface syntax and shallow semantics.
Producing a consistent multistratal structure is
a conceptually and computationally complex task,
and most previous methods have employed a
purely pipeline-based decomposition of the task.
This includes the majority of work on shallow se-
mantic analysis (Gildea and Jurafsky, 2002, in-
ter alia). Nevertheless, since it is obvious that
syntax and semantics are highly interdependent, it
has repeatedly been suggested that the problems of
syntactic and semantic analysis should be carried
out simultaneously rather than in a pipeline, and
that modeling the interdependency between syn-
tax and semantics would improve the quality of all
the substructures.
The purpose of the CoNLL-2008 Shared Task
was to study the feasibility of a joint analysis
of syntax and semantics, and while most partici-
pating systems used a pipeline-based approach to
the problem, there were a number of contribu-
tions that attempted to take the interdependence
between syntax and semantics into account. The
top-performing system in the task (Johansson and
Nugues, 2008) applied a very simple reranking
scheme by means of a k-best syntactic output,
similar to previous attempts (Gildea and Juraf-
sky, 2002; Toutanova et al, 2005) to improve se-
mantic role labeling performance by using mul-
561
tiple parses. The system by Henderson et al
(2008) extended previous stack-based algorithms
for dependency parsing by using two separate
stacks to build the syntactic and semantic graphs.
Llu??s and Ma`rquez (2008) proposed a model that
simultaneously predicts syntactic and semantic
links, but since its search algorithm could not take
the syntactic?semantic interdependencies into ac-
count, a pre-parsing step was still needed. In ad-
dition, before the CoNLL-2008 shared task there
have been a few attempts to jointly learn syntac-
tic and semantic structure; for instance, Merlo and
Musillo (2008) appended semantic role labels to
the phrase tags in a constituent treebank and ap-
plied a conventional constituent parser to predict
constituent structure and semantic roles.
In this paper, we propose a new approximate
search method for bistratal dependency analysis.
The search method is based on a beam search pro-
cedure that extends a k-best version of the stan-
dard cubic-time search algorithm for projective
dependency parsing. This is similar to the search
method for constituent parsing used by Huang
(2008), who referred to it as cube pruning, in-
spired by an idea from machine translation decod-
ing (Chiang, 2007). The cube pruning approach,
which is normally used to solve the argmax prob-
lem, was also recently extended to summing prob-
lems, which is needed in some learning algorithms
(Gimpel and Smith, 2009).
We apply the algorithm on the CoNLL-2008
Shared Task data, and obtain the same evalua-
tion score as the best previously published system
that simultaneously learns syntactic and semantic
structure (Titov et al, 2009).
2 Bistratal Dependency Parsing
In the tradition of dependency representation of
sentence structure, starting from Tesnie`re (1959),
the linguistic structure of the sentence is repre-
sented as a directed graph of relations between
words. In most theories, certain constraints are im-
posed on this graph; the most common constraint
on dependency graphs in syntax, for instance, is
that the graph should form a tree (i.e. it should be
connected, acyclic, and every node should have at
most one incoming edge). This assumption un-
derlies almost all dependency parsing, although
there are also a few parsers based on slightly more
general problem formulations (Sagae and Tsuji,
2008).
In this paper, we assume a different type of con-
straint: that the graph can be partitioned into two
subgraphs that we will refer to as strata or layers,
where the first of the layers forms a tree. For the
second layer, the only assumption we make is that
there is at most one link between any two words.
However, we believe that for any interesting lin-
guistic structure, the second layer will be highly
dependent on the structure of the first layer.
Figure 1 shows an example of a bistratal depen-
dency graph such as in the CoNLL-2008 Shared
Task on syntactic and semantic dependency pars-
ing. The figure shows the representation of the
sentence We were expecting prices to fall. The pri-
mary layer represents surface-syntactic relations,
shown above the sentence, and the secondary layer
consists of predicate?argument links (here, we
have two predicates expecting and fall).
SBJ
ROOT
We were expecting prices to fall
VC IM
OPRD
OBJ
C?A1
A1 A1
A0
Figure 1: Example of a bistratal dependency
graph.
We now give a formal model of the statistical
parsing problem of prediction of a bistratal depen-
dency graph. For a given input sentence x, the task
of our algorithm is to predict a structure y? consist-
ing of a primary layer y?
p
and a secondary layer
y?
s
. In a discriminative modeling framework, we
model this prediction problem as the search for the
highest-scoring output from the candidate space Y
under a scoring function F :
?y?
p
, y?
s
? = argmax
?y
p
,y
s
??Y
F (x, y
p
, y
s
)
The learning problem consists of searching in the
model space for a scoring function F that mini-
mizes the cost of predictions on unseen examples
according to a given cost function ?. In this work,
we consider linear scoring functions of the follow-
ing form:
F (x, y
p
, y
s
) = w ??(x, y
p
, y
s
)
where ?(x, y) is a numeric feature representation
of the tuple (x, y
p
, y
s
) and w a high-dimensional
vector of feature weights.
562
Based on the structural assumptions made
above, we now decompose the feature represen-
tation into three parts:
? = ?
p
+?
i
+?
s
Here, ?
p
represents the primary layer, assumed to
be a tree, ?
s
the secondary layer, and finally ?
i
is the representation of the interdependency be-
tween the layers. For the feature representations
of the primary and secondary layers, we employ
edge factorization, a decomposition widely used
in statistical dependency parsing, and assume that
all edges can be scored independently:
?
p
(x, y
p
) =
?
f?y
p
?
p
(x, f)
The representation of the interdependency be-
tween the layers assumes that each secondary link
is dependent on the primary layer, but independent
of other secondary links.
?
i
(x, y
p
, y
s
) =
?
f?y
s
?
i
(x, f, y
p
)
The interdependency between layers is the bottle-
neck for the search algorithm that we will present
in Section 3. For semantic role analysis, this in-
volves all features that rely on a syntactic repre-
sentation, most importantly the PATH feature that
represents the grammatical relation between pred-
icate and argument words. For instance, in Fig-
ure 1, we can represent the surface-syntactic re-
lation between the tokens fall and prices as the
string IM?OPRD?OBJ?. In this work, all interde-
pendency features will be based on paths in the
primary layer.
3 A Bistratal Search Algorithm
This section presents an algorithm to approxi-
mately solve the argmax problem for prediction
of bistratal dependency structures. We present the
algorithm in two steps: first, we review a k-best
version of the standard search algorithm for pro-
jective monostratal dependency parsing, based on
the work by Huang and Chiang (2005).1 In the
second step, starting from the k-best monostratal
search, we devise a search method for the bistratal
problem.
1Huang and Chiang (2005) described an even more effi-
cient k-best algorithm based on lazy evaluation, which we
will not use here since it is not obviously adaptable to the
situation where the search is inexact.
3.1 Review of k-Best Dependency Parsing
The search method commonly used in dependency
parsers is a chart-based dynamic programming al-
gorithm that finds the highest-scoring projective
dependency tree under an edge-factored scoring
function. It runs in cubic time with respect to the
sentence length. In a slightly more general for-
mulation, it was first published by Eisner (1996).
Starting from McDonald et al (2005), it has been
widely used in recent statistical dependency pars-
ing frameworks.
The algorithm works by creating open struc-
tures, which consist of a dependency link and the
set of links that it spans, and closed structures,
consisting of the left or right half of a complete
subtree. An open structure is created by a proce-
dure LINK that adds a dependency link to connect
a right-pointing and a left-pointing closed struc-
ture, and a closed structure by a procedure JOIN
that joins an open structure with a closed structure.
Figure 2 shows schematic illustrations: a LINK
operation connects the right-pointing closed struc-
ture between s and j with the left-pointing closed
structure between j + 1 and e, and a JOIN oper-
ation connects an open structure between s and j
with a closed structure between j and e.
es j j+1 es j
Figure 2: Illustrations of the LINK and JOIN oper-
ations.
The search algorithm can easily be extended to
find the k best parses, not only the best one. In
k-best parsing, we maintain a k-best list in every
cell in the dynamic programming table. To create
the k-best list of derivations for an open structure
between the positions s and e, for instance, there
are up to |L| ? (e ? s) ? k2 possible combinations
to consider if the set of allowed labels is L. The
key observation by Huang and Chiang (2005) is to
make use of the fact that the lists are sorted. For
every position between s and e, we add the best
combination to a priority queue, from which we
then repeatedly remove the front item. For every
item we remove, we add three successors: an item
with a next-best left part, an item with a next-best
right part, and finally an item with a next-best edge
563
label.
The pseudocode of the search algorithm for
k-best dependency parsing is given in Algo-
rithms 1 and 2. For brevity, we omitted the
code for ADVANCE-LEFT and ADVANCE-RIGHT,
which are similar to ADVANCE-EDGE, as well as
ADVANCE-LOWER, which resembles ADVANCE-
UPPER. The FST function used in the pseudocode
returns the first element of a tuple.
The algorithm uses a priority queue with stan-
dard operations ENQUEUE, which enqueues an
element, and DEQUEUE, which removes the
highest-scoring item from the queue. With a stan-
dard binary heap implementation of the priority
queue, these two operations execute in logarithmic
time. To build the queue, we use a constant-time
TOSS operation, which appends an item to the
queue without enforcing the priority queue con-
straint, and a HEAPIFY operation that constructs a
consistent priority queue in linear time.
3.2 Extension to Bistratal Dependency
Parsing
The k-best algorithm forms the core of the inexact
bistratal search algorithm. Our method is similar
to the forest reranking method by Huang (2008),
although there is no forest pruning or reranking in-
volved here. Crucially, we divide the features into
local features, which can be computed ?offline?,
and nonlocal features, which must be computed
during search. In our case, the local features are
?
p
and ?
s
, while the nonlocal features are the in-
terdependent features ?
i
.
Algorithm 3 shows pseudocode for the main
part of the bistratal search algorithm, and Algo-
rithm 4 for its support functions. The algorithm
works as follows: for every span ?s, e?, the algo-
rithm first uses the LINK procedure from the k-
best monostratal search to construct a k-best list of
open structures without semantic links. In the next
step, secondary links are added in the procedure
LINK-SECONDARY. For brevity, we show only
the procedures that create open structures; they are
very similar to their closed-structure counterparts.
The LINK-SECONDARY procedure starts by
creating an initial candidate (FIRST-SEC-OPEN)
based on the best open structure for the primary
layer. FIRST-SEC-OPEN creates the candidate
space for secondary links for a single primary
open structure. To reduce search complexity, it
makes use of a problem-specific function SCOPE
Algorithm 1 k-best search algorithm for depen-
dency parsing.
function k-BEST-SEARCH(k)
n? length of the sentence
initialize the table O of open structures
initialize the table C of closed structures
for m ? [1, . . . , n]
for s ? [0, . . . , n?m]
LINK(s, s + m,?, k)
LINK(s, s + m,?, k)
JOIN(s, s + m,?, k)
JOIN(s, s + m,?, k)
return C[0, n,?]
procedure LINK(s, e, dir, k)
E ? CREATE-EDGES(s,e, dir, k)
q ? empty priority queue
for j ? [s, . . . , e? 1]
l ? C[s, j,?]
r ? C[j + 1, e,?]
o? CREATE-OPEN(E,l, r, 1, 1, 1)
TOSS(q, o)
HEAPIFY(q)
while |O[s, e, dir]| < k and |q| > 0
o? DEQUEUE(q)
if o /? O[s, e, dir]
APPEND(O[s, e, dir], o)
ENQUEUE(q,ADVANCE-EDGE(o))
ENQUEUE(q,ADVANCE-LEFT(o))
ENQUEUE(q,ADVANCE-RIGHT(o))
procedure JOIN(s, e, dir, k)
q ? empty priority queue
if dir =?
for j ? [s + 1, . . . , e]
u? O[s, j,?]
l? C[j, e,?]
c? CREATE-CLOSED(u, l, 1, 1)
TOSS(q, c)
else
for j ? [s, . . . , e? 1]
u? O[j, e,?]
l? C[s, j,?]
c? CREATE-CLOSED(u, l, 1, 1)
TOSS(q, c)
HEAPIFY(q)
while |C[s, e, dir]| < k and |q| > 0
c? DEQUEUE(q)
if c /? C[s, e, dir]
APPEND(C[s, e, dir], c)
ENQUEUE(q,ADVANCE-UPPER(c))
ENQUEUE(q,ADVANCE-LOWER(c))
that defines which secondary links are possible
from a given token, given a primary-layer context.
An important insight by Huang (2008) is that
nonlocal features should be computed as early as
possible during search. In our case, we assume
that the interdependency features are based on tree
paths in the primary layer. This means that sec-
ondary links between two tokens can be added
when there is a complete path in the primary layer
between the tokens. When we create an open
564
Algorithm 2 Support operations for the k-best
search.
function CREATE-EDGES(s,e, dir, k)
E ? ?
for l ? ALLOWED-LABELS(s,e, dir)
score
L
? w ? ?
p
(s, e, dir, l)
edge? ?score
L
, s, e, dir, l?
APPEND(E, edge)
return the top k edges in E
function CREATE-OPEN(E,l, r, i
e
, i
l
, i
r
)
score
L
? FST(E[i
e
]) + FST(l[i
l
]) + FST(r[i
r
])
return ?score
L
+ score
N
, E, l, r, i
e
, i
l
, i
r
?
function CREATE-CLOSED(u,l, i
u
, i
r
)
score
L
? FST(u[i
u
]) + FST(l[i
l
])
return ?score
L
+ score
N
, u, l, i
u
, i
l
?
function ADVANCE-EDGE(o)
where o = (score,E, l, r, i
e
, i
l
, i
r
)
if i
e
= LENGTH(E)
return ?
else
return CREATE-OPEN(E,l, r, i
e
+ 1, i
l
, i
r
)
function ADVANCE-UPPER(c)
where c = (u, l, i
u
, i
l
)
if i
u
= LENGTH(u)
return ?
else
return CREATE-CLOSED(u,l, i
u
+ 1, i
l
)
structure by adding a link between two substruc-
tures, a complete path is created between the to-
kens in the substructures. We thus search for pos-
sible secondary links only between the two sub-
structures that are joined.
Figure 3 illustrates this process. A primary open
structure between s and e has been created by
adding a link from the right-pointing closed struc-
ture between s and j to the left-pointing closed
structure between j + 1 and e. We now try to
add secondary links between the two substruc-
tures. For instance, in the semantic role parsing
task described in subsection 3.3, if we know that
there is a predicate between s and j, then we look
for arguments between j + 1 and e, i.e. we apply
the SCOPE function to the right substructure.
When computing the scores for secondary links,
note that for efficiency only the interdependent
part ?
i
should be computed in CREATE-SEC-
EDGES; the part of the score that does not depend
on the primary layer can be computed before en-
tering the search procedure.
es j j+1
p a
Figure 3: Illustration of the secondary linking pro-
cess: When two substructures are connected, we
can compute the path between a predicate in the
left substructure and an argument in the right sub-
structure.
Algorithm 3 Search algorithm for bistratal depen-
dency parsing.
function BISTRATAL-SEARCH(k)
n? length of the sentence
initialize the table O of open structures
initialize the table C of closed structures
using ?
s
, compute a table scores
s
for all
possible secondary edges ?h, d, l?
for m ? [1, . . . , n]
for s ? [0, . . . , n?m]
LINK(s, s + m,?, k)
LINK-SECONDARY(s,s + m,?, k)
LINK(s, s + m,?, k)
LINK-SECONDARY(s,s + m,?, k)
JOIN(s, s + m,?, k)
JOIN-SECONDARY(s,s + m,?, k)
JOIN(s, s + m,?, k)
JOIN-SECONDARY(s,s + m,?, k)
return FIRST(C[0, n,?])
procedure LINK-SECONDARY(s,e, dir, k)
q ? empty priority queue
o? FIRST-SEC-OPEN(O[s,e, dir], 1, k)
ENQUEUE(q, o)
buf ? empty list
while |buf | < k and |q| > 0
o? DEQUEUE(q)
if o /? buf
APPEND(buf, o)
for o? ? ADVANCE-SEC-OPEN(o, k)
ENQUEUE(q,o?)
SORT(buf) to O[s, e, dir]
3.3 Application on the CoNLL-2008 Shared
Task Treebank
We applied the bistratal search method in Algo-
rithm 3 on the data from the CoNLL-2008 Shared
Task (Surdeanu et al, 2008). Here, the primary
layer is the tree of surface-syntactic relations such
as subject and object, and the secondary layer con-
tains the links between the predicate words in the
sentence and their respective logical arguments,
such as agent and patient. The training corpus con-
sists of sections 02 ? 21 of the Penn Treebank, and
contains roughly 1 million words.
565
Algorithm 4 Support operations in bistratal
search.
function FIRST-SEC-OPEN(L,i
L
, k)
if i = LENGTH(L)
return ?
l?GET-LEFT(L[i
L
]), r ?GET-RIGHT(L[i
L
])
for h ? [START(l), . . . , END(l)]
for d ? SCOPE(r, h)]
E[h][d]? CREATE-SEC-EDGES(h, d, L[i
L
], k)]
I
E
[h][d]? 1
for h ? [START(r), . . . , END(r)]
for d ? SCOPE(l, h)]
E[h][d]? CREATE-SEC-EDGES(h, d, L[i
L
], k)]
I
E
[h][d]? 1
return CREATE-SEC-OPEN(L, i
L
, E, I)
function CREATE-SEC-EDGES(h,d, o, k)
E ? ?
for l ? ALLOWED-SEC-LABELS(h,d)
score? w ? ?
i
(h, d, l, o) + scores
s
[h, d, l]
edge? ?score, h, d, l?
APPEND(E, edge)
return the top k edges in E
function CREATE-SEC-OPEN(L,i
L
, E, I)
score? FST(L[i
L
]) +
?
h,d
FST(E[h, d, I
E
[h, d]])
return ?score, L, i
L
, E, I
E
?
function ADVANCE-SEC-OPEN(o,k)
where o = ?score, L, i
L
, E, I
E
?
buf ? ?
if i
L
< LENGTH(L) and I
E
= [1, . . . , 1]
APPEND(buf, FIRST-SEC-OPEN(L, i
L
+ 1, k))
for h, d
if I
E
[h, d] < LENGTH(E[h, d])
I
?
E
? COPY(I
E
)
I
?
E
[h, d]? I
?
E
[h, d] + 1
APPEND(buf, CREATE-SEC-OPEN(L, i
L
, E, I
?
E
))
return buf
To apply the bistratal search algorithm to
the problem of syntactic?semantic parsing, a
problem-specific implementation of the SCOPE
function is needed. In this case, we made two as-
sumptions. First, we assumed that the identities
of the predicate words are known a priori2. Sec-
ondly, we assumed that every argument of a given
predicate word is either a direct dependent of the
predicate, one of its ancestors, or a direct depen-
dent of one of its ancestors. This assumption is a
simple adaptation of the pruning algorithm by Xue
and Palmer (2004), and it holds for the vast major-
ity of arguments in the CoNLL-2008 data; in the
training set, we measured that this covers 99.04%
of the arguments of verbs and 97.55% of the argu-
2Since our algorithm needs to know the positions of the
predicates, we trained a separate classifier using the LIBLIN-
EAR toolkit (Fan et al, 2008) to identify the predicate words.
As features for the classifier, we used the words and part-of-
speech tags in a ?3 window around the word under consid-
eration.
ments of nouns.
Figure 4 shows an example of how the SCOPE
function works in our case. If a predicate is con-
tained in the right substructure, we find two po-
tential arguments: one at the start of the left sub-
structure, and one more by recursively searching
the left structure.
pa a21
Figure 4: Illustration of the SCOPE function for
predicate?argument links. If the right substructure
contains a predicate, we can find potential argu-
ments in the left substructure.
While the primary layer is assumed to be pro-
jective in Algorithm 3, the syntactic trees in the
CoNLL-2008 data have a small number of nonpro-
jective links. We used a pseudo-projective edge la-
bel encoding to handle nonprojectivity (Nivre and
Nilsson, 2005).
To implement the model, we constructed fea-
ture representations ?
p
, ?
s
, and ?
i
. The surface-
syntactic representation ?
p
was a standard first-
order edge factorization using the same features
as McDonald et al (2005). The features in?
s
and
?
i
are shown in Table 1 and are standard features
in statistical semantic role classification.
?
s
?
i
Predicate word Path
Predicate POS Path + arg. POS
Argument word Path + pred. POS
Argument POS Path + arg. word
Pred. + arg. words Path + pred. word
Predicate word + label Path + label
Predicate POS + label Path + arg. POS + label
Argument word + label Path + pred. POS + label
Argument POS + label Path + arg. word + label
Pred. + arg. words + label Path + pred. word + label
Table 1: Feature representation for secondary
links.
We trained the discriminative model using
the Online Passive?aggressive algorithm (Cram-
mer et al, 2006), which is an efficient online
learning method that can be used to train mod-
els for learning problems with structured out-
put spaces. A cost function ? is needed in the
learning algorithm; we decomposed it into a pri-
566
mary part ?
p
and a secondary part ?
s
. We com-
puted the primary part as the sum of link errors:
?
p
(y
p
, y?
p
) =
?
l?y?
p
c
p
(l, y
p
), where
c
p
(l, y
p
) =
0 if l ? y
p
and its label is correct
0.5 if l ? y
p
but its label is incorrect
1 if l /? y
p
In a similar vein, we computed the secondary part
?
s
of the cost function as #fp+#fn+0.5 ?#fl,
where #fp is the number of false positive sec-
ondary links, #fn the number of false negative
links, and #fl the number of links with correct
endpoints but incorrect label.
The training procedure took roughly 24 hours
on an 2.3 GHz AMD Athlon processor. The mem-
ory consumption was about 1 GB during training.
4 Experiments
We evaluated the performance of our system on
the test set from the CoNLL-2008 shared task,
which consists of section 23 of the WSJ part of
the Penn Treebank, as well as a small part of the
Brown corpus. A beam width k of 4 was used
in this experiment. Table 2 shows the results of
the evaluation. The table shows the three most
important scores computed by the official evalua-
tion script: labeled syntactic dependency accuracy
(LAS), labeled semantic dependency F
1
-measure
(Sem. F1), and the macro-averaged F
1
-measure, a
weighted combination of the syntactic and seman-
tic scores (M. F1). Our result is competitive; we
obtain the same macro F1 as the newly published
result by Titov et al (2009), which is the high-
est published figure for a joint syntactic?semantic
parser so far. Importantly, our system clearly out-
performs the system by Llu??s and Ma`rquez (2008),
which is the most similar system in problem mod-
eling, but which uses a different search strategy.
System LAS Sem. F1 M. F1
This paper 86.6 77.1 81.8
Titov et al (2009) 87.5 76.1 81.8
H. et al(2008) 87.6 73.1 80.5
L. & M. (2008) 85.8 70.3 78.1
Table 2: Results of published joint syntactic?
semantic parsers on the CoNLL-2008 test set.
Since the search procedure is inexact, it is im-
portant to quantify roughly how much of a detri-
mental impact the approximation has on the pars-
ing quality. We studied the influence of the beam
width parameter k on the performance of the
parser. The results on the development set can be
seen in Table 3. As can be seen, a modest increase
in performance can be obtained by increasing the
beam width, at the cost of increased parsing time.
k LAS Sem. F1 M. F1 Time
1 85.14 77.05 81.10 242
2 85.43 77.17 81.30 369
4 85.49 77.20 81.35 625
8 85.58 77.20 81.40 1178
Table 3: Influence of beam width on parsing accu-
racy.
In addition, to have a rough indication of the im-
pact of search errors on the quality of the parses,
we computed the fraction of sentences where the
gold-standard parse had a higher score accord-
ing to the model than the parse returned by the
search3. Table 4 shows the results of this exper-
iment. This suggests that the search errors, al-
though they clearly have an impact, are not the ma-
jor source of errors, even with small beam widths.
k Fraction
1 0.121
2 0.104
4 0.096
8 0.090
Table 4: Fraction of sentences in the development
set where the gold-standard parse has a higher
score than the parse returned by the search pro-
cedure.
To investigate where future optimization efforts
should be spent, we used the built-in hprof pro-
filing tool of Java to locate the bottlenecks. Once
again, we ran the program on the development
set with a beam width of 4, and Table 5 shows
the three types of operations where the algorithm
spent most of its time. It turns out that 74% of the
time was spent on the computation and scoring of
interdependency features. To make our algorithm
truly useful in practice, we thus need to devise a
way to speed up or cache these computations.
3To be able to compare the scores of the gold-standard
and predicted parses, we disabled the automatic classifier for
predicate identification and provided the parser with gold-
standard predicates in this experiment.
567
Operation Fraction
w ? ?
i
0.64
Queue operations 0.15
Computation of ?
i
0.10
Table 5: The three most significant bottlenecks
and their fraction of the total runtime.
5 Discussion
In this paper, we have presented a new approxi-
mate search method to solve the problem of jointly
predicting the two layers in a bistratal dependency
graph. The algorithm shows competitive perfor-
mance on the treebank used in the CoNLL-2008
Shared Task, a bistratal treebank consisting of a
surface-syntactic and a shallow semantic layer. In
addition to the syntactic?semantic task that we
have described in this paper, we believe that our
method can be used in other types of multistratal
syntactic frameworks, such as a representation of
surface and deep syntax as in Meaning?Text The-
ory (Mel?c?uk, 1988).
The optimization problem that we set out to
solve is intractable, but we have shown that rea-
sonable performance can be achieved with an in-
exact, beam search-based search method. This is
not obvious: it has previously been shown that us-
ing an inexact search procedure when the learn-
ing algorithm assumes that the search is exact
may lead to slow convergence or even divergence
(Kulesza and Pereira, 2008), but this does not
seem to be a problem in our case.
While we used a beam search method as the
method of approximation, other methods are cer-
tainly possible. An interesting example is the re-
cent system by Smith and Eisner (2008), which
used loopy belief propagation in a dependency
parser using highly complex features, while still
maintaining cubic-time search complexity.
An obvious drawback of our approach com-
pared to traditional pipeline-based semantic role
labeling methods is that the speed of the algo-
rithm is highly dependent on the size of the in-
terdependency feature representation ?
i
. Also,
extracting these features is fairly complex, and it
is of critical importance to implement the feature
extraction procedure efficiently since it is one of
the bottlenecks of the algorithm. It is plausible
that our performance suffers from the absence of
other frequently used syntax-based features such
as dependent-of-dependent and voice.
It is thus highly dubious that a joint modeling
of syntactic and semantic structure is worth the
additional implementational effort. So far, no sys-
tem using tightly integrated syntactic and semantic
processing has been competitive with the best sys-
tems, which have been either completely pipeline-
based (Che et al, 2008; Ciaramita et al, 2008)
or employed only a loose syntactic?semantic cou-
pling (Johansson and Nugues, 2008). It has been
conjectured that modeling the semantics of the
sentence would also help in syntactic disambigua-
tion; however, it is likely that this is already im-
plicitly taken into account by the lexical features
present in virtually all modern parsers.
In addition, a problem that our beam search
method has in common with the constituent pars-
ing method by Huang (2008) is that highly non-
local features must be computed late. In our case,
this means that if there is a long distance between a
predicate and an argument, the secondary link be-
tween them will be unlikely to influence the final
search result.
Acknowledgements
The author is grateful for the helpful comments by
the reviewers. This work has been funded by the
LivingKnowledge project under the seventh EU
framework program.
References
Matthias Buch-Kromann. 2006. Discontinuous Gram-
mar. A dependency-based model of human parsing
and language learning. Ph.D. thesis, Copenhagen
Business School.
Wanxiang Che, Zhenghua Li, Yuxuan Hu, Yongqiang
Li, Bing Qin, Ting Liu, and Sheng Li. 2008. A
cascaded syntactic and semantic dependency pars-
ing system. In CoNLL 2008: Proceedings of the
Twelfth Conference on Natural Language Learning.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Massimiliano Ciaramita, Giuseppe Attardi, Felice
Dell?Orletta, and Mihai Surdeanu. 2008. DeSRL:
A linear-time semantic role labeling system. In Pro-
ceedings of the Shared Task Session of CoNLL-2008.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Schwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research, 2006(7):551?585.
Ralph Debusmann, Denys Duchier, Alexander Koller,
Marco Kuhlmann, Gert Smolka, and Stefan Thater.
568
2004. A relational syntax-semantics interface based
on dependency grammar. In Proceedings of the
20th International Conference on Computational
Linguistics (COLING 2004).
Jason M. Eisner. 1996. Three new probabilistic mod-
els for dependency parsing: An exploration. In Pro-
ceedings of the 16th International Conference on
Computational Linguistics, pages 340?345.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
Kevin Gimpel and Noah A. Smith. 2009. Cube
summing, approximate inference with non-local fea-
tures, and dynamic programming without semirings.
In Proceedings of the Twelfth Conference of the Eu-
ropean Chapter of the Association for Computa-
tional Linguistics (EACL).
Jan Hajic?. 1998. Building a syntactically annotated
corpus: The Prague Dependency Treebank. In Is-
sues of Valency and Meaning, pages 106?132.
James Henderson, Paola Merlo, Gabriele Musillo, and
Ivan Titov. 2008. A latent variable model of
synchronous parsing for syntactic and semantic de-
pendencies. In CoNLL 2008: Proceedings of the
Twelfth Conference on Natural Language Learning.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the 9th International
Workshop on Parsing Technologies (IWPT 2005).
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL-08: HLT, pages 586?594.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic?semantic analysis with
PropBank and NomBank. In Proceedings of the
Shared Task Session of CoNLL-2008.
Alex Kulesza and Fernando Pereira. 2008. Structured
learning with approximate inference. In Advances
in Neural Information Processing Systems 20.
Xavier Llu??s and Llu??s Ma`rquez. 2008. A joint model
for parsing syntactic and semantic dependencies. In
CoNLL 2008: Proceedings of the Twelfth Confer-
ence on Natural Language Learning.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of 43rd Annual
Meeting of the Association for Computational Lin-
guistics (ACL?05), pages 91?98.
Igor A. Mel?c?uk. 1988. Dependency Syntax: Theory
and Practice. State University Press of New York.
Paola Merlo and Gabriele Musillo. 2008. Semantic
parsing for high-precision semantic role labelling.
In Proceedings of the 12th Conference on Computa-
tional Natural Language Learning (CoNLL?2008).
Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishman. 2004. The NomBank project:
An interim report. In HLT-NAACL 2004 Workshop:
Frontiers in Corpus Annotation.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-
projective dependency parsing. In Proceedings of
the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL?05).
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Kenji Sagae and Jun?ichi Tsuji. 2008. Shift?reduce
dependency DAG parsing. In Proceedings of the
22nd International Conference on Computational
Linguistics (Coling 2008).
Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?. 1986.
The Meaning of the Sentence in Its Semantic and
Pragmatic Aspects. Dordrecht:Reidel Publishing
Company and Prague:Academia.
David Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing (EMNLP), Honolulu, United
States.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The
CoNLL?2008 shared task on joint parsing of syn-
tactic and semantic dependencies. In Proceedings
of CoNLL?2008.
Lucien Tesnie`re. 1959. ?Ele?ments de syntaxe struc-
turale. Klincksieck, Paris.
Ivan Titov, James Henderson, Paola Merlo, and
Gabriele Musillo. 2009. Online graph planarisation
for synchronous parsing of semantic and syntactic
dependencies. In Proceedings of the International
Joint Conferences on Artificial Intelligence.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2005. Joint learning improves semantic
role labeling. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Lin-
guistics (ACL?05), pages 589?596.
Nianwen Xue and Martha Palmer. 2004. Calibrating
features for semantic role labeling. In Proceedings
of the 2004 Conference on Empirical Methods in
Natural Language Processing, pages 88?94.
569
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 1?18,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
The CoNLL-2009 Shared Task:
Syntactic and Semantic Dependencies in Multiple Languages
Jan Hajic?? Massimiliano Ciaramita? Richard Johansson? Daisuke Kawahara?
Maria Anto`nia Mart???? Llu??s Ma`rquez?? Adam Meyers?? Joakim Nivre?? Sebastian Pado???
Jan ?Ste?pa?nek? Pavel Stran?a?k? Mihai Surdeanu?? Nianwen Xue?? Yi Zhang??
?: Charles University in Prague, {hajic,stepanek,stranak}@ufal.mff.cuni.cz
?: Google Inc., massi@google.com
?: University of Trento, johansson@disi.unitn.it
?: National Institute of Information and Communications Technology, dk@nict.go.jp
??: University of Barcelona, amarti@ub.edu
??: Technical University of Catalonia, Barcelona, lluism@lsi.upc.edu
??: New York University, meyers@cs.nyu.edu
??: Uppsala University and Va?xjo? University, joakim.nivre@lingfil.uu.se
??: Stuttgart University, pado@ims.uni-stuttgart.de
??: Stanford University, mihais@stanford.edu
??: Brandeis University, xuen@brandeis.edu
??: Saarland University, yzhang@coli.uni-sb.de
Abstract
For the 11th straight year, the Conference
on Computational Natural Language Learn-
ing has been accompanied by a shared task
whose purpose is to promote natural language
processing applications and evaluate them in
a standard setting. In 2009, the shared task
was dedicated to the joint parsing of syntac-
tic and semantic dependencies in multiple lan-
guages. This shared task combines the shared
tasks of the previous five years under a unique
dependency-based formalism similar to the
2008 task. In this paper, we define the shared
task, describe how the data sets were created
and show their quantitative properties, report
the results and summarize the approaches of
the participating systems.
1 Introduction
Every year since 1999, the Conference on Com-
putational Natural Language Learning (CoNLL)
launches a competitive, open ?Shared Task?. A
common (?shared?) task is defined and datasets are
provided for its participants. In 2004 and 2005, the
shared tasks were dedicated to semantic role label-
ing (SRL) in a monolingual setting (English). In
2006 and 2007 the shared tasks were devoted to
the parsing of syntactic dependencies, using corpora
from up to 13 languages. In 2008, the shared task
(Surdeanu et al, 2008) used a unified dependency-
based formalism, which modeled both syntactic de-
pendencies and semantic roles for English. The
CoNLL-2009 Shared Task has built on the 2008 re-
sults by providing data for six more languages (Cata-
lan, Chinese, Czech, German, Japanese and Span-
ish) in addition to the original English1. It has thus
naturally extended the path taken by the five most
recent CoNLL shared tasks.
As in 2008, the CoNLL-2009 shared task com-
bined dependency parsing and the task of identify-
ing and labeling semantic arguments of verbs (and
other parts of speech whenever available). Partici-
pants had to choose from two tasks:
? Joint task (syntactic dependency parsing and
semantic role labeling), or
? SRL-only task (syntactic dependency parses
have been provided by the organizers, using
state-of-the art parsers for the individual lan-
guages).
1There are some format changes and deviations from the
2008 task data specification; see Sect. 2.3
1
In contrast to the previous year, the evaluation data
indicated which words were to be dealt with (for the
SRL task). In other words, (predicate) disambigua-
tion was still part of the task, whereas the identi-
fication of argument-bearing words was not. This
decision was made to compensate for the significant
differences between languages and between the an-
notation schemes used.
The ?closed? and ?open? challenges have been
kept from last year as well; participants could have
chosen one or both. In the closed challenge, systems
had to be trained strictly with information contained
in the given training corpus; in the open challenge,
systems could have been developed making use of
any kind of external tools and resources.
This paper is organized as follows. Section 2 de-
fines the task, including the format of the data, the
evaluation metrics, and the two challenges. A sub-
stantial portion of the paper (Section 3) is devoted
to the description of the conversion and develop-
ment of the data sets in the additional languages.
Section 4 shows the main results of the submitted
systems in the Joint and SRL-only tasks. Section 5
summarizes the approaches implemented by partic-
ipants. Section 6 concludes the paper. In all sec-
tions, we will mention some of the differences be-
tween last year?s and this year?s tasks while keeping
the text self-contained whenever possible; for details
and observations on the English data, please refer to
the overview paper of the CoNLL-2008 Shared Task
(Surdeanu et al, 2008) and to the references men-
tioned in the sections describing the other languages.
2 Task Definition
In this section we provide the definition of the shared
task; after introducing the two challenges and the
two tasks the participants were to choose, we con-
tinue with the format of the shared task data, fol-
lowed by a description of the evaluation metrics
used.
For three of the languages (Czech, English and
German), out-of-domain data (OOD) have also been
prepared for the final evaluation, following the same
guidelines and formats.
2.1 Closed and Open Challenges
Similarly to the CoNLL-2005 and CoNLL-2008
shared tasks, this shared task evaluation is separated
into two challenges:
Closed Challenge The aim of this challenge was to
compare performance of the participating systems in
a fair environment. Systems had to be built strictly
with information contained in the given training cor-
pus, and tuned with the development section. In
addition, the lexical frame files (such as the Prop-
Bank and NomBank for English, the valency dictio-
nary PDT-Vallex for Czech etc.) were provided and
may have been used. These restrictions mean that
outside parsers (not trained by the participants? sys-
tems) could not be used. However, we did provide
the output of a single, state-of-the-art dependency
parser for each language so that participants could
build a SRL-only system (using the provided parses
as inputs) within the closed challenge (as opposed to
the 2008 shared task).
Open Challenge Systems could have been devel-
oped making use of any kind of external tools and
resources. The only condition was that such tools or
resources must not have been developed with the an-
notations of the test set, both for the input and output
annotations of the data. In this challenge, we were
interested in learning methods which make use of
any tools or resources that might improve the per-
formance. The comparison of different systems in
this setting may not be fair, and thus ranking of sys-
tems is not necessarily important.
2.2 Joint and SRL-only tasks
In 2008, systems participating in the open challenge
could have used state-of-the-art parsers for the syn-
tactic dependency part of the task. This year, we
have provided the output of these parsers for all the
languages in an uniform way, thus allowing an or-
thogonal combination of the two tasks and the two
challenges. For the SRL-only task, participants in
the closed challenge simply had to use the provided
parses only.
Despite the provisions for the SRL-only task, we
are more interested in the approaches and results of
the Joint task. Therefore, primary system ranking is
provided for the Joint task while additional measures
2
are computed for various combinations of parsers
and SRL methods across the tasks and challenges.
2.3 Data Format
The data format used in this shared task has been
based on the CoNLL-2008 shared task, with some
differences. The data follows these general rules:
? The files contain sentences separated by a blank
line.
? A sentence consists of one or more tokens and
the information for each token is represented on
a separate line.
? A token consists of at least 14 fields. The fields
are separated by one or more whitespace char-
acters (spaces or tabs). Whitespace characters
are not allowed within fields.
The data is thus a large table with whitespace-
separated fields (columns). The fields provided in
the data are described in Table 1. They are identical
for all languages, but they may differ in contents;
for example, some fields might not be filled for all
the languages provided (such as the FEAT or PFEAT
fields).
For the SRL-only task, participants have been
provided will all the data but the PRED and
APREDs, which they were supposed to fill in with
their correct values. However, they did not have
to determine which tokens are predicates (or more
precisely, which are the argument-bearing tokens),
since they were marked by ?Y? in the FILLPRED
field.
For the Joint task, participants could not (in ad-
dition to the PRED and APREDs) see the gold-
standard nor the predicted syntactic dependencies
(HEAD, PHEAD) and their labels (DEPREL, PDE-
PREL). These syntactic dependencies were also to
be filled by participants? systems.
In both tasks, participants have been free to
use any other data (columns) provided, except the
LEMMA, POS and FEAT columns (to get more ?re-
alistic? results using only their automatically pre-
dicted variants PLEMMA, PPOS and PFEAT).
Besides the corpus proper, predicate dictionaries
have been provided to participants in order to be able
to properly match the predicates to the tokens in the
corpus; their contents could have been used e.g. as
features for the PRED/APREDs predictions (or even
for the syntactic dependencies, i.e., for filling in the
PHEAD and PDEPREL fields).
The system of filling-in the APREDs follows
the 2008 pattern; for each argument-bearing token
(predicate), a new APREDn column is created in the
order in which the predicate token is encountered
within the sentence (i.e., based on its ID seen as a
numerical value). Then, for each token in the sen-
tence, the value in the intersection of the APREDn
column and the token row is either left unfilled
(if the token is not an argument), or a predicate-
argument label(s) is(are) filled in.
The differences between the English-only 2008
task and this year?s multilingual task can be briefly
summarized as follows:
? only ?split?2 lemmas and forms have been pro-
vided in the English datasets (for the other lan-
guages, original tokenization from the respec-
tive treebanks has been used);
? rich morphological features have been added
wherever available;
? syntactic dependencies by state-of-the-art
parsers have been provided (for the SRL-only
task);
? multiple semantic labels for a single token have
been allowed (and properly evaluated) in the
APREDs columns;
? predicates have been pre-identified and marked
in both the training and test data;
? some of the fields (e.g. the APREDx) and val-
ues (ARG0? A0 etc.) have been renamed.
2.4 Evaluation Measures
It was required that participants submit results in all
seven languages in the chosen task and in any of (or
both) the challenges. Submission of out-of-domain
data files has been optional.
The main evaluation measure, according to which
systems are primarily compared, is the Joint task,
2Splitting of forms and lemmas in English has been intro-
duced in the 2008 shared task to match the tokenization con-
vention for the arguments in NomBank.
3
Field # Name Description
1 ID Token counter, starting at 1 for each new sentence
2 FORM Form or punctuation symbol (the token; ?split? for English)
3 LEMMA Gold-standard lemma of FORM
4 PLEMMA Automatically predicted lemma of FORM
5 POS Gold-standard POS (major POS only)
6 PPOS Automatically predicted major POS by a language-specific tagger
7 FEAT Gold-standard morphological features (if applicable)
8 PFEAT Automatically predicted morphological features (if applicable)
9 HEAD Gold-standard syntactic head of the current token (ID or 0 if root)
10 PHEAD Automatically predicted syntactic head
11 DEPREL Gold-standard syntactic dependency relation (to HEAD)
12 PDEPREL Automatically predicted dependency relation to PHEAD
13 FILLPRED Contains ?Y? for argument-bearing tokens
14 PRED (sense) identifier of a semantic ?predicate? coming from a current token
15... APREDn Columns with argument labels for each semantic predicate (in the ID order)
Table 1: Description of the fields (columns) in the data provided. The values of columns 9, 11 and 14 and above are
not provided in the evaluation data; for the Joint task, columns 9?12 are also empty in the evaluation data.
closed challenge, Macro F1 score. However, scores
can also be computed for a number of other condi-
tions:
? Task: Joint or SRL-only
? Challenge: open or closed
? Domain: in-domain data (IDD, separated from
training corpus) or out-of-domain data (OOD)
Joint task participants are also evaluated separately
on the syntactic dependency task (labeled attach-
ment score, LAS). Finally, systems competing in
both tasks are compared on semantic role labeling
alone, to assess the impact of the the joint pars-
ing/SRL task compared to an SRL-only task on pre-
parsed data.
Finally, as an explanatory measure, precision and
recall of the semantic labeling task have been com-
puted and tabulated.
We have decided to omit several evaluation fig-
ures that were reported in previous years, such as the
percentage of completely correct sentences (?Exact
Match?), unlabeled scores, etc. With seven lan-
guages, two tasks (plus two challenges, and the
IDD/OOD distinction), there are enough results to
get lost even as it is.
2.4.1 Syntactic Dependency Measures
The LAS score is defined similarly as in the pre-
vious shared tasks, as the percentage of tokens for
which a system has predicted the correct HEAD and
DEPREL columns. The unlabeled attachment score
(UAS), i.e., the percentage of tokens with correct
HEAD regardless if the DEPREL is correct, has not
been officially computed this year. No precision and
recall measures are applicable, since all systems are
supposed to output a single dependency with a single
label (see also below the footnote to the description
of the combined score).
2.4.2 Semantic Labeling Measures
The semantic propositions are evaluated by con-
verting them to semantic dependencies, i.e., we cre-
ate n semantic dependencies from every predicate
to its n arguments. These dependencies are labeled
with the labels of the corresponding arguments. Ad-
ditionally, we create a semantic dependency from
each predicate to a virtual ROOT node. The latter
dependencies are labeled with the predicate senses.
This approach guarantees that the semantic depen-
dency structure conceptually forms a single-rooted,
connected (but not necessarily acyclic) graph. More
importantly, this scoring strategy implies that if a
system assigns the incorrect predicate sense, it still
receives some points for the arguments correctly as-
signed. For example, for the correct proposition:
verb.01: A0, A1, AM-TMP
the system that generates the following output for
the same argument tokens:
4
verb.02: A0, A1, AM-LOC
receives a labeled precision score of 2/4 because two
out of four semantic dependencies are incorrect: the
dependency to ROOT is labeled 02 instead of 01
and the dependency to the AM-TMP is incorrectly la-
beled AM-LOC. Using this strategy we compute pre-
cision, recall, and F1 scores for semantic dependen-
cies (labeled only).
For some languages (Czech, Japanese) there may
be more than one label in a given argument position;
for example, this happens in Czech in special cases
of reciprocity when the same token serves as two or
more arguments to the same predicate. The scorer
takes this into account and considers such cases to
be (as if) multiple predicate-argument relations for
the computation of the evaluation measures.
For example, for the correct proposition:
v1f1: ACT|EFF, ADDR
the system that generates the following output for
the same argument tokens:
v1f1: ACT, ADDR|PAT
receives a labeled precision score of 3/4 because
the PAT is incorrect and labeled recall 3/4 be-
cause the EFF is missing (should the ACT|EFF and
ADDR|PAT be taken as atomic values, the scores
would then be zero).
2.4.3 Combined Syntactic and Semantic Score
We combine the syntactic and semantic measures
into one global measure using macro averaging. We
compute macro precision and recall scores by aver-
aging the labeled precision and recall for semantic
dependencies with the LAS for syntactic dependen-
cies:3
LMP = Wsem ? LPsem + (1?Wsem) ? LAS (1)
LMR = Wsem ? LRsem + (1 ?Wsem) ? LAS (2)
where LMP is the labeled macro precision and
LPsem is the labeled precision for semantic depen-
dencies. Similarly, LMR is the labeled macro re-
call and LRsem is the labeled recall for semantic
dependencies. Wsem is the weight assigned to the
3We can do this because the LAS for syntactic dependen-
cies is a special case of precision and recall, where the predicted
number of dependencies is equal to the number of gold depen-
dencies.
semantic task.4 The macro labeled F1 score, which
was used for the ranking of the participating sys-
tems, is computed as the harmonic mean of LMP
and LMR.
3 Data
The unification of the data formats for the various
languages appeared to be a challenge in itself. We
will briefly describe the processes of the conversion
of the existing treebanks in the seven languages of
the CoNLL-2009 shared task. In many instances,
the original treebanks had to be not only converted
format-wise, but also merged with other resources in
order to generate useful training and testing data that
fit the task description.
3.1 The Input Corpora
The data used as the input for the transformations
aimed at arriving at the data contents and format de-
scribed in Sect. 2.3 are described in (Taule? et al,
2008), (Xue and Palmer, 2009), (Hajic? et al, 2006),
(Surdeanu et al, 2008), (Burchardt et al, 2006) and
(Kawahara et al, 2002).
In the subsequent sections, the procedures for the
data conversion for the individual languages are de-
scribed. The data has been collected by the main
organization site and checked for format errors, and
repackaged for distribution.
There were three packages of the data distributed
to the participants: Trial, Training plus Develop-
ment, and Evaluation. The Trial data were rather
small, just to give the feeling of the format and
languages involved. A visual representation of the
Trial data was also created to make understanding
of the data easier. Any data in the same format
can be transformed and displayed in the Tree Editor
TrEd5 (Pajas and ?Ste?pa?nek, 2008) with the CoNLL
2009 Shared Task extension that can be installed
from within the editor. A sample visualization of an
English sentence after its conversion to the shared
task format (Sect. 2.3) is in Fig. 1.
Due to licensing requirements, every package of
the data had to be split into two portions. One
portion (Catalan, German, Japanese, and Spanish
data) was published on the task?s webpage for down-
4We assign equal weight to the two tasks, i.e., Wsem = 0.5.
5http://ufal.mff.cuni.cz/?pajas/tred
5
$QG
'(3 &&
VRPHWLPHV
703 5%
D
102' '7
UHSXWDEOH
102' --
FKDULW\
6%- 11
ZLWK
102' ,1
D
102' '7
KRXVHKROG
102' 11
QDPH QDPH
302' 11
JHWV JHW
5227 9%=
XVHG XVH
9& 9%1
DQG
&225' &&
GRHV
&21- 9%=
Q
W
$'9 5%
HYHQ
$'9 5%
NQRZ NQRZ
9& 9%
LW
2%- 353

3  
$0703$0703$0703
 

$$$$

 
 

$
 

 

$



$01(*

$0$'9
 


$

Figure 1: Visualisation of the English sentence ?And sometimes a reputable charity with a houshold name gets used
and doesn?t even know it.? (Penn Treebank, wsj 0559) showing jointly the labeled syntactic and semantic depen-
dencies. The basic tree shape comes from the syntactic dependencies; syntactic labels and POS tags are on the 2nd
line at each node. Semantic dependencies which do not follow the syntactic ones use dotted lines. Predicate senses
in parentheses (use:01, ...) follow the word label. SRLs (A0, AM-TMP, ...) are on the last line. Please note that
multiple semantic dependencies (e.g., there are four for charity: A0? know, A1? gets, A1? used, A1? name)
and self-dependencies (name) appear in this sentence.
load, the other portion (Czech, English, and Chinese
data) was invoiced and distributed by the Linguistic
Data Consortium under a special agreement free of
charge.
Distribution of the Evaluation package was a bit
more complicated, because there were two types of
the packages - one for the Joint task and one for the
SRL-only task. Every participant had to subscribe
to one of the two tasks; subsequently, they obtained
the appropriate data (again, from the webpage and
LDC).
Prior to release, each data file was checked to
eliminate errors. The following test were carried
out:
? For every sentence, number of PREDs rows
matches the number of APREDs columns.
? The first line of each file is never empty, while
the last line always is.
? The first character on a non-empty line is al-
ways a digit, the last one is never a whitespace.
? The number of empty lines (i.e. the number
of sentences) equals the number of lines begin-
ning with ?1?.
? The data contain no spaces nor double tabs.
Some statistics on the data can be seen in Ta-
bles 2, 3 and 4. Whereas the training sizes of the
data have not been that different as they were e.g.
for the 2007 shared task on multilingual dependency
parsing (Nivre et al, 2007)6, substantial differences
existed in the distribution of the predicates and ar-
guments, the input features, the out-of-vocabulary
rates, and other statistical characteristics of the data.
Data sizes have been relatively uniform in all the
datasets, with Japanese having the smallest dataset
6http://nextens.uvt.nl/depparse-wiki/
DataOverview
6
containing data for SRL annotation training. To
compensate at least for the dependency parsing part,
an additional, large Japanese corpus with syntactic
dependency annotation has been provided.
The average sentence length, the vocabulary sizes
for FORM and LEMMA fields and the OOV rates
characterize quite naturally the properties of the re-
spective languages (in the domain of the training and
evaluation data). It is no surprise that the FORM
OOV rate is the highest for Czech, a highly inflec-
tional language, and that the LEMMA OOV rate is
the highest for German (as a consequence of keeping
compounds as a single lemma). The other statistics
also reflect (to a large extent) the annotation speci-
fication and conventions used for the original tree-
banks and/or the result of the conversion process to
the unified CoNLL-2009 Shared Task format.
Starting with the POS and FEAT fields, it can be
seen that Catalan, Czech and Spanish use only the
12 major part-of-speech categories as values of the
POS field (with richly populated FEAT field); En-
glish and Chinese are the opposite extreme, disre-
garding the use of the FEAT field completely and
coding everything as a POS value. While for Chi-
nese this is quite understandable, English follows the
PTB tradition in this respect. German and Japanese
use relatively rich set of values in both the POS and
FEAT fields.
For the dependency relations (DEPREL), all
the languages use a similarly-sized set except for
Japanese, which only encodes the distinction be-
tween a root and a dependent node (and some in-
frequent special ones).
Evaluation data are over 10% of the size of the
training data for Catalan, Chinese, Czech, Japanese
and Spanish and roughly 5% for English and Ger-
man.
Table 3 shows the distribution of the five most fre-
quent dependency relations (determined as part of
the subtask of syntactic parsing). With the exception
of Japanese, which essentially does not label depen-
dency relations at this level, all the other languages
show little difference in this distribution. For exam-
ple, the unconditioned probability of ?subjects? is
almost the same for all the six other languages (be-
tween 6 and 8 percent). The probability mass cov-
ered by the first five most frequent DEPRELs is also
almost the same (again, except for Japanese), sug-
gesting that the labeling task might have similar dif-
ficulty7. The most skewed one is for Czech (after
Japanese).
Table 4 shows similar statistics for the argument
labels (PRED/APREDs); it also adds the average
number of arguments per ?predicate? token, since
this is part of the SRL task8. It is apparent from the
comparison of the ?Total? rows in this table and Ta-
ble 3 that the first five argument labels cover more
that their syntactic counterparts. For example, the
arguments A0-A4 account for all but 3% of all ar-
guments labels, whereas Spanish and Catalan have
much more rich set of argument labels, with a high
entropy of the most-frequent-label distribution.
3.2 Catalan and Spanish
The Catalan and Spanish datasets (Taule? et al, 2008)
were generated from the AnCora corpora9 through
an automatic conversion process from a constituent-
based formalism to dependencies (Civit et al, 2006).
AnCora corpora contain about half million words
for Catalan and Spanish annotated with syntactic
and semantic information. Text sources for the Cata-
lan corpus are EFE news agency (?75Kw), ACN
Catalan news agency (?225Kw), and ?El Perio?dico?
newspaper (?200Kw). The Spanish corpus comes
from the Lexesp Spanish balanced corpus (?75Kw),
the EFE Spanish news agency (?225Kw), and the
Spanish version of ?El Perio?dico? (?200Kw). The
subset from ?El Perio?dico? corresponds to the same
news in Catalan and Spanish, spanning from January
to December 2000.
Linguistic annotation is the same in both lan-
guages and includes: PoS tags with morphologi-
cal features (gender, number, person, etc.), lemma-
tization, syntactic dependencies (syntactic func-
tions), semantic dependencies (arguments and the-
matic roles), named entities and predicate semantic
classes (Lexical Semantic Structure, LSS). Tag sets
are shared by the two languages.
If we take into account the complete PoS tags,
7Yes, this is overgeneralization since this distribution does
not condition on the features, dependencies etc. But as a rough
measure, it often correlates well with the results.
8A number below 1 means there are some argument-bearing
words (often nouns) which have no arguments in the particular
sentence in which they appear.
9http://clic.ub.edu/ancora
7
Characteristic Catalan Chinese Czech English German Japanese Spanish
Training data size (sentences) 13200 22277 38727 39279 36020 4393a 14329
Training data size (tokens) 390302 609060 652544 958167 648677 112555a 427442
Avg. sentence length (tokens) 29.6 27.3 16.8 24.4 18.0 25.6 29.8
Tokens with argumentsb (%) 9.6 16.9 63.5 18.7 2.7 22.8 10.3
DEPREL types 50 41 49 69 46 5 49
POS types 12 41 12 48 56 40 12
FEAT types 237 1 1811 1 267 302 264
FORM vocabulary size 33890 40878 86332 39782 72084 36043 40964
LEMMA vocabulary size 24143 40878 37580 28376 51993 30402 26926
Evaluation data size (sent.) 1862 2556 4213 2399 2000 500 1725
Evaluation data size (tokens) 53355 73153 70348 57676 31622 13615 50630
Evaluation FORM OOVc 5.40 3.92 7.98/8.62d 1.58/3.76d 7.93/7.57d 6.07 5.63
Evaluation LEMMA OOVc 4.14 3.92 3.03/4.29d 1.08/2.30d 5.83/7.36d 5.21 3.69
Table 2: Elementary data statistics for the CoNLL-2009 Shared Task languages. The data themselves, the original
treebanks they were derived from and the conversion process are described in more detail in sections 3.2-3.7. All
evaluation data statistics are derived from the in-domain evaluation data.
aThere were additional 33257 sentences (839947 tokens) available for syntactic dependency parsing of Japanese; the type and
vocabulary statistics are computed using this larger dataset.
bPercentage of tokens with FILLPRED=?Y?.
cPercentage of FORM/LEMMA tokens not found in the respective vocabularies derived solely from the training data.
dOOV percentage for in-domain/out-of-domain data.
DEPREL Catalan Chinese Czech English German Japanese Spanish
sn 0.16 COMP 0.21 Atr 0.26 NMOD 0.27 NK 0.31 D 0.93 sn 0.16
spec 0.15 NMOD 0.14 AuxP 0.10 P 0.11 PUNC 0.14 ROOT 0.04 spec 0.15
Labels f 0.11 ADV 0.10 Adv 0.10 PMOD 0.10 MO 0.12 P 0.03 f 0.12
sp 0.09 UNK 0.09 Obj 0.07 SBJ 0.07 SB 0.07 A 0.00 sp 0.08
suj 0.07 SBJ 0.08 Sb 0.06 OBJ 0.06 ROOT 0.06 I 0.00 suj 0.08
Total 0.58 0.62 0.59 0.61 0.70 1.00 0.59
Table 3: Unigram probability for the five most frequent DEPREL labels in the training data of the CoNLL-2009
Shared Task is shown. Total is the probability mass covered by the five dependency labels shown.
APRED Catalan Chinese Czech English German Japanese Spanish
arg1-pat 0.22 A1 0.30 RSTR 0.30 A1 0.37 A0 0.40 GA 0.33 arg1-pat 0.20
arg0-agt 0.18 A0 0.27 PAT 0.18 A0 0.25 A1 0.39 WO 0.15 arg0-agt 0.19
Labels arg1-tem 0.15 ADV 0.20 ACT 0.17 A2 0.12 A2 0.12 NO 0.15 arg1-tem 0.15
argM-tmp 0.08 TMP 0.07 APP 0.06 AM-TMP 0.06 A3 0.06 NI 0.09 arg2-atr 0.08
arg2-atr 0.08 DIS 0.04 LOC 0.04 AM-MNR 0.03 A4 0.01 DE 0.06 argM-tmp 0.08
Total 0.71 0.91 0.75 0.83 0.97 0.78 0.70
Avg. 2.25 2.26 0.88 2.20 1.97 1.71 2.26
Table 4: Unigram probability for the five most frequent APRED labels in the training data of the CoNLL-2009
Shared Task is shown. Total is the probability mass covered by the five argument labels shown. The ?Avg.? line
shows the average number of arguments per predicate or other argument-bearing token (i.e. for those marked by
FILLPRED=?Y?).
8
AnCora has 280 different labels. Considering only
the main syntactic categories, the tag set is reduced
to 47 tags. The syntactic tag set consists of 50 dif-
ferent syntactic functions. Regarding semantic ar-
guments, we distinguish Arg0, Arg1, Arg2, Arg3,
Arg4, ArgM, and ArgL. The first five tags are num-
bered from less to more obliqueness with respect
to the verb, ArgM corresponds to adjuncts. The
list of thematic roles consists of 20 different labels:
AGT (Agent), AGI (Induced Agent), CAU (Cause),
EXP (Experiencer), SCR (Source), PAT (Patient),
TEM (Theme), ATR (Attribute), BEN (Beneficiary),
EXT (Extension), INS (Instrument), LOC (Loca-
tive), TMP (Time), MNR (Manner), ORI (Origin),
DES (Goal), FIN (Purpose), EIN (Initial State), EFI
(Final State), and ADV (Adverbial). Each argument
position can map onto specific thematic roles. By
way of example, Arg1 can be PAT, TEM or EXT. For
Named Entities, we distinguish six types: Organiza-
tion, Person, Location, Date, Number, and Others.
An incremental process guided the annotation of
AnCora, since semantics depends on morphosyntax,
and syntax relies on morphology. This procedure
made it possible to check, correct, and complete
the previous annotations, thus guaranteeing the final
quality of the corpora and minimizing the error rate.
The annotation process was carried out sequentially
from lower to upper layers of linguistic description.
All resulting layers are independent of each other,
thus making easier the data management. The ini-
tial annotation was performed manually for syntax,
semiautomatically in the case of arguments and the-
matic roles, and fully automatically for PoS (Mart??
et al, 2007; Ma`rquez et al, 2007).
The Catalan and Spanish AnCora corpora were
straightforwardly translated into the CoNLL-2009
shared task formatting (information about named
entities was skipped in this process). The resulting
Catalan corpus (including training, development and
test partitions) contains 16,786 sentences with an av-
erage length of 29.59 lexical tokens per sentence.
Long sentences abound in this corpus. For instance,
10.73% of the sentences are longer than 50 tokens,
and 4.42% are longer than 60. The corpus con-
tains 47,537 annotated predicates (2.83 predicates
per sentence, on average) with 107,171 arguments
(2.25 arguments per predicate, on average). From
the latter, 73.89% correspond to core arguments and
26.11% to adjuncts. Numbers for the Spanish cor-
pus are comparable in all aspects: 17,709 sentences
with 29.84 lexical tokens on average (11.58% of the
sentences longer than 50 tokens, 4.07% longer than
60); 54,075 predicates (3.05 per sentence, on aver-
age) and 122,478 arguments (2.26 per predicate, on
average); 73.34% core arguments and 26.66% ad-
juncts.
The following are important features of the Cata-
lan and Spanish corpora in the CoNLL-2009 shared
task setting: (1) all dependency trees are projective;
(2) no word can be the argument of more than one
predicate in a sentence; (3) semantic dependencies
completely match syntactic dependency structures
(i.e., no new edges are introduced by the semantic
structure); (4) only verbal predicates are annotated
(with exceptional cases referring to words that can
be adjectives and past participles); (5) the corpus is
segmented so multi-words, named entities, temporal
expressions, compounds, etc. are grouped together;
and (6) segmentation also accounts for elliptical pro-
nouns (there are marked as empty lexical tokens ?_?
with a pronoun POS tag).
Finally, the predicted columns (PLEMMA,
PPOS, and PFEAT) have been generated with the
FreeLing Open source suite of Language Analyz-
ers10. Accuracy in PLEMMA and PPOS columns
is above 95% for the two languages. PHEAD
and PDEPREL columns have been generated using
MaltParser11. Parsing accuracy (LAS) is above 86%
for the the two languages.
3.3 Chinese
The Chinese Corpus for the 2009 CoNLL Shared
Task was generated by merging the Chinese Tree-
bank (Xue et al, 2005) and the Chinese Proposition
Bank (Xue and Palmer, 2009) and then converting
the constituent structure to a dependency formalism
as specified in the CoNLL Shared Task. The Chi-
nese data used in the shared task is based on Chinese
Treebank 6.0 and the Chinese Proposition Bank 2.0,
both of which are publicly available via the Linguis-
tic Data Consortium.
The Chinese Treebank Project originated at Penn
and was later moved to University of Colorado at
10http://www.lsi.upc.es/?nlp/freeling
11http://w3.msi.vxu.se/?jha/maltparser
9
Boulder. Now it is the process of being to moved
to Brandeis University. The data sources of the Chi-
nese Treebank range from Xinhua newswire (main-
land China), Hong Kong news, and Sinorama Maga-
zine (Taiwan). More recently under DARPA GALE
funding it has been expanded to include broadcast
news, broadcast conversation, news groups and web
log data. It currently has over one million words
and is fully segmented, POS-tagged and annotated
with phrase structure. The version of the Chinese
Treebank used in this shared task, CTB 6.0, includes
newswire, magazine articles, and transcribed broad-
cast news 12. The training set has 609,060 tokens,
the development set has 49,620 tokens, and the test
set has 73,153 tokens.
The Chinese Proposition Bank adds a layer of se-
mantic annotation to the syntactic parses in the Chi-
nese Treebank. This layer of semantic annotation
mainly deals with the predicate-argument structure
of Chinese verbs and their nominalizations. Each
major sense (called frameset) of a predicate takes a
number of core arguments annotated with numeri-
cal labels Arg0 through Arg5 which are defined in
a predicate-specific manner. The Chinese Proposi-
tion Bank also annotates adjunctive arguments such
as locative, temporal and manner modifiers of the
predicate. The version of the Chinese Propbank used
in this CoNLL Shared Task is CPB 2.0, but nominal
predicates are excluded because the annotation is in-
complete.
Since the Chinese Treebank is annotated with
constituent structures, the conversion and merging
procedure converts the constituent structures to de-
pendencies by identifying the head for each con-
stituent in a parse tree and making its sisters its de-
pendents. The Chinese Propbank pointers are then
shifted from the entire constituent to the head of that
constituent. The conversion procedure identifies the
head by first exploiting the structural information
in the syntactic parse and detecting six broad cate-
gories of syntactic relations that hold between the
head and its dependents (predication, modification,
complementation, coordination, auxiliary, and flat)
and then designating the head based on these rela-
tions. In particular, the first conjunct of a coordina-
12A small number of files were taken out of the CoNLL
shared task data due to conversion problems and time con-
straints to fix them.
tion structure is designated as the head and the heads
of the other conjuncts are the conjunctions preced-
ing them. The conjunctions all ?modify? the first
conjunct.
3.4 Czech
For the training, development and evaluation data,
Prague Dependency Treebank 2.0 was used (Hajic?
et al, 2006). For the out-of-domain evaluation data,
part of the Czech side of the Prague Czech-English
Dependency Treebank (version 2, under construc-
tion) was used13, see also ( ?Cmejrek et al, 2004). For
the OOD data, no manual annotation of LEMMA,
POS, and FEAT existed, so the predicted values
were used. The same conversion procedure has been
applied to both sources.
The FORM column was created from the form
element of the morphological layer, not from the
?token? from the word-form layer. Therefore, most
typos, errors in word segmentation and tokenization
are corrected and numerals are normalized.
The LEMMA column was created from the
lemma element of the morphological layer. Only
the initial string of the element was used, so there is
no distinction between homonyms. However, some
components of the detailed lemma explanation were
incorporated into the FEAT column (see below).
The POS column was created form the morpho-
logical tag element, its first character more pre-
cisely.
The FEAT column was created from the remain-
ing characters of the tag element. In addition, the
special feature ?Sem? corresponds to a semantic fea-
ture of the lemma.
For the HEAD and DEPREL columns, the PDT
analytical layer was used. The DEPREL was taken
from the analytic function (the afun node at-
tribtue). There are 27 possible values for afun el-
ement: Pred, Pnom, AuxV, Sb, Obj, Atr, Adv,
Atv, AtvV, Coord, Apos, ExD, and a number
of auxiliary and ?double-function? labels. The first
nine of these are the ?most interesting? from the
point of view of the shared task, since they relate to
semantics more closely than the rest (at least from
the linguistic point of view). The HEAD is a pointer
to its parent, which means the PDT?s ord attribute
13http://ufal.mff.cuni.cz/pedt
10
(within-sentence ID / word position number) of the
parent. If a node is a member of a coordination
or apposition (is_member element), its DEPREL
obtains the _M suffix. The parenthesis annotation
(is_parenthesis_root element) was ignored.
The PRED and APREDs columns were created
from the tectogrammatical layer of PDT 2.0 and the
valency lexicon PDT-Vallex according to the follow-
ing rules:
? Every line corresponding to an analytical node
referenced by a lexical reference (a/lex.rf)
from the tectogrammatical layer has a PRED
value filled. If the referring non-generated
tectogrammatical node (is_generated not
equal to 1) has a valency frame assigned
(val_frame.rf), the value of PRED is the
identifier of the frame. Otherwise, it is set to
the same value as the LEMMA column.
? For every tectogrammatical node, a corre-
sponding analytical node is searched for:
1. If the tectogrammatical node is not
generated and has a lexical reference
(a/lex.rf), the referenced node is
taken.
2. Otherwise, if the tectogrammatical node
has a coreference (coref_text.rf or
coref_gram.rf) or complement refer-
ence (compl.rf) to a node that has an
analytical node assigned (by 1. or 2.), the
assigned node is taken.
APRED columns are filled with respect to the
following correspondence: for a tectogrammatical
node P and its effective child C with functor F, the
column for P?s corresponding analytical node at the
row for C?s corresponding analytical node is filled
with F. Some nodes can thus have several functors
in one APRED column, separated by a vertical bar
(see Sect. 2.4.2).
PLEMMA, PPOS and PFEAT were gener-
ated by the (cross-trained) morphological tagger
MORCE (Spoustova? et al, 2009), which gives full
combined accuracy (PLEMMA+PPOS+PFEAT)
slightly under 96%.
PHEAD and PDEPREL were generated by
the (cross-trained) MST parser for Czech (Chu?
Liu/Edmonds algorithm, (McDonald et al, 2005)),
which has typical dependency accuracy around
85%.
The valency lexicon, converted from (Hajic? et al,
2003), has four columns:
1. lemma (can occur several times in the lexicon,
with different frames)
2. frame identifier (as found in the PRED column)
3. list of space-separated actants and obligatory
members of the frame
4. example(s)
The source of the out-of-domain data uses an
extended valency lexicon (because of out-of-
vocabulary entries). For simplicity, the extended
lexicon was not provided; instead, such words were
not marked as predicates in the OOD data (their
FILLPRED was set to ?_?) and thus not evaluated.
3.5 English
The English corpus is almost identical to the cor-
pus used in the closed challenge in the CoNLL-2008
shared task evaluation (Surdeanu et al, 2008). This
corpus was generated through a process that merges
several input corpora and converts them from the
constituent-based formalism to dependencies. The
following corpora were used as input to the merging
procedure:
? Penn Treebank 3 ? The Penn Treebank 3 cor-
pus (Marcus et al, 1994) consists of hand-
coded parses of the Wall Street Journal (test,
development and training) and a small subset
of the Brown corpus (W. N. Francis and H.
Kucera, 1964) (test only).
? BBN Pronoun Coreference and Entity Type
Corpus ? BBN?s NE annotation of the Wall
Street Journal corpus (Weischedel and Brun-
stein, 2005) takes the form of SGML inline
markup of text, tokenized to be completely
compatible with the Penn Treebank annotation.
For the CoNLL-2008 shared task evaluation,
this corpus was extended by the task organizers
to cover the subset of the Brown corpus used as
a secondary testing dataset. From this corpus
we only used NE boundaries to derive NAME
11
dependencies between NE tokens, e.g., we cre-
ate a NAME dependency from Mary to Smith
given the NE mention Mary Smith.
? Proposition Bank I (PropBank) ? The Prop-
Bank annotation (Palmer et al, 2005) classifies
the arguments of all the main verbs in the Penn
Treebank corpus, other than be. Arguments are
numbered (Arg0, Arg1, . . .) based on lexical
entries or frame files. Different sets of argu-
ments are assumed for different rolesets. De-
pendent constituents that fall into categories in-
dependent of the lexical entries are classified as
various types of adjuncts (ArgM-TMP, -ADV,
etc.).
? NomBank ? NomBank annotation (Meyers et
al., 2004) uses essentially the same framework
as PropBank to annotate arguments of nouns.
Differences between PropBank and NomBank
stem from differences between noun and verb
argument structure; differences in treatment of
nouns and verbs in the Penn Treebank; and dif-
ferences in the sophistication of previous re-
search about noun and verb argument structure.
Only the subset of nouns that take arguments
are annotated in NomBank and only a subset of
the non-argument siblings of nouns are marked
as ArgM.
The complete merging process and the conversion
from the constituent representation to dependencies
is detailed in (Surdeanu et al, 2008).
The main difference between the 2008 and 2009
version of the corpora is the generation of word lem-
mas. In the 2008 version the only lemmas pro-
vided were predicted using the built-in lemmatizer
in WordNet (Fellbaum, 1998) based on the most fre-
quent sense for the form and the predicted part-of-
speech tag. These lemmas are listed in the 2009
corpus under the PLEMMA column. The LEMMA
column in the 2009 version of the corpus contains
lemmas generated using the same algorithm but us-
ing the correct Treebank part-of-speech tags. Addi-
tionally, the PHEAD and PDEPREL columns were
generated using MaltParser14, similarly to the open
challenge corpus in the CoNLL 2008 shared task.
14http://w3.msi.vxu.se/?nivre/research/
MaltParser.html
3.6 German
The German in-domain dataset is based on the an-
notated verb instances of the SALSA corpus (Bur-
chardt et al, 2006), a total of around 40k sen-
tences15. SALSA provides manual semantic role
annotation on top of the syntactically annotated
TIGER newspaper corpus, one of the standard Ger-
man treebanks. The original SALSA corpus uses se-
mantic roles in the FrameNet paradigm. We con-
structed mappings between FrameNet frame ele-
ments and PropBank argument positions at the level
of frame-predicate pairs semi-automatically. For the
frame elements of each frame-predicate pair, we first
identified the semantically defined PropBank Arg-
0 and Arg-1 positions. To do so, we annotated a
small number of very abstract frame elements with
these labels (Agent, Actor, Communicator as Arg-
0, and Theme, Effect, Message as Arg-1) and per-
colated these labels through the FrameNet hierar-
chy, adding further manual labels where necessary.
Then, we used frequency and grammatical realiza-
tion information to map the remaining roles onto
higher-numbered Arg roles. We considerably sim-
plified the annotations provided by SALSA, which
use a rather complex annotation scheme. In partic-
ular, we removed annotation for multi-word expres-
sions (which may be non-contiguous), annotations
involving multiple frames for the same predicate
(metaphors, underspecification), and inter-sentence
roles.
The out-of-domain dataset was taken from a study
on the multi-lingual projection of FrameNet annota-
tion (Pado and Lapata, 2005). It is sampled from
the EUROPARL corpus and was chosen to maxi-
mize the lexical coverage, i.e., it contains of a large
number of infrequent predicates. Both syntactic and
semantic structure were annotated manually, in the
TIGER and SALSA format, respectively. Since it
uses a simplified annotation schemes, we did not
have to discard any annotation.
For both datasets, we converted the syntactic
TIGER (Brants et al, 2002) representations into de-
pendencies with a similar set of head-finding rules
used for the preparation of the CoNLL-X shared task
German dataset. Minor modifications (for the con-
15Note, however, that typically not all predicates in each sen-
tence are annotated (cf. Table 2).
12
version of person names and coordinations) were
made to achieve better consistency with datasets
of other languages. Since the TIGER annotation
allows non-contiguous constituents, the resulting
dependencies can be non-projective. Secondary
edges were discarded in the conversion. As for the
automatically constructed features, we used Tree-
Tagger (Schmid, 1994) to produce the PLEMMA
and PPOS columns, and the Morphisto morphol-
ogy (Zielinski and Simon, 2008) for PFEAT.
3.7 Japanese
For Japanese, we used the Kyoto University Text
Corpus (Kawahara et al, 2002), which consists of
approximately 40k sentences taken from Mainichi
Newspapers. Out of them, approximately 5k sen-
tences are annotated with syntactic and semantic de-
pendencies, and are used the training, development
and test data of this year?s shared task. The remain-
ing sentences, which are annotated with only syntac-
tic dependencies, are provided for the training cor-
pus of syntactic dependency parsers.
This corpus adopts a dependency structure repre-
sentation, and thus the conversion to the CoNLL-
2009 format was relatively straightforward. How-
ever, since the original dependencies are annotated
on the basis of phrases (Japanese bunsetsu), we
needed to automatically convert the original annota-
tions to word-based ones using several criteria. We
used the following basic criteria: the words except
the last word in a phrase depend on the next (right)
word, and the last word in a phrase basically depends
on the head word of the governing phrase.
Semantic dependencies are annotated for both
verbal predicates and nominal predicates. The se-
mantic roles (APRED columns) consist of 41 sur-
face cases, many of which are case-marking post-
positions such as ga (nominative), wo (accusative)
and ni (dative). Semantic frame discrimination is not
annotated, and so the PRED column is the same as
the LEMMA column. The original corpus contains
coreference annotations and inter-sentential seman-
tic dependencies, such as inter-sentential zero pro-
nouns and bridging references, but we did not use
these annotations, which are not the target of this
year?s shared task.
To produce the PLEMMA, PPOS and PFEAT
columns, we used the morphological analyzer JU-
MAN 16 and the dependency and case structure an-
alyzer KNP 17. To produce the PHEAD and PDE-
PREL columns, we used the MSTParser 18.
4 Submissions and Results
Participants uploaded the results through the shared
task website, and the official evaluation was per-
formed centrally. Feedback was provided if any for-
mal problems were encountered (for a list of checks,
see the previous section). One submission had to
be rejected because only English results were pro-
vided. After the evaluation period had passed, the
results were anonymized and published on the web.
A total of 20 systems participated in the closed
challenge; 13 of them in the Joint task and seven in
the SRL-only task. Two systems participated in the
open challenge (Joint task). Moreover, 17 systems
provided output in the out-of-domain part of the task
(11 in the OOD Joint task and six in the OOD SRL-
only task).
The main results for the core task - the Joint task
(dependency syntax and semantic relations) in the
context of the closed challenge - are summarized and
ranked in Table 5.
The largest number of systems can be compared
in the SRL results table (Table 6), where all the sys-
tems have been evaluated solely on the SRL perfor-
mance regardless whether they participated in the
Joint or SRL-only task. However, since the results
might have been influenced by the supplied parser,
separate ranking is provided for both types of the
systems.
Additional breakdown of the results (open chal-
lenge, precision and recall tables for the semantic
labeling task, etc.) are available from the CoNLL-
2009 Shared Task website19.
5 Approaches
Table 7 summarizes the properties of the systems
that participated in the closed the open challenges.
16http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/juman-e.html
17http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/knp-e.html
18http://sourceforge.net/projects/
mstparser
19http://ufal.mff.cuni.cz/conll2009-st
13
Rank System Average Catalan Chinese Czech English German Japanese Spanish
1 Che 82.64 81.84 76.38 83.27 87.00 82.44 85.65 81.90
2 Chen 82.52 83.01 76.23 80.87 87.69 81.22 85.28 83.31
3 Merlo 82.14 82.66 76.15 83.21 86.03 79.59 84.91 82.43
4 Bohnet 80.85 80.44 75.91 79.57 85.14 81.60 82.51 80.75
5 Asahara 78.43 75.91 73.43 81.43 86.40 69.84 84.86 77.12
6 Brown 77.27 77.40 72.12 75.66 83.98 77.86 76.65 77.21
7 Zhang 76.49 75.00 73.42 76.93 82.88 73.76 78.17 75.25
8 Dai 73.98 72.09 72.72 67.14 81.89 75.00 80.89 68.14
9 Lu Li 73.97 71.32 65.53 75.85 81.92 70.93 80.49 71.72
10 Llu??s 71.49 56.64 66.18 75.95 81.69 72.31 81.76 65.91
11 Vallejo 70.81 73.75 67.16 60.50 78.19 67.51 77.75 70.78
12 Ren 67.81 59.42 75.90 60.18 77.83 65.77 77.63 57.96
13 Zeman 51.07 49.61 43.50 57.95 50.27 49.57 57.69 48.90
Table 5: Official results of the Joint task, closed challenge. Teams are denoted by the last name (first name added
only where needed) of the author who registered for the evaluation data. Results are sorted in descending order of the
language-averaged macro F1 score on the closed challenge Joint task. Bold numbers denote the best result for a given
language.
Rank Rank in task System Average Catalan Chinese Czech English German Japanese Spanish
1 1 (SRLonly) Zhao 80.47 80.32 77.72 85.19 85.44 75.99 78.15 80.46
2 2 (SRLonly) Nugues 80.31 80.01 78.60 85.41 85.63 79.71 76.30 76.52
3 1 (Joint) Chen 79.96 80.10 76.77 82.04 86.15 76.19 78.17 80.29
4 2 (Joint) Che 79.94 77.10 77.15 86.51 85.51 78.61 78.26 76.47
5 3 (Joint) Merlo 78.42 77.44 76.05 86.02 83.24 71.78 77.23 77.19
6 3 (SRLonly) Meza-Ruiz 77.46 78.00 77.73 75.75 83.34 73.52 76.00 77.91
7 4 (Joint) Bohnet 76.00 74.53 75.29 79.02 80.39 75.72 72.76 74.31
8 5 (Joint) Asahara 75.65 72.35 74.17 84.69 84.26 63.66 77.93 72.50
9 6 (Joint) Brown 72.85 72.18 72.43 78.02 80.43 73.40 61.57 71.95
10 7 (Joint) Dai 70.78 66.34 71.57 75.50 78.93 67.43 71.02 64.64
11 8 (Joint) Zhang 70.31 67.34 73.20 78.28 77.85 62.95 64.71 67.81
12 9 (Joint) Lu Li 69.72 66.95 67.06 79.08 77.17 61.98 69.58 66.23
13 4 (SRLonly) Baoli Li 69.26 74.06 70.37 57.46 69.63 67.76 72.03 73.54
14 10 (Joint) Vallejo 68.95 70.14 66.71 71.49 75.97 61.01 68.82 68.48
15 5 (SRLonly) Moreau 66.49 65.60 67.37 71.74 72.14 66.50 57.75 64.33
16 11 (Joint) Llu??s 63.06 46.79 59.72 76.90 75.86 62.66 71.60 47.88
17 6 (SRLonly) Ta?ckstro?m 61.27 57.11 63.41 71.05 67.64 53.42 54.74 61.51
18 7 (SRLonly) Lin 57.18 61.70 70.33 60.43 65.66 59.51 23.78 58.87
19 12 (Joint) Ren 56.69 41.00 72.58 62.82 67.56 54.31 58.73 39.80
20 13 (Joint) Zeman 32.14 24.19 34.71 58.13 36.05 16.44 30.13 25.36
Table 6: Official results of the semantic labeling, closed challenge, all systems. Teams are denoted by the last name
(first name added only where needed) of the author who registered for the evaluation data. Results are sorted in
descending order of the semantic labeled F1 score (closed challenge). Bold numbers denote the best result for a given
language. Separate ranking is provided for SRL-only systems.
The second column of the table highlights the over-
all architectures. We used + to indicate that the
components are sequentially connected. The lack of
a + sign indicates that the corresponding tasks are
performed jointly.
It is perhaps not surprising that most of the obser-
vations from the 2008 shared task still hold; namely,
the best systems overall do not use joint learning or
optimization (the best such system was placed third
in the Joint task, and there were only four systems
where the learning methodology can be considered
?joint?).
Therefore, most of the observations and conclu-
sions from 2008 shared task hold as well for the
current results. For details, we will leave it to the
reader to interpret the architectures and methods
14
O
v
er
a
ll
D
D
D
PA
PA
PA
Jo
in
t
M
L
Sy
st
em
a
A
rc
h.
b
A
rc
h.
C
o
m
b.
In
fe
re
n
ce
c
A
rc
h.
C
o
m
b.
In
fe
re
n
ce
Le
a
rn
in
g/
O
pt
.
M
et
ho
ds
Zh
ao
PA
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
gr
ee
dy
/g
lo
ba
l
se
ar
ch
(S
R
L-
o
n
ly
)
M
E
N
u
gu
es
(P
C+
A
I+
A
C)
+
A
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
be
am
se
ar
ch
+
re
ra
n
ki
n
g
(S
R
L-
o
n
ly
)
L2
-
re
gu
la
riz
ed
lin
.
re
gr
es
sio
n
Ch
en
P
+
PC
+
A
I+
A
C
gr
ap
h
pa
rt
ia
lly
M
ST
C
L
/E
cl
as
s
n
o
gr
ee
dy
(?)
n
o
M
E
Ch
e
D
+
PC
+
A
IC
gr
ap
h
n
o
M
ST
H
O
E
cl
as
s
n
o
IL
P
n
o
SV
M
,
M
E
M
er
lo
D
PA
IC
+
D
ge
n
er
at
iv
e,
tr
an
s
n
o
be
am
se
ar
ch
tr
an
s
n
o
be
am
se
ar
ch
sy
n
ch
ro
n
iz
ed
de
riv
at
io
n
IS
B
N
M
ez
a-
R
u
iz
PA
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
M
ar
ko
v
LN
n
o
Cu
tti
n
g
Pl
an
e
(S
R
L-
o
n
ly
)
M
IR
A
B
o
hn
et
D
+
A
I+
A
C
+
PC
gr
ap
h
n
o
M
ST
C
+
re
ar
ra
n
ge
cl
as
s
n
o
gr
ee
dy
n
o
SV
M
(M
IR
A
)
A
sa
ha
ra
D
+
PI
C
+
A
IC
gr
ap
h
n
o
M
ST
C
cl
as
s
n
o
n
-
be
st
re
la
x
.
n
o
pe
rc
ep
tr
o
n
D
ai
D
+
PC
+
A
C
gr
ap
h
n
o
M
ST
C
cl
as
s
n
o
pr
o
b
ite
ra
tiv
e
M
E
Zh
an
g
D
+
A
I+
A
C
+
PC
gr
ap
h
n
o
M
ST
E
cl
as
s
n
o
cl
as
sifi
ca
tio
n
n
o
M
IR
A
,
M
E
Lu
Li
D
+
(P
C
||
A
IC
)
gr
ap
h
fo
r
ea
ch
la
n
g.
M
ST
C
L
/E
,
M
ST
E
cl
as
s
n
o
gr
ee
dy
n
o
M
E
B
ao
li
Li
PC
+
A
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
gr
ee
dy
(S
R
L-
o
n
ly
)
SV
M
,
kN
N
,
M
E
Va
lle
jod
[D
+
P+
A
]C
+
D
I
cl
as
s
n
o
re
ra
n
ki
n
g
cl
as
s
n
o
re
ra
n
ki
n
g
u
n
ifi
ed
la
be
ls
M
B
L
M
o
re
au
D
+
PI
+
Cl
u
st
er
in
g
+
A
I+
A
C
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
CR
F
(S
R
L-
o
n
ly
)
CR
F
Ll
u
??s
D
+
D
A
IC
+
PC
gr
ap
h
n
o
M
ST
E
gr
ap
h
n
o
M
ST
E
ye
s,
M
ST
E
Av
g.
Pe
rc
ep
tr
o
n
Ta?
ck
st
ro?
m
D
+
PI
+
A
I
+
A
C
+
Co
n
st
ra
in
tS
at
isf
ac
tio
n
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
gr
ee
dy
(S
R
L-
o
n
ly
)
SV
M
R
en
D
+
PC
+
A
IC
tr
an
s
n
o
gr
ee
dy
cl
as
s
n
o
gr
ee
dy
n
o
SV
M
(M
al
t),
M
E
Ze
m
an
D
I+
D
C+
PC
+
A
I+
A
C
tr
an
s
n
o
gr
ee
dy
w
ith
he
u
ris
tic
s
cl
as
s
n
o
gr
ee
dy
n
o
co
o
cc
u
rr
en
ce
Ta
bl
e
7:
Su
m
m
ar
y
o
fs
ys
te
m
ar
ch
ite
ct
u
re
s
fo
r
th
e
Co
N
LL
-
20
09
sh
ar
ed
ta
sk
;
al
ls
ys
te
m
s
ar
e
in
cl
u
de
d.
SR
L-
o
n
ly
sy
st
em
s
do
n
o
t
ha
v
e
th
e
D
co
lu
m
n
s
an
d
th
e
Jo
in
t
Le
ar
in
g/
O
pt
.
co
lu
m
n
s
fil
le
d
in
.
Th
e
sy
st
em
s
ar
e
so
rt
ed
by
th
e
se
m
an
tic
la
be
le
d
F 1
sc
o
re
av
er
ag
ed
o
v
er
al
lt
he
la
n
gu
ag
es
(sa
m
e
as
in
Ta
bl
e
6).
O
n
ly
th
e
sy
st
em
s
th
at
ha
v
e
a
co
rr
es
po
n
di
n
g
pa
pe
r
in
th
e
pr
o
ce
ed
in
gs
ar
e
in
cl
u
de
d.
A
cr
o
n
ym
s
u
se
d:
D
-
sy
n
ta
ct
ic
de
pe
n
de
n
ci
es
,
P
-
pr
ed
ic
at
e,
A
-
ar
gu
m
en
t,
I-
id
en
tifi
ca
tio
n
,
C
-
cl
as
sifi
ca
tio
n
.
O
v
er
a
ll
a
rc
h.
st
an
ds
fo
r
th
e
co
m
pl
et
e
sy
st
em
ar
ch
ite
ct
u
re
;D
A
rc
h.
st
an
ds
fo
r
th
e
ar
ch
ite
ct
u
re
o
ft
he
sy
n
ta
ct
ic
pa
rs
er
;D
C
o
m
b.
in
di
ca
te
s
if
th
e
fin
al
pa
rs
er
o
u
tp
u
tw
as
ge
n
er
at
ed
u
sin
g
pa
rs
er
co
m
bi
n
at
io
n
;D
In
fe
re
n
ce
st
an
ds
fo
r
th
e
ty
pe
o
fi
n
fe
re
n
ce
u
se
d
fo
r
sy
n
ta
ct
ic
pa
rs
in
g;
PA
A
rc
h.
st
an
ds
th
e
ty
pe
o
fa
rc
hi
te
ct
u
re
u
se
d
fo
r
PA
IC
;P
A
C
o
m
b.
in
di
ca
te
s
if
th
e
PA
o
u
tp
u
t
w
as
ge
n
er
at
ed
th
ro
u
gh
sy
st
em
co
m
bi
n
at
io
n
;P
A
In
fe
re
n
ce
st
an
ds
fo
r
th
e
th
e
ty
pe
o
fi
n
fe
re
n
ce
u
se
d
fo
r
PA
IC
;J
o
in
tL
ea
rn
in
g/
O
pt
.
in
di
ca
te
s
if
so
m
e
fo
rm
o
fjo
in
tl
ea
rn
in
g
o
r
o
pt
im
iz
at
io
n
w
as
im
pl
em
en
te
d
fo
r
th
e
sy
n
ta
ct
ic
+
se
m
an
tic
gl
o
ba
lt
as
k;
M
L
M
et
ho
ds
lis
ts
th
e
M
L
m
et
ho
ds
u
se
d
th
ro
u
gh
o
u
tt
he
co
m
pl
et
e
sy
st
em
.
a
A
u
th
o
rs
o
ft
w
o
sy
st
em
s:
?
B
ro
w
n
?
an
d
?
Li
n
?
di
dn
?
ts
u
bm
it
a
pa
pe
r,
so
th
ei
r
sy
st
em
s?
ar
ch
ite
ct
u
re
s
ar
e
u
n
kn
ow
n
.
b T
he
sy
m
bo
l+
in
di
ca
te
s
se
qu
en
tia
lp
ro
ce
ss
in
g
(ot
he
rw
ise
,
pa
ra
lle
l/jo
in
t).
Th
e
||
m
ea
n
s
th
at
se
v
er
al
di
ffe
re
n
ta
rc
hi
te
ct
u
re
s
sp
an
n
in
g
m
u
lti
pl
e
su
bt
as
ks
ra
n
in
pa
ra
lle
l.
c
M
ST
C
L
/E
as
u
se
d
by
M
cD
o
n
al
d
(20
05
),
M
ST
C
by
Ca
rr
er
as
(20
07
),M
ST
E
by
Ei
sn
er
(20
00
),
M
ST
H
O
E
=
M
ST
E
w
ith
hi
gh
er
-
o
rd
er
fe
at
u
re
s
(si
bl
in
gs
+
al
lg
ra
n
dc
hi
ld
re
n
).
d T
he
sy
st
em
u
n
ifi
es
th
e
sy
n
ta
ct
ic
an
d
se
m
an
tic
la
be
ls
in
to
o
n
e
la
be
l,
an
d
tr
ai
n
s
cl
as
sifi
er
s
o
v
er
th
em
.
It
is
th
u
s
di
ffi
cu
lt
to
sp
lit
th
e
sy
st
em
ch
ar
ac
te
ris
tic
in
to
a
?
D
?
/?
PA
?
pa
rt
.
15
when comparing Table 7 with the Tables 5 and 6).
6 Conclusion
This year?s task has been demanding in several re-
spects, but certainly the most difficulty came from
the fact that participants had to tackle all seven lan-
guages. It is encouraging that despite this added af-
fort the number of participating systems has been
almost the same as last year (20 vs. 22 in 2008).
There are several positive outcomes from this
year?s enterprise:
? we have prepared a unified format and data for
several very different lanaguages, as a basis
for possible extensions towards other languages
and unified treatment of syntactic depenndecies
and semantic role labeling across natural lan-
guages;
? 20 participants have produced SRL results for
all seven languages, using several different
methods, giving hope for a combined system
with even substantially better performance;
? initial results have been provided for three lan-
guages on out-of-domain data (being in fact
quite close to the in-domain results).
Only four systems tried to apply what can be de-
scribed as joint learning for the syntactic and seman-
tic parts of the task. (Morante et al, 2009) use a true
joint learning formulation that phrases syntactico-
semantic parsing as a series of classification where
the class labels are concatenations of syntactic and
semantic edge labels. They predict (a), the set of
syntactico-semantic edge labels for each pair of to-
kens; (b), the set of incoming syntactico-semantic
edge labels for each individual token; and (c), the
existence of an edge between each pair of tokens.
Subsequently, they combine the (possibly conflict-
ing) output of the three classifiers by a ranking ap-
proach to determine the most likely structure that
meets all well-formedness constraints. (Llu??s et al,
2009) present a joint approach based on an exten-
sion of Eisner?s parser to accommodate also seman-
tic dependency labels. This architecture is similar
to the one presented by the same authors in the past
edition, with the extension to a second-order syn-
tactic parsing and a particular setting for Catalan
and Spanish. (Gesmundo et al, 2009) use an in-
cremental parsing model with synchronous syntac-
tic and semantic derivations and a joint probability
model for syntactic and semantic dependency struc-
tures. The system uses a single input queue but two
separate stacks and synchronizes syntactic and se-
mantic derivations at every word. The synchronous
derivations are modeled with an Incremental Sig-
moid Belief Network that has latent variables for
both syntactic and semantic states and connections
from syntax to semantics and vice versa. (Dai et
al., 2009) designed an iterative system to exploit
the inter-connections between the different subtasks
of the CoNLL shared task. The idea is to decom-
pose the joint learning problem into four subtasks
? syntactic dependency identification, syntactic de-
pendency labeling, semantic dependency identifica-
tion and semantic dependency labeling. The initial
step is to use a pipeline approach to use the input of
one subtask as input to the next, in the order speci-
fied. The iterative steps then use additional features
that are not available in the initial step to improve the
accuracy of the overall system. For example, in the
iterative steps, semantic information becomes avail-
able as features to syntactic parsing, so on and so
forth.
Despite these results, it is still not clear whether
joint learning has a significant advantage over other
approaches (and if yes, then for what languages). It
is thus necessary to carefully plan the next shared
tasks; it might be advantageous to bring up a sim-
ilar task in the future once again, and/or couple it
with selected application(s). There, (we hope) the
benefits of the dependency representation combined
with semantic roles the way we have formulated it
in 2008 and 2009 will really show up.
Acknowledgments
We would like to thank the Linguistic Data Consor-
tium, mainly to Denise DiPersio, Tony Casteletto
and Christopher Cieri for their help and handling
of invoicing and distribution of the data for which
LDC has a license. For all of the trial, training and
evaluation data they had to act a very short notice.
All the data has been at the participants? disposal
(again) free of charge. We are grateful to all of them
for LDC?s continuing support of the CoNLL Shared
16
Tasks.
We would also like to thank organizers of the pre-
vious four shared tasks: Sabine Buchholz, Xavier
Carreras, Ryan McDonald, Amit Dubey, Johan Hall,
Yuval Krymolowski, Sandra Ku?bler, Erwin Marsi,
Jens Nilsson, Sebastian Riedel and Deniz Yuret.
This shared task would not have been possible with-
out their previous effort.
We also acknowledge the support of the M?SMT
of the Czech Republic, projects MSM0021620838
and LC536; the Grant Agency of the Academy of
sciences of the Czech Republic 1ET201120505 (for
Jan Hajic?, Jan ?Ste?pa?nek and Pavel Stran?a?k).
Llu??s Ma`rquez and M. Anto`nia Mart?? partici-
pation was supported by the Spanish Ministry of
Education and Science, through the OpenMT and
TextMess research projects (TIN2006-15307-C03-
02, TIN2006-15265-C06-06).
The following individuals directly contributed to
the Chinese Treebank (in alphabetic order): Meiyu
Chang, Fu-Dong Chiou, Shizhe Huang, Zixin Jiang,
Tony Kroch, Martha Palmer, Mitch Marcus, Fei
Xia, Nianwen Xue. The contributors to the Chi-
nese Proposition Bank include (in alphabetic order):
Meiyu Chang, Gang Chen, Helen Chen, Zixin Jiang,
Martha Palmer, Zhiyi Song, Nianwen Xue, Ping Yu,
Hua Zhong. The Chinese Treebank and the Chinese
Proposition Bank were funded by DOD, NSF and
DARPA.
Adam Meyers? work on the shared task has been
supported by the NSF Grant IIS-0534700 ?Structure
Alignment-based MT.?
We thank the Mainichi Newspapers for the per-
mission of distributing the sentences of the Kyoto
University Text Corpus for this shared task.
References
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER tree-
bank. In Proceedings of the Workshop on Treebanks
and Linguistic Theories, Sozopol.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC-2006), Genoa, Italy.
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proceedings of
EMNLP-CoNLL 2007, pages 957?961, June. Prague,
Czech Republic.
Montserrat Civit, M. Anto`nia Mart??, and Nu?ria Buf??.
2006. Cat3LB and Cast3LB: from constituents to
dependencies. In Proceedings of the 5th Interna-
tional Conference on Natural Language Processing,
FinTAL, pages 141?153, Turku, Finland. Springer Ver-
lag, LNAI 4139.
Qifeng Dai, Enhong Chen, and Liu Shi. 2009. An it-
erative approach for joint dependency parsing and se-
mantic role labeling. In Proceedings of the 13th Con-
ference on Computational Natural Language Learning
(CoNLL-2009), June 4-5, Boulder, Colorado, USA.
June 4-5.
Jason Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. In Harry Bunt and Anton
Nijholt, editors, Advances in Probabilistic and Other
Parsing Tehcnologies, pages 29?62. Kluwer Academic
Publishers.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. The MIT Press, Cambridge.
Andrea Gesmundo, James Henderson, Paola Merlo, and
Ivan Titov. 2009. A latent variable model of syn-
chronous syntactic-semantic parsing for multiple lan-
guages. In Proceedings of the 13th Conference on
Computational Natural Language Learning (CoNLL-
2009), June 4-5, Boulder, Colorado, USA. June 4-5.
Jan Hajic?, Jarmila Panevova?, Zden?ka Ures?ova?, Alevtina
Be?mova?, Veronika Kola?r?ova?- ?Rezn??c?kova??, and Petr
Pajas. 2003. PDT-VALLEX: Creating a Large-
coverage Valency Lexicon for Treebank Annotation.
In J. Nivre and E. Hinrichs, editors, Proceedings of The
Second Workshop on Treebanks and Linguistic Theo-
ries, pages 57?68, Vaxjo, Sweden. Vaxjo University
Press.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan ?Ste?pa?nek, Jir??? Havelka, Marie
Mikulova?, and Zdene?k ?Zabokrtsky?. 2006. Prague De-
pendency Treebank 2.0.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), pages 2008?2013, Las Palmas, Canary
Islands.
Xavier Llu??s, Stefan Bott, and Llu??s Ma`rquez. 2009.
A second-order joint eisner model for syntactic and
semantic dependency parsing. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning (CoNLL-2009), June 4-5, Boulder,
Colorado, USA. June 4-5.
17
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1994. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguistics,
19(2):313?330.
Llu??s Ma`rquez, Luis Villarejo, M. Anto`nia Mart??, and
Mariona Taule?. 2007. SemEval-2007 Task 09: Mul-
tilevel semantic annotation of catalan and spanish.
In Proceedings of the 4th International Workshop on
Semantic Evaluations (SemEval-2007), pages 42?47,
Prague, Czech Republic.
M. Anto`nia Mart??, Mariona Taule?, Llu??s Ma`rquez, and
Manu Bertran. 2007. Anotacio?n semiautoma?tica
con papeles tema?ticos de los corpus CESS-ECE.
Procesamiento del Lenguaje Natural, SEPLN Journal,
38:67?76.
Ryan McDonald, Fernando Pereira, Jan Hajic?, and Kiril
Ribarov. 2005. Non-projective dependency parsing
using spanning tree algortihms. In Proceedings of
NAACL-HLT?05, Vancouver, Canada, pages 523?530.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. The Nom-
Bank Project: An Interim Report. In NAACL/HLT
2004 Workshop Frontiers in Corpus Annotation,
Boston.
Roser Morante, Vincent Van Asch, and Antal van den
Bosch. 2009. A simple generative pipeline approach
to dependency parsing and semantic role labeling. In
Proceedings of the 13th Conference on Computational
Natural Language Learning (CoNLL-2009), Boulder,
Colorado, USA. June 4-5.
Joakim Nivre, Johann Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The conll 2007 shared task on depen-
dency parsing. In Proceedings of the EMNLP-CoNLL
2007 Conference, pages 915?932, Prague, Czech Re-
public.
Sebastian Pado and Mirella Lapata. 2005. Cross-lingual
projection of role-semantic information. In Proceed-
ings of the Human Language Technology Conference
and Conference on Empirical Methods in Natural Lan-
guage Processing (HLT/EMNLP-2005), pages 859?
866, Vancouver, BC.
Petr Pajas and Jan ?Ste?pa?nek. 2008. Recent advances in
a feature-rich framework for treebank annotation. In
The 22nd International Conference on Computational
Linguistics - Proceedings of the Conference (COL-
ING?08), pages 673?680, Manchester.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of Interna-
tional Conference on New Methods in Language Pro-
cessing.
Drahom??ra ?Johanka? Spoustova?, Jan Hajic?, Jan Raab,
and Miroslav Spousta. 2009. Semi-supervised train-
ing for the averaged perceptron POS tagger. In Pro-
ceedings of the European ACL Cenference EACL?09,
Athens, Greece.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proceedings of the 12th Con-
ference on Computational Natural Language Learning
(CoNLL-2008), pages 159?177.
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proceedings of the 6th
International Conference on Language Resources and
Evaluation (LREC-2008), Marrakesh, Morroco.
Martin ?Cmejrek, Jan Cur???n, Jan Hajic?, Jir??? Havelka,
and Vladislav Kubon?. 2004. Prague Czech-English
Dependency Treebank: Syntactically Anntoated Re-
sources for Machine Translation. In Proceedings of
the 4th International Conference on Language Re-
sources and Evaluation (LREC-2004), pages 1597?
1600, Lisbon, Portugal.
W. N. Francis and H. Kucera. 1964. Brown Corpus Man-
ual of Information to accompany A Standard Corpus
of Present-Day Edited American English, for use with
Digital Computers. Revised 1971, Revised and Am-
plified 1979, available at www.clarinet/brown.
R. Weischedel and A. Brunstein. 2005. BBN pronoun
coreference and entity type corpus. Technical report,
Lin- guistic Data Consortium.
Nianwen Xue and Martha Palmer. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143?172.
Nianwen Xue, Fei Xia, Fu Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
Structure Annotation of a Large Corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Andrea Zielinski and Christian Simon. 2008. Morphisto:
An open-source morphological analyzer for german.
In Proceedings of the Conference on Finite State Meth-
ods in Natural Language Processing.
18
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 519?527,
Beijing, August 2010
Reranking Models in Fine-grained Opinion Analysis
Richard Johansson and Alessandro Moschitti
University of Trento
{johansson, moschitti}@disi.unitn.it
Abstract
We describe the implementation of
reranking models for fine-grained opinion
analysis ? marking up opinion expres-
sions and extracting opinion holders. The
reranking approach makes it possible
to model complex relations between
multiple opinions in a sentence, allowing
us to represent how opinions interact
through the syntactic and semantic
structure. We carried out evaluations on
the MPQA corpus, and the experiments
showed significant improvements over a
conventional system that only uses local
information: for both tasks, our system
saw recall boosts of over 10 points.
1 Introduction
Recent years have seen a surge of interest in the
automatic processing of subjective language. The
technologies emerging from this research have ob-
vious practical uses, either as stand-alone appli-
cations or supporting other NLP tools such as
information retrieval or question answering sys-
tems. While early efforts in subjectivity analysis
focused on coarse-grained tasks such as retriev-
ing the subjective documents from a collection,
most recent work on this topic has focused on fine-
grained tasks such as determining the attitude of a
particular person on a particular topic. The devel-
opment and evaluation of such systems has been
made possible by the release of manually anno-
tated resources using fairly fine-grained represen-
tations to describe the structure of subjectivity in
language, for instance the MPQA corpus (Wiebe
et al, 2005).
A central task in the automatic analysis of sub-
jective language is the indentification of subjective
expressions: the text pieces that allow us to draw
the conclusion that someone has a particular feel-
ing about something. This is necessary for fur-
ther analysis, such as the determination of opin-
ion holder and the polarity of the opinion. The
MPQA corpus defines two types of subjective ex-
pressions: direct subjective expressions (DSEs),
which are explicit mentions of attitude, and ex-
pressive subjective elements (ESEs), which signal
the attitude of the speaker by the choice of words.
The prototypical example of a DSE would be a
verb of statement or categorization such as praise
or disgust, and the opinion holder would typi-
cally be a direct semantic argument of this verb.
ESEs, on the other hand, are less easy to cate-
gorize syntactically; prototypical examples would
include value-expressing adjectives such as beau-
tiful and strongly charged words like appease-
ment, while the relation between the expression
and the opinion holder is typically less clear-cut
than for DSEs. In addition to DSEs and ESEs, the
MPQA corpus also contains annotation for non-
subjective statements, which are referred to as ob-
jective speech events (OSEs).
Examples (1) and (2) show two sentences from
the MPQA corpus where DSEs and ESEs have
been manually annotated.
(1) He [made such charges]DSE [despite the
fact]ESE that women?s political, social and cul-
tural participation is [not less than that]ESE of
men.
(2) [However]ESE , it is becoming [rather
fashionable]ESE to [exchange harsh words]DSE
with each other [like kids]ESE .
The task of marking up these expressions has
usually been approached using straightforward
sequence labeling techniques using using simple
features in a small contextual window (Choi et
al., 2006; Breck et al, 2007). However, due to
519
the simplicity of the feature sets, this approach
fails to take into account the fact that the semantic
and pragmatic interpretation of sentences is not
only determined by words but also by syntactic
and shallow-semantic relations. Crucially, taking
grammatical relations into account allows us to
model how expressions interact in various ways
that influence their interpretation as subjective
or not. Consider, for instance, the word said in
examples (3) and (4) below, where the interpre-
tation as a DSE or an OSE is influenced by the
subjective content of the enclosed statement.
(3) ?We will identify the [culprits]ESE of these
clashes and [punish]ESE them,? he [said]DSE .
(4) On Monday, 80 Libyan soldiers disembarked
from an Antonov transport plane carrying military
equipment, an African diplomat [said]OSE .
In addition, the various opinions expressed in
a sentence are very interdependent when it comes
to the resolution of their holders, i.e. determining
the entity that harbors the sentiment manifested
textually in the opinion expression. Clearly, the
structure of the sentence is influential also for this
task: an ESE will be quite likely to be linked to
the same opinion holder as a DSE directly above
it in the syntactic tree.
In this paper, we demonstrate how syntactic
and semantic structural information can be used
to improve the detection of opinion expressions
and the extraction of opinion holders. While this
feature model makes it impossible to use the stan-
dard sequence labeling method, we show that with
a simple strategy based on reranking, incorporat-
ing structural features results in a significant im-
provement. In an evaluation on the MPQA corpus,
the best system we evaluated, a reranker using the
Passive?Aggressive learning algorithm, achieved
a 10-point absolute improvement in soft recall,
and a 5-point improvement in F-measure, over the
baseline sequence labeler. Similarly, the recall is
boosted by almost 11 points for the holder extrac-
tion (3 points in F-measure) by modeling the inter-
action of opinion expressions with respect to hold-
ers.
2 Related Work
Since the most significant body of work in sub-
jectivity analysis has been dedicated to coarse-
grained tasks such as document polarity classi-
fication, most approaches to analysing the senti-
ment of natural-language text have relied funda-
mentally on purely lexical information (see (Pang
et al, 2002; Yu and Hatzivassiloglou, 2003), in-
ter alia) or low-level grammatical information
such as part-of-speech tags and functional words
(Wiebe et al, 1999). This is not unexpected since
these problems have typically been formulated as
text categorization problems, and it has long been
agreed in the information retrieval community that
very little can be gained by complex linguistic
processing for tasks such as text categorization
and search (Moschitti and Basili, 2004).
As the field moves towards increasingly sophis-
ticated tasks requiring a detailed analysis of the
text, the benefit of syntactic and semantic analy-
sis becomes more clear. For the task of subjec-
tive expression detection, Choi et al (2006) and
Breck et al (2007) used syntactic features in a se-
quence model. In addition, syntactic and shallow-
semantic relations have repeatedly proven useful
for subtasks of subjectivity analysis that are in-
herently relational, above all for determining the
holder or topic of a given opinion. Choi et al
(2006) is notable for the use of a global model
based on hand-crafted constraints and an integer
linear programming optimization step to ensure a
globally consistent set of opinions and holders.
Works using syntactic features to extract top-
ics and holders of opinions are numerous (Bethard
et al, 2005; Kobayashi et al, 2007; Joshi and
Penstein-Rose?, 2009; Wu et al, 2009). Seman-
tic role analysis has also proven useful: Kim
and Hovy (2006) used a FrameNet-based seman-
tic role labeler to determine holder and topic of
opinions. Similarly, Choi et al (2006) success-
fully used a PropBank-based semantic role labeler
for opinion holder extraction. Ruppenhofer et al
(2008) argued that semantic role techniques are
useful but not completely sufficient for holder and
topic identification, and that other linguistic phe-
nomena must be studied as well. One such lin-
guistic pheonomenon is the discourse structure,
520
which has recently attracted some attention in the
subjectivity analysis community (Somasundaran
et al, 2009).
3 Modeling Interaction over Syntactic
and Semantic Structure
Previous systems for opinion expression markup
have typically used simple feature sets which have
allowed the use of efficient off-the-shelf sequence
labeling methods based on Viterbi search (Choi et
al., 2006; Breck et al, 2007). This is not pos-
sible in our case since we would like to extract
structural, relational features that involve pairs of
opinion expressions and may apply over an arbi-
trarily long distance in the sentence.
While it is possible that search algorithms for
exact or approximate inference can be construc-
tured for the argmax problem in this model, we
sidestepped this issue by using a reranking de-
composition of the problem:
? Apply a standard Viterbi-based sequence la-
beler based on local context features but no
structural interaction features. Generate a
small candidate set of size k.
? Generate opinion holders for every proposed
opinion expression.
? Apply a complex model using interaction
features to pick the top candidate from the
candidate set.
The advantages of a reranking approach com-
pared to more complex approaches requiring ad-
vanced search techniques are mainly simplicity
and efficiency: this approach is conceptually sim-
ple and fairly easy to implement provided that k-
best output can be generated efficiently, and fea-
tures can be arbitrarily complex ? we don?t have to
think about how the features affect the algorithmic
complexity of the inference step. A common ob-
jection to reranking is that the candidate set may
not be diverse enough to allow for much improve-
ment unless it is very large; the candidates may
be trivial variations that are all very similar to the
top-scoring candidate.
3.1 Syntactic and Semantic Structures
We used the syntactic?semantic parser by Johans-
son and Nugues (2008) to annnotate the sen-
tences with dependency syntax (Mel?c?uk, 1988)
and shallow semantic structures in the PropBank
(Palmer et al, 2005) and NomBank (Meyers et al,
2004) frameworks. Figure 1 shows an example
of the annotation: The sentence they called him a
liar, where called is a DSE and liar is an ESE, has
been annotated with dependency syntax (above
the text) and PropBank-based semantic role struc-
ture (below the text). The predicate called, which
is an instance of the PropBank frame call.01,
has three semantic arguments: the Agent (A0), the
Theme (A1), and the Predicate (A2), which are re-
alized on the surface-syntactic level as a subject,
a direct object, and an object predicative comple-
ment, respectively.
]ESEThey called
call.01
SBJ
OPRD
liarhim[ [a
A1A0 A2
]DSE
NMODOBJ
Figure 1: Syntactic and shallow semantic struc-
ture.
3.2 Base Sequence Labeling Model
To solve the first subtask, we implemented a stan-
dard sequence labeler for subjective expression
markup, similar to the approach by Breck et al
(2007). We encoded the opinionated expression
brackets using the IOB2 encoding scheme (Tjong
Kim Sang and Veenstra, 1999) and trained the
model using the metod by Collins (2002).
The sequence labeler used word, POS tag, and
lemma features in a window of size 3. In addi-
tion, we used prior polarity and intensity features
derived from the lexicon created by Wilson et al
(2005). It is important to note that prior subjec-
tivity does not always imply subjectivity in a par-
ticular context; this is why contextual features are
essential for this task.
This sequence labeler was used to generate the
candidate set for the reranker. To generate rerank-
ing training data, we carried out a 5-fold hold-out
procedure: We split the training set into 5 pieces,
521
trained a sequence labeler on pieces 1 to 4, applied
it to piece 5 and so on.
3.3 Base Opinion Holder Extractor
For every opinion expression, we extracted opin-
ion holders, i.e. mentions of the entity holding
the opinion denoted by the opinion expression.
Since the problem of holder extraction is in many
ways similar to semantic argument detection ?
when the opinion expression is a verb, finding the
holder typically entails finding a SPEAKER argu-
ment ? we approached this problem using meth-
ods inspired by semantic role labeling. We thus
trained support vector machines using the LIB-
LINEAR software (Fan et al, 2008), and applied
them to the noun phrases in the same sentence
as the holder. Separate classifiers were trained to
extract holders for DSEs, ESEs, and OSEs. The
classifiers used the following feature set:
SYNTACTIC PATH. Similarly to the path fea-
ture widely used in SRL, we extract a feature
representing the path in the dependency tree
between the expression and the holder (Jo-
hansson and Nugues, 2008). For instance,
the path from the DSE called to the holder
They is SBJ?.
SHALLOW-SEMANTIC RELATION. If there is a
direct shallow-semantic relation between the
expression and the holder, use a feature rep-
resenting its semantic role, such as A0 for
They with respect to called.
EXPRESSION HEAD WORD AND POS.
HOLDER HEAD WORD AND POS.
DOMINATING EXPRESSION TYPE.
CONTEXT WORDS AND POS FOR HOLDER.
EXPRESSION VERB VOICE.
However, there are also differences compared
to typical argument extraction in SRL. First, it is
important to note that the MPQA corpus does not
annotate direct links from opinions to a holders,
but from opinions to holder coreference chains.
To handle this issue, we created positive training
instances for allmembers of the coreference chain
in the same sentence as the opinion, and negative
instances for the other noun phrases.
Secondly, an opinion may be linked not to an
overt noun phrase in a sentence, but to an im-
plicit holder; a special case of implicit holder is
the writer of the text. We trained separate clas-
sifiers to detect these situations. These classifiers
did not use the features requiring a holder phrase.
Finally, there is a restriction that every expres-
sion may have at most one holder, so at test time
we select only the highest-scoring opinion holder
candidate.
3.4 Opinion Expression Reranker Features
The rerankers use two types of structural fea-
tures: syntactic features extracted from the depen-
dency tree, and semantic features extracted from
the predicate?argument (semantic role) graph.
The syntactic features are based on paths
through the dependency tree. This creates a small
complication for multiword opinion expressions;
we select the shortest possible path in such cases.
For instance, in example (1) above, the path will
be computed betweenmade and despite, and in (2)
between fashionable and exchange.
We used the following syntactic interaction fea-
tures:
SYNTACTIC PATH. Given a pair opinion ex-
pressions, we use a feature representing
the labels of the two expressions and the
path between them through the syntactic
tree. For instance, for the DSE called
and the ESE liar in Figure 1, we represent
the syntactic configuration using the feature
DSE:OPRD?:ESE, meaning that the path
from the DSE to the ESE follows an OPRD
link downward.
LEXICALIZED PATH. Same as above,
but with lexical information attached:
DSE/called:OPRD?:ESE/liar.
DOMINANCE. In addition to the features based
on syntactic paths, we created a more generic
feature template describing dominance re-
lations between expressions. For instance,
from the graph in Figure 1, we extract the
feature DSE/called?ESE/liar, mean-
ing that a DSE with the word called domi-
nates an ESE with the word liar.
The semantic features were the following:
522
PREDICATE SENSE LABEL. For every pred-
icate found inside an opinion expression,
we add a feature consisting of the expres-
sion label and the predicate sense identi-
fier. For instance, the verb call which is
also a DSE is represented with the feature
DSE/call.01.
PREDICATE AND ARGUMENT LABEL. For ev-
ery argument of a predicate inside an opin-
ion expression, we also create a feature
representing the predicate?argument pair:
DSE/call.01:A0.
CONNECTING ARGUMENT LABEL. When a
predicate inside some opinion expression is
connected to some argument inside another
opinion expression, we use a feature con-
sisting of the two expression labels and the
argument label. For instance, the ESE liar
is connected to the DSE call via an A2 la-
bel, and we represent this using a feature
DSE:A2:ESE.
Apart from the syntactic and semantic features,
we also used the score output from the base se-
quence labeler as a feature. We normalized the
scores over the k candidates so that their expo-
nentials summed to 1.
3.5 Opinion Holder Reranker Features
In addition, we modeled the interaction between
different opinions with respect to their holders.
We used the following two features to represent
this interaction:
SHARED HOLDERS. A feature representing
whether or not two opinion expressions have
the same holder. For instance, if a DSE
dominates an ESE and they have the same
holder as in Figure 1 where the holder
is They, we represent this by the feature
DSE:ESE:true.
HOLDER TYPES + PATH. A feature repre-
senting the types of the holders, combined
with the syntactic path between the expres-
sions. The types take the following pos-
sible values: explicit, implicit, writer. In
Figure 1, we would thus extract the feature
DSE/Expl:OPRD?:ESE/Expl.
Similar to base model feature for the expression
detection, we also used a feature for the output
score from the holder extraction classifier.
3.6 Training the Reranker
We trained the reranker using the method em-
ployed by many rerankers following Collins
(2002), which learns a scoring function that is
trained to maximize performance on the rerank-
ing task. While there are batch learning algo-
rithms that work in this setting (Tsochantaridis
et al, 2005), online learning methods have been
more popular for performance reasons. We inves-
tigated two online learning algorithms: the popu-
lar structured perceptron (Collins, 2002) and the
Passive?Aggressive (PA) algorithm (Crammer et
al., 2006). To increase robustness, we used an
averaged implementation (Freund and Schapire,
1999) of both algorithms.
The difference between the two algorithms is
the way the weight vector is incremented in each
step. In the perceptron, for a given input x, we
update based on the difference between the correct
output y and the predicted output y?, where? is the
feature representation function:
y? ? argmaxh w ? ?(x, h)
w ? w + ?(x, y)? ?(x, y?)
In the PA algorithm, which is based on the the-
ory of large-margin learning, we instead find the
y? that violates the margin constraints maximally.
The update step length ? is computed based on the
margin; this update is bounded by a regularization
constant C:
y? ? argmaxh w ? ?(x, h) +
?
?(y, h)
? ? min
(
C, w(?(x,y?)??(x,y))+
?
?(y,y?)
??(x,y?)??(x,y)?2
)
w ? w + ?(?(x, y)? ?(x, y?))
The algorithm uses a cost function ?. We used
the function ?(y, y?) = 1 ? F (y, y?), where F is
the soft F-measure described in Section 4.1. With
this approach, the learning algorithm thus directly
optimizes the measure we are interested in, i.e. the
F-measure.
4 Experiments
We carried out the experiments on version 2 of
the MPQA corpus (Wiebe et al, 2005), which we
523
split into a test set (150 documents, 3,743 sen-
tences) and a training set (541 documents, 12,010
sentences).
4.1 Evaluation Metrics
Since expression boundaries are hard to define ex-
actly in annotation guidelines (Wiebe et al, 2005),
we used soft precision and recall measures to
score the quality of the system output. To de-
rive the soft precision and recall, we first define
the span coverage c of a span s with respect to
another span s?, which measures h ow well s? is
covered by s:
c(s, s?) = |s ? s
?|
|s?|
In this formula, the operator | ? | counts tokens, and
the intersection ? gives the set of tokens tha t two
spans have in common. Since our evaluation takes
span labels (DSE, ESE, OSE) into account, we set
c(s, s?) to zero if the labels associated with s and
s? are different.
Using the span coverage, we define the span set
coverage C of a set of spans S with respect to a
set S?:
C(S,S?) =
?
sj?S
?
s?k?S?
c(sj , s?k)
We now define the soft precision P and recall
R of a proposed set of spans S? with respect to a
gold standard set S as follows:
P (S, S?) = C(S,S?)|S?| R(S, S?) =
C(S?,S)
|S|
Note that the operator | ? | counts spans in this for-
mula.
Conventionally, when measuring the quality of
a system for an information extraction task, a pre-
dicted entity is counted as correct if it exactly
matches the boundaries of a corresponding en-
tity in the gold standard; there is thus no reward
for close matches. However, since the boundaries
of the spans annotated in the MPQA corpus are
not strictly defined in the annotation guidelines
(Wiebe et al, 2005), measuring precision and re-
call using exact boundary scoring will result in
figures that are too low to be indicative of the
usefulness of the system. Therefore, most work
using this corpus instead use overlap-based preci-
sion and recall measures, where a span is counted
as correctly detected if it overlaps with a span in
the gold standard (Choi et al, 2006; Breck et al,
2007). As pointed out by Breck et al (2007), this
is problematic since it will tend to reward long
spans ? for instance, a span covering the whole
sentence will always be counted as correct if the
gold standard contains any span for that sentence.
The precision and recall measures proposed
here correct the problem with overlap-based mea-
sures: If the system proposes a span covering the
whole sentence, the span coverage will be low
and result in a low soft precision. Note that our
measures are bounded below by the exact mea-
sures and above by the overlap-based measures:
replacing c(s, s?) with ?c(s, s?)? gives the exact
measures and replacing c(s, s?) with ?c(s, s?)? the
overlap-based measures.
To score the extraction of opinion holders, we
started from the same basic approach. However,
the evaluation of this task is more complex be-
cause a) we only want to give credit for holders
for correctly extracted opinion expressions; b) the
gold standard links opinion expressions to coref-
erence chains rather than individual mentions of
holders; c) the holder may be the writer or im-
plicit (see 3.3). We therefore used the following
method: Given a holder h linked to an expres-
sion e, we first located the expression e? in the
gold standard that most closely corresponds to e,
that is e? = argmaxx c(x, e), regardless of the
labels of e and e?. We then located the gold stan-
dard holder h? by finding the closest correspond-
ing holder in the coreference chain H linked to e?:
h? = argmaxx?H c(x, h). If h is proposed as the
writer, we score it as perfectly detected (coverage
1) if the coreference chain H contains the writer,
and a full error (coverage 0) otherwise, and simi-
lar if h is implicit.
4.2 Machine Learning Methods
We compared the machine learning methods de-
scribed in Section 3. In these experiments, we
used a candidate set size k of 8. Table 1 shows
the results of the evaluations using the precision
and recall measures described above. The base-
line is the result of taking the top-scoring labeling
524
from the base model.
System P R F
Baseline 63.36 46.77 53.82
Perceptron 62.84 48.13 54.51
PA 63.50 51.79 57.04
Table 1: Evaluation of reranking learning meth-
ods.
We note that the best performance was obtained
using the PA algorithm. While these results are
satisfactory, it is possible that they could be im-
proved further if we would use a batch learning
method such as SVMstruct (Tsochantaridis et al,
2005) instead of the online learning methods used
here.
4.3 Candidate Set Size
In any method based on reranking, it is important
to study the influence of the candidate set size on
the quality of the reranked output. In addition, an
interesting question is what the upper bound on
reranker performance is ? the oracle performance.
Table 2 shows the result of an experiment that in-
vestigates these questions. We used the reranker
based on the Passive?Aggressive method in this
experiment since this reranker gave the best re-
sults in the previous experiment.
Reranked Oracle
k P R F P R F
1 63.36 46.77 53.82 63.36 46.77 53.82
2 63.70 48.17 54.86 72.66 55.18 62.72
4 63.57 49.78 55.84 79.12 62.24 69.68
8 63.50 51.79 57.04 83.72 68.14 75.13
16 63.00 52.94 57.54 86.92 72.79 79.23
32 62.15 54.50 58.07 89.18 76.76 82.51
64 61.02 55.67 58.22 91.08 80.19 85.28
128 60.22 56.45 58.27 92.63 83.00 87.55
256 59.87 57.22 58.51 94.01 85.27 89.43
Table 2: Oracle and reranker performance as a
function of candidate set size.
As is common in reranking tasks, the reranker
can exploit only a fraction of the potential im-
provement ? the reduction of the F-measure error
is between 10 and 15 percent of the oracle error
reduction for all candidate set sizes.
The most visible effect of the reranker is that
the recall is greatly improved. However, this does
not seem to have an adverse effect on the precision
until the candidate set size goes above 16 ? in fact,
the precision actually improves over the baseline
for small candidate set sizes. After the size goes
above 16, the recall (and the F-measure) still rises,
but at the cost of decreased precision.
4.4 Syntactic and Semantic Features
We studied the impact of syntactic and seman-
tic structural features on the performance of the
reranker. Table 3 shows the result of the investi-
gation for syntactic features. Using all the syntac-
tic features (and no semantic features) gives an F-
measure roughly 4 points above the baseline, us-
ing the PA reranker with a k of 64. We then mea-
sured the F-measure obtained when each one of
the three syntactic features has been removed. It
is clear that the unlexicalized syntactic path is the
most important syntactic feature; the effect of the
two lexicalized features seems to be negligible.
System P R F
Baseline 63.36 46.77 53.82
All syntactic 62.45 53.19 57.45
No SYN PATH 64.40 48.69 55.46
No LEX PATH 62.62 53.19 57.52
No DOMINANCE 62.32 52.92 57.24
Table 3: Effect of syntactic features.
A similar result was obtained when studying the
semantic features (Table 4). Removing the con-
necting labels feature, which is unlexicalized, has
a greater effect than removing the other two se-
mantic features, which are lexicalized.
System P R F
Baseline 63.36 46.77 53.82
All semantic 61.26 53.85 57.31
No PREDICATE SL 61.28 53.81 57.30
No PRED+ARGLBL 60.96 53.61 57.05
No CONN ARGLBL 60.73 50.47 55.12
Table 4: Effect of semantic features.
4.5 Opinion Holder Extraction
Table 5 shows the performance of the opinion
holder extractor. The baseline applies the holder
525
classifier (3.3) to the opinions extracted by the
base sequence labeler (3.2), without modeling any
interactions between opinions. A large perfor-
mance boost is then achieved simply by applying
the opinion expression reranker (k = 64); this is
simply the consequence of improved expression
detection, since a correct expression is required to
get credit for a holder).
However, we can improve on this by adding
the holder interaction features: both the SHARED
HOLDERS and HOLDER TYPES + PATH features
contribute to improving the recall even further.
System P R F
Baseline 57.66 45.14 50.64
Reranked expressions 52.35 52.54 52.45
SHARED HOLDERS 52.43 55.21 53.78
HTYPES + PATH 52.22 54.41 53.30
Both 52.28 55.99 54.07
Table 5: Opinion holder extraction experiments.
5 Conclusion
We have shown that features derived from gram-
matical and semantic role structure can be used
to improve two fundamental tasks in fine-grained
opinion analysis: the detection of opinionated ex-
pressions in subjectivity analysis, and the extrac-
tion of opinion holders. Our feature sets are based
on interaction between opinions, which makes ex-
act inference intractable. To overcome this issue,
we used an implementation based on reranking:
we first generated opinion expression sequence
candidates using a simple sequence labeler sim-
ilar to the approach by Breck et al (2007). We
then applied SRL-inspired opinion holder extrac-
tion classifiers, and finally a global model apply-
ing to all opinions and holders.
Our experiments show that the interaction-
based models result in drastic improvements. Sig-
nificantly, we see significant boosts in recall (10
points for both tasks) while the precision de-
creases only slightly, resulting in clear F-measure
improvements. This result compares favorably
with previously published results, which have
been precision-oriented and scored quite low on
recall.
We analyzed the impact of the syntactic and se-
mantic features and saw that the best model is the
one that makes use of both types of features. The
most effective features we have found are purely
structural, i.e. based on tree fragments in a syn-
tactic or semantic tree. Features involving words
did not seem to have the same impact.
There are multiple opportunities for future
work in this area. An important issue that we have
left open is the coreference problem for holder ex-
traction, which has been studied by Stoyanov and
Cardie (2006). Similarly, recent work has tried to
incorporate complex, high-level linguistic struc-
ture such as discourse representations (Somasun-
daran et al, 2009); it is clear that these structures
are very relevant for explaining the way humans
organize their expressions of opinions rhetori-
cally. However, theoretical depth does not nec-
essarily guarantee practical applicability, and the
challenge is as usual to find a middle ground that
balances our goals: explanatory power in theory,
significant performance gains in practice, compu-
tational tractability, and robustness in difficult cir-
cumstances.
6 Acknowledgements
The research described in this paper has received
funding from the European Community?s Seventh
Framework Programme (FP7/2007-2013) under
grant 231126: LivingKnowledge ? Facts, Opin-
ions and Bias in Time, and under grant 247758:
Trustworthy Eternal Systems via Evolving Soft-
ware, Data and Knowledge (EternalS).
References
Bethard, Steven, Hong Yu, Ashley Thornton, Vasileios
Hatzivassiloglou, and Dan Jurafsky. 2005. Extract-
ing opinion propositions and opinion holders using
syntactic and lexical cues. In Shanahan, James G.,
Yan Qu, and Janyce Wiebe, editors, Computing At-
titude and Affect in Text: Theory and Applications.
Breck, Eric, Yejin Choi, and Claire Cardie. 2007.
Identifying expressions of opinion in context. In
Proceedings of IJCAI-2007, Hyderabad, India.
Choi, Yejin, Eric Breck, and Claire Cardie. 2006.
Joint extraction of entities and relations for opinion
recognition. In Proceedings of EMNLP 2006.
526
Collins, Michael. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and ex-
periments with perceptron algorithms. In Proceed-
ings of the 2002 Conference on Empirical Methods
in Natural Language Processing (EMNLP 2002),
pages 1?8.
Crammer, Koby, Ofer Dekel, Joseph Keshet, Shai
Shalev-Schwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research, 2006(7):551?585.
Fan, Rong-En, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
Freund, Yoav and Robert E. Schapire. 1999. Large
margin classification using the perceptron algo-
rithm. Machine Learning, 37(3):277?296.
Johansson, Richard and Pierre Nugues. 2008.
Dependency-based syntactic?semantic analysis
with PropBank and NomBank. In CoNLL 2008:
Proceedings of the Twelfth Conference on Natural
Language Learning, pages 183?187, Manchester,
United Kingdom.
Joshi, Mahesh and Carolyn Penstein-Rose?. 2009.
Generalizing dependency features for opinion min-
ing. In Proceedings of ACL/IJCNLP 2009, Short
Papers Track.
Kim, Soo-Min and Eduard Hovy. 2006. Extract-
ing opinions, opinion holders, and topics expressed
in online news media text. In Proceedings of
ACL/COLING Workshop on Sentiment and Subjec-
tivity in Text.
Kobayashi, Nozomi, Kentaro Inui, and Yuji Mat-
sumoto. 2007. Extracting aspect-evaluation and
aspect-of relations in opinion mining. In Proceed-
ings of Conference on Empirical Methods in Natural
Language Processing (EMNLP-CoNLL-2007).
Mel?c?uk, Igor A. 1988. Dependency Syntax: Theory
and Practice. State University Press of New York,
Albany.
Meyers, Adam, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishman. 2004. The NomBank project:
An interim report. In HLT-NAACL 2004 Work-
shop: Frontiers in Corpus Annotation, pages 24?31,
Boston, United States.
Moschitti, Alessandro and Roberto Basili. 2004.
Complex linguistic features for text classification:
A comprehensive study. In Proceedings of ECIR.
Palmer, Martha, Dan Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?105.
Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification us-
ing machine learning techniques. In Proceedings of
EMNLP.
Ruppenhofer, Josef, Swapna Somasundaran, and
Janyce Wiebe. 2008. Finding the sources and tar-
gets of subjective expressions. In Proceedings of
LREC.
Somasundaran, Swapna, Galileo Namata, Janyce
Wiebe, and Lise Getoor. 2009. Supervised and
unsupervised methods in employing discourse rela-
tions for improving opinion polarity classification.
In Proceedings of EMNLP.
Stoyanov, Veselin and Claire Cardie. 2006. Partially
supervised coreference resolution for opinion sum-
marization through structured rule learning. In Pro-
ceedings of EMNLP 2006.
Tjong Kim Sang, Erik F. and Jorn Veenstra. 1999.
Representing text chunks. In Proceedings of
EACL99, pages 173?179, Bergen, Norway.
Tsochantaridis, Iannis, Thorsten Joachims, Thomas
Hofmann, and Yasemin Altun. 2005. Large margin
methods for structured and interdependent output
variables. Journal of Machine Learning Research,
6(Sep):1453?1484.
Wiebe, Janyce, Rebecca Bruce, and Thomas O?Hara.
1999. Development and use of a gold standard data
set for subjectivity classifications. In Proceedings
of the 37th Annual Meeting of the Association for
Computational Linguistics.
Wiebe, Janyce, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation, 39(2-3):165?210.
Wilson, Theresa, Janyce Wiebe, and Paul Hoff-
mann. 2005. Recognizing contextual polarity in
phrase-level sentiment analysis. In Proceedings of
HLT/EMNLP 2005.
Wu, Yuanbin, Qi Zhang, Xuanjing Huang, and Lide
Wu. 2009. Phrase dependency parsing for opinion
mining. In Proceedings of EMNLP.
Yu, Hong and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: Separating
facts from opinions and identifying the polarity of
opinion sentences. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP-2003), pages 129?136, Sap-
poro, Japan.
527
Relational Features in Fine-Grained
Opinion Analysis
Richard Johansson?
University of Gothenburg
Alessandro Moschitti??
University of Trento
Fine-grained opinion analysis methods often make use of linguistic features but typically do
not take the interaction between opinions into account. This article describes a set of experiments
that demonstrate that relational features, mainly derived from dependency-syntactic and se-
mantic role structures, can significantly improve the performance of automatic systems for a
number of fine-grained opinion analysis tasks: marking up opinion expressions, finding opinion
holders, and determining the polarities of opinion expressions. These features make it possible
to model the way opinions expressed in natural-language discourse interact in a sentence over
arbitrary distances. The use of relations requires us to consider multiple opinions simultaneously,
which makes the search for the optimal analysis intractable. However, a reranker can be used as
a sufficiently accurate and efficient approximation.
A number of feature sets and machine learning approaches for the rerankers are evaluated.
For the task of opinion expression extraction, the best model shows a 10-point absolute improve-
ment in soft recall on the MPQA corpus over a conventional sequence labeler based on local
contextual features, while precision decreases only slightly. Significant improvements are also
seen for the extended tasks where holders and polarities are considered: 10 and 7 points in recall,
respectively. In addition, the systems outperform previously published results for unlabeled (6
F-measure points) and polarity-labeled (10?15 points) opinion expression extraction. Finally,
as an extrinsic evaluation, the extracted MPQA-style opinion expressions are used in practical
opinion mining tasks. In all scenarios considered, the machine learning features derived from the
opinion expressions lead to statistically significant improvements.
1. Introduction
Automatic methods for the analysis of opinions (textual expressions of emotions, be-
liefs, and evaluations) have attracted considerable attention in the natural language
? Spra?kbanken, Department of Swedish, University of Gothenburg, Box 100, SE-40530 Gothenburg,
Sweden. E-mail: richard.johansson@gu.se. The work described here was mainly carried out at the
University of Trento.
?? DISI ? Department of Information Engineering and Computer Science, University of Trento,
Via Sommarive 14, 38123 Trento (TN), Italy. E-mail: moschitti@disi.unitn.it.
Submission received: 11 January 2012; revised version received: 11 May 2012; accepted for publication:
11 June 2012.
doi:10.1162/COLI a 00141
? 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 3
processing community in recent years (Pang and Lee 2008). Apart from their interest
from a linguistic and psychological point of view, the technologies emerging from this
research have obvious practical uses, either as stand-alone applications or supporting
other tools such as information retrieval or question answering systems.
The research community initially focused on high-level tasks such as retrieving doc-
uments or passages expressing opinion, or classifying the polarity of a given text, and
these coarse-grained problem formulations naturally led to the application of methods
derived from standard retrieval or text categorization techniques. The models under-
lying these approaches have used very simple feature representations such as purely
lexical (Pang, Lee, and Vaithyanathan 2002; Yu and Hatzivassiloglou 2003) or low-level
grammatical features such as part-of-speech tags and functional words (Wiebe, Bruce,
and O?Hara 1999). This is in line with the general consensus in the information retrieval
community that very little can be gained by complex linguistic processing for tasks such
as text categorization and search (Moschitti and Basili 2004). There are a few exceptions,
such as Karlgren et al (2010), who showed that construction features added to a bag-of-
words representation resulted in improved performance on a number of coarse-grained
opinion analysis tasks. Similarly, Greene and Resnik (2009) argued that a speaker?s
attitude can be predicted from syntactic features such as the selection of a transitive
or intransitive verb frame.
In contrast to the early work, recent years have seen a shift towards more detailed
problem formulations where the task is not only to find a piece of opinionated text,
but also to extract a structured representation of the opinion. For instance, we may
determine the person holding the opinion (the holder) and towards which entity or fact
it is directed (the topic), whether it is positive or negative (the polarity), and the strength
of the opinion (the intensity). The increasing complexity of representation leads us
from retrieval and categorization deep into natural language processing territory; the
methods used here have been inspired by information extraction and semantic role
labeling, combinatorial optimization, and structured machine learning. For such tasks,
deeper representations of linguistic structure have seen more use than in the coarse-
grained case. Syntactic and shallow-semantic relations have repeatedly proven useful
for subtasks of opinion analysis that are relational in nature, above all for determining
the holder or topic of a given opinion, in which case there is considerable similarity to
tasks such as semantic role labeling (Kim and Hovy 2006; Ruppenhofer, Somasundaran,
and Wiebe 2008).
There has been no systematic research, however, on the role played by linguistic
structure in the relations between opinions expressed in text, despite the fact that the
opinion expressions in a sentence are not independent but organized rhetorically to
achieve a communicative effect intended by the speaker. We therefore expect that the
interplay between opinion expressions can be exploited to derive information useful for
the analysis of opinions expressed in text. In this article, we start from this intuition and
propose several novel features derived from the interdependencies between opinion
expressions on the syntactic and shallow-semantic levels.
Based on these features, we devised structured prediction models for (1) extraction
of opinion expressions, (2) joint expression extraction and holder extraction, and (3) joint
expression extraction and polarity labeling. The models were trained using a number
of discriminative machine learning methods. Because the interdependency features
required us to consider more than one opinion expression at a time, the inference steps
carried out at training and prediction time could not rely on commonly used opinion
expression mark-up methods based on Viterbi search, but we show that an approximate
search method using reranking suffices for this purpose: In a first step a base system
474
Johansson and Moschitti Relational Features in Fine-Grained Opinion Analysis
using local features and efficient search generates a small set of hypotheses, and in a
second step a classifier using the complex features selects the final output from the
hypothesis set. This approach allows us to make use of arbitrary features extracted from
the complete set of opinion expressions in a sentence, without having to impose any
restriction on the expressivity of the features. An additional advantage is that it is fairly
easy to implement as long as the underlying system is able to generate k-best output.
The interaction-based reranking systems were evaluated on a test set extracted
from the MPQA corpus, and compared to strong baselines consisting of stand-alone
systems for opinion expression mark-up, opinion holder extraction, and polarity classi-
fication. Our evaluations showed that (1) the best opinion expression mark-up system
we evaluated achieved a 10-point absolute improvement in soft recall, and a 5-point im-
provement in F-measure, over the baseline sequence labeler. Our system outper-
formed previously described opinion expression mark-up tools by six points in overlap
F-measure. (2) The recall was boosted by almost 10 points for the holder extraction task
(over three points in F-measure) by modeling the interaction of opinion expressions
with respect to holders. (3) We saw an improvement for the extraction of polarity-
labeled expression of four F-measure points. Our result for opinion extraction and po-
larity labeling is especially striking when compared with the best previously published
end-to-end system for this task: 10?15 points in F-measure improvement. In addition to
the performance evaluations, we studied the impact of features on the subtasks, and the
effect of the choice of the machine learning method for training the reranker.
As a final extrinsic evaluation of the system, we evaluated the usefulness of its
output in a number of applications. Although there have been several publications
detailing the extraction of MPQA-style opinion expressions, as far as we are aware there
has been no attempt to use them in an application. In contrast, we show that the opinion
expressions as defined by the MPQA corpus may be used to derive machine learning
features that are useful in two practical opinion mining tasks; the addition of these
features leads to statistically significant improvements in all scenarios we evaluated.
First, we develop a system for the extraction of evaluations of product attributes
from product reviews (Hu and Liu 2004a, 2004b; Popescu and Etzioni 2005; Titov and
McDonald 2008), and we show that the features derived from opinion expressions lead
to significant improvement. Secondly, we show that fine-grained opinion structural
information can even be used to build features that improve a coarse-grained sentiment
task: document polarity classification of reviews (Pang, Lee, and Vaithyanathan 2002;
Pang and Lee 2004).
After the present introduction, Section 2 gives a linguistic motivation and an
overview of the related work; Section 3 describes the MPQA opinion corpus and its
underlying representation; Section 4 illustrates the baseline systems: a sequence labeler
for the extraction of opinion expressions and classifiers for opinion holder extraction
and polarity labeling; Section 5 reports on the main contribution: the description of the
interaction models and their features; finally, Section 7 presents the experimental results
and Section 8 derives the conclusions.
2. Motivation and Related Work
Intuitively, interdependency features could be useful in the process of locating and
disambiguating expressions of opinion. These expressions tend to occur in patterns, and
the presence of one opinionated piece of text may influence our interpretation of another
as opinionated or not. Consider, for instance, the word said in sentences (a) and (b) in
Example (1), where the presence or non-presence of emotionally loaded words in the
475
Computational Linguistics Volume 39, Number 3
complement clause provides evidence for the interpretation as a subjective opinion or
an objective speech event. (In the example, opinionated expressions are marked S for
subjective and the non-opinionated speech event O for objective.)
Example (1)
(a) ?We will identify the [culprits]S of these clashes and [punish]S them,? he [said]S.
(b) On Monday, 80 Libyan soldiers disembarked from an Antonov transport plane
carrying military equipment, an African diplomat [said]O.
Moreover, opinions expressed in a sentence are interdependent when it comes to the
resolution of their holders?the person or entity having the attitude expressed in the
opinion expression. Clearly, the structure of the sentence is also influential for this task
because certain linguistic constructions provide evidence for opinion holder correlation.
In the most obvious case, shown in the two sentences in Example (2), pejorative words
share the opinion holder with the communication and categorization verbs dominating
them. (Here, opinions are marked S and holders H.)
Example (2)
(a) [Domestic observers]H [tended to side with]S the MDC, [denouncing]S the election
as [fraud-tainted]S and [unfair]S.
(b) [Bush]H [labeled]S North Korea, Iran and Iraq an ?[axis of evil]S.?
In addition, interaction is important when determining opinion polarity. Here, relations
that influence polarity interpretation include coordination, verb?complement, as well as
other types of discourse relations. In particular, the presence of a COMPARISON discourse
relation, such as contrast or concession (Prasad et al 2008), may allow us to infer that
opinion expressions have different polarities. In Example (3), we see how contrastive
discourse connectives (underlined) are used when there are contrasting polarities in the
surrounding opinion expressions. (Positive opinions are tagged ?+?, negative ?-?.)
Example (3)
(a) ?[This is no blind violence but rather targeted violence]-,? Annemie Neyts [said]-.
?However, the movement [is more than that]+.?
(b) ?[Trade alone will not save the world]-,? Neyts [said]-, but it constitutes an
[important]+ factor for economic development.
The problems we focus on in this article?extracting opinion expressions with holders
and polarity labeling?have naturally been studied previously, especially since the
release of the MPQA corpus (Wiebe, Wilson, and Cardie 2005). For the first subtask,
because the MPQA corpus uses span-based annotation to represent opinions, it is
natural to apply straightforward sequence labeling techniques to extract them. This idea
has resulted in a number of publications (Choi, Breck, and Cardie 2006; Breck, Choi, and
Cardie 2007). Such systems do not use any features describing the interaction between
opinions, and it would not be possible to add interaction features because a Viterbi-
based sequence labeler by construction is restricted to using local features in a small
contextual window.
Works using syntactic features to extract topics and holders of opinions are numer-
ous (Bethard et al 2005; Kobayashi, Inui, and Matsumoto 2007; Joshi and Penstein-Rose?
476
Johansson and Moschitti Relational Features in Fine-Grained Opinion Analysis
2009; Wu et al 2009). Semantic role analysis has also proven useful: Kim and Hovy
(2006) used a FrameNet-based semantic role labeler to determine holder and topic of
opinions. Similarly, Choi, Breck, and Cardie (2006) successfully used a PropBank-based
role labeler for opinion holder extraction, and Wiegand and Klakow (2010) recently ap-
plied tree kernel learning methods on a combination of syntactic and semantic role trees
for extracting holders, but did not consider their relations to the opinion expressions.
Ruppenhofer, Somasundaran, and Wiebe (2008) argued that semantic role techniques
are useful but not completely sufficient for holder and topic identification, and that
other linguistic phenomena must be studied as well. Choi, Breck, and Cardie (2006)
built a joint model of opinion expression extraction and holder extraction and applied
integer linear programming to carry out the optimization step.
While the tasks of opinion expression detection and polarity classification of opin-
ion expressions (Wilson, Wiebe, and Hoffmann 2009) have mostly been studied in
isolation, Choi and Cardie (2010) developed a sequence labeler that simultaneously
extracted opinion expressions and assigned them polarity values and this is so far the
only published result on joint opinion segmentation and polarity classification. Their
experiment, however, lacked the obvious baseline: a standard pipeline consisting of an
expression tagger followed by a polarity classifier. In addition, although their model
is the first end-to-end system for opinion expression extraction and polarity classifica-
tion, it is still based on sequence labeling and thus by construction limited in feature
expressivity.
On a conceptual level, discourse-oriented approaches (Asher, Benamara, and
Mathieu 2009; Somasundaran et al 2009; Zirn et al 2011) applying interaction features
for polarity classification are arguably the most related because they are driven by
a vision similar to ours: Individual opinion expressions interplay in discourse and
thus provide information about each other. On a practical level there are obvious
differences, since our features are extracted from syntactic and shallow-semantic
linguistic representations, which we argue are reflections of discourse structure, while
they extract features directly from a discourse representation. It is doubtful whether
automatic discourse representation extraction in text is currently mature enough to
provide informative features, whereas syntactic parsing and shallow-semantic analysis
are today fairly reliable. Another related line of work is represented by Choi and Cardie
(2008), where interaction features based on compositional semantics were used in a joint
model for the assignment of polarity values to pre-segmented opinion expressions in a
sentence.
3. The MPQA Corpus and its Annotation of Opinion Expressions
The most detailed linguistic resource useful for developing automatic systems for opin-
ion analysis is theMPQA corpus (Wiebe,Wilson, and Cardie 2005). In this article, we use
the word opinion in its broadest sense, equivalent to the word private state used by the
MPQA annotators (page 2): ?opinions, emotions, sentiments, speculations, evaluations,
and other private states (Quirk et al 1985), i.e., internal states that cannot be directly
observed by others.?
The central building block in the MPQA annotation is the opinion expression
(or subjective expression): A text piece that expresses a private state, allowing us to
draw the conclusion that someone has a particular emotion or belief about some topic.
Identifying these units allows us to carry out further analysis, such as the determination
of opinion holder and the polarity of the opinion. The annotation scheme defines two
types of opinion expressions: direct subjective expressions (DSEs), which are explicit
477
Computational Linguistics Volume 39, Number 3
mentions of attitude or evaluation, and expressive subjective elements (ESEs), which
signal the attitude of the speaker by the choice of words. The prototypical example of
a DSE would be a verb of statement, feeling, or categorization such as praise or disgust.
ESEs, on the other hand, are less easy to categorize syntactically; prototypical examples
would include simple value-expressing adjectives such as beautiful and strongly charged
words like appeasement or big government. In addition to DSEs and ESEs, the corpus also
contains annotation for non-subjective statements, which are referred to as objective
speech events (OSEs). Some words such as say may appear as DSEs or OSEs depending
on the context, so for an automatic system there is a need for disambiguation.
Example (4) shows a number of sentences from the MPQA corpus where DSEs and
ESEs have been manually annotated.
Example (4)
(a) He [made such charges]DSE [despite the fact]ESE that women?s political, social, and
cultural participation is [not less than that]ESE of men.
(b) [However]ESE, it is becoming [rather fashionable]ESE to [exchange harsh words]DSE
with each other [like kids]ESE.
(c) For instance, he [denounced]DSE as a [human rights violation]ESE the banning and
seizure of satellite dishes in Iran.
(d) This [is viewed]DSE as the [main impediment]ESE to the establishment of political
order in the country.
Every expression in the corpus is connected to an opinion holder,1 an entity that
experiences the sentiment or utters the evaluation that appears textually in the opinion
expression. For DSEs, it is often fairly straightforward to find the opinion holders
because they tend to be realized as direct semantic arguments filling semantic roles
such as SPEAKER or EXPERIENCER?and the DSEs tend to be verbs or nominalizations.
For ESEs, the connection between the expression and the opinion holder is typically
less clear-cut than for DSEs; the holder is more frequently implicit or located outside
the sentence for ESEs than for DSEs.
The MPQA corpus does not annotate links directly from opinion expressions to
particular mentions of a holder entity. Instead, the opinions are connected to holder
coreference chains that may span the whole text. Some opinion expressions are linked
to entities that are not explicitly mentioned in the text. If this entity is the author of the
text, it is called the writer, otherwise implicit. Example (5) shows sentences annotated
with expressions and holders.
Example (5)
(a) For instance, [he]H1 [denounced]DSE/H1 as a [human rights violation]ESE/H1 the
banning and seizure of satellite dishes in Iran.
(b) [(writer)]H1: [He]H2 [made such charges]DSE/H2 [despite the fact]ESE/H1 that
women?s political, social, and cultural participation is [not less than that]ESE/H1 of men.
(c) [(implicit)]H1: This [is viewed]DSE/H1 as the [main impediment]ESE/H1 to the estab-
lishment of political order in the country.
1 The MPQA uses the term source but we prefer the term holder because it seems to be more common.
478
Johansson and Moschitti Relational Features in Fine-Grained Opinion Analysis
Finally, MPQA associates opinion expressions (DSEs and ESEs, but not OSEs) with
a polarity feature taking the values POSITIVE, NEGATIVE, NEUTRAL, and BOTH, and
with an intensity feature taking the values LOW, MEDIUM, HIGH, and EXTREME. The
two sentences in Example (6) from the MPQA corpus show opinion expressions with
polarities. Positive polarity is represented with a ?+? and negative with a ?-?.
Example (6)
(a) We foresaw electoral [fraud]- but not [daylight robbery]-.
(b) Join in this [wonderful]+ event and help Jameson Camp continue to provide the
year-round support that gives kids a [chance to create dreams]+.
The corpus does not currently contain annotation of topics (evaluees) of opinions,
although there have been efforts to add this separately (Stoyanov and Cardie 2008).
4. Baseline Systems for Fine-Grained Opinion Analysis
The assessment of our reranking-based systems requires us to compare against strong
baselines. We developed (1) a sequence labeler for opinion expression extraction similar
to that by Breck, Choi, and Cardie (2007), (2) a set of classifiers to determine the
opinion holder, and (3) a multiclass classifier that assigns polarity to a given opinion
expression similar to that described by Wilson, Wiebe, and Hoffmann (2009). These
tools were also used to generate the hypothesis sets for the rerankers described in
Section 5.
4.1 Sequence Labeler for Opinion Expression Mark-up
To extract opinion expressions, we implemented a standard sequence labeler for sub-
jective expression mark-up similar to the approach by Breck, Choi, and Cardie (2007).
The sequence labeler extracted basic grammatical and lexical features (word, lemma,
and POS tag), as well as prior polarity and intensity features derived from the lexicon
created by Wilson, Wiebe, and Hoffmann (2005), which we refer to as subjectivity
clues. It is important to note that prior subjectivity does not always imply subjectivity
in a particular context; this is why contextual features are essential for this task. The
grammatical features and subjectivity clues were extracted in a window of size 3 around
the word in focus. We encoded the opinionated expression brackets by means of the
IOB2 scheme (Tjong Kim Sang and Veenstra 1999). When using this representation, we
are unable to handle overlapping opinion expressions, but they are fortunately rare.
To exemplify, Figure 1 shows an example of a sentence and how it is processed
by the sequence labeler. The ESE defenseless situation is encoded in IOB2 as two tags
B-ESE and I-ESE. There are four input columns (words, lemmas, POS tags, subjectivity
clues) and one output column (opinion expression tags in IOB2 encoding). The figure
also shows the sliding window from which the feature extraction function can extract
features when predicting an output tag (at the arrow).
We trained the model using the method by Collins (2002), with a Viterbi decoder
and the online Passive?Aggressive algorithm (Crammer et al 2006) to estimate the
model weights. The learning algorithm parameters were tuned on a development set.
When searching for the best value of the C parameter, we varied it along a log scale from
479
Computational Linguistics Volume 39, Number 3
HRW
has
denounced
the
situation
of
these
prisoners
HRW
have
denounce
the
defenseless
situation
of
this
prisoner
defenseless
NNP
VBZ
VBN
DT
JJ
NN
IN
DT
NNS
?
?
?
?
?
?
str/neg
?
weak/neg
O
O
B?ESE
I?ESE
B?DSE
O
Figure 1
Sequence labeling example.
0.001 to 100, and the best value was 0.1. We used the max-loss version of the algorithm
and ten iterations through the training set.
4.2 Classifiers for Opinion Holder Extraction
The problem of extracting opinion holders for a given opinion expression is in many
ways similar to argument detection in semantic role labeling (Choi, Breck, and Cardie
2006; Ruppenhofer, Somasundaran, and Wiebe 2008). For instance, in the simplest
case when the opinion expression is a verb of evaluation or categorization, finding
the holder would entail finding a semantic argument such as an EXPERIENCER or
COMMUNICATOR. We therefore approached this problem using methods inspired by
semantic role labeling: Given an opinion expression in a sentence, we define binary
classifiers that decide whether each noun phrase of the sentence is its holder or not.
Separate classifiers were trained to extract holders for DSEs, ESEs, and OSEs.
Hereafter, we describe the feature set used by the classifiers. Our walkthrough
example is given by the sentence in Figure 1. Some features are derived from the
syntactic and shallow semantic analysis of the sentence, shown in Figure 2 (Section 6.1
gives more details on this).
SYNTACTIC PATH. Similarly to the path feature widely used in semantic role labeling
(SRL), we extract a feature representing the path in the dependency tree between
the expression and the holder (Johansson and Nugues 2008). For instance, the path
from denounced to HRW in the example is VC?SBJ?.
SHALLOW-SEMANTIC RELATION. If there is a direct shallow-semantic relation between
the expression and the holder, we use a feature representing its semantic role, such
as A0 between denounced and HRW.
EXPRESSION HEAD WORD, POS, AND LEMMA. denounced, VBD, denounce for de-
nounced; situation, NN, situation for defenseless situation.
HEAD WORD AND POS OF HOLDER CANDIDATE. HRW, NNP for HRW.
DOMINATING EXPRESSION TYPE. When locating the holder for the ESE defenseless
situation, we extract a feature representing the fact that it is syntactically dominated
by a DSE. At test time, the expression and its type were extracted automatically.
480
Johansson and Moschitti Relational Features in Fine-Grained Opinion Analysis
CONTEXT WORDS AND POS FOR HOLDER CANDIDATE. Words adjacent to the left
and right; for HRW we extract Right:has, Right:VBZ.
EXPRESSION VERB VOICE. Similar to the common voice feature used in SRL. Takes
the values Active, Passive, and None (for non-verbal opinion expressions). In the
example, we get Active for denounced and None for defenseless situation.
EXPRESSION DEPENDENCY RELATION TO PARENT. VC and OBJ, respectively.
There are also differences compared with typical argument extraction in SRL, however.
First, as outlined in Section 3, it is important to note that the MPQA corpus does not
annotate direct links from opinions to holders, but from opinions to holder coreference
chains. To handle this issue, we used the following approach when training the classi-
fier: We created a positive training instance for each member of the coreference chain
occurring in the same sentence as the opinion, and negative training instances for all
other noun phrases in the sentence. We do not use coreference information at test time,
in order for the system not to rely on automatic coreference resolution.
A second complication is that in addition to the explicit noun phrases in the same
sentence, an opinion may be linked to an implicit holder; a special case of implicit holder
is the writer of the text. To detect implicit and writer links, we trained two separate
classifiers that did not use the features requiring a holder phrase. We did not try to
link opinion expressions to explicitly expressed holders outside the sentence; this is
an interesting open problem with some connections to inter-sentential semantic role
labeling, a problem whose study is in its infancy (Gerber and Chai 2010).
We implemented the classifiers as linear support vector machines (SVMs; Boser,
Guyon, and Vapnik 1992) using the LIBLINEAR software (Fan et al 2008). To handle
the restriction that every expression can have at most one holder, the classifier selects
only the highest-scoring opinion holder candidate at test time. We tuned the learning
parameters on a development set, and the best results were obtained with an L2-
regularized L2-loss SVM and a C value of 1.
4.3 Polarity Classifier
Given an expression, we use a classifier to assign a polarity value: POSITIVE, NEUTRAL,
or NEGATIVE. Following Choi and Cardie (2010), the polarity value BOTH was mapped
to NEUTRAL?the expressions having this value were in any case very few. In the cases
where the polarity value was empty or missing, we set the polarity to NEUTRAL. In
addition, the annotators of the MPQA corpus may use special uncertainty labels in the
case where the annotator was unsure of which polarity to assign, such as UNCERTAIN-
POSITIVE. In these cases, we just removed the uncertainty label.
We again trained SVMs to carry out this classification. The problem of polarity clas-
sification has been studied in detail by Wilson, Wiebe, and Hoffmann (2009), who used
a set of carefully devised linguistic features. Our classifier is simpler and is based on
fairly shallow features. Like the sequence labeler for opinion expressions, this classifier
made use of the lexicon of subjectivity clues.
The feature set used by the polarity classifier consisted of the following features.
The examples come from the opinion expression defenseless situation in Figure 1.
WORDS IN EXPRESSION: defenseless, situation.
POS TAGS IN EXPRESSION: JJ, NN
481
Computational Linguistics Volume 39, Number 3
SUBJECTIVITY CLUES OF WORDS IN EXPRESSION: None.
WORD BIGRAMS IN EXPRESSION: defenseless situation.
WORDS BEFORE AND AFTER EXPRESSION: B:the, A:of.
POS TAGS BEFORE AND AFTER EXPRESSION: B:DT, A:IN.
To train the support vector classifiers, we again used LIBLINEAR with the same param-
eters. The three-class classification problem was binarized using the one-versus-all
method.
5. Fine-Grained Opinion Analysis with Interaction Features
Because there is a combinatorial number of ways to segment a sentence into opin-
ion expressions, and label the opinion expressions with type labels (DSE, ESE, OSE)
as well as polarities and opinion holders, the tractability of the opinion expression
segmentation task will obviously depend on whether we impose restrictions on the
problem in a way that allows for efficient inference. Most previous work (Choi, Breck,
and Cardie 2006; Breck, Choi, and Cardie 2007; Choi and Cardie 2010) used Markov
factorizations and could thus apply standard sequence labeling techniques where the
argmax step was carried out using the Viterbi algorithm (as described in Section 4.1).
As we argued in the introduction, however, it makes linguistic sense that opinion
expression segmentation and other tasks could be improved if relations between possible
expressions were considered; these relations can be syntactic or semantic in nature,
for instance.
We will show that adding relational features causes the Markov assumption to
break down and the problem of finding the best analysis to become computationally
intractable. We thus have to turn to approximate inference methods based on reranking,
which can be trained efficiently.
5.1 Formalization of the Model
We formulate the problem of extracting the opinion structure (the set of opinion expres-
sions, and possibly also their holders or polarities) from a given sentence as a structured
prediction problem
y? = argmax
y
w ? ?(x, y) (1)
where w is a weight vector and ?(x, y) a feature extraction function representing a sen-
tence x and an opinion structure y as a high-dimensional feature vector. We now further
decompose the feature representation ? into a local part ?L and a nonlocal part ?NL:
? = ?L +?NL (2)
where ?L is a standard first-order Markov factorization, and ?NL represents the non-
local interactions between pairs of opinion expressions:
?NL(x, y) =
?
ei,ej?y,ei 6=ej
?p(ei, ej, x) (3)
482
Johansson and Moschitti Relational Features in Fine-Grained Opinion Analysis
The feature function ?p represents a pair of opinion expressions ei and ej and their
interaction in the sentence x, such as the syntactic and semantic relations connecting
them.
5.2 Approximate Inference with Interaction Features
It is easy to see that the inference step argmaxy w ? ?(x, y) is NP-hard for unrestricted
pairwise interaction feature representations ?: This class of models includes simpler
ones such as loopy Markov random fields, where inference is known to be NP-hard and
require the use of approximate approaches such as belief propagation. Although it is
possible that search algorithms for approximate inference in ourmodel could be devised
along similar lines, we sidestepped this issue by using a reranking decomposition of the
problem:
r Apply a simple model based on local context features ?L but no structural
interaction features. Generate a small hypothesis set of size k.
r Apply a complex model using interaction features ?NL to pick the top
hypothesis from the hypothesis set.
The advantages of a reranking approach compared with more complex approaches re-
quiring advanced search techniques are mainly simplicity and efficiency: This approach
is conceptually simple and fairly easy to implement provided that k-best output can
be generated efficiently, which is certainly true for the Viterbi-based sequence labeler
described in Section 4.1. The features can then be arbitrarily complex because we do
not have to think about how the problem structure affects the algorithmic complexity of
the inference step. Reranking has been used in a wide range of applications, starting in
speech recognition (Schwartz and Austin 1991) and very commonly in syntactic parsing
of natural language (Collins 2000).
The hypothesis generation procedure becomes slightly more complex when polar-
ity values and opinion holders of the opinion expressions enter the picture. In that case,
we not only need hypotheses generated by a sequence labeler, but also the outputs
of a secondary classifier: the holder extractor (Section 4.2) or the polarity classifier
(Section 4.3). The hypothesis set generation thus proceeds as follows:
r For a given sentence, let the base sequence labeler generate up to k1
sequences of unlabeled opinion expressions;
r for every sequence, apply the secondary classifier to generate up to k2
outputs.
The hypothesis set size is thus at most k1 ? k2.
To illustrate this process we give a hypothetical example, assuming k1 = k2 = 2
and the sentence The appeasement emboldened the terrorists. We first apply the opinion
expression extractor to generate a set of k1 possible segmentations of the sentence:
The [appeasement] emboldened the [terrorists]
The [appeasement] [emboldened] the [terrorists]
483
Computational Linguistics Volume 39, Number 3
In the second step, we add polarity values, up to k2 labelings for every segmentation
candidate:
The [appeasement]? emboldened the [terrorists]?
The [appeasement]? [emboldened]+ the [terrorists]?
The [appeasement]0 emboldened the [terrorists]?
The [appeasement]? [emboldened]0 the [terrorists]?
5.3 Training the Rerankers
In addition to the approximate inference method to carry out the maximization of Equa-
tion (1), we still need a machine learning method to assign weights to the vector w by
estimating on a training set. We investigated a number of machine learning approaches
to train the rerankers.
5.3.1 Structured SVM Learning. We first applied the method of large-margin estimation
for structured output spaces, also known as structured support vector machines. In
this method, we use quadratic optimization to find the smallest weight vector w that
satisfies the constraint that the difference in output score between the correct output y
and an incorrect output y? should be at least ?(y, y?), where ? is a loss function based
on the degree of error in the output y? with respect to the gold standard y. This is a
generalization of the well-known support vector machine from binary classification to
prediction of structured objects.
Formally, for a given training set T = {?xi, yi?} where the output space for the
input xi is Yi, we state the learning problem as a constrained quadratic optimization
program:
minimizew ?w?2
subject to w ? (?(xi, yi)? ?(xi, yij)) ? ?(yi, yij),
??xi, yi? ? T , yij ? Yi
(4)
Because real-world data tend to be noisy, this optimization problem is usually also
regularized to reduce overfitting, which leads to the introduction of a parameter C as in
regular support vector machines (see Taskar, Guestrin, and Koller [2004] inter alia for
details).
The optimization problem (4) is usually not solved directly because the number of
constraints makes a direct solution of the optimization program intractable for most
realistic types of problems. Instead, an approximation has to be used in practice, and
we used the SVMstructsoftware package (Tsochantaridis et al 2005; Joachims, Finley,
and Yu 2009), which finds a solution to the quadratic program by successively finding
its most violated constraints and adding them to a working set. We used the default
values for the learning parameters, except for the parameter C, which was determined
by optimizing on a development set. This resulted in a C value of 500.
We defined the loss function ? as 1 minus the intersection F-measure defined in
Section 7.1. When training rerankers for the complex extraction tasks (expressions +
holders or expressions + polarities), we used a weighted combination of F-measures
for the primary task (expressions) and the secondary task (holders or polarities, see
Sections 7.1.1 and 7.1.2, respectively).
484
Johansson and Moschitti Relational Features in Fine-Grained Opinion Analysis
5.3.2 On-line Learning. In addition to the structured SVM learning method, we trained
models using two variants of on-line learning. Such learning methods are a feasible
alternative for performance reasons. We investigated two on-line learning algorithms:
the popular structured perceptron (Collins 2002) and the Passive?Aggressive (PA)
algorithm (Crammer et al 2006). To increase robustness, we used an averaged imple-
mentation (Freund and Schapire 1999) of both algorithms.
The difference between the two algorithms is the way the weight vector is incre-
mented in each step. In the perceptron, for a given input x, we compute an update to
the current weight vector by computing the difference between the correct output y and
the predicted output y?. Pseudocode is given in Algorithm 1.
Algorithm 1 The structured perceptron algorithm.
function PERCEPTRON(T ,N)
input Training set T = {(xi, yi)}Ti=1
Number of iterations N
w0 ? (0, . . . , 0)
repeat N times
for (x, y) in T
y?? argmaxh w ? ?(x, h)
wi+1 ? wi +?(x, y)? ?(x, y?)
i? i+ 1
return 1NT
?NT
i=1 wi
The PA algorithm, with pseudocode in Algorithm 2, is based on the theory of large-
margin learning similar to the structured SVM. Here we instead base the update step
on the y? that violates the margin constraints maximally, also taking the loss function ?
into account. The update step length ? is computed based on the margin; this update
is bounded by a regularization constant C, which we set to 0.005 after tuning on a
development set. The number N of iterations through the training set was 8 for both
on-line algorithms.
Algorithm 2 The on-line passive?aggressive algorithm.
function PASSIVE?AGGRESSIVE(T ,N,C)
input Training set T = {(xi, yi)}Ti=1
Number of iterations N
Regularization parameter C
w0 ? (0, . . . , 0)
repeat N times
for (x, y) in T
y?? argmaxh w ? (?(x, h)? ?(x, y))+
?
?(y, h)
?? min
(
C, w?(?(x,y?)??(x,y))+
?
?(y,y?)
??(x,y?)??(x,y)?2
)
wi+1 ? wi + ?(?(x, y)? ?(x, y?))
i? i+ 1
return 1NT
?NT
i=1 wi
485
Computational Linguistics Volume 39, Number 3
6. Overview of the Interaction Features
The feature extraction function ?NL extracts three groups of interaction features: (1)
features considering the opinion expressions only; (2) features considering opinion
holders; and (3) features considering polarity values.
In addition to the interaction features ?NL, the rerankers used features representing
the scores output by the base models (opinion expression sequence labeler and sec-
ondary classifiers); they did not directly use the local features ?L. We normalized the
scores over the k candidates so that their exponentials summed to 1.
6.1 Syntactic and Shallow Semantic Analysis
The features used by the rerankers, as well as the opinion holder extractor in Section 4.2,
are to a large extent derived from syntactic and semantic role structures. To extract
them, we used the syntactic?semantic parser by Johansson and Nugues (2008), which
annotates the sentences with dependency syntax (Mel?c?uk 1988) and shallow semantics
in the PropBank (Palmer, Gildea, and Kingsbury 2005) and NomBank (Meyers et al
2004) frameworks, using the format of the CoNLL-2008 Shared Task (Surdeanu et al
2008). The system includes a sense disambiguator that assigns PropBank or NomBank
senses to the predicate words.
Figure 2 shows an example of the structure of the annotation: The sentence HRW
denounced the defenseless situation of these prisoners, where denounced is a DSE and de-
fenseless situation is an ESE, has been annotated with dependency syntax (above the
text) and semantic role structure (below the text). The predicate denounced, which is an
instance of the PropBank frame denounce.01, has two semantic arguments: the Speaker
(A0, or Agent in VerbNet terminology) and the Subject (A1, or Theme), which are
realized on the surface-syntactic level as a subject and a direct object, respectively. Sim-
ilarly, situation has the NomBank frame situation.01 and an EXPERIENCER semantic
argument (A0).
6.2 Opinion Expression Interaction Features
The rerankers use two types of structural features: syntactic features extracted from
the dependency tree, and semantic features extracted from the predicate?argument
(semantic role) graph.
The syntactic features are based on paths through the dependency tree. This leads to
a minor complication for multiword opinion expressions; we select the shortest possible
path in such cases. For instance, in the sentence in Figure 2, the path will be computed
between denounced and situation.
HRW thehas [denounced] defenseless[ situation]ESE
denounce.01
A0
NMOD
NMOD
OBJ
SBJ VC
DSE of these prisoners
NMOD NMOD
PMOD
situation.01
A0A1
Figure 2
Example sentence and its syntactic and shallow-semantic analysis.
486
Johansson and Moschitti Relational Features in Fine-Grained Opinion Analysis
We used the following syntactic interaction features. All examples refer to Figure 2.
SYNTACTIC PATH. Given a pair of opinion expressions, we use a feature representing
the labels of the two expressions and the path between them through the syntactic
tree, following standard practice from dependency-based semantic role labeling
(Johansson and Nugues 2008). For instance, for the DSE denounced and the ESE
defenseless situation in Figure 2, we represent the syntactic configuration using the
feature DSE:OBJ?:ESE, meaning that the syntactic relation between the DSE and the
ESE consists of a single link representing a grammatical object.
LEXICALIZED PATH. Same as above, but with lexical information attached: DSE/
denounced:OBJ?:ESE/situation.
DOMINANCE. In addition to the features based on syntactic paths, we created a
more generic feature template describing dominance relations between expres-
sions. For instance, from the graph in Figure 2, we extract the feature DSE/
denounced?ESE/situation, meaning that a DSE with the word denounced domi-
nates an ESE with the word situation.
The features based on semantic roles were the following:
PREDICATE SENSE LABEL. For every predicate found inside an opinion expression, we
add a feature consisting of the expression label and the predicate sense identifier.
For instance, the verb denounced, which is also a DSE, is representedwith the feature
DSE/denounce.01.
PREDICATE AND ARGUMENT LABEL. For every argument of a predicate inside an
opinion expression, we also create a feature representing the predicate?argument
pair: DSE/denounced.01:A0.
CONNECTING ARGUMENT LABEL. When a predicate inside some opinion expression
is connected to some argument inside another opinion expression, we use a feature
consisting of the two expression labels and the argument label. For instance, the
ESE defenseless situation is connected to the DSE denounced via an A1 label, and we
represent this using a feature DSE:A1:ESE.
6.3 Opinion Holder Interaction Features
In addition, we modeled the interaction between different opinions with respect to their
holders. We used the following two features to represent this interaction:
SHARED HOLDERS. A feature representing whether or not two opinion expressions
have the same holder. For instance, if a DSE dominates an ESE and they have the
same holder as in Figure 2, where the holder is HRW, we represent this by the
feature DSE:ESE:true.
HOLDER TYPES + PATH. A feature representing the types of the holders, combined with
the syntactic path between the expressions. The types take the following possible
values: explicit, implicit, writer. In Figure 2, we would thus extract the feature
DSE/Expl:OBJ?:ESE/Expl.
487
Computational Linguistics Volume 39, Number 3
6.4 Polarity Interaction Features
The model used the following features that take the polarities of the expressions into
account. These features are extracted from DSEs and ESEs only, because the OSEs have
no polarity values. The examples of extracted features are given with respect to the two
opinion expressions (denounced and defenseless situation) in Figure 2, both of which have
a negative polarity value.
POLARITY PAIR. For every pair of opinion expressions in the sentence, we create a
feature consisting of the pair of polarity values, such as NEGATIVE:NEGATIVE.
POLARITY PAIR AND SYNTACTIC PATH. NEGATIVE:OBJ?:NEGATIVE.
POLARITY PAIR AND SYNTACTIC DOMINANCE. NEGATIVE?NEGATIVE.
POLARITY PAIR AND WORD PAIR. NEGATIVE-denounced:NEGATIVE-situation.
POLARITY PAIR AND EXPRESSION TYPES. Adds the expression types (ESE or DSE) to
the polarity pair: DSE-NEGATIVE:ESE-NEGATIVE.
POLARITY PAIR AND TYPES AND SYNTACTIC PATH. Adds syntactic information to the
type and polarity combination: DSE-NEGATIVE:OBJ?:ESE-NEGATIVE.
POLARITY PAIR AND SHALLOW-SEMANTIC RELATION. When two opinion expres-
sions are directly connected through a link in the shallow-semantic structure, we
create a feature based on the semantic role label of the connecting link: NEGA-
TIVE:A1:NEGATIVE.
POLARITY PAIR AND WORDS ALONG SYNTACTIC PATH. We follow the syntactic path
between the two expressions and create a feature for every word we pass on
the way. In the example, no such feature is extracted because the expressions are
directly connected.
7. Experiments
We trained and evaluated the rerankers on version 2.0 of the MPQA corpus,2 which
contains 692 documents. We discarded one document whose annotation was garbled
and we split the remaining 691 into a training set (541 documents) and a test set (150
documents). We also set aside a development set of 90 documents from the training set
that we used when developing features and tuning learning algorithm parameters; all
experiments described in this article, however, usedmodels that were trained on the full
training set. Table 1 shows some statistics about the training and test sets: the number
of documents and sentences; the number of DSEs, ESEs, and OSEs; and the number of
expressions marked with the various polarity labels.
We considered three experimental settings: (1) opinion expression extraction;
(2) joint opinion expression and holder extraction; and (3) joint opinion expression and
polarity classification. Finally, the polarity-based opinion extraction system was used in
an extrinsic evaluation: document polarity classification of movie reviews.
To generate the training data for the rerankers, we carried out a 5-fold hold-out
procedure: We split the training set into five pieces, trained a sequence labeler and
secondary classifiers on pieces 1?4, applied them to piece 5, and so on.
2 http://www.cs.pitt.edu/mpqa/databaserelease/.
488
Johansson and Moschitti Relational Features in Fine-Grained Opinion Analysis
Table 1
Statistics for the training and test splits of the MPQA collection.
Training Test
Documents 541 150
Sentences 12,010 3,743
DSE 8,389 2,442
ESE 10,279 3,370
OSE 3,048 704
POSITIVE 3,192 1,049
NEGATIVE 6,093 1,675
NEUTRAL 9,105 3,007
BOTH 278 81
7.1 Evaluation Metrics
Because expression boundaries are hard to define rigorously (Wiebe,Wilson, and Cardie
2005), our evaluations mainly used intersection-based precision and recallmeasures to
score the quality of the system output. The idea is to assign values between 0 and 1, as
opposed to traditional precision and recall where a span is counted as either correctly
or incorrectly detected. We thus define the span coverage c of a span s (a set of token
indices) with respect to another span s?, which measures how well s? is covered by s:
c(s, s?) = |s ? s
?|
|s?|
In this formula, |s|means the length of the span s, and the intersection ? gives the set of
token indices that two spans have in common. Because our evaluation takes span labels
(DSE, ESE, OSE) into account, we set c(s, s?) to zero if the labels associated with s and s?
are different.
Using the span coverage, we define the span set coverage C of a set of spans S with
respect to a set S? :
C(S,S? ) =
?
sj?S
?
s?k?S?
c(sj, s?k)
We now define the intersection-based precision P and recall R of a proposed set of spans
S? with respect to a gold standard set S as follows:
P(S, S?) = C(S, S?)|S?| R(S, S?) =
C(S?,S)
|S|
Note that in this formula, |S|means the number of spans in a set S.
Conventionally, when measuring the quality of a system for an information extrac-
tion task, a predicted entity is counted as correct if it exactly matches the boundaries of
a corresponding entity in the gold standard; there is thus no reward for close matches.
Because the boundaries of the spans annotated in the MPQA corpus are not strictly
489
Computational Linguistics Volume 39, Number 3
defined in the annotation guidelines (Wiebe, Wilson, and Cardie 2005), however, mea-
suring precision and recall using exact boundary scoring will result in figures that are
too low to be indicative of the usefulness of the system. Therefore, most work using
this corpus instead use overlap-based precision and recall measures, where a span
is counted as correctly detected if it overlaps with a span in the gold standard (Choi,
Breck, and Cardie 2006; Breck, Choi, and Cardie 2007). As pointed out by Breck, Choi,
and Cardie (2007), this is problematic because it will tend to reward long spans?for
instance, a span covering the whole sentence will always be counted as correct if the
gold standard contains any span for that sentence. Conversely, the overlap metric does
not give higher credit to a span that is perfectly detected than to one that has a very low
overlap with the gold standard.
The precision and recall measures proposed here correct the problem with overlap-
based measures: If the system proposes a span covering the whole sentence, the span
coverage will be low and result in a low soft precision, and a low soft recall will be
assigned if only a small part of a gold standard span is covered. Note that our measures
are bounded below by the exact measures and above by the overlap-based measures.
7.1.1 Opinion Holders. To score the extraction of opinion holders, we started from the
same basic idea: Assign a score based on intersection. The evaluation of this task
is more complex, however, because (1) we only want to give credit for holders for
correctly extracted opinion expressions; (2) the gold standard links opinion expressions
to coreference chains rather than individual mentions of holders; and (3) the holder
entity may be the writer or implicit (see Section 4.2).
We therefore used the following method: If the system has proposed an opinion
expression e and its holder h, we first located the expression e? in the gold standard that
most closely corresponds to e, that is e? = argmaxx c(x, e), regardless of the span labels
of e and e?. To assign a score to the proposed holder entity, we then selected the most
closely corresponding gold standard holder entity h? in the coreference chain H? linked
to e?: h? = argmaxx?H? c(x, h). Finally, we computed the precision and recall scores using
c(h?, h) and c(h, h?). We stress again that the gold standard coreference chains were used
for evaluation purposes only, and that our system did not make use of them at test time.
If the system guesses that the holder of some opinion is the writer entity, we score
it as perfectly detected (coverage 1) if the coreference chain H annotated in the gold
standard contains the writer, and a full error (coverage 0) otherwise, and similar if h is
implicit.
7.1.2 Polarity. In our experiments involving opinion expressions with polarities, we
report precision and recall values for polarity-labeled opinion expression segmentation:
In order to be assigned an intersection score above zero, a segment must be labeled with
the correct polarity. In the gold standard, all polarity labels were changed as described
in Section 4.3. In these evaluations, OSEs were ignored and DSEs and ESEs were not
distinguished.
7.2 Experiments in Opinion Expression Extraction
The first task we considered was the extraction of opinion expression (labeled with
expression types). We first studied the impact of the machine learning method and
hypothesis set size on the reranker performance. Then, we carried out an analysis of the
effectiveness of the features used by the reranker. We finally compared the performance
of the expression extraction system with previous work (Breck, Choi, and Cardie 2007).
490
Johansson and Moschitti Relational Features in Fine-Grained Opinion Analysis
Table 2
Evaluation of reranking learning methods.
Learning method P R F
Baseline 63.4 ? 1.5 46.8 ? 1.2 53.8 ? 1.1
Structured SVM 61.8 ? 1.5 52.5 ? 1.3 56.8 ? 1.1
Perceptron 62.8 ? 1.5 48.1 ? 1.3 54.5 ? 1.2
Passive?Aggressive 63.5 ? 1.5 51.8 ? 1.3 57.0 ? 1.1
Table 3
Oracle and reranker performance as a function of candidate set size.
Reranked Oracle
k P R F P R F
1 63.36 46.77 53.82 63.36 46.77 53.82
2 63.70 48.17 54.86 72.66 55.18 62.72
4 63.57 49.78 55.84 79.12 62.24 69.68
8 63.50 51.79 57.05 83.72 68.14 75.13
16 63.00 52.94 57.54 86.92 72.79 79.23
32 62.15 54.50 58.07 89.18 76.76 82.51
64 61.03 55.67 58.23 91.09 80.19 85.29
128 60.22 56.45 58.27 92.63 83.00 87.55
256 59.87 57.22 58.51 94.01 85.27 89.43
7.2.1 Evaluation of Machine Learning Methods. We compared the machine learning meth-
ods described in Section 5. In these experiments, we used a hypothesis set size k of 8. All
features from Section 6.2 were used. Table 2 shows the results of the evaluations using
the precision and recall measures described earlier.3 The baseline is the result of taking
the top-scoring labeling from the base sequence labeler.
We note that the margin-based methods?structured SVM and the on-line PA
algorithm?outperform the perceptron soundly, which shows the benefit of learning
methods that make use of the cost function ?. Comparing the two best-performing
learning methods, we note that the reranker using the structured SVM is more recall-
oriented whereas the PA-based reranker more precision-oriented; the difference in
F-measure is not statistically significant. In the remainder of this article, all rerankers are
trained using the PA learning algorithm (with the same parameters) because its training
process is much faster than that of the structured SVM.
7.2.2 Candidate Set Size. In any method based on reranking, it is important to study the
influence of the hypothesis set size on the quality of the reranked output. In addition,
an interesting question is what the upper bound on reranker performance is?the
oracle performance. Table 3 shows the result of an experiment that investigates these
questions.
3 All confidence intervals in this article are at the 95% level and were estimated using a resampling method
(Hjorth 1993). The significance tests for differences were carried out using permutation tests.
491
Computational Linguistics Volume 39, Number 3
Table 4
Investigation of the contribution of syntactic features.
Feature set P R F
Baseline 63.4 ? 1.5 46.8 ? 1.2 53.8 ? 1.1
All syntactic features 62.5 ? 1.4 53.2 ? 1.2 57.5 ? 1.1
Removed SYNTACTIC PATH 64.4 ? 1.5 48.7 ? 1.2 55.5 ? 1.1
Removed LEXICAL PATH 62.6 ? 1.4 53.2 ? 1.2 57.5 ? 1.1
Removed DOMINANCE 62.3 ? 1.5 52.9 ? 1.2 57.2 ? 1.1
As is common in reranking tasks, the reranker can exploit only a fraction of the
potential improvement?the reduction of the F-measure error ranges between 10% and
15% of the oracle error reduction for all hypothesis set sizes.
The most visible effect of the reranker is that the recall is greatly improved. This
does not seem to have an adverse effect on the precision, however, until the candidate
set size goes above eight?in fact, the precision actually improves over the baseline for
small candidate set sizes. After the size goes above eight, the recall (and the F-measure)
still rises, but at the cost of decreased precision. In the remainder of this article, we used
a k value of 64, which we thought gave a good balance between processing time and
performance.
7.2.3 Feature Analysis. We studied the impact of syntactic and semantic structural fea-
tures on the performance of the reranker. Table 4 shows the result of an investigation
of the contribution of the syntactic features. Using all the syntactic features (and no
semantic features) gives an F-measure roughly four points above the baseline. We then
carried out an ablation test and measured the F-measure obtained when each one of
the three syntactic features has been removed. It is clear that the unlexicalized syntactic
path is the most important syntactic feature; this feature causes a two-point drop in
F-measure when removed, which is clearly statistically significant (p < 0.0001). The
effect of the two lexicalized features is smaller, with only DOMINANCE causing a sig-
nificant (p < 0.05) drop when removed.
A similar result was obtained when studying the semantic features (Table 5). Re-
moving the connecting label feature, which is unlexicalized, has a greater effect than
removing the other two semantic features, which are lexicalized. Only the connecting
label causes a statistically significant drop when removed (p < 0.0001).
Because our most effective structural features combine a pair of opinion expression
labels with a tree fragment, it is interesting to study whether the expression labels alone
would be enough. If this were the case, we could conclude that the improvement is
Table 5
Investigation of the contribution of semantic features.
Feature set P R F
Baseline 63.4 ? 1.5 46.8 ? 1.2 53.8 ? 1.1
All semantic features 61.3 ? 1.4 53.8 ? 1.3 57.3 ? 1.1
Removed PREDICATE SENSE LABEL 61.3 ? 1.4 53.8 ? 1.3 57.3 ? 1.1
Removed PREDICATE+ARGUMENT LABEL 61.0 ? 1.4 53.6 ? 1.3 57.0 ? 1.1
Removed CONNECTING ARGUMENT LABEL 60.7 ? 1.4 50.5 ? 1.2 55.1 ? 1.1
492
Johansson and Moschitti Relational Features in Fine-Grained Opinion Analysis
Table 6
Structural features compared to label pairs.
Feature set P R F
Baseline 63.4 ? 1.5 46.8 ? 1.2 53.8 ? 1.1
Label pairs 62.0 ? 1.5 52.7 ? 1.2 57.0 ? 1.1
All syntactic features 62.5 ? 1.4 53.2 ? 1.2 57.5 ? 1.1
All semantic features 61.3 ? 1.4 53.8 ? 1.3 57.3 ? 1.1
Syntactic + semantic 61.0 ? 1.4 55.7 ? 1.2 58.2 ? 1.1
Syntactic + semantic + label pairs 61.6 ? 1.4 54.8 ? 1.3 58.0 ? 1.1
caused not by the structural features, but just by learning which combinations of labels
are common in the training set, such as that DSE+ESE would be more common than
OSE+ESE. We thus carried out an experiment comparing a reranker using label pair
features against rerankers based on syntactic features only, semantic features only, and
the full feature set. Table 6 shows the results. We see that the reranker using label pairs
indeed achieves a performance well above the baseline. Its performance is below that of
any reranker using structural features, however. In addition, we see no improvement
when adding label pair features to the structural feature set; this is to be expected
because the label pair information is subsumed by the structural features.
7.2.4 Analysis of the Performance Depending on Expression Type. In order to better under-
stand the performance details of the expression extraction, we analyzed how well it
extracted the three different classes of expressions. Table 7 shows the results of this
evaluation. The DSE row in the table thus shows the results of the performance on DSEs,
without taking ESEs or OSEs into account.
Apart from evaluations of the three different types of expressions, we evaluated
the performance for a number of combined classes that we think may be interesting
for applications: DSE & ESE, finding all opinionated expressions and ignoring objective
speech events; DSE & OSE, finding opinionated and non-opinionated speech and cat-
egorization events and ignoring expressive elements; and unlabeled evaluation of all
types of MPQA expressions. The same extraction system was used in all experiments,
and it was not retrained to maximize the different measures of performance.
Again, the strongest overall tendency is that the reranker boosts the recall. Going
into the details, we see that the reranker gives very large improvements for DSEs and
OSEs, but a smaller improvement for the combined DSE & OSE class. This shows that
Table 7
Performance depending on the type of expression.
Baseline Reranked
P R F P R F
DSE 68.5 ? 2.1 57.8 ? 2.0 62.7 ? 1.7 67.0 ? 2.0 64.9 ? 1.9 66.0 ? 1.6
ESE 63.0 ? 2.1 36.9 ? 1.5 46.5 ? 1.4 58.2 ? 1.9 46.2 ? 1.6 51.5 ? 1.3
OSE 53.5 ? 3.5 62.3 ? 3.5 57.5 ? 3.0 57.0 ? 3.3 73.9 ? 3.2 64.3 ? 2.7
DSE & ESE 71.1 ? 1.6 48.1 ? 1.2 57.4 ? 1.1 68.0 ? 1.5 57.6 ? 1.3 62.4 ? 1.0
DSE & OSE 76.6 ? 1.8 70.3 ? 1.6 73.3 ? 1.3 72.6 ? 1.8 75.8 ? 1.5 74.2 ? 1.2
Unlabeled 75.6 ? 1.5 55.3 ? 1.2 63.9 ? 1.0 71.0 ? 1.4 63.7 ? 1.2 67.2 ? 1.0
493
Computational Linguistics Volume 39, Number 3
one of the most clear benefits of the complex features is to help disambiguate these
expressions. This also affects the performance for general opinionated expressions (DSE
& ESE).
7.2.5 Comparison with Breck, Choi, and Cardie (2007). Comparison of systems in opinion
expression detection is often nontrivial because evaluation settings have differed
widely. Since our problem setting?marking up and labeling opinion expressions in the
MPQA corpus?is most similar to that described by Breck, Choi, and Cardie (2007), we
carried out an evaluation using the setting from their experiment.
For compatibility with their experimental set-up, this experiment differed from the
ones described in the previous sections in the following ways:
r The results were measured using the overlap-based precision and recall,
although this is problematic as pointed out in Section 7.1.
r The system did not need to distinguish DSEs and ESEs and did not have to
detect the OSEs.
r Instead of the training/test split used in the previous evaluations, the
systems were evaluated using a 10-fold cross-validation over the same set
of 400 documents and the same cross-validation split as used in Breck,
Choi, and Cardie?s experiment. Each of the 10 rerankers was evaluated on
one fold and trained on data generated in a cross-validation over the
remaining nine folds.
Again, our reranker uses the PA learning method with the full feature set (Section 6.2)
and a hypothesis set size k of 64. Table 8 shows the performance of our baseline (Section
4.1) and reranked system, along with the best results reported by Breck, Choi, and
Cardie (2007).
We see that the performance of our system is clearly higher?in both precision and
recall?than all results reported by Breck, Choi, and Cardie (2007). Note that our system
was optimized for the intersection metric rather than the overlap metric and that we
did not retrain it for this evaluation.
7.3 Opinion Holder Extraction
Table 9 shows the performance of our holder extraction systems, evaluated using
the scoring method described in Section 7.1.1. We compared the performance of the
reranker using opinion holder interaction features (Section 6.3) to two baselines: The
first of them consisted of the opinion expression sequence labeler (ES, Section 4.1) and
the holder extraction classifier (HC, Section 4.2), without modeling any interactions
between opinions. The second and more challenging baseline was implemented by
Table 8
Results using the evaluation setting from Breck, Choi, and Cardie (2007).
System P R F
Breck, Choi, and Cardie (2007) 71.64 74.70 73.05
Baseline 86.1 ? 1.0 66.7 ? 0.8 75.1 ? 0.7
Reranked 83.4 ? 1.0 75.0 ? 0.8 79.0 ? 0.6
494
Johansson and Moschitti Relational Features in Fine-Grained Opinion Analysis
Table 9
Opinion holder extraction results.
System P R F
ES+HC 57.7 ? 1.7 45.3 ? 1.3 50.8 ? 1.3
ES+ER+HC 53.3 ? 1.5 52.0 ? 1.4 52.6 ? 1.3
ES+HC+EHR 53.2 ? 1.6 55.1 ? 1.5 54.2 ? 1.4
adding the opinion expression reranker (ER) without holder interaction features to
the pipeline. This results in a large performance boost simply as a consequence of
improved expression detection, because a correct expression is required to get credit
for a holder. However, both baselines are outperformed by the reranker using holder
interaction features, which we refer to as the expression/holder reranker (EHR); the
differences to the strong baseline in recall and F-measure are both statistically significant
(p < 0.0001).
We carried out an ablation test to gauge the impact of the two holder interaction
features; we see in Table 10 that both of them contribute to improving the recall, and the
effect on the precision is negligible. The statistical significance for the recall improve-
ment is highest for SHARED HOLDERS (p < 0.0001) and lower for HOLDER TYPES +
PATH (p < 0.02).
We omit a comparison with previous work in holder extraction because our formu-
lation of the opinion holder extraction problem is different from those used in previous
publications. Choi, Breck, and Cardie (2006) used the holders of a simplified set of
opinion expressions, whereas Wiegand and Klakow (2010) extracted every entity tagged
as ?source? inMPQA regardless of whether it was connected to any opinion expression.
Neither of them extracted implicit or writer holders.
Table 11 shows a detailed breakdown of the holder extraction results based on
opinion expression type (DSE, OSE, and ESE), and whether the holder is internally or
externally located; that is, whether or not the holder is textually realized in the same
sentence as the opinion expression. In addition, Table 12 shows the performance for the
two types of externally located holders.
As we noted in previous evaluations, the most obvious change between the baseline
system and the reranker is that the recall and F-measure are improved; this is the case
in every single evaluation. As previously, a large share of the improvement is explained
simply by improved expression detection, which can be seen by comparing the reranked
system to the strong baseline (ES+ER+HC). For the most important situations, however,
we see improvement when using the reranker with holder interaction features. In
those cases it outperforms the strong baseline significantly: DSE internal: p < 0.001,
ESE internal p < 0.001, ESE external p < 0.05 (Table 11), writer p < 0.05 (Table 12).
Table 10
Opinion holder reranker feature ablation test.
Feature set P R F
Both features 53.2 ? 1.6 55.1 ? 1.5 54.2 ? 1.4
Removed HOLDER TYPES + PATH 53.1 ? 1.6 54.6 ? 1.5 53.8 ? 1.3
Removed SHARED HOLDERS 53.1 ? 1.5 53.6 ? 1.5 53.3 ? 1.3
495
Computational Linguistics Volume 39, Number 3
Table 11
Detailed opinion holder extraction results.
DSE Internal External
P R F P R F
ES+HC 57.4 ? 2.4 48.9 ? 2.2 52.8 ? 1.9 32.3 ? 6.8 25.8 ? 5.8 28.7 ? 5.8
ES+ER+HC 56.7 ? 2.2 54.2 ? 2.2 55.5 ? 1.9 33.3 ? 5.9 34.2 ? 6.1 33.7 ? 5.5
ES+HC+EHR 55.6 ? 2.2 58.8 ? 2.3 57.2 ? 1.9 35.2 ? 6.2 32.1 ? 6.0 33.6 ? 5.6
OSE Internal External
P R F P R F
ES+HC 46.2 ? 3.6 57.2 ? 3.9 51.1 ? 3.3 39.7 ? 12.0 35.2 ? 11.2 37.3 ? 10.5
ES+ER+HC 48.6 ? 3.4 66.8 ? 3.7 56.2 ? 3.1 36.8 ? 11.0 39.4 ? 11.4 38.1 ? 10.2
ES+HC+EHR 50.4 ? 3.6 65.9 ? 3.9 57.1 ? 3.2 35.9 ? 10.9 39.4 ? 11.4 37.6 ? 10.1
ESE Internal External
P R F P R F
ES+HC 50.5 ? 4.7 19.2 ? 2.1 27.8 ? 2.7 45.1 ? 3.0 41.2 ? 2.5 43.0 ? 2.4
ES+ER+HC 48.3 ? 3.9 29.3 ? 2.8 36.4 ? 2.9 40.7 ? 2.6 48.4 ? 2.7 44.2 ? 2.3
ES+HC+EHR 40.4 ? 3.4 36.5 ? 3.2 39.8 ? 3.0 43.2 ? 2.8 47.7 ? 2.9 45.3 ? 2.4
The only common case where the improvement is not statistically significant is OSE
internal.
The improvements are most notable for internally located holders, and especially
for the ESEs. Extracting the opinion holder for ESEs is often complex because the ex-
pression and the holder are typically not directly connected on the syntactic or shallow-
semantic level, as opposed to the typical situation for DSEs. When we use the reranker,
however, the interaction features may help us make use of the holders of other opinion
expressions in the same sentence; for instance, the interaction features make it easier
to distinguish cases like ?the film was [awful]ESE? with an external (writer) holder from
cases such as ?I [thought]DSE the film was [awful]ESE? with an internal holder directly
connected to a DSE.
Table 12
Opinion holder extraction results for external holders.
Writer P R F
ES+HC 44.8?3.0 42.8?2.6 43.8?2.4
ES+ER+HC 40.6?2.6 50.3?2.7 44.9?2.3
ES+HC+EHR 42.7?2.8 49.7?2.9 45.9?2.4
Implicit P R F
ES+HC 41.2?6.4 28.3?4.8 33.6?4.9
ES+ER+HC 38.7?5.4 34.4?5.1 36.4?4.7
ES+HC+EHR 43.1?5.9 32.9?5.0 37.4?4.8
496
Johansson and Moschitti Relational Features in Fine-Grained Opinion Analysis
Table 13
Overall evaluation of polarity-labeled opinion expression extraction.
System P R F
ES+PC 56.5 ? 1.7 38.4 ? 1.2 45.7 ? 1.2
ES+ER+PC 53.8 ? 1.6 44.5 ? 1.3 48.8 ? 1.2
ES+PC+EPR 54.7 ? 1.6 45.6 ? 1.3 49.7 ? 1.2
7.4 Polarity Classification
To evaluate the effect of the polarity-based reranker, we carried out experiments to
compare it with two baseline systems similarly to the evaluations of holder extraction
performance. Table 13 shows the precision, recall, and F-measures. The evaluation used
the polarity-based intersection metric (Section 7.1.2). The first baseline consisted of an
expression segmenter and a polarity classifier (ES+PC), and the second also included an
expression reranker (ER). The reranker using polarity interaction features is referred to
as the expression/polarity reranker (EPR).
The result shows that the polarity-based reranker gives a significant boost in recall,
which is in line with our previous results that also mainly improved the recall. The
precision shows a slight decrease from the ES+PC baseline but much lower than the
recall improvement. The differences between the polarity reranker and the strongest
baseline are all statistically significant (precision p < 0.02, recall and F-measure
p < 0.005).
In addition, we evaluated the performance for individual polarity values. The
figures are shown in Table 14. We see that the differences in performance when adding
the polarity reranker are concentrated to the more frequent polarity values (NEUTRAL
and NEGATIVE).
Table 14
Intersection-based evaluation for individual polarity values.
POSITIVE P R F
ES+PC 53.5 ? 3.7 37.3 ? 3.0 43.9 ? 2.8
ES+ER+PC 50.5 ? 3.4 41.8 ? 3.0 45.8 ? 2.6
ES+PC+EPR 51.0 ? 3.5 41.6 ? 3.1 45.8 ? 2.7
NEUTRAL P R F
ES+PC 56.4 ? 2.3 37.8 ? 1.7 45.3 ? 1.7
ES+ER+PC 54.0 ? 2.1 45.2 ? 1.8 49.2 ? 1.6
ES+PC+EPR 55.8 ? 2.1 46.1 ? 1.8 50.5 ? 1.6
NEGATIVE P R F
ES+PC 58.4 ? 2.8 40.1 ? 2.4 47.6 ? 2.2
ES+ER+PC 55.5 ? 2.7 45.0 ? 2.3 49.7 ? 2.0
ES+PC+EPR 54.9 ? 2.7 47.0 ? 2.4 50.6 ? 2.0
497
Computational Linguistics Volume 39, Number 3
Table 15
Overlap-based evaluation for individual polarity values, and comparison with the results
reported by Choi and Cardie (2010).
POSITIVE P R F
ES+PC 59.4 ? 2.6 46.1 ? 2.1 51.9 ? 2.0
ES+ER+PC 53.1 ? 2.3 50.9 ? 2.2 52.0 ? 1.9
ES+PC+EPR 58.2 ? 2.5 49.3 ? 2.2 53.4 ? 2.0
ES+PC+EPRp 63.6 ? 2.8 44.9 ? 2.2 52.7 ? 2.1
Choi and Cardie (2010) 67.1 31.8 43.1
NEUTRAL P R F
ES+PC 60.9 ? 1.4 49.2 ? 1.2 54.5 ? 1.0
ES+ER+PC 55.1 ? 1.2 57.7 ? 1.2 56.4 ? 1.0
ES+PC+EPR 60.3 ? 1.3 55.8 ? 1.2 58.0 ? 1.1
ES+PC+EPRp 68.3 ? 1.5 48.2 ? 1.2 56.5 ? 1.2
Choi and Cardie (2010) 66.6 31.9 43.1
NEGATIVE P R F
ES+PC 72.1 ? 1.8 52.0 ? 1.5 60.4 ? 1.4
ES+ER+PC 65.4 ? 1.7 58.2 ? 1.4 61.6 ? 1.3
ES+PC+EPR 67.6 ? 1.7 59.9 ? 1.5 63.5 ? 1.3
ES+PC+EPRp 75.4 ? 2.0 55.0 ? 1.5 63.6 ? 1.4
Choi and Cardie (2010) 76.2 40.4 52.8
Finally, we carried out an evaluation in the setting4 of Choi and Cardie (2010) and
the figures are shown in Table 15. The table shows our baseline and integrated systems
along with the figures5 from Choi and Cardie. Instead of a single value for all polarities,
we show the performance for every individual polarity value (POSITIVE, NEUTRAL,
NEGATIVE). This evaluation uses the overlap metric instead of the intersection-based
one. As we have pointed out, we use the overlap metric for compatibility although it is
problematic.
As can be seen from the table, the system by Choi and Cardie (2010) shows a large
precision bias despite being optimizedwith respect to the recall-promoting overlapmet-
ric. In recall and F-measure, their system is significantly outperformed for all polarity
values by our baseline consisting of a pipeline of opinion expression extraction and
polarity classifier. In addition, our joint model clearly outperforms the pipeline. The
precision is slightly lower overall, but this is offset by large boosts in recall in all cases.
In order to rule out the hypothesis that our F-measure improvement compared with
the Choi and Cardie system could be caused just by rebalancing precision and recall, we
additionally trained a precision-biased reranker EPRp by changing the loss function ?
(see Section 5.3) from 1? Fi to 1? 13Fi ? 23Po, where Fi is the intersection F-measure and
Po the overlap precision. When we use this reranker, we achieve almost the same levels
of precision as reported by Choi and Cardie, even outperforming their precision for the
4 In addition to polarity, their system also assigned opinion intensity, which we do not consider here.
5 Confidence intervals for Choi and Cardie (2010) are omitted because we had no access to their output.
498
Johansson and Moschitti Relational Features in Fine-Grained Opinion Analysis
NEUTRAL polarity value, while the recall values are still massively higher. The precision
bias causes slight drops in F-measure for the POSITIVE and NEUTRAL polarities.
7.5 First Extrinsic Evaluation: Extraction of Evaluations of Product Attributes
As an extrinsic evaluation of the opinion expression extraction system, we evaluated
the impact of the expressions on a practical application: extraction of evaluations of
attributes from product reviews. We first describe the collection we used and then the
implementation of the extractor.
We used the annotated data set by Hu and Liu (2004a, 2004b)6 for the experiments
in extraction of attribute evaluations from product reviews. The collection contains
reviews of five products: one DVD player, two cameras, one MP3 player, and one
cellular phone. In this data set, every sentence is associated with a set of attribute eval-
uations. An evaluation consists of an attribute name and an evaluation value between
?3 and +3, where ?3 means a strongly negative evaluation and +3 strongly positive.
For instance, the sentence this player boasts a decent size and weight, a relatively-intuitive
navigational system that categorizes based on id3 tags, and excellent sound is tagged with the
attribute evaluations size +2, weight +2, navigational system +2, sound +2. In this
work, we do not make use of the exact value of the evaluation but only its sign. We
removed the product attribute mentions in the form of anaphoric pronouns referring
to entities mentioned in previous sentences; these cases are directly marked in the
data set.
7.5.1 Implementation.We considered two problems: (1) extraction of attribute evaluations
without taking the polarity into account, and (2) extraction with polarity (positive or
negative). The former is modeled as a binary classifier that tags each word in the review
(except the punctuation) as an evaluation or not, and the latter requires the definition of
a three-class polarity classifier. For both tasks, we compared three feature sets: a baseline
using simple features, a stronger baseline using a lexicon, and finally a system using
features derived from opinion expressions.
Similarly to the opinion expression polarity classifier, we implemented the clas-
sifiers as SVMs that we trained using LIBLINEAR. For the extraction task without
polarities, the best results were obtained using an L2-regularized L2-loss SVM and a
C value of 0.1. For the polarity task, we used a multiclass SVM (Crammer and Singer
2001) with the same parameters. To handle the precision/recall tradeoff, we varied the
class weighting for the null class.
The baseline classifier used features based on lexical information (word, POS tag,
and lemma) in a window of size 3 around the word under consideration (the focus
word). In addition, it had two features representing the overall sentence polarities. To
compute the polarities, we trained bag-of-words classifiers following the implemen-
tation by Pang, Lee, and Vaithyanathan (2002). Two separate classifiers were used:
one for positive and one for negative polarity. Note that these classifiers detect the
presence of positive or negative polarity, which may thus occur in the same sentence. The
classifiers were trained on the MPQA corpus, where we counted a sentence as positive
if it contained a positive opinion expression with an intensity of at least MEDIUM, and
conversely for the negative polarity.
6 http://www.cs.uic.edu/?liub/FBS/CustomerReviewData.zip.
499
Computational Linguistics Volume 39, Number 3
7.5.2 Features Using a Sentiment Lexicon. Many previous implementations for several
opinion-related tasks make use of sentiment lexicons, so the stronger baseline system
used features based on the subjectivity lexicon by Wilson, Wiebe, and Hoffmann (2005),
which we previously used for opinion expression segmentation in Section 4.1 and for
polarity classification in Section 4.3. We created a classifier using a number of features
based on this lexicon.
These features make use of the syntactic and semantic structure of the sentence.
In the following examples, we use the sentence The software itself was not so easy to use,
presented in Figure 3. In this sentence, consider the focus word software. One word is
listed in the lexicon as associated with positive sentiment: easy. The system then extracts
the following features:
SENTIMENT LEXICON POLARITIES. For every word in the sentence that is listed in
the lexicon, we add a feature. Given the example sentence, we will thus add a
feature lex pol:positive because of the word easy, which is listed as positive in
the sentiment lexicon.
CLOSEST PREVIOUS AND FOLLOWING SENTIMENT WORD. If there are sentiment
words before or after the focus word, we add the closest of them to the feature
vector. In this case, there is no previous sentiment word, so we only extract fol-
lowing word:easy.
SYNTACTIC PATHS TO SENTIMENT WORDS. For every sentiment word in the sentence,
we extract a syntactic path similar to our previous feature sets. This represents a
syntactic pattern describing the relation between the sentiment word and the focus
words. For instance, in the example we extract the path SBJ?PRD?, representing a
copula construction: The word software is connected to the sentiment word easy first
up through a subject link and then down through a predicative complement link.
SEMANTIC LINKS TO SENTIMENT WORDS. When there is a direct semantic role link
between a sentiment word and the focus word, we add a feature for the semantic
role label. No such features are extracted in the example sentence. The focus word
is an argument but no sentiment word is also a predicate.
7.5.3 Extended Feature Set Based on MPQA Opinion Expressions. We finally created an
extended feature set incorporating the following features derived from MPQA-style
opinion expressions, which we extracted automatically. The features are similar in
construction to those extracted by means of the sentiment lexicon. The following list
describes the new features exemplified with the same sentence above, which contains a
negative opinion expression not so easy.
softwareThe itself was not so easy
A1
NMOD APPO
SBJ PRD
ADV AMOD
[ ]
ESE
useto
IM
use.01
AMOD
Figure 3
Example sentence for product feature evaluation extraction.
500
Johansson and Moschitti Relational Features in Fine-Grained Opinion Analysis
Table 16
Product attribute evaluation extraction performance.
Feature representation Unlabeled Polarity-labeled
Baseline 49.8 ? 2.0 39.6 ? 2.0
Lexicon 53.8 ? 2.0 46.2 ? 2.0
Opinion expressions 54.8 ? 2.0 49.0 ? 2.0
OPINION EXPRESSION POLARITIES. For every opinion expression extracted by the
automatic system, we add a feature representing the polarity of the expression.
In the example, we get op expr:negative.
CLOSEST PREVIOUS AND FOLLOWING OPINION EXPRESSION WORD. We extract fea-
tures for the closest words before and after the focus word that are contained in
some opinion expression. In the example, there is an expression not so easy after the
focus word software, so we get a single feature following expr:not.
SYNTACTIC PATHS TO OPINION EXPRESSIONS. For every opinion expression in the
sentence, we extract a path from the expression to the focus word. Because opinion
expressions frequently consist of more than one word, we use the shortest path. In
this case, we will thus again get SBJ?PRD?.
SEMANTIC LINKS TO OPINION EXPRESSIONS. Finally, we extracted features in case
there were semantic role links. Again, we get no features based on the semantic
role structure in the example since the opinion expression contains no predicate or
argument.
7.5.4 Results. We evaluated the performance of the product attribute evaluation extrac-
tion using a 10-fold cross-validation procedure on the whole data set. We evaluated
three classifiers: a baseline that did not use the lexicon or the opinion expressions, a
classifier that adds the lexicon-based features, and finally the classifier that adds the
MPQA opinion expressions. The F-measures are shown in Table 16 for the extraction
task, and Figure 4 shows the precision/recall plots. There are clear improvements when
adding the lexicon features, but the highest performing system is the one that also
used the opinion expression features. The difference between the two top-performing
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.4 0.45 0.5 0.55 0.6 0.65 0.7
Re
ca
ll
Precision
Unlabeled
Baseline
Lexicon
Opinion Expressions
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 0.65
Re
ca
ll
Precision
Polarity-labeled
Baseline
Lexicon
Opinion Expressions
Figure 4
Precision / recall curves for extraction of product attribute evaluations.
501
Computational Linguistics Volume 39, Number 3
classifiers is statistically significant (p < 0.001). For the extraction task where we also
consider the polarities, the difference is even greater: almost three F-measure points.
7.6 Second Extrinsic Evaluation: Document Polarity Classification Experiment
In a second extrinsic evaluation of the opinion expression extractor, we investigated
how expression-based features affect the performance of a document-level polarity
classifier of reviews as positive or negative. We followed the same evaluation protocol
as in the first extrinsic evaluation, where we compare three classifiers of increasing
complexity: (1) a baseline using a pure word-based representation, (2) a stronger base-
line adding features derived from a sentiment lexicon, and (3) a classifier with features
extracted from opinion expressions.
The task of categorizing a full document as positive or negative can be viewed
as a document categorization task, and this has led to the application of standard
text categorization techniques (Pang, Lee, and Vaithyanathan 2002). We followed this
approach and implemented the document polarity classifier as a binary linear SVM;
this learning method has a long tradition of successful application in text categorization
(Joachims 2002).
For these experiments, we used six collections. The first one consisted of movie
reviews written in English extracted from the Web by Pang and Lee (2004).7 This data
set is an extension of a smaller set (Pang, Lee, and Vaithyanathan 2002) that has been
used in a large number of experiments. The remaining five sets consisted of product
reviews gathered by Blitzer, Dredze, and Pereira (2007).8 We used five of the largest
subsets: reviews of DVDs, software, books, music, and cameras. In all six collections,
1,000 documents were labeled by humans as positive and 1,000 as negative.
Following Pang and Lee (2004), the documents were represented as bag-of-word
feature vectors based on presence features for individual words. No weighting such as
IDF was used. The vectors were normalized to unit length. Again, we trained the SVMs
using LIBLINEAR, and the best results were obtained using an L2-regularized L2-loss
version of the SVM with a C value of 1.
7.6.1 Features Based on the Subjectivity Lexicon. We used features based on the subjectivity
lexicon by Wilson, Wiebe, and Hoffmann (2005) that we used for opinion expression
segmentation in Section 4.1 and for polarity classification in Section 4.3. For every word
whose lemma is listed in the lexicon, we added a feature consisting of the word and its
prior polarity and intensity to the bag-of-words feature vector.
The feature examples are taken from the sentence HRW has denounced the defenseless
situation of these prisoners, where denounce is listed in the lexicon as strong/negative
and prisoner as weak/negative.
LEXICON POLARITY. negative.
LEXICON POLARITY AND INTENSITY. strong/negative, weak/negative.
LEXICON POLARITY AND WORD. denounced/negative, prisoners/negative.
7 http://www.cs.cornell.edu/people/pabo/movie-review-data/.
8 http://www.cs.jhu.edu/?mdredze/datasets/sentiment/unprocessed.tar.gz.
502
Johansson and Moschitti Relational Features in Fine-Grained Opinion Analysis
7.6.2 Features Extracted from Opinion Expressions. Finally, we created a feature set based
on the opinion expressions with polarities. We give examples from the same sentence;
here, denounced is a negative DSE and defenseless situation is a negative ESE.
EXPRESSION POLARITY. negative.
EXPRESSION POLARITY AND WORD. negative/denounced, negative/defenseless,
negative/situation.
EXPRESSION TYPE AND WORD. DSE/denounced, ESE/defenseless, ESE/situation.
7.6.3 Evaluation Results. To evaluate the performance of the document polarity classifiers,
we carried out a 10-fold cross-validation procedure for every review collection. We
evaluated three classifiers: one using only bag-of-words features (?Baseline?); one using
features extracted from the subjectivity lexicon (?Lexicon?); and finally one also using
the expression-based features (?Expressions?).
In order to abstract away from the tuning threshold, the performances were mea-
sured using AUC, the area under ROC curve. The AUC values are given in Table 17.
These evaluations show that the classifier adding features extracted from the opin-
ion expressions significantly outperforms the classifier using only a bag-of-words fea-
ture representation and also that using the lexicon-based features. This demonstrates
that the extraction and disambiguation of opinion expressions in their context is useful
for a coarse-grained task such as document polarity classification. The differences in
AUC values between the two best configurations are statistically significant (p < 0.005
for all six collections). In addition, we show the precision/recall plots in Figure 5; we
see that for all six collections, the expression-based set-up outperforms the other two
near the precision/recall breakeven point.
The collection where we can see the most significant difference is the movie review
set. The main difference of this collection compared with the other collections is that its
documents are larger: The average size of a document here is about four times larger
than in the other collections. In addition, its reviews often contain large sections that are
purely factual in nature, mainly plot descriptions. The opinion expression identification
may be seen as a way to process the document to highlight the interesting parts on
which the classifier should focus.
8. Conclusion
We have shown that features derived from grammatical and semantic role structure
can be used to improve three fundamental tasks in fine-grained opinion analysis: the
detection of opinionated expressions, the extraction of opinion holders, and finally the
Table 17
Document polarity classification evaluation (AUC values).
Feature set Movie DVD Software Books Music Cameras
Baseline 93.1 ? 1.0 85.1 ? 1.7 91.0 ? 1.3 85.7 ? 1.6 84.7 ? 1.7 91.9 ? 1.2
Lexicon 93.8 ? 1.0 86.6 ? 1.6 92.3 ? 1.2 87.4 ? 1.5 86.6 ? 1.5 92.9 ? 1.1
Expressions 94.7 ? 0.9 87.2 ? 1.5 92.9 ? 1.1 88.1 ? 1.5 87.5 ? 1.5 93.6 ? 1.1
503
Computational Linguistics Volume 39, Number 3
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1
Re
ca
ll
Precision
Movies
Baseline
Lexicon
Opinion Expressions
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1
Re
ca
ll
Precision
DVDs
Baseline
Lexicon
Opinion Expressions
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1
Re
ca
ll
Precision
Software
Baseline
Lexicon
Opinion Expressions
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1
Re
ca
ll
Precision
Books
Baseline
Lexicon
Opinion Expressions
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1
Re
ca
ll
Precision
Music
Baseline
Lexicon
Opinion Expressions
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1
Re
ca
ll
Precision
Cameras
Baseline
Lexicon
Opinion Expressions
Figure 5
Precision / recall curves for detection of positive reviews.
assignment of polarity labels to opinion expressions. The main idea is to use relational
features describing the interaction of opinion expressions through linguistic structures
such as syntax and semantics. This is not only interesting from a practical point of
view (improving performance) but also confirms our linguistic intuitions that surface-
linguistic structure phenomena such as syntax and shallow semantics are used in the
encoding of the rhetorical organization of the sentence, and that we can thus extract
useful information from those structures.
504
Johansson and Moschitti Relational Features in Fine-Grained Opinion Analysis
Because our feature sets are based on interaction between opinion expressions
that can appear anywhere in a sentence, exact inference in this model becomes in-
tractable. To overcome this issue, we used an approximate search strategy based on
reranking: In the first step, we used the baseline systems, which use only simple local
features, to generate a relatively small hypothesis set; we then applied a classifier using
interaction features to pick the final result. A common objection to reranking is that
the candidate set may not be diverse enough to allow for much improvement unless
it is very large; the candidates may be trivial variations that are all very similar to
the top-scoring candidate. Investigating inference methods that take a less brute-force
approach than plain reranking is thus another possible future direction. Interesting
examples of such inference methods include forest reranking (Huang 2008) and loopy
belief propagation (Smith and Eisner 2008). Nevertheless, although the development of
such algorithms is a fascinating research problem, it will not necessarily result in a more
usable system: Rerankers impose very few restrictions on feature expressivity andmake
it easy to trade accuracy for efficiency.
We investigated the effect of machine learning features, as well as other design
parameters such as the choice of machine learning method and the size of the
hypothesis set. For the features, we analyzed the impact of using syntax and semantics
and saw that the best models are those making use of both. The most effective features
we have found are purely structural, based on tree fragments in a syntactic or semantic
tree. Features involving words generally did not seem to have the same impact. Sparsity
may certainly be an issue for features defined in terms of tree fragments. Possible future
extensions in this area could include bootstrapping methods to mine for meaningful
fragments unseen in the training set, or methods to group such features into clusters to
reduce the sparsity.
In addition to the core results on fine-grained opinion analysis, we have described
experiments demonstrating that features extracted from opinion expressions can be
used to improve practical applications: extraction of evaluations of product attributes,
and document polarity classification. Although for the first task it may be fairly obvious
that it is useful to carry out a fine-grained analysis of the sentence opinion structure,
the second result is more unexpected because the document polarity classification task
is a high-level and coarse-grained task. For both tasks, we saw statistically significant
increases in performance compared not only to simple baselines, but also compared
to strong baselines using a lexicon of sentiment words. Although the lexicon leads to
clear improvements, the best classifiers also used the features extracted from the opinion
expressions.
It is remarkable that the opinion expressions as defined by the MPQA corpus are
useful for practical applications on reviews from several domains, because this corpus
mainly consists of news documents related to political topics; this shows that the expres-
sion identifier has been able to generalize from the specific domains. It would still be
relevant, however, to apply domain adaptation techniques (Blitzer, Dredze, and Pereira
2007). It could also be interesting to see how domain-specific opinion word lexicons
could improve over the generic lexicon we used here; especially if such a lexicon were
automatically constructed (Jijkoun, de Rijke, and Weerkamp 2010).
There are multiple additional opportunities for future work in this area. An impor-
tant issue that we have left open is the coreference problem for holder extraction, which
has been studied by Stoyanov and Cardie (2006). Similarly, recent work has tried to
incorporate complex, high-level linguistic structure such as discourse representations
(Asher, Benamara, and Mathieu 2009; Somasundaran et al 2009; Zirn et al 2011); it is
clear that these structures are very relevant for explaining the way humans organize
505
Computational Linguistics Volume 39, Number 3
their expressions of opinions rhetorically. Theoretical depth does not necessarily guar-
antee practical applicability, however, and the challenge is as usual to find a middle
ground that balances our goals: explanatory power in theory, significant performance
gains in practice, computational tractability, and robustness in difficult circumstances.
Acknowledgments
We would like to thank Eric Breck and
Yejin Choi for clarifying their results and
experimental set-up, and for sharing their
cross-validation split. In addition, we are
grateful to the anonymous reviewers,
whose feedback has helped to improve
the clarity and readability of this article.
The research described here has received
funding from the European Community?s
Seventh Framework Programme
(FP7/2007-2013) under grant 231126:
LivingKnowledge?Facts, Opinions and Bias
in Time, and under grant 247758:
Trustworthy Eternal Systems via Evolving
Software, Data and Knowledge (EternalS).
References
Asher, Nicholas, Farah Benamara, and
Yannick Mathieu. 2009. Appraisal of
opinion expressions in discourse.
Lingvisticae Investigations, 31(2):279?292.
Bethard, Steven, Hong Yu, Ashley Thornton,
Vasileios Hatzivassiloglou, and
Dan Jurafsky. 2005. Extracting opinion
propositions and opinion holders
using syntactic and lexical cues.
In James G. Shanahan, Yan Qu, and
Janyce Wiebe, editors, Computing
Attitude and Affect in Text: Theory and
Applications. Springer, New York,
chapter 11, pages 125?140.
Blitzer, John, Mark Dredze, and Fernando
Pereira. 2007. Biographies, Bollywood,
boom-boxes and blenders: Domain
adaptation for sentiment classification. In
Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics
(ACL-07), pages 440?447, Prague.
Boser, Bernhard, Isabelle Guyon, and
Vladimir Vapnik. 1992. A training
algorithm for optimal margin classifiers.
In Proceedings of the Fifth Annual Workshop
on Computational Learning Theory,
pages 144?152, Pittsburgh, PA.
Breck, Eric, Yejin Choi, and Claire Cardie.
2007. Identifying expressions of opinion
in context. In IJCAI 2007, Proceedings of
the 20th International Joint Conference on
Artificial Intelligence, pages 2,683?2,688,
Hyderabad.
Choi, Yejin, Eric Breck, and Claire Cardie.
2006. Joint extraction of entities and
relations for opinion recognition. In
Proceedings of the 2006 Conference on
Empirical Methods in Natural Language
Processing, pages 431?439, Sydney.
Choi, Yejin and Claire Cardie. 2008.
Learning with compositional semantics
as structural inference for subsentential
sentiment analysis. In Proceedings of the
2008 Conference on Empirical Methods
in Natural Language Processing,
pages 793?801, Honolulu, HI.
Choi, Yejin and Claire Cardie. 2010.
Hierarchical sequential learning for
extracting opinions and their attributes.
In Proceedings of the 48th Annual Meeting
of the Association for Computational
Linguistics, pages 269?274, Uppsala.
Collins, Michael. 2000. Discriminative
reranking for natural language
parsing. In Proceedings of the
Seventeenth International Conference on
Machine Learning, pages 175?182,
San Francisco, CA.
Collins, Michael. 2002. Discriminative
training methods for hidden Markov
models: Theory and experiments with
perceptron algorithms. In Proceedings of the
2002 Conference on Empirical Methods in
Natural Language Processing (EMNLP 2002),
pages 1?8, Philadelphia, PA.
Crammer, Koby, Ofer Dekel, Joseph Keshet,
Shai Shalev-Schwartz, and Yoram Singer.
2006. Online passive-aggressive
algorithms. Journal of Machine Learning
Research, 2006(7):551?585.
Crammer, Koby and Yoram Singer. 2001.
On the algorithmic implementation of
multiclass kernel-based vector machines.
Journal of Machine Learning Research,
2001(2):265?585.
Fan, Rong-En, Kai-Wei Chang, Cho-Jui
Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008. LIBLINEAR: A library for large linear
classification. Journal of Machine Learning
Research, 9:1871?1874.
Freund, Yoav and Robert E. Schapire. 1999.
Large margin classification using the
perceptron algorithm. Machine Learning,
37(3):277?296.
Gerber, Matthew and Joyce Chai. 2010.
Beyond NomBank: A study of implicit
506
Johansson and Moschitti Relational Features in Fine-Grained Opinion Analysis
arguments for nominal predicates.
In Proceedings of the 48th Annual Meeting
of the Association for Computational
Linguistics, pages 1,583?1,592, Uppsala.
Greene, Stephan and Philip Resnik.
2009. More than words: Syntactic
packaging and implicit sentiment.
In Proceedings of Human Language
Technologies: The 2009 Annual Conference
of the North American Chapter of the
Association for Computational Linguistics,
pages 503?511, Boulder, CO.
Hjorth, J. S. Urban. 1993. Computer Intensive
Statistical Methods. Chapman and Hall,
London.
Hu, Minqing and Bing Liu. 2004a. Mining
and summarizing customer reviews.
In Proceedings of the ACM SIGKDD
International Conference on Knowledge
Discovery and Data Mining (KDD-04),
pages 168?177, Seattle, WA.
Hu, Minqing and Bing Liu. 2004b. Mining
opinion features in customer reviews.
In Proceedings of the Nineteeth National
Conference on Artificial Intellgience
(AAAI-2004), pages 755?760,
San Jose, CA.
Huang, Liang. 2008. Forest reranking:
Discriminative parsing with non-local
features. In Proceedings of ACL-08: HLT,
pages 586?594, Columbus, OH.
Jijkoun, Valentin, Maarten de Rijke, and
Wouter Weerkamp. 2010. Generating
focused topic-specific sentiment lexicons.
In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics,
pages 585?594, Uppsala.
Joachims, Thorsten. 2002. Learning to Classify
Text using Support Vector Machines.
Kluwer/Springer, Boston.
Joachims, Thorsten, Thomas Finley, and
Chun-Nam Yu. 2009. Cutting-plane
training of structural SVMs. Machine
Learning, 77(1):27?59.
Johansson, Richard and Pierre Nugues. 2008.
Dependency-based syntactic?semantic
analysis with PropBank and NomBank.
In CoNLL 2008: Proceedings of the Twelfth
Conference on Natural Language Learning,
pages 183?187, Manchester.
Joshi, Mahesh and Carolyn Penstein-Rose?.
2009. Generalizing dependency features
for opinion mining. In Proceedings of the
ACL-IJCNLP 2009 Conference Short Papers,
pages 313?316, Singapore.
Karlgren, Jussi, Gunnar Eriksson, Magnus
Sahlgren, and Oscar Ta?ckstro?m. 2010.
Between bags and trees?Constructional
patterns in text used for attitude
identification. In Proceedings of ECIR 2010,
32nd European Conference on Information
Retrieval, pages 38?49, Milton Keynes.
Kim, Soo-Min and Eduard Hovy. 2006.
Extracting opinions, opinion holders, and
topics expressed in online news media
text. In Proceedings of the Workshop on
Sentiment and Subjectivity in Text,
pages 1?8, Sydney.
Kobayashi, Nozomi, Kentaro Inui, and
Yuji Matsumoto. 2007. Extracting
aspect-evaluation and aspect-of relations
in opinion mining. In Proceedings of the
2007 Joint Conference on Empirical Methods
in Natural Language Processing and
Computational Natural Language Learning
(EMNLP-CoNLL), pages 1,065?1,074,
Prague.
Mel?c?uk, Igor A. 1988. Dependency Syntax:
Theory and Practice. State University Press
of New York, Albany.
Meyers, Adam, Ruth Reeves, Catherine
Macleod, Rachel Szekely, Veronika
Zielinska, Brian Young, and Ralph
Grishman. 2004. The NomBank project:
An interim report. In HLT-NAACL 2004
Workshop: Frontiers in Corpus Annotation,
pages 24?31, Boston, MA.
Moschitti, Alessandro and Roberto Basili.
2004. Complex linguistic features for
text classification: A comprehensive
study. In Proceedings of the 26th European
Conference on Information Retrieval
Research (ECIR 2004), pages 181?196,
Sunderland.
Palmer, Martha, Dan Gildea, and Paul
Kingsbury. 2005. The proposition bank:
An annotated corpus of semantic roles.
Computational Linguistics, 31(1):71?105.
Pang, Bo and Lillian Lee. 2004. A sentimental
education: Sentiment analysis using
subjectivity summarization based on
minimum cuts. In Proceedings of the 42nd
Meeting of the Association for Computational
Linguistics (ACL?04), Main Volume,
pages 271?278, Barcelona.
Pang, Bo and Lillian Lee. 2008. Opinion
mining and sentiment analysis.
Foundations and Trends in Information
Retrieval, 2(1?2):1?135.
Pang, Bo, Lillian Lee, and Shivakumar
Vaithyanathan. 2002. Thumbs up?
Sentiment classification using machine
learning techniques. In Proceedings of the
2002 Conference on Empirical Methods in
Natural Language Processing, pages 79?86,
Philadelphia, PA.
Popescu, Ana-Maria and Oren Etzioni. 2005.
Extracting product features and opinions
507
Computational Linguistics Volume 39, Number 3
from reviews. In Proceedings of Human
Language Technology Conference and
Conference on Empirical Methods in Natural
Language Processing, pages 339?346,
Vancouver.
Prasad, Rashmi, Nikhil Dinesh, Alan Lee,
Eleni Miltsakaki, Livio Robaldo,
Aravind Joshi, and Bonnie Webber.
2008. The Penn Discourse Treebank 2.0.
In Proceedings of the 6th International
Conference on Languages Resources
and Evaluations (LREC 2008),
pages 2,961?2,968, Marrakech.
Quirk, Randolph, Sidney Greenbaum,
Geoffrey Leech, and Jan Svartvik. 1985.
A Comprehensive Grammar of the English
Language. Longman, New York.
Ruppenhofer, Josef, Swapna Somasundaran,
and Janyce Wiebe. 2008. Finding the
sources and targets of subjective
expressions. In Proceedings of the Sixth
International Language Resources and
Evaluation (LREC?08), pages 2,781?2,788,
Marrakech.
Schwartz, Richard and Steve Austin. 1991.
A comparison of several approximate
algorithms for finding multiple (n-best)
sentence hypotheses. In Proceedings of the
IEEE International Conference on Acoustics,
Speech, and Signal Processing (ICASSP),
pages 701?704, Toronto.
Smith, David and Jason Eisner. 2008.
Dependency parsing by belief
propagation. In Proceedings of the
Conference on Empirical Methods in
Natural Language Processing (EMNLP),
pages 145?156, Honolulu, HI.
Somasundaran, Swapna, Galileo Namata,
Janyce Wiebe, and Lise Getoor. 2009.
Supervised and unsupervised methods
in employing discourse relations for
improving opinion polarity classification.
In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language
Processing, pages 170?179, Singapore.
Stoyanov, Veselin and Claire Cardie.
2006. Partially supervised coreference
resolution for opinion summarization
through structured rule learning.
In Proceedings of the 2006 Conference
on Empirical Methods in Natural
Language Processing, pages 336?344,
Sydney.
Stoyanov, Veselin and Claire Cardie.
2008. Annotating topics of opinions.
In Proceedings of the Sixth International
Language Resources and Evaluation
(LREC?08), pages 3,213?3,217,
Marrakech.
Surdeanu, Mihai, Richard Johansson,
Adam Meyers, Llu??s Ma`rquez, and
Joakim Nivre. 2008. The CoNLL-2008
shared task on joint parsing of
syntactic and semantic dependencies.
In Proceedings of CoNLL 2008,
pages 159?177, Manchester.
Taskar, Ben, Carlos Guestrin, and Daphne
Koller. 2004. Max-margin Markov
networks. In Advances in Neural Information
Processing Systems 16, pages 25?32,
Cambridge, MA, MIT Press.
Titov, Ivan and Ryan McDonald. 2008.
A joint model of text and aspect
ratings for sentiment summarization.
In Proceedings of ACL-08: HLT,
pages 308?316, Columbus, OH.
Tjong Kim Sang, Erik F., and Jorn Veenstra.
1999. Representing text chunks.
In Proceedings of the Ninth Conference
of the European Chapter of the Association
for Computational Linguistics,
pages 173?179, Bergen.
Tsochantaridis, Ioannis, Thorsten Joachims,
Thomas Hofmann, and Yasemin Altun.
2005. Large margin methods for
structured and interdependent
output variables. Journal of Machine
Learning Research, 6(Sep):1453?1484.
Wiebe, Janyce, Rebecca Bruce, and Thomas
O?Hara. 1999. Development and use of a
gold standard data set for subjectivity
classifications. In Proceedings of the 37th
Annual Meeting of the Association for
Computational Linguistics, pages 246?253,
College Park, MD.
Wiebe, Janyce, Theresa Wilson, and Claire
Cardie. 2005. Annotating expressions of
opinions and emotions in language.
Language Resources and Evaluation,
39(2-3):165?210.
Wiegand, Michael and Dietrich Klakow.
2010. Convolution kernels for opinion
holder extraction. In Human Language
Technologies: The 2010 Annual Conference
of the North American Chapter of the
Association for Computational Linguistics,
pages 795?803, Los Angeles, CA.
Wilson, Theresa, Janyce Wiebe, and
Paul Hoffmann. 2005. Recognizing
contextual polarity in phrase-level
sentiment analysis. In Proceedings of
Human Language Technology Conference
and Conference on Empirical Methods
in Natural Language Processing,
pages 347?354, Vancouver.
Wilson, Theresa, Janyce Wiebe, and Paul
Hoffmann. 2009. Recognizing contextual
polarity: An exploration of features
508
Johansson and Moschitti Relational Features in Fine-Grained Opinion Analysis
for phrase-level sentiment analysis.
Computational Linguistics, 35(3):399?433.
Wu, Yuanbin, Qi Zhang, Xuangjing
Huang, and Lide Wu. 2009. Phrase
dependency parsing for opinion
mining. In Proceedings of the 2009
Conference on Empirical Methods in
Natural Language Processing,
pages 1,533?1,541, Singapore.
Yu, Hong and Vasileios Hatzivassiloglou.
2003. Towards answering opinion
questions: Separating facts from opinions
and identifying the polarity of opinion
sentences. In Proceedings of the Conference
on Empirical Methods in Natural Language
Processing (EMNLP-2003), pages 129?136,
Sapporo.
Zirn, Ca?cilia, Mathias Niepert, Heiner
Stuckenschmidt, and Michael Strube. 2011.
Fine-grained sentiment analysis with
structural features. In Proceedings of 5th
International Joint Conference on Natural
Language Processing, pages 336?344,
Chiang Mai.
509

Proceedings of NAACL-HLT 2013, pages 127?137,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Training Parsers on Incompatible Treebanks
Richard Johansson
Spra?kbanken, Department of Swedish, University of Gothenburg
Box 200, SE-40530 Gothenburg, Sweden
richard.johansson@gu.se
Abstract
We consider the problem of training a sta-
tistical parser in the situation when there are
multiple treebanks available, and these tree-
banks are annotated according to different lin-
guistic conventions. To address this problem,
we present two simple adaptation methods:
the first method is based on the idea of using
a shared feature representation when parsing
multiple treebanks, and the second method on
guided parsing where the output of one parser
provides features for a second one.
To evaluate and analyze the adaptation meth-
ods, we train parsers on treebank pairs in four
languages: German, Swedish, Italian, and En-
glish. We see significant improvements for
all eight treebanks when training on the full
training sets. However, the clearest benefits
are seen when we consider smaller training
sets. Our experiments were carried out with
unlabeled dependency parsers, but the meth-
ods can easily be generalized to other feature-
based parsers.
1 Introduction
When developing a data-driven syntactic parser, we
need to fit the parameters of its statistical model on
a collection of syntactically annotated sentences ? a
treebank. Generally speaking, a larger collection of
examples in the training treebank will give a higher
quality of the resulting parser, but the cost in time
and effort of annotating training sentences is fairly
high. Most existing treebanks are in the range of a
few thousand sentences.
However, there is an abundance of theoretical
models of syntax and there is no consensus on how
treebanks should be annotated. For some languages,
there exist multiple treebanks annotated according
to different syntactic theories. Apart from German,
Swedish, and Italian, which will be considered in
this paper, there are important examples among the
world?s major languages, such as Arabic and Chi-
nese.
To exemplify how syntactic annotation conven-
tions may differ in even such a simple case as un-
labeled dependency annotation, consider the Italian
sentence fragment la sospensione o l?interruzione
(?the suspension or the interruption?) in Figure 1. As
we will see in detail in ?3.1.3, there are two Ital-
ian treebanks: the ISST and TUT. If annotating as
in the ISST treebank (drawn above the sentence)
determiners (la, l?) are annotated as dependents of
the following nouns (sospensione, interruzione); in
TUT (drawn below the sentence), we have the re-
verse situation. There are also differences in how
coordinate structures are represented: in ISST, the
two conjuncts are directly conjoined and the con-
junction attached to the first of them, while in TUT
the conjunction acts as a link between the conjuncts.
osospensionela interruzionel?
Figure 1: Differences in dependency annotation styles.
Given the high cost of treebank annotation and the
importance of a proper amount of data for parser de-
velopment, this situation is frustrating. How could
we then make use of multiple treebanks when train-
ing a parser? A na??ve way would be simply to con-
catenate them, but as we will see this results in a
parser that performs badly on all the treebanks.
In this paper, we investigate two simple adapta-
tion methods to bridge the gap between differing
127
syntactic annotation styles, allowing us to use more
data for parser training. The first approach treats
the problem of parsing with multiple syntactic an-
notation styles as a multiview learning problem and
addresses it by using feature representation that is
partly shared between the views. In the second one
we use a parser trained on one treebank to guide a
new parser trained on another treebank. We evaluate
these methods as well as their combination on four
languages: German, Swedish, Italian, and English.
In all four languages, we see a similar picture: the
shared features approach is generally better when
one of the treebanks is very small, while the guided
parsing approach is better when the treebanks are
more similar in size. However, for most training
set sizes the combination of the the two methods
achieves a higher performance than either of them
individually.
2 Methods for Training Parsers on
Multiple Treebanks
We now describe the two adaptation methods to
leverage multiple treebanks for parser training. For
clarity of presentation, we assume that there are
two treebanks, although we can easily generalize to
more. We use a common graph-based parsing tech-
nique (Carreras, 2007); the approaches described
here could be used in transition-based parsing as
well.
In a graph-based parser, for a given sentence x
the task of finding the top-scoring parse y? is stated
as an optimization problem of maximizing a linear
objective function:
y? = argmax
y
w ? f(x, y).
Here w is a weight vector produced by some learn-
ing algorithm and f(x, y) a feature representation
that maps the sentence x with a parse tree y to
a high-dimensional vector; the adaptation methods
presented in this work is implemented as modifica-
tions of the feature representation function f . Since
the search space is too large to be enumerated, the
maximization must be handled carefully, and how
this is done determines the expressivity of the fea-
ture representation f . In the parser by Carreras
(2007) the maximization is carried out by a dynamic
programming procedure relying on crucial indepen-
dence assumptions to break down the search space
into tractable parts. The factorization used in this
approach allows f to express features extracted not
only from single edges, as McDonald et al (2005),
but also from sibling and grandchild edges.
To understand the machine learning problem of
training parsers on incompatible treebanks, we com-
pare it to the related problem of domain adapta-
tion: training a system for a target domain, using
a large collection of training data from a source do-
main combined with a small labeled or large unla-
beled set from the target domain. Some algorithms
for domain adaptation rely on the assumption that
the differences between source and target distribu-
tions Ps and Pt can be explained in terms of a co-
variate shift: Ps(y|x) = Pt(y|x) for all x, y, but
Ps(x) 6= Pt(x) for some x. In our case, we have the
reverse situation: the input distribution is at least in
theory unchanged between the two treebanks, while
the input?output relation (i.e. the treebank annota-
tion style) is different. However, domain adaptation
and cross-treebank training can be seen as instances
of the more general problem of multitask learning
(Caruana, 1997). Indeed, one of the simplest and
most well-known approaches to domain adaptation
(Daume? III, 2007), which will also be considered in
this paper, should more correctly be seen as a trick
to handle multitask learning with any machine learn-
ing algorithm. On the other hand, there is no point
in trying to use domain adaptation methods assum-
ing a covariate shift, e.g. instance weighting, or any
method in which the target data is unlabeled (Blitzer
et al, 2007; Ben-David et al, 2010).
2.1 Sharing Feature Representations
Our first adaptation method relies on the intuition
that some properties of two treebanks are shared,
while others are unique to each of them. For in-
stance, as we have seen in Figure 1 the two Ital-
ian treebanks annotate coordination differently; on
the other hand, these treebanks also annotate sev-
eral other linguistic phenomena in the same way.
This observation can then be used to devise a model
where we train two parsers at the same time and use
a feature representation that is partly shared between
the two models, allowing the machine learning algo-
rithm to automatically determine which properties
128
of the two datasets are common and which are dif-
ferent. The idea of using features that are shared be-
tween the source and target training sets is a slight
generalization of a well-known method for super-
vised domain adaptation (Daume? III, 2007).
In practice, this is implemented as follows. As-
sume that originally a sentence x with a parse tree y
was represented as f1(x, y) if it came from the first
treebank, and f2(x, y) if from the second treebank.
We then add a shared feature representation fs to f1
and f2, and embed them into a single feature space.
The resulting feature vectors then become
f1(x, y) ? 02 ? fs(x, y) (1)
for a sentence from the first treebank, and
01 ? f2(x, y) ? fs(x, y) (2)
for the second treebank. Here, 01 means an all-zero
vector with the dimensionality of the feature space
of f1, and ? is vector concatenation. Using this new
representation, the two datasets are combined and a
single model trained. The hope is then that the learn-
ing algorithm will store the information about the re-
spective particularities in the weights for f1 and f2,
and about the commonalities in the weights for fs.
The result of this process is a symmetric parser that
can handle both treebank formats: when we parse
a sentence at test time, we just use the representa-
tion (1) if we want an output according to the first
treebank and (2) for the second treebank.
In this work, f1, f2, and fs are identical: all of
them correspond to the feature set described by Car-
reras (2007). However, it is certainly imaginable
that fs could consist of specially tailored features
that make generalization easier. In particular, using
a generalized fs would allow us to use this approach
in more complex cases than considered here, for in-
stance if the dependencies would be labeled with
two different sets of grammatical function labels, or
if one of the treebanks would use constituents rather
than dependencies.
2.2 Using One Parser to Guide Another
The second method is inspired by work in parser
combination, an idea that has been applied success-
fully several times and relies on the fact that dif-
ferent parsing methods have different strengths and
weaknesses (McDonald and Nivre, 2007), so that
combining them may result in a better overall pars-
ing accuracy. There are several ways to combine
parsers; one of the simplest and most successful
methods of parsing combination uses one parser as
a guide for a second parser. This is normally im-
plemented as a pipeline where the second parser ex-
tracts features based on the output of the first parser.
Nivre and McDonald (2008) used this approach
for combining a graph-based and a transition-based
parser and achieved excellent results on test sets for
several languages, and similar ideas were proposed
by Martins et al (2008).
We added guide features to the parser feature rep-
resentation. However, the features by Nivre and
McDonald (2008) are slightly too simple since they
only describe whether two words are directly con-
nected or not. That makes sense if the two parsers
are trying to predict the same type of representation,
but will not help us if there are systematic annota-
tion differences between the two treebanks, for in-
stance in whether to annotate a function word or a
lexical word as the head. Instead, following work
in semantic role labeling and similar areas, we use a
generalized notion of syntactic relationship that we
encode by determining a path between two nodes
in a syntactic tree. We defined the function Path(x,
y) as a representation describing the steps required
to traverse the parse tree from x to y, first the steps
up from x to the common ancestor a and then down
from a to y. Since we are working with unlabeled
trees, the path can be represented as just two inte-
gers; to generalize to labeled dependency parsing,
we could have used a full path representation as
commonly used in dependency-based semantic role
labeling (Johansson and Nugues, 2008).
We added the following path-based feature tem-
plates, assuming we have a potential head h with
dependent d, a sibling dependent s and grandchild
(dependent-of-dependent) g:
? POS(h)+POS(d)+Path(h, d)
? POS(h)+POS(s)+Path(h, s)
? POS(h)+POS(d)+POS(s)+Path(h, s)
? POS(h)+POS(g)+Path(h, g)
? POS(h)+POS(d)+POS(g)+Path(h, g)
To exemplify, consider again the example la
sospensione o l?interruzione shown in Figure 1. As-
129
sume that we are parsing according to the ISST rep-
resentation (drawn above the sentence) and we con-
sider adding an edge with sospensione as head and
la as dependent, and another parser following the
TUT representation (below the sentence) has cre-
ated an edge in the opposite direction. The first
feature template above would then result in a fea-
ture NOUN+DET+(1,0), where (1,0) represents the
path relationship between the two words in the TUT
tree (one step up, no step down). Similarly, when
the ISST parser adds the coordination edge between
sospensione and interruzione, it can make use of
the information that these two nouns are indirectly
connected in the output by the TUT parser; this is
represented as a path (1,3). This is an example of
a situation where we have a systematic correspon-
dence where a single edge in one representation cor-
responds to several edges in the other.
Like the multiview approach described above, this
method is trivially adaptable to more complex situ-
ations such as labeled dependency parsers with dif-
fering label sets, or dependency/constituent parsing.
2.3 Combining Methods
The two adaptation methods are orthogonal and can
easily be combined. When trying to improve the per-
formance of a parser trained on the primary treebank
T1 by leveraging a supporting treebank T2, we then
use T2 in two different ways: first by training a guide
parser, and secondly by concatenating it to T1 using
a shared feature representation.
3 Experiments
We carried out experiments to evaluate the cross-
framework adaptation methods. The evaluations
were carried out using the official CoNLL-X eval-
uation script using the default parameters. Since our
parsers do not predict edge labels, we report unla-
beled attachment scores in all tables and plots.
3.1 Treebanks Used in the Experiments
In our experiments, we used four languages: Ger-
man, Swedish, Italian, and English. For each lan-
guage, we had two treebanks. Our approaches cur-
rently require that the treebanks use the same tok-
enization conventions, so for Italian and Swedish we
automatically retokenized the treebanks. We also
made sure that the two treebanks for one language
used the same part-of-speech tag sets, by applying
an automatic tagger when necessary.
3.1.1 German: Tiger and Tu?Ba-D/Z
For German, there are two treebanks available:
Tiger (Brants et al, 2002) and Tu?Ba-D/Z (Telljo-
hann et al, 2004). These treebanks are constituent
treebanks, but dependency versions are available:
Tu?Ba-D/Z (version 7.0) includes the dependency
version in the distribution, while for Tiger we used
the version from CoNLL-X (Buchholz and Marsi,
2006). The constituent annotation styles in the two
treebanks are radically different: Tiger uses a very
flat structure with a minimal amount of intermediate
nodes, while Tu?Ba-D/Z uses a more elaborate struc-
ture including topological field information. How-
ever, the dependency versions are actually quite sim-
ilar, at least with respect to attachment. The most
common systematic difference we observed is in the
annotation of coordination.
Both treebanks are large: for Tiger, the training
set was 31,243 sentences and the test set 7,973 sen-
tences, and for Tu?Ba-D/Z 40,000 and 11,428 sen-
tences respectively. We did not use the Tiger test set
from the CoNLL-X shared task since it is very small.
We applied the TreeTagger POS tagger (Schmid,
1994) to both treebanks, using the pre-trained Ger-
man model.
3.1.2 Swedish: Talbanken05 and Syntag
As previously noted by Nivre (2002) inter alia,
Swedish has a venerable tradition in treebanking:
there are not only one but two treebanks which must
be counted among the earliest efforts of that kind.
The oldest one is the Talbanken or MAMBA tree-
bank (Einarsson, 1976), which has later been repro-
cessed for modern use (Nilsson et al, 2005). The
original annotation is a function-tagged constituent
syntax without phrase labels, but the reprocessed re-
lease includes a version converted to dependency
syntax. The dependency treebank was used in the
CoNLL-X Shared Task (Buchholz and Marsi, 2006),
and we used that version version in this work.
The second treebank is called Syntag (Ja?rborg,
1986). Similar to Talbanken, its representation uses
function-tagged constituents but no phrase labels.
We developed a conversion to dependency trees,
which was straightforward since many constituents
130
have explicitly defined heads (Johansson, 2013).
The two treebank annotation styles have signifi-
cant differences. Most prominently, the Syntag an-
notation is fairly semantically oriented in its treat-
ment of function words such as prepositions and
subordinating conjunctions: in Talbanken, a prepo-
sition is the head of a prepositional phrase, while
in Syntag the head is the prepositional complement.
There are also some domain differences: Talbanken
consists of student essays and public information,
while Syntag consists of news text.
To make the two treebanks compatible on the to-
ken level, we retokenized Syntag ? which handles
punctuation in an idiosyncratic way ? and applied a
POS tagger trained on the Stockholm?Umea? Corpus
(Gustafson-Capkova? and Hartmann, 2006) to both
treebanks. For Talbanken, we used 7,362 sentences
for training and set aside a new test set of 3,680 sen-
tences since the CoNLL-X test set is too small for
serious experimental purposes ? only 389 sentences.
For Syntag, we split the treebank into 3,524 sen-
tences for training and 1,763 sentences for testing.
3.1.3 Italian: ISST and TUT
There are two Italian treebanks. The first is the
Italian Syntactic?Semantic Treebank or ISST (Mon-
temagni et al, 2003). Here, we used the version that
was prepared (Montemagni and Simi, 2007) for the
CoNLL-2007 Shared Task (Nivre et al, 2007).
The TUT treebank1 is a more recent effort. This
treebank is available in multiple constituent and de-
pendency formats, and we have used the CoNLL-
formatted dependency version in this work. The
representation used in TUT is inspired by the Word
Grammar theory (Hudson, 1984) and tends to be
more surface-oriented than that of ISST. For in-
stance, as pointed out above in the discussion of
Figure 1, TUT differs from ISST in its treatment of
determiner?noun constructions and coordination. It
has been noted (Bosco and Lavelli, 2010; Bosco et
al., 2010) that the TUT representation is easier to
parse than the ISST representation.
We simplified the tokenization of both treebanks.
In ISST, we split multiwords into separate tokens
and reattached clitics to nonfinite verb forms. For in-
stance, a single token a causa di was converted into
1http://www.di.unito.it/
?
tutreeb/
three tokens a, causa, di, and the three tokens trovar-
se-lo into a single token trovarselo. In TUT, we
applied the same conversions and also recomposed
preposition?article and multiple-clitic contractions
that had been split by the annotators, e.g. della,
glielo etc.2 After changing the tokenization, we ap-
plied the TreeTagger POS tagger (Schmid, 1994) to
both treebanks, using the pre-trained Italian model
with the Baroni tagset3.
After preprocessing the data, we created training
and test sets. For ISST, the training set was 2,239
and the test set 1,120 sentences, while for TUT the
training set was 1,906 and the test set 954 sentences.
3.1.4 English: Two Different Conversions of
the Penn Treebank
For English, there is no significant dependency
treebank so we followed most previous work in us-
ing dependency trees automatically derived from
constituent trees in the large Penn Treebank WSJ
corpus (Marcus et al, 1993). Due to the fact
that there is a highly parametrizable constituent-
to-dependency conversion tool available (Johansson
and Nugues, 2007), we could create two dependency
treebanks with very different annotation styles.
The first training set was created from sections
02?12 of the WSJ corpus. By default, the conversion
tool outputs a treebank using the annotation style
of the CoNLL-2008 Shared Task (Surdeanu et al,
2008); however we wanted to create a more surface-
oriented style for this treebank, so we turned on op-
tions to make wh-words heads of relative clauses,
and possessive markers heads of noun phrases. This
corpus had 20,706 sentences, and will be referred to
as WSJ Part 1 in the experimental section.
The second training treebank was built from sec-
tions 13?22. For this treebank, we inverted the
value of most options in order to get a more seman-
tically oriented treebank where content words are
connected directly. In this treebank, we also used
?Prague-style? annotation of coordination: the con-
juncts are annotated as dependents of the conjunc-
tion. This set contained 20,826 sentences, and will
2It should be noted that these conversions also make sense
from a practical NLP point of view, since a number of contrac-
tions are homonymic with other words.
3http://sslmit.unibo.it/
?
baroni/
collocazioni/itwac.tagset.txt
131
be called WSJ Part 2.
We finally applied both conversion methods to
sections 24 and 23 to create development and test
sets. The development set contained 1,346 and the
test set 2,416 sentences. We did not change the tok-
enization or part-of-speech tags of the WSJ corpora.
Here, we should note that we have a slightly more
synthetic and controlled experimental setting than
for Swedish and German: the parsers are evaluated
on the same test set, so we know that there is no
difference in test set difficulty. We also know a pri-
ori that performance differences are not due to any
significant differences in genre, since all texts come
from the same source (the Wall Street Journal) and
tend to focus on business-oriented news.
3.2 Baseline Parsing Performance
As a starting point, we trained parsers on all tree-
banks. In addition, we created a parser using a na??ve
adaptation method by combining the training sets for
each language, and training parsers on those three
sets. We then applied all three parsers for every lan-
guage on both test sets for that language. The re-
sults for German, Swedish, Italian, and English are
presented in Table 1.
Every parser performed well on the test set anno-
tated in the same annotation style as its training set.
As has been observed previously, surface-oriented
styles are easier to parse than semantically oriented
styles: The Talbanken and WSJ Part 1 parsers all
achieve much higher performance on their respec-
tive test sets than the Syntag and WSJ Part 2 parsers.
The better performance of the Talbanken parser is
also partly explainable by the fact that its training
set is more than twice as large as the Syntag training
set. Similarly for German, we see slightly higher
performance for Tu?Ba-D/Z than for Tiger.
However, as can be expected every parser per-
formed very poorly when applied to the test set us-
ing the annotation style it was not trained on. For
Swedish and English, the accuracy figures are in the
range of 50-60, while the figure are a bit less poor
for German since the two treebanks are more simi-
lar. We also see, again unsurprisingly, that the na??ve
combination baseline performs poorly in all situa-
tions: we just get a ?worst-of-both-worlds? parser
that performs badly on both test sets.
GERMAN Acc. on Tiger Acc. on TBDZ
Tiger 87.8 72.0
Tu?Ba-D/Z 71.8 89.4
Tiger+TBDZ 77.7 87.7
SWEDISH Acc. on ST Acc. on TB
Syntag 81.4 52.6
Talbanken 50.3 88.2
Syntag+Talbanken 61.8 82.7
ITALIAN Acc. on ISST Acc. on TUT
ISST 81.1 57.4
TUT 55.9 84.0
ISST+TUT 73.9 71.6
ENGLISH Acc. on WSJ 1 Acc. on WSJ 2
WSJ part 1 92.6 57.4
WSJ part 2 57.4 89.5
WSJ parts 1+2 75.3 72.1
Table 1: Baseline performance figures.
3.3 Evaluation on the Full Training Sets
We trained new parsers using the shared features and
guided parsing adaptation methods described in ?2.
Additionally, we trained parsers using both methods
at the same time; we refer to these parsers as com-
bined. Including the baseline parsers, this gave us
24 parsers to evaluate on their respective test sets.
The results for German are given in Table 2. Here,
we see that all three adaptation methods give statis-
tically significant4 improvements over the baseline
when parsing the Tiger treebank. In particular, the
combined method gives a strong 0.7-point improve-
ment, a 6% error reduction. For Tu?Ba-D/Z, the im-
provements are smaller, although still significant ex-
cept for the guided parsing method.
Method Acc. on Tiger Acc. on Tu?Ba-D/Z
Baseline 87.8 89.4
Shared 88.1 89.6
Guided 88.4 89.5
Combined 88.5 89.6
Table 2: Performance figures for the German adapted
parsers. Results that are significantly different from the
baseline performances are written in boldface.
4At the 95% level. The significance levels of differences
were computed using permutation tests.
132
Method Acc. on ST Acc. on TB
Baseline 81.4 88.2
Shared 81.3 88.3
Guided 82.5 88.4
Combined 82.5 88.5
Table 3: Performance of the Swedish adapted parsers.
For Swedish, we have a similar story: we see
stronger improvements in the weak parser. Since
the Talbanken treebank is twice as large as the Syn-
tag treebank and has a surface-oriented representa-
tion that is easier to parse, this parser is useful as
a guide for the Syntag parser: the improvements of
the guided and combined Syntag parsers are statis-
tically significant. However, it is harder to improve
the Talbanken parser, for which the baseline is much
stronger. 3 shows the results for the Swedish parsers.
Method Acc. on ISST Acc. on TUT
Baseline 81.1 84.0
Shared 81.5 84.4
Guided 81.7 84.3
Combined 81.8 84.7
Table 4: Performance of the Italian adapted parsers.
When we turn to the English corpora, the adapta-
tion methods again gave us a number of very large
improvements. The results are shown in Table 5.
The shared features and combined methods gave sta-
tistically significant improvements for the WSJ Part
1 parser, and the guided parsing method an improve-
ment that is nearly significant. However the most
dramatic change is the 1.2-point improvement of the
WSJ Part 2 parser, given by the guided parsing and
combined methods. It is possible that this result
partly can be explained by the fact that this exper-
iment is a bit cleaner: in particular, as outlined in
?3.1.4, there are no domain differences.
Method Acc. on WSJ 1 Acc. on WSJ 2
Baseline 92.6 89.5
Shared 92.8 89.5
Guided 92.8 90.7
Combined 92.9 90.7
Table 5: Performance of the English adapted parsers.
For WSJ Part 2, we analyzed the differences
between the baseline and the best adapted parser.
While there were improvements for all POS tags, the
most notable one was in the attachment of conjunc-
tions, where we got an increase from 69% to 75%
in attachment accuracy, an 18% relative error reduc-
tion. Here we saw a very clear benefit of guided
parsing: since this treebank uses ?Prague-style? co-
ordination annotation (i.e. the conjunction governs
the conjuncts), it is hard for the parser to handle va-
lencies and selectional preferences when there is a
conjunction involved. It has been noted (Nilsson et
al., 2007) that this style of annotating coordination
is hard to parse. Since the WSJ Part 1 parser uses
a coordination style that is easier to parse, the WSJ
Part 2 parser can rely on its judgment.
Although conclusions must be very tentative since
we are testing on just four languages, we can make
a few general observations.
? The largest improvements (absolute and rela-
tive) all happen in treebanks that are harder to
parse. In particular, Syntag and WSJ Part 2 are
harder to parse due to their representation, and
to some extent this may be true for Tiger as well
? its learning curve rises more slowly than for
Tu?Ba-D/Z. Of course, in some cases (in partic-
ular Syntag, but also Tiger) this may partly be
explained by the training set being smaller, but
not for WSJ Part 2. In these cases, the guided
parsing method seems to be more effective.
? The languages where the shared features
method gives significant improvement for both
treebanks are German and Italian, where we do
not have the situation that one treebank is much
larger or much easier to parse.
? The combination of the two methods gave sig-
nificant improvements in all eight cases, and
had the highest performance in six cases.
3.4 The Effect of the Training Set Size
In order to better understand the differences between
the adaptation methods, we analyzed the impact of
training set size on the improvement given by the
respective methods. Let us refer to the training tree-
bank annotated according to the same style as the
test set as the primary treebank, and the other one
as the supporting treebank. We carried out the ex-
periments in this section by varying the number of
133
10
1
10
2
10
3
10
4
10
5
Training set size (sentences)
0
10
20
30
40
50
E
r
r
o
r
 
r
e
d
u
c
t
i
o
n
 
(
p
e
r
c
e
n
t
)
Tiger error reduction
Shared
Guided
Combined
10
1
10
2
10
3
10
4
10
5
Training set size (sentences)
0
10
20
30
40
50
E
r
r
o
r
 
r
e
d
u
c
t
i
o
n
 
(
p
e
r
c
e
n
t
)
T Ba-D/Z error reduction
Shared
Guided
Combined
Figure 2: Error reduction by training set size, German.
training sentences in the primary treebank and keep-
ing the size of the supporting treebank constant.
In order to highlight the differences between the
three adaptation methods, we show error reduction
plots in Figures 2, 3, 4, and 5 for German, Swedish,
Italian, and English respectively. For each training
set size on the x axis, the plot shows the reduction
in relative error with respect to the baseline.
We note that every single one of the 24 adapted
parsers learns faster than the corresponding baseline
parser. While we saw a number of significant im-
provements in ?3.3 when using the full training sets,
the relative improvements are much stronger when
the training sets are small- and medium-sized.
These plots illustrate the different properties of
the two methods. Using a shared feature represen-
tation tends to be very effective when the primary
treebank is small: the error reductions are over 40
percent for German and over 25 percent for English.
Guided parsing works best for mid-sized sets, and
the relative effectiveness of both methods decreases
as the size of the primary treebank increases. Again,
we see that guided parsing is less effective if the
guide uses an annotation style that is hard to parse.
10
1
10
2
10
3
10
4
Training set size (sentences)
0
2
4
6
8
10
12
14
16
18
E
r
r
o
r
 
r
e
d
u
c
t
i
o
n
 
(
p
e
r
c
e
n
t
)
Syntag error reduction
Shared
Guided
Combined
10
1
10
2
10
3
10
4
Training set size (sentences)
0
5
10
15
20
25
E
r
r
o
r
 
r
e
d
u
c
t
i
o
n
 
(
p
e
r
c
e
n
t
)
Talbanken error reduction
Shared
Guided
Combined
Figure 3: Error reduction by training set size, Swedish.
10
0
10
1
10
2
10
3
10
4
Training set size (sentences)
0
2
4
6
8
10
12
14
16
18
E
r
r
o
r
 
r
e
d
u
c
t
i
o
n
 
(
p
e
r
c
e
n
t
)
ISST error reduction
Shared
Guided
Combined
10
0
10
1
10
2
10
3
10
4
Training set size (sentences)
0
5
10
15
20
25
30
E
r
r
o
r
 
r
e
d
u
c
t
i
o
n
 
(
p
e
r
c
e
n
t
)
TUT error reduction
Shared
Guided
Combined
Figure 4: Error reduction by training set size, Italian.
134
In particular, for Swedish the Syntag parser never
gives a very large improvement when guiding the
Talbanken parser, and this is also true of both Italian
parsers. To a smaller extent, this also holds for En-
glish and German: the WSJ Part 2 and Tiger parsers
are less useful as guides than their counterparts.
The combination method generally performs very
well: in all eight experiments, it outperforms the
other two for almost every training set size. Its per-
formance is very close to that of the guided parsing
method for larger training sets, when the effect of
the shared features method is less pronounced.
10
1
10
2
10
3
10
4
10
5
Training set size (sentences)
0
5
10
15
20
25
30
35
40
E
r
r
o
r
 
r
e
d
u
c
t
i
o
n
 
(
p
e
r
c
e
n
t
)
WSJ part 1 error reduction
Shared
Guided
Combined
10
1
10
2
10
3
10
4
10
5
Training set size (sentences)
0
5
10
15
20
25
30
35
40
E
r
r
o
r
 
r
e
d
u
c
t
i
o
n
 
(
p
e
r
c
e
n
t
)
WSJ part 2 error reduction
Shared
Guided
Combined
Figure 5: Error reduction by training set size, English.
4 Conclusion
We have considered the problem of training a de-
pendency parser on incompatible treebanks, and we
studied two very simple methods for addressing this
problem, the shared features and guided parsing
methods. These methods allow us to use more than
one treebank when training dependency parsers. We
evaluated the methods on eight treebanks in four
languages, and had statistically significant improve-
ments in all eight cases. In particular, for English
we saw a strong 1.2-point absolute improvement (an
11% relative error reduction) in the performance of a
semantically oriented parser when trained on the full
training set. For German, we also had very strong
results for the Tiger treebank: a 6% error reduction.
For Swedish, the parser trained on the small Syntag
treebank got a boost from a guide parser trained on
the larger Talbanken. In general, it seems to be eas-
ier to improve parsers that use representations that
are harder to parse.
For all eight treebanks, both methods achieved
large improvements for small training set sizes,
while the effect gradually diminished as the training
set size increased. The shared features method was
the most effective for very small training sets, while
guided parsing surpassed it when training sets got
larger. The combination of the twomethods was also
effective, in most cases outperforming both methods
on their own. In particular, when using the full train-
ing sets, this was the only method that had statisti-
cally significant improvements for all treebanks.
While this work used an unlabeled graph-based
dependency parser, our methods generalize naturally
to other parsing approaches, including transition-
based dependency parsing. Labeled parsing with
incompatible label sets is easy to implement in the
shared features framework by removing the label in-
formation from the shared feature representation fs,
and similar modifications of fs could be carried out
to handle more complex situations such as combined
constituent and dependency parsing. Furthermore,
the paths used by the feature extractor in the guided
parser can be extended without much effort as well.
The models presented here are very simple, and in
future work we would like to explore more com-
plex approaches such as quasi-synchronous gram-
mars (Smith and Eisner, 2009; Li et al, 2012) or au-
tomatic treebank transformation (Niu et al, 2009).
Acknowledgements
I am grateful to the anonymous reviewers, whose
feedback has helped to clarify the description of the
methods. This research was supported by University
of Gothenburg through its support of the Centre for
Language Technology and Spra?kbanken. It has been
partly funded by the Swedish Research Council un-
der grant number 2012-5738.
135
References
Shai Ben-David, John Blitzer, Koby Crammer, Alex
Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. 2010. A theory of learning from different
domains. Machine Learning, 2010(79):151?175.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, Bollywood, Boom-boxes and Blenders:
Domain adaptation for sentiment classification. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 440?447,
Prague, Czech Republic.
Cristina Bosco and Alberto Lavelli. 2010. Annota-
tion schema oriented validation for dependency pars-
ing evaluation. In Proceedings of the Ninth Workshop
on Treebanks and Linguistic Theories (TLT9), Tartu,
Estonia.
Cristina Bosco, Simonetta Montemagni, Alessandro
Mazzei, Vincenzo Lombardo, Felice Dell?Orletta,
Alessandro Lenci, LeonardoLesmo, GiuseppeAttardi,
Maria Simi, Alberto Lavelli, Johan Hall, Jens Nils-
son, and Joakim Nivre. 2010. Comparing the influ-
ence of different treebank annotations on dependency
parsing. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC?10), pages 1794?1801, Valletta, Malta.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER tree-
bank. In Proceedings of the Workshop on Treebanks
and Linguistic Theory, pages 24?41, Sozopol, Bul-
garia.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning (CoNLL-X), pages
149?164, New York City, United States.
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings the
CoNLL Shared Task, pages 957?961, Prague, Czech
Republic.
Rich Caruana. 1997. Multitask learning. Machine
Learning, 28(1):41?75.
Hal Daume? III. 2007. Frustratingly easy domain adapta-
tion. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 256?
263, Prague, Czech Republic.
Jan Einarsson. 1976. Talbankens skrift-
spra?kskonkordans. Department of Scandinavian
Languages, Lund University.
Sofia Gustafson-Capkova? and Britt Hartmann. 2006.
Manual of the Stockholm Umea? Corpus version 2.0.
Stockholm University.
Richard Hudson. 1984. Word Grammar. Blackwell.
Jerker Ja?rborg. 1986. Manual fo?r syntaggning. De-
partment of Linguistic Computation, University of
Gothenburg.
Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for En-
glish. In NODALIDA 2007 Conference Proceedings,
pages 105?112, Tartu, Estonia.
Richard Johansson and Pierre Nugues. 2008. The ef-
fect of syntactic representation on semantic role label-
ing. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (Coling 2008),
pages 393?400, Manchester, United Kingdom.
Richard Johansson. 2013. Bridging the gap between
two Swedish treebanks. Northern European Journal
of Language Technology. Submitted.
Zhenghua Li, Ting Liu, and Wanxiang Che. 2012. Ex-
ploiting multiple treebanks for parsing with quasi-
synchronous grammars. In Proceedings of the 50th
Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 675?684,
Jeju Island, Korea.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
Andre? F. T. Martins, Dipanjan Das, Noah A. Smith, and
Eric P. Xing. 2008. Stacking dependency parsers.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages 157?
166, Honolulu, United States.
Ryan McDonald and Joakim Nivre. 2007. Charac-
terizing the errors of data-driven dependency parsing
models. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 122?131, Prague, Czech Re-
public.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of 43rd Annual Meeting of the
Association for Computational Linguistics (ACL?05),
pages 91?98, Ann Arbor, United States.
Simonetta Montemagni and Maria Simi. 2007. The Ital-
ian dependency annotated corpus developed for the
CoNLL-2007 shared task. Technical report, ILC-
CNR.
Simonetta Montemagni, Francesco Barsotti, Marco Bat-
tista, Nicoletta Calzolari, Ornella Corazzari, Alessan-
dro Lenci, Antonio Zampolli, Francesca Fanciulli,
Maria Massetani, Remo Raffaelli, Roberto Basili,
Maria Teresa Pazienza, Dario Saracino, Fabio Zan-
zotto, Nadia Mana, Fabio Pianesi, and Rodolfo Del-
monte. 2003. Building the Italian Syntactic?Semantic
136
Treebank. In Anne Abeille?, editor, Building and Using
Syntactically Annotated Corpora. Kluwer, Dordrecht.
Jens Nilsson, Johan Hall, and Joakim Nivre. 2005.
MAMBA meets TIGER: Reconstructing a Swedish
treebank from antiquity. In Proceedings of NODAL-
IDA Special Session on Treebanks.
Jens Nilsson, Joakim Nivre, and Johan Hall. 2007.
Generalizing tree transformations for inductive depen-
dency parsing. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics, pages 968?975, Prague, Czech Republic.
Zheng-Yu Niu, Haifeng Wang, and Hua Wu. 2009. Ex-
ploiting heterogeneous treebanks for parsing. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 46?54, Suntec, Singapore.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proceedings of ACL-08: HLT, pages 950?958,
Columbus, United States.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on depen-
dency parsing. In Proceedings of the CoNLL Shared
Task Session of EMNLP-CoNLL 2007, pages 915?932,
Prague, Czech Republic.
Joakim Nivre. 2002. What kinds of trees grow
in Swedish soil? A comparison of four annota-
tion schemes for Swedish. In Proceedings of the
First Workshop on Treebanks and Linguistic Theories
(TLT2002), Sozopol, Bulgaria.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of Interna-
tional Conference on New Methods in Language Pro-
cessing, Manchester, United Kingdom.
David A. Smith and Jason Eisner. 2009. Parser adapta-
tion and projection with quasi-synchronous grammar
features. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 822?831, Suntec, Singapore.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and JoakimNivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. InCoNLL 2008: Proceedings of
the Twelfth Conference on Natural Language Learn-
ing, pages 159?177, Manchester, United Kingdom.
Heike Telljohann, Erhard Hinrichs, and Sandra Kbler.
2004. The Tu?ba-D/Z treebank: Annotating German
with a context-free backbone. In In Proceedings of
the Fourth International Conference on Language Re-
sources and Evaluation (LREC 2004), pages 2229?
2235.
137
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 101?106,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Extracting Opinion Expressions and Their Polarities ? Exploration of
Pipelines and Joint Models
Richard Johansson and Alessandro Moschitti
DISI, University of Trento
Via Sommarive 14, 38123 Trento (TN), Italy
{johansson, moschitti}@disi.unitn.it
Abstract
We investigate systems that identify opinion
expressions and assigns polarities to the ex-
tracted expressions. In particular, we demon-
strate the benefit of integrating opinion ex-
traction and polarity classification into a joint
model using features reflecting the global po-
larity structure. The model is trained using
large-margin structured prediction methods.
The system is evaluated on the MPQA opinion
corpus, where we compare it to the only previ-
ously published end-to-end system for opinion
expression extraction and polarity classifica-
tion. The results show an improvement of be-
tween 10 and 15 absolute points in F-measure.
1 Introduction
Automatic systems for the analysis of opinions ex-
pressed in text on the web have been studied exten-
sively. Initially, this was formulated as a coarse-
grained task ? locating opinionated documents ?
and tackled using methods derived from standard re-
trieval or categorization. However, in recent years
there has been a shift towards a more detailed task:
not only finding the text expressing the opinion, but
also analysing it: who holds the opinion and to what
is addressed; it is positive or negative (polarity);
what its intensity is. This more complex formula-
tion leads us deep into NLP territory; the methods
employed here have been inspired by information
extraction and semantic role labeling, combinatorial
optimization and structured machine learning.
A crucial step in the automatic analysis of opinion
is to mark up the opinion expressions: the pieces of
text allowing us to infer that someone has a partic-
ular feeling about some topic. Then, opinions can
be assigned a polarity describing whether the feel-
ing is positive, neutral or negative. These two tasks
have generally been tackled in isolation. Breck et al
(2007) introduced a sequence model to extract opin-
ions and we took this one step further by adding a
reranker on top of the sequence labeler to take the
global sentence structure into account in (Johansson
and Moschitti, 2010b); later we also added holder
extraction (Johansson and Moschitti, 2010a). For
the task of classifiying the polarity of a given expres-
sion, there has been fairly extensive work on suitable
classification features (Wilson et al, 2009).
While the tasks of expression detection and polar-
ity classification have mostly been studied in isola-
tion, Choi and Cardie (2010) developed a sequence
labeler that simultaneously extracted opinion ex-
pressions and assigned polarities. This is so far
the only published result on joint opinion segmenta-
tion and polarity classification. However, their ex-
periment lacked the obvious baseline: a standard
pipeline consisting of an expression identifier fol-
lowed by a polarity classifier.
In addition, while theirs is the first end-to-end sys-
tem for expression extraction with polarities, it is
still a sequence labeler, which, by construction, is
restricted to use simple local features. In contrast, in
(Johansson and Moschitti, 2010b), we showed that
global structure matters: opinions interact to a large
extent, and we can learn about their interactions on
the opinion level by means of their interactions on
the syntactic and semantic levels. It is intuitive that
this should also be valid when polarities enter the
101
picture ? this was also noted by Choi and Cardie
(2008). Evaluative adjectives referring to the same
evaluee may cluster together in the same clause or
be dominated by a verb of categorization; opinions
with opposite polarities may be conjoined through a
contrastive discourse connective such as but.
In this paper, we first implement two strong base-
lines consisting of pipelines of opinion expression
segmentation and polarity labeling and compare
them to the joint opinion extractor and polarity clas-
sifier by Choi and Cardie (2010). Secondly, we ex-
tend the global structure approach and add features
reflecting the polarity structure of the sentence. Our
systems were superior by between 8 and 14 absolute
F-measure points.
2 The MPQA Opinion Corpus
Our system was developed using version 2.0 of the
MPQA corpus (Wiebe et al, 2005). The central
building block in the MPQA annotation is the opin-
ion expression. Opinion expressions belong to two
categories: Direct subjective expressions (DSEs)
are explicit mentions of opinion whereas expressive
subjective elements (ESEs) signal the attitude of the
speaker by the choice of words. Opinions have two
features: polarity and intensity, and most expres-
sions are also associated with a holder, also called
source. In this work, we only consider polarities,
not intensities or holders. The polarity takes the val-
ues POSITIVE, NEUTRAL, NEGATIVE, and BOTH;
for compatibility with Choi and Cardie (2010), we
mapped BOTH to NEUTRAL.
3 The Baselines
In order to test our hypothesis against strong base-
lines, we developed two pipeline systems. The first
part of each pipeline extracts opinion expressions,
and this is followed by a multiclass classifier assign-
ing a polarity to a given opinion expression, similar
to that described by Wilson et al (2009).
The first of the two baselines extracts opinion ex-
pressions using a sequence labeler similar to that by
Breck et al (2007) and Choi et al (2006). Sequence
labeling techniques such as HMMs and CRFs are
widely used for segmentation problems such as
named entity recognition and noun chunk extraction.
We trained a first-order labeler with the discrimi-
native training method by Collins (2002) and used
common features: words, POS, lemmas in a sliding
window. In addition, we used subjectivity clues ex-
tracted from the lexicon by Wilson et al (2005).
For the second baseline, we added our opinion ex-
pression reranker (Johansson and Moschitti, 2010b)
on top of the expression sequence labeler.
Given an expression, we use a classifier to assign
a polarity value: positive, neutral, or negative. We
trained linear support vector machines to carry out
this classification. The problem of polarity classi-
fication has been studied in detail by Wilson et al
(2009), who used a set of carefully devised linguis-
tic features. Our classifier is simpler and is based
on fairly shallow features: words, POS, subjectivity
clues, and bigrams inside and around the expression.
4 The Joint Model
We formulate the opinion extraction task as a struc-
tured prediction problem y? = arg maxy w ??(x, y).
where w is a weight vector and ? a feature extractor
representing a sentence x and a set y of polarity-
labeled opinions. This is a high-level formulation ?
we still need an inference procedure for the arg max
and a learner to estimate w on a training set.
4.1 Approximate Inference
Since there is a combinatorial number of ways to
segment a sentence and label the segments with po-
larities, the tractability of the arg max operation will
obviously depend on whether we can factorize the
problem for a particular ?.
Choi and Cardie (2010) used a Markov factor-
ization and could thus apply standard sequence la-
beling with a Viterbi arg max. However, in (Jo-
hansson and Moschitti, 2010b), we showed that a
large improvement can be achieved if relations be-
tween possible expressions are considered; these re-
lations can be syntactic or semantic in nature, for
instance. This representation breaks the Markov as-
sumption and the arg max becomes intractable. We
instead used a reranking approximation: a Viterbi-
based sequence tagger following Breck et al (2007)
generated a manageable hypothesis set of complete
segmentations, from which the reranking classifier
picked one hypothesis as its final output. Since the
set is small, no particular structure assumption (such
102
as Markovization) needs to be made, so the reranker
can in principle use features of arbitrary complexity.
We now adapt that approach to the problem of
joint opinion expression segmentation and polarity
classification. In that case, we not only need hy-
potheses generated by a sequence labeler, but also
the polarity labelings output by a polarity classifier.
The hypothesis generation thus proceeds as follows:
? For a given sentence, let the base sequence la-
beler generate up to ks sequences of unlabeled
opinion expressions;
? for every sequence, apply the base polarity
classifier to generate up to kp polarity labelings.
Thus, the hypothesis set size is at most ks ? kp. We
used a ks of 64 and a kp of 4 in all experiments.
To illustrate this process we give a hypothetical
example, assuming ks = kp = 2 and the sentence
The appeasement emboldened the terrorists. We
first generate the opinion expression sequence
candidates:
The [appeasement] emboldened the [terrorists]
The [appeasement] [emboldened] the [terrorists]
and in the second step we add polarity values:
The [appeasement]? emboldened the [terrorists]?
The [appeasement]? [emboldened]+ the [terrorists]?
The [appeasement]0 emboldened the [terrorists]?
The [appeasement]? [emboldened]0 the [terrorists]?
4.2 Features of the Joint Model
The features used by the joint opinion segmenter and
polarity classifier are based on pairs of opinions: ba-
sic features extracted from each expression such as
polarities and words, and relational features describ-
ing their interaction. To extract relations we used the
parser by Johansson and Nugues (2008) to annotate
sentences with dependencies and shallow semantics
in the PropBank (Palmer et al, 2005) and NomBank
(Meyers et al, 2004) frameworks.
Figure 1 shows the sentence the appeasement em-
boldened the terrorists, where appeasement and ter-
rorists are opinions with negative polarity, with de-
pendency syntax (above the text) and a predicate?
argument structure (below). The predicate em-
boldened, an instance of the PropBank frame
embolden.01, has two semantic arguments: the
Agent (A0) and the Theme (A1), realized syntacti-
cally as a subject and a direct object, respectively.
[appeasement] emboldened terroriststhe [
embolden.01
]The
NMOD SBJ OBJNMOD
A1A0
Figure 1: Syntactic and shallow semantic structure.
The model used the following novel features that
take the polarities of the expressions into account.
The examples are given with respect to the two ex-
pressions (appeasement and terrorists) in Figure 1.
Base polarity classifier score. Sum of the scores
from the polarity classifier for every opinion.
Polarity pair. For every pair of opinions in the
sentence, we add the pair of polarities: NEG-
ATIVE+NEGATIVE.
Polarity pair and syntactic path. For a pair
of opinions, we use the polarities and a
representation of the path through the syn-
tax tree between the expressions, follow-
ing standard practice from dependency-based
SRL (Johansson and Nugues, 2008): NEGA-
TIVE+SBJ?OBJ?+NEGATIVE.
Polarity pair and syntactic dominance. In addition
to the detailed syntactic path, we use a simpler
feature based on dominance, i.e. that one ex-
pression is above the other in the syntax tree. In
the example, no such feature is extracted since
neither of the expressions dominates the other.
Polarity pair and word pair. The polarity pair
concatenated with the words of the clos-
est nodes of the two expressions: NEGA-
TIVE+NEGATIVE+appeasement+terrorists.
Polarity pair and types and syntactic path. From
the opinion sequence labeler, we get the expres-
sion type as in MPQA (DSE or ESE): ESE-
NEGATIVE:+SBJ?OBJ?+ESE-NEGATIVE.
Polarity pair and semantic relation. When two
opinions are directly connected through a link
in the semantic structure, we add the role label
as a feature.
103
Polarity pair and words along syntactic path. We
follow the path between the expressions and
add a feature for every word we pass: NEG-
ATIVE:+emboldened+NEGATIVE.
We also used the features we developed in (Jo-
hansson and Moschitti, 2010b) to represent relations
between expressions without taking polarity into ac-
count.
4.3 Training the Model
To train the model ? find w ? we applied max-margin
estimation for structured outputs, a generalization of
the well-known support vector machine from binary
classification to prediction of structured objects.
Formally, for a training set T = {?xi, yi?}, where
the output space for the input xi is Yi, we state the
learning problem as a quadratic program:
minimize w ?w?
2
subject to w(?(xi, yi)? ?(xi, yij)) ? ?(yi, yij),
??xi, yi? ? T , yij ? Yi
Since real-world data tends to be noisy, we may
regularize to reduce overfitting and introduce a pa-
rameter C as in regular SVMs (Taskar et al, 2004).
The quadratic program is usually not solved directly
since the number of constraints precludes a direct
solution. Instead, an approximation is needed in
practice; we used SVMstruct (Tsochantaridis et al,
2005; Joachims et al, 2009), which finds a solu-
tion by successively finding the most violated con-
straints and adding them to a working set. The
loss ? was defined as 1 minus a weighted combi-
nation of polarity-labeled and unlabeled intersection
F-measure as described in Section 5.
5 Experiments
Opinion expression boundaries are hard to define
rigorously (Wiebe et al, 2005), so evaluations of
their quality typically use soft metrics. The MPQA
annotators used the overlap metric: an expression
is counted as correct if it overlaps with one in the
gold standard. This has also been used to evaluate
opinion extractors (Choi et al, 2006; Breck et al,
2007). However, this metric has a number of prob-
lems: 1) it is possible to ?fool? the metric by creat-
ing expressions that cover the whole sentence; 2) it
does not give higher credit to output that is ?almost
perfect? rather than ?almost incorrect?. Therefore,
in (Johansson and Moschitti, 2010b), we measured
the intersection between the system output and the
gold standard: every compared segment is assigned
a score between 0 and 1, as opposed to strict or over-
lap scoring that only assigns 0 or 1. For compatibil-
ity we present results in both metrics.
5.1 Evaluation of Segmentation with Polarity
We first compared the two baselines to the new
integrated segmentation/polarity system. Table 1
shows the performance according to the intersec-
tion metric. Our first baseline consists of an expres-
sion segmenter and a polarity classifier (ES+PC),
while in the second baseline we also add the ex-
pression reranker (ER) as we did in (Johansson and
Moschitti, 2010b). The new reranker described in
this paper is referred to as the expression/polarity
reranker (EPR). We carried out the evaluation using
the same partition of the MPQA dataset as in our
previous work (Johansson and Moschitti, 2010b),
with 541 documents in the training set and 150 in
the test set.
System P R F
ES+PC 56.5 38.4 45.7
ES+ER+PC 53.8 44.5 48.8
ES+PC+EPR 54.7 45.6 49.7
Table 1: Results with intersection metric.
The result shows that the reranking-based mod-
els give us significant boosts in recall, following
our previous results in (Johansson and Moschitti,
2010b), which also mainly improved the recall. The
precision shows a slight drop but much lower than
the recall improvement.
In addition, we see the benefit of the new reranker
with polarity interaction features. The system using
this reranker (ES+PC+EPR) outperforms the expres-
sion reranker (ES+ER+PC). The performance dif-
ferences are statistically significant according to a
permutation test: precision p < 0.02, recall and F-
measure p < 0.005.
5.2 Comparison with Previous Results
Since the results by Choi and Cardie (2010) are the
only ones that we are aware of, we carried out an
104
evaluation in their setting.1 Table 2 shows our fig-
ures (for the two baselines and the new reranker)
along with theirs, referred to as C & C (2010).
The table shows the scores for every polarity value.
For compatibility with their evaluation, we used the
overlap metric and carried out the evaluation us-
ing a 10-fold cross-validation procedure on a 400-
document subset of the MPQA corpus.
POSITIVE P R F
ES+PC 59.3 46.2 51.8
ES+ER+PC 53.1 50.9 52.0
ES+PC+EPR 58.2 49.3 53.4
C & C (2010) 67.1 31.8 43.1
NEUTRAL P R F
ES+PC 61.0 49.3 54.3
ES+ER+PC 55.1 57.7 56.4
ES+PC+EPR 60.3 55.8 58.0
C & C (2010) 66.6 31.9 43.1
NEGATIVE P R F
ES+PC 71.6 52.2 60.3
ES+ER+PC 65.4 58.2 61.6
ES+PC+EPR 67.6 59.9 63.5
C & C (2010) 76.2 40.4 52.8
Table 2: Results with overlap metric.
The C & C system shows a large precision
bias despite being optimized with respect to the
recall-promoting overlap metric. In recall and F-
measure, their system scores much lower than our
simplest baseline, which is in turn clearly outper-
formed by the stronger baseline and the polarity-
based reranker. The precision is lower than for C
& C overall, but this is offset by recall boosts for
all polarities that are much larger than the precision
drops. The polarity-based reranker (ES+PC+EPR)
soundly outperforms all other systems.
6 Conclusion
We have studied the implementation of end-to-end
systems for opinion expression extraction and po-
larity labeling. We first showed that it was easy to
1In addition to polarity, their system also assigned opinion
intensity which we do not consider here.
improve over previous results simply by combining
an opinion extractor and a polarity classifier; the im-
provements were between 7.5 and 11 points in over-
lap F-measure.
However, our most interesting result is that a joint
model of expression extraction and polarity label-
ing significantly improves over the sequential ap-
proach. This model uses features describing the in-
teraction of opinions through linguistic structures.
This precludes exact inference, but we resorted to
a reranker. The model was trained using approx-
imate max-margin learning. The final system im-
proved over the baseline by 4 points in intersection
F-measure and 7 points in recall. The improvements
over Choi and Cardie (2010) ranged between 10 and
15 in overlap F-measure and between 17 and 24 in
recall.
This is not only of practical value but also con-
firms our linguistic intuitions that surface phenom-
ena such as syntax and semantic roles are used in
encoding the rhetorical organization of the sentence,
and that we can thus extract useful information from
those structures. This would also suggest that we
should leave the surface and instead process the dis-
course structure, and this has indeed been proposed
(Somasundaran et al, 2009). However, automatic
discourse structure analysis is still in its infancy
while syntactic and shallow semantic parsing are rel-
atively mature.
Interesting future work should be devoted to ad-
dress the use of structural kernels for the proposed
reranker. This would allow to better exploit syn-
tactic and shallow semantic structures, e.g. as in
(Moschitti, 2008), also applying lexical similarity
and syntactic kernels (Bloehdorn et al, 2006; Bloe-
hdorn and Moschitti, 2007a; Bloehdorn and Mos-
chitti, 2007b; Moschitti, 2009).
Acknowledgements
The research described in this paper has received
funding from the European Community?s Sev-
enth Framework Programme (FP7/2007-2013) un-
der grant 231126: LivingKnowledge ? Facts, Opin-
ions and Bias in Time, and under grant 247758:
Trustworthy Eternal Systems via Evolving Software,
Data and Knowledge (EternalS).
105
References
Stephan Bloehdorn and Alessandro Moschitti. 2007a.
Combined syntactic and semantic kernels for text clas-
sification. In Proceedings of ECIR 2007, Rome, Italy.
Stephan Bloehdorn and Alessandro Moschitti. 2007b.
Structure and semantics for expressive text kernels. In
In Proceedings of CIKM ?07.
Stephan Bloehdorn, Roberto Basili, Marco Cammisa, and
Alessandro Moschitti. 2006. Semantic kernels for text
classification based on topological measures of feature
similarity. In Proceedings of ICDM 06, Hong Kong,
2006.
Eric Breck, Yejin Choi, and Claire Cardie. 2007. Iden-
tifying expressions of opinion in context. In IJCAI
2007, Proceedings of the 20th International Joint Con-
ference on Artificial Intelligence, pages 2683?2688,
Hyderabad, India.
Yejin Choi and Claire Cardie. 2008. Learning with com-
positional semantics as structural inference for subsen-
tential sentiment analysis. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 793?801, Honolulu, United
States.
Yejin Choi and Claire Cardie. 2010. Hierarchical se-
quential learning for extracting opinions and their at-
tributes. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
269?274, Uppsala, Sweden.
Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint
extraction of entities and relations for opinion recog-
nition. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
pages 431?439, Sydney, Australia.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the 2002 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2002), pages 1?8.
Thorsten Joachims, Thomas Finley, and Chun-Nam Yu.
2009. Cutting-plane training of structural SVMs. Ma-
chine Learning, 77(1):27?59.
Richard Johansson and Alessandro Moschitti. 2010a.
Reranking models in fine-grained opinion analysis. In
Proceedings of the 23rd International Conference of
Computational Linguistics (Coling 2010), pages 519?
527, Beijing, China.
Richard Johansson and Alessandro Moschitti. 2010b.
Syntactic and semantic structure for opinion expres-
sion detection. In Proceedings of the Fourteenth Con-
ference on Computational Natural Language Learn-
ing, pages 67?76, Uppsala, Sweden.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic?semantic analysis
with PropBank and NomBank. In CoNLL 2008:
Proceedings of the Twelfth Conference on Natural
Language Learning, pages 183?187, Manchester,
United Kingdom.
Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel
Szekely, Veronika Zielinska, Brian Young, and Ralph
Grishman. 2004. The NomBank project: An interim
report. In HLT-NAACL 2004 Workshop: Frontiers
in Corpus Annotation, pages 24?31, Boston, United
States.
Alessandro Moschitti. 2008. Kernel methods, syntax and
semantics for relational text categorization. In Pro-
ceeding of CIKM ?08, NY, USA.
Alessandro Moschitti. 2009. Syntactic and Seman-
tic Kernels for Short Text Pair Categorization. In
Proceedings of the 12th Conference of the European
Chapter of the ACL (EACL 2009), pages 576?584,
Athens, Greece, March. Association for Computa-
tional Linguistics.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The proposition bank: An annotated corpus of seman-
tic roles. Computational Linguistics, 31(1):71?105.
Swapna Somasundaran, Galileo Namata, Janyce Wiebe,
and Lise Getoor. 2009. Supervised and unsupervised
methods in employing discourse relations for improv-
ing opinion polarity classification. In Proceedings of
EMNLP 2009: conference on Empirical Methods in
Natural Language Processing.
Ben Taskar, Carlos Guestrin, and Daphne Koller. 2004.
Max-margin Markov networks. In Advances in Neu-
ral Information Processing Systems 16, Vancouver,
Canada.
Iannis Tsochantaridis, Thorsten Joachims, Thomas Hof-
mann, and Yasemin Altun. 2005. Large margin meth-
ods for structured and interdependent output variables.
Journal of Machine Learning Research, 6(Sep):1453?
1484.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation, 39(2-
3):165?210.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of Human Lan-
guage Technology Conference and Conference on Em-
pirical Methods in Natural Language Processing.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analysis.
Computational Linguistics, 35(3):399?433.
106
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 759?767,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Modeling Topic Dependencies in Hierarchical Text Categorization
Alessandro Moschitti and Qi Ju
University of Trento
38123 Povo (TN), Italy
{moschitti,qi}@disi.unitn.it
Richard Johansson
University of Gothenburg
SE-405 30 Gothenburg, Sweden
richard.johansson@gu.se
Abstract
In this paper, we encode topic dependencies
in hierarchical multi-label Text Categoriza-
tion (TC) by means of rerankers. We rep-
resent reranking hypotheses with several in-
novative kernels considering both the struc-
ture of the hierarchy and the probability of
nodes. Additionally, to better investigate the
role of category relationships, we consider two
interesting cases: (i) traditional schemes in
which node-fathers include all the documents
of their child-categories; and (ii) more gen-
eral schemes, in which children can include
documents not belonging to their fathers. The
extensive experimentation on Reuters Corpus
Volume 1 shows that our rerankers inject ef-
fective structural semantic dependencies in
multi-classifiers and significantly outperform
the state-of-the-art.
1 Introduction
Automated Text Categorization (TC) algorithms for
hierarchical taxonomies are typically based on flat
schemes, e.g., one-vs.-all, which do not take topic
relationships into account. This is due to two major
problems: (i) complexity in introducing them in the
learning algorithm and (ii) the small or no advan-
tage that they seem to provide (Rifkin and Klautau,
2004).
We speculate that the failure of using hierarchi-
cal approaches is caused by the inherent complexity
of modeling all possible topic dependencies rather
than the uselessness of such relationships. More pre-
cisely, although hierarchical multi-label classifiers
can exploit machine learning algorithms for struc-
tural output, e.g., (Tsochantaridis et al, 2005; Rie-
zler and Vasserman, 2010; Lavergne et al, 2010),
they often impose a number of simplifying restric-
tions on some category assignments. Typically, the
probability of a document d to belong to a subcate-
gory Ci of a category C is assumed to depend only
on d and C, but not on other subcategories of C,
or any other categories in the hierarchy. Indeed, the
introduction of these long-range dependencies lead
to computational intractability or more in general to
the problem of how to select an effective subset of
them. It is important to stress that (i) there is no
theory that can suggest which are the dependencies
to be included in the model and (ii) their exhaustive
explicit generation (i.e., the generation of all hierar-
chy subparts) is computationally infeasible. In this
perspective, kernel methods are a viable approach
to implicitly and easily explore feature spaces en-
coding dependencies. Unfortunately, structural ker-
nels, e.g., tree kernels, cannot be applied in struc-
tured output algorithms such as (Tsochantaridis et
al., 2005), again for the lack of a suitable theory.
In this paper, we propose to use the combination
of reranking with kernel methods as a way to han-
dle the computational and feature design issues. We
first use a basic hierarchical classifier to generate a
hypothesis set of limited size, and then apply rerank-
ing models. Since our rerankers are simple binary
classifiers of hypothesis pairs, they can encode com-
plex dependencies thanks to kernel methods. In par-
ticular, we used tree, sequence and linear kernels ap-
plied to structural and feature-vector representations
describing hierarchical dependencies.
Additionally, to better investigate the role of topi-
cal relationships, we consider two interesting cases:
(i) traditional categorization schemes in which node-
759
fathers include all the documents of their child-
categories; and (ii) more general schemes, in which
children can include documents not belonging to
their fathers. The intuition under the above setting
is that shared documents between categories create
semantic links between them. Thus, if we remove
common documents between father and children, we
reduce the dependencies that can be captured with
traditional bag-of-words representation.
We carried out experiments on two entire hierar-
chies TOPICS (103 nodes organized in 5 levels) and
INDUSTRIAL (365 nodes organized in 6 levels) of
the well-known Reuters Corpus Volume 1 (RCV1).
We first evaluate the accuracy as well as the ef-
ficiency of several reranking models. The results
show that all our rerankers consistently and signif-
icantly improve on the traditional approaches to TC
up to 10 absolute percent points. Very interestingly,
the combination of structural kernels with the lin-
ear kernel applied to vectors of category probabil-
ities further improves on reranking: such a vector
provides a more effective information than the joint
global probability of the reranking hypothesis.
In the rest of the paper, Section 2 describes the hy-
pothesis generation algorithm, Section 3 illustrates
our reranking approach based on tree kernels, Sec-
tion 4 reports on our experiments, Section 5 illus-
trates the related work and finally Section 6 derives
the conclusions.
2 Hierarchy classification hypotheses from
binary decisions
The idea of the paper is to build efficient models
for hierarchical classification using global depen-
dencies. For this purpose, we use reranking mod-
els, which encode global information. This neces-
sitates of a set of initial hypotheses, which are typ-
ically generated by local classifiers. In our study,
we used n one-vs.-all binary classifiers, associated
with the n different nodes of the hierarchy. In the
following sections, we describe a simple framework
for hypothesis generation.
2.1 Top k hypothesis generation
Given n categories, C1, . . . , Cn, we can define
p1Ci(d) and p
0
Ci(d) as the probabilities that the clas-
sifier i assigns the document d to Ci or not, respec-
tively. For example, phCi(d) can be computed from
M132 
M11 M12 M13 M14 
M143 M142 M141 
MCAT 
M131 
Figure 1: A subhierarchy of Reuters.
-M132 
M11 -M12 M13 M14 
 M143 -M142 -M141 
MCAT 
-M131 
Figure 2: A tree representing a category assignment hy-
pothesis for the subhierarchy in Fig. 1.
the SVM classification output (i.e., the example mar-
gin). Typically, a large margin corresponds to high
probability for d to be in the category whereas small
margin indicates low probability1. Let us indicate
with h = {h1, .., hn} ? {0, 1}n a classification hy-
pothesis, i.e., the set of n binary decisions for a doc-
ument d. If we assume independence between the
SVM scores, the most probable hypothesis on d is
h? = argmax
h?{0,1}n
n?
i=1
phii (d) =
(
argmax
h?{0,1}
phi (d)
)n
i=1
.
Given h?, the second best hypothesis can be ob-
tained by changing the label on the least probable
classification, i.e., associated with the index j =
argmin
i=1,..,n
ph?ii (d). By storing the probability of the
k ? 1 most probable configurations, the next k best
hypotheses can be efficiently generated.
3 Structural Kernels for Reranking
Hierarchical Classification
In this section we describe our hypothesis reranker.
The main idea is to represent the hypotheses as a
tree structure, naturally derived from the hierarchy
and then to use tree kernels to encode such a struc-
tural description in a learning algorithm. For this
purpose, we describe our hypothesis representation,
kernel methods and the kernel-based approach to
preference reranking.
3.1 Encoding hypotheses in a tree
Once hypotheses are generated, we need a represen-
tation from which the dependencies between the dif-
1We used the conversion of margin into probability provided
by LIBSVM.
760
M11 M13 M14 
 M143 
MCAT 
Figure 3: A compact representation of the hypothesis in
Fig. 2.
ferent nodes of the hierarchy can be learned. Since
we do not know in advance which are the important
dependencies and not even the scope of the interac-
tion between the different structure subparts, we rely
on automatic feature engineering via structural ker-
nels. For this paper, we consider tree-shaped hier-
archies so that tree kernels, e.g. (Collins and Duffy,
2002; Moschitti, 2006a), can be applied.
In more detail, we focus on the Reuters catego-
rization scheme. For example, Figure 1 shows a sub-
hierarchy of the Markets (MCAT) category and its
subcategories: Equity Markets (M11), Bond Mar-
kets (M12), Money Markets (M13) and Commod-
ity Markets (M14). These also have subcategories:
Interbank Markets (M131), Forex Markets (M132),
Soft Commodities (M141), Metals Trading (M142)
and Energy Markets (M143).
As the input of our reranker, we can simply use
a tree representing the hierarchy above, marking the
negative assignments of the current hypothesis in the
node labels with ?-?, e.g., -M142 means that the doc-
ument was not classified in Metals Trading. For ex-
ample, Figure 2 shows the representation of a classi-
fication hypothesis consisting in assigning the target
document to the categories MCAT, M11, M13, M14
and M143.
Another more compact representation is the hier-
archy tree from which all the nodes associated with
a negative classification decision are removed. As
only a small subset of nodes of the full hierarchy will
be positively classified the tree will be much smaller.
Figure 3 shows the compact representation of the hy-
pothesis in Fig. 2. The next sections describe how to
exploit these kinds of representations.
3.2 Structural Kernels
In kernel-based machines, both learning and classi-
fication algorithms only depend on the inner prod-
uct between instances. In several cases, this can be
efficiently and implicitly computed by kernel func-
tions by exploiting the following dual formulation:
?
i=1..l yi?i?(oi)?(o) + b = 0, where oi and o are
two objects, ? is a mapping from the objects to fea-
ture vectors ~xi and ?(oi)?(o) = K(oi, o) is a ker-
nel function implicitly defining such a mapping. In
case of structural kernels,K determines the shape of
the substructures describing the objects above. The
most general kind of kernels used in NLP are string
kernels, e.g. (Shawe-Taylor and Cristianini, 2004),
the Syntactic Tree Kernels (Collins and Duffy, 2002)
and the Partial Tree Kernels (Moschitti, 2006a).
3.2.1 String Kernels
The String Kernels (SK) that we consider count
the number of subsequences shared by two strings
of symbols, s1 and s2. Some symbols during the
matching process can be skipped. This modifies
the weight associated with the target substrings as
shown by the following SK equation:
SK(s1, s2) =
?
u???
?u(s1) ? ?u(s2) =
?
u???
?
~I1:u=s1[~I1]
?
~I2:u=s2[~I2]
?d(
~I1)+d(~I2)
where, ?? =
??
n=0 ?
n is the set of all strings, ~I1 and
~I2 are two sequences of indexes ~I = (i1, ..., i|u|),
with 1 ? i1 < ... < i|u| ? |s|, such that u = si1 ..si|u| ,
d(~I) = i|u| ? i1 + 1 (distance between the first and
last character) and ? ? [0, 1] is a decay factor.
It is worth noting that: (a) longer subsequences
receive lower weights; (b) some characters can be
omitted, i.e. gaps; (c) gaps determine a weight since
the exponent of ? is the number of characters and
gaps between the first and last character; and (c)
the complexity of the SK computation is O(mnp)
(Shawe-Taylor and Cristianini, 2004), where m and
n are the lengths of the two strings, respectively and
p is the length of the largest subsequence we want to
consider.
In our case, given a hypothesis represented as
a tree like in Figure 2, we can visit it and derive
a linearization of the tree. SK applied to such
a node sequence can derive useful dependencies
between category nodes. For example, using the
Breadth First Search on the compact representa-
tion, we get the sequence [MCAT, M11, M13,
M14, M143], which generates the subsequences,
[MCAT, M11], [MCAT, M11, M13, M14],
[M11, M13, M143], [M11, M13, M143]
and so on.
761
M11 -M12  M13 M14 
MCAT 
M11 -M12  M13 M14 
MCAT 
-M132 -M131 
-M132 -M131 
  M14 
 M143 -M142 -M141 
M11 -M12  M13 M14 
MCAT 
 M143 -M142 -M141   M13 
Figure 4: The tree fragments of the hypothesis in Fig. 2
generated by STK
M14 
-M143 -M142 -M141 -M132 
M13 
-M131 
M11 -M12  M13 M14 
MCAT 
M11 
  MCAT 
-M132 
 M13 
-M131 
M13 
MCAT 
-M131 
-M132 
  M13 M14 
-M142 -M141 M11 -M12 M13 
MCAT MCAT MCAT 
Figure 5: Some tree fragments of the hypothesis in Fig. 2
generated by PTK
3.2.2 Tree Kernels
Convolution Tree Kernels compute the number
of common substructures between two trees T1
and T2 without explicitly considering the whole
fragment space. For this purpose, let the set
F = {f1, f2, . . . , f|F|} be a tree fragment space and
?i(n) be an indicator function, equal to 1 if the
target fi is rooted at node n and equal to 0 oth-
erwise. A tree-kernel function over T1 and T2 is
TK(T1, T2) =
?
n1?NT1
?
n2?NT2
?(n1, n2), NT1
and NT2 are the sets of the T1?s and T2?s nodes,
respectively and ?(n1, n2) =
?|F|
i=1 ?i(n1)?i(n2).
The latter is equal to the number of common frag-
ments rooted in the n1 and n2 nodes. The ? func-
tion determines the richness of the kernel space and
thus different tree kernels. Hereafter, we consider
the equation to evaluate STK and PTK.2
Syntactic Tree Kernels (STK) To compute STK,
it is enough to compute ?STK(n1, n2) as follows
(recalling that since it is a syntactic tree kernels, each
node can be associated with a production rule): (i)
if the productions at n1 and n2 are different then
?STK(n1, n2) = 0; (ii) if the productions at n1
and n2 are the same, and n1 and n2 have only
leaf children then ?STK(n1, n2) = ?; and (iii) if
the productions at n1 and n2 are the same, and n1
and n2 are not pre-terminals then ?STK(n1, n2) =
?
?l(n1)
j=1 (1 + ?STK(c
j
n1 , c
j
n2)), where l(n1) is the
2To have a similarity score between 0 and 1, a normalization
in the kernel space, i.e. TK(T1,T2)?
TK(T1,T1)?TK(T2,T2)
is applied.
number of children of n1 and c
j
n is the j-th child
of the node n. Note that, since the productions
are the same, l(n1) = l(n2) and the computational
complexity of STK is O(|NT1 ||NT2 |) but the aver-
age running time tends to be linear, i.e. O(|NT1 | +
|NT2 |), for natural language syntactic trees (Mos-
chitti, 2006a; Moschitti, 2006b).
Figure 4 shows the five fragments of the hypothe-
sis in Figure 2. Such fragments satisfy the constraint
that each of their nodes includes all or none of its
children. For example, [M13 [-M131 -M132]] is an
STF, which has two non-terminal symbols, -M131
and -M132, as leaves while [M13 [-M131]] is not an
STF.
The Partial Tree Kernel (PTK) The compu-
tation of PTK is carried out by the following
?PTK function: if the labels of n1 and n2 are dif-
ferent then ?PTK(n1, n2) = 0; else ?PTK(n1, n2) =
?
(
?2 +
?
~I1,~I2,l(~I1)=l(~I2)
?d(~I1)+d(~I2)
l(~I1)?
j=1
?PTK(cn1(~I1j), cn2(~I2j))
)
where d(~I1) = ~I1l(~I1) ?
~I11 and d(~I2) = ~I2l(~I2) ?
~I21. This way, we penalize both larger trees and
child subsequences with gaps. PTK is more gen-
eral than STK as if we only consider the contribu-
tion of shared subsequences containing all children
of nodes, we implement STK. The computational
complexity of PTK isO(p?2|NT1 ||NT2 |) (Moschitti,
2006a), where p is the largest subsequence of chil-
dren that we want consider and ? is the maximal out-
degree observed in the two trees. However the aver-
age running time again tends to be linear for natural
language syntactic trees (Moschitti, 2006a).
Given a target T , PTK can generate any subset of
connected nodes of T , whose edges are in T . For
example, Fig. 5 shows the tree fragments from the
hypothesis of Fig. 2. Note that each fragment cap-
tures dependencies between different categories.
3.3 Preference reranker
When training a reranker model, the task of the ma-
chine learning algorithm is to learn to select the best
candidate from a given set of hypotheses. To use
SVMs for training a reranker, we applied Preference
Kernel Method (Shen et al, 2003). The reduction
method from ranking tasks to binary classification is
an active research area; see for instance (Balcan et
al., 2008) and (Ailon and Mohri, 2010).
762
Category
Child-free Child-full
Train Train1 Train2 TEST Train Train1 Train2 TEST
C152 837 370 467 438 837 370 467 438
GPOL 723 357 366 380 723 357 366 380
M11 604 309 205 311 604 309 205 311
.. .. .. .. .. .. .. .. ..
C31 313 163 150 179 531 274 257 284
E41 191 89 95 102 223 121 102 118
GCAT 345 177 168 173 3293 1687 1506 1600
.. .. .. .. .. .. .. .. ..
E31 11 4 7 6 32 21 11 19
M14 96 49 47 58 1175 594 581 604
G15 5 4 1 0 290 137 153 146
Total: 103 10,000 5,000 5,000 5,000 10,000 5,000 5,000 5,000
Table 1: Instance distributions of RCV1: the most populated categories are on the top, the medium sized ones follow
and the smallest ones are at the bottom. There are some difference between child-free and child-full setting since for
the former, from each node, we removed all the documents in its children.
In the Preference Kernel approach, the reranking
problem ? learning to pick the correct candidate h1
from a candidate set {h1, . . . , hk} ? is reduced to a
binary classification problem by creating pairs: pos-
itive training instances ?h1, h2?, . . . , ?h1, hk? and
negative instances ?h2, h1?, . . . , ?hk, h1?. This train-
ing set can then be used to train a binary classifier.
At classification time, pairs are not formed (since the
correct candidate is not known); instead, the stan-
dard one-versus-all binarization method is still ap-
plied.
The kernels are then engineered to implicitly
represent the differences between the objects in
the pairs. If we have a valid kernel K over the
candidate space T , we can construct a preference
kernel PK over the space of pairs T ?T as follows:
PK(x, y) =
PK(?x1, x2?, ?y1, y2?) = K(x1, y1)+
K(x2, y2)?K(x1, y2)?K(x2, y1),
(1)
where x, y ? T ? T . It is easy to show (Shen et al,
2003) that PK is also a valid Mercer?s kernel. This
makes it possible to use kernel methods to train the
reranker.
We explore innovative kernels K to be used in
Eq. 1:
KJ = p(x1) ? p(y1) + S, where p(?) is the global
joint probability of a target hypothesis and S is
a structural kernel, i.e., SK, STK and PTK.
KP = ~x1 ? ~y1 + S, where ~x1={p(x1, j)}j?x1 ,
~y1 = {p(y1, j)}j?y1 , p(t, n) is the classifica-
tion probability of the node (category) n in the
F1 BL BOL SK STK PTK
Micro-F1 0.769 0.771 0.786 0.790 0.790
Macro-F1 0.539 0.541 0.542 0.547 0.560
Table 2: Comparison of rerankers using different kernels,
child-full setting (KJ model).
F1 BL BOL SK STK PTK
Micro-F1 0.640 0.649 0.653 0.677 0.682
Macro-F1 0.408 0.417 0.431 0.447 0.447
Table 3: Comparison of rerankers using different kernels,
child-free setting (KJ model).
tree t ? T and S is again a structural kernel,
i.e., SK, STK and PTK.
For comparative purposes, we also use for S a lin-
ear kernel over the bag-of-labels (BOL). This is
supposed to capture non-structural dependencies be-
tween the category labels.
4 Experiments
The aim of the experiments is to demonstrate that
our reranking approach can introduce semantic de-
pendencies in the hierarchical classification model,
which can improve accuracy. For this purpose, we
show that several reranking models based on tree
kernels improve the classification based on the flat
one-vs.-all approach. Then, we analyze the effi-
ciency of our models, demonstrating their applica-
bility.
4.1 Setup
We used two full hierarchies, TOPICS and INDUS-
TRY of Reuters Corpus Volume 1 (RCV1)3 TC cor-
3trec.nist.gov/data/reuters/reuters.html
763
pus. For most experiments, we randomly selected
two subsets of 10k and 5k of documents for train-
ing and testing from the total 804,414 Reuters news
from TOPICS by still using all the 103 categories
organized in 5 levels (hereafter SAM). The distri-
bution of the data instances of some of the dif-
ferent categories in such samples can be observed
in Table 1. The training set is used for learning
the binary classifiers needed to build the multiclass-
classifier (MCC). To compare with previous work
we also considered the Lewis? split (Lewis et al,
2004), which includes 23,149 news for training and
781,265 for testing.
Additionally, we carried out some experiments on
INDUSTRY data from RCV1. This contains 352,361
news assigned to 365 categories, which are orga-
nized in 6 levels. The Lewis? split for INDUSTRY in-
cludes 9,644 news for training and 342,117 for test-
ing. We used the above datasets with two different
settings: the child-free setting, where we removed
all the document belonging to the child nodes from
the parent nodes, and the normal setting which we
refer to as child-full.
To implement the baseline model, we applied the
state-of-the-art method used by (Lewis et al, 2004)
for RCV1, i.e.,: SVMs with the default parameters
(trade-off and cost factor = 1), linear kernel, normal-
ized vectors, stemmed bag-of-words representation,
log(TF + 1) ? IDF weighting scheme and stop
list4. We used the LIBSVM5 implementation, which
provides a probabilistic outcome for the classifica-
tion function. The classifiers are combined using the
one-vs.-all approach, which is also state-of-the-art
as argued in (Rifkin and Klautau, 2004). Since the
task requires us to assign multiple labels, we simply
collect the decisions of the n classifiers: this consti-
tutes our MCC baseline.
Regarding the reranker, we divided the training
set in two chunks of data: Train1 and Train2. The
binary classifiers are trained on Train1 and tested on
Train2 (and vice versa) to generate the hypotheses
on Train2 (Train1). The union of the two sets con-
stitutes the training data for the reranker. We imple-
4We have just a small difference in the number of tokens,
i.e., 51,002 vs. 47,219 but this is both not critical and rarely
achievable because of the diverse stop lists or tokenizers.
5http://www.csie.ntu.edu.tw/?cjlin/
libsvm/
0.626
0.636
0.646
0.656
0.666
0.676
2 7 12 17 22 27 32
Micr
o-F1
Training Data Size (thousands of instances)
BL (Child-free)
RR (Child-free)
FRR (Child-free)
Figure 6: Learning curves of the reranking models using
STK in terms of MicroAverage-F1, according to increas-
ing training set (child-free setting).
0.365
0.375
0.385
0.395
0.405
0.415
0.425
0.435
0.445
2 7 12 17 22 27 32
Mac
ro-F
1
Training Data Size (thousands of instances)
BL (Child-free)
RR (Child-free)
FRR (Child-free)
Figure 7: Learning curves of the reranking models using
STK in terms of MacroAverage-F1, according to increas-
ing training set (child-free setting).
mented two rerankers: RR, which use the represen-
tation of hypotheses described in Fig. 2; and FRR,
i.e., fast RR, which uses the compact representation
described in Fig. 3.
The rerankers are based on SVMs and the Prefer-
ence Kernel (PK) described in Sec. 1 built on top of
SK, STK or PTK (see Section 3.2.2). These are ap-
plied to the tree-structured hypotheses. We trained
the rerankers using SVM-light-TK6, which enables
the use of structural kernels in SVM-light (Joachims,
1999). This allows for applying kernels to pairs of
trees and combining them with vector-based kernels.
Again we use default parameters to facilitate replica-
bility and preserve generality. The rerankers always
use 8 best hypotheses.
All the performance values are provided by means
of Micro- and Macro-Average F1, evaluated on test
6disi.unitn.it/moschitti/Tree-Kernel.htm
764
Cat.
Child-free Child-full
BL KJ KP BL KJ KP
C152 0.671 0.700 0.771 0.671 0.729 0.745
GPOL 0.660 0.695 0.743 0.660 0.680 0.734
M11 0.851 0.891 0.901 0.851 0.886 0.898
.. .. .. .. .. .. ..
C31 0.225 0.311 0.446 0.356 0.421 0.526
E41 0.643 0.714 0.719 0.776 0.791 0.806
GCAT 0.896 0.908 0.917 0.908 0.916 0.926
.. .. .. .. .. .. ..
E31 0.444 0.600 0.600 0.667 0.765 0.688
M14 0.591 0.600 0.575 0.887 0.897 0.904
G15 0.250 0.222 0.250 0.823 0.806 0.826
103 cat.
Mi-F1 0.640 0.677 0.731 0.769 0.794 0.815
Ma-F1 0.408 0.447 0.507 0.539 0.567 0.590
Table 4: F1 of some binary classifiers along with the
Micro and Macro-Average F1 over all 103 categories
of RCV1, 8 hypotheses and 32k of training data for
rerankers using STK.
data over all categories (103 or 363). Additionally,
the F1 of some binary classifiers are reported.
4.2 Classification Accuracy
In the first experiments, we compared the different
kernels using the KJ combination (which exploits
the joint hypothesis probability, see Sec. 3.3) on
SAM. Tab. 2 shows that the baseline (state-of-the-
art flat model) is largely improved by all rerankers.
BOL cannot capture the same dependencies as the
structural kernels. In contrast, when we remove the
dependencies generated by shared documents be-
tween a node and its descendants (child-free setting)
BOL improves on BL. Very interestingly, TK and
PTK in this setting significantly improves on SK
suggesting that the hierarchical structure is more im-
portant than the sequential one.
To study how much data is needed for the
reranker, the figures 6 and 7 report the Micro and
Macro Average F1 of our rerankers over 103 cate-
gories, according to different sets of training data.
This time, KJ is applied to only STK. We note that
(i) a few thousands of training examples are enough
to deliver most of the RR improvement; and (ii) the
FRR produces similar results as standard RR. This is
very interesting since, as it will be shown in the next
section, the compact representation produces much
faster models.
Table 4 reports the F1 of some individual cate-
gories as well as global performance. In these exper-
iments we used STK in KJ and KP . We note that
0
50
100
150
200
250
300
350
400
450
2 12 22 32 42 52 62
Time 
(min)
Training Data Size (thousands of instances)
RR trainingTime
RR testTime
FRR trainingTime
FRR testTime
Figure 8: Training and test time of the rerankers trained
on data of increasing size.
KP highly improves on the baseline on child-free
setting by about 7.1 and 9.9 absolute percent points
in Micro-and Macro-F1, respectively. Also the im-
provement on child-full is meaningful, i.e., 4.6 per-
cent points. This is rather interesting as BOL (not
reported in the table) achieved a Micro-average of
80.4% and a Macro-average of 57.2% when used in
KP , i.e., up to 2 points below STK. This means that
the use of probability vectors and combination with
structural kernels is a very promising direction for
reranker design.
To definitely assess the benefit of our rerankers
we tested them on the Lewis? split of two different
datasets of RCV1, i.e., TOPIC and INDUSTRY. Ta-
ble 5 shows impressive results, e.g., for INDUSTRY,
the improvement is up to 5.2 percent points. We car-
ried out statistical significance tests, which certified
the significance at 99%. This was expected as the
size of the Lewis? test sets is in the order of several
hundreds thousands.
Finally, to better understand the potential of
reranking, Table 6 shows the oracle performance
with respect to the increasing number of hypothe-
ses. The outcome clearly demonstrates that there is
large margin of improvement for the rerankers.
4.3 Running Time
To study the applicability of our rerankers, we have
analyzed both the training and classification time.
Figure 8 shows the minutes required to train the dif-
ferent models as well as to classify the test set ac-
cording to data of increasing size.
It can be noted that the models using the compact
hypothesis representation are much faster than those
765
F1
Topic Industry
BL (Lewis) BL (Ours) KJ (BOL) KJ KP BL (Lewis) BL (Ours) KJ (BOL) KJ KP
Micro-F1 0.816 0.815 0.818 0.827 0.849 0.512 0.562 0.566 0.576 0.628
Macro-F1 0.567 0.566 0.571 0.590 0.615 0.263 0.289 0.243 0.314 0.341
Table 5: Comparison between rankers using STK or BOL (when indicated) with the KJ and KP schema. 32k
examples are used for training the rerankers with child-full setting.
k Micro-F1 Macro-F1
1 0.640 0.408
2 0.758 0.504
4 0.821 0.566
8 0.858 0.610
16 0.898 0.658
Table 6: Oracle performance according to the number of
hypotheses (child-free setting).
using the complete hierarchy as representation, i.e.,
up to five times in training and eight time in test-
ing. This is not surprising as, in the latter case,
each kernel evaluation requires to perform tree ker-
nel evaluation on trees of 103 nodes. When using
the compact representation the number of nodes is
upper-bounded by the maximum number of labels
per documents, i.e., 6, times the depth of the hierar-
chy, i.e., 5 (the positive classification on the leaves
is the worst case). Thus, the largest tree would con-
tain 30 nodes. However, we only have 1.82 labels
per document on average, therefore the trees have
an average size of only about 9 nodes.
5 Related Work
Tree and sequence kernels have been successfully
used in many NLP applications, e.g.: parse rerank-
ing and adaptation (Collins and Duffy, 2002; Shen
et al, 2003; Toutanova et al, 2004; Kudo et al,
2005; Titov and Henderson, 2006), chunking and
dependency parsing (Kudo and Matsumoto, 2003;
Daume? III and Marcu, 2004), named entity recog-
nition (Cumby and Roth, 2003), text categorization
(Cancedda et al, 2003; Gliozzo et al, 2005) and re-
lation extraction (Zelenko et al, 2002; Bunescu and
Mooney, 2005; Zhang et al, 2006).
To our knowledge, ours is the first work explor-
ing structural kernels for reranking hierarchical text
categorization hypotheses. Additionally, there is a
substantial lack of work exploring reranking for hi-
erarchical text categorization. The work mostly re-
lated to ours is (Rousu et al, 2006) as they directly
encoded global dependencies in a gradient descen-
dent learning approach. This kind of algorithm is
less efficient than ours so they could experiment
with only the CCAT subhierarchy of RCV1, which
only contains 34 nodes. Other relevant work such
as (McCallum et al, 1998) and (Dumais and Chen,
2000) uses a rather different datasets and a different
idea of dependencies based on feature distributions
over the linked categories. An interesting method is
SVM-struct (Tsochantaridis et al, 2005), which has
been applied to model dependencies expressed as
category label subsets of flat categorization schemes
but no solution has been attempted for hierarchical
settings. The approaches in (Finley and Joachims,
2007; Riezler and Vasserman, 2010; Lavergne et al,
2010) can surely be applied to model dependencies
in a tree, however, they need that feature templates
are specified in advance, thus the meaningful depen-
dencies must be already known. In contrast, kernel
methods allow for automatically generating all pos-
sible dependencies and reranking can efficiently en-
code them.
6 Conclusions
In this paper, we have described several models for
reranking the output of an MCC based on SVMs
and structural kernels, i.e., SK, STK and PTK.
We have proposed a simple and efficient algorithm
for hypothesis generation and their kernel-based
representations. The latter are exploited by SVMs
using preference kernels to automatically derive
features from the hypotheses. When using tree
kernels such features are tree fragments, which can
encode complex semantic dependencies between
categories. We tested our rerankers on the entire
well-known RCV1. The results show impressive
improvement on the state-of-the-art flat TC models,
i.e., 3.3 absolute percent points on the Lewis? split
(same setting) and up to 10 absolute points on
samples using child-free setting.
Acknowledgements This research is partially sup-
ported by the EC FP7/2007-2013 under the grants:
247758 (ETERNALS), 288024 (LIMOSINE) and 231126
(LIVINGKNOWLEDGE). Many thanks to the reviewers
for their valuable suggestions.
766
References
Nir Ailon and Mehryar Mohri. 2010. Preference-based
learning to rank. Machine Learning.
Maria-Florina Balcan, Nikhil Bansal, Alina Beygelzimer,
Don Coppersmith, John Langford, and Gregory B.
Sorkin. 2008. Robust reductions from ranking to clas-
sification. Machine Learning, 72(1-2):139?153.
Razvan Bunescu and Raymond Mooney. 2005. A short-
est path dependency kernel for relation extraction. In
Proceedings of HLT and EMNLP, pages 724?731,
Vancouver, British Columbia, Canada, October.
Nicola Cancedda, Eric Gaussier, Cyril Goutte, and
Jean Michel Renders. 2003. Word sequence kernels.
Journal of Machine Learning Research, 3:1059?1082.
Michael Collins and Nigel Duffy. 2002. New ranking
algorithms for parsing and tagging: Kernels over dis-
crete structures, and the voted perceptron. In Proceed-
ings of ACL?02, pages 263?270.
Chad Cumby and Dan Roth. 2003. On kernel methods
for relational learning. In Proceedings of ICML 2003.
Hal Daume? III and Daniel Marcu. 2004. Np bracketing
by maximum entropy tagging and SVM reranking. In
Proceedings of EMNLP?04.
Susan T. Dumais and Hao Chen. 2000. Hierarchical clas-
sification of web content. In Nicholas J. Belkin, Peter
Ingwersen, and Mun-Kew Leong, editors, Proceedings
of SIGIR-00, 23rd ACM International Conference on
Research and Development in Information Retrieval,
pages 256?263, Athens, GR. ACM Press, New York,
US.
T. Finley and T. Joachims. 2007. Parameter learning
for loopy markov random fields with structural support
vector machines. In ICML Workshop on Constrained
Optimization and Structured Output Spaces.
Alfio Gliozzo, Claudio Giuliano, and Carlo Strapparava.
2005. Domain kernels for word sense disambiguation.
In Proceedings of ACL?05, pages 403?410.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. Advances in Kernel Methods ? Sup-
port Vector Learning, 13.
Taku Kudo and Yuji Matsumoto. 2003. Fast methods for
kernel-based text analysis. In Proceedings of ACL?03.
Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005.
Boosting-based parse reranking with subtree features.
In Proceedings of ACL?05.
T. Lavergne, O. Cappe?, and F. Yvon. 2010. Practical very
large scale CRFs. In Proc. of ACL, pages 504?513.
D. D. Lewis, Y. Yang, T. Rose, and F. Li. 2004. Rcv1: A
new benchmark collection for text categorization re-
search. The Journal of Machine Learning Research,
(5):361?397.
Andrew McCallum, Ronald Rosenfeld, Tom M. Mitchell,
and Andrew Y. Ng. 1998. Improving text classifica-
tion by shrinkage in a hierarchy of classes. In ICML,
pages 359?367.
Alessandro Moschitti. 2006a. Efficient convolution ker-
nels for dependency and constituent syntactic trees. In
Proceedings of ECML?06.
Alessandro Moschitti. 2006b. Making tree kernels prac-
tical for natural language learning. In Proccedings of
EACL?06.
S. Riezler and A. Vasserman. 2010. Incremental feature
selection and l1 regularization for relaxed maximum-
entropy modeling. In EMNLP.
Ryan Rifkin and Aldebaro Klautau. 2004. In defense of
one-vs-all classification. J. Mach. Learn. Res., 5:101?
141, December.
Juho Rousu, Craig Saunders, Sandor Szedmak, and John
Shawe-Taylor. 2006. Kernel-based learning of hierar-
chical multilabel classification models. The Journal of
Machine Learning Research, (7):1601?1626.
John Shawe-Taylor and Nello Cristianini. 2004. Kernel
Methods for Pattern Analysis. Cambridge University
Press.
Libin Shen, Anoop Sarkar, and Aravind k. Joshi. 2003.
Using LTAG Based Features in Parse Reranking. In
Empirical Methods for Natural Language Processing
(EMNLP), pages 89?96, Sapporo, Japan.
Ivan Titov and James Henderson. 2006. Porting statisti-
cal parsers with data-defined kernels. In Proceedings
of CoNLL-X.
Kristina Toutanova, Penka Markova, and Christopher
Manning. 2004. The Leaf Path Projection View of
Parse Trees: Exploring String Kernels for HPSG Parse
Selection. In Proceedings of EMNLP 2004.
Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hof-
mann, and Yasemin Altun. 2005. Large margin
methods for structured and interdependent output vari-
ables. J. Machine Learning Reserach., 6:1453?1484,
December.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2002. Kernel methods for relation
extraction. In Proceedings of EMNLP-ACL, pages
181?201.
Min Zhang, Jie Zhang, and Jian Su. 2006. Explor-
ing Syntactic Features for Relation Extraction using a
Convolution tree kernel. In Proceedings of NAACL.
767
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 95?99,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Non-atomic Classification to Improve a Semantic Role Labeler for a
Low-resource Language
Richard Johansson
Spra?kbanken, Department of Swedish, University of Gothenburg
Box 100, SE-40530 Gothenburg, Sweden
richard.johansson@gu.se
Abstract
Semantic role classification accuracy for most
languages other than English is constrained by
the small amount of annotated data. In this pa-
per, we demonstrate how the frame-to-frame
relations described in the FrameNet ontology
can be used to improve the performance of
a FrameNet-based semantic role classifier for
Swedish, a low-resource language. In order
to make use of the FrameNet relations, we
cast the semantic role classification task as
a non-atomic label prediction task. The ex-
periments show that the cross-frame general-
ization methods lead to a 27% reduction in
the number of errors made by the classifier.
For previously unseen frames, the reduction is
even more significant: 50%.
1 Introduction
The FrameNet lexical database and annotated cor-
pora, based on the theory of semantic frames (Fill-
more et al, 2003), have allowed the implementa-
tion of automatic systems to extract semantic roles
(Gildea and Jurafsky, 2002; Johansson and Nugues,
2007; Ma`rquez et al, 2008; Das et al, 2010).
Since the original FrameNet is developed for the
English language, most research on semantic role
extraction has focused exclusively on English. How-
ever, the English FrameNet has inspired similar ef-
forts for other languages. For instance, the ongo-
ing development of a Swedish FrameNet (Borin et
al., 2010) allows us to investigate the feasibility
of using this resource in constructing an automatic
role-semantic analyzer for Swedish. However, due
to the fact that the Swedish FrameNet annotation
process is in a fairly early stage, not much anno-
tated material is available, and this limits the perfor-
mance attainable by automatic classifiers trained on
these data. In particular, the scarce amount of data
makes it very hard for the machine learning meth-
ods to discern general linguistic facts concerning the
syntactic?semantic linking patterns, such as the rela-
tion between the voice of a verb, the syntactic func-
tions of its arguments, and the semantic roles of the
arguments (Dowty, 1991).
In this paper, we show that the inter-frame rela-
tions described in the FrameNet ontology allow us
to generalize across frames. This allows the clas-
sifier to learn general linguistic facts, and it also
leads to more efficient use of the annotated data.
To allow this kind of generalization, we formulate
the semantic role selection problem as a classifica-
tion task with non-atomic labels. This cross-frame
generalization method reduces the number of errors
made by the classifier by 27%, improving the accu-
racy from 54.4 to 66.5. When evaluating on frames
for which the classifier has not been trained, the ac-
curacy improves from 7.2 (random performance) to
53.4, a 50% error reduction.
2 The Swedish FrameNet
The Swedish FrameNet, SweFN, is a lexical re-
source under development (Friberg Heppin and
Toporowska Gronostaj, 2012), based on the English
version of FrameNet constructed by the Berkeley re-
search group (Baker et al, 1998). It is found on the
SweFN website1, and is available as a free resource.
The SweFN frames and frame names correspond
to the English ones, with some exceptions, as do
the selection of frame elements including defini-
tions and internal relations. The meta-information
about the frames, such as semantic relations be-
tween frames, is also transferred from the Berkley
FrameNet. Compared to the Berkeley FrameNet,
SweFN is expanded with information about the do-
main of the frames, at present: general language, the
medical and the art domain.
1http://spraakbanken.gu.se/eng/swefn
95
At the time of writing this paper, SweFN cov-
ered 519 frames with around 18,000 lexical units.
The lexical units are gathered from SALDO, a free
Swedish electronic association lexicon (Borin and
Forsberg, 2009). A lexical unit from SALDO can-
not populate more than one frame. At present there
are 31 frames in SweFNwhich do not match a frame
in the Berkeley FrameNet. Of these, there are eight
completely new frames while the others have been
modified in some way.
Crucially for the work presented in this paper,
each frame is exemplified with at least one sentence.
The number of sentences is currently 2,974. The
most well-annotated frames are EXPERIENCER OBJ
with 38 sentences, CAUSE MOTION with 21, and
CAUSE HARM with 19. These sentences form the
training material used in the following sections.
3 System Implementation
In this section, we describe the implementation of
our semantic role labeling system. In order to be
useful on its own, such a system needs to solve sev-
eral tasks: (1) identification of predicate words; (2)
assignment of frames to predicate words; (3) iden-
tification of role fillers; (4) assignment of semantic
role labels to role fillers. In this paper, we focus ex-
clusively on the semantic role classification task.
3.1 Baseline: A Classifier for Swedish
Semantic Roles
Following most previous implementations, we used
a syntactic parse tree as the basis of the semantic
role extraction; we assumed that every semantic role
span coincides with the projection of a subtree in
the syntactic tree. The tasks of segmentation and
labeling then reduce to a classification problem on
syntactic tree nodes. Each sentence was parsed by
the LTH dependency parser (Johansson and Nugues,
2008a), which we trained on a Swedish treebank
(Nilsson et al, 2005). Figure 1 shows a sentence an-
notated with a dependency tree and semantic roles.
The semantic role labeling classifier was imple-
mented as a linear multiclass classifier with a flexi-
ble output space depending on the frame of the given
predicate; we trained this classifier using an online
learning algorithm (Crammer et al, 2006). In ad-
dition, we imposed a uniqueness constraint on the
labels output by the classifier, so that every role may
appear only once for a given predicate.
We considered a large number of features for the
classifier (Table 1). Most of these are commonly
used features taken from the standard literature on
semantic role labeling. We then applied a standard
greedy forward feature selection procedure to deter-
mine which of them to use. The features contain-
ing SALDO ID refer to the entry identifiers in the
SALDO lexicon. Note that the POS tags have coarse
and fine variants, such as VERB and VERB-FINITE-
PRESENT-ACTIVE respectively, and we used both of
them.
Semantic role classifiers rely heavily on lexical
features (Johansson and Nugues, 2008b), and this
may lead to brittleness; in order to increase robust-
ness, we added features based on hierarchical clus-
ters constructed using the Brown algorithm (Brown
et al, 1992). The Brown algorithm clusters word
into hierarchies represented as bit strings. Based on
tuning on a development set, we found that it was
best not to use the full bit string, but only a prefix if
the string was longer than 12 bits.
FRAME
DEPENDENCY RELATION PATH
FRAME ELEMENTS
POSITION
VOICE
ARGUMENT HEAD SALDO ID
ARGUMENT HEAD LEMMA
ARGUMENT HEAD POS (FINE)
PREDICATE POS (FINE)
ARGUMENT POS (COARSE)
ARGUMENT RIGHT CHILD POS (COARSE)
ARGUMENT WORD
PREDICATE WORD CLUSTER
ARGUMENT WORD CLUSTER
Table 1: List of classifier features.
3.2 A Classifier Using Non-atomic Semantic
Role Labels
The classifier described above is a quite typical ex-
ample of how semantic role classifiers are normally
implemented: each frame is independent of all other
frames. However, in our case, when the amount of
training data is quite small, the limitations of this
standard approach become apparent:
? Since there are many frames, the amount of
training data for each frame is very limited.
96
Vi promenerar so?derut fra?n Lindholmen la?ngs Norra ?Alvstrandens brokiga kontur .
SS
RA
RA PA
RA
DT DT
AT
PA
IP
SELF MOVER DIRECTION
SOURCE
PATH
SELF MOTION
Figure 1: A sentence with dependency syntax (above) and semantic role structure (below).
? Basic linguistic facts, such as which roles are
likely to appear in subject position, have to be
relearned for each frame.
To remedy these problems, we developed a classi-
fier using non-atomic labels: instead of just a simple
label INGESTION:INGESTOR, the classifier can pre-
dict several labels, using some sort of decomposition
into meaningful parts. In ?3.3, we will describe sev-
eral such decompositions.
As described above, our baseline classifier is a
standard linear classifier. Assume that the frame F
defines a set of semantic roles r1, . . . , rn, then the
classifier predicts a semantic role r? for a given ar-
gument a using this model:
r? = argmax
r?F
w ? ?(a, r)
Here ? is a feature function describing features of
the argument a taking the semantic role r, and w is a
weight vector produced by some training algorithm.
This classifier model can easily be generalized to
the non-atomic case. We then assume that each role
r can be decomposed using a decomposition func-
tion D, which returns a set of labels. We now apply
the feature function to each sub-label l instead of the
main label r.
r? = argmax
r?F
?
l?D(r)
w ? ?(a, l)
Non-atomic classification has been described in a
number of publications. It is fairly common in text
categorization, where hierarchical classification is
probably the most common type. One of the most
similar to ours is the action classifier by Roth and
Tu (2009), which handled a large label set by de-
composing the labels into meaningful parts.
3.3 Generalization Methods
We investigated several ways of analyzing the labels,
and most of them were based on the properties of
the frames, defined in the FrameNet ontology. The
Swedish FrameNet currently does not define such
properties, but since the frames and frame elements
are for the most part based on their English coun-
terparts, we used the English ontology. In case of
mismatch, we just left the label in its original state.
The first method we tried was based on frame-to-
frame relations. We used the following relations:
INHERITANCE: specific to general, e.g. COMMU-
NICATION NOISE to COMMUNICATION.
SUBFRAME: from component to complex, e.g.
SETTING OUT to TRAVEL.
CAUSATIVE-OF: causative to inchoative,
e.g. CAUSE TEMPERATURE CHANGE to
INCH. CHANGE OF TEMP..
INCHOATIVE-OF: inchoative to stative, e.g.
INCH. CHANGE OF TEMP. to TEMPERATURE.
USING: child to parent, e.g. COMMUNICA-
TION NOISE to MAKE NOISE.
PERSPECTIVE-ON: perspectivized to neutral, e.g.
RIDE VEHICLE to USE VEHICLE.
To analyze a label in terms of frame-to-frame
relations, we applied the transitive closure of
each relation and returned the resulting set. For
instance, when applying the Inheritance rela-
tion to the INGESTION:INGESTOR label, we get
the following set: { INGESTION:INGESTOR,
INGEST SUBSTANCE:INGESTOR, MANIPU-
LATION:AGENT, INTENT. AFFECT:AGENT,
INTENT. ACT:AGENT, TRANS. ACTION:AGENT }.
The second method was based on the semantic
type of the semantic role. For instance, the INGES-
TION:INGESTOR role needs to be filled by an en-
tity of the semantic type SENTIENT. The decom-
position of this role then simply becomes { INGES-
TION:INGESTOR, SENTIENT }.
The third method was based on the simple no-
tion label generalization: if two semantic roles
97
in two different frames have the same name,
then we use the same label. For instance,
we change the INGESTION:INGESTOR and IN-
GEST SUBSTANCE:INGESTOR to INGESTOR. We
normalized the spelling, punctuation, and capitaliza-
tion of the labels before generalizing.
4 Experiments
We evaluated the classifier on the example sentences
in the Swedish FrameNet. The frame and the ar-
gument were given to the classifier, which then had
to predict the semantic role. We evaluated in two
different ways: In-frame evaluation, where a 5-fold
cross-validation was carried out over the set of sen-
tences, and Out-frame evaluation, where the cross-
validation was done over the set of frames. The out-
frame setting simulates the situation where a new
frame has been defined, but no training data have
been annotated. Without any sort of cross-frame
generalization, the classification in the out-frame
setting becomes a random baseline.
Table 2 shows the results of using the frame-to-
frame relations for analyzing the semantic role la-
bels. We see that decomposition based on Inheri-
tance is by far the most effective of these, although
the highest performance is obtained when combin-
ing all types of relation-based decompositions.
Classifier In-frame Out-frame
Baseline 54.4 7.2
Inheritance 58.7 28.1
Using 55.8 20.5
Subframe 54.8 11.5
Causative-of 54.5 9.7
Perspective-on 54.5 8.1
Inchoative-of 54.4 8.0
All except Inheritance 56.0 24.0
All relations 59.6 36.9
Table 2: Classification results with generalization based
on frame-to-frame relations.
The effect of analyzing labels in terms of semantic
type is similar. The in-frame performance is higher
than that of relation-based decomposition, while the
out-frame performance is a bit lower. The two gen-
eralization methods seem to complement each other,
since we get a higher performance by combining
them. Table 3 shows the results.
Classifier In-frame Out-frame
Semantic type 61.7 31.7
Semantic type + relations 63.5 42.6
Table 3: Adding semantic type generalization.
Finally, Table 4 shows the effect of using label
generalization. This is by far the most effective
method. However, we get even higher performance
by combining it with the other two methods.
Classifier In-frame Out-frame
Label generalization 65.9 51.5
LG + ST + relations 66.5 53.4
Table 4: Results with label generalization.
5 Discussion
When developing NLP systems for a low-resource
language, it is crucial to make effective use of the
available data. In the case of FrameNet semantic
role classification, one way to improve the use of the
data is to generalize the roles across the frames. This
also makes sense from a theoretical point of view,
since predicting multiple labels allows the machine
learner to learn general facts as well as specifics.
This work builds on previous work in multi-label
classification. For the task of FrameNet semantic
role classification, the work most closely related to
ours is that by Matsubayashi et al (2009), which de-
fined a classifier making use of role groups; the ef-
fect of the role groups turns out to be similar to our
non-atomic classification approach.
Our experiments showed very significant error re-
ductions. This was especially notable in the case of
out-frame evaluation, which is to be expected since
the baseline in this case was a random selection. The
best classifier used all three types of label decom-
position, and achieved a 26% in-frame and a 50%
out-frame error reduction.
Acknowledgements
The research presented here was supported by the
Swedish Research Council (the project Swedish
Framenet++, VR dnr 2010-6013) and by the Uni-
versity of Gothenburg through its support of the
Centre for Language Technology and Spra?kbanken
(the Swedish Language Bank).
98
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceed-
ings of the 36th Annual Meeting of the Association
for Computational Linguistics and 17th International
Conference on Computational Linguistics, pages 86?
90, Montre?al, Canada.
Lars Borin and Markus Forsberg. 2009. All in the fam-
ily: A comparison of SALDO and WordNet. In Pro-
ceedings of the Nodalida 2009 Workshop on WordNets
and other Lexical Semantic Resources ? between Lexi-
cal Semantics, Lexicography, Terminology and Formal
Ontologies, Odense, Denmark.
Lars Borin, Dana Danne?lls, Markus Forsberg,
Maria Toporowska Gronostaj, and Dimitrios Kokki-
nakis. 2010. The past meets the present in the
Swedish FrameNet++. In Proceedings of EURALEX.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467?479.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Schwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 2006(7):551?585.
Dipanjan Das, Nathan Schneider, Desai Chen, and
Noah A. Smith. 2010. Probabilistic frame-semantic
parsing. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
948?956, Los Angeles, United States.
David R. Dowty. 1991. Thematic proto-roles and argu-
ment selections. Language, 67(3):574?619.
Charles Fillmore, Christopher Johnson, and Miriam
Petruck. 2003. Background to FrameNet. Interna-
tional Journal of Lexicography, 16(3):235?250.
Karin Friberg Heppin and Maria Toporowska Gronostaj.
2012. The rocky road towards a Swedish FrameNet.
In Proceedings of LREC-2012 (to appear).
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Richard Johansson and Pierre Nugues. 2007. Semantic
structure extraction using nonprojective dependency
trees. In Proceedings of SemEval-2007, pages 227?
230, Prague, Czech Republic, June 23-24.
Richard Johansson and Pierre Nugues. 2008a.
Dependency-based syntactic?semantic analysis with
PropBank and NomBank. In Proceedings of the
CoNLL Shared Task, pages 183?187, Manchester,
United Kingdom.
Richard Johansson and Pierre Nugues. 2008b. The ef-
fect of syntactic representation on semantic role label-
ing. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (Coling 2008),
pages 393?400, Manchester, United Kingdom.
Llu??s Ma`rquez, Xavier Carreras, Ken Litkowski, and
Suzanne Stevenson. 2008. Semantic role labeling:
An introduction to the special issue. Computational
Linguistics, 34(2):145?159.
Yuichiroh Matsubayashi, Naoaki Okazaki, and Jun?ichi
Tsujii. 2009. A comparative study on generaliza-
tion of semantic roles in FrameNet. In Proceedings
of the Joint Conference of the 47th Annual Meeting
of the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP, pages
19?27, Suntec, Singapore.
Jens Nilsson, Johan Hall, and Joakim Nivre. 2005.
MAMBA meets TIGER: Reconstructing a Swedish
treebank from antiquity. In Proceedings of NODAL-
IDA Special Session on Treebanks.
Dan Roth and Yuancheng Tu. 2009. Aspect guided text
categorization with unobserved labels. In Proceed-
ings of the IEEE Conference on Data Mining, Miami,
United States.
99
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 497?502,
Dublin, Ireland, August 23-24, 2014.
RTRGO: Enhancing the GU-MLT-LT System
for Sentiment Analysis of Short Messages
Tobias G?unther
Retresco GmbH
retresco.de
email@tobias.io
Jean Vancoppenolle
ferret go GmbH
ferret-go.com
jean.vcop@gmail.com
Richard Johansson
University of Gothenburg
www.svenska.gu.se
richard.johansson@gu.se
Abstract
This paper describes the enhancements
made to our GU-MLT-LT system (G?unther
and Furrer, 2013) for the SemEval-2014
re-run of the SemEval-2013 shared task on
sentiment analysis in Twitter. The changes
include the usage of a Twitter-specific to-
kenizer, additional features and sentiment
lexica, feature weighting and random sub-
space learning. The improvements result
in an increase of 4.18 F-measure points on
this year?s Twitter test set, ranking 3rd.
1 Introduction
Automatic analysis of sentiment expressed in text
is an active research area in natural language pro-
cessing with obvious commercial interest. In the
simplest formulation of the problem, sentiment
analysis is framed as a categorization problem
over documents, where the set of categories is
typically a set of polarity values, such as posi-
tive, neutral, and negative. Many approaches to
document-level sentiment classification have been
proposed. For an overview see e.g. Liu (2012).
Text in social media and in particular microblog
messages are a challenging text genre for senti-
ment classification, as they introduce additional
problems such as short text length, spelling vari-
ation, special tokens, topic variation, language
style and multilingual content. Following Pang et
al. (2002), most sentiment analysis systems have
been based on standard text categorization tech-
niques, e.g. training a classifier using some sort of
bag-of-words feature representation. This is also
true for sentiment analysis of microblogs. Among
This work is licenced under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
the first to work specifically with Twitter
1
data
were Go et al. (2009), who use emoticons as labels
for the messages. Similarly, Davidov et al. (2010),
Pak and Paroubek (2010), and Kouloumpis et al.
(2011) use this method of distant supervision to
overcome the data acquisition barrier. Barbosa
and Feng (2010) make use of three different senti-
ment detection websites to label messages and use
mostly non-lexical features to improve the robust-
ness of their classifier. Bermingham and Smeaton
(2010) investigate the impact of the shortness of
Tweets on sentiment analysis and Speriosu et al.
(2011) propagate information from seed labels
along a linked structure that includes Twitter?s
follower graph. There has also been work on
lexicon-based approaches to sentiment analysis of
microblogs, such as O?Connor et al. (2010), Thel-
wall et al. (2010) and Zhang et al. (2011). For a
detailed discussion see G?unther (2013).
In 2013, the International Workshop on Se-
mantic Evaluation (SemEval) organized a shared
task on sentiment analysis in Twitter (Nakov et
al., 2013) to enable a better comparison of dif-
ferent approaches for sentiment analysis of mi-
croblogs. The shared task consisted of two sub-
tasks: one on recognizing contextual polarity of
a given subjective expression (Task A), and one
on document-level sentiment classification (Task
B). For both tasks, the training sets consisted of
manually labeled Twitter messages, while the test
sets consisted of a Twitter part and an SMS part
in order to test domain sensitivity. Among the
best performing systems were Mohammad et al.
(2013), G?unther and Furrer (2013) and Becker et
al. (2013), who all train linear models on a vari-
ety of task-specific features. In this year the cor-
pus resources were used for a re-run of the shared
task (Rosenthal et al., 2014), introducing two new
Twitter test sets, as well as LiveJournal data.
1
A popular microblogging service on the internet, its mes-
sages are commonly referred to as ?Tweets.?
497
2 System Desciption
This section describes the details of our sentiment
analysis system, focusing on the differences to our
last year?s implementation. This year we only par-
ticipated in the subtask on whole message polarity
classification (Subtask B).
2.1 Preprocessing
For tokenization of the messages we use the
tokenizer of Owoputi et al. (2013)?s Twitter
NLP Tools
2
, which include a tokenizer and part-
of-speech tagger optimized for the usage with
Tweets. The tokenizer contains a regular expres-
sion grammar for recognizing emoticons, which is
an especially valuable property in the context of
sentiment analysis due to the high emotional ex-
pressiveness of emoticons.
It is well known that the way word tokens are
represented may have a significant impact on the
performance of a lexical classifier. This is par-
ticularly true in natural language processing of
social media, where we run into the problem of
spelling variation causing extreme lexical sparsity.
To deal with this issue we normalize the tokens
with the following technique: First, all tokens are
converted to lowercase and the hashtag sign (#) is
removed if present. If the token is not present in
an English word list or any of the used sentiment
lexica (see below), we remove all directly repeated
letters after the first repetition (e.g. greeeeaaat?
greeaat). If the resulting token is still not present
in any of the lexical resources, we allow no direct
repetition of letters at all. While this might lead
to lexical collisions in some cases (e.g. goooodd
? goodd ? god), it is an easy and efficient way
to remove some lexical sparsity. While generating
all possible combinations of deletions and check-
ing the resulting tokens against a lexical resource
is another option, a correct disambiguation of the
intended word would require a method making use
of context knowledge (e.g. goooodd? good, vs.
goooodd? god).
2.2 Features
We use the following set of features as input to our
supervised classifier:
? The normalized tokens as unigrams and bi-
grams, where stopword and punctuation to-
kens are excluded from bigrams
2
http://www.ark.cs.cmu.edu/TweetNLP
? The word stems of the normalized tokens,
reducing inflected forms of a word to a com-
mon form. The stems were computed using
the Porter stemmer algorithm (Porter, 1980)
? The IDs of the token?s word clusters.
The clusters were generated by performing
Brown clustering (Brown et al., 1992) on
56,345,753 Tweets by Owoputi et al. (2013)
and are available online.
2
? The presence of a hashtag or URL in the mes-
sage (one feature each)
? The presence of a question mark token in the
message
? We use the opinion lexicon by Bing Liu (Hu
and Liu, 2004), the MPQA subjectivity lex-
icon (Wiebe et al., 2005) and the Twitrratr
wordlist, which all provide a list of positive
and negative words, to compute a prior polar-
ity of the message. For each of the three sen-
timent lexica two features capture whether
the majority of the tokens in the message
were in the positive or negative sentiment list.
The same is done for hashtags using the NRC
hashtag sentiment lexicon (Mohammad et al.,
2013).
? We apply special handling to features in a
negation context. A token is considered as
negated if it occurs after a negation word (up
to the next punctuation). All token, stem and
word cluster features are marked with a nega-
tion prefix. Additionally, the polarity for to-
ken in a negation context is inverted when
computing the prior lexicon polarity.
? We use the part-of-speech tags computed by
the part-of-speech tagger of the Twitter NLP
tools by Owoputi et al. (2013) to exclude
certain tokens. Assuming they do not carry
any helpful sentiment information, no fea-
tures are computed for token recognized as
name (tag ?) or user mention (tag @).
? We also employ feature weighting to give
more importance to certain features and indi-
cation of emphasis by the author. Normally,
all features described above receive weight 1
if they are present and weight 0 if they are ab-
sent. For each of the following cases we add
+1 to the weight of a token?s unigram, stem
and word cluster features:
498
? The original (not normalized) token is
all uppercase
? The original token has more than three
adjacent repetitions of one letter
? The token is an adjective or emoticon
(according to its part-of-speech tag)
Furthermore, the score of each token is di-
vided in half, if the token occurs in a ques-
tion context. A token is considered to be in
a question context, if it occurs before a ques-
tion mark (up to the next punctuation).
2.3 Machine Learning Methods
All training was done using the open-source ma-
chine learning toolkit scikit-learn
3
(Pedregosa et
al., 2011). Just as in our last year?s system
we trained linear one-versus-all classifiers us-
ing stochastic gradient descent optimization with
hinge loss and elastic net regularization.
4
For fur-
ther details see G?unther and Furrer (2013). The
number of iterations was set to 1000 for the final
model and 100 for the experiments.
It is widely observed that training on a lot of
lexical features can lead to brittle NLP systems,
that are easily overfit to particular domains. In so-
cial media messages the brittleness is particularly
acute due to the wide variation in vocabulary and
style. While this problem can be eased by using
corpus-induced word representations such as the
previously introduced word cluster features, it can
also be addressed from a learning point of view.
Brittleness can be caused by the problem that very
strong features (e.g. emoticons) drown out the ef-
fect of other useful features.
The method of random subspace learning
(S?gaard and Johannsen, 2012) seeks to handle
this problem by forcing learning algorithms to pro-
duce models with more redundancy. It does this
by randomly corrupting training instances during
learning, so if some useful feature is correlated
with a strong feature, the learning algorithm has
a better chance to assign it a nonzero weight. We
implemented random subspace learning by train-
ing the classifier on a concatenation of 25 cor-
rupted copies of the training set. In a corrupted
copy, each feature was randomly disabled with a
probability of 0.2. Just as for the classifier, the hy-
perparameters were optimized empirically.
3
Version 0.13.1, http://scikit-learn.org.
4
SGDClassifier(penalty=?elasticnet?,
alpha=0.001, l1 ratio=0.85, n iter=1000,
class weight=?auto?)
3 Experiments
For the experiments and the training of the final
model we used the joined training and develop-
ment sets of subtask B. We were able to retrieve
10368 Tweets, of which we merged all samples
labeled as objective into the neutral class. This re-
sulted in a training set of 3855 positive, 4889 neu-
tral and 1624 negative tweets. The results of the
experiments were obtained by performing 10-fold
cross-validation, predicting positive, negative and
neutral class. Just as in the evaluation of the shared
task the results are reported as average F-measure
(F
1
) between positive and negative class.
To be able to evaluate the contribution of the
different features groups to the final model we per-
form an ablation study. By disabling one feature
group at the time one can easily compare the per-
formance of the model without a certain feature to
the model using the complete feature set. In Ta-
ble 1 we present the results for the feature groups
bigrams (2gr), stems (stem), word clusters (wc),
sentiment lexica (lex), negation (neg), excluding
names and user mentions (excl), feature weighting
(wei) and random subspace learning (rssl).
Negative Positive Avg.
Prec Rec Prec Rec F
1
ALL 54.80 71.67 76.70 75.41 69.08
-2gr -0.55 -0.49 -0.35 +0.20 -0.31
-stem -1.47 -1.72 -0.49 -0.03 -0.92
-wc -1.45 -1.60 -0.40 -1.66 -1.29
-lex -1.73 -5.11 +1.06 -2.75 -1.99
-neg -1.90 -3.14 -1.30 +0.36 -1.43
-excl +0.31 -0.99 +0.59 +0.08 +0.08
-wei -1.57 +0.43 -0.84 -0.34 -0.73
-rssl +2.04 -4.37 +1.38 -2.88 -0.67
Table 1: Feature ablation study
Looking at Table 1, we can see that removing
the sentiment lexica features causes the biggest
drop in performance. This is especially true for
the recall of the negative class, which is underrep-
resented in the training data and can thus profit the
most from prior domain knowledge. When com-
paring to the features of our last year?s system, it
becomes clear that the used sentiment lexica can
provide a much bigger gain in performance than
the previously used SentiWordNet. Even though
they are outperformed by the sentiment lexica, the
word cluster features still provide an additional in-
499
GU-MLT-LT (2013) RTRGO (2014)
F
1
pos/neg F
1
3-class Accuracy F
1
pos/neg F
1
3-class Accuracy
Twitter2013 65.42 68.13 70.42 69.10 70.92 72.54
Twitter2014 65.77 66.59 69.40 69.95 69.99 72.53
SMS2013 62.65 66.93 69.09 67.51 72.15 75.54
LiveJournal2014 68.97 68.42 68.39 72.20 72.29 72.33
Twitter2014Sarcasm 54.11 56.91 58.14 47.09 49.34 51.16
Table 2: Final results of our submissions on the different test sets (Subtask B)
crease in performance and can, in contrast to sen-
timent lexica, be learned in a completely unsu-
pervised manner. Negation handling is an impor-
tant feature to boost the precision of the classifier,
while using random subspace learning increases
the recall of the classes, which indicates that the
technique indeed leads to more redundant models.
Another interesting question in sentiment anal-
ysis is, how machine learning methods com-
pare to simple methods only relying on sentiment
wordlists and how much training data is needed
to outperform them. Figure 1 shows the results
of a training size experiment, in which we tested
classifiers, trained on different portions of a train-
ing set, on the same test set (10-fold cross val-
idated). The two horizontal lines indicate the
performance of two simple classifiers, using the
Twitrratr wordlist (359 entries, labeled TRR) or
Bing Liu opinion lexicon (6789 entries, labeled
LIU) with a simple majority-vote strategy (choos-
ing the neutral class in case of no hits or no ma-
jority and including a polarity switch for token in
a negation context). The baseline of the machine
learning classifiers is a logistic regression
45
50
55
60
65
Training samples per class
Ave
ra
ge 
3?c
las
s F
1
50 250 500 750 1000 1250
TR
R
LIU
+LEX
+REST
BOW
Figure 1: Training size experiment
classifier using only uni- and bigram features and
negation handling (labeled BOW). To this baseline
we add either the lexicon features for the Bing Liu
opinion lexicon and the Twitrratr wordlist (labeled
+LEX) or all other features described in section
2.2 excluding lexicon features (labeled +REST).
Looking at the results, we can see that a simple
bag of words classifier needs about 250 samples
of each class to outperform the TRR list and about
700 samples of each class to outperform the LIU
lexicon on the common test set. Adding the fea-
tures that can be obtained without having senti-
ment lexica available (+REST) reduces the needed
training samples about half. It is worth noting that
from a training set size of 1250 samples per class
the +REST-classifier is able to match the results of
the classifier combining bag of words and lexicon
features (+LEX).
4 Results and Conclusion
The results of our system are presented in Table 2,
where the bold column marks the results relevant
to our submission to this year?s shared task. We
also give results for our last year?s system. Be-
side the average F-measure between positive and
negative class, on which the shared task is evalu-
ated, we also provide the results of both systems as
average F-measure over all three classes and accu-
racy to create possibilities for better comparison
to other research. In this paper we showed sev-
eral ways to improve a machine learning classifier
for the use of sentiment analysis in Twitter. Com-
pared to our last year?s system we were able to
increase the performance about several F-measure
points on all non-sarcastic datasets.
Acknowledgements
We would like to thank the organizers of the
shared task for their effort, as well as the anony-
mous reviewers for their helpful comments on the
paper.
500
References
Luciano Barbosa and Junlan Feng. 2010. Robust sen-
timent detection on twitter from biased and noisy
data. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
pages 36?44. Association for Computational Lin-
guistics.
Lee Becker, George Erhart, David Skiba, and Valen-
tine Matula. 2013. Avaya: Sentiment analysis on
twitter with self-training and polarity lexicon expan-
sion. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation (SemEval 2013), pages 333?
340, Atlanta, Georgia, USA, June. Association for
Computational Linguistics.
Adam Bermingham and Alan F Smeaton. 2010. Clas-
sifying sentiment in microblogs: is brevity an advan-
tage? In Proceedings of the 19th ACM international
conference on Information and knowledge manage-
ment, pages 1833?1836. ACM.
Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467?479.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics:
Posters, pages 241?249. Association for Computa-
tional Linguistics.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
CS224N Project Report, Stanford.
Tobias G?unther and Lenz Furrer. 2013. GU-MLT-
LT: Sentiment analysis of short messages using lin-
guistic features and stochastic gradient descent. In
Second Joint Conference on Lexical and Computa-
tional Semantics (*SEM), Volume 2: Proceedings
of the Seventh International Workshop on Seman-
tic Evaluation (SemEval 2013), pages 328?332, At-
lanta, Georgia, USA, June. Association for Compu-
tational Linguistics.
Tobias G?unther. 2013. Sentiment analysis of mi-
croblogs. Master?s thesis, University of Gothenburg,
June.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 168?177.
ACM.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the omg. In Proceedings of the Fifth In-
ternational AAAI Conference on Weblogs and Social
Media, pages 538?541.
Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1?167.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. Nrc-canada: Building the state-of-the-
art in sentiment analysis of tweets. In Proceedings
of the seventh international workshop on Seman-
tic Evaluation Exercises (SemEval-2013), Atlanta,
Georgia, USA, June.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis in
twitter. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation (SemEval 2013), pages 312?
320, Atlanta, Georgia, USA, June. Association for
Computational Linguistics.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R Routledge, and Noah A Smith. 2010.
From tweets to polls: Linking text sentiment to
public opinion time series. In Proceedings of the
International AAAI Conference on Weblogs and
Social Media, pages 122?129.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL 2013.
Alexander Pak and Patrick Paroubek. 2010. Twitter as
a corpus for sentiment analysis and opinion mining.
In Proceedings of LREC, volume 2010.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 conference on Empirical methods in natural
language processing-Volume 10, pages 79?86. As-
sociation for Computational Linguistics.
Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and
?
Edouard Duchesnay. 2011.
Scikit-learn: Machine learning in Python. Journal
of Machine Learning Research, 12:2825?2830.
Martin F Porter. 1980. An algorithm for suffix strip-
ping. Program: electronic library and information
systems, 14(3):130?137.
Sara Rosenthal, Preslav Nakov, Alan Ritter, and
Veselin Stoyanova. 2014. Semeval-2014 task 9:
Sentiment analysis in twitter. In Proceedings of
the International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland, August.
501
Anders S?gaard and Anders Johannsen. 2012. Robust
learning in random subspaces: Equipping NLP for
OOV effects. In COLING (Posters), pages 1171?
1180.
Michael Speriosu, Nikita Sudan, Sid Upadhyay, and
Jason Baldridge. 2011. Twitter polarity classifica-
tion with label propagation over lexical links and the
follower graph. In Proceedings of the First Work-
shop on Unsupervised Learning in NLP, pages 53?
63. Association for Computational Linguistics.
Mike Thelwall, Kevan Buckley, Georgios Paltoglou,
Di Cai, and Arvid Kappas. 2010. Sentiment
strength detection in short informal text. Journal of
the American Society for Information Science and
Technology, 61(12):2544?2558.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation, 39(2-3):165?210.
Ley Zhang, Riddhiman Ghosh, Mohamed Dekhil, Me-
ichun Hsu, and Bing Liu. 2011. Combining lex-
iconbased and learning-based methods for twitter
sentiment analysis. HP Laboratories, Technical Re-
port HPL-2011-89.
502
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 159?177
Manchester, August 2008
The CoNLL-2008 Shared Task on
Joint Parsing of Syntactic and Semantic Dependencies
Mihai Surdeanu
?,?
Richard Johansson
?
Adam Meyers
?
Llu??s M
`
arquez
??
Joakim Nivre
??,??
?: Barcelona Media Innovation Center, mihai.surdeanu@barcelonamedia.org
?: Yahoo! Research Barcelona, mihais@yahoo-inc.com
?: Lund University, richard@cs.lth.se
?: New York University, meyers@cs.nyu.edu
??: Technical University of Catalonia, lluism@lsi.upc.edu
??: V?axj?o University, joakim.nivre@vxu.se
??: Uppsala University, joakim.nivre@lingfil.uu.se
Abstract
The Conference on Computational Natu-
ral Language Learning is accompanied ev-
ery year by a shared task whose purpose
is to promote natural language processing
applications and evaluate them in a stan-
dard setting. In 2008 the shared task was
dedicated to the joint parsing of syntactic
and semantic dependencies. This shared
task not only unifies the shared tasks of
the previous four years under a unique
dependency-based formalism, but also ex-
tends them significantly: this year?s syn-
tactic dependencies include more informa-
tion such as named-entity boundaries; the
semantic dependencies model roles of both
verbal and nominal predicates. In this pa-
per, we define the shared task and describe
how the data sets were created. Further-
more, we report and analyze the results and
describe the approaches of the participat-
ing systems.
1 Introduction
In 2004 and 2005 the shared tasks of the Confer-
ence on Computational Natural Language Learn-
ing (CoNLL) were dedicated to semantic role la-
beling (SRL), in a monolingual setting (English).
In 2006 and 2007 the shared tasks were devoted to
the parsing of syntactic dependencies, using cor-
pora from up to 13 languages. The CoNLL-2008
shared task
1
proposes a unified dependency-based
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1
http://www.yr-bcn.es/conll2008
formalism, which models both syntactic depen-
dencies and semantic roles. Using this formalism,
this shared task merges both the task of syntactic
dependency parsing and the task of identifying se-
mantic arguments and labeling them with semantic
roles. Conceptually, the 2008 shared task can be
divided into three subtasks: (i) parsing of syntactic
dependencies, (ii) identification and disambigua-
tion of semantic predicates, and (iii) identification
of arguments and assignment of semantic roles for
each predicate. Several objectives were addressed
in this shared task:
? SRL is performed and evaluated using a
dependency-based representation for both
syntactic and semantic dependencies. While
SRL on top of a dependency treebank has
been addressed before (Hacioglu, 2004),
our approach has several novelties: (i) our
constituent-to-dependency conversion strat-
egy transforms all annotated semantic argu-
ments in PropBank and NomBank not just a
subset; (ii) we address propositions centered
around both verbal (PropBank) and nominal
(NomBank) predicates.
? Based on the observation that a richer set
of syntactic dependencies improves seman-
tic processing (Johansson and Nugues, 2007),
the syntactic dependencies modeled are more
complex than the ones used in the previous
CoNLL shared tasks. For example, we now
include apposition links, dependencies de-
rived from named entity (NE) structures, and
better modeling of long-distance grammatical
relations.
? A practical framework is provided for the
joint learning of syntactic and semantic de-
pendencies.
159
Given the complexity of this shared task, we
limited the evaluation to a monolingual, English-
only setting. The evaluation is separated into two
different challenges: a closed challenge, where
systems have to be trained strictly with informa-
tion contained in the given training corpus, and an
open challenge, where systems can be developed
making use of any kind of external tools and re-
sources. The participants could submit results in
either one or both challenges.
This paper is organized as follows. Section 2
defines the task, including the format of the data,
the evaluation metrics, and the two challenges.
Section 3 introduces the corpora used and our
constituent-to-dependency conversion procedure.
Section 4 summarizes the results of the submit-
ted systems. Section 5 discusses the approaches
implemented by participants. Section 6 analyzes
the results using additional non-official evaluation
measures. Section 7 concludes the paper.
2 Task Definition
In this section we provide the definition of the
shared task, starting with the format of the shared
task data, followed by a description of the eval-
uation metrics used and a discussion of the two
shared task challenges, i.e., closed and open.
2.1 Data Format
The data format used in this shared task was highly
influenced by the formats used in the 2004?2007
shared tasks. The data follows these general rules:
? The files contain sentences separated by a
blank line.
? A sentence consists of one or more tokens and
the information for each token is represented
on a separate line.
? A token consists of at least 11 fields. The
fields are separated by one or more whites-
pace characters (spaces or tabs). Whitespace
characters are not allowed within fields.
Table 1 describes the fields stored for each token
in the closed-track data sets. Columns 1?3 and
5?8 are available at both training and test time.
Column 4, which contains gold-standard part-of-
speech (POS) tags, is not given at test time. The
same holds for columns 9 and above, which con-
tain the syntactic and semantic dependency struc-
tures that the systems should predict.
The PPOS and PPOSS fields were automati-
cally predicted using the SVMTool POS tagger
(Gim?enez, 2004). To predict the tags in the train-
ing set, a 5-fold cross-validation procedure was
used. The LEMMA and SPLIT LEMMA fields
were predicted using the built-in lemmatizer in
WordNet (Fellbaum, 1998) based on the most fre-
quent sense for the form and part-of-speech tag.
Since NomBank uses a sub-word anal-
ysis in some hyphenated words (such as
[finger]
ARG
-[pointing]
PRED
), the data for-
mat represents the parts in hyphenated words as
separate tokens (columns 6?8). However, the
format also represents how the parts originally fit
together before splitting (columns 2?5). Padding
characters (? ?) are used in columns 2?5 to
ensure the same number of rows for all columns
corresponding to one sentence. All syntactic and
semantic dependencies are annotated relative to
the split word forms (columns 6?8).
Table 2 shows the columns available to the sys-
tems participating in the open challenge: named-
entity labels as in the CoNLL-2003 Shared Task
(Tjong Kim San and De Meulder, 2003) and
from the BBN Wall Street Journal Entity Corpus,
2
WordNet supersense tags, and the output of an off-
the-shelf dependency parser (Nivre et al, 2007b).
Columns 1?3 were predicted using the tagger of
Ciaramita and Altun (2006). Because the BBN
corpus shares lexical content with the Penn Tree-
bank, we generated the BBN tags using a 2-fold
cross-validation procedure.
2.2 Evaluation Measures
We separate the evaluation measures into two
groups: (i) official measures, which were used for
the ranking of participating systems, and (ii) addi-
tional unofficial measures, which provide further
insight into the performance of the participating
systems.
2.2.1 Official Evaluation Measures
The official evaluation measures consist of three
different scores: (i) syntactic dependencies are
scored using the labeled attachment score (LAS),
(ii) semantic dependencies are evaluated using a
labeled F
1
score, and (iii) the overall task is scored
with a macro average of the two previous scores.
We describe all these scoring measures next.
The LAS score is defined similarly as in the pre-
vious two shared tasks, as the percentage of to-
2
LDC catalog number LDC2005T33.
160
Number Name Description
1 ID Token counter, starting at 1 for each new sentence.
2 FORM Unsplit word form or punctuation symbol.
3 LEMMA Predicted lemma of FORM.
4 GPOS Gold part-of-speech tag from the Treebank (empty at test time).
5 PPOS Predicted POS tag.
6 SPLIT FORM Tokens split at hyphens and slashes.
7 SPLIT LEMMA Predicted lemma of SPLIT FORM.
8 PPOSS Predicted POS tags of the split forms.
9 HEAD Syntactic head of the current token, which is either a value of ID or zero (0).
10 DEPREL Syntactic dependency relation to the HEAD.
11 PRED Rolesets of the semantic predicates in this sentence.
12. . . ARG Columns with argument labels for each semantic predicate following textual order.
Table 1: Column format in the closed-track data. The columns in the lower part of the table are unseen
at test time and are to be predicted by systems.
Number Name Description
1 CONLL2003 Named entity labels using the tag set from the CoNLL-2003 shared task.
2 BBN NE labels using the tag set from the BBN Wall Street Journal Entity Corpus.
3 WNSS WordNet super senses.
4 MALT HEAD Head of the syntactic dependencies generated by MaltParser.
5 MALT DEPREL Label of syntactic dependencies generated by MaltParser.
Table 2: Column format in the open-track data.
kens for which a system has predicted the correct
HEAD and DEPREL columns (see Table 1). Same
as before, our scorer also computes the unlabeled
attachment score (UAS), i.e., the percentage of to-
kens with correct HEAD, and label accuracy, i.e.,
the percentage of tokens with correct DEPREL.
The semantic propositions are evaluated by con-
verting them to semantic dependencies, i.e., we
create a semantic dependency from every predicate
to all its individual arguments. These dependen-
cies are labeled with the labels of the correspond-
ing arguments. Additionally, we create a seman-
tic dependency from each predicate to a virtual
ROOT node. The latter dependencies are labeled
with the predicate senses. This approach guaran-
tees that the semantic dependency structure con-
ceptually forms a single-rooted, connected (but not
necessarily acyclic) graph. More importantly, this
scoring strategy implies that if a system assigns
the incorrect predicate sense, it still receives some
points for the arguments correctly assigned. For
example, for the correct proposition:
verb.01: ARG0, ARG1, ARGM-TMP
the system that generates the following output for
the same argument tokens:
verb.02: ARG0, ARG1, ARGM-LOC
receives a labeled precision score of 2/4 because
two out of four semantic dependencies are incor-
rect: the dependency to ROOT is labeled 02 in-
stead of 01 and the dependency to the ARGM-TMP
is incorrectly labeled ARGM-LOC. Using this strat-
egy we compute precision, recall, and F
1
scores
for both labeled and unlabeled semantic dependen-
cies.
Finally, we combine the syntactic and semantic
measures into one global measure using macro av-
eraging. We compute macro precision and recall
scores by averaging the labeled precision and re-
call for semantic dependencies with the LAS for
syntactic dependencies:
3
LMP = W
sem
? LP
sem
+ (1?W
sem
) ? LAS (1)
LMR = W
sem
? LR
sem
+ (1?W
sem
) ? LAS (2)
where LMP is the labeled macro precision and
LP
sem
is the labeled precision for semantic depen-
dencies. Similarly, LMR is the labeled macro re-
call and LR
sem
is the labeled recall for semantic
dependencies. W
sem
is the weight assigned to the
semantic task.
4
The macro labeled F
1
score, which
was used for the ranking of the participating sys-
tems, is computed as the harmonic mean of LMP
and LMR.
3
We can do this because the LAS for syntactic dependen-
cies is a special case of precision and recall, where the pre-
dicted number of dependencies is equal to the number of gold
dependencies.
4
We assign equal weight to the two tasks, i.e., W
sem
=
0.5.
161
2.2.2 Additional Evaluation Measures
We used several additional evaluation measures
to further analyze the performance of the partici-
pating systems.
The first additional measure used is Exact
Match, which reports the percentage of sentences
that are completely correct, i.e., all the generated
syntactic dependencies are correct and all the se-
mantic propositions are present and correct. While
this score is significantly lower than any of the of-
ficial scores, it will award systems that performed
joint learning or optimization for all subtasks.
In the same spirit but focusing on the seman-
tic subtasks, we report the Perfect Proposition F
1
score, where we score entire semantic frames or
propositions. This measure is similar to the PProps
accuracy score from the 2005 shared task (Carreras
and M`arquez, 2005), with the caveat that this year
this score is implemented as an F
1
measure, be-
cause predicates are not provided in the test data.
Hence, propositions may be over or under gener-
ated at prediction time.
Lastly, we analyze systems based on the ratio
between labeled F
1
score for semantic dependen-
cies and the LAS for syntactic dependencies. In
other words, this measure normalizes the seman-
tic scores relative to the performance of the pars-
ing component. This measure estimates the true
overall performance of the semantic subtasks, in-
dependent of the syntactic parser.
5
For example,
this score addresses the situations where the se-
mantic labeled F
1
score of one system is artificially
low because the corresponding syntactic compo-
nent does not perform well.
2.3 Closed and Open Challenges
Similarly to the CoNLL-2005 shared task, this
shared task evaluation is separated into two chal-
lenges:
Closed Challenge - systems have to be built
strictly with information contained in the given
training corpus, and tuned with the development
section. In addition, the PropBank and NomBank
lexical frames can be used. These restrictions
mean that constituent-based parsers or SRL sys-
tems can not be used in this challenge because the
constituent-based annotations are not provided in
our training set. The aim of this challenge is to
5
A correct evaluation of the stand-alone SRL systems
would require the usage of gold syntactic dependencies, but
these were not provided for the testing corpora.
compare the performance of the participating sys-
tems in a fair environment.
Open Challenge - systems can be developed mak-
ing use of any kind of external tools and resources.
The only condition is that such tools or resources
must not have been developed with the annota-
tions of the test set, both for the input and out-
put annotations of the data. In this challenge,
we are interested in learning methods which make
use of any tools or resources that might improve
the performance. For example, we encourage the
use of semantic information, as provided by NE
recognition or word-sense disambiguation (WSD)
systems (such state-of-the-art annotations are pro-
vided by the organizers, see Table 2). Also, in
this challenge participants are encouraged to use
constituent-based parsers and SRL systems, as
long as these systems were trained only with the
sections of Penn Treebank used in the shared task
training corpus. To encourage the participation of
the groups that are only interested in SRL, the or-
ganizers provide also the output of a state-of-the-
art dependency parser as input in this challenge.
The comparison of different systems in this setting
may not be fair, and thus ranking of systems is not
necessarily important.
3 Data
The corpora used in the shared task evaluation
were generated through a process that merges
several input corpora and converts them from
the constituent-based formalism to dependencies.
This section starts with an introduction of the in-
put corpora used, followed by a description of
the constituent-to-dependency conversion process.
The section concludes with an overview of the
shared task corpora.
3.1 Input Corpora
Input to our merging procedures includes the Penn
Treebank, BBN?s named entity corpus, PropBank
and NomBank. In this section, we will pro-
vide brief descriptions of these annotations in
terms of both form and content. All annotations
are currently being distributed by the Linguistic
Data Consortium, with the exception of NomBank,
which is freely downloadable.
6
6
http://nlp.cs.nyu.edu/meyers/NomBank.
html
162
3.1.1 Penn Treebank 3
The Penn Treebank 3 corpus (Marcus et al,
1993) consists of hand-coded parses of the Wall
Street Journal (test, development and training) and
a small subset of the Brown corpus (W. N. Fran-
cis and H. Ku?cera, 1964) (test only). These hand
parses are notated in-line and sometimes involve
changing the strings of the input data. For ex-
ample, in file wsj 0309, the token fearlast in the
text corresponds to the two tokens fear and last
in the annotated data. In a similar way, cannot
is regularly split to can and not. It is significant
that the other annotations assume the tokeniza-
tion of the Penn Treebank, as this makes it easier
for us to merge the annotation. The Penn Tree-
bank syntactic annotation includes phrases, parts
of speech, empty category representations of vari-
ous filler/gap constructions and other phenomena,
based on a theoretical perspective similar to that
of Government and Binding Theory (Chomsky,
1981).
3.1.2 BBN Pronoun Coreference and Entity
Type Corpus
BBN?s NE annotation of the Wall Street Journal
corpus (Weischedel and Brunstein, 2005) takes the
form of SGML inline markup of text, tokenized
to be completely compatible with the Penn Tree-
bank annotation, e.g., fearlast and cannot are split
in the same ways. Named entity categories in-
clude: Person, Organization, Location, GPE, Fa-
cility, Money, Percent, Time and Date, based on
the definitions of these categories in MUC (Chin-
chor and Robinson, 1998) and ACE
7
tasks. Sub-
categories are included as well. Note however that
from this corpus we only use NE boundaries to
derive NAME dependencies between NE tokens,
e.g., we create a NAME dependency from Mary to
Smith given the NE mention Mary Smith.
3.1.3 Proposition Bank I (PropBank)
The PropBank annotation (Palmer et al, 2005)
classifies the arguments of all the main verbs in the
Penn Treebank corpus, other than be. Arguments
are numbered (ARG0, ARG1, . . .) based on lexical
entries or frame files. Different sets of arguments
are assumed for different rolesets. Dependent con-
stituents that fall into categories independent of
the lexical entries are classified as various types
7
http://projects.ldc.upenn.edu/ace/
of ARGM (TMP, ADV, etc.).
8
Rather than us-
ing PropBank directly, we used the version created
for the CoNLL-2005 shared task (Carreras and
M`arquez, 2005). PropBank?s pointers to subtrees
are converted into the list of leaves of those sub-
trees, minus the empty categories. On occasion,
arguments of verbs end up being two non-adjacent
substrings. For example, the argument of claims in
the following sentence is indicated in bold: This
sentence, Mary claims, is self-referential. The
CoNLL-2005 format handles this by marking both
strings A1 (This sentence and is self-referential),
but adding a C- prefix to the argument tag on the
second argument. Another difference between the
PropBank annotation and the CoNLL-2005 ver-
sion of it is their treatments of filler gap construc-
tions involving empty categories. PropBank an-
notation includes the whole chain of empty cate-
gories, as well as the antecedent of the empty cate-
gory (the filler of the gap). In contrast, the CoNLL-
2005 version only includes the filler of the gap and
if there is no filler, the argument is omitted, e.g.,
no ARG0 (subject) for leave would be included in
I said to leave because the subject of leave is un-
specified.
3.1.4 NomBank
NomBank annotation (Meyers et al, 2004) uses
essentially the same framework as PropBank to an-
notate arguments of nouns. Differences between
PropBank and NomBank stem from differences
between noun and verb argument structure; differ-
ences in treatment of nouns and verbs in the Penn
Treebank; and differences in the sophistication of
previous research about noun and verb argument
structure. Only the subset of nouns that take ar-
guments are annotated in NomBank and only a
subset of the non-argument siblings of nouns are
marked as ARGM. These limitations were nec-
essary to make the NomBank task consistent and
tractable. In addition, long distance dependencies
of nouns, e.g., the relation between Mary and walk
in Mary took dozens of walks is handled as fol-
lows: Mary is marked as the ARG0 of walk and
took + dozens + of is marked as a support chain
in NomBank. In contrast, verbal long distance de-
pendencies can be handled by means of empty cat-
egories in the Penn Treebank, e.g., the relation be-
8
PropBank I is used here. Later versions of PropBank
mark instances of be in addition to other verbs. PropBank?s
use of the terms roleset and ARGM correspond approximately
to sense and adjunct in common usage.
163
tween John and walked in John seemed t to walk.
Support chains are needed because nominal long
distance dependencies are not captured under the
Penn Treebank?s system of empty categories.
3.2 Conversion to Dependencies
3.2.1 Syntactic Dependencies
There exists no large-scale dependency tree-
bank for English, and we thus had to construct a
dependency-annotated corpus automatically from
the Penn Treebank (Marcus et al, 1993). Since
dependency syntax represents grammatical struc-
ture by means of labeled binary head?dependent
relations rather than phrases, the task of the con-
version procedure is to identify and label the
head?dependent pairs. The idea underpinning
constituent-to-dependency conversion algorithms
(Magerman, 1994; Collins, 1999; Yamada and
Matsumoto, 2003) is that head?dependent pairs are
created from constituents by selecting one word in
each phrase as the head and setting all other as its
dependents. The dependency labels are then in-
ferred from the phrase?subphrase or phrase?word
relations.
Our conversion procedure (Johansson and
Nugues, 2007) differs from this basic approach by
exploiting the rich structure of the constituent for-
mat used in Penn Treebank 3:
? Grammatical function labels that often can be
directly used in the dependency framework.
? Long-distance grammatical relations repre-
sented by means of empty categories and sec-
ondary edges, which can be used to create (of-
ten nonprojective) dependency links.
Of the grammatical function tags available in the
Treebank, we removed the HLN, NOM, TPC, and
TTL tags since they represent structural properties
of single phrases rather than binary relations. For
compatibility between the WSJ and Brown cor-
pora, we removed the ETC, UNF, and IMP tags
from Brown and the CLR tag from WSJ.
Algorithms 1 and 2 show the constituent-to-
dependency conversion algorithm and function la-
beling, respectively. The first steps apply structural
transformations to the constituent trees. Next, a
head word is assigned to each constituent. After
this, grammatical functions are inferred, allowing
a dependency tree to be created.
To find head children (used in
assign-heads), a system of rules is used
Algorithm 1: Pseudocode for constituent-to-
dependency conversion.
procedure constituents-to-dependencies(T )
import-glarf(T )
reattach-traces(T )
split-small-clauses(T )
assign-heads(T.root)
assign-functions(T )
return create-dependency-tree(T )
procedure import-glarf(T )
Import a GLARF surface dependency graph G
for each multi-word name N in G
for each token d in N
Set the function tag of d to NAME
for each dependency link h ?
L
d in G
if L ? { APPOSITE, A-POS, N-POS, POST-HON, Q-POS,
RED-RELATIVE, SUFFIX, T-POS, TITLE }
or if h and d are inside a split word
Set the function tag of d to L in T
if h and d are part of a larger constituent
Add an NX constituent to T that brackets h and d
procedure reattach-traces(T )
for each empty category t in T
if t is linked to a constituent C via a secondary edge label L
and L ? {
*
ICH
*
,
*
T
*
,
*
RNR
*
}
disconnect C
disconnect the secondary edge
attach C to the parent of t
procedure split-small-clauses(T )
for each verb phrase C in T
if C has a child S and the phrase label of S is S
and S is not preceded by a ?? or , tag
and S has a subject child s
disconnect s
attach s to C
set the function tag of s to OBJ
set the function tag of S to OPRD
procedure assign-heads(N)
for each child C of N
assign-heads(C)
if is-coordinated(N)
e ? index of first CC or CONJP or , or :
else
e ? index of last child of N
find head child H between 1 and e according to head rules (Table 3)
N.head ? H.head
procedure is-coordinated(N)
if N has the label UCP return True
if N has a CC or CONJP child which is not leftmost return True
if N has a , or : child c, and c is not leftmost or rightmost or
crossed by an apposition link, return True
else return False
procedure create-dependency-tree(T )
D ? {}
for each token t in T
let C be the highest constituent that t is the head of
let P be the parent of C
let L be the function tag of C
D ? D ? P.head ?
L
t
return D
(Table 3). The first column in the table indicates
the phrase type, the second is the search direction,
and the third is a priority list of phrase types to
look for. For instance, to find the head of an S
phrase, we look from right to left for a VP. If
no VP is found, look for anything with a PRD
function tag, and so on.
Moreover, since the grammatical structure in-
164
ADJP ? NNS QP NN $ ADVP JJ VBN VBG ADJP JJR NP JJS DT
FW RBR RBS SBAR RB
ADVP ? RB RBR RBS FW ADVP TO CD JJR JJ IN NP JJS NN
CONJP ? CC RB IN
FRAG ? (NN
*
| NP) W
*
SBAR (PP | IN) (ADJP | JJ) ADVP
RB
INTJ ?
*
LST ? LS :
NAC, NP, NX, WHNP ? (NN
*
| NX) NP-? JJR CD JJ JJS RB QP NP
PP, WHPP ? IN TO VBG VBN RP FW
PRN ? S
*
N
*
W
*
PP|IN ADJP|JJ
*
ADVP|RB
*
PRT ? RP
QP ? $ IN NNS NN JJ RB DT CD NCD QP JJR JJS
RRC ? VP NP ADVP ADJP PP
S ? VP
*
-PRD S SBAR ADJP UCP NP
SBAR ? S SQ SINV SBAR FRAG IN DT
SBARQ ? SQ S SINV SBARQ FRAG
SINV ? VBZ VBD VBP VB MD VP
*
-PRD S SINV ADJP NP
SQ ? VBZ VBD VBP VB MD
*
-PRD VP SQ
UCP ?
*
VP ? VBD VBN MD VBZ VB VBG VBP VP
*
-PRD ADJP NN NNS
NP
WHADJP ? CC WRB JJ ADJP
WHADVP ? CC WRB
X ?
*
Table 3: Head rules.
Algorithm 2: Pseudocode for the function la-
beling procedure.
procedure assign-functions(T )
for each constituent C in T
if C has no function tag from Penn or GLARF
L ? infer-function(C)
Set the function tag of C to L
procedure infer-function(C)
let c be the head of C, P the parent of C, and p the head of P
if C is an object return OBJ
if C is PRN return PRN
if h is punctuation return P
if C is coordinated with P return COORD
if C is PP, ADVP, or SBAR and P is VP return ADV
if C is PRT and P is VP return PRT
if C is VP and P is VP, SQ, or SINV return VC
if C is TO and P is VP return IM
if P is SBAR and p is IN return SUB
if P is VP, S, SBAR, SBARQ, SINV, or SQ and C is RB return ADV
if P is NP, NX, NAC, or WHNP return NMOD
if P is ADJP, ADVP, WHADJP, or WHADVP return AMOD
if P is PP or WHPP return PMOD
else return DEP
side noun phrases (NP) is under-specified in the
Penn Treebank, we imported dependencies in-
side NPs and hyphenated words from a version
of the Penn Treebank mapped into GLARF, the
Grammatical and Logical Argument Representa-
tion Framework (Meyers et al, 2007).
The parts of GLARF?s NP analysis that are most
relevant to this task include: (i) identifying ap-
posites (APPO, e.g., that book depends on gift in
Mary?s gift, a book about cheese; (ii) the iden-
tification of name boundaries taken from BBN?s
NE annotation, e.g., identifying that Smith de-
pends on Mary which depends on appointment
in the Mary Smith appointment; (iii) identifying
TITLE and POSTHON dependencies, e.g., deter-
mining that Ms. and III depend on Mary in Ms.
Mary Smith III. These identifications were car-
ried out by hand-coded rules that have been fine
tuned as part of GLARF, over the past several
years. For example, identifying apposition con-
structions requires identifying that both the head
and the apposite can stand alone ? proper nouns
(John Smith), plural nouns (books), and singular
common nouns with determiners (the book) are
stand-alone cases, whereas singular nouns without
determiners (green book) do not qualify.
We split Treebank tokens at a hyphen (-) or a
forward slash (/) if the segments on either side of
these delimiters are: (a) a word in a dictionary
(COMLEX Syntax or any of the dictionaries avail-
able on the NOMLEX website); (b) part of a mark-
able Named Entity;
9
or (c) a prefix from the list:
co, pre, post, un, anti, ante, ex, extra, fore, non,
over, pro, re, super, sub, tri, bi, uni, ultra. For ex-
ample, York-based was split into 3 segments: (1)
York, (2) - and (3) based.
9
The CoNLL-2008 website contains a Named Entity To-
ken gazetteer to aid in this segmentation.
165
3.2.2 Semantic Dependencies
When encoding the semantic dependencies, it
was necessary to convert the underlying con-
stituent analysis of PropBank and NomBank into
a dependency analysis. Because semantic predi-
cates are already assigned to individual tokens in
both PropBank (the version used for the CoNLL-
2005 shared task) and NomBank, constituent-to-
dependency conversion is thus necessary only for
semantic arguments. Conceptually, this conver-
sion can be handled using similar heuristics as de-
scribed in Section 3.2.1. However, in order to
avoid replicating this effort and to ensure compat-
ibility between syntactic and semantic dependen-
cies, we decided to generate semantic dependen-
cies using only argument boundaries and the syn-
tactic dependencies generated in Section 3.2.1, i.e.,
ignoring syntactic constituents. Given this input,
we identify the head of a semantic argument using
the following heuristic:
The head of a semantic argument is as-
signed to the token inside the argument
boundaries whose head is a token out-
side the argument boundaries.
This heuristic works remarkably well: over 99%
of the PropBank arguments in the training corpus
have a single token whose head is located outside
of the argument boundaries. As a simple example,
consider the following annotated text: [sold]
PRED
[1214 cars]
ARG1
[in the U.S.]
ARGM-LOC
. Us-
ing the above heuristic, the head of the ARG1 ar-
gument is set to cars, because it has an OBJ de-
pendency to sold, and the head of the ARGM-
LOC argument is set to in, because it modifies sold
through a LOC dependency.
While this heuristic processes the vast majority
of arguments, there are several cases that require
special treatment. We discuss these situations in
the remainder of this section.
Arguments with several syntactic heads
For 0.7% of the semantic arguments, the above
heuristic detects several syntactic heads for the
given boundary. For example, in the text [it]
ARG0
[expects]
PRED
[its U.S. sales to remain steady
at about 1200 cars]
ARG1
, the above heuris-
tic assigns two syntactic heads to ARG1: sales,
which modifies expects through an OBJ depen-
dency, and to, which modifies expects through a
PRD dependency. These situations are caused
by the constituent-to-dependency conversion pro-
cess described in Section 3.2.1, which in some
cases interprets syntax differently than the orig-
inal Treebank annotation, e.g., the raising phe-
nomenon for the PRD dependency in the above
example. In such cases, we split the original argu-
ment into a sequence of discontinuous arguments,
e.g., the ARG1 in the above example becomes [its
U.S. sales]
ARG1
[to remain steady at about 1200
cars]
C-ARG1
.
Merging discontinuous arguments
While in the above case we split arguments, there
are situations where we can merge arguments that
were initially discontinuous in PropBank or Nom-
Bank. This typically happens when the Prop-
Bank/NomBank predicate is infixed inside one of
its arguments. For example, in the text [Million-
dollar conferences]
ARG1
were [held]
PRED
[to
chew on subjects such as... ]
C-ARG1
, PropBank
lists multiple constituents as aggregately filling the
ARG1 slot of held. These cases are detected au-
tomatically because the least common ancestor of
the argument pieces is actually one of the argument
segments. In the above example, to chew on sub-
jects such as... depends on Million-dollar confer-
ences because to modifies conferences through a
NMOD dependency. In these situations, we treat
the least common ancestor, e.g., conferences in the
above text, as the true argument. This heuristic al-
lowed us to merge 1665 (or 0.6% of total) argu-
ments that were initially discontinuous in the Prop-
Bank training corpus.
Empty categories
PropBank and NomBank both encode chains of
empty categories. As with the 2005 shared task
(Carreras and M`arquez, 2005), we used the head
of the antecedent of empty categories as arguments
rather than empty categories. Furthermore, empty
category arguments with no antecedents were ig-
nored.
10
For example, given The man wanted t to
make a speech, we assume that the A0 of make and
speech is man, rather than the chain consisting of
the empty category represented as t and man.
Annotation disagreements
NomBank and Penn Treebank annotators some-
times disagree about constituent structure. Nom-
10
Under our approach to filler gap constructions, the filler
is a shared argument (as in Relational Grammar, most Feature
Structure and Dependency Grammar frameworks), in con-
trast with the Penn Treebank?s empty category antecedent ap-
proach (more closely resembling the various Chomskian ap-
proaches).
166
Label Freq. Description
NMOD 324834 Modifier of nominal
P 135260 Punctuation
PMOD 115988 Modifier of preposition
SBJ 89371 Subject
OBJ 66677 Object
ROOT 49178 Root
ADV 47379 General adverbial
NAME 41138 Name-internal link
VC 35250 Verb chain
COORD 31140 Coordination
DEP 29456 Unclassified
TMP 26305 Temporal adverbial or nominal modifier
CONJ 24522 Second conjunct (dependent on conjunction)
LOC 18500 Locative adverbial or nominal modifier
AMOD 17868 Modifier of adjective or adverbial
PRD 16265 Predicative complement
APPO 16163 Apposition
IM 16071 Infinitive verb (dependent on infinitive marker to)
HYPH 14073 Token part of a hyphenated word (dependent on a preceding part of the hyphenated word)
HMOD 13885 Token inside a hyphenated word (dependent on the head of the hyphenated word)
SUB 12995 Subordinated clause (dependent on subordinating conjuction)
OPRD 11707 Predicative complement of raising/control verb
SUFFIX 10548 Possessive suffix (dependent on possessor)
DIR 6145 Adverbial of direction
TITLE 5917 Title (dependent on name)
MNR 4753 Adverbial of manner
POSTHON 4377 Posthonorific modifier of nominal
PRP 4013 Adverbial of purpose or reason
PRT 3235 Particle (dependent on verb)
LGS 3115 Logical subject of a passive verb
EXT 2374 Adverbial of extent
PRN 2176 Parenthetical
EXTR 658 Extraposed element in cleft
DTV 496 Dative complement (to) in dative shift
PUT 271 Complement of the verb put
BNF 44 Benefactor complement (for) in dative shift
VOC 24 Vocative
Table 4: Statistics for atomic syntactic labels.
Bank annotators are in effect assuming that the
constituents provided form a phrase. In this case,
the constituents are adjacent to each other. For ex-
ample, consider the NP the human rights discus-
sion. In this case, the Penn Treebank would treat
each of the four words the, human, rights, discus-
sion as daughters of a single NP node. However,
NomBank would treat human rights as a single
ARG1 of discussion. Since noun noun modifica-
tion constructions are head final, we can easily de-
termine (via GLARF) that rights is the markable
dependent of discussion.
Support chains
Finally, NomBank?s encoding of support chains is
handled as chains of dependencies in the data (al-
though these are not scored). For example, given
Mary took dozens of walks, where Mary is the
ARG0 of walks, the support chain took + dozens +
of is represented as a sequence of dependencies: of
depends on Mary, dozens depends on of and took
depends on dozens. Each of these dependencies is
labeled SU.
3.3 Overview of Corpora
The syntactic dependency types are divided into
atomic types that consist of a single label, and non-
atomic types consisting of more than one label.
There are 38 atomic and 70 non-atomic labels in
the corpus. There are three types of non-atomic
labels: those consisting of a PRD or OPRD con-
catenated with an adverbial label such as LOC or
TMP; gapping labels such as GAP-SBJ; and com-
bined adverbial tags such as LOC-TMP.
Table 4 shows statistics for the atomic syntac-
tic dependencies: label type, the frequency of the
label in the complete corpus, and a description of
the label. Table 5 shows the corresponding statis-
tics for non-atomic dependencies, excluding gap-
ping dependencies. The non-atomic labels are rare,
which made it difficult to learn these relations ef-
167
Label Frequency
LOC-PRD 798
PRD-TMP 51
PRD-PRP 45
LOC-OPRD 31
DIR-PRD 4
MNR-PRD 3
LOC-TMP 2
MNR-TMP 1
LOC-MNR 1
DIR-OPRD 1
Table 5: Statistics for non-atomic syntactic labels
excluding gapping labels.
Label Frequency
GAP-SBJ 116
GAP-OBJ 102
DEP-GAP 83
GAP-TMP 69
GAP-PRD 66
GAP-LGS 44
GAP-LOC 42
DIR-GAP 37
GAP-PMOD 22
GAP-VC 20
EXT-GAP 16
ADV-GAP 15
GAP-NMOD 13
GAP-LOC-PRD 6
DTV-GAP 6
AMOD-GAP 6
GAP-MNR 5
GAP-PRP 4
EXTR-GAP 3
GAP-SUB 1
GAP-PUT 1
GAP-OPRD 1
Table 6: Statistics for non-atomic labels containing
a gapping label.
fectively. Table 6 shows the table for non-atomic
labels containing a gapping label.
A dependency link w
i
? w
j
is said to be pro-
jective if all words occurring between w
i
and w
j
in
the surface word order are dominated by w
i
(where
dominance is the transitive closure of the direct
link relation). Nonprojective links are impossible
to handle for the search procedures in many types
of dependency parsers. It has been previously ob-
served that the majority of dependencies in all lan-
guages are projective, and this is particularly true
for English ? in the complete corpus, only 4118
links (0.4%) are nonprojective. 3312 sentences, or
7.6%, contain at least one nonprojective link.
Table 7 shows statistics for different types of
nonprojective links: nonprojectivity caused by
wh-movement, such as in Where are you going?
or What have you done?; split clauses such as
Type Frequency
wh-movement 1709
Split clause 734
Split noun phrase 590
Other 1085
Table 7: Statistics for nonprojective links.
POS Frequency
NN 68477
NNS 30048
VBD 24106
VB 23650
VBN 19339
VBG 14245
VBZ 10883
VBP 6330
Other 83
Table 8: Statistics for predicates, by POS tags.
Even to make love, he says, you need experience;
split noun phrases such as hold a hearing tomor-
row on the topic; and all other types of nonprojec-
tive links.
Lastly, Tables 8 and 9 summarizes statistics for
semantic predicates and roles. Table 8 shows the
number of non-support predicates with a given
POS tag in the whole corpus (we used GPOS or
PPOSS for predicates inside hyphenated words).
The last line shows the number of predicates with
a POS tag that does not start with NN or VB. This
last table entry is generated by POS tagger mis-
takes when producing the PPOSS tags, or by errors
in our NomBank/PropBank conversion software.
11
Nevertheless, the overall picture given by the table
indicates that predicates are almost perfectly dis-
tributed between nouns and verbs: there are 98525
nominal and 98553 verbal predicates.
Table 9 shows the number of arguments with a
given role label. For brevity we list only labels that
are instantiated at least 10 times in the whole cor-
pus. The total number of arguments labeled with a
role label with frequency lower than 10 is listed
in the last line in the table. The table indicates
that, while the top three most common role labels
are ?core? labels (A1, A0, A2), modifier arguments
(AM-
*
) account for approximately 20% of the total
number of arguments. On the other hand, discon-
tinuous arguments are not common: only 0.7% of
the total number of arguments have a continuation
label (C-
*
).
11
In very few situations, we select incorrect head tokens for
multi-word predicates.
168
Label Frequency
A1 161409
A0 109437
A2 51197
AM-TMP 25913
AM-MNR 13080
AM-LOC 11409
A3 10269
AM-MOD 9986
AM-ADV 9496
AM-DIS 5369
R-A0 4432
AM-NEG 4097
A4 3281
C-A1 3118
R-A1 2565
AM-PNC 2445
AM-EXT 1428
AM-CAU 1346
AM-DIR 1318
R-AM-TMP 797
R-A2 307
R-AM-LOC 246
R-AM-MNR 155
A5 91
AM-PRD 78
C-A0 70
C-A2 65
R-AM-CAU 50
C-A3 37
R-A3 29
C-AM-MNR 24
C-AM-ADV 20
AM-REC 16
AA 14
R-AM-PNC 12
C-AM-EXT 11
C-AM-TMP 11
C-A4 11
Frequency < 10 70
Table 9: Statistics for semantic roles.
4 Submissions and Results
Nineteen groups submitted test runs in the closed
challenge and five groups participated in the open
challenge. Three of the latter groups participated
only in the open challenge, and two of these sub-
mitted results only for the semantic subtask. These
results are summarized in Tables 10 and 11.
Table 10 summarizes the official results ? i.e.,
results at evaluation deadline ? for the closed chal-
lenge. Note that several teams corrected bugs
and/or improved their systems and they submit-
ted post-evaluation scores (accounted in the shared
task website). The table indicates that most of the
top results cluster together: three systems had a
labeled macro F
1
score on the WSJ+Brown cor-
pus around 82 points (che, ciaramita, and zhao);
five systems scored around 79 labeled macro F
1
points (yuret, samuelsson, zhang, henderson, and
watanabe). Remarkably, the top-scoring system
(johansson) is in a class of its own, with scores
2?3 points higher than the next system. This is
most likely caused by the fact that Johansson and
Nugues (2008) implemented a thorough system
that addressed all facets of the task with state-of-
the-art methods: second-order parsing model, ar-
gument identification/classification models sepa-
rately tuned for PropBank and NomBank, rerank-
ing inference for the SRL task, and, finally, joint
optimization of the complete task using meta-
learning (more details in Section 5).
Table 11 lists the official results in the open chal-
lenge. The results in this challenge are lower than
in the closed challenge, but this was somewhat
to be expected considering that there were fewer
participants in this challenge and none of the top
five groups in the closed challenge submitted re-
sults in the open challenge. Only one of the sys-
tems that participated in both challenges (zhang)
improved the results submitted in the closed chal-
lenge. Zhang et al (2008) achieved this by ex-
tracting features for their semantic subtask mod-
els both from the parser used in the closed chal-
lenge and a secondary parser that was trained on
a different corpus. The improvements measured
were relatively small for the in-domain WSJ cor-
pus (0.2 labeled macro F
1
points) but larger for the
out-of-domain Brown corpus (approximately 1 la-
beled macro F
1
point).
Tables 10 and 11 indicate that in both chal-
lenges the results on the out-of-domain corpus
(Brown) are much lower than the results measured
in-domain (WSJ). The difference is around 7?8
LAS points for the syntactic subtask and 12?14 la-
beled F
1
points for semantic dependencies. Over-
all, this yields a drop of approximately 10 labeled
macro F
1
points for most systems. This perfor-
mance decrease on out-of-domain corpora is con-
sistent with the results reported in CoNLL-2005
on SRL (using the same Brown corpus). These
results indicate that domain adaptation is a prob-
lem that is far from being solved for both syntactic
and semantic analysis of text. Furthermore, as the
scores on the syntactic and semantic subtasks in-
dicate, domain adaptation becomes even harder as
the task to be solved gets more complex.
We describe the participating systems in the next
section. Then, in Section 6, we revert to result
analysis using different evaluation measures and
different views of the data.
169
Labeled Macro F
1
Labeled Attachment Score Labeled F
1
(complete task) (syntactic dependencies) (semantic dependencies)
WSJ+Brown WSJ Brown WSJ+Brown WSJ Brown WSJ+Brown WSJ Brown
johansson 84.86 (1) 85.95 75.95 89.32 (1) 90.13 82.81 80.37 (1) 81.75 69.06
che 82.66 (2) 83.78 73.57 86.75 (5) 87.51 80.73 78.52 (2) 80.00 66.37
ciaramita 82.06 (3) 83.25 72.46 86.60 (11) 87.47 79.67 77.50 (3) 79.00 65.24
zhao 81.44 (4) 82.62 71.78 86.66 (8) 87.52 79.83 76.16 (4) 77.67 63.69
yuret 79.84 (5) 80.97 70.55 86.62 (10) 87.39 80.46 73.06 (5) 74.54 60.62
samuelsson 79.79 (6) 80.92 70.49 86.63 (9) 87.36 80.77 72.94 (6) 74.47 60.18
zhang 79.32 (7) 80.41 70.48 87.32 (2) 88.14 80.80 71.31 (7) 72.67 60.16
henderson 79.11 (8) 80.19 70.34 86.91 (4) 87.78 80.01 70.97 (8) 72.26 60.38
watanabe 79.10 (9) 80.30 69.29 87.18 (3) 88.06 80.17 70.84 (9) 72.37 58.21
morante 78.43 (10) 79.52 69.55 86.07 (12) 86.88 79.58 70.51 (10) 71.88 59.23
li 78.35 (11) 79.38 70.01 86.69 (6) 87.42 80.8 69.95 (11) 71.27 59.17
baldridge 77.49 (12) 78.57 68.53 86.67 (7) 87.42 80.64 67.92 (14) 69.35 55.95
chen 77.00 (13) 77.95 69.23 84.47 (16) 85.20 78.58 69.45 (12) 70.62 59.81
lee 76.90 (14) 77.96 68.34 84.82 (15) 85.69 77.83 68.71 (13) 69.95 58.63
sun 76.28 (15) 77.10 69.58 85.75 (13) 86.37 80.75 66.61 (15) 67.62 58.26
choi 71.23 (16) 72.22 63.44 77.56 (17) 78.58 69.46 64.78 (16) 65.72 57.4
trandabat 63.45 (17) 64.21 57.41 85.21 (14) 85.96 79.24 40.63 (17) 41.36 34.75
lluis 63.29 (18) 63.74 59.65 71.95 (18) 72.30 69.14 54.52 (18) 55.09 49.95
neumann 19.93 (19) 20.13 18.14 16.25 (19) 16.22 16.47 22.36 (19) 22.86 17.94
Table 10: Official results in the closed challenge (post-evaluation scores are available on the shared
task website). Teams are denoted by the last name of the first author of the corresponding paper in
the proceedings or the last name of the person who registered the team if no paper was submitted.
Italics indicate that there is no corresponding paper in the proceedings. Results are sorted in descending
order of the labeled macro F
1
score on the WSJ+Brown corpus. The number in parentheses next to the
WSJ+Brown scores indicates the system rank in the corresponding task.
Labeled Macro F
1
Labeled Attachment Score Labeled F
1
(complete task) (syntactic dependencies) (semantic dependencies)
WSJ+Brown WSJ Brown WSJ+Brown WSJ Brown WSJ+Brown WSJ Brown
vickrey ? ? ? ? ? ? 76.17 (1) 77.38 66.23
riedel ? ? ? ? ? ? 74.59 (2) 75.72 65.38
zhang 79.61 (1) 80.61 71.45 87.32 (1) 88.14 80.80 71.89 (3) 73.08 62.11
li 77.84 (2) 78.87 69.51 86.69 (2) 87.42 80.80 68.99 (4) 70.32 58.22
wang 76.19 (3) 78.39 59.89 84.56 (3) 85.50 77.06 67.12 (5) 70.41 42.67
Table 11: Official results in the open challenge (post-evaluation scores are available on the shared task
website). Teams are denoted by the last name of the first author of the corresponding paper in the
proceedings or the last name of the person who registered the team if no paper was submitted. Italics
indicate that there is no corresponding paper in the proceedings. Results are sorted in descending order of
the labeled F
1
score for semantic dependencies on the WSJ+Brown corpus. The number in parentheses
next to the WSJ+Brown scores indicates the system rank in the corresponding task.
5 Approaches
Table 5 summarizes the properties of the sys-
tems that participated in the closed the open chal-
lenges. The second column of the table high-
lights the overall architectures. We used + to in-
dicate that the components are sequentially con-
nected. The lack of a + sign indicates that the cor-
responding tasks are performed jointly. For exam-
ple, Riedel and Meza-Ruiz (2008) perform pred-
icate and argument identification and classifica-
tion jointly, whereas Ciaramita et al (2008) im-
plemented a pipeline architecture of three compo-
nents. We use the || to indicate that several differ-
ent architectures that span multiple subtasks were
deployed in parallel.
This summary of system architectures indicates
that it is common that systems combine sev-
eral components in the semantic or syntactic sub-
tasks ? e.g., nine systems jointly performed pred-
icate/argument identification and classification ?
but only four systems combined components be-
tween the syntactic and semantic subtasks: Hen-
derson et al (2008), who implemented a generative
history-based model (Incremental Sigmoid Belief
Networks with vectors of latent variables) where
syntactic and semantic structures are separately
170
generated but using a synchronized derivation (se-
quence of actions); Samuelsson et al (2008),
who, within an ensemble-based architecture, im-
plemented a joint syntactic-semantic model using
MaltParser with labels enriched with semantic in-
formation; Llu??s and M`arquez, who used a modi-
fied version of the Eisner algorithm to jointly pre-
dict syntactic and semantic dependencies; and fi-
nally, Sun et al (2008), who integrated depen-
dency label classification and argument identifi-
cation using a maximum-entropy Markov model.
Additionally, Johansson and Nugues (2008), who
had the highest ranked system in the closed chal-
lenge, integrate syntactic and semantic analysis in
a final reranking step, which maximizes the joint
syntactic-semantic score in the top k solutions. In
the same spirit, Chen et al (2008) search in the
top k solutions for the one that maximizes a global
measure, in this case the joint probability of the
complete problem. These joint learning strategies
are summarized in the Joint Learning/Opt. col-
umn in the table. The system of Riedel and Meza-
Ruiz (2008) deserves a special mention: even
though Riedel and Meza-Ruiz did not implement
a syntactic parser, they are the only group that per-
formed the complete SRL subtask ? i.e., predicate
identification and classification, argument identifi-
cation and classification ? jointly, simultaneously
for all the predicates in a sentence. They imple-
mented a joint SRL model using Markov Logic
Networks and they selected the overall best solu-
tion using inference based on the cutting-plane al-
gorithm.
Although some of the systems that implemented
joint approaches obtained good results, the top
five systems in the closed challenge are essen-
tially systems with pipeline architectures. Further-
more, Johansson and Nugues (2008) and Riedel
and Meza-Ruiz (2008) showed that joint learn-
ing/optimization improves the overall results, but
the improvement is not large. These initial ef-
forts indicate at least that the joint modeling of this
problem is not a trivial task.
The D Arch. and D Inference columns summa-
rize the parsing architectures and the correspond-
ing inference strategies. Similar to last year?s
shared task (Nivre et al, 2007), the vast majority of
parsing models fall in two classes: transition-based
(?trans? in the table) or graph-based (?graph?)
models. By and large, transition-based models use
a greedy inference strategy, whereas graph-based
models used different Maximum Spanning Tree
(MST) algorithms: Carreras (2007) ? MST
C
, Eis-
ner (2000) ? MST
E
, or Chu-Liu/Edmonds (Mc-
Donald et al, 2005; Chu and Liu, 1965; Edmonds,
1967) ? MST
CL/E
. More interestingly, most of
the best systems used some strategy to mitigate
parsing errors. In the top three systems in the
closed challenge, two (che and ciaramita) used
parser combination through voting and/or stacking
of different models (see the D Comb. column).
Samuelsson et al (2008) perform a MST infer-
ence with the bag of all dependencies output by
the individual MALT parser variants. Johansson
and Nugues (2008) use a single parsing model, but
this model is extended with second-order features.
The PA Arch. and PA Inference columns sum-
marize the architectures and inference strategies
used for the identification and classification of
predicates and arguments. The columns indicate
that most systems modeled the SRL problem as a
token-by-token classification problem (?class? in
the table) with a corresponding greedy inference
strategy. Some systems (e.g., yuret, samuelsson,
henderson, lluis) incorporate SRL within parsing,
in which case we report the corresponding parsing
architecture and inference approach. Vickrey and
Koller (2008) simplify the sentences to be labeled
using a set of hand-crafted rules before deploying
a classification model on top of a constituent-based
representation. Unlike in the case of parsing, few
systems (yuret, samuelssson, and morante) com-
bine several PA models and the combination is lim-
ited to simple voting strategies (see the PA Comb.
column).
Finally, the ML Methods column lists the Ma-
chine Learning (ML) methods used. The column
indicates that maximum entropy (ME) was the
most popular method (12 distinct systems relied
on it). Support Vector Machines (SVM) (eight sys-
tems) and the Perceptron algorithm (three systems)
were also popular ML methods.
6 Analysis
Section 4 summarized the results in the closed
and open challenges using the official evaluation
measures. In this section, we analyze the sub-
mitted runs using different evaluation measures,
e.g., Exact Match or Perfect Proposition F
1
scores,
and different views of the data, e.g., only non-
projective dependencies or NomBank versus Prop-
Bank frames.
171
O
v
e
r
a
l
l
D
D
D
P
A
P
A
P
A
J
o
i
n
t
M
L
c
l
o
s
e
d
A
r
c
h
.
A
r
c
h
.
C
o
m
b
.
I
n
f
e
r
e
n
c
e
A
r
c
h
.
C
o
m
b
.
I
n
f
e
r
e
n
c
e
L
e
a
r
n
i
n
g
/
O
p
t
.
M
e
t
h
o
d
s
j
o
h
a
n
s
s
o
n
D
+
P
I
+
P
C
+
A
I
+
A
C
g
r
a
p
h
n
o
M
S
T
C
c
l
a
s
s
n
o
r
e
r
a
n
k
r
e
r
a
n
k
P
e
r
c
e
p
t
r
o
n
,
M
E
c
h
e
D
+
P
I
+
P
C
+
A
I
C
g
r
a
p
h
s
t
a
c
k
i
n
g
M
S
T
C
L
/
E
c
l
a
s
s
n
o
I
L
P
n
o
M
E
c
i
a
r
a
m
i
t
a
D
+
P
I
C
+
A
I
C
t
r
a
n
s
v
o
t
i
n
g
,
g
r
e
e
d
y
c
l
a
s
s
n
o
r
e
r
a
n
k
n
o
S
V
M
,
M
E
,
s
t
a
c
k
i
n
g
P
e
r
c
e
p
t
r
o
n
z
h
a
o
D
+
A
I
C
+
P
I
+
P
C
t
r
a
n
s
n
o
g
r
e
e
d
y
c
l
a
s
s
n
o
g
r
e
e
d
y
n
o
M
E
y
u
r
e
t
D
+
(
P
I
C
+
A
I
+
A
C
|
|
g
r
a
p
h
n
o
M
S
T
E
c
l
a
s
s
,
v
o
t
i
n
g
g
r
e
e
d
y
n
o
M
L
E
,
P
I
C
+
A
I
C
)
g
e
n
e
r
a
t
i
v
e
M
B
L
s
a
m
u
e
l
s
s
o
n
D
+
P
I
+
t
r
a
n
s
M
S
T
C
L
/
E
g
r
e
e
d
y
c
l
a
s
s
,
v
o
t
i
n
g
g
r
e
e
d
y
u
n
i
fi
e
d
S
V
M
(
A
I
+
A
C
|
|
D
A
I
C
)
+
P
C
b
l
e
n
d
i
n
g
t
r
a
n
s
l
a
b
e
l
s
z
h
a
n
g
D
+
P
I
+
A
I
+
A
C
+
P
C
g
r
a
p
h
,
m
e
t
a
-
M
S
T
C
L
/
E
,
c
l
a
s
s
n
o
g
r
e
e
d
y
n
o
S
V
M
,
M
E
t
r
a
n
s
l
e
a
r
n
i
n
g
g
r
e
e
d
y
h
e
n
d
e
r
s
o
n
D
P
A
I
C
+
D
g
e
n
e
r
a
t
i
v
e
,
n
o
b
e
a
m
t
r
a
n
s
n
o
b
e
a
m
s
y
n
c
h
r
o
n
i
z
e
d
I
S
B
N
t
r
a
n
s
s
e
a
r
c
h
s
e
a
r
c
h
d
e
r
i
v
a
t
i
o
n
w
a
t
a
n
a
b
e
D
I
+
D
C
+
P
I
+
P
C
+
A
I
+
A
C
r
e
l
a
t
i
v
e
p
r
e
f
e
r
e
n
c
e
n
o
g
r
e
e
d
y
t
o
u
r
n
a
m
e
n
t
c
l
a
s
s
n
o
n
o
n
o
S
V
M
,
m
o
d
e
l
m
o
d
e
l
,
V
i
t
e
r
b
i
C
R
F
,
M
B
L
m
o
r
a
n
t
e
D
+
P
I
+
A
I
+
A
C
t
r
a
n
s
n
o
g
r
e
e
d
y
c
l
a
s
s
v
o
t
i
n
g
g
r
e
e
d
y
n
o
S
V
M
,
M
B
L
l
i
D
+
P
I
C
+
A
I
C
g
r
a
p
h
n
o
M
S
T
C
L
/
E
c
l
a
s
s
v
o
t
i
n
g
g
r
e
e
d
y
n
o
M
E
c
h
e
n
D
+
P
I
+
P
C
+
A
I
C
t
r
a
n
s
n
o
p
r
o
b
c
l
a
s
s
n
o
p
r
o
b
g
l
o
b
a
l
p
r
o
b
a
b
i
l
i
t
y
M
E
o
p
t
i
m
i
z
a
t
i
o
n
l
e
e
D
+
P
I
+
A
I
C
+
P
C
t
r
a
n
s
n
o
g
r
e
e
d
y
c
l
a
s
s
n
o
p
r
o
b
n
o
S
V
M
,
M
E
s
u
n
D
I
+
P
I
+
D
C
A
I
+
A
C
g
r
a
p
h
n
o
M
S
T
E
,
g
r
a
p
h
n
o
V
i
t
e
r
b
i
,
M
E
M
M
,
M
E
V
i
t
e
r
b
i
I
L
P
V
i
t
e
r
b
i
l
l
u
i
s
D
+
P
I
+
D
A
I
C
+
P
C
g
r
a
p
h
n
o
M
S
T
E
g
r
a
p
h
n
o
M
S
T
E
M
S
T
E
P
e
r
c
e
p
t
r
o
n
,
S
V
M
n
e
u
m
a
n
n
D
+
P
I
+
P
C
+
A
I
+
A
C
t
r
a
n
s
n
o
g
r
e
e
d
y
c
l
a
s
s
n
o
n
o
n
o
M
E
o
p
e
n
v
i
c
k
r
e
y
A
I
+
A
C
+
P
I
+
P
C
?
?
?
s
e
n
t
e
n
c
e
n
o
g
r
e
e
d
y
?
M
E
s
i
m
p
l
i
fi
c
a
t
i
o
n
,
c
l
a
s
s
r
i
e
d
e
l
P
A
I
C
?
?
?
M
a
r
k
o
v
n
o
C
u
t
t
i
n
g
?
M
I
R
A
L
o
g
i
c
P
l
a
n
e
N
e
t
w
o
r
k
w
a
n
g
P
I
+
A
I
C
t
r
a
n
s
,
n
o
g
r
e
e
d
y
,
c
l
a
s
s
n
o
p
r
o
b
n
o
S
V
M
,
M
E
g
r
a
p
h
M
S
T
C
L
/
E
M
I
R
A
T
a
b
l
e
1
2
:
S
u
m
m
a
r
y
o
f
s
y
s
t
e
m
a
r
c
h
i
t
e
c
t
u
r
e
s
t
h
a
t
p
a
r
t
i
c
i
p
a
t
e
d
i
n
t
h
e
c
l
o
s
e
d
a
n
d
o
p
e
n
c
h
a
l
l
e
n
g
e
s
.
T
h
e
c
l
o
s
e
d
-
c
h
a
l
l
e
n
g
e
s
y
s
t
e
m
s
a
r
e
s
o
r
t
e
d
b
y
m
a
c
r
o
l
a
b
e
l
e
d
F
1
s
c
o
r
e
o
n
t
h
e
W
S
J
+
B
r
o
w
n
c
o
r
p
u
s
.
B
e
c
a
u
s
e
s
o
m
e
o
p
e
n
-
c
h
a
l
l
e
n
g
e
s
y
s
t
e
m
s
d
i
d
n
o
t
i
m
p
l
e
m
e
n
t
s
y
n
t
a
c
t
i
c
p
a
r
s
i
n
g
,
t
h
e
s
e
s
y
s
t
e
m
s
a
r
e
s
o
r
t
e
d
b
y
l
a
b
e
l
e
d
F
1
s
c
o
r
e
o
f
t
h
e
s
e
m
a
n
t
i
c
d
e
p
e
n
d
e
n
c
i
e
s
o
n
t
h
e
W
S
J
+
B
r
o
w
n
c
o
r
p
u
s
.
O
n
l
y
t
h
e
s
y
s
t
e
m
s
t
h
a
t
h
a
v
e
a
c
o
r
r
e
s
p
o
n
d
i
n
g
p
a
p
e
r
i
n
t
h
e
p
r
o
c
e
e
d
i
n
g
s
a
r
e
i
n
c
l
u
d
e
d
.
S
y
s
t
e
m
s
t
h
a
t
p
a
r
t
i
c
i
p
a
t
e
d
i
n
b
o
t
h
c
h
a
l
l
e
n
g
e
s
a
r
e
l
i
s
t
e
d
o
n
l
y
i
n
t
h
e
c
l
o
s
e
d
c
h
a
l
l
e
n
g
e
.
A
c
r
o
n
y
m
s
u
s
e
d
:
D
-
s
y
n
t
a
c
t
i
c
d
e
p
e
n
d
e
n
c
i
e
s
,
P
-
p
r
e
d
i
c
a
t
e
,
A
-
a
r
g
u
m
e
n
t
,
I
-
i
d
e
n
t
i
fi
c
a
t
i
o
n
,
C
-
c
l
a
s
s
i
fi
c
a
t
i
o
n
.
O
v
e
r
a
l
l
a
r
c
h
.
s
t
a
n
d
s
f
o
r
t
h
e
c
o
m
p
l
e
t
e
s
y
s
t
e
m
a
r
c
h
i
t
e
c
t
u
r
e
;
D
A
r
c
h
.
s
t
a
n
d
s
f
o
r
t
h
e
a
r
c
h
i
t
e
c
t
u
r
e
o
f
t
h
e
s
y
n
t
a
c
t
i
c
p
a
r
s
e
r
;
D
C
o
m
b
.
i
n
d
i
c
a
t
e
s
i
f
t
h
e
fi
n
a
l
p
a
r
s
e
r
o
u
t
p
u
t
w
a
s
g
e
n
e
r
a
t
e
d
u
s
i
n
g
p
a
r
s
e
r
c
o
m
b
i
n
a
t
i
o
n
;
D
I
n
f
e
r
e
n
c
e
s
t
a
n
d
s
f
o
r
t
h
e
t
y
p
e
o
f
i
n
f
e
r
e
n
c
e
u
s
e
d
f
o
r
s
y
n
t
a
c
t
i
c
p
a
r
s
i
n
g
;
P
A
A
r
c
h
.
s
t
a
n
d
s
t
h
e
t
y
p
e
o
f
a
r
c
h
i
t
e
c
t
u
r
e
u
s
e
d
f
o
r
P
A
I
C
;
P
A
C
o
m
b
.
i
n
d
i
c
a
t
e
s
i
f
t
h
e
P
A
o
u
t
p
u
t
w
a
s
g
e
n
e
r
a
t
e
d
t
h
r
o
u
g
h
s
y
s
t
e
m
c
o
m
b
i
n
a
t
i
o
n
;
P
A
I
n
f
e
r
e
n
c
e
s
t
a
n
d
s
f
o
r
t
h
e
t
h
e
t
y
p
e
o
f
i
n
f
e
r
e
n
c
e
u
s
e
d
f
o
r
P
A
I
C
;
J
o
i
n
t
L
e
a
r
n
i
n
g
/
O
p
t
.
i
n
d
i
c
a
t
e
s
i
f
s
o
m
e
f
o
r
m
o
f
j
o
i
n
t
l
e
a
r
n
i
n
g
o
r
o
p
t
i
m
i
z
a
t
i
o
n
w
a
s
i
m
p
l
e
m
e
n
t
e
d
f
o
r
t
h
e
s
y
n
t
a
c
t
i
c
+
s
e
m
a
n
t
i
c
g
l
o
b
a
l
t
a
s
k
;
M
L
m
e
t
h
o
d
s
l
i
s
t
s
t
h
e
M
L
m
e
t
h
o
d
s
u
s
e
d
t
h
r
o
u
g
h
o
u
t
t
h
e
c
o
m
p
l
e
t
e
s
y
s
t
e
m
.
172
Exact Match Perfect Proposition F
1
(complete task) (semantic dependencies)
closed WSJ+Brown WSJ Brown WSJ+Brown WSJ Brown
johansson 12.46 (1) 12.46 12.68 54.12 (1) 56.12 36.90
che 10.37 (2) 10.21 11.50 48.05 (2) 50.15 30.90
ciaramita 9.27 (3) 9.04 10.80 46.05 (3) 48.05 28.61
zhao 9.20 (4) 9.00 10.56 43.19 (4) 45.23 26.14
henderson 8.11 (5) 7.75 10.33 39.24 (5) 40.64 27.51
watanabe 7.79 (6) 7.54 9.39 36.44 (6) 38.09 22.72
yuret 7.65 (7) 7.33 9.62 34.61 (9) 36.13 21.78
zhang 7.40 (8) 7.46 7.28 34.96 (8) 36.25 24.22
li 7.12 (9) 6.71 9.62 32.08 (10) 33.45 20.62
samuelsson 6.94 (10) 6.62 8.92 35.20 (7) 36.96 20.22
chen 6.83 (11) 6.46 9.15 31.02 (12) 32.08 22.14
lee 6.69 (12) 6.29 9.15 31.40 (11) 32.52 22.18
morante 6.44 (13) 6.04 8.92 30.41 (14) 31.97 17.49
sun 5.38 (14) 4.96 7.98 30.43 (13) 31.51 21.40
baldridge 5.24 (15) 4.92 7.28 25.35 (15) 26.57 15.26
choi 3.33 (16) 3.50 2.58 24.77 (16) 25.71 17.37
trandabat 3.26 (17) 3.08 4.46 6.59 (18) 6.81 4.76
lluis 2.55 (18) 1.96 6.10 16.07 (17) 16.46 13.00
neumann 0.11 (19) 0.12 0.23 0.30 (19) 0.31 0.20
open
vickrey ? ? ? 44.94 (1) 46.68 30.28
riedel ? ? ? 42.77 (2) 44.18 31.15
zhang 8.14 (1) 8.04 8.92 35.46 (3) 36.74 24.84
li 6.90 (2) 6.46 9.62 29.91 (4) 31.30 18.41
wang 5.17 (3) 5.12 5.63 18.63 (5) 20.31 7.09
Table 13: Exact Match and Perfect Proposition F
1
scores for runs submitted in the closed and open
challenges. The closed-challenge systems are sorted in descending order of Exact Match scores on
the WSJ+Brown corpus. Open-challenge submissions are sorted in descending order of the Perfect
Proposition F
1
score. The number in parentheses next to the WSJ+Brown scores indicates the system
rank according to the corresponding scoring measure.
6.1 Exact Match and Perfect Propositions
Table 13 lists the Exact Match and Perfect Propo-
sition F
1
scores for test runs submitted in both
challenges. Both these scores measure the capac-
ity of a system to correctly parse structures with
granularity much larger than a simple dependency,
i.e., entire sentences for Exact Match and complete
propositions for Perfect Proposition F
1
(see Sec-
tion 2.2.2 for a formal definition of these evalua-
tion measures). The table indicates that these val-
ues are much smaller than the scores previously
reported, e.g., labeled macro F
1
. This is to be
expected: the probability of an incorrectly parsed
unit (sentence or proposition) is much larger given
its granularity. However, the main purpose of this
analysis is to investigate if systems that focused
on joint learning or optimization performed bet-
ter than others with respect to these global mea-
sures. This indeed seems to be the case for at
least two systems. The system of Johansson and
Nugues (2008), which jointly optimizes the la-
beled F
1
score (for semantic dependencies) and
then the labeled macro F
1
score (for the complete
task), increases its distance from the next ranked
system: its Perfect Proposition F
1
score is over
6 points higher than the score of the second sys-
tem in Table 13. The system of Henderson et
al. (2008), which was designed for joint learning
of the complete task, improves its rank from eighth
to fifth compared to the official results (Table 10).
6.2 Nonprojectivity
Table 14 shows the unlabeled F1 scores for pre-
diction of nonprojective syntactic dependencies.
Since nonprojectivity is quite rare, many teams
chose to ignore this issue. The table shows only
those systems that submitted well-formed depen-
dency trees, and whose output contained at least
one nonprojective link. The small number of non-
projective links in the training set makes it hard to
learn to predict such links, and this is also reflected
in the figures. In general, the figures for nonpro-
jective wh-movements and split clauses are higher,
and they are also the most common types. Also,
they are detectable by fairly simple patterns, such
as the presence of a wh-word or a pair of commas.
173
System All wh-mov. SpCl SpNP
choi 25.43 49.49 45.47 8.72
lee 46.26 50.30 64.84 20.69
nugues 46.15 58.96 59.26 11.32
samuelsson 24.47 38.15 0 9.83
titov 42.32 50.56 48.71 0
zhang 13.39 5.71 12.33 7.3
Table 14: Unlabeled F1-measures for nonprojec-
tive links. Results are given for all links, wh-
movements, split clauses, and split noun phrases.
6.3 Normalized SRL Performance
Table 6.3 lists the scores for the semantic sub-
task measured as the ratio of the labeled F
1
score
and LAS. As previously mentioned, this score es-
timates the performance of the SRL component
independent of the performance of the syntactic
parser. This analysis is not a substitute for the
actual experiment where the SRL components are
evaluated using correct syntactic information but,
nevertheless, it indicates several interesting facts.
First, the ranking of the top three systems in Ta-
ble 10 changes: the system of Che et al (2008)
is now ranked first, and the system of Johansson
and Nugues (2008) is second. This shows that Che
et al have a relatively stronger SRL component,
whereas Johansson and Nugues developed a bet-
ter parser. Second, several other systems improved
their ranking compared to Table 10: e.g., chen
from position thirteenth to ninth and choi from six-
teenth to eighth. This indicates that these systems
were penalized in the official ranking mainly due
to the relative poor performance of their parsers.
Note that this experiment is relevant only for
systems that implemented pipeline architectures,
where the semantic components are in fact sep-
arated from the syntactic ones; this excludes the
systems that blended syntax with SRL: henderson,
sun, and lluis. Furthermore, systems that had sig-
nificantly lower scores in syntax will receive an un-
reasonable boost in ranking according to this mea-
sure. Fortunately, there was only one such outlier
in this evaluation (neumann), shown in gray in the
table.
6.4 PropBank versus NomBank
Table 16 lists the labeled F
1
scores for semantic
dependencies for two different views of the test-
ing data sets: for propositions centered around ver-
bal predicates, i.e., from PropBank, and for propo-
sitions centered around nominal predicates, i.e.,
from NomBank.
Labeled F
1
/ LAS
closed WSJ+Brown WSJ Brown
neumann 137.60 (1) 140.94 108.93
che 90.51 (2) 91.42 82.21
johansson 89.98 (3) 90.70 83.40
ciaramita 89.49 (4) 90.32 81.89
zhao 87.88 (5) 88.75 79.78
yuret 84.35 (6) 85.30 75.34
samuelsson 84.20 (7) 85.24 74.51
choi 83.52 (8) 83.63 82.64
chen 82.22 (9) 82.89 76.11
morante 81.92 (10) 82.73 74.43
zhang 81.67 (11) 82.45 74.46
henderson 81.66 (12) 82.32 75.47
watanabe 81.26 (13) 82.18 72.61
lee 81.01 (14) 81.63 75.33
li 80.69 (15) 81.53 73.23
baldridge 78.37 (16) 79.33 69.38
sun 77.68 (17) 78.29 72.15
lluis 75.77 (18) 76.20 72.24
trandabat 47.68 (19) 48.12 43.85
open
zhang 82.33 82.91 76.87
li 79.58 80.44 72.05
wang 79.38 82.35 55.37
Table 15: Ratio of the labeled F
1
score for seman-
tic dependencies and LAS for syntactic dependen-
cies. Systems are sorted in descending order of this
ratio score on the WSJ+Brown corpus. We only
show systems that participated in both the syntac-
tic and semantic subtasks.
The table indicates that, generally, systems per-
formed much worse on nominal predicates than
on verbal predicates. This is to be expected con-
sidering that there is significant body of previ-
ous work that analyzes the SRL problem on Prop-
Bank, but minimal work for NomBank. On aver-
age, the difference between the labeled F
1
scores
for verbal predicates and nominal predicates on the
WSJ+Brown corpus is 7.84 points. Furthermore,
the average difference between labeled F
1
scores
on the Brown corpus alone is 12.36 points. This in-
dicates that the problem of SRL for nominal predi-
cates is more sensitive to domain changes than the
equivalent problem for verbal predicates. Our con-
jecture is that, because there is very little syntac-
tic structure between nominal predicates and their
arguments, SRL models for nominal predicates se-
lect mainly lexical features, which are more brittle
than syntactic or other non-lexicalized features.
Remarkably, there is one system (baldridge)
which performed better on the WSJ+Brown for
nominal predicates than verbal predicates. Un-
fortunately, this group did not submit a system-
description paper so it is not clear what was their
approach.
174
Labeled F
1
Labeled F
1
(verbal predicates) (nominal predicates)
closed WSJ+Brown WSJ Brown WSJ+Brown WSJ Brown
johansson 84.45 (1) 86.37 71.87 74.32 (2) 75.42 60.13
che 80.46 (2) 82.17 69.33 75.18 (1) 76.64 56.87
ciaramita 80.15 (3) 82.09 67.62 73.17 (4) 74.42 57.69
zhao 77.67 (4) 79.40 66.38 73.28 (3) 74.69 54.81
samuelsson 76.17 (5) 78.03 64.00 68.13 (7) 69.58 49.24
yuret 75.91 (6) 77.88 63.02 68.81 (5) 69.98 53.58
zhang 74.82 (7) 76.62 63.15 65.61 (11) 66.82 50.18
li 74.36 (8) 76.14 62.92 62.61 (14) 63.76 47.09
henderson 73.80 (9) 75.40 63.36 66.26 (10) 67.44 50.73
watanabe 73.06 (10) 75.02 60.34 67.15 (8) 68.37 50.92
sun 72.97 (11) 74.45 63.50 58.68 (15) 59.73 45.75
morante 72.81 (12) 74.36 62.72 66.50 (9) 67.92 47.97
lee 72.34 (13) 74.15 60.49 62.83 (13) 63.66 52.18
chen 72.02 (14) 73.49 62.46 65.02 (12) 66.14 50.48
choi 70.00 (15) 71.28 61.71 56.16 (16) 57.19 44.05
baldridge 67.02 (16) 68.64 56.50 68.57 (6) 69.78 52.96
lluis 62.42 (17) 63.49 55.49 42.15 (17) 42.81 34.22
trandabat 42.88 (18) 43.79 37.06 37.14 (18) 37.89 27.50
neumann 22.87 (19) 23.53 18.24 21.7 (19) 22.04 17.14
open
vickrey 78.41 (1) 79.75 69.57 71.86 (1) 73.29 53.25
riedel 77.13 (2) 78.72 66.75 70.25 (2) 71.03 60.17
zhang 75.00 (3) 76.62 64.44 66.76 (3) 67.79 53.76
li 73.74 (4) 75.57 62.05 61.24 (5) 62.38 46.36
wang 67.50 (5) 70.34 49.72 66.53 (4) 69.83 28.96
Table 16: Labeled F
1
scores for frames centered around verbal and nominal predicates. The number in
parentheses next to the WSJ+Brown scores indicates the system rank in the corresponding data set.
Systems can mitigate the inherent differences
between verbal and nominal predicates with dif-
ferent models for the two sub-problems. This was
indeed the approach taken by two out of the top
three systems (johansson and che). Johansson and
Nugues (2008) developed different models for ver-
bal and nominal predicates and implemented sep-
arate feature selection processes for each model.
Che et al (2008) followed the same method but
they also implemented separate domain constraints
for inference for the two models.
7 Conclusion
The previous four CoNLL shared tasks popular-
ized and, without a doubt, boosted research in se-
mantic role labeling and dependency parsing. This
year?s shared task introduces a new task that es-
sentially unifies the problems addressed in the past
four years under a unique, dependency-based for-
malism. This novel task is attractive both from
a research perspective and an application-oriented
perspective:
? We believe that the proposed dependency-
based representation is a better fit for many
applications (e.g., Information Retrieval, In-
formation Extraction) where it is often suffi-
cient to identify the dependency between the
predicate and the head of the argument con-
stituent rather than extracting the complete ar-
gument constituent.
? It was shown that the extraction of syntac-
tic and semantic dependencies can be per-
formed with state-of-the-art performance in
linear time (Ciaramita et al, 2008). This can
give a significant boost to the adoption of this
technology in real-world applications.
? We hope that this shared task will motivate
several important research directions. For ex-
ample, is the dependency-based representa-
tion better for SRL than the constituent-based
formalism? Does joint learning improve syn-
tactic and semantic analysis?
? Surface (string related patterns, syntax, etc.)
linguistic features can often be detected with
greater reliability than deep (semantic) fea-
tures. In contrast, deep features can cover
more ground because they regularize across
differences in surface strings. Machine learn-
ing systems can be more effective by using
evidence from both deep and surface features
jointly (Zhao, 2005).
175
Even though this shared task was more complex
than the previous shared tasks, 22 different teams
submitted results in at least one of the challenges.
Building on this success, we hope to expand this
effort in the future with evaluations on multiple
languages and on larger out-of-domain corpora.
Acknowledgments
We want to thank the following people who helped
us with the generation of the data sets: Jes?us
Gim?enez, for generating the predicted POS tags
with his SVMTool POS tagger, and Massimiliano
Ciaramita, for generating columns 1, 2 and 3 in the
open-challenge corpus with his semantic tagger.
We also thank the following people who helped
us with the organization of the shared task: Paola
Merlo and James Henderson for the idea and the
implementation of the Exact Match measure, Se-
bastian Riedel for his dependency visualization
software,
12
Hai Zhao, for the the idea of the F
1
ratio score, and Carlos Castillo, for help with the
shared task website. Last but not least, we thank
the organizers of the previous four shared tasks:
Sabine Buchholz, Xavier Carreras, Ryan McDon-
ald, Amit Dubey, Johan Hall, Yuval Krymolowski,
Sandra K?ubler, Erwin Marsi, Jens Nilsson, Sebas-
tian Riedel, and Deniz Yuret. This shared task
would not have been possible without their previ-
ous effort.
Mihai Surdeanu is a research fellow in the
Ram?on y Cajal program of the Spanish Ministry of
Science and Technology. Richard Johansson was
funded by the Swedish National Graduate School
of Language Technology (GSLT). Adam Meyers?
participation was supported by the National Sci-
ence Foundation, award CNS-0551615 (Towards
a Comprehensive Linguistic Annotation of Lan-
guage) and IIS-0534700 (Collaborative Research:
Structure Alignment-based Machine Translation).
Llu??s M`arquez?s participation was supported by
the Spanish Ministry of Education and Science,
through research projects Trangram (TIN2004-
07925-C03-02) and OpenMT (TIN2006-15307-
C03-02).
References
X. Carreras. 2007. Experiments with a Higher-Order
Projective Dependency Parser. In Proc. of CoNLL-
2007 Shared Task.
12
http://code.google.com/p/whatswrong/
X. Carreras and L. M`arquez. 2005. Introduction to the
CoNLL-2005 Shared Task: Semantic Role Labeling.
In Proc. of CoNLL-2005.
X. Carreras and L. M`arquez. 2004. Introduction to the
CoNLL-2004 Shared Task: Semantic Role Labeling.
In Proc. of CoNLL-2004.
W. Che, Z. Li, Y. Hu, Y. Li, B. Qin, T. Liu and S.
Li. 2008. A Cascaded Syntactic and Semantic De-
pendency Parsing System. In Proc. of CoNLL-2008
Shared Task.
E. Chen, L. Shi and D. Hu. 2008. Probabilistic Model
for Syntactic and Semantic Dependency Parsing. In
Proc. of CoNLL-2008 Shared Task.
Chinchor, N. and P. Robinson. 1998. MUC-7
Named Entity Task Definition. In Proc. of Seventh
Message Understanding Conference (MUC-7).
http://www.itl.nist.gov/iaui/894.02/related projects/
/muc/proceedings/muc 7 toc.html.
Chomsky, Noam. 1981. Lectures on Government and
Binding. Foris Publications, Dordrecht.
Y.J. Chu and T.H. Liu. 1965. On the Shortest Ar-
borescence of a Directed Graph. In Science Sinica,
14:1396-1400.
M. Ciaramita, G. Attardi, F. Dell?Orletta, and M. Sur-
deanu. 2008. DeSRL: A Linear-Time Semantic
Role Labeling System. In Proc. of CoNLL-2008
Shared Task.
M. Ciaramita and Y. Altun. 2006. Broad Coverage
Sense Disambiguation and Information Extraction
with a Supersense Sequence Tagger. In Proc. of
EMNLP.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
J. Edmonds. 1967. Optimum Branchings. In Jour-
nal of Research of the National Bureau of Standards,
71B:233?240.
J. Eisner. 2000. Bilexical Grammars and Their Cubic-
Time Parsing Algorithms. New Developments in
Parsing Algorithms, Kluwer Academic Publishers.
W. N. Francis and H. Ku?cera. 1964. Brown Corpus.
Manual of Information to accompany A Standard
Corpus of Present-Day Edited American English, for
use with Digital Computers. Revised 1971, Revised
and Amplified 1979.
C. Fellbaum, editor. 1998. WordNet: An electronic
lexical database. MIT Press.
J. Gim?enez and L. M`arquez. 2004. SVMTool: A gen-
eral POS tagger generator based on Support Vector
Machines. In Proc. of LREC.
K. Hacioglu. 2004. Semantic Role Labeling Using De-
pendency Trees. In Proc. of COLING-2004.
176
J. Henderson, P. Merlo, G. Musillo and I. Titov. 2008.
A Latent Variable Model of Synchronous Parsing for
Syntactic and Semantic Dependencies. In Proc. of
CoNLL-2008 Shared Task.
R. Johansson and P. Nugues. 2008. Dependency-
based Syntactic?Semantic Analysis with PropBank
and NomBank. In Proc. of CoNLL-2008 Shared
Task.
R. Johansson and P. Nugues. 2007. Extended
Constituent-to-Dependency Conversion for English.
In Proc. of NODALIDA.
X. Llu??s and L. M`arquez. 2008. A Joint Model for
Parsing Syntactic and Semantic Dependencies. In
Proc. of CoNLL-2008 Shared Task.
D. Magerman. 1994. Natural Language Parsing as Sta-
tistical Pattern Recognition. Ph.D. thesis, Stanford
University.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1993. Building a Large Annotated Corpus of En-
glish: the Penn Treebank. Computational Linguis-
tics, 19.
R. McDonald, F. Pereira, K. Ribarov and J. Hajic.
2005. Non-Projective Dependency Parsing using
Spanning Tree Algorithms In Proc. of HLT-EMNLP.
A. Meyers, R. Grishman, M. Kosaka, and S. Zhao.
2001. Covering Treebanks with GLARF. In Proc.
of the ACL/EACL 2001 Workshop on Sharing Tools
and Resources for Research and Education.
Meyers, A., R. Reeves, C. Macleod, R. Szekely,
V. Zielinska, B. Young, and R. Grishman. 2004.
The NomBank Project: An Interim Report. In
NAACL/HLT 2004 Workshop Frontiers in Corpus
Annotation, Boston.
J. Nivre, J. Hall, J. Nilsson and G. Eryigit. 2006. La-
beled Pseudo-Projective Dependency Parsing with
Support Vector Machines. In Proc. of CoNLL-X
Shared Task.
J. Nivre, J. Hall, S. K?ubler, R. McDonald, J. Nilsson,
S. Riedel, D. Yuret. 2007. The CoNLL 2007 Shared
Task on Dependency Parsing. In Proc. of CoNLL-
2007.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryi?git, S.
K?ubler, S. Marinov, and E. Marsi. 2007b. Malt-
Parser: A language-independent system for data-
driven dependency parsing. Natural Language En-
gineering, 13(2):95?135.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An Annotated Corpus of Seman-
tic Roles. Computational Linguistics, 31(1).
S. Riedel and I. Meza-Ruiz. 2008. Collective Seman-
tic Role Labelling with Markov Logic. In Proc. of
CoNLL-2008 Shared Task.
Y. Samuelsson, O. T?ackstr?om, S. Velupillai, J. Eklund,
M. Fishel and M. Saers. 2008. Mixing and Blending
Syntactic and Semantic Dependencies. In Proc. of
CoNLL-2008 Shared Task.
W. Sun, H. Li and Z. Sui. 2008. The Integration of De-
pendency Relation Classification and Semantic Role
Labeling Using Bilayer Maximum Entropy Markov
Models. In Proc. of CoNLL-2008 Shared Task.
E. F. Tjong Kim San and F. De Meulder. 2003. Intro-
duction to the CoNLL-2003 Shared Task: Language-
Independent Named Entity Recognition. In Proc. of
CoNLL-2003.
D. Vickrey and D. Koller. 2008. Applying Sentence
Simplification to the CoNLL-2008 Shared Task. In
Proc. of CoNLL-2008 Shared Task.
R. Weischedel and A. Brunstein. 2005. BBN pronoun
coreference and entity type corpus. Technical report,
Linguistic Data Consortium.
H. Yamada and Y. Matsumoto. 2003. Statistical De-
pendency Analysis with Support Vector Machines.
In Proc. of IWPT.
Y. Zhang, R. Wang and H. Uszkoreit. 2008. Hybrid
Learning of Dependency Structures from Heteroge-
neous Linguistic Resources. In Proc. of CoNLL-
2008 Shared Task.
Zhao, S. 2005. Information Extraction from Multiple
Syntactic Sources. Ph.D. thesis, NYU.
177
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 183?187
Manchester, August 2008
Dependency-based Syntactic?Semantic Analysis with PropBank and
NomBank
Richard Johansson and Pierre Nugues
Lund University, Sweden
{richard, pierre}@cs.lth.se
Abstract
This paper presents our contribution in the
closed track of the 2008 CoNLL Shared
Task (Surdeanu et al, 2008). To tackle the
problem of joint syntactic?semantic anal-
ysis, the system relies on a syntactic and
a semantic subcomponent. The syntactic
model is a bottom-up projective parser us-
ing pseudo-projective transformations, and
the semantic model uses global inference
mechanisms on top of a pipeline of clas-
sifiers. The complete syntactic?semantic
output is selected from a candidate pool
generated by the subsystems.
The system achieved the top score in the
closed challenge: a labeled syntactic accu-
racy of 89.32%, a labeled semantic F1 of
81.65, and a labeled macro F1 of 85.49.
1 Introduction: Syntactic?Semantic
Analysis
Intuitively, semantic interpretation should help
syntactic disambiguation, and joint syntactic?
semantic analysis has a long tradition in linguis-
tic theory. This motivates a statistical modeling of
the problem of finding a syntactic tree y?
syn
and a
semantic graph y?
sem
for a sentence x as maximiz-
ing a function F that scores the joint syntactic?
semantic structure:
?y?
syn
, y?
sem
? = arg max
y
syn
,y
sem
F (x, y
syn
, y
sem
)
The dependencies in the feature representation
used to compute F determine the tractability of the
search procedure needed to perform the maximiza-
tion. To be able to use complex syntactic features
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
such as paths when predicting semantic structures,
exact search is clearly intractable. This is true even
with simpler feature representations ? the problem
is a special case of multi-headed dependency anal-
ysis, which is NP-hard even if the number of heads
is bounded (Chickering et al, 1994).
This means that we must resort to a simplifica-
tion such as an incremental method or a reranking
approach. We chose the latter option and thus cre-
ated syntactic and semantic submodels. The joint
syntactic?semantic prediction is selected from a
small list of candidates generated by the respective
subsystems.
2 Syntactic Submodel
We model the process of syntactic parsing of
a sentence x as finding the parse tree y?
syn
=
argmax
y
F (x, y) that maximizes a scoring func-
tion F . The learning problem consists of fitting
this function so that the cost of the predictions is
as low as possible according to a cost function ?.
In this work, we consider linear scoring functions
of the following form:
F (x, y) = w ??(x, y)
where ?(x, y) is a numeric feature representation
of the pair (x, y) andw a vector of feature weights.
We defined the syntactic cost ? as the sum of link
costs, where the link cost was 0 for a correct de-
pendency link with a correct label, 0.5 for a correct
link with an incorrect label, and 1 for an incorrect
link.
A widely used framework for fitting the weight
vector is the max-margin model (Taskar et al,
2003), which is a generalization of the well-
known support vector machines to general cost-
based prediction problems. Since the large num-
ber of training examples and features in our case
make an exact solution of the max-margin opti-
mization problem impractical, we used the on-
line passive?aggressive algorithm (Crammer et al,
183
2006), which approximates the optimization pro-
cess in two ways:
? The weight vector w is updated incremen-
tally, one example at a time.
? For each example, only the most violated con-
straint is considered.
The algorithm is a margin-based variant of the per-
ceptron (preliminary experiments show that it out-
performs the ordinary perceptron on this task). Al-
gorithm 1 shows pseudocode for the algorithm.
Algorithm 1 The Online PA Algorithm
input Training set T = {(x
t
, y
t
)}
T
t=1
Number of iterations N
Regularization parameter C
Initialize w to zeros
repeat N times
for (x
t
, y
t
) in T
let y?
t
= argmax
y
F (x
t
, y) + ?(y
t
, y)
let ?
t
= min
?
C,
F (x
t
,y?
t
)?F (x
t
,y
t
)+?(y
t
,y?
t
)
??(x,y
t
)??(x,y?
t
)?
2
?
w ? w + ?
t
(?(x, y
t
)??(x, y?
t
))
returnwaverage
We used a C value of 0.01, and the number of
iterations was 6.
2.1 Features and Search
The feature function ? is a second-order edge-
factored representation (McDonald and Pereira,
2006; Carreras, 2007). The second-order repre-
sentation allows us to express features not only of
head?dependent links, but also of siblings and chil-
dren of the dependent. This feature set forces us
to adopt the expensive search procedure by Car-
reras (2007), which extends Eisner?s span-based
dynamic programming algorithm (1996) to allow
second-order feature dependencies. Since the cost
function ? is based on the cost of single links, this
procedure can also be used to find the maximizer
of F (x
i
, y
ij
)+?(y
i
, y
ij
), which is needed at train-
ing time. The search was constrained to disallow
multiple root links.
2.2 Handling Nonprojective Links
Although only 0.4% of the links in the training set
are nonprojective, 7.6% of the sentences contain at
least one nonprojective link. Many of these links
represent long-range dependencies ? such as wh-
movement ? that are valuable for semantic pro-
cessing. Nonprojectivity cannot be handled by
span-based dynamic programming algorithms. For
parsers that consider features of single links only,
the Chu-Liu/Edmonds algorithm can be used in-
stead. However, this algorithm cannot be gen-
eralized to the second-order setting ? McDonald
and Pereira (2006) proved that this problem is NP-
hard, and described an approximate greedy search
algorithm.
To simplify implementation, we instead opted
for the pseudo-projective approach (Nivre and
Nilsson, 2005), in which nonprojective links are
lifted upwards in the tree to achieve projectivity,
and special trace labels are used to enable recovery
of the nonprojective links at parse time. The use
of trace labels in the pseudo-projective transfor-
mation leads to a proliferation of edge label types:
from 69 to 234 in the training set, many of which
occur only once. Since the running time of our
parser depends on the number of labels, we used
only the 20 most frequent trace labels.
3 Semantic Submodel
Our semantic model consists of three parts:
? A SRL classifier pipeline that generates a list
of candidate predicate?argument structures.
? A constraint system that filters the candidate
list to enforce linguistic restrictions on the
global configuration of arguments.
? A global classifier that rescores the predicate?
argument structures in the filtered candidate
list.
Rather than training the models on gold-
standard syntactic input, we created an automati-
cally parsed training set by 5-fold cross-validation.
Training on automatic syntax makes the semantic
classifiers more resilient to parsing errors, in par-
ticular adjunct labeling errors.
3.1 SRL Pipeline
The SRL pipeline consists of classifiers for predi-
cate identification, predicate disambiguation, sup-
port identification (for noun predicates), argument
identification, and argument classification. We
trained one set of classifiers for verb predicates
and another for noun predicates. For the pred-
icate disambiguation classifiers, we trained one
subclassifier for each lemma. All classifiers in the
pipeline were L2-regularized linear logistic regres-
sion classifiers, implemented using the efficient
LIBLINEAR package (Lin et al, 2008). For multi-
class problems, we used the one-vs-all binarization
184
method, which makes it easy to prevent outputs not
allowed by the PropBank or NomBank frame.
Since our classifiers were logistic, their output
values could be meaningfully interpreted as prob-
abilities. This allowed us to combine the scores
from subclassifiers into a score for the complete
predicate?argument structure. To generate the can-
didate lists used by the global SRL models, we ap-
plied beam search based on these scores using a
beam width of 4.
The features used by the classifiers are listed in
Tables 1 and 2. In the tables, the features used
by the classifiers for noun and verb predicates are
indicated by N and V, respectively. We selected the
feature sets by greedy forward subset selection.
Feature PredId PredDis
PREDWORD N,V N,V
PREDLEMMA N,V N,V
PREDPARENTWORD/POS N,V N,V
CHILDDEPSET N,V N,V
CHILDWORDSET N,V N,V
CHILDWORDDEPSET N,V N,V
CHILDPOSSET N,V N,V
CHILDPOSDEPSET N,V N,V
DEPSUBCAT N,V N,V
PREDRELTOPARENT N,V N,V
Table 1: Classifier features in predicate identifica-
tion and disambiguation.
Feature Supp ArgId ArgCl
PREDPARENTWORD/POS N N,V
CHILDDEPSET N N,V N,V
PREDLEMMASENSE N N,V N,V
VOICE V V
POSITION N N,V N,V
ARGWORD/POS N N,V N,V
LEFTWORD/POS N N,V
RIGHTWORD/POS N N,V N,V
LEFTSIBLINGWORD/POS N,V
RIGHTSIBLINGWORD/POS N N
PREDPOS N N,V V
RELPATH N N,V N,V
POSPATH N
RELPATHTOSUPPORT N N
VERBCHAINHASSUBJ V V
CONTROLLERHASOBJ V N
PREDRELTOPARENT N N,V N,V
FUNCTION N,V
Table 2: Classifier features in argument identifica-
tion and classification and support detection.
Features Used in Predicate Identification and
Disambiguation
PREDWORD, PREDLEMMA. The lexical form
and lemma of the predicate.
PREDPARENTWORD and PREDPARENTPOS.
Form and part-of-speech tag of the parent
node of the predicate.
CHILDDEPSET, CHILDWORDSET, CHILD-
WORDDEPSET, CHILDPOSSET, CHILD-
POSDEPSET. These features represent the
set of dependents of the predicate using
combinations of dependency labels, words,
and parts of speech.
DEPSUBCAT. Subcategorization frame: the con-
catenation of the dependency labels of the
predicate dependents.
PREDRELTOPARENT. Dependency relation be-
tween the predicate and its parent.
Features Used in Argument Identification and
Classification
PREDLEMMASENSE. The lemma and sense
number of the predicate, e.g. give.01.
VOICE. For verbs, this feature is Active or Pas-
sive. For nouns, it is not defined.
POSITION. Position of the argument with respect
to the predicate: Before, After, or On.
ARGWORD and ARGPOS. Lexical form and
part-of-speech tag of the argument node.
LEFTWORD, LEFTPOS, RIGHTWORD, RIGHT-
POS. Form/part-of-speech tag of the left-
most/rightmost dependent of the argument.
LEFTSIBLINGWORD, LEFTSIBLINGPOS,
RIGHTSIBLINGWORD, RIGHTSIBLING-
POS. Form/part-of-speech tag of the
left/right sibling of the argument.
PREDPOS. Part-of-speech tag of the predicate.
RELPATH. A representation of the complex
grammatical relation between the predicate
and the argument. It consists of the sequence
of dependency relation labels and link direc-
tions in the path between predicate and argu-
ment, e.g. IM?OPRD?OBJ?.
POSPATH. An alternative view of the grammat-
ical relation, which consists of the POS tags
passed when moving from predicate to argu-
ment, e.g. VB?TO?VBP?PRP.
RELPATHTOSUPPORT. The RELPATH from the
argument to a support chain.
VERBCHAINHASSUBJ. Binary feature that is set
to true if the predicate verb chain has a sub-
ject. The purpose of this feature is to resolve
verb coordination ambiguity as in Figure 1.
CONTROLLERHASOBJ. Binary feature that is
true if the link between the predicate verb
chain and its parent is OPRD, and the parent
has an object. This feature is meant to resolve
control ambiguity as in Figure 2.
185
FUNCTION. The grammatical function of the ar-
gument node. For direct dependents of the
predicate, this is identical to the RELPATH.
I
SBJ
eat drinkyouand
COORD SBJ
CONJROOT
SBJ COORD
ROOT
drinkandeatI
CONJ
Figure 1: Coordination ambiguity: The subject I is
in an ambiguous position with respect to drink.
I to
IMSBJ
want sleephim
OBJ
OPRD
ROOT
IM
sleepI
SBJ
want
ROOT
to
OPRD
Figure 2: Subject/object control ambiguity: I is in
an ambiguous position with respect to sleep.
3.2 Linguistically Motivated Global
Constraints
The following three global constraints were used
to filter the candidates generated by the pipeline.
CORE ARGUMENT CONSISTENCY. Core argu-
ment labels must not appear more than once.
DISCONTINUITY CONSISTENCY. If there is a la-
bel C-X, it must be preceded by a label X.
REFERENCE CONSISTENCY. If there is a label
R-X and the label is inside a relative clause, it
must be preceded by a label X.
3.3 Global SRL Model
Toutanova et al (2005) have showed that a
global model that scores the complete predicate?
argument structure can lead to substantial perfor-
mance gains. We therefore created a global SRL
classifier using the following global features in ad-
dition to the features from the pipeline:
CORE ARGUMENT LABEL SEQUENCE. The
complete sequence of core argument labels.
The sequence also includes the predicate and
voice, for instance A0+break.01/Active+A1.
MISSING CORE ARGUMENT LABELS. The set
of core argument labels declared in the Prop-
Bank/NomBank frame that are not present in
the predicate?argument structure.
Similarly to the syntactic submodel, we trained
the global SRL model using the online passive?
aggressive algorithm. The cost function ? was
defined as the number of incorrect links in the
predicate?argument structure. The number of it-
erations was 20 and the regularization parameter
C was 0.01. Interestingly, we noted that the global
SRL model outperformed the pipeline even when
no global features were added. This shows that the
global learning model can correct label bias prob-
lems introduced by the pipeline architecture.
4 Syntactic?Semantic Integration
Our baseline joint feature representation contained
only three features: the log probability of the syn-
tactic tree and the log probability of the semantic
structure according to the pipeline and the global
model, respectively. This model was trained on the
complete training set using cross-validation. The
probabilities were obtained using the multinomial
logistic function (?softmax?).
We carried out an initial experiment with a more
complex joint feature representation, but failed to
improve over the baseline. Time prevented us from
exploring this direction conclusively.
5 Results
The submitted results on the development and test
corpora are presented in the upper part of Table 3.
After the submission deadline, we corrected a bug
in the predicate identification method. This re-
sulted in improved results shown in the lower part.
Corpus Syn acc Sem F1 Macro F1
Development 88.47 80.80 84.66
Test WSJ 90.13 81.75 85.95
Test Brown 82.81 69.06 75.95
Test WSJ + Brown 89.32 80.37 84.86
Development 88.47 81.86 85.17
Test WSJ 90.13 83.75 86.61
Test Brown 82.84 69.85 76.34
Test WSJ + Brown 89.32 81.65 85.49
Table 3: Results.
5.1 Syntactic Results
Table 4 shows the effect of adding second-order
features to the parser in terms of accuracy as well
as training and parsing time on a Mac Pro, 3.2
GHz. The training times were measured on the
complete training set and the parsing time and ac-
curacies on the development set. Similarly to Car-
reras (2007), we see that these features have a very
large impact on parsing accuracy, but also that the
parser pays dearly in terms of efficiency as the
search complexity increases fromO(n3) toO(n4).
186
Since the low efficiency of the second-order parser
restricts its use to batch applications, we see an in-
teresting research direction to find suitable com-
promises between the two approaches, for instance
by sacrificing the exact search procedure.
System Training Parse Labeled Unlabeled
1st order 65 min 28 sec 85.78 89.51
2nd order 60 hours 34 min 88.33 91.43
Table 4: Impact of second-order features.
Table 5 shows the dependency types most af-
fected by the addition of second-order features to
the parser when ordered by the increase in F1. As
can be seen, they are all verb adjunct categories,
which demonstrates the effect of grandchild fea-
tures on PP attachment and labeling.
Label ?R ?P ?F
1
TMP 14.7 12.9 13.9
DTV 0 19.9 10.5
LOC 7.8 12.3 9.9
PRP 12.4 6.7 9.6
DIR 5.9 7.2 6.5
Table 5: Labels affected by second-order features.
5.2 Semantic Results
To assess the effect of the components in the se-
mantic submodel, we tested their performance on
the top-scoring parses from the syntactic model.
Table 6 shows the results. The baseline system
consists of the SRL pipeline only (P). Adding lin-
guistic constraints (C) results in a more precision-
oriented system with slightly lower recall, but sig-
nificantly higher F1. Even higher performance is
obtained when adding the global SRL model (G).
System P R F1
P 80.74 77.98 79.33
P+C 82.42 77.66 79.97
P+C+G 83.64 78.14 80.40
Table 6: SRL results on the top-scoring parse trees.
5.3 Syntactic?Semantic Integration
The final experiment concerned the integration of
syntactic and semantic analysis. In this setting,
the system chooses the output that maximizes the
joint syntactic?semantic score, based on the top N
syntactic trees. Table 7 shows the results on the
development set. We see that syntactic?semantic
integration improves both syntactic accuracy and
semantic F1. This holds for the constraint-based
SRL system as well as for the full system.
Sem model N Syn acc Sem F1 Macro F1
P+C 1 88.33 79.97 84.17
P+C 16 88.42 80.42 84.44
P+C+G 1 88.33 80.40 84.39
P+C+G 16 88.47 80.80 84.66
Table 7: Syntactic?semantic integration.
6 Conclusion
We have described a system1 for syntactic and se-
mantic dependency analysis based on PropBank
and NomBank, and detailed the implementation
of its subsystems. Crucial to our success was the
high performance of the syntactic parser, which
achieved a high accuracy. In addition, we recon-
firmed the benefits of global inference in semantic
analysis: both constraint-based and learning-based
methods resulted in improvements over a baseline.
Finally, we showed that integration of syntactic
and semantic analysis is beneficial for both sub-
tasks. We hope that this shared task will spur fur-
ther research that leads to new feature representa-
tions and search procedures to handle the problem
of joint syntactic and semantic analysis.
References
Carreras, Xavier. 2007. Experiments with a higher-order pro-
jective dependency parser. In Proceedings of CoNLL.
Chickering, David M., Dan Geiger, and David Heckerman.
1994. Learning Bayesian networks: The combination of
knowledge and statistical data. Technical Report MSR-
TR-94-09, Microsoft Research.
Crammer, Koby, Ofer Dekel, Joseph Keshet, Shai Shalev-
Schwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. JMLR, 2006(7):551?585.
Eisner, Jason M. 1996. Three new probabilistic models for
dependency parsing: An exploration. In Proc. of ICCL.
Lin, Chih-Jen, Ruby C. Weng, and S. Sathiya Keerthi. 2008.
Trust region Newton method for large-scale logistic regres-
sion. JMLR, 2008(9):627?650.
McDonald, Ryan and Fernando Pereira. 2006. Online learn-
ing of approximate dependency parsing algorithms. In
Proceedings of EACL-2006.
Nivre, Joakim and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In Proceedings of ACL-2005.
Surdeanu, Mihai, Richard Johansson, Adam Meyers, Llu?s
M?rquez, and Joakim Nivre. 2008. The CoNLL?2008
shared task on joint parsing of syntactic and semantic de-
pendencies. In Proceedings of CoNLL?2008.
Taskar, Ben, Carlos Guestrin, and Daphne Koller. 2003.
Max-margin Markov networks. In Proceedings of NIPS.
Toutanova, Kristina, Aria Haghighi, and Christopher D. Man-
ning. 2005. Joint learning improves semantic role label-
ing. In Proceedings of ACL-2005.
1Our system is freely available for download at
http://nlp.cs.lth.se/lth_srl.
187
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 67?76,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Syntactic and Semantic Structure for Opinion Expression Detection
Richard Johansson and Alessandro Moschitti
DISI, University of Trento
Via Sommarive 14 Povo, 38123 Trento (TN), Italy
{johansson, moschitti}@disi.unitn.it
Abstract
We demonstrate that relational features
derived from dependency-syntactic and
semantic role structures are useful for the
task of detecting opinionated expressions
in natural-language text, significantly im-
proving over conventional models based
on sequence labeling with local features.
These features allow us to model the way
opinionated expressions interact in a sen-
tence over arbitrary distances.
While the relational features make the pre-
diction task more computationally expen-
sive, we show that it can be tackled effec-
tively by using a reranker. We evaluate
a number of machine learning approaches
for the reranker, and the best model re-
sults in a 10-point absolute improvement
in soft recall on the MPQA corpus, while
decreasing precision only slightly.
1 Introduction
The automatic detection and analysis of opinion-
ated text ? subjectivity analysis ? is potentially
useful for a number of natural language processing
tasks. Examples include retrieval systems answer-
ing queries about how a particular person feels
about a product or political question, and various
types of market analysis tools such as review min-
ing systems.
A primary task in subjectivity analysis is to
mark up the opinionated expressions, i.e. the
text snippets signaling the subjective content of
the text. This is necessary for further analysis,
such as the determination of opinion holder and
the polarity of the opinion. The MPQA corpus
(Wiebe et al, 2005), a widely used corpus anno-
tated with subjectivity information, defines two
types of subjective expressions: direct subjective
expressions (DSEs), which are explicit mentions
of opinion, and expressive subjective elements
(ESEs), which signal the attitude of the speaker
by the choice of words. DSEs are often verbs of
statement and categorization, where the opinion
and its holder tend to be direct semantic arguments
of the verb. ESEs, on the other hand, are less easy
to categorize syntactically; prototypical examples
would include value-expressing adjectives such
as beautiful, biased, etc. In addition to DSEs and
ESEs, the MPQA corpus also contains annotation
for non-subjective statements, which are referred
to as objective speech events (OSEs). Examples
(1) and (2) show two sentences from the MPQA
corpus where DSEs and ESEs have been manually
annotated.
(1) For instance, he [denounced]DSE as a [human
rights violation]ESE the banning and seizure of
satellite dishes in Iran.
(2) This [is viewed]DSE as the [main
impediment]ESE to the establishment of po-
litical order in the country .
The task of marking up these expressions has
usually been approached using straightforward
sequence labeling techniques using simple fea-
tures in a small contextual window (Choi et al,
2006; Breck et al, 2007). However, due to the
simplicity of the feature sets, this approach fails
to take into account the fact that the semantic
and pragmatic interpretation of sentences is not
only determined by words but also by syntactic
and shallow-semantic relations. Crucially, taking
grammatical relations into account allows us to
model how expressions interact in various ways
that influence their interpretation as subjective
or not. Consider, for instance, the word said in
examples (3) and (4) below, where the interpre-
tation as a DSE or an OSE is influenced by the
subjective content of the enclosed statement.
67
(3) ?We will identify the [culprits]ESE of these
clashes and [punish]ESE them,? he [said]DSE .
(4) On Monday, 80 Libyan soldiers disembarked
from an Antonov transport plane carrying military
equipment, an African diplomat [said]OSE .
In this paper, we demonstrate how syntactic
and semantic structural information can be used
to improve opinion detection. While this fea-
ture model makes it impossible to use the stan-
dard sequence labeling method, we show that with
a simple strategy based on reranking, incorporat-
ing structural features results in a significant im-
provement. We investigate two different reranking
strategies: the Preference Kernel approach (Shen
and Joshi, 2003) and an approach based on struc-
ture learning (Collins, 2002). In an evaluation
on the MPQA corpus, the best system we evalu-
ated, a structure learning-based reranker using the
Passive?Aggressive learning algorithm, achieved
a 10-point absolute improvement in soft recall,
and a 5-point improvement in F-measure, over the
baseline sequence labeler .
2 Motivation and Related Work
Most approaches to analysing the sentiment of
natural-language text have relied fundamentally
on purely lexical information (see (Pang et al,
2002; Yu and Hatzivassiloglou, 2003), inter alia)
or low-level grammatical information such as part-
of-speech tags and functional words (Wiebe et al,
1999). This is in line with the general consensus
in the information retrieval community that very
little can be gained by complex linguistic process-
ing for tasks such as text categorization and search
(Moschitti and Basili, 2004).
However, it has been suggested that subjectiv-
ity analysis is inherently more subtle than cate-
gorization and that structural linguistic informa-
tion should therefore be given more attention in
this context. For instance, Karlgren et al (2010)
argued from a Construction Grammar viewpoint
(Croft, 2005) that grammatical constructions not
only connect words, but can also be viewed as lex-
ical items in their own right. Starting from this
intuition, they showed that incorporating construc-
tion items into a bag-of-words feature representa-
tion resulted in improved results on a number of
coarse-grained opinion analysis tasks. These con-
structional features were domain-independent and
were manually extracted from dependency parse
trees. They found that the most prominent con-
structional feature for subjectivity analysis was the
Tense Shift construction.
While the position by Karlgren et al (2010)
? that constructional features signal opinion ?
originates from a particular theoretical framework
and may be controversial, syntactic and shallow-
semantic relations have repeatedly proven useful
for subtasks of subjectivity analysis that are in-
herently relational, above all for determining the
holder or topic of a given opinion. Works us-
ing syntactic features to extract topics and holders
of opinions are numerous (Bethard et al, 2005;
Kobayashi et al, 2007; Joshi and Penstein-Rose?,
2009; Wu et al, 2009). Semantic role analysis has
also proven useful: Kim and Hovy (2006) used
a FrameNet-based semantic role labeler to deter-
mine holder and topic of opinions. Similarly, Choi
et al (2006) successfully used a PropBank-based
semantic role labeler for opinion holder extrac-
tion, and Wiegand and Klakow (2010) recently ap-
plied tree kernel learning methods on a combina-
tion of syntactic and semantic role trees for the
same task. Ruppenhofer et al (2008) argued that
semantic role techniques are useful but not com-
pletely sufficient for holder and topic identifica-
tion, and that other linguistic phenomena must be
studied as well. One such linguistic pheonomenon
is the discourse structure, which has recently at-
tracted some attention in the opinion analysis com-
munity (Somasundaran et al, 2009).
3 Opinion Expression Detection Using
Syntactic and Semantic Structures
Previous systems for opinionated expression
markup have typically used simple feature sets
which have allowed the use of efficient off-the-
shelf sequence labeling methods based on Viterbi
search (Choi et al, 2006; Breck et al, 2007). This
is not possible in our case since we would like to
extract structural, relational features that involve
pairs of opinionated expressions and may apply
over an arbitrarily long distance in the sentence.
While it is possible that search algorithms for
exact or approximate inference can be construc-
tured for the arg max problem in this model, we
sidestepped this issue by using a reranking decom-
position of the problem: We first apply a standard
Viterbi-based sequence labeler using no structural
features and generate a small candidate set of size
k. Then, a second and more complex model picks
68
the top candidate from this set without having to
search the whole candidate space.
The advantages of a reranking approach com-
pared to more complex approaches requiring ad-
vanced search techniques are mainly simplicity
and efficiency: this approach is conceptually sim-
ple and fairly easy to implement provided that k-
best output can be generated efficiently, and fea-
tures can be arbitrarily complex ? we don?t have
to think about how the features affect the algorith-
mic complexity of the inference step. A common
objection to reranking is that the candidate set may
not be diverse enough to allow for much improve-
ment unless it is very large; the candidates may
be trivial variations that are all very similar to the
top-scoring candidate (Huang, 2008).
3.1 Syntactic and Semantic Structures
We used the syntactic?semantic parser by Johans-
son and Nugues (2008a) to annnotate the sen-
tences with dependency syntax (Mel?c?uk, 1988)
and shallow semantic structures in the PropBank
(Palmer et al, 2005) and NomBank (Meyers et
al., 2004) frameworks. Figure 1 shows an example
of the annotation: The sentence they called him a
liar, where called is a DSE and liar is an ESE, has
been annotated with dependency syntax (above the
text) and PropBank-based semantic role structure
(below the text). The predicate called, which is
an instance of the PropBank frame call.01, has
three semantic arguments: the Agent (A0), the
Theme (A1), and the Predicate (A2), which are re-
alized on the surface-syntactic level as a subject,
a direct object, and an object predicative comple-
ment, respectively.
]
ESE
They called
call.01
SBJ
OPRD
liarhim[ [a
A1A0 A2
]
DSE
NMODOBJ
Figure 1: Syntactic and shallow semantic struc-
ture.
3.2 Sequence Labeler
We implemented a standard sequence labeler fol-
lowing the approach of Collins (2002), while
training the model using the Passive?Aggressive
algorithm (Crammer et al, 2006) instead of the
perceptron. We encoded the opinionated expres-
sion brackets using the IOB2 encoding scheme
(Tjong Kim Sang and Veenstra, 1999). Figure 2
shows an example of a sentence with a DSE and
an ESE and how they are encoded in the IOB2 en-
coding.
This O
is O
viewed B-DSE
as O
the O
main B-ESE
impediment I-ESE
Figure 2: Sequence labeling example.
The sequence labeler used word, POS tag, and
lemma features in a window of size 3. In addi-
tion, we used prior polarity and intensity features
derived from the lexicon created by Wilson et al
(2005). In the example, viewed is listed as hav-
ing strong prior subjectivity but no polarity, and
impediment has strong prior subjectivity and neg-
ative polarity. Note that prior subjectivity does not
always imply subjectivity in a particular context;
this is why contextual features are essential for this
task.
This sequence labeler is used to generate the
candidate set for the reranker; the Viterbi algo-
rithm is easily modified to give k-best output. To
generate training data for the reranker, we carried
out a 5-fold cross-validation procedure: We split
the training set into 5 pieces, trained a sequence
labeler on pieces 1 to 4, applied it to piece 5 and
so on.
3.3 Reranker Features
The rerankers use two types of structural fea-
tures: syntactic features extracted from the depen-
dency tree, and semantic features extracted from
the predicate?argument (semantic role) graph.
The syntactic features are based on paths
through the dependency tree. This creates a small
complication for multiword opinionated expres-
sions; we select the shortest possible path in such
cases. For instance, in Example (1), the path will
be computed between denounced and violation,
and in Example (2) between viewed and impedi-
ment.
We used the following syntactic features:
69
SYNTACTIC PATH. Given a pair of opinion ex-
pressions, we use a feature representing the
labels of the two expressions and the path be-
tween them through the syntactic tree. For
instance, for the DSE called and the ESE liar
in Figure 1, we represent the syntactic config-
uration using the feature DSE:OPRD?:ESE,
meaning that the path from the DSE to the
ESE consists of a single link, where the de-
pendency edge label is OPRD (object predica-
tive complement).
LEXICALIZED PATH. Same as above,
but with lexical information attached:
DSE/called:OPRD?:ESE/liar.
DOMINANCE. In addition to the features based
on syntactic paths, we created a more generic
feature template describing dominance re-
lations between expressions. For instance,
from the graph in Figure 1, we extract the
feature DSE/called?ESE/liar, mean-
ing that a DSE with the word called domi-
nates an ESE with the word liar.
The semantic features were the following:
PREDICATE SENSE LABEL. For every predi-
cate found inside an opinion expression, we
add a feature consisting of the expression la-
bel and the predicate sense identifier. For in-
stance, the verb call which is also a DSE is
represented with the feature DSE/call.01.
PREDICATE AND ARGUMENT LABEL. For
every argument of a predicate inside an
opinion expression, we create a feature
representing the predicate?argument pair:
DSE/call.01:A0.
CONNECTING ARGUMENT LABEL. When a
predicate inside some opinion expression is
connected to some argument inside another
opinion expression, we use a feature con-
sisting of the two expression labels and the
argument label. For instance, the ESE liar
is connected to the DSE call via an A2 la-
bel, and we represent this using a feature
DSE:A2:ESE.
Apart from the syntactic and semantic features,
we also used the score output from the base se-
quence labeler as a feature. We normalized the
scores over the k candidates so that their exponen-
tials summed to 1.
3.4 Preference Kernel Approach
The first reranking strategy we investigated was
the Preference Kernel approach (Shen and Joshi,
2003). In this method, the reranking problem ?
learning to select the correct candidate h1 from a
candidate set {h1, . . . , hk} ? is reduced to a bi-
nary classification problem by creating pairs: pos-
itive training instances ?h1, h2?, . . . , ?h1, hk? and
negative instances ?h2, h1?, . . . , ?hk, h1?. This ap-
proach has the advantage that the abundant tools
for binary machine learning can be exploited.
It is also easy to show (Shen and Joshi, 2003)
that if we have a kernel K over the candidate space
T , we can construct a valid kernel PK over the
space of pairs T ? T as follows:
PK(h1, h2) = K(h
1
1
, h1
2
) + K(h2
1
, h2
2
)
? K(h1
1
, h2
2
) ? K(h2
1
, h1
2
),
where hi are the pairs of hypotheses ?h1i , h
2
i ? gen-
erated by the base model. This makes it possible
to use kernel methods to train the reranker. We
tried two types of kernels: linear kernels and tree
kernels.
3.4.1 Linear Kernel
We created feature vectors extracted from the can-
didate sequences using the features described in
Section 3.3. We then trained linear SVMs using
the LIBLINEAR software (Fan et al, 2008), using
L1 loss and L2 regularization.
3.4.2 Tree Kernel
Tree kernels have been successful for a number of
structure extraction tasks, such as relation extrac-
tion (Zhang et al, 2006; Nguyen et al, 2009) and
opinion holder extraction (Wiegand and Klakow,
2010). A tree kernel implicitly represents a large
space of fragments extracted from trees and could
thus reduce the need for manual feature design.
Since the paths that we extract manually (Sec-
tion 3.3) can be expressed as tree fragments, this
method could be an interesting alternative to the
manually extracted features used with the linear
kernel.
We therefore implemented a reranker using
the Partial Tree Kernel (Moschitti, 2006), and
we trained it using the SVMLight-TK software1,
which is a modification of SVMLight (Joachims,
1Available at http://dit.unitn.it/?moschitt
70
1999)2. It is still an open question how depen-
dency trees should be represented for use with
tree kernels (Suzuki et al, 2003; Nguyen et al,
2009); we used the representation shown in Fig-
ure 3. Note that we have concatenated the opinion
expression labels to the POS tag nodes. We did not
use any of the features from Section 3.3 except for
the base sequence labeler score.
TOP
ROOT
OBJSBJ
PRP
they him
OPRD
PRP
NMOD
DT
NN?ES
VBD?DS
called
a
liar
Figure 3: Representation of a dependency tree
with opinion expressions for tree kernels.
3.5 Structure Learning Approach
The Preference Kernel approach reduces the
reranking problem to a binary classification task
on pairs, after which a standard SVM optimizer is
used to train the reranker. A problem with this
method is that the optimization problem solved
by the SVM ? maximizing the classification ac-
curacy on a set of independent pairs ? is not di-
rectly related to the performance of the reranker.
Instead, the method employed by many rerankers
following Collins and Duffy (2002) directly learn
a scoring function that is trained to maximize per-
formance on the reranking task. We will refer to
this approach as the structure learning method.
While there are batch learning algorithms that
work in this setting (Tsochantaridis et al, 2005),
online learning methods have been more popular
for efficiency reasons. We investigated two online
learning algorithms: the popular structured per-
ceptron Collins and Duffy (2002) and the Passive?
Aggressive (PA) algorithm (Crammer et al, 2006).
To increase robustness, we averaged the weight
vectors seen during training as in the Voted Per-
ceptron (Freund and Schapire, 1999).
The difference between the two algorithms is
the way the weight vector is incremented in each
step. In the perceptron, for a given input x, we up-
date based on the difference between the correct
2http://svmlight.joachims.org
output y and the predicted output y?, where ? is
the feature representation function:
y? ? arg maxh w ? ?(x, h)
w ? w + ?(x, y) ? ?(x, y?)
In the PA algorithm, which is based on the the-
ory of large-margin learning, we instead find the
y? that violates the margin constraints maximally.
The update step length ? is computed based on the
margin; this update is bounded by a regularization
constant C:
y? ? arg maxh w ? ?(x, h) +
?
?(y, h)
? ? min
(
C,
w(?(x,y?)??(x,y))+
?
?(y,y?)
??(x,y?)??(x,y)?2
)
w ? w + ?(?(x, y) ? ?(x, y?))
The algorithm uses a cost function ?. We used
the function ?(y, y?) = 1 ? F (y, y?), where F is
the soft F-measure described in Section 4.1. With
this approach, the learning algorithm thus directly
optimizes the measure we are interested in, i.e. the
F-measure.
4 Experiments
We carried out the experiments on version 2 of the
MPQA corpus (Wiebe et al, 2005), which we split
into a test set (150 documents, 3,743 sentences)
and a training set (541 documents, 12,010 sen-
tences).
4.1 Evaluation Metrics
Since expression boundaries are hard to define ex-
actly in annotation guidelines (Wiebe et al, 2005),
we used soft precision and recall measures to score
the quality of the system output. To derive the soft
precision and recall, we first define the span cov-
erage c of a span s with respect to another span s?,
which measures how well s? is covered by s:
c(s, s?) =
|s ? s?|
|s?|
In this formula, the operator | ? | counts tokens, and
the intersection ? gives the set of tokens that two
spans have in common. Since our evaluation takes
span labels (DSE, ESE, OSE) into account, we set
c(s, s?) to zero if the labels associated with s and
s? are different.
Using the span coverage, we define the span set
coverage C of a set of spans S with respect to a
set S?:
C(S,S?) =
?
s
j
?S
?
s?
k
?S
?
c(sj, s?k)
71
We now define the soft precision P and recall R
of a proposed set of spans ?S with respect to a gold
standard set S as follows:
P (S, ?S) = C(S,
?
S)
|
?
S|
R(S, ?S) = C(
?
S,S)
|S|
Note that the operator | ? | counts spans in this for-
mula.
Conventionally, when measuring the quality of
a system for an information extraction task, a pre-
dicted entity is counted as correct if it exactly
matches the boundaries of a corresponding en-
tity in the gold standard; there is thus no reward
for close matches. However, since the boundaries
of the spans annotated in the MPQA corpus are
not strictly defined in the annotation guidelines
(Wiebe et al, 2005), measuring precision and re-
call using exact boundary scoring will result in fig-
ures that are too low to be indicative of the use-
fulness of the system. Therefore, most work us-
ing this corpus instead use overlap-based preci-
sion and recall measures, where a span is counted
as correctly detected if it overlaps with a span in
the gold standard (Choi et al, 2006; Breck et al,
2007). As pointed out by Breck et al (2007), this
is problematic since it will tend to reward long
spans ? for instance, a span covering the whole
sentence will always be counted as correct if the
gold standard contains any span for that sentence.
The precision and recall measures proposed
here correct the problem with overlap-based mea-
sures: If the system proposes a span covering the
whole sentence, the span coverage will be low and
result in a low soft precision. Note that our mea-
sures are bounded below by the exact measures
and above by the overlap-based measures.
4.2 Reranking Approaches
We compared the reranking architectures and the
machine learning methods described in Section 3.
In these experiments, we used a candidate set size
k of 8. Table 1 shows the results of the evaluations
using the precision and recall measures described
above. The baseline is the result of taking the top-
scoring output from the sequence labeler without
applying any reranking.
The results show that the rerankers using man-
ual feature extraction outperform the tree-kernel-
based reranker, which obtains a score just above
the baseline. It should be noted that the mas-
sive training time of kernel-based machine learn-
ing precluded a detailed tuning of parameters and
System P R F
Baseline 63.36 46.77 53.82
Pref-linear 64.60 50.17 56.48
Pref-TK 63.97 46.94 54.15
Struct-Perc 62.84 48.13 54.51
Struct-PA 63.50 51.79 57.04
Table 1: Evaluation of reranking architectures and
learning methods.
representation ? on the other hand, we did not need
to spend much time on parameter tuning and fea-
ture design for the other rerankers.
In addition, we note that the best performance
was obtained using the PA algorithm and the struc-
ture learning architecture. The PA algorithm is
a simple online learning method and still out-
performs the SVM used in the preference-kernel
reranker. This suggests that the structure learning
approach is superior for this task. It is possible
that a batch learning method such as SVMstruct
(Tsochantaridis et al, 2005) could improve the re-
sults even further.
4.3 Candidate Set Size
In any method based on reranking, it is important
to study the influence of the candidate set size on
the quality of the reranked output. In addition, an
interesting question is what the upper bound on
reranker performance is ? the oracle performance.
Table 2 shows the result of an experiment that in-
vestigates these questions. We used the reranker
based on the Passive?Aggressive method in this
experiment since this reranker gave the best results
in the previous experiment.
Reranked Oracle
k P R F P R F
1 63.36 46.77 53.82 63.36 46.77 53.82
2 63.70 48.17 54.86 72.66 55.18 62.72
4 63.57 49.78 55.84 79.12 62.24 69.68
8 63.50 51.79 57.04 83.72 68.14 75.13
16 63.00 52.94 57.54 86.92 72.79 79.23
32 62.15 54.50 58.07 89.18 76.76 82.51
64 61.02 55.67 58.22 91.08 80.19 85.28
128 60.22 56.45 58.27 92.63 83.00 87.55
256 59.87 57.22 58.51 94.01 85.27 89.43
Table 2: Oracle and reranker performance as a
function of candidate set size.
As is common in reranking tasks, the reranker
can exploit only a fraction of the potential im-
provement ? the reduction of the F-measure error
72
is between 10 and 15 percent of the oracle error
reduction for all candidate set sizes.
The most visible effect of the reranker is that
the recall is greatly improved. However, this does
not seem to have an adverse effect on the precision
until the candidate set size goes above 8 ? in fact,
the precision actually improves over the baseline
for small candidate set sizes. After the size goes
above 8, the recall (and the F-measure) still rises,
but at the cost of decreased precision.
4.4 Impact of Features
We studied the impact of syntactic and seman-
tic structural features on the performance of the
reranker. Table 3 shows the result of the inves-
tigation for syntactic features. Using all the syn-
tactic features (and no semantic features) gives an
F-measure roughly 4 points above the baseline, us-
ing the PA reranker with a k of 64. We then mea-
sured the F-measure obtained when each one of
the three syntactic features had been removed. It
is clear that the unlexicalized syntactic path is the
most important syntactic feature; the effect of the
two lexicalized features seems to be negligible.
System P R F
Baseline 63.36 46.77 53.82
All syntactic 62.45 53.19 57.45
No SYN PATH 64.40 48.69 55.46
No LEX PATH 62.62 53.19 57.52
No DOMINANCE 62.32 52.92 57.24
Table 3: Effect of syntactic features.
A similar result was obtained when studying the
semantic features (Table 4). Removing the CON-
NECTING ARGUMENT LABEL feature, which is
unlexicalized, has a greater effect than removing
the other two semantic features, which are lexical-
ized.
System P R F
Baseline 63.36 46.77 53.82
All semantic 61.26 53.85 57.31
No PREDICATE SL 61.28 53.81 57.30
No PRED+ARGLBL 60.96 53.61 57.05
No CONN ARGLBL 60.73 50.47 55.12
Table 4: Effect of semantic features.
Since our most effective structural features
combine a pair of opinion expression labels with
a tree fragment, it is interesting to study whether
the expression labels alone would be enough. If
this were the case, we could conclude that the
improvement is caused not by the structural fea-
tures, but just by learning which combinations
of labels are common in the training set, such
as that DSE+ESE would be more common than
OSE+ESE. We thus carried out an experiment
comparing a reranker using label pair features
against rerankers based on syntactic features only,
semantic features only, and the full feature set. Ta-
ble 5 shows the results. We see that the reranker
using label pairs indeed achieves a performance
well above the baseline. However, its performance
is below that of any reranker using structural fea-
tures. In addition, we see no improvement when
adding label pair features to the structural feature
set; this is to be expected since the label pair infor-
mation is subsumed by the structural features.
System P R F
Baseline 63.36 46.77 53.82
Label pairs 62.05 52.68 56.98
All syntactic 62.45 53.19 57.45
All semantic 61.26 53.85 57.31
Syn + sem 61.02 55.67 58.22
Syn + sem + pairs 61.61 54.78 57.99
Table 5: Structural features compared to label
pairs.
4.5 Comparison with Breck et al (2007)
Comparison of systems in opinion expression de-
tection is often nontrivial since evaluation settings
have differed widely. Since our problem setting
? marking up and labeling opinion expressions in
the MPQA corpus ? is most similar to that de-
scribed by Breck et al (2007), we carried out an
evaluation using the setting used in their experi-
ment.
For compatibility with their experimental setup,
this experiment differed from the ones described
in the previous sections in the following ways:
? The system did not need to distinguish DSEs
and ESEs and did not have to detect the
OSEs.
? The results were measured using the overlap-
based precision and recall, although this is
problematic as pointed out in Section 4.1.
73
? Instead of the training/test split we used in the
previous evaluations, the systems were evalu-
ated using a 10-fold cross-validation over the
same set of 400 documents as used in Breck?s
experiment.
Again, our reranker uses the PA method with a
k of 64. Table 6 shows the results.
System P R F
Breck et al (2007) 71.64 74.70 73.05
Baseline 80.85 64.38 71.68
Reranked 76.40 78.23 77.30
Table 6: Results using the Breck et al (2007) eval-
uation setting.
We see that the performance of our system is
clearly higher ? in both precision and recall ? than
that reported by Breck et al (2007). This shows
again that the structural features are effective for
the task of finding opinionated expressions.
We note that the performance of our base-
line sequence labeler is lower than theirs; this
is to be expected since they used a more com-
plex batch learning algorithm (conditional random
fields) while we used an online learner, and they
spent more effort on feature design. This indicates
that we should be able to achieve even higher per-
formance using a stronger base model.
5 Conclusion
We have shown that features derived from gram-
matical and semantic role structure can be used to
improve the detection of opinionated expressions
in subjectivity analysis. Most significantly, the re-
call is drastically increased (10 points) while the
precision decreases only slightly (3 points). This
result compares favorably with previously pub-
lished results, which have been biased towards
precision and scored low on recall.
The long-distance structural features gives us a
model that has predictive power as well as being of
theoretical interest: this model takes into account
the interactions between opinion expressions in a
sentence. While these structural features give us
a powerful model, they come at a computational
cost; prediction is more complex than in a stan-
dard sequence labeler based on purely local fea-
tures. However, we have shown that a prediction
strategy based on reranking suffices for this task.
We analyzed the impact of the syntactic and se-
mantic features and saw that the best model in-
cludes both types of features. The most effective
features we have found are purely structural, i.e.
based on tree fragments in a syntactic or seman-
tic tree. Features involving words did not seem to
have the same impact. We also showed that the im-
provement is not explainable by mere correlations
between opinion expression labels.
We investigated a number of implementation
strategies for the reranker and concluded that the
structural learning framework seemed to give the
best performance. We were not able to achieve
the same performance using tree kernels as with
manually extracted features. It is possible that this
could be improved with a better strategy for rep-
resenting dependency structure for tree kernels, or
if the tree kernels could be incorporated into the
structural learning framework.
The flexible architecture we have presented en-
ables interesting future research: (i) a straight-
forward improvement is the use of lexical simi-
larity to reduce data sparseness, e.g. (Basili et
al., 2005; Basili et al, 2006; Bloehdorn et al,
2006). However, the similarity between subjective
words, which have multiple senses against other
words may negatively impact the system accu-
racy. Therefore, the use of the syntactic/semantic
kernels, i.e. (Bloehdorn and Moschitti, 2007a;
Bloehdorn and Moschitti, 2007b), to syntactically
contextualize word similarities may improve the
reranker accuracy. (ii) The latter can be fur-
ther boosted by studying complex structural ker-
nels, e.g. (Moschitti, 2008; Nguyen et al, 2009;
Dinarelli et al, 2009). (iii) More specific pred-
icate argument structures such those proposed in
FrameNet, e.g. (Baker et al, 1998; Giuglea and
Moschitti, 2004; Giuglea and Moschitti, 2006; Jo-
hansson and Nugues, 2008b) may be useful to
characterize the opinion holder and the sentence
semantic context.
Finally, while the strategy based on reranking
resulted in a significant performance boost, it re-
mains to be seen whether a higher accuracy can
be achieved by developing a more sophisticated
inference algorithm based on dynamic program-
ming. However, while the development of such
an algorithm is an interesting problem, it will not
necessarily result in a more usable system ? when
using a reranker, it is easy to trade accuracy for
efficiency.
74
Acknowledgements
The research leading to these results has received
funding from the European Community?s Sev-
enth Framework Programme (FP7/2007-2013) un-
der grant agreement 231126: LivingKnowledge ?
Facts, Opinions and Bias in Time, and from Trust-
worthy Eternal Systems via Evolving Software,
Data and Knowledge (EternalS, project number
FP7 247758). In addition, we would like to thank
Eric Breck for clarifying his results and experi-
mental setup.
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet Project. In Proceed-
ings of COLING/ACL-1998.
Roberto Basili, Marco Cammisa, and Alessandro Mos-
chitti. 2005. Effective use of WordNet seman-
tics via kernel-based learning. In Proceedings of
CoNLL-2005, pages 1?8, Ann Arbor, Michigan.
Roberto Basili, Marco Cammisa, and Alessandro Mos-
chitti. 2006. A semantic kernel to classify texts with
very few training examples. In in Informatica, an in-
ternational journal of Computing and Informatics.
Steven Bethard, Hong Yu, Ashley Thornton, Vasileios
Hatzivassiloglou, and Dan Jurafsky. 2005. Extract-
ing opinion propositions and opinion holders using
syntactic and lexical cues. In James G. Shanahan,
Yan Qu, and Janyce Wiebe, editors, Computing Atti-
tude and Affect in Text: Theory and Applications.
Stephan Bloehdorn and Alessandro Moschitti. 2007a.
Combined syntactic and semantic kernels for text
classification. In Proceedings of ECIR 2007, Rome,
Italy.
Stephan Bloehdorn and Alessandro Moschitti. 2007b.
Structure and semantics for expressive text kernels.
In In Proceedings of CIKM ?07.
Stephan Bloehdorn, Roberto Basili, Marco Cammisa,
and Alessandro Moschitti. 2006. Semantic kernels
for text classification based on topological measures
of feature similarity. In Proceedings of ICDM 06,
Hong Kong, 2006.
Eric Breck, Yejin Choi, and Claire Cardie. 2007. Iden-
tifying expressions of opinion in context. In Pro-
ceedings of IJCAI-2007, Hyderabad, India.
Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint
extraction of entities and relations for opinion recog-
nition. In Proceedings of EMNLP 2006.
Michael Collins and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: Kernels over
discrete structures, and the voted perceptron. InPro-
ceedings of ACL?02.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and ex-
periments with perceptron algorithms. In Proceed-
ings of the 2002 Conference on Empirical Methods
in Natural Language Processing (EMNLP 2002),
pages 1?8.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Schwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research, 2006(7):551?585.
William Croft. 2005. Radical and typological argu-
ments for radical construction grammar. In J.-O.
O?stman and M. Fried, editors, Construction Gram-
mars: Cognitive grounding and theoretical exten-
sions.
Marco Dinarelli, Alessandro Moschitti, and Giuseppe
Riccardi. 2009. Re-ranking models based-on small
training data for spoken language understanding.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1076?1085, Singapore, August.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
Yoav Freund and Robert E. Schapire. 1999. Large
margin classification using the perceptron algorithm.
Machine Learning, 37(3):277?296.
Ana-Maria Giuglea and Alessandro Moschitti. 2004.
Knowledge Discovering using FrameNet, VerbNet
and PropBank. In In Proceedings of the Workshop
on Ontology and Knowledge Discovering at ECML
2004, Pisa, Italy.
Ana-Maria Giuglea and Alessandro Moschitti. 2006.
Semantic role labeling via FrameNet, VerbNet and
PropBank. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 929?936, Sydney, Aus-
tralia, July.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL-08: HLT, pages 586?594, Columbus, United
States.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. Advances in Kernel Methods ?
Support Vector Learning, 13.
Richard Johansson and Pierre Nugues. 2008a.
Dependency-based syntactic?semantic analysis with
PropBank and NomBank. In CoNLL 2008: Pro-
ceedings of the Twelfth Conference on Natural
Language Learning, pages 183?187, Manchester,
United Kingdom.
75
Richard Johansson and Pierre Nugues. 2008b. The
effect of syntactic representation on semantic role
labeling. In Proceedings of the 22nd International
Conference on Computational Linguistics (Coling
2008), pages 393?400, Manchester, UK.
Mahesh Joshi and Carolyn Penstein-Rose?. 2009. Gen-
eralizing dependency features for opinion mining.
In Proceedings of ACL/IJCNLP 2009, Short Papers
Track.
Jussi Karlgren, Gunnar Eriksson, Magnus Sahlgren,
and Oscar Ta?ckstro?m. 2010. Between bags
and trees ? constructional patterns in text used for
attitude identification. In Proceedings of ECIR
2010, 32nd European Conference on Information
Retrieval, Milton Keynes, United Kingdom.
Soo-Min Kim and Eduard Hovy. 2006. Extract-
ing opinions, opinion holders, and topics expressed
in online news media text. In Proceedings of
ACL/COLING Workshop on Sentiment and Subjec-
tivity in Text.
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Extracting aspect-evaluation and aspect-of re-
lations in opinion mining. In Proceedings of Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-CoNLL-2007).
Igor A. Mel?c?uk. 1988. Dependency Syntax: Theory
and Practice. State University Press of New York,
Albany.
Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishman. 2004. The NomBank project:
An interim report. In HLT-NAACL 2004 Work-
shop: Frontiers in Corpus Annotation, pages 24?31,
Boston, United States.
Alessandro Moschitti and Roberto Basili. 2004. Com-
plex linguistic features for text classification: A
comprehensive study. In Proceedings of ECIR.
AlessandroMoschitti. 2006. Making tree kernels prac-
tical for natural language learning. In Proccedings
of EACL?06.
Alessandro Moschitti. 2008. Kernel methods, syntax
and semantics for relational text categorization. In
Proceeding of CIKM ?08, NY, USA.
Truc-Vien T. Nguyen, Alessandro Moschitti, and
Giuseppe Riccardi. 2009. Convolution kernels on
constituent, dependency and sequential structures
for relation extraction. In Proceedings of EMNLP.
Martha Palmer, DanGildea, and Paul Kingsbury. 2005.
The proposition bank: An annotated corpus of se-
mantic roles. Computational Linguistics, 31(1):71?
105.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification us-
ing machine learning techniques. In Proceedings of
EMNLP.
Josef Ruppenhofer, Swapna Somasundaran, and Janyce
Wiebe. 2008. Finding the sources and targets of
subjective expressions. In Proceedings of LREC.
Libin Shen and Aravind Joshi. 2003. An SVM based
voting algorithmwith application to parse reranking.
In Proceedings of the CoNLL.
Swapna Somasundaran, Galileo Namata, Janyce
Wiebe, and Lise Getoor. 2009. Supervised and
unsupervised methods in employing discourse rela-
tions for improving opinion polarity classification.
In Proceedings of EMNLP 2009: conference on Em-
pirical Methods in Natural Language Processing.
Jun Suzuki, Tsutomu Hirao, Yutaka Sasaki, and Eisaku
Maeda. 2003. Hierarchical directed acyclic graph
kernel: Methods for structured natural language
data. In Proceedings of the 41th Annual Meeting of
Association for Computational Linguistics (ACL).
Erik F. Tjong Kim Sang and Jorn Veenstra. 1999. Rep-
resenting text chunks. In Proceedings of EACL99,
pages 173?179, Bergen, Norway.
Iannis Tsochantaridis, Thorsten Joachims, Thomas
Hofmann, and Yasemin Altun. 2005. Large margin
methods for structured and interdependent output
variables. Journal of Machine Learning Research,
6(Sep):1453?1484.
Janyce Wiebe, Rebecca Bruce, and Thomas O?Hara.
1999. Development and use of a gold standard data
set for subjectivity classifications. In Proceedings
of the 37th Annual Meeting of the Association for
Computational Linguistics.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation, 39(2-3):165?210.
Michael Wiegand and Dietrich Klakow. 2010. Con-
volution kernels for opinion holder extraction. In
Proceedings of HLT-NAACL 2010. To appear.
Theresa Wilson, Janyce Wiebe, and Paul Hoff-
mann. 2005. Recognizing contextual polarity in
phrase-level sentiment analysis. In Proceedings of
HLT/EMNLP 2005.
YuanbinWu, Qi Zhang, XuanjingHuang, and LideWu.
2009. Phrase dependency parsing for opinion min-
ing. In Proceedings of EMNLP.
Hong Yu and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: Separating facts
from opinions and identifying the polarity of opin-
ion sentences. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2003), pages 129?136, Sapporo, Japan.
Min Zhang, Jie Zhang, and Jian Su. 2006. Exploring
Syntactic Features for Relation Extraction using a
Convolution tree kernel. In Proceedings of NAACL.
76
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 150?159,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Global Features for Shallow Discourse Parsing
Sucheta Ghosh Giuseppe Riccardi Richard Johansson
University of Trento, Italy University of Gothenburg, Sweden
{ghosh,riccardi}@disi.unitn.it richard.johansson@gu.se
Abstract
A coherently related group of sentences
may be referred to as a discourse. In
this paper we address the problem of pars-
ing coherence relations as defined in the
Penn Discourse Tree Bank (PDTB). A
good model for discourse structure anal-
ysis needs to account both for local depen-
dencies at the token-level and for global
dependencies and statistics. We present
techniques on using inter-sentential or
sentence-level (global), data-driven, non-
grammatical features in the task of parsing
discourse. The parser model follows up
previous approach based on using token-
level (local) features with conditional ran-
dom fields for shallow discourse parsing,
which is lacking in structural knowledge
of discourse. The parser adopts a two-
stage approach where first the local con-
straints are applied and then global con-
straints are used on a reduced weighted
search space (n-best). In the latter stage
we experiment with different rerankers
trained on the first stage n-best parses,
which are generated using lexico-syntactic
local features. The two-stage parser yields
significant improvements over the best
performing model of discourse parser on
the PDTB corpus.
1 Introduction
There are relevant studies on the impact of global
and local features on the models for natural
language understanding. In this work we ad-
dress a similar problem in the context of dis-
course parsing. Although a good number of
the papers in this area heavily rely on local
classifiers (Grosz et al, 1995; Soricut et al, 2003;
Lapata, 2003; Barzilay et al, 2005), there are still
some important works using global and local
informations together to form a model of dis-
course (Grosz et al, 1992; Barzilay et al, 2004;
Soricut et al, 2006).
One of the main issues is the basis of the choice
between a global or local or a joint model for dis-
course parsing: it all depends on the criteria to be
able to capture maximum amount of information
inside the discourse model. The policy for dis-
course segmentation plays a big role to formulate
the maximizing criteria (Grosz et al, 1992). We
study in the literature that defining a discourse seg-
ment is mostly a data-driven process: some argue
for prosodic units, some for intentional structure
and some for clause-like structures. We work with
PDTB 2.0 annotation framework, therefore use a
clause-like structure. Soricut et al (2003) empiri-
cally showed that at the sentence level, there is a
strong correlation between syntax and discourse,
Ghosh et al (2011b) found the same. Since the
discourse structure may span over multiple sen-
tences, intersentential features are needed to im-
prove the performance of a discourse parser.
Linguistic theory suggests that a core argument
frame (i.e. a pair of the Arg1 and the Arg2 con-
nected with one and only one connective) is a joint
structure, with strong dependencies between ar-
guments (Toutanova et al, 2008). Following this,
Ghosh et al (2011a) also injected some structure-
level information through the token-level features,
for eg. the previous sentence feature. Still there
is a room for improvement with more structure-
level information to that discourse model; though
it is cost-intensive to modify this discourse model.
Therefore in this paper we re-use the model
(Ghosh et al, 2011a) and optimize the current loss
function adding the global features through re-
ranking of the single-best model.
Reranking has been a popular technique
applied in a variety of comparable NLP
problems including parsing (Collins, 2000;
150
Charniak and Johnson, 2005), semantic
role labeling (Toutanova et al, 2008), NP
Bracketing (Daume III et al, 2004), NER
(Collins, 2002), opinion expression detection
(Johansson and Moschitti, 2010), now we employ
this technique in the area of discourse parsing.
In the next sections, we detail on the back-
grounds and motivations of this work, before this
we also add a short discussion on PDTB (Penn
Discourse TreeBank), i.e. the data we used to train
the system. Then we proceed to the reranking ap-
proaches and results sections after describing our
global feature set. Finally we state and analyze the
results.
2 The Penn Discourse Treebank 2.0
The Penn Discourse Treebank (PDTB) is a re-
source containing one million words from theWall
Street Journal corpus (Marcus et al, 1993) anno-
tated with discourse relations.
Connectives in the PTDB are treated as dis-
course predicates taking two text spans as ar-
guments (Arg), i.e. parts of the text that de-
scribe events, propositions, facts, situations. Such
two arguments in the PDTB are called Arg1 and
Arg2, with the numbering not necessarily corre-
sponding to their order in text. Indeed, Arg2 is
the argument syntactically bound to the connec-
tive, while Arg1 is the other one.
In the PDTB, discourse relations can be either
overtly or implicitly expressed. However, we fo-
cus here exclusively on explicit connectives and
the identification of their arguments, including the
exact spans. This kind of classification is very
complex, since Arg1 and Arg2 can occur in
many different configurations (see Table 1).
Explicit connectives (tokens) 18, 459
Explicit connectives (types) 100
Arg1 in same sentence as connective 60.9%
Arg1 in previous, adjacent sentence 30.1%
Arg1 in previous, non adjacent sentence 9.0%
Table 1: Statistics about PDTB annotation from Prasad et
al(2008).
In PDTB the senses are assigned according to a
three-layered hierarchy: the top-level classes are
the most generic ones and include TEMPORAL,
CONTINGENCY, COMPARISON and EXPANSION
labels. We used these four surface senses only in
our task.
2.1 Backgrounds & Motivation
Currently we are using the single-best discourse
parser by Ghosh et al (2011a). This discourse
parser can automatically extract of discourse ar-
guments using a pipeline, illustrated in Fig 1.
First, we input the explicit discourse connec-
tives (with senses) to the system. These can
be the gold labeled or automatically identified
(Pitler and Nenkova, 2009); for simplicity here we
use Penn Discourse TreeBank (PDTB 2.0) gold-
standard connectives (cf. see 2). Then a cascaded
module is applied extracting the Arg2 arguments,
then the Arg1s are extracted.
Figure 1: Pipeline for argument detection given a connec-
tive.
The Arg2 and Arg1 extractors are imple-
mented as conditional random field sequence la-
belers, which use a set of syntactic and structural
features (cf. Ghosh et al (2011a)). In order to re-
duce the complexities, the sentence containing the
connective, and a context window of up to two
sentences before and after are supplied to the se-
quence labelers.
We present a passage of 6 sentences from a nu-
trition journal article parsed with that parser 1.:
<Conn id=1,sense=Comparison>
Although</Conn id=1> <ARG2 id=1>
the mechanism of obesity development
is not fully understood, it is confirmed
<ARG1 id=2>that obesity occurs</ARG1 id=2>
<Conn id=2,sense=Temporal>when</Conn id=2>
<ARG2 id=2>energy intake exceeds energy
expenditure</ARG2 id=2> </ARG2 id=1>.
There are multiple etiologies
for this imbalance, hence,
<Conn id=3, sense=Expansion>
and </Conn id=3> <ARG2 id=3>the rising
prevalence of obesity cannot be addressed
by a single etiology</ARG2 id=3>.
<ARG1 id=4>Genetic factors influence
the susceptibility of a given child to an
obesity-conducive environment</ARG1 id=4>.
<Conn id=4, sense=Comparison>However
1we used best model of (Ghosh et al, 2011b;
Ghosh et al, 2011a) and Stanford lexicalized parser
(Klein and Manning, 2003) to parse the text also used
AddDiscourse tool to parse the connective and the senses
(Pitler and Nenkova, 2009);parser took 17 second to parse
151
</Conn id=4>, <ARG2 id=4>environmental
factors, lifestyle preferences, and
cultural environment seem to play major
roles in the rising prevalence of obesity
worldwide</ARG2 id=4>. In a small number
of cases, childhood obesity is due to
genes such as leptin deficiency or
medical causes such as hypothyroidism
and growth hormone deficiency or side
effects due to drugs (e.g. - steroids).
Most of the time, <Conn id=5, sense=
Comparison> however </Conn id=5>,
<ARG2 id=5>personal lifestyle choices
and cultural environment significantly
influence obesity</ARG2 id=5>.
In the evaluations of Ghosh et al (2011a), it
states that recall was much lower than preci-
sion for both the arguments, especially in case of
Arg1. The system often failed to predict Arg1. It
is harder to identify since it is not always syntac-
tically bound to the connective, like Arg2, more-
over it is typically more distant than the Arg2s.
We notice the same in the parser output. The
parser found all five Arg2s for all five connec-
tives, though there may be disagreement on the se-
lected boundaries; the number of parsed Arg1s is
only two, whereas the second one with id of 4 is a
previous sentence argument.
To improve the recall, (Ghosh et al, 2012)
implemented a weighted constraint-based
handcrafted postprocessor to force the
Ghosh et al (2011a) system to output argu-
ments of each type abiding the requirements
defined by the PDTB annotation guidelines.
In order to find the best solution with a min-
imum of constraint violations, the top k analy-
ses output are generated by the CRF (Conditional
Random Field) (Lafferty et al, 2001) for every
sentence; these analyses can then be combined to
form the k top analyses for the whole 5-sentence
window around the connective. This combina-
tion is most efficiently carried out using a prior-
ity queue similar to a chart cell in the k-best pars-
ing algorithm by Huang and Chiang (2005). (see
Ghosh et al (2012) for details)
2.2 Feature Set of Baseline System
We summarize the feature set of the base system
(Ghosh et al, 2011a) to emphasize the distinction
between the local and global feature set for this
work.
The token-level (local) feature set in the Table 2
can be divided into four categories:
Features used for Arg1 and Arg2 segmentation and labeling.F1. Token (T)F2. Sense of Connective (CONN)F3. IOB chain (IOB)F4. PoS tagF5. Lemma (L)F6. Inflection (INFL)F7. Main verb of main clause (MV)F8. Boolean feature for MV (BMV)F9. Previous sentence feature (PREV)Additional feature used only for Arg1F10. Arg2 Labels
Table 2: Feature sets for Arg1 and Arg2 segmentation and
labeling in base system (Ghosh et al2011a).
1. Syntactic. {F3, F4, F6} 2
2. Semantic. {F2}
3. Lexical {F5, F7, F8}
4. Structure related token-level features.
{F9, F10}
The remaining one (F1) is the token itself. The
sense of the connective feature (F2) extracted from
PDTB for the base system, though for the fully au-
tomatic one (Ghosh et al, 2011b) it needs the PTB
(Penn TreeBank)-style syntactic parse trees as in-
put (Pitler and Nenkova, 2009). The IOB(Inside-
Outside-Begin) chain (F3) 3 (F3) is extracted from
a full parse tree and corresponds to the syntactic
categories of all the constituents on the path be-
tween the root note and the current leaf node of
the tree. Experiments with other syntactic fea-
tures proved that IOB chain conveys all deep syn-
tactic information needed in the task, and makes
all other syntactic information redundant, for ex-
ample clause boundaries, token distance from the
connective, constituent label, etc.
In order to extract the morphological
features needed, we use the morpha tool
(Minnen et al, 2001), which outputs lemma (F5)
and inflection information (F6) of the candidate
token. The latter is the ending usually added to
the word root to convey inflectional information.
It includes for example the -ing and -ed suffixes
in verb endings as well as the -s to form the plural
of nouns.
As for features (F7) and (F8), they rely on in-
formation about the main verb of the current sen-
tence. More specifically, feature (F7) is the main
verb token , extracted following the head-finding
2Infection can be defined as morpho-syntactic feature.
3We extracted this feature using the Chun-
klink.pl script made available by Sabine Buchholz at
ilk.uvt.nl/team/sabine/chunklink/README.html
152
strategy by Yamada and Matsumoto (2003), while
feature (F8) is a boolean feature that indicates for
each token if it is the main verb in the sentence or
not.4
The structure related token-level features do
not use any parse tree. The Arg2 label (F10)
features are generated from the word sequence
index in PDTB for the base system (for au-
tomatic system it is generated by the pipeline
(Ghosh et al, 2011b)); this feature is used to clas-
sify Arg1 . The previous sentence feature ?Prev?
(F9) is a connective-surface feature and is used
to capture if the following sentence begins with
a connective. This is meant for the classification
of the Arg1 that resides in the previous sentence
of the connective. The feature value for each can-
didate token of a sentence corresponds to the con-
nective token that appears at the beginning of the
following sentence, if any. Otherwise, it is equal
to 0.
Although both of the structure-related features
are strong features according to the feature analy-
sis in Ghosh et al (2011a), the base system is not
able to capture all available global features inside
the 5-sentence discourse context, merely uses 2-
sentence context. This is due to the fact that CRF
classifier uses a narrow window, that can only cap-
ture the information nearby the token under con-
sideration. Therefore it becomes impossible to
inject more information about the 5-sentence dis-
course window structure.
3 Global Feature Set
We use a global feature-set. The global features
are defined as the data-driven, hand-crafted rule
generated and non-grammatical (i.e. no syntactic
parse tree is used to generate this features) fea-
tures.
The model of Ghosh et al (2011a) is based on
Conditional Random Fields (CRF), and incorpo-
rating a set of structural and lexical features. At
the core part of the model lies a local classi-
fier, which labels each token sequentially with one
of the possible argument labels or OTHER in a
pipeline. Now global information can be inte-
grated into the model using global features at a
longer-distance context, by defining a small set of
global constraints (if too many dependencies are
encoded, the model will over-fit the training data
4We used the head rules by Yamada & Matsumoto
(http://www.jaist.ac.jp/?h-yamada/)
and will not generalize well).
The global features are computed using each
list of k-best lists, in contrast to the lexico-
syntactically generated local features for each to-
ken item for each sentence of n-best lists. The
usage of global feature is meant for exploring the
yet undiscovered dimension of the each 5-sentence
discourse window. Global feature set consists of
the eight features that works on a full 5-sentence
discourse window (cf. sec. 2.1). The first six (i.e.
GF0-GF5) of these are same with the constrained
system 2.1.
None of the features are extracted from any
parse tree. All the seven features (GF1-GF7) are
derived from the generated Arg tags of the n-
best lists, the first one is the logarithm of poste-
rior probability computed from the CRF posterior
probability output for each list of the n-best lists.
The finer description of each feature is given be-
low.
GF0. logarithm of Posterior Probability. this
feature is generated by the base CRF classifier.
The CRF generates probability per sentence, for
each list of the n-best lists. We calculate sum of
the log of each probability during generation of k-
best lists forming 5-sentence discourse window.
GF1. Overgeneration. It is possible for an ar-
gument to be split into more than one part in same
sentence, we found these cases several times in
PDTB. This constraint is violated if an Arg1 or
Arg2 is split over multiple sentences. This is a
predominant problem for those lists of the n-best
lists those are generated with low posteriors. This
feature exhibits the problem of overgeneration to
the reranker with the counts.
GF2. Undergeneration. According to PDTB
annotation scheme every connective must have ar-
guments of each type, this constraint is violated if
an argument is missing. This is the prevalent prob-
lem in the single-best system, especially for the
Arg1 classification. This feature works to spec-
ify where a discourse structure missing the argu-
ment(s) - one of the main problems that motivated
this work.
GF3. Intersentential Arg2 (used only for Arg2
reranker). Count of Arg2, if any, occurs classified
outside connective sentence - this way the system
is constrained to have any inter-sentential Arg2.
This is a hypothetically motivated feature to re-
duce the complexity of the classification problem;
although in fact in PDTB 2.0, there are a few cases
153
of Arg2 of explicit connective (i.e. the 114 out
of 18459), where it extends beyond the connec-
tives sentence to include additional sentences in
the subsequent discourse (Prasad et al, 2008).
GF4. Arg1 after the connective sentence. Count
of Arg1, if any, occurs classified after connec-
tive sentence. Through this feature we attempt to
constrain the system to have Arg1s always occur-
ring in the previous sentence or before the previ-
ous sentence of the connective sentence.
GF5. Argument overlapping with the connec-
tive. Count of the cases if there is any token over-
lap between Args and connective tokens. This is
also not possible for the PDTB-style annotation,
so we intend to constrain the overlapping, if any.
GF6. Argument begins with -I tag. Count of
the cases if the generated Arg chunks begins with
the -I (inside) tag, violating the principle of IOB
tags for chunking. This is only possible if the CRF
chunker fails to tag the boundaries properly.
GF7. Argument begins with -E tag. Count of
the cases if the generated Arg chunks begins with
the -E (end) tag instead of a -B(begin) tag. This is
also possible if only the CRF chunker fails to tag
the chunk boundaries properly.
We attempt to categorize this feature set accord-
ing to the properties they bear: {GF0} is the in-
trinsic global feature - it is the evidence of confi-
dence on decisions made by the single-best model;
{GF1, GF2} check the prevalent problems seen
through the evaluation of decisions by the sin-
gle best model; {GF3, GF4, GF5} are the hy-
pothetical global features those reduce classifica-
tion complexities, they are inspired by the general
trends or rules for annotation in PDTB. {G6, G7}
check the mistakes in IOB tagging by the CRF
chunker.
4 Reranking Approaches
We formalize the reranking algorithm as follows:
for a given sentence s, a reranker selects the best
parse y? among the set of candidates candidate(s)
according to some scoring function:
y? = argmaxy?candidate(s)score(y) (1)
In n-best reranking, candidate(s) is simply a set
of n-best parses from the baseline parser, that is,
candidate(s) = {y1, y2, ..., yn}.
In this paper we followed two approaches for
the reranking task:
1. Structured Learning Approach: in this case
the reranker learns directly from a scoring func-
tion that is trained to maximize the performance of
the reranking task (Collins and Duffy, 2002). We
also investigate two popular and efficient online
structured learning algorithms: the structured
voted perceptron by Collins and Duffy (2002)
and Passive-Aggressive(PA) algorithm by
Crammer et al (2006). The weight-vectors
observed from the training phase are averaged
following Schapire and Freund (1999). In case of
structured perceptron for each of the candidate in
a ranked list the scoring function of equation 1 is
computed as follows:
score(yi) = w ??(xi,j) (2)
where w is the parameter weight-vector and ? is
the feature representing function of xi,j ; xi,j de-
notes the j-th token of the i-th sentence. Since
the PA algorithm is based on the theory of large-
margin, it attempts find a score that violates the
margin maximally by adding an extra cost i.e.?
?(xi,j) to the basic score function for structured
perceptron i.e. equation 2. Here ? is computed
as 1 ? F (xi.j), F: F-measure. The online PA also
takes care of the learning rate of perceptron, which
is considered as 1 in structured perceptron. The
learning rate in online PA is min-value between a
regularization constant and normalized score func-
tion value.
2. Best vs. rest Approach: in the prefer-
ence kernel approach (Shen and Joshi, 2003) the
reranking problem is reduced to a binary classi-
fication task on pairs. This reduction enables even
a standard support vector machine to optimize the
problem. We use a component of this task. We
define the best scored discourse window (section
4.1) as a positive example and the rest are the neg-
atives to the system. We use a standard support
vector machine (Vapnik, 1995) with linear kernel.
3. Preference Kernel Approach: we also inves-
tigated the classical approach of preference ker-
nel, as it is introduced by (Shen and Joshi, 2003).
In this method, the reranking problem learning to
select the correct candidate h1 from a candidate
set {h1, ? ? ? , hk} is reduced to a binary classifi-
cation problem by creating pairs: positive training
instances ?h1, h2?, ? ? ? , ?h1, hk? and negative in-
stances ?h2, h1?, ? ? ? , ?hk, h1?. The advantage of
using this approach is that there are abundant tools
for binary machine learning.
If we have a kernel K over the candidate
space T , we can construct a preference kernel
154
(Shen and Joshi, 2003) PK over the space of pairs
T ? T as follows:
PK = K(h11, h12) + K(h21, h22)
? K(h11, h22) ? K(h21, h12) (3)
In our case, we make pair from the n-best
hypotheses hi as ?h1i , h2i ? generated by the basemodel. We used linear kernel to train the reranker.
Thus we create the feature vectors extracted
from the candidate sequences using the features
described in Section 3. We then trained linear
SVMs (Support Vector Machine) using the LIB-
LINEAR software (Fan et al, 2008), using L1 loss
and L2 regularization.
4.1 Experiments
We use PennDiscourse TreeBank
(Prasad et al, 2008) and Penn TreeBank
(Marcus et al, 1993) data through this entire
work. We keep the split of data as follows:
02 ? 22 folders of PDTB (& PTB) are used for
training, 23 ? 24 folders of the same are used
for testing; remaining 00-01 folders are meant
for development split, it is used only to study the
impact of feature (cf. 5).
We prepare the n-best outputs of sentences from
the base system (cf. 2.1). The training data is pre-
pared from the input of n-best lists of the train
split, using a oracle module, which generates k-
best oracle lists from the n-best single outputs. We
procure k-best lists from oracle using the evaluator
module (see section 4.2), ordered by the highest to
the lowest probability score. Each of the list of the
k-best list is a 5-sentence discourse window.
We prepare the test data given the n-best lists
of the test split. We obtain k-best list for test-
ing, prepared with the module described in sec-
tion 2.1. We re-integrate the sentences con-
nected with the same discourse connective id
into the 5-sentence discourse window keeping the
connective-bearing sentence in the middle. This
re-integration done using a priority queue in the
style of Huang and Chiang (2005). Each of the list
from the k-best list are ordered by the highest to
the lowest score with sum of the log of posterior
probabilities of each sentence in the n-best list.
Therefore, in short, the n-best list is the list of
sentence-level analyses whereas the k-best list is
the list of 5-sentence discourse window-level anal-
yses.
Baseline: we consider the performance of the
single-best output from the base implementation
(cf. 2.1) as the baseline.
4.2 Evaluation
We present our results using precision,
recall and F1 measures. Following
Johansson and Moschitti (2010), we use three
scoring schemes: exact, intersection (or partial),
and overlap scoring. In the exact scoring scheme,
a span extracted by the system is counted as
correct if its extent exactly coincides with one
in the gold standard. We also include two other
scoring schemes to have a rough approximation
of the argument spans. In the overlap scheme,
an expression is counted as correctly detected if
it overlaps with a gold standard argument. The
intersection scheme assigns a score between 0 and
1 for every predicted span based on how much it
overlaps with a gold standard span, so unlike the
other two schemes it will reward close matches.
4.3 Classifier Results
ARG1 Results ARG2 Results
Exact P R F P R F
Baseline 69.88 48.51 57.26 83.44 75.14 79.07
Online PA 66.10 53.92 59.39(16) 82.59 76.39 79.37(4)
Struct Per 67.18 52.64 59.03(4) 82.96 76.28 79.48(8)
BestVsRest 66.19 52.83 58.94(8) 81.69 77.14 79.35(4)
Pref-Linear 66.54 53.31 59.20(4) 82.82 76.28 79.42(4)
Table 3: Exact Match Results for four classifiers. Baseline
scores in the first row. Used n-best list numbers in parenthe-
sis. The best performances are boldfaced.
We observe that reranking with global features
improved the F1 scores for Arg1 significantly, al-
though for Arg2 the improvement is insignificant
5. Since in most of the cases the Arg2 is syntacti-
cally bound with the connective, it is obvious that
lexico-syntactically motivated local features help
the classification of Arg2. On the other hand, the
classification of Arg1 is considerably dependent
on non-grammatical, hand-crafted rule generated
features. If we compare to our reranking clas-
sification results of Arg1 with that one without
previous sentence feature in Ghosh et al (2011a)
then we observe that the global and globally moti-
vated structural feature improved the classification
5Throughout this work the permutation test is used
to compute the significance of difference, whereas to
compute the confidence interval bootstrap resampling is
used(Hjorth, 1993). We determined the significant digits for
presenting results using the methods illustrated by Weisstein
E. W. (Weisstein, 2012)
155
of Arg1 by more than 10 points.
We also notice from the table for both the argu-
ment classification cases that we achieve balanced
scores in terms of the precision and the recall with
the structured global features. In fact there is a
good improvement of recall without much loss
in terms of precision.There is not any significant
improvement in case of Arg2 reranking because
the problem of the classification mostly resides on
boundary detection of Arg2; also we know that
estimation of position of an Arg2 is pretty easy
task given the connective is correctly identified.
ARG1 Results ARG2 Results
Exact P R F P R F
Baseline 82.90 61.65 70.72 93.40 84.20 88.56
Online PA 80.11 69.43 74.39(16) 92.94 85.73 89.19(4)
Struct Per 81.18 67.03 73.43(4) 93.20 85.50 89.17(8)
BestVsRest 81.25 66.46 73.11(8) 93.03 85.16 89.1(4)
Pref-linear 80.55 68.49 74.03(4) 93.12 85.56 89.18(4)
Table 4: Partial Match Results for four classifiers. Baseline
scores in the first row. Used n-best list numbers in parenthe-
sis. The best performances are boldfaced.
We mark an improvement of the Arg1 in table
4, with softer partial evaluation metrics; we also
observe the same trend in results for Arg2 classi-
fication as in the table 3.
4.3.1 Candidate Set Size
We conduct experiments to study the influence of
candidate set size on the quality of reranked out-
put. In addition we also attempt to notice the
upper-bound of reranker performance, i.e. the ora-
cle performance. We choose the reranker based on
online PA among the four classifier. Since all the
four classifiers performed comparably the same
way, it is enough to study the performance of one
of them on candidate set size, that will reflect the
performance of the other classifiers. We also de-
scribe and discuss the results on the exact partial
measures only, as we notice from the previous sec-
tion that the effect of reranking is comparable with
the exact measure and softer measures.
Reranked ARG1 Oracle
k P R F P R F
1 69.88 48.51 57.26 69.88 48.51 57.26
2 67.26 52.34 58.87 81.26 61.70 70.14
4 66.39 53.56 59.29 88.35 71.91 79.29
8 66.11 53.86 59.36 92.47 79.09 85.26
16 66.10 53.92 59.39 93.80 83.77 88.50
Table 5: Oracle and reranker performance as a function of
candidate set size of Arg1.
In both the tables (5, 6) we notice that the ora-
Reranked ARG2 Oracle
k P R F P R F
1 83.44 75.14 79.07 83.44 75.14 79.07
2 82.90 75.69 79.13 90.13 82.43 86.11
4 82.59 76.39 79.37 92.27 86.53 89.31
8 82.41 76.44 79.32 92.81 88.13 90.41
16 83.41 76.44 79.32 92.82 88.54 90.63
Table 6: Oracle and reranker performance as a function of
candidate set size of Arg2.
cle performance is steadily increasing with 16-best
lists. We observe that the performance of classi-
fication of both Arg1 and Arg2 increases at the
level of 2-best list then it stagnates after 4-best per-
formance. This nature of increment is may be re-
lated to the simple but high-level feature set used
in this task of the discourse parsing; and it can also
be some issues involved with local feature set, as
we observed a huge difference of posterior proba-
bilities between the single-best and the each of the
(n ? 1) lists of a n-best decision by CRF.
4.3.2 Reranked Intersentential ARG1
We also attempt to observe the effect with respect
to inter-sentential classification in case of Arg1,
with the results obtained with online PA percep-
tron. As expected, the change we notice the ef-
fects in the table 7 is a fraction of potential im-
provement. We find comparing the inter-sentential
vs. overall classification results of Arg1 that the
increment in inter-sentential Arg1 classification
considerably contribute to the overall Arg1 clas-
sification.
P R F1
Baseline Exact 52.87 27.80 36.44Partial 68.93 41.06 51.48
Overlap 79.62 41.88 54.88
Best Reranked ARG1 Exact 50.41 30.04 37.56Partial 66.51 44.95 53.78
Overlap 76.13 44.54 56.23
Table 7: Inter-sentential Reranked Arg1 Results.
5 Impact of Feature on ARG1
We study the impact of global features on the per-
formance on Arg1 reranker with the development
set (cf. Section 4.1). We are leaving behind the
feature performance of the Arg2, as the improve-
ment by the reranker for this case is not significant.
The Table 8 shows the results of investigation
through an incremental greedy-search based fea-
ture selection. All the performance steps are eval-
uated with a k of 16.
156
This impact table starts with the log posterior
only (GF0). This results to the best result achieved
by Ghosh et al (2011a) through the hill-climbing
feature analysis. Beside this, we also checked that
if we run the reranker with this feature only, then
it results to the baseline performance with the test
split.
Then the undergeneration feature (GF2) is cho-
sen through greedy search among the other fea-
tures. It gives us, jointly with the log posterior,
a significant improvement over the baseline. The
impact is predictable as GF2 addresses the basic
problem that has driven us to the current task.
The addition of the overgeneration (GF1) fea-
ture also increased the performance, though non-
significantly; this feature is important for the
reranker because this is meant for fixing a predom-
inant overgeneration problem in the n-best lists.
We observe that the F1 measure increases sig-
nificantly after adding the next important feature:
Arg1 after the connective sentence (GF4); in this
case the recall increases more in comparison to the
increment in the precision.
In the next step, the feature: Argument over-
lapping with connective (GF5) is added. This de-
creases the F1 score a bit, though it increases the
precision lowering the recall.
We reach to the second-best performance of the
Arg1 reranker after adding the feature: Argument
begins with -I tag (GF6).
The addition of the feature: Argument begins
with -E tag (GF7) does not improve the perfor-
mance much. It is possible that there was no such
mistake by CRF inside the test data.
The scores with partial and overlap matches
show the same trend so we leave the discussion
with them in order to avoid the redundancy.
Additionally, we also perform the individ-
ual effect of each of features from the set
(GF1,GF2,GF4,GF5,GF6,GF7), jointly with the
intrinsic feature GF0, but none other than the un-
dergeneration feature increased the performance
over the baseline.
The intrinsic GF0 is contributing to achieve the
baseline performance; the undergeneration (GF2)
feature is also contributing significantly. In sum-
mary, the combination of features optimizes the
performance of system in terms of F1-measure by
decreasing the value of precision and raising the
value of recall.
System P R F1GF0 (Posterior Only) 73.12 50.36 59.64GF0+GF2 69.62 55.34 61.67GF0+GF2+GF1 69.92 55.21 61.70GF0+GF2+GF1+GF4 70.12 56.05 62.30GF0+GF2+GF1+GF4+GF5 72.36 53.72 61.66GF0+GF2+GF1+GF4+GF5+GF6 71.10 55.28 62.20GF0+GF2+GF1+GF4+GF5+GF6+GF7 71.84 54.82 62.19
Table 8: Exact Match Results for Arg1 through Incremen-
tal Feature Selection.
6 Conclusion
We note a significant improvement over the best
performing model of discourse parser on the
PDTB corpus. This is mostly contributed by the
better performance in Arg1 classification.
We also find that global features have greater
impact on Arg1 classification than that of Arg2.
We investigate that that the performance of Arg1
improved by more than 10 points in terms of F1
measure using the global (see Section 3) and struc-
ture related features (see Ghosh et al (2011a)).
This happens perhaps due to the fact Arg2 is syn-
tactically bound to the connective, whereas Arg1
is not. Arg2 depends more on local features (cf.
Section 2.1) than global one. Basically this nature
of dependency of Arg1 on both local and global
features are inherited through the PDTB annota-
tion corpus, as well the local feature dependency
of Arg2 are completely data-driven.
The motivation of the paper is to make a bal-
anced classification for both the Arg1 and Arg2,
achieved by implementing the constrained-system
with global features. This enables to increase a
huge recall without losing much in terms of preci-
sion.
It is also observed that while the performances
of oracle of Arg1 and Arg2 are increasing
steadily, the performances of both the rerankers
stagnate at or before the point of 16-best lists; this
is perhaps due to our effective, simple and small
feature set.
In this task we emphasized on and studied the
data-driven, global and non-grammatical feature
set. This syntactic parse tree independent feature
set may also be effective with the dialogue data
annotated with PDTB annotation style.
7 Acknowledgement
This work was partially funded by IBM Collabo-
rative Faculty Award 2011 grant.
157
References
[Barzilay et al2004] Regina Barzilay, Lillian Lee, et al
2004. Catching the drift: Probabilistic content mod-
els, with applications to generation and summariza-
tion. In Proc. of NAACL-HLT, 2004.
[Barzilay et al2005] Regina Barzilay, Mirella Lapata,
et al 2005. Modeling local coherence: an entity-
based approach. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Lin-
guistics (ACL05).
[Charniak and Johnson2005] E. Charniak and M. John-
son. 2005. Coarse-to-fine n-best parsing and max-
ent discriminative reranking. In Proceedings of the
43rd Annual Meeting of the ACL.
[Collins and Duffy2002] Michael Collins and Nigel
Duffy. 2002. New ranking algorithms for parsing
and tagging: Kernels over discrete structures, and
the voted perceptron. In ACL02.
[Collins2000] Michael Collins. 2000. Discriminative
reranking for natural language parsing. In Compu-
tational Linguistics, pages 175?182. Morgan Kauf-
mann.
[Collins2002] Michael Collins. 2002. Ranking algo-
rithms for named-entity extraction: Boosting and the
voted perceptron. In Proceedings of ACL 2002.
[Crammer et al2006] Koby Crammer, Ofer Dekel,
Joseph Keshet, Shai Shalev-Schwartz, and Yoram
Singer. 2006. Online passive-aggressive algo-
rithms. Journal of Machine Learning Research,
7:551?585.
[Daume III et al2004] Hal Daume III, Daniel Marcu,
et al 2004. Np bracketing by maximum en-
tropy tagging and svm reranking. In Proceedings
of EMNLP?04.
[Fan et al2008] Rong-En Fan, Chih-Jen Lin, Kai-Wei
Chang, Xiang-Rui Wang, et al 2008. Liblinear:
A library for large linear classification. Journal of
Machine Learning Research.
[Ghosh et al2011a] Sucheta Ghosh, Richard Johans-
son, Giuseppe Riccardi, and Sara Tonelli. 2011a.
Shallow discourse parsing with conditional random
fields. In Proceedings of the 5th International Joint
Conference on Natural Language Processing (IJC-
NLP), Chiang Mai, Thailand.
[Ghosh et al2011b] Sucheta Ghosh, Sara Tonelli,
Giuseppe Riccardi, and Richard Johansson. 2011b.
End-to-end discourse parser evaluation. In Pro-
ceedings of 5th IEEE International Conference on
Semantic Computing, Palo Alto, CA, USA.
[Ghosh et al2012] Sucheta Ghosh, Richard Johansson,
Giuseppe Riccardi, and Sara Tonelli. 2012. Improv-
ing the recall of a discourse parser by constraint-
based postprocessing. In Proceedings of Inter-
national Conference on Languages Resources and
Evaluations (LREC 2012).
[Grosz et al1992] B.J. Grosz, J. Hircshberg, et al
1992. Some intonational characteristics of discourse
structure. In Ohala et al, editors, Proceedings of the
International Conference on Spoken Language Pro-
cessing, Vol. 1, volume 1, pages 429?432.
[Grosz et al1995] B.J. Grosz, A. K. Joshi, S. Weinstein,
et al 1995. Centering: A framework for model-
ing the local coherence of discourse. Computational
Linguistics, 21(2).
[Hjorth1993] J. S. Urban Hjorth. 1993. Computer
Intensive Statistical Methods. Chapman and Hall,
London.
[Huang and Chiang2005] Liang Huang and David Chi-
ang. 2005. Better k-best parsing. In Proceed-
ings of the 9th International Workshop on Parsing
Technologies (IWPT 2005), pages 53?64, Vancou-
ver, Canada.
[Johansson and Moschitti2010] Richard Johansson and
Alessandro Moschitti. 2010. Syntactic and seman-
tic structure for opinion expression detection. In
Proceedings of the Fourteenth Conference on Com-
putational Natural Language Learning, pages 67?
76.
[Klein and Manning2003] Dan Klein and Christo-
pher D. Manning. 2003. Fast exact inference
with a factored model for natural language parsing.
Advances in Neural Information Processing Systems
15 (NIPS 2002), Cambridge, MA: MIT Press.
[Lafferty et al2001] John Lafferty, Andrew McCallum,
and Fernando Pereira. 2001. Conditional random
fields: Probabilistic models for segmenting and la-
beling sequence data. In 18th International Conf.
on Machine Learning. Morgan Kaufmann.
[Lapata2003] Mirella Lapata. 2003. Probabilistic text
structuring: Experiments with sentence ordering. In
Proceedings of the 41st Meeting of the Association
of Computational Linguistics, pages 545?552.
[Marcus et al1993] Mitchell P. Marcus, Beatrice San-
torini, and Mary Ann Marcinkiewicz. 1993. Build-
ing a Large Annotated Corpus of English: the Penn
Treebank. Computational Linguistics, 19(2):313?
330.
[Minnen et al2001] Guido Minnen, John Carroll, and
Darren Pearce. 2001. Applied morphological pro-
cessing of English. Natural Language Engineering.
[Pitler and Nenkova2009] Emily Pitler and Ani
Nenkova. 2009. Using syntax to disambiguate ex-
plicit discourse connectives in text. In Proceedings
of the 47th Annual Meeting of the Association for
Computational Linguistics and the 4th International
Joint Conference on Natural Language Processing.
[Prasad et al2008] Rashmi Prasad, Nikhil Dinesh, Alan
Lee, Eleni Miltsakaki, Livio Robaldo, Aravind
Joshi, and Bonnie Webber. 2008. The Penn Dis-
course Treebank 2.0. In Proceedings of the 6th
158
International Conference on Languages Resources
and Evaluations (LREC 2008), Marrakech, Mo-
rocco.
[Schapire and Freund1999] Robert E. Schapire and
Yoav Freund. 1999. Large margin classification
using the perceptron algorithm. Machine Learning
Journal, 37(3):277?296.
[Shen and Joshi2003] Libin Shen and Aravind Joshi.
2003. An svm based voting algorithm with appli-
cation to parse reranking. In CoNLL 2003.
[Soricut et al2003] Radu Soricut, Daniel Marcu, et al
2003. Sentence level discourse parsing using syn-
tactic and lexical information. In Proceedings of the
Human Language Technology and North American
Association for Computational Linguistics Confer-
ence (HLT/NAACL), May 27-June 1.
[Soricut et al2006] Radu Soricut, Daniel Marcu, et al
2006. Stochastic coherence modeling, parameter
estimation and decoding for text planning applica-
tions. In Proceedings of ACL-2006 (Poster), pages
803?810.
[Toutanova et al2008] Kristina Toutanova, Aria
Haghighi, Christopher D. Manning, et al 2008.
Kristina toutanova, aria haghighi, and christopher
d. manning, a global joint model for semantic role
labeling. Computational Linguistics.
[Vapnik1995] V. Vapnik. 1995. The Nature of Statisti-
cal Learning Theory. Springer-Verlag.
[Weisstein2012] Eric W. Weisstein. 2012. ?significant
digits.? from mathworld?a wolfram web resource.
[Yamada and Matsumoto2003] Hiroyasu Yamada and
Yuji Matsumoto. 2003. Statistical dependency anal-
ysis with support vector machines. In Proceedings
of 8th International Workshop on Parsing Technolo-
gies.
159
NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 8?15,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Transferring Frames: Utilization of Linked Lexical Resources
Lars Borin
Markus Forsberg
Richard Johansson
Kaarlo Voionmaa
University of Gothenburg
first.last@svenska.gu.se
Kristiina Muhonen
Tanja Purtonen
University of Helsinki
first.last@helsinki.fi
Abstract
In our experiment, we evaluate the transfer-
ability of frames from Swedish to Finnish in
parallel corpora. We evaluate both the theo-
retical possibility of transferring frames and
the possibility of performing it using avail-
able lexical resources. We add the frame in-
formation to an extract of the Swedish side
of the Kotus and JRC-Acquis corpora using
an automatic frame labeler and copy it to the
Finnish side. We focus on evaluating the re-
sults to get an estimation on how often the
parallel sentences can be said to express the
same frame. This sheds light on the questions:
Are the same situations in the two languages
expressed using different frames, i.e. are the
frames transferable even in theory? How well
can the frame information of running text be
transferred from one language to another?
1 Introduction
To our knowledge, there is no ongoing effort to cre-
ate a framenet for Finnish. This experiment gives in-
formation on whether it is feasible to build a prelimi-
nary framenet for Finnish by transferring the frames
with their lexical units from Swedish. The building
of semantically annotated language resources from
scratch is a costly and time consuming effort. In
this experiment, we test the feasibility of utilizing
Swedish and Finnish lexical resources for building a
Finnish framenet.
Transferring lexical units from Swedish to
Finnish is possible because of the wordnet connec-
tions of both languages: both the Swedish wordnet
and the Finnish wordnet are linked to the Princeton
wordnet. This connection is described in more detail
in Section 2.
We evaluate the transferability of the frames and
their lexical units from Swedish to Finnish. In the
evaluation, we use Swedish?Finnish parallel corpora
to see whether the same sentence is expressed using
the same frames in both languages. Using parallel
corpora, we can evaluate not only the theoretically
similar content of frames in two different languages,
but also their use in actual texts.
The idea of semantic role transfer across paral-
lel corpora is not novel (see Section 2.3), but to our
knowledge, the use of linked lexical resources pro-
posed here is. The language pair Swedish?Finnish
is also one for which this methodology has not
been attempted earlier. With our experiment we
can see whether transferring the frame information
from Swedish to Finnish could work, given that the
languages are not demonstrably related, and struc-
turally quite different. The work presented here
consequently provides a data point for the evalua-
tion of the language-independence of this kind of
methodology, which can arguably only be convinc-
ingly demonstrated by actually attempting to apply it
on a range of typologically diverse languages (Ben-
der, 2011).
From a more practical point of view, there may
well be as much Finnish?Swedish as Finnish?
English parallel data, since Finnish and Swedish
are the two official languages of Finland, and all
public documents must by law be available in both
languages, and for practical reasons also a large
amount of other texts. In addition, despite their non-
relatedness and large structural differences, the two
8
languages have a long history of contact and bilin-
gualism. Finnish has borrowed words and struc-
tures from Swedish on a large scale, and the lexi-
cal semantics of the two languages have converged
in many domains. This means that we may expect
frames to transfer well across the two languages,
whereas the structural differences may make us
more pessimistic about the transferability of frame
elements.
2 Language Resources
2.1 Wordnet Connections
Wordnets are lexical databases that group words of
a language into synonym sets ? or synsets ? each
synset supposedly expressing one distinct concept in
the language. Wordnets further provide general def-
initions of the synsets, and encode the semantic rela-
tions between the synsets. Typically they are mono-
lingual, but efforts have been made to produce mul-
tilingual wordnets as well; see e.g. Vossen (1998).
FinnWordNet (Lind?n and Carlson, 2010) is a
wordnet for Finnish that complies with the format
of the Princeton WordNet (PWN) (Fellbaum, 1998).
It was built by translating the Princeton WordNet 3.0
synsets into Finnish by human translators. It is open
source and contains 117 000 synsets. The Finnish
translations were inserted into the PWN structure re-
sulting in a bilingual lexical database.
SweFN++ is an integrated open-source lexical
resource for Swedish (Borin et al, 2010; Borin,
2010). It includes the Swedish framenet (SweFN)
and Swesaurus, a Swedish wordnet. The wordnet
has been semi-automatically assembled from freely
available Swedish lexical resources (Borin and Fors-
berg, 2011), and part of it has been linked to the Core
WordNet, a 5000-synset subset of PWN. All re-
sources in SweFN++ are linked together on the word
sense level using the persistent sense identifiers of
the SweFN++ pivot resource SALDO, a large-scale
lexical-semantic resource (Borin et al, 2008; Borin
and Forsberg, 2009). Using these links, we can col-
lect a set of 434 frames and 2 694 word senses that
have a direct PWN ? Swedish wordnet ? SweFN
? FinnWordNet connection. Using these connec-
tions, we can transfer the frame information of the
words from Swedish to Finnish. We used the Korp
pipeline (Borin et al, 2012) to analyze the Swedish
part of the parallel text to get hold of the SALDO
sense identifiers. The analysis is not able to distin-
guish senses that do not differentiate themselves for-
mally (by different word forms or morphosyntactic
descriptions).
2.2 Framenet and the Semantic Labeler
Framenets are lexical databases that define seman-
tic relations. The best-known framenet is Berkeley
FrameNet which is based on the theory of frame se-
mantics (Fillmore, 1976). SweFN is built using the
same principles as the Berkeley Framenet (Ruppen-
hofer et al, 2006) of English. The frames are mostly
the same as in English.
In the experiment, we use an automatic seman-
tic role labeler for Swedish, developed by Johansson
et al (2012). The labeler is based on the Swedish
framenet and it uses the same frame and frame ele-
ment labels.
2.3 Related Work
From a methodological point of view, the first
question to ask should be whether the semantic
frames are meaningful in both languages: for in-
stance, if the Swedish FrameNet has defined a frame
SELF_MOTION and a list of associated frame ele-
ments (SELF_MOVER, GOAL, PATH etc.), does it
make sense to define an identical frame in a Finnish
FrameNet? This question has been studied by Pad?
(2007) for English?German and English?French,
and although most frames were cross-linguistically
meaningful, a number of interesting discrepancies
were found. Whether the number of discrepancies is
higher in a pair of more typologically different lan-
guages is an important question.
As far as we are aware, there has been no previ-
ous attempt in using multilingual WordNets or simi-
lar lexicons when deriving lexical units in frames in
new languages. The WordNet?FrameNet combina-
tion has seen some use in monolingual applications:
for instance, Burchardt et al (2005) and Johansson
and Nugues (2007) attempted to extend the coverage
of FrameNet by making use of WordNet. Pad? and
Lapata (2005a) used word alignment in sentence-
aligned parallel corpora to find possible lexical units
in new languages.
There have been several studies of the feasibil-
ity of automatically producing the role-semantic an-
9
notation in new languages, although never for lan-
guages as structurally dissimilar as Swedish and
Finnish. Pad? and Lapata (2005b) projected anno-
tation from English to German, and Johansson and
Nugues (2006) implemented a complete pipeline for
English?Swedish by (1) automatic annotation on the
English side; (2) annotation transfer; and (3) training
a Swedish semantic role labeler using the automati-
cally produced annotation.
3 Frames from Swedish to Finnish
3.1 Outline of the Experiment
We start off by locating such Swedish word senses
that are both represented in SweFN and linked to
PWN in two Finnish?Swedish parallel corpora. The
sentences that include such a word make up the eval-
uation data set. After this, the Swedish half is en-
riched with frame labels using the framenet-based
semantic role labeler for Swedish.
After running the semantic labeler on the evalu-
ation data, we pick the 20 most commonly occur-
ring frames from both corpora. For each of the
most common frames, we pick the 6 first occur-
rences for closer scrutiny. Due to the differing na-
ture of Swedish and Finnish, we make one change
before selecting the 20 most frequent frames: We ex-
clude the frame which is evoked (erroneously) only
by the Swedish indefinite articles en/ett ? homony-
mous with the numeral ?one?? among the 6 first oc-
currences. We take the 21st most frequent frame in-
stead because there are no articles in Finnish. To
sum up, the frames under examination are selected
based on the frequency of the frame, and the sen-
tences including the frame are selected in the order
in which they occur.
After picking 120 (6 x 20) sentences from both
corpora, the correctness of the semantic labeler is
manually checked. A linguist marks the correctness
of both the frame and the frame element label. At
this stage, the linguist does not consider the trans-
ferability of the frame, but merely checks the output
of the automatic role labeler, marking the frame and
the frame element either correct or incorrect. E.g
problematic analyses caused by polysemous words
are marked incorrect. We check the output of the
labeler before analyzing the transferability of the
frames because if the frame information is incorrect
in the Swedish text to begin with, there is no point
in transferring it to Finnish.
After checking the Swedish frame information,
the Swedish?Finnish parallel sentences are com-
pared. Two native Finnish speakers estimate,
whether the frame and frame element label is trans-
ferable to Finnish or not. Because FrameNet is
based on Frame Semantics (Fillmore, 1976), accord-
ing to which the meanings of most words can best be
understood by a description of a situation, the work-
ing hypothesis is that the semantic frames should be
more or less language neutral. Hence, the semantic
frame we assign for a certain situation in Swedish,
should be transferable to Finnish.
In addition to the theoretical frame transferability,
we also report the practical applicability of the trans-
fer via the wordnet connections. We check whether
the Swedish word is expressed in the Finnish par-
allel corpus with a word that has a direct link from
the Swedish wordnet to the Finnish wordnet via the
Princeton Wordnet. If there is no direct Wordnet link
from the Swedish word to the Finnish one, we re-
port whether the Finnish word used in the sentence
and the Finnish word linked to the Swedish word via
wordnets are in the same synset.
In sum, we manually evaluate whether the 20
most commonly occurring frames of the Swedish
test sentences are the same in the equivalent Finnish
sentences. After reporting whether the frames are
equivalent in both languages, we evaluate, how
many of the frame element labels can be transferred
to Finnish.
3.2 The Test Corpora
Presumably, transferability of the frames between
parallel corpora depends on the translation of the
corpus. Our hypothesis is that if the translator
follows the original expression very carefully, the
frames can be more similar than in a more freely
translated text. To see whether the transferability of
the frames varies according to a corpus, we used two
test corpora.
The test corpora consist of extracts from the
JRC-Acquis Corpus (Steinberger et al, 2006) and
the KOTUS Swedish?Finnish Parallel Corpus (Re-
search Institute for the Languages of Finland, 2004).
Both are Swedish?Finnish parallel corpora that are
sentence aligned. In both corpora, the text type is
10
formal: the former is a collection of legislative text
and the latter consists of press releases of different
Finnish companies.
4 Results
The evaluation consists of three parts: First and
foremost, we concentrate on estimating whether the
frame used in Swedish can be transferred to Finnish
even in theory. These results are presented in Sec-
tion 4.1. If the sentence is expressed using the same
frames, we also report how many of the frame ele-
ments encoded correctly in Swedish are realized in
Finnish (Section 4.2). In Section 4.3, we discuss the
possibility of transferring the frames via the word-
net connections. The results for the two different
corpora are presented separately enabling us to see
whether the text type impacts frame transferring.
4.1 Possibility of Transferring Frames
In Tables 1 and 2, the first column lists the 20 most
frequent frames of the evaluation corpora. The sec-
ond column shows that for all 20 frames, we took
the first six Swedish occurrences. The third column
shows how many of the Swedish frame labels are
correct. Finally, the right-most column portrays how
many of the correct Swedish frames can be trans-
ferred to Finnish. The result we are mostly inter-
ested in is the difference between the third and the
fourth columns.
As can be seen from Tables 1 and 2, most of
the correct labels for Swedish are transferable to
Finnish. In the JRC-Acquis corpus, the semantic la-
beler succeeded in 75%, and 72% of the frame la-
bels can be transferred to Finnish. The correspond-
ing success rates for the Kotus corpus are 80% and
72%.
Many of the words that are not correctly labeled
in Swedish occur in idiomatic expressions, and by
chance, some idioms are so frequent in the corpus
that they end up to our evaluation corpus. E.g. the
idiom tr?da i kraft / astua voimaan / come into effect
is expressed in the same way in both Swedish and
Finnish (lit. ?tread into force?). In both languages, a
verb usually belonging to the frame SELF_MOTION
is used in this idiom, but in the idiom, the meaning
of it cannot be said to be expressing self motion.
Some sentences in which the frames are consid-
Frame N Correct Correct
in Swe in Fin
Being_necessary 6 6 6
Calendric_unit 6 6 6
Capability 6 3 3
Coming_to_believe 6 0 0
Commitment 6 6 6
Deciding 6 6 6
Dimension 6 5 4
Leadership 6 6 6
Part_orientational 6 4 4
Political_locales 6 6 6
Possession 6 2 1
Questioning 6 1 1
Removing 6 6 6
Request 6 6 6
Scrutiny 6 6 6
Self_motion 6 0 0
Substance 6 4 4
Suitability 6 6 5
Text 6 5 5
Using 6 6 5
Total (N) 120 90 86
Total (%) 100 75 72
Table 1: Frames from the JRC-Acquis Corpus
Frame N Correct Correct
in Swe in Fin
Assistance 6 6 6
Attempt_suasion 6 6 6
Becoming 6 6 3
Business 6 6 6
Calendric_unit 6 6 6
Capability 6 3 3
Change_position_ 6 6 5
on_a_scale_increase
Commitment 6 5 5
Create_physical_artwork 6 0 0
Create_representation 6 1 1
Deciding 6 6 6
Dimension 6 3 2
Employing 6 6 6
Leadership 6 4 4
Measure_duration 6 6 6
People 6 6 6
Possession 6 3 1
Relative_time 6 5 5
Supporting 6 6 2
Transfer 6 6 6
Total (N) 120 96 85
Total (%) 100 80 72
Table 2: Frames from the Kotus Corpus
11
ered non-transferable already on a theoretical level
are expressed in Finnish completely without the
frame, as demonstrated in Example (1) and (2).
(1) Tillv?xten
growth
var
was
dock
still
mindre
smaller
?n
than
det
the
ursprungliga
original
m?let.
goal.
Still, growth was lower than what was the origi-
nal goal.
(2) Se
it
j?i
remained
kuitenkin
still
alkuper?ist?
original
tavoitetta
goal
heikommaksi.
weaker.
However, it remained weaker than what was the
original goal.
In the Swedish example (1), the word mindre
?smaller? is used when expressing the decrease of
economical growth. The word mindre fits the frame
DIMENSION, but it is used in a figurative way. The
Finnish parallel sentence could be expressed us-
ing the direct translation pienempi ?smaller? but the
translation is different. Mindre in the Finnish Ko-
tus corpus is translated as heikompi ?weaker?, which
is not expressing dimension even in a metaphorical
way.
When focusing only on the correct Swedish la-
bels, transferring frames seems to be beneficial, as
reported in Table 3. The success rate of a theoretical
possibility to use Swedish as a source language for
Finnish frames is 92%.
Correct Transferable Success %
Frames Frames
Kotus 90 86 96%
JRC-A 96 85 89%
Total 186 171 92%
Table 3: The Success Rate of Frame Transfer
Table 3 sums up the comparison of the two cor-
pora. The difference (7%) between the corpora is
not remarkable, so based on these test corpora, the
impact of the translation type is not big. In other
words, in both corpora, the correct Swedish frames
can be transferred to Finnish successfully.
4.2 Success of Transferring Frame Elements
When the sentence is expressed using the same
frames in both languages, we also report, how many
of the frame elements encoded correctly in Swedish
are realized in Finnish. These results are presented
in Tables 4 and 5. The numbers show how benefi-
cial it is to transfer the frame element labels of the
Swedish semantic labeler to Finnish.
The most common frame elements of the Swedish
corpora are listed in the first column. We scrutinize
such elements in detail which occur in the corpora
at least four times. The rest are added up and pre-
sented on the last lines of the tables. The second
column shows the frequency of the frame element,
while the third column gives the number of correct
frame element labels in the Swedish corpora. The
last column shows the number of transferable frame
elements.
As can be seen from Table 6 that sums up the re-
sults of the frame element transfer, frame element la-
bels do not transfer from Swedish to Finnish as well
as the frame labels. The success rate of the frame
transfer is 92%, where as the frame elements can be
successfully transferred in 83% of the cases.
In the Kotus corpus, 75% of the frame element la-
bels are transferable. However, there is a difference
between the two corpora: In the JRC-Acquis corpus,
91% of the elements can be transferred to Finnish.
4.3 Transferring Frames via Wordnets
Next we report how many of the Swedish frame-
evoking words are expressed using such words that
have the same wordnet identifier in Finnish. If the
parallel sentences are not expressed using words that
are equivalent in the wordnets, we examine whether
the words are in equivalent synsets. This informa-
tion is needed when estimating the usefulness of lex-
ical resources and their internal links in the frame
transferring.
In Tables 7 and 8, the first row displays the total
number of frame-evoking words. The second row
shows how many of the frames are transferable to
Finnish even in theory. The numbers on the third
row reflect the possibility of using the WordNet con-
nections in frame transferring; this number shows
how many of the words under examination are ex-
pressed both in Swedish and in Finnish with the
equivalent wordnet words. The fourth row shows
how many of the words are not directly linked with
each other but are located in equivalent synsets. On
the fifth row, we report how many of the words are
12
Frame N Correct Correct
Element in Swe in Fin
Entity 9 8 5
Speaker 8 2 2
Item 7 3 2
Theme 6 4 4
Supported 6 2 0
Recipient 6 5 5
Place 6 2 2
Whole 5 3 3
Landmark_occasion 5 5 5
Count 5 5 5
Content 5 4 4
Time_of_creation 4 0 0
Time 4 4 3
Supporter 4 1 1
Employer 4 0 0
Cognizer 4 4 4
Agent 4 2 2
Other (32 FEs) 60 35 20
Total (N) 152 89 67
Total (%) 100 59 44
Table 4: Frame Elements from the Kotus Corpus
Frame N Correct Correct
Element in Swe in Fin
Time 10 6 9
Speaker 9 2 2
Entity 9 7 5
Instrument 7 4 4
Theme 6 6 5
Evaluee 6 6 5
Ground 5 4 3
Final_category 5 5 4
Decision 5 2 2
Topic 4 0 0
Leader 4 2 2
Landmark_occasion 4 3 3
Dependent 4 4 3
Author 4 1 1
Other (32 FEs) 66 44 39
Total (N) 148 96 87
Total (%) 66 65 58
Table 5: Frame Elements from the JRC-Acquis Corpus
Correct Transferable Success %
Frame E. Frame E.
Kotus 89 67 75%
JRC-A 96 87 91%
Total 185 154 83%
Table 6: The Success Rate of Frame Element Transfer
Frame-evoking words 120
Transferable to Finnish 85
Same word as in FWN 37
In the same synset 2
Could be in the same synset 31
Table 7: Wordnet Links in the Kotus Corpus
Frame-evoking words 120
Transferable to Finnish 86
Same word as in FWN 41
In the same synset 0
Could be in the same synset 16
Table 8: Wordnet Links in the JRC-Acquis Corpus
synonyms of the word in question and could there-
fore be located in the same synset in the wordnets.
As can be seen in Tables 7 and 8, only 46% (37/85
and 41/86) of the theoretically transferable words
can be transferred to Finnish directly using the word-
net links. Our hypothesis was that we could get bet-
ter results when looking at all the words in a synset.
This appears to be a wrong assumption: There are
only 2 words that come from the same synset that
are not equivalent words used in the translations.
The numbers on the fifth rows are remarkably big,
especially when compared to the number of real-
ized synonyms on the fourth row. These 47 words
could (or should) be located in the same synset as the
words in question. If the wordnets were complete,
i.e. if all words that could be in the same synset
were in the same synset, the theoretically transfer-
able LUs would be 82% (70/85) and 65% (56/86).
5 Conclusion and Future Work
The main point of the experiment was to see if build-
ing a preliminary Finnish framenet and labeling se-
mantic roles for Finnish using Swedish resources
is feasible at all. In particular, we wanted to see
whether the same situations are expressed using the
same frames in both languages and whether it is pos-
sible to transfer the frames and frame elements with
their lexical units from one language to the other.
In our experiment, we have evaluated how well
the frames and frame elements can be transferred
from a Swedish corpus to its Finnish parallel corpus.
We have shown that in theory, 92% of the correct
Swedish frame labels and 83% of the correct frame
13
element labels can be transferred to Finnish.
We also investigated whether linked wordnets
could be used for the transfer of frame-evoking
words between Swedish and Finnish. The results
here are more ambiguous, however. On the one
hand, only about half of the words could be linked
in this way. On the other hand, it turns out that this
in part is because of many synsets being incomplete
in these wordnets which are still under construction.
Thus we should not dismiss out of hand the useful-
ness of lexical-semantic resources such as wordnets
for the task of cross-language frame transfer, but
rather explore further how the knowledge encoded
in them could be best put to use.
The result of our experiment encourages us to find
ways of performing frame transfer automatically.
This can be accomplished using a word aligned par-
allel corpus for Swedish and Finnish. The automatic
word alignment of Finnish is generally seen as a
complicated task because of the free constituent or-
der and rich morphology of Finnish. However, our
future work is to examine the success of using au-
tomatic word alignment, e.g. Giza++, in automat-
ically transferring the frame information from one
language to another.
Acknowledgements
The research presented here was supported by the
Swedish Research Council (the project Swedish
Framenet++, VR dnr 2010-6013) and by the Uni-
versity of Gothenburg through its support of the
Centre for Language Technology and its support of
Spr?kbanken (the Swedish Language Bank). The
work on linking the Swedish wordnet to the Prince-
ton Core WordNet was conducted with funding by
the European Commission through its support of
the META-NORD project under the ICT PSP Pro-
gramme, grant agreement no 270899. We would like
to thank the anonymous reviewers for their construc-
tive comments and Jyrki Niemi for his valuable help
with FinnWordNet.
References
Emily M. Bender. 2011. On achieving and evaluating
language-independence in NLP. Linguistic Issues in
Language Technology, 6(3).
Lars Borin and Markus Forsberg. 2009. All in the fam-
ily: A comparison of SALDO and WordNet. In Pro-
ceedings of the Nodalida 2009 Workshop on WordNets
and other Lexical Semantic Resources ? between Lexi-
cal Semantics, Lexicography, Terminology and Formal
Ontologies, Odense, Denmark.
Lars Borin and Markus Forsberg. 2011. Swesaurus ? ett
svenskt ordn?t med fria tyglar. LexicoNordica, 18:17?
39.
Lars Borin, Markus Forsberg, and Lennart L?nngren.
2008. The hunting of the BLARK ? SALDO, a freely
available lexical database for Swedish language tech-
nology. In Joakim Nivre, Mats Dahll?f, and Beata
Megyesi, editors, Resourceful language technology.
Festschrift in honor of Anna S?gvall Hein, number 7
in Acta Universitatis Upsaliensis: Studia Linguistica
Upsaliensia, pages 21?32. Uppsala University, Depart-
ment of Linguistics and Philology, Uppsala.
Lars Borin, Dana Dann?lls, Markus Forsberg,
Maria Toporowska Gronostaj, and Dimitrios Kokki-
nakis. 2010. The past meets the present in the
Swedish FrameNet++. In Proc. of EURALEX, pages
269?281, Leeuwarden. EURALEX.
Lars Borin, Markus Forsberg, and Johan Roxendal. 2012.
Korp ? the corpus infrastructure of Spr?kbanken. In
Proceedings of LREC 2012.
Lars Borin. 2010. Med Zipf mot framtiden ? en inte-
grerad lexikonresurs f?r svensk spr?kteknologi. Lexi-
coNordica, 17:35?54.
Aljoscha Burchardt, Katrin Erk, and Anette Frank. 2005.
A WordNet detour to FrameNet. In Proceedings of the
GLDV 2005 workshop GermaNet II, Bonn, Germany.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. Kluwer The MIT Press.
Charles J. Fillmore. 1976. Frame semantics and the na-
ture of language. Annals of the New York Academy of
Sciences: Conference on the Origin and Development
of Language and Speech, 280(1):20?32.
Richard Johansson and Pierre Nugues. 2006. A
FrameNet-based semantic role labeler for Swedish. In
Proc. of Coling/ACL.
Richard Johansson and Pierre Nugues. 2007. Using
WordNet to extend FrameNet coverage. In Proceed-
ings of the Workshop on Building Frame-semantic Re-
sources for Scandinavian and Baltic Languages, at
NODALIDA, pages 27?30, Tartu, Estonia.
Richard Johansson, Karin Friberg Heppin, and Dimitrios
Kokkinakis. 2012. Semantic role labeling with the
Swedish FrameNet. In Proceedings of LREC-2012.
Krister Lind?n and Lauri Carlson. 2010. FinnWordNet ?
WordNet p? finska via ?vers?ttning. LexicoNordica ?
Nordic Journal of Lexicography, 17:119?140.
14
Sebastian Pad? and Mirella Lapata. 2005a. Cross-
lingual bootstrapping for semantic lexicons: The case
of Framenet. In Proceedings of AAAI-05, pages 1087?
1092, Pittsburgh, United States.
Sebastian Pad? and Mirella Lapata. 2005b. Cross-
linguistic projection of role-semantic information. In
Proceedings of Human Language Technology Confer-
ence and Conference on Empirical Methods in Natu-
ral Language Processing, pages 859?866, Vancouver,
Canada.
Sebastian Pad?. 2007. Translational equivalence
and cross-lingual parallelism: The case of FrameNet
frames. In Proceedings of the NODALIDA Workshop
on Building Frame Semantics Resources for Scandina-
vian and Baltic Languages, Tartu, Estonia.
Research Institute for the Languages of Finland. 2004.
KFSPC: KOTUS Swedish-Finnish Parallel Corpus.
http://www.csc.fi/kielipankki.
Josef Ruppenhofer, Michael Ellsworth, Miriam R. L.
Petruck, Christopher R. Johnson, and Jan Scheffczyk.
2006. FrameNet II: Extended theory and practice. Un-
published Manuscript.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Toma? Erjavec, Dan Tufis?, and D?niel
Varga. 2006. The JRC-Acquis: A multilingual
aligned parallel corpus with 20+ languages. In Pro-
ceedings of the Fifth International Conference on Lan-
guage Resources and Evaluation (LREC), pages 2142?
2147.
Piek Vossen, editor. 1998. EuroWordNet: a multilingual
database with lexical semantic networks for European
Languages. Kluwer Academic Publishers.
15
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 174?184,
Baltimore, Maryland USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Rule-based and machine learning approaches for second language
sentence-level readability
Ildik
?
o Pil
?
an, Elena Volodina and Richard Johansson
Spr?akbanken, University of Gothenburg
Box 200, Gothenburg, Sweden
{ildiko.pilan, elena.volodina, richard.johansson}@svenska.gu.se
Abstract
We present approaches for the identifica-
tion of sentences understandable by sec-
ond language learners of Swedish, which
can be used in automatically generated ex-
ercises based on corpora. In this work we
merged methods and knowledge from ma-
chine learning-based readability research,
from rule-based studies of Good Dictio-
nary Examples and from second language
learning syllabuses. The proposed selec-
tion methods have also been implemented
as a module in a free web-based lan-
guage learning platform. Users can use
different parameters and linguistic filters
to personalize their sentence search with
or without a machine learning component
assessing readability. The sentences se-
lected have already found practical use as
multiple-choice exercise items within the
same platform. Out of a number of deep
linguistic indicators explored, we found
mainly lexical-morphological and seman-
tic features informative for second lan-
guage sentence-level readability. We ob-
tained a readability classification accuracy
result of 71%, which approaches the per-
formance of other models used in simi-
lar tasks. Furthermore, during an empir-
ical evaluation with teachers and students,
about seven out of ten sentences selected
were considered understandable, the rule-
based approach slightly outperforming the
method incorporating the machine learn-
ing model.
1 Introduction and motivation
Despite the fact that there is a vast selection of ex-
isting materials, many language teachers opt for
completing course syllabuses with either invented
examples or authentic resources, customized to
the need of specific learners (Howard and Major,
2004). Collections with millions of tokens of dig-
ital text are available for several languages today,
part of which would offer adequate practice mate-
rial for learners of a second or foreign language
(L2) to develop their skills further. However, a
necessary first step representing a major challenge
when reusing copora for automatic exercise gen-
eration is how to assess the suitability of the avail-
able material. In this study, we explored how we
could exploit existing Natural Language Process-
ing (NLP) tools and resources for this purpose.
To overcome copyright issues often limiting
full-text access to certain corpora, we decided to
work with sentences as linguistic unit when as-
sessing the characteristics of suitability and when
generating exercise items. Although a large num-
ber of studies exist investigating readability, i.e.
understandability, at the text level, the sentence
level remains little explored. Similarly, the focus
of previous investigations has mainly been read-
ability from native language (L1) readers? per-
spective, but aspects of L2 readability have been
less widely studied. To our knowledge no previ-
ous research have explored this latter dimension
for Swedish before, hence we aim at filling this
gap, which can be useful, besides the purposes
mentioned above, also in future sentence and text
simplification and adaptation tasks.
We propose a rule-based as well as a combi-
nation of rule-based and machine learning meth-
ods for the identification of sentences understand-
able by L2 learners and suitable as exercise items.
During the selection of linguistic indicators, we
have taken into consideration previously studied
features of readability (Franc?ois and Fairon, 2012;
Heimann M?uhlenbock, 2013; Vajjala and Meur-
ers, 2012), L2 Swedish curricula (Levy Scherrer
and Lindemalm, 2009; Folkuniversitet, 2013) and
aspects of Good Dictionary Examples (GDEX)
174
(Hus?ak, 2010; Kilgarriff et al., 2008), being that
we believe they have some properties in common
with exercise items. The current version of the
machine learning model distinguishes sentences
readable by students at an intermediate level of
proficiency from sentences of a higher readabil-
ity level. The approaches have been implemented
and integrated into an online Intelligent Computer-
Assisted Language Learning (ICALL) platform,
L?arka (Volodina et al., 2013). Besides a module
where users can experiment with the filtering of
corpus hits, a module with inflectional and vocab-
ulary exercises (making use of the selected sen-
tences with our method) is also available. An ini-
tial evaluation with students, teachers and linguists
indicated that more than 70% of the sentences
selected were understandable, and about 60% of
them would be suitable as exercise items accord-
ing to the two latter respondent groups.
2 Background
2.1 Text-level readability
Readability of texts in different languages has
been the subject of several studies and they
range from simpler formulas, taking into ac-
count superficial text properties, to more sophis-
ticated NLP methods. Traditional readability
measures for L1 Swedish at the text level in-
clude LIX (L?asbarthetsindex, ?Readability index?)
(Bj?ornsson, 1968) and the Nominal Ratio (Hult-
man and Westman, 1977). In recent years a num-
ber of studies, mostly focusing on the L1 con-
text, appeared which take into consideration lin-
guistic features based on a deeper text processing.
Morphosyntactic aspects informative for L1 read-
ability include, among others, parse tree depth,
subordination features and dependency link depth
(length) (Dell?Orletta et al., 2011). Language
models have also been commonly used for read-
ability predictions (Collins-Thompson and Callan,
2004; Schwarm and Ostendorf, 2005). A recently
proposed measure, the Coh-Metrix (Graesser et
al., 2011), aims at a multilevel analysis of texts, in-
spired by psycholinguistic principles. It measures
not only linguistic difficulty, but also cohesion in
texts.
Research on L1 readability for Swedish,
using machine learning, is described in
Heimann M?uhlenbock (2013) and Falkenjack
et al. (2013). Heimann M?uhlenbock (2013)
examined readability along five dimensions:
surface features, word usage, sentence structure,
idea density and human interest. Mean depen-
dency distance, subordinate clauses and modifiers
proved good predictors for L1 Swedish.
Although a number of readability formulas ex-
ist for native language users, these might not be
suitable predictors of L2 difficulty being that the
acquisition processes of L1 and L2 present a num-
ber of differences (Beinborn et al., 2012). Studies
focusing on L2 readability are considerably fewer
in the literature. The linguistic features in this con-
text include, among others, relative clauses, pas-
sive voice (Heilman et al., 2007) and the num-
ber of coordinate phrases per clause (Vajjala and
Meurers, 2012). Crossley et al. (2008) applied
some Coh-Metrix indicators to English L2 read-
ability. The authors found that lexical corefer-
entiality, syntactic similarity and word frequency
measures outperformed traditional L1 readability
formulas. A language-independent approach to
L2 readability assessment, using an online ma-
chine learning algorithm, is presented by Shen et
al. (2013) which, however, employed only the sur-
face features of average sentence and word length,
and word frequencies as lexical feature. The au-
thors found that none of the features in isolation
was able to clearly distinguish between the levels.
In the second language teaching scenario, a
widely used scale is the Common European
Framework of Reference for Languages (CEFR)
(Council of Europe, 2001), which, however, has
been less frequently adopted so far in readability
studies. The CEFR guidelines for L2 teaching and
assessment define six different proficiency levels:
A1 (beginner), A2 (elementary), B1 (intermedi-
ate), B2 (upper intermediate), C1 (advanced) and
C2 (proficiency). Franc?ois and Fairon (2012) pro-
posed a CEFR-based readability formula for L2
French. Some of the predictive features proved to
be structural properties, including shallow length
features as well as different morpho-syntactic cat-
egories (e.g. present participles) and the presence
of words in a list of easy words.
2.2 Sentence-level readability
Many of the text readability measures mentioned
above have shortcomings when used on very short
passages containing 100 words or less (Kilgarriff
et al., 2008). The concept of readability at the sen-
tence level can be related to the selection of ap-
propriate vocabulary example sentences. GDEX
175
(Hus?ak, 2010; Kilgarriff et al., 2008) is a sentence
evaluation algorithm, which, on the basis of lex-
ical and syntactical criteria, automatically ranks
example candidates from corpora. Some of the
influential linguistic aspects of appropriate exam-
ple sentences are: their length and structure, the
presence of short and common vocabulary items
which do not need disambiguation and the ab-
sence of anaphoric pronouns. Segler (2007) fo-
cuses on the L2 rather than on the lexicographic
context. He explores the characteristics of helpful
vocabulary examples to be used via an ICALL sys-
tem for L2 German and underlines the importance
of syntactic complexity. Research about ranking
Swedish corpus examples is presented in Volodina
et al. (2012b). Their first algorithm includes four
heuristic rules concerning sentence length, infre-
quent lexical items, keyword position and the pres-
ence of finite verbs, complemented by a sentence
similarity measure in the second algorithm. Read-
ability experiments focusing at the sentence level
have started to appear recently both for language
learning purposes (Pil?an et al., 2013) and for de-
tecting differences between simplified and unsim-
plified sentence pairs (Vajjala and Meurers, 2014).
3 Resources
Our sentence selection module utilizes a number
of tools, resources and web services available for
Swedish. Korp
1
, an infrastructure for accessing
and maintaining corpora (Borin et al., 2012), con-
tains a large number of Swedish texts which are
equipped with automatic annotations (with some
exceptions) for part-of-speech (POS), syntactic
(dependency) relations, lemma forms and sense
ids. Korp offers, among others, a web service
for concordances, which makes a search in cor-
pora based on a query (e.g. a keyword and its
POS) and returns hits with a sentence-long con-
text. Moreover, with the corpus pipeline of Korp,
tools for automatically annotating corpora are also
available. A variety of different modern Swedish
corpora from Korp have been used throughout this
study including novel, newspaper and blog texts.
Another source for sentences was the CEFR
corpus (Volodina and Johansson Kokkinakis,
2013), a collection of CEFR-related L2 Swedish
course book texts. The corpus contains: (a) man-
ual annotations indicating the structure of each les-
son in the book (exercises, instructions, texts etc.);
1
http://spraakbanken.gu.se/korp/
(b) automatic linguistic annotations obtained with
the annotation tools available through Korp. The
CEFR corpus at the time of writing included B1
texts from three course books and B2 texts from
one course book. The annotation of additional ma-
terial covering other CEFR levels was ongoing.
Not only corpora, but also information from fre-
quency word lists has been used for determining
the appropriateness of a sentence. The Kelly list
(Volodina and Kokkinakis, 2012) is a frequency-
based vocabulary list mostly built on a corpus of
web texts from 2010. Besides frequency infor-
mation, an associated CEFR level is available for
each item. Another frequency-based word list em-
ployed for the machine learning experiments is the
Wikipedia list (Volodina et al., 2012b). It contains
the POS and the number of occurrences for each
word form in a corpus of Swedish Wikipedia texts.
A central resource of the present study is L?arka
2
(Volodina et al., 2013), a freely available online
ICALL platform. Currently its exercise generator
module offers tasks both for students of linguistics
and learners of L2 Swedish (Figure 1). Additional
parts include a corpus editor used for the annota-
tion of the CEFR corpus and the sentence selection
module presented in this paper, Hit-Ex
3
(Hitta Ex-
empel, ?Find Examples? or Hit Examples). The
version under development contains also dictation
and spelling exercises (Volodina et al., 2013).
4 Machine learning experiments for
readability
4.1 Dataset
We distinguished two different classes in the
dataset for the machine learning experiments: (a)
sentences understandable at (within) B1 level and
(b) sentences above B1 level. For the former
group, sentences were collected from B1-level
texts from the CEFR corpus. Sentences above B1
level consisted partly of B2-level sentences from
the CEFR corpus, and partly of native language
sentences from Korp retrieved on the basis of key-
words between B2 and C2 levels according to the
Kelly list. Only sentences between the length of
5 and 30 tokens were collected from all resources
to decrease the influence of sentence length on the
decisions made by the classifiers and to increase
the importance of other linguistic features. The
2
http://spraakbanken.gu.se/larka/
3
http://spraakbanken.gu.se/larka/larka hitex index.html
176
Figure 1: Inflectional exercise.
size of the dataset and the number of sentences per
level are illustrated in Table 1.
Level Source Nr. sentences
Within B1 B1 (CEFR) texts 2358
Above B1
B2 (CEFR) texts 795
Korp corpora 1528
Total size of dataset 4681
Table 1: The source and the number of sentences
in the dataset.
4.2 Method
We performed supervised classification using as
training and test data the set of sentences described
in section 4.1. Thus, we aimed at a two-way clas-
sification distinguishing sentences within B1 level
from those above. This level, besides being ap-
proximately a middle point of the CEFR scale,
is typically divided into sub-levels in language
courses (Folkuniversitet, 2013) which indicates a
more substantial linguistic content. Consequently,
additional practice for learners can be beneficial at
this stage. Self-study activities may also be more
common in this phase since students have suffi-
cient L2 autonomy. We experimented with dif-
ferent classification algorithms
4
available through
the Scikit-learn Python package (Pedregosa et al.,
2011), out of which we present the results only
of the best performing one here, a linear Support
Vector Machine (SVM) classifier. The SVM clas-
sifier aims at separating instances into classes with
a hyperplane (Tanwani et al., 2009), equivalent to
a line in a two-dimensional space. This hyperplane
is defined based on the feature values of instances
and weights associated with them. Once extracted,
the values for each feature were scaled and cen-
tered.
Evaluation was carried out with stratified 10-
fold cross-validation, i.e. the proportion of labels
in each fold was kept the same as that in the whole
training set during the ten iterations of training
and testing. The evaluation measures taken into
consideration were accuracy, precision, recall and
the F1 score, a combination of precision and re-
call, the two of them being equally important (Pe-
dregosa et al., 2011).
4
The other classification methods used were a Na??ve
Bayes classifier, a decision tree and two linear algorithms:
perceptron and logistic regression.
177
4.3 Features
After a thorough overview of the machine learn-
ing approaches for readability in the literature, a
number of features were chosen to be tested in
our experiments. The features selected aimed at
a deep analysis of the sentences at different lin-
guistic levels. Besides traditional readability indi-
cators, a number of syntactic, morphological, lexi-
cal and semantic aspects have been taken into con-
sideration. Our initial set contained altogether 28
features, as presented in Table 2 on the next page.
A number of popular traditional (shallow) fea-
tures were included in the feature set (features
1-4). These required less sophisticated text pro-
cessing and had previously been used in sev-
eral studies with success (Beinborn et al., 2012;
Dell?Orletta et al., 2011; Franc?ois and Fairon,
2012; Heimann M?uhlenbock, 2013; Vajjala and
Meurers, 2012). We computed sentence length as
the number of tokens including punctuation, and
token length as the number of characters per to-
ken.
Part of the syntactic features was based on the
depth (length) and direction of dependency arcs
(features 5-8). Another group of these features
relied on the type of dependency relations. In
feature 9 (Mod) nominal pre-modifiers (e.g. ad-
jectives) and post-modifiers (e.g. relative clauses,
prepositional phrases) were counted, similarly to
Heimann M?uhlenbock (2013). Variation fea-
tures (ModVar, AdvVar) measured the ratio of a
morphosyntactic category to the number of lex-
ical (content) words in the sentence, as in Va-
jjala and Meurers (2012). These lexical cate-
gories comprised nouns, verbs, adverbs and ad-
jectives. Subordinates (11) were detected on
the basis of the ?UA? (subordinate clause minus
subordinating conjunction) dependency relation
tag (Heimann M?uhlenbock, 2013). Features De-
pDepth, Mod, Sub and RightDep, PrepComp have
previously been empoyed for Swedish L1 read-
ability at the text level in Heimann M?uhlenbock
(2013) and Falkenjack et al. (2013) respectively.
The lexical-morphological features (features
13-25) constituted the largest group. Difficulty
at the lexical level was determined based on both
the TTR feature mentioned above, expressing vo-
cabulary diversity, and on the basis of the rar-
ity of words (features 13-17) according to the
Kelly list and the Wikipedia word list. An anal-
ogous approach was adopted also by Franc?ois and
Fairon (2012), Vajjala and Meurers (2012) and
Heimann M?uhlenbock (2013) with positive re-
sults. The LexD feature considers the ratio of
lexical words (nouns, verbs, adjectives and ad-
verbs) to the sum of tokens in the sentence (Vajjala
and Meurers, 2012). The NN/VB ratio feature,
which has a higher value in written text, can also
indicate a more complex sentence (Biber et al.,
2004; Heimann M?uhlenbock, 2013). Features 21-
25 are based on evidence from the content of L2
Swedish course syllabuses (Folkuniversitet, 2013)
and course books (Levy Scherrer and Lindemalm,
2009), part of them being language-dependent,
namely S-VB/VB and S-VB%. These two features
cover different types of Swedish verbs ending in
-s which can indicate either a reciprocal verb, a
passive construction or a deponent verb, active in
meaning but passive in form (Fasth and Kanner-
mark, 1989).
Our feature set included three semantic fea-
tures (26-28). The intuition behind 28 is that
words with multiple senses (polysemous words),
increase reading complexity as, in order to under-
stand the sentence, word senses need to be dis-
ambiguated (Graesser et al., 2011). This feature
was computed by counting the number of sense
IDs per token according to a lexical-semantic re-
source for Swedish, SALDO (Borin et al., 2013),
and dividing this value by the number of tokens
in the sentence. As pronouns indicate a poten-
tially more difficult text (Graesser et al., 2011),
we included PN/NN in our set. Both NomR
and PN/NN capture idea density, i.e. how com-
plex the relation between the ideas expressed are
(Heimann M?uhlenbock, 2013).
4.4 Classification results
The results obtained using the complete set of 28
features is shown in Table 3. The results of the
SVM are presented in comparison to a baseline
classifier assigning the most frequent output label
in the dataset to each instance.
Classifier Acc F1 B1 Prec B1 Recall
Baseline 0.50 0.66 0.50 1.00
SVM 0.71 0.70 0.73 0.68
Table 3: Classification results with the complete
feature set.
The baseline classifier tagged sentences with
50% accuracy being that the split between the two
178
Nr. Feature Name Feature
ID
Nr. Feature Name Feature
ID
Traditional Lexical-morphological
1 Sentence length SentLen 13 Average word frequency
(Wikipedia list)
WikiFr
2 Average token length TokLen 14 Average word frequency (Kelly
list)
KellyFr
3 Percentage of words longer
than 6 characters
LongW% 15 Percentage of words above B1
level
DiffW%
4 Type-token ratio TTR 16 Number of words above B1
level
DiffWs
Syntactic 17 Percentage of words at B1 level B1W%
5 Average dependency depth DepDepth 18 Lexical density LexD
6 Dependency arcs deeper than 4 DeepDep 19 Nouns/verbs NN/VB
7 Deepest dependency / sentence
length
DDep /
SentLen
20 Adverb variation AdvVar
8 Ratio of right dependency arcs RightDep 21 Modal verbs / verbs MVB/VB
9 Modifiers Mod 22 Participles / verbs PCVB/VB
10 Modifier variation ModVar 23 S-verbs / verbs S-VB/VB
11 Subordinates Sub 24 Percentage of S-verbs S-VB%
12 Prepositional complements PrepComp 25 Relative pronouns RelPN
Semantic
26 Nominal ratio NomR
27 Pronoun/noun PN/NN
28 Average number of senses per
word
Sense/W
Table 2: The complete feature set.
classes was about 50-50%. The SVM classified 7
out of 10 sentences accurately. The precision and
recall values for the identification of B1 sentences
was 73% and 68%. Previous classification results
for a similar task obtained an average of 77.25%
of precision for the classification of easy-to-read
texts within an L1 Swedish text-level readabil-
ity study (Heimann M?uhlenbock, 2013). Another
classification at the sentence level, but for Italian
and from an L1 perspective achieved an accuracy
of 78.2%, thus 7% higher compared to our results
(Dell?Orletta et al., 2011). The 73% precision
of our SVM model for classifying B1 sentences
was close to the precision of 75.1% obtained for
the easy-to-read sentences from Dell?Orletta et al.
(2011). Franc?ois and Fairon (2012) in a classi-
fication study from the L2 perspective, aiming at
distinguishing all 6 CEFR levels for French at the
text level, concluded that intermediate levels are
harder to distinguish than the levels at the edges
of the CEFR scale. The authors reported an adja-
cent accuracy of 67% for B1 level, i.e. the level
of almost 7 out of 10 texts was predicted either
correctly or with only one level of difference com-
pared to the original level. Precise comparison
with previous results is, however, difficult since,
to our knowledge, there are no results reported for
L2 readability at the sentence level. Thus, the val-
ues mentioned above serve more as a side-by-side
illustration.
Besides experimenting with the complete fea-
ture set, groups of features were also separately
tested. The results are presented in Table 4.
Feature group
(Nr of features)
Acc F1
Traditional (4) 0.59 0.55
Syntactic (8) 0.59 0.54
Lexical (13) 0.70 0.70
Semantic (3) 0.61 0.55
Table 4: SVM results per feature group.
The group of traditional and syntactic features
performed similarly, with an accuracy of 59%. In-
179
Rank Feature ID Weight
1 DiffW% 0.576
2 Sense/W 0.438
3 DiffWs 0.422
4 SentLen 0.258
5 Mod 0.223
6 KellyFr 0.215
7 NomR 0.132
8 AdvVar 0.114
9 Ddep/SentLen 0.08
10 DeepDep 0.08
Table 5: The 10 most informative features
according to the SVM weights.
terestingly, although semantic features represented
the smallest group, they performed 2% better than
traditional or syntactic features. The largest group
of features including lexical-morphological indi-
cators performed around 10% more accurately
than other feature groups.
Among the 10 features that influenced most the
decisions of our SVM classifier, we can find at-
tributes from different feature groups. The ID of
these features together with the SVM weights are
reported in Table 5. An informative traditional
measure was sentence length, similarly to the re-
sults of previous studies (Beinborn et al., 2012;
Dell?Orletta et al., 2011; Franc?ois and Fairon,
2012; Heimann M?uhlenbock, 2013; Vajjala and
Meurers, 2012). Lexical-morphological features
based on information about the frequency and the
CEFR level of items in the Kelly list (DiffW%,
DiffWs and KellyFr) also proved to be influential
for the classification, as well as AdvVar. Two out
of our three semantic features, namely NomR and,
in particular, Sense/W, were also highly predictive.
Syntactic features Ddep/SentLen and DeepDep,
based on information about dependency arcs, were
also among the ten features with highest weights,
but they were somewhat less useful, as the weights
in Table 5 show.
Contrary to our results, Franc?ois and Fairon
(2012) found syntactic features more informative
than semantic ones for L2 French. This may de-
pend either on the difference between the features
used or the target languages. Moreover, in the case
of Swedish L1 text readability the noun/pronoun
ratio and modifiers proved to be indicative of text-
level difficulty (Heimann M?uhlenbock, 2013), but
at the sentence level from the L2 perspective only
the latter seemed influential in our experiments.
The data used for the experiments was labeled
for CEFR levels at the text level, not at the sen-
tence level. This introduced some noise in the data
and made the classification task somewhat harder.
In the future, the availability of data labeled at
the sentence level could contribute to more ac-
curate results. Excluding potentially lower level
sentences from those appearing in higher level
texts based on the distance between feature vec-
tors could also be explored, in a similar fashion to
Dell?Orletta et al. (2011).
5 Heuristics: GDEX parameters for
sentence filtering and ranking
Besides SVM classification, our sentence selec-
tion module, Hit-Ex, offers also a number of
heuristic parameter options
5
, usable either in com-
bination or as an alternative to the machine learn-
ing model (for further details see section 6). Part
of these search parameters are generic preferences
including the keyword to search for, its POS, the
corpora from Korp to be used during selection and
the desired CEFR level of the sentences. Further-
more, it is possible to avoid sentences containing:
abbreviations, proper names, keyword repetition,
negative formulations (inte ?not? or utom ?except?
in the sentence), modal verbs, participles, s-verbs
and sentences lacking finite verbs. Users can also
allow these categories and choose a penalty point
between 0 and -50 for them. The penalty score
for each filtering criteria is summed for obtain-
ing a final score per sentence, based on which
a final ranking is produced for all sentences re-
trieved from Korp, the ranking reflecting the ex-
tent to which they satisfy the search criteria. Some
additional parameters, partly overlapping with the
machine learning model?s features, are also avail-
able for users to experiment with, being that the
machine learning model does not cover all CEFR
levels. Based on statistical evidence from corpora,
we suggested default values for all parameters for
retrieving sentences of B1, B2, C1 level with rule-
based parameters only. However, additional data
and further testing is required to verify the appro-
priateness of the proposed values.
5
See Pil?an (2013) or the Hit-Ex webpage,
http://spraakbanken.gu.se/larka/larka hitex index.html,
for a complete list of parameters.
180
6 Combined approach
As mentioned in the previous subsection, the
heuristic parameters and the machine learning ap-
proach have been implemented and tested also
in combination. Parameters are kept to perform
a GDEX-like filtering, whilst the SVM model is
employed to ensure that hits were of a suitable
level for learners. During this combined filtering,
first a ranking for each unfiltered sentence coming
from the web service of Korp is computed with
heuristics. During these calculations, the parame-
ters partly or fully overlapping with certain fea-
tures of the machine learning model are deacti-
vated, i.e. receive penalty points set to 0, thus,
they do not influence the ranking. Instead, those
aspects are taken care of by the machine learning
model, in a subsequent step. Only the 100 sen-
tences ranked highest are given for classification
to the machine learning model for efficiency rea-
sons. Finally, once the classification has been per-
formed, sentences classified as understandable at
B1 level are returned in the order of their heuris-
tic ranking. Figure 2 shows part of the interface
of Hit-Ex, as well as the highest ranked three sen-
tences
6
of an example search for the noun hund
?dog? at B1 level. Besides the Hit-Ex page, both
the heuristics-only and the combined approaches
are available also as web services.
7 Evaluation
The purpose of the evaluation was to explore how
many sentences, collected from native language
corpora in Korp with our algorithms, were under-
standable at B1 level (at B1 or below) and thus, ap-
propriate to be presented to learners of L2 Swedish
of that CEFR level. Participants included three L2
Swedish teachers, twenty-six L2 Swedish students
at B1 level, according to their current or most re-
cent language course, and five linguists familiar
with the CEFR scale. Besides the criteria of un-
derstandability (readability), the aspect of being
an appropriate exercise item was also explored.
We selected altogether 196 sentences using both
our approaches, with two different parameter set-
tings for the rule-based method (See Pil?an et al.
(2013) and Pil?an (2013) for further details about
the evaluation). Evaluators were asked to indicate
whether they found the sentences understandable
6
English translations of the selected sentences: (1)?It
would be enough for a normal dog.?; (2)?They left the body
in the form of a dog.?; (3)?There was a person with a dog.?
at B1 level or not. Teachers and linguists (TL)
rated the sentences also as potential exercise items.
The results of the evaluation are presented in Table
6.
Understandability Exercise item
TL Students TL
76% 69% 59%
73%
Table 6: Evaluation results.
Respondents found overall 73% percent of the
sentences selected by both our methods under-
standable at B1 level, whilst somewhat less, about
six out of ten items, proved to be suitable for being
included in exercises for L2 Swedish learning.
According to our evaluators, the two settings
of the rule-based approach (Alg1-s1 and Alg1-s2)
satisfied the two criteria observed between 1-5%
more of the cases. On average, teachers, linguists
and students considered 75% of the sentences se-
lected with Alg1-s1 understandable, but only 70%
of those identified with the combined approach
(Alg2). The detailed results per algorithm, crite-
ria and user group are shown in Figure 3.
Figure 3: Comparison of algorithms.
According to our evaluators? comments, some
of the selected sentences contained difficult as-
pects at the syntactic level, among others, diffi-
cult word order, subordinates and relative clauses.
Moreover, at the lexical level, a stricter lexical fil-
tering, and checking for a sufficient amount of lex-
ical words in the sentence would be required. Re-
spondents? comments revealed also the potential
future improvement of filtering for context depen-
dency which would make sentences more suitable
as exercise items.
181
Figure 2: Part of the user interface and example search results.
8 Conclusion
In this study we investigated linguistic fac-
tors influencing the sentence-level readability of
Swedish from a L2 learning point of view. The
main contribution of our work consists of two
sentence selection methods and their implemen-
tation for identifying sentences from a variety
of Swedish corpora which are not only readable,
but potentially suitable also as automatically gen-
erated exercise items for learners at intermedi-
ate (CEFR B1) level and above. We proposed
a heuristics-only and a combined selection ap-
proach, the latter merging rule-based parameters
(targeting mainly the filtering of ?undesired? lin-
guistic elements), and machine learning methods
for classifying the readability of sentences from
L2 learners? perspective. We obtained a classi-
fication accuracy of 71% with an SVM classifier
which compares well to previously reported re-
sults for similar tasks. Our results indicate the suc-
cess of lexical-morphological and semantic fac-
tors over syntactic ones in the L2 context. The
most predictive indicators include, besides sen-
tence length, the amount of difficult words in the
sentence, adverb variation, nominal pre- and post-
modifiers and two semantic criteria, the average
number of senses per word and nominal ratio (Ta-
ble 5). Within a smaller-scale evaluation, about
73% of the sentences selected by our methods
were understandable at B1 level, whilst about 60%
of the sentences proved to be suitable as exercise
items, the heuristics-only approach being slightly
preferred by evaluators. Further investigation of
the salient properties of exercise items may con-
tribute to the improvement of the current selection
approach. The method, as well as most of the pa-
rameters and features used, are language indepen-
dent and could, thus, be applied also to languages
other than Swedish, provided that NLP tools per-
forming similarly deep linguistic processing are
available. Future additions to the filtering param-
eters may include aspects of word order, indepen-
dence from a wider context, valency information
and collocations. The optimization of the classifier
could also be studied further; different algorithms
and additional features could be tested to improve
the classification results. The machine learning
approach might show improvements in the future
with training instances tagged at the sentence level
and it can be easily extended, once additional data
for other CEFR levels becomes available. Finally,
additional evaluations could be carried out to con-
firm the appropriateness of the sentences ranked
by the extended and improved selection method.
To indicate the extent to which a sentence is un-
derstandable, 4- or 5-point scales may be used,
and the employment of exercises instead of a list
of sentences to read could also be investigated for
verifying the suitability of the examples.
182
References
Lisa Beinborn, Torsten Zesch, and Iryna Gurevych.
2012. Towards fine-grained readability measures for
self-directed language learning. In Electronic Con-
ference Proceedings, volume 80, pages 11?19.
Douglas Biber, Susan Conrad, Randi Reppen, Pat
Byrd, Marie Helt, Victoria Clark, Viviana Cortes,
Eniko Csomay, and Alfredo Urzua. 2004. Repre-
senting language use in the university: Analysis of
the TOEFL 2000 spoken and written academic lan-
guage corpus. Test of English as a Foreign Lan-
guage.
Carl Hugo Bj?ornsson. 1968. L?asbarhet. Liber.
Lars Borin, Markus Forsberg, and Johan Roxen-
dal. 2012. Korp - the corpus infrastructure of
Spr?akbanken. In Proceedings of LREC, pages 474?
478.
Lars Borin, Markus Forsberg, and Lennart L?onngren.
2013. SALDO: a touch of yin to WordNet?s yang.
Language Resources and Evaluation, 47(4):1191?
1211.
Kevyn Collins-Thompson and James P Callan. 2004.
A language modeling approach to predicting reading
difficulty. In HLT-NAACL, pages 193?200.
Council of Europe. 2001. Common European Frame-
work of Reference for Languages: Learning, Teach-
ing, Assessment. Cambridge University Press.
Scott A Crossley, Jerry Greenfield, and Danielle S
McNamara. 2008. Assessing text readability us-
ing cognitively based indices. Tesol Quarterly,
42(3):475?493.
Felice Dell?Orletta, Simonetta Montemagni, and Giu-
lia Venturi. 2011. Read-it: Assessing readability
of Italian texts with a view to text simplification.
In Proceedings of the Second Workshop on Speech
and Language Processing for Assistive Technolo-
gies, pages 73?83. Association for Computational
Linguistics.
Johan Falkenjack, Katarina Heimann M?uhlenbock, and
Arne J?onsson. 2013. Features indicating readability
in Swedish text. In Proceedings of the 19th Nordic
Conference of Computational Linguistics (NODAL-
IDA 2013), pages 27?40.
Cecilia Fasth and Anita Kannermark. 1989. Goda
grunder. Folkuniversitetets F?orlag.
Folkuniversitet. 2013. Kurser i svenska. Svenska
B1. http://www.folkuniversitetet.
se/Kurser--Utbildningar/
Sprakkurser/Svenska_Swedish/
Svenska-B1--Swedish-B1/.
Thomas Franc?ois and C?edrick Fairon. 2012. An AI
readability formula for French as a foreign language.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 466?477. Association for Computational Lin-
guistics.
Arthur C Graesser, Danielle S McNamara, and
Jonna M Kulikowich. 2011. Coh-Metrix providing
multilevel analyses of text characteristics. Educa-
tional Researcher, 40(5):223?234.
Michal J. Heilman, Kevyn Collins-Thompson, Jamie
Callan, and Maxine Eskenazi. 2007. Combining
lexical and grammatical features to improve read-
ability measures for first and second language texts.
In Proceedings of NAACL HLT, pages 460?467.
Katarina Heimann M?uhlenbock. 2013. I see what you
mean. Ph.D. thesis, University of Gothenburg.
Jocelyn Howard and Jae Major. 2004. Guidelines for
designing effective English language teaching mate-
rials. In 9th Conference of Pan Pacific Association
of Applied Linguistics.
Tor G Hultman and Margareta Westman. 1977. Gym-
nasistsvenska. Liber.
Milos Hus?ak. 2010. Automatic retrieval of good dic-
tionary examples. Bachelor Thesis, Brno.
Adam Kilgarriff, Milos Hus?ak, Katy McAdam,
Michael Rundell, and Pavel Rychl`y. 2008. GDEX:
Automatically finding good dictionary examples in
a corpus. In Proceedings of Euralex.
Paula Levy Scherrer and Karl Lindemalm. 2009. Rivs-
tart B1 + B2. Textbok. Natur och Kultur, Stockholm.
Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, et al. 2011. Scikit-learn:
Machine learning in Python. The Journal of Ma-
chine Learning Research, 12:2825?2830.
Ildik?o Pil?an, Elena Volodina, and Richard Johans-
son. 2013. Automatic selection of suitable sen-
tences for language learning exercises. In 20 Years
of EUROCALL: Learning from the Past, Looking
to the Future. 2013 EUROCALL Conference, 11th
to 14th September 2013
?
Evora, Portugal, Proceed-
ings., pages 218?225.
Ildik?o Pil?an. 2013. NLP-based Approaches to
Sentence Readability for Second Language Learn-
ing Purposes. Master?s Thesis, University of
Gothenburg. https://www.academia.edu/
6845845/NLP-based_Approaches_to_
Sentence_Readability_for_Second_
Language_Learning_Purposes.
Sarah E Schwarm and Mari Ostendorf. 2005. Read-
ing level assessment using support vector machines
and statistical language models. In Proceedings of
the 43rd Annual Meeting on Association for Com-
putational Linguistics, pages 523?530. Association
for Computational Linguistics.
183
Thomas M Segler. 2007. Investigating the selection
of example sentences for unknown target words in
ICALL reading texts for L2 German. PhD Thesis.
University of Edinburgh.
Wade Shen, Jennifer Williams, Tamas Marius, and
Elizabeth Salesky. 2013. A language-independent
approach to automatic text difficulty assessment for
second-language learners. In Proceedings of the 2nd
Workshop on Predicting and Improving Text Read-
ability for Target Reader Populations, pages 30?38.
Association for Computational Linguistics.
Ajay Kumar Tanwani, Jamal Afridi, M Zubair Shafiq,
and Muddassar Farooq. 2009. Guidelines to se-
lect machine learning scheme for classification of
biomedical datasets. In Evolutionary Computation,
Machine Learning and Data Mining in Bioinformat-
ics, pages 128?139. Springer.
Sowmya Vajjala and Detmar Meurers. 2012. On im-
proving the accuracy of readability classification us-
ing insights from second language acquisition. In
Proceedings of the Seventh Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 163?173. Association for Computational Lin-
guistics.
Sowmya Vajjala and Detmar Meurers. 2014. Assess-
ing the relative reading level of sentence pairs for
text simplification. In Proceedings of the 14th Con-
ference of the European Chapter of the Association
for Computational Linguistics (EACL-14), Gothen-
burg, Sweden. Association for Computational Lin-
guistics.
Elena Volodina and Sofie Johansson Kokkinakis. 2013.
Compiling a corpus of CEFR-related texts. In Pro-
ceedings of the Language Testing and CEFR confer-
ence, Antwerpen, Belgium, May 27-29, 2013.
Elena Volodina and Sofie Johansson Kokkinakis. 2012.
Introducing the Swedish Kelly-list, a new lexical
e-resource for Swedish. In Proceedings of LREC,
pages 1040?1046.
Elena Volodina, Richard Johansson, and Sofie Johans-
son Kokkinakis. 2012b. Semi-automatic selec-
tion of best corpus examples for Swedish: Ini-
tial algorithm evaluation. In Workshop on NLP
in Computer-Assisted Language Learning. Proceed-
ings of the SLTC 2012 workshop on NLP for CALL.
Link?oping Electronic Conference Proceedings, vol-
ume 80, pages 59?70.
Elena Volodina, Dijana Pijetlovic, Ildik?o Pil?an, and
Sofie Johansson Kokkinakis. 2013. Towards a
gold standard for Swedish CEFR-based ICALL.
In Proceedings of the Second Workshop on NLP
for Computer-Assisted Language Learning. NEALT
Proceedings Series 17. Nodalida 2013, Oslo, Nor-
way.
184
