Unsupervised Learning of the 
Morphology of a Natural Language 
John Goldsmith* 
University of Chicago 
This study reports the results of using minimum description length (MDL) analysis to model 
unsupervised learning of the morphological segmentation fEuropean languages, using corpora 
ranging in size from 5,000 words to 500,000 words. We develop aset of heuristics that rapidly 
develop aprobabilistic morphological grammar, and use MDL as our primary tool to determine 
whether the modifications proposed by the heuristics will be adopted or not. The resulting rammar 
matches well the analysis that would be developed by a human morphologist. 
In the final section, we discuss the relationship of this style of MDL grammatical nalysis to 
the notion of evaluation metric in early generative grammar. 
1. Introduction 
This is a report on the present results of a study on unsupervised acquisition 
of morphology. 1 The central task of morphological nalysis is the segmentation f 
words into the components that form the word by the operation of concatenation. 
While that view is not free of controversy, it remains the traditional conception of 
morphology, and the one that we shall employ here. 2Issues of interface with phonol- 
ogy, traditionally known as morphophonology, and with syntax are not directly 
addressed. 3 While some of the discussion is relevant o the unrestricted set of 
languages, ome of the assumptions made in the implementation restrict he use- 
ful application of the algorithms to languages in which the average number of affixes 
per word is less than what is found in such languages as Finnish, Hungarian, and 
Swahili, and we restrict our testing in the present report to more widely studied Eu- 
ropean languages. Our general goal, however, is the treatment of unrestricted natural 
languages. 
* Department of Linguistics, University of Chicago, 1010 E. 59th Street, Chicago, IL 60637. E-mail: 
ja-goldsmith@uchicago.edu. 
1 Some of the work reported here was done while I was a visitor at Microsoft Research in the winter of 
1998, and I am grateful for the support I received there. A first version was written in September, 1998, 
and a much-revised version was completed in December, 1999. This work was also supported in part 
by a grant from the Argonne National Laboratory-University of Chicago consortium, which I thank for 
its support. I am also grateful for helpful discussion of this material with a number of people, 
including Carl de Marcken, Jason Eisner, Zhiyi Chi, Derrick Higgins, Jorma Rissanen, Janos Simon, 
Svetlana Soglasnova, Hisami Suzuki, and Jessie Pinkham. As noted below, I owe a great deal to the 
remarkable work reported in de Marcken's dissertation, without which I would not have undertaken 
the work described here. I am grateful as well to several anonymous reviewers for their considerable 
improvements to the content of this paper. 
2 Sylvain Neuvel has recently produced an interesting computational implementation f a theory of 
morphology that does not have a place for morphemes, asdescribed at http://www.neuvel.net. It is 
well established that nonconcatenative morphology is found in some scattered language families, 
notably Semitic and Penutian. African tone languages require simultaneous morphological nalyses of 
the tonal and the segmental material. 
3 But see the following note. 
@ 2001 Association for Computational Linguistics 
Computational Linguistics Volume 27, Number 2 
The program in question takes a text file as its input (typically in the range of 5,000 
to 1,000,000 words) and produces a partial morphological nalysis of most of the words 
of the corpus; the goal is to produce an output hat matches as closely as possible the 
analysis that would be given by a human morphologist. It performs unsupervised 
learning in the sense that the program's ole input is the corpus; we provide the 
program with the tools to analyze, but no dictionary and no morphological rules 
particular to any specific language. At present, the goal of the program is restricted to 
providing the correct analysis of words into component pieces (morphemes), though 
with only a rudimentary categorical labeling. 
The underlying model that is utilized invokes the principles of the minimum 
description length (MDL) framework (Rissanen 1989), which provides a helpful per- 
spective for understanding the goals of traditional linguistic analysis. MDL focuses 
on the analysis of a corpus of data that is optimal by virtue of providing both the 
most compact representation f the data and the most compact means of extracting 
that compression from the original data. It thus requires both a quantitative account 
whose parameters match the original corpus reasonably well (in order to provide 
the basis for a satisfactory compression) and a spare, elegant account of the overall 
structure. 
The novelty of the present account lies in the use of simple statements of mor- 
phological patterns (called signatures below), which aid both in quantifying the MDL 
account and in constructively building a satisfactory morphological grammar (for MDL 
offers no guidance in the task of seeking the optimal analysis). In addition, the system 
whose development is described here sets reasonably high goals: the reformulation i
algorithmic terms of the strategies of analysis used by traditional morphologists. 
Developing an unsupervised learner using raw text data as its sole input offers 
several attractive aspects, both theoretical and practical. At its most theoretical, un- 
supervised learning constitutes a (partial) linguistic theory, producing a completely 
explicit relationship between data and analysis of that data. A tradition of consider- 
able age in linguistic theory sees the ultimate justification of an analysis A of any single 
language L as residing in the possibility of demonstrating that analysis A derives from 
a particular linguistic theory LT, and that that LT works properly across a range of 
languages (not just for language L). There can be no better way to make the case that 
a particular analysis derives from a particular theory than to automate that process, 
so that all the linguist has to do is to develop the theory-as-computer-algorithm; the 
application of the theory to a particular language is carried out with no surreptitious 
help. 
From a practical point of view, the development of a fully automated morphology 
generator would be of considerable interest, since we still need good morphologies 
of many European languages and to produce a morphology of a given language "by 
hand" can take weeks or months. With the advent of considerable historical text avail- 
able on-line (such as the ARTFL database of historical French), it is of great interest 
to develop morphologies of particular stages of a language, and the process of auto- 
matic morphology writing can simplify this stage--where there are no native speakers 
available---considerably. 
A third motivation for this project is that it can serve as an excellent preparatory 
phase (in other words, a bootstrapping phase) for an unsupervised grammar acqui- 
sition system. As we will see, a significant proportion of the words in a large corpus 
can be assigned to categories, though the labels that are assigned by the morpholog- 
ical analysis are corpus internal; nonetheless, the assignment of words into distinct 
morphologically motivated categories can be of great service to a syntax acquisition 
device. 
154 
Goldsmith Unsupervised Learning of the Morphology of a Natural Language 
Table 1 
Some signatures from Tom Sawyer. 
Signature Example Stem Count (type) Token Count 
NULL.ed.ing betray betrayed betraying 69 864 
NULL.ed.ing.s remain remained remaining remains 14 516 
NULL.s. cow cows 253 3,414 
e.ed.es.ing notice noticed notices noticing 4 62 
The problem, then, involves both the determination of the correct morphological 
split for individual words, and the establishment of accurate categories of stems based 
on the range of suffixes that they accept: 
. 
. 
Splitting words: We wish to accurately analyze any word into successive 
morphemes in a fashion that corresponds to the traditional linguistic 
analysis. Minimally, we wish to identify the stem, as opposed to any 
inflectional suffixes. Ideally we would also like to identify all the 
inflectional suffixes on a word which contains a stem that is followed by 
two or more inflectional suffixes, and we would like to identify 
derivational prefixes and suffixes. We want to be told that in this corpus, 
the most important suffixes are -s, -ing, -ed, and so forth, while in the 
next corpus, the most important suffixes are -e, -en, -heit, -ig, and so on. 
Of course, the program is not a language identification program, so it 
will not name the first as "English" and the second as "German" (that is 
a far easier task), but it will perform the task of deciding for each word 
what is stem and what is affix. 
Range of suffixes: The most salient characteristic of a stem in the languages 
that we will consider here is the range of suffixes with which it can 
appear. Adjectives in English, for example, will appear with some subset 
of the suffixes -er, -est, -ity, -hess, etc. We would like to determine 
automatically what the range of the most regular suffix groups is for the 
language in question, and rank suffix groupings by order of frequency in 
the corpus. 4
To give a sense of the results of the program, consider one aspect of its analysis 
of the novel The Adventures of Tom Sawyer--and this result is consistent, by and large, 
regardless of the corpus one chooses. Consider the top-ranked signatures, illustrated 
in Table 1: a signature is an alphabetized list of affixes that appear with a particular 
stem in a corpus. (A larger list of these patterns of suffixation in English are given in 
Table 2, in Section 5.) 
The present morphology learning algorithm is contained in a C++ program called 
Linguistica that runs on a desktop PC and takes a text file as its input. 5 Analyzing a 
4 In addition, one would like a statement of general rules of allomorphy as well; for example, a 
statement that the stems hit and hitt (as in hits and hitting, respectively) are forms of the same linguistic 
stem. In an earlier version of this paper, we discussed a practical method for achieving this. The work 
is currently under considerable r vision, and we will leave the reporting on this aspect of the problem 
to a later paper; there is a very brief discussion below. 
5 The executable is available at http://humanities.uchicago.edu/faculty/goldsmith/Linguistica2000, 
along with instructions for use. The functions described in this paper can be incrementally applied to a 
corpus by the user of Linguistica. 
155 
Computational Linguistics Volume 27, Number 2 
corpus of 500,000 words in English requires about five minutes on a Pentium II 333. 
Perfectly respectable results can be obtained from corpora as small as 5,000 words. 
The system has been tested on corpora in English, French, German, Spanish, Italian, 
Dutch, Latin, and Russian; some quantitative results are reported below. The corpora 
that serve as its input are largely materials that have been obtained over the Internet, 
and I have endeavored to make no editorial changes to the files that are the input. 
In this paper, I will discuss prior work in this area (Section 2), the nature of the 
MDL model we propose (Section 3), heuristics for the task of the initial splitting of 
words into stem and affix (Section 4), the resulting signatures (Section 5), use of MDL 
to search the space of morphologies (Section 6), results (Section 7), the identification 
of entirely spurious generalizations ( ection 8), the grouping of signatures into larger 
units (Section 9), and directions for further improvements (Section 10). Finally, I will 
offer some speculative observations about the larger perspective that this work sug- 
gests and work in progress (Section 11). 
2. Previous Research in this Area 
The task of automatic word analysis has intrigued workers in a range of disciplines, 
and the practical and theoretical goals that have driven them have varied consider- 
ably. Some, like Zellig Harris (and the present writer), view the task as an essential 
one in defining the nature of the linguistic analysis. But workers in the area of data 
compression, dictionary construction, and information retrieval have all contributed 
to the literature on automatic morphological analysis. (As noted earlier, our primary 
concern here is with morphology and not with regular allomorphy or morphophonol- 
ogy, which is the study of the changes in the realization of a given morpheme that 
are dependent on the grammatical context in which it appears, an area occasionally 
confused for morphology. Several researchers have explored the morphophonologies 
of natural anguage in the context of two-level systems in the style of the model de- 
veloped by Kimmo Koskenniemi \[1983\], Lauri Karttunen \[1993\], and others.) The only 
general review of work in this area that I am aware of is found in Langer (1991), which 
is ten years old and unpublished. 
Work in automatic morphological nalysis can be usefully divided into four major 
approaches. The first approach proposes to identify morpheme boundar ies  first, and 
thus indirectly to identify morphemes, on the basis of the degree of predictability of the 
n + 1st letter given the first n letters (or the mirror-image measure). This was first pro- 
posed by Zellig Harris (1955, 1967), and further developed by others, notably by Hafer 
and Weiss (1974). The second approach seeks to identify bigrams (and trigrams) that 
have a high likelihood of being morpheme internal, a view pursued in work discussed 
below by Klenk, Langer, and others. The third approach focuses on the discovery of 
patterns (we might say, of rules) of phonological relationships between pairs of related 
words. The fourth approach, which includes that used in this paper, is top-down, and 
seeks an analysis that is globally most concise. In this section, we shall review some 
of the work that has pursued these approaches--briefly, necessarily. 6 While not all 
of the approaches discussed here use no prior language-particular knowledge (which 
is the goal of the present system), I exclude from discussions those systems that are 
based essentially on a prior human-designed analysis of the grammatical morphemes 
of a language, aiming at identifying the stem(s) and the correct parsing; such is the 
6 Another effort is that attributed toAndreev (1965) and discussed in Altmann and Lehfeldt (1980), 
especially p.195 and following, though their description does not facilitate stablishing a comparison 
with the present approach. 
156 
Goldsmith Unsupervised Learning of the Morphology of a Natural Language 
case, for example, in Pacak and Pratt (1976), Koch, K~stner, and Riidiger (1989), and 
Wothke and Schrnidt (1992). With the exception of Harris's algorithm, the complex- 
ity of the algorithms is such as to make implementation for purposes of comparison 
prohibitively time-consuming. 
At the heart of the first approach, due to Harris, is the desire to place boundaries 
between letters (respectively, phonemes) in a word based on conditional entropy, in 
the following sense. We construct a device that generates a finite list of words, our 
corpus, letter by letter and with uniform probability, in such a way that at any point 
in its generation (having generated the first n letters 111213 ? ? ? In )  we can inquire of it 
what the entropy is of the set consisting of the next letter of all the continuations it
might make. (In current parlance, we would most naturally think of this as a path 
from the root of a trie to one of its terminals, inquiring of each node its associated 
one-letter entropy, based on the continuations from that node.) Let us refer to this as 
the prefix conditional entropy; clearly we may be equally interested in constructing 
a trie from the right edge of words, which then provides us with a suffix conditional 
entropy, in mirror-image fashion. 
Harris himself employed no probabilistic notions, and the inclusion of entropy 
in the formulation had to await Hafer and Weiss (1974); but allowing ourselves the 
anachronism, we may say that Harris proposed that local peaks of prefix (and suffix) 
conditional entropy should identify morpheme breaks. The method proposed in Harris 
(1955) appealed to what today we would call an oracle for information about the lan- 
guage under scrutiny, but in his 1967 article, Harris implemented a similar procedure 
on a computer and a fixed corpus, restricting his problem to that of finding morpheme 
boundaries within words. Harris's method is quite good as a heuristic for finding a 
good set of candidate morphemes, comparable in quality to the mutual information- 
based heuristic that I have used, and which I describe below. It has the same problem 
that good heuristics frequently have: it has many inaccuracies, and it does not lend 
itself to a next step, a qualitatively more reliable approximation of the correct solution. 7
Hafer and Weiss (1974) explore in detail various ways of clarifying and improving 
on Harris's algorithm while remaining faithful to the original intent. A brief summary 
does not do justice to their fascinating discussion, but for our purposes, their results 
confirm the character of the Harrisian test as heuristic: with Harris's proposal, a quan- 
titative measure is proposed (and Hafer and Weiss develop a range of 15 different 
measures, all of them rooted in Harris's proposal), and best results for morphological 
analysis are obtained in some cases by seeking a local maximum of prefix conditional 
entropy, in others by seeking a value above a threshold, and in yet others, good results 
are obtained only when this measure is paired with a similar measure constructed in 
mirror-image fashion from the end of the word- -and then some arbitrary thresholds 
are selected which yield the best results. While no single method emerges as the  best, 
one of the best yields precision of 0.91 and recall of 0.61 on a corpus of approximately 
6,200 word types. (Precision here indicates proportion of predicted morpheme breaks 
that are correct, and recall denotes the proportion of correct breaks that are predicted.) 
The second approach that can be found in the literature is based on the hypothesis 
that local information in the string of letters (respectively, phonemes) is sufficient o 
identify morpheme boundaries. This hypothesis would be clearly correct if all mor- 
pheme boundaries were between pairs of letters 11-12 that never occur in that sequence 
7 But Harris's method oes lend itself to a generalization to more difficult cases of morphological 
analysis going beyond the scope of the present paper. In work in progress, we have used minimization 
of mutual information between successive candidate morphemes a part of a heuristic for preferring a
morphological analysis in languages with a large number of suffixes per word. 
157 
Computational Linguistics Volume 27, Number 2 
morpheme internally, and the hypothesis would be invalidated if conditional proba- 
bilities of a letter given the previous letter were independent of the presence of an 
intervening boundary. The question is where real languages distribute themselves 
along the continuum that stretches between these two extremes. 
A series of publications has explored this question, including Janssen (1992), Klenk 
(1992), and Flenner (1994, 1995). Any brief description that overlooks the differences 
among these publications i  certain to do less than full justice to all of them. The 
procedure described in Janssen (1992) and Flenner (1994, 1995) begins with a training 
corpus with morpheme boundaries inserted by a human, and hence the algorithm is 
not in the domain of unsupervised learning. Each bigram (and the algorithm has been 
extended in the natural way to treating trigrams as well) is associated with a triple 
(whose sum must be less than or equal to 1.0) indicating the frequency in the training 
corpus of a morpheme boundary occurring to the left of, between, or to the right 
of that bigram. In a test word, each space between letters (respectively, phonemes) 
is assigned a score that is the sum of the relevant values derived from the training 
session: in the word string, for example, the score for the potential cut between str 
and ing is the sum of three values: the probability of a morpheme boundary after tr 
(given tr), the probability of a morpheme boundary between r and i (given ri), and 
the probability of a morpheme boundary before in (given in). 
That these numbers hould give some indication of the presence of a morpheme 
boundary is certain, for they are the sums of numbers that were explicitly assigned 
on the basis of overtly marked morpheme boundaries. But it remains unclear how 
one should proceed further with the sum. As Hafer and Weiss discover with Harris's 
measure, it is unclear whether local peaks of this measure should predict morpheme 
boundaries, or whether a threshold should be set, above which a morpheme boundary 
is predicted. Flenner (1995, 64-65) and proponents of this approach ave felt some 
freedom on making this choice in an ad hoc fashion. Janssen (1992, 81-82) observes 
that the French word linguistique displays three peaks, predicting the analysis lin- 
guist-ique, employing a trigram model. The reason for the strong, but spurious, peak 
after lin is that lin occurs with high frequency word finally, just as gui appears with 
high frequency word initially. One could respond to this observation i several ways: 
word-final frequency should not contribute to word-internal, morpheme-final status; 
or perhaps frequencies ofthis sort should not be added. Indeed, it is not clear at all why 
these numbers hould be added; they do not, for example, represent probabilities that 
can be added. Janssen otes that the other two trigrams that enter into the picture (ing 
and ngu) had a zero frequency of morpheme break in the desired spot, and proposes 
that the presence of any zeros in the sum forces the sum to be 0, raising again the 
question of what kind of quantity is being modeled; there is no scholarly tradition 
according to which the presence of zero in a sum should lead to a total of 0. 
I do not have room to discuss the range of greedy affix-parsing algorithms these 
authors explore, but that aspect of their work has less bearing on the comparison with 
the present paper, whose focus is on data-driven learning. The major question to carry 
away from this approach is this: can the information that is expressed in the division 
of a set of words into morphemes be compressed into local information (bigrams, 
trigrams)? The answer, I believe, is in general negative. Morphology operates at a 
higher level, so to speak, and has only weak statistical links to local sequencing of 
phonemes or letters,  
8 On this score, language will surely vary to some degree. English, for example, tends to employ rules of 
morphophonology to modify the surface form of morphologically complex words o as to better match 
the phonological pattern of unanalyzed words. This is discussed atlength in Goldsmith (1990, Chap. 5). 
158 
Goldsmith Unsupervised Learning of the Morphology of a Natural Language 
The third approach focuses on the discovery of patterns explicating the overt 
shapes of related forms in a paradigm. Dzeroski and Erjavec (1997) report on work 
that they have done on Slovene, a South Slavic language with a complex morphology, 
in the context of a similar project. Their goal essentially was to see if an inductive 
logic program could infer the principles of Slovene morphology to the point where 
it could correctly predict the nominative singular form of a word if it were given an 
oblique (nonnominative) form. Their project apparently shares with the present one 
the requirement that the automatic learning algorithm be responsible for the decision 
as to which letters constitute the stem and which are part of the suffix(es), though the 
details offered by Dzeroski and Erjavec are sketchy as to how this is accomplished. 
In any event, they present heir learning algorithm with a labeled pair of words- -a  
base form and an inflected form. It is not clear from their description whether the 
base form that they supply is a surface form from a particular point in the inflectional 
paradigm (the nominative singular), or a more articulated underlying representation 
in a generative linguistic sense; the former appears to be their policy. 
Dzeroski and Erjavec's goal is the development of rules couched in traditional 
linguistic terms; the categories of analysis are decided upon ahead of time by the 
programmer (or, more specifically, by the tagger of the corpus), and each individual 
word is identified with regard to what morphosyntactic features it bears. The form 
bolecina is marked, for example, as a feminine noun singular genitive. In sum, their 
project thus gives the system a good deal more information than the present project 
does. 9 
Two recent papers, Jacquemin (1997) and Gaussier (1999), deserve consideration 
here. 1? Gaussier (1999) approaches a very similar task to that which we consider, and 
takes some similar steps. His goal is to acquire derivational rules from an inflectional 
lexicon, thus insuring that his algorithm has access to the lexical category of the words 
it deals with (unlike the present study, which is allowed no such access). Using the 
terminology of the present paper, Gaussier considers candidate suffixes if they appear 
with at least two stems of length 5. His first task is (in our terms) to infer paradigms 
from signatures (see Section 9), which is to say, to find appropriate clusters of signa- 
tures. One example cited is depart, departure, departer. He used a hierarchical agglomera- 
tive clustering method, which begins with all signatures forming distinct clusters, and 
successively collapses the two most similar clusters, where similarity between stems is 
defined as the number of suffixes that two stems share, and similarity between clusters 
is defined as the similarity between the two least similar stems in the respective clus- 
ters. He reports a success rate of 77%, but it is not clear how to evaluate this figure. 11 
The task that Gaussier addresses i defined from the start to be that of derivational 
morphology, and because of that, his analysis does not need to address the problem of 
inflectional morphology, but it is there (front and center, so to speak) that the difficult 
clustering problem arises, which is how to ensure that the signatures NULL.s.'s (for 
nouns in English) and the signature NULL.ed.s (or NULL.ed.ing.s) are not assigned to 
single clusters. 12 That is, in English both nouns and verbs freely occur with the suffixes 
9 Baroni (2000) reported success using an MDL-based model in the task of discovering English prefixes. I 
have not had access to further details of the operation of the system. 
10 I am grateful to a referee for drawing my attention to these papers. 
11 The analysis of a word w in cluster C counts as a success if most of the words that in fact are related to 
w also appear in the cluster C, and if the cluster "comprised inmajority words of the derivational 
family of w." I am not certain how to interpret this latter condition; it means perhaps that more than 
half of the words in C contain suffixes hared by forms related to w. 
12 In traditional terms, inflectional morphology isresponsible for marking different forms of the same 
lexical item (lemma), while derivational morphology isresponsible for the changes in form between 
distinct but morphologically related lexical items (lemmas). 
159 
Computational Linguistics Volume 27, Number 2 
NULL and -s, and while -ed and -~s disambiguate he two cases, it is very difficult to 
find a statistical and morphological basis for this knowledge, lB 
Jacquemin (1997) explores an additional source of evidence regarding clustering of 
hypothesized segmentation f words into stems and suffixes; he notes that the hypoth- 
esis that there is a common stem gen in gene and genetic, and a common stem express 
in expression and expressed, is supported by the existence of small windows in corpora 
containing the word pair genetic...expression a d the word pair gene.., expressed (as 
indicated, the words need not be adjacent in order to provide evidence for the rela- 
tionship). As this example suggests, Jacquemin's work is situated within the context 
of a desire for superior information retrieval. 
In terms of the present study, Jacquemin's algorithm consists of (1) finding sig- 
natures with the longest possible stems and (2) establishing pairs of stems that occur 
together in two or more windows of length 5 or less. He tests his results on 100 ran- 
dom pairs discovered in this fashion, placing upper bounds on the length of the suffix 
permitted between one and five letters, and independently varying the length of the 
window in question. He does not vary the minimum size of the stem, a consideration 
that turns out to be quite important in Germanic languages, though less so in Ro- 
mance languages. He finds that precision varies from 97% when suffixes are limited 
to a length of one letter, to 64% when suffixes may be five letters long, with both 
figures assuming an adjacency window of two words; precision falls to 15% when a 
window of four words is permitted. 
Jacquemin also employs the term signature in a sense not entirely dissimilar to 
that employed in the present paper, referring to the structured set of four suffixes 
that appear in the two windows (in the case above, the suffixes are -ion, -ed; NULL, 
-tic). He notes that incorrect signatures arise in a large number of cases (e.g., good: 
optical control ~ optimal control; adoptive transfer ~ adoptively tranfer, paralleled by bad: 
ear disease ~ early disease), and suggests a quality function along the following lines: 
Stems are linked in pairs (adopt-transfer, ear-disease); compute then the average length 
of the shorter stem in each pair (that is, create a set of the shorter member of each 
pair, and find the average length of that set). The quality function is defined as that 
average divided by the length of the largest suffix in the signature; reject any signature 
class for which that ratio is less than 1.0. This formula, and the threshold, is purely 
empirical, in the sense that there is no larger perspective that bears on determining 
the appropriateness of the formula, or the values of the parameters. 
The strength of this approach, clearly, is its use of information that co-occurrence 
in a small window provides regarding semantic relatedness. This allows a more ag- 
gressive stance toward suffix identification (e.g., alpha interferon ~ alpha2 interferon). 
There can be little question that the type of corpus studied (a large technical medical 
corpus, and a list of terms--partially multiword terms) lends itself particularly to this 
style of inference, and that similar patterns would be far rarer in unrestricted text such 
as Tom Sawyer or the Brown corpus. 14 
13 Gaussier also offers a discussion of inference of regular morphophonemics, which we do not treat in 
the present paper, and a discussion in a final section of additional analysis, though without est results. 
Gaussier aptly calls our attention to the relevance of minimum edit distance relating two potential 
allomorphs, and he proposes a probabilistic model based on patterns established between allomorphs. 
In work not discussed in this paper, I have explored the integration of minimum edit distance to an 
MDL account of allomorphy as well, and will discuss this material in future work. 
14 In a final section, Jacquemin considers how his notion of signatures can be extended to identify sets of 
related suffixes (e.g., onic/atic/ic--his example). He uses a greedy clustering algorithm to successively 
add nonclustered signatures to clusters, in a fashion similar to that of Gaussier (who Jacquemin thanks 
for discussion, and of course Jacquemin's paper preceded Gaussier's paper by two years), using a 
160 
Goldsmith Unsupervised Learning of the Morphology of a Natural Language 
I laughed laughing laughs 
walked walking walks 
jumped jumping jumps J 
(a) Word list with no internal structure 
Total etter count: 57 letters 
walk ~ i 
jump j ~g 
(b) Word list with morphological structure 
Total etter count: 19 letters 
Figure 1 
Naive description length. 
The fourth approach to morphology analysis is top-down, and seeks a globally 
optimal analysis of the corpus. This general approach is based on the insight that 
the number of letters in a list of words is greater than the number of letters in a 
list of the stems and affixes that are present in the original list. This is illustrated in 
Figure 1. This simple observation lends hope to the notion that we might be able to 
specify a relatively simple figure of merit independently of how we attempt o find 
analyses of particular data. This view, appropriately elaborated, is part of the minimum 
description length approach that we will discuss in detail in this paper. 
Kazakov (1997) presents an analysis in this fourth approach, using a straightfor- 
ward measurement of the success of a morphological nalysis that we have mentioned, 
counting the number of letters in the inventory of stems and suffixes that have been 
hypothesized; the improvement in this count over the number of letters in the origi- 
nal word list is a measure of the fitness of the analysis. 15 He used a list of 120 French 
words in one experiment, and 39 forms of the same verb in another experiment, and 
employed what he terms a genetic algorithm to find the best cut in each word. He 
associated each of the 120 words (respectively, 39) with an integer (between 1 and 
the length of the word minus 1) indicating where the morphological split was to be, 
and measured the fitness of that grammar in terms of its decrease in number of total 
letters. He does not describe the fitness function used, but seems to suggest hat the 
metric more complex than the familiar minimum edit distance, but no results are offered in support of 
the choice of the additional complexity. 
15 I am grateful to Scott Meredith for drawing my attention to this paper. 
161 
Computational Linguistics Volume 27, Number 2 
single top-performing grammar of each generation is preserved, all others are elim- 
inated, and the top-performing grammar is then subjected to mutation. That is, in a 
case-by-case fashion, the split between stems and suffixes is modified (in some cases 
by a shift of a single letter, in others by an unconstrained shift to another location 
within the word) to form a new grammar. In one experiment described by Kazakov, 
the population was set to 800, and 2,000 generations were modeled. On a Pentium 90 
and a vocabulary of 120 items, the computation took over eight hours. 
Work by Michael Brent (1993) and Carl de Marcken (1995) has explored analyses of 
the fourth type as well. Researchers have been aware of the utility of the information- 
theoretic notion of compression from the earliest days of information theory, and there 
have been efforts to discover useful, frequent chunks of letters in text, such as Rad- 
hakrishnan (1978), but to my knowledge, Brent's and de Marcken's works were the 
first to explicitly propose the guiding of linguistic hypotheses by such notions. Brent's 
work addresses the question of determining the correct morphological analysis of a 
corpus of English words, given their syntactic ategory, utilizing the notion of minimal 
encoding, while de Marcken's addresses the problem of determining the "breaking" of 
an unbroken stream of letters or phonemes into chunks that correspond as well as pos- 
sible to our conception of words, implementing a well-articulated algorithm couched 
in a min imum description length framework, and exploring its effects on several large 
corpora. 
Brent (1993) aims at finding the appropriate set of suffixes from a corpus, rather 
than the more comprehensive goal of finding the correct analysis for each word, both 
stem and suffix, and I think it would not be unfair to describe it as a test-of-concept 
trial on a corpus ranging in size from 500 to 8,000 words; while this is not a small 
number of words, our studies below focus on corpora with on the order of 30,000 
distinct words. Brent indicates that he places other limitations as well on the hypothesis 
space, such as permitting no suffix which ends in a sequence that is also a suffix (i.e., 
if s is a suffix, then less and ness are not suffixes, and if y is a suffix, ity is not). 
Brent's observation is very much in line with the spirit of the present analysis: "The 
input lexicons contained thousands of non-morphemic endings and mere dozens of 
morphemic suffixes, but the output contained primarily morphemic suffixes in all cases 
but one. Thus, the effects of non-morphemic regularities are minimal" (p. 35). Brent's 
corpora were quite different from those used in the experiments reported below; his 
were based on choosing the n most common words in a Wall Street Journal corpus, 
while the present study has used large and heterogeneous sources for corpora, which 
makes for a considerably more difficult task. In addition, Brent scored his algorithm 
solely on how well it succeeded in identifying suffixes (or combinations of suffixes), 
rather than on how well it simultaneously analysed stem and suffix for each word, 
the goal of the present study. ~6 Brent makes clear the relevance and importance of 
information-theoretic notions, but does not provide a synthetic and overall measure 
of the length of the morphological grammar. 
16 Brent's description of his algorithm isnot detailed enough to satisfy the curiosity of someone like the 
present writer, who has encountered problems that Brent's approach would seem certain to encounter 
equally. As we shall see below, the central practical problem to grapple with is the fact that when 
considering suffixes (or candidate suffixes) consisting of only a single letter (let us say, s, for example), 
it is extremely difficult o get a good estimate of how many of the potential occurrences (of word-final 
s) are suffixal sand how many are not. As we shall suggest towards the end of this paper, the only 
accurate way to make an estimate ison the basis of a multinomial estimate once larger suffix 
signatures have been established. Without his, it is difficult not to overestimate the frequency of 
single-letter suffixes, aresult hat may often, in my experience, deflect the learning algorithm from 
discovering a correct two-letter suffix (e.g., the suffix -al in French). 
162 
Goldsmith Unsupervised Learning of the Morphology of a Natural Language 
De Marcken (1995) addresses a similar but distinct task, that of determining the 
correct breaking of a continuous tream of segments into distinct words. This prob- 
lem has been addressed in the context of Asian (Chinese-Japanese-Korean) l guages, 
where standard orthography does not include white space between words, and it has 
been discussed in the context of language acquisition as well. De Marcken describes 
an unsupervised learning algorithm for the development of a lexicon using a mini- 
mum description length framework. He applies the algorithm to a written corpus of 
Chinese, as well as to written and spoken corpora of English (the English text has had 
the spaces between words removed), and his effort inspired the work reported here. 
De Marcken's algorithm begins by taking all individual characters to be the baseline 
lexicon, and it successively adds items to the lexicon if the items will be useful in 
creating a better compression of the corpus in question, or rather, when the improve- 
ment in compression yielded by the addition of a new item to the codebook is greater 
than the length (or "cost") associated with the new item in the codebook. In general, a
lexical item of frequency F can be associated with a compressed length of - log F, and 
de Marcken's algorithm computes the compressed length of the Viterbi-best parse of 
the corpus, where the compressed length of the whole is the sum of the compressed 
lengths of the individual words (or hypothesized chunks, we might say) plus that of 
the lexicon. In general, the addition of chunks to the lexicon (beginning with such 
high-frequency items as th) will improve the compression of the corpus as a whole, 
and de Marcken shows that successive iterations add successively arger pieces to the 
lexicon. De Marcken's procedure builds in a bottom-up fashion, looking for larger 
and larger chunks that are worth (in an MDL sense) assigning the status of dictionary 
entries. Thus, if we look at unbroken orthographic texts in English, the two-letter com- 
bination th will become the first candidate chosen for lexical status; later, is will achieve 
that status too, and soon this will as well. The entry this will not, in effect, point to 
its four letters directly, but will rather point to the chunks th and is, which still retain 
their status in the lexicon (for their robust integrity is supported by their appearance 
throughout the lexicon). The creation of larger constituents will occasionally lead to 
the elimination of smaller chunks, but only when the smaller chunk appears almost 
always in a single larger unit. 
An example of an analysis provided by de Marcken's algorithm is given in (1), 
taken from de Marcken (1995), in which I have indicated the smallest-level constituent 
by placing letters immediately next to one another, and then higher structure with 
various pair brackets (parentheses, etc.) for orthographic convenience; there is no the- 
oretical significance to the difference between "( )" and "0", etc. De Marcken's analysis 
succeeds quite well at identifying words, but does not make any significant effort at 
identifying morphemes as such. 
( \ [ the \ ]{ ( \ [un i t \ ]ed) ( \ [ s ta t \ ]es )}) (o f{ame( \ [ r i c \ ] )a})  (1) 
Applying de Marcken's algorithm to a "broken" corpus of a language in which 
word boundaries are indicated (for example, English) provides interesting results, but 
none that provide anything even approaching a linguistic analysis, such as identifica- 
tion of stems and affixes. The broken character of the corpus serves essentially as an 
upper bound for the chunks that are postulated, while the letters represent the lower 
bound. 
De Marcken's MDL-based figure of merit for the analysis of a substring of the 
corpus is the sum of the inverse log frequencies of the components of the string in 
question; the best analysis is that which minimizes that number (which is, again, the 
optimal compressed length of that substring), plus the compressed length of each 
163 
Computational Linguistics Volume 27, Number 2 
of the lexical items that have been hypothesized to form the lexicon of the corpus. 
It would certainly be natural to try using this figure of merit on words in English, 
along with the constraint that all words should be divided into exactly two pieces. 
Applied straightforwardly, however, this gives uninteresting results: words will always 
be divided into two pieces, where one of the pieces is the first or the last letter of 
the word, since individual etters are so much more common than morphemes. 17 (I 
will refer to this effect as peripheral cutting below.) In addition--and this is less 
obvious--the hierarchical character of de Marcken's model of chunking leaves no 
place for a qualitative difference between high-frequency "chunks," on the one hand, 
and true morphemes, on the other: str is a high-frequency hunk in English (as schl 
is in German), but it is not at all a morpheme. The possessive marker ~s, on the other 
hand, is of relatively low frequency in English, but is clearly a morpheme. 
MDL is nonetheless the key to understanding this problem. In the next section, 
I will present a brief description of the algorithm used to bootstrap the problem, 
one which avoids the trap mentioned briefly in note 21. This provides us with a 
set of candidate splittings, and the notion of the signature of the stem becomes the 
working tool for determining which of these splits is linguistically significant. MDL 
is a framework for evaluating proposed analyses, but it does not provide a set of 
heuristics that are nonetheless essential for obtaining candidate analyses, which will 
be the subject of the next two sections. 
3. Minimum Description Length Model 
The central idea of minimum description length analysis (Rissanen 1989) is composed 
of four parts: first, a model of a set of data assigns a probability distribution to the 
sample space from which the data is assumed to be drawn; second, the model can then 
be used to assign a compressed length to the data, using familiar information-theoretic 
notions; third, the model can itself be assigned a length; and fourth, the optimal anal- 
ysis of the data is the one for which the sum of the length of the compressed data 
and the length of the model is the smallest. That is, we seek a minimally compact 
specification of both the model and the data, simultaneously. Accordingly, we use the 
conceptual vocabulary of information theory as it becomes relevant to computing the 
length, in bits, of various aspects of the morphology and the data representation. 
3.1 A First Model 
Let us suppose that we know (part of) the correct analysis of a set of words, and we 
wish to create a model using that knowledge. In particular, we know which words 
have no morphological nalysis, and for all the words that do have a morphological 
analysis, we know the final suffix of the word. (We return in the next section to how we 
might arrive at that knowledge.) An MDL model can most easily be conceptualized if 
we encode all such knowledge by means of lists; see Figure 2. In the present case, we 
have three lists: a list of stems, of suffixes, and of signatures. We construct a list of the 
stems of the corpus defined as the set of the unanalyzed words, plus the material that 
precedes the final suffix of each morphologically analyzed word. We also construct 
a list of suffixes that occur with at least one stem. Finally, each stem is empirically 
associated with a set of suffixes (those with which it appears in the corpus); we call 
this set the stem's ignature, and we construct a third list, consisting of the signatures 
that appear in this corpus. This third list, however, contains no letters (as the other 
17 See note 21 below. 
164 
Goldsmith Unsupervised Learning of the Morphology of a Natural Language 
A. Affixes: 6 
1. NULL 
2. ed 
3. ing 
4. s 
5. e 
i 
6. es 
B. Stems: 9 
!1. cat 
2. dog 
3. hat 
4. John 
5. jump 
6. laugh 
7. sav 
8. the 
9. walk 
C. Signatures: 4 
Signature 1:/ treat 
SimpleStem : ptr(dog) 
SimpleStem : ptr(hat) L ptr(s) J 
ComplexStem : ptr(Sig2): ptr(sav) + ptr(ing) 
Signature 2:
f ptr(e) ~ 
{SimpleStem:ptr(sav)} ~ptr(es) ~ 
~, ptr(ing) ) 
Signature 3:
ptr(NULL) " 
f SimpleStem:ptr(jump) ~ ~ ptr(ed) 
~ SimpleStem :ptr(laugh) ~ | ptr(ing) 
I, SimpleStem : ptr(walk) ) I, ptr(s) 
Signature 4:
SimpleStem : ptr(John) 
SimpleStem : ptr(the) J 
Figure 2 
A sample morphology. This morphology covers the words: cat, cats, dog, dogs, hat, hats, save, 
saves, saving, savings, jump, jumped, jumping, jumps, laugh, laughed, laughing, laughs, walk, walked, 
walking, walks, the, John. 
lists do), but rather pointers to stems and suffixes. We do this, in one sense, because 
our goal is to construct he smallest morphology,  and in general a pointer requires less 
information than an explicit set of letters. But in a deeper sense, it is the signatures 
whose compactness provides the explicit measurement  of the conciseness of the entire 
analysis. Note that by  construction, each stem is associated with exactly one signature. 
165 
Computational Linguistics Volume 27, Number 2 
Since stem, suffix, and signature all begin with s, we opt for using t to represent 
a stem, f to represent a suffix, and cr to represent a signature, while the uppercase 
T, F, E represent the sets of stems, suffixes, and signatures, respectively. The number 
of members of such a set will be represented (T) , (F/, etc., while the number of 
occurrences of a stem, suffix, etc., will be represented as \[t\], \[f\], etc. The set of all 
words in the corpus will be represented as W; hence the length of the corpus is \[W\], 
and the size of the vocabulary is (W). 
Note the structure of the signatures in Figure 2. Logically a signature consists 
of two lists of pointers, one a list of pointers to stems, the other a list of pointers to 
suffixes. To specify a list of length N, we must specify at the beginning of the signature 
that N items will follow, and this requires just slightly more than log 2 N bits to do (see 
Rissanen \[1989, 33-34\] for detailed discussion); I will use the notation A(N) to indicate 
this function. 
A pointer to a stem t, in turn, is of length - log  prob (t), a basic principle of 
information theory (Li and Vit8nyi 1997). Hence the length of a signature is the sum 
of the (inverse) log probabilities of its stems, plus that of its suffixes, plus the number 
of bits it takes to specify the number of its stems and suffixes, using the A function. 
We will return in a moment o how we determine the probabilities of the stems and 
suffixes; looking ahead, it will be the empirical frequency. 
Let us consider the length of stem list T. As we have already observed, its length 
is ),((T))--this is the length of the information specifying how long the list is--plus 
the length of each stem specification. In most of our work, we make the assumption 
that the length of a stem is the number of letters in it, weighted by the factor log 26 
converting to binary bits, in a language with 26 lettersJ 8 The same reasoning holds 
for the suffix list F: its length is X((F)) plus the length of each suffix, which we may 
take to be the total number of letters in the suffix times log 26. 
We return to the question of how long the pointer (found inside a signature) to a 
stem or suffix is. The probability of a stem is its (empirical) frequency, i.e., the total 
number of words in the corpus corresponding to the words whose analysis includes 
the stem in question; the probability of a suffix is defined in parallel fashion. Using 
W to indicate all the words of the corpus, we may say that the length of a pointer to 
a stem t is of length 
a pointer to suffix f is of length 
log \[w\] 
\[t\] '
log \[% K' 
18 This is a reasonable, and convenient, assumption, but it may not be precise nough for all work. A 
more refined measure would take the length of a letter to be -1 times the binary log of its frequency. 
A still more refined measure would base the probability of a letter on bigram context; his matters for 
English, where stem final t is very common. In addition, there is information i the linear order in 
which the letters are stored, roughly equal to 
n 
~-~ log 2 k 
k=l  
for a string of length n (compare the information that distinguishes the lexical representation f 
anagrams). This is an additional consideration in an MDL analysis of morphology pressing in favor of 
breaking words into morphemes when possible. 
166 
Goldsmith Unsupervised Learning of the Morphology of a Natural Language 
and a pointer to a signature cr is of length 
\[w\] log -\[cr\] " 
We have now settled the question of how to determine the length of our initial 
model; we next must determine the probability that the model assigns to each word 
in the corpus, and armed with that knowledge, we will be able to compute the com- 
pressed length of the corpus. 
The morphology assigns a probability to each word w as the product of the prob- 
ability of w's signature times w's stem, given its signature, and w's suffix, given its 
signature: prob (w = t +f )  = prob (c 0 prob (t I or) prob (f \] or), where cr is the signa- 
ture associated with t: cr = sig(t). Thus while stems and suffixes, which are defined 
relative to a particular morphological model, are assigned their empirical frequency 
as their probability, words are assigned a probability based on the model, one which 
will always depart from the empirical frequency. The compression to the corpus is 
thus worse than would be a compression based on word frequency alone, 19 or to put 
it another way, the morphological analysis in which all words are unanalyzed is the 
analysis in which each word is trivially assigned its own empirical frequency (since 
the word equals the stem). But this decrease in compression that comes with morpho- 
logical analysis is the price willingly paid for not having to enter every distinct word 
in the stem list of the morphology. 
Summarizing, the compressed length of the corpus is 
Z \[w\](log prob(cr(w)) + log prob(t) + log prob(f \] or(w))), 
w~tq-f  
where we have summed over the words in the corpus, and or(w) is the signature to 
which word w is assigned. The compressed length of the model is the length of the 
stem list, the suffix list, and the signature list. The length in bits of the stem list is 
&((T)) + ~ Ltypo(t) 
tCStems 
and the length of the suffix list is 
A((r)) + L, po(f), 
f ff Suff ixes 
where LtvpoO is the measurement of the length of a string of letters in bits, which we 
take to be log 2 26 times the number of letters (but recall note 18). The length of the 
signature list is 
A((~,)) + Z L(?), 
c~ ff Sign atures 
where L(~) is the length of signature or. If the set of stems linked to signature a is 
T(~r) and the set of suffixes linked to signature a is F(a), then 
+ + S-" log \[w\] + fcr(?)Z log \[words(f) N words(cr)\]" 
19 Due to the fact that the cross-entropy is always greater than or equal to the entropy. 
167 
Computational Linguistics Volume 27, Number 2 
(The denominator in the last term consists of the token count of words in a particular 
signature with the given suffix f ,  and we will refer to this below more simply as 
in cr\].) 
It is no doubt easy to get lost in the formalism, so it may be helpful to point out 
what the contribution of the additional structure accomplishes. We observed above that 
the MDL analysis is an elaboration of the insight that the best morphological nalysis 
of a corpus is obtained by counting the total number of letters in the list of stems and 
suffixes according to various analyses, and choosing the analysis for which this sum is 
the least (cf. Figure 2). This simple insight fails rapidly when we observe in a language 
such as English that there are a large number of verb stems that end in t. Verbs appear 
with a null suffix (that is, in bare stem form), with the suffixes -s, -ed, and -ing. But 
once we have 11 stems ending in t, the naive letter-counting approach will judge it a 
good idea to create a new set of suffixes: -t, -ted, -ts, and -ting, because those 10 letters 
will allow us to remove 11 or more letters from the list of stems. It is the creation of the 
lists, notably the signature list, and an information cost which increases as probability 
decreases, that overcomes that problem. Creating a new signature may save some 
information associated with the stem list in the morphology, but since the length of 
pointers to a signature cr is - log freq (0), the length of the pointers to the signatures 
for all of the words in the corpus associated with the old signature (-O, -ed, -s, -ing) or 
the new signature (-ts, -ted, -ting, -ts) will be longer than the length of the pointers to a 
signature whose token count is the sum of the token count of the two combined, i.e., 
x l?g (~-~)+y l?g  (~)~ (x+y) l ?g  (x -~y)  ? 
3.2 Recursive Morphological Structure 
The model presented above is too simple in that it underestimates the gain achieved 
by morphological analysis in case the word that is analyzed is also a stem of a larger 
word. For example, if a corpus contains the words work and working, then morpholog- 
ical analysis will allow us to dispense with the form working; it is modeled by the stem 
work and the suffixes -O and -ing. If the corpus also includes workings, the analysis 
working-s additionally lowers the cost of the stem working. Clearly we would like stems 
to be in turn analyzable as stems + suffixes. Implementing this suggestion involves 
the following modifications: (i) Each pointer to a stem (and these are found both in the 
compressed representation f each individual word in the corpus, and inside the indi- 
vidual signatures of the morphological model) must contain a flag indicating whether 
what follows is a pointer to a simple member of the stem list (as in the original model), 
or a triple pointer to a signature, stem, and suffix. In the latter case, which would be 
the case for the word \[work-ing\]-s, the pointer to the stem consists of a triple identical 
to the signature for the word work-ing. (ii) The number of words in the corpus has 
now changed, in that the word \[work-ing\]-s now contains two words, not one. We will 
need to distinguish between counts of a word w where w is a freestanding word, and 
counts where it is part of a larger word; we shall refer to the latter class as secondary 
counts. In order to simplify computation and exposition, we have adopted the con- 
vention that the total number of words remains fixed, even when nested structure is 
posited by the morphology, thus forcing the convention that counts are distributed in 
a nonintegral fashion over the two or more nested word structures found in complex 
words. We consider the more complex case in the appendix. 2? 
20 In addition, the number of words in a corpus will change if the analysis determines that all 
occurrences of (let us say) -ings are to be reanalyzed as complex words, and the stem in question 
168 
Goldsmith Unsupervised Learning of the Morphology of a Natural Language 
We may distinguish between those words, like work or working, whose immediate 
analysis involves a stem appearing in the stem list (we may call these WSIMPLE ) and 
those whose analysis, like workings, involves recursive structure (we may call these 
WCOMPLEX ). AS we have noted, every stern entry in a signature begins with a flag 
indicating which kind of stem it is, and this flag will be of length 
\[wl 
log \[WsIMPLE \]
for simple stems, and of length 
\[w\] 
log \[WcoMPrZX\] 
for complex stems. We also keep track separately of the total number of words in the 
corpus (token count) that are morphologically analyzed, and refer to this set as WA; 
this consists of all words except hose that are analyzed as having no suffix (see item 
(ii) in (2), below). 
(2) Compressed length of morphology 
(i) 
(ii) 
(ii) 
(iii) 
(iv) 
hiT) + a(r) + 
Suffixlist: fc s~E (/~*lf\[ + lOg \[WA\]'~  
Suffixlist: E (l?g26*lengthOC)+l?g~ -~) 
f ff Suffizes 
Stem list: t~cT (lOg26*length(t) + log (~)  ) 
Signature component 
Stated once for the whole component: 
(a) Signature list: E log \[w\] 
For each signature: 
(b) Size of the count of the number of stems plus size of the 
count of the number of suffixes: 
;~((stems(a))) + ~((suffixes(a))) 
(c) A pointer to each stem, consisting of a simple/complex flag, 
and a pointer to either a simple or complex stem: 
(i) Case of simple stem: flag of length 
\[w\] 
log \[WsIMPLE\] 
(perhaps work-ing) did not appear independently as a freestanding word in the corpus; we will refer to 
these inferred words as being "virtual" words with virtual counts. 
169 
Computational Linguistics Volume 27, Number 2 
plus a pointer to a stem of length 
log \[w\]. 
\[t\] '
(ii) 
or  
Case of complex stem: flag of length 
\[w\] 
log \ [WcoMPLEX\]"  
followed by a sequence of two pointers of total 
length 
\[w\] \[~\] 
log \[stem(t)~-~ + log \[suffix(t) in cr\]" 
(d) a pointer to each suffix, of total length 
v'z_. log ~ in ~\] 
f c suyfixe~ ( ~ ) 
(3) Compressed length of corpus 
\[w\] \[~(w)\] \[~(w)\] \] \[w\] log ~ + log + log 
\[stem(w)\] \[suffix(w)in a(w)\]\] wEW 
MDL thus provides a figure of merit that we wish to minimize, and we will seek 
heuristics that modify the morphological nalysis in such a fashion as to decrease this 
figure of merit in a large proportion of cases. In any given case, we will accept a 
modification to our analysis just in case the description length decreases, and we will 
suggest hat this strategy coincides with traditional linguistic judgment in all clear 
cases. 
4. Heuristics for Word Segmentation 
The MDL model designed in the preceding section will be of use only if we can provide 
a practical means of creating one or more plausible morphologies for a given corpus. 
That is, we need bootstrapping heuristics that enable us to go from a corpus to such 
a morphology. As we shall see, it is not in fact difficult to come up with a plausible 
initial morphology, but I would like to consider first an approach which, though it 
might seem like the most natural one to try, fails, and for an interesting reason. 
The problem we wish to solve can be thought of as one suited to an expectation- 
maximization (EM) approach (Dempster, Laird, and Rubin 1977). Along such a line, 
each word w of length N would be initially conceived of as being analyzed in N 
different ways, cutting the word into stem + suffix after i letters, 1 K i < N, with each 
of these N analyses being assigned probability mass of 
\[w\] 
N\[W\]" 
170 
Goldsmith Unsupervised Learning of the Morphology of a Natural Language 
That probability mass is then summed over the resulting set of stems and suffixes, 
and on successive iterations, each of the N cuts into stem + suffix is weighted by its 
probability; that is, if the ith cut of word w, of length I, cuts it into a stem t of length i
and suffix of length 1 - i, then the probability of that cut is defined as 
pv(stem t = wl,i)pr(suffix f = Wi+l,l) 
N 
pr(stem t = Wl,k)pf(suffl"x f = Wk+l,l) 
k=l 
where ZOj,k refers to the substring of w from the jth to the kth letter. Probability mass 
for the stem and the suffix in each such cut is then augmented by an amount equal 
to the frequency of word w times the probability of the cut. After several iterations 
(approximately four), estimated probabilities tabilize, and each word is analyzed on 
the basis of the cut with the largest probability. 
This initially plausible approach fails because it always prefers an analysis in which 
either the stem or (more often) the suffix consists of a single letter. More importantly, 
the probability that a sequence of one or more word-final etters is a suffix is very 
poorly modeled by the sequence's frequency. 21To put the point another way, even the 
initial heuristic analyzing one particular word must take into account all of the other 
analyses in a more articulated way than this particular approach does. 
I will turn now to two alternative heuristics that succeed in producing an initial 
morphological nalysis (and refer to a third in a note). It seems likely that one could 
construct a number of additional heuristics of this sort. The point to emphasize is 
that the primary responsibility of the overall morphology is not that of the initial 
heuristic, but rather of the MDL model described in the previous ection. The heuristics 
described in this section create an initial morphology that can serve as a starting point 
in a search for the shortest overall description of the morphology. We deal with that 
process in Section 5. 
4.1 First Heuristic 
A heuristic that I will call the take-all-splits heuristic, and which considers all cuts of a 
word of length 1 into stem+suffix Wl,i -t- Wi+l,l, where 1 G i < 1, much like the EM ap- 
proach mentioned immediately above, works much more effectively if the probability 
is assigned on the basis of a Boltzmann distribution; see (4) below. The function H(.) 
in (4) assigns a value to a split of word w of length h w U + wi+l,l. H does not assign a 
proper distribution; we use it to assign a probability to the cut of w into w~,i + wi+u as 
in (5). Clearly the effect of this model is to encourage splits containing relatively long 
suffixes and stems. 
H(Wl,i q- Wi+l,1) = - ( / log  freq (stem = Wl , i )  q -  ( l  - i)log freq (suffix = wi+u)) (4) 
prob (w = Wl, i q- Wi+l,l) = le-H(w"i+Wi+l,l) (5) 
z, 
21 It is instructive to think about why this should be so. Consider a word such as diplomacy. If we cut the 
word into the pieces diplomac + y, its probability is freq (diplomac)* freq (y), and constrast that value 
with the corresponding values of two other analyses: freq (diploma)* freq (cy), and 
freq (diplom)* freq (acy). Now, the ratio of the frequency of words that begin with diploma and those 
that begin with diplomac is less than 3, while the ratio of the frequency of words that end in y and 
those that end in cy is much greater. In graphical terms, we might note that tries (the data structure) 
based on forward spelling have by far the greatest branching structure arly in the word, while tries 
based on backward spelling have the greatest branching structure close to the root node, which is to 
say at the end of the word. 
171 
Computational Linguistics Volume 27, Number 2 
where 
n--1 
Z = ~ H(Wl,i q- Wi+l,1) 
i=1 
For each word, we note what the best parse is, that is, which parse has the highest 
rating by virtue of the H-function. We iterate until no word changes its optimal parse, 
which empirically is typically less than five iterations on the entire lexicon. 22 We now 
have an initial split of all words into stem plus suffix. Even for words like this and 
stomach we have such an initial split. 
4.2 Second Heuristic 
The second approach that we have employed provides a much more rapid conver- 
gence on the suffixes of a language. Since our goal presently is to identify word-final 
suffixes, we assume by convention that all words end with an end-of-word symbol 
(traditionally "#') ,  and we then tally the counts of all n-grams of length between two 
and six letters that appear word finally. Thus, for example, the word elephant# contains 
one occurrence of the word-final bigram t#, one occurrence of the word-final trigram 
nt#, and so forth; we stop at 6-grams, on the grounds that no grammatical morphemes 
require more than five letters in the languages we are dealing with. We also require 
that the n-gram in question be a proper substring of its word. 
We employ as a rough indicator of likelihood that such an n-gram nln2.., nk is a 
grammatical morpheme the measure: 
\[nln2...nk\] log \[nln2...nk\] 
Total count of k-grams \[n1-~2\] -(~-k\]' 
which we may refer to as the weighted mutual information. We choose the top 100 
n-grams on the basis of this measure as our set of candidate suffixes. 
We should bear in mind that this ranking will be guaranteed to give incorrect 
results as well as correct ones; for example, while ing is very highly ranked in an 
English corpus, ting and ng will also be highly ranked, the former because so many 
stems end in t, the latter because all ings end in ng, but of the three, only ing is a 
morpheme in English. 
We then parse all words into stem plus suffix if such a parse is possible using a 
suffix from this candidate set. A considerable number of words will have more than 
one such parse under those conditions, and we utilize the figure of merit described in 
the preceding section to choose among those potential parses. 
4.3 Evaluating the Results of Initial Word Splitting 
Regardless of which of the two approaches we have taken, our task now is to decide 
which splits are worth keeping, which ones need to be dropped, and which ones need 
to be modified. 23 In addition, if we follow the take-all-splits approach, we have many 
22 Experimenting with other functions uggests empirically that the details of our choices for a figure of 
merit, and the distribution reported in the text, are relatively unimportant. As long as the measurement 
is capable of ensuring that the cuts are not strongly pushed towards the periphery, the results we get 
are robust. 
23 Various versions of Harris's method of morpheme identification can be used as well. Harris's approach 
has the interesting characteristic (unlike the heuristics discussed in the text) that it is possible to impose 
restrictions that improve its precision while at the same time worsening its recall to unacceptably low 
levels. In work in progress, we are exploring the consequences of using such an initial heuristic with 
significantly higher precision, while depending on MDL considerations to extend the recall of the 
entire morphology. 
172 
Goldsmith Unsupervised Learning of the Morphology of a Natural Language 
splits which (from our external vantage point) are splits between prefix and stem: 
words begim~ng with de (defense, demand, elete, etc.) will at this point all be split after 
the initial de. So there is work to be done, and for this we return to the central notion 
of the signature. 
5. Signatures 
Each word now has been assigned an optimal split into stem and suffix by the initial 
heuristic hosen, and we consider henceforth only the best parse for that word, and we 
retain only those stems and suffixes that were optimal for at least one word. For each 
stem, we make a list of those suffixes that appear with it, and we call an alphabetized 
list of such suffixes (separated by an arbitrary symbol, such as period) the stem's 
signature; we may think of it as a miniparadigm. For example, in one English corpus, 
the stems despair, pity, appeal, and insult appear with the suffixes ing and ingly. However, 
they also appear as freestanding words, and so we use the word NULL, to indicate 
a zero suffix. Thus their signature is NULL.ing.ingly. Similarly, the stems assist and 
ignor are assigned the signature ance.ant.ed.ing  a certain corpus. Because each stem 
is associated with exactly one signature, we will also use the term signature to refer to 
the set of affixes along with the associated set of stems when no ambiguity arises. 
We establish a data structure of all signatures, keeping track for each signature of 
which stems are associated with that signature. As an initial heuristic, subject o cor- 
rection below, we discard all signatures that are associated with only one stem (these 
latter form the overwhelming majority, well over 90%) and all signatures with only 
one suffix. The remaining signatures we shall call regular signatures, and we will call 
all of the suffixes that we find in them the regular suffixes. As we shall see, the regular 
suffixes are not quite the suffixes we would like to establish for the language, but they 
are a very good approximation, and constitute a good initial analysis. The nonregu- 
lar signatures produced by the take-all-splits approach are typically of no interest, as 
examples uch as ch.e.erial.erials.rimony.rons.uring and el.ezed.nce.reupon.ther illustrate. 
The reader may identify the single English pseudostem that occurs with each of these 
signatures. 
The regular signatures are thus those that specify exactly the entire set of suffixes 
used by at least two stems in the corpus. The presence of a signature rests upon the 
existence of a structure as in (6), where there are at least two members present in each 
column, and all combinations indicated in this structure are present in the corpus, 
and, in addition, each stem is found with no other suffix. (This last condition does 
not hold for the suffixes; a suffix may well appear in other signatures, and this is the 
difference between stems and af f ixes. )  24 
stem1} f suffi.Xl ~ stem2 
stem3 ~ suffix2 J 
(6) 
If we have a morphological pattern of five suffixes, let us say, and there is a large 
set of stems that appear with all five suffixes, then that set will give rise to a reg- 
ular signature with five suffixal members. This simple pattern would be perturbed 
by the (for our purpose) extraneous fact that a stem appearing with these suffixes 
24 Langer 1991 discusses some of the historical origins of this criterion, known in the literature as a 
Greenburg square (Greenberg 1957). As Langer points out, important antecedents in the literature 
include Bloomfield's brief discussion (1933, 161) as well as Nida (1948, 1949). 
173 
Computational Linguistics Volume 27, Number 2 
should also appear with some other suffix; and if all stems that associate with these 
five suffixes appear with idiosyncratic suffixes (i.e., each different from the others), 
then the signature of those five suffixes would never emerge. In general, however, in 
a given corpus, a good proportion of stems appears with a complete set of what a 
grammarian would take to be the paradigmatic set of suffixes for its class: this will 
be neither the stems with the highest nor the stems with the lowest frequency, but 
those in between. In addition, there will be a large range of words with no accept- 
able morphological nalysis, which is just as it should be: John, stomach, the, and so 
forth. 
To get a sense of what are identified as regular signatures in a language such as 
English, let us look at the results of a preliminary analysis in Table 2 of the 86,976 words 
of The Adventures ofTom Sawyer, by Mark Twain. The signatures in Table 2 are ordered 
by the breadth of a signature, defined as follows. A signature ?r has both a stem count 
(the number of stems associated with it) and an affix count (the number of affixes 
it contains), and we use log (stem count) ~ log (affix count) as a rough guide to the 
centrality of a signature in the corpus. The suffixes identified are given in Table 3 for 
the final analysis of this text. 
In this corpus of some 87,000 words, there are 202 regular signatures identified 
through the procedure we have outlined so far (that is, preceding the refining opera- 
tions described in the next section), and 803 signatures composed entirely of regular 
suffixes (the 601 additional signatures either have only one suffix, or pertain to only 
a single stem). 
The top five signatures are: NULL.ed.ing, e.ed.ing, NULL.s, NULL.ed.s, and 
NULL.ed.ing.s; the third is primarily composed of noun stems (though it includes 
a few words from other categories--hundred, bleed, new), while the others are verb 
stems. Number 7, NULL.ly, identifies 105 words, of which all are adjectives (appre- 
hensive, sumptuous, gay . . . .  ) except for Sal, name, love, shape, and perhaps earth. The 
results in English are typical of the results in the other European languages that I 
have studied. 
These results, then, are derived by the application of the heuristics described above. 
The overall sketch of the morphology of the language is quite reasonable already in 
its outlines. Nevertheless, the results, when studied up close, show that there remain 
a good number of errors that must be uncovered using additional heuristics and 
evaluated using the MDL measure. These errors may be organized in the following 
ways: 
. 
2. 
. 
The collapsing of two suffixes into one: for example, we find the suffix 
ings here; in most corpora, the equally spurious uffix ments is found. 
The systematic nclusion of stem-final material into a set of (spurious) 
suffixes. In English, for example, the high frequency of stem-final ts can 
lead the system to analyze a set of suffixes as in the spurious ignature 
ted.ting.ts, or ted.tion. 
The inclusion of spurious ignatures, largely derived from short stems 
and short suffixes, and the related question of the extent of the inclusion 
of signatures based on real suffixes but overapplied. For example, s is a 
real suffix of English, but not every word ending in s should be analyzed 
as containing that suffix. On the other hand, every word ending in ness 
should be analyzed as containing that suffix (in this corpus, this reveals 
the stems: selfish, uneasi, wretched, loveli, unkind, cheeri, wakeful, drowsi, 
cleanli, outrageous, and loneli). In the initial analysis of Tom Sawyer, the 
stem ca is posited with the signature n.n't.p.red.st.t. 
174 
Goldsmith Unsupervised Learning of the Morphology of a Natural Language 
Tab le  2 
Top 81 signatures from Tom Sawyer. 
Number Number 
Rank Signature Stems Rank Signature Stems 
1 NULL.ed.ing 69 42 's.NULL,lys 3 
2 e.ed.ing 35 43 NULL.ed.s.y 3 
3 NULL.s 253 44 t.tion 8 
4 NULL.ed.s 30 45 NULL.less 8 
5 NULL.ed.ing.s 14 46 e.er 8 
6 's.NULL.s 23 47 NULL.ment 8 
7 NULL.ly 105 48 le.ly 8 
8 NULL.ing.s 18 49 NULL.ted 7 
9 NULL.ed 89 50 NULL.tion 7 
10 NULL.ing 77 51 1.t 7 
11 ed.ing 74 52 ence.ent 6 
12 's.NULL 65 53 NULL.ity 6 
13 e.ed 44 54 NULL.est.ly 3 
14 e.es 42 55 ed.er.ing 3 
15 NULL.er.est.ly 5 56 NULL.ed.ive 3 
16 e.es.ing 7 57 NULL.led.s 3 
17 NULL.ly.ness 7 58 NULL.er.ly 3 
18 NULL.ness 20 59 NULL.ily.y 3 
19 e.ing 18 60 NULL.n.s 3 
20 NULL.ly.s 6 61 NULL.ed.ings 3 
21 NULL.y 17 62 NULL.ed.es 3 
22 NULL.er 16 63 e.en.ing 3 
23 e.ed.es.ing 4 64 NULL.ly.st 3 
24 NULL.ed.er.ing 4 65 NULL.s.ter 3 
25 NULL.es 16 66 NULL.ed.ing.ings.s 2 
26 NULL.ful 13 67 NULL.i.ii.v.x 2 
27 NULL.e 13 68 NULL.ed.ful.ing.s 2 
28 ed.s 13 69 ious.y 5 
29 e.ed.es 5 70 NULL.en 5 
30 ed.es.ing 5 71 ation.ed 5 
31 NULL.ed.ly 5 72 NULL.able 5 
32 NULL.n't 10 73 ed.er 5 
33 NULL.t 10 74 nce.nt 5 
34 'll.'s.NULL 4 75 NULL.an 4 
35 ed.ing.ings 4 76 NUL.ed.ing.y 2 
36 NULL.s.y 4 77 NULL.en.ing.s 2 
37 NULL.ed.er 4 78 NULL.ed.ful.ing 2 
38 NULL.ed.ment 4 79 NULL.st 4 
39 NULL.ful.s 4 80 e.ion 4 
40 NULL.ed.ing.ings 3 81 NULL.al.ed.s 2 
41 ted.tion 9 
. 
. 
The failure to break all words actually containing the same stem in a 
consistent fashion: for example, the stem abbreviate with the signature 
NULL.d.s is not related to abbreviat with the signature ing. 
Stems may be related in a language without being identical. The stem 
win may be identified as appearing with the signature NULL.s and the 
stem winn may be identified with the signature r.ing, but these stems 
should be related in the morphology. 
In the next section, we discuss some of the approaches we have taken to resolving 
these problems. 
Computational Linguistics Volume 27, Number 2 
Table 3 
Suffixes from Tom Sawyer. 
Suffix Remarks Suffix Remarks 
s ted chat-ted, fit-ted, submit-ted, etc. 
ed est 
ing ity 
er ous 
e ard drunk-ard 
ly able 
's ious 
d less 
y ment 
n id id.or for stems horr-, splend-, liqu- 
on Spurious (bent-on, rivers-on): ure 
triage issue 
es ive 
t ty 
st Signature NULL.ly.st, for stems ence 
such as safe- 
en behold, dea l  weak, sunk, etc. ily 
le Error: analyzed le.ly for e.y (stems ward 
such as feeb-, audib-, simp-). 
al ation 
n't led 
nce Signature nce.nt, for stems fragr-, 'd 
dista-, indiffere- 
ent Spurious: triage problem (pot-ent) ry 
rious tion 
r rs 
ter triage problem ned 
k triage problem ning 
ful age 
ion h 
'11 te 
an triage problem ant 
ness r 's 
nt see above ance 
novel, uncertain, six, proper 
triage problem 
error: stems such as glo- with sig- 
nature rious.ry 
error: stems such as glo- with sig- 
nature rious.ry 
error: r should be in stem 
awake-ned, white-ned, thin-ned 
begin-ning, run-ning 
triage problem 
should be -ate (e.g., punctua-te) 
triumph-ant, expect-ant 
error 
6. Optimizing Description Length Using Heuristics and MDL 
We can use the descr ip t ion  length  of the grammar  fo rmulated  in (2) and  (3) to eva luate  
any proposed  revis ion,  as we have  a l ready  observed:  note the descr ip t ion  length of the 
grammar  and  the compressed  corpus,  per fo rm a modi f i cat ion  of the grammar ,  recom- 
pute  the two lengths,  and  see if the modi f i cat ion  improved the resu l t ing descr ip t ion  
length.  25 
25 This computation is rather lengthy, and in actual practice it may be preferable to replace it with far 
faster approaches totesting a change. One way to speed up the task is to compute the differential of 
the MDL function, so that we can directly compute the change in description length given some prior 
changes in the variables that define the morphology that are modified in the hypothetical change being 
evaluated (see the Appendix). The second way to speed up the task is, again, to use heuristics to 
identify clear cases for which full description length computation is not necessary, and to identify a 
smaller number of cases where fine description length is appropriate. For example, in the case 
176 
Goldsmith Unsupervised Learning of the Morphology of a Natural Language 
Following the morphological nalysis of words described in the previous ection, 
suffixes are checked to determine if they are spurious amalgams of independently mo- 
tivated suffixes: ments is typically, but wrongly, analyzed as a suffix. Upon identifica- 
tion of such suffixes as spurious, the vocabulary containing these words is reanalyzed. 
For example, in Tom Sawyer, the suffix ings is split into ing and s, and thus the word 
beings is split into being plus s; the word being is, of course, already in the lexicon. 
The word breathings i  similarly reanalyzed as breathing plus s, but the word breathing 
is not found in the lexicon; it is entered, with the morphological nalysis breath+ing. 
Words that already existed include chafing, dripping, evening, feeling, and flogging, while 
new "virtual" words include belonging, bustling, chafing, and fastening. The only new 
word that arises that is worthy of notice is jing, derived from the word jings found 
in Twain's expression by jings! In a larger corpus of 500,000 words, 64 suffixes are 
tested for splitting, and 31 are split, including tions, ists, ians, ened, lines, ents, and ively. 
Note that what it means to say that "suffixes are checked to see if they are spurious 
amalgams" is that each suffix is checked to see if it is the concatenation f two inde- 
pendently existing suffixes, and then if that is the case, the entire description length 
of the corpus is recomputed under the alternative analysis; the reanalysis i adopted 
if and only if the description length decreases. The same holds for the other heuristics 
discussed immediately below. 26 
Following this stage, the signatures are studied to determine if there is a consistent 
pattern in which all suffixes from the signature begin with the same letter or sequence 
of letters, as in te.ting.ts. 27 Such signatures are evaluated to determine if the description 
length improves when such a signature ismodified to become .ing.s, etc. It is necessary 
to precede this analysis by one in which all signatures are removed which consist of a 
single suffix composed of a single letter. This set of signatures includes, for example, 
the singleton signature , which is a perfectly valid suffix in English; however, if we 
permit all words ending in e, but having no other related forms, to be analyzed as 
containing the suffix e, then the e will be inappropriately highly valued in the analysis. 
(We return to this question in Section 11, where we address the question of how many 
occurrences of a stem with a single suffix we would expect o find in a corpus.) 
In the next stage of analysis, triage, signatures containing a small number of stems 
or a single suffix are explored in greater detail. The challenge of triage is to determine 
when the data is rich and strong enough to support he existence of a linguistically 
real signature. A special case of this is the question of how many stems must ex- 
ist to motivate the existence of a signature (and hence, a morphological nalysis for 
the words in question) when the stems only appear with a single suffix. For exam- 
ple, if a set of words appear in English ending with hood, should the morphological 
analysis split the words in that fashion, even if the stems thereby created appear 
with no other suffixes? And, at the other extreme, what about a corpus which con- 
tains the words look, book, loot, and boot? Does that data motivate the signature l.k, 
for the stems boo and loo? The matter is rendered more complex by a number of fac- 
tors. The length of the stems and suffixes in question clearly plays a role: suffixes 
of one letter are, all other things being equal, suspicious; the pair of stems Ioo and 
boo, appearing with the signature k.t, does not provide an example of a convincing 
mentioned in the text, that of determining whether a suffix such as ments should always be split into 
two independently motivated suffixes ment and s, we can compute the fraction of words ending in 
ments that correspond to freestanding words ending in ment. Empirical observation suggests that ratios 
over 0.5 should always be split into two suffixes, ratios under 0.3 should not be split, and those in 
between must be studied with more care. 
26 This is accomplished by the command am4 in Linguistica. 
27 This is accomplished by the command am5 in Linguistica. 
177 
Computational Linguistics Volume 27, Number 2 
linguistic pattern. On the other hand, if the suffix is long enough, even one stem 
may be enough to motivate a signature, especially if the suffix in question is oth- 
erwise quite frequent in the language. A single stem occurring with a single pair 
of suffixes may be a very convincing signature for other reasons as well. In Ital- 
ian, for example, even in a relatively small corpus we are likely to find a signa- 
ture such as a.ando.ano.are.ata.ate.ati.ato.azione.~ with several stems in it; once we are 
sure that the 10-suffix signature is correct, then the discovery of a subsignature along 
with a stem is perfectly natural, and we would not expect o find multiple stems 
associated with each of the occurring combinations. (A similar example in English 
from Tom Sawyer is NULL.ed.ful.ing.ive.less for the single stem rest.) And a signature 
may be "contaminated," so to speak, by a spurious intruder. A corpus containing 
rag, rage, raged, raging, and rags gave rise to a signature: NULL.e.ed.ing.s for the stem 
rag. It seems clear that we need to use information that we have obtained regard- 
ing the larger, robust patterns of suffix combinations in the language to influence 
our decisions regarding smaller combinations. We return to the matter of triage be- 
low. 
We are currently experimenting with methods to improve the identification of re- 
lated stems. Current efforts yield interesting but inconclusive results. We compare all 
pairs of stems to determine whether they can be related by a simple substitution pro- 
cess (one letter for none, one letter for one letter, one letter for two letters), ignoring 
those pairs that are related by virtue of one being the stem of the other already within 
the analysis. We collect all such rules, and compare by frequency. In a 500,000-word 
English corpus, the top two such pairs of 1:1 relationships are (1) 46 stems related by 
a final d/s alternation, including intrud/intrus, apprendend/apprenhens, provid/provis, us- 
pend/suspens, and elud/elus, and (2) 43 stems related by a final i/y alternation, includ- 
ing reli/rely, ordinari/ordinary, decri/decry, suppli/supply, and accompani/accompany. This 
approach can quickly locate patterns of allomorphy that are well known in the Eu- 
ropean languages (e.g., alternation between a and/~ in German, between o and ue in 
Spanish, between c and q in French). However, we do not currently have a satisfactory 
means of segregating meaningful cases, such as these, from the (typically less frequent 
and) spurious cases of stems whose forms are parallel but ultimately not related. 
7. Results 
On the whole, the inclusion of the strategies described in the preceding sections leads 
to very good, but by no means perfect, results. In this section we shall review some 
of these results qualitatively, some quantitatively, and discuss briefly the origin of the 
incorrect parses. 
We obtain the most striking result by looking at the top list of signatures in a 
language, if we have some familiarity with the language: it is almost as if the textbook 
patterns have been ripped out and placed in a chart. As these examples uggest, 
the large morphological patterns identified tend to be quite accurately depicted. To 
illustrate the results on European languages, we include signatures found from a 
500,000-word corpus of English (Table 4), a 350,000-word corpus of French (Table 5), 
Don Quijote, which contains 124,716 words of Spanish (Table 6), a 125,000-word corpus 
of Latin (Table 7), and 100,000 words and 1,000,000 words of Italian (Tables 8 and 9). 
The 500,000-word (token-count) corpus of English (the first part of the Brown Corpus) 
contains lightly more than 30,000 distinct words. 
To illustrate the difference of scale that is observed epending on the size of 
the corpus, compare the signatures obtained in Italian on a corpus of 100,000 words 
(Table 8) and a corpus of 1,000,000 words (Table 9). When one sees the rich inflectional 
178 
Goldsmith Unsupervised Learning of the Morphology of a Natural Language 
Table 4 
Top 10 signatures, 500,000-word English corpus. 
1. NULL.ed.ing.s 4. NULL.s 
accent 
add 
administer 
afford 
alert 
amount 
appeal 
assault 
attempt 
2. 's.NULL.s 
7. NULL.ed.ing 
abberation applaud 
abolitionist arrest 
abortion astound 
absence blast 
abstractionist bless 
abutment bloom 
accolade boast 
accommodation bolster 
accomodation broaden 
cater 
5. e.ed.es.ing 
adolescent achiev 8. NULL.er.ing.s 
afternoon assum blow 
airline brac bomb 
ambassador chang broadcast 
amendment charg deal 
announcer compris draw 
architect conced drink 
assessor conclud dwell 
association decid farm 
describ feed 
3. NULL.ed.er.ing.s feel 
attack 6. e.ed.er.es.ing 
back advertis 9. NULL.d.s 
bath announc abbreviate 
boil bak accommodate 
borrow challeng aggravate 
charm consum apprentice 
condition enforc arcade 
demand gaz balance 
down glaz barbecue 
flow invad bruise 
liv catalogue 
pac costume 
10. NULL.ed.s 
acclaim 
beckon 
benefit 
blend 
blister 
bogey 
bother 
breakfast 
buffet 
burden 
179 
Computational Linguistics Volume 27, Number 2 
Table 5 
Top 10 signatures, 350,000-word French corpus. 
1. NULL.e.es.s 4. NULL.e.es 7. NULL.e 
abondant acquis accueillant 
abstrait a6ropostal acharn6 
adjacent afghan admis 
appropri6 albanais adsorbant 
atteint allong6 albigeois 
bantou anglais alicant 
bleu appel6 ali6nant 
brillant arrondi all6chant 
byzantin bavarois amarant 
carthaginois ambiant 
2. NULL.s 
abandonn6e 5. NULL.e.s 8. NULL.es.s 
abbaye adh6rent antioxydant 
abdication adolescent bassin 
abdominale affili6 civil 
ab61ienne aLn6 craint 
aberration assign6 cristallin 
abolitionniste assistant cutan6 
abord6e bovin descendant 
abrasif cinglant dot6 
abr6viation colorant emulsifiant 
ennemi 
3. NULL.ment.s 6. NULL.ne.s 
administrative ab61ien 
agressive acheul6en 
anatomique alsacien 
ancienne am6rindien 
annuelle ancien 
automatique anglo-saxon 
biologique aram6en 
chimique aristot61icien 
classique ath6nien 
9. a.aient.ait.ant.e.ent.er.es.6rent.4.4e.6s 
contr61 
jou 
laiss 
rest 
10. NULL.es 
adopt6 
ag6 
alli6 
annul6 
apparent6 
apprdci6 
arm6 
assi6g6 
associ6 
attach6 
pat tern  emerg ing,  as w i th  the example  of the 10 suffixes on f i rst -conjugat ion stems 
(a.ando.ano.are.ata.ate.ati.ato.azione.~), one cannot  but  be st ruck by  the grammat ica l  deta i l  
that  is emerg ing  f rom the s tudy  of a larger corpus,  as 
28 Signature 1 is formed from adjectival stems in the fem.sg., fem.pl., masc.pl, and masc.sg, forms; 
Signature 2 is entirely parallel, based on stems ending with the morpheme -ic/-ich, where ich is used 
before i and e. Signature 4 is an extension of Signature 2, including nominalized (sg. and pl.) forms. 
Signature 5 is the large regular verb inflection pattern (seven such verb stems are identified). Signature 
3 is a subset of Signature 1, composed of stems accidentally not found in the feminine plural form. 
Signatures 6 and 8 are primarily masculine nouns, sg., and pl., Signature 10 is feminine nouns, sg., and 
pl., and the remaining Signatures 7 and 9 are again subsets of the regular adjective pattern of 
Signature 1. 
180 
Goldsmith Unsupervised Learning of the Morphology of a Natural Language 
Table 6 
Top 10 signatures, 130,000-word Spanish corpus. 
1. a.as.o.os 4. NULL.n 7. NULL.a.as.o.os 
abiert abrfa algun 
aficionad abriria buen 
ajen acabase es 
amig acabe mf 
antigu acaece primer 
compuest acertaba un 
cortesan acometfa 
cubiert acompafiaba 8. NULL.es 
cuy acordaba - ~ngel 
delicad aguardaba animal 
~rbol 
2. NULLs 5. NULL.n.s azul 
aborrecido caballero bachiller 
abrasado cante belianis 
abundante debfa bien 
acaecimiento dice buey 
accidente dijere calidad 
achaque duerme cardenal 
acompafiado entiende 
acontecimiento fuerza 9. da.do.r 
acosado hubiera - amanceba 
acostumbrado miente ata 
3. a.o.os 6. a.as.o averigua 
afligid agradezc colga 
~inim anch emplea 
asalt at6nit feri 
caballeriz confus fingi 
desagradecid conozc heri 
descubiert decill pedi 
despiert dificultos persegui 
dorad estrech 
enemig extrafi 10. NULL.le 
flac fresc abraz6 
acomodar 
aconsej6 
afligi6se 
agradeci6 
aguardar 
alegr6 
arroj6 
atraer 
bes6 
Turning to French, we may briefly inspect he top 10 signatures that we find in a 
350,000-word corpus in Table 5. It is instructive to consider the signature a.aient.ait.ant.e. 
ent.er.es.~rent.d.de.ds, which is ranked ninth among signatures. It contains a large part 
of the suffixal pattern from the most common regular conjugation, the first conjuga- 
tion. 
Within the scope of the effort covered by this project, the large-scale generaliza- 
tions extracted about these languages appear to be quite accurate (leaving for further 
discussion below the questions of how to link related signatures and related stems). It 
is equally important o take a finer-grained look at the results and quantify them. To 
181 
Computational Linguistics Volume 27, Number 2 
Table 7 
Top 10 signatures, 125,000-word Latin corpus. 
1. NULL.que 4. NULL.m 7. NULL.e.m 
abierunt abdia angustia 
acceperunt abia baptista 
accepit abira barachia 
accinctus abra bethania 
accipient adonira blasphemia 
addidit adsistente causa 
adiuvit adulescente conscientia 
adoravit adulescentia corona 
adplicabis adustione ignorantia 
adprehendens aetate lorica 
2. NULL.m.s 5. i . is.o.orum.os.um.us 8. a.ae.am.as.i.is.o.orum.os.um.us 
acie 
aquaeductu 
byssina 
civitate 
coetu 
die 
ezechia 
facultate 
fide 
fimbria 
3. a.ae.am.as.is 
ancill 
aqu 
lucern 
parabol 
plag 
puell 
stell 
synagog 
tabul 
tunic 
angel 
cubit 
discipul 
iust 
ocul 
popul 
6. e.em.es.i. ibus.is.um 
fratr 
greg 
homin 
reg 
vic 
voc 
ann  
magn 
mult 
univers 
9. NULL.e.m.s 
azaria 
banaia 
esaia 
iosia 
iuda 
lucusta 
massa 
matthathia 
pluvia 
sagitta 
10. i .o.um 
brachi 
carmel 
cenacul 
danm 
evangeli 
hysop 
lectul 
liban 
offici 
ole 
do this, we have selected from the English and the French analyses a set of 1,000 con- 
secutive words in the alphabetical list of words from the corpus and divided them into 
distinct sets regarding the analysis provided by the present algorithm. See Tables 10 
and 11. 
The first category of analyses, labeled Good, is self-explanatory in the case of most 
words (e.g., proceed, proceeded, proceeding, proceeds), and many of the errors are equally 
easy to identify by eye (abide with no analysis, next to abid-e and abid-ing, or Abn-er). 
Quite honestly, I was surprised how many words there were in which it was difficult 
to say what the correct analysis was. For example, consider the pair aboli-tion and abol- 
ish. The words are clearly related, and abolition clearly has a suffix; but does it have the 
suffix -ion, -tion, or -ition, and does abolish have the suffix -ish, or -sh? It is hard to say. 
182 
Goldsmith Unsupervised Learning of the Morphology of a Natural Language 
Table 8 
Top 10 signatures, 100,000-word Italian corpus. 
Rank Signature Number of Stems Participating in this Signature 
1 a.e.i.o 55 
2 ica.iche.ici.ico 17 
3 a.i.o 33 
4 e.i 221 
5 i.o 164 
6 e.i.o 24 
7 a.e.o 23 
8 a.e.i 23 
9 a.e 131 
10 NULL.o 71 
11 e.i.it~ 14 
Table 9 
Top 10 signatures, 1,000,000-word Italian corpus. 
Rank Signature Number of Stems Participating 
in this Signature 
1 .a.e.i.o. 136 
2 .ica.iche.ici.ico. 43 
3 .a.i.o. 114 
4 .ia.ica.iche.ici.ico.ie. 13 
5 .a.ando.ano.are.ata.ate 7 
.ati.ato.azione.6. 
6 .e.i. 583 
7 .a.e.i. 47 
8 .i.o. 383 
9 .a.e.o. 32 
10 .a.e. 236 
Table 10 
Results (English). 
Category Count Percent 
Good 829 82.9% 
Wrong analysis 52 5.2% 
Failed to analyze 36 3.6% 
Spurious analysis 83 8.3% 
Table 11 
Results (French). 
Category Count Percent 
Good 833 83.3% 
Wrong analysis 61 6.1% 
Failed to analyze 42 4.2% 
Spurious analysis 64 6.4% 
In a case of this sort, my pol icy for assigning success or failure has been inf luenced by 
two criteria. The first is that analyses are better insofar as they explicitly relate words  
that are appropr iate ly parallel in semantics, as in the abolish~abolition case; thus I wou ld  
183 
Computational Linguistics Volume 27, Number 2 
give credit to either the analysis aboli-tion/aboli-sh or the analysis abol-ition/abol-ish. The 
second criterion is a bit more subtle. Consider the pair of words alumnus and alumni. 
Should these be morphologically analyzed in a corpus of English, or rather, should 
failure to analyze them be penalized for this morphology algorithm? (Compare in like 
manner alibi or allegretti; do these English words contain suffixes?). My principle has 
been that if I would have given the system additional credit by virtue of discovering 
that relationship, I have penalized it if it did not discover it; that is a relatively harsh 
criterion to apply, to be sure. Should proper names be morphologically analyzed? 
The answer is often unclear. In the 500,000 word English corpus, we encounter Alex 
and Alexis, and the latter is analyzed as alex-is. I have scored this as correct, much 
as I have scored as correct the analyses of Alexand-er and Alexand-re. On the other 
hand, the failure to analyze Alexeyeva despite the presence of Alex and Alexei does 
not seem to me to be an error, while the analysis Anab-el has been scored as an 
error, but John-son (and a bit less obviously Wat-son) have not been treated as errors. 29 
Difficult to classify, too, is the treatment of words such as abet~abetted~abetting. The 
present algorithm selects the uniform stem abet in that case, assigning the signature 
NULL.ted.ting. Ultimately what we would like to have is a means of indicating that 
the doubled t is predictable, and that the correct signature is NULL.ed.ing. At present 
this is not implemented, and I have chosen to mark this as correct, on the grounds 
that it is more important o identify words with the same stem than to identify the 
(in some sense) correct signature. Still, unclear cases remain: for example, consider the 
words accompani-ed/accompani-ment/accompani-st. The word accompany does not appear 
as such, but the stem accompany is identified in the word accompany-ing. The analysis 
accompani-st fails to identify the suffix -ist, but it will successfully identify the stem as 
being the same as the one found in accompanied and accompaniment, which it would 
not have done if it had associated the i with the suffix. I have, in any event, marked 
this analysis as wrong, but without much conviction behind the decision. Similarly, 
the analysis of French putative stem embelli with suffixes e/rent/t passes the low test 
of treating related words with the same stem, but I have counted it as in error, on the 
grounds that the analysis is unquestionably one letter off from the correct, traditional 
analysis of second-conjugation verbs. This points to a more general issue regarding 
French morphology, which is more complex than that of English. The infinitive ~crire 
'to write' would ideally be analyzed as a stem &r plus a derivational suffix i followed 
by an infinitival suffix re. Since the derivational suffix i occurs in all its inflected forms, 
it is not unreasonable to find an analysis in which the i is integrated into the stem 
itself. This is what the algorithm does, employing the stem dcri for the words dcri-re and 
~cri-t. Ecrit in turn is the stem for dcrite, ~crite, ~crites, &rits, and ~criture. An alternate 
stem form dcriv is used for past tense forms (and the nominalization dcrivain) with 
the suffixes aient, ait, ant, irent, it. The algorithm does not make explicit the connection 
between these two stems, as it ideally would. 
Thus in the tables, Good indicates the categories of words where the analysis was 
clearly right, while the incorrect analyses have been broken into several categories. 
Wrong Analysis is for bimorphemic words that are analyzed, but incorrectly analyzed, 
by the algorithm. Failed to Analyze are the cases of words that are bimorphemic but 
29 My inability to determine the correct morphological nalysis in a wide range of words that I know 
perfectly well seems to me to be essentially the same response as has often been observed in the case 
of speakers of Japanese, Chinese, and Korean when forced to place word boundaries in e-mail 
romanizations of their language. Ultimately the quality of a morphological nalysis must be measured 
by how well the algorithm handles the clear cases, how well it displays the relationships between 
words perceived to be related, and how well it serves as the language model for a stochastic 
morphology of the language in question. 
184 
Goldsmith Unsupervised Learning of the Morphology of a Natural Language 
for which no analysis was provided by the algorithm, and Spurious Analysis are the 
cases of words that are not morphologically complex but were analyzed as containing 
a suffix. 
For both English and French, correct performance is found in 83% of the words; 
details are presented in Tables 10 and 11. For English, these figures correspond to 
precision of 829/(829 + 52 + 83) = 85.9%, and recall of 829/(829 + 52 + 36) = 90.4%. 
8. Triage 
As noted above, the goal of triage is to determine how many stems must occur in 
order for the data to be strong enough to support he existence of a linguistically real 
signature. MDL provides a simple but not altogether satisfactory method of achieving 
this end. 
Using MDL for this task amounts to determining whether the total description 
length decreases when a signature is eliminated by taking all of its words and elim- 
inating their morphological structure, and reanalyzing the words as morphologically 
simple (i.e., as having no morphological structure). This is how we have implemented 
it, in any event; one could well imagine a variant under which some or all subparts 
of the signature that comprised other signatures were made part of those other sig- 
natures. For example, the signature NULL.ine.ly is motivated just for the stem just. 
Under the former triage criterion, justine and justly would be treated as unanalyzed 
words, whereas under the latter, just and justly would be made members of the (large) 
NULL.ly signature, and just and justine might additionally be treated as comprising 
parts of the signature NULL.ine along with bernard, gerald, eng, capitol, elephant, def, and 
sup (although that would involve permitting a single stem to participate in two distinct 
signatures). 
Our MDL-based measure tests the goodness of a signature by testing each sig- 
nature cr to see if the analysis is better when that signature is deleted. This deletion 
entails treating the signature's words as members of the signature of unanalyzed words 
(which is the largest signature, and hence such signature pointers are relatively short). 
Each word member of the signature, however, now becomes a separate stem, with all 
of the increase in pointer length that that entails, as well as increase in letter content 
for the stem component. 
One may draw the following conclusions, I believe, from the straightforward ap- 
plication of such a measure. On the whole, the effects are quite good, but by no means 
as close as one would like to a human's decisions in a certain number of cases. In 
addition, the effects are significantly influenced by two decisions that we have al- 
ready discussed: (i) the information associated with each letter, and (ii) the decision 
as to whether to model suffix frequency based solely on signature-internal frequences, 
or based on frequency across the entire morphology. The greater the information as- 
sociated with each letter, the more worthwhile morphology is (because maintaining 
multiple copies of nearly similar stems becomes increasingly costly and burdensome). 
When suffix frequencies (which are used to compute the compressed length of any 
analyzed word) are based on the frequency of the suffixes in the entire lexicon, rather 
than conditionally within the signature in question, the loss of a signature ntails a hit 
on the compression of all other words in the lexicon that employed that suffix; hence 
triage is less dramatic under that modeling assumption. 
Consider the effect of this computation on the signatures produced from a 500,000- 
word corpus of English. After the modifications discussed to this point, but before 
triage, there were 603 signatures with two or more stems and two or more suffixes, 
and there were 1,490 signatures altogether. Application of triage leads to the loss 
185 
Computational Linguistics Volume 27, Number 2 
of only 240 signatures. The single-suffix signatures that were eliminated were: ide, 
it, rs, he, ton, o, and ie, all of which are spurious. However, a number of signatures 
that should not have been lost were eliminated, most strikingly: NULL.ness, with 51 
good analyses, NULL.ful, with 18 good analyses, and NULL.ish with only 8 analyses. 
Most of the cases eliminated, however, were indeed spurious. Counting only those 
signatures that involves suffixes (rather than compounds) and that were in fact correct, 
the percentage of the words whose analysis was incorrectly eliminated by triage was 
21.9% (236 out of 1,077 changes). Interestingly, in light of the discussion on results 
above, one of the signatures that was lost was i.us for the Latin plural (based in this 
particular case on genii~genius). Also eliminated (and this is most regrettable) was 
NULL.n't (could~had~does~were~would/did). 
Because maximizing correct results is as important as testing the MDL model 
proposed here, I have also utilized a triage algorithm that departs from the MDL- 
based optimization in certain cases, which I shall identify in a moment. I believe that 
when the improvements identified in Section 10 below are made, the purely MDL- 
based algorithm will be more accurate; that prediction remains to be tested, to be 
sure. On this account, we discard any signature for which the total number of stem 
letters is less than five, and any signature consisting of a single, one-letter suffix; we 
keep, then, only signatures for which the savings in letter counts is greater than 15 
(where savings in letter counts is simply the difference between the sum of the length 
of words spelled out as a monomorphemic word and the sum of the lengths of the 
stems and the suffixes); 15 is chosen empirically. 
9. Paradigms 
As we noted briefly above, the existence of a regular pattern of suffixation with n 
distinct suffixes will generally give rise to a large set of stems displaying all n suffixes, 
but it will also give rise in general to stems displaying most possible combinations 
of subsets of these suffixes. Thus, if there is a regular paradigm in English consisting 
of the suffixes NULL, -s, -ing, and -ed, we expect o find stems appearing with most 
possible combinations of these suffixes as well. As this case clearly shows, not all such 
predicted subpatterns are merely partially filled paradigms. Of stems appearing with 
the signature NULL.s, some are verbs (such as occur~occurs), but the overwhelming 
majority, of course, are nouns. 
In the present version of the algorithm, no effort is made to directly relate signa- 
tures to one another, and this has a significant and negative impact on performance, 
because analyses in which stems are affiliated with high-frequency signatures are more 
highly valued than those in which they are affiliated with low-frequency signatures; it
is thus of capital importance not to underestimate he total frequency of a signature. B?
When two signatures as we have defined them here are collapsed, there are two major 
effects on the description length: pointers to the merged signature are shorter-- leading 
to a shorter total description length--but,  in general, predicted frequencies of the com- 
30 As long as we keep the total number of words fixed, the global task of minimizing description length 
can generally be obtained by the local strategy of finding the largest cohort for a group of forms to 
associate with: if the same data can be analyzed in two ways, with the data forming groups of sizes 
{a}} in one case, and {a2}in the other, maximal compression is obtained by choosing the case (k -- 1, 2) 
for which 
~ log(a~) 
i 
is the greatest. 
186 
Goldsmith Unsupervised Learning of the Morphology of a Natural Language 
posite words are worse than they were, leading to a poorer description (via increased 
cross-entropy, we might say). In practice, the collapsing of signatures is rejected by 
the MDL measure that we have implemented here. 
In work in progress, we treat groups of signatures (as defined here) as parts of 
larger groups, called paradigms. A paradigm consisting of tile suffixes NULL.ed.ing.s, 
for example, includes all 15 possible combinations of these suffixes. We can in general 
estimate the number of stems we would expect o appear with zero counts for one or 
more of the suffixes, given a frequency distribution, such as a multinomial distribution, 
for the su f f ixes .  31 In this way, we can establish some reasonable frequencies for the case 
of stems appearing in a corpus with only a single suffix. It appears at this time that the 
unavailability of this information is the single most significant cause of inaccuracies 
in the present algorithm. It is thus of considerable importance to get a handle on such 
estimates. 32
10. Remaining Issues 
A number of practical questions remain at this point. The most important are the 
following: 
. Identifying related stems (allomorphs). Languages typically have 
principles at work relating pairs of stems, as in English many stems (like 
win) are related to another stem with a doubled consonant (winn, as in 
winn-ing). We have been reasonably successful in identifying such 
semiregular morphology, and will report this in a future publication. 
There is a soft line between the discovery of related stems, on the one 
hand, and the parsing of a word into several suffixes. For example, in 
the case mentioned briefly above for French, it is not unreasonable to
propose two stems for 'to write' ~cri and dcriv, each used in distinct 
forms. It would also be reasonable, in this case, to analyze the latter stem 
dcriv as composed of ~cri plus a suffix -v, although in this case, there are 
no additional benefits to be gained from the more fine-grained analysis. 
31 In particular, consider a paradigm with a set {j~i} of suffixes. We may represent a subsignature of that 
signature as a string of 0s and ls (a Boolean string b, of the form {0,1}*, abbreviated bk) indicating 
whether (or not) the ith suffix is contained in the subsignature. If a stem t occurs \[t\] times, then the 
probability that it occurs without a particular suffix~ is (1 -prob(fi))\[tJ; the probability that it occurs 
without all of the suffixes missing from the particular subsignature b = {bk} is 
I - I (1  -- bk)(1 -- prob(fi))\[t\]; 
k 
and the probability that the particular subsignature b will arise at all is the sum of those values over 
all of the stems in the signature: 
l - I (1  - bk)(1 -- probOCi)) \[tn\] . 
tn C stems(G) k 
Thus all that is necessary is to estimate the hidden parameters of the frequencies of the individual 
suffixes in the entire paradigm. See the following note as well. 
32 There may appear to be a contradiction between this observation about paradigms and the statement 
in the preceding paragraph that MDL rejects ignature mergers- -but  there is no contradiction. The 
rejection of signature mergers is performed (so to speak) by the model which posits that frequencies of 
suffixes inside a signature are based only on suffix frequencies of the stems that appear with exactly 
the same set of suffixes in the corpus. It is that modeling assumption that needs to be dropped, and 
replaced by a multinomial-based frequency prediction based on counts over the 2 n - 1 signatures 
belonging to each paradigm of length n. 
187 
Computational Linguistics Volume 27, Number 2 
. 
. 
. 
. 
Identi fy ing parad igms f rom signatures. We wou ld  like to automatical ly 
identify NULL.ed. ing as a subcase of the more general NULL.ed.ing.s. This 
is a difficult task to accompl ish well, as Engl ish illustrates, for we wou ld  
like to be able to determine that NULL.s  is pr imari ly  a subcase of 
's.NULL.s, and not of (e.g.) NULL.ed.s. 33 
Determining the relationship between prefixation and suffixation. The 
system currently assumes that prefixes are to be str ipped off the stem 
that has already been identif ied by  suffix stripping. In future work,  we  
wou ld  like to see alternative hypotheses regarding the relationship of 
prefixation and suffixation tested by  the MDL criterion. 
Ident i fy ing compounds .  In work  reported in Goldsmith  and Reutter 
(1998), we have explored the usefulness of the present system for 
determin ing the l inking elements used in German compounds ,  but  more 
work  remains to be done to identify compounds  in general. Here we run 
straight into the prob lem of assigning very short strings a lower 
l ikel ihood of being words  than longer strings. That is, it is difficult to 
avoid posit ing a certain number  of very short stems, as in Engl ish m and 
an, the first because of pairs such as me and my, the second because of 
pairs such as an and any, but  these facts should  not  be taken as strong 
evidence that man is a compound.  
As noted at the outset, the present a lgor i thm is l imited in its ability to 
discover the morpho logy  of a language in wh ich  there are not  a 
sufficient number  of words  with only one suffix in the corpus. In work  
in progress, we are developing a related a lgor i thm that deals with the 
33 We noted in the preceding section that we can estimate the likelihood of a subsignature assuming a
multinomial distribution. We can in fact do better than was indicated there, in the sense that for a 
given observed signature a*, whose suffixes constitute asubset of a larger signature ~r, we can 
compute the likelihood that a is responsible for the generation of ?*, where {?i} are the frequencies 
(summing to 1.0) associating with each of the suffixes in a, and {ci} are the counts of the 
corresponding suffixes in the observed signature a*: 
it1 ) it\[, 
~,\[Cl\], \[c2\] . . . . .  \[c,\] ~(i)c' - -  \[C11!\[C21\[ ... Cn\]! 
i= I  i=1 
The log likelihood is then 
or approximately 
/t 
log\[t\]! + ~ ci log ~i - log\[ci\] !, 
i=1  
,,ogt 
from Stirling's approximation. If we normalize the cis to form a distribution (by dividing by \[t\]) and 
denote these by di, then this can be simply expressed in terms of the Kullback-Leibler distance 
D(a* II a): 
It\] log\[t\] - ~ ci log = It\] log\[t\] - It\] 
= \[t\] log\[t\]- \[t\]D(?* \[I~) -  \[t\] ~_dilog(\[t\]) 
= \[t\]log\[t\] - [t\]D(a* II a) - \[t\] log\[t\] 
= -\[t\]D(?* I\]c~). 
188 
Goldsmith Unsupervised Learning of the Morphology of a Natural Language 
. 
more general case. In the more general case, it is even more important to 
develop a model that deals with the layered relationship among suffixes 
in a language. The present system does not explicitly deal with these 
relationships: for example, while it does break up ments into ment and s, 
it does not explicitly determine which suffixes may attach to, etc. This 
must be done in a more adequate version. 
In work in progress, we have added to the capability of the algorithm 
the ability to posit suffixes that are in part subtractive morphemes. That 
is, in English, we would like to establish a single signature that combines 
NULL.ed.ing.s and e.ed.es.ing (for jump and love, respectively). We posit an 
operator Ix/which deletes a preceding character x, and with the 
mechanism, we can establish a single signature NULL.leled.leling.s, 
composed of familiar suffixes NULL and s, plus two suffixes leled and 
leling, which delete a preceding (stem-final) e if one is present. 
11. Conclusion 
Linguists face at the present ime the question of whether, and to what extent, 
information-theoretic notions will play a significant role in our understanding of lin- 
guistic theory over the years to come, and the present system perhaps casts a small 
ray of light in this area. As we have already noted, MDL analysis makes clear what the 
two areas are in which an analysis can be judged: it can be judged in its ability to deal 
with the data, as measured by its ability to compress the data, and it can be judged on 
its complexity as a theory. While the former view is undoubtedly controversial when 
viewed from the light of mainstream linguistics, it is the prospect of being able to say 
something about he complexity of a theory that is potentially the most exciting. Even 
more importantly, to the extent hat we can make these notions explicit, we stand a 
chance of being able to develop an explicit model of language acquisition employing 
these ideas. 
A natural question to ask is whether the algorithm presented here is intended 
to be understood as a hypothesis regarding the way in which human beings acquire 
morphology. I have not employed, in the design of this algorithm, a great deal of innate 
knowledge regarding morphology, but that is for the simple reason that knowledge of 
how words divide into subpieces i  an area of knowledge which no one would take 
to be innate in any direct fashion: if sanity is parsed as san + ity in one language, it
may perfectly well be parsed as sa + nity in another language. 
That is, while passion may flame disagreements between partisans of Universal 
Grammar and partisans of statistically grounded empiricism regarding the task of 
syntax acquisition, the task which we have studied here is a considerably more humble 
one, which must in some fashion or other be figured out by grunt work by the language 
learner. It thus allows us a much sharper image of how powerful the tools are likely 
to be that the language acquirer brings to the task. And does the human child perform 
computations atall like the ones proposed here? 
From most practical points of view, nothing hinges on our answer to this question, 
but it is a question that ultimately we cannot avoid facing. Reformulated a bit, one 
might pose the question, does the young language learner--who has access not only 
to the spoken language, but perhaps also to the rudiments of the syntax and to the 
intended meaning of the words and sentences--does the young learner have access 
to additional information that simplifies the task of morpheme identification? It is 
the belief that the answer to this question is yes that drives the intuition (if one has 
189 
Computational Linguistics Volume 27, Number 2 
this intuition) that an MDL-based analysis of the present sort is an unlikely model of 
human language acquisition. 
But I think that such a belief is very likely mistaken. Knowledge of semantics and 
even grammar is unlikely to make the problem of morphology discovery significantly 
easier. In surveying the various approaches to the problem that I have explored (only 
the best of which have been described here), I do not know of any problem (of those 
which the present algorithm deals with successfully) that would have been solved 
by having direct access to either syntax or semantics. To the contrary: I have tried to 
find the simplest algorithm capable of dealing with the facts as we know them. The 
problem of determining whether two distinct signatures derive from a single larger 
paradigm would be simplified with such knowledge, but that is the exception and not 
the rule. 
So in the end, I think that the hypothesis that the child uses an MDL-like analysis 
has a good deal going for it. In any event, it is far from clear to me how one could 
use information, either grammatical or contextual, to elucidate the problem of the 
discovery of morphemes without recourse to notions along the lines of those used in 
the present algorithm. 
Of course, in all likelihood, the task of the present algorithm is not the same 
as the language learner's task; it seems unlikely that the child first determines what 
the words are in the language (at least, the words as they are defined in traditional 
orthographic terms) and then infers the morphemes. The more general problem of 
language acquisition is one that includes the problems of identifying morphemes, 
of identifying words both morphologically analyzed and nonanalyzed, of identifying 
syntactic ategories of the words in question, and of inferring the rules guiding the 
distribution of such syntactic categories. It seems to me that the only manageable 
kind of approach to dealing with such a complex task is to view it as an optimization 
problem, of which MDL is one particular style. 
Chomsky's early conception of generative grammar (Chomsky 1975 \[1955\]; hence- 
forth LSLT) was developed along these lines as well; his notion of an evaluation metric 
for grammars was equivalent in its essential purpose to the description length of the 
morphology utilized in the present paper. The primary difference between the LSLT 
approach and the MDL approach is this: the LSLT approach conjectured that the gram- 
mar of a language could be factored into two parts, one universal and one language- 
particular; and when we look for the simplest grammatical description of a given 
corpus (the child's input) it is only the language-particular part of the description that 
contributes to complexity--that is what the theory stipulates. By contrast, the MDL 
approach makes minimal universal assumptions, and so the complexity of everything 
comprising the description of the corpus must be counted in determining the complex- 
ity of the description. The difference between these hypotheses vanishes asymptotically 
(as Janos Simon has pointed out to me) as the size of the language increases, or to put it 
another way, strong Chomskian rationalism is indistinguishable from pure empiricism 
as the information content of the (empiricist) MDL-induced grammar increases in size 
relative to the information content of UG. Rephrasing that slightly, the significance 
of Chomskian-style rationalism is greater, the simpler language-particular grammars 
are, and it is less significant as language-particular grammars grow larger, and in the 
limit, as the size of grammars grows asymptotically, traditional generative grammar 
is indistinguishable from MDL-style rationalism. We return to this point below. 
There is a striking point that has so far remained tacit regarding the treatment 
of this problem in contemporary linguistic theory. That point is this: the problem ad- 
dressed in this paper is not mentioned, not defined, and not addressed. The problem 
of dividing up words into morphemes i generally taken as one that is so trivial and 
190 
Goldsmith Unsupervised Learning of the Morphology of a Natural Language 
devoid of interest hat morphologists, or linguists more generally, simply do not feel 
obliged to think about the problem. 34 In a very uninteresting sense, the challenge pre- 
sented by the present paper to current morphological theory is no challenge at all, 
because morphological theory makes no claims to knowing how to discover morpho- 
logical analysis; it claims only to know what to do once the morphemes have been 
identified. 
The early generative grammar view, as explored in LSLT, posits a grammar of 
possible grammars, that is, a format in which the rules of the morphology and syntax 
must be written, and it establishes the semantics of these rules, which is to say, how 
they function. This grammar of grammars is called variously Universal Grammar, or 
Linguistic Theory, and it is generally assumed to be accessible to humans on the basis 
of an innate endowment, hough one need not buy into that assumption to accept 
the rest of the theory. In Syntactic Structures (Chomsky 1957, 51ff.), Chomsky famously 
argued that the goal of a linguistic theory that produces a grammar automatically, 
given a corpus as input, is far too demanding a goal. His own theory cannot do that, 
and he suggests that no one else has any idea how to accomplish the task. He suggests 
furthermore that the next weaker position--that of developing a linguistic theory that 
could determine, given the data and the account (grammar), whether this was the best 
grammar--was still significantly past our theoretical reach, and he suggests finally that 
the next weaker position is a not unreasonable one to expect of linguistic theory: that 
it be able to pass judgment on which of two grammars is superior with respect o a 
given corpus. 
That position is, of course, exactly the position taken by the MDL framework, 
which offers no help in coming up with analyses, but which is excellent at judging the 
relative merits of two analyses of a single corpus of data. In this paper, we have seen 
this point throughout, for we have carefully distinguished between heuristics, which 
propose possible analyses and modifications of analyses, on the one hand, and the 
MDL measurement, which makes the final judgment call, deciding whether to accept 
a modification proposed by the heuristics, on the other. 
On so much, the early generative grammar of LSLT and MDL agree. But they 
disagree with regard to two points, and on these points, MDL makes clearer, more 
explicit claims, and both claims appear to be strongly supported by the present study. 
The two points are these: the generative view is that there is inevitably an idiosyn- 
cratic character to Universal Grammar that amounts to a substantive innate capacity, 
on the grounds (in part) that the task of discovering the correct grammar of a human 
language, given only the corpus available to the child, is insurmountable, because this 
corpus is not sufficient o home in on the correct grammar. The research strategy asso- 
ciated with this position is to hypothesize certain compression techniques (generally 
called "rule formalisms" in generative grammar) that lead to significant reduction in 
the size of the grammars of a number of natural anguages, compared to what would 
have been possible without them. Sequential rule ordering is one such suggestion 
discussed at length in LSLT. 
To reformulate this in a fashion that allows us to make a clearer comparison with 
MDL, we may formulate early generative grammar in the following way: To select 
the correct Universal Grammar out of a set of proposed Universal Grammars {UGi}, 
given corpora for a range of human languages, elect hat UG for which the sum of the 
sizes of the grammars for all of the corpora is the smallest. It does not follow--it need not 
be the case--that the grammar of English (or German, etc.) selected by the winning 
34 Though see Dobrin (1999) for a sophisticated look at this problem. 
191 
Computational Linguistics Volume 27, Number 2 
UG is the shortest one of all the candidate English grammars, but the winning UG is 
all-round the supplier of the shortest grammars around the worldJ  s 
MDL could be formulated in those terms, undoubtedly, but it also can be formu- 
lated in a language-particular f shion, which is how it has been used in this paper. 
Generative grammar is inherently universalist; it has no language-particular format, 
other than to say that the best grammar for a given language is the shortest grammar. 
But we know that such a position is untenable, and it is precisely out of that 
knowledge that MDL was born. The position is untenable because we can always 
make an arbitrarily small compression of a given set of data, if we are allowed to 
make the grammar arbitrarily complex, to match and, potentially, to overfit the data, 
and it is untenable because generative grammar offers no explicit notion of how well 
a grammar must match the training data. MDUs insight is that it is possible to make 
explicit the trade-off between complexity of the analysis and snugness of fit to the 
data-corpus in question. 
The first tool in that computational trade-off is the use of a probabilistic model 
to compress the data, using stock tools of classical information theory. These notions 
were rejected as irrelevant by early workers in early generative grammar (Goldsmith 
2001). Notions of probabilistic grammar due to Solomonoff (1995) were not integrated 
into that framework, and the possibility of using them to quantify the goodness of fit 
of a grammar to a corpus was not exploited. 
It seems to me that it is in this context that we can best understand the way 
in which traditional generative grammar and contemporary probabilistic grammar 
formalism can be understood as complementing each other. I, at least, take it in that 
way, and this paper is offered in that spirit. 
Appendix 
Since what we are really interested in computing is not the min imum description 
length as such, but rather the difference between the description length of one model 
and that of a variant, it is convenient to consider the general form of the difference 
between two MDL computations. In general, let us say we will compare two analyses 
$1 and $2 for the same corpus, where $2 typically contains some item(s) that $1 does 
not (or they may differ by where they break a string into factors). Let us write out the 
difference in length between these two analyses, as in (7)-(11), calculating the length 
of $1 minus the length of $2. The general formulas derived in (7)-(11) are not of direct 
computational interest; they serve rather as a template that can be filled in to compute 
the change in description length occasioned by a particular structural change in the 
morphology proposed by a particular heuristic. This template is rather complex in 
its most general form, but it simplifies considerably in any specific application. The 
heuristic determines which of the terms in these formulas take on nonzero values, 
and what their values are; the overall formula determines whether the change in 
question improves the description length. In addition, we may regard the formulas in 
35 As the discussion i  the text may suggest, I arn skeptical of the generative position, and I would like to 
identify what empirical result would confirm the generative position and dissolve my skepticism. The 
result would be the discovery of two grammars of English, G1 and G2, with the following properties: 
G1 is inherently simpler than G2, using some appropriate notion of Turing machine program 
complexity, and yet G2 is the correct grammar of English, based on some of the complexity of G2 being 
the responsibility oflinguistic theory, hence "free" in the complexity competition between G1 and G2. 
That is, the proponent of the generative iew must be willing to acknowledge that overall complexity 
of the grammar of a language may be greater than logically necessary due to evolution's investment in
one particular style of programming language. 
192 
Goldsmith Unsupervised Learning of the Morphology of a Natural Language 
(7)-(11) as offering us an exact and explicit statement of how a morphology can be 
improved. 
The notation can be considerably simplified if we take some care in advance. 
Note first that in (7) and below, several items are subscripted to indicate whether they 
should be counted as in $1 or $2. Much of the simplification comes from observing, 
first, that 
log MI~ _ log 9 = log M22 - log ; 
second, that this difference is generally computed inside a summation over a set of 
morphemes, and hence the first term simplifies to a constant times the type count of 
the morphemes in the set in question. Indeed, so prevalent in these calculations i the 
formula 
log X~t~t~l 
Xstate2 
that the introduction of a new abbreviation considerably simplifies the notation. We 
use A(x) to denote 
log ~,  
where the numerator is a count in $1, and the denominator a count of the same variable 
in $2; if no confusion would result, we write Ax. 36 
Let us review the terms listed in (7)-(11). AN is a measure of the change in the 
number of total words due to tile proposed modification (the difference between the $1 
and $2 analyses); an increase in the total number of words results in a slightly negative 
value. In the text above, I indicated that we could, by judicious choice of word count 
distribution, keep Wl = W2; I have included the more general case in (7)-(11) where 
the two may be different. AWs and AWc are similar measures in the change of words 
that have morphologically simple, and morphologically complex, stems, respectively. 
They measure the global effects of the typically small changes brought about by a 
hypothetical change in morphological model. In the derivation of each formula, we 
consider first the case of those morphemes that are found in both $1 and $2 (indicated 
($1, $2)), followed by those found only in S1 ($l, ~ $2), and then those only found in 
$2 ('-~ $1, $2). Recall that angle brackets are used to indicate the type count of a set, the 
number of typographically distinct members of a set. 
In (8), we derive a formula for the change in length of the suffix component of 
the morphology. Observe the final formulation, in which the first two terms involve 
suffixes present in both $1 and $2, while the third term involves uffixes present only 
in $1 and the fourth term involves uffixes present only in $2. This format will appear 
in all of the components of this computation. Recall that the function Ltypo specifies 
the length of a string in bits, which we may take here to be simply log(26) times the 
number of characters in the string. 
In (9), we derive the corresponding formula for the stem component. 
The general form of the computation of the change to the signature component 
(10) is more complicated, and this complexity motivates a little bit more notation to 
simplify it. First, we can compute the change in the pointers to the signatures, and the 
information that each signature contains regarding the count of its stems and suffixes 
36 We beg the reader's indulgence in recognizing that we prepend the operator A immediately to the left 
of the name of a set to indicate the change in the size of the counts of the set, which is to say, "AW" is 
shorthand for "A(\[W\])", and "A(W}" for "A((W))". 
193 
Computational Linguistics Volume 27, Number 2 
as in (10a). But the heart of the matter is the treatment of the stems and suffixes within 
the signatures, given in (10b)-(10d). 
Bear in mind, first of all, that each signature consists of a list of pointers to stems, 
and a list of pointers to suffixes. The treatment of suffixes is given in (10d), and is 
relatively straightforward, but the treatment of stems (10c) is a bit more complex. 
Recall that all items on the stem list will be pointed to by exactly one stem pointer, 
located in some particular signature. All stem pointers in a signature that point to 
stems on the suffix list are directly described a "simple" word, a notion we have 
already encountered: a word whose stem is not further analyzable. But other words 
may be complex, that is, may contain a stem whose pointer is to an analyzable word, 
and hence the stem's representation consists of a pointer triple: a pointer to a signature, 
a stem within the signature, and a suffix within the signature. And each stem pointer 
is preceded by a flag indicating which type of stem it is. 
We thus have three things whose difference in the two states, $1 and $2, we wish 
to compute. The difference of the lengths of the flag is given in (10c.i). In (10c.ii), we 
need change in the total length of the pointers to the stems, and this has actually 
already been computed, during the computation of (9). 37 Finally in (10c.iii), the set of 
pointers from certain stem positions to words consists of pointers to all of the words 
that we have already labeled as being in Wo and we can compute the length of these 
pointers by adding counts to these words; the length of the pointers to these words 
needs to be computed anyway in determining the compressed length of the corpus. 
This completes the computations needed to compare two states of the morphology. 
In addition, we must compute the difference in the compressed length of the 
corpus in the two states, and this is given in (11). 
(7) Differences in description length due to organizational information: 
A (Suffixes) + A (Stems) + A (Signatures) 
(8) Difference in description length for suffix component of the morphology: 
AW(Suffixes)(1,2) - ~ Af q- ~_~ 
f~  S'tLYff:~2:e'~(1,2 ) f~  Sud~xe$(1,~2) 
fE Suffixes(~l,2) 
(9) Difference in description length for stem component of the morphology: 
r \[W\]l  \] AW (Stems)(1,2) - ~_, At + ~ \[log ~ + Ltypo(t) 
t6 Steads(i,2 ) t6 Ste11"~,8(1,~2) 
- }2 \[,og \[w\]2 -~  +Ltyvo(t)\] 
tC St e~7,s (~1,2) 
37 The equivalence between the number computed in (9) and the number needed here is not exactly 
fortuitous, but it is not an error either. The figure computed in (9) describes an aspect of the complexity 
of the morphology as a whole, whereas the computation described here in the text is what it is because 
we have made the assumption that each stem occurs in exactly one signature. That assumption is not, 
strictly speaking, correct in natural anguage; we could well imagine an analysis that permitted the 
same stem to appear in several distinction signatures, and in that case, the computation here would not 
reduce to (9). But the assumption made in the text is entirely reasonable, and simplifies the 
construction for us. 
194 
Goldsmith Unsupervised Learning of the Morphology of a Natural Language 
(10) Difference in description length for the signature component of the morphology: 
(a) + (b) + (c) + (d) 
(a) Change in size of list of pointers to the signatures, 
& W I Signatures (1,2 ) ) - 
\[wll 
+ E log 
crC Sigrtatures(1N2) 
crC Signatures(l,2 ) 
E log \[WIz 
crC Signatures(~l,2) 
(b) Change in counts of stems and suffixes within each signature, summed 
over all signatures: 
Z \[A (stems(a)} + A (suffixes(a)}\] 
crff Signatures(i,2 ) 
- ~ \[log (stems(a)) + log (suffi'xes(er))\] 
ere Signature~(1,~2) 
+ Z \[log (stems(a)) + log (suffixes(c@\] 
~C Sigctature8(~l,2) 
(c) 
(c.i) 
(c.ii) 
Change in the lengths of the stem pointers within the signatures = (c.i) 
+ (c.ii) + (c.iii), as follows: 
Change in total length of flags for each stem indicating whether 
simple or complex: 
(WfiIMPLE)I,2 (&W - &WsIMPLE)  
q- ( WCOMPLEX ) 1,2 * (A  W - & WCOMPLEX ) 
\[wh 
q- (WsIMPLE) I ,~ 2 log \[WsIMPLE\]I 
-- (WsIMPLE)~I ,  2 log \[W\]2 \[Ws~veL.\]2 
\[wh + (WcoMPLEX)1 ~2 log - -  
' \ [WCOMULEXh 
\[w\]2 
- (WcoMPLEX)~I,2 log \[WcoMPLUX\]2 
Set of simple stems, change of pointers to stems: 
\[w\]2 AW (Stems)o,2) - Z At + Z log \[W\]I E log -~-  
Ste~q,8(1 2) tE Stems(l ~2) It\] tff , , tff Stems (~1,2) 
(c.iii) Change in length of pointers to complex stems from within 
signatures: 
&W (WcoMPLEX}(1,2) q- Z &stem(w) 
wffWcoMPLIdJX(L2) 
195 
Computational Linguistics Volume 27, Number 2 
\[w\]l \[w\]2 
+ E log \[stem(w)\]1 E log \[stem(w)\]2 
wE WCOMPLE X (1,~2) wE WCOMPLE X (~1,2) 
+ E&a(w)  - &\[suff(w)in ?(w)\] 
wE WCOMPLE X 
+ \[~(w)\] 
K-"Z.., log \[suff(w)in a(w)\] 
wE WCOMPLE X O,~2 ) 
\[?(w)\] 
K-"/_._, log \[suff(w)in a(w)\] 
wE WCOMPLE X (~1,2) 
(d) Change in size of suffix information in signatures: 
cr E Signatures (1,2) 
+ 
:EoZ as\] 
E E l ?g  ~In\]a\] 
ere Signatures(i,~2) fEo" 
E E l?g ~fi\[n\]a\] 
aE Signatures(~l,2) fE
(11) Change in compressed length of corpus 
\ [W\ ] ra~AW -- 
WEWA(1,2) 
+ ~ \[Wlra~aW 
wEWuN(1,2) 
+ ~ \[Wlra~lOg 
WEWA(1,~2) 
- ~ \[Wlra~ log 
wEWa(~l,2) 
E \[w\],.a~\[&stem(w) + &\[suffix(w) Na(w)\] - &?(w)\] 
\[stem(w)\]l \[suffix(w)1 N o-(W)l\] 
\[o-(w)\]l\[W\]2 , 
\[stem(w) \]2\[suffix(w)2 V~ o-(w)2 \] 
\[o-(w)\]2 \[w\] 1 
References 
Altmann, Gabriel and Werner Lehfeldt. 
1980. Einfiihrung in die Quantitative 
Phonologie. Quantitative Linguistics, 
vol. 7. Studienverlag Dr. N. Brockmeyer, 
Bochum. 
Andreev, Nikolai Dmitrievich, editor. 1965. 
Statistiko-kombinatornoe m delirovanie 
iazykov. Nauka, Moscow, Russia. 
Baroni, Marco. 2000. Paper presented atthe 
annual meeting of the Linguistics Society 
of America. Chicago, IL. 
Bloomfield, Leonard. 1933. Language. H. 
Holt and Company, New York. 
Brent, Michael R. 1993. Minimal generative 
models: A middle ground between 
neurons and triggers. In Proceedings ofthe 
15th Annual Conference of the Cognitive 
Science Society. pages 28-36, Lawrence 
Erlbaum Associates, Hillsdale, NJ. 
Chomsky, Noam. 1957. Syntactic Structures. 
Mouton, The Hague. 
Chomsky, Noam. 1975 \[1955\]. The Logical 
Structure of Linguistic Theory. Plenum 
Press, New York. 
de Marcken, Carl. 1995. Unsupervised 
Language Acquisition. Ph.D. dissertation, 
MIT, Cambridge, MA. 
196 
Goldsmith Unsupervised Learning of the Morphology of a Natural Language 
Dempster, Arthur Pentland, Nan M. Laird, 
and Donald B. Rubin. 1977. Maximum 
likelihood from incomplete data via the 
EM algorithm. Journal of the Royal 
Statistical Society, B 39(1):1-38. 
Dobrin, Lise. 1999. Phonological Form, 
Morphological Class, and Syntactic Gender: 
The Noun Class Systems of Papua New 
Guinea Arapeshan. Ph.D. dissertation, 
Department of Linguistics, University of 
Chicago, Chicago, IL. 
Dzeroski, Saso and Tomaz Erjavec. 1997. 
Induction of Slovene nominal paradigms. 
In Nada Lavrac and Saso Dzeroski, 
editors, Inductive Logic Programming, 7th 
International Workshop, ILP-97, 
pages 17-20, Prague, Czech Republic, 
September. Lecture Notes in Computer 
Science, Vol. 1297. Springer, Berlin. 
Flenner, Gudrun. 1994. Ein quantitatives 
Morphsegmentierungssystem f~ir 
spanische Wortformen. In Ursula Klenk, 
editor, Computatio Linguae II. Steiner 
Verlag, Stuttgart, pages 31-62. 
Flenner, Gudrun. 1995. Quantitative 
Morphsegmentierung im Spanischen auf 
phonologischer Basis. Sprache und 
Datenverarbeitung, 19(2):63-79. 
Gaussier, Eric. 1999. Unsupervised learning 
of derivational morphology from 
inflectional lexicons. In Proceedings ofthe 
Workshop on Unsupervised Learning in 
Natural Language Processing, pages 24-30. 
Association for Computational 
Linguistics. 
Goldsmith, John. 1990. Autosegmental and 
Metrical Phonology. Basil Blackwell, 
Oxford, England. 
Goldsmith, John. 2001. On information 
theory, entropy, and phonology in the 
20th century. Folia Linguistica XXXIV(1-2): 
85-100. 
Goldsmith, John and Tom Reutter. 1998. 
Automatic ollection and analysis of 
German compounds. In Frederica Busa, 
Inderjeet Mani, and Patrick Saint-Dizier, 
editors, The Computational Treatment of
Nominals: Proceedings ofthe Workshop, 
pages 61-69, COLING-ACL '98, Montreal. 
Greenberg, Joseph Harold. 1957. Essays in 
Linguistics. University of Chicago Press, 
Chicago, IL. 
Hafer, Margaret A. and Stephen F. Weiss. 
1974. Word segmentation by letter 
successor varieties. Information Storage and 
Retrieval, 10:371-385. 
Harris, Zellig. 1955. From phoneme to 
morpheme. Language, 31:190-222. 
Reprinted in Harris 1970. 
Harris, Zellig. 1967. Morpheme boundaries 
within words: Report on a computer test. 
Transformations and Discourse Analysis 
Papers 73, Department of Linguistics, 
University of Pennsylvania. Reprinted in 
Harris 1970. 
Harris, Zellig. 1970. Papers in Structural and 
Transformational Linguistics. D. Reidel, 
Dordrecht. 
Jacquemin, Christian. 1997. Guessing 
morphology from terms and corpora. 
Proceedings ofSIGIR 97, pages 156--165, 
ACM, Philadelphia. 
Janssen, Axel. 1992. Segmentierung 
franzOsischer Wortformen i  Morphe 
ohne Verwendung eines Lexikons. In 
Ursula Klenk, editor, Computatio Linguae. 
Steiner Verlag, Stuttgart, pages 74-95. 
Karttunen, Lauri. 1993. Finite state 
constraints. In John Goldsmith, editor, The 
Last Phonological Rule. University of 
Chicago Press, pages 173-194. 
Kazakov, Dimitar. 1997. Unsupervised 
learning of naive morphology with 
genetic algorithms. In W. Daelemans, A.
van den Bosch, and A. Weijtera, editors, 
Workshop Notes of the ECML/Mlnet 
Workshop on Empirical Learning of Natural 
Language Processing Tasks, April 26, 1997. 
Klenk, Ursula. 1992. Verfahren 
morphologischer Segmentierung und die 
Wortstruktur im Spanischen. In Ursula 
Klenk, editor, Computatio Linguae. Steiner 
Verlag, Stuttgart. 
Koch, Sabine, Andreas K(istner, and Barbara 
R~idiger. 1989. Deutsche 
Wortformensegmentierung oh e Lexicon. 
Sprache und Datenverarbeitung, 13:35-44. 
Koskenniemi, Kimmo. 1983. Two-level 
morphology: A general computational 
model for word-form recognition and 
production. Publication o. 11, 
Department of General Linguistics, 
University of Helsinki, Helsinki. 
Langer, Hagen. 1991. Ein automatisches 
Morphosegmentierungsver fahren f6r 
deutsche Wortformen. Manuscript. 
Li, Ming and Paul Vit~nyi. 1997. An 
Introduction to Kolmogorov Complexity and 
its Applications (2nd ed.). Springer, New 
York. 
Nida, Eugene. 1948. The identification of 
morphemes. In Martin Joos, editor, 
Readings in Linguistics I. University of 
Chicago Press, Chicago, IL, 
pages 255-271. 
Nida, Eugene. 1949. Morphology: The 
Descriptive Analysis of Words. University of 
Michigan, Ann Arbor. 
Pacak, Milos and Arnold W. Pratt. 1976. 
Automated morphosyntactic analysis of 
medical language. Information Processing 
and Management, 12:71-76. 
197 
Computational Linguistics Volume 27, Number 2 
Radhakrishnan, T. 1978. Selection of prefix 
and postfix word fragments for data 
compression. Information Processing and 
Management, 14(2):97-106. 
Rissanen, Jorma. 1989. Stochastic Complexity 
in Statistical Inquiry. World Scientific 
Publishing Co, Singapore. 
Solomonoff, Ray. 1995. The discovery of 
algorithmic probability: A guide for the 
programming of true creativity. In P. 
Vitdnyi, editor, Computational Learning 
Theory. Spring Verlag, Berlin. 
Wothke, Klaus and Rudolf Schmidt. 1992. A 
morphological segmentation procedure 
for German. Sprache und Datenverarbeitung, 
16(1):15-28. 
198 
Using eigenvectors of the bigram graph  
to infer morpheme identity 
 
Mikhail Belkin     John Goldsmith 
 
Department of Mathematics Department of Linguistics 
University of Chicago 
Chicago IL 60637 
misha@math.uchicago.edu  ja-goldsmith@uchicago.edu 
 
Abstract 
This paper describes the results of 
some experiments exploring statistical 
methods to infer syntactic categories 
from a raw corpus in an unsupervised 
fashion. It shares certain points in 
common with Brown et at (1992) and 
work that has grown out of that: it 
employs statistical techniques to 
derive categories based on what 
words occur adjacent to a given word. 
However, we use an eigenvector 
decomposition of a nearest-neighbor 
graph to produce a two-dimensional 
rendering of the words of a corpus in 
which words of the same syntactic 
category tend to form clusters and 
neighborhoods. We exploit this 
technique for extending the value of 
automatic learning of morphology. In 
particular, we look at the suffixes 
derived from a corpus by 
unsupervised learning of morphology, 
and we ask which of these suffixes 
have a consistent syntactic function 
(e.g., in English, -ed is primarily a 
mark of verbal past tense, does but ?s 
marks both noun plurals and 3rd 
person present on verbs). 
1 Introduction 
This paper describes some results of our efforts 
to develop statistical techniques for 
unsupervised learning of syntactic word-
behavior, with two specific goals: (1) the 
development of visualization tools displaying 
syntactic behavior of words, and (2) the 
development of quantitative techniques to test 
whether a given candidate set of words acts in a 
syntactically uniform way, in a given corpus.1  
 
In practical terms, this means the development 
of computational techniques which accept a 
corpus in an unknown language as input, and 
produce as output a two-dimensional image, 
with each word identified as a point on the 
image, in such a fashion that words with similar 
syntactic behavior will be placed near to each 
other on the image.  
We approach the problem in two stages: first, 
a nearest-neighbor analysis, in which a graph is 
constructed which links words whose 
distribution is similar, and second, what we 
might call a planar projection of this graph onto 
R2, that is to say, a two-dimensional region, 
which is maximally faithful to the relations 
expressed by the nearest-neighbor graph.   
2 Method 
The construction of the nearest-neighbor graph 
is a process which allows for many linguistic 
and practical choices. Some of these we have 
experimented with, and others we have not, 
simply using parameter values that seemed to us 
to be reasonable.  Our goal is to develop a graph 
in which vertices represent words, and edges 
represent pairs of words whose distribution in a 
corpus is similar. We then develop a 
representation of the graph by a symmetric 
matrix, and compute a small number of the 
eigenvectors of the normalized laplacian for 
                                                     
1
 We are grateful to Yali Amit for drawing our 
attention to Shi and Malik 1997, to Partha Niyogi for 
helpful comments throughout the development of this 
material, and to Jessie Pinkham for suggestions on an 
earlier draft of this paper. 
                     July 2002, pp. 41-47.  Association for Computational Linguistics.
        ACL Special Interest Group in Computational Phonology (SIGPHON), Philadelphia,
       Morphological and Phonological Learning: Proceedings of the 6th Workshop of the
which the eigenvalues are smallest. These 
eigenvectors provide us with the coordinates 
necessary for our desired planar representation, 
as explained in section 2.2. 
2.1 Nearest-neighbor graph 
construction. 
We begin with the reasonable working 
assumption that to determine the syntactic 
category of a given word w, it is the set of words 
which appears immediate before w, and the set 
of words that appears immediately after w, that 
gives the best immediate evidence of a word?s 
syntactic behavior. In a natural sense, under that 
assumption, an explicit description of the 
behavior of a word w in a corpus is a sparse 
vector L = [l1, l2, ?, lV], of length V (where ?V? 
is the number of words in the vocabulary of the 
corpus), indicating by li how often each word vi 
occurs immediately to the left of w, and also an 
similar vector R, also of length V, indicating 
how often each word occurs immediately to the 
right of w. Paraphrasing this, we may view the 
syntactic behavior of a word in a corpus as being 
expressed by its location in a space of 2V 
dimensions, or a vector from the origin to this 
location; this space has a natural decomposition 
into two spaces, called Left and Right, each of 
dimension V.  
Needless to say, such a representation is not 
directly illuminating -- nor does it provide a way 
to cogently present similarities or clusterings 
among words.  We now construct a symmetrical 
graph (?LeftGraph?), whose vertices are the K 
most frequent words in the corpus. (We have 
experimented with K = 500 and K = 1000). For 
each word w, we compute the cosine of the 
angle between the vector w and the K-1 other 
words wi: |||| i
i
ww
ww ?
, and use this figure to select 
the N words closest to w. We have experimented 
with N = 5,10,20 and 50. We insert an edge (vi, 
vj) in LeftGraph if vi is one of the N words 
closest to vj or vj is one of the N words closest to 
vi.. We follow the same construction for 
RightGraph in the parallel fashion. In much of 
the discussion that follows, the reader may take 
whatever we say about LeftGraph to hold 
equally true of RightGraph when not otherwise 
stated. 
2.2 Projection of nearest-neighbor 
graph by spectral decomposition 
In the canonical matrix representation of a 
(unweighted) graph, an entry M(i,j), with i 
distinct from j, is 1 if the graph includes an edge 
(i,j) and 0 otherwise. All diagonal elements are 
zero. The degree of a vertex of a graph is the 
number of edges adjacent to it; the degree of the 
m
th
 vertex, d(vm)is thus the sum of the values in 
the mth row of M. If we define D as the diagonal 
matrix whose entry D(m,m) is d(vm), the degree 
of vm, then the laplacian of the graph is defined 
as D ? M. The normalized laplacian L is defined 
as D? ( D ? M ) D?. The effect of normalization 
on the laplacian is to divide the weight of an 
entry M(i,j) that represents the edge between vi 
and vj by )()(
1
ji vdvd
, and to set the values of 
the diagonal elements to 1.2 
The laplacian is a symmetric matrix which is 
known to be positive semi-definite (Chung 
1997). Therefore all the eigenvalues of the 
laplacian are non-negative. We return to the 
space of our observations by premultiplying the 
eigenvectors by D?. We will refer to these 
eigenvectors derived from LeftGraph (pre-
multiplied by D?) as {L0, L1, ?} and those 
derived from RightGraph as {R0, R1, ?}.  
Now, L0 (and R0) are trivial (they merely 
express the frequency of the words in the 
corpus), but L1 and L2 provide us with very 
useful information. They each consist of a 
vector with one coordinate for each word among 
the K most frequent words in the corpus, and 
thus can be conceived of as a 1-dimensional 
representation of the vocabulary. In particular, 
L1 is the 1-dimensional representation that 
optimally preserves the notion of locality 
described by the graph we have just constructed, 
and the choice of the top N eigenvectors 
provides a representation which optimally 
preserves the graph-locality in N-space. By 
virtue of being eigenvectors in the same 
eigenvector decomposition, L1 and L2 are 
orthogonal, but subject to that limitation, the 
projection to R2 using the coordinates of L1 and 
L2 is the 2-dimensional representation that best 
                                                     
2
 Our attention was drawn to the relevance of the 
normalized laplacian by Shi and Malik 1997, who 
explore a problem in the domain of vision. We are 
indebted to Chung 1997 on spectral graph theory. 
preserves the locality described by the graph in 
question (Chung 1997, Belkin and Niyogi 
2002). 
Thus, to the extent that the syntactic behavior 
of a word can be characterized by the set of its 
immediate right- and left-hand neighbors (which 
is, to be sure, a great simplification of syntactic 
reality), using the lowest-valued eigenvectors 
provides a good graphical representation of 
words, in the sense that words with similar left-
hand neighbors will be close together in the 
representation derived from the LeftGraph (and 
similarly for RightGraph).  
2.3 Choice of graphs 
We explore below two types of projection to 2 
dimensions: plotting the 1st and 2nd eigenvectors 
of LeftGraph (and RightGraph), and plotting the 
1st eigenvectors of LeftGraph and RightGraph 
against each other. In all of these cases, we have 
built a graph using the 20 nearest neighbors. In 
future work, we would like to look at varying 
the number of nearest neighbors that are linked 
to a given word. From manual inspection, one 
can see that in all cases, the nearest two or three 
words are very similar; but the depth of the 
nearest neighbor list that reflects words of truly 
similar behavior is, roughly, inversely 
proportional to the frequency of the word. This 
is not surprising, in the sense that higher 
frequency words tend to be grammatical words, 
and for such words there are fewer members of 
the same category. 
 
2.4  English 
Figure 1 illustrates the results of plotting the 1st 
and 2nd eigenvectors of LeftGraph based on the 
first 1,000,000 words of the Brown corpus, and 
using the 1,000 most frequent words and 
constructing a graph based on the 20 nearest 
neighbors. Figure 2 illustrates the results derived 
from the first two eigenvectors of RightGraph. 
Figures 1 and 2 suggest natural clusterings, 
based both on density and on the extreme values 
of the coordinates. In Figure 1 (LeftGraph), the 
bottom corner consists primarily of non-finite 
verbs (be, do, make); the left corner of finite 
verbs (was, had, has); the right corner primarily 
of nouns (world, way, system); while the top 
shows little homogeneity, though it includes the 
prepositions. See Appendix 1 for details; the 
words given in the appendix are a complete list 
of the words in a neighborhood that includes the 
extreme tip of the representation. As we move 
away from the extremes, in some cases we find 
a less homogeneous distribution of categories, 
while in others we find local pockets of 
linguistic homogeneity: for example, regions 
containing names of cities, others containing 
names of countries or languages. 
 
 
Figure 1 English based on left-neighbors 
 
 
Figure 2 English based on right-neighbors 
 
In Figure 2, the bottom corner consists of 
adjectives (social, national, white), the left 
corner of words that often are followed by of 
(most, number, kind, secretary), the right corner 
primarily by prepositions (of, in for, on by) and 
the top corner of words that often are followed 
by to (going, wants, according), (See Appendix 
2 for details).  
 
2.5 French 
Figure 3 illustrates the results of plotting the 1st 
and 2nd eigenvectors of LeftGraph based on the 
first 1,000,000 words of a French encyclopedia, 
using the 1,000 most frequent words and 
constructing a graph based on the 20 nearest 
neighbors. 
The bottom left tip of the figure consists 
entirely of feminine nouns (guerre, population, 
fin), the right tip of plural nouns (ann?es, ?tats-
unis, r?gions), the top tip of finite verbs (est, fut, 
a, avait) plus se and y. A bit under the top tip 
one finds two sharp-tipped clusters; the one on 
the left consists of masculine nouns (pays, sud, 
monde). Other internal clusters, not surprisingly, 
are composed of words which, with high 
frequency, are preceded by a specific pre-
position (e.g., preceded by ?: peu, l?est, Paris; 
by en: particulier, effet, and feminine names of 
geographical areas such as France). 
Figure 4 illustrates plotting the 1st 
eigenvector of LeftGraph against the 1st 
eigenvector of RightGraph. We find a striking 
?striped? effect which is due to the 
masculine/feminine gender system of French. 
There are three stripes that stand out at the top 
of the figure. The one on the extreme left 
consists of singular feminine nouns, the one to 
its right, but left of center, consists of singular 
masculine nouns, and the one on the extreme 
right consists of plural nouns of both genders. 
The lowest region of the graph, somewhat left of 
center, contains grammatical morphemes. At the 
very bottom are found relative and 
subordinating conjunctions (o?, car, lequel, 
laquel, lesquelles, lesquels, quand, si), and just 
above them are the prepositions: selon, durant, 
malgr?, pendant, apr?s, entre, jusqu??, contre, 
sur, etc.) 
We find it striking that the gender system of 
French has such a pervasive impact upon the 
global form of the 1st eigenvector map as in 
Figure 4, and we plan to run further experiments 
with other language with gender systems to see 
the extent to which this result obtains 
consistently. 
Figure 3 French based on left-neighbors 
Figure 4 French 1st eigenvector of Left and Right 
3 Identifying syntactic behavior of 
automatically identified suffixes 
Interesting as they are, the representations we 
have seen are not capable of specifying 
membership in grammatical categories in an 
absolute sense. In this section, we explore the 
application of this representation to a text which 
has been morphologically analyzed by a 
language-neutral morphological analyzer. For 
this purpose, we employ the algorithm described 
in Goldsmith (2001), which takes an unanalyzed 
corpus and provides an analysis of the words 
into stems and suffixes. What is useful about 
that algorithm for our purposes is that it shares 
the same commitment to analysis based only on 
a raw (untreated) natural text, and neither hand-
coding nor prior linguistic knowledge. 
The algorithm in Goldsmith (2001) links 
each stem in the corpus to the set of suffixes 
(called its signature) with which it appears in 
the corpus. Thus the stem jump might appear 
with the three suffixes ed-ing-s in a given 
corpus. 
But a morphological analyzer alone is not 
capable of determining whether the ?ed that 
appears in the signature ed-ing-s is the same ?ed 
suffix that appears in the signature ed-ing (for 
example), or whether the suffix ?s in ed-ing-s is 
the same suffix that appears in the signature 
NULL-s-?s (this last signature is the one 
associated with the stem boy in a corpus 
containing the words boy-boys-boy?s). A 
moment?s reflection shows that the suffix ?ed is 
indeed the same verbal past tense suffix in both 
cases, but the suffix ?s is different: in the first 
case, it is a verbal suffix, while in the second it 
is a noun suffix. 
In general, morphological information alone 
will not be able to settle these questions, and 
thus automatic morphology alone will not be 
able to determine which signatures should be 
?collapsed? (that is, ed-ing-s should be viewed 
as a special sub-case of the signature NULL-ed-
ing-s, but NULL-s is not to be treated as a 
special case of NULL-ed-ing-s).  
We therefore have asked whether the 
rudimentary syntactic analysis described in the 
present paper could provide the information 
needed for the automatic morphological 
analyzer. 
The answer appears to be that if a suffix has 
an unambiguous syntactic function, then that 
suffix?s identity can be detected automatically 
even when it appears in several different 
signatures. As we will see momentarily, the 
clear example of this is English -ed, which is 
(almost entirely) a verbal suffix. When a suffix 
is not syntactically homogeneous, then the 
words in which that suffix appears are scattered 
over a much larger region, and this difference 
appears to be quite sharply measurable. 
3.1 The case of  the verbal suffix ?ed 
In the automatic morphological analysis of the 
first 1,000,000 words of the Brown corpus that 
we produced, there are 26 signatures that 
contain the suffix ?ed: NULL.ed.s, e.ed.ing, 
NULL.ed.er.es.ing, and 23 others of similar sort. 
We calculated a nearest neighbor graph as 
described above, with a slight variation. We 
considered the 1000 most frequent words to be 
atomic and unanalyzed morphologically, and 
then of the remaining words, we automatically 
replaced each stem with its corresponding 
signature. Thus as jumped is analyzed as 
jump+ed, and jump is assigned the signature 
NULL.ed.er.s.ing (based on the actual forms of 
the stem found in the corpus), the word jumped 
is replaced in the bigram calculations by the 
pseudo-word NULL.ed.er.s.ing_ed: the stem 
jump is replaced by its signature, and the actual 
suffix -ed remains unchanged, but is separated 
from its stem by an underscore _. Thus all words 
ending in ?ed whose stems show the same 
morphological variations are treated as a single 
element, from the point of view of our present 
syntactic analysis.  
We hoped, therefore, that these 26 signatures 
with ?ed appended to them would appear very 
close to each other in our 2-dimensional 
representation, and this was exactly what we 
found. 
To quantify this result, we calculated the 
coordinates of these 26 signatures in the 
following way. We normalize coordinates so 
that the lowest x-coordinate value is 0.0 and the 
highest is 1.0; likewise for the y-coordinates. 
Using these natural units, then, on the LeftGraph 
data, the average distance from each of the 
signatures to the center of these 26 points is 
0.050. While we do not have at present a 
criterion to evaluate the closeness of this 
clustering, this appears to us at this point to be 
well within the range that an eventual criterion 
will establish. (A distance of 0.05 by this 
measure is a distance equal to 5% along either 
one of the axes, a fairly small distance.) On the 
RightGraph data, the average distance is 0.054. 
3.2 The cases of ?s and ?ing  
By contrast, when we look at the range of the 19 
signatures that contain the suffix ?s, the average 
distance to mean in the LeftGraph is 0.265, and 
in the RightGraph, 0.145; these points are much 
more widely scattered.  We interpret this as 
being due to the fact that ?s serves at least two 
functions: it marks the 3rd person present form 
of the verb, as well as the nominal plural. 
Similarly, the suffix ?ing marks both the 
verbal progressive form as well as the 
gerundive, used both as an adjective and as a 
noun, and we expect a scattering of these forms 
as a result. We find an average to mean of 0.096 
in the LeftGraph, and of 0.143 in the 
RightGraph. 
By way of even greater contrast, we can 
calculate the scatter of the NULL suffix, which 
is identified in all stems that appear without a 
suffix (e.g., the verb play, the noun boy). This 
?suffix? has an average distance to mean of  
0.312 in the LeftGraph, and 0.192 in the 
RightGraph. This is the scatter we would expect 
of a group of words that have no linguistic 
coherence. 
3.3 Additional suffixes tested 
Suffix ?ly occurs with five signatures, and an 
average distance to mean of 0.032 in LeftGraph, 
and 0.100 in RightGraph.3 The suffix ?s occurs 
in only two signatures, but their average 
distance to mean is 0.000 [sic] in LeftGraph, and  
0.012 in RightGraph. Similarly, the suffix ?al 
appears in two signatures (NULL.al.s and 
NULL.al), and their average distance to mean is 
0.002 in LeftGraph, and also 0.002 in 
RightGraph. The suffix ?ate appears in three 
signatures, with an average distance to mean of 
0.069 in LeftGraph, and 0.080 in RightGraph. 
The suffix ?ment appears in two signatures, with 
an average distance to mean of 0.012 in 
LeftGraph, and 0.009 in RightGraph.  
3.4 French suffixes ?ait, -er, -a, -ant, -e 
We performed the same calculation for the 
French suffix ?ait as for the English suffixes 
discussed above. ?ait is the highest frequency 
3rd person singular imperfect verbal suffix, and 
as such is one of the most common verbal 
suffixes, and it has no other syntactic functions. 
It appears in seven signatures composed of 
verbal suffixes, and they cluster well in the 
spaces of both LeftGraph and RightGraph, with 
an average distance to mean of 0.068 in the 
LeftGraph, and 0.034 in the RightGraph. 
The French suffix ?er is by far the most 
frequent infinitival marker, and it appears in 14 
signatures, with an average distance to mean of 
0.055 in LeftGraph, and 0.071 in RightGraph. 
The 3rd singular simple past suffix ?a appears 
in 11 signatures, and has an average distance to 
mean of 0.023 in LeftGraph, and 0.029 in 
RightGraph. 
The present participle verbal suffix ?ant 
appears in 10 suffixes, and has an average 
                                                     
3
 This latter figure deserves a bit more scrutiny; one 
of the five is an outlier: if we restricted our attention 
to four of them, the average distance to mean is 
0.014. 
distance to mean of 0.063 in LeftGraph, and of 
0.088 in RightGraph. 
On the other hand, the suffix ?e appears as 
the last suffix in a syntactically heterogeneous 
set of words: nouns, verbs, and adjectives. It has 
an average distance to mean of 0.290 in 
LeftGraph and of 0.130 in RightGraph. This is 
as we expect: it is syntactically heterogeneous, 
and therefore shows a large average distance to 
mean. 
3.5 Summary 
Here are the average distances to mean for the 
cases where we expect syntactic coherence and 
the cases where we do not expect syntactic 
coherence. Our hypothesis is that the numbers 
will be small for the suffixes where we expect 
coherence, and large for those where we do not 
expect coherence, and this hypothesis is strongly 
borne out.  We note empirically that we may 
take an average value of the two columns of .10 
as a reasonable cut-off point.  
 
 LeftGraph RightGraph 
Expect coherence:  
ed 0.050 0.054 
-ly 0.032 0.100 
?s  0.000 0.012 
-al 0.002 0.002 
-ate 0.069 0.080 
-ment 0.012 0.009 
-ait 0.068 0.034 
-er 0.055 0.071 
-a 0.023 0.029 
-ant 0.063 0.088 
 LeftGraph RightGraph 
Expect little/no coherence: 
-s 0.265 0.145 
-ing  0.096 0.143 
NULL 0.312 0.192 
-e 0.290 0.130 
Figure 5 Average distance to mean of suffixes 
4 Conclusions 
We have presented a simple yet mathematically 
sound method for representing the similarity of 
local syntactic behavior of words in a large 
corpus, and suggested one practical application. 
We have by no means exhausted the possibilities 
of this treatment. For example, it seems very 
reasonable to adjust the number of nearest 
neighbors permitted in the graph based on word-
frequency: the higher the frequency, the fewer 
the number of nearest neighbors would be 
permitted in the graph. We leave this and other 
questions for future research. 
This method does not appear strong enough 
at present to establish syntactic categories with 
sharp boundaries, but it is strong enough to 
determine with some reliability whether  sets of 
words proposed by other, independent heuristics 
(such as presence of suffixes determined by 
unsupervised learning of morphology) are 
syntactically homogenous. 
 
 
 
The reader can download the files discussed in 
this paper and a graphical viewer from 
http://humanities.uchicago.edu/faculty/ 
goldsmith/eigenvectors/. 
Appendix 1 
Typical examples from corners of Figure 1. 
Bottom: 
be do me make see 
get take go say put 
find give provide keep run 
tell leave pay hold live 
Left: 
was had has would said 
could did might went thought 
told took asked knew felt 
began saw gave looked became 
Right: 
world way same united right 
system city case church problem 
company past field cost department 
university rate center door surface 
Top: 
and to in that for 
he as with on by 
at or from but I 
they we there you who 
 
 
Appendix 2 
Typical examples from corners of Figure 2. 
Bottom: 
social national white local  political 
personal private strong medical final 
black French technical nuclear british 
health husband blue  
 
Left: 
most number kind full type 
secretary amount front instead member 
sort series rest types  piece 
image lack    
Right: 
of in for on by 
at from into after through 
under since  during against among 
within along across including near 
Top: 
going want seems seemed able 
wanted likely difficult according due 
tried decided trying related try 
References 
Mikhail Belkin and Partha Niyogi. 2002. 
Laplacian Eigenmaps for Dimensionality 
Reduction and Data Representation. TR-2002-
01. Available at http://www.cs.uchicago.edu/ 
research/publications/techreports/TR-2002-02 
Peter F. Brown, Vincent J. Della Pietra, Peter 
V. deSouza, Jenifer C. Lai, Robert L. Mercer. 
1992.  Class-based n-gram models of natural 
language. Computational Linguistics 18(4): 467 
?479.  
Chung, Fan R.K. 1997. Spectral Graph 
Theory. Regional Conf. Series in Math., Vol. 92, 
Amer. Math. Soc., Providence, RI. 
Goldsmith, J. 2001. Unsupervised learning of 
the morphology of a natural language. 
Computational Linguistics 27(2): 153-198. 
Shi, Jianbo and Jitendra Malik. 2000. 
Normalized Cuts and Image Segmentation. 
IEEE Transactions on Pattern Analysis and 
Machine Intelligence 22(8): 888-905. 
Proceedings of the Second Workshop on Psychocomputational Models of Human Language Acquisition, pages 20?27,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Using Morphology and Syntax Together 
in Unsupervised Learning 
Yu Hu and Irina Matveeva  
Department of  
Computer Science 
The University of Chicago 
Chicago IL 60637 
yuhu@cs.uchicago.edu
matveeva 
@uchicago.edu 
John Goldsmith 
Departments of Linguistics and 
Computer Science  
The University of Chicago 
Chicago IL 60637 
ja-goldsmith 
@uchicago.edu 
 
Colin Sprague 
Department of Linguistics 
The University of Chicago 
Chicago IL 60637 
sprague 
@uchicago.edu 
  
Abstract 
Unsupervised learning of grammar is a 
problem that can be important in many 
areas ranging from text preprocessing 
for information retrieval and 
classification to machine translation. 
We describe an MDL based grammar 
of a language that contains morphology 
and lexical categories. We use an 
unsupervised learner of morphology to 
bootstrap the acquisition of lexical 
categories and use these two learning 
processes iteratively to help and 
constrain each other. To be able to do 
so, we need to make our existing 
morphological analysis less fine 
grained. We present an algorithm for 
collapsing morphological classes 
(signatures) by using syntactic context. 
Our experiments demonstrate that this 
collapse preserves the relation between 
morphology and lexical categories 
within new signatures, and thereby 
minimizes the description length of the 
model. 
1 Introduction 
Our long term goal is the development of 
methods which will allow one to produce 
optimal analyses from arbitrary natural language 
corpora, where by optimization we understand 
an MDL (minimum description length; 
Rissanen, 1989) interpretation of the term: an 
optimal analysis is one which finds a grammar 
which simultaneously minimizes grammar 
length and data compression length. Our specific 
and primary focus is on morphology, and on 
how knowledge of morphology can be a useful 
step towards a more complete knowledge of a 
language?s linguistic structure. 
Our strategy is based on the following 
observation: knowing the rightmost suffix of a 
word is very useful information in inferring (or 
guessing) a word?s part of speech (POS), but due 
to the ambiguity of many suffixes, it is even 
better to know both a word?s suffix and the 
range of other suffixes that the word?s stem 
appears with elsewhere, i.e., its signature. As we 
will see below, this conjunction of ?better? 
information is what we call the signature 
transform, and in this paper, we explore how 
knowledge of signature transform can be merged 
with knowledge of the context vector to draw 
conclusions about morphology and syntax.  
In the distant future, we would like to be able 
to use the signature transform in a general 
process of grammar induction, but that day is 
not here; we therefore test our experiments by 
seeing how well we are able to predict POS as 
assigned by an available tagger (TreeTagger; 
Schmid 1994). In particular, we wish to decrease 
the uncertainty of a word?s POS through the 
morphological analysis described here. This 
decrease of uncertainty will enter into our 
calculation through an increase in the 
probability assigned to our test corpus once the 
corpus has been augmented with TreeTagger 
assigned POS tags. But to be clear on our 
20
process: we analyze a completely raw text 
morphologically, and use the POS tags from 
TreeTagger only to evaluate the signature 
transforms that we generate. 
We assume without argument here that any 
adequate natural language grammar will contain 
a lexicon which includes both lexical stems 
which are specified for morphological 
properties, such as the specific affixes with 
which they may occur, and affixes associated 
with lexical categories. We also explicitly note 
that many affixes are homophonous: they are 
pronounced (or written) identically, but have 
different morphological or syntactic 
characteristics, such as the English plural ?s and 
the verbal 3rd person singular present ?s. 
We focus initially on unsupervised learning 
of morphology for three reasons: first, because 
we already have a quite successful unsupervised 
morphological learner; second, the final suffix of 
a word is typically the strongest single indicator 
of its syntactic category; and third, analysis of a 
word into a stem T plus suffix F allows us 
(given our knowledge that the suffix F is a 
stronger indicator of category than the stem T) 
to collapse many distinct stems into a single 
cover symbol for purposes of analysis, 
simplifying our task, as we shall see.1 We 
eschew the use of linguistic resources with hand-
(i.e., human-)assigned morphological infor-
mation in order for this work to contribute, 
eventually, to a better theoretical understanding 
of human language acquisition. 
We present in this paper an algorithm that 
modifies the output of the morphology analyzer 
by combining redundant signatures. Since we 
ultimately want to use signatures and signature 
transforms to learn syntactic categories, we 
developed an algorithm that uses the syntactic 
contextual information. We evaluate the changes 
to the morphological analysis from the 
standpoint of efficient and adequate 
representation of lexical categories. This paper 
presents a test conducted on English, and thus 
can only be considered a preliminary step in the 
                                                          
1 See Higgins 2002 for a study similar in some ways; 
Higgins uses morphology as a bootstrap heuristic in one 
experimental set-up. This paper is heavily indebted to prior 
work on unsupervised learning of position categories such 
as Brown et al1992, Sch?tze 1997, Higgins 2002, and 
others cited there.  
eventually development of a language-
independent tool for grammar induction based 
on morphology. Nonetheless, the concepts that 
motivate the process are language-independent, 
and we are optimistic that similar results would 
be found in tests based on texts from other 
languages.  
In section 2 we discuss the notion of 
signature and signature transform, and section 3 
present a more explicit formulation of the 
general problem. In section 4 we present our 
algorithm for signature collapse. Section 5 
describes the experiments we ran to test the 
signature collapsing algorithm, and section 6 
presents and discusses our results. 
2 Signatures and signature transforms 
We employ the unsupervised learning of 
morphology developed by Goldsmith 
(Goldsmith, 2001). Regrettably, some of the 
discussion below depends rather heavily on 
material presented there, but we attempt to 
summarize the major points here. 
Two critical terms that we employ in this 
analysis are signature and signature transform. 
A signature found in a given corpus is a pair of 
lists: a stem-list and a suffix-list (or in the 
appropriate context, a prefix-list). By definition 
of signature ?, the concatenation of every stem 
in the stem-list of ? with every suffix in the 
suffix-list of ? is found in the corpus, and a 
morphological analysis of a corpus can be 
viewed as a set of signatures that uniquely 
analyze each word in the corpus. For example, a 
corpus of English that includes the words jump, 
jumps, jumped, jumping, walk, walks, walked, 
and walking might include the signature ?1 
whose stem list is { jump, walk } and whose 
suffix list is { ?, ed, ing , s }. For convenience, 
we label a signature with the concatenation of its 
suffixes separated by period ?.?. On such an 
analysis, the word jump is analyzed as belonging 
to the signature ?.ed.ing.s, and it bears the 
suffix ?. We say, then, that the signature 
transform of jump is ?.ed.ing.s_ ?, just as the 
signature transform of jumping is 
?.ed.ing.s_ing; in general, the signature 
transform of a word W, when W is morpho-
logically analyzed as stem T followed by suffix 
F, associated with signature ?, is defined as ?_F. 
21
In many of the experiments described below, 
we use a corpus in which all words whose 
frequency rank is greater than 200 have been 
replaced by their signature transforms. This 
move is motivated by the observation that high 
frequency words in natural languages tend to 
have syntactic distributions poorly predictable 
by any feature other than their specific identity, 
whereas the distribution properties of lower 
frequency words (which we take to be words 
whose frequency rank is 200 or below) are better 
predicted by category membership.  
In many cases, there is a natural connection 
between a signature transform and a lexical 
category. Our ultimate goal is to exploit this in 
the larger context of grammar induction. For 
example, consider the signature ?.er.ly, which 
occurs with stems such as strong and weak; in 
fact, words whose signature transform is 
?.er.ly_ ? are adjectives, those whose signature 
transform is ?.er.ly_er are comparative 
adjectives, and those whose signature transform 
is ?.er.ly_ly are adverbs. 
The connection is not perfect, however. 
Consider the signature ?.ed.ing.s and its four 
signature transforms. While most words whose 
? -transform is ?.ed.ing.s_s are verbs (indeed, 
3rd person singular present tense verbs, as in he 
walks funny), many are in fact plural nouns (e.g., 
walks in He permitted four walks in the eighth 
inning is a plural noun). We will refer to this 
problem as the signature purity problem?it is 
essentially the reflex of the ambiguity of 
suffixes. 
In addition, many 3rd person singular present 
tense verbs are associated with other signature 
transforms, such as ?.ing.s_s, ?.ed.s_s, and so 
forth; we will refer to this as the signature-
collapsing problem, because all other things 
being equal, we would like to collapse certain 
signatures, such as ?.ed.ing.s and ?.ed.ing, 
since a stem that is associated with the latter 
signature could have appeared in the corpus with 
an -s suffix; removing the ?.ed.ing signature and 
reassigning its stems to the ?.ed.ing.s signature 
will in general give us a better linguistic analysis 
of the corpus, one that can be better used in the 
problem of lexical category induction. This is 
the reflex of the familiar data sparsity concern.2   
Since we ultimately want to use signatures 
and signature transforms to learn syntactic 
categories, we base the similarity measure 
between the signatures on the context.   
3 A more abstract statement of the 
problem  
A minimum description length (MDL) analysis 
is especially appropriate for machine learning of 
linguistic analysis because simultaneously it 
puts a premium both on analytical simplicity and 
on goodness of fit between the model and the 
data (Rissanen 1989).  
We will present first the mathematical 
statement of the MDL model of the morphology, 
in (1), following the analysis in Goldsmith 
(2001), followed by a description of the meaning 
of the terms of the expressions, and then present 
the modified version which includes additional 
terms regarding part of speech (POS) 
information, in (2) and (3).  
(1) Morphology 
a. Grammar g =   
[ ])|(log)(minarg gDataprobgLength
Gg
?
?
 
b. =)(gLength  
 ? ?
=? <?
??
???
? +
stemsofsetTt ti itfreqt
W
||0 ][
1log
)]([
][log ?  
? ?
=? <?
+
affixesofsetFf fi iffreq||0 ][
1log  
??
?? ?
??
???
? +?+ ? ? ?
?
f f
W
f ][
][log
][
][log  
                                                          
2 The signature-collapsing problem has another side to it as 
well. An initial morphological analysis of English will 
typically give rise to a morphological analysis of words 
such as move, moves, moved, moving with a signature 
whose stems include mov and whose affixes are e.ed.es.ing. 
A successful solution to the signature-collapsing problem 
will collapse ?.ed.ing.s with e.ed.es.ing, noting that ? ~ e, 
ed ~ed, es ~ s, and ing ~ ing in an obvious sense. 
22
c. =)|(log gDataprob  
?
+=? ?
??
?
?
??
?
?
?
+
+
? ?
?
?
, ),|(log
)|(log
)(log
ftw
Dataw tfprob
tprob
prob
 
 
Equation (1a) states that our goal is to find 
the (morphological) grammar that 
simultaneously minimizes the sum of its own 
length and the compressed length of the data it 
analyzes, while (1b) specifies the grammar 
length (or model length) as the sum of the 
lengths of the links between the major 
components of the morphology: the list of letters 
(or phonemes) comprising the morphemes, the 
morphemes (stems and affixes), and the 
signatures. We use square brackets ?[.]? to 
denote the token counts in a corpus containing a 
given morpheme or word. The first line of (1b) 
expresses the notion that each stem consists of a 
pointer to its signature and a list of pointers to 
the letters that comprise it; ?(t) is the signature 
associated with stem t, and we take its 
probability to be 
][
)]([
W
t? , the empirical count of 
the words associated with ?(t) divided by the 
total count of words in the data. The second line 
expresses the idea that the morphology contains 
a list of affixes, each of which contains a list of 
pointers to the letters that comprise it. The third 
line of (1b) expresses the notion that a signature 
consists of a list of pointers to the component 
affixes. (1c) expresses the compressed length of 
each word in the data.3 
We now consider extending this model to 
include part of speech labeling, as sketched in 
(2). The principal innovation in (2) is the 
addition of part of speech tags; each affix is 
associated with one or more POS tags. As we 
                                                          
3 We do not sum over all occurrences of a word in the 
corpus; we count the compressed length of each word type 
found in the corpus. This decision was made based on the 
observation that the (compressed length of the) data term 
grows much faster than the length of the grammar as the 
corpus gets large, and the loss in ability of the model to 
predict word frequencies overwhelms any increase in 
model simplicity when we count word tokens in the data 
terms. We recognize the departure from the traditional 
understanding of MDL here, and assume the responsibility 
to explain this in a future publication. 
have seen, a path from a particular signature ? to 
a particular affix f constitutes what we have 
called a particular signature transform ?_f ; and 
we condition the probabilities of the POS tags in 
the data on the preceding signature 
transformation. As a result, our final model takes 
the form in (3). 
 
(2)  
t1
t2
t3
tn
...
Stems Signatures Affixes POSs
?1
?2
?m
...
f1
f2
f3
fk
...
?1
?2
?3
?l
...
 
(3) 
a. Grammar g =   [ ])|(log)(minarg gDataprobgLength
Gg
?
?
 
b. =)(gLength  
? ?
=? <?
??
???
? +
stemsofsetTt ti itfreqt
W
||0 ][
1log
)]([
][log ?
 
? ?
=? <?
+
affixesofsetFf fi iffreq||0 ][
1log  
?? ??? ?
?? ?
??
?
?
?
??
??
?
?
??
?
++?+
? ?
? ??
?
?
?
f
f
f
f
W
f
][
][log
][
][log
][
][log
 
c. =)|(log gDataprob  
 ?
+=? ?
??
?
?
??
?
?
?
+
+
+
? ??
?
??
, ),|(log
),|(log
)|(log)(log
ftw
Dataw fprob
tfprob
tprobprob
 
 
The differences between the models are 
found in the added final term in (3b), which 
specifies the information required to predict, or 
specify, the part of speech given the signature 
23
transform, and the corresponding term in the 
corpus compression expression (3c).  
The model in (3) implicitly assumes that the 
true POSs are known; in a more complete 
model, the POSs play a direct role in assigning a 
higher probability to the corpus (and hence a 
smaller compressed size to the data). In the 
context of such a model, an MDL-based learning 
device searches for the best assignment of POS 
tags over all possible assignments. Instead of 
doing that in this paper, we employ the 
TreeTagger (Schmid, 1994) based tags (see 
section 5 below), and make the working 
assumption that optimization of description 
length over all signature-analyses and POS tags 
can be approximated by optimization over all 
signature-analyses, given the POS tags provided 
by TreeTagger. 
4 The collapsing of signatures 
We describe in this section our proposed 
algorithm, using context vectors to collapse 
signatures together, composed of a sequence of 
operations, all but the first of which may be 
familiar to the reader:  
Replacement of words by signature-
transforms: The input to our algorithm for 
collapsing signatures is a modified version of 
the corpus which integrates the (unsupervised) 
morphological analyses in the following way. 
First of all, we leave unchanged the 200 most 
frequent words (word types). Next, we replace 
words belonging to the K most reliable 
signatures (where K=50 in these experiments) 
by their associated signature transforms, and we 
in effect ignore all other words, by replacing 
them by a distinguished ?dummy? symbol. In 
the following, we refer to our high frequency 
words and signature transforms together as 
elements?so an element is any member of the 
transformed corpus other than the ?dummy?.   
Context vectors based on mutual 
information: By reading through the corpus, we 
populate both left and right context vectors for 
each element (=signature-transform and high-
frequency word)  by observing the elements that 
occur adjacent to it. The feature indicating the 
appearance of a particular word on the left is 
always kept distinct from the feature indicating 
the appearance of the same word on the right. 
The features in a context vector are thus 
associated with the members of the element 
vocabulary (and indeed, each member of the 
element vocabulary occurs as two features: one 
on the left, one on the right). We assign the 
value of each feature y of x?s context vector as 
the pointwise mutual information of the 
corresponding element pair (x, y), defined as 
)()(
),(log
yprxpr
yxpr . 
Simplifying context vectors with ?idf?: In 
addition, because of the high dimensionality of 
the context vector and the fact that some features 
are more representative than others, we trim the 
original context vector. For each context vector, 
we sort features by their values, and then keep 
the top N (in general, we set N to 10) by setting 
these values to 1, and all others to 0. However, 
in this resulting simplified context vector, not all 
features do equally good jobs of distinguishing 
syntactical categories. As Wicentowski (2002) 
does in a similar context, we assign a weight  
if
w  to each feature fi in a fashion parallel to 
inverse document frequency (idf; see Sparck 
Jones 1973), or 
inappearsfeaturethiselements
elementsdistincttotal
#
#log . 
We view these as the diagonal elements of a 
matrix M (that is, mi,i = ifw ). We then check the 
similarity between two simplified context 
vectors by computing the weighted sum of the 
dot product of them. That is, given two 
simplified context vectors c and d, their 
similarity is defined as cTMd. If this value is 
larger than a threshold ? that is set as one 
parameter, we deem these two context vectors to 
be similar. Then we determine the similarity 
between elements by checking whether both left 
and right simplified context vectors of them are 
similar (i.e., their weighted dot products exceed 
a threshold ?). In the experiments we describe 
below, we explore four settings ? for this 
threshold: 0.8 (the most ?liberal? in allowing 
greater signature transform collapse, and hence 
greater signature collapse), 1.0, 1.2, and 1.5. 
Calculate signature similarity: To avoid 
considering many unnecessary pairs of 
signatures, we narrow the candidates into 
signature pairs in which the suffixes of one 
constitute a subset of suffixes of the other, and 
we set a limit to the permissible difference in the 
24
lengths of the signatures in the collapsed pairs, 
so that the difference in number of affixes 
cannot exceed 2. For each such pair, if all 
corresponding signature transforms are similar 
in the sense defined in the preceding paragraph, 
we deem the two signatures to be similar. 
Signature graph: Finally, we construct a 
signature graph, in which each signature is 
represented as a vertex, and an edge is drawn 
between two signatures iff they are similar, as 
just defined. In this graph, we find a number of 
cliques, each of which, we believe, indicates a 
cluster of signatures which should be collapsed. 
If a signature is a member of two or more 
cliques, then it is assigned to the largest clique 
(i.e., the one containing the largest number of 
signatures).4  
5 Experiments 
We obtain the morphological analysis of the 
Brown corpus (Ku?era and Francis, 1967) using 
the Linguistica software (http://linguistica. 
uchicago.edu), and we use the TreeTagger to 
assign a Penn TreeBank-style part-of-speech tag 
to each token in the corpus. We then carry out 
our experiment using the Brown corpus 
modified in the way we described above. Thus, 
for each token of the Brown corpus that our 
morphology analyzer analyzed, we have the 
following information: its stem, its signature 
                                                          
4 Our parameters are by design restrictive, so 
that we declare only few signatures to be similar, 
and therefore the cliques that we find in the 
graph are relatively small. One way to enlarge 
the size of collapsed signatures would be to 
loosen the similarity criterion. This, however, 
introduces too many new edges in the signatures 
graph, leading in turn to spurious collapses of 
signatures. We take a different approach, and 
apply our algorithms iteratively. The idea is that 
if in the first iteration, two cliques did not have 
enough edges between their elements to become 
a single new signature, they may be more 
strongly connected in the second iteration if 
many of their elements are sufficiently similar. 
On the other hand, cliques that were dissimilar 
in the first iteration remain weakly connected in 
the second.  
 
(i.e., the signature to which the stem is 
assigned), the suffix which the stem attains in 
this occurrence of the word (hence, the 
signature-transform), and the POS tag. For 
example, the token polymeric is analyzed into 
the stem polymer and the suffix ic, the stem is 
assigned to the signature ?.ic.s, and thus this 
particular token has the signature transform 
?.ic.s_ic. Furthermore, it was assigned POS-tag 
JJ, so that we have the following entry: 
?polymeric JJ ?.ic.s_ic?. 
Before performing signature collapsing, we 
calculate the description length of the 
morphology and the compressed length of the 
words that our algorithm analyzes and call it 
baseline description length (DL0). 
Now we apply our signature collapsing 
algorithm under several different parameter 
settings for the similarity threshold ?, and 
calculate the description length DL? of the 
resulting morphological and lexical analysis 
using  (3).  We know that the smaller the set of 
signatures, the smaller is the cost of the model. 
However, a signature collapse that combines 
signatures with different distributions over the 
lexical categories will result in a high cost of the 
data term (3c). The goal was therefore to find a 
method of collapsing signatures such that the 
reduction in the model cost will be higher than 
the increase in the compressed length of the data 
so that the total cost will decrease.  
As noted above, we perform this operation 
iteratively, and refer to the description length of 
the ith iteration, using a threshold ?, as ? iiterDL = . 
We used random collapsing in our 
experiments to ensure the expected relationship 
between appropriate collapses and description 
length. For each signature collapsing, we created 
a parallel situation in which the number of 
signatures collapsed is the same, but their choice 
is random.  We calculate the description length 
using this ?random? analysis as 
?
randomDL . We 
predict that this random collapsing will not 
produce an improvement in the total description 
length. 
25
6 Results and discussion 
Table 1 presents the description length, broken 
into its component terms (see (3)), for the 
baseline case and the alternative analyses 
resulting from our algorithm. The table shows 
the total description length of the model, as well 
as the individual terms: the signature term 
DL(?), the suffix term DL(F), the lexical 
categories term, DL(P), total morphology, 
DL(M), and the compressed length of the data, 
DL(D). We present results for two iterations for 
four threshold values (?=0.8,1.0,1.2,1.5) using 
our collapsing algorithm.  
Table 2 presents 
?
randomDL  derived from the 
random collapsing, in a fashion parallel to Table                
1. We show the results for only one iteration of 
random collapsing, since the first iteration 
already shows a substantial increase in 
description length. 
Figure 1 and Figure 2 present graphically the 
total description length from Tables 1 and 2 
respectively. The reader will see that all 
collapsing of signatures leads to a shortening of 
the description length of the morphology per se, 
and an increase in the compressed length of the 
data. This is an inevitable formal consequence of 
the MDL-style model used here. The empirical 
question that we care about is whether the 
combined description length increases or 
decreases, and what we find is that when 
collapsing the signatures in the way that we 
propose to do, the combined description length 
decreases, leading us to conclude that this is, 
overall, a superior linguistic description of the 
data. On the other hand, when signatures are 
collapsed randomly, the combined description 
length increases. This makes sense; randomly 
decreasing the formal simplicity of the 
grammatical description should not improve the 
overall analysis. Only an increase in the formal 
simplicity of a grammar that is grammatically 
sensible should have this property. Since our 
goal is to develop an algorithm that is 
completely data-driven and can operate in an  
Compa rison of DL 
362,500
363,000
363,500
364,000
364,500
365,000
365,500
366,000
DL0 DL1 DL2
?=0.8 ?=1 ?=1.2 ?=1.5  
Figure 1 Comparison of DL, 2 iterations and 4 
threshold values 
Compa rison of ra ndomly c olla psing DL
364,000
364,500
365,000
365,500
366,000
366,500
367,000
367,500
368,000
DL0 Drandom
D
L
?=0.8 ?=1 ?=1.2 ?=1.5
 
Figure 2 Comparison of DLs with random 
collapse of signatures (see text)
 DL0 8.0 1
=
=
?
iterDL  
8.0
2
=
=
?
iterDL
0.1
1
=
=
?
iterDL
0.1
2
=
=
?
iterDL
2.1
1
=
=
?
iterDL
2.1
2
=
=
?
iterDL  
5.1
1
=
=
?
iterDL  
5.1
2
=
=
?
iterDL
#? 50 41 35 41 34 44 42 46 45 
DL(?) 47,630 45,343 42,939 45,242 43,046 44,897 44,355 46,172 45,780 
DL(F) 160 156 156 153 143 158 147 163 164 
DL(P) 2,246 2,087 1,968 2,084 1,934 2,158 2,094 2,209 2,182 
DL(M) 50,218 47,768 45,244 47,659 45,304 47,395 46,777 48,724 48,306 
DL(D) 315,165 316,562 318,687 316,615 318,172 316,971 317,323 315,910 316,251 
Total 
DL 
365,383 364,330 363,931 364,275 363,476 364,367 364,101 364,635 364,558 
Table 1.   DL and its individual components for baseline and the resulting cases when collapsing 
signatures using our algorithm. 
26
 
 DL0 8.0=?
randomDL  
0.1=?
randomDL  
2.1=?
randomDL  
5.1=?
randomDL  
#? 50 41 41 44 46 
DL(?) 47,630 44,892 45,126 45,788 46,780 
DL(F) 160 201 198 187 177 
DL(P) 2,246 2,193 2,195 2,212 2,223 
DL(M) 50,218 47,468 47,700 48,369 49,362 
DL(D) 315,165 320,200 319,551 318,537 316,874 
Total DL 365,383 367,669 367,252 366,907 366,237 
Table 2. DL and its individual components for baseline and the 
resulting cases when collapsing signatures randomly.
 
 
 
unsupervised fashion, we take this evidence as 
supporting the appropriateness of our algorithm as 
a means of collapsing signatures in a 
grammatically and empirically reasonable way. 
We conclude that the collapsing of signatures 
on the basis of similarity of context vectors of 
signature transforms (in a space consisting of high 
frequency words and signature transforms) 
provides us with a useful and significant step 
towards solving the signature collapsing problem. 
In the context of the broader project, we will be 
able to use signature transforms as a more effective 
means for projecting lexical categories in an 
unsupervised way. 
As Table 1 shows, we achieve up to 30% 
decrease in the number of signatures through our 
proposed collapse. We are currently exploring 
ways to increase this value through powers of the 
adjacency matrix of the signature graph. 
In other work in progress, we explore the 
equally important signature purity problem in 
graph theoretic terms: we split ambiguous 
signature transforms into separate categories when 
we can determine that the edges connecting left-
context features and right-context features can be 
resolved into two sets (corresponding to the 
distinct categories of the transform) whose left-
features have no (or little) overlap and whose right 
features have no (or little) overlap. We employ the 
notion of minimum cut of a weighted graph to 
detect this situation.
 
References  
Brown, Peter F., Vincent J. Della Pietra, Peter V. 
deSouza, Jennifer C. Lai, and Robert L. Mercer. 
1992. Class-based n-gram models of natural 
language. Computational Linguistics, 18(4): 467-
479.  
Goldsmith, John. 2001. Unsupervised learning of the 
morphology of a natural language. Computational 
Linguistics, 27(2): 153-198.  
Higgins, Derrick. 2002. A Multi-modular Approach to 
Model Selection in Statistical NLP. University of 
Chicago Ph.D. thesis. 
Schmid, Helmut. 1994. Probabilistic part-of-speech 
tagging using decision trees.. International 
Conference on New Methods in Language 
Processing 
Kucera, Henry and W. Nelson Francis. 1967. 
Computational Analysis of Present-day American 
English. Brown University Press.  
Rissanen, Jorma. 1989. Stochastic Complexity in 
Statistical Inquiry. Singapore: World Scientific.  
Sch?tze, Hinrich. 1997. Ambiguity Resolution in 
Language Learning. CSLI Publications. Stanford 
CA.  
Sparck Jones, Karen. 1973. Index term weighting. 
Information Storage and Retrieval 9:619-33. 
Wicentowski, Richard. 2002. Modeling and Learning 
Multilingual Inflectional Morphology in a Minimally 
Supervised Framework. Johns Hopkins University 
Ph.D. thesis. 
27
Proceedings of the Second Workshop on Psychocomputational Models of Human Language Acquisition, pages 28?35,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
The SED heuristic for morpheme discovery:  
a  look at Swahili 
 
 
Yu Hu and Irina Matveeva  
Department of  
Computer Science 
The University of Chicago 
Chicago IL 60637 
yuhu@cs.uchicago.edu
matveeva 
@uchicago.edu  
John Goldsmith 
Departments of Linguistics and 
Computer Science  
The University of Chicago 
Chicago IL 60637 
ja-goldsmith 
@uchicago.edu 
 
Colin Sprague 
Department of Linguistics 
The University of Chicago 
Chicago IL 60637 
sprague 
@uchicago.edu  
  
Abstract 
This paper describes a heuristic for 
morpheme- and morphology-learning 
based on string edit distance. 
Experiments with a 7,000 word corpus 
of Swahili, a language with a rich 
morphology, support the effectiveness 
of this approach. 
1 Introduction 
This paper describes work on a technique for the 
unsupervised learning of the morphology of 
natural languages which employs the familiar 
string edit distance (SED) algorithm (Wagner 
and Fischer 1974 and elsewhere) in its first 
stage;  we refer to it here as the SED heuristic. 
The heuristic finds 3- and 4-state finite state 
automata (FSAs) from untagged corpora. We 
focus on Swahili, a Bantu language of East 
Africa, because of the very high average number 
of morphemes per word, especially in the verbal 
system, a system that presents a real challenge to 
other systems discussed in the literature.1 
In Section 2, we present the SED heuristic, 
with precision and recall figures for its 
application to a corpus of Swahili. In Section 3, 
we propose three elaborations and extensions of 
                                                     
1 An earlier version of this paper, with a more detailed 
discussion of the material presented in Section 3, is 
available at Goldsmith et al(2005). 
this approach, and in Section 4, we describe and 
evaluate the results from applying these 
extensions to the corpus of Swahili.2  
2 SED-based heuristic 
Most systems designed to learn natural language 
morphology automatically can be viewed as 
being composed of an initial heuristic 
component and a subsequent explicit model. The 
initial or bootstrapping heuristic, as the name 
suggests, is designed to rapidly come up with a 
set of candidate strings of morphemes, while the 
model consists of an explicit formulation of 
either (1) what constitutes an adequate 
morphology for a set of data, or (2) an objective 
function that must be optimized, given a corpus 
of data, in order to find the correct 
morphological analysis.  
The best known and most widely used 
heuristic is due to Zellig Harris (1955) (see also 
Harris (1967) and Hafer and Weiss (1974) for an 
evaluation based on an English corpus), using a 
notion that Harris called successor frequency 
(henceforth, SF). Harris' notion can be 
succinctly described in contemporary terms: if 
we encode all of the data in the data structure 
known as a trie, with each node in the trie 
dominating all strings which share a common 
                                                     
2 SED has been used in unsupervised language learning in a 
number of studies; see, for example, van Zaanen (2000) 
and references there, where syntactic structure is studied in 
a similar context. To our knowledge, it has not been used in 
the context of morpheme detection. 
28
string prefix,3 then each branching node in the 
trie is associated with a morpheme break. For 
example, a typical corpus of English may 
contain the words governed, governing, 
government, governor, and governs. If this data 
is encoded in the usual way in a trie, then a 
single node will exist in the trie which represents 
the string prefix govern and which dominates 
five leaves corresponding to these five words. 
Harris's SF-based heuristic algorithm would 
propose a morpheme boundary after govern on 
this basis. In contemporary terms, we can 
interpret Harris?s heuristic as providing sets of 
simple finite state automata, as in (1), which 
generate a string prefix (PF1) followed by a set 
of string suffixes (SFi) based on the 
measurement of a successor frequency greater 
than 1 (or some threshold value) at the string 
position following PF1. 
(1)  
SF1
SF3
PF1 SF2
 
A variant on the SF-based heuristic, 
predecessor frequency (henceforth, PF), calls for 
encoding words in a trie from right to left. In 
such a PF-trie, each node dominates all strings 
that share a common string suffix. In general, we 
expect SF to work best in a suffixing language, 
and PF to work best in prefixing language; 
Swahili, like all the Bantu languages, is 
primarily a prefixing language, but it has a 
significant number of important suffixes in both 
the verbal and the nominal systems. 
Goldsmith (2001) argues for using the 
discovery of signatures as the bootstrapping 
heuristic, where a signature is a maximal set of 
stems and suffixes with the property that all 
combinations of stems and suffixes are found in 
the corpus in question. We interpret Goldsmith?s 
signatures as extensions of FSAs as in (1) to 
                                                     
3 We use the terms string prefix and string suffix in the 
computer science sense: a string S is a string prefix of a 
string X iff there exists a string T such that X = S.T, where 
?.? is the string concatenation operator; under such 
conditions, T is likewise a string suffix of X. Otherwise, we 
use the terms prefix and suffix in the linguistic sense, and a 
string prefix (e.g., jump) may be a linguistic stem, as in 
jump-ing. 
FSAs as in (2); (2) characterizes Goldsmith?s 
notion of signature in term of FSAs. In 
particular, a signature is a set of forms that can 
be characterized by an FSA of 3 states. 
(2)  
PF1 SF1
PF3 SF2
PF2
 
 
We propose a simple alternative heuristic 
which utilizes the familiar dynamic 
programming algorithm for calculating string-
edit distance, and finding the best alignment 
between two arbitrary strings (Wagner and 
Fischer 1974). The algorithm finds subsets of 
the data that can be exactly-generated by 
sequential finite state automata of 3 and 4 states, 
as in (3), where the labels mi should be 
understood as cover terms for morphemes in 
general. An automaton exactly-generates a set of 
strings S if it generates all strings in S and no 
other strings; a sequential FSA is one of the 
form sketched graphically in (1)-(3), where there 
is a unique successor to each state. 
(3)  
M1 M4
M3 M6
M2
M7
M9
M5 M8
 
2.1 First stage: alignments. 
If presented with the pair of strings anapenda 
and anamupenda from an unknown language, it 
is not difficult for a human being to come up 
with the hypothesis that mu is a morpheme 
inside a larger word that is composed of at least 
two morphemes, perhaps ana- and -penda. The 
SED heuristic makes this observation explicit by 
building small FSAs of the form in (4), where at 
most one of m1 or m4 may be null, and at most 
one of m2 and m3 may be null: we refer to these 
as elementary alignments. The strings m2 and m3 
are called counterparts; the pairs of strings m1 
and m4 are called the context (of the 
counterparts). (Indeed, we consider this kind of 
string comparison to be a plausible candidate for 
human language learning; see Dahan and Brent 
1999). 
 
29
 
 
(4)  
1 432m1 m4
m3
m2
 
The first stage of the algorithm consists of 
looking at all pairs of words S, T in the corpus, 
and passing through the following steps:  
We apply several initial heuristics to 
eliminate a large proportion of the pairs of 
strings before applying the familiar SED 
algorithm to them, in view of the relative 
slowness of the SED algorithm; see Goldsmith 
et al(2005) for further details.  
We compute the optimal alignment of S and 
T using the SED algorithm, where alignment 
between two identical letters (which we call 
twins) is assigned a cost of 0, alignment between 
two different letters (which we call siblings) is 
assigned a cost of 1.5, and a letter in one string 
not aligned with a segment on the other string 
(which we call an orphan) is assigned a cost of 
1. An alignment as in (5) is thus assigned a cost 
of 5, based on a cost of 1.5 assigned to each 
broken line, and 1  to each dotted line that ends 
in a square box. 
(5)   
n i l i m u p e n d a
n i t a k a m u p e n d a  
There is a natural map from every alignment 
to a unique sequence of pairs, where every pair 
is either of the form (S[i], T[j]) (representing 
either a twin or sibling case) or of the form (S[i], 
0) or (0, T[j]) (representing the orphan case). We 
then divide the alignment up into perfect and 
imperfect spans: perfect spans are composed of 
maximal sequences of twin pairs, while 
imperfect spans are composed of maximal 
sequences of sibling or orphan pairs. This is 
illustrated in (6). 
(6)  
 
 
 
 
 
 
There is a natural equivalence between 
alignments and sequential FSAs as in (4), where 
perfect spans correspond to pairs of adjacent 
states with unique transitions and imperfect 
spans correspond to pairs of adjacent states with 
two transitions, and we will henceforth use the 
FSA notation to describe our algorithm. 
2.2 Collapsing alignments 
As we noted above (4), for any elementary 
alignment, a context is defined: the pair of 
strings (one of them possibly null) which 
surround the pair of counterparts. Our first goal 
is to collapse alignments that share their context. 
We do this in the following way. 
Let us define the set of strings associated 
with the paths leaving a state S as the production 
of state S. A four-state sequential FSA, as in (4), 
has three states with non-null productions; if this 
particular FSA corresponds to an elementary 
alignment, then two of the state-productions 
contain exactly one string?and these state-
productions define the context? and one of the 
state-productions contains exactly two strings 
(one possibly the null string)?this defines the 
counterparts. If we have two such four-state 
FSAs whose context are identical, then we 
collapse the two FSAs into a single conflated 
FSA in which the context states and their 
productions are identical, and the states that 
produced the counterparts are collapsed by 
creating a state that produces the union of the 
productions of the original states. This is 
illustrated in (7): the two FSAs in (7a) share a 
context, generated by their states 1 and 3, and 
they are collapsed to form the FSA in (7b), in 
which the context states remain unchanged, and 
the counterpart states, labeled ?2?, are collapsed 
to form a new state ?2? whose production is the 
union of the productions of the original states. 
(7)  
a.  
1 432m1 m4
1 432m1 m4
m7
m8
m3
m2
 
 
n i   l i   m u p e n d a
n i   t a k a   m u p e n d a
30
 
 
b. 
1 432m1 m4
m8
m7
m3
m2
 
2.3 Collapsing the resulting sequential 
FSAs 
We now generalize the procedure described in 
the preceding section to collapse any two 
sequential FSAs for which all but one of the 
corresponding states have exactly the same 
production. For example, the two sequential 
FSAs in (8a) are collapsed into (8b). 
Three and four-state sequential FSAs as in 
(8b), where at least two of the state-transitions 
generate more than one morpheme, form the set 
of templates derived from our bootstrapping 
heuristic. Each such template can be usefully 
assigned a quantitative score based on the 
number of letters ?saved? by the use of the 
template to generate the words, in the following 
sense. The template in (8b) summarizes four 
words: aliyesema, alimfuata, anayesema, and 
anamfuata. The total string length of these 
words is 36, while the total number of letters in 
the strings associated with the transitions in the 
FSA is 1+4+12 = 17; we say that the FSA saves 
36-17 = 19 letters. In actual practice, the 
significant templates discovered save on the 
order of 200 to 5,000 letters, and ranking them 
by the number of letters saved is a good measure 
of how significant they are in the overall 
morphology of the language. We refer to this 
score as a template?s robustness; we employ this 
quantity again in section 3.1 below. 
By this ranking, the top template found in our 
Swahili corpus of 50,000 running words was one 
that generated a and wa (class 1 and 2 subject 
markers) and followed by 246 correct verb 
continuations (all of them polymorphemic); the 
first 6 templates are summarized informally in 
Table 1. We note that the third and fourth 
template can also be collapsed to form a 
template of the form in (3), a point we return to 
below. Precision, recall, and F-score for these 
experiments are given in Table 2.  
 
(8)   
a. 
1 432a yesema
na
li
1 432a mfuata
na
li
 
 
b.  
1 432a
na
li yesema
mfuata  
 
State 1 State 2 State 3 
a, wa (sg., pl. 
human subject 
markers) 
246 stems  
ku, hu 
(infinitive, 
habitual 
markers) 
51 stems  
wa (pl. subject 
marker) 
ka, li (tense 
markers) 
25 stems 
a (sg. subject 
marker) 
ka, li (tense 
markers) 
29 stems 
a (sg. subject 
marker) 
ka, na (tense 
markers 
28 stems 
37 strings w (passive 
marker) 
a 
Table 1 Top templates in Swahili 
 
 Precision Recall  F-score 
SED 0.77 0.57 0.65 
SF 0.54 0.14 0.22 
PF 0.68 0.20 0.31 
Table 2 Results 
31
3 Further developments 
In this section, we describe three developments 
of the SED-based heuristic sketched in section 2. 
The first disambiguates which state it is that 
string material should be associated with in 
cases of ambiguity; the second collapses 
templates associated with similar morphological 
structure; the third uses the FSAs to predict 
words that do not actually occur in the corpus by 
hypothesizing stems on the basis of the 
established FSAs and as yet unanalyzed words 
in the corpus. 
3.1 Disambiguating FSAs 
In the case of a sequential FSA, when the final 
letter of the production of a (non-final) state S 
are identical, then that letter can be moved from 
being the string-suffix of all of the productions 
of state S to being the string-prefixes of all of 
the productions of the following state. More 
generally, when the n final letters of the 
productions of a state are identical, there is an n-
way ambiguity in the analysis, and the same 
holds symmetrically for the ambiguity that arises 
when the n initial letters of the production of a 
(non-initial) state.  
Thus two successive states, S and T, must (so 
to speak) fight over which will be responsible 
for generating the ambiguous string. We employ 
two steps to disambiguate these cases.  
Step 1: The first step is applicable when the 
number of distinct strings associated with states 
S and T are quite different in size (typically 
corresponding to the case where one generates 
grammatical morphemes and the other generates 
stems); in this case, we assign the ambiguous 
material to the state that generates the smaller 
number of strings. There is a natural motivation 
for this choice from the perspective of our desire 
to minimize the size of the grammar, if we 
consider the size of the grammar to be based, in 
part, on the sum of the lengths of the morphemes 
produced by each state. 
Step 2: It often happens that an ambiguity 
arises with regard to a string of one or more 
letters that could potentially be produced by 
either of a pair of successive states involving 
grammatical morphemes. To deal with this case, 
we make a decision that is also (like the 
preceding step) motivated by a desire to 
minimize the description length of the grammar. 
In this case, however, we think of the FSA as 
containing explicit strings (as we have assumed 
so far), but rather pointers to strings, and the 
?length? of a pointer to a string is inversely 
proportional to the logarithm of its frequency. 
Thus the overall use of a string in the grammar 
plays a crucial role in determining the length of 
a grammar, and we wish to maximize the 
appearance in our grammar of morphemes that 
are used frequently, and minimize the use of 
morphemes that are used rarely. 
We implement this idea by collecting a table 
of all of the morphemes produced by our FSA, 
and assigning each a score which consists of the 
sum of the robustness scores of each template 
they occur in (see discussion just above (8)). 
Thus morphemes occurring in several high 
robustness templates will have high scores; 
morphemes appearing in a small number of 
lowly ranked templates will have low scores. 
To disambiguate strings which could be 
produced by either of two successive states, we 
consider all possible parsings of the string 
between the states, and score each parsing as the 
sum of the scores of the component morphemes; 
we chose the parsing for which the total score is 
a maximum. 
 For example, Swahili has two common tense 
markers, ka and ki, and this step corrected a 
template from {ak}+{a,i}+{stems} to 
{a}+{ka,ki}+{stems}, and others of similar 
form. It also did some useful splitting of joined 
morphemes, as when it modified a template 
{wali} + {NULL, po} + {stems} to {wa} + {li, 
lipo} + {stems}. In this case, wali should indeed 
be split into wa + li (subject and tense markers, 
resp.), and while the change creates an error (in 
the sense that lipo is, in fact, two morphemes; po 
is a subordinate clause marker), the resulting 
error occurs considerably less often in the data, 
and the correct template will better be able to be 
integrated with out templates. 
3.2 Template collapsing 
From a linguistic point of view, the SED-based 
heuristic creates too many FSAs because it stays 
too close to the data provided by the corpus. The 
only way to get a more correct grammar is by 
collapsing the FSAs, which will have as a 
32
consequence the generation of new words not 
found in the corpus. We apply the following 
relatively conservative strategy for collapsing 
two templates. 
We compare templates of the same number 
of states, and distinguish between states that 
produce grammatical morphemes (five or fewer 
in number) and those that produce stems (that is, 
lexical morphemes, identified as being six or 
more in number). We collapse two templates if 
the productions of the corresponding states 
satisfy the following conditions: if the states 
generate stems, then the intersection of the 
productions must be at least two stems, while if 
the states are grammatical morphemes, then the 
productions of one pair of corresponding states 
must be identical, while for the other pair, the 
symmetric difference of the productions must be 
no greater than two in number (that is, the 
number of morphemes produced by the state of 
one template but not the other must not exceed 
2).  
3.3 Reparsing words in the corpus and 
predicting new words 
When we create robust FSAs?that is, FSAs that 
generate a large number of words?we are in a 
position to go back to the corpus and reanalyze a 
large number of words that could not be 
previously analyzed. That is, a 4-state FSA in 
which each state produced two strings generates 
8 words, and all 8 words must appear in the 
corpus for the method described so far in order 
for this particular FSA to generate any of them. 
But that condition is unlikely to be satisfied for 
any but the most common of morphemes, so we 
need to go back to the corpus and infer the 
existence of new stems (as defined operationally 
in the preceding paragraph) based on their 
occurrence in several, but not all possible, 
words.  If there exist 3 distinct words in the 
corpus which would all be generated by a 
template if a given stem were added to the 
template, we add that stem to the template. 
4 Experiments and Results 
In this section, we present three sets of 
evaluations of the refinements of the SED 
heuristics described in the preceding section. We 
used a corpus of 7,180 distinct words occurring 
in 50,000 running words from a Swahili 
translation of the Bible obtained on the internet. 
4.1 Disambiguating FSAs 
In order to evaluate the effects of the 
disambiguating of FSAs described in section 
3.1, we compare precision and recall of the 
identification of morpheme boundaries using the 
SED method with and without the 
disambiguation procedure described above. In 
Figures 1 and 2, we graph precision and recall 
for the top 10% of the templates, displayed as 
the leftmost point, for the top 20% of the 
templates, displayed as the second point from 
the left; and so on, because the higher ranked 
FSAs are more intrinsically more reliable than 
the lower ranked ones. We see that 
disambiguation repairs almost 50% of the 
previous errors, and increases recalls by about 
10%. With these increases in precision and 
recall, it is clear that the disambiguating step 
provides a considerably more accurate 
morpheme boundary discovery procedure. 
Precision
0.7
0.75
0.8
0.85
0.9
0.95
1
10 20 30 40 50 60 70 80 90 10
0
Deciles(%)
Pr
ec
is
io
n
Without
With
 
Figure 1 Comparison of precision 
 
Compare Recalls 
0.3
0.34
0.38
0.42
0.46
0.5
10 20 30 40 50 60 70 80 90 100
Deciles(%)
R
ec
al
ls
Without
With
Figure 2 Comparison of recall 
33
4.2 Template collapsing 
The second refinement discussed above 
consists of finding pairs of similar templates, 
collapsing them as appropriate, and thus creating 
patterns that generate new words that did not 
participate in the formation of the original 
templates. These new words may or may not 
themselves appear in the corpus. We are, 
however, able to judge their morphological well-
formedness by inspection. We list in Table 3 the 
entire list of eight templates that are collapsed in 
this step. 
All of the templates which are collapsed in 
this step are in fact of the same morphological 
structure (with one very minor exception4): they 
are of the form subject marker + tense marker + 
stem, and the collapsing induced in this 
procedure correctly creates larger templates of 
precisely the same structure, generating new 
words not seen in the corpus that are in fact 
correct from our (non-native speaker) 
inspection. We submitted the new words to 
Yahoo to test the words ?existence? by their 
existence on the internet, and actually found an 
average of 87% of the predicted words in a 
template; see the last column in Table 3 for 
details. 
4.3 Reparsing 
After previous refinements, we obtain a 
number of robust FSAs, for example, those 
collapsed templates in Table 3. With them, we 
then search the corpus for those words that can 
only be partly fitted into these FSAs and 
generate associated stems. Table 4 shows the 
reparsed words that had not been parsed by 
earlier templates and also newly added stems for 
some robust FSAs (the four collapsed templates 
in Table 3).  Stems such as anza ?begin? and 
fanya ?do? are thus added to the first template, 
and all words derived by prepending a tense 
marker and a subject marker are indeed accurate 
words. As the words in Table 4 suggest, the 
reparsing process adds new, common stems to 
the stem-column of the templates, thus making it 
                                                     
4 The exception involves the distinct morpheme po, a 
subordinate clause marker which must ultimately be 
analyzed as appearing in a distinct template column to the 
right of the tense markers. 
easier for the collapsing function to find 
similarities across related templates. 
In future work, we will take use the larger 
templates, populated with more stems, and input 
them to the collapsing function described in 3.2.  
5 Conclusions 
On the basis of the experiments with Swahili 
described in this paper, the SED heuristic 
appears to be a useful tool for the discovery of 
morphemes in languages with rich 
morphologies, and for the discovery of the FSAs 
that constitute the morphologies of those 
languages. 
Ultimately, the value of the heuristic is best 
tested against a range of languages with complex 
concatenative morphologies. While a thorough 
discussion would take us well beyond the limits 
of this paper, we have applied the SED heuristic 
to English, Hungarian, and Finnish as well as 
Swahili. For English, unsurprisingly, the method 
works as well as the SF and PF methods, though 
a bit more slowly, while for Hungarian and 
Finnish, the results appear promising, and a 
comparison with Creutz and Lagus (2004) for 
Finnish, for example, would be appealing. 
34
 
  
One Template 
 
The other template Collapsed Template 
% found on 
Yahoo search 
1 {a}-{ka,na}-{stems} {a}-{ka,ki}-{stems} {a}-{ka,ki,na}-{stems} 86 (37/43) 
2 {wa}-{ka,na}-{stems} {wa}-{ka,ki}-{stems} {wa}-{ka,ki,na}-{stems} 95 (21/22) 
3 {a}-{ka,ki,na}-{stems} {wa}-{ka,ki,na}-{stems} {a,wa}-{ka,ki,na}-{stems} 84 (154/183) 
4 {a}-{liye,me}-{stems} {a}-{liye,li}-{stems} {a}-{liye,li,me}-{stems} 100 (21/21) 
5 {a}-{ki,li}-{stems} {wa}-{ki,li}-{stems} {a,wa}-{ki,li}-{stems} 90 (36/40) 
6 {a}-{lipo,li}-{stems} {wa}-{lipo,li}-{stems} {a,wa}-{lipo,li}-{stems} 90 (27/30) 
7 {a,wa}-{ki,li}-{stems} {a,wa}-{lipo,li}-{stems} {a,wa}-{ki,lipo,li}-{stems} 74 (52/70) 
8 {a}-{na,naye}-{stems} {a}-{na,ta}-{stems} {a}-{na,ta,naye}-{stems} 80 (12/15) 
Table 3  Collapsed Templates and Created Words Sample. 
 
 
 
 
 Template Reparsed Words Not Parsed 
Before 
Added Stems  
1 {a, wa}-{ka,ki,na}-{stems} akawakweza, akiwa, anacho, 
akibatiza,  ? 
toka, anza, waita, fanya, enda, ? 
2 {a}-{li,liye,me }-{stems} ameinuka, ameugua, alivyo,  
aliyoniagiza,  ? 
zaliwa, kuwa, fanya, sema 
3 {a, wa}-{ki,li,lipo}-{stems} alimtoboa,  alimtaka,  
waliamini,  ? 
pata, kuwa, kaa, fanya, chukua, 
fika, ? 
4 {a} ? {na,naye,ta}-{stems} analazwa,  atanitukuza,  anaye,  
anakuita,   ? 
ingia, sema 
Table 4 Reparsed words and "discovered" stems 
 
References 
Creutz, Mathias, and Krista Lagus. (2004). Induction 
of a simple morphology for highly inflecting 
languages. Proceedings of the Workshop of 
SIGPHON (Barcelona). 
Dahan, Delphine, and Michael Brent. (1999). On the 
discovery of novel world-like units from 
utterances. Journal of Experimental Psychology: 
General  128: 165-185. 
Goldsmith, John. (2001).  Unsupervised Learning of 
the Morphology of a Natural Language. 
Computational Linguistics 27(2): 153-198. 
Goldsmith, John, Yu Hu, Irina Matveeva, and Colin 
Sprague. 2005. A heuristic for morpheme 
discovery based on string edit distance. Technical 
Report TR-2005-4. Department of Computer 
Science. University of Chicago. 
Hafer, M. A., Weiss, S. F.  (1974). Word 
segmentation by letter successor varieties. 
Information Storage and Retrieval 10: 371-385. 
Harris, Zellig. (1955). From Phoneme to Morpheme. 
Language 31: 190-222. 
Harris, Zellig. (1967). Morpheme Boundaries within 
Words: Report on a Computer Test. 
Transformations and Discourse Analysis Papers 
73.  
Oliver, Antoni, Irene Bastell?n, and Llu?s M?rquez. 
(2003). Uso de Internet para aumentar la cobertura 
de un sistema de adquisici?n l?xica del ruso. 
SEPLN 2003. 
Wagner, R. A., Fischer, M. J.  (1974). The string-to-
string correction problem. Journal of the 
Association for Computing Machinery 21(1): 168-
73. 
van Zaanen,  Menno. 2000. ABL: Alignment-Based 
Learning. Proceedings of the 17th Conference on 
Computational Linguistics, vol. 2. p. 961-67.
 
35
Proceedings of the Eighth Meeting of the ACL Special Interest Group on Computational Phonology at HLT-NAACL 2006, pages 32?40,
New York City, USA, June 2006. c?2006 Association for Computational Linguistics
Exploring variant definitions of pointer length in MDL
Aris Xanthos
Department of Linguistics
University of Chicago
Chicago IL 60637
axanthos@uchicago.edu
Yu Hu
Department of
Computer Science
University of Chicago
Chicago IL 60637
yuhu@uchicago.edu
John Goldsmith
Departments of Linguistics and
Computer Science
University of Chicago
Chicago IL 60637
goldsmith@uchicago.edu
Abstract
Within the information-theoretical frame-
work described by (Rissanen, 1989; de
Marcken, 1996; Goldsmith, 2001), point-
ers are used to avoid repetition of phono-
logical material. Work with which we
are familiar has assumed that there is only
one way in which items could be pointed
to. The purpose of this paper is to de-
scribe and compare several different meth-
ods, each of which satisfies MDL?s ba-
sic requirements, but which have different
consequences for the treatment of linguis-
tic phenomena. In particular, we assess
the conditions under which these different
ways of pointing yield more compact de-
scriptions of the data, both from a theoret-
ical and an empirical perspective.
1 Introduction
The fundamental hypothesis underlying the Mini-
mum Description Length (MDL) framework (Rissa-
nen, 1989; de Marcken, 1996; Goldsmith, 2001) is
that the selection of a model for explaining a set of
data should aim at satisfying two constraints: on the
one hand, it is desirable to select a model that can be
described in a highly compact fashion; on the other
hand, the selected model should make it possible to
model the data well, which is interpreted as being
able to describe the data in a maximally compact
fashion. In order to turn this principle into an op-
erational procedure, it is necessary to make explicit
the notion of compactness. This is not a trivial prob-
lem, as the compactness (or conversely, the length)
of a description depends not only on the complexity
of the object being described (in this case, either a
model or a set of data given a model), but also on
the ?language? that is used for the description.
Consider, for instance, the model of morphology
described in Goldsmith (2001). In this work, the
data consist in a (symbolically transcribed) corpus
segmented into words, and the ?language? used to
describe the data contains essentially three objects:
a list of stems, a list of suffixes, and a list of sig-
natures, i.e. structures specifying which stems asso-
ciate with which suffixes to form the words found in
the corpus. The length of a particular model (or mor-
phology) is defined as the sum of the lengths of the
three lists that compose it; the length of each list is in
turn defined as the sum of the lengths of elements in
it, plus a small cost for the list structure itself1. The
length of an individual morpheme (stem or suffix) is
taken to be proportional to the number of symbols in
it.
Calculating the length of a signature involves the
notion of pointer, with which this paper is primar-
ily concerned. The function of a signature is to re-
late a number of stems with a number of suffixes.
Since each of these morphemes is spelled once in
the corresponding list, there is no need to spell it
again in a signature that contains it. Rather, each
signature comprises a list of pointers to stems and
a list of pointers to suffixes. A pointer is a sym-
bol that stands for a particular morpheme, and the
recourse to pointers relies on the assumption that
1More on this in section 2.1 below
32
their length is lesser than that of the morphemes
they replace. Following information-theoretic prin-
ciples (Shannon, 1948), the length of a pointer to a
morpheme (under some optimal encoding scheme)
is equal to -1 times the binary logarithm of that mor-
pheme?s probability. The length of a signature is the
sum of the lengths of the two lists it contains, and
the length of each list is the sum of the lengths of
the pointers it contains (plus a small cost for the list
itself).
This work and related approaches to unsupervised
language learning have assumed that there is only
one way in which items could be pointed to, or iden-
tified. The purpose of this paper is to describe, com-
pare and evaluate several different methods, each of
which satisfies MDL?s basic requirements, but which
have different consequences for the treatment of lin-
guistic phenomena. One the one hand, we contrast
the expected description length of ?standard? lists of
pointers with polarized lists of pointers, which are
specified as either (i) pointing to the relevant mor-
phemes (those that belong to a signature, or undergo
a morpho-phonological rule, for instance) or (ii)
pointing to their complement (those that do not be-
long to a signature, or do not undergo a rule). On the
other hand, we compare (polarized) lists of pointers
with a method based on binary strings specifying
each morpheme as relevant or not (for a given sig-
nature, rule, etc.). In particular, we discuss the con-
ditions under which these different ways of pointing
are expected to yield more compact descriptions of
the data.
The remainder of this paper is organized as fol-
lows. In the next section, we give a formal review
of the standard treatment of lists of pointers as de-
scribed in (Goldsmith, 2001); then we successively
introduce polarized lists of pointers and the method
of binary strings, and make a first, theoretical com-
parison of them. Section three is devoted to an em-
pirical comparison of these methods on a large nat-
ural language corpus. In conclusion, we discuss the
implications of our results in the broader context of
unsupervised language learning.
2 Variant definitions of pointers
In order to simplify the following theoretical discus-
sion, we temporarily abstract away from the com-
plexity of a full-blown model of morphology. Given
a set of N stems and their distribution, we consider
the general problem of pointing to a subset of M
stems (with 0 < M ? N ), first by means of ?stan-
dard? lists of pointers, then by means of polarized
ones, and finally by means of binary strings.
2.1 Expected length of lists of pointers
Let ? denote a set of N stems; we assume that the
length of a pointer to a specific stem t ? ? is its
inverse log probability ? log pr(t).2 Now, let {M}
denote the set of all subsets of ? that contain exactly
0 < M ? N stems. The description length of a
list of pointers to a particular subset ? ? {M} is
defined as the sum of the lengths of the M pointers
it contains, plus a small cost of for specifying the list
structure itself, defined as ?(M) := 0 if M = 0 and
logM bits otherwise3:
DLptr(?) := ?(M)?
?
t??
log pr(t)
The expected length of a pointer is equal to the
entropy over the distribution of stems:
hstems := Et?? [? log pr(t)] = ?
?
t??
pr(t) log pr(t)
Thus, the expected description length of a list of
pointers to M stems (over all subsets ? ? {M})
is:
E??{M} [DLptr(?)] = 1|{M}|
?
??{M}
DLptr(?)
= ?(M) +Mhstems
(1)
This value increases as a function of both the num-
ber of stems which are pointed to and the entropy
over the distribution of stems. Since 0 ? hstems ?
logN , the following bounds hold:
0 ? hstems ? E??{M} [DLptr(?)]
? logN +Nhstems ? (N + 1) logN
2Here and throughout the paper, we use the notation log x
to refer to the binary logarithm of x; thus entropy and other
information-theoretic quantities are expressed in terms of bits.
3Cases where the argument of this function can have the
value 0 will arise in the next section.
33
2.2 Polarization
Consider a set of N = 3 equiprobable stems, and
suppose that we need to specify that a given morpho-
phonological rule applies to one of them. In this con-
text, a list with a single pointer to a stem requires
log 1 ? log 13 = 1.58 bits. Suppose now that the
rule is more general and applies to two of the three
stems. The length of the new list of pointers is thus
log 2 ? 2 log 13 = 4.17 bits. It appears that for such
a general rule, it is more compact to list the stems to
which it does not apply, and mark the list with a flag
that indicates the ?negative? meaning of the point-
ers. Since the flag signals a binary choice (either the
list points to stems that undergo the rule, or to those
that do not), log 2 = 1 bit suffices to encode it, so
that the length of the new list is 1.58 + 1 = 2.58
bits.
We propose to use the term polarized to refer to
lists of pointers bearing a such flag. If it is useful to
distinguish between specific settings of the flag, we
may speak of positive versus negative lists of point-
ers (the latter being the case of our last example).
The expected description length of a polarized list
of M pointers is:
E??{M} [DLpol(?)] = 1 + ?(M?) + M?hstems
with M? := min(M,N ?M)
(2)
From (1) and (2), we find that in general, the ex-
pected gain in description length by polarizing a list
of M pointers is:
E??{M} [DLptr(?)?DLpol(?)]
=
?
??
??
?1 iff M ? N2
?1 + ?(M)? ?(N ?M) + (2M ?N)hstems
otherwise
Thus, if the number of stems pointed to is lesser than
or equal to half the total number of stems, using a
polarized list rather than a non-polarized one means
wasting exactly 1 bit for encoding the superfluous
flag. If the number of stems pointed to is larger than
that, we still pay 1 bit for the flag, but the reduced
number of pointers results in an expected saving of
?(M) ? ?(N ? M) bits for the list structure, plus
(2M ?N) ? hstems bits for the pointers themselves.
Now, let us assume that we have no informa-
tion regarding the number M of elements which are
0 500 1000 1500 2000
0
100
0
300
0
500
0
Polarized vs. non?polarized lists
Total number of stems N
De
scr
ipti
on 
len
gth
 ga
in (i
n bi
ts)
s=0
s=1
s=2
s=10
Figure 1: Expected gain in description length by us-
ing polarized rather than non-polarized lists of point-
ers.
pointed to, i.e. that it has a uniform distribution be-
tween 1 and N (M ? U [1, N ]). Let us further as-
sume that stems follow a Zipfian distribution of pa-
rameter s, so that the probability of the k-th most
frequent stem is defined as:
f(k,N, s) := 1/k
s
HN,s with HN,s :=
N?
n=1
1/ns
where HN,s stands for the harmonic number of order
N of s. The entropy over this distribution is:
hZipfN,s :=
s
HN,s
N?
k=1
log k
ks + logHN,s
Armed with these assumptions, we may now com-
pute the expected description length gain of polar-
ization (over all values of M ) as a function of N
and s:
EM
(E??{M} [DLptr(?)?DLpol(?)]
)
=?1+ 1N
?N
M=1 ?(M)? ?(M?) + (M ? M?)hZipfN,s
Figure 1 shows the gain calculated for N = 1,
400, 800, 1200, 1600 and 2000, and s = 0, 1, 2
and 10. In general, it increases with N , with a
slope that depends on s: the greater the value of s,
the lesser the entropy over the distribution of stems;
since the entropy corresponds to the expected length
34
Figure 2: Two ways of pointings to stems: by means
of a polarized list of pointers, or a binary string.
of a pointer, its decrease entails a decrease in the
number of bits that can be saved by using polarized
lists (which generally use less pointers). However,
even for an aberrantly skewed distribution of stems4,
the expected gain of polarization remains positive.
Since the value of s is usually taken to be slightly
greater than 1 for natural languages (Mandelbrot,
1953), it seems that polarized lists generally entail
a considerable gain in description length.
2.3 Binary strings
Consider again the problem of pointing to one out
of three equiprobable stems. Suppose that the list of
stems is ordered, and that we want to point to the
first one, for instance. An alternative to the recourse
to a list of pointers consists in using a binary string
(in this case 100) where the i-th symbol is set to 1
(or +) if the i-th stem is being pointed to, and to 0
(or -) otherwise. Figure 2 gives a schematic view of
these two ways of pointing to items.
There are two main differences between this
method and the previous one. On the one hand,
the number of symbols in the string is constant and
equal to the total number N of stems, regardless of
the number M of stems that are pointed to. On the
other hand, the compressed length of the string de-
pends on the distribution of symbols in it, and not on
the distribution of stems. Thus, by comparison with
the description length of a list of pointers, there is a
loss due to the larger number of encoded symbols,
and a gain due to the use of an encoding specifically
4In the case s = 10, the probability of the most frequent
stem is .999 for N = 2000.
tailored for the relevant distribution of pointed ver-
sus ?unpointed? elements.
The entropy associated with a binary string is en-
tirely determined by the number of 1?s it contains,
i.e. the number M of stems which are pointed to,
and the length N of the string:
hbinN,M := ?
M
N log
M
N ?
N ?M
N log
N ?M
N
The compressed length of a binary string pointing to
M stems is thus:
DLbin(M) := NhbinN,M (3)
It is maximal and equal to N bits when M = N2 ,
and minimal and equal to 0 when M = N , i.e. when
all stems have a pointer on them. Notice that binary
strings are intrinsically polarized, so that intervert-
ing 0?s and 1?s results in the same description length
regardless of their distribution.5
The question naturally arises, under which con-
ditions would binary strings be more or less com-
pact than polarized lists of pointers. If we assume
again that the distribution of the number of elements
pointed to is uniform and the distribution of stems is
Zipfian of parameter s, (2) and (3) justify the follow-
ing expression for the expected description length
gain by using binary strings rather than polarized
lists (as a function of N and s):
EM
[E??{M}[DLpol(?)]?DLbin(M)
]
= 1 + 1N
?N
M=1 ?(M?) + M?hZipfN,s ?NhbinN,M
Figure 3 shows the gain calculated for N = 1, 400,
800, 1200, 1600 and 2000, and s = 0, 1, 2 and 3.
For s small, i.e. when the entropy over the distri-
bution of stems is greater or not much lesser than
that of natural languages, the description length of
binary strings is considerably lesser than that of po-
larized lists. The difference decreases as s increases,
5As one the reviewers has indicated to us, the binary strings
approach is actually very similar to the method of combinato-
rial codes described by (Rissanen, 1989). This method con-
sists in pointing to one among  NM
 possible combinations of
M stems out of N . Under the assumption that these combi-
nations have a uniform probability, the cost for pointing to M
stems is log  NM
 bits, which is in general slightly lesser than
the description length of the corresponding binary string (the
difference being maximal for M = N/2, i.e. when the binary
string encoding cannot take advantage of any compression).
35
0 500 1000 1500 2000
?
100
0
0
100
0
300
0
Binary strings vs. polarized lists(uniform distribution of M)
Total number of stems N
De
scr
ipti
on 
len
gth
 ga
in (i
n bi
ts)
s=0
s=1
s=2
s=3
Figure 3: Expected gain in description length by us-
ing binary strings rather than polarized lists under
the assumption that M ? U [1, N ].
until at some point (around s = 2), the situation re-
verses and polarized lists become more compact. In
both cases, the trend increases with the number N
of stems (within the range of values observed).
By contrast, it is instructive to consider a case
where the distribution of the number of elements
pointed to departs from uniformity. For instance, we
can make the assumption that M follows a binomial
distribution (M ? B[N, p]).6 Under this assump-
tion (and, as always, that of a Zipfian distribution of
stems), the expected description length gain by us-
ing binary strings rather than polarized lists is:
EM
[E??{M}[DLptr(?)]?DLbin(M)
]
= ?NM=1 pr(M)
(
1+?(M?)+M?hZipfN,s?NhbinN,M
)
with pr(M) = (NM
)pM (1? p)N?M
Letting N and s vary as in the previous computation,
we set the probability for a stem to have a pointer on
it to p = 0.01, so that the distribution of pointed ver-
sus ?unpointed? elements is considerably skewed.7
6This model predicts that most of the time, the number M
of elements pointed to is equal to N ? p (where p denotes the
probability for a stem to have a pointer on it), and that the prob-
ability pr(M) of other values of M decreases as they diverge
from N ? p.
7By symmetry, the same results would be found with p =
0.99.
0 500 1000 1500 2000
?
150
?
50
0
50
Binary strings vs. polarized lists(binomial distribution of M, p = 0.01)
Total number of stems N
De
scr
ipti
on 
len
gth
 ga
in (i
n bi
ts) s=0
s=1
s=2
s=3
Figure 4: Expected gain in description length by us-
ing binary strings rather than polarized lists under
the assumption that M ? B[N, 0.01].
As shown on figure 4, under these conditions, the ab-
solute value of the gain of using binary strings gets
much smaller in general, and the value of s for which
the gain becomes negative for N large gets close to 1
(for this particular value, it becomes positive at some
point between N = 1200 and N = 1600).
Altogether, under the assumptions that we have
used, these theoretical considerations suggest that
binary strings generally yield shorter description
lengths than polarized lists of pointers. Of course,
data for which these assumptions do not hold could
arise. In the perspective of unsupervised learning,
it would be particularily interesting to observe that
such data drive the learner to induce a different
model depending on the representation of pointers
being adopted.
It should be noted that nothing prevents binary
strings and lists of pointers from coexisting in a sin-
gle system, which would select the most compact
one for each particular case. On the other hand, it is
a logical necessity that all lists of pointers be of the
same kind, either polarized or not.
3 Experiments
In the previous section, by assuming frequencies of
stems and possible distributions of M (the num-
ber of stems per signature), we have explored the-
oretically the differences between several encoding
36
0 500 1000 1500 2000 25000
.00
00
0.0
010
0.0
020
Frequency as a function of rank
 (English corpus)
Rank
Fre
que
ncy
Figure 5: Frequency versus rank (stems) in English
corpus.
methods in the MDL framework. In this section, we
apply these methods to the problem of suffix discov-
ery in natural language corpora, in order to verify the
theoretical predictions we made previously. Thus,
the purpose of these experiments is not to state that
one encoding is preferable to the others; rather, we
want to answer the three following questions:
1. Are our assumptions on the frequency of stems
and size of signatures appropriate for natural
language corpora?
2. Given these assumptions, do our theoretical
analyses correctly predict the difference in de-
scription length of two encodings?
3. What is the relationship between the gain in de-
scription length and the size of the corpus?
3.1 Experimental methodology
In this experiment, for the purpose of calculating
distinct description lengths while using different en-
coding methods, we modified Linguistica8 by imple-
menting list of pointers and binary strings as alter-
native means to encode the pointers from signatures
to their associated stems9. As a result, given a set
8The source and binary files can be freely downloaded at
http://linguistica.uchicago.edu.
9Pointers to suffixes are not considered here.
0 50 100 150 200
0.0
0.1
0.2
0.3
0.4
0.5
Distribution of the number of stems
 per signature (English corpus)
Number of stems
Pro
por
tion
 of 
sig
nat
ure
s
Figure 6: Distribution of number of stems per signa-
ture (English corpus)
of signatures, we are able to compute a description
length for each encoding methods.
Within Linguistica, the morphology learning pro-
cess can be divided into a sequence of heuristics,
each of which searches for possible incremental
modifications to the current morphology. For exam-
ple, in the suffix-discovery procedure, ten heuristics
are carried out successively; thus, we have a dis-
tinct set of signatures after applying each of the ten
heuristics. Then, for each of these sets, we encode
the pointers from each signature to its correspond-
ing stems in three rival ways: as a list of pointers
(polarized or not), as traditionally understood, and
as a binary string. This way, we can compute the to-
tal description length of the signature-stem-linkage
for each of the ten sets of signatures and for each of
three two ways of encoding the pointers. We also
collect statistics on word frequencies and on the dis-
tribution of the size of signatures M , i.e. the number
M of stems which are are pointed to, both of which
are important parametric components in our theoret-
ical analysis.
Experiments are carried out on two orthographic
corpora (English and French), each of which has
100,000 word tokens.
3.2 Frequency of stems and size of signatures
The frequency of stems as a function of their rank
and the distribution of the size of signatures are plot-
37
0 100 200 300 400 500 6000
.00
00
0.0
010
0.0
020
Frequency as a function of rank
 (French corpus)
Rank
Fre
que
ncy
Figure 7: Frequency versus rank (stems) in French
corpus.
ted in figures 5 and 6 for the English corpus, and in
figures 7 and 8 for the French corpus. These graphs
show that in both the English and the French cor-
pora, stems appear to have a distribution similar to a
Zipfian one. In addition, in both corpora, M follows
a distribution whose character we are not sure of, but
which appears more similar to a binomial distribu-
tion. To some extent, these observations are consis-
tent with the assumptions we made in the previous
theoretical analysis.
3.3 Description length of each encoding
The description length obtained with each encoding
method is displayed in figures 9 (English corpus)
and 10 (French corpus), in which the x-axis refers to
the set of signatures resulting from the application
of each successive heuristics, and the y-axis corre-
sponds to the description length in bits. Note that
we only plot description lengths of non-polarized
lists of pointers, because the number of stems per
signature is always less than half the total number of
stems in these data (and we expect that this would
be true for other languages as well).10
These two plots show that in both corpora, there is
always a gain in description length by using binary
strings rather than lists of pointers for encoding the
pointers from signatures to stems. This observation
is consistent with our conclusion in section 2.3, but
10See figures 6 and 8 as well as section 2.2 above.
0 50 100 150
0.0
0.1
0.2
0.3
0.4
0.5
Distribution of the number of stems
 per signature (French corpus)
Number of stems
Pro
por
tion
 of 
sig
nat
ure
s
Figure 8: Distribution of number of stems per signa-
ture (French corpus)
it is important to emphasize again that for other data
(or other applications), lists of pointers might turn
out to be more compact.
3.4 Description length gain as a function of
corpus size
In order to evaluate the effect of corpus size on
the gain in description length by using binary string
rather than lists of variable-length pointers, we ap-
plied Linguistica to a number of English corpora of
different sizes ranging between 5,000 to 200,000 to-
kens. For the final set of signatures obtained with
each corpus, we then compute the gain of binary
strings encoding over lists of pointers as we did in
the previous experiments. The results are plotted in
figure 11.
This graph shows a strong positive correlation be-
tween description length gain and corpus size. This
is reminiscent of the results of our theoretical simu-
lations displayed in figures 3 and 4. As before, we
interpret the match between the experimental results
and the theoretical expectations as evidence support-
ing the validity of our theoretical predictions.
3.5 Discussion of experiments
These experiments are actually a number of case
studies, in which we verify the applicability of our
theoretical analysis on variant definitions of pointer
lengths in the MDL framework. For the particu-
38
2 4 6 8 10
0
200
00
600
00
DL of lists and binary strings(English corpus)
Heuristics
De
scr
ipti
on 
len
gth
 (in 
bits)
ListsBinary strings
1 3 5 7 9
Figure 9: Comparison of DL of 10 successive mor-
phologies using pointers versus binary strings (En-
glish corpus).
lar application we considered, learning morphology
with Linguistica, binary strings encoding proves to
be more compact than lists of variable-length point-
ers. However, the purpose of this paper is not to
predict that one variant is always better, but rather to
explore the mathematics behind different encodings.
Armed with the mathematical analysis of different
encodings, we hope to be better capable of making
the right choice under specific conditions. In partic-
ular, in the suffix-discovery application (and for the
languages we examined), our results are consistent
with the assumptions we made and the predictions
we derived from them.
4 Conclusion
The overall purpose of this paper has been to illus-
trate what was for us an unexpected aspect of us-
ing Minimum Description Length theory: not only
does MDL not specify the form of a grammar (or
morphology), but it does not even specify the pre-
cise form in which the description of the abstract
linkages between concepts (such as stems and sig-
natures) should be encoded and quantitatively eval-
uated. We have seen that in a range of cases, us-
ing binary strings instead of the more traditional
frequency-based pointers leads to a smaller overall
grammar length, and there is no guarantee that we
will not find an even shorter way to accomplish the
2 4 6 8 10
0
500
0
100
00
150
00
DL of lists and binary strings(French corpus)
Heuristics
De
scr
ipti
on 
len
gth
 (in 
bits)
ListsBinary strings
1 3 5 7 9
Figure 10: Comparison of DL of 10 successive
morphologies using pointers versus binary strings
(French corpus)
same thing tomorrow11. Simply put, MDL is em-
phatically an evaluation procedure, and not a discov-
ery procedure.
We hope to have shown, as well, that a system-
atic exploration of the nature of the difference be-
tween standard frequency-based pointer lengths and
binary string based representations is possible, and
we can develop reasonably accurate predictions or
expectations as to which type of description will be
less costly in any given case.
Acknowledgements
This research was supported by a grant of the Swiss
National Science Foundation to the first author.
References
C. de Marcken. 1996. Unsupervised Language Acquisi-
tion. Ph.D. thesis, MIT, Cambridge, MA.
J. Goldsmith. 2001. The unsupervised learning of natu-
ral language morphology. Computational Linguistics,
27(2):153?198.
B. Mandelbrot. 1953. An informational theory of the
statistical structure of language. In Willis Jackson, ed-
itor, Communication Theory, the Second London Sym-
posium, pages 486?502. Butterworth: London.
11See note 5.
39
0 50000 100000 150000 200000
0
200
00
400
00
600
00
DL gain of binary strings vs. lists(English corpus)
Corpus size
De
scr
ipti
on 
len
gth
 ga
in (i
n bi
ts)
Figure 11: DL gain from using binary string versus
size of corpus (English corpus)
J. Rissanen. 1989. Stochastic Complexity in Statistical
Inquiry. World Scientific Publishing Co, Singapore.
C.E. Shannon. 1948. A mathematical theory of commu-
nication. Bell Systems Technical Journal, 27:379?423.
40
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 713?716,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
An MDL-based approach to extracting subword units for
grapheme-to-phoneme conversion
Sravana Reddy
Department of Computer Science
The University of Chicago
Chicago, IL 60637
sravana@cs.uchicago.edu
John Goldsmith
Departments of Linguistics
and Computer Science
The University of Chicago
Chicago, IL 60637
goldsmith@uchicago.edu
Abstract
We address a key problem in grapheme-to-
phoneme conversion: the ambiguity in map-
ping grapheme units to phonemes. Rather than
using single letters and phonemes as units, we
propose learning chunks, or subwords, to re-
duce ambiguity. This can be interpreted as
learning a lexicon of subwords that has min-
imum description length. We implement an
algorithm to build such a lexicon, as well as a
simple decoder that uses these subwords.
1 Introduction
A system for converting written words to their pro-
nunciations is an important component of speech-
related applications, especially in large vocabulary
tasks. This problem, commonly termed ?grapheme-
to-phoneme conversion?, or g2p, is non-trivial for
several written languages, including English, since a
given letter (grapheme) may represent one of several
possible phonemes, depending on the context. Be-
cause the length of the context varies throughout the
dictionary, fixed-length contexts may overfit some
words, or inaccurately model others.
We approach this problem by treating g2p as a
function from contiguous sequences of graphemes,
which we call ?grapheme subwords?, to sequences
of phonemes (?phoneme subwords?), so that there is
minimal ambiguity in finding the phoneme subword
that corresponds to a given grapheme subword. That
is, we seek to minimize both these quantities:
1. The conditional entropy of the phoneme sub-
words given a grapheme subword. This di-
rectly tackles the problem of ambiguity ? a per-
fectly unambiguous phoneme subword condi-
tional distribution would have entropy = 0.
2. The entropy of the grapheme subwords. This
prevents the model from getting arbitrarily
complex.
As a toy example, consider the following word-
pronunciation1 pairs:
time T AY M
sting S T IH NG
negation N AH G EY SH AH N
There are at least 5 graphemes whose correspond-
ing phoneme distribution is ambiguous (?i?, ?e?, ?t?,
?n?, ?g?). In the segmentation below, every grapheme
subword corresponds to only one phoneme subword:
t + ime T + AY M
s + t + ing S + T + IH NG
neg + a + tion N AH G + EY + SH AH N
2 Related Work
Many grapheme-to-phoneme algorithms rely on
something resembling subwords; these are mainly
used to account for sequences of letters representing
a single phoneme (?ph? for F), or vice versa (?x? for
K S). Some of the early works that create one-to-
one alignments between a word and its pronuncia-
tion address these cases by allowing a letter to map
to one phoneme, a null phoneme, or 2-3 phonemes.
Jiampojamarn and Kondrak (2009) use
expectation-maximization (EM) to learn many-
to-many alignments between words and pro-
nunciations, effectively obtaining subwords.
1All phonemes are denoted by their Arpabet representations.
713
Joint-sequence models divide a word-pronunciation
pair into a sequence of disjoint graphones or
graphonemes ? tuples containing grapheme and
phoneme subwords. Such segmentations may
include only trivial graphones containing subwords
of length at most 1 (Chen, 2003). Other such
models use EM to learn the maximum likelihood
segmentation into graphones (Deligne and Bimbot,
1995; Bisani and Ney, 2008; Vozilla et al, 2003).
Subwords ? or phrases ? are used widely in ma-
chine translation. There is a large body of work on
phrase extraction starting from word alignments; see
Koehn et al (2003) for a review. Marcu and Wong
(2002) learn phrases directly from sentence pairs us-
ing a joint probability model.
3 Subword Extraction
3.1 Motivation for using MDL
Consider a lexicon of grapheme subwords G and
phoneme subwords P that is extracted from a dic-
tionary of word-pronunciation pairs, along with a
joint probability distribution over G and P . As
stated earlier, our objective is to minimize the en-
tropy of phoneme subwords conditioned on a given
grapheme subword, as well as the entropy of the
grapheme subwords. That is, we would like to min-
imize H(P|G) +H(G), which is
H(G,P) = ?
?
g?G
?
p?P
pr(g, p) log pr(g, p) (1)
This objective can be restated as minimizing the
expected description length of the lexicon, which is
given by its entropy. This is reflected in the MDL
principle (Rissanen, 1978), which seeks to find a
lexicon such that the description length of the lex-
icon (and the compression of the data under the lex-
icon) is minimized.
3.2 Lexicon Induction
We begin with an initial alignment between a word?s
graphemes and the phonemes in its pronunciation
for all word-pronunciation pairs in the training dic-
tionary. These alignments are derived using the stan-
dard string edit distance dynamic programming al-
gorithm (Wagner and Fischer, 1974), giving a list
of tuples t = [(w1, r1), (w2, r2), . . .] for each word-
pronunciation pair.2 The set of all tuple lists t com-
poses the training dictionary T .
The initial lexicon is composed of all singleton
graphemes and phonemes (including null). The
probability pr(g, p) is taken to be the number of
times the tuple (g, p) occurs in T divided by the total
number of tuples over all alignments in T .
Following a procedure similar the word-discovery
algorithm of de Marcken (1996), the lexicon is iter-
atively updated as sketched in Table 1. At no point
do we delete singleton graphemes or phonemes.
The subwords in the final updated lexicon are then
used to decode the pronunciations of unseen words.
4 G2P Decoding
4.1 Joint segmentation and decoding
Finding the pronunciation of a word based on the
induced subword lexicon involves segmenting the
word into a sequence of grapheme subwords, and
mapping it to a sequence of phoneme subwords.
One possibility is carry these steps out sequen-
tially: first parse the word into grapheme subwords,
and then use a sequence labeling algorithm to find
the best corresponding sequence of phoneme sub-
words. However, it is likely that the true pronuncia-
tion of a word is not derived from its best parse into
grapheme units. For example, the best parse of the
word ?gnat? is ?g nat?, which yields the pronuncia-
tion G N AE T, while the parse ?gn at? would give
the correct pronunciation N AE T.
Therefore, we search for the best pronunciation
over all segmentations of the word, adapting the
monotone search algorithm proposed by Zens and
Ney (2004) for phrase-based machine translation.3
4.2 Smoothing
A bigram model is used over both the grapheme
and phoneme subwords. These bigrams need to be
smoothed before the decoding step. Adding an equal
probability mass to unseen bigrams would fail to re-
flect simple phonotactics (patterns that govern sound
2Phoneme insertions and deletions are represented by the
null grapheme and null phoneme respectively.
3The key adaptation is in using a bigram model over both
graphemes and phonemes, rather than only phonemes as in the
original algorithm.
714
Table 1: Concatenative algorithm for building a subword lexicon that minimizes description length. The input is T ,
the set of alignments, and a threshold integer k, which is tuned using a held-out development set.
1 Update pr(g, p) by computing the posterior probabilities of the tuple (g, p) in T ,
using the forward-backward algorithm. Repeat once more to get an intermediate lexicon.
2 Compute the Viterbi parse of each t ? T under the lexicon derived in step 1.
3 Let A, the set of candidate tuples for addition to the lexicon, contain all tuples (wiwi+1, riri+1) such that
(wi, ri) and (wi+1, ri+1) are adjacent more than k times in the computed Viterbi parses. For each (g, p) ? A,
estimate the change in description length of the lexicon if (g, p) is added. If description length decreases,
remove any null symbols within g and p, and add (g, p) to the lexicon.
4 Repeat steps 1 and 2.
5 Delete all tuples that do not occur in any of the Viterbi parses.
6 Compare the description length of the new lexicon with the lexicon at the start of the iteration. If the
difference is sufficiently small, return the new lexicon; else, repeat from step 1.
sequences) in several cases. For example, the bi-
gram L UW K + S is much more likely than L UW
K + Z, since S is more likely than Z to follow K.
To introduce a bias towards phonotacticaly likely
bigrams, we define the smoothed bigram probability
of the subword a following a subword b. Given that
b is made up of a sequence of l phonemes b1b2 . . . bl,
the probability is defined as the interpolation4:
prnew(a|b) = ?1pr(a|b1b2 . . . bl) +
?2pr(a|b1b2 . . . bl?1) + ?3pr(a|b1b2 . . . bl?2)
Both the grapheme and phoneme subword bi-
grams are smoothed as described.
5 Results
We test our algorithm on the CMU Pronouncing
Dictionary5. The dictionary is divided randomly
into a training (90% of the data) and a test set. Per-
formance is evaluated by measuring the phoneme er-
ror rate (PER) and the word error rate (WER).
The subword extraction algorithm converges in 3
iterations.We run the g2p decoder using the lexicon
after 3 iterations, as well as after 0, 1 and 2 itera-
tions. The results are shown in Table 2.
Figure 1 compares the results of our method (de-
noted by ?MDL-Sub?) to two baselines, at different
values of maximum subword length. To evaluate the
quality of our subwords, we substitute another ex-
traction algorithm to create the lexicon ? the grow-
diag-final phrase extraction method (Koehn et al,
4In our experiments, we set ?1 = 0.5, ?2 = 0.3, ?3 = 0.2.
5The CMU Pronouncing Dictionary. Available online at
http://www.speech.cs.cmu.edu/cgi-bin/cmudict
Table 2: Results after each iteration of subword extrac-
tion. While the maximum subword length after iteration
3 is 8, the vast majority of subwords have length 6 or less.
# subwords Max subword WER PER
length
0 |G| : 27, |P| : 40 1 73.16 24.20
1 |G| : 819, |P| : 1254 2 48.39 12.43
2 |G| : 5430, |P| : 4954 4 28.32 7.16
3 |G| : 6417, |P| : 5358 6 26.31 6.29
2005), denoted by ?GD? in the figure. We also run
the implementation of Bisani and Ney (2008) ? de-
noted by ?BN? ? on the same data. BN is an example
of a joint-sequence n-gram model, which uses a joint
distribution pr(G,P) of graphemes and phonemes
(?graphones?), conditioned on the preceding n-1 gra-
phones for context information. Since this algorithm
outperforms most of the existing g2p algorithms, it
serves as a good point of comparison to the state of
the art in g2p. The results of BN using an n-gram
model are compared to MDL-Sub with an n-1 max-
imum subword length6.
The MDL-Sub lexicon does significantly better
than the phrases extracted by GD. While BN starts
off doing better than MDL-Sub, the latter outper-
forms BN at longer subword lengths. Most of the ad-
ditional errors in BN at that stage involve grapheme-
to-phoneme ambiguity ? phonemes like AE, AA, and
AH being confused for one another when mapping
6The contextual information of (n-1)-length subwords with
bigrams is assumed to be roughly comparable to that of very
short subwords over n-grams.
715
the grapheme ?a?, and so on. Far fewer of these er-
rors are produced by our algorithm. However, some
of the longer subwords in MDL-Sub do introduce
additional errors, mainly because the extraction al-
gorithm merges smaller subwords from previous it-
erations. For example, one of the items in the ex-
tracted lexicon is ?icati? ? a product of merging ?ic?
and ?ati? ? corresponding to IH K EY SH, thus
generating incorrect pronunciations for words con-
taining the string ?icating?.
Figure 1: Comparison of error rates.
6 Conclusion
This paper deals with translational ambiguity, which
is a major issue in grapheme-to-phoneme conver-
sion. The core of our system consists of extract-
ing subwords of graphemes and phonemes from the
training data, so that the ambiguity of deriving a
phoneme subword from a grapheme subword is min-
imized. This is achieved by formalizing ambiguity
in terms of the minimum description length princi-
ple, and using an algorithm that reduces the descrip-
tion length of the subword lexicon at each iteration.
In addition, we also introduce a smoothing mech-
anism which retains some of the phonotactic depen-
dencies that may be lost when using subwords rather
than singleton letters and phonemes.
While retaining the basic approach to minimizing
ambiguity, there are some avenues for improvement.
The algorithm that builds the lexicon creates a more
or less hierarchical structure ? subwords tend to be
composed from those extracted at the previous iter-
ation. This appears to be the cause of many of the
errors produced by our method. A subword extrac-
tion algorithm that does not use a strictly bottom-up
process may create a more robust lexicon.
Our method of subword extraction could also be
applied to phrase extraction for machine transla-
tion, or in finding subwords for related problems like
transliteration. It may also be useful in deriving sub-
word units for speech recognition.
References
Maximilian Bisani and Hermann Ney. 2008. Joint-
sequence models for grapheme-to-phoneme conver-
sion. Speech Communication, 50:434?451.
Stanley F Chen. 2003. Conditional and joint models for
grapheme-to-phoneme conversion. In Proceedings of
Eurospeech.
Carl G de Marcken. 1996. Unsupervised Language Ac-
quisistion. Ph.D. thesis, MIT.
Sabine Deligne and Frederic Bimbot. 1995. Language
modeling by variable length sequences: theoretical
formulation and evaluation of multigrams. In Pro-
ceedings of ICASSP.
Sittichai Jiampojamarn and Grzegorz Kondrak. 2009.
Online discriminative training for grapheme-to-
phoneme conversion. In Proceedings of Interspeech.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT-NAACL.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh System Description
for the 2005 IWSLT Speech Translation Evaluation.
In Proceedings of IWSLT.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proceedings of EMNLP.
Jorma Rissanen. 1978. Modeling by the shortest data
description. Automatica.
Paul Vozilla, Jeff Adams, Yuliya Lobacheva, and Ryan
Thomas. 2003. Grapheme to phoneme conversion and
dictionary verification using graphonemes. In Pro-
ceedings of Eurospeech.
Robert Wagner and Michael Fischer. 1974. The string-
to-string correction problem. Journal of the ACM.
Richard Zens and Hermann Ney. 2004. Improvements in
phrase-based statistical machine translation. In Pro-
ceedings of HLT-NAACL.
716
