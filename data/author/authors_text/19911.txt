Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2042?2051, Dublin, Ireland, August 23-29 2014.
RED: A Reference Dependency Based MT Evaluation Metric
Hui Yu
??
Xiaofeng Wu
?
Jun Xie
?
Wenbin Jiang
?
Qun Liu
??
Shouxun Lin
?
?
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
?
University of Chinese Academy of Sciences
{yuhui,xiejun,jiangwenbin,sxlin}@ict.ac.cn
?
CNGL, School of Computing, Dublin City University
{xiaofengwu,qliu}@computing.dcu.ie
Abstract
Most of the widely-used automatic evaluation metrics consider only the local fragments of the
references and translations, and they ignore the evaluation on the syntax level. Current syntax-
based evaluation metrics try to introduce syntax information but suffer from the poor pars-
ing results of the noisy machine translations. To alleviate this problem, we propose a novel
dependency-based evaluation metric which only employs the dependency information of the ref-
erences. We use two kinds of reference dependency structures: headword chain to capture the
long distance dependency information, and fixed and floating structures to capture the local con-
tinuous ngram. Experiment results show that our metric achieves higher correlations with human
judgments than BLEU, TER and HWCM on WMT 2012 and WMT 2013. By introducing extra
linguistic resources and tuning parameters, the new metric gets the state-of-the-art performance
which is better than METEOR and SEMPOS on system level, and is comparable with METEOR
on sentence level on WMT 2012 and WMT 2013.
1 Introduction
Automatic machine translation (MT) evaluation plays an important role in the evolution of MT. It not
only evaluates the performance of MT systems, but also makes the development of MT systems rapider
(Och, 2003). According to the type of the employed information, the automatic MT evaluation metrics
can be classified into three categories: lexicon-based metrics, syntax-based metrics and semantic-based
metrics.
The lexicon-based metrics, such as BLEU (Papineni et al., 2002), TER (Snover et al., 2006), METEOR
(Lavie and Agarwal, 2007) and AMBER (Chen and Kuhn, 2011; Chen et al., 2012), are good at capturing
the lexicon or phrase level information, e.g. fixed phrases or idioms. But they cannot adequately reflect
the syntax similarity. Current efforts in syntax-based metrics, such as the headword chain based metric
(HWCM) (Liu and Gildea, 2005), the LFG dependency tree based metric (Owczarzak et al., 2007) and
syntactic/semantic-role overlap (Gim?enez and M`arquez, 2007) , suffer from the parsing of the potentially
noisy machine translations, so the improvement of their performance is restricted due to the serious
parsing errors. Semantic-based metrics, such as MEANT (Lo et al., 2012; Lo and Wu, 2013), have the
similar problem that the accuracy of semantic role labeling (SRL) can also drop due to the errors in
translations. To avoid the parsing of potentially noisy translations, the CCG based metric (Mehay and
Brew, 2007) only uses the parsing result of reference and employs 2-gram dependents, but it did not
achieve the state-of-the-art performance.
In this paper, we propose a novel dependency tree based MT evaluation metric. The new metric only
employs the reference dependency tree, leaving the translation unparsed to avoid the error propagation.
We use two kinds of reference dependency structures in our metric. One is the headword chain (Liu and
Gildea, 2005) which can capture long distance dependency information. The other is fixed and floating
structure (Shen et al., 2010) which can capture local continuous ngram. When calculating the matching
score between the headword chain and the translation, we use a distance-based similarity. Experiment
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
2042
results show that our metric achieves higher correlations with human judgments than BLEU, TER and
HWCM on WMT 2012 and WMT 2013. After introducing extra resources and tuning parameters on
WMT 2010, the new metric is better than METEOR and SEMPOS on system level and comparable with
METEOR on sentence level on WMT 2012 and WMT2013.
The remainder of this paper is organized as follows. Section 2 describes our new reference dependency
based MT evaluation metric. In Section 3, we introduce some extra resources to this new metric. Section
4 presents the parameter tuning for the new metric. Section 5 gives the experiment results. Conclusions
and future work are discussed in Section 6.
2 RED: A Reference Dependency Based MT Evaluation Metric
The new metric is a REference Dependency based automatic evaluation metric, so we name it RED.
We present the new metric detailedly in this section. The description of dependency ngrams is given in
Section 2.1. The method to score the dependency ngram is presented in Section 2.2. At last, the method
of calculating the final score is introduced in Section 2.3.
2.1 Two Kinds of Dependency Ngrams
To capture both the long distance dependency information and the local continuous ngrams, we use both
the headword chain and the fixed-floating structures in our new metric, which correspond to the two
kinds of dependency ngram (dep-ngram), headword chain ngram and fixed-floating ngram.
Figure 1: An example of dependency tree.
Figure 2: Different kinds of structures extracted
from the dependency tree in Figure 1. (a): Head-
word chain. (b): Fixed structure. (c): Floating struc-
ture.
2.1.1 Headword chain
Headword chain is a sequence of words which corresponds to a path in the dependency tree (Liu and
Gildea, 2005). For example, Figure 2(a) is a 3-word headword chain extracted from the dependency tree
in Figure 1. Headword chain can represent the long distance dependency information, but cannot capture
most of the continuous ngrams. In our metric, headword chain corresponds to the headword chain ngram
in which the positions of the words are considered. So the form of headword chain ngram is expressed
as (w1
pos1
, w2
pos2
, ..., wn
posn
), where n is the length of the headword chain ngram. For example, the
headword chain in Figure 2(a) is expressed as (saw
2
, with
5
,magnifier
7
).
2.1.2 Fixed and floating structures
Fixed and floating structures are defined in Shen et al. (2010). Fixed structures consist of a sub-root with
children, each of which must be a complete constituent. They are called fixed dependency structures
because the head is known or fixed. For example, Figure 2(b) shows a fixed structure. Floating structures
consist of a number of consecutive sibling nodes of a common head, but the head itself is unspecified.
Each of the siblings must be a complete constituent. Figure 2(c) shows a floating structure. Fixed-
floating structures correspond to fixed-floating ngrams in our metric. Fixed-floating ngrams don?t need
the position information, and can be simply expressed as (w1, w2, ..., wn), where n is the length of the
2043
Figure 3: An example of calculating matching score for a headword chain ngram
(saw
2
, with
5
,magnifier
7
). dis r
1
and dis r
2
are the distances between the corresponding two
words in the reference. dis h
1
and dis h
2
are the distances between the corresponding two words in the
hypothesis.
fixed-floating ngram. For example, the fixed structure in Figure 2(b) and the floating structre in Figure
2(c) can be expressed as (I, saw, an, ant) and (an, ant, with, a,magnifier) respectively.
2.2 Scoring Dep-ngrams
Headword chain ngrams may not be continuous, while fixed-floating ngrams must be continuous. So the
scoring methods of the two kinds of dep-ngrams are different, and we introduce the two scoring methods
in Section 2.2.1 and Section 2.2.2 respectively.
2.2.1 Scoring headword chain ngram
For a headword chain ngram (w1
pos1
, w2
pos2
, ..., wn
posn
), if we can find all these n words in the string
of the translation with the same order as they appear in the reference sentence, we consider it a match and
the matching score is a distance-based similarity which is calculated by the relative distance, otherwise it
is not a match and the score is 0. The matching score is a decimal value between 0 and 1, which is more
suitable than just use integer 0 and 1. For example, if the distance between two words in reference is 1,
but the distance in two different hypotheses are 2 and 5 respectively. It?s more reasonable to score them
0.5 and 0.2 rather than 1 and 0.
The relative distance dis r
i
between every two adjacent words in this kind of dep-ngram is calculated
by Formula (1), where pos
wi
is the position of word wi in the sentence. In Formula (1), we have
1 ? i ? n ? 1 and n is the length of the dep-ngram. Then a vector (dis r
1
, dis r
2
, ..., dis r
n?1
) is
obtained. In the same way, we obtain vector (dis h
1
, dis h
2
, ..., dis h
n?1
) for the translation side.
dis r
i
= |pos
w(i+1)
? pos
wi
| (1)
The matching score p
(d,hyp)
for a headword chain ngram (d) and the translation (hyp) is calculated
according to Formula (2), where n > 1. When the length of the dep-ngram equals 1, the matching score
equals 1 if the translation has the same word, otherwise, the matching score equals 0.
p
(d,hyp)
=
?
?
?
exp(?
?
n?1
i=1
|dis r
i
? dis h
i
|
n? 1
) if match
0 if unmatch
(2)
An example illustrating the calculation of the matching score p
(d,hyp)
is shown in Figure 3. There is
a 3-word headword chain ngram (saw
2
, with
5
,magnifier
7
) in the dependency tree of the reference.
2044
For this dep-3gram, the words are represented with underline in the reference dependency tree and the
reference sentence in Figure 3. We can also find all the same three underlined words in the translation
with the same order as they appear in the reference. Therefore, there is a match for this dep-3gram. To
compute the matching score between this dep-3gram and the translation, we have:
? Calculate the distance
dis r
1
= |pos
with
? pos
saw
| = |5? 2| = 3 dis r
2
= |pos
magnifier
? pos
with
| = |7? 5| = 2
dis h
1
= |pos
with
? pos
saw
| = |5? 2| = 3 dis h
2
= |pos
magnifier
? pos
with
| = |6? 5| = 1
? Get the matching score as Formula (3) according to Formula (2). d denotes
(saw
2
, with
5
,magnifier
7
) and hyp denotes the translation in the example.
p
(d,hyp)
= exp(?
|dis r
1
? dis h
1
|+ |dis r
2
? dis h
2
|
3? 1
) = exp(?
|3? 3|+ |2? 1|
3? 1
) = exp(?0.5)
(3)
We also tried other methods to calculate the matching score, such as the cosine distance and the
absolute distance, but the relative distance performed best. For a headword chain ngram with more than
one matches in the translation, we choose the one with the highest matching score.
2.2.2 Scoring fixed-floating ngram
The words in the fixed-floating ngram are continuous, so we restrict the matched string in the translation
also to being continuous. That means, for a fixed-floating ngram (w1, w2, ..., wn), if we can find all these
n words continuous in the translation with the same order as they appear in the reference, we think the
dep-ngram can match with the translation. The matching score can be obtained by Formula (4), where d
stands for a fixed-floating ngram and hyp stands for the translation.
p
(d,hyp)
=
{
1 if match
0 if unmatch
(4)
2.3 Scoring RED
In the new metric, we use Fscore to obtain the final score. Fscore is calculated by Formula (5), where ?
is a value between 0 and 1.
Fscore =
precision ? recall
? ? precision+ (1? ?) ? recall
(5)
The dep-ngrams of the reference and the string of the translation are used to calculate the precision and
recall. In order to calculate precision, the number of the dep-ngrams in the translation should be given,
but there is no dependency tree for the translation in our method. We know that the number of dep-
ngrams has an approximate linear relationship with the length of the sentence, so we use the length of
the translation to replace the number of the dep-ngrams in the translation dependency tree. Recall can
be calculated directly since we know the number of the dep-ngrams in the reference. The precision and
recall are computed as follows.
precision =
?
d?D
n
p
(d,hyp)
len
h
, recall =
?
d?D
n
p
(d,hyp)
count
n(ref)
D
n
is the set of dep-ngrams with the length of n. len
h
is the length of the translation. count
n(ref)
is the
number of the dep-ngrams with the length of n in the reference.
2045
The final score of RED is achieved using Formula (6), in which a weighted sum of the dep-ngrams?
Fscore is calculated. w
ngram
(0 ? w
ngram
? 1) is the weight of dep-ngram with the length of n. Fscore
n
is the Fscore for the dep-ngrams with the length of n.
RED =
N
?
n=1
(w
ngram
? Fscore
n
) (6)
3 Introducing Extra Resources
Many automatic evaluation metrics can only find the exact match between the reference and the transla-
tion, and the information provided by the limited number of references is not sufficient. Some evaluation
metrics, such as TERp (Snover et al., 2009) and METOER, introduce extra resources to expand the
reference information. We also introduce some extra resources to RED, such as stem, synonym and
paraphrase. The words within a sentence can be classified into content words and function words. The
effects of the two kinds of words are different and they shouldn?t have the same matching score, so we
introduce a parameter to distinguish them. The methods of applying these resources are introduced as
follows.
? Stem and Synonym
Stem(Porter, 2001) and synonym (WordNet
1
) are introduced to RED in the following three steps.
First, we obtain the alignment with Meteor Aligner (Denkowski and Lavie, 2011) in which not only
exact match but also stem and synonym are considered. We use stem and synonym together with
exact match as three match modules. Second, the alignment is used to match for a dep-ngram. We
think the dep-ngram can match with the translation if the following conditions are satisfied. 1) Each
of the words in the dep-ngram has a matched word in the translation according to the alignment;
2) The words in dep-ngram and the matched words in translation appear in the same order; 3) The
matched words in translation must be continuous if the dep-ngram is a fixed-floating ngram. At last,
the match module score of a dep-ngram is calculated according to Formula (7). Different match
modules have different effects, so we give them different weights.
s
mod
=
?
n
i=1
w
m
i
n
, 0 ? w
m
i
? 1 (7)
m
i
is the match module (exact, stem or synonym) of the ith word in a dep-ngram. w
m
i
is the match
module weight of the ith word in a dep-ngram. n is the number of words in a dep-ngram.
? Paraphrase
When introducing paraphrase, we don?t consider the dependency tree of the reference, because
paraphrases may not be contained in the headword chain and fixed-floating structures. First, the
alignment is obtained with METEOR Aligner, only considering paraphrase. Second, the matched
paraphrases are extracted from the alignment and defined as paraphrase-ngram. The score of a
paraphrase is 1? w
par
, where w
par
is the weight of paraphrase-ngram.
? Function word
We introduce a parameter w
fun
(0 ? w
fun
? 1) to distinguish function words and content words.
w
fun
is the weight of function words. The function word score of a dep-ngram or paraphrase-ngram
is computed according to Formula (8).
s
fun
=
C
fun
? w
fun
+ C
con
? (1? w
fun
)
C
fun
+ C
con
(8)
C
fun
is the number of function words in the dep-ngram or paraphrase-ngram. C
con
is the number
of content words in the dep-ngram or paraphrase-ngram.
1
http://wordnet.princeton.edu/
2046
We use RED-plus (REDp) to represent RED with extra resources, and the final score are calculated as
Formula (9), in which Fscore
p
is obtained using precison
p
and recall
p
as Formula (10).
REDp =
N
?
n=1
(w
ngram
? Fscore
p
n
) (9)
Fscore
p
=
precision
p
? recall
p
? ? precision
p
+ (1? ?) ? recall
p
(10)
precision
p
and recall
P
in Formula (10) are calculated as follows.
precision
p
=
score
par
n
+ score
dep
n
len
h
, recall
p
=
score
par
n
+ score
dep
n
count
n
(ref) + count
n
(par)
len
h
is the length of the translation. count
n(ref)
is the number of the dep-ngrams with the length of n
in the reference. count
n
(par) is the number of paraphrases with length of n in reference. score
par
n
is
the match score of paraphrase-ngrams with the length of n. score
dep
n
is the match score of dep-ngrams
with the length of n. score
par
n
and score
dep
n
are calculated as follows.
score
par
n
=
?
par?P
n
(1? w
par
? s
fum
) , score
dep
n
=
?
d?D
n
(p
(d,hyp)
? s
mod
? s
fun
)
P
n
is the set of paraphrase-ngrams with the length of n. D
n
is the set of dep-ngrams with the length of n.
4 Parameter Tuning
There are several parameters in REDp, and different parameter values can make the performance of
REDp different. For example,w
ngram
represents the weight of dep-ngram with the length of n. The
effect of ngrams with different lengths are different, and they shouldn?t have the same weight. So we can
tune the parameters to find their best values.
We try a preliminary optimization method to tune parameters in REDp. A heuristic search is employed
and the parameters are classified into two subsets. The parameter optimization is a grid search over the
two subsets of parameters. When searching Subset 1, the parameters in Subset 2 are fixed, and then
Subset 1 and Subset 2 are exchanged to finish this iteration. Several iterations are executed to finish the
parameter tuning process. This heuristic search may not find the global optimum but it can save a lot of
time compared with exhaustive search. The optimization goal is to maximize the sum of Spearman?s ?
rank correlation coefficient on system level and Kendall?s ? correlation coefficient on sentence level. ?
is calculated using the following equation.
? = 1?
6
?
d
2
i
n(n
2
? 1)
where d
i
is the difference between the human rank and metric?s rank for system i. n is the number of
systems. ? is calculated as follows.
? =
number of concordant pairs? number of discordant pairs
number of concordant pairs + number of discordant pairs
The data of into-English tasks in WMT 2010 are used to tune parameters. The tuned parameters are
listed in Table 1.
5 Experiments
5.1 Data
The test sets in experiments are WMT 2012 and WMT 2013. The language pairs are German-to-English
(de-en), Czech-to-English (cz-en), French-to-English (fr-en), Spanish-to-English (es-en) and Russian-to-
English (ru-en). The number of translation systems for each language pair are showed in Table 2. For
each language pair, there are 3003 sentences in WMT 2012 and 3000 sentences in WMT 2013.
2047
Parameter ? w
fun
w
exact
w
stem
w
syn
w
par
w
1gram
w
2gram
w
3gram
tuned values 0.9 0.2 0.9 0.6 0.6 0.6 0.6 0.5 0.1
Table 1: Parameter values after tuning on WMT 2010. ? is from Formula (10). w
fun
is the weight of
function word. w
exact
, w
stem
andw
syn
are the weights of the three match modules ?exact stem synonym?
respectively. w
par
is the weight of paraphrase-ngram. w
1gram
, w
2gram
and w
3gram
are the weights of
dep-ngram with the length of 1, 2 and 3 respectively.
Language pairs cz-en de-en es-en fr-en ru-en
WMT2012 6 16 12 15 -
WMT2013 12 23 17 19 23
Table 2: The number of translation systems for each language pair on WMT 2012 and WMT 2013.
We parsed the reference into constituent tree by Berkeley parser
2
and then converted the constituent
tree into dependency tree by Penn2Malt
3
. Presumably, the performance of the new metric will be better
if the dependency trees are labeled by human. Reference dependency trees are labeled only once and can
be used forever so it will not increase costs.
5.2 Baselines
In the experiments, we compare the performance of our metric with the widely-used lexicon-based met-
rics such as BLEU
4
, TER
5
and METEOR
6
, dependency-based metric HWCM and semantic-based metric
SEMPOS (Mach?a?cek and Bojar, 2011) which has the best performance on system level according to the
published results of WMT 2012.
The results of BLEU are obtained using 4-gram with smoothing option. The version of TER is 0.7.25.
The results of METEOR are obtained by Version 1.4 with task option ?rank?. We re-implement HWCM
which employs an epsilon value of 10
?3
to replace zero for smoothing purpose. The correlations of
SEMPOS are obtained from the published results of WMT 2012 and WMT 2013.
5.3 Experiment Results
The experiments on both system level and sentence level are carried out. On system level, the correlations
are calculated using Spearman?s rank correlation coefficient ? (Pirie, 1988). Kendall?s rank correlation
coefficient ? (Kendall, 1938) is employed to evaluate the sentence level correlation. Our method performs
best when the maximum length of dep-ngram is set to 3, so we only present the results with the maximum
length of 3. RED represents the new metric with exact match and the parameter values are set as follows.
? = 0.5. w
1gram
= w
2gram
= w
3gram
= 1/3. REDp represents the new metric with extra resources
and tuned parameter values which are listed in Table (1).
5.3.1 System level correlations
The system level correlations are shown in Table 3. RED is better than BLEU, TER and HWCM on
average on both WMT 2012 and WMT 2013, which reflects that using syntactic information and only
parsing the reference side are helpful. REDp gets the best result on all of the language pairs except
cz-en on WMT 2012. The significant improvement from RED to REDp illustrates the effect of extra
resources and the parameter tuning. Stem, synonym and paraphrase can enrich the reference and provide
extra knowledge for automatic evaluation metric. There are several parameters in REDp, and different
parameter values can make the performance of REDp different. So the performance can be optimized
through parameter tuning. SEMPOS got the best correlation according to the published results of WMT
2
http://code.google.com/p/berkeleyparser/downloads/list
3
http://stp.lingfil.uu.se/
?
nivre/research/Penn2Malt.html
4
ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13a.pl
5
http://www.cs.umd.edu/
?
snover/tercom
6
http://www.cs.cmu.edu/
?
alavie/METEOR/download/meteor-1.4.tgz
2048
2012, and METEOR got the best correlation according to the published results of WMT 2013 on into-
English task on system level. REDp gets better result than SEMPOS and METEOR on both WMT 2012
and WMT 2013, so REDp achieves the state-of-the-art performance on system level.
data WMT 2012 WMT 2013
Metrics cz-en de-en es-en fr-en ave cz-en de-en es-en fr-en ru-en ave
BLEU .886 .671 .874 .811 .811 .936 .895 .888 .989 .670 .876
TER .886 .624 .916 .821 .812 .800 .833 .825 .951 .581 .798
HWCM .943 .762 .937 .818 .865 .902 .904 .886 .951 .756 .880
METEOR .657 .885 .951 .843 .834 .964 .961 .979 .984 .789 .935
SEMPOS .943 .924 .937 .804 .902 .955 .919 .930 .938 .823 .913
RED 1.0 .759 .951 .818 .882 .964 .951 .930 .989 .725 .912
REDp .943 .947 .965 .843 .925 .982 .973 .986 .995 .800 .947
Table 3: System level correlations on WMT 2012 and WMT 2013. The value in bold is the best result in
each column. ave stands for the average result of the language pairs on WMT 2012 or WMT 2013.
5.3.2 Sentence level correlations
The sentence level correlations on WMT 2012 and WMT 2013 are shown in Table 4. RED is better than
BLEU and HWCM on all the language pairs, which reflects the effectiveness of syntactic information
and only parsing the reference. By introducing extra resources and parameter tuning, REDp achieves
significant improvement over RED. Stem, synonym and paraphrase can enrich the reference and provide
extra knowledge for automatic evaluation metric. There are several parameters in REDp, and different
parameter values can make the performance of REDp different. A better performance can be exploited
through parameter tuning. From the results of REDp and METEOR, we can see that REDp gets the
comparable results with METEOR on sentence level on both WMT 2012 and WMT 2013.
data WMT 2012 WMT 2013
Metrics cz-en de-en es-en fr-en ave cz-en de-en es-en fr-en ru-en ave
BLEU .157 .191 .189 .210 .187 .199 .220 .259 .224 .162 .213
HWCM .158 .207 .203 .204 .193 .187 .208 .247 .227 .175 .209
METEOR .212 .275 .249 .251 .247 .265 .293 .324 .264 .239 .277
RED .165 .218 .203 .221 .202 .210 .239 .292 .246 .196 .237
REDp .212 .271 .234 .250 .242 .259 .290 .323 .260 .223 .271
Table 4: Sentence level correlations on WMT 2012 and WMT 2013. The value in bold is the best result
in each column. ave stands for the average result of the language pairs on WMT 2012 or WMT 2013.
6 Conclusion and Future Work
In this paper, we propose a reference dependency based automatic MT evaluation metric RED. The
new metric only uses the dependency trees of the reference, which avoids the parsing of the potentially
noisy translations. Both long distance dependency information and the local continuous ngrams are
captured by the new metric. The experiment results indicate that RED achieves better correlations than
BLEU, TER and HWCM on both system level and sentence level. REDp, the improved version of RED
through adding extra resources and preliminary parameter tuning, gets state-of-the-art results which are
better than METEOR and SEMPOS on system level. On sentence level, REDp gets the comparable
performance with METEOR.
In the future, we will use the dependency forest instead of the dependency tree to reduce the effect
of parsing errors. We will also apply RED and REDp to the tuning process of SMT to improve the
translation quality.
2049
Acknowledgements
The authors were supported by National Natural Science Foundation of China (Contract 61202216)
and National Natural Science Foundation of China (Contract 61379086). Qun Liu?s work was partially
supported by the Science Foundation Ireland (Grant No. 07/CE/I1142) as part of the CNGL at Dublin
City University. Sincere thanks to the three anonymous reviewers for their thorough reviewing and
valuable suggestions.
References
Boxing Chen and Roland Kuhn. 2011. Amber: A modified bleu, enhanced ranking metric. In Proceedings of
the Sixth Workshop on Statistical Machine Translation, pages 71?77, Edinburgh, Scotland, July. Association for
Computational Linguistics.
Boxing Chen, Roland Kuhn, and George Foster. 2012. Improving amber, an mt evaluation metric. In Proceedings
of the Seventh Workshop on Statistical Machine Translation, WMT ?12, pages 59?63, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Michael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic metric for reliable optimization and evaluation
of machine translation systems. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages
85?91. Association for Computational Linguistics.
Jes?us Gim?enez and Llu??s M`arquez. 2007. Linguistic features for automatic evaluation of heterogenous mt systems.
In Proceedings of the Second Workshop on Statistical Machine Translation, pages 256?264. Association for
Computational Linguistics.
Maurice G Kendall. 1938. A new measure of rank correlation. Biometrika, 30(1/2):81?93.
Alon Lavie and Abhaya Agarwal. 2007. Meteor: an automatic metric for mt evaluation with high levels of
correlation with human judgments. In Proceedings of the Second Workshop on Statistical Machine Translation,
StatMT ?07, pages 228?231, Stroudsburg, PA, USA. Association for Computational Linguistics.
Ding Liu and Daniel Gildea. 2005. Syntactic features for evaluation of machine translation. In Proceedings of the
ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,
pages 25?32.
Chi-kiu Lo and Dekai Wu. 2013. MEANT at WMT 2013: A tunable, accurate yet inexpensive semantic frame
based MT evaluation metric. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages
422?428, Sofia, Bulgaria, August. Association for Computational Linguistics.
Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu. 2012. Fully automatic semantic mt evaluation. In Pro-
ceedings of the Seventh Workshop on Statistical Machine Translation, pages 243?252, Montr?eal, Canada, June.
Association for Computational Linguistics.
Matou?s Mach?a?cek and Ond?rej Bojar. 2011. Approximating a deep-syntactic metric for mt evaluation and tun-
ing. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 92?98. Association for
Computational Linguistics.
Dennis Mehay and Chris Brew. 2007. BLEUTRE: Flattening Syntactic Dependencies for MT Evaluation. In
Proceedings of the 11th Conference on Theoretical and Methodological Issues in Machine Translation (TMI).
F.J. Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual
Meeting on Association for Computational Linguistics-Volume 1, pages 160?167. Association for Computa-
tional Linguistics.
Karolina Owczarzak, Josef van Genabith, and Andy Way. 2007. Dependency-based automatic evaluation for
machine translation. In Proceedings of the NAACL-HLT 2007/AMTA Workshop on Syntax and Structure in Sta-
tistical Translation, SSST ?07, pages 80?87, Stroudsburg, PA, USA. Association for Computational Linguistics.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002. BLEU: a method for automatic evaluation of machine
translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages
311?318. Association for Computational Linguistics.
W Pirie. 1988. Spearman rank correlation coefficient. Encyclopedia of statistical sciences.
2050
Martin F Porter. 2001. Snowball: A language for stemming algorithms.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010. String-to-dependency statistical machine translation. Compu-
tational Linguistics, 36(4):649?671.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In Proceedings of Association for Machine Translation in the
Americas, pages 223?231.
Matthew Snover, Nitin Madnani, Bonnie J Dorr, and Richard Schwartz. 2009. Fluency, adequacy, or hter?:
exploring different human judgments with a tunable mt metric. In Proceedings of the Fourth Workshop on
Statistical Machine Translation, pages 259?268. Association for Computational Linguistics.
2051
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 435?439,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
DCU Participation in WMT2013 Metrics Task
Xiaofeng Wu?, Hui Yu?,Qun Liu?
?CNGL, Dublin City University, Ireland
?ICT, Chinese Academy of Sciences, China
?{xfwu, qliu}@computing.dcu.ie
?yuhui@ict.ac.cn
Abstract
In this paper, we propose a novel syntac-
tic based MT evaluation metric which only
employs the dependency information in
the source side. Experimental results show
that our method achieves higher correla-
tion with human judgments than BLEU,
TER, HWCM and METEOR at both sen-
tence and system level for all of the four
language pairs in WMT 2010.
1 Introduction
Automatic evaluation plays a more important role
in the evolution of machine translation. At the ear-
liest stage, the automatic evaluation metrics only
use the lexical information, in which, BLEU (Pap-
ineni et al, 2002) is the most popular one. BLEU
is simple and effective. Most of the researchers
regard BLEU as their primary evaluation metric
to develop and compare MT systems. However,
BLEU only employs the lexical information and
cannot adequately reflect the structural level sim-
ilarity. Translation Error Rate (TER) (Snover et
al., 2006) measures the number of edits required to
change the hypothesis into one of the references.
METEOR (Lavie and Agarwal, 2007), which de-
fines loose unigram matching between the hypoth-
esis and the references with the help of stem-
ming and Wordnet-looking-up, is also a lexical
based method and achieves the first-class human-
evaluation-correlation score. AMBER (Chen and
Kuhn, 2011; Chen et al, 2012) incorporates recall,
extra penalties and some text processing variants
on the basis of BLEU. The main weakness of all
the above lexical based methods is that they cannot
adequately reflect the structural level similarity.
To overcome the weakness of the lexical based
methods, many syntactic based metrics were pro-
posed. Liu and Gildea (2005) proposed STM, a
constituent tree based approach, and HWCM, a
dependency tree based approach.
Both of the two methods compute the similar-
ity between the sub-trees of the hypothesis and the
reference. Owczarzak et al(2007a; 2007b; 2007c)
presented a method using the Lexical-Functional
Grammar (LFG) dependency tree. MAXSIM
(Chan and Ng, 2008) and the method proposed
by Zhu et al(2010) also employed the syntac-
tic information in association with lexical infor-
mation.With the syntactic information which can
reflect structural information, the correlation with
the human judgments can be improved to a certain
extent.
As we know that the hypothesis is potentially
noisy, and these errors expand through the parsing
process. Thus the power of syntactic information
could be considerably weakened.
In this paper, we attempt to overcome the short-
coming of the syntactic based methods and pro-
pose a novel dependency based MT evaluation
metric. The proposed metric only employs the ref-
erence dependency tree which contains both the
lexical and syntactic information, leaving the hy-
pothesis side unparsed to avoid the error propaga-
tion. In our metric, F-score is calculated using the
string of hypothesis and the dependency based n-
grams which are extracted from the reference de-
pendency tree.
Experimental results show that our method
achieves higher correlation with human judgments
than BLEU, HWCM, TER and METEOR at both
sentence level and system level for all of the four
language pairs in WMT 2010.
2 Background: HWCM
HWCM is a dependency based metric which ex-
tracts the headword chains, a sequence of words
which corresponds to a path in the dependency
tree, from both the hypothesis and the reference
dependency tree. The score of HWCM is obtained
435
Figure 1: The dependency tree of the reference
Figure 2: The dependency tree of the hypothesis
by formula (1).
HWCM = 1D
D?
n=1
?
g?chainn(hyp) countclip(g)?
g?chainn(hyp) count(g)
(1)
In formula (1), D is the maximum length of the
headword chain. chainn(hyp) denotes the set of
the headword chains with length of n in the tree of
hypothesis. count(g) denotes the number of times
g appears in the headword chain of the hypothe-
sis dependency tree and countclip(g) denotes the
clipped number of times when g appears in the the
headword chain of the reference dependency trees.
Clipped means that the count computed from the
headword chain of the hypothesis tree should not
exceed the maximum number of times when g oc-
curs in headword chain of any single reference
tree. The following are two sentences represent-
ing as reference and hypothesis, and Figure 1 and
Figure 2 are the dependency trees respectively.
reference: It is not for want of trying .
hypothesis: This is not for lack of trying .
In the example above, there are 8 1-word, 7 2-
word and 3 3-word headword chains in the hy-
pothesis dependency tree. The number of 1-word
and 2-word headword chains in the hypothesis tree
which can match their counterparts in the refer-
ence tree is 5 and 4 respectively. The 3-word head-
word chains in the hypothesis dependency tree are
is for lack, for lack of and lack of trying. Due to
the difference in the dependency structures, they
have no matches in the reference side.
3 A Novel Dependency Based MT
Evaluation Method
In this new method, we calculate F-score using the
string of hypothesis and the dep-n-grams which
are extracted from the reference dependency tree.
The new method is named DEPREF since it is
a DEPendency based method only using depen-
dency tree of REference to calculate the F-score.
In DEPREF, after the parsing of the reference sen-
tences, there are three steps below being carried
out. 1) Extracting the dependency based n-gram
(dep-n-gram) in the dependency tree of the refer-
ence. 2) Matching the dep-n-gram with the string
of hypothesis. 3) Obtaining the final score of a hy-
pothesis. The detail description of our method will
be found in paper (Liu et al, 2013) . We only give
the experiment results in this paper.
4 Experiments
Both the sentence level evaluation and the system
level evaluation are conducted to assess the per-
formance of our automatic metric. At the sentence
level evaluation, Kendall?s rank correlation coeffi-
cient ? is used. At the system level evaluation, the
Spearman?s rank correlation coefficient ? is used.
4.1 Data
There are four language pairs in our experiments
including German-to-English, Czech-to-English,
French-to-English and Spanish-to-English, which
are all derived from WMT2010. Each of the
four language pairs consists of 2034 sentences and
the references of the four language pairs are the
same. There are 24 translation systems for French-
to-English, 25 for German-to-English, 12 for
Czech-to-English and 15 for Spanish-to-English.
We parsed the reference into constituent tree by
Berkeley parser and then converted the constituent
tree into dependency tree by Penn2Malt 1. Pre-
sumably, we believe that the performance will be
even better if the dependency trees are manually
revised.
In the experiments, we compare the perfor-
mance of our metric with the widely used lexical
based metrics BLEU, TER, METEOR and a de-
pendency based metric HWCM. In order to make
a fair comparison with METEOR which is known
to perform best when external resources like stem
and synonym are provided, we also provide results
of DEPREF with external resources.
1http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html
436
Metrics Czech-English German-English Spanish-English French-English
BLEU 0.2554 0.2748 0.2805 0.2197
TER 0.2526 0.2907 0.2638 0.2105
HWCM
N=1 0.2067 0.2227 0.2188 0.2022
N=2 0.2587 0.2601 0.2408 0.2399
N=3 0.2526 0.2638 0.2570 0.2498
N=4 0.2453 0.2672 0.2590 0.2436
DEPREF 0.3337 0.3498 0.3190 0.2656
Table 1.A Sentence level correlations of the metrics without external resources.
Metrics Czech-English German-English Spanish-English French-English
METEOR 0.3186 0.3482 0.3258 0.2745
DEPREF 0.3281 0.3606 0.3326 0.2834
Table 1.B Sentence level correlations of the metrics with stemming and synonym.
Table 1: The sentence level correlations with the human judgments for Czech-to-English, German-to-
English, Spanish-to-English and French-to-English. The number in bold is the maximum value in each
column. N stands for the max length of the headword chains in HWCM in Table 1.A.
4.2 Sentence-level Evaluation
Kendall?s rank correlation coefficient ? is em-
ployed to evaluate the correlation of all the MT
evaluation metrics and human judgements at the
sentence level. A higher value of ? means a bet-
ter ranking similarity with the human judges. The
correlation scores of the four language pairs and
the average scores are shown in Table 1.A (without
external resources) and Table 1.B (with stemming
and synonym), Our method performs best when
maximum length of dep-n-gram is set to 3, so we
present only the results when the maximum length
of dep-n-gram equals 3.
From Table 1.A, we can see that all our methods
are far more better than BLEU, TER and HWCM
when there is no external resources applied on all
of the four language pairs. In Table 1.B, external
resources is considered. DEPREF is also better
than METEOR on the four language pairs. From
the comparison between Table 1.A and Table 1.B,
we can conclude that external resources is help-
ful for DEPREF on most of the language pairs.
When comparing DEPREF without external re-
sources with METEOR, we find that DEPREF ob-
tains better results on Czech-English and German-
English.
4.3 System-level Evaluation
We also evaluated the metrics with the human
rankings at the system level to further investigate
the effectiveness of our metrics. The matching of
the words in DEPREF is correlated with the posi-
tion of the words, so the traditional way of com-
puting system level score, like what BLEU does,
is not feasible for DEPREF. Therefore, we resort
to the way of adding the sentence level scores to-
gether to obtain the system level score. At system
level evaluation, we employ Spearman?s rank cor-
relation coefficient ?. The correlations of the four
language pairs and the average scores are shown
in Table 2.A (without external resources) and Ta-
ble 2.B (with stem and synonym).
From Table 2.A, we can see that the correla-
tion of DEPREF is better than BLEU, TER and
HWCM on German-English, Spanish-English and
French-English. On Czech-English, our metric
DEPREF is better than BLEU and TER. In Table
2.B (with stem and synonym), DEPREF obtains
better results than METEOR on all of the language
pairs except one case that DEPREF gets the same
result as METEOR on Czech-English. When com-
paring DEPREF without external resources with
METEOR, we can find that DEPREF gets bet-
ter result than METEOR on Spanish-English and
French-English.
From Table 1 and Table 2, we can conclude
that, DEPREF without external resources can ob-
tain comparable result with METEOR, and DE-
PREF with external resources can obtain better re-
sults than METEOR. The only exception is that at
the system level evaluation, Czech-English?s best
score is abtained by HWCM. Notice that there are
only 12 systems in Czech-English, which means
there are only 12 numbers to be sorted, we believe
437
Metrics Czech-English German-English Spanish-English French-English
BLEU 0.8400 0.8808 0.8681 0.8391
TER 0.7832 0.8923 0.9033 0.8330
HWCM
N=1 0.8392 0.7715 0.7231 0.6730
N=2 0.8671 0.8600 0.7670 0.8026
N=3 0.8811 0.8831 0.8286 0.8209
N=4 0.8811 0.9046 0.8242 0.8148
DEPREF 0.8392 0.9238 0.9604 0.8687
Table 2.A System level correlations of the metrics without external resources.
Metrics Czech-English German-English Spanish-English French-English
METEOR 0.8392 0.9269 0.9516 0.8652
DEPREF 0.8392 0.9331 0.9692 0.8730
Table 2.B System level correlations of the metrics with stemming and synonym.
Table 2: The system level correlations with the human judgments for Czech-to-English, German-to-
English, Spanish-to-English and French-to-English. The number in bold is the maximum value in each
column. N stands for the max length of the headword chains in HWCM in Table 2.A.
the spareness issure is more serious in this case.
5 Conclusion
In this paper, we propose a new automatic MT
evaluation method DEPREF. The experiments are
carried out at both sentence-level and system-level
using four language pairs from WMT 2010. The
experiment results indicate that DEPREF achieves
better correlation than BLEU, HWCM, TER and
METEOR at both sentence level and system level.
References
Yee Seng Chan and Hwee Tou Ng. 2008. Maxsim: A
maximum similarity metric for machine translation
evaluation. In Proceedings of ACL-08: HLT, pages
55?62.
Boxing Chen and Roland Kuhn. 2011. Amber: A
modified bleu, enhanced ranking metric. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 71?77, Edinburgh, Scot-
land, July. Association for Computational Linguis-
tics.
Boxing Chen, Roland Kuhn, and George Foster. 2012.
Improving amber, an mt evaluation metric. In Pro-
ceedings of the Seventh Workshop on Statistical Ma-
chine Translation, WMT ?12, pages 59?63, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Alon Lavie and Abhaya Agarwal. 2007. Meteor: an
automatic metric for mt evaluation with high levels
of correlation with human judgments. In Proceed-
ings of the Second Workshop on Statistical Machine
Translation, StatMT ?07, pages 228?231, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Ding Liu and Daniel Gildea. 2005. Syntactic features
for evaluation of machine translation. In Proceed-
ings of the ACL Workshop on Intrinsic and Extrin-
sic Evaluation Measures for Machine Translation
and/or Summarization, pages 25?32.
Q. Liu, H. Yu, X. Wu, J. Xie, Y. Lu, and S. Lin.
2013. A Novel Dependency Based MT Evaluation
Method. Under Review.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. 2007a. Dependency-based automatic eval-
uation for machine translation. In Proceedings of
the NAACL-HLT 2007/AMTA Workshop on Syntax
and Structure in Statistical Translation, SSST ?07,
pages 80?87, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. 2007b. Evaluating machine translation with
lfg dependencies. Machine Translation, 21(2):95?
119, June.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. 2007c. Labelled dependencies in machine
translation evaluation. In Proceedings of the Sec-
ond Workshop on Statistical Machine Translation,
StatMT ?07, pages 104?111, Stroudsburg, PA, USA.
Association for Computational Linguistics.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In Proceedings of the 40th annual
meeting on association for computational linguis-
tics, pages 311?318. Association for Computational
Linguistics.
438
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas, pages 223?231.
Junguo Zhu, Muyun Yang, Bo Wang, Sheng Li, and
Tiejun Zhao. 2010. All in strings: a powerful string-
based automatic mt evaluation metric with multi-
ple granularities. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics:
Posters, COLING ?10, pages 1533?1540, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
439
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 420?425,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
RED: DCU-CASICT Participation in WMT2014 Metrics Task
Xiaofeng Wu
?
, Hui Yu
?
, Qun Liu
??
?
CNGL Centre for Global Intelligent Content
School of Computing, Dublin City University
Dublin 9, Ireland
?
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
Beijing, China
{xiaofengwu,qliu}@computing.dcu.ie, yuhui@ict.ac.cn
Abstract
Based on the last year?s DCU-CASIST
participation on WMT metrics task, we
further improve our model in the follow-
ing ways: 1) parameter tuning 2) support
languages other than English. We tuned
our system on all the data of WMT 2010,
2012 and 2013. The tuning results as well
as the WMT 2014 test results are reported.
1 Introduction
Automatic evaluation plays a more and more im-
portant role in the evolution of machine transla-
tion. There are roughly two categories can be seen:
namely lexical information based and structural
information based.
1) Lexical information based approaches,
among which, BLEU (?), Translation Error Rate
(TER) (?) and METEOR (?) are the most popular
ones and, with simplicity as their merits, cannot
adequately reflect the structural level similarity.
2) A lot of structural level based methods
have been exploited to overcome the weakness
of the lexical based methods, including Syntactic
Tree Model(STM)(?), a constituent tree based ap-
proach, and Head Word Chain Model(HWCM)(?),
a dependency tree based approach. Both of
the methods compute the similarity between the
sub-trees of the hypothesis and the reference.
Owczarzak et al (?; ?; ?) presented a method
using the Lexical-Functional Grammar (LFG) de-
pendency tree. MAXSIM (?) and the method pro-
posed by Zhu et al (?) also employed the syntac-
tic information in association with lexical infor-
mation. As we know that the hypothesis is poten-
tially noisy, and these errors are enlarged through
the parsing process. Thus the power of syntactic
information could be considerably weakened.
In this paper, based on our attempt on WMT
metrics task 2013 (?), we propose a metrics named
RED ( REference Dependency based automatic
evaluation method). Our metrics employs only the
reference dependency tree which contains both the
lexical and syntactic information, leaving the hy-
pothesis side unparsed to avoid error propagation.
2 Parameter Tuning
In RED, we use F -score as our final score.
F -score is calculated by Formula (1), where ? is
a value between 0 and 1.
F -score =
precision ? recall
? ? precision+ (1? ?) ? recall
(1)
The dependency tree of the reference and the
string of the translation are used to calculate the
precision and recall. In order to calculate preci-
sion, the number of the dep-ngrams (the ngrams
obtained from dependency tree
1
) should be given,
but there is no dependency tree for the transla-
tion in our method. We know that the number
of dep-ngrams has an approximate linear relation-
ship with the length of the sentence, so we use the
length of the translation to replace the number of
the dep-ngrams in the translation dependency tree.
Recall can be calculated directly since we know
the number of the dep-ngrams in the reference.
The precision and recall are computed as follows.
precision
n
=
?
d?D
n
p
(d,hyp)
len
h
recall
n
=
?
d?D
n
p
(d,hyp)
count
n(ref)
D
n
is the set of dep-ngrams with the length of n.
len
h
is the length of the translation. count
n(ref)
is the number of the dep-ngrams with the length
of n in the reference. p
(d,hpy)
is 0 if there is no
match and a positive number between 0 and 1 oth-
erwise(?).
1
We define two types of dep-ngrams: 1) the head word
chain(?); 2) fix-floating(?))
420
The final score of RED is achieved using For-
mula (2), in which a weighted sum of the dep-
ngrams? F -score is calculated. w
ngram
(0 ?
w
ngram
? 1) is the weight of dep-ngram with the
length of n. F -score
n
is the F -score for the dep-
ngrams with the length of n.
RED =
N
?
n=1
(w
ngram
? F -score
n
) (2)
Other parameters to be tuned includes:
? Stem and Synonym
Stem(?) and synonym (WordNet
2
) are intro-
duced with the following three steps. First,
we obtain the alignment with METEOR
Aligner (?) in which not only exact match
but also stem and synonym are considered.
We use stem, synonym and exact match as the
three match modules. Second, the alignment
is used to match for a dep-ngram. We think
the dep-ngram can match with the transla-
tion if the following conditions are satisfied.
1) Each of the words in the dep-ngram has
a matched word in the translation according
to the alignment; 2) The words in dep-ngram
and the matched words in translation appear
in the same order; 3) The matched words
in translation must be continuous if the dep-
ngram is a fixed-floating ngram. At last, the
match module score of a dep-ngram is cal-
culated according to Formula (3). Different
match modules have different effects, so we
give them different weights.
s
mod
=
?
n
i=1
w
m
i
n
, 0 ? w
m
i
? 1 (3)
m
i
is the match module (exact, stem or syn-
onym) of the ith word in a dep-ngram. w
m
i
is the match module weight of the ith word in
a dep-ngram. n is the number of words in a
dep-ngram.
? Paraphrase
When introducing paraphrase, we don?t con-
sider the dependency tree of the reference,
because paraphrases may not be contained in
the head word chain and fixed-floating struc-
tures. Therefore we first obtain the align-
2
http://wordnet.princeton.edu/
ment with METEOR Aligner, only consid-
ering paraphrase; Then, the matched para-
phrases are extracted from the alignment and
defined as paraphrase-ngram. The score of
a paraphrase is 1 ? w
par
, where w
par
is the
weight of paraphrase-ngram.
? Function word
We introduce a parameterw
fun
(0 ? w
fun
?
1) to distinguish function words and content
words. w
fun
is the weight of function words.
The function word score of a dep-ngram or
paraphrase-ngram is computed according to
Formula (4).
s
fun
=
C
fun
? w
fun
+ C
con
? (1? w
fun
)
C
fun
+ C
con
(4)
C
fun
is the number of function words in the
dep-ngram or paraphrase-ngram. C
con
is the
number of content words in the dep-ngram or
paraphrase-ngram.
REDp =
N
?
n=1
(w
ngram
? F -score
pn
) (5)
F -score
p
=
precision
p
? recall
p
? ? precision
p
+ (1? ?) ? recall
p
(6)
precision
p
and recall
P
in Formula (6) are cal-
culated as follows.
precision
p
=
score
par
n
+ score
dep
n
len
h
recall
p
=
score
par
n
+ score
dep
n
count
n
(ref) + count
n
(par)
len
h
is the length of the translation. count
n(ref)
is the number of the dep-ngrams with the length
of n in the reference. count
n
(par) is the num-
ber of paraphrases with length of n in refer-
ence. score
par
n
is the match score of paraphrase-
ngrams with the length of n. score
dep
n
is the
match score of dep-ngrams with the length of n.
score
par
n
and score
dep
n
are calculated as follows.
score
par
n
=
?
par?P
n
(1? w
par
? s
fum
)
score
dep
n
=
?
d?D
n
(p
(d,hyp)
? s
mod
? s
fun
)
421
Metrics BLEU TER HWCM METEOR RED RED-sent RED-syssent
WMT 2010
cs-en 0.255 0.253 0.245 0.319 0.328 0.342 0.342
de-en 0.275 0.291 0.267 0.348 0.361 0.371 0.375
es-en 0.280 0.263 0.259 0.326 0.333 0.344 0.347
fr-en 0.220 0.211 0.244 0.275 0.283 0.329 0.328
ave 0.257 0.254 0.253 0.317 0.326 0.346 0.348
WMT 2012
cs-en 0.157 - 0.158 0.212 0.165 0.218 0.212
de-en 0.191 - 0.207 0.275 0.218 0.283 0.279
es-en 0.189 - 0.203 0.249 0.203 0.255 0.256
fr-en 0.210 - 0.204 0.251 0.221 0.250 0.253
ave 0.186 - 0.193 0.246 0.201 0.251 0.250
WMT 2013
cs-en 0.199 - 0.153 0.265 0.228 0.260 0.256
de-en 0.220 - 0.182 0.293 0.267 0.298 0.297
es-en 0.259 - 0.220 0.324 0.312 0.330 0.326
fr-en 0.224 - 0.194 0.264 0.257 0.267 0.266
ru-en 0.162 - 0.136 0.239 0.200 0.262 0.225
ave 0.212 - 0.177 0.277 0.252 0.283 0.274
WMT 2014
hi-en - - - 0.420 - 0.383 0.386
de-en - - - 0.334 - 0.336 0.338
cs-en - - - 0.282 - 0.283 0.283
fr-en - - - 0.406 - 0.403 0.404
ru-en - - - 0.337 - 0.328 0.329
ave - - - 0.355 - 0.347 0.348
Table 1: Sentence level correlations tuned on WMT 2010, 2012 and 2013; tested on WMT 2014. The
value in bold is the best result in each raw. ave stands for the average result of the language pairs on each
year. RED stands for our untuned system, RED-sent is G.sent.2, RED-syssent is G.sent.1
P
n
is the set of paraphrase-ngrams with the
length of n. D
n
is the set of dep-ngrams with the
length of n.
There are totally nine parameters in RED. We
tried two parameter tuning strategies: Genetic
search algorithm (?) and a Grid search over two
subsets of parameters. The results of Grid search
is more stable, therefore our final submission is
based upon Grid search. We separate the 9 pa-
rameters into two subsets. When searching Sub-
set 1, the parameters in Subset 2 are fixed, and
vice versa. Several iterations are executed to fin-
ish the parameter tuning process. For system
level coefficient score, we set two optimization
goals: G.sys.1) to maximize the sum of Spear-
man?s ? rank correlation coefficient on system
level and Kendall?s ? correlation coefficient on
sentence level or G.sys.2) only the former; For
sentence level coefficient score, we also set two
optimization goals: G.sent.1) the same as G.sys.1,
G.sent.2) only the latter part of G.sys.1.
3 Experiments
In this section we report the experimental results
of RED on the tuning set, which is the combi-
nation of WMT2010, WMT2012 and WMT2013
data set, as well as the test results on the
WMT2014. Both the sentence level evaluation and
the system level evaluation are conducted to assess
the performance of our automatic metrics. At the
sentence level evaluation, Kendall?s rank correla-
tion coefficient ? is used. At the system level eval-
uation, the Spearman?s rank correlation coefficient
? is used.
3.1 Data
There are four language pairs in WMT2010 and
WMT2012 including German-English, Czech-
English, French-English and Spanish-English. For
WMT2013, except these 4 language pairs, there is
also Russian-English. As the test set, WMT 2014
has also five language pairs, but the organizer re-
moved Spanish-English and replace it with Hindi-
English. For into-English tasks, we parsed the En-
422
Metrics BLEU TER HWCM METEOR RED RED-sys RED-syssent
WMT 2010
cs-en 0.840 0.783 0.881 0.839 0.839 0.937 0.881
de-en 0.881 0.892 0.905 0.927 0.933 0.95 0.948
es-en 0.868 0.903 0.824 0.952 0.969 0.965 0.969
fr-en 0.839 0.833 0.815 0.865 0.873 0.875 0.905
ave 0.857 0.852 0.856 0.895 0.903 0.931 0.925
WMT 2012
cs-en 0.886 0.886 0.943 0.657 1 1 1
de-en 0.671 0.624 0.762 0.885 0.759 0.935 0.956
es-en 0.874 0.916 0.937 0.951 0.951 0.965 0.958
fr-en 0.811 0.821 0.818 0.843 0.818 0.871 0.853
ave 0.810 0.811 0.865 0.834 0.882 0.942 0.941
WMT 2013
cs-en 0.936 0.800 0.818 0.964 0.964 0.982 0.972
de-en 0.895 0.833 0.816 0.961 0.951 0.958 0.978
es-en 0.888 0.825 0.755 0.979 0.930 0.979 0.965
fr-en 0.989 0.951 0.940 0.984 0.989 0.995 0.984
ru-en 0.670 0.581 0.360 0.789 0.725 0.847 0.821
ave 0.875 0.798 0.737 0.834 0.935 0.952 0.944
WMT 2014
hi-en 0.956 0.618 - 0.457 - 0.676 0.644
de-en 0.831 0.774 - 0.926 - 0.897 0.909
cs-en 0.908 0.977 - 0.980 - 0.989 0.993
fr-en 0.952 0.952 - 0.975 - 0.981 0.980
ru-en 0.774 0.796 - 0.792 - 0.803 0.797
ave 0.826 0.740 - 0.784 - 0.784 0.770
Table 2: System level correlations tuned on WMT 2010, 2012 and 2013, tested on 2014. The value in
bold is the best result in each raw. ave stands for the average result of the language pairs on each year.
RED stands for our untuned system, RED-sys is G.sys.2, RED-syssent is G.sys.1
Metrics BLEU TER METEOR RED RED-sent RED-syssent
WMT 2010
en-fr 0.33 0.31 0.369 0.338 0.390 0.369
en-de 0.15 0.08 0.166 0.141 0.214 0.185
WMT 2012
en-fr - - 0.26 0.171 0.273 0.266
en-de - - 0.180 0.129 0.200 0.196
WMT 2013
en-fr - - 0.236 0.220 0.237 0.239
en-de - - 0.203 0.185 0.215 0.219
WMT 2014
en-fr - - 0.278 - 0.297 0.293
en-de - - 0.233 - 0.236 0.229
Table 3: Sentence level correlations tuned on WMT 2010, 2012 and 2013, and tested on 2014. The
value in bold is the best result in each raw. RED stands for our untuned system, RED-sent is G.sent.2,
RED-syssent is G.sent.1
glish reference into constituent tree by Berkeley
parser and then converted the constituent tree into
dependency tree by Penn2Malt
3
. We also con-
ducted English-to-French and English-to-German
experiments. The German and French dependency
parser we used is Mate-Tool
4
.
3
http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html
4
https://code.google.com/p/mate-tools/
In the experiments, we compare the perfor-
mance of our metric with the widely used lexi-
cal based metrics BLEU, TER, METEOR and a
dependency based metrics HWCM. The results of
RED are provided with exactly the same external
resources like METEOR. The results of BLEU,
TER and METOER are obtained from official re-
port of WMT 2010, 2012, 2013 and 2014 (if they
423
Metrics BLEU TER METEOR RED RED-sys RED-syssent
WMT 2010
en-fr 0.89 0.89 0.912 0.881 0.932 0.928
en-de 0.66 0.65 0.688 0.657 0.734 0.734
WMT 2012
en-fr 0.80 0.69 0.82 0.639 0.914 0.914
en-de 0.22 0.41 0.180 0.143 0.243 0.243
WMT 2013
en-fr 0.897 0.912 0.924 0.914 0.931 0.936
en-de 0.786 0.854 0.879 0.85 0.8 0.8
WMT 2014
en-fr 0.934 0.953 0.940 - 0.942 0.943
en-de 0.065 0.163 0.128 - 0.047 0.047
Table 4: System level correlations for English to Franch and German, tuned on WMT 2010, 2012 and
2013; tested on WMT 2014. The value in bold is the best result in each raw. RED stands for our untuned
system, RED-sys is G.sys.2, RED-syssent is G.sys.1
are available). The experiments of HWCM is per-
formed by us.
3.2 Sentence-level Evaluation
Kendall?s rank correlation coefficient ? is em-
ployed to evaluate the correlation of all the MT
evaluation metrics and human judgements at the
sentence level. A higher value of ? means a bet-
ter ranking similarity with the human judges. The
correlation scores of are shown in Table 1. Our
method performs best when maximum length of
dep-n-gram is set to 3, so we present only the
results when the maximum length of dep-n-gram
equals 3. From Table 1, we can see that: firstly, pa-
rameter tuning improve performance significantly
on all the three tuning sets; secondly, although
the best scores in the column RED-sent are much
more than RED-syssent, but the outperform is
very small, so by setting these two optimization
goals, RED can achieve comparable performance;
thirdly, without parameter tuning, RED does not
perform well on WMT 2012 and 2013, and even
with parameter tuning, RED does not outperform
METEOR as much as WMT 2010; lastly, on the
test set, RED does not outperform METEOR.
3.3 System-level Evaluation
We also evaluated the RED scores with the human
rankings at the system level to further investigate
the effectiveness of our metrics. The matching of
the words in RED is correlated with the position
of the words, so the traditional way of computing
system level score, like what BLEU does, is not
feasible for RED. Therefore, we resort to the way
of adding the sentence level scores together to ob-
tain the system level score. At system level evalu-
ation, we employ Spearman?s rank correlation co-
efficient ?. The correlations and the average scores
are shown in Table 2.
From Table 2, we can see similar trends as in
Table 1 with the following difference: firstly, with-
out parameter tuning, RED perform comparably
with METEOR on all the three tuning sets; sec-
ondly, on test set, RED also perform comparably
with METEOR. thirdly, RED perform very bad on
Hindi-English, which is a newly introduced task
this year.
3.4 Evaluation of English to Other
Languages
We evaluate both sentence level and system level
score of RED on English to French and German.
The reason we only conduct these two languages
are that the dependency parsers are more reliable
in these two languages. The results are shown in
Table 3 and 4.
From Table 3 and 4 we can see that the tuned
version of RED still perform slightly better than
METEOR with the only exception of system level
en-de.
4 Conclusion
In this paper, based on the last year?s DCU-
CASICT submission, we further improved our
method, namely RED. The experiments are car-
ried out at both sentence-level and system-level
using to-English and from-English corpus. The
experiment results indicate that although RED
achieves better correlation than BLEU, HWCM,
TER and comparably performance with METEOR
at both sentence level and system level, the per-
formance is not stable on all language pairs, such
as the sentence level correlation score of Hindi to
424
English and the system level score of English to
German. To further study the sudden diving of the
performance is our future work.
Acknowledgments
This research is supported by the Science Foun-
dation Ireland (Grant 12/CE/I2267) as part of
the CNGL Centre for Global Intelligent Content
(www.cngl.ie) at Dublin City University and Na-
tional Natural Science Foundation of China (Grant
61379086).
References
Ergun Bic?ici and Deniz Yuret. 2011. Instance selec-
tion for machine translation using feature decay al-
gorithms. In Proceedings of the Sixth Workshop on
Statistical Machine Translation, pages 272?283. As-
sociation for Computational Linguistics.
Yee Seng Chan and Hwee Tou Ng. 2008. Maxsim: A
maximum similarity metric for machine translation
evaluation. In Proceedings of ACL-08: HLT, pages
55?62.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, WMT ?11, pages 85?91, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Alon Lavie and Abhaya Agarwal. 2007. Meteor: an
automatic metric for mt evaluation with high levels
of correlation with human judgments. In Proceed-
ings of the Second Workshop on Statistical Machine
Translation, StatMT ?07, pages 228?231, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Ding Liu and Daniel Gildea. 2005. Syntactic features
for evaluation of machine translation. In Proceed-
ings of the ACL Workshop on Intrinsic and Extrin-
sic Evaluation Measures for Machine Translation
and/or Summarization, pages 25?32.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. 2007a. Dependency-based automatic eval-
uation for machine translation. In Proceedings of
the NAACL-HLT 2007/AMTA Workshop on Syntax
and Structure in Statistical Translation, SSST ?07,
pages 80?87, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. 2007b. Evaluating machine translation with
lfg dependencies. Machine Translation, 21(2):95?
119, June.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. 2007c. Labelled dependencies in machine
translation evaluation. In Proceedings of the Sec-
ond Workshop on Statistical Machine Translation,
StatMT ?07, pages 104?111, Stroudsburg, PA, USA.
Association for Computational Linguistics.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In Proceedings of the 40th annual
meeting on association for computational linguis-
tics, pages 311?318. Association for Computational
Linguistics.
Martin F Porter. 2001. Snowball: A language for stem-
ming algorithms.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010.
String-to-dependency statistical machine transla-
tion. Computational Linguistics, 36(4):649?671.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas, pages 223?231.
Xiaofeng Wu, Hui Yu, and Qun Liu. 2013. Dcu partic-
ipation in wmt2013 metrics task. In Proceedings of
the Eighth Workshop on Statistical Machine Trans-
lation. Association for Computational Linguistics.
H. Yu, X. Wu, Q. Liu, and S. Lin. 2014. RED: A
Reference Dependency Based MT Evaluation Met-
ric. In To be published.
Junguo Zhu, Muyun Yang, Bo Wang, Sheng Li, and
Tiejun Zhao. 2010. All in strings: a powerful string-
based automatic mt evaluation metric with multi-
ple granularities. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics:
Posters, COLING ?10, pages 1533?1540, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
425
