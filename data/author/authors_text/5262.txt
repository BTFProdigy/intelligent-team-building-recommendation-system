Detecting Article Errors Based on
the Mass Count Distinction
Ryo Nagata1, Takahiro Wakana2, Fumito Masui2,
Atsuo Kawai2, and Naoki Isu2
1 Hyogo University of Teacher Education,
942-1 Shimokume, Yashiro, 673-1494 Japan
rnagata@info.hyogo-u.ac.jp
2 Mie University, 1577, Kurimamachiya, Tsu, 514-8507, Japan
{wakana, masui, kawai, isu}@ai.info.mie-u.ac.jp
Abstract. This paper proposes a method for detecting errors concern-
ing article usage and singular/plural usage based on the mass count
distinction. Although the mass count distinction is particularly impor-
tant in detecting these errors, it has been pointed out that it is hard
to make heuristic rules for distinguishing mass and count nouns. To
solve the problem, first, instances of mass and count nouns are auto-
matically collected from a corpus exploiting surface information in the
proposed method. Then, words surrounding the mass (count) instances
are weighted based on their frequencies. Finally, the weighted words are
used for distinguishing mass and count nouns. After distinguishing mass
and count nouns, the above errors can be detected by some heuristic
rules. Experiments show that the proposed method distinguishes mass
and count nouns in the writing of Japanese learners of English with
an accuracy of 93% and that 65% of article errors are detected with a
precision of 70%.
1 Introduction
Although several researchers [1,2,3] have shown that heuristic rules are effective
to detecting grammatical errors in the English writing of second language learn-
ers, it has been pointed out that it is hard to write heuristic rules for detecting
article errors [1]. To be precise, it is hard to write heuristic rules for distinguish-
ing mass and count nouns which are particularly important in detecting article
errors. The major reason for this is that whether a noun is a mass noun or a
count noun greatly depends on its meaning or its surrounding context (Refer to
Pelletier and Schubert [4] for detailed discussion on the mass count distinction).
Article errors are very common among Japanese learners of English [1,5].
This is perhaps because the Japanese language does not have an article system
similar to that of English. Thus, it is favorable for error detecting systems aiming
at Japanese learners of English to be capable of detecting article errors. In other
words, such systems need to somehow distinguish mass and count nouns in the
writing of Japanese learners of English.
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 815?826, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
816 R. Nagata et al
In view of this background, we propose a method for automatically dis-
tinguishing mass and count nouns in context to complement the conventional
heuristic rules for detecting grammatical errors. In this method, mass and count
nouns are distinguished by words surrounding the target noun. Words surround-
ing the target noun are collected from a corpus and weighted based on their
occurrences. The weighted words are used for distinguishing mass and count
nouns in detecting article errors.
Given the mass count distinction, errors concerning singular/plural usage,
which are also common in the writing of Japanese learners of English, can be
detected as well as article errors. For example, given that the noun information
is a mass noun, informations can be detected as an error. Considering this, we
include errors concerning singular/plural usage in the target errors of this paper.
Hereafter, to keep the notation simple, the target errors1 will be referred to as
article errors.
The next section describes related work on distinguishing mass and count
nouns. Section 3 proposes the method for automatically distinguishing mass and
count nouns. Section 4 describes heuristic rules for detecting article errors based
on the mass count distinction given by the proposed method. Section 5 discusses
results of experiments conducted to evaluate the proposed method.
2 Related Work
Several researchers have proposed methods for distinguishing mass and count
nouns in the past. Allan [6] has presented an approach to distinguishing nouns
that are used only as either mass or count based on countability environments.
This distinction is called countability preferences. Baldwin and Bond [7,8] have
proposed several methods for learning the countability preferences from cor-
pora2. Bond and Vatikitis-Bateson [9] have shown that nouns? countability can
be predicted using an ontology3. O?Hara et al [10] have proposed a method for
classifying mass and count nouns based on semantic information (Cyc ontological
types [11]).
Unfortunately, it is difficult to apply the above methods to complement
the conventional heuristic rules for detecting grammatical errors. The meth-
ods [6,7,8,9] are not enough for the purpose, because the majority of nouns can
be used as both mass and count depending on the surrounding context [12].
The methods [9,10] cannot be readily applicable to the purpose because they
work only when semantic information on nouns is given. It would be difficult to
extract semantic information from nouns in the writing of learners of English.
1 The details of the target errors are shown in Sect. 4.
2 They define four way countability preferences: fully countable, uncountable, bipar-
tite, and plural only.
3 They define five way countability preferences: fully countable, strongly countable,
weakly countable, uncountable, and plural only.
Detecting Article Errors Based on the Mass Count Distinction 817
3 Distinguishing Mass and Count Nouns
In the proposed method, decision lists [13] are used to distinguish mass and count
nouns. Generally, decision lists are learned from a set of manually tagged training
data. In the proposed method, however, training data can be automatically
generated from a raw corpus.
Section 3.1 describes how to generate training data. Section 3.2 describes how
to learn decision lists from the training data. Section 3.3 explains the method
for distinguishing mass and count nouns using the decision lists.
3.1 Generating Training Data
To generate training data, first, instances of the target noun that head their
noun phrase (NP) are collected from a corpus with their surrounding words.
This can be simply done by an existing chunker or parser.
Then, the collected instances are tagged with mass or count by tagging rules.
For example, the underlined chicken:
Example 1. ... are a lot of chickens in the roost ...
is tagged as
Example 2. ... are a lot of chickens/count in the roost ...
because it is in plural form.
We have made tagging rules based on linguistic knowledge [6,14,12]. Figure 1
and Table 1 represent the tagging rules. Figure 1 shows the framework of the tag-
ging rules. Eachnode in Fig. 1 represents a question applied to the instance in ques-
tion. For example, the root node reads ?Is the instance in question plural??. Each
leaf represents a result of the classification. For example, if the answer is ?yes? at the
root node, the instance in question is tagged with count. Otherwise, the question
at the lower node is applied and so on. The tagging rules do not classify instances
as mass or count in some cases. These unclassified instances are tagged with the
symbol ???. Unfortunately, they cannot readily be included in training data. For
simplicity of implementation, they are excluded from training data.
Table 1. Words used in the tagging rules
(a) (b) (c)
the indefinite article much the definite article
another less demonstrative adjectives
one enough possessive adjectives
each all interrogative adjectives
? sufficient quantifiers
? ? ?s genitives
818 R. Nagata et al




























yes
yes
yes
yes
no
no
no
no










yes no
COUNT
modified by a little?
?
COUNT
MASS
? MASS
plural?
modified by one of the words
in Table 1(a)?
modified by one of the words
in Table 1(b)?
modified by one of the words
in Table 1(c)?
Fig. 1. Framework of the tagging rules
Note that the tagging rules can be used only for distinguishing mass and
count nouns in texts containing no errors. They cannot be used in the writing
of Japanese learners of English that may contain errors including article errors;
they are based on article and the distinction between singular and plural.
Finally, the tagged instances are stored in a file with their surrounding words.
Each line in the file consists of one of the tagged instances and its surrounding
words as shown in Example 2. The file is used as training data for learning a
decision list.
3.2 Learning Decision Lists
A decision list consists of a set of rules that are learned from training data. Each
rule matches with the template as follows:
If a condition is true, then a decision . (1)
To define the template in the proposed method, let us have a look at the
following two examples:
Example 3. I read the paper .
Example 4. The paper is made of hemp pulp.
The underlined papers in both sentences cannot simply be classified as mass or
count by the tagging rules presented in Sect. 3.1 because both are singular and
modified by the definite article. Nevertheless, we can tell that the former is a
count noun and that the latter is a mass noun from the contexts. This suggests
that the mass count distinction is often determined by words surrounding the
target noun. In Example 3, we can tell that the paper refers to something that
can be read from read , and therefore it is a count noun. Likewise, in Example 4,
the paper refers to a certain substance from made and pulp, and therefore it is
a mass noun.
Detecting Article Errors Based on the Mass Count Distinction 819
Taking this observation into account, we define the template based on words
surrounding the target noun. To formalize the template, we will use a random
variable MC that takes either mass or count to denote that the target noun is
a mass noun or a count noun, respectively. We will also use w and C to denote
a word and a certain context around the target noun, respectively. We define
three types of C: np, ?k, and +k that denote the contexts consisting of the noun
phrase that the target noun heads, k words to the left of the noun phrase, and
k words to its right, respectively. Then the template is formalized by
If a word w appears in the context C of the target noun,
then the target noun is distinguished as MC.
Hereafter, to keep the notation simple, the template is abbreviated to
wC ? MC. (2)
Now rules that match with the template can be learned from the training
data generated in Sect. 3.1. All we need to do is to collect words in C from the
training data. Here, the words in Table 1 are excluded. Also, function words
such as pronouns and auxiliary verbs, cardinal and quasi-cardinal numerals, and
the target noun are excluded. All words are reduced to their morphological stem
and converted entirely to lower case when collected. For example, the following
tagged instance:
Example 5. She ate a piece of fried chicken/mass for dinner.
would give a set of rules that match with the template:
Example 6.
piece
?3 ? mass, frynp ? mass, dinner+3 ? mass
for the target noun chicken being mass when k = 3.
In addition to the above rules, a default rule is defined. It is based on the
target noun itself and used when no other confident rules4 are found in the
decision list for the target noun. It is defined by
t ? MCmajor (3)
where t and MCmajor denote the target noun and the major case of MC in the
training data, respectively. Equation (3) reads ?If the target noun appears, then
it is distinguished as the major case?.
The log-likelihood ratio [15] decides in which order rules in a decision list are
applied to the target noun in novel context. It is defined by
log
p(MC|wC)
p(MC|wC)
(4)
4 Confidence is given by the log-likelihood ratio, which will be defined by (4).
820 R. Nagata et al
where MC is the exclusive event of MC and p(MC|wC) is the probability that
the target noun is used as MC when w appears in the context C. For the default
rule, the log-likelihood ratio is defined by
log
p(MCmajor|t)
p(MCmajor|t)
(5)
It is important to exercise some care in estimating p(MC|wC). In principle,
we could simply count the number of times that w appears in the context C of
the target noun used as MC in the training data. However, this estimate can be
unreliable, when w does not appear often in the context. To solve this problem,
using a smoothing parameter ? [16], p(MC|wC) is estimated by
p(MC|wC) =
f(wC , MC) + ?
f(wC) + m?
(6)
where f(wC) and f(wC , MC) are occurrences of w appearing in C and those in
C of the target noun used as MC, respectively. The constant m is the number
of possible classes, that is, m = 2 (mass or count) in our case, and introduced
to satisfy p(MC|wC) + p(MC|wC) = 1. In this paper, ? is set to 0.5. Likewise,
p(MCmajor|t) is estimated by
p(MCmajor|t) =
f(t, MCmajor) + ?
f(t) + m?
(7)
Rules in a decision list are sorted in descending order by (4) and (5). They are
tested on the target noun in novel context in this order. Rules sorted below the
default rule are discarded because they are never used as we will see in Sect. 3.3.
Table 2 shows part of a decision list for the target noun chicken that was
learned from a subset of the BNC (British National Corpus) [17]. Note that the
rules are divided into two columns for the purpose of illustration in Table 2; in
practice, they are merged into one just as shown in Table 3.
On one hand, we associate the words in the left half with food or cooking.
On the other hand, we associate those in the right half with animals. From
this observation, we can say that chicken is a count noun in the sense of an
animal but a mass noun when referring to food or cooking, which agrees with
the knowledge presented in previous work [18].
Table 2. Rules in a decision list (target noun: chicken, k = 3)
Mass Count
wC Log-likelihood ratio wC Log-likelihood ratio
piece?3 1.49 count?3 1.49
fish?3 1.28 peck+3 1.32
dish?3 1.23 pignp 1.23
skin+3 1.23 run?3 1.23
serve+3 1.18 eggnp 1.18
Detecting Article Errors Based on the Mass Count Distinction 821
Table 3. An example of a decision list (target noun: chicken, k = 3)
Rules Log-likelihood ratio
piece?3 ? mass, count?3 ? count 1.49
peck+3 ? count 1.32
fish?3 ? mass 1.28
dish?3 ? mass, pignp ? count, ? ? ? 1.23
: :
3.3 Distinguishing the Target Noun in Novel Context
To distinguish the target noun in novel context, each rule in the decision list
is tested on it in the sorted order until the first applicable one is found. It is
distinguished by the first applicable one. If two or more applicable rules (e.g.,
?piece
?3 ? mass? and ?count?3 ? count? in Table 3) are found, it is distin-
guished by the major decisions of the two or more applicable rules. For example,
suppose there are three applicable rules and two of them are for mass nouns
(one of them is for count nouns). In this case, the target noun is distinguished as
mass. Ties are broken by rules sorted below the ties. If ties include the default
rule, it is distinguished by the default rule.
The following is an example of distinguishing the target noun chicken. Sup-
pose that the decision list shown in Table 3 and the following sentence are given:
Example 7. I ate a piece of chicken with salad.
It turns out that the first rule ?piece3 ? mass? in Table 3 is applicable to the
instance. Thus, it is distinguished as a mass noun.
It should be noted that rules sorted below the default rule are never used
because the default rule is always applicable to the target noun. This is the
reason why rules sorted below the default rule are discarded as mentioned in
Sect. 3.2.
4 Heuristic Rules for Detecting Article Errors
So far, a method for distinguishing mass and count nouns has been described.
This section describes heuristic rules for detecting article errors based on the
mass count distinction given by the method.
Article errors are detected by the following three steps. Rules in each step
are examined on each target noun in the target text.
In the first step, any mass noun in plural form is detected as an article error.
If an article error is detected in the first step, the rest of the steps are not applied.
In the second step, article errors are detected by the rules described in Ta-
ble 4. The symbol ?? in Table 4 denotes that the combination of the corre-
sponding row and column is erroneous. For example, the third row denotes that
822 R. Nagata et al
Table 4. Detection rules used in the second step
Count Mass
Pattern Singular Plural Singular Plural
{another, each, one} ?   
{a lot of, all, enough, lots of, sufficient}  ? ? 
{much}   ? 
{kind of, sort of, that, this} ?  ? 
{few, many, these,those}  ?  
{countless, numerous, several, various}  ?  
cardinal number except one  ?  
{any, some, no, ?s genitives} ? ? ? 
{interrogative adjectives, possessive adjectives} ? ? ? 
Table 5. Detection rules used in the third step
Singular Plural
a the ? a the ?
Mass  ? ?   
Count ? ?   ? ?
plural count nouns, singular mass nouns, and plural mass nouns that are mod-
ified by another , each, or one are erroneous. The symbol ??? denotes that no
error can be detected by the table. If one of the rules in Table 4 is applied to
the target noun, the third step is not applied.
In the third step, article errors are detected by the rules described in Table 5.
The symbols ?a?, ?the?, and ??? in Table 5 denote the indefinite article, the
definite article, and no article, respectively. The symbols ?? and ??? are the
same as in Table 4. For example, ?? in the third row and second column denotes
that the singular mass nouns modified by the indefinite article is erroneous.
In addition to the three steps, article errors are detected by exceptional rules.
The indefinite article that modifies other than the head noun is judged to be
erroneous (e.g., *an expensive). Likewise, the definite article that modifies other
than the head noun and adjectives is judged to be erroneous (e.g., *the them).
5 Experiments
5.1 Experimental Conditions
A subset of essays5 written by Japanese learners of English were used as the
target texts in the experiments. The subset contained 30 essays (1747 words). A
native speaker of English who was a professional rewriter of English recognized
62 article errors in the subset.
5 http://www.lb.u-tokai.ac.jp/lcorpus/index-j.html
Detecting Article Errors Based on the Mass Count Distinction 823
The British National Corpus (BNC) [17] was used to learn decision lists.
Spoken data were excluded from the corpus. Also, sentences the OAK system6,
which was used to extract NPs from the corpus, failed to analyze were excluded.
After these operations, the size of the corpus approximately amounted to 80
million words (the size of the original BNC is approximately 100 million words).
Hereafter, unless otherwise specified, the corpus will be referred to as the BNC.
Performance of the proposed method was evaluated by accuracy, recall, and
precision. Accuracy is defined by
No. of mass and count nouns distinguished correctly
No. of distinguished target nouns
. (8)
Namely, accuracy measures how accurately the proposed method distinguishes
mass and count nouns. Recall is defined by
No. of article errors detected correctly
No. of article errors in the target essays
. (9)
Recall measures how well the proposed method detects all the article errors in
the target essays. Precision is defined by
No. of article errors detected correctly
No. of detected article errors
. (10)
Precision measures how well the proposed method detects only the article errors
in the target essays.
5.2 Experimental Procedures
First, decision lists for each target noun in the target essays were learned from
the BNC. To extract noun phrases and their head nouns, the OAK system was
used7. An optimal value for k (window size of context) was estimated as follows.
For 23 nouns8 shown in [12] as examples of nouns used as both mass and count
nouns, accuracy was calculated using the BNC and ten-fold cross validation. As
a result of setting k = 3, 10, 50, it turned out that k = 3 maximized the average
accuracy. Following this result, k = 3 was selected in the experiments.
Second, the target nouns were distinguished whether they were mass or count
by the proposed method, and then article errors were detected by the mass
6 OAK System Homepage: http://nlp.cs.nyu.edu/oak/
7 We evaluated how accurately training data can be generated by the tagging rules
using the OAK system. It turned out that the accuracy was 0.997 against 2903
instances of 23 nouns shown in [12] which were randomly selected from the BNC;
1694 of those were tagged with mass or count by the tagging rules and 1689 were
tagged correctly. The five errors were due to the OAK system.
8 In [12], 25 nouns are shown. Of those, two nouns (hate and spelling) were excluded
because they only appeared 12.1 and 15.6 times on average in the ten-fold cross
validation, respectively.
824 R. Nagata et al
count distinction and the heuristic rules described in Sect. 4. As a preprocessing,
spelling errors in the target essays were corrected using a spell checker.
Finally, the results of the detection were compared to those done by the
native-speaker of English. From the comparison, accuracy, recall, and precision
were calculated.
Comparison of performance of the proposed method to that of other meth-
ods is difficult because there is no generally accepted test set or performance
baseline [19]. Given this limitation, we compared performance of the proposed
method to that of Grammarian9, a commercial grammar checker. We also com-
pared it to that of a method that used only the default rules in the decision lists.
We tested them on the same target essays to measure their performances.
5.3 Experimental Results and Discussion
In the experiments, the proposed method distinguished mass and count nouns in
the target essays with accuracy of 0.93. This means that the proposed method
is effective to distinguishing mass and count nouns in the writing of Japanese
learners of English. From this result, we can say that the proposed method can
complement the conventional heuristic rules for detecting grammatical errors.
Because of the high accuracy of the proposed method, it detected more than
half of the article errors in the target essays (Table 6). Of the undetected article
errors (22 out of 62), only four were due to the misclassification of mass and
count nouns by the proposed method. The rest were article errors that were not
detected even if the mass count distinction was given. For example, extra definite
articles such as ?I like *the gardening.? cannot be detected even if whether the
noun ?gardening? is a mass noun or a count noun is given. Therefore, it is
necessary to exploit other sources of information than the mass count distinction
to detect these kinds of article error. For instance, exploiting the relation between
sentences could be used to detect these kinds of article error.
The proposed method outperformed the method using only the default rules
in both recall and precision. This means that words surrounding the target nouns
are good indicators of the mass count distinction. For example, the proposed
method correctly distinguished the target noun place in the phrase beautiful
place as a count noun by ?beautifulnp ? count? and detected an article error
from it whereas the method using only the default rules did not.
Table 6. Experimental results
Method Recall Precision
Proposed 0.65 0.70
Default only 0.60 0.69
Grammarian 0.13 1.00
9 Grammarian Pro X ver. 1.5: http://www.mercury-soft.com/
Detecting Article Errors Based on the Mass Count Distinction 825
In precision, the proposed method was outperformed by Grammarian; since
Grammarian is a commercial grammar checker, it seems to be precision-oriented.
The proposed method made 17 false-positives. Of the 17 false-positives, 13
were due to the misclassification of mass and count nouns by the proposed
method. Especially, the proposed method often made false-positives in idiomatic
phrases (e.g., by plane). This result implies that some methods for handling
idiomatic phrases may improve the performance. Four were due to the chun-
ker used to analyze the target essays. Since the chunker is designed for ana-
lyzing texts that contain no errors, it is possible that a chunker designed for
analyzing texts written by Japanese learners of English reduces this kind of
false-positive.
6 Conclusions
This paper has proposed a method for distinguishing mass and count nouns to
complement the conventional heuristic rules for detecting grammatical errors.
The experiments have shown that the proposed method distinguishes mass and
count nouns with a high accuracy (0.93) and that the recall and precision are
0.65 and 0.70, respectively. From the results, it follows that the proposed method
can complement the conventional heuristic rules for detecting grammatical errors
in the writing of Japanese learners of English.
The experiments have also shown that approximately 35% of article errors
in the target essays are not detected by the mass count distinction. For future
work, we will study methods for detecting the undetected article errors.
Acknowledgments
The authors would like to thank Sekine Satoshi who has developed the OAK
System. The authors also would like to thank three anonymous reviewers for
their advice on this paper.
References
1. Kawai, A., Sugihara, K., Sugie, N.: ASPEC-I: An error detection system for English
composition. IPSJ Journal (in Japanese) 25 (1984) 1072?1079
2. McCoy, K., Pennington, C., Suri, L.: English error correction: A syntactic user
model based on principled ?mal-rule? scoring. In: Proc. 5th International Confer-
ence on User Modeling. (1996) 69?66
3. Schneider, D., McCoy, K.: Recognizing syntactic errors in the writing of second
language learners. In: Proc. 17th International Conference on Computational Lin-
guistics. (1998) 1198?1204
4. Pelletier, F., Schubert, L.: Two theories for computing the logical form of mass
expressions. In: Proc.10th International Conference on Computational Linguistics.
(1984) 108?111
826 R. Nagata et al
5. Izumi, E., Uchimoto, K., Saiga, T., Supnithi, T., Isahara, H.: Automatic error
detection in the Japanese learners? English spoken data. In: Proc. 41st Annual
Meeting of the Association for Computational Linguistics. (2003) 145?148
6. Allan, K.: Nouns and countability. J. Linguistic Society of America 56 (1980)
541?567
7. Baldwin, T., Bond, F.: A plethora of methods for learning English countability.
In: Proc. 2003 Conference on Empirical Methods in Natural Language Processing.
(2003) 73?80
8. Baldwin, T., Bond, F.: Learning the countability of English nouns from corpus
data. In: Proc. 41st Annual Meeting of the Association for Computational Lin-
guistics. (2003) 463?470
9. Bond, F., Vatikiotis-Bateson, C.: Using an ontology to determine English count-
ability. In: Proc. 19th International Conference on Computational Linguistics.
(2002) 99?105
10. O?Hara, T., Salay, N., Witbrock, M., Schneider, D., Aldag, B., Bertolo, S., Panton,
K., Lehmann, F., Curtis, J., Smith, M., Baxter, D., Wagner, P.: Inducing criteria
for mass noun lexical mappings using the Cyc KB, and its extension to WordNet.
In: Proc. 5th International Workshop on Computational Semantics. (2003) 425?441
11. Lenat, D.: CYC: A large-scale investment in knowledge infrastructure. Communi-
cations of the ACM 38 (1995) 33?38
12. Huddleston, R., Pullum, G.: The Cambridge Grammar of the English Language.
Cambridge University Press, Cambridge (2002)
13. Rivest, R.: Learning decision lists. Machine Learning 2 (1987) 229?246
14. Gillon, B.: The lexical semantics of English count and mass nouns. In: Proc. Special
Interest Group on the Lexicon of the Association for Computational Linguistics.
(1996) 51?61
15. Yarowsky, D.: Unsupervised word sense disambiguation rivaling supervised meth-
ods. In: Proc. 33rd Annual Meeting of the Association for Computational Linguis-
tics. (1995) 189?196
16. Yarowsky, D.: Homograph Disambiguation in Speech Synthesis. Springer-Verlag
(1996)
17. Burnard, L.: Users Reference Guide for the British National Corpus. version 1.0.
Oxford University Computing Services, Oxford (1995)
18. Ostler, N., Atkins, B.: Predictable meaning shift: Some linguistic properties of
lexical implication rules. In: Proc. of 1st SIGLEX Workshop on Lexical Semantics
and Knowledge Representation. (1991) 87?100
19. Chodorow, M., Leacock, C.: An unsupervised method for detecting grammatical
errors. In: Proc. 1st Meeting of the North America Chapter of the Association for
Computational Linguistics. (2000) 140?147
Handling Information Access Dialogue through QA Technologies
? A novel challenge for open-domain question answering ?
Tsuneaki Kato
The University of Tokyo
kato@boz.c.u-tokyo.ac.jp
Fumito Masui
Mie University
masui@ai.info.mie-u.ac.jp
Jun?ichi Fukumoto
Ritsumeikan University
fukumoto@media.ritsumei.ac.jp
Noriko Kando
National Institute of Informatics
kando@nii.ac.jp
Abstract
A novel challenge for evaluating open-domain
question answering technologies is proposed.
In this challenge, question answering systems
are supposed to be used interactively to an-
swer a series of related questions, whereas in
the conventional setting, systems answer iso-
lated questions one by one. Such an interac-
tion occurs in the case of gathering informa-
tion for a report on a specific topic, or when
browsing information of interest to the user. In
this paper, first, we explain the design of the
challenge. We then discuss its reality and show
how the capabilities measured by the challenge
are useful and important in practical situations,
and that the difficulty of the challenge is proper
for evaluating the current state of open-domain
question answering technologies.
1 Introduction
Open-domain question answering technologies allow
users to ask a question in natural language and obtain
the answer itself rather than a list of documents that con-
tain the answer. These technologies make it possible
to retrieve information itself rather than merely docu-
ments, and will lead to new styles of information access
(Voorhees, 2000).
The recent research on open-domain question answer-
ing concentrates on answering factoid questions one by
one in isolation from each other. Such systems that an-
swer isolated factoid questions are the most basic level of
question answering technologies, and will lead to more
sophisticated technologies that can be used by profes-
sional reporters and information analysts. On some stage
of that sophistication, a cub reporter writing an article on
a specific topic will be able to translate the main issue ad-
dressed by his report into a set of simpler questions and
then pose those questions to the question answering sys-
tem (Burger et al, 2001).
In addition, there is a relation between multi-document
summarization and question answering. In his lecture,
Eduard Hovy mentioned that multi-document summa-
rization may be able to be reduced into a series of ques-
tion answering (Hovy, 2001). In SUMMAC, an intrinsic
evaluation was conducted which measures the extent to
which a summary provides answers to a set of obligatory
questions on a given topic (Mani et al, 1998). Those sug-
gest such question answering systems that can answer a
series of related questions would surely be a useful aid to
summarization work by human and by machine.
Against this background, question answering systems
need to be able to answer a series of questions, which
have a common topic and/or share a local context. In
this paper, we propose a challenge to measure objectively
and quantitatively such an ability of question answering
systems. We call this challenge QACIAD (Question An-
swering Challenge for Information Access Dialogue). In
this challenge, question answering systems are used in-
teractively to participate in dialogues for accessing infor-
mation. Such information access dialogue occurs such
as when gathering information for a report on a specific
topic, or when browsing information of interest to the
user. Actually, in QACIAD, the interaction is only simu-
lated and systems answer a series of questions in a batch
mode. Although such a simulation may neglect the in-
herent dynamics of dialogue, it is a practical compromise
for objective evaluation and, as a result, the test sets of
the challenge are reusable.
Question answering systems need a wide range of abil-
ities in order to participate in information access dia-
logues (Burger et al, 2001). First, the systems must re-
spond in real time to make interaction possible. They
must also properly interpret a given question within the
context of a specific dialogue, and also be cooperative
by adding appropriate information not mentioned explic-
itly by the user. Moreover, the systems should be able
to pose a question for clarification to resolve ambiguity
concerning the user?s goal and intentions, and to partici-
pate in mixed initiative dialogue by making suggestions
and leading the user toward solving the problem. Among
these various capabilities, QACIAD focuses on the most
fundamental aspect of dialogue, that is, interpreting a
given question within the context of a specific dialogue.
It measures context processing abilities of systems such
as anaphora resolution and ellipses handling.
This paper is organized as follows. The next chap-
ter explains the design of QACIAD. The following three
chapters discuss the reality of the challenge. First, we ex-
plain the process of constructing the test set of the chal-
lenge and introduce the results of a study conducted dur-
ing this process which show the validity of QACIAD.
That is, QACIAD measures valid abilities needed for
participating in information access dialogues. In other
words, the ability measured by the challenge is crucial
to the systems for realizing information access dialogues
for writing reports and summaries. Second, we show the
statistics of pragmatic phenomena in the constructed test
set, and demonstrate that the challenge covers a wide va-
riety of pragmatic phenomena observed in real dialogues.
Third, based on a preliminary analysis of the QACIAD
run, we show that the challenge has a proper difficulty
for evaluating the current state of open-domain question
answering technologies. In the last two chapters, we dis-
cuss problems identified while constructing the test set
and conducting the run, and draw some conclusions.
2 Design of QACIAD
2.1 History
The origin of QACIAD comes from QAC1 (Question
Answering Challenge), one of the tasks of the NTCIR3
workshop conducted from March 2001 through October
2002 (NTCIR, 2001). QACIAD was originally proposed
in March 2001 as the third subtask of QAC1, its formal
run was conducted in May 2002 (Fukumoto et al, 2001;
Fukumoto et al, 2002; Fukumoto et al, 2003), and the re-
sults were reported at the NTCIR3 workshop meeting in
October 2002. The current design of QACIAD reported
in this paper is based on that challenge and is the result
of extensive elaboration. The design of the challenge and
construction of the test set were performed from January
2003 through December 2003. The formal run was con-
ducted in December 2003, as a subtask of QAC2, which
in turn is a task of the NTCIR4 workshop (NTCIR, 2003).
2.2 QAC as a common ground
QAC is a challenge for evaluating question answering
technologies in Japanese. It consists of three subtasks
including QACIAD, and the common scope of those sub-
tasks covers factoid questions that have names as an-
swers. Here, names mean not only names of proper items
(named entities) including date expressions and monetary
values, but also common names such as names of species
and names of body parts. Although the syntactical range
of the names approximately corresponds to compound
nouns, some of them, such as the titles of novels and
movies, deviate from that range. The underlying docu-
ment set consists of two years of articles of two newspa-
pers in QAC2, and one newspaper in QAC1. Using those
documents as the data source, the systems answer various
open-domain questions.
From the outset, QAC has focused on question answer-
ing technologies that can be used as components of larger
intelligent systems and technologies that can handle re-
alistic problems. It persists in requesting exact answers
rather than the text snippets that contain them with the
cost of avoiding handling definition questions and why
questions, because such answers are crucial in order to be
used as inputs to other intelligent systems such as multi-
document summarization systems. Moreover, as such a
situation is considered to be more realistic, the systems
must collect all the possible correct answers and detect
the absence of an answer. Therefore two subtasks, one of
which is QACIAD, request systems to return one list of
answers that contains all and only correct answers, while
the other subtask requests systems to return a ranked list
of possible answers as in TREC-8. In both subtasks, the
presence of answers in the underlying documents is not
guaranteed and the number of answers is not specified, so
these subtasks are similar to the list question task in the
TREC-2003 style rather than the TREC-10 style (TREC,
2003).
2.3 Information access dialogue
Considering scenes in which those question answering
systems participate in a dialogue, we classified informa-
tion access dialogues into the following two categories.
As discussed later, dialogues in a real situation may have
different features in their different portions; the classifi-
cation just shows two extremes.
Gathering Type The user has a concrete objective such
as writing a report and summary on a specific topic,
and asks a system a series of questions all concern-
ing that topic. The dialogue has a common global
topic, and, as a result, each consecutive question
shares a local context.
Browsing Type The user does not have any fixed topic
of interest; the topic of interest varies as the dialogue
progresses. No global topic covers a whole dialogue
but each consecutive question shares a local context.
This paper proposes the design of the challenge, which
can measure the abilities of question answering systems
useful in such dialogues.
2.4 The setting
QACIAD requests participant systems to return all pos-
sible answers to a series of questions, each of which is a
factoid question that has names as answer. This series of
questions and the answers to those questions comprise an
information access dialogue. Two examples of the series
of questions are shown in Figure 1, which were picked
up from our test set discussed in the next chapter. Se-
ries 14 is a series of a typical gathering type, while series
22 of a typical browsing type. In QACIAD, a number of
series (in the case of our test set, 36 series) are given to
the system at once and systems are requested to answer
those series in a batch mode. One series consists of seven
questions on average. The systems must identify the type
to which a series belongs, as it is not given. The systems
need not identify the changes of series, as the boundary
of series is given. Those, however, must not look ahead
to the questions following the one currently being han-
dled. This restriction reflects the fact that QACIAD is a
simulation of interactive use of question answering sys-
tems in dialogues. This restriction, accompanied with the
existence of two types of series, increases the complexity
of the context processing that the systems must employ.
For example, the systems need to identify that series 22
is a browsing type and the focus of the second question is
Yankee stadium rather than New York Yankees without
looking ahead to the following questions. Especially in
Japanese, since anaphora are not realized often and the
definite and indefinite are not clearly distinguished, those
problems are more serious.
2.5 Evaluation measure
In QACIAD, as the systems are requested to return one
list consisting all and only correct answers and the num-
ber of correct answers differs for each question1, mod-
ified F measure is used for the evaluation, which takes
account of both precision and recall. Two modifications
were needed. The first is for the case where an answer
list returned by a system contains the same answer more
than once or answers in different expressions denoting
the same item. In that case, only one answer is regarded
as the correct one, and so the precision of such answer
list decreases. Cases regarded as different expressions
denoting the same item include a person?s name with and
without the position name, variations of foreign name no-
tation, differences of monetary units used, differences of
time zone referred to, and so on. The second modifica-
tion is for questions with no answer. For those questions,
modified F measure is 1.0 if a system returns an empty
list as the answer, and is 0.0 otherwise.
1It is a special case that the number of answers is just one
for all questions shown in Figure 1.
Series 14
When was Seiji Ozawa born?
Where was he born?
Which university did he graduate from?
Who did he study under?
Who recognized him?
Which orchestra was he conducting in 1998?
Which orchestra will he begin to conduct in 2002?
Series 22
Which stadium is home to the New York Yankees?
When was it built?
How many persons? monuments have been
displayed there?
Whose monument was displayed in 1999?
When did he come to Japan on honeymoon?
Who was the bride at that time?
Who often draws pop art using her as a motif?
What company?s can did he often draw also?
Figure 1: Examples of series of questions
The judgment as to whether a given answer is correct
or not takes into account not only an answer itself but
also the accompanying article from which the answer was
extracted. When the article does not validly support the
answer, that is, assessors cannot understand that the an-
swer is the correct one for a given question by reading
that article, it is regarded as incorrect even though the
answer itself is correct. The correctness of an answer
is determined according to the interpretation of a given
question done by human assessors within the given con-
text. The system?s answers to previous questions, and its
understanding of the context from which those answers
were derived, are irrelevant. For example, the correct an-
swer to the second question of series 22, namely when the
Yankee stadium was built, is 1923. If the system wrongly
answers the Shea stadium to the first question, and then
?correctly? answers the second question 1964, the year
when the Shea stadium was built, that answer to the sec-
ond question is not correct. On the other hand, if the
system answers 1923 to the second question with an ap-
propriate article supporting it, that answer is correct no
matter how the system answered the first question.
3 Constructing a Test Set and Usefulness
of the Challenge
We collected and analyzed questions for two purposes.
The first purpose was to establish a methodology for con-
structing a test set based on the design of QACIAD dis-
cussed in the previous chapter. The second purpose was
to confirm the reality of the challenge, that is, to deter-
mine whether it is useful for information access dialogues
to use question answering systems that can answer ques-
tions that have names as answers.
3.1 Collecting questions
Questions were collected as follows. Subjects were pre-
sented various topics, which included persons, organiza-
tions, and events selected from newspaper articles, and
were requested to make questions that ask for informa-
tion to be used in the report on that topic. The report is
supposed to describe facts on a given topic, rather than
contain opinions or prospects on the topic. The ques-
tions are restricted to wh-type questions, and natural se-
ries of questions containing anaphoric expressions and so
on were constructed. The topics were presented in three
different ways: only by a short description of the topic,
which corresponds to the title part of the TREC topic def-
inition; with a short article or the lead of a longer article,
which is representative of that topic and corresponds to
the narrative part of the TREC topic definition; and with
five articles concerning that topic. The number of top-
ics was 60, selected from two years of newspaper arti-
cles. Thirty subjects participated in the experiment. Each
subject made questions for ten topics for each topic pre-
sentation pattern, and was instructed to make around ten
questions for each topic. It is worth noting that the ques-
tions obtained were natural in both content and expres-
sion since in this experiment the subjects did not consider
whether the answers to their questions would be found in
the newspapers, and some subjects did not read the arti-
cles at all.
This time, for the test set construction and preliminary
analysis, 1,033 questions on 40 topics, made by three sub-
jects for each topic with different topic presentation pat-
terns, were used. All of the questions collected are now
being analyzed extensively, especially on the differences
among questions according to the topic presentation pat-
tern.
3.2 Analysis of the questions
Our main concern here is how many of the questions
collected fall into the category of questions that the cur-
rent question answering systems could answer. In other
words, how many of the questions can be answered by a
list of names? In the case the majority of them fall into
such a category, it is realistic to use question answering
systems for information access dialogues and the chal-
lenge on such abilities must be useful.
Table 1 shows the classification of questions according
to the subject asked. In the case where users ask ques-
tions to get information for a report, the number of why
questions is relatively small. Moreover, there were fewer
questions requesting an explanation or definition than ex-
Table 1: Categorization of questions by subject
Asking about
4W (Who, When, Where, What)
70%incl. several types of numerical values
Why 4%
How, for a procedure or method 10%
Definitions, descriptions or explanations 16%
Table 2: Categorization of questions by answer type
Answered in
Numerical values or date expressions 28%
Proper names 22%
Common names (in compound nouns) 8%
Names probably 14%
Clauses, sentences, or texts 28%
pected, probably because questions such as ?Who is Seiji
Ozawa? were decomposed into relatively concrete ques-
tions such as those asking for his birthday and birth place.
However, not all questions that were categorized as
4W questions could be answered by names. For exam-
ple, whereas questions asking where, such as ?Where was
Shakespeare born??, could be answered by a place name,
questions like ?Where do lobsters like to live?? need a
description and not a proper name as the answer. Table 2
shows the result of categorization according to this as-
pect. This categorization was conducted by inspecting
questions only, and some of the questions were hard to
determine decisively whether those could be answered
by names or not, and so were categorized as ?Names
probably?. For example, the question ?Where does the
name ?AIBO? come from?? could be answered by name
if AIBO is an acronym, but there may be a long story as
to its origin. Although such cases happened in other com-
binations of categories, those questions were categorized
into a more complex category as only the border of names
and descriptions are important in the current analysis.
As Table 2 shows, 58% to 72% of questions could be
answered by names. The amount of those questions is al-
most same as the amount of 4W questions, since while
some 4W questions could not be answered by names,
some definition and explanation questions might be able
to be answered by names. The fact that 58% to 72% of
questions for writing reports could be answered by names
demonstrates that question answering systems that an-
swer these questions are useful in such situations.
In addition, the answers to 84% of those 72% questions
could be found by humans from newspaper articles. This
indicates that the setting is realistic where users write re-
ports through interacting with a question answering sys-
tem that uses newspaper articles as its data source.
3.3 Constructing a test set
Using the questions collected, we constructed a test set
as follows. We selected 26 from 40 topics, and chose
appropriate questions and rearranged them for construct-
ing gathering type series. Some of the questions were
edited in order to resolve semantic or pragmatic ambigui-
ties, though we tried to use the questions without modifi-
cation where possible. The topics of the gathering series
consisted of 5 persons, 2 organizations, 11 events, 5 ar-
tifacts, and 3 animals and fishes, among which 4 topics
concerned sets of organizations and events, such as the
big three companies in the beer industry, simultaneous
terrorist attacks, and annual festival events.
Browsing type series were constructed by using some
of the remaining questions as seeds of a sequence and by
adding new questions to create a flow to/from those ques-
tions. For example, series 22 shown in Figure 1 was com-
posed by adding the last four newly created questions to
the first four questions which were collected for the Yan-
kee stadium2. For such seeds, we also used the collection
of questions for evaluating summarization constructed for
TSC (Text Summarization Challenge), another challenge
in the NTCIR workshop (TSC, 2003). Some topics used
for the question collection were the same as the topics
used in TSC also. We made 10 browsing series in this
way.
Finally, the test set constructed this time contained 36
series and 251 questions, with 26 series of the gather-
ing type and 10 series of the browsing type. The average
number of questions in one series was 6.92.
4 Characteristics of the Test Set
This chapter describes the pragmatic characteristics of
the constructed test set. Japanese has four major types
of anaphoric devices: pronouns, zero pronouns, definite
noun phrases, and ellipses. Zero pronouns are very com-
mon in Japanese in which pronouns are not realized on
the surface. As Japanese also has a completely different
determiner system from English, the difference between
definite and indefinite is not apparent on the surface,
and definite noun phrases usually have the same form
as generic noun phrases. Table 3 shows the summary
of such pragmatic phenomena observed in 215 questions
obtained by removing the first one of each series from
the 251 questions in the test set. The total number is
more than 215 as 12 questions contain more than one phe-
nomenon. The sixth question in series 22, ?Who was the
bride at that time?? is an example of such a question with
2The question focus of the first one was changed.
Table 3: Pragmatic phenomena observed in the test set
Type Occurence
Pronouns 76 (21)
Zero pronouns 134 (33)
Definite noun phrases 11 (4)
Ellipses 7
multiple anaphoric expressions. The numbers in paren-
theses show the number of cases in which the referenced
item is an event. As the table indicates, a wide range of
pragmatic phenomena is observed in the test set.
Precisely speaking, the series in the test set can be
characterized through the pragmatic phenomena that they
contain. Gathering type series consist of questions that
have a common referent in a broad sense, which is a
global topic mentioned in the first question of the series.
Strictly gathering type series can be distinguished as a
special case of gathering type series. In those series, all
questions refer exactly to the same item mentioned in the
first question and do not have any other anaphoric ex-
pression. In other words, questions about the common
topic introduced by the first question comprise a whole
sequence. Series 14 in Figure 1 is an example of the
strictly gathering type and all questions can be interpreted
by supplying Seiji Ozawa, who is introduced in the first
question. The test set has 5 series of the strictly gathering
type. Other gathering type series have other two types of
questions. The first type of questions not only has a ref-
erence to the global topic but also refers to other items or
has an ellipsis. The second type of questions has a refer-
ence to a complex item, such as an event that contains the
global topic as its component. Series 20 shown in Fig-
ure 2 is such a series. The third question refers not only
to the global topic, George Mallory, in this case, but also
to his famous phrase. The sixth one refers to an event
George Mallory was concerned in.
On the other hand, the questions of a browsing type
series do not have such a global topic. Sometimes the
referent is the answer of the immediately preceding ques-
tion, such as the fifth, seventh and eighth questions in
series 22 in Figure 1. No series, however, consists solely
of questions that have only a reference to the answer to
the immediately previous questions. All series contain
references to the answers to non-immediately previous
questions or items mentioned in the previous questions,
or more than one pragmatic phenomenon. In series 22,
the third, fourth and sixth questions belong to such a case.
In both types, therefore, the shifting pattern of the fo-
cus is not simple, and so a sophisticated way is needed to
track it. Such focus tracking is indispensable to get cor-
rect answers. Systems cannot even retrieve articles con-
Series 20
In which country was George Mallory born?
What was his famous phrase?
When did he say it?
How old was he when he started climbing
mountains?
On which expedition did he go missing near the
top of Everest?
When did it happen?
At what altitude on Everest was he seen last?
Who found his body?
Figure 2: Another example of series of questions
taining the answer just by accumulating keywords. This
is clear for the browsing type, as an article is unlikely to
mention both the New York Yankees and Campbell soup.
In the gathering type, since the topics mentioned in rela-
tively many articles were chosen, it is not easy to locate
the answer to a question from those articles retrieved us-
ing that topic as the keyword. For example, there are 155
articles mentioning Seiji Ozawa in our document sets, of
which 22 articles mention his move to the Vienna Phil-
harmonic Orchestra, and only two articles also mention
his birthday. An extensive, quantitative analysis is now
in progress.
5 Difficulty of the Challenge and the
Current State of Technologies
Seven teams and fourteen systems participated in the run
using the test set mentioned in the previous chapter con-
ducted in December 2003. In this chapter, based on a
preliminary analysis of the run, the difficulty of the chal-
lenge and the current state of technologies for addressing
the challenge are discussed. The techniques employed in
the participant systems have not yet been published, but
will be published by the NTCIR workshop 4 meeting at
the latest.
Figure 3 shows the mean modified F measures of the
top 10 participant systems. The chart shows the mean
modified F measure of three categories: all of the test set
questions, the questions of the first of each series, and
questions of the second and after. As anticipated, it is
more difficult to answer correctly the questions other than
the first question of each series. This indicates that more
sophisticated context processing is needed.
The mean modified F measure is not high even for the
top systems. This is probably because of not only the
difficulties of context processing but also the difficulties
of returning the list of all and only correct answers. It
is difficult to achieve high recall since some of the ques-
 
 
  
  
  
  
  	

 
    
    
Proceedings of the Interactive Question Answering Workshop at HLT-NAACL 2006, pages 9?16,
New York City, NY, USA. June 2006. c?2006 Association for Computational Linguistics
WoZ Simulation of Interactive Question Answering
Tsuneaki Kato
The University of Tokyo
kato@boz.c.u-tokyo.ac.jp
Fumito Masui
Mie University
masui@ai.info.mie-u.ac.jp
Jun?ichi Fukumoto
Ritsumeikan University
fukumoto@media.ritsumei.ac.jp
Noriko Kando
National Institute of Informatics
kando@nii.ac.jp
Abstract
QACIAD (Question Answering Chal-
lenge for Information Access Dialogue)
is an evaluation framework for measur-
ing interactive question answering (QA)
technologies. It assumes that users inter-
actively collect information using a QA
system for writing a report on a given
topic and evaluates, among other things,
the capabilities needed under such cir-
cumstances. This paper reports an ex-
periment for examining the assumptions
made by QACIAD. In this experiment, di-
alogues under the situation that QACIAD
assumes are collected using WoZ (Wiz-
ard of Oz) simulating, which is frequently
used for collecting dialogue data for de-
signing speech dialogue systems, and then
analyzed. The results indicate that the set-
ting of QACIAD is real and appropriate
and that one of the important capabilities
for future interactive QA systems is pro-
viding cooperative and helpful responses.
1 Introduction
Open-domain question answering (QA) technolo-
gies allow users to ask a question using natural lan-
guage and obtain the answer itself rather than a list
of documents that contain the answer (Voorhees et
al.2000). While early research in this field concen-
trated on answering factoid questions one by one in
an isolated manner, recent research appears to be
moving in several new directions. Using QA sys-
tems in an interactive environment is one of those
directions. A context task was attempted in order
to evaluate the systems? ability to track context for
supporting interactive user sessions at TREC 2001
(Voorhees 2001). Since TREC 2004, questions in
the task have been given as collections of questions
related to common topics, rather than ones that are
isolated and independent of each other (Voorhees
2004). It is important for researchers to recognize
that such a cohesive manner is natural in QA, al-
though the task itself is not intended for evaluating
context processing abilities since, as it is given the
common topic, sophisticated context processing is
not needed.
Such a direction has also been envisaged as a re-
search roadmap, in which QA systems become more
sophisticated and can be used by professional re-
porters and information analysts (Burger et al2001).
At some stage of that sophistication, a young re-
porter writing an article on a specific topic will be
able to translate the main issue into a set of simpler
questions and pose those questions to the QA sys-
tem.
Another research trend in interactive QA has been
observed in several projects that are part of the
ARDA AQUAINT program. These studies concern
scenario-based QA, the aim of which is to handle
non-factoid, explanatory, analytical questions posed
by users with extensive background knowledge. Is-
sues include managing clarification dialogues in or-
der to disambiguate users? intentions and interests;
and question decomposition to obtain simpler and
more tractable questions (Small et al2003)(Hickl et
9
al.2004).
The nature of questions posed by users and pat-
terns of interaction vary depending on the users who
use a QA system and on the environments in which
it is used (Liddy 2002). The user may be a young re-
porter, a trained analyst, or a common man without
special training. Questions can be answered by sim-
ple names and facts, such as those handled in early
TREC conferences (Chai et al2004), or by short
passages retrieved like some systems developed in
the AQUAINT program do (Small et al2003). The
situation in which QA systems are supposed to be
used is an important factor of the system design and
the evaluation must take such a factor into account.
QACIAD (Question Answering Challenge for Infor-
mation Access Dialogue) is an objective and quan-
titative evaluation framework to measure the abil-
ities of QA systems used interactively to partici-
pate in dialogues for accessing information (Kato et
al.2004a)(Kato et al2006). It assumes the situation
in which users interactively collect information us-
ing a QA system for writing a report on a given topic
and evaluates, among other things, the capabilities
needed under such circumstances, i.e. proper inter-
pretation of questions under a given dialogue con-
text; in other words, context processing capabilities
such as anaphora resolution and ellipses handling.
We are interested in examining the assumptions
made by QACIAD, and conducted an experiment,
in which the dialogues under the situation QACIAD
assumes were simulated using the WoZ (Wizard of
Oz) technique (Fraser et al1991) and analyzed. In
WoZ simulation, which is frequently used for col-
lecting dialogue data for designing speech dialogue
systems, dialogues that become possible when a sys-
tem has been developed are simulated by a human, a
WoZ, who plays the role of the system, as well as a
subject who is not informed that a human is behav-
ing as the system and plays the role of its user. An-
alyzing the characteristics of language expressions
and pragmatic devices used by users, we confirm
whether QACIAD is a proper framework for eval-
uating QA systems used in the situation it assumes.
We also examine what functions will be needed for
such QA systems by analyzing intelligent behavior
of the WoZs.
2 QACIAD and the previous study
QACIAD was proposed by Kato et al as a task of
QAC, which is a series of challenges for evaluat-
ing QA technologies in Japanese (Kato et al2004b).
QAC covers factoid questions in the form of com-
plete sentences with interrogative pronouns. Any
answers to those questions should be names. Here,
?names? means not only names of proper items
including date expressions and monetary values
(called ?named entities?), but also common names
such as those of species and body parts. Although
the syntactical range of the names approximately
corresponds to compound nouns, some of them,
such as the titles of novels and movies, deviate from
that range. The underlying document set consists
of newspaper articles. Being given various open-
domain questions, systems are requested to extract
exact answers rather than text snippets that contain
the answers, and to return the answer along with the
newspaper article from which it was extracted. The
article should guarantee the legitimacy of the answer
to a given question.
In QACIAD, which assumes interactive use of
QA systems, systems are requested to answer series
of related questions. The series of questions and the
answers to those questions comprise an information
access dialogue. All questions except the first one of
each series have some anaphoric expressions, which
may be zero pronouns, while each question is in the
range of those handled in QAC. Although the sys-
tems are supposed to participate in dialogue inter-
actively, the interaction is only simulated; systems
answer a series of questions in batch mode. Such
a simulation may neglect the inherent dynamics of
dialogue, as the dialogue evolution is fixed before-
hand and therefore not something that the systems
can control. It is, however, a practical compromise
for an objective evaluation. Since all participants
must answer the same set of questions in the same
context, the results for the same test set are compa-
rable with each other, and the test sets of the task are
reusable by pooling the correct answers.
Systems are requested to return one list consisting
of all and only correct answers. Since the number of
correct answers differs for each question and is not
given, a modified F measure is used for the evalu-
ation, which takes into account both precision and
10
recall.
Two types of series were included in the QA-
CIAD, which correspond to two extremes of infor-
mation access dialogue: a gathering type in which
the user has a concrete objective such as writing a
report and summary on a specific topic, and asks
a system a series of questions related to that topic;
and a browsing type in which the user does not
have any fixed topic of interest. Although the QA-
CIAD assumes that users are interactively collect-
ing information on a given topic and the gathering-
type dialogue mainly occurs under such circum-
stances, browsing-type series are included in the task
based on the observation that even when focusing
on information access dialogue for writing reports,
the systems must handle focus shifts appearing in
browsing-type series. The systems must identify the
type of series, as it is not given, although they need
not identify changes of series, as the boundary is
given. The systems must not look ahead to questions
following the one currently being handled. This re-
striction reflects the fact that the QACIAD is a simu-
lation of interactive use of QA systems in dialogues.
Examples of series of QACIAD are shown in Fig-
ure 1. The original questions are in Japanese and the
figure shows their direct translations.
The evaluation of QA technologies based on QA-
CIAD were conducted twice in QAC2 and QAC3,
which are a part of the NTCIR-4 and NTCIR-5
workshops1, respectively (Kato et al2004b)(Kato et
al.2005). It was one of the three tasks of QAC2 and
the only task of QAC3. On each occasion, several
novel techniques were proposed for interactive QA.
Kato et al conducted an experiment for confirm-
ing the reality and appropriateness of QACIAD, in
which subjects were presented various topics and
were requested to write down series of questions
in Japanese to elicit information for a report on
that topic (Kato et al2004a)(Kato et al2006). The
report was supposed to describe facts on a given
topic, rather than state opinions or prospects on the
topic. The questions were restricted to wh-type
questions, and a natural series of questions that may
contain anaphoric expressions and ellipses was con-
1The NTCIR Workshop is a series of evaluation workshops
designed to enhance research in information access technolo-
gies including information retrieval, QA, text summarization,
extraction, and so on (NTCIR 2006).
Series 30002
What genre does the ?Harry Potter? series belong to?
Who is the author?
Who are the main characters in the series?
When was the first book published?
What was its title?
How many books had been published by 2001?
How many languages has it been translated into?
How many copies have been sold in Japan?
Series 30004
When did Asahi breweries Ltd. start selling their low-malt
beer?
What is the brand name?
How much did it cost?
What brands of low-malt beer were already on the
market at that time?
Which company had the largest share?
How much low-malt beer was sold compared to regular
beer?
Which company made it originally?
Series 30024
Where was Universal Studio Japan constructed?
What is the nearest train station?
Which actor attended the ribbon-cutting ceremony on the
opening day?
Which movie that he featured in was released in the New
Year season of 2001?
What movie starring Kevin Costner was released in the
same season?
What was the subject matter of that movie?
What role did Costner play in that movie?
Figure 1: Examples of Series in QACIAD
structed. Analysis of the question series collected
in such a manner showed that 58% to 75% of ques-
tions for writing reports could be answered by val-
ues or names; a wide range of reference expres-
sions is observed in questions in such a situation;
and sequences of questions are sometimes very com-
plicated and include subdialogues and focus shifts.
From these observations they concluded the reality
and appropriateness of the QACIAD, and validated
the needs of browsing-type series in the task.
One of the objectives of our experiment is to con-
firm these results in a more realistic situation. The
previous experiment setting is far from the actual
situations in which QA systems are used, in which
subjects have to write down their questions without
getting the answers. Using WoZ simulation, it is
confirmed whether or not this difference affected the
result. Moreover, observing the behavior of WoZs,
the capabilities and functions needed for QA sys-
11
tems used in such a situation are investigated.
3 Setting
Referring to the headlines in Mainichi and Yomi-
uri newspapers from 2000 and 2001, we selected
101 topics, which included events, persons, and or-
ganizations. On each of those topics, a summary
of between 800 and 1600 characters long and an
abstract of around 100 characters long were con-
structed using a full text search system on the news-
paper articles.2 Four experts shared this prepara-
tion work. Twenty topics were selected from among
the original 101 on the basis that enough informa-
tion was gathered and compiled into the summary.3
The topics consisted of 5 persons, 2 organizations,
7 events, 5 artifacts, and 1 syndrome, which include
Mr. Colin Powell, Charles, Prince of Wales, An ac-
cident of a Russian nuclear-powerd submarine, Con-
corde, ?Harry Potter?, and Windows XP.
The four experts who shared the preparation work
acted as a WoZ in the simulated QA systems, and
each expert participated in dialogues on the topic for
which she/he wrote the summary and abstract, and
tried to answer questions from users using the sum-
mary, a full text search system, and her/his memory.
Six subjects participated in the experiment and 10
topics were assigned to each subject. That is, each
topic was handled by three subjects. The subjects
behaved as users of simulated QA systems. Pre-
sented with the topic and abstract, the subjects were
directed to think about questions beforehand assum-
ing that they had to write a report on that topic, and
then they participated in an information access dia-
logue with the ?QA system.? The suggested number
of questions for each topic was around 10.
The dialogues were conducted through keyboards
and character displays. The subjects were told that
the system could answer only simple factoid ques-
tions, and the WoZs were instructed not to answer
complicated questions such as asking for reasons
and opinions. No restrictions on the usage of lan-
guage expressions were imposed but were left to
the discretion of the WoZs, though canned phrases
2Those lengths of Japanese texts roughly correspond to 300
to 600 words and 40 words of English text, respectively.
3We prepared 101 topics to meet the additional needs of
other experiments. It is absolutely not the case that only 20
out of 101 were acceptable for our experiments.
USR: What is the title of the first book of the ?Harry
Potter? series?
SYS: ?Harry Potter and the Sorcerer?s Stone.?
USR: How many books were sold?
SYS: The Japanese translation sold 600 thousand copies
in the first half year after its publication.
USR: Who is the author?
SYS: J. K. Rowling.
USR: Who are the main characters (in that series)?
SYS: Harry Potter, Ron Weasley, Hagrid and so on.
USR: What kind of person is Harry Potter?
SYS: A child with a wizard and a witch for parents.
USR: What is the plot?
SYS: It is an adventure story involving Harry Potter and his
friends who enter a school for witchcraft and wizardry.
USR: How many people went to the movie?
SYS: In the US, box-office sales of the movie ?Harry Potter
and the Sorcerer?s Stone? reached 188 million
dollars in the first 10 days after release.
Figure 2: Example of dialogues collected
such as ?Please wait a moment? and ?Sorry, the an-
swer could not be found? were prepared in advance.
The WoZs were also instructed that they could clar-
ify users? questions when they were ambiguous or
vague, and that their answers should be simple but
cooperative and helpful responses were not forbid-
den.
An example of the dialogues collected is shown in
Figure 2. In the figure, SYS stands for utterances of
the QA system simulated by a WoZ and USR repre-
sents that of the user, namely a subject. In the rest of
the paper, these are referred to as system?s utterances
and user?s utterances, respectively.
4 Coding and Results
Excluding meta-utterances for dialogue control such
as ?Please wait a moment? and ?That?s all,? 620
pairs of utterances were collected, of which 22 sys-
tem utterances were for clarification. Among the re-
maining 598 cases, the system gave some answers in
502 cases, and the other 94 utterances were negative
responses: 86 utterances said that the answer could
not found; 10 utterances said that the question was
too complicated or that they could not answer such
type of question.
4.1 Characteristics of questions and answers
The syntactic classification of user utterances and its
distribution is shown in Table 1. The numbers in
12
Table 1: Syntactic classification of user utterances
Syntactic form
Wh-type Question 87.7% (544)
Yes-no Question 9.5% (59)
Imperative (Information request) 2.6% (16)
Declarative (Answer to clarification) 0.2% (1)
Table 2: Categorization of user utterances by subject
Asking about
Who, Where, What 32.5% (201)
When 16.3% (101)
How much/many 16.8% (104)(for several types of numerical values)
Why 6.5% (40)
How (for procedures or situations) 17.0% (105)
Definitions, Descriptions, Explanations 10.8% (67)
Other (Multiple Whs) 0.2% (1)
parentheses are numbers of occurrences. In spite of
the direction of using wh-type questions, more than
10% of utterances are yes-no questions and impera-
tives for requesting information. Most of the user
responses to clarification questions from the sys-
tem are rephrasing of the question concerned; only
one response has a declarative form. Examples of
rephrasing will be shown in section 4.3.
The classification of user questions and requests
according to the subject asked or requested is shown
in Table 2; the classification of system answers ac-
cording to their syntactic and semantic categoriza-
tion is shown in Table 3. In Table 2, the classification
of yes-no questions was estimated based on the in-
formation provided in the helpful responses to those.
The classification in Table 3 was conducted based on
the syntactic and semantic form of the exact part of
the answer itself rather than on whole utterances of
the system. For example, the categorization of the
system utterance ?He was born on April 5, 1935,?
which is the answer to ?When was Mr. Colin Powell
born?? is not a sentence but a date expression.
4.2 Pragmatic phenomena
Japanese has four major types of anaphoric devices:
pronouns, zero pronouns, definite noun phrases,
Table 3: Categorization of user utterances by answer
type
Answered in
Numerical values 14.3% (72)
Date expressions 16.7% (84)
Proper names 22.1% (111)
Common names 8.8% (44)
Compound nouns except names 4.2% (21)
Noun phrases 6.2% (31)
Clauses, sentences, or texts 27.7% (139)
Table 4: Pragmatic phenomena observed
Type
No reference expression 203
Pronouns 14
Zero pronouns 317
Definite noun phrases 104
Ellipses 1
and ellipses. Zero pronouns are very common in
Japanese, in which pronouns are not apparent on the
surface. As Japanese also has a completely different
determiner system from English, the difference be-
tween definite and indefinite is not apparent on the
surface, and definite noun phrases usually have the
same form as generic noun phrases. Table 4 shows
a summary of such pragmatic phenomena observed.
The total number is more than 620 as some utter-
ances contain more than one anaphoric expression.
?How many crew members were in the submarine
when the accident happened?? is an example of such
a question with multiple anaphoric expressions.
Among 203 questions with no reference expres-
sion, 60 questions (30%) are the first one of a series
of utterances on the same topic. The others are di-
vided into two cases. In the first and major case, the
current foci appear literally rather than in pronouns
or definite noun phrases. The second case, which is
relatively minor, is the beginning of and the resume
from a subdialogue. For example, when a user him-
self resumed talking about Charles, Prince of Wales
after a few questions about his children, the user
used a literal expression like ?Who is the long-term
partner of Charles, Prince of Wales??; after talking
13
about the details of the accident, a user asked ?What
is the major characteristic of Concorde??
Out of 437 reference expressions excluding el-
lipses in Table 4, 94 expressions (22%) refer to items
other than the global topic. Moreover, 76 expres-
sions of those (81%) are a sole reference expression
in the utterance. The existence of questions with-
out referring to the global topic, accompanied with
the existence of some questions with no reference
expression, means that the focus shifts in those dia-
logues, and then the local focus can differ from the
global topic.
4.3 Clarifications
As mentioned above, only 22 clarification utterances
were observed in the experiment. They are catego-
rized into those asking about an ambiguous referent
(13 utterances) and those asking about a vague ques-
tion subject (9 utterances). Examples of each case
are shown below.
Clarification asking about an ambiguous referent
USR: When was he [Mr. Colin Powell] installed as head of
security of the United States?
SYS: December 6, 2000.
USR: Who was the president at that time?
SYS: Mr. George W. Bush.
USR: Which political party did he belong to?
SYS: Whose political party are you asking about?
USR: Which political party did Mr. Colin Powell belong to?
Clarification asking about a vague question subject
USR: Who developed Windows XP?
SYS: Are you asking for a company name?
USR: What company developed Windows XP?
4.4 Cooperative and helpful responses
Rather than simple and literal answers to the users?
questions and requests, cooperative and helpful re-
sponses, in which, for example, information not re-
quested directly by a user was additionally provided
(Kaplan 1983), were observed frequently. The re-
sults of analyzing such cooperative and helpful be-
havior in the system responses are shown in Table
5. Questions asking reasons, procedures, situations,
definitions, descriptions or explanations, which are
categorized into the last three categories except oth-
ers in Table 2, were excluded from this analysis,
since for those questions it is difficult to judge objec-
Table 5: Cooperative and helpful behavior observed
Type of behavior
Providing additional information 52 (15.6%)
Providing detailed information 38 (11.4%)
Elaboration of subject 38 (11.4%)
Providing alternative information 47 (14.1%)
Exemplification 28 (8.4%)
Providing speculations 3 (0.9%)
tively whether some information is additional or not
and, in addition, most of those are out of the scope
of QACIAD, which is our main interest. Table 5
shows the number of occurrences and its percentage
among positive responses. Sometimes, more than
one behavior was observed in one utterance. 148
utterances out of 334 positive responses (44%) con-
tained some helpful behavior. The definitions and
manually constructed examples of cooperative and
helpful behavior listed in Table 5 are shown below.
Providing additional information Information
not requested directly from a user is addition-
ally provided.
USR: What year was Mr. Colin Powell born?
SYS: He was born in 1935 in New York.
Providing detailed information More detailed in-
formation than that requested directly by a user
is provided.
USR: What year was Mr. Colin Powell born?
SYS: He was born on April 5, 1935.
Elaboration of subject A relatively vague subject
of a question is elaborated and made concrete.
In the following example, a question with a
vague expression, ?how well? is answered by
a value as a concrete statistic.
USR: How well did the movie do?
SYS: In the US, box-office sales reached 188 million
dollars in the first 10 days after release.
Providing alternative information Rather than
exact information a user requested, alternative
related information is provided. An alternative
may be proposed by relaxing the condition
imposed by the user. In the following example,
attendance figures were altered to box-office
sales.
USR: How many people went to the movie?
14
SYS: In the US, box-office sales reached 188 million
dollars in the first 10 days after release.
Exemplification Instead of a comprehensive list or
abstracted general description, some major in-
stances are listed.
USR: What kinds of character goods are on sale?
SYS: Stuffed owls, uniforms for schools of witchcraft and
wizardry, and so on.
Providing speculation Rather than fact, specula-
tions and opinions are provided, which can be
considered a special case of providing alterna-
tive information. Although there are few cases
of such behavior in Table 5, there were many
such answers in response to questions asking
for reasons and situations.
USR: What exploded?
SYS: It is surmised that torpedoes in the shed at the fore
exploded.
Detecting and correcting erroneous user assump-
tions is a famous helpful behavior discussed in the
literature (Pollack 1989) (Kaplan 1983). Only two
occurrences of such behavior were observed, one of
which is shown below.
USR: How long did it take from the explosion until the
shipwreck?
SYS: The explosion happened while the submarine was going
down, and ...
5 Discussion
The analysis of dialogue collected by the WoZ sim-
ulation showed very similar results to those obtained
in a previous study, in which sequences of questions
were written down by subjects without knowing the
answers to questions. That is, as shown in Table 2,
when users asked questions to get information for a
report, the number of why-questions was relatively
small. Moreover, there were fewer questions re-
questing an explanation or definition than expected,
probably because definition questions such as ?Who
is Mr. Colin Powell?? were decomposed into rela-
tively concrete questions such as those asking for his
birthday and birthplace. The remainder (65%) could
be answered in values and names. Table 3 indicates
that 62% of the questions in our experiments were
answered by values or names. If compound nouns
describing events or situations, which are usually
distinguished from names, are considered to be in
the range of answers, the percentage of answerable
questions reaches 68%. From these results, the set-
ting of QACIAD looks realistic where users write re-
ports interacting with a QA system handling factoid
questions that have values and names as answers.
A wide range of reference expressions is observed
in information access dialogues for writing reports.
Moreover, our study confirmed that those sequences
of questions were sometimes very complicated and
included subdialogues and focus shifts. It is ex-
pected that using an interactive QA system that can
manage those pragmatic phenomena will enable flu-
ent information access dialogue for writing reports.
In this sense, the objective of QACIAD is appropri-
ate.
It could be concluded from these results that the
reality and appropriateness of QACIAD was recon-
firmed in a more realistic situation. And yet suspi-
cion remains that even in our WoZ simulation, the
subjects were not motivated appropriately, as sug-
gested by the lack of dynamic dialogue development
in the example shown in Figure 2. Especially, the
users often gave up too easily when they did not
obtain answers to prepared questions.4 The truth,
however, may be that in the environment of gath-
ering information for writing reports, dynamic dia-
logue development is limited compared to the case
when trained analysts use QA systems for problem
solving. If so, research on this type of QA systems
represents a proper milestone toward interactive QA
systems in a broad sense.
Another finding of our experiment is the impor-
tance of cooperative and helpful responses. Nearly
half of WoZ utterances were not simple literal re-
sponses but included some cooperative and helpful
behavior. This situation contrasts with a relatively
small number of clarification dialogues. The im-
portance of this behavior, which was emphasized
in research on dialogues systems in the 80s and
90s, was reconfirmed in the latest research, although
question-answering technologies were redefined in
the late 90s. Some behavior such as providing alter-
native information could be viewed as a second-best
4It is understandable, however, that there were few rephras-
ing attempts since users were informed that paraphrasing such
as ?What is the population of the US?? to ?How many people
are living in the US?? are usually in vain.
15
strategy of resource-bounded human WoZs. Even
so, it is impossible to eliminate completely the need
for such a strategy by improving core QA technolo-
gies. In addition, intrinsic cooperative and helpful
behavior such as providing additional information
was also often observed. These facts, accompanied
by the fact that such dialogues are perceived as fluent
and felicitous, suggest that the capability to behave
cooperatively and helpfully is essential for interac-
tive QA technologies.
6 Conclusion
Through WoZ simulation, the capabilities and func-
tions needed for interactive QA systems used as a
participant in information access dialogues for writ-
ing reports were examined. The results are compati-
ble with those of previous research, and reconfirmed
the reality and appropriateness of QACIAD. A new
finding of our experiment is the importance of coop-
erative and helpful behavior of QA systems, which
was frequently observed in utterances of the WoZs
who simulated interactive QA systems. Designing
such cooperative functions is indispensable. While
this fact is well known in the context of past research
on dialogue systems, it has been reconfirmed in the
context of the latest interactive QA technologies.
References
Joyce Y. Chai and Rong Jin. 2004. Discource Struc-
ture for Context Question Answering. Proceedings of
HLT-NAACL2004 Workshop on Pragmatics of Ques-
tion Answering, pp. 23-30.
John Burger, Claire Cardie, Vinay Chaudhri, et al 2001.
Issues, Tasks and Program Structures to Roadmap Re-
search in Question & Answering (Q&A)
http://www-nlpir.nist.gov/projrcts/
duc/roadmpping.html.
Norma M. Fraser and G. Nigel Gilbert. 1991. Simulating
speech systems. Computer Speech and Language, Vol
5, No.1, pp. 81-99.
Andrew Hickl, Johm Lehmann, John Williams, and
Sanda Harabagiu. 2004. Experiments with Interactive
Question Answering in Complex Scenarios. Proceed-
ings of HLT-NAACL2004 Workshop on Pragmatics of
Question Answering, pp. 60-69.
Joerrold Kaplan. 1983. Cooperative Responses from a
Portable Natural Language Database Query System.
Michael Brady and Robert C. Berwick eds. Compu-
tational Models of Discourse, pp. 167?208, The MIT
Press.
Tsuneaki Kato, Jun?ichi Fukumoto, Fumito Masui and
Noriko Kando. 2004a. Handling Information Access
Dialogue through QA Technologies ? A novel chal-
lenge for open-domain question answering ?. Pro-
ceedings of HLT-NAACL2004 Workshop on Pragmat-
ics of Question Answering, pp. 70-77.
Tsuneaki Kato, Jun?ici Fukumoto and Fumito Masui.
2004b. Question Answering Challenge for Informa-
tion Access Dialogue ? Overview of NTCIR4 QAC2
Subtask 3 ?. Proceedings of NTCIR-4 Workshop Meet-
ing.
Tsuneaki Kato, Jun?ici Fukumoto and Fumito Masui.
2005. An Overview of NTCIR-5 QAC3. Proceedings
of Fifth NTCIR Workshop Meeting, pp. 361?372.
Tsuneaki Kato, Jun?ici Fukumoto, Fumito Masui and
Noriko Kando. 2006. Are Open-domain Question
Answering Technologies Useful for Information Ac-
cess Dialogues? ? An empirical study and a proposal
of a novel challenge ? ACL Trans. of Asian Language
Information Processing, In Printing.
Elizabeth D. Liddy. 2002. Why are People Asking
these Questions? : A Call for Bringing Situation into
Question-Answering System Evaluation. LREC Work-
shop Proceedings on Question Answering ? Strategy
and Resources, pp. 5-8.
NTCIR Project Home Page. 2006.
http://research.nii.ac.jp/?ntcadm/
index-en.html
Martha E. Pollack. 1989. Plans as Complex Mental At-
titudes. Philip R. Cohen, Jerry Morgan and Martha E.
Pollack eds. Intentions in Communication, pp. 77?103,
The MIT Press.
Sharon Small, Nobuyuki Shimizu, Tomek Strzalkowski,
and Liu Ting 2003. HITIQA: A Data Driven Ap-
proach to Interactive Question Answering: A Prelim-
inary Report AAAI 2003 Spring Symposium New Di-
rections in Question Answering, pp. 94-104.
Ellen M. Voorhees and Dawn M. Tice. 2000. Building a
Question Answering Test Collection the Proceedings
of the 23rd Annual International ACM SIGIR Confer-
ence on Research and Development in Information Re-
trieval, pp. 200 - 207.
Ellen M. Voorhees. 2001. Overview of the TREC 2001
Question Answering Track. Proceedings of TREC
2001.
Ellen M. Voorhees. 2004. Overview of the TREC 2004
Question Answering Track. Proceedings of TREC
2004.
16
Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 59?65,
Baltimore, Maryland, USA. June 27, 2014.
c?2014 Association for Computational Linguistics
Emotive or Non-emotive: That is The Question
Michal Ptaszynski Fumito Masui
Department of Computer Science,
Kitami Institute of Technology
{ptaszynski,f-masui}@
cs.kitami-it.ac.jp
Rafal Rzepka Kenji Araki
Graduate School of Information Science
and Technology, Hokkaido University
{rzepka,araki}@
ist.hokudai.ac.jp
Abstract
In this research we focus on discriminat-
ing between emotive (emotionally loaded)
and non-emotive sentences. We define the
problem from a linguistic point of view as-
suming that emotive sentences stand out
both lexically and grammatically. We
verify this assumption experimentally by
comparing two sets of such sentences in
Japanese. The comparison is based on
words, longer n-grams as well as more so-
phisticated patterns. In the classification
we use a novel unsupervised learning algo-
rithm based on the idea of language com-
binatorics. The method reached results
comparable to the state of the art, while
the fact that it is fully automatic makes it
more efficient and language independent.
1 Introduction
Recently the field of sentiment analysis has at-
tracted great interest. It has become popular to
try different methods to distinguish between sen-
tences loaded with positive and negative senti-
ments. However, a few research focused on a task
more generic, namely, discriminating whether a
sentence is even loaded with emotional content or
not. The difficulty of the task is indicated by three
facts. Firstly, the task has not been widely un-
dertaken. Secondly, in research which addresses
the challenge, the definition of the task is usually
based on subjective ad hoc assumptions. Thirdly,
in research which do tackle the problem in a sys-
tematic way, the results are usually unsatisfactory,
and satisfactory results can be obtained only with
large workload.
We decided to tackle the problem in a standard-
ized and systematic way. We defined emotionally
loaded sentences as those which in linguistics are
described as fulfilling the emotive function of lan-
guage. We assumed that there are repetitive pat-
terns which appear uniquely in emotive sentences.
We performed experiments using a novel unsu-
pervised clustering algorithm based on the idea
of language combinatorics. By using this method
we were also able to minimize human effort and
achieve F-score comparable to the state of the art
with much higher Recall rate.
The outline of the paper is as follows. We
present the background for this research in Section
2. Section 3 describes the language combinatorics
approach which we used to compare emotive and
non-emotive sentences. In section 4 we describe
our dataset and experiment settings. The results of
the experiment are presented in Section 5. Finally
the paper is concluded in Section 6.
2 Background
There are different linguistic means used to in-
form interlocutors of emotional states in an ev-
eryday communication. The emotive meaning is
conveyed verbally and lexically through exclama-
tions (Beijer, 2002; Ono, 2002), hypocoristics (en-
dearments) (Kamei et al., 1996), vulgarities (Crys-
tal, 1989) or, for example in Japanese, through
mimetic expressions (gitaigo) (Baba, 2003). The
function of language realized by such elements of
language conveying emotive meaning is called the
emotive function of language. It was first distin-
guished by B?uhler (1934-1990) in his Sprachthe-
orie as one of three basic functions of language
1
.
B?uhler?s theory was picked up later by Jakobson
(1960), who by distinguishing three other func-
tions laid the grounds for structural linguistics and
communication studies.
2.1 Previous Research
Detecting whether sentences are loaded with emo-
tional content has been undertaken by a number
1
The other two being descriptive and impressive.
59
of researchers, most often as an additional task
in either sentiment analysis (SA) or affect analy-
sis (AA). SA, in great simplification, focuses on
determining whether a language entity (sentence,
document) was written with positive or negative
attitude toward its topic. AA on the other hand
focuses on specifying which exactly emotion type
(joy, anger, etc.) has been conveyed. The fact,
that the task was usually undertaken as a subtask,
influences the way it was formulated. Below we
present some of the most influential works on the
topic, but formulating it in slightly different terms.
Emotional vs. Neutral: Discriminating whe-
ther a sentence is emotional or neutral is to answer
the question of whether it can be interpreted as
produced in an emotional state. This way the task
was studied byMinato et al. (2006), Aman and Sz-
pakowicz (2007) or Neviarouskaya et al. (2011).
Subjective vs. Objective: Discriminating be-
tween subjective and objective sentences is to
say whether the speaker presented the sentence
contents from a first-person-centric perspective or
from no specific perspective. The research formu-
lating the problem this way include e.g, Wiebe et
al. (1999), who classified subjectivity of sentences
using naive Bayes classifier, or later Wilson and
Wiebe (2005). In other research Yu and Hatzi-
vassiloglou (2003) used supervised learning to de-
tect subjectivity and Hatzivassiloglou and Wiebe
(2012) studied the effect of gradable adjectives on
sentence subjectivity.
Emotive vs. Non-emotive: Saying that a sen-
tence is emotive means to specify the linguistic
features of language which where used to produce
a sentence uttered with emphasis. Research that
formulated and tackled the problem this way was
done by, e.g., Ptaszynski et al. (2009).
Each of the above nomenclature implies sim-
ilar, though slightly different assumptions. For
example, a sentence produced without any emo-
tive characteristics (non-emotive) could still im-
ply emotional state in some situations. Also Bing
and Zhang (2012) notice that ?not all subjective
sentences express opinions and those that do are
a subgroup of opinionated sentences.? A compari-
son of the scopes and overlaps of different nomen-
clature is represented in Figure 1. In this research
we formulate the problem similarly to Ptaszynski
et al. (2009), therefore we used their system to
compare with our method.
Figure 1: Comparison of between different
nomenclature used in sentiment analysis research.
3 Language Combinatorics
The idea of language combinatorics (LC) assumes
that patterns with disjoint elements provide bet-
ter results than the usual bag-of-words or n-gram
approach (Ptaszynski et al., 2011). Such patterns
are defined as ordered non-repeated combinations
of sentence elements. They are automatically ex-
tracted by generating all ordered combinations of
sentence elements and verifying their occurrences
within a corpus.
In particular, in every n-element sentence there
is k-number of combination clusters, such as that
1 ? k ? n, where k represents all k-element com-
binations being a subset of n. The number of com-
binations generated for one k-element cluster of
combinations is equal to binomial coefficient, like
in eq. 1. Thus the number of all possible combina-
tions generated for all values of k from the range
of {1, ..., n} is equal to the sum of all combina-
tions from all k-element clusters, like in eq. 2.
(
n
k
)
=
n!
k!(n? k)!
(1)
n
?
k=1
(
n
k
)
=
n!
1!(n? 1)!
+
n!
2!(n? 2)!
+ ... +
n!
n!(n? n)!
= 2
n
? 1
(2)
One problem with combinatorial approach is the
phenomenon of exponential and rapid growth of
function values during combinatorial manipula-
tions, called combinatorial explosion (Krippen-
dorff, 1986). Since this phenomenon causes long
processing time, combinatorial approaches have
been often disregarded. We assumed however,
that it could be dealt with when the algorithm
is optimized to the requirements of the task. In
preliminary experiments Ptaszynski et al. (2011)
used a generic sentence pattern extraction archi-
tecture SPEC to compare the amounts of generated
sophisticated patterns with n-grams, and noticed
that it is not necessary to generate patterns of all
lengths, since the most useful ones usually appear
in the group of 2 to 5 element patterns. Follow-
ing their experience we limit the pattern length in
our research to 6 elements. All non-subsequent el-
60
Table 1: Some examples from the dataset representing emotive and non-emotive sentences close in
content, but differing in emotional load expressed in the sentence (Romanized Japanese / Translation).
emotive non-emotive
Takasugiru kara ne / ?Cause its just too expensive K?ogaku na tame desu. / Due to high cost.
Un, umai, kangeki da. / Oh, so delicious, I?m impressed. Kono kar?e wa karai. / This curry is hot.
Nanto ano hito, kekkon suru rashii yo! / Have you heard? She?s getting married! Ano hito ga kekkon suru rashii desu. / They say she is gatting married.
Ch?o ha ga itee / Oh, how my tooth aches! Ha ga itai / A tooth aches
Sugoku kirei na umi da naaa / Oh, what a beautiful sea! Kirei na umi desu / This is a beautiful sea
ements are also separated with an asterisk (?*?) to
mark disjoint elements.
The weight w
j
of each pattern generated this
way is calculated, according to equation 3, as a
ratio of all occurrences of a pattern in one corpus
O
pos
to the sum of occurrences in two compared
corporaO
pos
+O
neg
. The weights are also normal-
ized to fit in range from +1 (representing purely
emotive patterns) to -1 (representing purely non-
emotive patterns). The normalization is achieved
by subtracting 0.5 from the initial score and mul-
tiplying this intermediate product by 2. The score
of one sentence is calculated as a sum of weights
of patterns found in the sentence, like in eq. 4.
w
j
=
(
O
pos
O
pos
+ O
neg
? 0.5
)
? 2 (3)
score =
?
w
j
, (1 ? w
j
? ?1) (4)
The weight can be further modified by either
? awarding length k, or
? awarding length k and occurrence O.
The list of generated frequent patterns can also be
further modified. When two collections of sen-
tences of opposite features (such as ?emotive vs.
non-emotive?) are compared, a generated list will
contain patterns appearing uniquely on only one
of the sides (e.g. uniquely emotive patterns and
uniquely non-emotive patterns) or in both (am-
biguous patterns). Therefore the pattern list can
be modified by deleting
? all ambiguous patterns, or
? only ambiguous patterns appearing in the same
number on both sides (later called ?zero pat-
terns?, since their weight is equal 0).
Moreover, since a list of patterns will contain both
the sophisticated patterns as well usual n-grams,
the experiments were performed separately for all
patterns and n-grams only. Also, if the initial col-
lection was biased toward one of the sides (sen-
tences of one kind were longer or more numer-
ous), there will be more patterns of a certain sort.
To mitigate this bias, instead of applying a rule of
thumb, the threshold was optimized automatically.
4 Experiments
4.1 Dataset Preparation
In the experiments we used a dataset developed by
Ptaszynski et al. (2009) for the needs of evaluating
their affect analysis system ML-Ask for Japanese
language. The dataset contains 50 emotive and 41
non-emotive sentences. It was created as follows.
Thirty people of different age and social groups
participated in an anonymous survey. Each partic-
ipant was to imagine or remember a conversation
with any person they know and write three sen-
tences from that conversation: one free, one emo-
tive, and one non-emotive. Additionally, the par-
ticipants were asked to make the emotive and non-
emotive sentences as close in content as possible,
so the only difference was whether a sentence was
loaded with emotion or not. The participants also
annotated on their own free utterances whether or
not they were emotive. Some examples from the
dataset are represented in Table 1.
In our research the above dataset was further
preprocessed to make the sentences separable into
elements. We did this in three ways to check how
the preprocessing influences the results. We used
MeCab
2
, a morphological analyzer for Japanese
to preprocess the sentences from the dataset in the
three following ways:
? Tokenization: All words, punctuation marks,
etc. are separated by spaces.
? Parts of speech (POS): Words are replaced
with their representative parts of speech.
? Tokens with POS: Both words and POS infor-
mation is included in one element.
The examples of preprocessing are represented
in Table 2. In theory, the more generalized a sen-
tence is, the less unique patterns it will produce,
but the produced patterns will be more frequent.
This can be explained by comparing tokenized
sentence with its POS representation. For exam-
ple, in the sentence from Table 2 we can see that
a simple phrase kimochi ii (?feeling good?) can be
2
https://code.google.com/p/mecab/
61
Table 2: Three kinds of preprocessing of a sen-
tence in Japanese; N = noun, TOP = topic marker,
ADV = adverbial particle, ADJ = adjective, COP
= copula, EXCL = exclamation mark.
Sentence:
Transliteration: Ky?owanantekimochiiihinanda!
Glossing: Today TOP what pleasant day COP EXCL
Translation: What a pleasant day it is today!
Preprocessing examples
1. Words: Ky?o wa nante kimochi ii hi nanda !
2. POS: N TOP ADV N ADJ N COP EXCL
3.Words+POS: Ky?o[N] wa[TOP] nante[ADV]
kimochi[N] ii[ADJ] hi[N] nanda[COP] ![EXCL]
represented by a POS pattern N ADJ. We can eas-
ily assume that there will be more N ADJ patterns
than kimochi ii, because many word combinations
can be represented as N ADJ. Therefore POS pat-
terns will come in less variety but with higher oc-
currence frequency. By comparing the result of
classification using different preprocessing meth-
ods we can find out whether it is better to represent
sentences as more generalized or as more specific.
4.2 Experiment Setup
The experiment was performed three times, once
for each kind of preprocessing. Each time 10-
fold cross validation was performed and the results
were calculated using Precision (P), Recall (R)
and balanced F-score (F) for each threshold. We
verified which version of the algorithm achieves
the top score within the threshold span. However,
an algorithm could achieve the best score for one
certain threshold, while for others it could perform
poorly. Therefore we also looked at which ver-
sion achieves high scores for the longest threshold
span. This shows which algorithm is more bal-
anced. Finally, we checked the statistical signifi-
cance of the results. We used paired t-test because
the classification results could represent only one
of two classes (emotive or non-emotive). We also
compared the performance to the state of the art,
namely the affect analysis system ML-Ask devel-
oped by Ptaszynski et al. (2009).
5 Results and Discussion
The overall F-score results were generally the best
for the datasets containing in order: both tokens
and POS, tokens only and POS only. The F-
scores for POS-preprocessed sentences revealed
the least constancy. For many cases n-grams
scored higher than all patterns, but almost none of
Table 3: Best results for each version of the
method compared with the ML-Ask system.
ML-Ask
SPEC
tokenized POS token-POS
n-grams patterns n-grams patterns n-grams patterns
Precision 0.80 0.61 0.6 0.68 0.59 0.65 0.64
Recall 0.78 1.00 0.96 0.88 1.00 0.95 0.95
F-score 0.79 0.75 0.74 0.77 0.74 0.77 0.76
the results reached statistical significance. The F-
score results for the tokenized dataset were also
not unequivocal. For higher thresholds patterns
scored higher, while for lower thresholds the re-
sults were similar. The scores were rarely sig-
nificant, utmost at 5% level (p<0.05), however,
in all situations where n-grams visibly scored
higher, the differences were not statistically sig-
nificant. Finally, for the preprocessing including
both tokens and POS information, pattern-based
approach achieved significantly better results (p-
value <0.01 or <0.001). The algorithm reached
its plateau at F-score around 0.73?0.74 for to-
kens and POS separately, and 0.75?0.76 for to-
kens with POS together. In the POS dataset the
elements were more abstracted, while in token-
POS dataset the elements were more specific, pro-
ducing a larger number, but less frequent patterns.
Lower scores for POS dataset could suggest that
the algorithm works better with less abstracted
preprocessing. Examples of F-score comparison
between n-grams and patterns for tokenized and
token-POS datasets are represented in Figures 2
and 3, respectively.
Results for Precision showed similar tenden-
cies. They were the most ambiguous for POS pre-
processing. For the tokenized dataset, although
there always was one or two thresholds for which
n-grams scored higher, scores for patterns were
more balanced, starting with a high score and de-
creasing slowly. As for the token-POS preprocess-
ing patterns achieved higher Precision for most of
the threshold span. The highest Precision of all
was achieved in this dataset by patterns with P =
0.87 for R = 0.50.
As for Recall, the scores were consistent for
all kinds of preprocessing, with higher scores for
patterns within most of the threshold span and
equaling while the threshold decreases. The high-
est scores achieved for each preprocessing for n-
grams and patterns are represented in Table 3.
The affect analysis system ML-Ask (Ptaszynski
et al., 2009) on the same dataset reached F = 0.79,
P = 0.8 and R = 0.78. The results were generally
62
comparable, however slightly higher for ML-Ask
when it comes to P and F-score. R was always bet-
ter for the proposed method. However, ML-Ask is
a system requiring handcrafted lexicons, while our
method is fully automatic, learning the patterns
from data, not needing any particular preparations,
which makes it more efficient.
5.1 Detailed Analysis of Learned Patterns
Within some of the most frequently appearing
emotive patterns there were for example: !
(exclamation mark), n*yo, cha (emotive verb
modification), yo (exclamative sentence ending
particle), ga*yo, n*! or naa (interjection). Some
examples of sentences containing those patterns
are below (patterns underlined). Interestingly,
most elements of those patterns appear in ML-Ask
handcrafted databases, which suggests it could
be possible to improve ML-Ask performance by
extracting additional patterns with SPEC.
Ex. 1. Megane, soko ni atta nda yo. (The glasses
were over there!)
Ex. 2. Uuun, butai ga mienai yo. (Ohh, I cannot
see the stage!)
Ex. 3. Aaa, onaka ga suita yo. (Ohh, I?m so
hungry)
Another advantage of our method is the fact that
it can mark both emotive and non-emotive ele-
ments in sentence, while ML-Ask is designed to
annotate only emotive elements. Some examples
of extracted non-emotive patterns were for exam-
ple: desu, wa*desu, mashi ta, or te*masu. All of
them were patterns described in linguistic litera-
ture as typically non-emotive, consisting in copu-
las (desu), verb endings (masu, mashi ta). Some
sentence examples with those patterns include:
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
-1-0.8-0.6-0.4-0.2 0 0.2 0.4 0.6 0.8 1
sc
or
e
threshold
F-score
all_patterns
ngrams
Figure 2: F-score comparison between n-grams
and patterns for tokenized detaset (p = 0.0209).
Ex. 4. K?ogaku na tame desu. (Due to high cost.)
Ex. 5. Kirei na umi desu (This is a beautiful sea)
Ex. 6. Kyo wa yuki ga futte imasu. (It is snowing
today.)
6 Conclusions and Future Work
We presented a method for automatic extraction
of patterns from emotive sentences. We assumed
emotive sentences are distinguishable both lex-
ically and grammatically and performed experi-
ments to verify this assumption. In the experi-
ments we used a set of emotive and non-emotive
sentences preprocessed in different ways (tokens,
POS, token-POS) The patterns extracted from
sentences were applied to recognize emotionally
loaded sentences.
The algorithm reached its plateau for F-score
around 0.75?0.76 for patterns containing both to-
kens and POS information. Precision for patterns
was balanced, while for n-grams, although occa-
sionally achieving high scores, it was quickly de-
creasing. Recall scores were almost always better
for patterns. The generally lower results for POS-
represented sentences suggest that the algorithm
works better with less abstracted elements.
The results of the proposed method and the af-
fect analysis system ML-Ask were comparable.
ML-Ask achieved better Precision, but lower Re-
call. However, our method is more efficient as
it does not require handcrafted lexicons. More-
over, automatically extracted patterns overlap with
handcrafted databases of ML-Ask, which suggests
it could be possible to improve ML-Ask perfor-
mance with our method. In the near future we plan
to perform experiments on larger datasets, also in
other languages, such as English or Chinese.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
-1-0.8-0.6-0.4-0.2 0 0.2 0.4 0.6 0.8 1
sc
or
e
threshold
F-score
all_patterns
ngrams
Figure 3: F-score comparison for n-grams and pat-
terns for dataset with tokens and POS (p = 0.001).
63
References
Saima Aman and Stan Szpakowicz. 2007. Iden-
tifying expressions of emotion in text. In Pro-
ceedings of the 10th International Conference
on Text, Speech, and Dialogue (TSD-2007),
Lecture Notes in Computer Science (LNCS),
Springer-Verlag.
Junko Baba. 2003. Pragmatic function of Japanese
mimetics in the spoken discourse of varying
emotive intensity levels. Journal of Pragmatics,
Vol. 35, No. 12, pp. 1861-1889, Elsevier.
Fabian Beijer. 2002. The syntax and pragmatics
of exclamations and other expressive/emotional
utterances. Working Papers in Linguistics 2,
The Dept. of English in Lund.
Bing Liu, Lei Zhang. 2012. A survey of opinion
mining and sentiment analysis. In Mining Text
Data, pp. 415-463. Springer.
Karl B?uhler. 1990. Theory of Language. Represen-
tational Function of Language. John Benjamins
Publ. (reprint from Karl B?uhler. Sprachtheorie.
Die Darstellungsfunktion der Sprache, Ullstein,
Frankfurt a. M., Berlin, Wien, 1934.)
David Crystal. 1989. The Cambridge Encyclope-
dia of Language. Cambridge University Press.
Vasileios Hatzivassiloglou and Janice Wiebe. Ef-
fects of adjective orientation and gradability on
sentence subjectivity. In Proceedings of Inter-
national Conference on Computational Linguis-
tics (COLING-2000), pp. 299-305, 2000.
Roman Jakobson. 1960. Closing Statement: Lin-
guistics and Poetics. Style in Language, pp.350-
377, The MIT Press.
Takashi Kamei, Rokuro Kouno and Eiichi Chino
(eds.). 1996. The Sanseido Encyclopedia of Lin-
guistics, Vol. VI, Sanseido.
Klaus Krippendorff. 1986. Combinatorial Explo-
sion, In: Web Dictionary of Cybernetics and
Systems. Princia Cybernetica Web.
Junko Minato, David B. Bracewell, Fuji Ren and
Shingo Kuroiwa. 2006. Statistical Analysis of a
Japanese Emotion Corpus for Natural Language
Processing. LNCS 4114, pp. 924-929.
Alena Neviarouskaya, Helmut Prendinger and
Mitsuru Ishizuka. 2011. Affect analysis model:
novel rule-based approach to affect sensing
from text. Natural Language Engineering, Vol.
17, No. 1 (2011), pp. 95-135.
Hajime Ono. 2002. An emphatic particle DA and
exclamatory sentences in Japanese. University
of California, Irvine.
Christopher Potts and Florian Schwarz. 2008. Ex-
clamatives and heightened emotion: Extracting
pragmatic generalizations from large corpora.
Ms., UMass Amherst.
Michal Ptaszynski, Pawel Dybala, Rafal Rzepka
and Kenji Araki. 2009. Affecting Corpora: Ex-
periments with Automatic Affect Annotation
System - A Case Study of the 2channel Forum
-, In Proceedings of The Conference of the Pa-
cific Association for Computational Linguistics
(PACLING-09), pp. 223-228.
Michal Ptaszynski, Rafal Rzepka, Kenji Araki and
Yoshio Momouchi. 2011. Language combina-
torics: A sentence pattern extraction architec-
ture based on combinatorial explosion. Inter-
national Journal of Computational Linguistics
(IJCL), Vol. 2, Issue 1, pp. 24-36.
Kaori Sasai. 2006. The Structure of Modern
Japanese Exclamatory Sentences: On the Struc-
ture of the Nanto-Type Sentence. Studies in the
Japanese Language, Vol, 2, No. 1, pp. 16-31.
Janyce M. Wiebe, Rebecca F. Bruce and Thomas
P. O?Hara. 1999. Development and use of a
gold-standard data set for subjectivity classi-
fications. In Proceedings of the Association
for Computational Linguistics (ACL-1999), pp.
246-253, 1999.
Theresa Wilson and Janyce Wiebe. 2005. Anno-
tating Attributions and Private States. Proceed-
ings of the ACL Workshop on Frontiers in Cor-
pus Annotation II, pp. 53-60.
Hong Yu and Vasileios Hatzivassiloglou. 2003.
Towards answering opinion questions: separat-
ing facts from opinions and identifying the po-
larity of opinion sentences. In Proceedings of
Conference on Empirical Methods in Natural
Language Processing (EMNLP-2003), pp. 129-
136, 2003.
64
Appendix: Comparison of experiment results in all experiment settings for all three ways of
dataset preprocessing.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
-1-0.8-0.6-0.4-0.2 0 0.2 0.4 0.6 0.8 1
sco
re
threshold
F-score
all_patternszero_deletedambiguous_deletedlength_awardedlength_awarded_zero_deletedlength_awarded_ambiguous_deletedlength_and_occurrence_awardedngramsngrams_zero_deletedngrams_ambiguous_deletedngrams_length_awardedngrams_length_awarded_zero_deletedngrams_length_awarded_ambiguous_deletedngrams_length_and_occurrence_awarded
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
-1-0.8-0.6-0.4-0.2 0 0.2 0.4 0.6 0.8 1
sco
re
threshold
Precision
all_patternszero_deletedambiguous_deletedlength_awardedlength_awarded_zero_deletedlength_awarded_ambiguous_deletedlength_and_occurrence_awardedngramsngrams_zero_deletedngrams_ambiguous_deletedngrams_length_awardedngrams_length_awarded_zero_deletedngrams_length_awarded_ambiguous_deletedngrams_length_and_occurrence_awarded
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
-1-0.8-0.6-0.4-0.2 0 0.2 0.4 0.6 0.8 1
sco
re
threshold
Recall
all_patternszero_deletedambiguous_deletedlength_awardedlength_awarded_zero_deletedlength_awarded_ambiguous_deletedlength_and_occurrence_awardedngramsngrams_zero_deletedngrams_ambiguous_deletedngrams_length_awardedngrams_length_awarded_zero_deletedngrams_length_awarded_ambiguous_deletedngrams_length_and_occurrence_awarded
(a) F-score comparison for tokenized
dataset.
(b) Precision comparison for tok-
enized dataset.
(c) Recall comparison for tokenized
dataset.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
-1-0.8-0.6-0.4-0.2 0 0.2 0.4 0.6 0.8 1
sco
re
threshold
F-score
all_patternszero_deletedambiguous_deletedlength_awardedlength_awarded_zero_deletedlength_awarded_ambiguous_deletedlength_and_occurrence_awardedngramsngrams_zero_deletedngrams_ambiguous_deletedngrams_length_awardedngrams_length_awarded_zero_deletedngrams_length_awarded_ambiguous_deletedngrams_length_and_occurrence_awarded
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
-1-0.8-0.6-0.4-0.2 0 0.2 0.4 0.6 0.8 1
sco
re
threshold
Precision
all_patternszero_deletedambiguous_deletedlength_awardedlength_awarded_zero_deletedlength_awarded_ambiguous_deletedlength_and_occurrence_awardedngramsngrams_zero_deletedngrams_ambiguous_deletedngrams_length_awardedngrams_length_awarded_zero_deletedngrams_length_awarded_ambiguous_deletedngrams_length_and_occurrence_awarded
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
-1-0.8-0.6-0.4-0.2 0 0.2 0.4 0.6 0.8 1
sco
re
threshold
Recall
all_patternszero_deletedambiguous_deletedlength_awardedlength_awarded_zero_deletedlength_awarded_ambiguous_deletedlength_and_occurrence_awardedngramsngrams_zero_deletedngrams_ambiguous_deletedngrams_length_awardedngrams_length_awarded_zero_deletedngrams_length_awarded_ambiguous_deletedngrams_length_and_occurrence_awarded
(d) F-score comparison for POS-
tagged dataset.
(e) Precision comparison for POS-
tagged dataset.
(f) Recall comparison for POS-tagged
dataset.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
-1-0.8-0.6-0.4-0.2 0 0.2 0.4 0.6 0.8 1
sco
re
threshold
F-score
all_patternszero_deletedambiguous_deletedlength_awardedlength_awarded_zero_deletedlength_awarded_ambiguous_deletedlength_and_occurrence_awardedngramsngrams_zero_deletedngrams_ambiguous_deletedngrams_length_awardedngrams_length_awarded_zero_deletedngrams_length_awarded_ambiguous_deletedngrams_length_and_occurrence_awarded
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
-1-0.8-0.6-0.4-0.2 0 0.2 0.4 0.6 0.8 1
sco
re
threshold
Precision
all_patternszero_deletedambiguous_deletedlength_awardedlength_awarded_zero_deletedlength_awarded_ambiguous_deletedlength_and_occurrence_awardedngramsngrams_zero_deletedngrams_ambiguous_deletedngrams_length_awardedngrams_length_awarded_zero_deletedngrams_length_awarded_ambiguous_deletedngrams_length_and_occurrence_awarded
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
-1-0.8-0.6-0.4-0.2 0 0.2 0.4 0.6 0.8 1
sco
re
threshold
Recall
all_patternszero_deletedambiguous_deletedlength_awardedlength_awarded_zero_deletedlength_awarded_ambiguous_deletedlength_and_occurrence_awardedngramsngrams_zero_deletedngrams_ambiguous_deletedngrams_length_awardedngrams_length_awarded_zero_deletedngrams_length_awarded_ambiguous_deletedngrams_length_and_occurrence_awarded
(g) F-score comparison for tokenized
dataset with POS tags.
(h) Precision comparison for tok-
enized dataset with POS tags.
(i) Recall comparison for tokenized
dataset with POS tags.
65
