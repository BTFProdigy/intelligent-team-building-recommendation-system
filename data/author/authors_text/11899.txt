Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 56?59,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Explorations in Automatic Image Annotation using Textual Features
Chee Wee Leong
Computer Science & Engineering
University of North Texas
cheeweeleong@my.unt.edu
Rada Mihalcea
Computer Science & Engineering
University of North Texas
rada@cs.unt.edu
Abstract
In this paper, we report our work on
automatic image annotation by combining
several textual features drawn from the
text surrounding the image. Evaluation of
our system is performed on a dataset of
images and texts collected from the web.
We report our findings through compar-
ative evaluation with two gold standard
collections of manual annotations on the
same dataset.
1 Introduction
Despite the usefulness of images in expressing
ideas, machine understanding of the meaning of
an image remains a daunting task for comput-
ers, as the interplay between the different visual
components of an image does not conform to any
fixed pattern that allows for formal reasoning of
its semantics. Often, the machine interpretation of
the concepts present in an image, known as auto-
matic image annotation, can only be inferred by
its accompanying text or co-occurrence informa-
tion drawn from a large corpus of texts and im-
ages (Li and Wang, 2008; Barnard and Forsyth,
2001). Not surprisingly, humans have the innate
ability to perform this task reliably, but given a
large database of images, manual annotation is
both labor-intensive and time-consuming.
Our work centers around the question : Pro-
vided an image with its associated text, can we
use the text to reliably extract keywords that rel-
evantly describe the image ? Note that we are not
concerned with the generation of keywords for an
image, but rather their extraction from the related
text. Our goal eventually is to automate this task
by leveraging on texts which are naturally occur-
ring with images. In all our experiments, we only
consider the use of nouns as annotation keywords.
2 Related Work
Although automatic image annotation is a popu-
lar task in computer vision and image processing,
there are only a few efforts that leverage on the
multitude of resources available for natural lan-
guage processing to derive robust linguistic based
image annotation models. Most of the work has
posed the annotation task as a classification prob-
lem, such as (Li and Wang, 2008), where images
are annotated using semantic labels associated to
a semantic class.
The most recent work on image annotation us-
ing linguistic features (Feng and Lapata, 2008)
involves implementing an extended version of
the continuous relevance model that is proposed
in (Jeon et al, 2003). The basic idea underlying
their work is to perform annotation of a test im-
age by using keywords shared by similar training
images. Evaluation of their system performance
is based on a dataset collected from the news do-
main (BBC). Unlike them, in this paper, we at-
tempt to perform image annotation on datasets
from unrestricted domains. We are also interested
in extending the work pursued in (Deschacht and
Moens, 2007), where visualness and salience are
proposed as important textual features for discov-
ering named entities present in an image, by ex-
tracting other textual features that can further im-
prove existing image annotation models.
3 Data Sets
We use 180 images collected from the Web, from
pages that have a single image within a specified
size range (width and height of 275 to 1000 pix-
els). 110 images are used for development, while
the remaining 70 are used for test. We create two
different gold standards. The first, termed as Intu-
itive annotation standard (GSintuition), presents a
user with the image in the absence of its associated
text, and asks the user for the 5 most relevant anno-
tations. The second, called Contextual annotation
standard (GScontext), provides the user with a list
of candidates1 for annotation, with the user free to
choose any of the candidates deemed relevant to
describe the image. The user, however, is not con-
1Union of candidates proposed by all systems participat-
ing in the evaluation, including the baseline system
56
strained to choose any candidate word, nor is she
obligated to choose a specified number of candi-
dates. For each image I in the evaluation set, we
invited five users to perform the annotation task
per gold standard. The agreement is 7.78% for
GSintuition and 22.27% for GScontext, where we
consider an annotation that is proposed by three or
more users as one that is being agreed upon. The
union of their inputs forms the set GSintuition(I)
and GScontext(I) respectively. We do not consider
image captions for use as a gold standard here due
to their absence in many of the images ? a ran-
dom sampling of 15 images reveals that 7 of them
lack captions. Contrary to their use as a proxy for
annotation keywords in (Feng and Lapata, 2008;
Deschacht and Moens, 2007), where evaluation is
performed on datasets gleaned from authoritative
news websites, most captions in our dataset are
not guaranteed to be noise free. However, they are
used as part of the text for generating annotations
where they exist.
4 Automatic Image Annotation
We approach the task of automatic image anno-
tation using four methods. Due to the orthogo-
nal nature in their search for keywords, the out-
put for each method is generated separately and
later combined in an unsupervised setting. How-
ever, all four methods perform their discrimination
of words by drawing information exclusively from
the text associated to the image, using no image
visual features in the process.
4.1 Semantic Cloud (Sem)
Every text describes at least one topic that can be
semantically represented by a collection of words.
Intuitively, there exists several ?clouds? of seman-
tically similar words that form several, possibly
overlapping, sets of topics. Our task is to se-
lect the dominant topic put forward in the text,
with the assumption that such a topic is being
represented by the largest set of words. We use
an adapted version of the K-means clustering ap-
proach, which attempts to find natural ?clusters?
of words in the text by grouping words with a com-
mon centroid. Each centroid is the semantic cen-
ter of the group of words and the distance between
each centroid and the words are approximated by
ESA (Gabrilovich and Markovitch, 2007). Fur-
ther, we perform our experiments with the follow-
ing assumptions : (1) To maximize recall, we as-
sume that there are only two topics in every text.
(2) Every word or collocation in the text must be
classified under one of these two topics, but not
both. In cases, where there is a tie, the classi-
fication is chosen randomly. For each dominant
cluster extracted, we rank the words in decreasing
order of their ESA distance to the centroid. To-
gether, they represent the gist of the topic and are
used as a set of candidates for labeling the image.
4.2 Lexical Distance (Lex)
Words that are lexically close to the picture in the
document are generally well-suited for annotat-
ing the image. The assumption is drawn from the
observation that the caption of an image is usu-
ally located close to the image itself. For images
without captions, we consider words surrounding
the image as possible candidates for annotation.
Whenever a word appears multiple times within
the text, its occurrence closest to the image is used
to calculate the lexical distance. To discriminate
against general words, we weigh the Lexical Dis-
tance Score (LDS) for each word by its tf * idf
score as in the equation shown below :
LDS(Wi) = tf * idf(Wi)/LS(Wi) (1)
where LS(Wi) is the minimum lexical distance of
Wi to the image, and idf is calculated using counts
from the British National Corpus.
4.3 Saliency (Sal)
To our knowledge, all word similarity metrics pro-
vide a symmetric score between a pair of words
w1 and w2 to indicate their semantic similarity.
Intuitively, this is not always the case. In psy-
cholinguistics terms, uttering w1 may bring into
mind w2, while the appearance of w2 without any
contextual clues may not associate with w1 at all.
Thus, the degree of similarity of w1 with respect
to w2 should be separated from that of w2 with
respect to w1. We use a directional measure of
similarity:
DSim(wi, wj) =
Cij
Ci
? Sim(wi, wj) (2)
where Cij is the count of articles in Wikipedia
containing words wi and wj , Ci is the count of ar-
ticles containing words wi, and Sim(wi, wj) is the
cosine similarity of the ESA vectors representing
the two words. The directional weight (Cij /Ci)
amounts to the degree of association of wi with re-
spect to wj . Using the directional inferential sim-
ilarity scores as directed edges and distinct words
as vertices, we obtain a graph for each text. The
directed edges denotes the idea of ?recommenda-
tion? where we sayw1 recommendsw2 if and only
if there is a directed edge from w1 to w2, with
the weight of the recommendation being the di-
rectional similarity score. By employing the graph
iteration algorithm proposed in (Mihalcea and Ta-
rau, 2004), we can compute the rank of a vertex in
57
the entire graph. The output generated is a sorted
list of words in decreasing order of their ranks,
which serves as a list of candidates for annotating
the image. Note that the top-ranked word must in-
fer some or all of the words in the text.
Table 1: An image annotation example
Sem symptoms, treatment, medical treat-
ment, medical care, sore throat, fluids,
cough, tonsils, strep throat, swab
Lex strep throat, cotton swab, lymph nodes,
rheumatic fever, swab, strep, fever, sore
throat, lab, scarlet fever
Sal strep, swab, nemours, teens, ginger ale,
grapefruit juice, sore, antibiotics, kids,
fever
Pic throat, runny nose, strep throat, sore
throat, hand washing, orange juice, 24
hours, medical care, beverages, lymph
nodes
Combined treatment, cough, tonsils, swab, fluids,
strep throat
Doc Title strep throat
tf * idf strep, throat, antibiotics, symptoms,
child, swab, fever, treatment, teens,
nemours
GScontext medical care, medical treatment, doc-
tor, cotton swab, treatment, tonsils, sore
throat, swab, throat, sore, sample, symp-
toms, throat, cough, medication, bacte-
ria, lab, scarlet fever, strep throat, teens,
culture, kids, child, streptococcus, doctor,
strep
GSintuition tongue, depressor, exam, eyes, cartoon,
doctor, health, child, tonsils, fingers, hair,
mouth, dentist, sample, cloth, curly, tip,
examine
4.4 Picturable Cues (Pic)
Some words are more picturable than others. For
instance, it is easy to find a picture that describes
the word banana than another word paradigm.
Clearly, picturable words in the associated text of
an image are natural candidates for labeling it. Un-
like the work in (Deschacht and Moens, 2007),
we employ a corpus-based approach to compute
word to word similarity. We collect a list of 200
manually-annotated words2 that are deemed to be
picturable by humans. We use this list of words
as our set of seed words, Sseed. We then iterate a
bootstrapping process where each word in the text
is compared to every word in the set of seed words,
and any word having a maximum ESA score of
2http://simple.wikipedia.org/wiki/Wikipedia:
Basic English picture wordlist
greater than 0.95 is added to Sseed. Similarly, the
maximum ESA score of each word over all Sseed
words is recorded. This is the picturability score
of the word.
5 Experiments and Evaluations
We investigate the performance of each of the four
annotation methods individually, followed by a
combined approach using all of them. In the in-
dividual setting, we simply obtain the set of candi-
dates proposed by each method as possible anno-
tation keywords for the image. In the unsupervised
combined setting, only the labels proposed by all
individual methods are selected, and listed in re-
verse order of their combined rankings.
We allow each system to produce a re-ranked
list of top k words to be the final annotations for a
given image. A system can discretionary generate
less (but not more) than k words that is appropri-
ate to its confidence level. Similar to (Feng and
Lapata, 2008), we evaluate our systems using pre-
cision, recall and F-measure for k=10, k=15 and
k=20 words.
For comparison, we also implemented two
baselines systems: tf * idf and Doc Title, which
simply takes all the words in the title of the
web page and uses them as annotation labels for
the image. In the absence of a document title,
we use the first sentence in the document. The
results for GSintuition and GScontext are tabu-
lated in Tables 2 and 3 respectively. We fur-
ther illustrate our results with an annotation ex-
ample (an image taken from a webpage discussing
strep throat among teens) in Table 1. Words in
bold matches GScontext while those underlined
matches GSintuition.
6 Discussion
As observed, the system implementing the Se-
mantic Cloud method significantly outperforms
the rest of the systems in terms of recall and F-
measure using the gold standard GSintuition. The
unsupervised combined system yields the high-
est precision at 16.26% (at k=10,15,20) but at
a low recall of 1.52%. Surprisingly, the base-
line system using tf * idf performs relatively well
across all the experiments using the gold stan-
dard GSintuition, outperforming two of our pro-
posed methods Salience (Sal) and Picturability
Cues (Pic) consistently for all k values. The other
baseline, Doc Title, records the highest precision
at 16.33% at k=10 with a low recall of 3.81%. For
k=15 and k=20, the F-measure scored 6.31 and
6.29 respectively, both lower than that scored by
tf * idf.
58
Table 2: Results for Automatic Image Annotation for GSintuition. In both Tables 2 and 3, statistically
significant results are marked with ?(measured against Doc Title, p<0.05, paired t-test), ?(measured
against tf*idf, p<0.1, paired t-test), ?(measured against tf*idf, p<0.05, paired t-test).
GSintuition
k=10 k=15 k=20
P R F P R F P R F
Sem 11.71 6.25? 8.15 11.31 8.91?? 9.97?? 10.36 9.45?? 9.88??
Lex 9.00 4.80 6.26 7.33 5.86 6.51 7.14 7.62 7.37
Sal 4.57 2.43 3.17 6.28 5.03 5.59 6.38 6.78 6.57
Pic 7.14 3.81 4.97 6.09 4.87 5.41 5.64 6.02 5.82
Combined 16.26 1.52 2.78 16.26? 1.52 2.78 16.26? 1.52 2.78
Doc Title 16.33 3.81 6.18 15.56 3.96 6.31 15.33 3.96 6.29
tf * idf 9.71 5.18 6.76 8.28 6.63 7.36 7.14 7.62 7.37
Table 3: Results for Automatic Image Annotation for GScontext
GScontext
k=10 k=15 k=20
P R F P R F P R F
Sem 71.57 26.20?? 38.36?? 68.00 37.34?? 48.21?? 64.56 47.17?? 54.51??
Lex 61.00 22.23 32.59 58.95 32.37 41.79 56.92 41.68 48.12
Sal 46.42 16.99 24.88 51.14 28.08 36.25 54.59 39.80 46.04
Pic 51.71 21.12 29.99 56.85 31.22 40.31 56.35 41.26 47.64
Combined 75.60?? 4.86 9.13 75.60?? 4.86 9.13 75.60?? 4.86 9.13
Doc Title 32.67 5.23 9.02 32.33 5.64 9.60 32.15 5.70 9.68
tf * idf 55.85 20.44 29.93 54.19 29.75 38.41 49.07 35.93 41.48
When performing evaluations using the gold
standard GScontext, significantly higher precision,
recall and F-measure values are scored by all the
systems, including both baselines. This is perhaps
due to the availability of candidates that suggests
a form of cued recall, rather than free recall, as
is the case with GSintuitive. The user is able to
annotate an image with higher accuracy e.g. la-
belling a Chihuahua as a Chihuahua instead of a
dog. Again, the Semantic Cloud method contin-
ues to outperform all the other systems in terms of
recall and F-measure consistently for k=10, k=15
and k=20 words. A similar trend as observed us-
ing the gold standard of GSintuition is seen here,
where again our combined system favors precision
over recall at all values of k.
A possible explanation for the poor perfor-
mance of the Saliency method is perhaps due to
over-specific words that infer all other words in the
text, yet unknown to the knowledge of most hu-
man annotators. For instance, the word Mussolini,
referring to the dictator Benito Mussolini, was not
selected as an annotation for an image showing
scenes of World War II depicting the Axis troops,
though it suggests the concepts of war, World War
II and so on. The Pic method is also not perform-
ing as well as expected under the two gold anno-
tation standards, mainly due to the fact that it fo-
cuses on selecting picturable nouns but not nec-
essarily those that are semantically linked to the
image itself.
7 Future Work
The use of the semantic cloud method to generate
automatic annotations is promising. Future work
will consider using additional semantic resources
such as ontological information and ency-
clopaedic knowledge to enhance existing models.
We are also interested to pursue human knowledge
modeling to account for the differences in annota-
tors in order create a more objective gold standard.
References
Kobus Barnard and David Forsyth. 2001. Learning the se-
mantics of words and pictures. In Proceedings of Interna-
tional Conference on Computer Vision.
Koen Deschacht and Marie-Francine Moens. 2007. Text
analysis for automatic image annotation. In Proceedings
of the Association for Computational Linguisticd.
Yansong Feng and Mirella Lapata. 2008. Automatic image
annotation using auxiliary text information. In Proceed-
ings of the Association for Computational Linguistics.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Comput-
ing semantic relatedness using wikipedia-based explicit
semantic analysis. In International Joint Conferences on
Artificial Intelligence.
J Jeon, V Lavrenko, and R Manmatha. 2003. Automatic im-
age annotation and retrieval using cross-media relevance
models. In Proceedings of the ACM SIGIR Conference on
Research and Development in Information Retrieval.
Jia Li and James Wang. 2008. Real-time computerized an-
notation of pictures. In Proceedings of International Con-
ference on Computer Vision.
Rada Mihalcea and Paul Tarau. 2004. Textrank: Bringing
order into texts. In in Proceedings of Empirical Methods
in Natural Language Processing.
59
Coling 2010: Poster Volume, pages 647?655,
Beijing, August 2010
Text Mining for Automatic Image Tagging
Chee Wee Leong and Rada Mihalcea and Samer Hassan
Department of Computer Science and Engineering
University of North Texas
cheeweeleong@my.unt.edu, rada@cs.unt.edu, samer@unt.edu
Abstract
This paper introduces several extractive
approaches for automatic image tagging,
relying exclusively on information mined
from texts. Through evaluations on two
datasets, we show that our methods ex-
ceed competitive baselines by a large mar-
gin, and compare favorably with the state-
of-the-art that uses both textual and image
features.
1 Introduction
With continuously increasing amounts of images
available on the Web and elsewhere, it is impor-
tant to find methods to annotate and organize im-
age databases in meaningful ways. Tagging im-
ages with words describing their content can con-
tribute to faster and more effective image search
and classification. In fact, a large number of ap-
plications, including the image search feature of
current search engines (e.g., Yahoo!, Google) or
the various sites providing picture storage services
(e.g., Flickr, Picasa) rely exclusively on the tags
associated with an image in order to search for rel-
evant images for a given query.
However, the task of developing accurate and
robust automatic image annotation models entails
daunting challenges. First, the availability of large
and correctly annotated image databases is cru-
cial for the training and testing of new annotation
models. Although a number of image databases
have emerged to serve as evaluation benchmarks
for different applications, including image anno-
tation (Duygulu et al, 2002), content-based im-
age retrieval (Li and Wang, 2008) and cross
language information retrieval (Grubinger et al,
2006), such databases are almost exclusively cre-
ated by manual labeling of keywords, requiring
significant human effort and time. The content of
these image databases is often restricted only to a
few domains, such as medical and natural photo
scenes (Grubinger et al, 2006), and specific ob-
jects like cars, airplanes, or buildings (Fergus et
al., 2003). For obvious practical reasons, it is im-
portant to develop models trained and evaluated
on more realistic and diverse image collections.
The second challenge concerns the extraction
of useful image and text features for the construc-
tion of reliable annotation models. Most tradi-
tional approaches relied on the extraction of image
colors and textures (Li and Wang, 2008), or the
identification of similar image regions clustered as
blobs (Duygulu et al, 2002) to derive correlations
between image features and annotation keywords.
In comparison, there are only a few efforts that
leverage on the multitude of resources available
for natural language processing to derive robust
linguistic-based image annotation models. One
of the earliest efforts involved the use of captions
for face recognition in photographs through the
construction of a specific lexicon that integrates
linguistic and photographic information (Srihari
and Burhans, 1994). More recently, several ap-
proaches have proposed the use of WordNet as
a knowledge-base to improve content-based im-
age annotation models, either by removing noisy
keywords through semantic clustering (Jin et al,
2005) or by inducing a hierarchical classification
of candidate labels (Srikanth et al, 2005).
In this paper, we explore the use of several natu-
ral language resources to construct image annota-
tion models that are capable of automatically tag-
ging images from unrestricted domains with good
accuracy. Unlike traditional image annotation
methodologies that generate tags using image-
based features, we propose to extract them in a
manner analogous to keyword extraction. Given a
target image and its surrounding text, we extract
those words and phrases that are most likely to
represent meaningful tags. More importantly, we
647
are interested to investigate the potential of such
linguistic-based models on image annotation ac-
curacy and reliability. Our work is motivated by
the need for annotation models that can be effi-
ciently applied on a very large scale (e.g. har-
vesting images from the web), which are required
in applications that cannot afford the complexity
and time associated with current image process-
ing techniques.
The paper makes the following contributions.
We first propose a new evaluation framework for
image tagging, which is based on an analogy
drawn between the tasks of image labeling and
lexical substitution. Next, we present three extrac-
tive approaches for the task of image annotation.
The methods proposed are based only on the text
surrounding an image, without the use of image
features. Finally, by combining several orthogo-
nal methods through machine learning, we show
that it is possible to achieve a performance that is
competitive to a state-of-the-art image annotation
system that relies on visual and textual features,
thus demonstrating the effectiveness of text-based
extractive annotation models.
2 Related Work
Several online systems have sprung into exis-
tence to achieve annotation of real world images
through human collaborative efforts (Flickr) and
stimulating competition (von Ahn and Dabbish,
2004). Although a large number of image tags can
be generated in short time, these approaches de-
pend on the availability of human annotators and
are far from being automatic. Similarly, research
in the other direction via text-to-image synthesis
(Li and Fei-Fei, 2008; Collins et al, 2008; Mi-
halcea and Leong, 2009) has also helped to har-
vest images, mostly for concrete words, by refin-
ing image search engines.
Most approaches to automatic image annota-
tion have focused on the generation of image la-
bels using annotation models trained with image
features and human annotated keywords (Barnard
and Forsyth, 2001; Jeon et al, 2003; Makadia et
al., 2008; Wang et al, 2009). Instead of predict-
ing specific words, these methods generally target
the generation of semantic classes (e.g. vegeta-
tion, animal, building, places etc), which they can
achieve with a reasonable amount of success. Re-
cent work has also considered the generation of
labels for real-world images (Li and Wang, 2008;
Feng and Lapata, 2008). To our knowledge, we
are unaware of any other work that performs ex-
tractive annotation for images from unrestricted
domains through the exclusive use of textual fea-
tures.
3 Dataset
As the methods we propose are extractive, stan-
dard image databases with no surrounding text
such as Corel (Duygulu et al, 2002) are not suit-
able, nor are they representative for the challenges
associated with raw data from unrestricted do-
mains. We thus create our own dataset using im-
ages randomly extracted from the Web.
To avoid sparse searches, we use a list of the
most frequent words in the British National Cor-
pus as seed words, and query the web using the
Google Image API. A webpage is randomly se-
lected from the query results if it contains a single
image in the specified size range (width and height
of 275 to 1000 pixels1) and its text contains more
than 10 words. Next, we use a Document Object
Model (DOM) HTML parser2 to extract the con-
tent of the webpage. Note that we do not perform
manual filtering of our images except where they
contain undesirable qualities (e.g. porn, corrupted
or blank images).
In total, we collected 300 image-text pairs from
the web. The average image size is 496 pixels
width and 461 pixels height. The average text
length is 278 tokens and the average document ti-
tle length is 6 tokens. In total, there are 83,522
words and the total vocabulary is 8,409 words.
For each image, we also create a gold stan-
dard of manually assigned tags, by using the la-
bels assigned by five human annotators. The im-
age annotation is conducted via Amazon Mechan-
ical Turk, which was shown in the past to produce
reliable annotations (Snow et al, 2008). For in-
creased annotation reliability, we only accept an-
notators with an approval rating of 98%.
Given an image, an annotator extracts from
the associated text a minimum of five words or
collocations.Annotators can choose words freely
from the text, while collocation candidates are re-
stricted to a fixed set obtained from the n-grams (n
? 7) in the text that also appear as article names or
surface forms in Wikipedia. Moreover, when in-
terpreting the image, the annotators are instructed
to focus on both the denotational and conotational
attributes present in the image3.
1Empirically determined to filter advertisements, banners
and undersized images.
2http://search.cpan.org/dist/HTML-ContentExtractor/
3Annotation instructions, dataset and gold standard can
648
Normal Image Mode Image
Gold standard czech (5), festival (5), oklahoma (4), yukon (4),
october (4), web page (2), the first (2), event (2),
success (1), every (1), year (1)
train (5), station (4), steam (4), trans siberian (4),
steam train (4), travel (3), park (3), siberian (3),
old (3), photo (1), trans (2), yekaterinburg (2),
the web (2), photo host (1)
Table 1: Two sample images. The number besides each label indicates the number of human annotators
agreeing on that label. Note that the mode image has a tag (i.e.?train?) in the gold standard set most
frequently selected by the annotators
4 A New Evaluation Framework : Image
Tagging as Lexical Substitution
While evaluations of previous work in image an-
notation were often based on labels provided with
the images, such as tags or image captions, in our
dataset such annotations are either missing or un-
reliable. We rely instead on human-produced ex-
tractive annotations (as described in the previous
section), and formulate a new evaluation frame-
work based on the intuition that an image can be
substituted with one or more tags that convey the
same meaning as the image itself. Ideally, there is
a single tag that ?best? describes the image over-
all (i.e. the gold standard tag agreed by the major-
ity of human annotators), but there are also mul-
tiple tags that describe the fine-grained concepts
present in the image. Our evaluation framework
is inspired by the lexical substitution task (Mc-
Carthy and Navigli, 2007), where a system at-
tempts to generate a word (or a set of words) to
replace a target word, such that the meaning of
the sentence is preserved.
Given this analogy, the evaluation metrics used
for lexical substitution can be adapted to the eval-
uation of image tagging. Specifically, we measure
the precision and the recall of a tagging method
using four subtasks: best normal: provides preci-
sion and recall for the top-ranked tag returned by a
method; best mode: provides precision and recall
only if the top-ranked tag by a method matches the
tag in the gold standard that was most frequently
selected by the annotators; out of ten (oot) nor-
be downloaded at
http://lit.csci.unt.edu/index.php/Downloads
mal: provides precision and recall for the top ten
tags by the system; and out of ten (oot) mode:
similar to best mode, but it considers the top ten
tags returned by the system instead of one. Table
1 show examples of a normal and a mode image.
Formally, let us assume that H is the set
of annotators, namely {h1, h2, h3, ...}, and I,
{i1, i2, i3, ...} is the set of images for which each
human annotator provide at least five tags. For
each ij, we calculate mj, which is the most fre-
quent tag for that image, if available. We also col-
lect all rkj, which is the set of tags for the image
ij from the annotator hk.
Let the set of those images where there is a tag
agreed upon by the most annotators (i.e. the im-
ages with a mode) be denoted by IM, such that
IM ? I. Also, let A ? I be the set of images for
which the system provides more than one tag. Let
the corresponding set for the images with modes
be denoted by AM, such that AM ? IM. Let aj ? A
be the set of system?s extracted tags for the image
ij.
Thus, for each image ij, we have the set of tags
extracted by the system, and the set of tags from
the human annotators. As the next step, the multi-
set union of the human tags is calculated, and the
frequencies of the unique tags is noted. Therefore,
for image ij, we calculate Rj, which is
?
rkj, and
the individual unique tag in Rj, say res, will have
a frequency associated with it, namely freqres.
Given this setting, the precision (P ) and recall
(R) metrics we use are defined below.
649
Best measures:
P =
?
aj :ij?A
?
res?aj
freqres
|aj |
|Rj |
|A|
R =
?
aj :ij?I
?
res?aj
freqres
|aj |
|Rj |
|I|
modeP =
?
bestguessj?AM (1if best guess = mj)
|AM |
modeR =
?
bestguessj?IM (1if best guess = mj)
|IM |
Out of ten (oot) measures:
P =
?
aj :ij?A
?
res?aj
freqres
|Rj |
|A|
R =
?
aj :ij?I
?
res?aj
freqres
|Rj |
|I|
modeP =
?
aj :ij?AM (1if any guess ? aj = mj)
|AM |
modeR =
?
aj :ij?IM (1if any guess ? aj = mj)
|IM |
As a simplified example (with less tags), con-
sider ij showing a picture of a Chihuahua being
labeled by five annotators with the following tags :
Annotator Tags
1 dog,pet
2 chihuahua
3 animal,dog
4 dog,chihuahua
5 dog
In this case, r1j = {dog,pet}, r2j = {chihuahua},
r3j = {animal,dog} and so on. The tag ?dog? ap-
pears the most frequent among the five annotators,
hence mj = {dog}. Rj={dog, dog, dog, dog, chi-
huahua, chihuahua, animal, pet}. The res with
associated frequencies would be dog 4, chihuahua
2, animal 1, pet 1. If the system?s proposed tag for
ij is {dog, animal}, then the numerator of P and
R for best subtask would be
4+1
2
8 = 0.313. Simi-
larly, the numerator of P and R for oot subtask is
4+1
8 = 0.625.
5 Extractive Image Annotation
The main idea underlying our work is that we can
perform effective image annotation using infor-
mation drawn from the associated text. Follow-
ing (Feng and Lapata, 2008), we propose that an
image can be annotated with keywords capturing
the denotative (entities or objects depicted) and
connotative (semantics or ideologies interpreted)
attributes in the image. For instance, a picture
showing a group of athletes and a ball may also be
tagged with words like ?soccer,? or ?sports activ-
ity.? Specifically, we use a combination of knowl-
edge sources to model the denotative quality of a
word as its picturability, and the connotative at-
tribute as its saliency. The idea of visualness and
salience as textual features for discovering named
entities in an image was first pursued by (De-
schacht and Moens, 2007), using data from the
news domain. In contrast, we are able to per-
form annotation of images from unrestricted do-
mains using content words (nouns, verbs and ad-
jectives). In the following, we first describe three
unsupervised extractive approaches for image an-
notation, followed by a supervised method using a
re-ranking hypothesis that combines all the meth-
ods.
5.1 Flickr Picturability
Featuring a repository of four billion images,
Flickr (http://www.flickr.com) is one of the most
comprehensive image resources on the web. As a
photo management and sharing application, it pro-
vides users with the ability to tag, organize, and
share their photos online. Interestingly, an inspec-
tion of Flickr tags for randomly selected images
reveal that users tend to describe the denotational
attributes of images, using concrete and picturable
words such as cat, bug, car etc. This observation
lends evidence to Flickr?s suitability as a resource
to model the picturability of words.
Given the text (T ) of an image, we can use
the getRelatedTags API to retrieve the most fre-
quent Flickr tags associated with a given word,
and use them as corpus evidence to filter or pro-
mote words in the text. In the filtering phase
we ignore any words that return an empty list of
Flickr?s related tags, based on the assumption that
these words are not used in the Flickr tags repos-
itory. We also discard words with a length that is
less than three characters (?=3). In the promotion
phase, we reward any retrieved tags that appear as
surface forms in the text. This reward is propor-
tional to the term frequency of these tags in the
650
Algorithm 1 Flickr Picturability Algorithm
Start : L[]=? , TF[]=tf of each word in T
for each word in T do
if length(word) ? ? then
RelatedTags=getRelatedTags(word);
if size(RelatedTags) > 0 then
L[word]+=?*TF[word]
for each tag in RelatedTags do
if exists TF [tag] then
L[tag]+=TF[tag]
end if
end for
end if
end if
end for
text. Additionally, we also include in the final la-
bel set any word that returns a non-empty related
tags set with a discounted weight (?=0.5) of its
term frequency, to the end of enriching our labels
set while assuring more credit are given to the pic-
turable words.
To extract multiword labels, we locate all n-
grams formed exclusively from our extracted set
of possible labels. The subsequent score for each
of these n-grams is:
L[wi..wi+k] = (
j=i+k?
j=i
L[wj])/k
By reverse sorting the associative array in L, we
can retrieve the top K words to label the image.
For illustration, let us consider the following text
snippet.
On the Origin of Species, published by
Charles Darwin in 1859, is considered
to be the foundation of evolutionary bi-
ology.
After removing stopwords, we consider the re-
maining words as candidate labels. For each
of these candidates wi (i.e. origin, species,
published, charles, darwin, foundation,
evolutionary, and biology), we query Flickr and
obtain their related tag set Ri. origin, published,
and foundation return an empty set of related
tags and hence are removed from our set of can-
didate labels, leaving species, charles, darwin,
evolutionary, and biology as possible annotation
keywords with the initial score of 0.5. In the pro-
motion phase, we score each wi based on the num-
ber of votes it receives from the remaining wj
(Figure 1). Each vote represents an occurrence
of the candidate tag wi in the related tag set Rj
of the candidate tag wj . For example, darwin
appeared in the Flickr related tags for charles,
evolutionary, and biology, hence it has a weight
of 3.5. The final list of candidate labels are shown
in Table 2.
... Species, published by Charles Darwin ? founda!on of evolu!onary biology
Figure 1: Flickr Picturability Labels
Label S(wi)
darwin 3.5
charles darwin 2.5
charles 1.5
biology 1.5
evolutionary biology 1.0
evolutionary 0.5
species 0.5
Table 2: Candidate labels obtained for a sample
text using the Flickr model
5.2 Wikipedia Salience
We hypothesize that an image often describes the
most important concepts in the associated text.
Thus, the keywords selected from a text could be
used as candidate labels for the image. We use
a graph-based keyword extraction method similar
to (Mihalcea and Tarau, 2004), enhanced with a
semantic similarity measure. Starting with a text,
we extract all the candidate labels and add them as
vertices in the graph. A measure of word similar-
ity is then used to draw weighted edges between
the nodes. Using the PageRank algorithm, the
words are assigned with a score indicating their
salience within the given text.
To determine the similarity between words, we
use a directed measure of similarity. Most word
similarity metrics provide a single-valued score
between a pair of words w1 and w2 to indicate
their semantic similarity. Intuitively, this is not al-
ways the case, as w1 may be represented by con-
cepts that are entirely embedded in other concepts,
represented by w2. In psycholinguistics terms, ut-
tering w1 may bring to mind w2, while the appear-
ance of w2 without any contextual clues may not
associate with w1. For example, Obama brings
to mind the concept of president, but president
651
may trigger other concepts such as Washington,
Lincoln, Ford etc., depending on the existing
contextual clues. Thus, the degree of similarity
of w1 with respect to w2 should be separated from
that of w2 with respect to w1. Specifically, we use
the following measure of similarity, based on the
Explicit Semantic Analysis (ESA) vectors derived
from Wikipedia (Gabrilovich and Markovitch,
2007):
DSim(wi, wj) =
Cij
Ci
? Sim(wi, wj)
where Cij is the count of articles in Wikipedia
containing words wi and wj , Ci is the count of ar-
ticles containing words wi, and Sim(wi, wj) is the
cosine similarity of the ESA vectors representing
the input words.The directional weight (Cij /Ci)
amounts to the degree of association of wi with re-
spect to wj . Using the directional inferential sim-
ilarity scores as directed edges and distinct words
as vertices, we obtain a graph for each text. The
directed edges denotes the idea of ?recommenda-
tion? where we say w1 recommends w2 if and
only if there is a directed edge from w1 to w2, with
the weight of the recommendation being the direc-
tional similarity score. Starting with this graph,
we use the graph iteration algorithm from (Mi-
halcea and Tarau, 2004) to calculate a score for
each vertex in the graph. The output is a sorted
list of words in decreasing order of their ranks,
which are used as candidate labels to annotate the
image. This is achieved by using Cj instead of Ci
for the denominator in the directional weight. As
an example, consider the text snippet :
Microsoft Corporation is a multina-
tional computer technology corporation
that develops, manufactures, licenses,
and supports a wide range of software
products for computing devices
after stopword removal, the list of nouns ex-
tracted is Microsoft, computer, corporation, de-
vices, products, technology, software. Note that
the top-ranked word must infer some or all of the
words in the text. In this case, the word Microsoft
infers the terms computer, technology and soft-
ware.
To calculate the semantic relatedness between
two collocations, we use a simplified version of
the text-to-text relatedness technique proposed by
and (Mihalcea et al, 2006) that incorporate the
directional inferential similarity as an underlying
semantic metric.
5.3 Topical Modeling
Intuitively, every text is written with a topic in
mind, and the associated image serves as an illus-
tration of the text meaning. In this paper, we in-
vestigate the effect of topical modeling on image
annotation accuracy directly. We use the Pachinko
Allocation Model (PAM) (Li and McCallum,
2006) to model the topics in a text, where key-
words forming the dominant topic are assumed as
our set of annotation keywords. Compared with
previous topic modeling approaches, such as La-
tent Dirichlet alocation (LDA) or its improved
variant Correlated Topic Model (CTM) (Blei and
Lafferty, 2007), PAM captures correlations be-
tween all the topic pairs using a directed acyclic
graph (DAG). It also supports finer-grained topic
modeling, and has state-of-the-art performance on
the tasks of document classification and topical
keyword coherence. Given a text, we use the PAM
model to infer a list of super-topics and sub-topics
together with words weighted according to the
likelihood that they belong to each of these topics.
For each text, we retrieve the top words belong-
ing to the dominant super-topic and sub-topic. We
use 50 super-topics and 100 sub-topics as operat-
ing parameters for PAM, since these values were
found to provide good results in previous work on
topic modeling. Default values are used for other
parameters in the model.
5.4 Supervised Learning
The three tagging methods target different aspects
of what constitutes a good label for an image. We
use them as features in a machine learning frame-
work, and introduce a final rank attribute S(tj),
which is a linear combination of the reciprocals of
the rank of each tag as given by each method,
S(tj) =
?
m?methods
?m
1
rmtj
where rmtj is the rank for tag tj given by method
m. The weight of each method ?m is estimated
from the training set using information gain val-
ues. Since our predicted variable (mode precision
or recall) is continuous, we use the Support Vec-
tor Algorithm (nu-SVR) implementation of SVM
(Chang and Lin, 2001) to perform regression anal-
ysis on the weights for each method via a radial
basis function kernel. A ten-fold cross-validation
is applied on the entire dataset of 300 images.
652
Best out-of-ten (oot)
Normal Mode Normal Mode
Models P R P R P R P R
Flickr picturability 6.32 6.32 78.57 78.57 35.61 35.61 92.86 92.86
Wikipedia Salience 6.40 6.40 7.14 7.14 35.19 35.19 92.86 92.86
Topic modeling 5.99 5.99 42.86 42.86 37.13 37.13 85.71 85.71
Combined (SVM) 6.87 6.87 67.49 67.49 37.85 37.85 100.00 100.00
Doc Title 6.40 6.40 75.00 75.00 18.97 18.97 82.14 82.14
tf * idf 5.94 5.94 14.29 14.29 38.40 38.40 78.57 78.57
Random 3.76 3.76 3.57 3.57 30.20 30.20 50.00 50.00
Upper bound (human) 12.23 12.07 81.48 81.48 82.44 81.55 100.00 100.00
Table 3: Results obtained on the Web dataset
6 Experiments and Evaluations
We evaluate the performance of each of the three
tagging methods separately, followed by an eval-
uation of the combined method. Each system pro-
duces a ranked list of K words or collocations
as tags assigned to a given image. A system can
discretionary generate less (but not more) than K
tags, depending on its confidence level.
For comparison, we implement three baselines:
tf*idf, Doc Title and Random. For tf*idf, we use
the British National Corpus to calculate the idf
scores, while the frequency of a term is calcu-
lated from the entire text associated with an im-
age. The Doc Title baseline is similar, except that
the term frequency is calculated based on the title
of the document. The Random baseline randomly
selects words from a co-occurrence window of
size K before and after an image as its annota-
tion. Following other tagging methods, we apply a
pre-processing stage, where we part-of-speech tag
the text (to retain only nouns), followed by stem-
ming. We also determine an upper bound, which
is calculated as follows. For each image, the la-
bels assigned by each of the five annotators are
in turn evaluated against a gold standard consist-
ing of the annotations of the other four annotators.
The best performing annotator is then recorded.
This process is repeated for each of the 300 im-
ages, and the average precision and recall are cal-
culated. This represents an upper bound, as it is
the best performance that a human can achieve on
this dataset. Table 3 shows our experimental re-
sults.
Among the individual methods, the method im-
plementing Flickr picturability has the highest in-
dividual score for best and oot modes, yielding
a precision and recall of 78.57% and 92.86% re-
spectively. The Wikipedia Saliency method also
scores the highest (jointly with Flickr) in the oot
mode, but for the best mode achieves a score only
marginally better than the random baseline. A
plausible explanation is that it tends to favor ?all-
inferring? over-specific labels, while the most fre-
quently selected tags in mode pictures are typi-
cally more ?picturable? than being specific (e.g.
?train? for the mode picture in Table 1). The topic
modeling method has mixed results: its scores
for oot normal and mode are somewhat compet-
itive with tf*idf, but it scores consistently lower
than the DocTitle in the best subtask, possibly
due to the absence of a more sophisticated re-
ranking algorithm tailored for the image annota-
tion task other than the intrinsic ranking mecha-
nism in PAM. It is worth noting that the combined
supervised system provides the overall best results
(6.87%) on the best normal, and achieves a perfect
precision and recall (100%) for oot mode, which
means perfect agreement with the human tagging.
7 Comparison with Related Work
We also compare our work against (Feng and Lap-
ata, 2008) as it allows for a direct comparison with
models using both image and textual features un-
der a standard evaluation framework. We obtained
the BBC dataset used in their experiments, which
consists of 3121 training and 240 testing images.
In this dataset, images are implicitly tagged with
captions by the author of the corresponding BBC
article. The evaluations are run against these cap-
tions.
In their experiments, Feng and Lapata created
four annotation models. The first two (tf*idf and
Document Title) are the same as used in our base-
line experiments. The third model (Lavrenko03)
is an application of the continuous relevance
model in (Jeon et al, 2003), trained with the BBC
image features and captions. Finally, the forth
(ExtModel) is an extension of the relevance model
using additional information in auxiliary texts.
Briefly, the model assumes a multiple Bernoulli
distribution for words in a caption, and generates
tags for a test image using a weighted combina-
tion of the accompanying document, caption and
image features learned during training.
653
Top 10 Top 15 Top 20
Models P R F1 P R F1 P R F1
tf*idf 4.37 7.09 5.41 3.57 8.12 4.86 2.65 8.89 4.00
DocTitle 9.22 7.03 7.20 9.22 7.03 7.20 9.22 7.03 7.20
Lavrenko03 9.05 16.01 11.81 7.73 17.87 10.71 6.55 19.38 9.79
ExtModel 14.72 27.95 19.82 11.62 32.99 17.18 9.72 36.77 15.39
Flickr picturability 12.13 22.82 15.84 9.52 26.82 14.05 8.23 29.80 12.90
Wikipedia Salience 11.63 21.89 15.18 9.28 26.20 13.70 7.81 29.41 12.35
Topic Modeling 11.42 21.49 14.91 9.28 26.20 13.70 7.86 29.57 12.42
Combined (SVM) 13.38 25.17 17.47 11.08 31.29 16.37 9.50 35.76 15.01
Table 4: Results obtained on the BBC dataset used in (Feng and Lapata, 2008)
The experimental setup is similar to the earlier
section, but a few modifications are made for a fair
and direct comparison. First, we extend our mod-
els coverage to include content words (i.e. nouns,
verbs, adjectives) determined using the Tree Tag-
ger (Schmid, 1994). Second, no collocations are
used. Third, we adopt the evaluation framework
used by Feng and Lapata to extract the top 10, 15
and 20 tags. Note that in our methods, the extrac-
tion of tags for a test image is only done on the
document surrounding the image, after excluding
the caption. As the number of negative examples
(words not present in the caption) greatly outnum-
ber the positive instances, we employ an under-
sampling method (Kubat and Matwin, 1997) to
balance the dataset for training.
The results are shown in Table 4. Interest-
ingly, all our unsupervised extraction-based mod-
els perform consistently above the supervised
Lavrenko03 model, indicating that textual fea-
tures are more informative than captions and im-
age features taken together. Comparing with mod-
els using significantly less document informa-
tion (tf*idf and Doc title), our models gain even
greater advantage. Note that the title of any BBC
article does not exceed 10 words, hence compar-
ison is only meaningful given the top 10 tags re-
trieved.
Feng and Lapata used LDA to perform rerank-
ing of final candidates in their ExtModel. How-
ever, when used as a model alone, the PAM topic
model achieved promising scores in all the cate-
gories, performing best for top 10 keywords (F1
of 14.91%). Flickr picturability stands out as
the best performing unsupervised method, scor-
ing the highest precision (12.13%, top 10), recall
(29.80%, top 20) and F1 (15.84%, top 10).
Overall, this comparative evaluation yields
some important insights. First, our combined
model using SVM is statistically better (p<0.1 for
top 10, 15, 20) than the Laverenko03 model, but
not statistically different from the ExtModel. This
demonstrates the effectiveness of textual-based
models over traditional models trained with im-
age features and captions. While it is intuitively
clear that image features help in improving tag-
ging performance, we show that mining only the
text surrounding an image, where it exists, can
yield a performance that is comparable to a state-
of-the-art system that uses both textual and vi-
sual features. Moreover, an increase in complex-
ity of a model by using more features may hinder
its applicability to large datasets, but not neces-
sarily improving annotation performance (Maka-
dia et al, 2008). On this, text-based annotation
models can provide a desirable compromise. For
instance, our unsupervised models implementing
Flickr picturability and Wikipedia Salience are
able to extract annotations from a BBC article (av-
erage 133.85 tokens) in approximately 1 second
and 20 seconds respectively.
8 Conclusions and Future Work
In this paper, we introduced several text-based ex-
tractive approaches for automatic image annota-
tion and showed that they compare favorably with
the state-of-the-art in image annotation using both
text and image features. We believe our work
has practical applications in mining and annotat-
ing images over the Web, where texts are nat-
urally associated with images, and scalability is
important. Our next direction seeks to derive ro-
bust annotation models using additional ontolog-
ical knowledge-bases. We would also like to ad-
vance the the state-of-the-art by augmenting cur-
rent textual models with image features.
Acknowledgments
This material is based in part upon work sup-
ported by the National Science Foundation CA-
REER award #0747340. Any opinions, findings,
and conclusions or recommendations expressed in
this material are those of the authors and do not
necessarily reflect the views of the National Sci-
ence Foundation.
654
References
Kobus Barnard and David Forsyth. 2001. Learning the
semantics of words and pictures. In Proceedings of
International Conference on Computer Vision.
David Blei and John Lafferty. 2007. A correlated topic
model of science. In Annals of Applied Statistics,
volume 1, pages 17?35.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM:
a library for support vector machines.
Brendan Collins, Jia Deng, Kai Li, and Li Fei-Fei.
2008. Towards scalable dataset construction: An
active learning approach. In Proceedings of Euro-
pean Conference on Computer Vision.
Koen Deschacht and Marie-Francine Moens. 2007.
Text analysis for automatic image annotation. In
Proceedings of the Association for Computational
Linguistics.
Pinar Duygulu, Kobus Barnard, Nando de Freitas, and
David Forsyth. 2002. Object recognition as ma-
chine translation:learning a lexicon for a fixed im-
age vocabulary. In Proceedings of the 7th European
Conference on Computer Vision.
Yansong Feng and Mirella Lapata. 2008. Automatic
image annotation using auxiliary text information.
In Proceedings of the Association for Computa-
tional Linguistics.
Rob Fergus, Pietro Perona, and Andrew Zisserman.
2003. Object class recognition by unsupervised
scale-invariant learning. In Proceedings of the In-
ternational Conference on Computer Vision and
Pattern Recognition.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In International
Joint Conferences on Artificial Intelligence.
Michael Grubinger, Clough Paul, Mller Henning, and
Deselaers Thomas. 2006. The iapr benchmark: A
new evaluation resource for visual information sys-
tems. In International Conference on Language Re-
sources and Evaluation.
Jiwoon Jeon, Victor Lavrenko, and R Manmatha.
2003. Automatic image annotation and retrieval us-
ing cross-media relevance models. In Proceedings
of the ACM SIGIR Conference on Research and De-
velopment in Information Retrieval.
Yohan Jin, Latifur Khan, Lei Wang, and Mamoun
Awad. 2005. Image annotations by combining mul-
tiple evidence & wordnet. In Proceedings of Annual
ACM Multimedia.
Miroslav Kubat and Stan Matwin. 1997. Addressing
the curse of imbalanced training sets: one-sided se-
lection. In Proceedings of International Conference
on Machine Learning.
Li-Jia Li and Li Fei-Fei. 2008. Optimol: au-
tomatic online picture collection via incremental
model learning. In International Journal of Com-
puter Vision.
Wei Li and Andrew McCallum. 2006. Pachinko allo-
cation: Dag-structured mixture models of topic cor-
relations. In Proceedings of the International Con-
ference on Machine learning.
Jia Li and James Wang. 2008. Real-time computer-
ized annotation of pictures. In Proceedings of Inter-
national Conference on Computer Vision.
Ameesh Makadia, Vladimir Pavlovic, and Sanjiv Ku-
mar. 2008. A new baseline for image annotation. In
Proceedings of European Conference on Computer
Vision.
Diana McCarthy and Roberto Navigli. 2007. The se-
meval English lexical substitution task. In Proceed-
ings of the ACL Semeval workshop.
Rada Mihalcea and Chee Wee Leong. 2009. To-
wards communicating simple sentences using pic-
torial representations. In Machine Translation, vol-
ume 22, pages 153?173.
Rada Mihalcea and Paul Tarau. 2004. Textrank:
Bringing order into texts. In Proceedings of Em-
pirical Methods in Natural Language Processing.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In Proceed-
ings of Association for the Advancement of Artificial
Intelligence, pages 775?780.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Ng. 2008. Cheap and fast - but is it good?
evaluating non-expert annotations for natural lan-
guage tasks. In Proceedings of Empirical Methods
in Natural Language Processing.
Srihari and Burhans. 1994. Visual semantics: Extract-
ing visual information from text accompanying pic-
tures. In Proceedings of the American Association
for Artificial Intelligence.
Munirathnam Srikanth, Joshua Varner, Mitchell Bow-
den, and Dan Moldovan. 2005. Exploiting ontolo-
gies for automatic image annotation. In Proceed-
ings of the ACM Special Interest Group on Research
and Development in Information Retrieval.
Luis von Ahn and Laura Dabbish. 2004. Labeling im-
ages with a computer game. In Proceedings of the
ACM Special Interest Group on Computer Human
Interaction.
Chong Wang, David Blei, and Li Fei-Fei. 2009. Si-
multaneous image classification and annotation. In
Proceedings of IEEE Conference on Computer Vi-
sion and Pattern Recognition.
655
Measuring the semantic relatedness between words and images
Chee Wee Leong and Rada Mihalcea
Department of Computer Science and Engineering
University of North Texas
cheeweeleong@my.unt.edu, rada@cs.unt.edu
Abstract
Measures of similarity have traditionally focused on computing the semantic relatedness between
pairs of words and texts. In this paper, we construct an evaluation framework to quantify cross-modal
semantic relationships that exist between arbitrary pairs of words and images. We study the effec-
tiveness of a corpus-based approach to automatically derive the semantic relatedness between words
and images, and perform empirical evaluations by measuring its correlation with human annotators.
1 Introduction
Traditionally, a large body of research in natural language processing has focused on formalizing word
meanings. Several resources developed to date (e.g., WordNet (Miller, 1995)) have enabled a systematic
encoding of the semantics of words and exemplify their usage in different linguistic frameworks. As a
result of this formalization, computing semantic relatedness between words has been possible and has
been used in applications such as information extraction and retrieval, query reformulation, word sense
disambiguation, plagiarism detection and textual entailment.
In contrast, while research has shown that the human cognitive system is sensitive to visual informa-
tion and incorporating a dual linguistic-and-pictorial representation of information can actually enhance
knowledge acquisition (Potter and Faulconer, 1975), the meaning of an image in isolation is not well-
defined and it is mostly task-specific. A given image, for instance, may be simultaneously labeled by a
set of words using an automatic image annotation algorithm, or classified under a different set of seman-
tic tags in the image classification task, or simply draw its meaning from a few representative regions
following image segmentation performed in an object localization framework.
Given that word meanings can be acquired and disambiguated using dictionaries, we can perhaps
express the meaning of an image in terms of the words that can be suitably used to describe it. Specif-
ically, we are interested to bridge the semantic gap (Smeulders et al, 2000) between words and images
by exploring ways to harvest the information extracted from visual data in a general framework. While a
large body of work has focused on measuring the semantic similarity of words (e.g., (Miller and Charles,
1998)), or the similarity between images based on image content (e.g., (Goldberger et al, 2003)), very
few researchers have considered the measure of semantic relatedness1 between words and images.
But, how exactly is an image related to a given word? In reality, quantification of such a cross-
modal semantic relation is impossible without supplying it with a proper definition. Our work seeks to
address this challenge by constructing a standard evaluation framework to derive a semantic relatedness
metric for arbitrary pairs of words and images. In our work, we explore methods to build a representa-
tion model consisting of a joint semantic space of images and words by combining techniques widely
adopted in computer vision and natural language processing, and we evaluate the hypothesis that we can
automatically derive a semantic relatedness score using this joint semantic space.
Importantly, we acknowledge that it is significantly harder to decode the semantics of an image, as its
interpretation relies on a subjective and perceptual understanding of its visual components (Biederman,
1In our paper, we are concerned with semantic relatedness, which is a more general concept than semantic similarity.
Similarity is concerned with entities related by virtues of their likeness, e.g., bank-trust company, but dissimilar entities may
also be related, e.g., hot-cold. A full treatment of the topic can be found in Budanitsky and Hirst (2005).
185
1987). Despite this challenge, we believe this is a worthy research direction, as many important problems
can benefit from the association of image content in relation to word meanings, such as automatic image
annotation, image retrieval and classification (e.g., (Leong et al, 2010)) as well as tasks in the domains
of of text-to-image synthesis, image harvesting and augmentative and alternative communication.
2 Related Work
Despite the large amount of work in computing semantic relatedness between words or similarity be-
tween images, there are only a few studies in the literature that associate the meaning of words and
pictures in a joint semantic space. The work most similar to ours was done by Westerveld (2000), who
employed LSA to combine textual words with simple visual features extracted from news images using
colors and textures. Although it was concluded that such a joint textual-visual representation model was
promising for image retrieval, no intensive evaluation was performed on datasets on a large scale, or
datasets other than the news domain. Similarly, Hare et al (2008) compared different methods such as
LSA and probabilistic LSA to construct joint semantic spaces in order to study their effects on automatic
image annotation and semantic image retrieval, but their evaluation was restricted exclusively to the
Corel dataset, which is somewhat idealistic and not reflective of the challenges presented by real-world,
noisy images.
Another related line of work by Barnard and Forsyth (2001) used a generative hierarchical model
to learn the associative semantics of words and images for improving information retrieval tasks. Their
approach was supervised and evaluated again only on the Corel dataset.
More recently, Feng and Lapata (2010) showed that it is possible to combine visual representations
of word meanings into a joint bimodal representation constructed by using latent topics. While their
work focused on unifying meanings from visual and textual data via supervised techniques, no effort
was made to compare the semantic relatedness between arbitrary pairs of word and image.
3 Bag of Visual Codewords
Inspired by the bag-of-words approach employed in information retrieval, the ?bag of visual codewords?
is a similar technique used mainly for scene classification (Yang et al, 2007). Starting with an image
collection, visual features are first extracted as data points from each image, characterizing its appear-
ance. By projecting data points from all the images into a common space and grouping them into a large
number of clusters such that similar data points are assigned to the same cluster, we can treat each cluster
as a ?visual codeword? and express every image in the collection as a ?bag of visual codewords?. This
representation enables the application of methods used in text retrieval to tasks in image processing and
computer vision.
Typically, the type of visual features selected can be global ? suitable for representation in all images,
or local ? specific to a given image type and task requirement. Global features are often described using a
continuous feature space, such as color histogram in three different color spaces (RGB, HSV and LAB),
or textures using Gabor and Haar wavelets (Makadia et al, 2008). In comparison, local features such as
key points (Fei-Fei and Perona, 2005) are often distinct across different objects or scenes. Regardless of
the features used, visual codeword generation involves the following three important phases.
1. Feature Detection: The image is divided into partitions of varying degrees of granularity from
which features can be extracted and represented. Typically, we can employ normalized cuts to
divide an image into irregular regions, or apply uniform segmentation to break it into smaller
but fixed grids, or simply locate information-rich local patches on the image using interest point
detectors.
2. Feature Description: A descriptor is selected to represent the features that are being extracted
from the image. Typically, feature descriptors (global or local) are represented as numerical vec-
tors, with each vector describing the feature extracted in each region. This way, an image is
represented by a set of vectors from its constituent regions.
186
Figure 1: An illustration of the process of generating ?Bag of Visual Codewords?
3. Visual Codeword Generation: Clustering methods are applied to group vectors into clusters,
where the center of each cluster is defined as a visual codeword, and the entire collection of clusters
defines the visual vocabulary for that image collection. Each image region or patch abstracted in
feature detection is now represented by the visual codeword mapped from its corresponding feature
vector.
The process of visual codeword generation is illustrated in Figure 1. Fei-Fei and Perona (2005) has
shown that, unlike most previous work on object or scene classification that focused on adopting global
features, local features are in fact extremely powerful cues. In our work, we use the Scale-Invariant
Feature Transform (SIFT) introduced by Lowe (2004) to describe distinctive local features of an image
in the feature description phase. SIFT descriptors are selected for their invariance to image scale, rotation,
differences in 3D viewpoints, addition of noise, and change in illumination. They are also robust across
affine distortions.
4 Semantic Vector Models
The underlying idea behind semantic vector models is that concepts can be represented as points in a
mathematical space, and this representation is learned from a collection of documents such that concepts
related in their meanings are near to one another in that space. In the past, semantic vector models
have been widely adopted by natural language processing researchers for tasks ranging from information
retrieval and lexical acquisition, to word sense disambiguation and document segmentation. Several
variants have been proposed, including the original vector space model (Salton et al, 1997) and the
Latent Semantic Analysis (Landauer and Dumais, 1997). Generally, vector models are attractive because
they can be constructed using unsupervised methods of distributional corpus analysis and assume little
language-specific requirements as long as texts can be reliably tokenized. Furthermore, various studies
(Kanerva, 1998) have shown that by using collaborative, distributive memory units to represent semantic
vectors, a closer correspondence to human cognition can be achieved.
While vector-space models typically require nontrivial algebraic machinery, reducing dimensions is
often key to uncover the hidden (latent) features of the terms distribution in the corpus, and to circumvent
the sparseness issue. There are a number of methods that have been developed to reduce dimensions ?
see e.g., Widdows and Ferraro (2008) for an overview. Here, we briefly describe one commonly used
187
technique, namely the Latent Semantic Analysis (LSA), noted for its effectiveness in previous works for
reducing dimensions.
In LSA, term co-occurrences in a corpus are captured by means of a dimensionality reduction op-
erated by a Singular Value Decomposition (SVD) on the term-by-document matrix T representing the
corpus. SVD is a well-known operation in linear algebra, which can be applied to any rectangular matrix
in order to find correlations among its rows and columns. SVD decomposes the term-by-document ma-
trix T into three matrices T = U?kVT where ?k is the diagonal k ? k matrix containing the singular k
values of T, ?1 ? ?2 ? ... ? ?k and U and V are column-orthogonal matrices. When the three matrices
are multiplied together the original term-by-document matrix is re-composed. Typically we can choose
k?  k obtaining the approximation T ' U?k?VT .
5 Semantic Relatedness between Words and Images
Although the bag of visual codewords has been extensively used in image classification and retrieval
tasks, and vector-space models are well explored in natural language processing, there has been little
connection between the two streams of research. Specifically, to our knowledge, there is no research work
that combines the two techniques to model multimodal meaning relatedness. Since we are exploring new
grounds, it is important to clarify what we mean by computing the semantic relatedness between a word
and an image, and how the nature of this task impacts our hypothesis. The assumptions below are
necessary to validate our findings:
1. Computing semantic relatedness between a word and an image involves comparing the concepts
invoked by the word and the salient objects in the image as well as their interaction. This goes
beyond simply identifying the presence or absence of specific objects indicated by a given word.
For instance, we expect a degree of relatedness between an image showing a soccer ball and the
word ?jersey,? since both invoke concepts like {sports, soccer, teamwork} and so on.
2. The semantics of an image is dependent on the focus, size and position of distinct objects identi-
fied through image segmentation. During labeling, we expect this segmentation to be performed
implicitly by the annotators. Although it is possible to focus one?s attention on specific objects via
bounding boxes, we are interested to harvest the meaning of an image using a holistic approach.
3. In the case of measuring the relatedness of a word that has multiple senses with a given image,
humans are naturally inclined to choose the sense that provides the highest relatedness inside the
pair. For example, an image of a river bank expectedly calls upon the ?river bank? sense of the
word ?bank? (and not ?financial bank? or other alternative word senses).
4. A degree of semantic relatedness can exist between any arbitrary word and image, on a scale
ranging from being totally unrelated to perfectly synonymous with each other. This is trivially
true, as the same property holds when measuring similarity between words and texts.
Next, we evaluate our hypothesis that we can measure the relatedness between a word and an image
empirically, using a parallel corpus of words and images as our dataset.
5.1 ImageNet
We use the ImageNet database (Deng et al, 2009), which is a large-scale ontology of images devel-
oped for advancing content-based image search algorithms, and serving as a benchmarking standard for
various image processing and computer vision tasks. ImageNet exploits the hierarchical structure of
WordNet by attaching relevant images to each synonym set (known as ?synset?), hence providing picto-
rial illustrations of the concept associated with the synset. On average, each synset contains 500-1000
images that are carefully audited through a stringent quality control mechanism.
Compared to other image databases with keyword annotations, we believe that ImageNet is suitable
for evaluating our hypothesis for three reasons. First, by leveraging on reliable keyword annotations in
WordNet (i.e., words in the synset and their gloss naturally serve as annotations for the corresponding
images), we can effectively circumvent the propagation of errors caused by unreliable annotations, and
consequently hope to reach more conclusive results for this study. Second, unlike other image databases,
188
ImageNet consists of millions of images, and it is a growing resource with more images added on a
regular basis. This aligns with our long-term goal of building a large-scale joint semantic space of images
and words. Finally, third, although we can search for relevant images using keywords in ImageNet,2
there is currently no method to query it in the reverse direction. Given a test image, we must search
through millions of images in the database to find the most similar image and its corresponding synset.
A joint semantic model can hopefully augment this shortcoming by allowing queries to be made in both
directions. Figure 2 shows an example of a synset and the corresponding images in ImageNet.
(a)
(b)
Joint Semantic Space of Words and Images
Synsets 167
Images 230,864
Words 1144
Nouns 783
Verbs 140
Adjectives 221
Image:Words ratio 202:1
Figure 2: (a) A subset of images associated with a node in ImageNet. The WordNet synset illustrated
here is {Dog, domestic dog, Canis familiaris} with the gloss: A member of the genus Canis (probably
descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in
many breeds; ?the dog barked all night? (b) A table showing statistical information on our joint semantic
space model
5.2 Dataset
For our experiments, we randomly select 167 synsets3 from ImageNet, covering a wide range of concepts
such as plants, mammals, fish, tools, vehicles etc. We perform a simple pre-processing step using Tree
Tagger (Schmid, 1994) and extract only the nouns. Multiwords are explicitly recognized as collocations
or named entities in the synset. Not considering part-of-speech distinctions, the vocabulary for synset
words is 352. The vocabulary for gloss words is 777. The shared vocabulary between them is 251.
There are a total of 230,864 images associated with the 167 synsets, with an average of 1383 images
per synset. We randomly select an image for each synset, thus obtaining a set of 167 test images in
total. The technique explained in Section 3 is used to generate visual codewords for each image in this
dataset.4 Each image is first pre-processed to have a maximum side length of 300 pixels. Next, SIFT
descriptors are obtained by densely sampling the image on 20x20 overlapping patches spaced 10 pixels
apart. K-means clustering is applied on a random subset of 10 million SIFT descriptors to derive a visual
vocabulary of 1,000 codewords. Each descriptor is then quantized into a visual codeword by assigning it
to the nearest cluster.
To create the gold-standard relatedness annotation, for each test image, six nouns are randomly se-
lected from its associated synset and gloss words, and six other nouns are again randomly selected from
the shared vocabulary words.5 In all, we have 167 x 12 = 2004 word-image pairs as our test dataset. Sim-
ilar to previous word similarity evaluations (Miller and Charles, 1998), we ask human annotators to rate
each pair on a scale of 0 to 10 to indicate their degree of semantic relatedness using the evaluation frame-
work outlined below, with 0 being totally unrelated and 10 being perfectly synonymous with each other.
To ensure quality ratings, for each word-image pair we used 15 annotators from Amazon Mechanical
2http://www.image-net.org/
3Not all synsets in ImageNet are annotated with images. We obtain our dataset from the Spring 2010 version of ImageNet
built around Wordnet 3.0.
4For our experiments, we obtained the visual codewords computed a priori from ImageNet. Test images are not used to
construct the model
512 data points are generally considered sufficient for reliable correlation measures (Vania Kovic, p.c.).
189
Synset {sunflower, helianthus} Synset {oxygen-mask} Synset {submarine , pigboat ,
sub , U-boat}
Gloss any plant of the genus
Helianthus having large flower
heads with dark disk florets and
showy yellow rays
Gloss a breathing device that
is placed over the mouth and
nose; supplies oxygen from an
attached storage tank
Gloss a submersible warship
usually armed with torpedoes
Relatedness Scores Relatedness Scores Relatedness Scores
color (5.13) dog (0.53) basketball (0.20) central (1.53) africa (0.80) brass (1.73)
floret (6.53) flower (9.67) device (5.47) family (0.80) door (1.67) good (2.40)
freshwater (2.40) hair (1.00) iron-tree (0.47) mouth (5.13) pacific (2.40) pigboat (6.47)
garden (6.60) head (3.80) oxygen-mask (7.73) tank (4.47) sub (8.20) submarine (9.67)
plant (8.47) ray (3.67) storage (3.07) supply (5.20) tail (0.93) torpedo (7.60)
sunflower (9.80) reed (2.27) nose (6.20) time (1.13) u-boat (7.47) warship (8.73)
Table 1: A sample of test images with their synset words and glosses : The number in parenthesis rep-
resents the numerical association of the word with the image (0-10). Human annotations reveal different
degree of semantic relatedness between the image and words in the synset or gloss.
Turk.6 Finally, the average of all 15 annotations for each word-image pair is taken as its gold-standard
relatedness score7. Note that only the pairs of images and words are provided to the annotators, and not
their synsets and gloss definitions.
The set of standard criteria underlying the cross-modal similarity evaluation framework shown here
is inspired by the semantic relations defined in Wordnet. These criteria were provided to the human
annotators, to help them decide whether a word and an image are related to each other.
1. Instance of itself: Does the image contain an entity that is represented by the word itself (e.g. an
image of ?Obama? vs the word ?Obama?) ?
2. Member-of Relation: Does the image contain an entity that is a member of the class suggested
by the word or vice versa (e.g. an image of an ?apple? vs the word ?fruits?) ?
3. Part-of Relation: Does the image contain an entity that is a part of a larger entity represented by
the word or vice versa (e.g. an image of a ?tree? vs the word ?forest?) ?
4. Semantically Related: Do both the word and the image suggest concepts that are related (e.g. an
image of troops at war vs the word ?peace?) ?
5. Semantically Close: Do both the word and the image suggest concepts that are not only related
but also close in meaning? (e.g. an image of troops at war vs the word ?gun?) ?
Criterion (1) basically tests for synonym relation. Criteria (2) and (3) are modeled after the hyponym-
hypernym and meronym-holonym relations in WordNet, which are prevalent among nouns. Note that
none of the criteria is preemptive over the others. Rather, we provide these criteria as guidelines in
a subjective evaluation framework, similar to the word semantic similarity task in Miller and Charles
(1998). Importantly, criterion (4) models dissimilar but related concepts, or any other relation that indi-
cates frequent association, while criterion (5) serves to provide additional distinction for pairs of words
and images on a higher level of relatedness toward similarity. In Table 1, we show sample images from
our test dataset, along with the annotations provided by the human annotators.
6We only allowed annotators with an approval rating of 97% or higher. Here, we expect some variance in the degree of
relatedness between the candidate words and images, hence annotations marked with all 10s or 0s are discarded due to lack of
distinctions in similarity relatedness
7Annotation guidelines and dataset can be downloaded at http://lit.csci.unt.edu/index.php/Downloads
190
5.3 Experiments
Following Erk and McCarthy (2009), who argued that word meanings are graded over their senses, we
believe that the meaning of an image is not limited to a set of ?best fitting? tags, but rather it exists as
a distribution over arbitrary words with varying degrees of association. Specifically, the focus of our
experiments is to investigate the correlation between automatic measures of such relatedness scores with
respect to human judgments.
To construct the joint semantic space of words and images, we use the SVD described in Section 4
to reduce the number of dimensions. To build each model, we use the 167 synsets from ImageNet and
their associated images (minus the held out test data), hence accounting for 167 latent dimensions. We
first represent the synsets as a collection of documents D, each document containing visual codewords
used to describe their associated images as well as textual words extracted from their gloss and synset
words. Thus, computing a cross-modal relatedness distance amounts to comparing the cosine similarity
of vectors representing an image to the vector representing a word in the term-document vector space.
Note that, unlike textual words, an image is represented by multiple visual codewords. Prior to computing
the actual cosine distance, we perform a weighted addition of vectors representing each visual codeword
for that image.
To illustrate, consider a single document di, representing the synset ?snail,? which consists of {cw0,
cw555, cw23, cw124, cw876, snail, freshwater, mollusk, spiral, shell}, where cwX represents a particular
visual codeword indexed from 0-9998, and the textual words are nouns extracted from the associated
synset and gloss. Given a test image I , it can be expressed as a bag of visual codewords {cw1 , ... , cwk}.
We first represent each visual codeword in I as a vector of length |D| using term-frequency inverse-
document-frequency (tf idf ) weighting, e.g., cwk=<0.4*d1, 0.2*d2, ... , 0.9*dm>, where m=167, and
perform an addition of k such vectors to form a final vector vi. To measure the semantic relatedness
between image I and a word w, e.g., ?snail,? we simply compute the cosine similarity between vi and
vw, where vw is also a vector of length |D| calculated using tf idf .
This paper seeks answers to the following questions. First, what is the relation between the discrim-
inability of the visual codewords and their ability to capture semantic relatedness between a word and an
image, as compared to the gold-standard annotation by humans? Second, given the unbalanced dataset
of images and words, can we use a relatively small number of visual codewords to derive such semantic
relatedness measures reliably? Third, what is the efficiency of an unsupervised vector semantic model in
measuring such relatedness, and is it applicable to large datasets?
Analogous to text-retrieval methods, we measure the discriminability of the visual codewords using
two weighting factors. The first is term-frequency (tf), which measures the number of times a codeword
appears in all images for a particular synset, while the second, image-term-frequency (itf), captures the
number of images using the codeword in a synset. For the two weighting schemes, we apply normal-
ization by using the total number of codewords for a synset (for tf weighting) and the total number of
images in a synset (for itf weighting).
We are interested to quantify the relatedness for pairs of words and images under two scenarios. By
ranking the 12 words associated with an image in reverse order of their relatedness to the image, we
can determine the ability of our models to identify the most related words for a given image (image-
centered). In the second scenario, we measure the relatedness of words and images regardless of the
synset they belong to, thus evaluating the ability of our methods to capture the relatedness between any
word and any image. This allows us to capture the correlation in an (arbitrary-image) scenario. For the
evaluations, we use the Spearman?s Rank correlation.
To place our results in perspective, we implemented two baselines and an upper bound for each of
the two scenarios above. The Random baseline randomly assigns ratings to each word-image pair on the
same 0 to 10 scale, and then measures the correlation to the human gold-standard. The Vector-Based (VB)
method is a stronger baseline aimed to study the correlation performance in the absence of dimensionality
reduction. As an upper bound, the Inter-Human-Agreement (IHA) measures the correlation of the rating
by each annotator against the average of the ratings of the rest of the annotators, averaged over the 167
synsets (for the image-centered scenario) and over the 2004 word-image pairs (for the arbitrary-image
scenario).
8For simplicity, we only show the top 5 visual codewords
191
Spearman?s Rank Coefficient (image-centered)
Top K codewords 100 200 300 400 500 600 700 800 900 1000
LSA tf 0.228 0.325 0.273 0.242 0.185 0.181 0.107 0.043 -0.018 0.000
LSA tf (norm) 0.233 0.339 0.293 0.254 0.202 0.180 0.124 0.047 -0.012 0.000
LSA tf*itf 0.268 0.317 0.256 0.248 0.219 0.166 0.081 -0.004 -0.037 0.000
LSA tf*itf (norm) 0.252 0.327 0.257 0.246 0.211 0.153 0.097 0.002 -0.042 0.000
VB tf 0.243 0.168 0.101 0.055 -0.021 -0.084 -0.157 -0.210 -0.236 -0.332
VB tf (norm) 0.240 0.181 0.110 0.062 -0.010 -0.082 -0.152 -0.204 -0.235 -0.332
VB tf*itf 0.262 0.181 0.107 0.065 -0.019 -0.081 -0.156 -0.211 -0.241 -0.332
VB tf*itf (norm) 0.257 0.180 0.116 0.068 -0.014 -0.079 -0.150 -0.250 -0.237 -0.332
Random 0.001 0.018 0.016 -0.008 0.008 0.005 -0.001 0.014 -0.035 0.012
IHA 0.687
Spearman?s Rank Coefficient (arbitrary-image)
Top K codewords 100 200 300 400 500 600 700 800 900 1000
LSA tf 0.236 0.341 0.291 0.249 0.208 0.183 0.106 0.033 -0.039 0.000
LSA tf (norm) 0.230 0.353 0.301 0.271 0.220 0.186 0.115 0.032 -0.029 0.000
LSA tf*itf 0.291 0.332 0.289 0.262 0.235 0.172 0.092 0.008 -0.041 0.000
LSA tf*itf (norm) 0.277 0.345 0.292 0.269 0.234 0.164 0.098 0.015 -0.046 0.000
VB tf 0.272 0.195 0.119 0.059 -0.012 -0.088 -0.164 -0.218 -0.240 -0.339
VB tf (norm) 0.277 0.207 0.130 0.069 -0.003 -0.083 -0.160 -0.215 -0.242 -0.339
VB tf*itf 0.287 0.206 0.127 0.062 -0.008 -0.085 -0.161 -0.214 -0.241 -0.339
VB tf*itf (norm) 0.286 0.212 0.132 0.071 -0.005 -0.081 -0.158 -0.214 -0.241 -0.339
Random -0.024 -0.014 0.015 -0.015 -0.004 -0.014 0.024 -0.009 -0.007 0.007
IHA 0.764
Table 2: Correlation of automatically generated scores with human annotations on cross-modal semantic
relatedness, as performed on the ImageNet test dataset of 2004 pairs of word and image. Correlation
figures scoring the highest within a weighting scheme are marked in bold, while those scoring the highest
across weighting schemes and within a visual vocabulary size are underlined.
6 Discussion
Our experimental results are shown in Table 2. A somewhat surprising observation is the consistency of
correlation figures between the two scenarios. In both scenarios, a representative set of 200 visual code-
words is sufficient to consistently score the highest correlation ratings across the 8 weighting schemes.
Intuitively, based on the experimental results, automatically choosing the top 10% or 20% of the visual
codewords seems to suffice and gives optimal correlation figures, but requires further justification. Con-
versely, the relatively simple weighting scheme using tf (normalized) produces the highest correlation in
six visual codeword sizes (K=200,300,400,700,800,900) for the image-centered scenario, as well as in
another six visual codeword sizes (K=200,300,400,600,700,900) for the arbitrary-image scenario. Un-
like stopwords in text retrieval accounting for most of the highest tf scores, visual codewords weighted
by the same scheme tf and a similar tf (normalized) scheme seem to be the most discriminative. The
correlation for including the entire visual vocabulary set (1000) produces identical results for all vector-
based and LSA weighting schemes, as images across synsets are now encoded by the same set of visual
codewords without discrimination between them.
Dimensionality reduction using SVD gains an advantage over the vector-based method for both sce-
narios, with the highest correlation rating in LSA (200 visual codeword, tf(norm)) achieving 0.077 points
better than the corresponding highest correlation in Vector-based (100 visual codeword, tf*itf ) for the
image-centered scenario, representing a 29.3% improvement. Similarly, in the arbitrary-image scenario,
the increase in correlation from 0.287 (VB tf*itf at 100 visual codeword) to 0.353 (LSA tf(norm) at
200 visual codeword) underlines a gain of approximately 23.0%. Overall, the arbitrary-image scenario
also scores consistently higher than the image-centered scenario under similar experimental conditions.
For instance, for the top 200 visual words, the same weighting schemes produce consistently lower
correlation figures for the image-centered scenario. This is also true for the Inter-Human-Agreement
score, which is higher in the arbitrary-image scenario (0.764) compared to the image-centered scenario
(0.687). Note that for all the experiments, the semantic relatedness scores generated from the semantic
vector space are significantly more correlated with the human gold-standard than the random baselines.
192
(a) (b)
Figure 3: (a) Correlation performance, and (b) Classification accuracy, as more data is added to construct
the semantic space model.
To investigate the effectiveness of the model when scaling up to large datasets, we employ the best
combination of weighting scheme and vocabulary size shown in Table 2, i.e., a visual vocabulary size
of 200 and tf (normalized) weighting for LSA, and vocabulary size of 100 and tf*itf weighting for the
vector-based model, and incrementally construct models ranging from 167 synsets to 800 synsets (all
randomly selected from ImageNet). We then measure the correlation of relatedness scores generated
using the same test dataset with respect to human annotations. The dataset was randomly selected to in-
crease by approximately five times, from a total of 230,864 images with 878 words to a total of 1,014,528
images with 3887 words. Furthermore, for each unseen test image taken from Synset Si and the associ-
ated 12 candidate words, we evaluate the ability of the model to identify which of the candidate words
actually appear in the gloss or the synset of Si, in a task we term as word classification. Here, the top
six words are predictably classified as those appearing in Si while the last six are classified as outside
of Si , after all 12 words are ranked in reverse order of their relatedness to the test image. We measure
the accuracy of the word classification task using TP+TN2004 , where TP is the number of words correctly
classified as synset or gloss words, and TN is the number of words correctly classified as outside of
synset or gloss, both summed over the 2004 pairs of words and images.
As shown in Figure 3, when a small number of synsets (33) was added to the original semantic space,
correlation with human ratings increased steeply to around 0.45 and higher for LSA in both scenarios,
while the vector-based method suffers a slight decrease in correlation ratings from 0.262 to 0.251 (image-
centered) and from 0.287 to 0.278 (arbitrary-image). As more images and words are added, correlation
for the vector-based model continues to decrease markedly. Comparatively, LSA is less sensitive to data
scaling, as correlation figures for both scenarios decreases slightly but stays within a 0.40 to 0.45 range.
Additionally, we infer that LSA is consistently more effective than the vector-based model in the words
classification task (as also seen in Figure 3). Even with more data added to the semantic space, word
classification accuracy stays consistently at 0.7 for LSA, while it drops to 0.535 for the vector-based
model at a synset size of 800.
7 Conclusion
In this paper, we provided a proof of concept in quantifying the semantic relatedness between words and
images through the use of visual codewords and textual words in constructing a joint semantic vector
space. Our experiments showed that the relatedness scores have a positive correlation to human gold-
standards, as measured using a standard evaluation framework.
We believe many aspects of this work can be explored further. For instance, other visual codeword
attributes, such as pixel coordinates, can be employed in a structured vector space along with the existing
model for improving vector similarity measures. To improve textual words coverage, a potentially effec-
193
tive way would be to create mappings from WordNet synsets to Wikipedia entries, where the concepts
represented by the synsets are discussed in detail. We also plan to study the applicability of the joint
semantic representation model to tasks such as automatic image annotation and image classification.
Acknowledgments
This material is based in part upon work supported by the National Science Foundation CAREER award
#0747340 and IIS award #1018613. Any opinions, findings, and conclusions or recommendations ex-
pressed in this material are those of the authors and do not necessarily reflect the views of the National
Science Foundation.
References
Barnard, K. and D. Forsyth (2001). Learning the semantics of words and pictures. In Proceedings of International
Conference on Computer Vision.
Biederman, I. (1987). Recognition-by-components: A theory of human image understanding. In Psychological
Review, Volume 94, pp. 115?147.
Budanitsky, A. and G. Hirst (2005). Evaluating wordnet-based measures of lexical semantic relatedness. In
Computational Linguistics, Volume 32.
Deng, J., W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei (2009). ImageNet: A Large-Scale Hierarchical Image
Database. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.
Erk, K. and D. McCarthy (2009). Graded word sense assignment. In Proceedings of Empirical Methods in Natural
Language Processing.
Fei-Fei, L. and P. Perona (2005). A bayesian hierarchical model for learning natural scene categories. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern Recognition.
Feng, Y. and M. Lapata (2010). Visual information in semantic representation. In Proceedings of the Annual
Conference of the North American Chapter of the ACL.
Goldberger, J., S. Gordon, and H. Greenspan (2003). An efficient image similarity measure based on approxima-
tions of kl-divergence between two gaussian mixtures. In Proceedings of IEEE International Conference on
Computer Vision.
Hare, J. S., S. Samangooei, P. H. Lewis, and M. S. Nixon (2008). Investigating the performance of auto-annotation
and semantic retrieval using semantic spaces. In Proceedings of the international conference on content-based
image and video retrieval.
Kanerva, P. (1998). Sparse distributed memory. In MIT Press.
Landauer, T. and S. Dumais (1997). A solution to platos problem: The latent semantic analysis theory of acquisi-
tion. In Psychological Review, Volume 104, pp. 211?240.
Leong, C. W., R. Mihalcea, and S. Hassan (2010). Text mining for automatic image tagging. In Proceedings of the
International Conference on Computational Linguistics.
Lowe, D. (2004). Distinctive image features from scale-invariant keypoints. In International Journal of Computer
Vision.
Makadia, A., V. Pavlovic, and S. Kumar (2008). A new baseline for image annotation. In Proceedings of European
Conference on Computer Vision.
Miller, G. (1995). Wordnet: A lexical database for english. In Communications of the ACM, Volume 38, pp. 39?41.
Miller, G. and W. Charles (1998). Contextual correlates of semantic similarity. Language and Cognitive Pro-
cesses 6(1).
Potter, M. C. and B. A. Faulconer (1975). Time to understand pictures and words. In Nature, Volume 253, pp.
437?438.
Salton, G., A. Wong, and C. Yang (1997). A vector space model for automatic indexing. In Readings in Information
Retrieval, pp. 273?280. San Francisco, CA: Morgan Kaufmann Publishers.
Schmid, H. (1994). Probabilistic part-of-speech tagging using decision trees. In Proceedings of the International
Conference on New Methods in Language Processing.
Smeulders, A. W., M. Worring, S. Santini, A. Gupta, and R. Jain (2000). Content-based image retrieval at the
end of the early years. In IEEE Transactions on Pattern Analysis and Machine Intelligence, Volume 22, pp.
1349?1380.
Westerveld, T. (2000). Image retrieval: Context versus context. In Content-Based Multimedia Information Access.
Widdows, D. and K. Ferraro (2008). Semantic vectors: a scalable open source package and online technology man-
agement application. In Proceedings of the Sixth International Language Resources and Evaluation (LREC?08).
Yang, J., Y.-G. Jiang, A. G. Hauptmann, and C.-W. Ngo (2007). Evaluating bag-of-visual-words representations
in scene classification. In ACM Multimedia Information Retrieval Workshop.
194
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 134?142,
Baltimore, Maryland USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Automated Scoring of Speaking Items in an Assessment for Teachers of
English as a Foreign Language
Klaus Zechner, Keelan Evanini, Su-Youn Yoon, Lawrence Davis,
Xinhao Wang, Lei Chen, Chong Min Lee, Chee Wee Leong
Educational Testing Service (ETS)
Princeton, NJ 08541, USA
{kzechner,kevanini,syoon,ldavis,xwang002,lchen,clee001,cleong}@ets.org
Abstract
This paper describes an end-to-end proto-
type system for automated scoring of spo-
ken responses in a novel assessment for
teachers of English as a Foreign Language
who are not native speakers of English.
The 21 speaking items contained in the as-
sessment elicit both restricted and moder-
ately restricted responses, and their aim is
to assess the essential speaking skills that
English teachers need in order to be effec-
tive communicators in their classrooms.
Our system consists of a state-of-the-art
automatic speech recognizer; multiple fea-
ture generation modules addressing di-
verse aspects of speaking proficiency, such
as fluency, pronunciation, prosody, gram-
matical accuracy, and content accuracy; a
filter that identifies and flags problematic
responses; and linear regression models
that predict response scores based on sub-
sets of the features. The automated speech
scoring system was trained and evaluated
on a data set involving about 1,400 test
takers, and achieved a speaker-level cor-
relation (when scores for all 21 responses
of a speaker are aggregated) with human
expert scores of 0.73.
1 Introduction
As English has become increasingly important as a
language of international business, trade, science,
and communication, efforts to promote teaching
English as a Foreign Language (EFL) have seen
substantially more emphasis in many non-English-
speaking countries worldwide in recent years. In
addition, the prevailing trend in English pedagogy
has been to promote the use of spoken English in
the classroom, as opposed to the respective native
languages of the EFL learners. However, due to
the high demand for EFL teachers in many coun-
tries, the training of these teachers has not always
caught up with these high expectations, so there is
a need for both governmental and private institu-
tions involved in the employment and training of
EFL teachers to assess their competence in the En-
glish language, as well as in English pedagogy.
Against this background, we developed a lan-
guage assessment for EFL teachers who are not
native speakers of English that addresses the four
basic English language skills of Reading, Listen-
ing, Writing and Speaking. This paper focuses
only on the speaking portion of the English assess-
ment, and, in particular, on the system that we de-
veloped to automatically compute scores for test
takers? spoken responses.
Several significant challenges needed to be ad-
dressed during the course of building this auto-
mated speech scoring system, including, but not
limited to:
? The 21 Speaking items belong to 8 differ-
ent task types with different characteristics;
therefore, we had to select features and build
scoring models for each task type separately.
? The test takers speak a variety of native lan-
guages, and thus have very different non-
native accents in their spoken English. Fur-
thermore, the test takers also exhibit a wide
range of speaking proficiency levels, which
contributes to the diversity of their spoken re-
sponses. Our speech recognizer therefore had
to be trained and adapted to a large database
of non-native speech.
? Since content accuracy is very important for
the types of tasks contained in the test, even
small error rates by the automatic speech
recognition (ASR) system can lead to a no-
ticeable impact on feature performance. This
fact motivated the development of a set of
134
features that are robust to speech recognition
errors.
? A significant amount of responses (more than
7%) exhibit issues that make them hard or
impossible to score automatically, e.g., high
noise levels, background speech, etc. We
therefore implemented a filter to identify
these non-scorable responses automatically.
The paper is organized as follows: Section 2
discusses related work; in Section 3, we present
the data used for system training and evaluation;
Section 4 describes the system architecture of the
automated speech scoring system. We detail the
methods we used to build our system in Section 5,
followed by an overview of the results in Section
6. Section 7 discusses our findings; finally, Sec-
tion 8 concludes the paper.
2 Related Work
Automated speech processing and scoring tech-
nology has been applied to a variety of domains
over the course of the past two decades, includ-
ing evaluation and tutoring of children?s literacy
skills (Mostow et al., 1994), preparation for high
stakes English proficiency tests for institutions of
higher education (Zechner et al., 2009), evalua-
tion of English skills of foreign-based call center
agents (Chandel et al., 2007), and evaluation of
aviation English (Pearson Education, Inc., 2011),
to name a few (for a comprehensive overview, see
(Eskenazi, 2009)).
Most of these applications elicit restricted
speech from the participants, and the most com-
mon item type by far is the Read Aloud, in which
the speaker reads a sentence or collection of sen-
tences out loud. Due to the constrained nature
of this task, it is possible to develop ASR sys-
tems that are relatively accurate, even with heav-
ily accented non-native speech. Several types of
features related to a non-native speaker?s ability
to produce English sounds and speech patterns
effectively have been extracted from these types
of responses. Some of the best performing of
these types of features include pronunciation fea-
tures, such as a phone?s spectral match to na-
tive speaker acoustic models (Witt, 1999) and a
phone?s duration compared to native speaker mod-
els (Neumeyer et al., 2000); fluency features, such
as the rate of speech, mean pause length, and num-
ber of disfluencies (Cucchiarini et al., 2000); and
prosody features, such as F0 and intensity slope
(Hoenig, 2002).
In addition to the large majority of applications
that elicit restricted speech, a small number of ap-
plications have also investigated automated scor-
ing of non-native spontaneous speech, in order
to more fully evaluate a speaker?s communicative
competence (e.g., (Cucchiarini et al., 2002) and
(Zechner et al., 2009)). In these systems, the same
types of pronunciation, fluency, and prosody fea-
tures can be extracted; furthermore, features re-
lated to additional aspects of a speaker?s profi-
ciency in the non-native language can be extracted,
such as vocabulary usage (Yoon et al., 2012), syn-
tactic complexity (Bernstein et al., 2010a; Chen
and Zechner, 2011), and topical content (Xie et al.,
2012).
As described in Section 1, the domain for the
automated speaking assessment investigated in
this study is teachers of EFL around the world.
Based on the fact that many of the item types are
designed to assess the test taker?s ability to pro-
ductively use English constructions and linguis-
tic units that commonly recur in English teach-
ing environments, several of the item types elicit
semi-restricted speech (see Table 1 below for a de-
scription of the different item types). These types
of responses fall somewhere between the heavily
restricted speech elicited by a Read Aloud task
and unconstrained spontaneous speech. In these
semi-restricted responses, the test taker may be
provided with a set of lexical items that should
be used to form a sentence; in addition, the test
taker is often asked to make the sentence conform
to a given grammatical template. Thus, the re-
sponses provided for a given prompt of this type
by multiple different speakers will often overlap
with each other; however, it is not possible to
specify a complete list of all possible responses.
These types of items have only infrequently been
examined in the context of automated speech scor-
ing. Some related item types that have been
explored previously include the Sentence Build
and Short item types described in (Bernstein et
al., 2010b); however, those item types typically
elicited a much narrower range of responses than
the semi-restricted ones in this study.
3 Data
The data used in this study was drawn from a pilot
administration of a language assessment for teach-
135
ers of English as a Foreign Language. This test
is designed to assess the ability of a non-native
teacher of English to use English in classroom set-
tings. The language forms and functions included
in this test are based on the materials included in a
curriculum that the test takers studied prior to tak-
ing the assessment. The assessment includes items
that cover the four language skills: Reading, Lis-
tening, Writing, and Speaking. There are a total of
8 different types of Speaking items included in the
assessment. These can be divided into the follow-
ing two categories, depending on how constrained
the test taker?s response is:
? Restricted Speech: In these item types, all
of the linguistic content expected in the
test taker?s response is presented in the test
prompt, and the test taker is asked to read or
repeat it aloud.
? Semi-restricted Speech: In these item types, a
portion of the linguistic content is presented
in the prompt, and the test taker is required to
provide the remaining content to formulate a
complete response.
Sets of 7 Speaking items are presented to the
test taker in thematic units, called ?lessons?, based
on their instructional goals; in total, each test taker
completed three lessons, and thus responded to 21
Speaking items. Table 1 presents descriptions of
the 8 different item types included in the assess-
ment.
The numbers of responses provided by the test
takers to each type (along with their respective re-
sponse durations) are as follows: four Multiple
Choice (10 seconds each), six Read Aloud (four 40
second responses and two 60 second responses),
two Repeat Aloud (15 seconds each), one Incom-
plete Sentence (20 seconds), one Key Words (15
seconds), five Chart (four 20 seconds and one 40
seconds), one Keyword Chart (15 seconds), and
one Visuals (15 seconds). Thus, each test taker
provided a total of approximately 9 minutes of au-
dio.
The responses were all double-scored by trained
human raters on a three-point scale (1 - 3). For
the Restricted Speech items, the raters assessed
the test taker?s pronunciation, pacing, and intona-
tion. For the Semi-restricted Speech items, the re-
sponses were also scored holistically on a 3-point
scale, but raters were also asked to take into ac-
count the appropriateness of the language used
Restricted Speech
Type Description
Multiple
Choice
(MC)
The test taker selects the correct
option and reads it aloud
Read Aloud
(RA)
The test taker reads aloud a set
of classroom instructions
Repeat
Aloud (RP)
The test taker listens to a student
utterance twice and then repeats
it
Semi-restricted Speech
Type Description
Incomplete
Sentence
(IS)
The test taker is given a sentence
fragment and completes the sen-
tence according to the instruc-
tions
Key Words
(KW)
The test taker uses the key words
provided to speak a sentence as
instructed
Chart (CH) The test taker uses an example
from a language chart and then
formulates a similar sentence us-
ing a given grammatical pattern
Keyword
Chart (KC)
The test taker constructs a sen-
tence using keywords provided
and information in a chart
Visuals (VI) The test taker is given two visu-
als and is asked to give instruc-
tions to students based on the
graphical information
Table 1: Types of speaking items included in the
assessment
(e.g., grammatical accuracy and content correct-
ness) in addition to aspects of fluency and pronun-
ciation. For some responses, the raters were not
able to provide a score on the 1 - 3 scale, e.g.,
because the audio response contained no speech
input, the test taker responded in their native lan-
guage, etc. These responses are labeled NS for
Non-Scoreable.
After receiving scores, all of the responses
were transcribed using standard English orthogra-
phy (disfluencies, such as filled pauses and par-
tial words are also included in the transcriptions).
Then, the responses were partitioned (with no
speaker overlap) into five sets for the training and
evaluation of the ASR system and the linear re-
gression scoring models. The amount of data and
136
human score distributions in each of these parti-
tions are displayed in Table 2.
4 System Architecture
The automated scoring system used for the teach-
ers? spoken language assessment consists of the
following four components, which are invoked
one after the other in a pipeline fashion (ETS
SpeechRater
SM
, (Zechner et al., 2009; Higgins et
al., 2011)):
? an automated speech recognizer, generating
word hypotheses from input audio recordings
of the test takers? responses
? a feature computation module that generates
features based on the ASR output, e.g., mea-
suring fluency, pronunciation, prosody, and
content accuracy
? a filtering model that flags responses that
should not be scored automatically due to is-
sues with audio quality, empty responses, etc.
? linear regression scoring models that predict
the score for each response based on a set of
selected features
Furthermore, we use Praat (Boersma and
Weenick, 2012) to extract power and pitch from
the speech signal; this information is used for
some of the feature computation modules, as well
as for the filtering model.
The ASR is an HMM-based triphone system
trained on approximately 800 hours of non-native
speech from a different data set; a background
Language Model (LM) was also trained on the
same data set. Subsequently, 8 adapted LMs were
trained (with an interpolation weight of 0.9 for the
in-domain data) using the responses in the ASR
Training partition for the 8 different item types
listed in Table 1. The ASR system obtained an
overall word error rate (WER) of 13.0% on the
ASR Evaluation partition and 15.6% on the Model
Evaluation partition. As would be expected, the
ASR system performed best on the responses that
were most restricted by the test item and per-
formed worse on the responses that were less re-
stricted. The WER ranged from 11.4% for the
RA responses to 41.4% for the IS responses in the
Model Evaluation partition.
5 Methodology
5.1 Speech features
The feature computation components of our
speech scoring system compute more than 100
features based on a speaker?s response. They be-
long to the following broad dimensions of speak-
ing proficiency: fluency, pronunciation, prosody,
vocabulary usage, grammatical complexity and
accuracy, and content accuracy (Zechner et al.,
2009; Chen and Yoon, 2012; Chen et al., 2009;
Zechner et al., 2011; Yoon et al., 2012; Yoon and
Bhat, 2012; Zechner and Wang, 2013).
After initial feature generation, we selected a set
of about 10 features for each of the 8 item types,
based on the following considerations
1
(Zechner
et al., 2009; Xi et al., 2008):
? empirical performance, i.e., feature correla-
tion with human scores
? construct
2
relevance, i.e., to what extent the
feature measures aspects of speaking profi-
ciency that are considered to be relevant and
important by content experts
? overall construct coverage, i.e., the feature set
should include features from all relevant con-
struct dimensions
? feature independence, i.e., the inter-
correlation between any two features of the
set should be low
Furthermore, some features were transformed
(e.g., by applying the inverse or log function), in
order to increase the normality of their distribu-
tions (an assumption of linear regression classi-
fiers). All feature values that exceeded a thresh-
old of 4 standard deviations from the mean were
replaced by the respective threshold (outlier trun-
cation).
The composition of feature sets is slightly dif-
ferent for the two item type categories: for the 3
restricted item types, features related to fluency,
pronunciation, prosody and read/repeat accuracy
were chosen, whereas for the 5 semi-restricted
item types, vocabulary and grammar features were
also added to the set. Further, while accuracy
1
While automated feature selection is conceivable in prin-
ciple, in our experience it typically does not result in a feature
set that meets all of these criteria well.
2
A construct is the set of knowledge, skills, and abilities
measured by a test.
137
Partition Spk. Resp. Dur. 1 2 3 NS
ASR Training 773 16,049 116.7 1,587 (9.9) 4,086 (25.5) 8,796 (54.8) 1,580 (9.8)
ASR Development 25 525 3.8 53 (10.1) 133 (25.3) 327 (62.3) 12 (2.3)
ASR Evaluation 25 525 3.8 31 (5.9) 114 (21.7) 326 (62.1) 54 (10.3)
Model Training 300 6,300 45.8 675 (10.7) 1,715 (27.2) 3,577 (56.8) 333 (5.3)
Model Evaluation 300 6,300 45.7 647 (10.3) 1,637 (26.0) 3,487 (55.3) 529 (8.4)
Total 1,423 29,699 215.8 2,993 (9.38) 7,685 (25.14) 16,513 (58.26) 2,508 (7.22)
Table 2: Amount of data contained in each partition (speakers, responses, hours of speech) and distribu-
tion of human scores (percentages of scores per partition in brackets).
features for the restricted items were based only
on string alignment measures, content accuracy
features for the semi-restricted items were more
diverse, e.g., based on regular expressions, key-
words, and language model scores (Zechner and
Wang, 2013). Table 3 lists the features that were
used in the scoring models for restricted and semi-
restricted item types, along with sub-constructs
they measure and their description.
5.2 Filtering model
In order to automatically identify responses that
have technical issues (e.g., loud background noise)
or are otherwise not scorable (e.g., empty re-
sponses), a decision tree-based filtering model was
developed using a combination of features derived
from ASR output and from pitch and energy in-
formation (Yoon et al., 2011; Jeon and Yoon,
2012). The filtering model was tested on the scor-
ing model evaluation data, and obtained an ac-
curacy rate (the exact agreement between the fil-
tering model and a human rater concerning the
distinction between scorable and non-scorable re-
sponses) of 97%; it correctly identified 90% of the
non-scorable responses in the data set with a false
positive rate of 21% (recall=0.90, precision=0.79,
F-score=0.84).
5.3 Scoring models
We used the Model Training set to train 8 linear
regression models for the 8 different item types,
using the previously determined feature sets. We
used the features as independent variables in these
models and the summed scores of two human
raters as the dependent variable. These trained
scoring models were then employed to score re-
sponses of the Model Evaluation data (exclud-
ing responses marked as non-scorable by human
raters) and rounded to the nearest integer to predict
the final scores for each response. These scores
were then evaluated against the first human rater
score (H1).
Item N S-H1 H1-H2 WER (%)
RA 1653 0.34 0.51 11.4
RP 543 0.41 0.73 21.8
MC 1036 0.67 0.83 17.1
CH 1372 0.44 0.67 26.3
KW 275 0.45 0.67 28.7
KC 274 0.57 0.74 28.8
IS 260 0.46 0.69 41.4
VI 272 0.43 0.80 30.4
Table 4: Correlations between system and first hu-
man rater (S-H1) and between two human raters
(H1-H2), for all responses of each item type in the
Model Evaluation partition (N). The last column
provides the average ASR word error rate (WER)
in percent.
Additionally, for responses flagged as non-
scorable by the automatic filtering model, the sec-
ond human rater score (H2) was used as final
item score in order to mimic the operational sce-
nario where human raters score responses that are
flagged by the filtering model.
We also compute the agreement between sys-
tem and human raters based on a set of all 21 re-
sponses of a speaker. Score imputation was used
for responses that were labeled as non-scorable by
both the system and H2; in this case, the response
was given the mean score of the total scorable
responses from the same speaker. Similarly, the
same score imputation rule was applied to the H1
scores.
6 Results
Table 4 presents the Pearson correlation coeffi-
cients between human and automated scores for
the responses from the 8 different item types along
with the human-human correlation for each item
type. Furthermore, we also provide the word error
rates of the ASR system for the same 8 item types
in the last column of the table.
138
Feature Sub-construct Description
Content Ed1 Read/repeat accu-
racy / Fluency
Correctly read words per minute
Content Ed2 Read/repeat accu-
racy
Read/repeat word error rate
Content RegEx Content accuracy Matching of regular expressions
Content WER Content accuracy Response discrepancy from high scoring responses
Content NGram Content accuracy N-grams in response matching high scoring response n-
grams
Fluency Rate Fluency Speaking rate
Fluency Chunk Fluency Average length of contiguous word chunks
Fluency Sil1 Fluency Frequency of long silences
Fluency Sil2 Fluency / Grammar Proportion of long within-clause-silences to all within-
clause-silences
Fluency Sil3 Fluency Mean length of silences within a clause
Fluency Disfl1 Fluency Frequency of interruption points (repair, repetition, false
start)
Fluency Disfl2 Fluency Number of disfluencies per second
Fluency Disfl3 Fluency Frequency of repetitions
Pron Vowels Pronunciation Average vowel duration differences relative to a native-
speaker model
Prosody1 Prosody Percentage of stressed syllables
Prosody2 Prosody Mean deviation of time intervals between stressed syllables
Prosody3 Prosody Mean distance between stressed syllables
Vocab1 Vocabulary / Flu-
ency
Number of word types divided by utterance duration
Grammar POS Grammar Part-of-speech based distributional similarity score be-
tween a response and responses with different score levels
Grammar LM Grammar Global language model score (normalized by response
length)
Table 3: List of features used for item type scoring models, with the sub-constructs they represent and
descriptions.
139
Comparison Pearson r
S-H1 0.725
S-H2 0.742
H1-H2 0.934
Table 5: Speaker-level performance (Pearson r
correlations) computed over the sum of all 21
scores from each speaker, N=272
Sub-construct Restricted Semi-restricted
Content 0.33?0.67 0.34?0.61
Fluency 0.19?0.33 0.20?0.33
Pronunciation 0.20?0.22 0.13?0.31
Prosody 0.18?0.24 0.12?0.27
Grammar ? 0.23?0.49
Vocabulary ? 0.21?0.32
Table 6: Range of Pearson r correlations for dif-
ferent features with human scores (H1) by sub-
construct for restricted and semi-restricted item
types.
Table 5 presents the Pearson correlation coeffi-
cients between the speaker-level scores produced
by the automated scoring system (S) and the two
sets of human scores (H1 and H2). These speaker-
level scores were computed based on the sum of
all 21 scores from each speaker in the Model Eval-
uation partition. Responses that received a non-
scorable rating from the human raters were im-
puted, as described above. Furthermore, 28 speak-
ers were excluded from this analysis because they
had more than 7 non-scorable responses each.
3
Finally, Table 6 provides an overview of Pear-
son correlation ranges with human rater scores
(H1) for the different features used in the scoring
models, summarized by the sub-constructs that the
features represent.
7 Discussion
When looking at Table 4, we see that the inter-rater
reliability for human raters ranges between 0.51
(for RA items) and 0.83 (for MC items). Inter-
rater reliability varies less for the 5 semi-restricted
item types (0.67?0.80), compared to the 3 re-
stricted item types (0.51?0.83). As for automated
score correlations with human raters, the Pearson
r coefficients range from 0.34 (RA) to 0.67 (MC).
3
In an operational setting, these test takers would not re-
ceive a test score; instead, they would have the opportunity to
take the test again.
Again, the variability of Pearson r coefficients is
larger for the 3 restricted item types (0.34?0.67)
than for the 5 semi-restricted item types (0.43?
0.57). The degradation in correlation between the
inter-human results and the machine-human re-
sults varies from 0.16 (MC) to 0.37 (VI).
Speech recognition word error rate does not
seem to have a strong influence on model perfor-
mance (RA items have the lowest WER with S-
H1 r=0.34, but r=0.46 for IS items that have the
highest WER). However, we found other factors
that affect model performance negatively; for ex-
ample, multiple repeats of responses by test tak-
ers contribute to the large performance difference
between S-H1 and H1-H2 for the RP items. In
general, we conjecture that using features for a
larger set of sub-construct areas?in the case of
semi-restricted item types?may contribute to the
lower variation of scoring model performance for
this subset of the data.
As for speaker-level results (Table 5), the over-
all degradation between the inter-human correla-
tion and the system-human correlations is of a
similar magnitude (around 0.2) as observed for
most of the individual item types. Still, the
speaker-level correlation of 0.73 is 0.26 higher
than the average item type correlation between the
system and H1.
When we look into more detail at the Pearson
r correlations between individual features used in
the item type scoring models and human scores
(Table 6), we can see that features related to con-
tent accuracy exhibit a substantially stronger per-
formance (r=0.33?0.67) than features related to
most other sub-constructs of speaking proficiency,
namely fluency, pronunciation, prosody, and vo-
cabulary (r ? 0.2). One exception is features
related to grammar, where correlations with hu-
man scores are as high as 0.49. Since related work
on scoring speech using features indicative of flu-
ency, pronunciation, etc. showed higher correla-
tions (e.g., (Cucchiarini et al., 1997; Franco et al.,
2000; Zechner et al., 2009)), we conjecture that
the reason behind this difference is likely to be
found in the fact that the responses in this assess-
ment for teachers of English are quite short (6?
14 words on average for all items except for Read
Aloud items that are about 46 words on average).
Since content features are less reliant on longer
stretches of speech, they still work fairly well for
most items in our corpus.
140
Finally, while the proportion of words contained
in responses in restricted items is much larger than
those contained in responses in semi-restricted
items, these two item type categories are more
evenly distributed over the whole test, i.e., each
test taker responds to 9 semi-restricted and 12 re-
stricted items, and the item scores are then aggre-
gated for a final score with equal weight given to
each item score.
8 Conclusion
This paper presented an overview of an automated
speech scoring system that was developed for a
language assessment for teachers of English as a
Foreign Language (EFL) whose native language
is not English. We described the main compo-
nents of this prototype system and their perfor-
mance: the ASR system, features generated from
ASR output, a filtering model to flag non-scorable
responses, and finally a set of linear regression
models, one for each of 8 different types of test
items.
We found that overall, the correlation between
our speech scoring system?s predicted scores and
human rater scores range between 0.34 and 0.67,
evaluated on responses from 8 item types. Further-
more, we found that correlations based on com-
plete sets of 21 spoken responses per test taker im-
prove to around r = 0.73.
Given the many significant challenges of this
work, including 8 different item types in the as-
sessment, responses from speakers from different
native languages and speaking proficiency levels,
sub-optimal audio conditions for a part of the data,
and a relatively small data set for both ASR system
adaptation and linear regression model training,
we find that the overall performance achieved by
our automated speech scoring system was a good
starting point for an eventual deployment in a low-
stakes assessment context.
Future work will aim at improving the perfor-
mance of the prediction models by the addition of
more features addressing different aspects of the
construct as well as an improved filtering model
for flagging the different types of problematic re-
sponses. Furthermore, agreement between human
raters, in particular for read-aloud items, could be
improved by refining rater rubrics and additional
rater training and monitoring.
Acknowledgments
The authors would like to thank Anastassia Louk-
ina and Jidong Tao for their comments on an ear-
lier version of this paper, and are also indebted
to the anonymous reviewers of BEA-9 and ASRU
2013 for their valuable comments and suggestions.
References
Jared Bernstein, Jian Cheng, and Masanori Suzuki.
2010a. Fluency and structural complexity as pre-
dictors of L2 oral proficiency. In Proceedings of In-
terspeech.
Jared Bernstein, Alistair Van Moere, and Jian Cheng.
2010b. Validating automated speaking tests. Lan-
guage Testing, 27(3):355?377.
Paul Boersma and David Weenick. 2012. Praat: Doing
phonetics by computer, version 5.3.32. http://
www.praat.org.
Abhishek Chandel, Abhinav Parate, Maymon Ma-
dathingal, Himanshu Pant, Nitendra Rajput, Shajith
Ikbal, Om Deshmuck, and Ashish Verma. 2007.
Sensei: Spoken language assessment for call cen-
ter agents. In Proceedings of the IEEE Workshop on
Automatic Speech Recognition and Understanding
(ASRU).
Lei Chen and Su-Youn Yoon. 2012. Application of
structural events detected on ASR outputs for auto-
mated speaking assessment. In Proceedings of In-
terspeech.
Miao Chen and Klaus Zechner. 2011. Computing
and evaluating syntactic complexity features for au-
tomated scoring of spontaneous non-native speech.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics, pages 722?
731.
Lei Chen, Klaus Zechner, and Xiaoming Xi. 2009. Im-
proved pronunciation features for construct-driven
assessment of non-native spontaneous speech. In
Proceedings of NAACL-HLT.
Catia Cucchiarini, Helmer Strik, and Lou Boves. 1997.
Automatic evaluation of Dutch pronunciation by us-
ing speech recognition technology. In Proceedings
of the IEEE Workshop on Auotmatic Speech Recog-
nition and Understanding (ASRU).
Catia Cucchiarini, Helmer Strik, and Lou Boves. 2000.
Quantitative assessment of second language learn-
ers? fluency by means of automatic speech recogni-
tion technology. Journal of the Acoustical Society of
America, 107(2):989?999.
Catia Cucchiarini, Helmer Strik, and Lou Boves. 2002.
Quantitative assessment of second language learn-
ers? fluency: Comparisons between read and spon-
taneous speech. Journal of the Acoustical Society of
America, 111(6):2862?2873.
141
Maxine Eskenazi. 2009. An overview of spoken lan-
guage technology for education. Speech Communi-
cation, 51(10):832?844.
Horacio Franco, Leonardo Neumeyer, Vassilios Di-
galakis, and Orith Ronen. 2000. Combination of
machine scores for automatic grading of pronuncia-
tion quality. Speech Communication, 30(1-2):121?
130.
Derrick Higgins, Xiaoming Xi, Klaus Zechner, and
David M. Williamson. 2011. A three-stage ap-
proach to the automated scoring of spontaneous spo-
ken responses. Computer Speech and Language,
25(2):282?306.
Florian Hoenig. 2002. Automatic assessment of non-
native prosody ? Annotation, modelling, and evalu-
ation. In Proceedings of the International Sympo-
sium on Automatic Detection of Errors in Pronun-
ciation Training (ISADEPT), pages 21?30, Stock-
holm, Sweden.
Je Hun Jeon and Su-Youn Yoon. 2012. Acoustic
feature-based non-scorable response detection for an
automated speaking proficiency assessment. In Pro-
ceedings of Interspeech.
Jack Mostow, Steven F. Roth, Alexander G. Haupt-
mann, and Matthew Kane. 1994. A prototype read-
ing coach that listens. In Proceedings of the Twelfth
National Conference on Artificial Intelligence.
Leonardo Neumeyer, Horacio Franco, Vassilios Di-
galakis, and Mitchel Weintraub. 2000. Automatic
scoring of pronunciation quality. Speech Communi-
cation, 30:83?93.
Pearson Education, Inc. 2011. Versant
TM
Aviation English Test. http://www.
versanttest.com/technology/
VersantAviationEnglishTestValidation.
pdf.
Silke Witt. 1999. Use of speech recognition in
computer-assisted language learning. Ph.D. thesis,
Cambridge University.
Xiaoming Xi, Derrick Higgins, Klaus Zechner, and
David M. Williamson. 2008. Automated scoring of
spontaneous speech using SpeechRater v1.0. Edu-
cational Testing Service Research Report RR-08-62.
Shasha Xie, Keelan Evanini, and Klaus Zechner. 2012.
Exploring content features for automated speech
scoring. In Proceedings of the 2012 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 103?111, Montr?eal, Canada. Asso-
ciation for Computational Linguistics.
Su-Youn Yoon and Suma Bhat. 2012. Assessment of
ESL learners? syntactic competence based on sim-
ilarity measures. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 600?608, Jeju Island, Korea.
Association for Computational Linguistics.
Su-Youn Yoon, Keelan Evanini, and Klaus Zechner.
2011. Non-scorable response detection for auto-
mated speaking proficiency assessment. In Proceed-
ings of NAACL-HLT Workshop on Innovative Use of
NLP for Building Educational Applications.
Su-Youn Yoon, Suma Bhat, and Klaus Zechner. 2012.
Vocabulary profile as a measure of vocabulary so-
phistication. In Proceedings of the 7th Workshop on
Innovative Use of NLP for Building Educational Ap-
plications, NAACL-HLT, Montr?eal, Canada. Associ-
ation for Computational Linguistics.
Klaus Zechner and Xinhao Wang. 2013. Automated
content scoring of spoken responses in an assess-
ment for teachers of english. In Proceedings of
the 8th Workshop on Innovative Use of NLP for
Building Educational Applications, NAACL-HLT,
Atlanta. Association for Computational Linguistics.
Klaus Zechner, Derrick Higgins, Xiaoming Xi, and
David M. Williamson. 2009. Automatic scoring
of non-native spontaneous speech in tests of spoken
English. Speech Communication, 51(10):883?895.
Klaus Zechner, Xiaoming Xi, and Lei Chen. 2011.
Evaluating prosodic features for automated scoring
of non-native read speech. In Proceedings of the
IEEE Workshop on Automatic Speech Recognition
and Understanding (ASRU).
142
