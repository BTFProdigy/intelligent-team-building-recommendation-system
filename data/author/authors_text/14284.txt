Proceedings of NAACL HLT 2007, Companion Volume, pages 65?68,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
??ROVER: Improving System Combination with Classification
D. Hillard?, B. Hoffmeister?, M. Ostendorf?, R. Schlu?ter?, H. Ney?
?SSLI, Electrical Engineering Dept., University of Washington, Seattle, WA
{hillard,mo}@ee.washington.edu
?Informatik 6, Computer Science Dept., RWTH Aachen University, Aachen, Germany
{hoffmeister,schlueter,ney}@cs.rwth-aachen.de
Abstract
We present an improved system combination
technique, ??ROVER. Our approach obtains sig-
nificant improvements over ROVER, and is
consistently better across varying numbers of
component systems. A classifier is trained on
features from the system lattices, and selects
the final word hypothesis by learning cues to
choose the system that is most likely to be
correct at each word location. This approach
achieves the best result published to date on
the TC-STAR 2006 English speech recognition
evaluation set.
1 Introduction
State-of-the-art automatic speech recognition (ASR) sys-
tems today usually include multiple contrasting systems,
which are ultimately combined to produce the final hy-
pothesis. There is consensus that improvements from
combination are usually best when systems are suffi-
ciently different, but there is uncertainty about which sys-
tem combination method performs the best. In addition,
the success of commonly used combination techniques
varies depending on the number of systems that are com-
bined (Hoffmeister et al, 2007). In this work, we develop
a system combination method that outperforms all previ-
ously known techniques and is also robust to the number
of component systems. The relative improvements over
ROVER are particularly large for combination when only
using two systems.
The aim of system combination for ASR is to mini-
mize the expected word error rate (WER) given multiple
system outputs, which are ideally annotated with word
confidence information. The most widely used system
combination approach to date is ROVER (Fiscus, 1997).
It is a simple voting mechanism over just the top hy-
pothesis from each component system. Two alternatives
that incorporate information about multiple hypotheses
and leverage word posterior probabilities are confusion
network (CN) combination (Mangu et al, 2000; Ever-
mann and Woodland, 2000) and minimum Time Frame
Word Error (min-fWER) decoding (Hoffmeister et al,
2006), discussed further in the next section. Previous
work found that among ROVER, CN combination, and
min-fWER combination, no one method was consistently
superior across varying numbers and types of systems
(Hoffmeister et al, 2007).
The main contribution of this work is to develop an
approach that always outperforms other possible system
combination methods. We train a classifier to learn which
system should be selected for each output word, using
features that describe the characteristics of the compo-
nent systems. ROVER alignments on the 1-best hypothe-
ses are used for decoding, but many of the features are
derived from the system lattices. The classifier learns a
selection strategy (i.e. a decision function) from a devel-
opment set and then is able to make better selections on
the evaluation data then the current 1-best or lattice-based
system combination approaches.
Next, Section 2 describes previous work in system
combination techniques. Section 3 describes our ap-
proach, and Section 4 provides experiments and results.
Finally, we summarize the approach and findings in Sec-
tion 5.
2 Previous Work
Previous work in speech recognition system combination
has produced significant improvements over the results
possible with just a single system. The most popular, and
often best performing method is ROVER (Fiscus, 1997),
which selects the word that the most systems agree on
at a particular location (majority voting). An extended
version of ROVER also weights system votes by the word
confidence produced by the system (confidence voting).
Further improvements have been achieved by includ-
ing multiple system alternatives, with methods such as
Confusion Network Combination (CNC) (Evermann and
Woodland, 2000), or N-Best ROVER (Stolcke et al,
2000), which is a special case of CNC. Alternatively, the
combination can be performed at the frame level (min-
fWER) (Hoffmeister et al, 2006). Recent work found
that the best system combination method depended on the
number of systems being combined (Hoffmeister et al,
2007). When only two systems are available, approaches
considering multiple alternatives per system were bet-
65
ter, but as the number of systems increased the standard
ROVER with confidence scores was more robust and
sometimes even better than CNC or min-fWER combi-
nation.
Another approach (Zhang and Rudnicky, 2006) used
two stages of neural networks to select a system at each
word, with features that capture word frequency, posteri-
ors at the frame, word, and utterance level, LM back-off
mode, and system accuracy. They obtained consistent but
small improvements over ROVER: between 0.7 and 1.7%
relative gains for systems with about 30% WER.
3 Approach
We develop a system that uses the ROVER alignment but
learns to consistently make better decisions than those
of standard ROVER. We call the new system ??ROVER,
where the ?? stands for improved results, and/or intelligent
decisions. The following sections discuss the compo-
nents of our approach. First, we emulate the approach
of ROVER in our lattice preprocessing and system align-
ment. We then introduce new methods to extract hypoth-
esis features and train a classifier that selects the best
system at each slot in the alignment.
3.1 Lattice Preparation
Our experiments use lattice sets from four different sites.
Naturally, these lattice sets differ in their vocabulary,
segmentation, and density. A compatible vocabulary is
essential for good combination performance. The main
problems are related to contractions, e.g. ?you?ve? and
?you have?, and the alternatives in writing foreign names,
e.g. ?Schro?der? and ?Schroder?. In ASR this problem is
well-known and is addressed in scoring by using map-
pings that allow alternative forms of the same word.
Such a mapping is provided within the TC-STAR Eval-
uation Campaign and we used it to normalize the lat-
tices. In case of multiple alternative forms we used only
the most frequent one. Allowing multiple parallel alter-
natives would have distorted the posterior probabilities
derived from the lattice. Furthermore, we allowed only
one-to-one or one-to-many mappings. In the latter case
we distributed the time of the lattice arc according to the
character lengths of the target words.
In order to create comparable posterior probabilities
over the lattice sets we pruned them to equal average
density. The least dense lattice set defined the target
density: around 25 for the development and around 30
for the evaluation set.
Finally, we unified the segmentation by concatenat-
ing the lattices recording-wise. The concatenation was
complicated by segmentations with overlapping regions,
but our final concatenated lattices scored equally to the
original lattice sets. The unified segmentation is needed
for lattice-based system combination methods like frame-
based combination.
3.2 System Alignments
In this work we decided to use the ROVER alignment as
the basis for our system combination approach. At first
glance the search space used by ROVER is very limited
because only the first-best hypothesis from each compo-
nent system is used. But the oracle error rate is often very
low, normally less than half of the best system?s error rate.
The ROVER alignment can be interpreted as a confu-
sion network with an equal number of arcs in each slot.
The number of arcs per slot equals the number of compo-
nent systems and thus makes the training and application
of a classifier straightforward.
For the production of the alignments we use a stan-
dard, dynamic programming-based matching algorithm
that minimizes the global cost between two hypothesis.
The local cost function is based on the time overlap of
two words and is identical to the one used by the ROVER
tool. We also did experiments with alternative local cost
functions based on word equalities, but could not outper-
form the simple, time overlap-based distance function.
3.3 Hypothesis Features
We generate a cohort of features for each slot in the
alignment, which is then used as input to train the classi-
fier. The features incorporate knowledge about the scores
from the original systems, as well as comparisons among
each of the systems. The following paragraphs enumerate
the six classes of feature types used in our experiments
(with their names rendered in italics).
The primary, and most important feature class covers
the basic set of features which indicate string matches
among the top hypotheses from each system. In addition,
we include the systems? frame-based word confidence.
These features are all the information available to the
standard ROVER with confidences voting.
An additional class of features provides extended con-
fidence information about each system?s hypothesis. This
feature class includes the confusion network (CN) word
confidence, CN slot entropy, and the number of alter-
natives in the CN slot. The raw language model and
acoustic scores are also available. In addition, it in-
cludes a frame-based confidence that is computed from
only the acoustic model, and a frame-based confidence
that is computed from only the language model score.
Frame-based confidences are calculated from the lattices
according to (Wessel et al, 1998); the CN-algorithm is
an extension of (Xue and Zhao, 2005).
The next class of features describes durational aspects
of the top hypothesis for each system, including: charac-
ter length, frame duration, frames per character, and if the
word is the empty or null word. A feature that normalizes
the frames per character by the average over a window
of ten words is also generated. Here we use characters
as a proxy for phones, because phone information is not
available from all component systems.
We also identify the system dependent top error words
for the development set, as well as the words that occur
to the left and right of the system errors. We encode this
information by indicating if a system word is on the list
of top ten errors or the top one hundred list, and likewise
if the left or right system context word is found in their
corresponding lists.
In order to provide comparisons across systems, we
compute the character distance (the cost of aligning the
words at the character level) between the system words
66
and provide that as a feature. In addition, we include the
confidence of a system word as computed by the frame-
wise posteriors of each of the other systems. This allows
each of the other systems to ?score? the hypothesis of
a system in question. These cross-system confidences
could also act as an indicator for when one system?s hy-
pothesis is an OOV-word for another system. We also
compute the standard, confidence-based ROVER hypoth-
esis at each slot, and indicate whether or not a system
agrees with ROVER?s decision.
The last set of features is computed relative to the
combined min-fWER decoding. A confidence for each
system word is calculated from the combined frame-wise
posteriors of all component systems. The final feature
indicates whether each system word agrees with the com-
bined systems? min-fWER hypothesis.
3.4 Classifier
After producing a set of features to characterize the sys-
tems, we train a classifier with these features that will
decide which system will propose the final hypothesis at
each slot in the multiple alignment. The target classes
include one for each system and a null class (which is
selected when none of the system outputs are chosen, i.e.
a system insertion).
The training data begins with the multiple alignment
of the hypothesis systems, which is then aligned to the
reference words. The learning target for each slot is the
set of systems which match the reference word, or the
null class if no systems match the reference word. Only
slots where there is disagreement between the systems?
1-best hypotheses are included in training and testing.
The classifier for our work is Boostexter (Schapire and
Singer, 2000) using real Adaboost.MH with logistic loss
(which outperformed exponential loss in preliminary ex-
periments). Boostexter trains a series of weak classifiers
(tree stumps), while also updating the weights of each
training sample such that examples that are harder to
classify receive more weight. The weak classifiers are
then combined with the weights learned in training to
predict the most likely class in testing. The main dimen-
sions for model tuning are feature selection and number
of iterations, which are selected on the development set
as described in the next section.
4 Experiments
We first perform experiments using cross-validation on
the development set to determine the impact of different
feature classes, and to select the optimal number of iter-
ations for Boostexter training. We then apply the models
to the evaluation set.
4.1 Experimental setup
In our experiments we combine lattice sets for the English
task of the TC-STAR 2006 Evaluation Campaign from
four sites. The TC-STAR project partners kindly pro-
vided RWTH their development and evaluation lattices.
Systems and lattice sets are described in (Hoffmeister et
al., 2007).
Table 1 summarizes the best results achieved on the
single lattice sets. The latter columns show the results of
Viterbi min-fWER CN
dev eval dev eval dev eval
1 10.5 9.0 10.3 8.6 10.4 8.6
2 11.4 9.0 11.4 9.5 11.6 9.1
3 12.8 10.4 12.5 10.4 12.6 10.2
4 13.9 11.9 13.9 11.8 13.9 11.8
Table 1: WER[%] results for single systems.
CN and min-fWER based posterior decoding (Mangu et
al., 2000; Wessel et al, 2001).
4.2 Feature analysis on development data
We evaluate the various feature classes from Section 3.3
on the development set with a cross validation testing
strategy. The results in Tables 2 and 3 are generated
with ten-fold cross validation, which maintains a clean
separation of training and testing data. The total number
of training samples (alignment slots where there is system
disagreement) is about 3,700 for the 2 system case, 5,500
for the 3 system case, and 6,800 for the 4 system case.
The WER results for different feature conditions on the
development set are presented in Table 2. The typical
ROVER with word confidences is provided in the first
row for comparison, and the remainder of the rows con-
tain the results for various configurations of features that
are made available to the classifier.
The basic features are just those that encode the same
information as ROVER, but the classifier is still able to
learn better decisions than ROVER with only these fea-
tures. Each of the following rows provides the results for
adding a single feature class to the basic features, so that
the impact of each type can be evaluated.
The last two rows contain combinations of feature
classes. First, the best three classes are added, and then
all features. Using just the best three classes achieves
almost the best results, but a small improvement is gained
when all features are added. The number of iterations in
training is also optimized on the development set by se-
lecting the number with the lowest average classification
error across the ten splits of the training data.
Features 2 System 3 System 4 System
ROVER 10.2% 8.8% 9.0%
basic 9.4% 8.6% 8.5%
+confidences 9.3% 8.7% 8.4%
+durational 9.2% 8.6% 8.4%
+top error 9.0% 8.5% 8.4%
+comparisons 8.9% 8.6% 8.4%
+min-fWER 8.5% 8.5% 8.4%
+top+cmp+fWER 8.3% 8.3% 8.2%
all features 8.3% 8.2% 8.2%
Table 2: WER results for development data with different
feature classes.
67
 8
 8.5
 9
 9.5
 10
 10.5
 11
4321
[%
] W
ER
ROVER(maj.)
ROVER(conf.)
min-fWER
iROVER
2 System 3 System 4 System
ROVER (maj.) 10.8% 9.1% 9.1%
ROVER (conf.) 10.1% 8.8% 9.0%
min-fWER 9.6% 9.2 % 8.9 %
??ROVER 8.3% 8.2% 8.2%
oracle 6.5% 5.4% 4.7%
Table 3: WER[%] results for development data with
manual segmentation, and using cross-validation for
??ROVER.
4.3 Results on evaluation data
After analyzing the features and selecting the optimal
number of training iterations on the development data,
we train a final model on the full development set and
then apply it to the evaluation set. In all cases our clas-
sifier achieves a lower WER than ROVER (statistically
significant by NIST matched pairs test). Table 3 and Ta-
ble 4 present a comparison of the ROVER with majority
voting, confidence voting, frame-based combination, and
our improved ROVER (??ROVER).
5 Conclusions
In summary, we develop ??ROVER, a method for sys-
tem combination that outperforms ROVER consistently
across varying numbers of component systems. The rela-
tive improvement compared to ROVER is especially large
for the case of combining two systems (14.5% on the
evaluation set). The relative improvements are larger than
any we know of to date, and the four system case achieves
the best published result on the TC-STAR English evalu-
ation set. The classifier requires relatively little training
data and utilizes features easily available from system
lattices.
Future work will investigate additional classifiers, clas-
sifier combination, and expanded training data. We are
also interested in applying a language model to decode
an alignment network that has been scored with our clas-
sifier.
References
G. Evermann and P. Woodland. 2000. Posterior probability
decoding, confidence estimation and system combination. In
NIST Speech Transcription Workshop.
 6.5
 7
 7.5
 8
 8.5
 9
 9.5
4321
[%
] W
ER
ROVER(maj.)
ROVER(conf.)
min-fWER
iROVER
2 System 3 System 4 System
ROVER(maj.) 9.0% 7.2% 7.3%
ROVER(conf.) 8.2% 7.1% 7.0%
min-fWER 7.6 % 7.4 % 7.2 %
??ROVER 7.1% 6.9% 6.7%
oracle 5.2% 4.1% 3.6%
Table 4: WER[%] results for evaluation data.
J.G. Fiscus. 1997. A post-processing system to yield reduced
word error rates: Recognizer Output Voting Error Reduction
(ROVER). In Proc. ASRU.
B. Hoffmeister, T. Klein, R. Schlu?ter, and H. Ney. 2006. Frame
based system combination and a comparison with weighted
ROVER and CNC. In Proc. ICSLP.
B. Hoffmeister, D. Hillard, S. Hahn, R. Schu?lter, M. Ostendorf,
and H. Ney. 2007. Cross-site and intra-site ASR system
combination: Comparisons on lattice and 1-best methods. In
Proc. ICASSP.
L. Mangu, E. Brill, and A. Stolcke. 2000. Finding consensus
in speech recognition: word error minimization and other
applications of confusion networks. Computer Speech and
Language, 14:373?400.
R. E. Schapire and Y. Singer. 2000. Boostexter: A boosting-
based system for text categorization. Machine Learning,
39(2/3):135?168.
A. Stolcke, H. Bratt, J. Butzberger, H. Franco, V. Gadde,
M. Plauche, C. Richey, E. Shriberg, K. Sonmez, J. Zheng,
and F. Weng. 2000. The SRI March 2000 Hub-5 conver-
sational speech transcription system. In NIST Speech Tran-
scription Workshop.
F. Wessel, K. Macherey, and R. Schlu?ter. 1998. Using word
probabilities as confidence measures. In Proc. ICASSP.
F. Wessel, R. Schlu?ter, and H. Ney. 2001. Explicit word error
minimization using word hypothesis posterior probabilities.
In Proc. ICASSP, volume 1.
Jian Xue and Yunxin Zhao. 2005. Improved confusion network
algorithm and shortest path search from word lattice. In
Proc. ICASSP.
R. Zhang and A. Rudnicky. 2006. Investigations of issues
for using multiple acoustic models to improve continuous
speech recognition. In Proc. ICSLP.
68
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 701?704,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
A Hybrid Morphologically Decomposed Factored Language Models for
Arabic LVCSR
Amr El-Desoky, Ralf Schlu?ter, Hermann Ney
Lehrstuhl fu?r Informatik 6 ? Computer Science Department
RWTH Aachen University, D-52056 Aachen, Germany
{desoky,schluter,ney}@cs.rwth-aachen.de
Abstract
In this work, we try a hybrid methodology for
language modeling where both morphological
decomposition and factored language model-
ing (FLM) are exploited to deal with the com-
plex morphology of Arabic language. At the
end, we are able to obtain from 3.5% to 7.0%
relative reduction in word error rate (WER)
with respect to a traditional full-words sys-
tem, and from 1.0% to 2.0% relative WER re-
duction with respect to a non-factored decom-
posed system.
1 Introduction
Arabic language is characterized by a complex mor-
phological structure where different kinds of pre-
fixes and suffixes are appended to the word stems
producing a very large number of inflectional forms.
This leads to poor language model (LM) probabil-
ity estimates, and thus high LM perplexities (PPLs)
causing problems in large vocabulary continuous
speech recognition (LVCSR). One successful ap-
proach to deal with this problem is to consider LMs
including morphologically decomposed words. An-
other approach is to use the factored language mod-
els (FLMs) which are powerful models that com-
bine multiple sources of information and efficiently
integrate them via a complex backoff mechanism
(Bilmes and Kirchhoff, 2003).
Morphological decomposition is successfully
used for Arabic LMs in several previous works.
Some are based on linguistic knowledge, and oth-
ers are based on unsupervised methods. Some of the
linguistic methods are based on the Buckwalter Ara-
bic Morphological Analyzer (BAMA) like in (Lamel
et al, 2008). Alternatively, in our previous work
(El-Desoky et al, 2009), we use the Morphological
Analyzer and Disambiguator for Arabic (MADA)
(Habash and Rambow, 2007). On the other side,
most of the unsupervised methods are based on the
minimum description length principle (MDL) like in
(Creutz et al, 2007).
Another type of models is the FLM, in which
words are viewed as vectors of K factors, so that
wt := {f1:Kt }. A factor could be any feature of the
word such as morphological class, stem, root or even
a semantic feature. An FLM is a model over factors,
i.e., p(f1:Kt |f
1:K
t?1 , f
1:K
t?2 , ..., f
1:K
t?n+1), which could be
reformed as a product of probabilities of the form
p(f |f1, f2, ..., fN ). The main idea of the model is to
backoff to other factors when some word n-gram is
not observed in the training data, thus improving the
probability estimates.
In this work we try to combine the strengths
of morphological decomposition and factored lan-
guage modeling. Therefore, language models with
factored morphemes are used. For this purpose, the
LM training data are processed such that full-words
are decomposed into prefix-stem-suffix format with
different added features. We compare our approach
with the standard full-word, decomposed word, and
factored full-word n-gram approaches.
2 Factorization and Decomposition
We use MADA 2.0 in order to perform morphologi-
cal analysis and attach a complete set of morpholog-
ical tags to Arabic words in context. From those tags
701
we derive three different features. Moreover, we de-
rive a fourth feature based on the root of the word
generated by ?Sebawai? (Darwish, 2002). The list
of features is:
? ?W? (Word): word surface form.
? ?L? (Lexeme): word lexeme.
? ?M? (Morph): morphological description.
? ?P? (Pattern): word after subtracting root.
The LM training corpora are processed so that
words are replaced by the factored representation as
required by SRILM-FLM extensions (Kirchhoff et
al., 2008). Then, word decomposition is performed
based on MADA as described in our previous publi-
cation (El-Desoky et al, 2009).
3 FLM topologies
In order to obtain a good performance via FLMs, we
need to optimize the FLM parameters: the combi-
nation of the conditioning factors, backoff path, and
smoothing options. For this purpose, we use a Ge-
netic Algorithm based FLM optimization tool (GA-
FLM) developed by Kirchhoff (2006) which seeks to
minimize the PPL over some held-out text. Further-
more, we apply some manual optimization to fine
tune the FLM parameters. For memory limitations,
we only use factors up to 2 previous time slots (tri-
gram like models). Finally, we come up with a set
of competing FLMs with rather close PPLs. In Ta-
ble 1, we record the PPLs measured for some held-
out text. The first column gives the combination of
the parent factors. So that, FLM1 corresponds to
the model P (Wt|Wt?1,Wt?2), which is the FLM
equivalent of the standard tri-gram LM (our base-
line model), while FLM2 & FLM3 correspond to
the model P (Wt|Wt?1,Mt?1, Lt?1, Pt?1,Wt?2),
however FLM4 & FLM5 correspond to the model
P (Wt|Wt?1,Mt?1, Lt?1,Wt?2,Mt?2, Lt?2). The
?gtmin? refers to the count threshold that is suffi-
cient to have a language model hit at some node of
the the backoff graph (for exact topologies, contact
the first author). From Table 1, comparing PPLs
(non-normalized) across factored and non-factored
LMs, we see that using more factors other than the
normal word could help decreasing the PPL. This is
true for all the used types of vocabulary units.
vocabulary
FLMx parent factors FW PD FD
1: W1 W2 (baseline) 302.6 284.1 82.7
W1 M1 L1 P1 W2
2: gtmin = 1 306.2 296.9 83.2
3: gtmin = 2-4 290.9 279.1 79.8
W1 M1 L1 W2 M2 L2
4: gtmin = 1 300.2 291.1 83.6
5: gtmin = 2-4 294.5 283.7 81.1
Table 1: perplexities of the FLMs using vocabularies:
(FW: 70k full-words; PD: partially decomposed with 20k
ful-words + 50k morphemes; FD: 70k fully decomposed).
FLMx parent factors WER [%]
1: W1 W2 (baseline) 20.4
W1 M1 L1 P1 W2
2: gtmin = 1 20.2
3: gtmin = 2-4 20.4
W1 M1 L1 W2 M2 L2
4: gtmin = 1 19.9
5: gtmin = 2-4 20.3
Table 2: WERs using FLMs based on 70k full-words.
In order to select the best FLM topology, we run
a simple one pass recognition for a small internal
dev corpus derived from GALE data sets, consists
of 40 minutes of audio data recorded during January
to March 2007. The acoustic models are within-
word tri-phone models trained using 1100h of au-
dio material. The basic acoustic models are trained
based on Maximum Likelihood (ML) method. Then,
a discriminative training based on Minimum Phone
Error (MPE) criterion is performed to enhance the
models. A 70k full-words lexicon is used. The
FLM training data consists of 206 Million running
full-words. A standard bi-gram LM based on full-
words is used to generate N-best lists, then N-best
list rescoring is performed using the different FLM
topologies shown in Table 1. We start by N = 1000-
best down to 3-best sentences. Using N = 10 always
gives the best results. The recognition WERs are
recorded in Table 2. The least WER is obtained with
FLM4. We note that the best FLM does not corre-
spond to the least PPL. This is because a higher ?gt-
min? value causes more backoff in cases of insuffi-
cient data leading to better estimates. Therefore, we
select FLM4 for the coming experiments.
702
4 Experimental Setup
Our recognition system is close to the one described
in section 3. However, we use within and across-
word models at different recognition passes. In ad-
dition, we use 70k or 256k lexicon of full-words or
partially decomposed words. Alternatively, we eval-
uate the results on the GALE 2007 development and
evaluation sets (dev07: 2.5h; eval07: 4h). Our rec-
ognizer works in 3 passes. In the first pass, within-
word acoustic models are used with no adaptation,
along with a standard bi-gram LM to generate lat-
tices, followed by a standard tri-gram or 4-gram LM
rescoring of lattices. The second pass does the same,
but it uses across-word models with Constrained
Maximum Likelihood Linear Regression (CMLLR)
adaptation. Then, a third pass with additional Max-
imum Likelihood Linear Regression (MLLR) adap-
tation is performed, using a standard bi-gram LM to
generate lattices or N-best lists. Then, one of the fol-
lowing is performed: 1) lattice rescoring using stan-
dard tri-gram or 4-gram LM, 2) N-best list rescoring
using FLMs based on full-words, partially or fully
decomposed words.
5 Experiments
In this section, we record our recognition results
for: 1) systems based on full-words, and 2) systems
based on decomposed words. Also, we introduce
additional results for larger lexicon sizes.
5.1 Systems Based on Full-words
In this section, we present the WERs of our recogni-
tion systems based on full-words. Where, during the
search, we use a lexicon of 70K full-words. In the
first 2 passes, we use a standard bi-gram LM to gen-
erate lattices, followed by a standard tri-gram LM
rescoring of lattices. However, in the third pass, we
generate both lattices and N-best lists based on the
same bi-gram LM. The final lattices and N-best lists
are rescored using different LMs as shown in Table
3. In case we perform N-best list rescoring with a
FLM, the N-best lists are processed to produce fac-
tored representation, followed by partial or full de-
composition as previously described in section 2.
It is clear from Table 3 that the least WER is
achieved when using N-best list rescoring using a
full-words based FLM. This gives an absolute im-
LM rescoring (3rd pass) Dev07 [%]
tri-gram lattice resc. (baseline) 16.5
4-gram lattice resc. 16.3
N-best FLM resc.:
+ FW (original N-best) 15.7
+ PD (decomposed N-best) 15.8
+ FD (decomposed N-best) 16.0
Table 3: WERs for 70k full-words systems.
LM rescoring (3rd pass) Dev07 [%]
tri-gram lattice resc. (baseline) 14.7
4-gram lattice resc. 14.5
N-best FLM resc.:
+ FW (re-joint N-best) 14.6
+ PD (original N-best) 14.3
+ FD (decomposed N-best) 14.4
Table 4: WERs for 70k partially decomposed systems
(20k full-words + 50k morphemes).
provement of 0.8% (about 4.8% relative) compared
to the standard tri-gram lattice rescoring. On the
other hand, we have 0.6% absolute improvement
(about 3.7% relative) compared to the standard 4-
gram lattice rescoring. Decomposition does not help
in this case. This is because the original N-best lists
are generated in full-words format, whose decom-
position might not lead to better LM scores. For this
reason, we expect that it is better to start with a de-
composed LM for lattice and N-best generation.
5.2 Systems Based on Decomposed Words
This section introduces the WERs of our systems
based on decomposed words. We use a similar setup
as in section 5.1. However, we use a lexicon and a
bi-gram LM based on a 70k partially decomposed
words (20k full-words + 50k morphemes). Table 4
presents the results. As expected, we get the best
WER when using N-best list rescoring with a FLM
based on partially decomposed words. An absolute
improvement of 0.4% (2.7% relative) is achieved
compared to the new baseline. Compared to the old
baseline of Table 3, we get an absolute improvement
of 2.2% (13.3% relative).
5.3 Larger Lexicon Sizes
Now, we increase the size of our lexicon to 256k
partially decomposed words (20k full-words + 236k
703
Dev07 Eval07
System [%] [%]
traditional full-words 14.9 16.5
partially decomposed
+ 4-gram lat. resc. (baseline) 14.2 16.1
+ N-best FLM resc.:
+ FW (re-joint N-best) 14.1 -
+ PD (original N-best) 13.9 15.9
+ FD (decomposed N-best) 14.0 -
Table 5: WERs for 256k full-words, and partially decom-
posed systems (20k full-words + 236k morphemes).
70k vocabularies 256k vocabularies
Corpus FW PD FD FW PD FD
Dev07 3.65 1.33 0.75 1.36 0.51 0.24
Eval07 4.82 1.94 1.13 1.85 0.64 0.41
Table 6: OOVs [%] of the used vocabularies.
morphemes). In addition, we use a standard 4-
gram LM for rescoring the bi-gram lattices in the
first 2 passes. To complete our comparisons, we
record the WERs using traditional 256k full-words
lexicon, standard bi-gram search, and standard 4-
gram LM for lattice rescoring, with no decomposi-
tion or factorization. In Table 5, we see that the im-
provement persists for the larger lexicon. Compared
to the new baseline, the 256k decomposed system
achieves WER reductions of [dev07: 0.3% absolute
(2.1% relative); eval07: 0.2% absolute (1.2% rela-
tive)] when using N-best list rescoring with a FLM
based on partially decomposed words. Moreover, it
improves over the traditional full-words by [dev07:
1.0% absolute (6.7% relative); eval07: 0.6% abso-
lute (3.6% relative)]. The out-of-vocabulary rates
(OOVs) are given in Table 6. It is worth noting that
using fully decomposed lexicons as well as higher
order LMs could not improve WERs, this we previ-
ously proved in (El-Desoky et al, 2009).
6 Conclusions
We have introduced a hybrid approach to Ara-
bic language modeling. Our approach combines
the strengths of both morphological decomposition
and factored language modeling. Thus, we have
used language models with factored morphemes.
We have compared our approach to traditional ap-
proaches like: standard full-word n-grams, standard
decomposed n-grams, and full-word based factored
language models. Finally, we could achieve some
improvements over all the traditional approaches.
Nevertheless, we have only considered the use of
factored language models in the rescoring phase.
Acknowledgments
This material is based upon work supported by the
DARPA under Contract No. HR0011-06-C-0023.
Any opinions, findings and conclusions expressed in
this material are those of the authors and do not nec-
essarily reflect the views of DARPA.
References
J. Bilmes and K. Kirchhoff. 2003. Factored language
models and generalized parallel backoff. In Proc. Hu-
man Language Technology Conf. of the North Ameri-
can Chapter of the ACL, volume 2, pages 4 ? 6, Ed-
monton, Canada, May.
M. Creutz, T. Hirsimki, M. Kurimo, A. Puurula,
J. Pylkknen, V. Siivola, M. Varjokallio, E. Arisoy,
M. Saraclar, and A. Stolcke. 2007. Morph-based
speech recognition and modeling of out-of-vocabulary
words across languages. ACM Transactions on Speech
and Language Processing, 5(1), December.
K. Darwish. 2002. Building a shallow Arabic morpho-
logical analyzer in one day. In ACL workshop on Com-
putational approaches to semitic languages, Philadel-
phia, PA, USA, July.
A. El-Desoky, C. Gollan, D. Rybach, R. Schlu?ter, and
H. Ney. 2009. Investigating the use of morphological
decomposition and diacritization for improving Arabic
LVCSR. In Interspeech, pages 2679 ? 2682, Brighton,
UK, September.
N. Habash and O. Rambow. 2007. Arabic diacritiza-
tion through full morphological tagging. In Proc. Hu-
man Language Technology Conf. of the North Ameri-
can Chapter of the ACL, volume Companion, pages 53
? 56, Rochester, NY, USA, April.
K. Kirchhoff, D. Vergyri, J. Bilmes, K. Duh, and A. Stol-
cke. 2006. Morphology-based language modeling for
conversational Arabic speech recognition. Computer
Speech and Language, 20(4):589 ? 608, October.
K. Kirchhoff, J. Bilmes, and K. Duh. 2008. Factored
language model tutorial. Technical report, Department
of Electrical Engineering, University of Washington,
Seattle, Washington, USA, February.
L. Lamel, A. Messaoudi, and J.L Gauvain. 2008. Investi-
gating morphological decomposition for transcription
of Arabic broadcast news and broadcast conversation
data. In Interspeech, volume 1, pages 1429 ? 1432,
Brisbane, Australia, September.
704
