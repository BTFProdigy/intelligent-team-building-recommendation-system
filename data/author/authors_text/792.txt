Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 953?960
Manchester, August 2008
Using Syntactic Information for Improving Why-Question Answering
Suzan Verberne, Lou Boves, Nelleke Oostdijk and Peter-Arno Coppen
Department of Linguistics
Radboud University Nijmegen
s.verberne@let.ru.nl
Abstract
In this paper, we extend an existing para-
graph retrieval approach to why-question
answering. The starting-point is a system
that retrieves a relevant answer for 73%
of the test questions. However, in 41%
of these cases, the highest ranked relevant
answer is not ranked in the top-10. We
aim to improve the ranking by adding a re-
ranking module. For re-ranking we con-
sider 31 features pertaining to the syntactic
structure of the question and the candidate
answer. We find a significant improvement
over the baseline for both success@10 and
MRR@150. The most important features
for re-ranking are the baseline score, the
presence of cue words, the question?s main
verb, and the relation between question fo-
cus and document title.
1 Introduction
Recently, some research has been directed at prob-
lems involved in why-question answering (why-
QA). About 5% of all questions asked to QA
systems are why-questions (Hovy et al, 2002).
They need a different approach from factoid ques-
tions, since their answers cannot be stated in a sin-
gle phrase. Instead, a passage retrieval approach
seems more suitable. In (Verberne et al, 2008),
we proposed an approach to why-QA that is based
on paragraph retrieval. We reported mediocre per-
formance and suggested that adding linguistic in-
formation may improve ranking power.
c
?Suzan Verberne, 2008. Licensed under the Creative
Commons Attribution-Noncommercial-Share Alike 3.0 Un-
ported license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved.
In the present paper, we implement a simi-
lar paragraph retrieval approach and extend it by
adding a re-ranking module based on structural lin-
guistic information. Our aim is to find out whether
syntactic knowledge is relevant for discovering re-
lations between question and answer, and if so,
which type of information is the most beneficial.
In the following sections, we first discuss related
work (section 2). In sections 3 and 4, we introduce
the data that we used for development purposes
and the baseline retrieval and ranking method that
we implemented. In section 5, we present our re-
ranking method and the results obtained, followed
by a discussion in section 6, and directions for fur-
ther research in section 7.
2 Related work
A substantial amount of work has been done in
improving QA by adding syntactic information
(Tiedemann, 2005; Quarteroni et al, 2007; Hi-
gashinaka and Isozaki, 2008). All these studies
show that syntactic information gives a small but
significant improvement on top of the traditional
bag-of-words (BOW) approaches.
The work of (Higashinaka and Isozaki, 2008)
focuses on the problem of ranking candidate an-
swer paragraphs for Japanese why-questions. They
find a success@10 score of 70.3% with an MRR
of 0.328. They conclude that their system for
Japanese is the best-performing fully implemented
why-QA system. In (Tiedemann, 2005), passage
retrieval for Dutch factoid QA is enriched with
syntactic information from dependency structures.
The baseline approach, using only the BOW, re-
sulted in an MRR of 0.342. With the addition of
syntactic structure, MRR improved to 0.406.
The work by (Quarteroni et al, 2007) consid-
ers the problem of answering definition questions.
953
They use predicate-argument structures (PAS) for
improved answer ranking. Their results show that
PAS make a very small contribution compared to
BOW only (F-scores 70.7% vs. 69.3%).
The contribution of this paper is twofold: (1) we
consider the relatively new problem of why-QA for
English and (2) we not only improve a simple pas-
sage retrieval approach by adding syntactic infor-
mation but we also perform extensive feature se-
lection in order to find out which syntactic features
contribute to answer ranking and to what extent.
3 Data
As data for developing and testing our system
for why-QA, we use the Webclopedia question set
by (Hovy et al, 2002). This set contains ques-
tions that were asked to the online QA system
answers.com. 805 of these questions are why-
questions. As answer corpus, we use the off-line
Wikipedia XML corpus, which consists of 659,388
articles (Denoyer and Gallinari, 2006). We manu-
ally inspect a sample of 400 of the Webclopedia
why-questions. Of these, 93 have an answer in the
Wikipedia corpus. Manual extraction of one rele-
vant answer for each of these questions results in a
set of 93 why-questions and their reference answer.
We also save the title of the Wikipedia article in
which each of the answers is embedded, in order
to be able to evaluate document retrieval together
with answer retrieval.
4 Paragraph retrieval for why-QA
4.1 Baseline method
We index the Wikipedia XML corpus using the
Wumpus Search Engine (Buttcher, 2007). In
Wumpus, queries can be formulated in the GCL
format, which is especially geared to retrieving
XML items. Since we consider paragraphs as re-
trieval units, we let the engine retrieve text frag-
ments marked with ?p? as candidate answers.
We implement a baseline method for question
analysis in which first stop words are removed1.
Also, any punctuation is removed from the ques-
tion. What remains is a set of question content
words. Next, we automatically create a query for
each question that retrieves paragraphs containing
(a subset of) these question terms. For ranking
1To this end the stop word list is used that can be found
at http://marlodge.supanet.com/museum/ funcword.html. We
use all categories except the numbers and the word why
the paragraphs retrieved, we use the QAP algo-
rithm created by MultiText, which has been im-
plemented in Wumpus. QAP is a passage scor-
ing algorithm specifically developed for QA tasks
(Buttcher et al, 2004). For each question, we re-
trieve and rank the top 150 of highest scoring an-
swer candidates.
4.2 Evaluation method
For evaluation of the results, we perform manual
assessment of all answers retrieved, starting at the
highest-ranked answer and ending as soon as we
encounter a relevant answer2. Then we count the
proportion of questions that have at least one rele-
vant answer in the top n of the results for n = 10
and n = 150, giving us success@10 and suc-
cess@150. For the highest ranked relevant answer
per question, we determine the reciprocal rank
(RR). If there is no relevant answer retrieved by
the system at n = 150, the RR is 0. Over all ques-
tions, we calculate the Mean RR (MRR@150).
We also measure the performance of our system
for document retrieval: the proportion of questions
for which at least one of the answers in the top 10
comes from the reference document (success@10
for document retrieval) and the MRR@150 for the
highest position of the reference document3.
4.3 Results and discussion
Table 1: Baseline results for the why passage retrieval sys-
tem for answer retrieval and document retrieval in terms of
success@10, success@150 and MRR@150
S@10 S@150 MRR@150
Answer retrieval 43.0% 73.1% 0.260
Document retrieval 61.8% 82.2% 0.365
There are two possible directions for improving
our system: (1) by improving retrieval and (2) by
improving ranking. Since success@150 is 73.1%,
for 68 of the 93 questions in our set at least one
relevant answer is retrieved in the top 150. For the
other 25 questions, the reference answer was not
included in the long list of 150 results.
In the present paper we focus on improving an-
swer ranking. The results show that for 30.1% of
2We don?t need to assess the tail since we are only in-
terested in the highest-ranked relevant answer for calculating
MRR
3Note that we consider as relevant all documents in which
a relevant answer is embedded. So the relevant document with
the highest rank is either the reference document or the doc-
ument in which the relevant answer with the highest rank is
embedded.
954
the questions4, a relevant answer is retrieved but
is not placed in the top 10 by the ranking algo-
rithm. For these 28 questions in our set, re-ranking
may be an option. Since re-ranking will not im-
prove the results for the questions for which there
is no relevant answer in the top-150, the maximum
success@10 that we can achieve by re-ranking is
73.1% for answer paragraphs and 82.8% for docu-
ments.
5 Answer re-ranking
Before we can decide on our re-ranking approach,
we take a closer look at the ranking method that is
applied in the baseline system. The QAP algorithm
includes the following variables: (1) term overlap
between query and passage, (2) passage length and
(3) total corpus frequency for each term (Buttcher
et al, 2004). Let us consider three example ques-
tions from our collection to see the strengths and
weaknesses of these variables.
1. Why do people sneeze?
2. Why do women live longer than men on average?
3. Why are mountain tops cold?
In (1), the corpus frequencies of the question
terms people and sneeze ensure that the relatively
unique term sneeze is weighted heavier for ranking
than the very common noun people. This matches
the goal of the query, which is finding an explana-
tion for sneezing. However, in (2), the frequency
variables used by QAP do not reflect the impor-
tance of the terms. Thus, women, live, longer and
average are considered to be of equal importance,
while obviously the latter term is only peripheral to
the goal of the query. This cannot be derived from
its corpus frequency, but may be inferred from its
syntactic function in the question: an adverbial on
sentence level. In (3), mountain and tops are in-
terpreted as two distinct terms by the baseline sys-
tem, whereas the interpretation of mountain tops
as compound item is more appropriate.
Examples 2 and 3 above show that a question-
answer pair may contain more information than
is represented by the frequency variables imple-
mented in the QAP algorithm. Our aim is to find
out which features from a question-answer pair
constitute the information that discloses a relation
between the question and its answer. Moreover, we
aim at weighting these features in such a way that
we can optimize ranking performance.
4
73.1%? 43.0%
5.1 Features for re-ranking
As explained above, baseline ranking is based on
term overlap. The features that we propose for
re-ranking are also based on term overlap, but in-
stead of considering all question content words in-
discriminately in one overlap function, we select a
subset of question terms for each of the re-ranking
features. By defining different subsets based on
syntactic functions and categories, we can investi-
gate which syntactic features of the question, and
which parts of the answer are most important for
re-ranking.
The following subsections list the syntactic fea-
tures that we consider. Each feature consists of two
item sets: a set of question items and a set of an-
swer items. The value that is assigned to a feature
is a function of the intersection between these two
sets. For a set of question items Q and a set of
answer items A, the proportion P of their intersec-
tion is:
P =
|Q ? A|+ |A ? Q|
|Q|+ |A|
(1)
Our approach to composing the set of features is
described in subsections 5.1.1 to 5.1.4 below. We
label the features using the letter f followed by a
number so that we can back-reference to them.
5.1.1 The syntactic structure of the question
Example 2 in the previous section shows that
some syntactic functions in the question may be
more important than other functions. Since we do
not know as yet which syntactic functions are the
most important, we include both heads (f1) and
modifiers (f2) as item sets. We also include the
four main syntactic constituents for why-questions:
subject (f4), main verb (f6), nominal predicate (f8)
and direct object (f10) to be matched against the
answer terms. For these features, we add a vari-
ant where as answer items only words/phrases with
the same syntactic function are included (f5, f7, f9,
f11).
Example 3 in the previous section exemplifies
the potential relevance of noun phrases (f3).
5.1.2 The semantic structure of the question
The features f12 to f15 come from earlier data
analyses that we performed. We saw that often
there is a link between a specific part of the ques-
tion and the title of the document in which the ref-
erence answer is found. For example, the answer
to the question ?Why did B.B. King name his gui-
tar Lucille?? is in the Wikipedia article with the ti-
955
tle B.B. King. The answer document and the ques-
tion apparently share the same topic (B.B. King).
In analogy to linguistically motivated approaches
to factoid QA (Ferret et al, 2002) we introduce the
term question focus for this topic.
The focus is often the syntactic subject of the
question. From our data, we found the follow-
ing two exceptions to this general rule: (1) If the
subject is semantically poor, the question focus is
the (verbal or nominal) predicate: ?Why do peo-
ple sneeze??, and (2) in case of etymology ques-
tions (which cover about 10% of why-questions),
the focus is the subject complement of the pas-
sive sentence: ?Why are chicken wings called
Buffalo Wings?? In all other cases, the question
focus is the grammatical subject: ?Why do cats
sleep so much??
We include a feature (f13) for matching words
from the question focus to words from the docu-
ment title. We also add a feature (f12) for the re-
lation between all question words and words from
the document title, and a feature (f14) for the rela-
tion between question focus words and all answer
words.
5.1.3 Synonyms
For each of the features f1 to f15, we add an
alternative feature (f16 to f30) covering the set of
all WordNet synonyms for all items in the origi-
nal feature. Note that the original words are no
longer included for these features; we only include
the terms from their synonym sets. For synonyms,
we apply a variant of equation 1 in which |Q ? A|
is interpreted as the number of question items that
have at least one synonym in the set of answer
items and |A ? Q| as the number of answer items
that occur in at least one of the synonym sets of the
question items.
5.1.4 Cue words
Finally, we add a closed set of cue words that
often occur in answers to why-questions5 (f31).
5.2 Extracting feature values from the data
For the majority of features we need the syntactic
structure of the input question, and for some of the
features also of the answer. We experimented with
two different parsers for these tasks: a develop-
5These cue words come from earlier work that we did on
the analysis of why-answers: because, since, therefore, why,
in order to, reason, reasons, due to, cause, caused, causing,
called, named
ment version of the Pelican parser6 and the EP4IR
dependency parser (Koster, 2003).
Given a question-answer pair and the parse trees
of both question and answer, we extract values
from each parser?s output for all features in sec-
tion 5.1 by means of a Perl script.
Our script has access to the following external
components: A stop word list (see section 4.1), a
fixed set of cue words, the CELEX Lemma lexi-
con (Burnage et al, 1990), all WordNet synonym
sets, and a list of pronouns and semantically poor
nouns7.
Given one question-answer pair, the feature
extraction script performs the following actions.
Based on the question?s parse tree, it extracts the
subject, main verb, direct object (if present) and
nominal predicate (if present) from the question.
The script decides on question focus using the
rules suggested in section 5.1.2. For the answer, it
extracts the document title. From the parse trees
created for the answer paragraph, it extracts all
subjects, all verbs, all direct objects, and all nomi-
nal predicates.
For each feature, the script composes the re-
quired sets of question items and answer items. All
items are lowercased and punctuation is removed.
In multi-word items, spaces are replaced by un-
derscores before stop words are removed from the
question and the answer. Then the script calculates
the proportion of the intersection of the two sets for
each feature following equation 18.
Whether or not to lemmatize the items before
matching them is open to debate. In the litera-
ture, there is some discussion on the benefit of
lemmatization for information extraction (Bilotti
et al, 2004). Lemmatization can be problematic
in the case of proper names (which are not always
recognizable by capitalization) and noun phrases
that are fixed expressions such as sailors of old.
Noun phrases are involved not only in the NP fea-
ture (f3), but also in our features involving sub-
ject, direct object, nominal predicate and question
focus. Therefore, we decided only to lemmatize
verbs (for features f6 and f7) in the current version
of our system.
For each question-answer pair in our data set,
we extract all feature values using our script. We
6The Pelican parser is a constituency parser that is cur-
rently being developed at Nijmegen University. See also
http://lands.let.ru.nl/projects/pelican/
7These are the nouns humans and people
8A multi-word term is counted as one item
956
use three different settings for feature extraction:
(1) feature extraction from gold standard con-
stituency parse trees of the questions in accordance
with the descriptive model of the Pelican parser9;
(2) feature extraction from the constituency parse
trees of the questions generated by Pelican10; and
(3) feature extraction from automatically gener-
ated dependency parse trees from EP4IR.
Our training and testing method using the ex-
tracted feature values is explained in the next sec-
tion.
5.3 Re-ranking method
As the starting point for re-ranking we run the
baseline system on the complete set of 93 ques-
tions and retrieve 150 candidate answers per ques-
tion, ranked by the QAP algorithm. As described
in section 5.2, we use two different parsers. Of
these, Pelican has a more detailed descriptive
model and gives better accuracy (see section 6.3 on
parser evaluation) but EP4IR is at present more ro-
bust for parsing long sentences and large amounts
of text. Therefore, we parse all answers (93 times
150 paragraphs) with EP4IR only. The questions
are parsed by both Pelican and EP4IR.
As presented in section 5.1, we have 31 re-
ranking features. To these, we add the score that
was assigned by QAP, which makes 32 features
in total. We aim to weight the feature values in
such a way that their contribution to the overall
system performance is optimal. We set each fea-
ture weight as an integer between 0 and 10, which
makes the number of possible weighting configu-
rations 1132. In order to choose the optimal con-
figuration from this huge set of possible configura-
tions, we use a genetic algorithm11 (Goldberg and
Holland, 1988). The variable that we optimize dur-
ing training is MRR. We tune the feature weights
over 100 generations of 1000 individuals. For eval-
uation, we apply cross valuation on five question
9Pelican aims at producing all possible parse trees for a
given sentence. A linguist can then decide on the correct parse
tree given the context. We created the gold standard for each
question by manually selecting the correct parse tree from the
parse trees generated by the parser.
10For this setting, we run the Pelican parser with the option
of only giving one parse (the most likely according to Pelican)
per question. As opposed to the gold standard setting, we do
not perform manual selection of the correct parse.
11We chose to work with a genetic algorithm because we
are mainly interested in feature selection and ranking. We
are currently experimenting with Support Vector Machines
(SVM) to see whether the results obtained from using the ge-
netic algorithm are good enough for reliable feature selection.
folds: in five turns, we train the feature weights on
four of the five folds and evaluate them on the fifth.
We use the feature values that come from the
gold standard parse trees for training the feature
weights, because the benefit of a syntactic item
type can only be proved if the extraction of that
item from the data is correct. At the testing stage,
we re-rank the 93 questions using all three fea-
ture extraction settings: feature values extracted
from gold standard parse trees, feature values ex-
tracted with Pelican and feature values extracted
with EP4IR. We again regard the distribution of
questions over the five folds: we re-rank the ques-
tions in fold five according to the weights found by
training on folds one to four.
5.4 Results from re-ranking
Table 2 on the next page shows the results for the
three feature extraction settings.
Using the Wilcoxon Signed-Rank Test we find
that all three re-ranking conditions give signifi-
cantly better results than the baseline (Z = ?1.91,
P = 0.0281 for paired reciprocal ranks). The dif-
ferences between the three re-ranking conditions
are, however, not significant12.
5.5 Which features made the improvement?
If we plot the weights that were chosen for the fea-
tures in the five folds, we see that for some features
very different weights were chosen in the different
folds. Apparently, for these features, the weight
values do not generalize over the five folds. In or-
der to only use reliable features, we only consider
features that get similar weights over all five folds:
their weight values have a standard deviation < 2
and an average weight > 0. We find that of the
32 features, 21 are reliable according to this def-
inition. Five of these features make a substantial
contribution to the re-ranking score (table 3). Be-
hind each feature is its reference number from sec-
tion 5.1 and its average weight on a scale of 0 to
10.
Moreover, there are three other features that to a
limited extent contribute to the overall score (table
4).
Thirteen other reliable features get a weight < 1.5
assigned during training and thereby slightly con-
tribute to the re-ranking score.
12The slightly lower success and MRR scores for re-
ranking with gold standard parse trees compared to Pelican
parse trees can be explained by the absence of the gold stan-
dard for one question in our set.
957
Table 2: Re-ranking results for three different parser settings in terms of success@10, success@150 and MRR@150.
Answer/paragraph retrieval Document retrieval
Version S@10 S@150 MRR S@10 S@150 MRR
Baseline 43.0% 73.1% 0.260 61.8% 82.8% 0.365
Re-ranking w/ gold standard parse trees 54.4% 73.1% 0.370 63.1% 82.8% 0.516
Re-ranking w/ Pelican parse trees 54.8% 73.1% 0.380 64.5% 82.8% 0.518
Re-ranking w/ EP4IR parse trees 53.8% 73.1% 0.349 63.4% 82.8% 0.493
Table 3: Features that substantially contribute to the re-
ranking score, with their average weight
Question focus synonyms to doctitle (f28) 9.2
Question verb synonyms to answer verbs (f22) 9
Cue words (f31) 9
QAP 8.8
Question focus to doctitle (f13) 7.8
Table 4: Features that to a limited extent contribute to the
re-ranking score, with their average weight
Question subject to answer subjects (f5) 2.2
Question nominal predicate synonyms (f23) 1.8
Question object synonyms to answer objects (f26) 1.8
6 Discussion
Our re-ranking method scores significantly better
than the baseline, with use of a small subset of
the 32 features. It reaches a success@10 score
of 54.8% with an MRR@150 of 0.380 for answer
retrieval. This compares to the MRR of 0.328
that Higashinaka and Isozaki found for why-QA
and the MRR of 0.406 that Tiedemann reaches
for syntactically enhanced factoid-QA (see sec-
tion 2), showing that our method performs reason-
able well. However, the MRR of 0.380 also shows
that a substantial part of the problem of why-QA is
still to be solved.
6.1 Error analysis
For analysis of our results, we counted for how
many questions the ranking was improved, and for
how many the ranking deteriorated. First of all,
ranking remained equal for 35 questions (37.6%).
25 of these are the questions for which no rele-
vant answer was retrieved by the baseline system
at n = 150 (26.9% of questions). For these ques-
tions the ranking obviously remained equal (RR is
0) after re-ranking. For the other 10 questions for
which ranking did not change, RR was 1 and re-
mained 1. Apparently, re-ranking does not affect
excellent rankings.
For two third (69%) of the remaining questions,
ranking improved and for one third (31%), it dete-
riorated. There are eleven questions for which the
reference answer was ranked in the top 10 by the
baseline system but it drops out of the top 10 by
re-ranking. On the other hand, there are 22 ques-
tions for which the reference answer enters the top
10 by re-ranking the answers, leading to an overall
improvement in success@10.
If we take a look at the eleven questions for
which the reference answer drops out of the top
10 by re-ranking, we see that these are all cases
where there is no lexical overlap between the ques-
tion focus and the document title. The importance
of features 13 and 28 in the re-ranking weights
works against the reference answer for these ques-
tions. Here are three examples (question focus as
detected by the feature extraction script is under-
lined):
1. Why do neutral atoms have the same number of protons
as electrons? (answer in ?Oxidation number?)
2. Why do flies walk on food? (answer in ?Insect Habitat?)
3. Why is Wisconsin called the Badger State? (answer in
?Wisconsin?)
In example 1, the reference answer is outranked
by answer paragraphs from documents with one of
the words neutral and atoms in its title. In example
2, there is actually a semantic relation between the
question focus (flies) and the document title (in-
sect); however, this relation is not synonymy but
hyperonymy and therefore not included in our re-
ranking features. One could dispute the definition
of question focus for etymology questions (exam-
ple 3), but there are simply more cases where the
subject complement of the question leads to doc-
ument title than cases where its subject (such as
Winsconsin) does.
6.2 Feature selection analysis
We think that the outcome of the feature selection
(section 5.5) is very interesting. We are not sur-
prised that the original score assigned by QAP is
still important in the re-ranking module: the fre-
quency variables apparently do provide useful in-
formation on the relevance of a candidate answer.
We also see that the presence of cue words
(f31) gives useful information in re-ranking an-
958
swer paragraphs. In fact, incorporating the pres-
ence of cue words is a first step towards recogniz-
ing that a paragraph is potentially an answer to a
why-question. We feel that identifying a paragraph
as a potential answer is the most salient problem
of why-QA, since answers cannot be recognized
by simple semantic-syntactic units such as named
entities as is the case for factoid QA. The current
results show that surface patterns (the literal pres-
ence of items from a fixed set of cue words) are a
first step in the direction of answer selection.
More interesting than the baseline score and cue
words are the high average weights assigned to
the features f13 and f28. These two features refer
to the relation between question focus and docu-
ment title. As explained in section 5.1.2, we al-
ready had the intuition that there is some relation
between the question focus of a why-question and
the document title. The high weights that are as-
signed to the question focus features show that our
procedure for extracting question focus is reliable.
The importance of question focus for why-QA is
especially interesting because it is a question fea-
ture that is specific to why-questions and does not
similarly apply to factoids or other question types.
Moreover, the link from the question focus to the
document title shows that Wikipedia as an answer
source can provide QA systems with more infor-
mation than a collection of plain texts without doc-
ument structure does.
From the other features discussed in section 5.5,
we learn that all four main question constituents
contribute to the re-ranking score, but that syn-
onyms of the main verb make the highest contri-
bution (f22). Subject (f5), object (f26) and nomi-
nal predicate (f23) make a lower contribution. We
suspect that this may be due to our decision to only
lemmatize verbs, and not nouns (see section 5.2).
It could be that since lemmatization leads to more
matches, a feature can make a higher contribution
if its items are lemmatized.
6.3 The quality of the syntactic descriptions
We already concluded in the previous section that
our feature extraction module is very well capable
of extracting the question focus, since f13 and f28
get assigned high weights by training. However,
in the training stage, we used gold standard parse
trees. In this section we evaluate the two automatic
syntactic parsers Pelican and EP4IR, in order to
be able to come up with fruitful suggestions for
improving our system in the future.
As a measure for parser evaluation, we con-
sider constituent extraction: how well do both
parsers perform in identifying and delimiting the
four main constituents from a why-question: sub-
ject, main verb, direct object and nominal pred-
icate? As the gold standard for this experiment
we use manually verified constituents that were
extracted from the gold standard parse trees. We
adapt our feature extraction script so that it prints
each of the four constituents per question. Then we
calculate the recall score for each parser for each
constituent type.
Recall is the number of correctly identified con-
stituents of a specific type divided by the total
number of constituents of this type in the goldstan-
dard parse tree. This total number is not exactly 93
for all constituent types: only 34 questions have a
direct object in their main clause and 31 questions
have a nominal predicate. The results of this exer-
cise are in Table 5.
Table 5: Recall for constituent extraction (in %)
subjs verbs objs preds all
Pelican 79.6 94.6 64.7 71.0 82.1
EP4IR 63.4 64.5 44.1 48.4 59.4
We find that over all constituent types, Peli-
can reaches significantly better recall scores than
EP4IR (Z = 5.57; P < 0.0001 using the
Wilcoxon Signed-Rank Test).
Although Pelican gives much better results on
constituent extraction than EP4IR, the results on
the re-ranking task do not differ significantly. The
most plausible explanation for this is that the high
accuracy of the Pelican parses is undone by the
poor syntactic analysis on the answer side, which
is in all settings performed by EP4IR.
7 Future directions
In section 4.3, we mentioned two directions for im-
proving our pipeline system: improving retrieval
and improving ranking. Recently we have been
working on optimizing the retrieval module of our
pipeline system by investigating the influence of
different retrieval modules and passage segmenta-
tion strategies on the retrieval performance. This
work has resulted in a better passage retrieval mod-
ule in terms of success@150. Details on these ex-
periments are in (Khalid and Verberne, 2008).
Moreover, we have been collecting a larger data
collection in order to do make feature selection for
959
our re-ranking experiments more reliable and less
depending on specific cases in our dataset. This
work has resulted in a total set of 188 why-question
answer pairs. We are currently using this data
collection for further research into improving our
pipeline system.
In the near future, we aim to investigate what
type of information is needed for further improv-
ing our system for why-QA. With the addition of
syntactic information our system reaches an MRR
score of 0.380. This compares to the MRR scores
reached by other syntactically enhanced QA sys-
tems (see section 2). However, an MRR of 0.380
also shows that a substantial part of the problem
of why-QA is still to be solved. We are currently
investigating what type information is needed for
further system improvement.
Finally, we also plan experiments with a number
of dependency parsers to be used instead of EP4IR
for the syntactic analysis of the answer para-
graphs. Current experiments with Charniak (Char-
niak, 2000) show better constituent extraction than
with EP4IR. It is still to be seen whether this also
influences the overall performance of our system.
8 Conclusion
We added a re-ranking step to an existing para-
graph retrieval method for why-QA. For re-
ranking, we took the score assigned to a question
answer pair by the ranking algorithm QAP in the
baseline system, and weighted it with a number of
syntactic features. We experimented with 31 fea-
tures and trained the feature weights on a set of 93
why-questions with 150 answers provided by the
baseline system for each question. Feature values
for training the weights for the 31 features were
extracted from gold standard parse trees for each
question answer pair.
We evaluated the feature weights on automat-
ically parsed questions and answers, in five folds.
We found a significant improvement over the base-
line for both success@10 and MRR@150. The
most important features were the baseline score,
the presence of cue words, the question?s main
verb, and the relation between question focus and
document title.
We think that, although syntactic information
gives a significant improvement over baseline pas-
sage ranking, more improvement is still to be
gained from other types of information. Investi-
gating the type of information needed is part of our
future directions.
References
Bilotti, M.W., B. Katz, and J. Lin. 2004. What works
better for question answering: Stemming or mor-
phological query expansion. Proc. IR4QA at SIGIR
2004.
Burnage, G., R.H. Baayen, R. Piepenbrock, and H. van
Rijn. 1990. CELEX: A Guide for Users.
Buttcher, S., C.L.A. Clarke, and G.V. Cormack. 2004.
Domain-Specific Synonym Expansion and Valida-
tion for Biomedical Information Retrieval.
Buttcher, S. 2007. The Wumpus Search Engine.
http://www.wumpus-search.org/.
Charniak, E. 2000. A maximum-entropy-inspired
parser. ACM International Conference Proceeding
Series, 4:132?139.
Denoyer, L. and P. Gallinari. 2006. The Wikipedia
XML corpus. ACM SIGIR Forum, 40(1):64?69.
Ferret, O., B. Grau, M. Hurault-Plantet, G. Illouz,
L. Monceaux, I. Robba, and A. Vilnat. 2002. Find-
ing an answer based on the recognition of the ques-
tion focus. Proc. of TREC 2001, pages 500?250.
Goldberg, D.E. and J.H. Holland. 1988. Genetic Algo-
rithms and Machine Learning. Machine Learning,
3(2):95?99.
Higashinaka, R. and H. Isozaki. 2008. Corpus-based
Question Answering for why-Questions. In Proc. of
IJCNLP, vol.1, pages 418?425.
Hovy, E.H., U. Hermjakob, and D. Ravichandran.
2002. A Question/Answer Typology with Surface
Text Patterns. In Proc. of HLT 2002.
Khalid, M. and S. Verberne. 2008. Passage Retrieval
for Question Answering using Sliding Windows. In
Proc. of IR4QA at COLING 2008.
Koster, CHA. 2003. Head-modifier frames for every-
one. Proc. of SIGIR 2003, page 466.
Quarteroni, S., A. Moschitti, S. Manandhar, and
R. Basili. 2007. Advanced Structural Represen-
tations for Question Classification and Answer Re-
ranking. In Proc. of ECIR 2007, volume 4425, pages
234?245.
Tiedemann, J. 2005. Improving passage retrieval in
question answering using NLP. In Proc. of EPIA
2005.
Verberne, S., L. Boves, N. Oostdijk, and P.A. Coppen.
2008. Evaluating paragraph retrieval for why-QA.
In Proc. of ECIR 2008.
960
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 561?569,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
The effect of domain and text type on text prediction quality
Suzan Verberne, Antal van den Bosch, Helmer Strik, Lou Boves
Centre for Language Studies
Radboud University Nijmegen
s.verberne@let.ru.nl
Abstract
Text prediction is the task of suggesting
text while the user is typing. Its main aim
is to reduce the number of keystrokes that
are needed to type a text. In this paper, we
address the influence of text type and do-
main differences on text prediction quality.
By training and testing our text predic-
tion algorithm on four different text types
(Wikipedia, Twitter, transcriptions of con-
versational speech and FAQ) with equal
corpus sizes, we found that there is a clear
effect of text type on text prediction qual-
ity: training and testing on the same text
type gave percentages of saved keystrokes
between 27 and 34%; training on a differ-
ent text type caused the scores to drop to
percentages between 16 and 28%.
In our case study, we compared a num-
ber of training corpora for a specific data
set for which training data is sparse: ques-
tions about neurological issues. We found
that both text type and topic domain play
a role in text prediction quality. The
best performing training corpus was a set
of medical pages from Wikipedia. The
second-best result was obtained by leave-
one-out experiments on the test questions,
even though this training corpus was much
smaller (2,672 words) than the other cor-
pora (1.5 Million words).
1 Introduction
Text prediction is the task of suggesting text while
the user is typing. Its main aim is to reduce the
number of keystrokes that are needed to type a
text, thereby saving time. Text prediction algo-
rithms have been implemented for mobile devices,
office software (Open Office Writer), search en-
gines (Google query completion), and in special-
needs software for writers who have difficulties
typing (Garay-Vitoria and Abascal, 2006). In most
applications, the scope of the prediction is the
completion of the current word; hence the often-
used term ?word completion?.
The most basic method for word completion is
checking after each typed character whether the
prefix typed since the last whitespace is unique
according to a lexicon. If it is, the algorithm sug-
gests to complete the prefix with the lexicon en-
try. The algorithm may also suggest to complete a
prefix even before the word?s uniqueness point is
reached, using statistical information on the pre-
vious context. Moreover, it has been shown that
significantly better prediction results can be ob-
tained if not only the prefix of the current word
is included as previous context, but also previ-
ous words (Fazly and Hirst, 2003) or characters
(Van den Bosch and Bogers, 2008).
In the current paper, we follow up on this work
by addressing the influence of text type and do-
main differences on text prediction quality. Brief
messages on mobile devices (such as text mes-
sages, Twitter and Facebook updates) are of a dif-
ferent style and lexicon than documents typed in
office software (Westman and Freund, 2010). In
addition, the topic domain of the text also influ-
ences its content. These differences may cause an
algorithm trained on one text type or domain to
perform poorly on another.
The questions that we aim to answer in this pa-
per are (1) ?What is the effect of text type dif-
ferences on the quality of a text prediction algo-
rithm?? and (2) ?What is the best choice of train-
ing data if domain- and text type-specific data is
sparse??. To answer these questions, we perform
three experiments:
1. A series of within-text type experiments on
four different types of Dutch text: Wikipedia
articles, Twitter data, transcriptions of con-
561
versational speech and web pages of Fre-
quently Asked Questions (FAQ).
2. A series of across-text type experiments in
which we train and test on different text
types;
3. A case study using texts from a specific do-
main and text type: questions about neuro-
logical issues. Training data for this combi-
nation of language (Dutch), text type (FAQ)
and domain (medical/neurological) is sparse.
Therefore, we search for the type of training
data that gives the best prediction results for
this corpus. We compare the following train-
ing corpora:
? The corpora that we compared in the
text type experiments: Wikipedia, Twit-
ter, Speech and FAQ, 1.5 Million words
per corpus.
? A 1.5 Million words training corpus that
is of the same domain as the target data:
medical pages from Wikipedia;
? The 359 questions from the neuro-QA
data themselves, evaluated in a leave-
one-out setting (359 times training on
358 questions and evaluating on the re-
maining questions).
The prospective application of the third series
of experiments is the development of a text predic-
tion algorithm in an online care platform: an on-
line community for patients seeking information
about their illness. In this specific case the target
group is patients with language disabilities due to
neurological disorders.
The remainder of this paper is organized as fol-
lows: In Section 2 we give a brief overview of text
prediction methods discussed in the literature. In
Section 3 we present our approach to text predic-
tion. Sections 4 and 5 describe the experiments
that we carried out and the results we obtained.
We phrase our conclusions in Section 6.
2 Text prediction methods
Text prediction methods have been developed for
several different purposes. The older algorithms
were built as communicative devices for people
with disabilities, such as motor and speech impair-
ments. More recently, text prediction is developed
for writing with reduced keyboards, specifically
for writing (composing messages) on mobile de-
vices (Garay-Vitoria and Abascal, 2006).
All modern methods share the general idea that
previous context (which we will call the ?buffer?)
can be used to predict the next block of charac-
ters (the ?predictive unit?). If the user gets correct
suggestions for continuation of the text then the
number of keystrokes needed to type the text is
reduced. The unit to be predicted by a text pre-
diction algorithm can be anything ranging from a
single character (which actually does not save any
keystrokes) to multiple words. Single words are
the most widely used as prediction units because
they are recognizable at a low cognitive load for
the user, and word prediction gives good results
in terms of keystroke savings (Garay-Vitoria and
Abascal, 2006).
There is some variation among methods in the
size and type of buffer used. Most methods use
character n-grams as buffer, because they are pow-
erful and can be implemented independently of the
target language (Carlberger, 1997). In many al-
gorithms the buffer is cleared at the start of each
new word (making the buffer never larger than
the length of the current word). In the paper
by (Van den Bosch and Bogers, 2008), two ex-
tensions to the basic prefix-model are compared.
They found that an algorithm that uses the previ-
ous n characters as buffer, crossing word borders
without clearing the buffer, performs better than
both a prefix character model and an algorithm
that includes the full previous word as feature. In
addition to using the previously typed characters
and/or words in the buffer, word characteristics
such as frequency and recency could also be taken
into account (Garay-Vitoria and Abascal, 2006).
Possible evaluation measures for text predic-
tion are the proportion of words that are correctly
predicted, the percentage of keystrokes that could
maximally be saved (if the user would always
make the correct decision), and the time saved by
the use of the algorithm (Garay-Vitoria and Abas-
cal, 2006). The performance that can be obtained
by text prediction algorithms depends on the lan-
guage they are evaluated on. Lower results are ob-
tained for higher-inflected languages such as Ger-
man than for low-inflected languages such as En-
glish (Matiasek et al 2002). In their overview of
text prediction systems, (Garay-Vitoria and Abas-
cal, 2006) report performance scores ranging from
29% to 56% of keystrokes saved.
An important factor that is known to influence
the quality of text prediction systems, is training
562
set size (Lesher et al 1999; Van den Bosch,
2011). The paper by (Van den Bosch, 2011) shows
log-linear learning curves for word prediction (a
constant improvement each time the training cor-
pus size is doubled), when the training set size is
increased incrementally from 102 to 3?107 words.
3 Our approach to text prediction
We implement a text prediction algorithm for
Dutch, which is a productive compounding lan-
guage like German, but has a somewhat simpler
inflectional system. We do not focus on the effect
of training set size, but on the effect of text type
and topic domain differences.
Our approach to text prediction is largely in-
spired by (Van den Bosch and Bogers, 2008). We
experiment with two different buffer types that are
based on character n-grams:
? ?Prefix of current word? contains all char-
acters of only the word currently keyed in,
where the buffer shifts by one character posi-
tion with every new character.
? ?Buffer15? buffer also includes any other
characters keyed in belonging to previously
keyed-in words.
Modeling character history beyond the current
word can naturally be done with a buffer model in
which the buffer shifts by one position per charac-
ter, while a typical left-aligned prefix model (that
never shifts and fixes letters to their positional fea-
ture) would not be able to do this.
In the buffer, all characters from the text are
kept, including whitespace and punctuation. The
predictive unit is one token (word or punctuation
symbol). In both the buffer and the prediction la-
bel, any capitalization is kept. At each point in the
typing process, our algorithm gives one sugges-
tion: the word that is the most likely continuation
of the current buffer.
We save the training data as a classification data
set: each character in the buffer fills a feature slot
and the word that is to be predicted is the classi-
fication label. Figures 1 and 2 give examples of
each of the buffer types Prefix and Buffer15 that
we created for the text fragment ?tot een niveau?
in the context ?stelselmatig bij elke verkiezing tot
een niveau van? ?(structurally with each election
to a level of ). We use the implementation of the
IGTree decision tree algorithm in TiMBL (Daele-
mans et al 1997) to train our models.
3.1 Evaluation
We evaluate our algorithms on corpus data. This
means that we have to make assumptions about
user behaviour. We assume that the user confirms
a suggested word as soon as it is suggested cor-
rectly, not typing any additional characters before
confirming. We evaluate our text prediction al-
gorithms in terms of the percentage of keystrokes
saved K:
K =
?n
i=0(Fi)?
?n
i=0(Wi)
?n
i=0(Fi)
? 100 (1)
in which n is the number of words in the test
set, Wi is the number of keystrokes that have been
typed before the word i is correctly suggested
and Fi is the number of keystrokes that would be
needed to type the complete word i. For example,
our algorithm correctly predicts the word niveau
after the context i n g t o t e e n n i
v in the test set. Assuming that the user confirms
the word niveau at this point, three keystrokes
were needed for the prefix niv. So, Wi = 3 and
Fi = 6. The number of keystrokes needed for
whitespace and punctuation are unchanged: these
have to be typed anyway, independently of the
support by a text prediction algorithm.
4 Text type experiments
In this section, we describe the first and second se-
ries of experiments. The case study on questions
from the neurological domain is described in Sec-
tion 5.
4.1 Data
In the text type experiments, we evaluate our text
prediction algorithm on four different types of
Dutch text: Wikipedia, Twitter data, transcriptions
of conversational speech, and web pages of Fre-
quently Asked Questions (FAQ). The Wikipedia
corpus that we use is part of the Lassy cor-
pus (Van Noord, 2009); we obtained a version
from the summer of 2010.1 The Twitter data
are collected continuously and automatically fil-
tered for language by Erik Tjong Kim Sang (Tjong
Kim Sang, 2011). We used the tweets from all
users that posted at least 19 tweets (excluding
retweets) during one day in June 2011. This is
a set of 1 Million Twitter messages from 30,000
1http://www.let.rug.nl/vannoord/trees/Treebank/Machine/
NLWIKI20100826/COMPACT/
563
t tot
t o tot
t o t tot
e een
e e een
e e n een
n niveau
n i niveau
n i v niveau
n i v e niveau
n i v e a niveau
n i v e a u niveau
Figure 1: Example of buffer type ?Prefix? for the text fragment ?(elke verkiezing) tot een niveau?. Un-
derscores represent whitespaces.
l k e v e r k i e z i n g tot
k e v e r k i e z i n g t tot
e v e r k i e z i n g t o tot
v e r k i e z i n g t o t tot
v e r k i e z i n g t o t een
e r k i e z i n g t o t e een
r k i e z i n g t o t e e een
k i e z i n g t o t e e n een
i e z i n g t o t e e n niveau
e z i n g t o t e e n n niveau
z i n g t o t e e n n i niveau
i n g t o t e e n n i v niveau
n g t o t e e n n i v e niveau
g t o t e e n n i v e a niveau
t o t e e n n i v e a u niveau
Figure 2: Example of buffer type ?Buffer15? for the text fragment ?(elke verkiezing) tot een niveau?.
Underscores represent whitespaces.
different users. The transcriptions of conversa-
tional speech are from the Spoken Dutch Corpus
(CGN) (Oostdijk, 2000); for our experiments, we
only use the category ?spontaneous speech?. We
obtained the FAQ data by downloading the first
1,000 pages that Google returns for the query ?faq?
with the language restriction Dutch. After clean-
ing the pages from HTML and other coding, the
resulting corpus contained approximately 1.7 Mil-
lion words of questions and answers.
4.2 Within-text type experiments
For each of the four text types, we compare the
buffer types ?Prefix? and ?Buffer15?. In each ex-
periment, we use 1.5 Million words from the cor-
pus to train the algorithm and 100,000 words to
test it. The results are in Table 1.
4.3 Across-text type experiments
We investigate the importance of text type differ-
ences for text prediction with a series of experi-
ments in which we train and test our algorithm on
texts of different text types. We keep the size of
the train and test sets the same: 1.5 Million words
and 100,000 words respectively. The results are in
Table 2.
4.4 Discussion of the results
Table 1 shows that for all text types, the buffer
of 15 characters that crosses word borders gives
better results than the prefix of the current word
only. We get a relative improvement of 35% (for
FAQ) to 62% (for Speech) of Buffer15 compared
to Prefix-only.
Table 2 shows that text type differences have
an influence on text prediction quality: all across-
text type experiments lead to lower results than
the within-text type experiments. From the re-
sults in Table 2, we can deduce that of the four
text types, speech and Twitter language resem-
ble each other more than they resemble the other
two, and Wikipedia and FAQ resemble each other
more. Twitter and Wikipedia data are the least
similar: training on Wikipedia data makes the text
prediction score for Twitter data drop from 29.2 to
16.5%.2
2Note that the results are not symmetric. For example,
564
Table 1: Results from the within-text type experiments in terms of percentages of saved keystrokes.
Prefix means: ?use the previous characters of the current word as features?. Buffer 15 means ?use a buffer
of the previous 15 characters as features?.
Prefix Buffer15
Wikipedia 22.2% 30.5%
Twitter 21.3% 29.2%
Speech 20.7% 33.4%
FAQ 20.2% 27.2%
Table 2: Results from the across-text type experiments in terms of percentages of saved keystrokes, using
the best-scoring configuration from the within-text type experiments: a buffer of 15 characters
Trained on Tested on Wikipedia Tested on Twitter Tested on Speech Tested on FAQ
Wikipedia 30.5% 16.5% 22.3% 24.9%
Twitter 17.9% 29.2% 27.9% 20.7%
Speech 19.7% 22.5% 33.4% 21.0%
FAQ 22.6% 18.2% 22.9% 27.2%
5 Case study: questions about
neurological issues
Online care platforms aim to bring together pa-
tients and experts. Through this medium, patients
can find information about their illness, and get in
contact with fellow-sufferers. Patients who suffer
from neurological damage may have communica-
tive disabilities because their speaking and writ-
ing skills are impaired. For these patients, existing
online care platforms are often not easily accessi-
ble. Aphasia, for example, hampers the exchange
of information because the patient has problems
with word finding.
In the project ?Communicatie en revalidatie
DigiPoli? (ComPoli), language and speech tech-
nologies are implemented in the infrastructure of
an existing online care platform in order to fa-
cilitate communication for patients suffering from
neurological damage. Part of the online care plat-
form is a list of frequently asked questions about
neurological diseases with answers. A user can
browse through the questions using a chat-by-click
interface (Geuze et al 2008). Besides reading the
listed questions and answers, the user has the op-
tion to submit a question that is not yet included in
training on Wikipedia, testing on Twitter gives a different re-
sult from training on Twitter, testing on Wikipedia. This is
due to the size and domain of the vocabularies in both data
sets and the richness of the contexts (in order for the algo-
rithm to predict a word, it has to have seen it in the train set).
If the test set has a larger vocabulary than the train set, a lower
proportion of words can be predicted than when it is the other
way around.
the list. The newly submitted questions are sent to
an expert who answers them and adds both ques-
tion and answer to the chat-by-click database. In
typing the question to be submitted, the user will
be supported by a text prediction application.
The aim of this section is to find the best train-
ing corpus for newly formulated questions in the
neurological domain. We realize that questions
formulated by users of a web interface are dif-
ferent from questions formulated by experts for
the purpose of a FAQ-list. Therefore, we plan to
gather real user data once we have a first version
of the user interface running online. For develop-
ing the text prediction algorithm that is behind the
initial version of the application, we aim to find
the best training corpus using the questions from
the chat-by-click data as training set.
5.1 Data
The chat-by-click data set on neurological issues
consists of 639 questions with corresponding an-
swers. A small sample of the data (translated to
English) is shown in Table 3. In order to create the
test data for our experiments, we removed dupli-
cate questions from the chat-by-click data, leaving
a set of 359 questions.3
In the previous sections, we used corpora of
100,000 words as test collections and we calcu-
lated the percentage of saved keystrokes over the
3Some questions and answers are repeated several times
in the chat-by-click data because they are located at different
places in the chat-by-click hierarchy.
565
Table 3: A sample of the neuro-QA data, translated to English.
question 0 505 Can (P)LS be cured?
answer 0 505 Unfortunately, a real cure is not possible. However, things can be done to combat the effects of the
diseases, mainly relieving symptoms such as stiffness and spasticity. The phisical therapist and reha-
bilitation specialist can play a major role in symptom relief. Moreover, there are medications that can
reduce spasticity.
question 0 508 How is (P)LS diagnosed?
answer 0 508 The diagnosis PLS is difficult to establish, especially because the symptoms strongly resemble HSP
symptoms (Strumpell?s disease). Apart from blood and muscle research, several neurological examina-
tions will be carried out.
Table 4: Results for the neuro-QA questions only in terms of percentages of saved keystrokes, using
different training sets. The text prediction configuration used in all settings is Buffer15. The test samples
are 359 questions with an average length of 7.5 words. The percentages of saved keystrokes are means
over the 359 questions.
Training corpus # words Mean % of saved keystrokes in
neuro-QA questions (stdev)
OOV-rate
Twitter 1.5 Million 13.3% (12.5) 28.5%
Speech 1.5 Million 14.1% (13.2) 26.6%
Wikipedia 1.5 Million 16.1% (13.1) 19.4%
FAQ 1.5 Million 19.4% (15.6) 20.0%
Medical Wikipedia 1.5 Million 28.1% (16.5) 7.0%
Neuro-QA questions (leave-one-out) 2,672 26.5% (19.9) 17.8%
complete test corpus. In the reality of our case
study however, users will type only brief frag-
ments of text: the length of the question they want
to submit. This means that there is potentially a
large deviation in the effectiveness of the text pre-
diction algorithm per user, depending on the con-
tent of the small text they are typing. Therefore,
we decided to evaluate our training corpora sepa-
rately on each of the 359 unique questions, so that
we can report both mean and standard deviation
of the text prediction scores on small (realistically
sized) samples. The average number of words per
question is 7.5; the total size of the neuro-QA cor-
pus is 2,672 words.
5.2 Experiments
We aim to find the training set that gives the best
text prediction result for the neuro-QA questions.
We compare the following training corpora:
? The corpora that we compared in the text type
experiments: Wikipedia, Twitter, Speech and
FAQ, 1.5 Million words per corpus.
? A 1.5 Million words training corpus that is
of the same topic domain as the target data:
Wikipedia articles from the medical domain;
? The 359 questions from the neuro-QA data
themselves, evaluated in a leave-one-out set-
ting (359 times training on 358 questions and
evaluating on the remaining questions).
In order to create the ?medical Wikipedia? cor-
pus, we consulted the category structure of the
Wikipedia corpus. The Wikipedia category ?Ge-
neeskunde? (Medicine) contains 69,898 pages and
in the deeper nodes of the hierarchy we see many
non-medical pages, such as trappist beers (or-
dered under beer, booze, alcohol, Psychoactive
drug, drug, and then medicine). If we remove all
pages that are more than five levels under the ?Ge-
neeskunde? category root, 21,071 pages are left,
which contain fairly over the 1.5 Million words
that we need. We used the first 1.5 Million words
of the corpus in our experiments.
The text prediction results for the different cor-
pora are in Table 4. For each corpus, the out-of-
vocabulary rate is given: the percentage of words
in the Neuro-QA questions that do not occur in the
corpus.4
5.3 Discussion of the results
We measured the statistical significance of the
mean differences between all text prediction
scores using a Wilcoxon Signed Rank test on
paired results for the 359 questions. We found that
4The OOV-rate for the Neuro-QA corpus itself is the av-
erage of the OOV-rate of each leave-one-out experiment: the
proportion of words that only occur in one question.
566
0 10 20 30 40 50 60
0.0
0.2
0.4
0.6
0.8
1.0
ECDFs for text prediction scores on Neuro?QA questions
 using six different training corpora
Text prediction scores
Cu
mu
lat
ive
 Pe
rce
nt 
of 
tes
t c
orp
us
Twitter
Speech
Wikipedia
FAQ
Neuro?QA (leave?one?out)
Medical Wikipedia
Figure 3: Empirical CDFs for text prediction scores on Neuro-QA data. Note that the curves that are at
the bottom-right side represent the better-performing settings.
the difference between the Twitter and Speech cor-
pora on the task is not significant (P = 0.18).
The difference between Neuro-QA and Medical
Wikipedia is significant with P = 0.02; all other
differences are significant with P < 0.01.
The Medical Wikipedia corpus and the leave-
one-out experiments on the Neuro-QA data give
better text prediction scores than the other corpora.
The Medical Wikipedia even scores slightly better
than the Neuro-QA data itself. Twitter and Speech
are the least-suited training corpora for the Neuro-
QA questions, and FAQ data gives a bit better re-
sults than a general Wikipedia corpus.
These results suggest that both text type and
topic domain play a role in text prediction qual-
ity, but the high scores for the Medical Wikipedia
corpus shows that topic domain is even more im-
portant than text type.5 The column ?OOV-rate?
shows that this is probably due to the high cover-
age of terms in the Neuro-QA data by the Medical
5We should note here that we did not control for domain
differences between the four different text types. They are
intended to be ?general domain? but Wikipedia articles will
naturally be of different topics than conversational speech.
Wikipedia corpus.
Table 4 also shows that the standard devia-
tion among the 359 samples is relatively large.
For some questions, we 0% of the keystrokes are
saved, while for other, scores of over 80% are ob-
tained (by the Neuro-QA and Medical Wikipedia
training corpora). We further analyzed the differ-
ences between the training sets by plotting the Em-
pirical Cumulative Distribution Function (ECDF)
for each experiment. An ECDF shows the devel-
opment of text prediction scores (shown on the X-
axis) by walking through the test set in 359 steps
(shown on the Y-axis).
The ECDFs for our training corpora are in Fig-
ure 3. Note that the curves that are at the bottom-
right side represent the better-performing settings
(they get to a higher maximum after having seen
a smaller portion of the samples). From Figure 3,
it is again clear that the Neuro-QA and Medical
Wikipedia corpora outperform the other training
corpora, and that of the other four, FAQ is the best-
performing corpus. Figure 3 also shows a large
difference in the sizes of the starting percentiles:
The proportion of samples with a text prediction
567
Histogram of text prediction scores for the Neuro?QA
 questions trained on Medical Wikipedia
percentage of keystrokes saved
Freq
uen
cy
0 20 40 60 80
0
20
40
60
80
Figure 4: Histogram of text prediction scores
for the Neuro-QA questions trained on Medical
Wikipedia. Each bin represents 36 questions.
score of 0% is less than 10% for the Medical
Wikipedia up to more than 30% for Speech.
We inspected the questions that get a text pre-
diction score of 0%. We see many medical terms
in these questions, and many of the utterances are
not even questions, but multi-word terms repre-
senting topical headers in the chat-by-click data.
Seven samples get a zero-score in the output of all
six training corpora, e.g.:
? glycogenose III.
? potassium-aggrevated myotonias.
26 samples get a zero-score in the output of all
training corpora except for Medical Wikipedia and
Neuro-QA itself. These are mainly short headings
with domain-specific terms such as:
? idiopatische neuralgische amyotrofie.
? Markesbery-Griggs distale myopathie.
? oculopharyngeale spierdystrofie.
Interestingly, the ECDFs show that the Med-
ical Wikipedia and Neuro-QA corpora cross at
around percentile 70 (around the point of 40%
saved keystrokes). This indicates that although the
means of the two result samples are close to each
other, the distribution the scores for the individ-
ual questions is different. The histograms of both
distributions (Figures 4 and 5) confirm this: the
algorithm trained on the Medical Wikipedia cor-
pus leads a larger number of samples with scores
Histogram of text prediction scores for leave?one?out
 experiments on Neuro?QA questions
percentage of keystrokes saved
Freq
uen
cy
0 20 40 60 80
0
20
40
60
80
Figure 5: Histogram of text prediction scores
for leave-one-out experiments on Neuro-QA ques-
tions. Each bin represents 36 questions.
around the mean, while the leave-one-out exper-
iments lead to a larger number of samples with
low prediction scores and a larger number of sam-
ples with high prediction scores. This is also re-
flected by the higher standard deviation for Neuro-
QA than for Medical Wikipedia.
Since both the leave-one-out training on the
Neuro-QA questions and the Medical Wikipedia
led to good results but behave differently for dif-
ferent portions of the test data, we also evaluated a
combination of both corpora on our test set: We
created training corpora consisting of the Medi-
cal Wikipedia corpus, complemented by 90% of
the Neuro-QA questions, testing on the remaining
10% of the Neuro-QA questions. This led to mean
percentage of saved keystrokes of 28.6%, not sig-
nificantly higher than just the Medical Wikipedia
corpus.
6 Conclusions
In Section 1, we asked two questions: (1) ?What
is the effect of text type differences on the quality
of a text prediction algorithm?? and (2) ?What is
the best choice of training data if domain- and text
type-specific data is sparse??
By training and testing our text prediction al-
gorithm on four different text types (Wikipedia,
Twitter, transcriptions of conversational speech
and FAQ) with equal corpus sizes, we found that
there is a clear effect of text type on text prediction
quality: training and testing on the same text type
568
gave percentages of saved keystrokes between 27
and 34%; training on a different text type caused
the scores to drop to percentages between 16 and
28%.
In our case study, we compared a number of
training corpora for a specific data set for which
training data is sparse: questions about neuro-
logical issues. We found significant differences
between the text prediction scores obtained with
the six training corpora: the Twitter and Speech
corpora were the least suited, followed by the
Wikipedia and FAQ corpus. The highest scores
were obtained by training the algorithm on the
medical pages from Wikipedia, immediately fol-
lowed by leave-one-out experiments on the 359
neurological questions. The large differences be-
tween the lexical coverage of the medical domain
played a central role in the scores for the different
training corpora.
Because we obtained good results by both
the Medical Wikipedia corpus and the neuro-QA
questions themselves, we opted for a combination
of both data types as training corpus in the initial
version of the online text prediction application.
Currently, a demonstration version of the appli-
cation is running for ComPoli-users. We hope to
collect questions from these users to re-train our
algorithm with more representative examples.
Acknowledgments
This work is part of the research programme
?Communicatie en revalidatie digiPoli? (Com-
Poli6), which is funded by ZonMW, the Nether-
lands organisation for health research and devel-
opment.
References
J. Carlberger. 1997. Design and Implementation of a
Probabilistic Word Prediciton Program. Master the-
sis, Royal Institute of Technology (KTH), Sweden.
W. Daelemans, A. Van Den Bosch, and T. Weijters.
1997. IGTree: Using trees for compression and clas-
sification in lazy learning algorithms. Artificial In-
telligence Review, 11(1):407?423.
A. Fazly and G. Hirst. 2003. Testing the efficacy of
part-of-speech information in word completion. In
Proceedings of the 2003 EACL Workshop on Lan-
guage Modeling for Text Entry Methods, pages 9?
16.
6http://lands.let.ru.nl/?strik/research/ComPoli/
N. Garay-Vitoria and J. Abascal. 2006. Text prediction
systems: a survey. Universal Access in the Informa-
tion Society, 4(3):188?203.
J. Geuze, P. Desain, and J. Ringelberg. 2008. Re-
phrase: chat-by-click: a fundamental new mode of
human communication over the internet. In CHI?08
extended abstracts on Human factors in computing
systems, pages 3345?3350. ACM.
G.W. Lesher, B.J. Moulton, D.J. Higginbotham, et al
1999. Effects of ngram order and training text size
on word prediction. In Proceedings of the RESNA
?99 Annual Conference, pages 52?54.
Johannes Matiasek, Marco Baroni, and Harald Trost.
2002. FASTY - A Multi-lingual Approach to Text
Prediction. In Klaus Miesenberger, Joachim Klaus,
and Wolfgang Zagler, editors, Computers Helping
People with Special Needs, volume 2398 of Lec-
ture Notes in Computer Science, pages 165?176.
Springer Berlin / Heidelberg.
N. Oostdijk. 2000. The spoken Dutch corpus:
overview and first evaluation. In Proceedings of
LREC-2000, Athens, volume 2, pages 887?894.
Erik Tjong Kim Sang. 2011. Het gebruik van Twit-
ter voor Taalkundig Onderzoek. In TABU: Bulletin
voor Taalwetenschap, volume 39, pages 62?72. In
Dutch.
A. Van den Bosch and T. Bogers. 2008. Efficient
context-sensitive word completion for mobile de-
vices. In Proceedings of the 10th international con-
ference on Human computer interaction with mobile
devices and services, pages 465?470. ACM.
A. Van den Bosch. 2011. Effects of context and re-
cency in scaled word completion. Computational
Linguistics in the Netherlands Journal, 1:79?94,
12/2011.
G. Van Noord. 2009. Huge parsed corpora in LASSY.
In Proceedings of The 7th International Workshop
on Treebanks and Linguistic Theories (TLT7).
S. Westman and L. Freund. 2010. Information Interac-
tion in 140 Characters or Less: Genres on Twitter. In
Proceedings of the third symposium on Information
Interaction in Context (IIiX), pages 323?328. ACM.
569
What Is Not in the Bag of Words forWhy-QA?
Suzan Verberne?
Radboud University Nijmegen
Lou Boves??
Radboud University Nijmegen
Nelleke Oostdijk?
Radboud University Nijmegen
Peter-Arno Coppen?
Radboud University Nijmegen
While developing an approach towhy-QA, we extended a passage retrieval system that uses off-
the-shelf retrieval technology with a re-ranking step incorporating structural information. We
get significantly higher scores in terms of MRR@150 (from 0.25 to 0.34) and success@10. The
23% improvement that we reach in terms of MRR is comparable to the improvement reached on
different QA tasks by other researchers in the field, although our re-ranking approach is based
on relatively lightweight overlap measures incorporating syntactic constituents, cue words, and
document structure.
1. Introduction
About 5% of all questions asked to QA systems are why-questions (Hovy, Hermjakob,
and Ravichandran 2002). Why-questions need a different approach than factoid ques-
tions, because their answers are explanations that usually cannot be stated in a single
phrase. Recently, research (Verberne 2006; Higashinaka and Isozaki 2008) has been
directed at QA forwhy-questions (why-QA). In earlier work on answeringwhy-questions
on the basis of Wikipedia, we found that the answers to most why-questions are pas-
sages of text that are at least one sentence and at most one paragraph in length (Verberne
et al 2007b). Therefore, we aim at developing a system that takes as input a why-
question and gives as output a ranked list of candidate answer passages.
In the current article, we propose a three-step setup for a why-QA system: (1) a
question-processing module that transforms the input question to a query; (2) an off-
the-shelf retrieval module that retrieves and ranks passages of text that share content
? Department of Linguistics, PO Box 9103, 6500 HD Nijmegen, the Netherlands.
E-mail: s.verberne@let.ru.nl.
?? Department of Linguistics, PO Box 9103, 6500 HD Nijmegen, the Netherlands.
E-mail: l.boves@let.ru.nl.
? Department of Linguistics, PO Box 9103, 6500 HD Nijmegen, the Netherlands.
E-mail: n.oostdijk@let.ru.nl.
? Department of Linguistics, PO Box 9103, 6500 HD Nijmegen, the Netherlands.
E-mail: p.a.coppen@let.ru.nl.
Submission received: 30 July 2008; revised submission received: 18 February 2009; accepted for publication:
4 September 2009.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 2
with the input query; and (3) a re-ranking module that adapts the scores of the re-
trieved passages using structural information from the input question and the retrieved
passages.
In the first part of this article, we focus on step 2, namely, passage retrieval. The
classic approach to finding passages in a text collection that share content with an
input query is retrieval using a bag-of-words (BOW) model (Salton and Buckley 1988).
BOWmodels are based on the assumption that text can be represented as an unordered
collection of words, disregarding grammatical structure. Most BOW-based models use
statistical weights based on term frequency, document frequency, passage length, and
term density (Tellex et al 2003).
Because BOW approaches disregard grammatical structure, systems that rely on
a BOW model have their limitations in solving problems where the syntactic relation
betweenwords or word groups is crucial. The importance of syntax for QA is sometimes
illustrated by the sentence Ruby killed Oswald, which is not an answer to the question
Who did Oswald kill? (Bilotti et al 2007). Therefore, a number of researchers in the field
investigated the use of structural information on top of a BOW approach for answer
retrieval and ranking (Tiedemann 2005; Quarteroni et al 2007; Surdeanu, Ciaramita, and
Zaragoza 2008). These studies show that although the BOW model makes the largest
contribution to the QA system results, adding structural (syntactic information) can give
a significant improvement.
In the current article, we hypothesize that for the relatively complex problem of
why-QA, a significant improvement?at least comparable to the improvement gained
for factoidQA?can be gained from the addition of structural information to the ranking
component of the QA system. We first evaluate a passage retrieval system for why-QA
based on standard BOW ranking (step 1 and 2 in our set-up). Then we perform an
analysis of the strengths and weaknesses of the BOW model for retrieving and ranking
candidate answers. In view of the observed weaknesses of the BOW model, we choose
our feature set to be applied to the set of candidate answer passages in the re-ranking
module (step 3 in our set-up).
The structural features that we propose are based on the idea that some parts of the
question and the answer passage are more important for relevance ranking than other
parts. Therefore, our re-ranking features are overlap-based: They tell us which parts of
a why-question and its candidate answers are the most salient for ranking the answers.
We evaluate our initial and adapted ranking strategies using a set of why-questions and
a corpus of Wikipedia documents, and we analyze the contribution of both the BOW
model and the structural features.
The main contributions of this article are: (1) we address the relatively new problem
of why-QA and (2) we analyze the contribution of overlap-based structural information
to the problem of answer ranking.
The paper is organized as follows. In Section 2, related work is discussed. Section 3
presents the BOW-based passage retrieval method forwhy-QA, followed by a discussion
of the strengths andweaknesses of the approach in Section 4. In Section 5, we extend our
system with a re-ranking component based on structural overlap features. A discussion
of the results and our conclusions are presented in Sections 6 and 7, respectively.
2. Related Work
Wedistinguish relatedwork in two directions: research into the development of systems
for why-QA (Section 2.1), and research into combining structural and BOW features for
QA (Section 2.2).
230
Verberne et al What Is Not in the Bag of Words forWhy-QA?
2.1 Research intoWhy-QA
In related work (Verberne et al 2007a), we focused on selecting and ranking explanatory
passages for why-QA with the use of rhetorical structures. We developed a system that
employs the discourse relations in a manually annotated document collection: the RST
Treebank (Carlson, Marcu, and Okurowski 2003). This system matches the input ques-
tion to a text span in the discourse tree of the document and it retrieves as answer the
text span that has a specific discourse relation to this question span. We evaluated our
method on a set of 336 why-questions formulated to seven texts from the WSJ corpus.
We concluded that discourse structure can play an important role in why-QA, but that
systems relying on these structures can only work if candidate answer passages have
been annotated with discourse structure. Automatic parsers for creating full rhetorical
structures are currently unavailable. Therefore, a more practical approach appears to
be necessary for work in why-QA, namely, one which is based on automatically created
annotations.
Higashinaka and Isozaki (2008) focus on the problem of ranking candidate answer
paragraphs for Japanese why-questions. They assume that a document retrieval module
has returned the top 20 documents for a given question. They extract features for content
similarity, causal expressions, and causal relations from two annotated corpora and a
dictionary. Higashinaka and Isozaki evaluate their ranking method using a set of 1,000
why-questions that were formulated to a newspaper corpus by a text analysis expert.
70.3% of the reference answers for these questions are ranked in the top 10 by their
system, and MRR1 was 0.328.
Although the approach of Higashinaka and Isozaki is very interesting, their eval-
uation collection has the same flaw as the one used by Verberne et al (2007a): Both
collections consist of questions formulated to a pre-selected answer text. Questions
elicited in response to newspaper texts tend to be unrepresentative of questions asked
in a real QA setting. In the current work, therefore, we work with a set of questions
formulated by users of an online QA system (see Section 3.1).
2.2 Combining Structural and Bag-of-Words Features for QA
Tiedemann (2005) investigates syntactic information from dependency structures in
passage retrieval for Dutch factoid QA. He indexes his corpus at different text layers
(BOW, part-of-speech, dependency relations) and uses the same layers for question
analysis and query creation. He optimizes the query parameters for the passage retrieval
task by having a genetic algorithm apply the weights to the query terms. Tiedemann
finds that the largest weights are assigned to the keywords from the BOW layer and
to the keywords related to the predicted answer type (such as ?person?). The baseline
approach, using only the BOW layer, gives an MRR of 0.342. Using the optimized IR
settings with additional layers, MRR improves to 0.406.
Quarteroni et al (2007) consider the problem of answering definition questions.
They use predicate?argument structures (PAS) for improved answer ranking. They find
that PAS as a stand-alone representation is inferior to parse tree representations, but
that together with the BOW it yields higher accuracy. Their results show a significant
1 The reciprocal rank (RR) for a question is 1 divided by the rank ordinal of the highest ranked relevant
answer. The Mean RR is obtained by averaging RR over all questions.
231
Computational Linguistics Volume 36, Number 2
improvement of PAS?BOW compared to parse trees (F-scores 70.7% vs. 59.6%) but PAS
makes only a very small contribution compared to BOW only (which gives an F-score
of 69.3%).
Recent work by Surdeanu, Ciaramita, and Zaragoza (2008) addresses the problem
of answer ranking for how-to-questions. From Yahoo! Answers,2 they extract a corpus
of 140,000 answers with 40,000 questions. They investigate the usefulness of a large
set of question and answer features in the ranking task. They conclude that the linguistic
features ?yield a small, yet statistically significant performance increase on top of the
traditional BOW and n-gram representation (page 726).?
All these authors conclude that the addition of structural information in QA
gives a small but significant improvement compared to using a BOW-model only. For
why-questions, we also expect to gain improvement from the addition of structural
information.
3. Passage Retrieval forWhy-QA Using a BOWModel
As explained in Section 1, our system comprises three modules: question2query, passage
retrieval, and re-ranking. In the current section, we present the first two system mod-
ules, and the re-ranking module, including a description of the structural features that
we consider, is presented in Section 5. First, however, we describe our data collection
and evaluation method.
3.1 Data and Evaluation Set-up
For our experiments, we use the Wikipedia INEX corpus (Denoyer and Gallinari 2006).
This corpus consists of all 659,388 articles from the online Wikipedia in the summer of
2006 in XML format.
For development and testing purposes, we exploit the Webclopedia question
set (Hovy, Hermjakob, and Ravichandran 2002), which contains questions asked to
the online QA system answers.com. Of these questions, 805 (5% of the total set) are
why-questions. For 700 randomly selected why-questions, we manually searched for an
answer in the Wikipedia XML corpus, saving the remaining 105 questions for future
testing purposes. 186 of these 700 questions have an answer in the corpus.3 Extraction
of one relevant answer for each of these questions resulted in a set of 186 why-questions
and their reference answers.4 Two examples illustrate the type of data we are working
with:
1. ?Why didn?t Socrates leave Athens after he was convicted?? ? ?Socrates
considered it hypocrisy to escape the prison: he had knowingly agreed to
live under the city?s laws, and this meant the possibility of being judged
guilty of crimes by a large jury.?
2 See http://answers.yahoo.com/.
3 Thus, about 25% of our questions have an answer in the Wikipedia corpus. The other questions are either
too specific (Why do ceiling fans turn counter-clockwise but table fans turn clockwise?) or too trivial (Why do
hotdogs come in packages of 10 and hotdog buns in packages of 8?) for the coverage of Wikipedia in 2006.
4 Just like factoid questions, most why-questions generally have one correct answer that can be formulated
in different ways.
232
Verberne et al What Is Not in the Bag of Words forWhy-QA?
2. ?Why do most cereals crackle when you add milk?? ? ?They are made of
a sugary rice mixture which is shaped into the form of rice kernels and
toasted. These kernels bubble and rise in a manner which forms very thin
walls. When the cereal is exposed to milk or juices, these walls tend to
collapse suddenly, creating the famous ?Snap, crackle and pop? sounds.?
To be able to do fast evaluation without elaborate manual assessments, we manually
created one answer pattern for each of the questions in our set. The answer pattern is a
regular expression that defines which of the retrieved passages are considered a relevant
answer to the input question. The first version of the answer patterns was directly
based on the corresponding reference answer, but in the course of the development
and evaluation process, we extended the patterns in order to cover as many as possible
of the Wikipedia passages that contain an answer. For example, for question 1, we
developed the following answer pattern based on two variants of the correct answer
that occur in the corpus: /(Socrates.* opportunity.* escape.* Athens.* considered.*
hypocrisy | leave.* run.* away.* community.* reputation)/.5
In fact, answer judgment is a complex task due to the presence of multiple answer
variants in the corpus. It is a time-consuming process because of the large number of
candidate answers that need to be judged when long lists of answers are retrieved per
question. In future work, we will come back to the assessment of relevant and irrelevant
answers.
After applying our answer patterns to the passages retrieved, we count the ques-
tions that have at least one relevant answer in the top n results. This number divided by
the total number of questions in a test set gives the measure success@n. In Section 3.2,
we explain the levels for n that we use for evaluation. For the highest ranked relevant
answer per question, we determine the RR. Questions for which the system did not
retrieve an answer in the list of 150 results get an RR of 0. Over all questions, we calculate
the mean reciprocal rank MRR.
3.2 Method and Results
In the question2query module of our system we convert the input question to a query
by removing stop words6 and punctuation, and simply list the remaining content words
as query terms.
The second module of our system performs passage retrieval using off-the-shelf
retrieval technology. In Khalid and Verberne (2008), we compared a number of settings
for our passage retrieval task. We considered two different retrieval engines (Lemur7
and Wumpus8), four different ranking models, and two types of passage segmentation:
disjoint and sliding passages. In each setting, 150 results were obtained by the retrieval
engine and ranked by the retrieval model. We evaluated all retrieval settings in terms of
5 Note that the vertical bar separates the two alternatives.
6 To this end we use the stop word list that can be found at http://marlodge.supanet.com/museum/
funcword.html.We use all items except the numbers and the word why.
7 Lemur is an open source toolkit for information retrieval that provides flexible support for different types
of retrieval models. See http://www.lemurproject.org.
8 Wumpus is an information retrieval system mainly geared at XML retrieval. See http://www.wumpus-
search.org/.
233
Computational Linguistics Volume 36, Number 2
MRR@n9 and success@n for levels n = 10 and n = 150. For the evaluation of the retrieval
module, we were mainly interested in the scores for success@150 because re-ranking
can only be successful if at least one relevant answer was returned by the retrieval
module.
We found that the best-scoring passage retrieval setting in terms of success@150 is
Lemur on an index of sliding passages with TF-IDF (Zhai 2001) as ranking model. We
obtained the following results with this passage retrieval setting: success@150 is 78.5%,
success@10 is 45.2%, and MRR@150 is 0.25. We do not include the results obtained with
the other retrieval settings here because the differences were small.
The results show that for 21.5% of the questions in our set, no answer was retrieved
in the top-150 results. We attempted to increase this coverage by retrieving 250 or
500 answers per question but this barely increased the success score at maximum
n. The main problems for the questions that we miss are infamous retrieval prob-
lems such as the vocabulary gap between a question and its answer. For example,
the answer to Why do chefs wear funny hats? contains none of the words from the
question.
4. The Strengths and Weaknesses of the BOWModel
In order to understand how answer ranking is executed by the passage retrieval mod-
ule, we first take a closer look at the TF-IDF algorithm as it has been implemented in
Lemur. TF-IDF is a pure BOW model: Both the query and the passages in the corpus
are represented by the term frequencies (numbers of occurrences) for each of the words
they contain. The terms are weighted using their inverse document frequency (IDF),
which puts a higher weight on terms that occur in few passages than on terms that
occur in many passages. The term frequency (TF) functions for the query and the doc-
ument, and the parameter values chosen for these functions in Lemur can be found in
Zhai (2001).
As explained in the previous section, we consider success@150 to be the most
important measure for the retrieval module of our system. However, for the system as a
whole, success@10 is a more important evaluation measure. This is because users tend
to pay much more attention to the top 10 results of a retrieval system than to results that
are ranked lower (Joachims et al 2005). Therefore, it is interesting to investigate which
questions are answered in the top 150 and not in the top 10 by our passage retrieval
module. This is the set of questions for which the BOW model is not effective enough
and additional (more specific) overlap information is needed for ranking a relevant
answer in the top 10.
We analyzed the set of questions that get a relevant answer at a rank between 10 and
150 (62 questions), which belowwewill refer to as our focus set. We compared our focus
set to the questions for which a relevant answer is in the top 10 (84 questions). Although
these numbers are too small to do a quantitative error analysis, a qualitative analysis
provides valuable insights into the strengths and weaknesses of a BOW representation
such as TF-IDF. In Sections 4.1 to 4.4 we discuss four different aspects of why-questions
that present problems for the BOWmodel.
9 Note that MRR is often used without the explicit cut-off point (n). We add it to clarify that RR is 0 for the
questions without a correct answer in the top-n.
234
Verberne et al What Is Not in the Bag of Words forWhy-QA?
4.1 Short Questions
Ten questions in our focus set contain only one or two content words. We can see the
effect of short queries if we compare three questions that contain only one semantically
rich content word.10 The rank of the highest ranked relevant answer is given between
parentheses; the last of these three questions is in our focus set.
1. Why do people hiccup? (2)
2. Why do people sneeze? (4)
3. Why do we dream? (76)
We found that the rank of the relevant answer is related to the corpus frequency of
the single semantically rich word, which is 64 for hiccup, 220 for sneeze, and 13,458 for
dream. This means that many passages are retrieved for question 3, making the chances
for the relevant answer to be ranked in the top 10 smaller. One way to overcome the
problem of long result lists for short queries is by adding words to the query that make
it more specific. In the case of why-QA, we know that we are not simply searching
for information on dreaming but for an explanation for dreaming. Thus, in the ranking
process, we can extend the query with explanatory cue words such as because.11 We
expect that the addition of explanatory cue phrases will give an improvement in ranking
performance.
4.2 The Document Context of the Answer
There are many cases where the context of the candidate answer gives useful infor-
mation. Consider, for example, the question Why does a snake flick out its tongue?, the
correct answer to which was ranked 29. A human searcher expects to find the answer
in a Wikipedia article about snakes. Within the Snake article he or she may search for
the words flick and/or tongue in order to find the answer. This suggests that in some
cases there is a direct relation between a specific part of the question and the context
(document and/or section) of the candidate answer. In cases like this, the answer
document and the question apparently share the same topic (snake). By analogy with
linguistically motivated approaches to factoid QA (Ferret et al 2002), we introduce the
term question focus for this topic.
In the example question flick is the word with the lowest corpus frequency (556),
followed by tongue (4,925) and snake (6,809). Using a BOW approach to document title
matching, candidate answers from documents with flick or tongue in their title would
be ranked higher than answers from documents with snake in their title. Thus, for
questions for which there is overlap between the question focus and the title of the
answer documents (two thirds of the questions in our set), we can improve the ranking
of candidate answers by correctly predicting the question focus. In Section 5.1.2, we
make concrete suggestions for achieving this.
10 The word people in subject position is a semantically poor content word.
11 The addition of cue words can also be considered to be applied in the retrieval step. We come back to this
in Section 6.3.
235
Computational Linguistics Volume 36, Number 2
4.3 Multi-Word Terms
A very important characteristic of the BOWmodel is that words are considered separate
terms. One of the consequences is that multi-word terms such as multi-word noun
phrases (mwNPs) are not treated as a single term. Here, three examples of questions
are shown in which the subject is realized by a mwNP (underlined in the examples; the
rank of the relevant answer is shown between brackets):
1. Why are hush puppies called hush puppies? (1)
2. Why is the coral reef disappearing? (29)
3. Why is a black hole black? (31)
We investigated the corpus frequencies for the separate parts of each mwNP. We found
that these are quite high for coral (3,316) and reef (2,597) compared to the corpus
frequency of the phrase coral reef (365). The numbers are even more extreme for black
(103,550) and hole (9,734) versus black hole (1,913). On the other hand, the answer to
the hush puppies question can more easily be ranked because the corpus frequencies
for the separate terms hush (594) and puppies (361) are relatively low. This shows that
multi-word terms do not necessarily give problems for the BOW model as long as the
document frequencies for the constituent words are relatively low. If (one of) the words
in the phrase is/are frequent, it is very difficult to rank the relevant answer high in the
result list with use of word overlap only.
In our focus set, 36 of the 62 questions contain a mwNP. For these questions, we can
expect improved ranking from the addition of NPs to our feature set.
4.4 Syntactic Structure
The BOW model does not take into account sentence structure. The potential impor-
tance of sentence structure for improved ranking can be exemplified by the following
two questions from our set. Note that both examples contain a subordinate clause (finite
or non-finite):
1. Why do baking soda and vinegar explode when you mix them together? (4)
2. Why are there 72 points to the inch when discussing fonts and printing? (36)
In both cases, the contents of the subordinate clause are less important to the goal of the
question than the contents of themain clause. In the first example, this is (coincidentally)
reflected by the corpus frequencies of the words in both clauses: mix (12,724) and
together (83,677) have high corpus frequencies compared to baking (832), soda (1,620),
vinegar (871), and explode (1,285). As a result, the reference answer containing these
terms is ranked in the top-10 by TF-IDF. In the second example, however, the corpus
frequencies do not reflect the importance of the terms. Fonts and printing have lower
corpus frequencies (1,243 and 6,978, respectively) than points (43,280) and inch (10,046).
Thus, fonts and printing are weighted heavier by TF-IDF although these terms are only
peripheral to the goal of the query, the core of which isWhy are there 72 points to the inch?
This cannot be derived from the corpus frequencies, but can only be inferred from the
syntactic function (adverbial) of when discussing fonts and printing in the question.
Thus, the lack of information about sentence structure in the BOW model does
not necessarily give rise to problems as long as the importance of the question terms
is reflected by their frequency counts. If term importance does not align with corpus
236
Verberne et al What Is Not in the Bag of Words forWhy-QA?
frequency, grammatical structure becomes potentially useful. Therefore, we expect that
syntactic structure can make a contribution to cases where the importance of the terms
is not reflected by their corpus frequencies but can be derived from their syntactic
function.
4.5 What Can We Expect from Structural Information?
In Sections 4.1 to 4.4 we discussed four aspects of why-questions that are problematic
for the BOW model. We expect contributions from the inclusion of information on cue
phrases, question focus and the document context of the answer, noun phrases, and
the syntactic structure of the question. We think that it is possible to achieve improved
ranking performance if features based on structural overlap are taken into account
instead of global overlap information.
5. Adding Overlap-Based Structural Information
From our analyses in Section 4, we found a number of question and answer aspects
that are potentially useful for improving the ranking performance of our system. In
this section, we present the re-ranking module of our system. We define a feature set
that is inspired by the findings from Section 4 and aims to find out which structural
features of a question?answer pair contribute the most to better answer ranking. We
aim to weigh these features in such a way that we can optimize ranking performance.
The input data for our re-ranking experiments is the output of the passage retrieval
module. A success@150 score of 78.5% for passage retrieval (see Section 3.2) means that
the maximum success@10 score that we can achieve by re-ranking is 78.5%.
5.1 Features for Re-ranking
The first feature in our re-ranking method is the score that was assigned to a candidate
answer by Lemur/TF-IDF in the retrieval module (f0). In the following sections we
introduce the other features that we implemented. Each feature represents the overlap
between two item bags:12 a bag of question items (for example: all the question?s noun
phrases, or the question?s main verb) and a bag of answer items (for example: all answer
words, or all verbs in the answer). The value that is assigned to a feature is a function of
the overlap between these two bags. We used the following overlap function:
S(Q,A) =
QA + AQ
Q+ A
(1)
in whichQA is the number of question items that occur at least once in the bag of answer
items, AQ is the number of answer items that occur at least once in the bag of question
items, and Q+ A is the number of items in both bags of items joined together.
5.1.1 The Syntactic Structure of the Question. In Section 4.4, we argued that some syntactic
parts of the question may be more important for answer ranking than others. Because
we have no quantitative evidence yet which syntactic parts of the question are the most
important, we created overlap features for each of the following question parts: phrase
12 Note that a ?bag? is a set in which duplicates are counted as distinct items.
237
Computational Linguistics Volume 36, Number 2
heads (f1), phrase modifiers (f2); the subject (f3), main verb (f4), nominal predicate (f5),
and direct object (f6) of the main clause; and all noun phrases (f11). For each of these
question parts, we calculated its word overlap with the bag of all answer words. For the
features f3?f6, we added a variant where as answer items only words/phrases with the
same syntactic function as the question token were included (f7, f8, f9, f10).
Consider for example question 1 from Section 3.1: Why didn?t Socrates leave Athens
after he was convicted?, and the reference answer as the candidate answer for which we
are determining the feature values: Socrates considered it hypocrisy to escape the prison: he
had knowingly agreed to live under the city?s laws, and this meant the possibility of being judged
guilty of crimes by a large jury.
From the parser output, our feature extraction script extracts Socrates as subject,
leave as main verb, and Athens as direct object. Neither leave nor Athens occur in the
answer passage, thus f4, f6, f8, and f10 are all given a value of 0. So are f5 and f9,
because the question has no nominal predicate. For the subject Socrates, our script finds
that it occurs once in the bag of answer words. The overlap count for the feature f3 is
thus calculated as 1+11+18 = 0.105.
13 For the feature f7, our script extracts the grammatical
subjects Socrates, he, and this from the parser?s representation of the answer passage.
Because the bag of answer subjects for f7 contains three items, the overlap is calculated
as 1+11+3 = 0.5.
5.1.2 The Semantic Structure of the Question. In Section 4.2, we saw that often there is a
link between the question focus and the title of the document in which the reference
answer is found. In those cases, the answer document and the question share the same
topic. For most questions, the focus is the syntactic subject: Why do cats sleep so much?
Judging from our data, there are two exceptions to this general rule: (1) If the subject
is semantically poor, the question focus is the (verbal or nominal) predicate: Why do
people sneeze?, and (2) in case of etymology questions (which cover about 10% of
why-questions), the focus is the subject complement of the passive sentence: Why are
chicken wings called Buffalo Wings?
We included a feature (f12) for matching words from the question focus to words
from the document title and a feature (f13) for the relation between question focuswords
and all answer words. We also include a feature (f14) for the other, non-focus question
words.
5.1.3 The Document Context of the Answer. Not only is the document title in relation to
the question focus potentially useful for answer ranking, but also other aspects of the
answer context. We include four answer context features in our feature set: overlap
between the question words and the title of the Wikipedia document (f15), overlap be-
tween question words and the heading of the answer section (f16), the relative position
of the answer passage in the document (f17), and overlap between a fixed set of words
that we selected as explanatory cues when they occur in a section heading and the set
of words that occur in the section heading of the passage (f18).14
13 The bag of question subjects contains one item (Socrates, the 1 in the denominator) and one item from
this bag occurs in the bag of answer words (the left 1 in the numerator). Without stopwords, the bag of
all answer words contains 18 items, one of which occurs in the bag of question subjects (the right 1 in
the numerator).
14 We found these section heading cues by extracting all section headings from the Wikipedia corpus,
sorting them by frequency, and then manually marking those section heading words that we expect
to occur with explanatory sections. The result is a small set of heading cues (history, origin, origins,
background, etymology, name, source, sources) that is independent of the test set we work with.
238
Verberne et al What Is Not in the Bag of Words forWhy-QA?
5.1.4 Synonyms. For each of the features f1 to f10 and f12 to f16 we add an alternative
feature (f19 to f34) covering the set of all WordNet synonyms for all question terms in
the original feature. For synonyms, we apply a variant of Equation (1) in which QA is
interpreted as the number of question items that have at least one synonym in the bag
of answer items and AQ as the number of answer items that occur in at least one of the
synonym sets of the question items.
5.1.5 WordNet Relatedness. Additionally, we included a feature representing the related-
ness between the question and the candidate answer using the WordNet Relatedness
tool (Pedersen, Patwardhan, and Michelizzi 2004) (f35). As a measure of relatedness,
we choose the Lesk measure, which incorporates information fromWordNet glosses.
5.1.6 Cue Phrases. Finally, as proposed in Section 4.1, we added a closed set of cue phrases
that are used to introduce an explanation (f36). We found these explanatory phrases in a
way that is commonly used for finding answer cues and that is independent of our own
set of question?answer pairs. We queried the key answer words to the most frequent
why-question on the Web Why is the sky blue? (blue sky rayleigh scattering) to the MSN
Search engine15 and crawled the first 250 answer fragments that are retrieved by the
engine. From these, we manually extracted all phrases that introduce the explanation.
This led to a set of 47 cue phrases such as because, as a result of, which explains why,
and so on.
5.2 Extracting Feature Values from the Data
For the majority of features we needed the syntactic structure of the input question,
and for some of the features also of the answer. We experimented with two different
syntactic parsers for these tasks: the Charniak parser (Charniak 2000) and a develop-
ment version of the Pelican parser.16 Of these, Pelican has a more detailed descriptive
model and gives better accuracy but Charniak is at present more robust for parsing
long sentences and large amounts of text. We parsed the questions with Pelican because
we need accurate parsings in order to correctly extract all constituents. We parsed all
answers (186 ? 150 passages) with Charniak because of its speed and robustness.
For feature extraction, we used the following external components: A stop word
list,17 the sets of cue phrases as described in Sections 5.1.3 and 5.1.6, the CELEX Lemma
lexicon (Burnage et al 1990), the WordNet synonym sets, the WordNet Similarity
tool (Pedersen, Patwardhan, and Michelizzi 2004), and a list of pronouns and semanti-
cally poor nouns.18 We used a Perl script for extracting feature values for each question?
answer pair. For each feature, the script composes the required bags of question items
and answer items. All words are lowercased and punctuation is removed. For terms
in the question set that consist of multiple words (for example, a multi-word subject),
spaces are replaced by underscores before stop words are removed from the question
and the answer. Then the script calculates the similarity between the two sets for each
feature following Equation (1).19
15 http://www.live.com.
16 See http://lands.let.ru.nl/projects/pelican/.
17 See Section 3.1.
18 Semantically poor nouns that we came across in our data set are the nouns humans and people.
19 A multi-word term from the question is counted as one item.
239
Computational Linguistics Volume 36, Number 2
Table 1
Results for the why-QA system: the complete system including re-ranking compared against
plain Lemur/TF-IDF for 187 why-questions.
Success@10 Success@150 MRR@150
Lemur/TF-IDF?sliding 45.2% 78.5% 0.25
TF-IDF + Re-ranking using 37 structural features 57.0% 78.5% 0.34
Whether or not to lemmatize the terms before matching them is open to debate.
In the literature, there is some discussion on the benefit of lemmatization for question
answering (Bilotti, Katz, and Lin 2004). Lemmatization can be especially problematic
in the case of proper names (which are not always recognizable by capitalization).
Therefore, we decided only to lemmatize verbs (for features f4 and f8) in the current
version of our system.
5.3 Re-ranking Method
Feature extraction led to a vector consisting of 37 feature values for each of the 27,900
items in the data set. We normalized the feature values over all 150 answer candidates
for the same question to a number between 0 and 1 using the L1 vector norm. Each
instance (representing one question?answer pair) was automatically labeled 1 if the
candidate answer matched the answer pattern for the question and 0 if it did not.
On average, a why-question had 1.6 correct answers among the set of 150 candidate
answers.
In the process of training our re-ranking module, we aim at combining the 37
features in a ranking function that is used for re-ordering the set of candidate answers.
The task of finding the optimal ranking function for ranking a set of items is referred to
as ?learning to rank? in the information retrieval literature (Liu et al 2007). In Verberne
et al (2009), we compared several machine learning techniques20 for our learning-
to-rank problem. We evaluated the results using 5-fold cross validation on the ques-
tion set.
5.4 Results from Re-ranking
The results for the complete system compared with passage retrieval with Lemur/
TF-IDF only are in Table 1.We show the results in terms of success@10, success@150, and
MRR@150. We only present the results obtained using the best-performing learning-
to-rank technique: logistic regression.21 A more detailed description of our machine
learning method and a discussion of the results obtained with other learning techniques
can be found in Verberne et al (2009).
20 Naive Bayes, Support Vector Classification, Support Vector Regression, Logistic regression, Ranking
SVM, and a genetic algorithm, all with several optimization functions.
21 We used the lrm function from the Design package in R (http://cran.r-project.org/web/packages/
Design) for training and evaluating models based on logistic regression.
240
Verberne et al What Is Not in the Bag of Words forWhy-QA?
After applying our re-ranking module, we found a significant improvement over
bare TF-IDF in terms of success@10 and MRR@150 (z = ?4.29, p < 0.0001 using the
Wilcoxon Signed-Rank test for paired reciprocal ranks).
5.5 Which Features Made the Improvement?
In order to evaluate the importance of our features, we rank them according to the
coefficient that was assigned to them in the logistic regression model (See Table 2). We
only consider features that are significant at the p = 0.05 level. We find that all eight
significant features are among the top nine features with the highest coefficient.
The feature ranking is discussed in Section 6.1.
6. Discussion
In the following sections, we discuss the feature ranking (Section 6.1), make a compari-
son to other re-ranking approaches (Section 6.2), and explain the attempts that we made
at solving the remaining problems (Section 6.3).
6.1 Discussion of the Feature Ranking
Table 2 shows that only a small subset (8) of our 37 features significantly contribute to
the re-ranking score. The highest ranked feature is TF-IDF (the bag of words), which is
not surprising since TF-IDF alone already reaches an MRR@150 of 0.25 (see Section 3.2).
In Section 4.5, we predicted a valuable contribution from the addition of cue phrases,
question focus, noun phrases, and the document context of the answer. This is partly
confirmed by Table 2, which shows that among the significant features are the feature
that links question focus to document title and the cue phrases feature. The noun
phrases feature (f11) is actually in the top nine features with the highest coefficient but
its contribution was not significant at the 0.05 level (p = 0.068).
The importance of question focus for why-QA is especially interesting because it
is a question feature that is specific to why-questions and does not similarly apply
Table 2
Features that significantly contribute to the re-ranking score (p < 0.05), ranked by their
coefficient in the logistic regression model (representing their importance).
Feature Coefficient
TF-IDF (f0) 0.39**
Overlap between question focus synonyms and document title (f30) 0.25**
Overlap between question object synonyms and answer words (f28) 0.22
Overlap between question object and answer objects (f10) 0.18*
Overlap between question words and document title synonyms (f33) 0.17
Overlap between question verb synonyms and answer words (f24) 0.16
WordNet Relatedness (f35) 0.16*
Cue phrases (f36) 0.15*
Asterisks on coefficients denote the level of significance for the feature: ** p < 0.001; * 0.001 <
p < 0.01; no asterisk means 0.01 < p < 0.05.
241
Computational Linguistics Volume 36, Number 2
to factoids or other question types. Moreover, the link from the question focus to the
document title shows that Wikipedia as an answer source can provide QA systems with
more information than a collection of plain texts with less discriminative document
titles does.
The significance of cue phrases is also an important finding. In fact, including cue
phrases in the why-QA process is the only feasible way of specifying which passages
are likely to contain an explanation (i.e., an answer to a why-question). In earlier work
(Verberne et al 2007a), we pointed out that higher-level annotation such as discourse
structure can give useful information in the why-answer selection process. However,
the development of systems that incorporate discourse structure suffers from the lack
of tools for automated annotation. The current results show that surface patterns (the
literal presence of items from a fixed set of cue words) are a step in the direction of
answer selection.
The significant features in Table 2 also show us which question constituents are
the most salient for answer ranking: focus, main verb, and direct object. We think that
features incorporating the question?s subject are not found to be significant because, in
a subset of the questions, the subject is semantically poor. Moreover, because for most
questions the subject is the question focus, the subject features and the focus features are
correlated. In our data, the question focus apparently is the more powerful predictor.
6.2 Comparison to Other Approaches
The 23% improvement that we reach in terms of MRR@150 (from 0.25 to 0.34) is com-
parable to that reached by Tiedemann in his work on improving factoid QA with the
use of structural information.
In order to see whether the improvement that we achieved with re-ranking is
on account of structural information or just the benefit of using word sequences, we
experimented with a set of re-ranking features based on sequences of question words
that are not syntactically defined. In this re-ranking experiment, we included TF-IDF,
word bigrams, and word trigrams as features. The resulting performance was around
baseline level (MRR = 0.25), significantly worse than re-ranking with structural overlap
features. This is still true if we add the cue word feature (which, in isolation, only gives
a small improvement to baseline performance) to the n-gram features.
6.3 Solving the Remaining Problems
Although the results in terms of success@10 and MRR@150 are satisfactory, there is still
a substantial proportion of why-questions that is not answered in the top 10 result list.
In this section, we discuss a number of attempts that we made to further improve our
system.
First, after we found that for some question parts synonym expansion leads to
improvement (especially the main verb and direct object), we experimented with the
addition of synonyms for these constituents in the retrieval step of our system (Lemur).
We found, however, that it does not improve the results due to the large synonym sets
of many verbs and nouns which add much noise and lead to very long queries. The
same holds for the addition of cue words in the retrieval step.
Second, although our re-ranking module incorporates expansion to synonym sets,
there are many question?answer pairs where the vocabulary gap between the question
242
Verberne et al What Is Not in the Bag of Words forWhy-QA?
and the answer is still a problem. There are cases where semantically related terms in
the question and the answer are of different word classes (e.g., hibernate?hibernation),
and there are cases of proper nouns that are not covered by WordNet (e.g., B.B. King).
We considered using dynamic stemming for verb?noun relations such as the hibernation
case but research has shown that stemming hurts as many queries as it helps (Bilotti,
Katz, and Lin 2004). Therefore, we experimented with a number of different semantic
resources, namely, the nominalization dictionary Nomlex (Meyers et al 1998) and the
wikiOntology by Ponzetto and Strube (2007). However, in their current state of develop-
ment these semantic resources cannot improve our system because their coverage is too
low to make a contribution to our re-ranking module. Moreover, the present version of
the wikiOntology is very noisy and requires a large amount of cleaning up and filtering.
Third, we considered that the use of cue phrases may not be sophisticated enough
for finding explanatory relations between question and answer. Therefore, we exper-
imented with the addition of cause?effect pairs from the English version of the EDR
Concept Dictionary (Yokoi 1995) ? as suggested by Higashinaka and Isozaki (2008).
Unfortunately, the list appeared to be extremely noisy, proving it not useful as a source
for answer ranking.
7. Conclusions and Directions for Future Research
In the current research, we extended a passage retrieval system for why-QA using off-
the-shelf retrieval technology (Lemur/TF-IDF) with a re-ranking step incorporating
structural information. We get significantly higher scores in terms of MRR@150 (from
0.25 to 0.34) and success@10. The 23% improvement that we reach in terms of MRR
is comparable to that reached on various other QA tasks by other researchers in the
field (see Section 6.3). This confirms our hypothesis in Section 1 that for the relatively
complex problem of why-QA, a significant improvement can be gained by the addition
of structural information to the ranking component of the QA system.
Most of the features that we implemented for answer re-ranking are based on word
overlap between part of the question and part of the answer. As a result of this set-up,
our features identify the parts of why-questions and their candidate answers that are the
most powerful/effective for ranking the answers. The question constituents that appear
to be the most important are the question focus, the main verb, and the direct object. On
the answer side, most important are the title of the document in which the candidate
answer is embedded and knowledge on the presence of cue phrases.
Because our features are overlap-based, they are relatively easy to implement. For
implementation of some of the significant features, a form of syntactic parsing is needed
that can identify subject, verb, and direct object from the question and sentences in
the candidate answers. An additional set of rules is needed for finding the question
focus. Finally, we need a fixed list for identifying cue phrases. Exploiting the title of
answer documents in the feature set is only feasible if the documents that may contain
the answers have titles and section headings similar to Wikipedia.
In conclusion, we developed a method for significantly improving a BOW-based
approach to why-QA that can be implemented without extensive semantic knowledge
sources. Our series of experiments suggest that we have reached the maximum per-
formance that can be obtained using a knowledge-poor approach. Experiments with
more complex types of information (discourse structure, cause?effect relations) show
that these information sources have not as yet developed sufficiently to be exploited in
a QA system.
243
Computational Linguistics Volume 36, Number 2
References
Bilotti, M. W., B. Katz, and J. Lin. 2004.
What works better for question
answering: Stemming or morphological
query expansion. In Proceedings of the
Workshop on Information Retrieval for
Question Answering (IR4QA) at SIGIR 2004,
Sheffield.
Bilotti, M. W., P. Ogilvie, J. Callan, and
E. Nyberg. 2007. Structured retrieval for
question answering. In Proceedings of the
30th Annual International ACM SIGIR
Conference on Research and Development
in Information Retrieval, pages 351?358,
Amsterdam.
Burnage, G., R. H. Baayen, R. Piepenbrock,
and H. van Rijn. 1990. CELEX: A Guide for
Users. CELEX, University of Nijmegen,
the Netherlands.
Carlson, Lynn, Daniel Marcu, and
Mary Ellen Okurowski. 2003. Building
a discourse-tagged corpus in the
framework of Rhetorical Structure Theory.
In Jan van Kuppevelt and Ronnie Smith,
editors, Current Directions in Discourse and
Dialogue. Kluwer Academic Publishers,
Dordrecht, pages 85?112.
Charniak, E. 2000. A maximum-entropy-
inspired parser. ACM International
Conference Proceeding Series, 4:132?139.
Denoyer, L. and P. Gallinari. 2006. The
Wikipedia XML corpus. ACM SIGIR
Forum, 40(1):64?69.
Ferret, O., B. Grau, M. Hurault-Plantet,
G. Illouz, L. Monceaux, I. Robba, and
A. Vilnat. 2002. Finding an answer
based on the recognition of the question
focus. NIST Special Publication,
pages 362?370.
Higashinaka, R. and H. Isozaki. 2008.
Corpus-based question answering for
why-questions. In Proceedings of IJCNLP,
pages 418?425, Hyderabad.
Hovy, E. H., U. Hermjakob, and
D. Ravichandran. 2002. A question/
answer typology with surface text
patterns. In Proceedings of the Human
Language Technology conference (HLT),
pages 247?251, San Diego, CA.
Joachims, T., L. Granka, B. Pan,
H. Hembrooke, and G. Gay. 2005.
Accurately interpreting clickthrough data
as implicit feedback. In Proceedings of the
28th Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval, pages 154?161,
Salvador, Brazil.
Khalid, M. and S. Verberne. 2008. Passage
retrieval for question answering using
Sliding Windows. In Proceedings of the
COLING 2008 Workshop IR4QA,
Manchester, UK.
Liu, T. Y., J. Xu, T. Qin, W. Xiong, and H. Li.
2007. Letor: Benchmark dataset for
research on learning to rank for
information retrieval. In Proceedings of
the Workshop on Learning to Rank for
Information Retrieval (LR4IR) at SIGIR 2007,
pages 3?10, Amsterdam.
Meyers, A., C. Macleod, R. Yangarber,
R. Grishman, L. Barrett, and R. Reeves.
1998. Using NOMLEX to produce
nominalization patterns for information
extraction. In Proceedings: The
Computational Treatment of Nominals,
volume 2, pages 25?32, Montreal.
Pedersen, T., S. Patwardhan, and
J. Michelizzi. 2004. WordNet::Similarity ?
measuring the relatedness of concepts. In
Proceedings of the National Conference on
Artificial Intelligence, pages 1024?1025,
San Jose, CA.
Ponzetto, S. P. and M. Strube. 2007. Deriving
a large scale taxonomy fromWikipedia.
In Proceedings of the National Conference on
Artificial Intelligence, pages 1440?1445,
Vancouver, BC.
Quarteroni, S., A. Moschitti, S. Manandhar,
and R. Basili. 2007. Advanced structural
representations for question classification
and answer re-ranking. In Proceedings of
ECIR 2007, pages 234?245, Rome.
Salton, G. and C. Buckley. 1988.
Term-weighting approaches in automatic
text retrieval. Information Processing and
Management, 24(5):513?523.
Surdeanu, M., M. Ciaramita, and
H. Zaragoza. 2008. Learning to rank
answers on large online QA collections. In
Proceedings of ACL 2008, pages 719?727,
Columbus, OH.
Tellex, S., B. Katz, J. Lin, A. Fernandes, and
G. Marton. 2003. Quantitative evaluation
of passage retrieval algorithms for
question answering. In Proceedings of the
26th Annual International ACM SIGIR
Conference on Research and Development
in Information Retrieval, pages 41?47,
Toronto.
Tiedemann, J. 2005. Improving passage
retrieval in question answering using
NLP. In Progress in Artificial Intelligence,
volume 3808. Springer, Berlin /
Heidelberg, pages 634?646.
Verberne, S. 2006. Developing an approach
for why-question answering. In Conference
Companion of the 11th Conference of the
European Chapter of the Association for
244
Verberne et al What Is Not in the Bag of Words forWhy-QA?
Computational Linguistics (EACL 2006),
pages 39?46, Trento.
Verberne, S., L. Boves, N. Oostdijk, and
P. A. Coppen. 2007a. Discourse-based
answering of why-questions. Traitement
Automatique des Langues (TAL), special issue
on ?Discours et document: traitements
automatiques?, 47(2):21?41.
Verberne, S., L. Boves, N. Oostdijk, and
P. A. Coppen. 2007b. Evaluating
discourse-based answer extraction for
why-question answering. In Proceedings of
the 30th Annual International ACM SIGIR
Conference on Research and Development
in Information Retrieval, pages 735?736,
Amsterdam.
Verberne, S., H. Van Halteren, D. Theijssen,
S. Raaijmakers, and L. Boves. 2009.
Learning to rank QA data. In Proceedings
of the Workshop on Learning to Rank for
Information Retrieval (LR4IR) at SIGIR 2009,
pages 41?48, Boston, MA.
Yokoi, T. 1995. The EDR electronic dictionary.
Communications of the ACM, 38(11):42?44.
Zhai, C. 2001. Notes on the Lemur TFIDF
model. Technical report, School of
Computer Science, Carnegie Mellon
University.
245

Text Representations for Patent Classification
Eva D?hondt?
Radboud University Nijmegen
Suzan Verberne??
Radboud University Nijmegen
Cornelis Koster?
Radboud University Nijmegen
Lou Boves?
Radboud University Nijmegen
With the increasing rate of patent application filings, automated patent classification is of rising
economic importance. This article investigates how patent classification can be improved by
using different representations of the patent documents. Using the Linguistic Classification
System (LCS), we compare the impact of adding statistical phrases (in the form of bigrams)
and linguistic phrases (in two different dependency formats) to the standard bag-of-words text
representation on a subset of 532,264 English abstracts from the CLEF-IP 2010 corpus. In
contrast to previous findings on classification with phrases in the Reuters-21578 data set, for
patent classification the addition of phrases results in significant improvements over the unigram
baseline. The best results were achieved by combining all four representations, and the second
best by combining unigrams and lemmatized bigrams. This article includes extensive analyses of
the class models (a.k.a. class profiles) created by the classifiers in the LCS framework, to examine
which types of phrases are most informative for patent classification. It appears that bigrams
contribute most to improvements in classification accuracy. Similar experiments were performed
on subsets of French and German abstracts to investigate the generalizability of these findings.
1. Introduction
Around the world, the patent filing rates in the national patent offices have been in-
creasing year after year, creating an enormous volume of texts, which patent examiners
? Center for Language Studies, PO Box 9103, 6500 HD Nijmegen, the Netherlands.
E-mail: e.dhondt@let.ru.nl.
?? Center for Language Studies / Institute for Computing and Information Sciences, PO Box 9103, 6500 HD
Nijmegen, the Netherlands. E-mail: s.verberne@let.ru.nl.
? Institute for Computing and Information Sciences, PO Box 9010, 6500 HD Nijmegen, the Netherlands.
E-mail: kees@cs.ru.nl.
? Center for Language Studies, PO Box 9103, 6500 HD Nijmegen, the Netherlands.
E-mail: l.boves@let.ru.nl.
Submission received: 19 March 2012; revised submission received: 8 August 2012; accepted for publication:
19 September 2012.
doi:10.1162/COLI a 00149
? 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 3
are struggling to manage (Benzineb and Guyot 2011). To speed up the examination
process, a patent application needs to be directed to patent examiners specialized
in the subfield(s) of that particular patent as quickly as possible (Smith 2002). This
preclassification is done automatically in most patent offices, but substantial addi-
tional manual labor is still necessary. Furthermore, since 2010, the International Patent
Classification1 (IPC) is revised every year to keep track of recent developments in the
various subdomains. Such a revision is followed by a reclassification of portions of
the existing patent corpus, which is currently done mainly by hand by the national
patent offices (Held, Schellner, and Ota 2011). Both preclassification and reclassification
could be improved, and a higher consistency of the classifications of the documents
in the patent corpus could be obtained, if more reliable and precise automatic text
classification algorithms were available (Benzineb and Guyot 2011).
Most approaches to text classification use the bag-of-words (BOW) text representa-
tion, which represents each document by the words that occur in it, irrespective of their
ordering in the original document. In the last decades much research has gone into
expanding this representation with additional information, such as statistical phrases2
(n-grams) or some forms of syntactic or semantic knowledge. Even though (statistical)
phrases are more representative units for classes than single words (Caropreso, Matwin,
and Sebastiani 2001), they are so sparsely distributed that they have limited impact
during the classification process. Therefore, it is not surprising that the best scoring
multi-class, multi-label3 classification results for the well-known Reuters-21578 data set
have been obtained using a BOW representation (Bekkerman and Allan 2003). But the
limited contribution of phrases in addition to the BOW-representation does not seem
to hold for all classification tasks: O?zgu?r and Gu?ngo?r (2010) found significant differ-
ences in the impact of linguistic phrases between short newswire texts (Reuters-21578),
scientific abstracts (NSF), and informal posts in usenet groups (MiniNg): Especially the
classification of scientific abstracts could be improved by using phrases as index terms.
In a follow-up study, O?zgu?r and Gu?ngo?r (2012) found that for the three different data
sets, different types of linguistic phrases have most impact. The authors conclude that
more formal text types benefit from more complex syntactic dependencies.
In this article, we investigate if similar improvements can be found for patent
classification and, more specifically, which types of phrases are most effective for this
particular task. In this article we investigate the value of phrases for classification by
comparing the improvements that can be gained from extending the BOW representa-
tion with (1) statistical phrases (in the form of bigrams); (2) linguistic phrases originating
from the Stanford parser (see Section 3.2.2); (3) aboutness-based4 linguistic phrases from
the AEGIR parser (Section 3.2.3); and (4) a combination of all of these. Furthermore, we
will investigate the importance of different syntactic relations for the classification task,
1 The IPC is a complex hierarchical classification system comprising sections, classes, subclasses, and
groups. For example, the ?A42B 1/12? class label, which groups designs for bathing caps, falls under
section A ?Human necessities,? class 42 ?Headwear,? subclass B ?Head coverings,? group 1 ?Hats; caps;
hoods.? The latest edition of the IPC contains eight sections, 129 classes, 639 subclasses, 7,352 groups,
and 61,847 subgroups. The IPC covers inventions in all technological fields in which inventions can
be patented.
2 By a phrase we mean an index unit consisting of two or more words, generated through either syntactic
or statistical methods.
3 Multi-class classification is the problem of classifying instances into more than two classes. Multi-label
signifies that documents in this test set are associated with more than one class, and must be assigned a
set of labels during classification.
4 The notion of aboutness refers to the conceptual content expressed by a dependency triple. For a more
detailed description, see Section 3.2.3.
756
D?hondt et al Text Representations for Patent Classification
and the extent to which the words in the phrases overlap with the unigrams. We also
investigate which syntactic relations capture most information in the opinion of human
annotators. Finally, we perform experiments to investigate if our findings are language-
dependent. We will then draw some conclusions on what information is most valuable
for improving automatic patent classification.
2. Background
2.1 Text Representations in Classification
Lewis (1992) was the first to investigate the use of phrases as index terms for text
classification. He found that phrases generally suffer from data sparseness and may
actually cause classification performance to deteriorate. These findings were confirmed
by Apte?, Damerau, and Weiss (1994). With the advent of increasing computational
power and bigger data sets, however, the topic has been revisited in the last two decades
(Bekkerman and Allan 2003).
In this section we will give an overview of the major findings in previous re-
search on the use of statistical and syntactic phrases for text classification. Except when
mentioned explicitly, all the classification experiments reported here were conducted
using the Reuters-21578 data set, a well-known benchmark of 21,578 short newswire
texts for multi-class classification into 118 categories (a document has an average of
1.24 class labels).
2.1.1 Combining Unigrams with Statistical Phrases. For an excellent overview of the work
on using phrases done up to 2002, see Bekkerman and Allan (2003), and Tan, Wang, and
Lee (2002).
Because they contain more specific information, one might think that phrases are
more powerful features for text classification. There are two ways of using phrases as
index terms: either index terms only or in combination with unigrams. All experimental
results, however, show that using only phrases as index terms leads to a decrease
in classification accuracy compared with the BOW baseline (Bekkerman and Allan
2003). Both Mladenic and Grobelnik (1998) and Fu?rnkranz (1998) showed that classifiers
trained on combinations of unigrams and n-grams composed of at most three words
performed better than classifiers that only use unigrams; no improvement was obtained
when using larger n-grams. Because trigrams are sparser than bigrams, most of the
subsequent research has focused on optimizing the combination of unigrams and
bigrams using different feature selection techniques.
2.1.2 Feature Selection. Obviously, unigrams and bigrams overlap: Bigrams are pairs of
unigrams. Caropreso, Matwin, and Sebastiani (2001) evaluated the relative importance
of unigrams and bigrams in a classifier-independent study: Instead of determining the
impact of features on the classification scores, they scored all unigrams and bigrams
using conventional feature evaluation functions to find the features that are most
representative for the document classes. For the Reuters-21578 data set, they found
that many bigram features scored higher than unigram features. These (theoretical)
findings were not confirmed in subsequent classification experiments, however. When
the bigram/unigram ratio for a fixed number of features is changed to favor bigrams,
classification performance tends to go down. It appears that the information in the
bigrams does not turn the unigrams redundant.
757
Computational Linguistics Volume 39, Number 3
Braga, Monard, and Matsubara (2009) used a Multinomial Naive Bayes classifier
to investigate classification performance with unigrams and bigrams by comparing
multiview classification (the results of two independent classifiers trained with unigram
and bigram features are merged) with monoview classification (unigrams and bigrams
are combined in a single feature set).5 They found that there is little difference between
the output of the mono- and multiview classifiers. In the multiview classifiers, the
unigram and bigram classifiers make similar decisions in assigning labels, although
the latter generally yielded lower confidence values. Consequently, in the merge the
unigram and bigram classifiers affirm each other?s decisions, which does not result
in an overall improvement in classification accuracy. The authors suggest combining
unigrams only with those bigrams for which it holds that the whole provides more
information than the sum of the parts.
Tan, Wang, and Lee (2002) proposed selecting highly representative and meaningful
bigrams based on the Mutual Information scores of the words in a bigram compared
with the unigram class model. They selected only the top 2% of the bigrams as index
terms, and found a significant improvement over their unigram baseline, which was low
compared to state-of-the-art results. Bekkerman and Allan (2003) failed to improve over
their unigram baseline when using similar selection criteria based on the distributional
clustering of unigram models. Crawford, Koprinska, and Patrick (2004) were not able to
improve e-mail classification when using the selection criteria proposed by Tan, Wang,
and Lee.
2.1.3 Combining Unigrams with Syntactic Phrases. Lewis (1992) and Apte?, Damerau, and
Weiss (1994) were the first to investigate the impact of syntactic phrases6 as features for
text classification. Dumais et al (1998) and Scott and Matwin (1999) did not observe
a significant improvement in classification on the Reuters-21578 collection when noun
phrases obtained with a shallow parser were used instead of unigrams. Moschitti and
Basili (2004) found that neither words augmented with word sense information, nor
syntactic phrases (acquired through shallow parsing) in combination with unigrams
improved over the BOW baseline. Syntactic phrases appear to be even sparser than
bigrams. Therefore, it is not surprising that most papers concluded that classifiers using
only syntactic phrases perform worse than the baseline, except when the BOW baseline
is low for that particular classification task (Mitra et al 1997; Fu?rnkranz 1999).
Deep syntactic parsing is a computationally expensive process, but thanks to the
increase in computational power it is now possible to use phrases acquired through
deep syntactic parsing in classification tasks. Nastase, Sayyad, and Caropreso (2007)
used Minipar to generate dependency triples that are combined with lemmatized and
unlemmatized unigrams to classify the 10 most frequent classes in the Reuters-21578
data set. Their criterion for selecting triples as index terms is document frequency ? 2.
The small improvement over the lemmatized unigram baseline was not statistically
significant. O?zgu?r and Gu?ngo?r (2010, 2012) achieve small but significant improvements
when combining unigrams with a subset of the dependency types from the Stanford
parser on three different data sets, including the Reuters-21578 set. They find that
separate pruning levels (based on the term frequency?inverse document frequency
[TF-IDF] score of the index units) for the unigrams and syntactic phrases influence
5 The difference between multiview and monoview classification corresponds to what is called late and
early fusion in the pattern recognition literature.
6 The concept ?syntactic phrase? can be given several different interpretations, such as noun phrases,
verb phrases, predicate structures, dependency triples, and so forth.
758
D?hondt et al Text Representations for Patent Classification
classification accuracy. Which dependency relations prove most relevant for a classifi-
cation task depends greatly on the language use in the different data sets: The informal
MiniNG data set (usenet posts) benefits a little from ?simple? dependencies such as part,
denoting a phrasal verb, for example write down, while classification in the more formal
Reuters-21578 (newswire) and NSF (scientific abstracts) data sets is more improved by
using dependencies on phrase and clause level (adjectival modifier, compound noun,
prepositional attachment; and subject and object, respectively). The highest-ranking
features for the NSF data set are compound noun (nn), adjectival modifier (amod),
subject (subj), and object (obj), respectively. Furthermore, they observe that splitting up
more generic relator types (such as prep) into different, more specific, subtypes increases
the classification accuracy.
2.2 Patent Classification
It is not possible to draw far-reaching conclusions from previous research on patent
classification, because there is no tradition of using a ?standard? data set, and a standard
split of patent corpora in a training and test set. Furthermore, there are differences
between the various experiments in task definitions (mono-label versus multi-label
classification); the granularity of the classification (depth in the IPC hierarchy); and the
choices of (sub)sets of data. Fall and Benzineb (2002) give an overview of the work done
in patent classification research up to 2002 and of the commercial patent classification
systems available; see Benzineb and Guyot (2011) for a general introduction to patent
classification.
Larkey (1999) was the first to present a fully automated patent classification system,
but she did not report her overall accuracy results. Larkey (1998) used a combination
of weighted words and noun phrases as index terms to classify a subset of the USPTO
database, but found no improvement over a BOW baseline. The weights were calculated
as follows: Frequency of a word or phrase in a particular section times the manually
assigned weight (importance) given to that section. The weights for each word or phrase
were then summed across sections. Term selection was based on a threshold for these
weights.
Krier and Zacca` (2002) organized a comparative study of various academic and
commercial systems for patent classification for a common data set. In this informal
benchmark Koster, Seutter, and Beney (2001) achieved the best results, using the Bal-
anced Winnow algorithm with a word-only text representation. Classification is per-
formed for 44 or 549 categories (which correspond to different levels of depth in the
then used version of the IPC), with around 78% and 68% precision at 100% recall,
respectively.
Fall et al (2003) introduced the EPO-alpha data set, attempting to create a common
benchmark for patent classification. Using only words as index terms, they tested
different classification algorithms and found that SVM outperform Naive Bayes, k-NN,
SNoW, and decision-based classifiers. They achieved P@3-scores7 of 73% and 59% on
114 classes and 451 subclasses, respectively. They also found that when using only
the first 300 words from the abstract, claims, and description sections, classification
accuracy is increased compared with using the complete sections. The same data set
was later used by Koster and Seutter (2003), who experimented with a combined
7 Precision at rank 3 (P@3) signifies the percentage correct labels in the first three labels by the classifier to a
given document.
759
Computational Linguistics Volume 39, Number 3
representation of words and phrases consisting of head-modifier pairs.8 They found
that head-modifier pairs could not improve on the BOW-baseline: The phrases were too
sparse to have much impact on the classification process.
Starting in 2009, the IRF9 has organized CLEF-IP patent classification tracks in an
attempt to bridge the gap between academic research and the patent industry. For this
purpose the IRF has put a lot of effort into providing very large patent data sets,10 which
have enabled academic researchers to train their algorithms on real-life data. In the
CLEF-IP 2010 classification track the best results were achieved by Guyot, Benzineb,
and Falquet (2010). Using the Balanced Winnow algorithm, they achieved a P@1-score
of 83%, while classifying on subclass level. They used a combination of words and
statistical phrases (collocations of variable length extracted from the corpus) as index
terms and used all available documents (in English, French, and German) in the corpus
as training data. In the same competition, Derieux et al (2010) came second (in terms of
P@1). They also used a mixed document representation of both single words and longer
phrases, which had been extracted from the corpus by counting word co-occurrences.
Verberne, Vogel, and D?hondt (2010) and Beney (2010) experimented with a combined
representation of words and syntactic phrases derived from an English and French
syntactic parser, respectively. They both found that adding syntactic phrases to words
improves classification accuracy slightly. Beney (2010) remarks that this improvement
may be language-dependent. As a follow-up, Koster et al (2011) investigated the added
value of syntactic phrases. They found that attributive phrases, that is, combinations
of adjective or nouns with nouns, were by far the most important syntactic phrases for
patent classification. On a subset of the CLEF-IP 2010 corpus11 they also found a small,
but significant, improvement when adding dependency triples to words.
3. Experimental Set-up
In this article, we investigate the relative contributions of different types of terms to
the performance of patent classification. We use four different types of terms, namely,
lemmatized unigrams, lemmatized bigrams (see Section 3.2.1), lemmatized dependency
triples obtained with the Stanford parser (see Section 3.2.2), and lemmatized depen-
dency triples obtained with the AEGIR parser (see Section 3.2.3). We will leave term
(feature) selection to the preprocessing module of the Linguistic Classification System
(LCS) which we used for all experiments (see Section 3.3). We will analyze the rela-
tion between unigrams and phrases in the class profiles in some detail, however (see
Sections 4.2 and 4.3).
3.1 Data Selection
We conducted classification experiments on a collection of patent documents obtained
from the CLEF-IP 2010 corpus,12 which is a subset of the larger MAREC patent col-
lection. The corpus contains 2.6 million patent documents, which roughly correspond
8 Head-modifier pairs were derived from the syntactic analysis output of the EP4IR syntactic parser.
9 Information Retrieval Facility, see www.irf.com.
10 The CLEF-IP 2009, CLEF-IP 2010, and CLEF-IP 2011 data sets can be obtained through the IRF. The more
recent data sets subsume the older sets.
11 The same data set as will be used in this article. For a more detailed description, see Section 3.1.
12 This test collection is available through the IRF (http://www.ir-facility.org/collection).
760
D?hondt et al Text Representations for Patent Classification
to 1.3 million individual patents, published between 1985 and 2001.13 The documents
in the collection are encoded in a customized XML format and may include text in
English, French, and German. In addition to the standard sections of a patent document
(title, abstract, claims, and description section), the documents also include meta-
information on inventor, date of application, assignee, and so forth. Because our focus
lies on text representation, we did not include any of the meta-data in our document
representations.
The most informative sections of a patent document are generally considered to
be the title, the abstract, and the beginning of the description (Benzineb and Guyot
2011). Verberne and D?hondt (2011) showed that using both the description and the
abstract gives a small, but significant, improvement in classification results on the
CLEF-IP 2011 corpus, compared with classification on abstracts only. The effort in-
volved in parsing the descriptions is considerable, however: Because of the long
sentences and the dense word use, a parser will have much more difficulty in pro-
cessing text from the description section than from the abstracts. The titles of the
patent documents also pose a parsing problem: These are generally short noun phrases
that contain ambiguous PP-attachments that are impossible to disambiguate without
any domain knowledge. This leads to incorrect syntactic analyses and, consequently,
noisy dependency triple features. Because we are interested in comparing classification
results for different text representations, and not in comparing results for different
sections, we opted to use only the abstract sections of the patent document in the
current article.
From the corpus, we extracted all files that contain both an abstract in English and at
least one IPC class14 in the <classification-ipcr> field. We extracted the IPC classes on the
document level; this means that we did not include the documents where the IPC class
is in a separate file than the English abstract. In total, there were 121 different classes in
our data set. Most documents have been assigned one to three different IPC classes (on
class level). On average, a patent abstract in our data set has 2.12 class labels. Previous
cross-validation experiments on the same document set showed very little variation
(standard deviation < 0.3%) between the classification accuracies in different training-
test splits (Verberne, Vogel, and D?hondt 2010). We therefore decided to use only one
training and test set split.15
The final data set contained 532,264 abstracts, divided into two sets: (1) a training
set (425,811 documents) and (2) a test set (106,453 documents). The distribution of the
data over the classes is in accordance with the Pareto Principle: 20% of the classes cover
80% of the data, and 80% of the classes comprise only 20% of the data.
3.2 Data Preprocessing
Preprocessing included cleaning up character conversion errors like Expression (1)
and removing claims and images references (Expression (2)) and list references
13 Note the difference between a patent and a patent document: A patent is not a physical document itself,
but a name for a group of patent documents that have the same patent ID number.
14 For our classification experiments we use the codes on the class level in the IPC8 classification.
15 The data split was performed using a perl script that randomly shuffles the documents and puts them
into a train set and test set, while ensuring that the class distribution of the examples in the train set
approximates that of the whole corpus. It can be downloaded as part of the LCS distribution.
761
Computational Linguistics Volume 39, Number 3
(Expression (3)) from the original texts. This was done automatically, using the follow-
ing regular expressions (based on Parapatics and Dittenbach 2009):
s/;gt&/>/g (1)
s/(\([ ]*[0-9][0-9a-z,.; ]*\))//g (2)
s/(\([ ]*[A-Za-z]\))//g (3)
We then used a perl script to divide the running text into sentences, by splitting on
end-of-sentence punctuation such as question marks and full stops. In order to mini-
mize incorrect splitting, the perl script was supplied with a list of common English
abbreviations and a list containing abbreviations and acronyms that occur frequently in
technical texts, derived from the Specialist lexicon.16
3.2.1 Unigrams and Bigrams. The sentences in the abstract documents were converted
to single words by splitting on whitespaces and removing punctuation. The words
were then lemmatized using the AEGIR lexicon. Bigrams were created through a
similar procedure. We did not create bigrams that spanned sentence boundaries. This
resulted in approximately 60 million unigram and bigram tokens for the present
corpus.
3.2.2 Stanford. The Stanford parser is a broad-coverage natural language parser that is
trained on newswire text, for which it achieves state-of-the-art performance. The parser
has not been optimized/retrained for the patent domain.17 In spite of the technical
difficulties (Parapatics and Dittenbach 2009) and loss of linguistic accuracy for patent
texts reported in Mille and Wanner (2008), most patent processing systems that use
linguistic phrases use the Stanford parser because its dependency scheme has a number
of properties that are valuable for Text Mining purposes (de Marneffe and Manning
2008). The Stanford parser collapsed typed dependency model has a set of 55 differ-
ent syntactic relators to capture semantically contentful relations between words. For
example, the sentence The system will consist of four separate modules is analyzed into the
following set of dependency triples in the Stanford representation:
det(system-2, The-1)
nsubj(consist-4, system-2)
aux(consist-4, will-3)
root(ROOT-0, consist-4)
num(modules-8, four-6)
amod(modules-8, separate-7)
prep_of(consist-4, modules-8)
The Stanford parser was compiled with a maximum memory heap of 1.2 GB.
Sentences longer than 100 words were automatically skipped. Combined with failed
parses this led to a 1.2% loss of parser output on the complete data set. Parsing the
16 The lexicon can be downloaded at http://lexsrv3.nlm.nih.gov/Specialist/Summary/lexicon.html.
17 For retraining a parser, a substantial amount of annotated data (in the form of syntactically annotated
dependency trees) is needed. Creating such annotations is a very expensive task and beyond the scope of
this article.
762
D?hondt et al Text Representations for Patent Classification
Table 1
Impact of lemmatization on the different text types in the training set (80% of the corpus).
# tokens # types (terms) token/type (lem.)
raw lemmatized gain
unigram 48,898,738 160,424 142,396 1.12 343.39
bigram 48,473,756 3,836,212 3,119,422 1.23 15.54
Stanford 35,772,003 8,750,839 7,430,397 1.18 4.81
AEGIR 31,004,525 ? 5,096,918 ? 6.08
entire set of abstracts took 1.5 weeks on a computer cluster consisting of 60 2.4GHz
cores with 4 GB RAM per core. The resulting dependency triples were stripped of the
word indexes and then lemmatized using the AEGIR lexicon.
3.2.3 AEGIR. AEGIR18 is a dependency parser that was designed specifically for ro-
bust parsing of technical texts. It combines a hand-crafted grammar with an exten-
sive word-form lexicon. The parser lexicon was compiled from different technical
terminologies, such as the SPECIALIST lexicon and the UMLS.19 The AEGIR parser
aims to capture the aboutness of sentences. Rather than outputting extensive linguis-
tic detail on the syntactic structure of the input sentence as in the Stanford parser,
AEGIR returns only the bare syntactic?semantic structure of the sentence. During the
parsing process, it effectively performs normalization at various levels, such as ty-
pography (for example, upper and lower case, spacing), spelling (for example, British
and American English, hyphenation), morphology (lemmatization of word forms), and
syntax (standardization of the word order and transforming passive structures into
active ones).
The AEGIR parser uses only eight syntactic relators and returns fewer unique
triples than the Stanford parser. The parser is currently still under development; for this
article we used the version AEGIR v.1.7.5. The parser was constrained to a time limit of
maximum three seconds per sentence. This caused a loss of 0.7% of parser output on the
complete data set. Parsing the entire set of abstracts took slightly less than a week on
the computer cluster described above. The AEGIR parser has several output formats,
among which its own dependency format. The example sentence used to illustrate the
output of the Stanford parser is analyzed as follows:
[system,SUBJ,consist]
[consist,PREPof,module]
[module,ATTR,separate]
[module,QUANT,four]
3.2.4 Lemmatization. Table 1 shows the impact of lemmatization (using the AEGIR lex-
icon) on the distribution of terms for the different text representations. Lemmatization
18 AEGIR stands for Accurate English Grammar for Information Retrieval. Using the AGFL compiler (found at
http://www.agfl.cs.ru.nl/) this grammar can be compiled into an operational parser. The grammar is
not freely distributed.
19 The Unified Medical Language System contains a widely-used terminology of the biomedical domain
and can be downloaded at http://www.nlm.nih.gov/research/umls/.
763
Computational Linguistics Volume 39, Number 3
and stemming are standard approaches to decreasing the sparsity of features; stemming
is more aggressive than lemmatization. Ozgu?r and Gu?ngo?r (2009) showed that?when
using only words as index terms?stemming (with the Porter Stemmer) appears to
improve performance; stemming dependency triples did not improve performance,
however.
We opted to use a less aggressive form of generalization: Lemmatizing the word
forms. We found that the bigrams gain20 most by lemmatizing the word forms, resulting
in a higher token/type ratio. From Table 1 it can be seen that there are fewer triple
tokens than bigram tokens: Whereas all the (high-frequency) function words are kept in
the bigram representations, both dependency formats discard some function words in
their parser output. For example, the AEGIR parser does not create triples for auxiliary
verbs, and in both dependency formats, the prepositions become part of the relator.
Consequently, the parsers will output fewer but more variable tokens, which results in
lower token/type ratios and a lower impact of lemmatization.
3.3 Classification Experiments
The classification experiments were carried out within the framework of the LCS
(Koster, Seutter, and Beney 2003). The LCS has been developed for the purpose of
comparing different text representations. Currently, three classifier algorithms are avail-
able: Naive Bayes, Balanced Winnow (Dagan, Karov, and Roth 1997), and SVM-light
(Joachims 1999). Verberne, Vogel, and D?hondt (2010) found that Balanced Winnow and
SVM-light yield comparable classification accuracy scores for patent texts on a similar
data set, but that Balanced Winnow is much faster than SVM-light for classification
problems with a large number of classes. The Naive Bayes classifier yielded a lower
accuracy. We therefore only used the Balanced Winnow algorithm for our classification
experiments, which were run with the following LCS configuration, based on tuning
experiments on the same data by Koster et al (2011):
 Global term selection (GTS): Document frequency minimum is 2, term frequency
minimum is 3. Although initial term selection is necessary when dealing with such
a large corpus, we deliberately aimed at keeping as many of the sparse phrasal
terms as possible.
 Local term selection (LTS): Simple Chi Square (Galavotti, Sebastiani, and Simi
2000). We used the LCS option to automatically select the most representative
terms for every class, with a hard maximum of 10,000 terms per class.21
 After LTS the selected terms of all classes are aggregated into one combined term
vocabulary, which is used as the starting point for training the individual classes
(see Table 3).
20 By ?gain? we mean the decrease in number of types for the lemmatized forms compared to the
non-lemmatized forms, which will result in higher corresponding token/type ratios.
21 Increasing the cut-off to 100,000 terms resulted in a small increase in accuracy (F1 values) for the
combined representations, mostly for the larger classes. Because the patent domain has a large lexical
variety, a large amount of low-frequency terms in the tail of the term distribution can have a large
impact on the accuracy scores. Because we are more interested in the relative gains between different
text representations and the corresponding top terms in the class profiles than in achieving maximum
classification scores, we opted to use only 10,000 terms for efficiency reasons.
764
D?hondt et al Text Representations for Patent Classification
Table 2
Impact of global term selection (GTS) criteria on the different text types in the training set (80%
of the corpus).
total # of terms # of terms selected in GTS % of terms removed in GTS
unigram 142,396 58,42322 58.97
bigram 3,119,422 1,115,170 64.25
stanford 7,430,397 1,618,478 78.22
AEGIR 5,096,918 1,312,715 74.24
 Term strength calculation: LTC algorithm (Salton and Buckley 1988) which is an
extension of the TF?IDF measure.
 Training method: Ensemble learning based on one-versus-rest binary classifiers.
 Winnow configuration: We performed tuning experiments for the Winnow param-
eters on a development set of around 100,000 documents. We arrived at using the
same setting as Koster et al (2011), namely, ? = 1.02, ? = 0.98, ?+ = 2.0, ?? = 0.5,
with a maximum of 10 training iterations.
 For each document the LCS returns a ranked list of all possible labels and the
attendant confidence scores. If the score assigned is higher than a predetermined
threshold, the document is assigned that category. The Winnow algorithm has a
default (natural) threshold equal to one. We configured the LCS to return a min-
imum of one label (with the highest score, even if it is lower than the threshold)
and a maximum of four labels for each document.
 The classification quality was determined by calculating the Precision, Recall, and
F1 measures per document/class combination (see, e.g., Koster, Seutter, and Beney
2003), on the document level (micro-averaged scores).
Table 2 shows the impact of our global term selection criteria for the different text
representations. This first feature reduction step is category-independent: The features
are discarded on the basis of the term and document frequencies over the corpus, dis-
regarding their distributions for the specific categories. We can see that the token/type
ratio of Table 1 is mirrored in this table: The sparsest syntactic phrases lose most terms.
Although the Stanford parser output is the sparsest text representation, it has the largest
pool of terms to select from at the end of the GTS process.
The impact of the second feature reduction phase is shown in Table 3. During local
term selection, the LCS finds the most representative terms for each class by selecting
the terms whose distributions in the sets of positive and negative training examples
for that class are maximally different from the general term distribution. We can see
that in the combined runs only around 50% of the selectable unigrams (after GTS) are
22 For the BOW baseline, the GTS criteria resulted in a too small term set that could then be used as a
starting point for the local term selection process for the individual classes. In such cases, the LCS has
a back-off mechanism that automatically (re)selects terms that were initially discarded during GTS.
In other words, the baseline classifier used terms that do not comply with the criteria in the GTS as
described in the text. In the combination runs, enough terms remained after GTS and no unigrams or
phrases that did not match the GTS criteria were selected.
765
Computational Linguistics Volume 39, Number 3
Table 3
Impact of local term selection (LTS) criteria in the training set (80% of the corpus).
# of terms after GTS # of terms after LTS
baseline uni 58,423 69,476
unigrams + bigrams uni 58,423 23,753bi 1,115,170 300,826
unigrams + stanford triples uni 58,423 26,630stanford 1,618,478 424,204
unigrams + AEGIR triples uni 58,423 29,348AEGIR 1,312,715 409,851
Table 4
Classification results on CLEF-IP 2010 English abstracts, with ranges for a 95% confidence
interval. Bold figures indicate the best results obtained with the five classifiers. (P: Precision;
R: Recall, F1: F1-score).
P R F1
weighted random guessing 6.09% ? 0.14 6.04% ? 0.14 6.06% ? 0.14
unigrams 76.27% ? 0.26 66.13% ? 0.28 70.84% ? 0.27
unigrams + bigrams 79.00% ? 0.24 70.19% ? 0.27 74.34% ? 0.26
unigrams + Stanford triples 78.35% ? 0.25 69.57% ? 0.28 73.70% ? 0.26
unigrams + AEGIR triples 78.51% ? 0.25 69.18% ? 0.28 73.55% ? 0.26
all representations 79.51% ? 0.24 71.11% ? 0.27 75.08% ? 0.26
selected as features during LTS. This means that the phrases replace at least a part of the
information contained in the possible unigrams.
4. Results and Discussion
4.1 Classification Accuracy
Table 4 shows the micro-averages of Precision, Recall, and F1 for five classification ex-
periments with different document representations. To give an idea of the complexity of
the task we have included a random guessing baseline in the first row.23 We found that
extending a unigram representation with statistical and/or linguistic phrases gives a
significant improvement in classification accuracy over the unigram baseline. The best-
performing classifier is the one that combines all four text representations. When adding
only type of phrase to unigrams, the unigrams + bigrams combination is significantly
better than the combinations with syntactic phrases. Combining all four representations
boosts recall, but has less impact on precision.
23 The script used to calculate the baseline can be downloaded at http://lands.let.ru.nl/~dhondt/.
We used a weighted randomization that takes the category label distributions and label frequency
distributions into account.
766
D?hondt et al Text Representations for Patent Classification
Table 5
Penetration of the bigrams and triples in the B60 class profiles (in % of terms at given rank).
rnk10 rnk20 rnk50 rnk100 rnk1000
bigrams 3.0 4.0 48.0 45.0 70.5
stanford 0.0 1.0 24.0 26.0 48.0
AEGIR 0.0 0.5 20.0 25.0 44.9
all representations
bigrams 2.0 2.0 34.0 36.0 43.2
stanford 0.0 0.0 4.0 6.0 13.0
AEGIR 0.0 0.0 2.0 4.0 18.0
The results are similar to O?zgu?r and Gu?ngo?r?s (2012) findings for scientific
abstracts: Adding phrases to unigrams can significantly improve classification. The text
in the patent corpus is vastly different from the newswire text in the Reuters corpus.
Like scientific abstracts, patents are full of jargon and terminology, often expressed in
multi-word units, which might favor phrasal representations. Moreover, the innovative
concepts in a patent are sometimes described in generalized terms combined with some
specifier (to ensure larger legal scope). For example, a hose might be referred to as a
watering device. The term hose can be captured with a unigram representation, but the
multi-word expression cannot. The difference with the results on the Reuters-21578
data set (discussed in Section 2.1.1), however, may not completely be due to genre
differences: Bekkerman and Allan (2003) remark that the unigram baseline for the
Reuters-21578 task is difficult to improve upon, because in that data set a few keywords
are enough to distinguish between the categories.
4.2 Unigram versus Phrases
In this section we investigate whether adding phrases suppresses, complements, or
changes unigram selection. To examine the impact of phrases in the classification
process, we analyzed the class profiles24 of two large classes (H04 ? Electric Communica-
tion Technique; and H01 ? Basic electric elements) that show significant improvements
in both Precision and Recall25 for the bigram classifier compared with the unigram
baseline. We look at (1) the overlap of the single words in the class profiles of the
unigram and combined representations; and (2) the overlap of the single words and the
words that make up the phrases (hereafter referred to as parts) within the class profile
of one text representation.
4.2.1 Overlap of Unigrams. The class profiles in the baseline unigram classifier contained
far fewer terms ( < 20%) than the profiles in the classifiers that combine unigrams and
phrases. This could be expected from the data in tables 2 and 3.
Unigrams are the highest ranked26 features in the combined representation class
profiles (see Table 5). Furthermore, words that are important terms for unigram clas-
sification also rank high in the combined class profiles: On average, there is an 80%
24 A class profile is the model built by the LCS classifier for a class during training. It consists of a ranked
list of terms that contribute most to distinguishing members from a class from all other classes.
25 H04: P: + 3.09%; R: + 1.83%; H01: P: + 3.61%; R: + 5.14%.
26 The rank of a term is based on the decreasing order of mass assigned to that term in the class profile.
(See Section 4.2.2.)
767
Computational Linguistics Volume 39, Number 3
overlap of the top 1,000 most important words in unigram and combined representation
class profiles. This decreases to 75% when looking at the 5,000 most important words.
This shows that the classifier tends to select mostly the same words as important terms
for the different text representation combinations. The relative ranking of the words is
very similar in the class profiles of all the text representations. Thus, adding phrases
to unigrams does not result in replacing the most important unigrams for a particular
class and the improvements in classification accuracy must derive from the additional
information in the selected phrases.
4.2.2 Overlap of Single Words and Parts of Bigrams. Like Caropreso, Matwin, and Sebastiani
(2001), we investigated to what extent the parts of the high-ranked phrases overlap with
words in the unigrams + bigrams class profile. We first looked at the lexical overlap of
the words and the parts of the bigrams in the H01 unigrams + bigrams class profile.
Interestingly, we found a relatively low overlap between the words and the parts of
the phrases: For the 20 most important bigrams, only 11 of the 32 unique parts of the
bigrams overlap with the 100 most important single word terms; in the complete class
profile only 56% of the 10,387 parts of the bigrams overlap with the 9,064 words in
the class profile. This means that a large part of the bigrams contains complementary
information not present in the unigrams in the class profile.
To gain a deeper insight into the relationship between the bigrams and their parts,
we also looked at the mass of the different terms in the class profiles. The mass of a
term for a certain class is the product of its TF?IDF score and its Winnow weight for
that class; ?mass? provides an estimate of how much a term contributes to the score of
documents for a particular class. We can divide the terms into three main categories:
(a) mass(partA) ? mass(partB) ? mass(bigram);
(b) mass(partA) ? mass(bigram) > mass(partB);
(c) mass(bigram) > mass(partA) ? mass(partB).
We note that 50% of the top 1,000 highest ranked bigrams fall within category (b) and
typically consist of one part with high mass accompanied by a part with a low mass,
which can be a function word (for example a transmitter), or a general term (for example,
device in optical device). The highest ranked bigrams can be found in category (a) where
two highly informative words are combined to form very specific concepts, for example,
fuel cell. These are specifications of a more general concept that is typical for that class in
the corpus. The bigrams in this category are similar to those investigated by Caropreso,
Matwin, and Sebastiani (2001) and Tan, Wang, and Lee (2002). Though highly ranked,
they only make up a small subset (22%) of the important bigram features.
The bigrams in category (c) (27%) are typically made up from low-ranked single
words, such as mobile station. Interestingly, most bigram parts in this subset do not occur
as word terms in the unigram and bigram class profiles, but occur in the negative class
profiles (a selection of terms that are considered to describe anything but that particular
class). The complementary information of bigram phrases (compared to unigrams) is
contained in this set of bigrams.
4.3 Statistical versus Linguistic Phrases
Results in Section 4.1 indicate that bigrams are most important additional features, but
the experiment combining all four representations showed that dependency triples do
768
D?hondt et al Text Representations for Patent Classification
complement bigrams. In this section we examine what information is captured by the
different phrases and how this accounts for the differences in classification accuracy.
4.3.1 Class Profile Analysis. We first examined the differences between the statistical
phrases and the two types of linguistic phrases to discover what information contained
in the bigrams leads to better classification results. We performed our analysis on the
different class profiles of B60 (?Vehicles in general?), a medium-sized class, which most
clearly shows the advantage of the bigram classifier compared to the classifiers with
linguistic phrases.27
All four class profiles with phrases contain roughly the same set of unigrams
(between 78% to 91% overlap) that occur quite high in the corresponding unigram class
profile. The AEGIR class profile contains 10% more unigrams than the other combined
representation class profiles; these are mainly words that appear in the negative class
profile of the corresponding unigram classifier. As in class H01, the relative position of
the words remains the same. The absolute position of the words in the list, however,
does change: Caropreso, Matwin, and Sebastiani (2001) introduced a measure for the
effectiveness of phrases as terms, called the penetration, that is, the percentage of
phrases in the top k terms when classifying with both words and phrases.
Comparing the penetration levels at the various ranks for the different classifiers,
we can see that the classification results correspond with the tendency of a classifier to
select phrases in the top k terms. Interestingly, we see a large disparity in the phrasal
features that are selected by the combination classifier. The preference for bigrams
is mirrored by the penetration levels of the unigrams + bigrams classifier which has
selected more bigrams at higher ranks in the class profile than the classifiers with the
linguistic phrases. This is in line with the findings of Caropreso, Matwin, and Sebastiani
(2001) that penetration levels are a reasonable way to compute the contribution of
n-grams to the quality of a feature set. On average, the linguistic phrases have much
smaller weight in the class profiles than the bigrams and, consequently, are likely
to have a smaller impact during the classification process. For the combination run,
however, it seems that a long tail of small-impact features does improve classification
accuracy.
Linguistic analysis of the top 100 phrases in the profiles of class B60 shows that
all classifiers select similar types of phrases. We manually annotated the bigrams with
the correct syntactic dependencies (in the Stanford collapsed typed dependency format)
and compared these with the syntactic relations expressed in the linguistic phrases. The
results are summarized in Table 6.
It appears that noun phrases and compounds such as circuit board and electric
device are by far the most important terms in the class profiles. Interestingly, phrases
that contain a determiner relation (e.g., the device) are deemed equally important in
all four different class profiles. It is unlikely that this is a semantic effect, that is, that
the determiner relation provides additional semantic information to the nouns in the
phrases, but rather it seems an artefact of the abundance of noun phrases which occur
in patent texts. We also looked into the lexical overlap between the parts of the different
types of phrases. We found that the selected phrases encode almost exactly the same
information in all three representations: There is an 80% overlap between the parts of
27 Precision is 77.34% for unigrams+bigrams, 75.67% for unigrams+Stanford, 73.47% for unigrams+AEGIR,
and 77.38% for unigrams+bigrams+Stanford+AEGIR. The Recall scores are essentially equal for all three,
that is, 68.81%, 68.38%, 69.7%, and 70.18%, respectively.
769
Computational Linguistics Volume 39, Number 3
Table 6
Distribution of the top 100 statistical and syntactic phrases in the B60 class profiles.
grammatical relation bigrams stanford AEGIR combination
noun?noun compounds 41 48
6228 4428adjectival modifier 11 8
determiner 34 28 27 41
subject 6 4 6 9
prepositions 2 4 1 2
<other> 7 8 4 4
the top 100 most important phrases. This decreases only to 75% when looking at the
10,000 most important phrases.
Given that the class profiles select the same set of words and contain phrases with
a high lexical overlap, therefore, how do we explain the marked differences in classifi-
cation accuracy between the three different representations? These must stem from the
different combinations of the words in the phrasal features. To examine in detail how the
features created through the different text represenations differ, we conducted a feature
quality assessment experiment against a manually created reference set.
4.3.2 Human Quality Assessment Experiment. To gain more insight in the syntactic and
semantic relations that are considered most informative by humans, we conducted
an experiment in which we asked human annotators to select the five to ten most
informative phrases29 for 15 sentences taken at random from documents in the three
largest classes in the corpus. We then compiled a reference set consisting of 70 phrases
(4.6 phrases per sentence) which were considered as ?informative? by at least three
out of four annotators. Of these, 57 phrases were noun?noun compounds and 11 were
combinations of an adjectival modifier with a noun. None of the annotators selected
phrases containing determiners.
We created bigrams from the input and extracted head?modifier pairs30 from the
parser output for the sentences in the test set. We then compared the overlap of the
generated phrases with the reference phrases. We found that bigrams overlap with
53 of the 70 reference phrases; Stanford triples overlap with 62 phrases and AEGIR
triples overlap with 57 phrases. Although three data points are not enough to compute
a formal measure, it is interesting to note the correspondence with the number of terms
kept for the three text representations after Local Term Selection (see Table 3). The fact
that the text representation with the smallest number of terms after LTC and with the
smallest overlap with ?contentful? phrases in a text as indicated by human annotators
still yields the best classification performance suggests that not all ?contentful? phrases
are important or useful for the task of classifying that text. This finding is reminiscent of
the fact that the ?optimal? summary of a text is dependent on the goal with which the
summary was produced (Nenkova and McKeown 2011).
Only 15% of the phrases extracted by the human annotators contain word combi-
nations that have long-distance dependencies in the original sentences. This suggests
28 As mentioned in Section 3.2.3 the AEGIR parser uses a more condensed dependency output format.
The Stanford?s nn and amod are collapsed into the attributive (ATTR) relation.
29 ?Phrase? was defined as a combination of two words that both occur in the sentence, irrespective of
the order in which they occur in the sentence.
30 Head?modifier pairs are syntactic triples that are stripped of their grammatical relations.
770
D?hondt et al Text Representations for Patent Classification
that the most meaningful phrases are expressed in local dependencies, that is, adjacent
words. Consequently, syntactic analysis aimed at discovering meaning expressed by
long-distance dependencies can only make a small contribution. A further analysis of
the phrases showed that the smaller coverage of the bigrams is due to the fact that some
of the relevant noun?noun combinations are missed because function words, typically
determiners or prepositions, occur between the nouns. For example, the annotators
constructed the reference phrase rotation axis for the noun phrase the rotation of the
second axis. This reference phrase cannot be captured by the bigram representation.
When intervening function words are removed from the sentences, the coverage of
the resulting bigrams on the reference set rises31 to 59 phrases (more than AEGIR, and
almost as many as Stanford). Despite the fact that generating more phrases does not
necessarily lead to better classification performance, we intend to use bigrams stripped
of function words as additional terms for patent classification in future experiments.
The analysis also revealed an indication why syntactic phrases may lead to inferior
classification results: Both syntactic parsers consistently fail to find the correct structural
analysis of the long and complex noun phrases such as an implantable, inflatable dual
chamber shape retention tissue expander, which are frequent in patent texts. Phrases like
this contain many compounds in an otherwise complex syntactic structure, namely
[an [implantable, inflatable [[dual chamber] [shape retention] [tissue expander]]]].
For a parser it is impossible to parse this correctly without knowing which word se-
quences are actually compounds. That knowledge might be gleaned from the frequency
with which sequences of nouns and adjectives occur in a given domain. For the time
being, the Stanford parser (and the AEGIR parser, to a lesser extent) will parse any
noun phrase by attaching the individual words to the right-most head noun, resulting
in the following analysis:
[an [implantable, [inflatable [dual [chamber [shape [retention [tissue expander]]]]]]]].
This effectively destroys many of the noun?noun compounds, which are the most
important features for patent classification (see Table 6). Bigrams are less prone to this
type of ?error.?
These findings are confirmed when looking at the overlap of the word combi-
nations: Although there is high lexical overlap between the phrases of the different
representations (80% overlap of the parts of phrases in Section 4.3.1), the overlap of
the word combinations that make up the phrases is much lower: Only 33% of the top
1,000 phrases are common between all three representations.
4.4 Stanford versus AEGIR Triples
The performance with the unigrams + Stanford triples is not significantly different from
the combination with AEGIR triples. Because the AEGIR triples are slightly less sparse
(see Table 1), we expected that these would have an advantage over Stanford triples.
Most of the normalization processes that make the AEGIR triples less sparse concern
syntactic variation on the clause level, however. But as was shown in Section 4.3,
31 This result is language-dependent: English has a fairly rigid phrase-internal word order but for a more
synthetic language with a more variable word order, like Russian, bigram coverage might suffer from
the variation in the surface form.
771
Computational Linguistics Volume 39, Number 3
Table 7
Classification results on CLEF-IP 2010 French and German abstracts, with ranges for
95% confidence intervals.
P R F1
French unigrams 70.65% ? 0.68 61.40% ? 0.73 65.70% ? 0.70unigrams + bigrams 72.31% ? 0.67 62.58% ? 0.72 67.09% ? 0.69
German unigrams 76.44% ? 0.34 65.82% ? 0.38 70.73% ? 0.37unigrams + bigrams 76.39% ? 0.34 65.41% ? 0.38 70.47% ? 0.37
the most important terms for classification in the patent domain are found in the
noun phrase, where Stanford and AEGIR perform similar syntactic analyses. Although
Stanford?s dependency scheme is more detailed (see Table 6), the noun-phrase internal
dependencies in the Stanford parser map practically one-to-one onto AEGIR?s set of
relators, resulting in very similar dependency triple features for classification. Con-
sequently, there is no normalization gain in using the AEGIR dependency format to
describe the internal structure of the noun phrases.
4.5 Comparison with French and German Patent Classification
We found that phrases contribute to improving classification on English patent ab-
stracts. The improvement might be language-dependent, however, because compounds
are treated differently in different languages. A compounding language like German
might benefit less from using phrases than English. To estimate the generalizability of
our findings, we conducted additional experiments in which we compared the impact
of adding bigrams to unigrams for both French and German.
Using the same methods described in sections 3.1 and 3.2, we extracted and pro-
cessed all French and German abstracts from the CLEF-IP 2010 corpus, resulting in two
new data sets that contained 86,464 and 294,482 documents, respectively (Table 7). Both
data sets contained the same set of 121 labels and had label distributions similar to the
English data set. The sentencing script was updated with the most common French and
German abbreviations to minimize incorrect sentence splitting. The resulting sentences
were then tagged using the French and German versions of the TreeTagger.32 From the
tagged output, we extracted the lemmas and used these to construct unigrams and
bigrams for both languages. We ran the experiments with the LCS using the settings
reported in Section 3.3.
The results show a much smaller but still significant improvement for using bigrams
when classifying French patent abstracts and even a deterioration for German. Due to
the difference in size between the English and French data set it is difficult to draw hard
conclusions on which language benefits most from adding bigrams. It is clear, however,
that our findings are not generalizable to German (and probably other compounding
languages).
5. Conclusion
In this article we have examined the usefulness of statistical and linguistic phrases
for patent classification. Similar to O?zgu?r and Gu?ngo?r?s (2010) results for scientific
32 http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/.
772
D?hondt et al Text Representations for Patent Classification
abstracts, we found that adding phrases to unigrams significantly improves classifi-
cation results for English. Of the three types of phrases examined in this article, bigrams
have the most impact, both in the experiment that combined all four text representa-
tions, and in combination with unigrams only.
The abundance of compounds in the terminology-rich language of the patent do-
main results in a relatively high importance for the phrases. The top phrases across
the different representations were mostly noun?noun compounds (for example watering
device), followed by phrases containing a determiner relation (for example the module)
and adjective modifier phrases (for example separate module).
The information in the phrases and unigrams overlaps to a large extent: Most of the
phrases consist of words that are important unigram features in the combined profile
and that also appear in the corresponding unigram class profile. When examining the
H01 class profiles, however, we found that 27% of the selected phrases contain words
that were not selected in the unigram profile (see Section 4.2.2).
When comparing the impact of features created from the output of the aboutness-
based AEGIR parser with those from the Stanford parser, we found the latter resulted in
slightly (but not significantly) better classification results. AEGIR?s normalization fea-
tures are not advantageous (compared with Stanford) in creating noun-phrase internal
triples, which are the most informative features for patent classification.
The parsers were not specifically trained for the patent domain and both experi-
enced problems with long, complex noun phrases consisting of sequences of words
that can function as adjective/adverb or noun and that are not interrupted by function
words that clarify the syntactic structure. The right-headed bias of both syntactic parsers
caused problems in analyzing those constructions, yielding erroneous and variable
data. As a consequence, parsers may miss potentially relevant noun?noun compounds
and noun phrases with adjectival modifiers. Because of the highly idiosyncratic nature
of the terminology used in the patent domain, it is not evident whether this prob-
lem can be solved by giving a parser access to information about the frequency with
which specific noun?noun, adjective?noun, and adjective/adverb?adjective pairs occur
in technical texts. Bigrams, on the other hand, are less variable (as seen in Table 1) and
therefore yield better classification results. This is the more important point because the
dependency relations marked as important for understanding a sentence by the human
annotators consist mainly of pairs of adjacent words.
We also performed additional experiments to examine the generalizability of our
findings for French and German: As could be expected, compounding languages like
German which express complex concepts in ?one word? do not gain from using
bigrams.
In line with Bekkerman and Allan (2003) we can conclude that with the large quan-
tities of text available today, the role of phrases as features in text classification must
be reconsidered. For the automated classification of English patents at least, adding
phrases and more specifically bigrams significantly improves classification accuracy.
References
Apte?, Chidanand, Fred Damerau, and
Sholom Weiss. 1994. Automated learning
of decision rules for text categorization.
ACM Transactions on Information Systems,
12(3):233?251.
Bekkerman, Ron and John Allan. 2003. Using
bigrams in text categorization. Technical
Report IR-408, Center of Intelligent
Information Retrieval, University of
Massachusetts, Amherst.
Beney, Jean. 2010. LCI-INSA linguistic
experiment for CLEF-IP classification
track. In Proceedings of the Conference on
Multilingual and Multimodal Information
Access Evaluation (CLEF 2010), Padua.
773
Computational Linguistics Volume 39, Number 3
Benzineb, Karim and Jacques Guyot. 2011.
Automated patent classification. In
Mihai Lupu, Katja Mayer, John Tait, and
Anthony J. Trippe, editors, Current
Challenges in Patent Information Retrieval,
volume 29. Springer, New York,
pages 239?261.
Braga, Igor, Maria Monard, and Edson
Matsubara. 2009. Combining unigrams
and bigrams in semi-supervised text
classification. In Proceedings of Progress
in Artificial Intelligence, 14th Portuguese
Conference on Artificial Intelligence
(EPIA 2009), pages 489?500, Aveiro.
Caropreso, Maria Fernanda, Stan Matwin,
and Fabrizio Sebastiani. 2001. A
learner-independent evaluation of the
usefulness of statistical phrases for
automated text categorization. In
A. G. Chin, editor, Text Databases &
Document Management. IGI Publishing,
Hershey, PA, pages 78?102.
Crawford, Elisabeth, Irena Koprinska,
and Jon Patrick. 2004. Phrases and
feature selection in e-mail classification.
In Proceedings of the 9th Australasian
Document Computing Symposium (ADCS),
pages 59?62, Melbourne.
Dagan, Ido, Yael Karov, and Dan Roth.
1997. Mistake-driven learning in text
categorization. In Proceedings of 2nd
Conference on Empirical Methods in NLP,
pages 55?63, Providence, RI.
de Marneffe, Marie-Catherine and
Christopher Manning. 2008. The Stanford
Typed Dependencies representation. In
Coling 2008: Proceedings of the Workshop on
Cross-Framework and Cross-Domain Parser
Evaluation, pages 1?8, Manchester.
Derieux, Franck, Mihaela Bobeica, Delphine
Pois, and Jean-Pierre Raysz. 2010.
Combining semantics and statistics for
patent classification. In Proceedings of the
Conference on Multilingual and Multimodal
Information Access Evaluation (CLEF 2010),
Padua.
Dumais, Susan, John Platt, David
Heckerman, and Mehran Sahami. 1998.
Inductive learning algorithms and
representations for text categorization.
In Proceedings of the Seventh International
Conference on Information and Knowledge
Management (CIKM ?98), pages 148?155,
Bethesda.
Fall, Caspar J. and Karim Benzineb. 2002.
Literature survey: Issues to be considered
in the automatic classification of patents.
Technical report, World Intellectual
Property Organization, Geneva.
Fall, Caspar J., Atilla To?rcsva?ri, Karim
Benzineb, and Gabor Karetka. 2003.
Automated categorization in the
international patent classification.
ACM SIGIR Forum, 37(1):10?25.
Fu?rnkranz, Johannes. 1998. A study using
n-gram features for text categorization.
Technical Report OEFAI-TR-98-30,
Austrian Research Institute for
Artificial Intelligence, Vienna.
Fu?rnkranz, Johannes. 1999. Exploiting
structural information for text
classification on the WWW. In Proceedings
of Advances in Intelligent Data Analysis
(IDA-99), pages 487?497, Amsterdam.
Galavotti, Luigi, Fabrizio Sebastiani, and
Maria Simi. 2000. Experiments on the
use of feature selection and negative
evidence in automated text categorization.
In Proceedings of Research and Advanced
Technology for Digital Libraries,
4th European Conference, pages 59?68,
Lisbon.
Guyot, Jacques, Karim Benzineb, and Gilles
Falquet. 2010. Myclass: A mature tool for
patent classification. In Proceedings of the
Conference on Multilingual and Multimodal
Information Access Evaluation (CLEF 2010),
Padua.
Held, Pierre, Irene Schellner, and Ryuichi
Ota. 2011. Understanding the world?s
major patent classification schemes. Paper
presented at the PIUG 2011 Annual
Conference Workshop, Vienna, 13 April.
Joachims, Thorsten. 1999. Making large-scale
support vector machine learning practical.
In Bernhard Scho?lkopf, Christopher J. C.
Burges, and Alexander J. Smola, editors,
Advances in Kernel Methods. MIT Press,
Cambridge, MA, pages 169?184.
Koster, Cornelis, Jean Beney, Suzan Verberne,
and Merijn Vogel. 2011. Phrase-based
document categorization. In Mihai Lupu,
Katja Mayer, John Tait, and Anthony J.
Trippe, editors, Current Challenges in Patent
Information Retrieval, volume 29. Springer,
New York, pages 263?286.
Koster, Cornelis, Marc Seutter, and
Jean Beney. 2001. Classifying patent
applications with winnow. In Proceedings
Benelearn 2001. pages 19?26, Antwerpen.
Koster, Cornelis, Marc Seutter, and
Jean Beney. 2003. Multi-classification
of patent applications with winnow.
In Manfred Broy and Alexandre V.
Zamulin, editors, Perspectives of
Systems Informatics: 5th International
Andrei Ershov Memorial Conference,
volume 2890 of Lecture Notes in
774
D?hondt et al Text Representations for Patent Classification
Computer Science. Springer, New York,
pages 546?555.
Koster, Cornelis and Mark Seutter. 2003.
Taming wild phrases. In Proceedings
of the 25th European conference on
IR research (ECIR?03), pages 161?176, Pisa.
Krier, Marc and Francesco Zacca`. 2002.
Automatic categorization applications at
the European patent office. World Patent
Information, 24(3):187?196.
Larkey, Leah. 1998. Some issues in the
automatic classification of U.S. patents.
In Working Notes of the Workshop on
Learning for Text Categorization, 15th
National Conference on AI, pages 87?90,
Madison, WI.
Larkey, Leah S. 1999. A patent search and
classification system. In Proceedings of the
Fourth ACM Conference on Digital Libraries
(DL?99), pages 179?187, Berkeley.
Lewis, David D. 1992. An evaluation of
phrasal and clustered representations on a
text categorization task. In Proceedings of
the 15th Annual International ACM SIGIR
Conference on Research and Development
in Information Retrieval (SIGIR ?92),
pages 37?50, Copenhagen.
Mille, Simon and Leo Wanner. 2008. Making
text resources accessible to the reader: The
case of patent claims. In Proceedings of the
6th International Conference on Language
Resources and Evaluation (LREC?08),
Marrakech.
Mitra, Mandar, Chris Buckley, Amit Singhal,
and Claire Cardie. 1997. An analysis
of statistical and syntactic phrases.
In Proceedings of RIAO?97 Computer-Assisted
Information Searching on Internet,
pages 200?214, Montreal.
Mladenic, Dunja and Marko Grobelnik.
1998. Word Sequences as Features in
Text-Learning. In Proceedings of the 17th
Electrotechnical and Computer Science
Conference (ERK98), pages 145?148,
Ljubljana.
Moschitti, Alessandro and Roberto Basili.
2004. Complex linguistic features for
text classification: A comprehensive
study. In Sharon McDonald and
John Tait, editors, Advances in Information
Retrieval, volume 2997 of Lecture Notes in
Computer Science. Springer, New York,
pages 181?196.
Nastase, Vivi, Jelber Sayyad, and
Maria Fernanda Caropreso. 2007.
Using dependency relations for text
classification. Technical Report TR-2007-12,
University of Ottawa.
Nenkova, Ani and Kathleen McKeown. 2011.
Automatic summarization. Foundations
and Trends in Information Retrieval,
5(2?3):103?233.
Ozgu?r, Levent and Tunga Gu?ngo?r.
2009. Analysis of stemming alternatives
and dependency pattern support in text
classification. In Proceedings of Tenth
International Conference on Intelligent
Text Processing and Computational
Linguistics (CICLing 2009),
pages 195?206, Mexico City.
O?zgu?r, Levent and Tunga Gu?ngo?r.
2010. Text classification with the support of
pruned dependency patterns. Pattern
Recognition Letters, 31(12):1598?1607.
O?zgu?r, Levent and Tunga Gu?ngo?r.
2012. Optimization of dependency and
pruning usage in text classification.
Pattern Analysis and Applications,
15(1):45?58.
Parapatics, Peter and Michael Dittenbach.
2009. Patent claim decomposition for
improved information extraction. In
Proceedings of the 2nd International
Workshop on Patent Information Retrieval
(PAIR?09), pages 33?36, Hong Kong.
Salton, Gerard and Christopher Buckley.
1988. Term-weighting approaches in
automatic text retrieval. Information
Processing Management, 24(5):513?523.
Scott, Sam and Stan Matwin. 1999. Feature
engineering for text classification.
In Proceedings of the Sixteenth International
Conference on Machine Learning (ICML ?99),
pages 379?388, Bled.
Smith, Harold. 2002. Automation of patent
classification. World Patent Information,
24(4):269?271.
Tan, Chade-Meng, Yuan-Fang Wang,
and Chan-Do Lee. 2002. The use of
bigrams to enhance text categorization.
Information Processing and Management,
38(4):529?546.
Verberne, Suzan and Eva D?hondt. 2011.
Patent classification experiments with
the Linguistic Classification System
LCS in CLEF-IP 2011. In Proceedings of the
Conference on Multilingual and Multimodal
Information Access Evaluation (CLEF 2011),
Amsterdam.
Verberne, Suzan, Merijn Vogel, and
Eva D?hondt. 2010. Patent classification
experiments with the Linguistic
Classification System LCS. In Proceedings
of the Conference on Multilingual and
Multimodal Information Access Evaluation
(CLEF 2010), Padua.
775
