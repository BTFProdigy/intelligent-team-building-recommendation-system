Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 91?99,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
An improved corpus of disease mentions in PubMed citations 
 
Rezarta Islamaj Do?an Zhiyong Lu 
National Center for Biotechnology Information National Center for Biotechnology Information 
8600 Rockville Pike 8600 Rockville Pike 
Bethesda, MD 20894, USA Bethesda, MD 20894, USA 
Rezarta.Islamaj@nih.gov Zhiyong.Lu@nih.gov 
 
 
 
 
 
Abstract 
The latest discoveries on diseases and their di-
agnosis/treatment are mostly disseminated in 
the form of scientific publications. However, 
with the rapid growth of the biomedical litera-
ture and a high level of variation and ambigui-
ty in disease names, the task of retrieving 
disease-related articles becomes increasingly 
challenging using the traditional keyword-
based approach. An important first step for 
any disease-related information extraction 
task in the biomedical literature is the disease 
mention recognition task. However, despite 
the strong interest, there has not been enough 
work done on disease name identification, 
perhaps because of the difficulty in obtaining 
adequate corpora. Towards this aim, we creat-
ed a large-scale disease corpus consisting of 
6900 disease mentions in 793 PubMed cita-
tions, derived from an earlier corpus. Our cor-
pus contains rich annotations, was developed 
by a team of 12 annotators (two people per 
annotation) and covers all sentences in a 
PubMed abstract. Disease mentions are cate-
gorized into Specific Disease, Disease Class, 
Composite Mention and Modifier categories. 
When used as the gold standard data for a 
state-of-the-art machine-learning approach, 
significantly higher performance can be found 
on our corpus than the previous one. Such 
characteristics make this disease name corpus 
a valuable resource for mining disease-related 
information from biomedical text. The NCBI 
corpus is available for download at 
http://www.ncbi.nlm.nih.gov/CBBresearch/Fe
llows/Dogan/disease.html. 
1 Introduction 
Identification of biomedical entities has been an 
active area of research in recent years (Rinaldi et 
al., 2011, Smith et al, 2008, Yeh et al, 2005). Au-
tomatic systems, both lexically-based and machine 
learning-based, have been built to identify medi-
cally relevant concepts and/or their relationships. 
Biomedical entity recognition research covers not 
only gene/protein mention recognition (Tanabe et 
al., 2005, Campos et al, 2012), but also other med-
ically relevant concepts such as disease names, 
chemical/drug names, treatments, procedures etc. 
Systems capable of achieving high performance on 
these tasks are highly desirable as entity recogni-
tion precedes all other information extraction and 
text mining tasks.   
Disease information is sought very frequently in 
biomedical search engines. Previous PubMed log 
usage analysis (Islamaj Dogan et al, 2009) has 
shown that disease is the most frequent non-
bibliographic information requested from PubMed 
users. Furthermore, disease information was often 
found to be queried together with Chemical/Drug 
or Gene/Protein information. Automatic recogni-
tion of disease mentions therefore, is essential not 
only for improving retrieval of relevant documents, 
but also for extraction of associations between dis-
eases and genes or between diseases and drugs. 
However, prior research shows that automatic dis-
ease recognition is a challenging task due to varia-
tions and ambiguities in disease names (Leaman et 
al., 2009, Chowdhury and Lavelli 2010).  
Lexically-based systems of disease name recog-
nition, generally refer to the Unified Medical Lan-
guage System (UMLS) (Burgun and Bodenreider 
91
2008). UMLS is a comprehensive resource of med-
ically relevant concepts and relationships and 
METAMAP(Aronson and Lang 2010) is an exam-
ple of a natural language processing (NLP) system 
that provides reliable mapping of the text of a bio-
medical document to UMLS concepts and their 
semantic types.  
Machine learning systems, on the other hand, 
have been employed in order to benefit from the 
flexibility they allow over the rule-based and other 
statistical systems. However, machine learning 
systems are strongly dependent on the data availa-
ble for their training; therefore a comprehensive 
corpus of examples representing as many varia-
tions as possible of the entity of interest is highly 
favorable. 
To our best knowledge, there is one corpus of 
disease mentions in MEDLINE citations developed 
by Leaman et al, 2009. This corpus, AZDC cor-
pus, was inspired by the work of Jimeno et al, 
2008 and its overall characteristics are given in 
Table 1. This corpus has been the study of at least 
two different groups in building automatic systems 
for disease name recognition in biomedical litera-
ture (Leaman et al, 2009, Chowdhury and Lavelli, 
2010). They both reported F-scores around 80% in 
10-fold cross-validation experiments.  
One common encountered difficulty in this do-
main is the fact that ?disease? as a category has a 
very loose definition, and covers a wide range of 
concepts. ?Disease? is a broadly-used term that 
refers to any condition that causes pain, suffering, 
distress, dysfunction, social problems, and/or 
death. In UMLS, the ?disease? concept is covered 
by twelve different semantic types as shown in 
Table 2. The disease definition issue has been dis-
cussed extensively in other studies (Neveol et al, 
2009, Neveol and Lu 2012).   
Disease mentions are also heavily abbreviated in 
biomedical literature (Yeganova et al, 2010). The-
se abbreviations are not always standard; the same 
abbreviated form may represent different defining 
strings in different documents. It is therefore, un-
clear whether these ambiguities could be resolved 
by an abbreviation look-up list from UMLS Me-
tathesaurus and other available databases.  
In this study, we present our efforts in improv-
ing the AZDC corpus by building a richer, broader 
and more complete disease name corpus. The 
NCBI corpus reflects a more representative view 
of what constitutes a disease name as it combines 
the decisions of twelve annotators. It also provides 
four different categories of disease mentions. Our 
work was motivated by the following observations:  
? The need of a pool of experts:  
The AZDC corpus is the work of one annota-
tor. While in terms of consistency this is gen-
erally a good thing, a pool of annotators 
guarantees a more representative view of the 
entity to be annotated and an agreement be-
tween annotators is preferred for categories 
with loose definitions such as ?disease?. 
Moreover, this would ensure that there would 
be fewer missed annotations within the corpus.  
? The need of annotating all sentences in a 
document:  
The AZDC corpus has disease mention annota-
tions of selected sentences in a collection of 
PubMed abstracts. In order to be able to per-
form higher level text mining tasks that ex-
plore relationships between diseases and other 
types of information such as genes or drugs, 
the disease name annotation has to include all 
sentences, as opposed to selected ones. 
Our work is also related to other corpus annota-
tion projects in the biomedical domain (Grouin et 
al., 2011, Tanabe at al., 2005, Thompson et al, 
2009, Neveol at al., 2009, Chapman et al, 2012). 
These studies generally agree on the need of multi-
ple experienced annotators for the project, the need 
of detailed annotation guidelines, and the need of 
large scale high-quality annotation corpora. The 
production of such annotated corpora facilitates the 
development and evaluation of entity recognition 
and information extraction systems. 
2 Methods 
Here we describe the NCBI corpus, and its annota-
tion process. We discuss the annotation guidelines 
and how they evolved through the process. 
2.1 The NCBI disease corpus 
The AZDC corpus contains 2,783 sentences cho-
sen from 793 PubMed abstracts. These selected 
Table 1 AZDC corpus characteristics 
Characteristics of the corpus  
Selected abstracts 793 
Sentences 2,783 
Sentences with disease mentions 1,757 
Total disease mentions 3,224 
 
92
sentences were annotated for disease mentions, 
resulting in 1,202 unique mentions and 3,224 total 
mentions. The NCBI corpus starts with this origi-
nal corpus; however, it is expanded to cover all the 
sentences in all the 793 PubMed abstracts. 
2.2 Annotation guidelines 
One fundamental problem in corpus annotation is 
the definition of what constitutes an entity to be 
tagged. Following the lead of the AZDC annota-
tions, the group of annotators working on the 
NCBI corpus decided that a textual string would be 
annotated as a disease mention if it could be 
mapped to a unique concept in the UMLS Me-
tathesaurus, if it corresponded to at least one of the 
semantic types listed in Table 2, and if it contained 
information that would be helpful to physicians 
and health care professionals. 
Annotators were invited to use their common 
knowledge, use public resources of the National 
Library of Medicine such as UMLS or PubMed 
Health, Disease Ontology (Warren et al, 2006) and 
Wikipedia and consider the viewpoint of an aver-
age user trying to find information on diseases. 
Initially, a set of 20 randomly chosen PubMed 
abstracts was used as a practice set for the devel-
opment of annotation guidelines. After each anno-
tator worked individually on the set, the results 
were shared and discussed among all annotators. 
The final annotation guidelines are summarized 
below and also made available at the corpus down-
load website. 
What to annotate? 
1. Annotate all specific disease mentions. 
A textual string referring to a disease name may 
refer to a Specific Disease, or a Disease Class. 
Disease mentions that could be described as a 
family of many specific diseases were annotated 
with an annotation category called Disease 
Class. The annotation category Specific Disease 
was used for those mentions which could be 
linked to one specific definition that does not in-
clude further categorization.  
e.g. <Specific Disease> Diastrophic dysplasia 
</> is an <Disease Class> autosomal recessive 
disease</> characterized by short stature, very 
short limbs and joint problems that restrict mo-
bility. 
2. Annotate contiguous text strings. 
A textual string may refer to two or more sepa-
rate disease mentions. Such mentions are anno-
tated with the Composite Mention category. 
e.g. The text phrase ?Duchenne and Becker 
muscular dystrophy? refers to two separate dis-
eases. If this phrase is separated into two strings: 
?Duchenne? and ?Becker muscular dystrophy?, 
it results in information loss, because the word 
?Duchenne? on its own is not a disease mention.  
3. Annotate disease mentions that are used as 
modifiers for other concepts 
A textual string may refer to a disease name, but 
it may not be a noun phrase and this is better ex-
pressed with the Modifier annotation category.  
e.g.: Although this mutation was initially de-
tected in four of 33 <Modifier> colorectal can-
cer </> families analysed from eastern England, 
more extensive analysis has reduced the fre-
quency to four of 52 English <Modifier> 
HNPCC </> kindreds analysed. 
4. Annotate duplicate mentions. 
Table 2 The set of UMLS semantic types that collectively cover concepts of the ?disease? category 
UMLS sematic types Disease name example 
Acquired Abnormality Hernia, Varicose Veins 
Anatomical Abnormality Bernheim aneurysm,  Fistula of thoracic duct 
Congenital Abnormality Oppenheim's Disease, Ataxia Telangiectasia 
Cell or Molecular Dysfunction Uniparental disomy, Intestinal metaplasia 
Disease or Syndrome   Acute pancreatitis, Rheumatoid Arthritis 
Experimental Model of Disease Collagen-Induced Arthritis, Jensen Sarcoma 
Injury or Poisoning Contusion and laceration of cerebrum 
Mental or Behavioral Dysfunction Schizophrenia, anxiety disorder, dementia 
Neoplastic Process Colorectal Carcinoma, Burkitt Lymphoma 
Pathologic Function Myocardial degeneration, Adipose Tissue Atrophy 
Sign or Symptom Back Pain, Seizures, Skeletal muscle paralysis 
Finding Abnormal or prolonged bleeding time 
 
93
For each sentence in the PubMed abstract and ti-
tle, the locations of all disease mentions are 
marked, including duplicates within the same 
sentence.  
5. Annotate minimum necessary span of text. 
The minimum span of text necessary to include 
all the tokens expressing the most specific form 
of the disease is preferred. For example, in case 
of the phrase ?insulin-dependent diabetes melli-
tus?, the disease mention including the whole 
phrase was preferred over its substrings such as 
?diabetes mellitus? or ?diabetes?.  
6. Annotate all synonymous mentions.  
Abbreviation definitions such as ?Huntington 
disease? (?HD?) are separated into two annotat-
ed mentions. 
What not to annotate?  
1. Do not annotate organism names. 
Organism names such as ?human? were exclud-
ed from the preferred mention. Viruses, bacteria, 
and other organism names were not annotated 
unless it was clear from the context that the dis-
ease caused by these organisms is discussed.  
e.g. Studies of biopsied tissue for the presence 
of <Specific Disease> Epstein-Barr virus</> and 
<Specific Disease> cytomegalovirus </> were 
negative.  
2. Do not annotate gender.  
Tokens such as ?male? and ?female? were only 
included if they specifically identified a new 
form of the disease, for example ?male breast 
cancer?.  
3. Do not annotate overlapping mentions. 
For example, the phrase ?von Hippel-Lindau 
(VHL) disease? was annotated as one single dis-
ease mention. 
4. Do not annotate general terms.  
Very general terms such as: disease, syndrome, 
deficiency, complications, abnormalities, etc. 
were excluded. However, the terms cancer and 
tumor were retained. 
5. Do not annotate references to biological 
processes.  
For example, terms corresponding to biological 
processes such as ?tumorigenesis? or ?cancero-
genesis?.  
6. Do not annotate disease mentions inter-
rupted by nested mentions.  
Basically, do not break the contiguous text 
rule. E.g. WT1 dysfunction is implicated in both 
neoplastic (Wilms tumor, mesothelioma, leuke-
mia, and breast cancer) and nonneoplastic (glo-
merulosclerosis) disease. 
In this example, the list of all disease mentions 
includes: ?neoplastic disease? and ?nonneo-
plastic disease? in addition to the underlined 
mentions. However, they were not annotated in 
our corpus, because other tokens break up the 
phrase. 
2.3 Annotators and the annotation process 
The annotator group consisted of 12 people with 
background in biomedical informatics research and 
experience in biomedical text corpus annotation. 
The 793 PubMed citations were divided into sets 
of 25 PubMed citations each. Every annotator 
worked on 5 or 6 sets of 25 PubMed abstracts. The 
sets were divided randomly among annotators. 
Each set was shared by two people to annotate. To 
avoid annotator bias, pairs of annotators were cho-
sen randomly for each set of 25 PubMed abstracts.  
As illustrated in Figure 1, first, each abstract 
was pre-annotated using our in-house-developed 
CRF disease mention recognizer trained on the 
AZDC corpus. This process involved a 10-fold 
 
Figure 1. The annotation process 
 
94
cross-validation scheme, where all sentences from 
the same PubMed abstract were assigned to the 
same split. The learning was performed on 9-folds 
and then, the PubMed abstracts assigned to the 
10th fold were annotated for disease mentions on a 
sentence-by-sentence basis.  
Annotation Phase I consisted of each pre-
annotated abstract in the corpus being read and 
reviewed by two annotators working independent-
ly. Annotators could agree with the pre-annotation, 
remove it, or adjust its text span. Annotators could 
also add new annotations. After this initial round 
of annotations, a summary document was created 
highlighting the agreement and differences be-
tween two annotators in the annotations they pro-
duced for each abstract. This constituted the end of 
phase I. The pair of annotators working on the 
same set at this stage was given the summary doc-
ument and their own annotations of Phase I. 
In annotation Phase II, each annotator examined 
and edited his or her own annotations by reviewing 
the different annotations reported in the Phase I 
summary document. This resulted in a new set of 
annotations. After this round, a second summary 
document highlighting the agreement and differ-
ences between two annotators was created for each 
pair of annotators to review.  
After phase II, each pair of annotators organized 
meetings where they reviewed, discussed and re-
solved their differences. After these meetings, a 
reconciled set of annotations was produced for 
each PubMed abstract. The final stage of the anno-
tation process consisted of the first author going 
over all annotated segments and ensuring that an-
notations were consistent both in category and in 
text span across different abstracts and different 
annotation sets. For example if the phrase ?classi-
cal galactosemia? was annotated in one abstract as 
a Specific Disease mention, all occurrences of that 
phrase throughout the corpus should receive con-
sistent annotation. Identified hard cases were dis-
cussed at a meeting where all annotators were 
present and a final decision was made to reconcile 
differences. The final corpus is available at: 
http://www.ncbi.nlm.nih.gov/CBBresearch/Fellow
s/Dogan/disease.html 
 
Figure 2. NCBI corpus annotation software. Each annotator selects a PubMed ID from the current 
working set, and is directed to this screen. Annotation categories are: Specific Disease (highlighted in 
yellow), Disease Class (green), Composite Mention (blue), or Modifier (purple). To annotate a disease 
mention in text, annotators highlight the phrase and click on the appropriate label on top of the editor 
screen. To delete a disease mention, annotators highlight the phrase and click on the Clear label on top 
of the editor. Annotators can retrieve the last saved version of their annotations for each particular 
document by clicking on ?Last Saved? button. Annotators save their work by clicking on Submit but-
ton at the bottom of editor screen.  
 
95
2.4 Annotation software 
Annotation was done using a web interface (the 
prototype of PubTator (Wei et al, 2012)), as 
shown in Figure 2. Each annotator was able to log 
into the system and work independently. The sys-
tem allowed flexibility to make annotations in the 
defined categories, modify annotations, correct the 
text span, delete as well as go back and review the 
process as often as needed. At the end of each an-
notation phase, annotators saved their work, and 
the annotation results were compared to find 
agreement and consistency among annotations.  
2.5 Annotation evaluation metrics  
We measured the annotators? agreement at phase I 
and II of the annotation process. One way to meas-
ure the agreement between two annotators is to 
measure their observed agreement on the sample of 
annotated items, as specified in Equation (1).  
Agreement statistics are measured for each an-
notator pair, for each shared annotation set. Then, 
for each annotator pair the average agreement sta-
tistic is computed over all annotation sets shared 
between the pair of annotators. The final agree-
ment statistic reflects the average and standard de-
viation computed over all annotator pairs. This is 
repeated for both phases.  
Agreement between two annotators is measured 
on two levels: one, both annotators tag the same 
exact phrase based on character indices as a dis-
ease mention, and two, both annotators tag the 
same exact phrase based on character indices as a 
disease mention of the same category. 
2.6  Application of the NCBI corpus 
To compare the two disease corpora with regard to 
their intended primary use in training and testing 
machine learning algorithms, we performed a 10-
fold cross validation experiment with BANNER 
(Leaman et al 2009). We evaluated BANNER per-
formance and compared Precision, Recall and F-
score values for BANNER when trained and tested 
on AZDC corpus and the NCBI disease name cor-
pus, respectively. In these experiments, disease 
mentions of all categories were included and are 
discussed in the Results section.  
To compare the effect of improvement in dis-
ease name recognition, the different disease cate-
gory annotations present in the NCBI corpus were 
        
Figure 3 Inter-annotator annotation consistency measured at the span and span-category level 
 
Table 3 The annotation results and corpus characteristics 
Characteristics of the corpus NCBI corpus AZDC 
Annotators 12 1 
Annotated sentences in citation ALL Selected 
PubMed Citations 793 793 
Sentences 6,651 2,784 
Sentences with disease annotations 3,752 1,757 
Total disease mentions 6,900 3,228 
Specific Disease 3,924 - 
Disease Class 1029 - 
Modifier 1,774 - 
Composite Mention 173 - 
 
96
Table 4 NCBI corpus as training, development and testing sets for disease name recognition 
Corpus Characteristics  Training set Development set Test set 
PubMed Citations 593 100 100 
Total disease mentions 5148 791 961 
Specific Disease 2959 409 556 
Disease Class 781 127 121 
Modifier 1292 218 264 
Composite Mention 116 37 20 
 
flattened into only one single category. This made 
the NCBI corpus compatible with the AZDC cor-
pus. 
3 Results and Discussion 
3.1 Results of Inter-Annotator Agreement  
Figure 3 shows the inter-annotator agreement re-
sults after Phase I and Phase II of the annotations. 
These statistics show a good agreement between 
annotators, especially after phase II of annotations. 
In particular, both span-consistency measure and 
span-category consistency measure is above 80% 
after phase II. These values show that our corpus 
reflects a high quality of annotations and that our 
two-stage annotation steps are effective in improv-
ing corpus consistency.  
3.2 Agreement between automatic pre-
annotation and final annotation results 
In our previous work (Neveol et al 2009) we have 
shown that automatic pre-annotation is found help-
ful by most annotators in assisting large-scale an-
notation projects with regard to speeding up the 
annotation time and improving annotation con-
sistency while maintaining the high quality of the 
final annotations. Thus, we again used pre-
annotation in this work. To demonstrate that hu-
man annotators were not biased towards the com-
puter-generated pre-annotation, we compared the 
final annotation with the pre-annotation results. 
There are a total of 3295 pre-annotated disease 
mentions: 1750 were found also in the final corpus 
while the remaining 1545 were either modified or 
deleted. Furthermore, the final corpus consists of 
additional 3605 new annotations. Overall, the 
agreement between pre-annotation and final anno-
tation results is only 35%. 
3.3 Statistics of the NCBI disease corpus 
After two rounds of annotation, several annotator 
meetings and resolving of inconsistencies, the 
NCBI corpus contains 793 fully annotated PubMed 
citations for disease mentions which are divided 
into these categories: Specific Disease, Disease 
Class, Composite Mention and Modifier. As shown 
in Table 3, the NCBI corpus contains more than 
6K sentences, of which more than half contain dis-
ease mentions. There are 2,161 unique disease 
mentions total, which can be divided into these 
categories: 1,349 unique Specific Disease men-
tions, 608 unique Disease Class mentions, 121 
unique Composite Disease mentions, and 356 
unique Modifier disease mentions. The NCBI dis-
ease name corpus is available for download and 
can be used for development of disease name 
recognition tools, identification of Composite Dis-
ease Mentions, Disease Class or Modifier disease 
mention in biomedical text. 
3.4 Characteristics of the NCBI corpus 
This annotation task was initially undertaken for 
purposes of creating a larger, broader and more 
complete corpus for disease name recognition in 
biomedical literature.  
The NCBI corpus addresses the inconsistencies 
of missed annotations by using a pool of experts 
for annotation and creating the annotation envi-
ronment of multiple discussions and multiple 
rounds of annotation. The NCBI corpus addresses 
the problem of recognition of abbreviated disease 
mentions by delivering annotations for all sentenc-
es in the PubMed abstract. Processing all sentences 
in a document allows for recognition of an abbre-
viated form of a disease name. An abbreviated 
term could be tagged for later occurrences within 
the same document, if an abbreviation definition is 
recognized in one of the preceding sentences.  
NCBI corpus provides a richer level of annota-
tions characterized by four different categories of 
disease mentions: Specific Disease, Disease Class, 
(1) 
s2Annotation12100  ???? sAn otation
AgreementyConsistenc
 
97
Composite Mention and Modifier. Specific Disease 
mentions could be linked to one specific definition 
without further categorization, allowing for future 
normalization tasks. Composite Disease Mentions 
identify intricate lexical strings that express two or 
more disease mentions, allowing for future natural 
language processing tasks to look at them more 
closely. Modifier disease mentions identify non-
noun phrase mentions, again useful for other text 
mining tasks. 
Finally, the corpus can be downloaded and used 
for development and testing for disease name 
recognition and other tasks. To facilitate future 
work, we have divided the corpus into training, 
development and testing sets as shown in Table 4. 
 
3.5 The NCBI corpus as training data for 
disease mention recognition 
We replicated the BANNER experiments by com-
paring their cross-validation results on the original 
corpus (AZDC) and on the NCBI corpus. Our re-
sults reveal that BANNER achieves significantly 
better performance on the NCBI corpus: a 10% 
increase in F-score from 0.764 to 0.840.  Table 5 
shows detailed results for BANNER processing in 
precision, recall and F-score, for both corpora. 
In addition, we performed BANNER experi-
ments on the newly divided NCBI corpus with the 
following results: BANNER achieves an F-score of 
0.845 on a 10 fold cross-validation experiment on 
the NCBI training set, an F-score of 0.819 when 
tested on the NCBI development set, after trained 
on the NCBI training set, and an F-score of 0.818 
when tested on NCBI test set, after trained on 
NCBI training set.   
 
3.6 Limitations of this work 
The NCBI corpus was annotated manually, thus 
the tags assigned were judgment calls by human 
annotators. Annotation guidelines were established 
prior to the annotation process and they were re-
fined during the annotation process, however grey 
areas still remained for which no explicit rules 
were formulated. In particular, inclusion of qualita-
tive terms as part of the disease mention is a matter 
of further investigation as illustrated by the follow-
ing example:  
? Acute meningococcal pericarditis ? Consti-
tutes a disease mention and, exists as a 
separate concept in UMLS, however 
? Acute Neisseria infection ? May or may 
not include the descriptive adjective.  
Similarly: 
? Classical galactosemia ? Includes the de-
scriptive adjective, because it corresponds 
to a particular form of the disease. 
? Inherited spinocerebellar ataxia ? May or 
may not include the descriptive adjective. 
Names containing conjunctions are difficult to 
tag. Although it might seem excessive to require a 
named entity recognizer to identify the whole ex-
pression for cases such as:  
? Adenomatous polyps of the colon and rec-
tum, 
? Fibroepithelial or epithelial hyperplasias, 
? Stage II or stage III colorectal cancer, 
The NCBI disease name corpus rectifies this sit-
uation by annotating them as Composite Mention 
disease name category, thus, allowing for future 
NLP application to develop more precise methods 
in identifying these expressions.  
Moreover, sentences which contained nested 
disease names require further attention, as the cur-
rent annotation rule of annotating only contiguous 
phrases cannot select the outer mentions. 
Finally, our current annotation guideline re-
quires that only one of the four categories be as-
signed to each disease mention. This is not ideal 
because a disease mention may actually fit more 
than one category. For instance, a mention can be 
tagged as both ?Modifier? and ?Disease Class?. In 
practice, for obtaining consistent annotations, the 
priority was given in the order of ?Modifier?, 
?Composite Mention?, ?Disease Class?, and ?Spe-
cific Disease? when more than one category deems 
appropriate. This aspect should be addressed at 
future work.   
4 Conclusions 
We have described the NCBI disease name corpus 
of tagged disease mentions in 793 PubMed titles 
and abstracts. The corpus was designed to capture 
Table 5 BANNER evaluation results on AZDC 
(original) corpus and on the NCBI corpus. 
CRF-
order 
Corpus Precision Recall F-score 
1 AZDC 0.788 0.743 0.764 
1 NCBI 0.859 0.824 0.840 
2 AZDC 0.804 0.752 0.776 
2 NCBI 0.857 0.820 0.838 
 
98
disease mentions in the most common sense of the 
word, and is particularly relevant for biomedical 
information retrieval tasks that involve diseases. 
Annotations were performed for all sentences in a 
document, facilitating the future applications of 
complex information retrieval tasks connecting 
diseases to treatments, causes or other types of in-
formation. Annotation guidelines were designed 
with the goal of allowing flexible matching to 
UMLS concepts, while retaining true meaning of 
the tagged concept. A more detailed definition on 
what constitutes a disease name, accompanied with 
additional annotation rules, could help resolve 
some existing inconsistencies.  The current corpus 
is reviewed several times by several annotators and 
describes a refined scale of annotation categories. 
It allows the separate definition and annotation of 
Composite mentions, Modifiers and distinguishes 
between Disease Class mentions versus Specific 
Diseases. The corpus is available for download1. 
Acknowledgments 
Funding: This research was supported by the Intramural 
Research Program of the NIH, National Library of Med-
icine.  
We sincerely thank Robert Leaman and Graciela Gon-
zalez for their help with BANNER, and the whole team 
of 12 annotators for their time and expertise on annota-
tion of this corpus.   
References  
Aronson, A., Lang, F. 2010. An overview of MetaMap: 
historical perspective and recent advances. J Am Med 
Inform Assoc, 17(3): 229-236. 
Burgun, A., Bodenreider, O. 2008. Accessing and inte-
grating data and knowledge for biomedical research. 
Yearb Med Inform, 91-101. 
Campos, D., Matos, S., Lewin, I., Oliveira, J., Rebholz-
Schuhmann, D. 2012. Harmonisation of gene/protein 
annotations: towards a gold standard MEDLINE. Bi-
oinformatics, 1;28(9):1253-61 
Chapman, W.W., Savova, G.K., Zheng, J., Tharp, M., 
Crowley, R. 2012. Anaphoric reference in clinical re-
ports: Characteristics of an annotated corpus. J Bio-
med Inform  
Chowdhury, F.M., Lavelli, A. 2010. Disease mention 
recognition with specific features. BioNLP, 91-98. 
Grouin, C., Rosset. S., Zweigenbaum, P., Fort, K., Gali-
bert, O., Quintard, L. 2011. Proposal for an extension 
                                                          
1 
http://www.ncbi.nlm.nih.gov/CBBresearch/Fellows/Dogan/dis
ease.html 
of traditional named entities: From guidelines to 
evalua-tion, an overview. 5th law workshop, 92-100.  
Islamaj Dogan, R., Murray, G. C., Neveol, A., Lu, Z. 
2009. Understanding PubMed user search behavior 
through log analysis. Database (Oxford): bap018. 
Jimeno,A., Jimnez-Ruiz, E., Lee, V., Gaudan, S., Ber-
langa,R., Reholz-Schuhmann, D.2008. Assessment of 
disease named entity recognition on a corpus of an-
no-tated sentences. BMC Bioinformatics, 9(S-3). 
Leaman, R., Miller, C., Gonzalez, G. 2009. Enabling 
Recognition of Diseases in Biomedical Text with 
Ma-chine Learning: Corpus and Benchmark. Sympo-
sium on Languages in Biology and Medicine, 82-89.  
Neveol, A., Li, J., Lu, Z. 2012. Linking Multiple Dis-
ease-related resources through UMLS. ACM Interna-
tional Health Informatics. 
Neveol, A., Islamaj Dogan, R., Lu, Z. 2011. Semi-
automatic semantic annotation of PubMed Queries: a 
study on quality, efficiency, satisfaction. J Biomed 
Inform, 44(2):310-8. 
Rinaldi, F., Kaljurand, K., S?tre, R. 2011. Terminologi-
cal resources for text mining over biomedical scien-
tific literature. Artificial intelligence in medicine 
52(2) 
Smith L., Tanabe L.K., Ando R.J., Kuo C.J., Chung I.F., 
Hsu C.N., Lin Y.S., Klinger R., Friedrich C.M., 
Ganchev K., Torii M., Liu H., Haddow B., Struble 
C.A., Povinelli R.J., Vlachos A., Baumgartner W.A. 
Jr., Hunter L., Carpenter B., Tsai R.T., Dai H.J., Liu 
F., Chen Y., Sun C., Katrenko S., Adriaans P., 
Blaschke C., Torres R., Neves M., Nakov P., Divoli 
A., Ma?a-L?pez M., Mata J., Wilbur W.J. 
2008.Overview of BioCreative II gene mention 
recognition. Genome Biology, 9 Suppl 2:S2. 
Tanabe, L., Xie, N., Thom, L., Matten, W., Wilbur, W.J. 
2005. GENETAG: a tagged corpus for gene /protein 
named entity recognition. BMC Bioinformatics, 6:S3. 
Thompson, P., Iqbal, S.A., McNaught, J., Ananiadou, S. 
2009. Construction of an annotated corpus to support 
biomedical information extraction. BMC Bioinfor-
matics, 10:349. 
Warren A., Kibbe J.D.O., Wolf W.A., Smith M.E., Zhu L., 
Lin S., Chisholm R., Disease Ontology. 2006 
Wei C., Kao, H., Lu, Z., 2012. PubTator: A PubMed-
like interactive curation system for document triage 
and literature Curation. In proceedings of BioCrea-
tive workshop, 145-150.  
Yeganova, L., Comeau, D.C., Wilbur, W.J. 2011. Ma-
chine learning with naturally labeled data for identi-
fying abbreviation definitions. BMC Bioinformatics.  
S3:S6 
Yeh, A., Morgan, A., Colosime, M., Hirschman, L. 
2005. BioCreAtIvE Task 1A: gene mention finding 
evaluation. BMC Bioinformatics, 6(Suppl 1):S2 
99
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 99?103,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
BioNLP Shared Task 2013: Supporting Resources
Pontus Stenetorp 1 Wiktoria Golik 2 Thierry Hamon 3
Donald C. Comeau 4 Rezarta Islamaj Dog?an 4 Haibin Liu 4 W. John Wilbur 4
1 National Institute of Informatics, Tokyo, Japan
2 French National Institute for Agricultural Research (INRA), Jouy-en-Josas, France
3 University Paris 13, Paris, France
4 National Center for Biotechnology Information, National Library of Medicine,
National Institutes of Health, Bethesda, MD, USA
pontus@nii.ac.jp wiktoria.golik@jouy.inra.fr thierry.hamon@univ-paris13.fr
{comeau,islamaj,liuh11,wilbur}@ncbi.nlm.nih.gov
Abstract
This paper describes the technical con-
tribution of the supporting resources pro-
vided for the BioNLP Shared Task 2013.
Following the tradition of the previous
two BioNLP Shared Task events, the task
organisers and several external groups
sought to make system development easier
for the task participants by providing auto-
matically generated analyses using a vari-
ety of automated tools. Providing analy-
ses created by different tools that address
the same task also enables extrinsic evalu-
ation of the tools through the evaluation of
their contributions to the event extraction
task. Such evaluation can improve under-
standing of the applicability and benefits
of specific tools and representations. The
supporting resources described in this pa-
per will continue to be publicly available
from the shared task homepage
http://2013.bionlp-st.org/
1 Introduction
The BioNLP Shared Task (ST), first organised in
2009, is an ongoing series of events focusing on
novel challenges in biomedical domain informa-
tion extraction. In the first BioNLP ST, the or-
ganisers provided the participants with automat-
ically generated syntactic analyses from a variety
of Natural Language Processing (NLP) tools (Kim
et al, 2009) and similar syntactic analyses have
since then been a key component of the best per-
forming systems participating in the shared tasks.
This initial work was followed up by a similar ef-
fort in the second event in the series (Kim et al,
2011), extended by the inclusion of software tools
and contributions from the broader BioNLP com-
munity in addition to task organisers (Stenetorp et
al., 2011).
Although no formal study was carried out to es-
timate the extent to which the participants utilised
the supporting resources in these previous events,
we note that six participating groups mention us-
ing the supporting resources in published descrip-
tions of their methods (Emadzadeh et al, 2011;
McClosky et al, 2011; McGrath et al, 2011;
Nguyen and Tsuruoka, 2011; Bjo?rne et al, 2012;
Vlachos and Craven, 2012). These resources have
been available also after the original tasks, and
several subsequent studies have also built on the
resources. Van Landeghem et al (2012) applied a
visualisation tool that was made available as a part
of the supporting resources, Vlachos (2012) em-
ployed the syntactic parses in a follow-up study
on event extraction, Van Landeghem et al (2013)
used the parsing pipeline created to produce the
syntactic analyses, and Stenetorp et al (2012) pre-
sented a study of the compatibility of two different
representations for negation and speculation anno-
tation included in the data.
These research contributions and the overall
positive reception of the supporting resources
prompted us to continue to provide supporting re-
sources for the BioNLP Shared Task 2013. This
paper presents the details of this technical contri-
bution.
2 Organisation
Following the practice established in the
BioNLP ST 2011, the organisers issued an
open call for supporting resources, welcoming
contributions relevant to the task from all authors
of NLP tools. In the call it was mentioned that
points such as availability for research purposes,
support for well-established formats and access
99
Name Annotations Availability
BioC Lemmas and syntactic constituents Source
BioYaTeA Terms, lemmas, part-of-speech and syntactic constituencies Source
Cocoa Entities Web API
Table 1: Summary of tools/analyses provided by external groups.
to technical documentation would considered
favourable (but not required) and each supporting
resource provider was asked to write a brief
description of their tools and how they could
potentially be applied to aid other systems in the
event extraction task. This call was answered
by three research groups that offered to provide
a variety of semantic and syntactic analyses.
These analyses were provided to the shared
task participants along with additional syntactic
analyses created by the organisers.
However, some of the supporting resource
providers were also participants in the main event
extraction tasks, and giving them advance access
to the annotated texts for the purpose of creating
the contributed analyses could have given those
groups an advantage over others. To address this
issue, the texts were made publicly available one
week prior to the release of the annotations for
each set of texts. During this week, the supporting
analysis providers annotated the texts using their
automated tools and then handed the analyses to
the shared task organisers, who made them avail-
able to the task participants via the shared task
homepage.
3 Analyses by External Groups
This section describes the tools that were applied
to create supporting resources by the three exter-
nal groups. These contributions are summarised in
Table 1.
BioC Don Comeau, Rezarta Islamaj, Haibin
Liu and John Wilbur of the National Center for
Biotechnology Information provided the output of
the shallow parser MedPost (Smith et al, 2004)
and the BioLemmatizer tool (Liu et al, 2012),
supplied in the BioC XML format1 for annota-
tion interchange (Comeau et al, 2013). The BioC
format address the problem of interoperability be-
tween different tools and platforms by providing a
unified format for use by various tools. Both Med-
Post and BioLemmatizer are specifically designed
1http://bioc.sourceforge.net/
for biomedical texts. The former annotates parts-
of-speech and performs sentence splitting and to-
kenisation, while the latter performs lemmatisa-
tion. In order to make it easier for participants
to get started with the BioC XML format, the
providers also supplied example code for parsing
the format in both the Java and C++ programming
languages.
BioYaTeA Wiktoria Golik of the French Na-
tional Institute for Agricultural Research (INRA)
and Thierry Hamon of University Paris 13 pro-
vided analyses created by BioYaTeA2 (Golik et
al., 2013). BioYaTeA is a modified version of the
YaTeA term extraction tool (Aubin and Hamon,
2006) adapted to the biomedical domain. Working
on a noun-phrase level, BioYaTeA provides anno-
tations such as lemmas, parts-of-speech, and con-
stituent analysis. The output formats used were a
simple tabular format as well as BioYaTeA-XML,
an XML representation specific to the tool.
Cocoa S. V. Ramanan of RelAgent Private Ltd
provided the output of the Compact cover anno-
tator (Cocoa) for biological noun phrases.3 Co-
coa provides noun phrase-level entity annotations
for over 20 different semantic categories such as
macromolecules, chemicals, proteins and organ-
isms. These annotations were made available for
the annotated texts for the shared task along with
the opportunity for the participants to use the Co-
coa web API to annotate any text they may con-
sider beneficial for their system. The data format
used by Cocoa is a subset of the standoff format
used for the shared task entity annotations, and it
should thus be easy to integrate into existing event
extraction systems.
4 Analyses by Task Organisers
This section describes the syntactic parsers ap-
plied by the task organisers and the pre-processing
2http://search.cpan.org/?bibliome/
Lingua-BioYaTeA/
3http://npjoint.com/
100
Name Model Availability
Enju Biomedical Binary
Stanford Combination Binary, Source
McCCJ Biomedical Source
Table 2: Parsers used for the syntactic analyses.
and format conversions applied to their output.
The applied parsers are listed in Table 2.
4.1 Syntactic Parsers
Enju Enju (Miyao and Tsujii, 2008) is a deep
parser based on the Head-Driven Phrase Struc-
ture Grammar (HPSG) formalism. Enju analyses
its input in terms of phrase structure trees with
predicate-argument structure links, represented in
a specialised XML-format. To make the analyses
of the parser more accessible to participants, we
converted its output into the Penn Treebank (PTB)
format using tools included with the parser. The
use of the PTB format also allow for its output to
be exchanged freely for that of the other two syn-
tactic parsers and facilitates further conversions
into dependency representations.
McCCJ The BLLIP Parser (Charniak and John-
son, 2005), also variously known as the Charniak
parser, the Charniak-Johnson parser, or the Brown
reranking parser, has been applied in numerous
biomedical domain NLP efforts, frequently using
the self-trained biomedical model of McClosky
(2010) (i.e. the McClosky-Charniak-Johnson or
McCCJ parser). The BLLIP Parser is a con-
stituency (phrase structure) parser and the applied
model produces PTB analyses as its native out-
put. These analyses were made available to par-
ticipants without modification.
Stanford The Stanford Parser (Klein and Man-
ning, 2002) is a widely used publicly available
syntactic parser. As for the Enju and BLLIP
parsers, a model trained on a dataset incorporating
biomedical domain annotations is available also
for the Stanford parser. Like the BLLIP parser,
the Stanford parser is constituency-based and pro-
duces PTB analyses, which were provided to task
participants. The Stanford tools additionally in-
corporate methods for automatic conversion from
this format to other representations, discussed fur-
ther below.
4.2 Pre-processing and Conversions
To create the syntactic analyses from the Enju,
BLLIP and Stanford Parser systems, we first ap-
plied a uniform set of pre-processing steps in order
to normalise over differences in e.g. tokenisation
and thus ensure that the task participants can eas-
ily swap the output of one system for another. This
pre-processing was identical to that applied in the
BioNLP 2011 Shared Task, and included sentence
splitting of the annotated texts using the Genia
Sentence Splitter,4 the application of a set of post-
processing heuristics to correct frequently occur-
ring sentence splitting errors, and Genia Treebank-
like tokenisation (Tateisi et al, 2004) using a to-
kenisation script created by the shared task organ-
isers. 5
Since several studies have indicated that repre-
sentations of syntax and aspects of syntactic de-
pendency formalism differ in their applicability to
support information extraction tasks (Buyko and
Hahn, 2010; Miwa et al, 2010; Quirk et al, 2011),
we further converted the output of each of the
parsers from the PTB representation into three
other representations: CoNNL-X, Stanford De-
pendencies and Stanford Collapsed Dependencies.
For the CoNLL-X format we employed the con-
version tool of Johansson and Nugues (2007), and
for the two Stanford Dependency variants we used
the converter provided with the Stanford CoreNLP
tools (de Marneffe et al, 2006). These analyses
were provided to participants in the output for-
mats created by the respective tools, i.e. the TAB-
separated column-oriented format CoNLL and the
custom text-based format of the Stanford Depen-
dencies.
5 Results and Discussion
Just like in previous years the supporting resources
were well-received by the shared task participants
and as many as five participating teams mentioned
utilising the supporting resources in their initial
submissions (at the time of writing, the camera-
ready versions were not yet available). This level
of usage of the supporting resources by the partici-
pants is thus comparable to what was observed for
the 2011 shared task.
Following in the tradition of the 2011 support-
4https://github.com/ninjin/geniass
5https://github.com/ninjin/bionlp_
st_2013_supporting/blob/master/tls/
GTB-tokenize.pl
101
ing resources, to aim for reproducibility, the pro-
cessing pipeline containing pre/post-processing
and conversion scripts for all the syntactic parses
has been made publicly available under an open
licence.6 The repository containing the pipeline
also contains detailed instructions on how to re-
produce the output and how it can potentially be
applied to other texts.
Given the experience of the organisers in
analysing medium-sized corpora with a variety of
syntactic parsers, many applied repeatedly over
several years, we are also happy to report that the
robustness of several publicly available parsers has
recently improved noticeably. Random crashes,
corrupt outputs and similar failures appear to be
transitioning from being expected to rare occur-
rences.
In this paper, we have introduced the supporting
resources provided for the BioNLP 2013 Shared
Task by the task organisers and external groups.
These resources included both syntactic and se-
mantic annotations and were provided to allow the
participants to focus on the various novel chal-
lenges of constructing event extraction systems by
minimizing the need for each group to separately
perform standard processing steps such as syntac-
tic analysis.
Acknowledgements
We would like to give special thanks to Richard
Johansson for providing and allowing us to dis-
tribute an improved and updated version of his for-
mat conversion tool.7 We would also like to ex-
press our appreciation to the broader NLP com-
munity for their continued efforts to improve the
availability of both code and data, thus enabling
other researchers to stand on the shoulders of gi-
ants.
This work was partially supported by the
Quaero programme funded by OSEO (the French
agency for innovation). The research of Donald
C. Comeau, Rezarta Islamaj Dog?an, Haibin Liu
and W. John Wilbur was supported by the Intra-
mural Research Program of the National Institutes
of Health (NIH), National Library of Medicine
(NLM).
6https://github.com/ninjin/bionlp_st_
2013_supporting
7https://github.com/ninjin/
pennconverter
References
Sophie Aubin and Thierry Hamon. 2006. Improving
term extraction with terminological resources. In
Advances in Natural Language Processing, pages
380?387. Springer.
Jari Bjo?rne, Filip Ginter, and Tapio Salakoski. 2012.
University of Turku in the BioNLP?11 Shared Task.
BMC Bioinformatics, 13(Suppl 11):S4.
Ekaterina Buyko and Udo Hahn. 2010. Evaluating
the Impact of Alternative Dependency Graph Encod-
ings on Solving Event Extraction Tasks. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing, pages 982?992,
Cambridge, MA, October.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 173?180. Association for Computational Lin-
guistics.
Donald C. Comeau, Rezarta Islamaj Dog?an, Paolo Ci-
ccarese, Kevin Bretonnel Cohen, Martin Krallinger,
Florian Leitner, Zhiyong Lu, Yifan Peng, Fabio Ri-
naldi, Manabu Torii, Alfonso Valencia, Karin Ver-
spoor, Thomas C. Wiegers, Cathy H. Wu, and
W. John Wilbur. 2013. BioC: A minimalist ap-
proach to interoperability for biomedical text pro-
cessing. submitted.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC, volume 6, pages 449?454.
Ehsan Emadzadeh, Azadeh Nikfarjam, and Graciela
Gonzalez. 2011. Double layered learning for bio-
logical event extraction from text. In Proceedings
of the BioNLP Shared Task 2011 Workshop, pages
153?154. Association for Computational Linguis-
tics.
Wiktoria Golik, Robert Bossy, Zorana Ratkovic, and
Claire Ne?dellec. 2013. Improving Term Extraction
with Linguistic Analysis in the Biomedical Domain.
In Special Issue of the journal Research in Comput-
ing Science, Samos, Greece, March. 14th Interna-
tional Conference on Intelligent Text Processing and
Computational Linguistics.
Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for
English. In Proc. of the 16th Nordic Conference
on Computational Linguistics (NODALIDA), pages
105?112.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction. In
Proceedings of the BioNLP 2009 Workshop Com-
panion Volume for Shared Task, pages 1?9, Boulder,
Colorado, June. Association for Computational Lin-
guistics.
102
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011. Overview of Genia Event
Task in BioNLP Shared Task 2011. In Proceedings
of BioNLP Shared Task 2011 Workshop.
Dan Klein and Christopher D Manning. 2002. Fast ex-
act inference with a factored model for natural lan-
guage parsing. Advances in neural information pro-
cessing systems, 15(2003):3?10.
Haibin Liu, Tom Christiansen, William Baumgartner,
and Karin Verspoor. 2012. BioLemmatizer: a
lemmatization tool for morphological processing of
biomedical text. Journal of Biomedical Semantics,
3(1):3.
David McClosky, Mihai Surdeanu, and Christopher D
Manning. 2011. Event Extraction as Dependency
Parsing for BioNLP 2011. In Proceedings of the
BioNLP Shared Task 2011 Workshop, pages 41?45.
Association for Computational Linguistics.
David McClosky. 2010. Any domain parsing: Auto-
matic domain adaptation for natural language pars-
ing. Ph.D. thesis, Brown University.
Liam R McGrath, Kelly Domico, Courtney D Cor-
ley, and Bobbie-Jo Webb-Robertson. 2011. Com-
plex biological event extraction from full text us-
ing signatures of linguistic and semantic features.
In Proceedings of the BioNLP Shared Task 2011
Workshop, pages 130?137. Association for Compu-
tational Linguistics.
Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and
Jun?ichi Tsujii. 2010. Evaluating Dependency Rep-
resentations for Event Extraction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (Coling 2010), pages 779?787,
Beijing, China, August.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Compu-
tational Linguistics, 34(1):35?80.
Nhung TH Nguyen and Yoshimasa Tsuruoka. 2011.
Extracting bacteria biotopes with semi-supervised
named entity recognition and coreference resolution.
In Proceedings of the BioNLP Shared Task 2011
Workshop, pages 94?101. Association for Compu-
tational Linguistics.
Chris Quirk, Pallavi Choudhury, Michael Gamon, and
Lucy Vanderwende. 2011. MSR-NLP Entry in
BioNLP Shared Task 2011. In Proceedings of
BioNLP Shared Task 2011 Workshop, pages 155?
163, Portland, Oregon, USA, June.
Larry Smith, Thomas Rindflesch, and W. John Wilbur.
2004. MedPost: a part-of-speech tagger for bio
medical text. Bioinformatics, 20(14):2320?2321.
Pontus Stenetorp, Goran Topic?, Sampo Pyysalo,
Tomoko Ohta, Jin-Dong Kim, and Jun?ichi Tsujii.
2011. BioNLP Shared Task 2011: Supporting Re-
sources. In Proceedings of BioNLP Shared Task
2011 Workshop, pages 112?120, Portland, Oregon,
USA, June.
Pontus Stenetorp, Sampo Pyysalo, Tomoko Ohta,
Sophia Ananiadou, and Jun?ichi Tsujii. 2012.
Bridging the gap between scope-based and event-
based negation/speculation annotations: a bridge not
too far. In Proceedings of the Workshop on Extra-
Propositional Aspects of Meaning in Computational
Linguistics, pages 47?56. Association for Computa-
tional Linguistics.
Y Tateisi, T Ohta, and J Tsujii. 2004. Annotation of
predicate-argument structure on molecular biology
text. Proceedings of the Workshop on the 1st In-
ternational Joint Conference on Natural Language
Processing (IJCNLP-04).
Sofie Van Landeghem, Kai Hakala, Samuel Ro?nnqvist,
Tapio Salakoski, Yves Van de Peer, and Filip Gin-
ter. 2012. Exploring biomolecular literature with
EVEX: connecting genes through events, homology,
and indirect associations. Advances in Bioinformat-
ics, 2012.
Sofie Van Landeghem, Jari Bjo?rne, Chih-Hsuan Wei,
Kai Hakala, Sampo Pyysalo, Sophia Ananiadou,
Hung-Yu Kao, Zhiyong Lu, Tapio Salakoski, Yves
Van de Peer, et al 2013. Large-scale event extrac-
tion from literature with multi-level gene normaliza-
tion. PloS one, 8(4):e55814.
Andreas Vlachos and Mark Craven. 2012. Biomedical
event extraction from abstracts and full papers using
search-based structured prediction. BMC Bioinfor-
matics, 13(Suppl 11):S5.
Andreas Vlachos. 2012. An investigation of imita-
tion learning algorithms for structured prediction. In
Workshop on Reinforcement Learning, page 143.
103
