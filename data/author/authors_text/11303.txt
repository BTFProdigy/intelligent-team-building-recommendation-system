A Structured Prediction Approach for Statistical Machine Translation
Dakun Zhang*          Le Sun?          Wenbo Li* 
*Institute of Software, Graduate University 
Chinese Academy of Sciences 
Beijing, China, 100080 
{dakun04,liwenbo02}@iscas.cn 
?Institute of Software 
Chinese Academy of Sciences 
Beijing, China, 100080 
sunle@iscas.cn 
 
 
Abstract 
We propose a new formally syntax-based 
method for statistical machine translation. 
Transductions between parsing trees are 
transformed into a problem of sequence 
tagging, which is then tackled by a search-
based structured prediction method. This 
allows us to automatically acquire transla-
tion knowledge from a parallel corpus 
without the need of complex linguistic 
parsing. This method can achieve compa-
rable results with phrase-based method 
(like Pharaoh), however, only about ten 
percent number of translation table is used. 
Experiments show that the structured pre-
diction approach for SMT is promising for 
its strong ability at combining words. 
1 Introduction 
Statistical Machine Translation (SMT) is attract-
ing more attentions than rule-based and example-
based methods because of the availability of large 
training corpora and automatic techniques. How-
ever, rich language structure is difficult to be inte-
grated in the current SMT framework. Most of the 
SMT approaches integrating syntactic structures 
are based on probabilistic tree transducers (tree-
to-tree model). This leads to a large increase in the 
model complexity (Yamada and Knight 2001; 
Yamada and Knight 2002; Gildea 2003; Galley et 
al. 2004; Knight and Graehl 2005; Liu et al 2006). 
However, formally syntax-based methods propose 
simple but efficient ways to parse and translate 
sentences (Wu 1997; Chiang 2005). 
In this paper, we propose a new model of SMT 
by using structured prediction to perform tree-to-
tree transductions. This model is inspired by Sa-
gae and Lavie (2005), in which a stack-based rep-
resentation of monolingual parsing trees is used. 
Our contributions lie in the extension of this rep-
resentation to bilingual parsing trees based on 
ITGs and in the use of a structured prediction 
method, called SEARN (Daum? III et al 2007), to 
predict parsing structures. 
Furthermore, in order to facilitate the use of 
structured prediction method, we perform another 
transformation from ITG-like trees to label se-
quence with the grouping of stack operations. 
Then the structure preserving problem in transla-
tion is transferred to a structured prediction one 
tackled by sequence labeling method such as in 
Part-of-Speech (POS) tagging. This transforma-
tion can be performed automatically without com-
plex linguistic information. At last, a modified 
search process integrating structure information is 
performed to produce sentence translation. Figure 
1 illustrates the process flow of our model. Be-
sides, the phrase extraction is constrained by ITGs. 
Therefore, in this model, most units are word 
based except that we regard those complex word 
alignments as a whole (i.e. phrase) for the simplic-
ity of ITG-like tree representations. 
B ilingual S en tences
G IZ A + +  T ra in ing
(B id irec tiona l)
W ord  A lignm en ts
(g row -d iag -fina l)
S truc tu red  Info rm ation
(T ra in ing  by  S E A R N )
L anguage  M odel
M ono lingual
S en tences
Search  e*
M ax im ize  P r(e)*P r(f|e )
Inpu t
Source L anguage
S en tence
O utpu t
T arge t L anguage
 S en tence
S tack -based  O pera tions
T rans la tion  M odel
IT G -like  T rees
 
Figure 1: Chart of model framework 
The paper is organized as follows: related work 
is show in section 2. The details of the transforma-
649
tion from word alignments to structured parsing 
trees and then to label sequence are given in sec-
tion 3. The structured prediction method is de-
scribed in section 4. In section 5, a beam search 
decoder with structured information is described. 
Experiments are given for three European lan-
guage pairs in section 6 and we conclude our pa-
per with some discussions. 
2 Related Work 
This method is similar to block-orientation model-
ing (Tillmann and Zhang 2005) and maximum 
entropy based phrase reordering model (Xiong et 
al. 2006), in which local orientations (left/right) of 
phrase pairs (blocks) are learned via MaxEnt clas-
sifiers. However, we assign shift/reduce labeling 
of ITGs taken from the shift-reduce parsing, and 
classifier is learned via SEARN. This paper is 
more elaborated by assigning detailed stack-
operations. 
The use of structured prediction to SMT is also 
investigated by (Liang et al 2006; Tillmann and 
Zhang 2006; Watanabe et al 2007). In contrast, 
we use SEARN to estimate one bilingual parsing 
tree for each sentence pair from its word corre-
spondences. As a consequence, the generation of 
target language sentences is assisted by this struc-
tured information. 
Turian et al (2006) propose a purely discrimi-
native learning method for parsing and translation 
with tree structured models. The word alignments 
and English parse tree were fed into the GenPar 
system (Burbank et al 2005) to produce binarized 
tree alignments. In our method, we predict tree 
structures from word alignments through several 
transformations without involving parser and/or 
tree alignments. 
3 Transformation 
3.1 Word Alignments and ITG-like Tree 
First, following Koehn et al (2003), bilingual sen-
tences are trained by GIZA++ (Och and Ney 2003) 
in two directions (from source to target and target 
to source). Then, two resulting alignments are re-
combined to form a whole according to heuristic 
rules, e.g. grow-diag-final. Second, based on the 
word alignment matrix, one unique parsing tree 
can be generated according to ITG constraints 
where the ?left-first? constraint is posed. That is to 
say, we always make the leaf nodes as the right 
sons as possible as they can. Here we present two 
basic operations for mapping tree items, one is in 
order and the other is in reverse order (see Figure 
2). Basic word alignments are in (a), while (b) is 
their corresponding alignment matrix. They can be 
described using ITG-like trees (c). 
f1 f1       f2
e1        *
e2                  * f1/e1 f2/e2
(1a) (1b) (1c)
f1       f2
e1                  *
e2        * f1/e2 f2/e1
(2a) (2b) (2c)
f1/e1 S
f2/e2 S,R+
(1d)
f1/e2 S
f2/e1 S,R-
(2d)
f2
f1 f2
e1 e2
e1 e2
 
Figure 2: Two basic representations for tree items 
 
Figure 3: ?inside-out? transpositions (a) and (b) with two 
typical complex sequences (c) and (d). In (c) and (d), word 
correspondence f2-e2 is also extracted as sub-alignments. 
The two widely known situations that cannot be 
described by ITGs are called ?inside-out? transpo-
sitions (Figure 3 a & b). Since they cannot be de-
composed in ITGs, we consider them as basic 
units. In this case, phrase alignment is used. In our 
model, more complex situations exist for the word 
correspondences are generated automatically from 
GIZA++. At the same time, we also keep the sub-
alignments in those complex situations in order to 
extend the coverage of translation options. The 
sub-alignments are restricted to those that can be 
described by the two basic operations. In other 
words, for our ITG-like tree, the nodes are mostly 
word pairs, except some indecomposable word 
sequences pairs. Figure 3 shows four typical com-
plex sequences viewed as phrases. 
Therefore, our ITG-like trees take some phrase 
alignments into consideration and we also keep 
the sub-alignments in these situations. Tree items 
in our model are restricted to minimum constitu-
ents for the simplicity of parsing tree generation. 
Then we extract those word pairs from tree items, 
instead of all the possible word sequences, as our 
translation table. In this way, we can greatly re-
duce the number of translation pairs to be consid-
eration. 
650
3.2 SHIFT and REDUCE Operations 
Sagae and Lavie (2005) propose a constituency-
based parsing method to determine sentence de-
pendency structures. This method is simple and 
efficient, which makes use of SHIFT and RE-
DUCE operations within a stack framework. This 
kind of representations can be easily learned by a 
classifier with linear time complexity. 
In their method, they build a parse tree of a sen-
tence one word at a time just as in a stack parser. 
At any time step, they either shift a new word on 
to the stack, or reduce the top two elements on the 
stack into a new non-terminal. 
Sagae and Lavie?s algorithms are designed for 
monolingual parsing problem. We extend it to 
represent our ITG-like tree. In our problem, each 
word pairs can be viewed as tree items (nodes). 
To handle our tree alignment problem, we need to 
define two REDUCE operations: REDUCE in 
order and REDUCE in reverse order. We define 
these three basic operations as follows: 
? S: SHIFT - push the current item onto the 
stack. 
? R+: REDUCE in order - pop the first two 
items from the stack, and combine them in 
the original order on the target side, then 
push back. 
? R-: REDUCE in reverse order - pop the 
first two items from the stack, and combine 
them in the reverse order on the target side, 
then push back. 
Using these operators, our ITG-like tree is 
transformed to serial stack operations. In Figure 2, 
(d) is such a representation for the two basic 
alignments. Therefore, the structure of word 
aligned sentences can be transformed to an opera-
tion sequence, which represents the bilingual pars-
ing correspondences. 
After that, we attach these operations to each 
corresponding tree item like a sequence labeling 
problem. We need to perform another ?grouping? 
step to make sure only one operation is assigned 
to each item, such as ?S,R+?, ?S,R-,R+?, etc. 
Then, those grouped operations are regarded as a 
whole and performed as one label. The number of 
this kind of labels is decided by the training cor-
pus1. Having defined such labels, the prediction of 
                                                 
1 This set of labels is quite small and only 16 for the French-
English training set with 688,031 sentences. 
tree structures is transformed to a label prediction 
one. That is, giving word pairs as input, we trans-
form them to their corresponding labels (stack 
operations) in the output. At the same time, tree 
transductions are encoded in those labels. Once all 
the ?labels? are performed, there should be only 
one element in the stack, i.e. the generating sen-
tence translation pairs. See Appendix A for a more 
complete example in Chinese-English with our 
defined operations. 
Another constraint we impose is to keep the 
least number of elements in stack at any time. If 
two elements on the top of the stack can be com-
bined, we combine them to form a single item. 
This constraint can avoid having too many possi-
ble operations for the last word pair, which may 
make future predictions difficult. 
4 Structured Prediction 
SEARN is a machine learning method proposed 
recently by Daum? III et al (2007) to solve struc-
tured prediction problems. It can produce a high 
prediction performance without compromising 
speed, simplicity and generality. By incorporating 
the search and learning process, SEARN can solve 
the complex problems without having to perform 
explicit decoding any more. 
In most cases, a prediction of input x in domain 
X into output y in domain Y, like SVM and deci-
sion trees, cannot keep the structure information 
during prediction. SEARN considers this problem 
as a cost sensitive classification one. By defining 
features and a loss function, it performs a cost 
sensitive learning algorithm to learn predictions. 
During each iteration, the optimal policy (decided 
by previous classifiers) generates new training 
examples through the search space. These data are 
used to adjust performance for next classifier. 
Then, iterations can keep this algorithm to per-
form better for prediction tasks. Structures are 
preserved for it integrates searching and learning 
at the same time.  
4.1 Parsing Tree Prediction 
For our problem, using SEARN to predict the 
stack-based ITG-like trees, given word alignments 
as input, can benefit from the advantages of this 
algorithm. With the structured learning method, 
we can account for the sentence structures and 
their correspondence between two languages at 
651
the same time. Moreover, it keeps the translating 
structures from source to target. 
As we have transformed the tree-to-tree transla-
tion problem into a sequence labeling one, all we 
need to solve is a tagging problem similar to a 
POS tagging (Daum? III et al 2006). The input 
sequence x is word pairs and output y is the group 
of SHIFT and REDUCE operations. For sequence 
labeling problem, the standard loss function is 
Hamming distance, which measures the difference 
between the true output and the predicting one: 
?=
t
tt yyyyHL )?,()?,( ?                 (1) 
where ? is 0 if two variables are equal, and 1 oth-
erwise. 
5 Decoder 
We use a left-to-right beam search decoder to find 
the best translation given a source sentence. Com-
pared with general phrase-based beam search de-
coder like Pharaoh (Koehn 2004), this decoder 
integrates structured information and does not 
need distortion cost and other costs (e.g. future 
costs) any more. Therefore, the best translation 
can be determined by: 
})()|({maxarg* )(elengthlm
e
epefpe ?=     (2) 
where ? is a factor of word length penalty. Simi-
larly, the translation probability  can be 
further decomposed into: 
)|( efp
?=
i
ii efefp )|()|( ?                  (3) 
and )|( ii ef?  represents the probability distribu-
tion of word pairs. 
Instead of extracting all possible phrases from 
word alignments, we consider those translation 
pairs from the nodes of ITG-like trees only. Like 
Pharaoh, we calculate their probability as a com-
bination of 5 constituents: phrase translation prob-
ability (in both directions), lexical translation 
probability (in both directions) and phrase penalty 
(default is set at 2.718). The corresponding weight 
is trained through minimum error rate method 
(Och 2003). Parameters of this part can be calcu-
lated in advance once tree structures are generated 
and can be stored as phrase translation table. 
5.1 Core Algorithm 
Another important question is how to preserve 
sentence structures during decoding. A left-to-
right monotonous search procedure is needed. 
Giving the source sentence, word translation can-
didates can be determined according to the trans-
lation table. Then, several rich features like cur-
rent and previous source words are extracted 
based on these translation pairs and source sen-
tence. After that, our structured prediction learn-
ing method will be used to predict the output ?la-
bels?, which produces a bilingual parsing tree. 
Then, a target output will be generated for the cur-
rent partial source sentence as soon as bilingual 
parsing trees are formed. The output of this part 
therefore contains syntactic information for struc-
ture. 
For instance, given the current source partial 
like ?f1 f2?, we can generate their translation 
word pair sequences with the translation table, 
like ?f1/e1 f2/e2?, ?f1/e3 f2/e4? and so on. The 
corresponding features are then able to be decided 
for the next predicting process. Once the output 
predictions (i.e. stack operations) are decided, the 
bilingual tree structures are formed at the same 
time. As a consequence, results of these opera-
tions are the final translations which we really 
need. 
At each stage of translation, language model 
parameters can be added to adjust the total costs 
of translation candidates and make the pruning 
process reasonable. The whole sentence is then 
processed by incrementally constructing the trans-
lation hypotheses. Lastly, the element in the last 
beam with the minimum cost is the final transla-
tion. In general, the translation process can be de-
scribed in the following way: 
 
5.2 Recombining and Pruning 
Different translation options can combine to form 
the same fragment by beam search decoder. Re-
combining is therefore needed here to reduce the 
search space. So, only the one with the lowest cost 
is kept when several fragments are identical. This 
recombination is a risk-free operation to improve 
searching efficiency. 
Another pruning method used in our system is 
histogram pruning. Only n-best translations are 
652
allowed for the same source part in each stack (e.g. 
n=100). In contrast with traditional beam search 
decoder, we generate our translation candidates 
from the same input, instead of all allowed word 
pairs elsewhere. Therefore the pruning is much 
more reasonable for each beam. There is no rela-
tive threshold cut off compared with Pharaoh. 
In the end, the complexities for decoding are 
the main concern of our method. In practice, how-
ever, it will not exceed the  (m for 
sentence length, N for stack size and Tn for al-
lowed translation candidates). This is based on the 
assumption that our prediction process (tackled by 
SEARN) is fed with three features (only one for-
mer item is associated), which makes it no need of 
full sentence predictions at each time. 
)**( TnNmO
6 Experiment 
We validate our method using the corpus from the 
shared task on NAACL 2006 workshop for statis-
tical machine translation2. The difference of our 
method lies in the framework and different phrase 
translation table. Experiments are carried on all 
the three language pairs (French-English, Ger-
man-English and Spanish-English) and perform-
ances are evaluated by the providing test sets. Sys-
tem parameters are adjusted with development 
data under minimum error rate training. 
For SEARN, three features are chosen to use: 
the current source word, the word before it and the 
current target word. As we do not know the real 
target word order before decoding, the corre-
sponding target word?s position cannot be used as 
features. Besides, we filter the features less than 5 
times to reduce the training complexities. 
The classifier we used in the training process is 
based on perceptron because of its simplicity and 
performance. We modified Daum? III?s script3 to 
fit our method and use the default 5 iterations for 
each perceptron-based training and 3 itertaions for 
SEARN. 
6.1 Results for different language pairs 
The  final  results  of  our  system,  named Amasis, 
and baseline system Pharaoh (Koehn and Monz 
2006) for three language pairs are listed in Table 1. 
The last three lines are the results of Pharaoh with 
phrase length from 1 to 3. However, the length of 
                                                 
2 http://www.statmt.org/wmt06/shared-task/ 
3 http://www.cs.utah.edu/~hal/searn/SimpleSearn.tgz 
0
5000
10000
15000
20000
k
Pharaoh 15724573 12667210 19367713
Amasis 1522468 1715732 1572069
F-E G-E S-E
 
Figure 4: Numbers of translation table 
0.0%
5.0%
10.0%
15.0%
20.0%
25.0%
30.0%
35.0%
40.0%
Pharaoh 3.7% 5.1% 3.5%
Amasis 32.2% 33.0% 36.4%
F-E G-E S-E
 
Figure 5: Percent of single word translation pairs (only one 
word in the source side) 
F-E G-E S-E  
In Out In Out In Out
Amasis 27.44 18.41 23.02 15.97 27.51 23.35
Pharaoh1 20.54 14.07 17.53 12.13 23.23 20.24
Pharaoh2 27.71 19.41 23.36 15.77 28.88 25.28
Pharaoh3 30.01 20.77 24.40 16.58 30.58 26.51
Table 1: BLEU scores for different language pairs. In - In-
domain test, Out - Out-of-domain test. 
 
phrases for Amasis is determined by ITG-like tree 
nodes and there is no restriction for it. 
Even without producing higher BLEU scores 
than Pharaoh, our approach is still interesting for 
the following reasons. First, the number of phrase 
translation pairs is greatly reduced in our system. 
The ratio of translation table number in our 
method (Amasis) to Pharaoh, for French-English 
is 9.68%, for German-English is 13.54%, for 
Spanish-English is 8.12% (Figure 4). This means 
that our method is more efficient at combining 
words and phrases during translation. The reasons 
for the different ratio for the three languages are 
not very clear, maybe are related to the flexibility 
of word order of source language. Second, we 
count the single word translation pairs (only one 
word in the source side) as shown in Figure 5. 
There are significantly more single word transla-
tions in our method. However, the translation 
quality can be kept at the same level under this 
circumstance. Third, our current experimental re-
sults are produced with only three common fea-
tures (the corresponding current source and target 
word and the last source one) without any linguis-
tics information. More useful features are ex-
pected to be helpful like POS tags. Finally, the 
performance can be further improved if we use a 
more powerful classifier (such as SVM or ME) 
with more iterations. 
653
7 Conclusion 
Our method provides a simple and efficient way 
to solve the word ordering problem partially 
which is NP-hard (Knight 1999). It is word based 
except for those indecomposable word sequences 
under ITGs. However, it can achieve comparable 
results with phrase-based method (like Pharaoh), 
while much fewer translation options are used. 
For the structure prediction process, only 3 com-
mon features are preserved and perceptron-based 
classifiers are chosen for the use of simplicity. We 
argue that this approach is promising when more 
features and more powerful classifiers are used as 
Daum? III et al (2007) stated. 
Our contributions lie in the integration of struc-
ture prediction for bilingual parsing trees through 
serial transformations. We reinforce the power of 
formally syntax-based method by using structured 
prediction method to obtain tree-to-tree transduc-
tions by the transforming from word alignments to 
ITG-like trees and then to label sequences. Thus, 
the sentence structures can be better accounted for 
during translating. 
Acknowledgements 
This work is partially supported by National Natural Science 
Foundation of China under grant #60773027, #60736044 and 
by ?863? Key Projects #2006AA010108. We would like to 
thank anonymous reviewers for their detailed comments. 
Appendix A. A Complete Example in Chinese-English 
with Our Defined Operations 
Word alignments 
 
ITG-like tree 
 
SHIFT-REDUCE label sequence 
??/a   S 
??/to learn about  S 
??/Chinese  S,R+ 
??/music   S,R+ 
?/?   S,R+ 
? ?/great   S 
?/?   S,R+ 
??/way   S,R+,R-,R+ 
Stack status when operations finish 
?? ?? ?? ?? ? ? ? ? ??  
/ a great way to learn about Chinese music 
References 
A. Burbank, M. Carpuat, et al 2005. Final Report of the 2005 
Language Engineering Workshop on Statistical Machine 
Translation by Parsing. Johns Hopkins University 
D. Chiang. 2005. A Hierarchical Phrase-Based Model for 
Statistical Machine Translation. In ACL, pages 263-270. 
M. Galley, M. Hopkins, et al 2004. What's in a translation 
rule? In HLT-NAACL, Boston, MA. 
D. Gildea. 2003. Loosely Tree-Based Alignment for Machine 
Translation. In ACL, pages 80-87, Sapporo, Japan. 
H. Daum? III, J. Langford, et al 2007. Search-based Struc-
tured Prediction. Under review by the Machine Learning 
Journal. http://pub.hal3.name/daume06searn.pdf. 
H. Daum? III, J. Langford, et al 2006. Searn in Practice. 
http://pub.hal3.name/daume06searn-practice.pdf.  
K. Knight. 1999. Decoding Complexity in Word-
Replacement Translation Models. Computational Linguis-
tics 25(4): 607-615. 
K. Knight and J. Graehl. 2005. An Overview of Probabilistic 
Tree Transducers for Natural Language Processing. In 
CICLing, pages 1-24. 
P. Koehn. 2004. Pharaoh: A Beam Search Decoder for 
Phrase-Based Statistical Machine Translation Models. In 
Proc. of AMTA, pages 115-124. 
P. Koehn and C. Monz. 2006. Manual and Automatic Evalua-
tion of Machine Translation between European Languages. 
In Proc. on the Workshop on Statistical Machine Transla-
tion, pages 102-121, New York City. 
P. Koehn, F. J. Och, et al 2003. Statistical Phrase-Based 
Translation. In HLT-NAACL, pages 127-133. 
P. Liang, A. Bouchard, et al 2006. An End-to-End 
Discriminative Approach to Machine Translation. In ACL. 
Y. Liu, Q. Liu, et al 2006. Tree-to-String Alignment Tem-
plate for Statistical Machine Translation. In ACL. 
F. J. Och. 2003. Minimum Error Rate Training in Statistical 
Machine Translation. In ACL, pages 160-167. 
F. J. Och and H. Ney. 2003. A Systematic Comparison of 
Various Statistical Alignment Models. Computational 
Linguistics 29(1): 19-51. 
K. Sagae and A. Lavie. 2005. A Classifier-Based Parser with 
Linear Run-Time Complexity. In IWPT, pages 125-132. 
C. Tillmann and T. Zhang. 2005. A Localized Prediction 
Model for Statistical Machine Translation. In ACL. 
C. Tillmann and T. Zhang. 2006. A Discriminative Global 
Training Algorithm for Statistical MT. in ACL. 
J. Turian, B. Wellington, et al 2006. Scalable Discriminative 
Learning for Natural Language Parsing and Translation. In 
Proceedings of NIPS, Vancouver, BC. 
T. Watanabe, J. Suzuki, et al 2007. Online Large-Margin 
Training for Statistical Machine Translation. In EMNLP. 
D. Wu. 1997. Stochastic Inversion Transduction Grammars 
and Bilingual Parsing of Parallel Corpora. Computational 
Linguistics 23(3): 377-404. 
D. Xiong, Q. Liu, et al 2006. Maximum Entropy Based 
Phrase Reordering Model for Statistical Machine Transla-
tion. In ACL, pages 521-528. 
K. Yamada and K. Knight. 2001. A Syntax-based Statistical 
Translation Model. In ACL, pages 523-530. 
K. Yamada and K. Knight. 2002. A Decoder for Syntax-
based Statistical MT. In ACL, pages 303-310. 
654
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 96?99,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
 A Syllable-based Name Transliteration System 
 
 
Xue Jiang 1, 2 
1Institute of Software, Chinese 
Academy of Science. 
Beijing China, 100190 
jiangxue1024@yahoo.com.cn 
Le Sun 1, Dakun Zhang 1 
2School of Software Engineering,  
Huazhong University of Science and 
Technology. Wuhan China, 430074 
sunle@iscas.ac.cn 
dakun04@iscas.ac.cn 
 
 
 
 
Abstract 
This paper describes the name entity transli-
teration system which we conducted for the 
?NEWS2009 Machine Transliteration Shared 
Task? (Li et al2009). We get the translitera-
tion in Chinese from an English name with 
three steps. We syllabify the English name 
into a sequence of syllables by some rules, 
and generate the most probable Pinyin se-
quence with the mapping model of English 
syllables to Pinyin (EP model), then we con-
vert the Pinyin sequence into a Chinese cha-
racter sequence with the mapping model of 
Pinyin to characters (PC model). And we get 
the final Chinese character sequence. Our 
system achieves an ACC of 0.498 and a 
Mean F-score of 0.786 in the official evalua-
tion result. 
1 Introduction 
The main subject of shared task is to translate 
English names (source language) to Chinese 
names (target language). Firstly, we fix some 
rules and syllabify the English names into a se-
quence of syllables by these rules, in the mean-
while, we convert the Chinese names into Pinyin 
sequence. Secondly, we construct an EP model 
referring to the method of phrase-based machine 
translation. In the next, we construct a 2-gram 
language model on characters and a chart reflect-
ing the using frequency of each character with 
the same pronunciation, both of which constitute 
the PC model converting Pinyin sequence into 
character sequence. When a Pinyin is mapped to 
several different characters, we can use them to 
make a choice. In our experiment, we adopt the 
corpus provided by NEWS2009 (Li et al2004) 
and the LDC Name Entity Lists 1 respectively to 
conduct two EP models, while the NEWS2009 
corpus for the PC model. The experiment indi-
cates that the larger a training corpus is, the more 
precise the transliteration is. 
2 Transliteration System Description 
Knowing from the definition of transliteration, 
we must make the translating result maintain the 
original pronunciation in source language. We 
found that most English letters and letter compo-
sitions? pronunciation are relatively fixed, so we 
can take a syllabification on an English name, 
therefore the syllable sequence can represent its 
pronunciation. In Chinese, Pinyin is used to 
represent a character?s pronunciation. Based on 
these analyses, we transliterate the English sylla-
ble sequence into a Pinyin sequence, and then 
translate the Pinyin sequence into characters. 
We suppose that the probability of a translitera-
tion from an English name to a Chinese name is 
denoted by P(Ch|En), the probability of a transla-
tion from an English syllable sequence to a Pi-
nyin  sequence is denoted by P(Py|En), and the 
probability of a translation from a Pinyin se-
quence to a characters is denoted by P(Ch|Py), 
then we can get the formula: 
P(Ch|En) = P(Ch|Py) * P(Py|En)      (1) 
The character sequence in candidates having 
the max value of P(Ch|En) is the best translitera-
tion(Wan and Verspoor, 1998). 
2.1 Syllabification of English Names 
English letters can be divided into vowel letters 
(VL) and consonant letters (CL). Usually, in a 
                                                 
1
: Chinese <-> English Name Entity Lists v 1.0, LDC Cata-
log No.: LDC2005T34 
96
word, a phonetic syllable can be constructed in a 
structure of CL+VL, CL+VL+CL, CL+VL+NL. 
To adapt for Chinese phonetic rule, we divide the 
continuous CLs into independent CLs(IC) and 
divide structure of CL+VL+CL into CL+VL and 
an IC. Take ?Ronald? as an example, it can be 
syllabified into ?Ro/na/l/d?, ?Ro? is CL+VL, 
?nal? is CL+VL+CL, and is divided into CL+VL 
and IC. ?d? is an independent CL(KUO et al 
2007). Of course there are some English names 
more complex to be syllabified, so we define 
seven rules for syllabification (JIANG et al 
2006): 
(1) Define English letter set as O, vowel set as 
V={a, e, i, o, u}, consonant set as C=O-V. 
(2) Replace all ?x? in a name with ?ks? before 
syllabification because it?s always pro-
nounced as  ?ks?. 
(3) The continuous VLs should be regarded as 
one VL. 
(4) There are some special cases in rule (3), 
the continuous VLs like ?oi?, ?io?, ?eo? 
are pronounced as two syllables, so they 
should be cut into two parts, so ?Wilhoit? 
will be syllabifyd into ?wi/l/ho/i/t?. 
(5) The continuous CLs should be cut into 
several independent CLs. If the last one is 
followed by some VLs, they will make up 
a syllable. 
(6) Some continuous CLs are pronounced as a 
syllable, such as ?ck?, ?th?, these CLs will 
not be syllabifyd and be regarded as a sin-
gle CL, ?Jack? is syllabifyd into ?Ja/ck?. 
(7) There are some other composition with the 
structure of VL+CL, such as ?ing?, ?er?, 
?an? and so on. If it?s a consonant behind 
these compositions in the name, we can 
syllabify it at the end of the composition, 
while if it?s a vowel behind them, we 
should double write the last letter and syl-
labify the word between the two same let-
ters. 
After syllabicating English names, we convert 
corresponding Chinese names into Pinyin. There 
are a few characters with multiple pronunciations 
in the training data, we find them out and ensure 
its pronunciation in a name manually.  
We record all of these syllables got from the 
training data set, if we meet a syllable out of vo-
cabulary when transliterating an English name, 
we will find a similar one with the shortest edit-
distance in the vocabulary to replace that. 
2.2 Mapping Model of English Syllables to 
Pinyins 
The EP model consists of a phrase-based ma-
chine translation model with a trigram language 
model.  
Given an English name f, we want to find its 
Chinese translation e, which maximize the condi-
tional probability )|Pr( fe , as shown below. 
)|Pr(maxarg* fee e?
    (2) 
Using Bayes rule, (1) can be decomposed into 
a Translation Model )|Pr( ef  and a Language 
Model )Pr(e  (Brown et al 1993), which can 
both be trained separately. These models are 
usually regarded as features and combined with 
scaling factors to form a log-linear model (Och 
and Ney 2002). It can then be written as: 
? ?
?
?
?
?
?
'
1
1
)],'(exp[
)],(exp[
              
)|()|Pr(
1
e
mm
M
m
mm
M
m
feh
feh
fepfe M
?
?
?
        (3) 
In our model, we use the following features: 
? phrase translation probability )|( fep  
? lexical weighting )|( felex  
? inverse phrase translation probability 
)|( efp  
? inverse lexical weighting )|( eflex   
? phrase penalty (always exp(1) = 2.718) 
? word penalty (target name length) 
? target language model, trigram 
The first five features can be seen as a whole 
phrase translation cost and used as one during 
decoding.  
In general, the translation process can be de-
scribed as follows: 
(1). Segmenting input English syllable se-
quence f into J syllables Jf 1  
(2). Translating each English syllable 
jf  
into several Pinyins 
jke  
(3). Selecting the N-best words 
nee ...1 , 
combined with reordering and Language 
Model and other features 
97
(4). Rescoring the translation word set with 
additional features to find the best one. 
We use SRI toolkit to train our trigram lan-
guage model with modified Kneser-Ney smooth-
ing (Chen and Goodman 1998). In the standard 
experiment, we use training data set provided by 
NEWS2009 (Li et al2004) to train this language 
model, in the nonstandard one, we use that and 
the LDC Name Entity Lists to train this language 
model. 
2.3 Mapping Model of Pinyins to Chinese 
Characters 
Since the Chinese characters used in people 
names are limited, most of the conversions from 
Pinyin to character are fixed. But some Pinyins 
still have several corresponding characters, and 
we should make a choice among these characters. 
To solve this problem, we conduct a PC model 
consisting a frequency chart which reflects the 
using frequency of each character at different 
positions in the names and a 2-gram language 
model with absolute discounting smoothing.  
A Chinese name is represented as C1C2?
Cn?Ci (1?i?n) is a Chinese character. C1 is at 
the first position, we call it FW; C2 ?Cn-1 are in 
the middle, we call them MW; Cn is at the last 
position, we call it LW. Usually, each character 
has different frequencies at these three positions. 
In the training data set of NEWS2009, Pinyin 
?luo? can be mapped to three characters: ???, 
???, and ???, each of them has different fre-
quencies at different positions. 
 
 FW MW LW 
? 0.677 0.647 0.501 
? 0.323 0.352 0.499 
? 0 0.001 0 
Table 1. Different frequencies at different positions 
 
From this table, we can see that at FW and 
MW position, ??? is more probable to be cho-
sen than the others, but sometimes ??? or ??? 
is the correct one. In order to ensure characters 
with lower frequency like ??? and ??? can be 
chosen firstly in a certain context, we conduct a 
2-gram language model.  
If a Pinyin can be mapped to several charac-
ters, the condition probability (P(Chi|py)) indicat-
ing that how possible a character should be cho-
sen is determined by the weighted average of its 
position frequency (P(Chi|pos)) and its probabili-
ty in the 2-gram language model (P(Chi|Chi-1)). 
P(Chi|py) = a*P(Chi|pos)+(1-a)*P(Chi|Chi-1)  (4) 
0 < a < 1. In our experiments, we set a = 0.1.  
2.4 Experiments and Results 
We carried out two experiments. The difference 
between them is the training data for EP model. 
The standard experiment adopts corpus provided 
by NEWS2009, while the nonstandard one 
adopts LDC Name Entity Lists. 
 
Corpora Name Num 
LDC2005T34 572213 
NEWS09_train_ench_31961 31961 
Table 2. Corpora used for training the EP model 
 
Considering that an English name may be 
translated to different Chinese names in different 
corpora, so we established a unique PC model 
with the training data set provided by 
NEWS2009 to avoid the model?s deviation 
caused by different corpora. 
The experimenting data is the development 
data set provided by NEWS2009 (Li et al2004), 
testing script is also provided by NEWS2009. 
First, we take a syllabification on testing 
names.  Then we use the EP model to generate 5-
best Pinyin sequences and their probabilities.  
For each Pinyin sequence, the PC model gives 3-
best character sequences and their probabilities. 
In the end, we sort the results by probabilities of 
character sequences and corresponding Pinyin 
sequences. 
The evaluation results are shown below. 
 
Metrics Standard Nonstandard 
ACC 0.490677 0.502417 
Mean F-score 0.782039 0.784203 
MRR 0.606424 0.611214 
MAP_ref 0.490677 0.502417 
MAP_10 0.189290 0.189782 
MAP_sys 0.191476 0.192129 
Table 3. Evaluation results of standard and             
nonstandard experiments 
It?s easy to see that nonstandard test is better 
than standard one on each metric. A larger cor-
pus does make a contribution to a more accurate 
model. 
98
For the official evaluation, we make two tests 
on the testing data set provided by NEWS2009 
(Li et al2004). The table 4 shows respectively 
the evaluation results of standard and nonstan-
dard tests given by NEWS2009. 
Metrics Standard Nonstandard 
ACC 0.498 0.500 
Mean F-score 0.786 0.786 
MRR 0.603 0.607 
MAP_ref 0.498 0.500 
MAP_10 0.187 0.189 
MAP_sys 0.189 0.191 
Table 4. Official evaluation results of standard and 
nonstandard tests 
3 Conclusion 
We construct a name entity transliteration system 
based on syllable. This system syllabifies Eng-
lish names by rules, then translates the syllables 
to Pinyin and Chinese characters by statistics 
model. We found that a larger corpus may im-
prove the transliteration. Besides, we can do 
something else to improve that. We need to fix 
more complex rules for syllabification. If we can 
get the name user?s gender from some features of 
the name itself, then translate the male and fe-
male names on different Chinese character sets, 
the results may be more precise. 
Acknowledgments 
This work was supported by the National 
Science Foundation of China (60736044, 
60773027), as well as 863 Hi-Tech Research and 
Development Program of China (2006AA010108 
-5, 2008AA01Z145).  
We also thank Haizhou Li, Min Zhang and 
Jian Su for providing the English-Chinese data. 
Reference 
Franz Josef Och and Hermann Ney. 2002. ?Discri-
minative Training and Maximum Entropy Models 
for Statistical Machine Translation?. In  Proceed-
ings of the 40th Annual Meeting of the Association 
for Computational Linguistics (ACL). 
Haizhou Li, A Kumaran, Min Zhang, Vladimir Per-
vouchine, "Whitepaper of NEWS 2009 Machine 
Transliteration Shared Task". In Proceedings of 
ACL-IJCNLP 2009 Named Entities Workshop 
(NEWS 2009), Singapore, 2009 
Haizhou Li, Min Zhang, Jian Su. 2004. ?A joint 
source channel model for machine transliteration?, 
In Proceedings of the 42nd ACL, 2004 
Jiang Long, Zhou Ming, and Chien Lee-feng. 2006. 
?Named Entity Translation with Web Mining and 
Transliteration?. Journal of Chinese Information 
Processing, 21(1):1629--1634. 
Jin-Shea Kuo, Haizhou Li, and Ying-Kuei Yang. 
2007.  ?A Phonetic Similarity Model for Automatic 
Extraction of Transliteration Pairs?. ACM Trans. 
Asian Language Information Processing, 6(2), Sep-
tember 2007. 
Peter F. Brown, Stephen A. Della Pietra, et al 1993. 
?The Mathematics of Statistical Machine Transla-
tion: Parameter Estimation?. Computational Lin-
guistics 19(2): 263-311. 
Stanley F. Chen and Joshua Goodman. 1998. ?An 
empirical study of smoothing techniques for lan-
guage modeling?. Technical Report TR-10-98, Har-
vard University. 
Stephen Wan and Cornelia Maria Verspoor. 1998. 
?Automatic English-Chinese name transliteration 
for development of multilingual resources?. In Pro-
ceedings of the 17th international conference on 
Computational linguistics, 2: 1352 ? 1356. 
99
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1897?1907, Dublin, Ireland, August 23-29 2014.
A Neural Reordering Model for Phrase-based Translation
Peng Li
?
Yang Liu
?
Maosong Sun
?
Tatsuya Izuha
?
Dakun Zhang
?
?
State Key Laboratory of Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Sci. and Tech., Tsinghua University, Beijing, China
pengli09@gmail.com, {liuyang2011,sms}@tsinghua.edu.cn
?
Toshiba Corporation Corporate Research & Development Center
tatsuya.izuha@toshiba.co.jp
?
Toshiba (China) R&D Center
zhangdakun@toshiba.com.cn
Abstract
While lexicalized reordering models have been widely used in phrase-based translation systems,
they suffer from three drawbacks: context insensitivity, ambiguity, and sparsity. We propose a
neural reordering model that conditions reordering probabilities on the words of both the current
and previous phrase pairs. Including the words of previous phrase pairs significantly improves
context sensitivity and reduces reordering ambiguity. To alleviate the data sparsity problem, we
build one classifier for all phrase pairs, which are represented as continuous space vectors. Ex-
periments on the NIST Chinese-English datasets show that our neural reordering model achieves
significant improvements over state-of-the-art lexicalized reordering models.
1 Introduction
Reordering plays a crucial role in phrase-based translation (Koehn et al., 2003; Och and Ney, 2004).
While local reordering can be directly memorized in phrases, modeling reordering at a phrase level still
remains a major challenge: it can be cast as a travelling salesman problem and proves to be NP-complete
(Knight, 1999; Zaslavskiy et al., 2009).
The past decade has witnessed the rapid development of phrase reordering models (e.g., (Och et al.,
2004; Tillman, 2004; Zens et al., 2004; Xiong et al., 2006; Al-Onaizan and Papineni, 2006; Koehn et
al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012;
Cherry, 2013), just to name a few). Among them, lexicalized reordering models (Tillman, 2004; Koehn
et al., 2007; Galley and Manning, 2008) have been widely used in practical phrase-based systems. Un-
like the distance-based reordering model (Koehn et al., 2003) that only penalizes phrase displacements
in terms of the degree of nonmonotonicity, lexicalized reordering models introduce reordering probabil-
ities conditioned on the words of each phrase pair. They often distinguish between three orientations
with respect to the previous phrase pair: monotone, swap, and discontinuous. As lexicalized reordering
models capture the phenomenon that some words are far more likely to be displaced than others, they
outperform unlexicalized reordering models substantially.
Despite their apparent success in statistical machine translation, lexicalized reordering models suffer
from the following three drawbacks:
1. Context insensitivity. Lexicalized reordering models determine the orientations only depending on
the words of current phrase pairs. In fact, a phrase pair usually has different orientations in different
contexts. It is important to include more contexts to improve the expressive power of reordering
models.
2. Ambiguity. Short phrase pairs, which are observed in the training data more frequently, usually have
multiple orientations. We observe that about 92.4% of one-word Chinese-English phrase pairs are
ambiguous. This makes it hard to decide which orientation should be properly used in decoding.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1897
Figure 1: Ambiguity in phrase reordering. The phrase pair ??yingyun?, ?business?? is labeled with
different orientations in different contexts: (a) monotone, (b) swap, and (c) discontinuous. Lexicalized
reordering models use fixed probability distributions (e.g., 17.50% for M, 1.59% for S, and 80.92% for
D) in decoding even though the surrounding contexts keep changing.
3. Sparsity. Lexicalized reordering models maintain a reordering probability distribution for each
phrase pair. As most long phrase pairs that are capable of memorizing local word selection and
reordering only occur once in the training data, maximum likelihood estimation can hardly train the
models accurately.
In this work, we propose a neural reordering model for phrase-based translation. The contribution is
twofold. Firstly, unlike conventional lexicalized reordering models, the neural reordering model condi-
tions reordering probabilities on the words of both the current and previous phrase pairs. Including the
words of previous phrase pairs significantly improves context sensitivity and reduces reordering ambi-
guity. Secondly, to alleviate the data sparsity problem, we build a neural classifier for all phrase pairs,
which are represented as continuous space vectors. Experiments on the NIST Chinese-English datasets
show that our neural reordering model achieves significant improvements over state-of-the-art lexicalized
models.
2 Lexicalized Reordering Models
The lexicalized reordering models (Tillman, 2004; Koehn et al., 2007; Galley and Manning, 2008) have
become the de facto standard in modern phrase-based systems. These models are called lexicalized
because they condition reordering probabilities on the words of each phrase pair. Depending on the
relationship between the current and previous phrase pairs, lexicalized reordering models often define
orientations to classify different reordering patterns.
More formally, we use f = {
?
f
1
, . . . ,
?
f
n
} to denote a sequence of source phrases, e = {e?
1
, . . . , e?
n
}
to denote the phrase sequence on the target side, and a = {a
1
, . . . , a
n
} to denote the alignment be-
tween source and target phrases. A source phrase
?
f
a
i
and a target phrase e?
i
form a phrase pair. Lex-
icalized reordering models aim to estimate the conditional probability of a sequence of orientations
o = {o
1
, . . . , o
n
}:
P (o|f , e,a) =
n
?
i=1
P (o
i
|f , e?
1
, . . . , e?
i
, a
1
, . . . , a
i
) (1)
where each o
i
takes values over a set of predefined orientations. For simplicity, current lexicalized
1898
model
source phrase length
1 2 3 4 5 6 7
P (o
i
|
?
f
a
i
, e?
i
, a
i?1
, a
i
) 92.74 54.01 24.09 14.40 10.78 8.47 6.95
P (o
i
|
?
f
a
i
, e?
i
,
?
f
a
i?1
, e?
a
i?1
, a
i?1
, a
i
) 21.72 5.22 2.63 1.48 0.98 0.67 0.54
Table 1: Percentages of phrase pairs that have multiple orientations. Including previous phrase pairs in
modeling significantly reduces the reordering ambiguity for the M/S/D orientations. For example, while
92.74% of 1-word Chinese-English phrase pairs have multiple orientations observed in the training data,
the ratio dramatically drops to 21.72% if the orientations are conditioned on both the current and previous
phrase pairs.
reordering models use orientations conditioned only on a
i?1
and a
i
:
P (o|f , e,a) ?
n
?
i=1
P (o
i
|
?
f
a
i
, e?
i
, a
i?1
, a
i
) (2)
The most widely used orientations are monotone (M), swap (S), and discontinuous (D):
1
o
i
=
?
?
?
M if a
i
? a
i?1
= 1
S if a
i
? a
i?1
= ?1
D if |a
i
? a
i?1
| =? 1
(3)
As lexicalized reordering models maintain a reordering probability distribution for each phrase pair,
it is hard to accurately learn reordering probabilities for long phrase pairs that are usually observed only
once in the training data. On the contrary, short phrase pairs that occur in the training data for many times
tend to be ambiguous. For example, as shown in Figure 1, a Chinese-English phrase pair ??yingyun?,
?business?? is observed to have different orientations in different contexts.
It is unreasonable to use fixed reordering probability distributions in decoding as the surrounding
contexts keep changing. Previous study shows that considering more contexts into reordering modeling
improves translation performance (Khalilov and Simaan, 2010). Therefore, we need a more powerful
mechanism to include more contexts, resolve the reordering ambiguity, and reduce the data sparsity.
3 A Neural Reordering Model
3.1 The Model
Intuitively, conditioning reordering probabilities on the words of both the current and previous phrase
pairs will significantly reduce both reordering ambiguity and context insensitivity. The new reordering
model is given by
P (o|f , e,a) ?
n
?
i=1
P (o
i
|
?
f
a
i
, e?
i
,
?
f
a
i?1
, e?
i?1
, a
i?1
, a
i
) (4)
where ?
?
f
a
i?1
, e?
i?1
? is the previous phrase pair.
Including the previous phrase pairs improves the context sensitivity. For example, given a phrase pair
??yingyun?, ?business??, its orientation is more likely to be monotone if it is preceded by a noun phrase
pair such as ??xinyongka?, ?credit card??. On the contrary, the probability of the discontinuous orienta-
tion is higher if the previous phrase pairs contain verbs such as ??gaishan?, ?improve??. Therefore, the
new model is capable of capturing the phenomenon that the orientation of a phrase pair depends on its
surrounding contexts.
Another advantage of including previous phrase pairs is the reduction of reordering ambiguity. As
shown in Table 1, 92.74% of 1-word Chinese-English phrase pairs have multiple orientations (i.e., M, S,
1
There are many variants of lexicalized reordering models depending on the model type, orientation, directionality, lan-
guage, and collapsing. See http://www.statmt.org/moses/?n=FactoredTraining.BuildReorderingModel for more details.
1899
and D) observed in the training data. The ratio decreases with the increase of phrase length. In contrast,
the new model is much less ambiguous (e.g., the ratio of ambiguous one-word phrase pairs dramatically
drops to 21.72%) as it is conditioned on both the current and previous phrase pairs.
Unfortunately, including more contexts in modeling also increases the data sparsity. We observe that
about 90% of reordering examples (i.e., the current and previous phrase pairs) are observed only once in
the training data. As a result, it is more difficult to train lexicalized reordering models accurately using
maximum likelihood estimation.
To alleviate the data sparsity problem, we use the following two strategies:
1. Reordering as classification. Instead of maintaining a reordering probability distribution for each
phrase pair, we build a reordering classifier for all phrase pairs (Xiong et al., 2006; Li et al., 2013).
This significantly reduces data sparsity by considering all occurrences of extracted phrase pairs as
training examples. We find that 500, 000 reordering examples suffice to train a robust classifier
(Section 4.5).
2. Continuous space representation. Instead of using a symbolic representation of phrases, we use
a continuous space representation that treats a phrase as a dense real-valued vector (Socher et al.,
2011b; Li et al., 2013). Consider two phrases ?in London? and ?in Centara Grand?. It is usually
easy to predict the orientations of ?in London? because it might be observed in the training data for
many times. This is not the case for ?in Centara Grand? as it might occur only once. However, if
the two phrases happen to have very similar continuous space representations, ?in Centara Grand?
is likely to have a similar reordering probability distribution with ?in London?.
To generate vector space representation for phrases, we follow Socher et al. (2011a) to use recursive
autoencoders. Given two words w
1
and w
2
, suppose their vector space representations are c
1
and c
2
.
The vector space representation p of the two-word phrase {w
1
, w
2
} can be computed using a two-layer
neural network:
p = g
(1)
(W
(1)
[c
1
; c
2
] + b
(1)
) (5)
where [c
1
; c
2
] ? R
2n
is the concatenation of c
1
and c
2
, W
(1)
? R
n?2n
is a weight matrix, b
(1)
is a bias
vector, and g
(1)
is an element-wise activation function.
In order to measure how well p represents c
1
and c
2
, they can be reconstructed using another two-layer
neural network:
[c
?
1
; c
?
2
] = g
(2)
(W
(2)
p + b
(2)
) (6)
where c
?
1
? R
n
and c
?
2
? R
n
are reconstructed vectors of c
1
and c
2
, W
(2)
? R
2n?n
is a weight matrix,
b
(2)
? R
n
is a bias vector, and g
(2)
is an element-wise activation function. The reconstruction error can
be measured by comparing c
1
and c
2
with c
?
1
and c
?
2
. This process runs recursively in a bottom-up style
to obtain the vector space representation of a multi-word phrase (Socher et al., 2011a). Socher et al.
(2011a) find that minimizing the norms of hidden layers leads to the reduction of reconstruction error in
an undesirable way. Therefore, we normalize p such that ||p||
2
= 1.
Treating phrase reordering as a classification problem, we propose a neural reordering classifier that
takes the current and previous phrase pairs as input. The neural network consists of four recursive
autoencoders and a softmax layer. The input of the classifier are the previous phrase pair and the current
phrase pair. Four recursive autoencoders are used to transform the four phrases (i.e.,
?
f
a
i
, e?
i
,
?
f
a
i?1
, e?
i?1
)
into vectors. Then, these vectors are fed to the softmax layer to predict reordering orientations. Note that
the recursive autoencoders for the same language share with the same parameters. Our neural network is
similar to that of Li et al. (2013). The major difference is that Li et al. (2013) need to compute vector
space representation for variable-sized blocks ranging from words to sentences on the fly both in training
and decoding. In contrast, we only need to compute vectors for phrases with up to 7 words in the training
phase, which makes our approach simpler and more scalable to large data.
1900
Formally, given the previous phrase pair ?
?
f
a
i?1
, e?
i?1
?, the current phrase pair ?
?
f
i
, e?
i
? and the orienta-
tion o
i
, the reordering probability is computed as
P (o
i
|
?
f
a
i
, e?
i
,
?
f
a
i?1
, e?
i?1
, a
i?1
, a
i
) = g(W
o
c(
?
f
a
i
, e?
i
,
?
f
a
i?1
, e?
i?1
) + b
o
), (7)
where W
o
is a weight matrix, b
o
is a bias vector, c(
?
f
a
i
, e?
i
,
?
f
a
i?1
, e?
i?1
) is the concatenation of the vectors
of the four phrases.
2
Following Och (2003), we use a linear model in our decoder with conventional features (e.g., trans-
lation probabilities and n-gram language model). The neural reordering model is incorporated into the
discriminative framework as an additional feature.
3.2 Training
Training the neural reordering model involves minimizing the following two kinds of errors:
? Reconstruction error: It measures how well the computed vector space representations represent
the input vectors. It is defined as the average reconstruction error of all the parent nodes in the trees
formed during computing the vector space representation for all the phrases in the training data.
? Classification error: It measures how well the resulting classifier predicts the reordering orienta-
tions. It is defined as the average cross-entropy errors of all the training examples.
In our experiments, the objective function is a linear interpolation of the reconstruction error and the
classification error.
Following Socher et al. (2011b), we use L-BFGS (Liu and Nocedal, 1989) to optimize the parameters.
At the beginning of each iteration, a binary tree for each phrase is constructed using a greedy algorithm
(Socher et al., 2011b).
3
With these trees fixed, the partial derivatives with respect to parameters are
computed via the backpropagation through structures algorithm (Goller and Kuchler, 1996).
When optimizing the parameters of the softmax layer, the training procedure keeps the parameters of
the recursive autoencoders and word embedding matrices fixed. The corresponding error function is the
classification error as described above. We also use L-BFGS to optimize the parameters and the standard
error backpropagation algorithm (Rumelhart et al., 1986) to compute the derivatives.
3.3 Decoding
As the vector space representation of a phrase is calculated based on all the words in the phrase, using
the neural reordering model complicates the conditions for risk-free hypothesis recombination (Koehn et
al., 2003). Therefore, many hypotheses are not likely to be recombined if the neural reordering model
is directly integrated in decoding, making the decoder to only explore in a much smaller search space.
4
Therefore, we use Moses to generate search graphs and then use hypergraph reranking (Huang and
Chiang, 2007; Huang, 2008) to find most probable derivations using the neural reordering model.
4 Experiments
4.1 Data Preparation
We evaluate our reordering model on Chinese-English translation. The training corpus consists of 1.23M
sentence pairs with 32.1M Chinese words and 35.4M English words. A 4-gram language model was
trained on the Xinhua portion of the English GIGAWORD corpus using KenLM (Heafield, 2011), which
contains 398.6M words. We used the NIST 2006 MT Chinese-English dataset as the development set,
and NIST 2002-2005, 2008 MT Chinese-English datasets as the test sets. Case-insensitive BLEU is used
2
In practice, as suggested by Socher et al. (2011b), we feed the four average vectors of the vectors present in each recursive
autoencoders to the softmax layer. Taking ?resident population? as an example, there are three vectors in the binary tree used
by the corresponding recursive autoencoder, denoted as x?
1
, x?
2
and x?
3
. The average vector is computed as x? =
1
3
?
3
i=1
x?
i
.
3
As phrases in phrase-based translation are not necessarily syntactic constituents, we do not use parse trees in this work.
4
Experimental results show that we can only achieve comparable performance with Moses by integrating neural reordering
model directly in decoding.
1901
Model Orientation MT06 MT02 MT03 MT04 MT05 MT08
distance N/A 29.56 31.40 31.27 31.34 29.98 23.87
word
M/S/D 30.19 32.03 31.86 32.09 30.55 24.20
left/right 30.17 31.98 31.52 31.98 30.19 24.30
phrase
M/S/D 30.24 32.35 31.85 32.00 30.78 24.33
left/right 29.57 32.64 31.53 31.90 30.70 24.28
hierarchical
M/S/D 30.46 32.52 31.89 32.09 30.39 24.11
left/right 30.03 32.13 31.59 31.91 30.21 24.41
neural
M/S/D 30.68 32.19 31.94 32.20 30.81 24.71
left/right 31.03** 33.03** 32.48** 32.52** 31.11* 25.20**
Table 2: Comparison of distance-based, lexicalized, and neural reordering models in terms of case-
insensitive BLEU-4 scores. ?distance? denotes the distance-based reordering model (Koehn et al., 2003),
?word? denotes the word-based lexicalized model (Tillman, 2004), ?phrase? denotes the phrase-based
lexicalized model (Koehn et al., 2007), ?hierarchical? denotes the hierarchical phrase-based reordering
model (Galley and Manning, 2008), and ?neural? denotes our model. The ?left? and ?right? orientations
only considers whether the current source phrase is on the left of the previous source phrase or not. We
use ?*? to highlight the result that is significantly better than the best baseline (highlighted in italic)
at p < 0.05 level and ?**? at p < 0.01 level. The neural model does not work well for the M/S/D
orientations due to the non-separability problem (Section 4.3).
as the evaluation metric. As a trade-off between expressive power and computational cost, we set the
dimension of the word embedding vectors to 25.
5
Both g
(1)
and g
(2)
are set to tanh(?). The other
hyperparameters are optimized via random search (Bergstra and Bengio, 2012).
4.2 Comparison of Distance-based, Lexicalized, and Neural Reordering Models
We compare three kinds of reordering models with increasing expressive power:
1. distance-based model: penalizing phrase displacements proportionally to the amount of nonmono-
tonicity (Koehn et al., 2003);
2. lexicalized models: conditioning the reordering probabilities on the current phrase pairs. The ori-
entations can be determined with respect to words (Tillman, 2004), phrases (Koehn et al., 2007), or
hierarchical phrases (Galley and Manning, 2008);
3. neural model: conditioning the reordering probabilities on both the current and previous phrase
pairs.
For lexicalized and neural models, we further distinguish between two kinds of orientation sets:
{monotone, swap, discontinuous} and {left, right}. The left/right orientations only consider whether
the current source phrase is on the left of the previous source phrase or not. Therefore, swap and
discontinuous-left are merged into left while monotone and discontinuous-right into right.
All these reordering models are tested using Moses (Koehn et al., 2007), except that the neural model
needs an additional hypergraph reranking procedure (Section 3.3). Implemented using Java, it takes the
reranker 0.748 second to rerank a hypergraph on average.
Table 2 shows the case-insensitive BLEU-scores of distance-based, lexicalized, and neural reordering
models on the NIST Chinese-English datasets. ?distance? denotes the distance-based reordering model
(Koehn et al., 2003), ?word? denotes the word-based lexicalized model (Tillman, 2004), ?phrase? denotes
the phrase-based lexicalized model (Koehn et al., 2007), ?hierarchical? denotes the hierarchical phrase-
based reordering model (Galley and Manning, 2008), and ?neural? denotes our model.
5
We find that the dimensions of vectors do not have a significant impact on translation performance. For efficiency, we set
the dimension to 25.
1902
Figure 2: The non-separability problem for the neural reordering model. Given an aligned Chinese-
English sentence pair, the unaligned Chinese word ?de? makes a big difference in determining M/S/D
orientations. In (a), ?de? is included in the previous source phrase and thus the orientation is monotone.
In (b), however, it is not included in the previous source phrase and the orientation is discontinuous. In
our neural reordering model, ?liu wan de? and ?liu wan? have very similar vector space representations
yet different orientations (i.e., M and D). In other words, training examples labeled with M, S, D are
prone to be mixed with each other in the vector space. Therefore, it is difficult to find a hyperplane to
separate M, S and D examples in the high-dimensional space.
We find that lexicalized reordering models obtain significant improvements over the distance-based
model, which indicates that conditioning reordering probabilities on the words of the current phrase
pairs does improve the expressive power. Our neural model using left/right orientations significantly
outperforms all variants of lexicalized models. We use ?*? to highlight the result that is significantly
better than the best baseline (highlighted in italic) at p < 0.05 level and ?**? at p < 0.01 level. This
suggests that conditioning reordering probabilities on the words of current and previous phrase pairs is
helpful for resolving reordering ambiguities and reducing context insensitivity.
4.3 The Non-Separability Problem
In Table 2, the neural model using the M/S/D orientations fails to outperform lexicalized models signifi-
cantly. One possible reason is that the neural model suffers from the non-separability problem due to the
M/S/D orientations.
As shown in Figure 2, given an aligned Chinese-English sentence pair, the unaligned Chinese function
word ?de? makes a big difference in determining M/S/D orientations. In (a), ?de? is included in the
previous source phrase and thus the orientation is monotone. In (b), however, ?de? is not included in the
previous source phrase and the orientation is discontinuous. In our neural reordering model, ?liu wan
de? and ?liu wan? have very similar vector space representations yet different orientations (i.e., M and
D). In other words, training examples labeled with M, S, D are prone to be mixed with each other in
the vector space. Therefore, it is difficult to find a hyperplane to separate M, S and D examples in the
high-dimensional space.
Fortunately, we find that using the left/right orientations can alleviate this problem. As the left/right
orientations only consider whether the current source phrase is on the left of the previous source phrase
or not, unaligned source words will not change orientations. For example, both Figure 2(a) and 2(b) are
identified as the right orientation.
As a result, using left/right orientations in the neural reordering model not only has a higher classifi-
cation accuracy (85%) over using the M/S/D orientations (69%), but also achieves higher BLEU scores
on all NIST datasets systematically.
4.4 The Effect of Distortion Limit
Figure 3 shows the performance of the lexicalized model and our neural model with various distortion
limits. The lexicalized model is the word-based model with M/S/D orientations. The neural model uses
left/right orientations. The neural model consistently outperforms the lexicalized model, especially for
large distortion limits. This finding suggests that the neural model is superior to lexicalized models in
predicting long-distance reordering.
1903
2 4 6 822
23
24
25
Distortion Limit
BLEU
 
 
neurallexicalized
Figure 3: BLEU with various distortion limits.
# examples Accuracy BLEU
100,000 83.55 30.92
200,000 84.40 31.03
300,000 84.55 31.01
400,000 84.95 30.93
500,000 85.25 31.27
3,000,000 85.55 31.03
Table 3: Effect of training corpus size.
Vectors MT06 MT02 MT03 MT04 MT05 MT08
ours 31.03 33.03 32.48 32.52 31.11 25.20
word2vec 30.44 32.28 32.00 32.07 30.24 24.54
Table 4: Comparison of neural reordering models trained based on word vectors produced by our model
(ours) and word2vec (Mikolov et al., 2013).
4.5 The Effect of Training Corpus Size
Table 3 shows the classification accuracy and translation performance with various number of randomly
sampled reordering examples for training the neural classifier. The classification accuracy and transla-
tion performance generally rise as the number of reordering example increases.
6
Surprisingly, both the
classification accuracy and translation performance of using 500,000 reordering examples are close to
using 3,000,000 reordering examples, suggesting that a relatively small amount of reordering examples
are enough for training a robust classifier.
4.6 Learned Vector Space Representations
We randomly sampled 200,000 English phrases and found 999 clusters according to the vector space
representations computed by recursive autoencoders using the k-means algorithm (MacQueen, 1967).
The distance between two phrases is calculated by the Euclidean distance between their vector space
representations.
Figure 4 shows 10 of the 999 clusters. An interesting finding is that phrase pairs that are close in the
vector space share with similar reordering patterns rather than semantic similarity. For example, ?by
june 1? and ?within the agencies? have similar distributions on the left/right orientations but are totally
unrelated in terms of meaning. As a result, the vector representations of words trained using unlabeled
data hardly helps in training the neural reordering model. Table 4 shows the results when we replace
the word vectors of our model with those trained using word2vec (Mikolov et al., 2013). The recursive
autoencoders and the classifier are retrained. The performance of the neural reordering model trained in
this way drops significantly, which confirms our analysis.
5 Related Work
Reordering as classification is a common way to alleviate the data sparsity problem. Xiong et al. (2006)
use a maximum entropy model to predict whether to merge two blocks in a straight or an inverted order
in their ITG decoder. Nguyen et al. (2009) build a similar model for hierarchical phrase reordering
models (Galley and Manning, 2008). Green et al. (2010) and Yahyaei and Monz (2010) predict finer-
grained distance bins instead. Another direction is to learn sparse reordering features and create more
flexible distributions (Cherry, 2013). Although these models are effective, feature engineering is a major
challenge. In contrast, our neural reordering model is capable of learning features automatically.
6
The reason why the BLEU scores oscillate slightly on the training set is that classification accuracy is not directly correlated
with BLEU scores. Optimizing the neural reordering model directly with respect to BLEU score may further improve the
performance. We leave this for future work.
1904
but is willing toeconomy is required to
range of services to said his visit is to
is making use of
june 18, 2001late 2011
as detention centergroup all togethertake care of oldby june 1
and complete by end 1998
or other economicand for other
within the agencies
Figure 4: Phrase clusters as calculated by the Euclidean distance in the vector space. English phrases
that have similar reordering probability distributions rather than similar semantic similarity fall into one
cluster.
Along another line, n-gram-based models (Mari`no et al., 2006; Durrani et al., 2011; Durrani et al.,
2013) treat translation as Markov chains over minimal translation units (Mari`no et al., 2006; Durrani et
al., 2013) or operations (Durrani et al., 2011) directly. Although naturally leveraging both the source and
target side contexts, these approaches still face the data sparsity problem.
Our work is closely related to Li et al. (2013). The major difference is that Li et al. (2013) need to
compute vector space representation for variable-sized blocks ranging from words to sentences on the
fly both in training and decoding. In contrast, we only need to compute vectors for phrases with up to 7
words in the training phase, which makes our approach simpler and more scalable to large data.
6 Conclusion
We have shown that surrounding context is effective for resolving reordering ambiguities in phrase-based
models. As the data sparseness problem is the major challenge for using context in reordering models,
we propose to use a single classifier based on recursive autoencoders to predict reordering orientations.
Experimental results show that our neural reordering model outperforms the state-of-the-art lexicalized
reordering models significantly and consistently across all the NIST datasets under various settings.
There are a few future directions we plan to explore. First, as the machine translation system and neu-
ral classifier are trained separately, the neural network training only has an indirect effect on translation
quality. Jointly training the machine translation system and neural classifier is an interesting topic. Sec-
ond, it is interesting to develop more efficient models to leverage larger contexts to resolve reordering
ambiguities. Third, we plan to extend our work to other translation models such as syntax-based and
n-gram based models (Mari`no et al., 2006; Durrani et al., 2011; Durrani et al., 2013). Finally, as we cast
phrase reordering as two-category classification problem (i.e, left vs. right), it is interesting to intersect
structured SVM (Tsochantaridis et al., 2005) with neural networks to develop a large margin training
algorithm for our neural reordering model.
Acknowledgements
This research is supported by the 973 Program (No. 2014CB340501), the National Natural Science
Foundation of China (No. 61331013), the 863 Program (No. 2012AA011102), Toshiba Corporation
Corporate Research & Development Center, and the Singapore National Research Foundation under its
International Research Centre @ Singapore Funding Initiative and administered by the IDM Programme.
1905
References
Yaser Al-Onaizan and Kishore Papineni. 2006. Distortion models for statistical machine translation. In Pro-
ceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, pages 529?536.
James Bergstra and Yoshua Bengio. 2012. Random search for hyper-parameter optimization. Journal of Machine
Learning Research, 13(1):281?305.
Arianna Bisazza and Marcello Federico. 2012. Modified distortion matrices for phrase-based statistical machine
translation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 478?487.
Colin Cherry. 2013. Improved reordering for phrase-based translation using sparse features. In Proceedings of
the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, pages 22?31.
Nadir Durrani, Helmut Schmid, and Alexander Fraser. 2011. A joint sequence translation model with integrat-
ed reordering. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:
Human Language Technologies, pages 1045?1054.
Nadir Durrani, Alexander Fraser, Helmut Schmid, Hieu Hoang, and Philipp Koehn. 2013. Can Markov models
over minimal translation units help phrase-based SMT? In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (Volume 2: Short Papers), pages 399?405.
Yang Feng, Haitao Mi, Yang Liu, and Qun Liu. 2010. An efficient shift-reduce decoding algorithm for phrased-
based machine translation. In Proceedings of the 23rd International Conference on Computational Linguistics:
Posters, pages 285?293.
Michel Galley and Christopher D. Manning. 2008. A simple and effective hierarchical phrase reordering model.
In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 848?856.
Christoph Goller and Andreas Kuchler. 1996. Learning task-dependent distributed representations by backprop-
agation through structure. In Proceedings of 1996 IEEE International Conference on Neural Networks (Vol-
ume:1), volume 1, pages 347?352.
Spence Green, Michel Galley, and Christopher D. Manning. 2010. Improved models of distortion cost for statis-
tical machine translation. In Proceedings of Human Language Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for Computational Linguistics, pages 867?875.
Kenneth Heafield. 2011. KenLM: faster and smaller language model queries. In Proceedings of the EMNLP 2011
Sixth Workshop on Statistical Machine Translation, pages 187?197.
Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 144?151.
Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In Proceedings of the
46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages
586?594.
Maxim Khalilov and Khalil Simaan. 2010. Source reordering using maxent classifiers and supertags. In Proceed-
ings of The 14th Annual Conference of the European Association for Machine Translation, pages 292?299.
Kevin Knight. 1999. Decoding complexity in word-replacement translation models. Computational Linguistics,
25(4):607?615.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings
of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on
Human Language Technology-Volume 1, pages 48?54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan
Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th
Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo
and Poster Sessions, pages 177?180.
Peng Li, Yang Liu, and Maosong Sun. 2013. Recursive autoencoders for ITG-based translation. In Proceedings
of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 567?577.
1906
DongC. Liu and Jorge Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Math-
ematical Programming, 45(1-3):503?528.
James MacQueen. 1967. Some methods for classification and analysis of multivariate observations. In Proceed-
ings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability, volume 1, pages 281?297.
Jos?e B. Mari`no, Rafael E. Banchs, Josep M. Crego, Adri`a de Gispert, Patrik Lambert, Jos?e A. R. Fonollosa, and
Marta R. Costa-juss`a. 2006. N-gram-based machine translation. Computational Linguistics, 32(4):527?549.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic regularities in continuous space word rep-
resentations. In Proceedings of the 2013 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, pages 746?751.
Vinh Van Nguyen, Akira Shimazu, Minh Le Nguyen, and Thai Phuong Nguyen. 2009. Improving a lexicalized
hierarchical reordering model using maximum entropy. In Proceedings of The twelfth Machine Translation
Summit (MT Summit XII).
Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation.
Computational Linguistics, 30(4):417?449.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar,
Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A smorgasbord
of features for statistical machine translation. In Proceedings of the Human Language Technology Conference
of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004: Main
Proceedings, pages 161?168.
Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st
Annual Meeting of the Association for Computational Linguistics, pages 160?167.
David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. 1986. Learning internal representations by
error propagation. Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 1:
Foundations, pages 318?362.
Richard Socher, Eric H. Huang, Jeffrey Pennin, Andrew Y. Ng, and Christopher D. Manning. 2011a. Dynamic
pooling and unfolding recursive autoencoders for paraphrase detection. In Proceedings of Advances in Neural
Information Processing Systems 24, pages 801?809.
Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Manning. 2011b. Semi-
supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the 2011 Conference
on Empirical Methods in Natural Language Processing, pages 151?161.
Christoph Tillman. 2004. A unigram orientation model for statistical machine translation. In Proceedings of the
Human Language Technology Conference of the North American Chapter of the Association for Computational
Linguistics: HLT-NAACL 2004: Short Papers, pages 101?104.
Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, and Yasemin Altun. 2005. Large margin methods
for structured and interdependent output variables. Journal of Machine Learning Research, 6:1453?1484.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maximum entropy based phrase reordering model for statistical
machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and
44th Annual Meeting of the Association for Computational Linguistics, pages 521?528.
Sirvan Yahyaei and Christof Monz. 2010. Dynamic distortion in a discriminative reordering model for statistical
machine translation. In Proceedings of the 7th International Workshop on Spoken Language Translation, pages
353?360.
Mikhail Zaslavskiy, Marc Dymetman, and Nicola Cancedda. 2009. Phrase-based statistical machine translation
as a traveling salesman problem. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 333?341.
Richard Zens, Hermann Ney, Taro Watanabe, and Eiichiro Sumita. 2004. Reordering constraints for phrase-
based statistical machine translation. In Proceedings of the 20th International Conference on Computational
Linguistics, pages 205?211.
1907
