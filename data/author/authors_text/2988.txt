75
76
77
78
Maximum Entropy Modeling in Sparse Semantic Tagging ?
Jia Cui
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD, 21210, USA
cuijia@jhu.edu
David Guthrie
Department of Computer Science
University of Sheffield
Sheffield, S1 4DP, UK
D.Guthrie@dcs.shef.ac.uk
Abstract
In this work, we are concerned with a coarse
grained semantic analysis over sparse data, which
labels all nouns with a set of semantic categories.
To get the benefit of unlabeled data, we propose
a bootstrapping framework with Maximum En-
tropy modeling (MaxEnt) as the statistical learn-
ing component. During the iterative tagging pro-
cess, unlabeled data is used not only for better
statistical estimation, but also as a medium to in-
tegrate non-statistical knowledge into the model
training. Two main issues are discussed in this
paper. First, Association Rule principles are sug-
gested to guide MaxEnt feature selections. Sec-
ond, to guarantee the convergence of the boot-
strapping process, three adjusting strategies are
proposed to soft tag unlabeled data.
1 Introduction
Semantic analysis is an open research field in natural lan-
guage processing. Two major research topics in this field are
Named Entity Recognition (NER) (N. Wacholder and Choi,
1997; Cucerzan and Yarowsky, 1999) and Word Sense Dis-
ambiguation (WSD) (Yarowsky, 1995; Wilks and Steven-
son, 1999). NER identifies different kinds of names such as
?person?,?location? or ?date?, while WSD distinguishes the
senses of ambiguous words. For example, ?bank? can be la-
beled as ?financial institution? or ?edge of a river?. Our task
of semantic analysis has a more general purpose, tagging
all nouns with one semantic label set. Compared with NE,
which only considers names, our task concerns all nouns.
Unlike WSD, in which every ambiguous word has its own
range of sense set, our task aims at another set of semantic
labels, shared by all nouns. The motivation behind this work
is that a semantic category assignment with reliable perfor-
mance can contribute to a number of applications including
statistical machine translation and sub-tasks of information
extraction.
?This work was supported in part by NSF grant numbers IIS-
0121285. Any opinions, fndings and conclusions or recommenda-
tions expressed in this material are the authors? and do not neces-
sarily reflect those of the sponsor.
The semantic categories adopted in this paper come from
the Longman Dictionary. These categories are neither par-
allel to nor independent of each other. One category may
denote a concept which is a subset of that of another. Exam-
ples of the category structures are illustrated in Figure 1.
Figure 1: Structure of some semantic categories used in the
Langman dictionary.
Maximum Entropy (MaxEnt) principle has been success-
fully applied in many classification and tagging tasks (Rat-
naparkhi, 1996; K. Nigam and A.McCallum, 1999; A. Mc-
Callum and Pereira, 2000). We use MaxEnt modeling as the
learning component. A major issue in MaxEnt training is
how to select proper features and determine the feature tar-
gets (Berger et al, 1996; Jebara and Jaakkola, 2000). To
discover useful features, we exploit the concept of Associ-
ation Rules (AR) (R. Agrawal and Swami, 1993; Srikant
and Agrawal, 1997), which is originally proposed in Data
Mining research field to identify frequent itemsets in a large
database.
Like many other classification tasks, human-annotated
data for semantic analysis is expensive and limited, while
a large amount of unlabeled data is easily obtained. Many
researchers (Blum and Mitchell, 1998; K. Nigam and
Mitchell, 2000; Corduneanu and Jaakkola, 2002) have at-
tempted to improve performance with unlabeled data. In
this paper, we also propose a framework to bootstrap with
unlabeled data. Fractional counts are assigned to unlabeled
instances based on current model and accessible knowledge
sources. Pooled with human-annotated data, unlabeled data
contributes to the next MaxEnt model.
We begin with an introduction of our bootstrapping
framework and MaxEnt training. An interesting MaxEnt
puzzle is presented, with its derivation showing possible di-
rections of utilizing unlabeled data. Then the feature selec-
tion criterion guided by AR is discussed as well as the indi-
cator selection (section 3). We discuss initialization meth-
ods for the unlabeled data. Strategies to guarantee conver-
gence of bootstrapping process and approaches of integrat-
ing non-statistical knowledge are proposed in section 4. Fi-
nally, experimental results are presented along with conclu-
sions and future work.
2 Bootstrapping and the MaxEnt puzzle
An instance in the corpus includes the headword, which is
the noun to be labeled, and its context. To integrating the un-
labeled instances in the training process, we propose a tag-
ging method called soft tagging. Unlike the normal tagging,
in which each instance is assigned only one label, soft tag-
ging allows one instance to be assigned several labels with
each of them being associated with a fractional credit. All
credits assigned to one instance should sum up to 1. For
example, a raw instance with the headword ?club? can be
soft tagged as (movable J:0.3, not-movable N:0.3, collective
U:0.4). Once all auxiliary instances have been assigned se-
mantic labels, we pool them together with human-annotated
data to select useful features and set up feature expectations.
Then a log-linear model is trained for several iterations to
maximize likelihood of the whole corpora. With the updated
MaxEnt model, unlabeled data will be soft tagged again.
The whole process is repeated until convergence condition
is satisfied. This framework is illustrated in Figure 2.
In building a mexent model, unlabeled data contributes
differently to the feature selection and target estimation
compared to the human-annotated data. While human-
annotated instances never change, tags in unlabeled data
keep updating according to the new MaxEnt model in each
bootstrapping iteration, which might lead to a different fea-
ture set.
Figure 2: The bootstrapping framework
Before presenting the MaxEnt puzzle, let?s first consider
a regular MaxEnt formulation. Let l be a label and x be
an instance. Let kh(l, x) be the h-th binary feature func-
tion which is equal to 1 if (l, x) activates the h-th feature
and 0 otherwise. P (l, x) is given by the model, denoting the
probability of observing both l and x. P? (l, x) is the empir-
ical frequency of (l, x) in the training data. The constraint
associated with feature kh(l, x) is represented as:
?
l,x
P (l,x) kh(l,x) =
?
l,x
P? (l,x) kh(l,x) (1)
In practice, we are interested in conditional models
P (l|x), which assign probability mass to a label set given
supporting evidence x. And we approximate P (x) with
P? (x) . Consequently,
?
x
P? (x)
?
l
P (l|x) kh(l,x) =
?
l,x
P? (l,x) kh(l,x) (2)
Next, we show how the effect of unlabeled data disap-
pears during the bootstrapping in a restricted situation. We
make the following assumptions:
1. The feature targets are the raw frequencies in the train-
ing data. No smoothing is applied.
2. During the bootstrapping, we maintain a fixed feature
set, while the feature targets might change.
3. The fractional credit assigned to label l and instance x
in the t+ 1 iteration is P t(l|x).
Let N be the total number of instance(1, ? ? ? , N ) in la-
beled data and M be the total number of instance(N +
1, ? ? ? , N + M ) in unlabeled data. Let ? be the weight as-
signed to unlabeled instances. The new constraint for the
h-th feature function can be rewritten as:
1
N + ?M
N?
i=1
?
l
P t+1(l|xi) kh(l,xi) (3)
+ ?N + ?M
N+M?
i=N+1
?
l
P t+1(l|xi) kh(l,xi)
= 1N + ?M
N?
i=1
kh(li,xi)
+ ?N + ?M
N+M?
i=N+1
?
l
P t(l|xi) kh(l,xi)
where t is the index of the bootstrap iterations. li is the
human-annotated label attached to the ith instance.
When the procedure converges, P t(l|xi) = P t+1(l|xi),
the second parts on both sides will cancel each other. Fi-
nally, the constraint will turn out to be:
N?
i=1
?
l
P t+1(l|xi) kh(l,xi) =
N?
i=1
kh(li,xi) (4)
which includes statistics only from labeled data. If the fea-
ture set stays the same, unlabeled data will make no con-
tribution to the whole constraint set. A model which can
satisfy Equation 3 must be able to satisfy Equation 4. If un-
labeled instances satisfy the same distribution as labeled in-
stances, given the same set of constraints, the model trained
on only labeled data would be equivalent to the one trained
on both.
The above derivation shows that with the three restric-
tions, the soft tagged instances make no contribution to the
constraint set, which is counter-intuitive. That is why we
call it a ?MaxEnt Puzzle?. Consequently, to utilize unla-
beled data, we should break the three restrictions: to re-
select feature set after each bootstrapping iteration, or adjust
the soft tagging results given by the model.
3 Feature selection
Berger et al(1996) proposed an iterative procedure of
adding news features to feature set driven by data. We
present a simple and effective approach using some statis-
tical heuristics for feature selection.
There are two kinds of features in MaxEnt modeling:
marginal features and conditional features. Marginal fea-
tures represent the priors of the semantic categories. Condi-
tional features are composed of an indicator and a label. An
indicator can be a single word, an array of words or a word
cluster. Similarly, a label can be either a single semantic
category or a set of categories.
One major motivation of combining unlabeled data with
labeled data in the training process is that unlabeled data
can provide a large pool of valuable feature candidates as
well as more statistical information. An (indicator, label)
pair may appear only once in labeled data, but it may oc-
cur very frequently in soft tagged data, it might be selected
as a feature during the bootstrapping. Similarly, a feature
selected only from human-annotated data may become rel-
atively infrequent when the soft tagged instances are added,
thus being eliminated from the feature set.
3.1 Indicators
A good indicator should have an obvious preference for def-
inite semantic categories. For instance, ?good?,?great? are
weak indicators because they can modify almost all seman-
tic categories; ?drinkable? and ?light-weighted? are usually
associated with ?liquid (L)? and ?movable (J)? respectively,
thus are strong indicators. The quality of an indicator can be
measured by the entropy of the semantic categories it mod-
ifies. The lower the entropy is, the stronger preference the
indicator has.
To overcome the data sparsity problem, several indica-
tors can be clustered together to form a new single indica-
tor. There are two possible ways of clustering. One is hard
clustering, which divides all words into non-overlapping
classes; The other is soft clustering, which permits one word
to belong to different classes. Usually, words with similar
semantic preferences are clustered together.
3.2 Association rules in feature selection
An indicator can be combined with different labels to form
different features. But not all pairs including good indica-
tors are good features. In a statistical learning algorithm,
a good feature should first be statistically reliable, which is
considered in most of the MaxEnt modeling implementa-
tions. However, with this single requirement, it is possible
that (x, l) is a feature if both x and l are frequent, but the
chance of seeing label l is tiny given the presence of indica-
tor x. More constraints are necessary to pinpoint indicative
features. We exploit Association Rule (AR) principles to put
more restrictions in feature selecting.
Let x be an indicator and l be a label. count(x, l) de-
notes the number of instances which include indicator x and
are attached with label l in the training corpus. N is the to-
tal number of instances. We define three measurements to
characterize the goodness of a given (x, l) pair from three
different points. We put thresholds on each of them.
support(x, l) denotes how frequently indicator x and la-
bel l occur together.
support(x, l) = count(x, l)N (5)
confidence(x, l) denotes how likely it is to observe label
l when indicator x is at present.
confidence(x, l) = count(x, l)count(x) (6)
improvement(x, l) shows how much more likely to see
label l when indicator x is observed relative to the overall
probability of seeing l.
improvement(x, l) = count(x, l)/count(x)count(l)/N (7)
The concept of improvement can be extended to each
part of the indicators. Let S be the feature set.
improvement(x, l) = minx??x,(x?,l)?S
confidenc(x, l)
confidenc(x?, l)
(8)
For instance, assume the improvement threshold is 1.
Let x=x1x2, if confidence(x1, l) > confidence(x1x2, l),
then (x1x2, l) will be ignored and only (x1, l) is selected
as a feature; otherwise, only (x1x2, l) remains and (x1, l)
is ignored. This idea can be applied to remove embedded
features, reducing the redundancy in the feature set.
The assumption behind those criteria is that most features
should be positive, i.e., given feature (x,l), the existence of
indicator x always increase the odds of seeing label l, other
than decrease it.
4 Soft tagging
Initialization of unlabeled instances is important for our
bootstrapping framework, because its effect will be carried
on from one iteration to another. Even if the auxiliary data
(unlabeled data included in training) has no effect on the
constraint set under some circumstances, the soft tagged
auxiliary data is still a part of the training corpus, the like-
lihood of which is to be maximized in the MaxEnt training.
On one hand, we wish the initialization to be as close as
possible to the real value, of which we have no idea except
for the knowledge from labeled data; on the other hand, we
wish the tagging of the auxiliary data could be slightly dif-
ferent from the model trained only on labeled data, to avoid
converging to this model itself.
As we have seen in section 2, using the model generated
in the previous iteration directly to soft tag the auxiliary data
is one of the causes diluting the effect of the auxiliary data.
Therefore, we propose to adjust the soft tagging results by
non-statistical knowledge or through fixing some of the tags.
4.1 Initialization of unlabeled data
Inspired by the MaxEnt puzzle, we realize the importance
of additional knowledge sources. To assign inital fractional
credits to unlabeled instances, we look up a dictionary to
narrow down choices. In particular,
1. If the headword of the instance has a unique semantic
label choice according to the dictionary, it gets credit 1
for this label and 0 for others.
2. If the headword has been observed in labeled data and
it has more than one possible labels in the dictionary, it
is initialized as follows:
Let w be the headword and l be a label. m is the to-
tal number of permitted labels for w according to the
dictionary. Let L(w) be the set of (w, l) pairs in la-
beled data and D(w) be the set of (w, l) permitted in
the dictionary.
If D(w) = L(w), then
credit(w, l) =
{
count(w,l)
count(w) if (w, l) ? L(w)
0 otherwise
Otherwise, we add one and renormalize:
credit(w, l) =
{
count(w,l)+1
count(w)+m if (w, l) ? D(w)
0 otherwise
3. If the headword never occurs in human-annotated data,
we initialize the credits with the flat distribution.
credit(w, l) =
{ 1
m if (w, l) ? D(w)
0 otherwise
4.2 Adjust credits according to their distributions
Considering the bootstrapping process as a tug-of-war be-
tween labeled and unlabed data, with the flag denoting the
model. In each bootstrapping iteration, tags of the auxil-
iary data are updated in accordance with the model. Unla-
beled data is moving towards the labeled data while the latter
stands still. One way to prevent the auxiliary data from be-
ing dragged completely to the point of the labeled data is to
nail down part of the auxiliary data tags. For example, sup-
pose an instance gets credits (0.8,0.15,0.05). Since the first
category is much more likely than the others, we can fix the
credits as (1,0,0) for ever. Another choice would be to re-
move the least probable category, i.e., changing the credits
to (0.84,0.16,0). An even more radical method is to remove
the whole instance from the auxiliary data if it has no strong
semantic preferences.
Here are the details of these adjusting strategies:
rmlow set the least probable category with credit zero and
renormalize the remaining credits. Whether to apply
this action or not can depend on thresholding on the
mininal credit, the ratio of the minimal credit to the
maximal credit, or the entropy of the credit distribution.
kphigh assign all credit 1 to the most probable category.
Possible thresholds can be set for the maximal credit,
the ratio of the maximal credit to the second maximal
credit, or the entropy of the credit distribution.
rmevent remove the instance if it has a flat credit distri-
bution after several iterations. Possible thresholds can
be set for the maximal credit, the ratio of the maximal
credit to the minimal credit, or the entropy of the credit
distribution.
The effect of those actions is permanent. That is, if a cat-
egory is forbidden (set to zero) for one instance, it will be
forbidden for this instance in the following iterations. Sim-
ilarly, if an instance is removed, it will never be used in the
training process again.
4.3 Adjust credits with non-statistical knowledge
Non-statistical knowledge is the knowlege which is not ob-
tainable from statistics in a sparse data set, but rather from
other resources like a dictionary or WordNet. This kind of
knowledge may not be expressed in labeled data, therefore
could not be used to form features. For instance, we can ob-
tain easily from a dictionary that word ?long-tailed? is used
to modify animals. But if ?long-tailed? is rarely observed in
the training data, it will not not selected as a feature, thus
being ignored by the model.
To take advantage of non-statistical knowledge, we use it
to adjust soft tagging results. One possibility is pruning the
illegal categories with a dictionary. By doing so, this kind
of knowledge can affect the MaxEnt modeling indirectly.
Similarly, the preference of a context word could also be
used to adjust the credit distributions.
5 Experiments
5.1 Corpus
The corpus used in our experiments is provided by Sheffield
University. The human-annotated data is divided into train-
ing (SHD) and test (Blind). SHD contains 197K instances
and Blind contains 13K instances.
The Longman dictionary provides 36 semantic cate-
gories, among which only 21 most frequently used cate-
gories with the exception of ?unknown (Z)? are used in our
experiments. The distribution of the semantic categories is
far from uniform. Semantic category ?abstract (T)? alone
forms 60% (126K) of the human-annotated instances. The
histogram of the other categories is shown in Figure 3.
The inter-annotator agreement for the labeled data is 95%
with exact match. Even though some semantic categories
form a hierarchical structure, we always assume that human
annotators chose the most specific categories. Evaluation of
the classification error rate (CER) in the following experi-
ments also uses exact match only.
5.2 Feature selection with Association Rule principles
The first group of experiments are designed to test different
indicators as well as the feature selection strategies, without
using unlabeled instances or bootstrapping. The indicators
and feature selection thresholds which result in the best per-
formance are used in the bootstrapping experiments.
Figure 3: The number of instances labeled with semantic
categories other than T
Since 96% of the headwords in Blind are also in SHD,
headwords are the most effective indicators. Using head-
words alone can get 9.47% classification error rate. Other
indicators investigated include adjectives, which modify
headwords, and their clusters. As we mentioned before,
clustering can alleviate data sparseness problem. So we
group headwords and adjectives. Headword clusters are soft
clusters. The ambiguous headwords can belong to several
clusters as long as the corresponding categories of those
clusters are permitted for these headwords in the dictionary.
Adjectives are hard clustered. We discard general common
adjectives according to their entropies. We keep only those
with strong semantic preferences and assign them to the
clusters corresponding to their best preferences.
We also include compound features such as headword-
adjective, headword-adjectivecluster and other combina-
tions.
Table 1 shows the experimental results with all types
of indicators which are mentioned above, and compared
the results with the baseline. For different types of fea-
tures, we set up different thresholds. In config-1, only the
support thresholds are used. From config-2 to config-5,
we raise confidence and improvement thresholds grad-
ually to lower the number of features. The CER results
show that with proper thresholds, using confidence and
improvement not only helps decreasing the feature set size
by as much as 26%, but also improves the performance.
The last column of the table 1 shows the p-value of sig-
nificnce test between each configuration and its precedence.
Clearly, config 1,2,and 3 perform similar to each other,
though they are significant better than using headwords only.
And config 4 and 5 are significant better than config 3.
5.3 Bootstrapping
In this group of experiments, we use Blind as the auxiliary
data in the bootstrapping process. Blind data is initialized
with the methods mentioned in section 4. No human labels
in Blind are included in the training process. The weight
for the auxiliary data ? is set to 1 in our experiments. The
Longman dictionary is always used in soft tagging to elimi-
nate the illegal categories.
The bootstrapping process will stop when the constraint
set does not change. Since the feature set is re-selected in
configuration # of features CER(%) p-value
headwords only 12514 9.47 -
config-1 49861 8.18 < 1.0e?12
config-2 44141 8.18 0.53
config-3 39136 7.94 0.034
config-4 36631 7.62 < 1.0e?3
config-5 32214 7.64 0.65
Table 1: Classification Error Rates (CER) with different fea-
ture sets
each iteration, the convergence of the iterative steps is diffi-
cult to achieve. Fluctuations might be observed. With strate-
gies proposed in 4.2, more and more auxiliary instances can
be fixed with one semantic category. The whole process will
stop at some point.
Figure 4 plots the CER in the first 20 iterations for dif-
ferent setups. It shows adding Blind into the bootstrapping
process can lower the error rate to 7.37%. Without any soft
tagging adjustment (normal), the training process does not
stop and the CER drops at first but then climbs up later.
Using entropy as thresholds is an efficient way to en-
sure the convergence for both rmlow and kphigh. Action
rmevent can also keep the results from getting much worse
during bootstrapping.
Figure 4: Error rate in each iteration for different soft tag-
ging strategies
6 Conclusions
We introduce a general semantic analysis task, which labels
all nouns with a unique set of semantic categories. In order
to get benefitted from unlabeled data, we propose a boot-
strapping framework with MaxEnt modeling as the main
learning component. Unlabeled data, after soft tagging, can
be combined with labeled data to provide evidence for Max-
Ent feature selection and target estimation. Moreover, non-
statistical knowledge can affect the modeling indirectly by
adjusting the soft tagging results of the auxiliary data.
For MaxEnt training, we suggest using Association Rule
principles to control the feature selection. The intuition be-
hind these criteria is that the very frequent, strongly prefered
(indicator,label) pairs are good enough for model training.
Using AR principles not only decreases the size of the fea-
ture set dramatically, but also improves the performances.
But there is no good way to set the AR thresholds properly
yet.
With the feature set changing in each iteration, the con-
vergence of the bootstrapping is hard to guarantee. By
sharpening the soft tagging results through removing the
least probable label or keeping only the most probable la-
bel, we can speed up convergence.
7 Future work
At present, only headwords and adjectives are used to com-
pose indicators. We plan to incorporate linguistically en-
riched features. Using parsing, we can locate verbs and use
their relationships with headwords as new indicators. An-
other type of potential indicators are the subject codes of the
headwords. Subject codes represent finer grained semantic
classifications. Some of the instances in human-annotated
data have been marked with subject codes. There are a to-
tal of 320 subject codes, such as ?architecture?,?agriculture?
and ?business?. We believe that knowing the subject codes
of the headwords will help to decrease the entropy of the
headword senses.
We have a very large data set BNC corpus, which comes
from the same source as the labeled data we use. BNC data
is composed of 1.2M unambiguous instances, 1.4M seen
instances (ambiguous instances with headwords being ob-
served in SHD) and 0.4M unseen instances (ambiguous in-
stances with headwords never being observed in SHD). In
the future, we will try bootstrapping with BNC data.
Alternative indicator clustering techniques will be ex-
plored too. We have used entropy as a heuristic in the ex-
periments; an alternative heuristic that can be employed is
mutual information.
We have provided a framework to utilize non-statistical
knowledge. In addition to using a dictionary to limit the
choices of unlabeled data, we can obtain plenty of informa-
tion about word sense preferences from the WordNet for soft
tagging adjustment.
The bootstrapping process is closely related to
Expectation-Maximization procedure, in which soft
tagging can be ragarded as the E-step. In many practical
EM implementations, however, updating at the E-step does
not use the exact theoretical value. The modification taken
in the E-step can be a linear combination of the old value
and the new calculated one. Similar re-estimation strategies
can be applied in our work. A theoretical description
of the relationship between EM and soft tagging would
potentially be able to identify convergency properties of the
bootstrapping framework.
Acknowledgments
The authors would like to thank Prof. Frederick Jelinek and
Dr. Louise Guthrie and all other members of the Seman-
tic group in 2003 JHU Summer Research Workshop. They
are also grateful to the anonymous reviewers for their very
insightful comments and suggestions.
References
D. Freitag A. McCallum and F. Pereira. 2000. Maximum
entropy markov models for information extraction and
segmentation. In Proc. 17th International Conf. on Ma-
chine Learning, pages 591?598.
A. L. Berger, S. D. Pietra, and V. J. D. Pietra. 1996. A max-
imum entropy approach to natural language processing.
Computational Linguistics, 22(1):39?71.
A. Blum and T. Mitchell. 1998. Combining labeled and
unlabeled data with co-training. In COLT: Proceedings
of the Workshop on Computational Learning Theory.
A. Corduneanu and T. Jaakkola. 2002. Continuation meth-
ods for mixing heterogeneous sources. In Proceedings
of the Eighteenth Annual Conference on Uncertainty in
Artificial Intelligence.
S. Cucerzan and D. Yarowsky. 1999. Language indepen-
dent named entity recognition combining morphological
and contextual evidence. In joint SIGDAT Conference
on Empirical Methods in NLP and Very Large Corpora,
pages 90?99.
T. Jebara and T. Jaakkola. 2000. Feature selection and dual-
ities in maximum entropy discrimination. In Uncertainity
In Artificial Intellegence.
J.Lafferty K. Nigam and A.McCallum. 1999. Using maxi-
mum entropy for text classification. In IJCAI-99 Work-
shop on Machine Learning for Information Filtering,
pages 61?67.
S. Thrun K. Nigam, A. K. McCallum and T. M. Mitchell.
2000. Text classification from labeled and unlabeled doc-
uments using em. Machine Learning, 39(2-3):103?134.
Y. Ravin N. Wacholder and M. Choi. 1997. Disambiguation
of proper names in text. In Proceedings of Fifth Con-
ference on Applied Natural Language Processing, pages
202?208.
T. Imielinski R. Agrawal and A. N. Swami. 1993. Mining
association rules between sets of items in large databases.
In Peter Buneman and Sushil Jajodia, editors, Proceed-
ings of the 1993 ACM SIGMOD International Conference
on Management of Data, pages 207?216, Washington,
D.C., 26?28 .
A. Ratnaparkhi. 1996. A maximum entropy model for
part of speech tagging. In Proceedings of the Conference
on Empirical Methods in Natural Language Processing,
pages 133?142.
R. Srikant and R. Agrawal. 1997. Mining generalized as-
sociation rules. Future Generation Computer Systems,
13(2?3):161?180.
Y. Wilks and M. Stevenson. 1999. The grammar of sense:
Using part-of-speech tags as a first step in semantic dis-
ambiguation. Journal of Natural Language Engineering,
4(3).
D. Yarowsky. 1995. Unsupervised word sense disambigua-
tion rivaling supervised methods. In Meeting of the Asso-
ciation for Computational Linguistics, pages 189?196.
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 407?411,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Towards the Orwellian Nightmare 
Separation of Business and Personal Emails 
 
Sanaz Jabbari, Ben Allison, David Guthrie, Louise Guthrie 
Department of Computer Science 
University of Sheffield 
211 Portobello St. 
Sheffield 
S1 4DP 
{s.jabbari, b.allison, d.guthrie, l.guthrie}@dcs.shef.ac.uk 
 
Abstract 
This paper describes the largest scale annotation pro-
ject involving the Enron email corpus to date. Over 
12,500 emails were classified, by humans, into the 
categories ?Business? and ?Personal?, and then sub-
categorised by type within these categories. The paper 
quantifies how well humans perform on this task 
(evaluated by inter-annotator agreement). It presents 
the problems experienced with the separation of these 
language types. As a final section, the paper presents 
preliminary results using a machine to perform this 
classification task. 
 
1 Introduction 
Almost since it became a global phenomenon, com-
puters have been examining and reasoning about our 
email. For the most part, this intervention has been 
well natured and helpful ? computers have been try-
ing to protect us from attacks of unscrupulous blanket 
advertising mail shots. However, the use of computers 
for more nefarious surveillance of email has so far 
been limited. The sheer volume of email sent means 
even government agencies (who can legally intercept 
all mail) must either filter email by some pre-
conceived notion of what is interesting, or they must 
employ teams of people to manually sift through the 
volumes of data. For example, the NSA has had mas-
sive parallel machines filtering e-mail traffic for at 
least ten years. 
 
The task of developing such automatic filters at re-
search institutions has been almost impossible, but for 
the opposite reason. There is no shortage of willing 
researchers, but progress has been hampered by the 
lack of any data ? one?s email is often hugely private, 
and the prospect of surrendering it, in its entirety, for 
research purposes is somewhat unsavoury. 
 
Recently, a data resource has become available where 
exactly this condition (several hundred people?s entire 
email archive) has been satisfied ? the Enron dataset. 
During the legal investigation of the collapse of En-
ron, the FERC (Federal Energy Regulatory Commis-
sion) seized the emails of every employee in that 
company. As part of the process, the collection of 
emails was made public and subsequently prepared 
for research use by researchers at Carnegie Melon 
University (Klimt and Yang, 2004).Such a corpus of 
authentic data, on such a large scale, is unique, and an 
invaluable research tool. It then falls to the prospec-
tive researcher to decide which divisions in the lan-
guage of email are interesting, which are possible, and 
how the new resource might best be used.  
 
Businesses which offer employees an email system at 
work (and there are few who do not) have always 
known that they possess an invaluable resource for 
monitoring their employees? work habits. During the 
1990s, UK courts decided that that an employee?s 
email is not private ? in fact, companies can read 
them at will. However, for exactly the reasons de-
scribed above, automatic monitoring has been impos-
sible, and few businesses have ever considered it suf-
ficiently important to employ staff to monitor the 
email use of other staff. However, in monitoring staff 
productivity, few companies would decline the use of 
a system which could analyse the email habits of its 
employees, and report the percentage of time which 
each employee was spending engaged in non-work 
related email activities. 
 
The first step in understanding how this problem 
might be tackled by a computer, and if it is even fea-
sible for this to happen, is to have humans perform the 
task. This paper describes the process of having hu-
mans annotate a corpus of emails, classifying each as 
to whether they are business or personal, and then 
attempting to classify the type of business or personal 
mail being considered. 
 
A resource has been created to develop a system able 
to make these distinctions automatically. Furthermore, 
the process of subcategorising types of business and 
personal has allowed invaluable insights into the areas 
407
where confusion can occur, and how these confusions 
might be overcome. 
 
The paper presents an evolution of appropriate sub-
categories, combined with analysis of performance 
(measured by inter-annotator agreement) and reasons 
for any alterations. It addresses previous work done 
with the Enron dataset, focusing particularly on the 
work of Marti Hearst at Berkeley who attempted a 
smaller-scale annotation project of the Enron corpus, 
albeit with a different focus. It concludes by suggest-
ing that in the main part (with a few exceptions) the 
task is possible for human annotators. The project has 
produced a set of labeled messages (around 14,000, 
plus double annotations for approximately 2,500) with 
arguably sufficiently high business-personal agree-
ment that machine learning algorithms will have suf-
ficient material to attempt the task automatically. 
2 Introduction to the Corpus 
Enron?s email was made public on the Web by FERC 
(Federal Energy Regulatory Commission), during a 
legal investigation on Enron Corporation. The emails 
cover 92 percent of the staff?s emails, because some 
messages have been deleted "as part of a redaction 
effort due to requests from affected employees". The 
dataset was comprised of 619,446 messages from 158 
users in 3,500 folders. However, it turned out that the 
raw data set was suffering from various data integrity 
problems. Various attempts were made to clean and 
prepare the dataset for research purposes. The dataset 
used in this project was the March 2, 2004 version 
prepared at Carnegie Mellon University, acquired 
from http://www.cs.cmu.edu/~enron/. This version of 
the dataset was reduced to 200,399 emails by remov-
ing some folders from each user. Folders like ?discus-
sion threads? and ?all documents?, which were ma-
chine generated and contained duplicate emails, were 
removed in this version.  
 
There were on average 757 emails per each of the 158 
users. However, there are between one and 100,000 
emails per user. There are 30,091 threads present in 
123,091 emails. The dataset does not include attach-
ments. Invalid email addresses were replaced with 
?user@enron.com?.  When no recipient was specified 
the address was replaced with 
?no_address@enron.com? (Klimt and Yang, 2005). 
 
3 Previous Work with the Dataset 
The most relevant piece of work to this paper was 
performed at Berkeley. Marti Hearst ran a small-scale 
annotation project to classify emails in the corpus by 
their type and purpose (Email annotation at Berkely). 
In total, approximately 1,700 messages were anno-
tated by two distinct annotators. Annotation catego-
ries captured four dimensions, but broadly speaking 
they reflected the following qualities of the email: 
coarse genre, the topic of the email if business was 
selected, information about any forwarded or included 
text and the emotional tone of the email. However, the 
categories used at the Berkeley project were incom-
patible with our requirements for several reasons: that 
project allowed multiple labels to be assigned to each 
email; the categories were not designed to facilitate 
discrimination between business and personal emails; 
distinctions between topic, genre, source and purpose 
were present in each of the dimensions; and no effort 
was made to analyse the inter-annotator agreement 
(Email annotation at Berkely). 
 
User-defined folders are preserved in the Enron data, 
and some research efforts have used these folders to 
develop and evaluate machine-learning algorithms for 
automatically sorting emails (Klimt and Yang, 2004). 
However, as users are often inconsistent in organising 
their emails, so the training and testing data in these 
cases are questionable.  For example, many users 
have folders marked ?Personal?, and one might think 
these could be used as a basis for the characterisation 
of personal emails. However, upon closer inspection it 
becomes clear that only a tiny percentage of an indi-
vidual?s personal emails are in these folders. Simi-
larly, many users have folders containing exclusively 
personal content, but without any obvious folder 
name to reveal this. All of these problems dictate that 
for an effective system to be produced, large-scale 
manual annotation will be necessary. 
 
Researchers at Queen?s University, Canada (Keila, 
2005) recently attempted to categorise and identify 
deceptive messages in the Enron corpus. Their 
method used a hypothesis from deception theory (e.g., 
deceptive writing contains cues such as reduced fre-
quency of first-person pronouns and increased fre-
quency of ?negative emotion? words) and as to what 
constitutes deceptive language. Single value decom-
position (SVD) was applied to separate the emails, 
and a manual survey of the results allowed them to 
conclude that this classification method for detecting 
deception in email was promising. 
 
Other researchers have attempted to analyse the Enron 
emails from a network analytic perspective (Deisner, 
2005).  Their goal was to analyse the flow of commu-
nication between employees at times of crisis, and 
develop a characterisation for the state of a communi-
cation network in such difficult times, in order to 
identify looming crises in other companies from the 
state of their communication networks. They com-
pared the network flow of email in October 2000 and 
October 2001.    
 
4 Annotation Categories for this Project 
Because in many cases there is no definite line be-
tween business emails and personal emails, it was 
decided to mark emails with finer categories than 
408
Business and Personal. This subcategorising not only 
helped us to analyse the different types of email 
within business and personal emails, but it helped us 
to find the nature of the disagreements that  occurred 
later on, in inter-annotation.  In other words, this 
process allowed us to observe patterns in disagree-
ment.  
 
Obviously, the process of deciding categories in any 
annotation project is a fraught and contentious one. 
The process necessarily involves repeated cycles of 
category design, annotation, inter-annotation, analysis 
of disagreement, category refinement. While the proc-
ess described above could continue ad infinitum, the 
sensible project manager must identify were this 
process is beginning to converge on a set of well-
defined but nonetheless intuitive categories, and final-
ise them. 
 
Likewise, the annotation project described here went 
through several evolutions of categories, mediated by 
input from annotators and other researchers. The final 
categories chosen were: 
 
Business: Core Business, Routine Admin, Inter-
Employee Relations, Solicited/soliciting mailing, Im-
age. 
 
Personal: Close Personal, Forwarded, Auto generated 
emails. 
 
5 Annotation and Inter-Annotation 
Based on the categories above, approximately 12,500 
emails were single-annotated by a total of four anno-
tators. 
 
The results showed that around 83% of the emails 
were business related, while 17% were personal. The 
company received one personal email for every five 
business emails. 
 
Fig 1: Distribution of Emails in the Corpus
BUSINESS
83%
PERSONAL
17%
BUSINESS
PERSONAL
 
 
A third of the received emails were ?Core Business? 
and a third were ?Routine Admin?. All other catego-
ries comprised the remaining third of the emails. One 
could conclude that approximately one third of emails 
received at Enron were discussions of policy, strategy, 
legislation, regulations, trading, and other high-level 
business matters. The next third of received emails 
were about the peripheral, routine matters of the com-
pany. These are emails related to HR, IT administra-
tion, meeting scheduling, etc. which can be regarded 
as part of the common infrastructure of any large 
scale corporation. 
 
The rest of the emails were distributed among per-
sonal emails, emails to colleagues, company news 
letters, and emails received due to subscription. The 
biggest portion of the last third, are emails received 
due to subscription, whether the subscription be busi-
ness or personal in nature. 
 
In any annotation project consistency should be 
measured. To this end 2,200 emails were double an-
notated between four annotators. As Figure 2 below 
shows, for 82% of the emails both annotators agreed 
that the email was business email and in 12% of the 
emails, both agreed on them being personal. Six per-
cent of the emails were disagreed upon. 
 
Fig 2: Agreements and Disagreements in Inter-Annotation
Disagreement
6%
Personal 
Agreement
12%
Business 
Agreement
82%
Business Agreement
Personal Agreement
Disagreement
 
 
By analysing the disagreed categories, some patterns 
of confusion were found.  
 
Around one fourth of the confusions were solicited 
emails where it was not clear whether the employee 
was subscribed to a particular newsletter group for his 
personal interest, private business, or Enron?s busi-
ness. While some subscriptions were clearly personal 
(e.g. subscription to latest celebrity news) and some 
were clearly business related (e.g. Daily Energy re-
ports), for some it was hard to identify the intention of 
the subscription (e.g. New York Times). 
 
Eighteen percent of the confusions were due to emails 
about travel arrangements, flight and hotel booking 
confirmations, where it was not clear whether the per-
sonal was acting in a business or personal role. 
 
409
Thirteen percent of the disagreements were upon 
whether an email is written between two Enron em-
ployees as business colleagues or friends. The emails 
such as ?shall we meet for a coffee at 2:00?? If insuf-
ficient information exists in the email, it can be hard 
to draw the line between a personal relationship and a 
relationship between colleagues. The annotators were 
advised to pick the category based on the formality of 
the language used in such emails, and reading be-
tween the lines wherever possible. 
 
About eight percent of the disagreements were on 
emails which were about services that Enron provides 
for its employees. For example, the Enron?s running 
club is seeking for runners, and sending an ad to En-
ron?s employers. Or Enron?s employee?s assistance 
Program (EAP), sending an email to all employees, 
letting them know that in case of finding themselves 
in stressful situations they can use some of the ser-
vices that Enron provides for them or their families.  
 
One theme was encountered in many types of confu-
sions: namely, whether to decide an e-mail?s category 
based upon its topic or its form. For example, should 
an email be categorised because it is scheduling a 
meeting or because of the subject of the meeting be-
ing scheduled? One might consider this a distinction 
by topic or by genre. 
 
As the result, final categories were created to reflect 
topic as the only dimension to be considered in the 
annotation. ?Solicited/Soliciting mailing?, ?Solic-
ited/Auto generated mailing? and ?Forwarded? were 
removed and ?Keeping Current?, ?Soliciting? were 
added as business categories and ?Personal Mainte-
nance? and ?Personal Circulation? were added as per-
sonal categories. The inter-annotation agreement was 
measured for one hundred and fifty emails, annotated 
by five annotators. The results confirmed that these 
changes had a positive effect on the accuracy of anno-
tation. 
 
6 Preliminary Results of Automatic 
Classification 
Some preliminary experiments were performed with 
an automatic classifier to determine the feasibility of 
separating business and personal emails by machine. 
The classifier used was a probabilistic classifier based 
upon the distribution of distinguishing words. More 
information can be found in (Guthrie and Walker, 
1994). 
 
Two categories from the annotation were chosen 
which were considered to typify the broad categories 
? these were Core Business (representing business) 
and Close Personal (representing personal). The Core 
Business class contains 4,000 messages (approx 
900,000 words), while Close Personal contains ap-
proximately 1,000 messages (220,000 words). 
 
The following table summarises the performance of 
this classifier in terms of Recall, Precision and F-
Measure and accuracy: 
 
Class Recall Precision F-
Measure 
Accuracy 
Business 0.99 0.92 0.95 0.99 
Personal 0.69 0.95 0.80 0.69 
AVERAGE 0.84 0.94 0.88 0.93 
 
Based upon the results of this experiment, one can 
conclude that automatic methods are also suitable for 
classifying emails as to whether they are business or 
personal. The results indicate that the business cate-
gory is well represented by the classifier, and given 
the disproportionate distribution of emails, the classi-
fier?s tendency towards the business category is un-
derstandable. 
 
Given that our inter-annotator agreement statistic tells 
us that humans only agree on this task 94% of the 
time, preliminary results with 93% accuracy (the sta-
tistic which correlates exactly to agreement) of the 
automatic method are encouraging. While more work 
is necessary to fully evaluate the suitability of this 
task for application to a machine, the seeds of a fully 
automated system are sown. 
 
7 Conclusion 
This paper describes the process of creating an email 
corpus annotated with business or personal labels. By 
measuring inter-annotator agreement it shows that this 
process was successful. Furthermore, by analysing the 
disagreements in the fine categories, it has allowed us 
to characterise the areas where the business/personal 
decisions are difficult.  
 
In general, the separation of business and personal 
mails is a task that humans can perform. Part of the 
project has allowed the identification of the areas 
where humans cannot make this distinction (as dem-
onstrated by inter-annotator agreement scores) and 
one would not expect machines to perform the task 
under these conditions either. In all other cases, where 
the language is not ambiguous as judged by human 
annotators, the challenge has been made to automatic 
classifiers to match this performance. 
 
Some initial results were reported where machines 
attempted exactly this task. They showed that accu-
racy almost as high as human agreement was 
achieved by the system. Further work, using much 
larger sets and incorporating all types of business and 
personal emails, is the next logical step. 
 
410
Any annotation project will encounter its problems in 
deciding appropriate categories. This paper described 
the various stages of evolving these categories to a 
stage where they are both intuitive and logical and 
also, produce respectable inter-annotator agreement 
scores. The work is still in progress in ensuring 
maximal consistency within the data set and refining 
the precise definitions of the categories to avoid pos-
sible overlaps. 
References 
Brian Klimt and Yiming Yang. 2004. Introducing the 
Enron Email Corpus, Carnegie Mellon University.  
 
Brian Klimt and Yiming Yang. 2004. The Enron Cor-
pus: A New Data Set for Email Classification Re-
search. Carnegie Mellon University. 
 
Email Annotation at Berkely   
http://bailando.sims.berkeley.edu/enron_email.html
 
Jana Diesner and Kathleen M. Karley. 2005. Explora-
tion of Communication Networks from the Enron 
Email Corpus, Carnegie Mellon University  
 
Louise Guthrie,  Elbert Walker and Joe Guthrie. 1994 
Document classification by machine: Theory and 
practice. Proc. of COLING'94 
 
Parambir S. Keila and David B. Skillcorn. 2005.  De-
tecting Unusual and Deceptive Communication in 
Email. Queen?s University, CA 
 
 
 
 
 
411
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 262?272,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Storing the Web in Memory: Space Efficient Language Models with
Constant Time Retrieval
David Guthrie
Computer Science Department
University of Sheffield
D.Guthrie@dcs.shef.ac.uk
Mark Hepple
Computer Science Department
University of Sheffield
M.Hepple@dcs.shef.ac.uk
Abstract
We present three novel methods of compactly
storing very large n-gram language models.
These methods use substantially less space
than all known approaches and allow n-gram
probabilities or counts to be retrieved in con-
stant time, at speeds comparable to modern
language modeling toolkits. Our basic ap-
proach generates an explicit minimal perfect
hash function, that maps all n-grams in a
model to distinct integers to enable storage of
associated values. Extensions of this approach
exploit distributional characteristics of n-gram
data to reduce storage costs, including variable
length coding of values and the use of tiered
structures that partition the data for more effi-
cient storage. We apply our approach to stor-
ing the full Google Web1T n-gram set and all
1-to-5 grams of the Gigaword newswire cor-
pus. For the 1.5 billion n-grams of Gigaword,
for example, we can store full count informa-
tion at a cost of 1.66 bytes per n-gram (around
30% of the cost when using the current state-
of-the-art approach), or quantized counts for
1.41 bytes per n-gram. For applications that
are tolerant of a certain class of relatively in-
nocuous errors (where unseen n-grams may
be accepted as rare n-grams), we can reduce
the latter cost to below 1 byte per n-gram.
1 Introduction
The availability of very large text collections, such
as the Gigaword corpus of newswire (Graff, 2003),
and the Google Web1T 1-5gram corpus (Brants and
Franz, 2006), have made it possible to build mod-
els incorporating counts of billions of n-grams. The
storage of these language models, however, presents
serious problems, given both their size and the need
to provide rapid access. A prevalent approach for
language model storage is the use of compact trie
structures, but these structures do not scale well and
require space proportional to both to the number
of n-grams and the vocabulary size. Recent ad-
vances (Talbot and Brants, 2008; Talbot and Os-
borne, 2007b) involve the development of Bloom fil-
ter based models, which allow a considerable reduc-
tion in the space required to store a model, at the cost
of allowing some limited extent of false positives
when the model is queried with previously unseen
n-grams. The aim is to achieve sufficiently compact
representation that even very large language models
can be stored totally within memory, avoiding the
latencies of disk access. These Bloom filter based
models exploit the idea that it is not actually neces-
sary to store the n-grams of the model, as long as,
when queried with an n-gram, the model returns the
correct count or probability for it. These techniques
allow the storage of language models that no longer
depend on the size of the vocabulary, but only on the
number of n-grams.
In this paper we give three different models for
the efficient storage of language models. The first
structure makes use of an explicit perfect hash func-
tion that is minimal in that it maps n keys to in-
tegers in the range 1 to n. We show that by us-
ing a minimal perfect hash function and exploit-
ing the distributional characteristics of the data we
produce n-gram models that use less space than all
know approaches with no reduction in speed. Our
two further models achieve even more compact stor-
age while maintaining constant time access by us-
ing variable length coding to compress the n-grams
values and by using tiered hash structures to parti-
262
tion the data into subsets requiring different amounts
of storage. This combination of techniques allows
us, for example, to represent the full count informa-
tion of the Google Web1T corpus (Brants and Franz,
2006) (where count values range up to 95 billion) at
a cost of just 2.47 bytes per n-gram (assuming 8-
bit fingerprints, to exclude false positives) and just
1.41 bytes per n-gram if we use 8-bit quantization
of counts. These costs are 36% and 57% respec-
tively of the space required by the Bloomier Filter
approach of Talbot and Brants (2008). For the Gi-
gaword dataset, we can store full count information
at a cost of only 1.66 bytes per n-gram. We re-
port empirical results showing that our approach al-
lows a look-up rate which is comparable to existing
modern language modeling toolkits, and much faster
than a competitor approach for space-efficient stor-
age. Finally, we propose the use of variable length
fingerprinting for use in contexts which can tolerate
a higher rate of ?less damaging? errors. This move
allows, for example, the cost of storing a quantized
model to be reduced to 1 byte per n-gram or less.
2 Related Work
A range of lossy methods have been proposed, to
reduce the storage requirements of LMs by discard-
ing information. Methods include the use of entropy
pruning techniques (Stolcke, 1998) or clustering (Je-
linek et al, 1990; Goodman and Gao, 2000) to re-
duce the number of n-grams that must be stored.
A key method is quantization (Whittaker and Raj,
2001), which reduces the value information stored
with n-grams to a limited set of discrete alternatives.
It works by grouping together the values (probabil-
ities or counts) associated with n-grams into clus-
ters, and replacing the value to be stored for each
n-gram with a code identifying its value?s cluster.
For a scheme with n clusters, codes require log2n
bits. A common case is 8-bit quantization, allow-
ing up to 256 distinct ?quantum? values. Differ-
ent methods of dividing the range of values into
clusters have been used, e.g. Whittaker and Raj
(2001) used the Lloyd-Max algorithm, whilst Fed-
erico and Bertoldi (2006) use the simpler Binning
method to quantize probabilities, and show that the
LMs so produced out-perform those produced us-
ing the Lloyd-Max method on a phrase-based ma-
chine translation task. Binning partitions the range
of values into regions that are uniformly populated,
i.e. producing clusters that contain the same num-
ber of unique values. Functionality to perform uni-
form quantization of this kind is provided as part of
various LM toolkits, such as IRSTLM. Some of the
empirical storage results reported later in the paper
relate to LMs recording n-gram count values which
have been quantized using this uniform binning ap-
proach. In the rest of this section, we turn to look
at some of the approaches used for storing language
models, irrespective of whether lossy methods are
first applied to reduce the size of the model.
2.1 Language model storage using Trie
structures
A widely used approach for storing language mod-
els employs the trie data structure (Fredkin, 1960),
which compactly represents sequences in the form
of a prefix tree, where each step down from the
root of the tree adds a new element to the sequence
represented by the nodes seen so far. Where two
sequences share a prefix, that common prefix is
jointly represented by a single node within the trie.
For language modeling purposes, the steps through
the trie correspond to words of the vocabulary, al-
though these are in practice usually represented by
24 or 32 bit integers (that have been uniquely as-
signed to each word). Nodes in the trie correspond-
ing to complete n-grams can store other informa-
tion, e.g. a probability or count value. Most mod-
ern language modeling toolkits employ some ver-
sion of a trie structure for storage, including SRILM
(Stolcke, 2002), CMU toolkit (Clarkson and Rosen-
feld, 1997), MITLM (Hsu and Glass, 2008), and
IRSTLM (Federico and Cettolo, 2007) and imple-
mentations exist which are very compact (Germann
et al, 2009). An advantage of this structure is that it
allows the stored n-grams to be enumerated. How-
ever, although this approach achieves a compact of
representation of sequences, its memory costs are
still such that very large language models require
very large storage space, far more than the Bloom
filter based methods described shortly, and far more
than might be held in memory as a basis for more
rapid access. The memory costs of such models
have been addressed using compression methods,
see Harb et al (2009), but such extensions of the
263
approach present further obstacles to rapid access.
2.2 Bloom Filter Based Language Models
Recent randomized language models (Talbot and
Osborne, 2007b; Talbot and Osborne, 2007a; Tal-
bot and Brants, 2008; Talbot and Talbot, 2008; Tal-
bot, 2009) make use of Bloom filter like structures
to map n-grams to their associated probabilities or
counts. These methods store language models in
relatively little space by not actually keeping the n-
gram key in the structure and by allowing a small
probability of returning a false positive, i.e. so that
for an n-gram that is not in the model, there is a
small risk that the model will return some random
probability instead of correctly reporting that the n-
gram was not found. These structures do not allow
enumeration over the n-grams in the model, but for
many applications this is not a requirement and their
space advantages make them extremely attractive.
Two major approaches have been used for storing
language models: Bloom Filters and Bloomier Fil-
ters. We give an overview of both in what follows.
2.2.1 Bloom Filters
A Bloom filter (Bloom, 1970) is a compact data
structure for membership queries, i.e. queries of the
form ?Is this key in the Set??. This is a weaker struc-
ture than a dictionary or hash table which also asso-
ciates a value with a key. Bloom filters use well be-
low the information theoretic lower bound of space
required to actually store the keys and can answer
queries in O(1) time. Bloom filters achieve this feat
by allowing a small probability of returning a false
positive. A Bloom filter stores a set S of n elements
in a bit array B of size m. Initially B is set to con-
tain all zeros. To store an item x from S in B we
compute k random independent hash functions on
x that each return a value in the range [0 . .m? 1].
These values serve as indices to the bit array B and
the bits at those positions are set to 1. We do this
for all elements in S, storing to the same bit array.
Elements may hash to an index inB that has already
been set to 1 and in this case we can think of these
elements as ?sharing? this bit. To test whether set S
contains a key w, we run our k hash functions on w
and check if all those locations in B are set to 1. If
w ? S then the bloom filter will always declare that
w belongs to S, but if x /? S then the filter can only
say with high probability that w is not in S. This er-
ror rate depends on the number of k hash functions
and the ratio of m/n. For instance with k = 3 hash
functions and a bit array of size m = 20n, we can
expect to get a false positive rate of 0.0027.
Talbot and Osborne (2007b) and Talbot and Os-
borne (2007a) adapt Bloom filters to the requirement
of storing values for n-grams by concatenating the
key (n-gram) and value to form a single item that is
inserted into the filter. Given a quantization scheme
allowing values in the range [1 . . V ], a quantized
value v is stored by inserting into the filter all pair-
ings of the n-gram with values from 1 up to v. To re-
trieve the value for a given key, we serially probe the
filter for pairings of the key with each value from 1
upwards, until the filter returns false. The last value
found paired with the key in the filter is the value re-
turned. Talbot and Osborne use a simple logarithmic
quantization of counts that produce limited quan-
tized value ranges, where most items will have val-
ues that are low in the range, so that the serial look-
up process will require quite a low number of steps
on average. For alternative quantization schemes
that involve greater value ranges (e.g. the 256 values
of a uniform 8-bit scheme) and/or distribute n-grams
more evenly across the quantized values, the average
number of look-up steps required will be higher and
hence the speed of access per n-gram accordingly
lower. In that case also, the requirement of insert-
ing n-grams more than once in the filter (i.e. with
values from 1 up to the actual value v being stored)
could substantially reduce the space efficiency of the
method, especially if low false positive rates are re-
quired, e.g. the case k = 3,m = 20n produces a
false positive rate of 0.0027, as noted above, but in a
situation where 3 key-value items were being stored
per n-gram on average, this error rate would in fact
require a storage cost of 60 bits per original n-gram.
2.2.2 Bloomier Filters
More recently, Talbot and Brants (2008) have pro-
posed an approach to storing large language mod-
els which is based on the Bloomier Filter technique
of Chazelle et al (2004). Bloomier Filters gener-
alize the Bloom Filter to allow values for keys to
be stored in the filter. To test whether a given key
is present in a populated Bloomier filter, we apply
k hash functions to the key and use the results as
264
indices for retrieving the data stored at k locations
within the filter, similarly to look-up in a Bloom fil-
ter. In this case, however, the data retrieved from the
filter consists of k bit vectors, which are combined
with a fingerprint of the key, using bitwise XOR, to
return the stored value. The risk of false positives
is managed by making incorporating a fingerprint of
the n-gram, and by making bit vectors longer than
the minimum length required to store values. These
additional error bits have a fairly predictable impact
on error rates, i.e. with e error bits, we anticipate the
probability of falsely construing an unseen n-gram
as being stored in the filter to be 2?e. The algo-
rithm required to correctly populate the Bloomier fil-
ter with stored data is complicated, and we shall not
consider its details here. Nevertheless, when using v
bits to represent values and e bits for error detection,
this approach allows a language model to be stored
at a cost of is 1.23 ? (v + e) bits per n-gram.
3 Single Minimal Perfect Hash Ranking
Approach
We first describe our basic structure we call Single
Minimal Perfect Hash Rank (S-MPHR) that is more
compact than that of Talbot and Brants (2008) while
still keeping a constant look up time. In the next
two sections we describe variations on this model to
further reduce the space required while maintaining
a constant look up time. The S-MPHR structure can
be divided into 3 parts as shown in Figure 1: Stage
1 is a minimal perfect hash function; Stage 2 is a
fingerprint and rank array; and Stage 3 is a unique
value array. We discuss each stage in turn.
FP
5
FP FP
1
FP
3
FP
2
FP FP
4
...
key
1
key
2
key
3
key
4
key
5
key
N
...
Minimal Perfect Hash Function
Array of K distinct probability values / frequency counts
p
1
p
2
p
3
p
4
p
5
p
6
... p
K
rank(key
5
) rank(key) rank(key
1
) rank(key
3
) rank(key
2
) rank(key) rank(key
4
) ...
Figure 1: The Single MPHR structure
3.1 Minimal Perfect Hash Function
The first part of the structure is a minimal perfect
hash function that maps every n-gram in the training
data to a distinct integer in the range 0 to N ? 1,
whereN is the total number of n-grams to store. We
use these integers as indices into the array of Stage
2 of our structure.
We use the Hash, displace, and compress (CHD)
(Belazzougui et al, 2009) algorithm to generate a
minimal perfect hash function that requires 2.07 bits
per key and has O(1) access. The algorithm works
as follows. Given a set S that contains N = |S|
keys (in our case n-grams) that we wish to map to
integers in the range 0 to N ? 1, so that every key
maps to a distinct integer (no collisions).
The first step is to use a hash function g(x), to
map every key to a bucket B in the range 0 to r.
(For this step we use a simple hash function like the
one used for generating fingerprints in the pervious
section.)
Bi = {x ? S|g(x) = i} 0 ? i ? r
The function g(x) is not perfect so several keys can
map to the same bucket. Here we choose r ? N ,
so that the number of buckets is less than or equal
to the number of keys (to achieve 2.07 bits per key
we use r = N5 , so that the average bucket size is 5).
The buckets are then sorted into descending order
according to the number of keys in each bucket |Bi|.
For the next step, a bit array, T , of size N is ini-
tialized to contain all zeros T [0 . . . N ? 1]. This bit
array is used during construction to keep track of
which integers in the range 0 to N ? 1 the minimal
perfect hash has already mapped keys to. Next we
must assume we have access to a family of random
and independent hash functions h1, h2, h3, . . . that
can be accessed using an integer index. In practice
it sufficient to use functions that behave similarly to
fully random independent hash functions and Belaz-
zougui et al (2009) demonstrate how such functions
can be generated easily by combining two simple
hash functions.
Next is the ?displacement? step. For each bucket,
in the sorted order from largest to smallest, they
search for a random hash function that maps all ele-
ments of the bucket to values in T that are currently
set to 0. Once this function has been found those
265
positions in T are set to 1. So, for each bucket Bi,
it is necessary to iteratively try hash functions, h`
for ` = 1, 2, 3, . . . to hash every element of Bi to a
distinct index j in T that contains a zero.
{h`(x)|x ? Bi} ? {j|T [j] = 1} = ?
where the size of {h`(x)|x ? Bi} is equal to the size
of Bi. When such a hash function is found we need
only to store the index, `, of the successful function
in an array ? and set T [j] = 1 for all positions j that
h` hashed to. Notice that the reason the largest buck-
ets are handled first is because they have the most el-
ements to displace and this is easier when the array
T contains more empty positions (zeros).
The final step in the algorithm is to compress the ?
array (which has length equal to the number of buck-
ets |B|), retaining O(1) access. This compression is
achieved using simple variable length encoding with
an index array (Fredriksson and Nikitin, 2007).
3.2 Fingerprint and Rank Array
The hash function used in Stage 1 is perfect, so it
is guaranteed to return unique integers for seen n-
grams, but our hash function will also return inte-
ger values in the range 0 to N ? 1 for n-grams that
have not been seen before (were not used to build the
hash function). To reduce the probability of these
unseen n-grams giving false positives results from
our model we store a fingerprint of each n-gram in
Stage 2 of our structure that can be compared against
the fingerprints of unseen n-grams when queried.
If these fingerprints of the queried n-gram and the
stored n-gram do not match then the model will
correctly report that the n-gram has not been seen
before. The size of this fingerprint determines the
rate of false positives. Assuming that the finger-
print is generated by a random hash function, and
that the returned integer of an unseen key from the
MPH function is also random, expected false posi-
tive rate for the model is the same as the probabil-
ity of two keys randomly hashing to the same value,
1
2m , where m is the number of bits of the finger-
print. The fingerprint can be generated using any
suitably random hashing algorithm. We use Austin
Appleby?s Murmurhash21 implementation to finger-
print each n-gram and then store the m highest or-
der bits. Stage 2 of the MPHR structure also stores
1http://murmurhash.googlepages.com/
a rank for every n-gram along with the fingerprint.
This rank is an index into the array of Stage 3 of
our structure that holds the unique values associated
with any n-gram.
3.3 Unique Value Array
We describe our storage of the values associated
with n-grams in our model assuming we are storing
frequency ?counts? of n-grams, but it applies also to
storing quantized probabilities. For every n-gram,
we store the ?rank? of the frequency count r(key),
(r(key) ? [0...R ? 1]) and use a separate array in
Stage 3 to store the frequency count value. This is
similar to quantization in that it reduces the num-
ber of bits required for storage, but unlike quanti-
zation it does not require a loss of any information.
This was motivated by the sparsity of n-gram fre-
quency counts in corpora in the sense that if we take
the lowest n-gram frequency count and the high-
est n-gram frequency count then most of the inte-
gers in that range do not occur as a frequency count
of any n-grams in the corpus. For example in the
Google Web1T data, there are 3.8 billion unique n-
grams with frequency counts ranging from 40 to 95
Billion yet these n-grams only have 770 thousand
distinct frequency counts (see Table 2). Because
we only store the frequency rank, to keep the pre-
cise frequency information we need only dlog2Ke
bits per n-gram, where K is the number of distinct
frequency counts. To keep all information in the
Google Web1T data we need only dlog2 771058e =
20 bits per n-gram. Rather than the bits needed
to store the maximum frequency count associated
with an n-gram, dlog2 maxcounte, which for Google
Web1T would be dlog2 95119665584e = 37 bits per
n-gram.
unique maximum n-gram unique
n-grams frequency count counts
1gm 1, 585, 620 71, 363, 822 16, 896
2gm 55, 809, 822 9, 319, 466 20, 237
3gm 250, 928, 598 829, 366 12, 425
4gm 493, 134, 812 231, 973 6, 838
5gm 646, 071, 143 86, 943 4, 201
Total 1, 447, 529, 995 71, 363, 822 60, 487
Table 1: n-gram frequency counts from Gigaword corpus
266
unique maximum n-gram unique
n-grams frequency count counts
1gm 13, 588, 391 95, 119, 665, 584 238, 592
2gm 314, 843, 401 8, 418, 225, 326 504, 087
3gm 977, 069, 902 6, 793, 090, 938 408, 528
4gm 1, 313, 818, 354 5, 988, 622, 797 273, 345
5gm 1, 176, 470, 663 5, 434, 417, 282 200, 079
Total 3, 795, 790, 711 95, 119, 665, 584 771, 058
Table 2: n-gram frequency counts from Google Web1T
corpus
3.4 Storage Requirements
We now consider the storage requirements of our S-
MPHR approach, and how it compares against the
Bloomier filter method of Talbot and Brants (2008).
To start with, we put aside the gains that can come
from using the ranking method, and instead con-
sider just the costs of using the CHD approach for
storing any language model. We saw that the stor-
age requirements of the Talbot and Brants (2008)
Bloomier filter method are a function of the number
of n-grams n, the bits of data d to be stored per n-
gram (with d = v + e: v bits for value storage, and
e bits for error detection), and a multiplying factor
of 1.23, giving an overall cost of 1.23d bits per n-
gram. The cost for our basic approach is also easily
computed. The explicit minimal PHF computed us-
ing the CHD algorithm brings a cost of 2.07 bits per
n-gram for the PHF itself, and so the comparable
overall cost to store a S-MPHR model is 2.07 + d
bits per n-gram. For small values of d, the Bloomier
filter approach has the smaller cost, but the ?break-
even? point occurs when d = 9. When d is greater
than 9 bits (as it usually will be), our approach wins
out, being up to 18% more efficient.
The benefits that come from using the ranking
method (Stage 3), for compactly storing count val-
ues, can only be evaluated in relation to the distribu-
tional characteristics specific corpora, for which we
show results in Section 6.
4 Compressed MPHR Approach
Our second approach, called Compressed MPHR,
further reduces the size of the model whilst main-
taining O(1) time to query the model. Most com-
pression techniques work by exploiting the redun-
dancy in data. Our fingerprints are unfortunately
random sequences of bits, so trying to compress
FP
5
FP FP
1
FP
3
FP
2
FP FP
4
...
key
1
key
2
key
3
key
4
key
5
key
N
...
Minimal Perfect Hash Function
Array of K distinct probability values / frequency counts
p
1
p
2
p
3
p
4
p
5
p
6
... p
K
rank(key
5
) rank(key) rank(key
1
) rank(key
3
) rank(key
2
) rank(key) rank(key
4
) ...
Fingerprint Array
Compressed Rank Array
Figure 2: Compressed MPHR structure
these is fruitless, but the ranks associated with n-
grams contain much redundancy and so are likely to
compress well. We therefore modify our original ar-
chitecture to put the ranks and fingerprints into sep-
arate arrays, of which the ranks array will be com-
pressed, as shown in Figure 2.
Much like the final stage of the CHD minimal
perfect hash algorithm we employ a random access
compression algorithm of Fredriksson and Nikitin
(2007) to reduce the size required by the array of
ranks. This method allows compression while re-
taining O(1) access to query the model.
The first step in the compression is to encode
the ranks array using a dense variable length cod-
ing. This coding works by assigning binary codes
with different lengths to each number in the rank ar-
ray, based on how frequent that number occurs. Let
s1, s2, s3, . . . , sK be the ranks that occur in the rank
array sorted by there frequency. Starting with most
frequent number in the rank array (clearly 1 is the
most common frequency count in the data unless it
has been pruned) s1 we assign it the bit code 0 and
then assign s2 the bit code 1, we then proceed by as-
signing bit codes of two bits, so s3 is assigned 00, s4
is assigned 01, etc. until all two bit codes are used
up. We then proceed to assign 3 bit codes and so on.
All of the values from the rank array are coded in
this form and concatenated to form a large bit vector
retaining their original ordering. The length in bits
for the ith number is thus blog2 (i+ 2)c and so the
number of bits required for the whole variable length
coded rank array is: b =
?K
i=0 f(si)blog2 (i+ 2)c.
Where f() gives the frequency that the rank occurs
267
andK is the total number of distinct ranks. The code
for the ith number is the binary representation with
length blog2 (i+ 2)c of the number obtained using
the formula:
code = i+ 2? 2blog2 (i+2)c
This variable length coded array is not useful by it-
self because we do not know where each number be-
gins and ends, so we also store an index array hold
this information. We create an additional bit array
D of the same size b as the variable length coded ar-
ray that simply contains ones in all positions that a
code begins in the rank array and zeros in all other
positions. That is the ith rank in the variable length
coded array occurs at position select1(D, i), where
select1 gives the position of the ith one in the ar-
ray. We do not actually store theD array, but instead
we build a more space efficient structure to answer
select1 queries. Due the distribution of n-gram fre-
quencies, the D array is typically dense in contain-
ing a large proportion of ones, so we build a rank9sel
dictionary structure (Vigna, 2008) to answer these
queries in constant time. We can use this structure
to identify the ith code in our variable length en-
coded rank array by querying for its starting posi-
tion, select1(D, i), and compute its length using its
ending position, select1(D, i+1)?1. The code and
its length can then be decoded to obtain the original
rank:
rank = code + 2(length in bits) ? 2
5 Tiered MPHR
In this section we describe an alternative route to ex-
tending our basic S-MPHR model to achieve better
space efficiency, by using multiple hash stores. The
method exploits distributional characteristics of the
data, i.e. that lower rank values (those assigned to
values shared by very many n-grams) are sufficient
for representing the value information of a dispro-
portionately large subset of the data. For the Google
Web 1T data, for example, we find that the first 256
ranks account for nearly 85% of distinct n-grams, so
if we could store ranks for these n-grams using only
the 8 bits they require, whilst allowing perhaps 20
bits per n-gram for the remaining 15%, we would
achieve an average of just under 10 bits per n-gram
to store all the rank values.
To achieve this gain, we might partition the n-
gram data into subsets requiring different amounts
of space for value storage, and put these subsets in
separate MPHRs, e.g. for the example just men-
tioned, with two MPHRs having 8 and 20 bit value
storage respectively. Partitioning to a larger number
h of MPHRs might further reduce this average cost.
This simple approach has several problems. Firstly,
it potentially requires a series of look up steps (i.e.
up to h) to retrieve the value for any n-gram, with
all hashes needing to be addressed to determine the
unseen status of an unseen n-gram. Secondly, mul-
tiple look ups will produce a compounding of error
rates, since we have up to h opportunities to falsely
construe an unseen n-gram as seen, or to construe
a seen n-gram as being stored in the wrong MPHR
and so return an incorrect count for it.
FP
5
FP FP
1
FP
3
FP
2
FP FP
4
...
key
1
key
2
key
3
key
4
key
5
key
N
...
Minimal Perfect Hash Function #1
rank(key
5
) Redirect 1 Redirect 2 rank(key
3
) rank(key
2
) Redirect 1 Redirect 2 ...
Minimal Perfect Hash Function  #2 Minimal Perfect Hash Function #3
rank(key) rank(key) ... rank(key) rank(key) rank(key) ... rank(key)
Figure 3: Tiered minimal perfect hash data structure
We will here explore an alternative approach that
we call Tiered MPHR, which avoids this compound-
ing of errors, and which limits the number of looks
ups to a maximum of 2, irrespective of how many
hashes are used. This approach employs a single
top-level MPHR which has the full set of n-grams
for its key-set, and stores a fingerprint for each. In
addition, space is allocated to store rank values, but
with some possible values being reserved to indicate
redirection to other secondary hashes where values
can be found. Each secondary hash has a minimal
perfect hash function that is computed only for the
n-grams whose values it stores. Secondary hashes
do not need to record fingerprints, as fingerprint test-
ing is done in the top-level hash.
For example, we might have a configuration of
268
three hashes, with the top-level MPHR having 8-bit
storage, and with secondary hashes having 10 and 20
bit storage respectively. Two values of the 8-bit store
(e.g. 0 and 1) are reserved to indicate redirection
to the specific secondary hashes, with the remaining
values (2 . . 255) representing ranks 1 to 254. The
10-bit secondary hash can store 1024 different val-
ues, which would then represent ranks 255 to 1278,
with all ranks above this being represented in the
20-bit hash. To look up the count for an n-gram,
we begin with the top-level hash, where fingerprint
testing can immediately reject unseen n-grams. For
some seen n-grams, the required rank value is pro-
vided directly by the top-level hash, but for others
a redirection value is returned, indicating precisely
the secondary hash in which the rank value will be
found by simple look up (with no additional finger-
print testing). Figure 3 gives a generalized presenta-
tion of the structure of tiered MPHRs. Let us repre-
sent a configuration for a tiered MPHR as a sequence
of bit values for their value stores, e.g. (8,10,20)
for the example above, or H = (b1, . . . .bh) more
generally (with b1 being the top-level MPHR).
The overall memory cost of a particular config-
uration depends on distributional characteristics of
the data stored. The top-level MPHR of config-
uration (b1, . . . .bh) stores all n-grams in its key-
set, so its memory cost is calculated as before as
N ? (2.07 +m + b1) (m the fingerprint size). The
top-level MPHR must reserve h? 1 values for redi-
rection, and so covers ranks [1 . . (2b1 ?h+1)]. The
second MPHR then covers the next 2b2 ranks, start-
ing at (2b1 ? h+2), and so on for further secondary
MPHRs. This range of ranks determines the pro-
portion ?i of the overall n-gram set that each sec-
ondary MPHR bi stores, and so the memory cost of
each secondary MPHR is N ??i? (2.07+ bi). The
optimal T-MPHR configuration for a given data set
is easily determined from distributional information
(of the coverage of each rank), by a simple search.
6 Results
In this section, we present some results comparing
the performance of our new storage methods to some
of the existing methods, regarding the costs of stor-
ing LMs, and regarding the data access speeds that
alternative systems allow.
Method
Gigaword Web1T
full quantized full quantized
Bloomier 6.00 3.08 7.53 3.08
S-MPHR 3.76 2.76 4.26 2.76
C-MPHR 2.19 2.09 3.40 2.09
T-MPHR 2.16 1.91 2.97 1.91
Table 3: Space usage in bytes/ngram using 12-bit finger-
prints and storing all 1 to 5 grams
Method
Gigaword Web1T
full quantized full quantized
Bloomier 5.38 2.46 6.91 2.46
S-MPHR 3.26 2.26 3.76 2.26
C-MPHR 1.69 1.59 2.90 1.59
T-MPHR 1.66 1.41 2.47 1.41
Table 4: Space usage in bytes/n-gram using 8-bit finger-
prints and storing all 1 to 5 grams
6.1 Comparison of memory costs
To test the effectiveness of our models we built mod-
els storing n-grams and full frequency counts for
both the Gigaword and Google Web1T corpus stor-
ing all 1,2,3,4 and 5 grams. These corpora are very
large, e.g. the Google Web1T corpus is 24.6GB
when gzip compressed and contains over 3.7 bil-
lion n-grams, with frequency counts as large as 95
billion, requiring at least 37 bits to be stored. Us-
ing the Bloomier algorithm of Talbot and Brants
(2008) with 37 bit values and 12 bit fingerprints
would require 7.53 bytes/n-gram, so we would need
26.63GB to store a model for the entire corpus.
In comparison, our S-MPHR method requires
only 4.26 bytes per n-gram to store full frequency
count information and stores the entire Web1T cor-
pus in just 15.05GB or 57% of the space required by
the Bloomier method. This saving is mostly due to
the ranking method allowing values to be stored at a
cost of only 20 bits per n-gram. Applying the same
rank array optimization to the Bloomier method sig-
nificantly reduces its memory requirement, but S-
MPHR still uses only 86% of the space that the
Bloomier approach requires. Using T-MPHR in-
stead, again with 12-bit fingerprints, we can store
full counts for the Web 1T corpus in 10.50GB,
which is small enough to be held in memory on
many modern machines. Using 8-bit fingerprints, T-
269
Method bytes/
ngram
SRILM Full, Compact 33.6
IRSTLM, 8-bit Quantized 9.1
Bloomier 12bit fp, 8bit Quantized 3.08
S-MPHR 12bit fp, 8bit Quantized 2.76
C-MPHR 12bit fp, 8bit Quantized 2.09
T-MPHR 12bit fp, 8bit Quantized 1.91
Table 5: Comparison between approaches for storing all
1 to 5 grams of the Gigaword Corpus
MPHR can store this data in just 8.74GB.
Tables 3, 4 and 5 show results for all methods2 on
both corpora, for storing full counts, and for when
8-bit binning quantization of counts is used.
6.2 Access speed comparisons
The three models we present in this paper perform
queries in O(1) time and are thus asymptotically
optimal, but this does not guarantee they perform
well in practice, therefore in this section we mea-
sure query speed on a large set of n-grams and com-
pare it to that of modern language modeling toolk-
its. We build a model of all unigrams and bigrams
in the Gigaword corpus (see Table 1) using the C-
MPHR method, SRILM (Stolcke, 2002), IRSTLM
(Federico and Cettolo, 2007), and randLM3 (Talbot
and Osborne, 2007a) toolkits. RandLM is a mod-
ern language modeling toolkit that uses Bloom filter
based structures to store large language models and
has been integrated so that it can be used as the lan-
guage model storage for the Moses statistical ma-
chine translation system (Koehn et al, 2007). We
use randLM with the BloomMap (Talbot and Tal-
bot, 2008) storage structure option with 8 bit quan-
tized values and an error rate equivalent to using 8
bit fingerprints (as recommended in the Moses doc-
umentation). All methods are implemented in C++
and are run on a machine with 2.80GHz Intel Xeon
E5462 processor and 64 GB of RAM. In addition
we show a comparison to using a modern database,
MySQL 5.0, to store the same data. We measure
the speed of querying all models for the 55 mil-
lion distinct bigrams that occur in the Gigaword,
2All T-MPHR results are for optimal configurations: Gi-
gaword full:(2,3,16), Gigaword quant:(1,8), Web1T
full:(8,6,7,8,9,10,13,20), Web1T quant:(1,8).
3http://sourceforge.net/projects/randlm/
Test Time Speed
(hr :min:sec) queries/sec
C-MPHR 00 : 01 : 50 507362
IRSTLM 00 : 02 : 12 422802
SRILM 00 : 01 : 29 627077
randLM 00 : 27 : 28 33865
MySQL 5 29 : 25 : 01 527
Table 6: Look-up speed performance comparison for C-
MPHR and several other LM storage methods
these results are shown in Table 6. Unsurprisingly
all methods perform significantly faster than using a
database as they build models that reside completely
in memory. The C-MPHR method tested here is
slower than both S-MPHR and T-MPHR models due
to the extra operations required for access to the vari-
able length encoded array yet still performs similarly
to SRILM and IRSTLM and is 14.99 times faster
than using randLM.
7 Variable Length Fingerprints
To conclude our presentation of new methods for
space-efficient language model storage, we suggest
an additional possibility for reducing storage costs,
which involves using different sizes of fingerprint
for different n-grams. Recall that the only errors al-
lowed by our approach are false-positives, i.e. where
an unseen n-gram is falsely construed as being part
of the model and a value returned for it. The idea be-
hind using different sizes of fingerprint is that, intu-
itively, some possible errors seem worse than others,
and in particular, it seems likely to be less damaging
if we falsely construe an unseen n-gram as being a
seen n-gram that has a low count or probability than
as being one with a high count or probability.
False positives arise when our perfect hashing
method maps an unseen n-gram to position where
the stored n-gram fingerprint happens to coincide
with that computed for the unseen n-gram. The risk
of this occurring is a simple function of the size
of fingerprints. To achieve a scheme that admits a
higher risk of less damaging errors, but enforces a
lower risk of more damaging errors, we need only
store shorter fingerprints for n-grams in our model
that have low counts or probabilities, and longer
fingerprints for n-grams with higher values. This
270
idea could be implemented in different ways, e.g.
by storing fingerprints of different lengths contigu-
ously within a bit array, and constructing a ?selection
structure? of the kind described in Section 4 to allow
us to locate a given fingerprint within the bit array.
FP
5
FP FP
1
FP
3
FP
2
FP FP
4
...
key
1
key
2
key
3
key
4
key
5
key
N
...
Minimal Perfect Hash Function
rank(key
5
) Redirect 1 Redirect 2 rank(key
3
) rank(key
2
) Redirect 1 Redirect 2 ...
Minimal Perfect Hash Function
rank(key) rank(key) ... rank(key)
Minimal Perfect Hash Function
first j bits of 
fingerprint
FP FP
...
FP
last m - j 
bits of 
fingerprint
rank(key) rank(key) ... rank(key)
FP FP
...
FP
Figure 4: Variable length fingerprint T-MPHR structure
using j bit fingerprints for the n-grams which are most
rare and m bit fingerprints for all others.
We here instead consider an alternative imple-
mentation, based on the use of tiered structures. Re-
call that for T-MPHR, the top-level MPHR has all
n-grams of the model as keys, and stores a fin-
gerprint for each, plus a value that may represent
an n-gram?s count or probability, or that may redi-
rect to a second-level hash where that information
can be found. Redirection is done for items with
higher counts or probabilities, so we can achieve
lower error rates for precisely these items by stor-
ing additional fingerprint information for them in
the second-level hash (see Figure 4). For example,
we might have a top-level hash with only 4-bit fin-
gerprints, but have an additional 8-bits of fingerprint
for items also stored in a second-level hash, so there
is quite a high risk (close to 116 ) of returning a low
count for an unseen n-gram, but a much lower risk
of returning any higher count. Table 7 applies this
idea to storing full and quantized counts of the Gi-
gaword and Web 1T models, when fingerprints in the
top-level MPHR have sizes in the range 1 to 6 bits,
with the fingerprint information for items stored in
secondary hashes being ?topped up? to 12 bits. This
approach achieves storage costs of around 1 byte per
n-gram or less for the quantized models.
Bits in
lowest
finger-
print
Giga-
word
Quan-
tized
Web1T
Quan-
tized
Giga-
word
All
Web1T
All
1 0.55 0.55 1.00 1.81
2 0.68 0.68 1.10 1.92
3 0.80 0.80 1.21 2.02
4 0.92 0.92 1.31 2.13
5 1.05 1.04 1.42 2.23
6 1.17 1.17 1.52 2.34
Table 7: Bytes per fingerprint for T-MPHR model using 1
to 6 bit fingerprints for rarest n-grams and 12 bit (in total)
fingerprints for all other n-grams. (All configurations are
as in Footnote 2.)
8 Conclusion
We have presented novel methods of storing large
language models, consisting of billions of n-grams,
that allow for quantized values or frequency counts
to be accessed quickly and which require far less
space than all known approaches. We show that it
is possible to store all 1 to 5 grams in the Gigaword
corpus, with full count information at a cost of just
1.66 bytes per n-gram, or with quantized counts for
just 1.41 bytes per n-gram. We have shown that our
models allow n-gram look-up at speeds comparable
to modern language modeling toolkits (which have
much greater storage costs), and at a rate approxi-
mately 15 times faster than a competitor approach
for space-efficient storage.
References
Djamal Belazzougui, Fabiano Botelho, and Martin Diet-
zfelbinger. 2009. Hash, displace, and compress. Al-
gorithms - ESA 2009, pages 682?693.
Burton H. Bloom. 1970. Space/time trade-offs in
hash coding with allowable errors. Commun. ACM,
13(7):422?426.
Thorsten Brants and Alex Franz. 2006. Google Web
1T 5-gram Corpus, version 1. Linguistic Data Con-
sortium, Philadelphia, Catalog Number LDC2006T13,
September.
Bernard Chazelle, Joe Kilian, Ronitt Rubinfeld, and
Ayellet Tal. 2004. The bloomier filter: an efficient
data structure for static support lookup tables. In
SODA ?04, pages 30?39, Philadelphia, PA, USA.
271
Philip Clarkson and Ronald Rosenfeld. 1997. Statis-
tical language modeling using the CMU-cambridge
toolkit. In Proceedings of ESCA Eurospeech 1997,
pages 2707?2710.
Marcello Federico and Nicola Bertoldi. 2006. How
many bits are needed to store probabilities for phrase-
based translation? In StatMT ?06: Proceedings of the
Workshop on Statistical Machine Translation, pages
94?101, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Marcello Federico and Mauro Cettolo. 2007. Efficient
handling of n-gram language models for statistical ma-
chine translation. In StatMT ?07: Proceedings of the
Second Workshop on Statistical Machine Translation,
pages 88?95, Morristown, NJ, USA. Association for
Computational Linguistics.
Edward Fredkin. 1960. Trie memory. Commun. ACM,
3(9):490?499.
Kimmo Fredriksson and Fedor Nikitin. 2007. Simple
compression code supporting random access and fast
string matching. In Proc. of the 6th International
Workshop on Efficient and Experimental Algorithms
(WEA?07), pages 203?216.
Ulrich Germann, Eric Joanis, and Samuel Larkin. 2009.
Tightly packed tries: How to fit large models into
memory, and make them load fast, too. Proceedings of
the Workshop on Software Engineering, Testing, and
Quality Assurance for Natural Language (SETQA-
NLP 2009), pages 31?39.
Joshua Goodman and Jianfeng Gao. 2000. Language
model size reduction by pruning and clustering. In
Proceedings of ICSLP?00, pages 110?113.
David Graff. 2003. English Gigaword. Linguistic Data
Consortium, catalog number LDC2003T05.
Boulos Harb, Ciprian Chelba, Jeffrey Dean, and Sanjay
Ghemawat. 2009. Back-off language model compres-
sion. In Proceedings of Interspeech, pages 352?355.
Bo-June Hsu and James Glass. 2008. Iterative language
model estimation:efficient data structure & algorithms.
In Proceedings of Interspeech, pages 504?511.
F. Jelinek, B. Merialdo, S. Roukos, and M. Strauss I.
1990. Self-organized language modeling for speech
recognition. In Readings in Speech Recognition, pages
450?506. Morgan Kaufmann.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: open source
toolkit for statistical machine translation. In ACL ?07:
Proceedings of the 45th Annual Meeting of the ACL on
Interactive Poster and Demonstration Sessions, pages
177?180, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Andreas Stolcke. 1998. Entropy-based pruning of
backoff language models. In Proceedings of DARPA
Broadcast News Transcription and Understanding
Workshop, pages 270?274.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
volume 2, pages 901?904, Denver.
David Talbot and Thorsten Brants. 2008. Randomized
language models via perfect hash functions. Proceed-
ings of ACL-08 HLT, pages 505?513.
David Talbot and Miles Osborne. 2007a. Randomised
language modelling for statistical machine translation.
In Proceedings of ACL 07, pages 512?519, Prague,
Czech Republic, June.
David Talbot and Miles Osborne. 2007b. Smoothed
bloom filter language models: Tera-scale LMs on the
cheap. In Proceedings of EMNLP, pages 468?476.
David Talbot and John M. Talbot. 2008. Bloom maps.
In 4th Workshop on Analytic Algorithmics and Com-
binatorics 2008 (ANALCO?08), pages 203?212, San
Francisco, California.
David Talbot. 2009. Succinct approximate counting of
skewed data. In IJCAI?09: Proceedings of the 21st
international jont conference on Artifical intelligence,
pages 1243?1248, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
Sebastiano Vigna. 2008. Broadword implementation of
rank/select queries. In WEA?08: Proceedings of the
7th international conference on Experimental algo-
rithms, pages 154?168, Berlin, Heidelberg. Springer-
Verlag.
Edward Whittaker and Bhinksha Raj. 2001.
Quantization-based language model compres-
sion. Technical report, Mitsubishi Electric Research
Laboratories, TR-2001-41.
272
