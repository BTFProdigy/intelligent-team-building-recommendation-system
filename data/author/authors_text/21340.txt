Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 681?692,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Unsupervised Induction of Cross-lingual Semantic Relations
Mike Lewis
School of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
mike.lewis@ed.ac.uk
Mark Steedman
School of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
steedman@inf.ed.ac.uk
Abstract
Creating a language-independent meaning
representation would benefit many cross-
lingual NLP tasks. We introduce the first un-
supervised approach to this problem, learn-
ing clusters of semantically equivalent English
and French relations between referring expres-
sions, based on their named-entity arguments
in large monolingual corpora. The clusters
can be used as language-independent semantic
relations, by mapping clustered expressions
in different languages onto the same relation.
Our approach needs no parallel text for train-
ing, but outperforms a baseline that uses ma-
chine translation on a cross-lingual question
answering task. We also show how to use the
semantics to improve the accuracy of machine
translation, by using it in a simple reranker.
1 Introduction
Identifying a language-independent semantics is a
major long term goal of computational linguistics,
and is interesting both theoretically and for practical
applications. It assumes that semantically equiva-
lent sentences in any language can be mapped onto
a common meaning representation. Such a repre-
sentation would be of great utility for tasks such
as translation, relation extraction, summarization,
question answering, and information retrieval. Re-
gardless of whether it is even possible to create such
a semantics, we show that an incomplete version can
be useful for downstream tasks.
Semantic machine translation aims to map a
source language to a language-independent meaning
representation, and then generate the target language
translation from this. It is hoped this would allevi-
ate the difficulties of simpler models when translat-
ing between languages with very different word or-
dering and syntax (Vauquois, 1968). Despite many
attempts to define interlingual representations (Mi-
tamura et al, 1991; Beale et al, 1995; Banarescu et
al., 2013), state-of-the-art machine translation still
uses phrase-based models (Koehn et al, 2007). The
major obstacle to defining interlinguas has been de-
vising a meaning representation that is language-
independent, but capable of expressing the limitless
number of meanings that natural languages can ex-
press (Dorr et al, 2004).
Our approach avoids this problem by utilizing the
methods of distributional semantics. Recent work
has shown that paraphrases of expressions can be
learned by clustering those with similar arguments
(Poon and Domingos, 2009; Yao et al, 2011; Lewis
and Steedman, 2013)?for example learning that X
wrote Y and X is the author of Y are equivalent if
they appear in a corpus with similar (X, Y) argument-
pairs such as {(Shakespeare, Macbeth), (Dickens,
Oliver Twist)}. We extend this to the multilingual
case, aiming to also map the French equivalents X
a e?crit Y and Y est un roman de X on to the same
cluster as the English paraphrases. Conceptually,
we treat a foreign expression as a paraphrase of an
English expression. The cluster identifier can be
used as a predicate in a logical form, suggesting that
the fundamental predicates of an interlingua can be
learnt in an unsupervised manner via clustering.
In this paper we focus on learning binary relations
between named entities. This problem is much sim-
pler than attempting complete interlingual semantic
681
interpretation, but the approach could be general-
ized. This class of expressions has proved extremely
useful in the monolingual case, with direct applica-
tions for question answering and relation extraction
(Poon and Domingos, 2009; Mintz et al, 2009), and
we demonstrate how to use them to improve ma-
chine translation. It is important to be able to ex-
tract knowledge across languages, as many facts will
not be expressed in all languages?either due to less-
complete encyclopedias being available in some lan-
guages, or facts being most relevant to a single coun-
try.
In contrast to most previous work on machine
translation and cross-lingual clustering, our method
requires no parallel text (see Section 8 for discussion
of some exceptions). It instead exploits an alignment
between named-entities in different languages. The
limited size of parallel corpora is a significant bot-
tleneck for machine translation (Resnik and Smith,
2003), whereas our approach can be used on much
larger monolingual corpora. This means it is poten-
tially useful for language-pairs where little parallel
text is available, for domain adaptation, or for semi-
supervised approaches.
2 Basic Approach
Our work builds on clustering-based approaches to
monolingual distributional semantics, aiming to cre-
ate clusters of semantically equivalent predicates,
based on their arguments in a corpus. In each lan-
guage, we first map each sentence in a large mono-
lingual corpus onto a simple logical form, by ex-
tracting binary predicates between named entities.
Then, we cluster predicates both within and between
languages into those with similar arguments.
When parsing a new sentence, instead of using
the monolingual predicate, we use the cluster identi-
fier as a language-independent semantic relation, as
shown in Figure 1. The resulting logical form can be
used for inference in question answering.
Unlike traditional approaches to translation, this
does not require parallel text?but it does impose
some additional constraints on language resources.
Our approach requires:
? A large amount of factual text, as we rely on
the same facts being expressed in different lan-
guages. We use Wikipedia, which contains ar-
ticles in 250 languages, including 121 with at
least 10,000 articles.1 Other domains, such as
Newswire, may also be effective.
? A method for extracting binary relations from
sentences. This is straightforward from depen-
dency parses, which are available for many lan-
guages. It is also possible without a parser,
with some language-specific work (Fader et al,
2011). We describe our approach in Section 3.
? A method for linking entities in the training
data to some canonical representation. Mc-
Namee et al (2011) report good results on this
task in 21 languages. We describe our method
for this in Section 4.1.
3 Predicate Extraction
Our method relies on extracting binary predicates
between entities from sentences. Various represen-
tations have been suggested for binary predicates,
such as Reverb patterns (Fader et al, 2011), de-
pendency paths (Lin and Pantel, 2001; Yao et al,
2011), and binarized predicate-argument relations
derived from a CCG-parse (Lewis and Steedman,
2013). Our approach is formalism-independent, and
is compatible with any method of expressing binary
predicates.
We choose the CCG-based parser of Lewis and
Steedman (2013) for several reasons. It out-
puts a logical form derived automatically from
the CCG-parse, containing predicates such as:
writearg0,arg1(shakespeare,macbeth). By using the
close relationship between the CCG syntax and se-
mantics, it is able to generalize over many seman-
tically equivalent syntactic constructions (such as
passives, conjunctions and relative clauses), mean-
ing we can map both Shakespeare wrote Macbeth
and Macbeth was written by Shakespeare to the
same logical form. Using a dependency-based rep-
resentation, these would have different predicates,
which would need to be clustered later. CCG also
has a well developed theory of operator semantics
(Steedman, 2012), so is able to represent semantic
operators such as quantifiers, negation and tense?
understanding these is crucial to high performance
on question answering or translation tasks. As in
1As of June 2013.
682
Shakespeare wrote Macbeth
Shakespeare wrote Macbeth
NP (S\NP)/NP NP
>
S\NP
<
S
writearg0:PER,arg1:BOOK(william shakespeare,
macbeth)
relation43(william shakespeare, macbeth)
Shakespeare a e?crit Macbeth
Shakespeare
subj

a
mod

e?crit Macbeth
obj

e?criresub j:PER,ob j:BOOK(william shakespeare,
macbeth)
CCG Parse
Initial Semantic Analysis
Lookup predicate
in clustering
Dependency Parse
Initial Semantic Analysis
Figure 1: Example showing how our system can map sentences in different languages to the same meaning represen-
tation, assuming we have clustered the equivalent predicates writearg0:PER,arg1:BOOK and e?criresub j:PER,ob j:BOOK.
Lewis and Steedman (2013), clusters derived from
the output from the parser can be integrated into the
lexicon, allowing us to build logical forms which
capture both operator and lexical semantics.
Accurate CCG syntactic parsers are currently
only available for English, whereas dependency
treebanks and parsers exist for many languages
(Buchholz and Marsi, 2006). Consequently, for
French we use the dependency path representation,
which captures the nodes and edges connecting two
named entities in a dependency parse. The extrac-
tion of these paths is language-independent, and
does not depend on the dependency grammar used,
which means our approach could be adapted to new
languages with minimal work.
4 Entity Semantics
4.1 Entity Linking
As discussed, our approach assumes that semanti-
cally similar predicates will have similar argument
entities. This requires us to be able to identify core-
ferring entities across languages during training. In
the monolingual case, it suffices to represent entities
by the string used in the sentence. This is inadequate
in the multilingual case, as many entities may be re-
ferred to by different names in different languages?
for example the United States translates as les E?tats-
Unis in French and die Vereinigte Staaten in Ger-
man. This problem is worsened by the ambiguity of
named-entity strings?for example, in the context of
a sports article, United States may refer specifically
to a team, rather than a country.
Recent work on multilingual named-entity link-
ing (McNamee et al, 2011) shows how to link
named entities in multiple languages onto English
Wikipedia articles, which can be used as unique
identifiers for entities. This means that we could
gain the information we need from unrestricted
text. However, as we use Wikipedia itself for
our training corpora, we can bootstrap entity infor-
mation directly from its markup. Wikipedia con-
tains cross-language links, e.g. between the United
States articles in different languages, allowing us
to determine the equivalence of entities in differ-
683
ent languages. Wikipedia links also help us au-
tomatically disambiguate entities to a given arti-
cle. For unlinked named-entity mentions, we per-
form some simple heuristic co-reference?based on
word-overlap with previously mentioned entities in
the document, whether the mention name is the ti-
tle of a Wikipedia article, or whether the mention
name is a Freebase (Bollacker et al, 2008) alias of
an entity. We emphasise that this does not mean our
approach is only applicable to the Wikipedia corpus.
4.2 Entity Typing
It has become standard in clustering approaches to
distributional semantics to assign types to predicates
before clustering, and only cluster predicates with
the same type (Schoenmackers et al, 2010; Berant
et al, 2011; Yao et al, 2012). This is useful for
resolving ambiguity?for example the phrase born
in may express a place-of-birth or date-of-birth rela-
tion depending on whether its second argument has
a LOC or DAT type. Ambiguous expressions may
translate differently in other languages?for exam-
ple, the two interpretations of was born in translate
in French as est ne? a` and est ne? en respectively. The
type of a predicate is determined by the type of its
arguments, and predicates with different types are
treated as distinct.
Lewis and Steedman (2013) induce an unsuper-
vised model of entity types using Latent Dirichlet
Allocation (Blei et al, 2003), based on selectional
preferences of verbs and argument-taking nouns.
When applied cross-linguistically, we found this
technique tended to create language-specific topics.
Instead, we exploit the fact that many Wikipedia en-
tities are linked to the Freebase database, which has
a detailed manually-built type-schema. This means
for a Wikipedia entity, we can look up its set of types
in Freebase.2 We use the simplified type-set of 112
types created by Ling and Weld (2012). Where en-
tities have multiple types (for example, Shakespeare
is both an author and a person), we create a separate
relation for each type.
2Named entities not present in Freebase are ignored during
training.
5 Relation Clustering
Predicates are clustered into those which are seman-
tically equivalent, based on their argument-pairs in
a corpus. The initial semantic analysis is run over
the corpora, and for each predicate we build a vector
containing counts for each of its argument-pairs (we
divide these counts by the overall frequency of an
argument-pair in the corpus, so that rarer argument-
pairs are more significant). These vectors are used
to compute similarity between predicates.
First, we run the clustering algorithm on each lan-
guage independently, and then we attempt to find an
alignment between the clusters. Duc et al (2011)
and Ta?ckstro?m et al (2012) use similar two-step ap-
proaches. Running the clustering on both languages
simultaneously was found to produce many clusters
only containing predicates from a single language.
This appears to be because even if predicates in two
different languages are truth-conditionally equiva-
lent, the language biases the sample of entity-pairs
found in a corpus. For example, the French verb
e?crire may contain more French author/book pairs
than the English equivalent write. This difference
can make the verbs appear to represent different
predicates to the clustering algorithm. Our two-step
approach also means that advances in monolingual
clustering should directly lead to improved cross-
lingual clusters.
5.1 Monolingual Clustering
Following Lewis and Steedman (2013), we use the
Chinese Whispers algorithm (Biemann, 2006) for
monolingual clustering?summarized in Algorithm
1. The algorithm is non-parametric, meaning that
the number of relation clusters is induced from the
data, and highly scalable. We create a separate graph
for each type of predicate in each language?for
example, predicates between types AUTHOR and
BOOK in French (so only predicates with the same
type will be clustered). We create one node per pred-
icate in the graph, and edges represent the distribu-
tional similarity between the predicates.
The distributional similarity between a pair of
predicates is calculated as the cosine-similarity of
their argument pair vectors in the corpus. Many
more sophisticated approaches to determining sim-
ilarity have been proposed (Kotlerman et al, 2010;
684
Weisman et al, 2012), and future work should ex-
plore these. We prune nodes with less than 25 oc-
currences, edges of weight less than 0.05, and a short
list of stop predicates. We find many of our French
dependency paths do not have a clear semantic inter-
pretation, so add the requirement that dependency
paths contain at least one content word, contain at
most 5 edges, and that one of the dependencies con-
nected to the root is subject, object or the French
preposition de.
Data: Set of predicates P
Result: A cluster assignment rp for all p ? P
?p ? P : rp?? unique cluster identifier;
while not converged do
randomize order of P
for p ? P do
rp?? argmax
r
?p? 1r=rp? sim(p, p
?)
end
end
Algorithm 1: Chinese Whispers algorithm, used
for monolingual predicate clustering. sim(p, p?) is
the distributional similarity between p and p?, and
1r=r? is 1 iff r=r? and 0 otherwise
5.2 Cross-lingual Cluster Alignment
We use a simple greedy procedure to find an align-
ment between the monolingual clusters in different
languages. First, the entity-pair vectors for each
predicate in a relation cluster are merged. Then,
the cosine similarity between entity-pair vectors for
clusters in different languages is calculated?we
base this only on argument-pairs that occur in both
languages, to reduce the potential bias of some en-
tities being more relevant to one language. Clus-
ters are then greedily aligned, in order of their sim-
ilarity, as in Algorithm 2 (pruning similarities less
than 0.01). This means that clusters are aligned with
their most similar foreign cluster. We only attempt
to align clusters with the same argument types.
6 Cross Lingual Question Answering
Experiments
We evaluate our system on English and French, us-
ing Wikipedia for corpora. The English corpus is
POS-tagged and CCG-parsed with the C&C tools
Data: Sets of monolingual relation clusters RL1
and RL2
Result: An alignment between the monolingual
clusters A
A?? {};
while RL1 6= {}?RL2 6= {} do
(r1,r2)?? argmax
(r1,r2)?RL1?RL2
sim(r1,r2);
A?? A?{(r1,r2)};
RL1?? RL1/{r1};
RL2?? RL2/{r2};
end
Algorithm 2: Cluster alignment algorithm
English French
X invades Y X envahit Y
invasion de Y par X
X orbits Y X est un satellite de Y
X est une lune de Y
X is a skyscraper in Y X est un gratte-ciel de Y
X is a novel by Y X est un roman de Y
X joins Y X adhe`re a` Y
X is a member of Y X entre dans Y
X rejoint Y
Table 1: Some example cross-lingual clusters. Predicates
are given in a human-readable form, and predicate types
are suppressed.
(Clark and Curran, 2004). The French corpus is
tagged with MElt (Denis et al, 2009) and parsed
with MaltParser (Nivre et al, 2007), trained on the
French Treebank (Candito et al, 2010). Wikipedia
markup is filtered using Wikiprep (Gabrilovich and
Markovitch, 2007)?replacing internal links with
the name of their target article, to help entity link-
ing. Some example clusters learnt by our model are
shown in Table 1. We find that the cross-lingual
clusters typically contain more French expressions
than English, possibly due to the differing sizes of
the corpora?adjusting the parameters in Section 5
results in larger clusters, but introduces noise.
6.1 Experimental Setup
We evaluate our system on a cross-lingual question
answering task, similar to monolingual QA evalua-
tions by Poon and Domingos (2009) and Lewis and
685
Steedman (2013). A question is asked in language
L, and is answered by the system from a corpus of
language L?. Human annotators are shown the ques-
tion, answer entity, and the sentence that provided
the answer, and are then asked whether the answer
is a reasonable conclusion based on the sentence.
Whilst this task is much easier than full translation,
it is both a practical application for our approach,
and a reasonably direct extrinsic evaluation for our
cross-lingual clusters.
Following Poon and Domingos (2009) and Lewis
and Steedman (2013), the question dataset is auto-
matically generated from the corpus. This approach
has the advantage of evaluating on expressions in
proportion to their corpus frequency, so understand-
ing frequent expressions is more important than rare
ones. We then sample 1000 questions for each lan-
guage, by extracting binary relations matching cer-
tain patterns (X
nsub j
? verb
dob j
? Y, X
nsub j
? verb
pob j
? Y or
X
nsub j
? be
dob j
? noun
pob j
? Y), and removing one of the
arguments. For example, from the sentence Obama
lives in Washington we create the questions X lives
in Washington?, and Obama lives in X?.3 Answers
are judged by fluent bilingual humans, and do not
have to match the entity that originally instantiated
X. Multiple answers can be returned for the same
question.
Our system attempts this task by mapping both
the question and candidate answer sentences (which
will be in a different language to the question) on
to a logical form using the clusters, and determin-
ing whether they express the same relation. This
tests the ability of our approach to cluster expres-
sions into those which are semantically equivalent
between languages. It is possible for entities to have
multiple types (see Section 4.2), and answers are
ranked by the number of types in which the entail-
ment relation is predicted to hold.
3Questions are given in a declarative form, to make the tasks
simpler for the machine translation baseline. We found the
machine translation performed poorly on questions such as
What is Obama the president of?, as inverted word-orders and
long-range dependencies are difficult to handle with re-ordering
models and language models (though are straightforward to
handle for a CCG system (Clark et al, 2004)). We find that
machine translation performs much better on declarative equiv-
alents, such as: Obama is the president of X.
6.2 Baseline
Our baseline makes use of the Moses machine trans-
lation system (Koehn et al, 2007), and is similar
to previous approaches to cross-lingual question an-
swering such as Ahn et al (2004). We train a Moses
model on the Europarl corpus (Koehn, 2005). First,
the question is translated from language L to L?,
taking the 50-best translations. As the questions
are typically shorter than corpus sentences, this is
substantially easier for the machine-translation than
translating the corpus. These are then parsed, and
patterns are extracted (as in Section 3). We also
manually supply a translation of the named-entity
in the question (based on the Freebase entity name
translation), to avoid penalizing the translation sys-
tem for failing to translate named-entities that have
not been seen in its training data. These patterns
are then used to find answers to the questions. An-
swers are ranked by the score of the best translation
that produced the pattern. Figure 2 illustrates this
pipeline.
The choice of languages is very favourable to
the machine-translation system, English and French
have similar word-order, and there is a large amount
of parallel text available (Koehn and Monz, 2006).
Our system works with any word-order, and does not
require parallel text for training, so we would expect
better performance relative to machine-translation
on other language pairs. Future work will experi-
ment with more diverse languages. The sentences to
be translated are also very short, reducing the poten-
tial for error.
6.3 Results
Results are shown in Table 3, based on a sample
of 100 answers from the output of each of the sys-
tems. Unsurprisingly, the machine-translation has
high accuracy on this task, given the choice of lan-
guages and the short queries. Pleasingly, our clusters
achieve similar accuracy, with much greater recall,
with no usage of parallel text.
Examining the results, we see that the distribu-
tion of answers is highly skewed for all systems,
with many answers to a smaller number of ques-
tions (multiple answers can be returned to the same
question). This is due to the Zipfian nature of lan-
guage, the difficulty of the task (which is far from
686
Question Answer
X dies in Moscow Sergue?? Guerassimov meurt d?une crise cardiaque le mardi
26 novembre 1985 a` Moscou
Germany invades X . . . depuis l?invasion de la Pologne par l?Allemagne et l?URSS
X wins the FA Cup Portsmouth FC remporte la FA Challenge Cup en s?imposant en
finale face a` Wolverhampton Wanderers FC
X is a band from Finland Yearning est un groupe Finlande de doom metal atmosphe?rique
X vit en France Dewi Sukarno . . . has lived in different countries including
Switzerland, France and the United States
X bat Kurt Angle Anderson defeated Kurt Angle and Abyss to advance to the finals
X est une ville de Kirghizistan Il?chibay is a village in the Issyk Kul Province of Kyrgyzstan
Table 2: Example questions correctly answered using our clusters, with the answer entity highlighted in bold.
Obama lives in X
Obama habite a` X
Obama
subj

habite
prep

a`
pobj

X
habitesub j,a`(barack obama, X)
Machine Translation
Syntactic Parse
Semantic Analysis
Figure 2: Pipeline used by baseline system for answering
French questions. The pattern extracted from the trans-
lated sentence is used to search for answers in an English
corpus.
English? French Answers Correct
Baseline 269 86%
Clusters (best 270) 270 100%
Clusters (all) 1032 72%
French? English Answers Correct
Baseline 274 85%
Clusters (all) 401 93%
Table 3: Results on wide-coverage Question Answer-
ing task. Best-N results are shown to illustrate the ac-
curacy of our cluster-based system at the same rank as
the baseline. It is not possible to give a recall figure, as
the total number of correct answers in the corpus is un-
known. English? French results are from the full French
Wikipedia corpus, whereas French? English results are
from a 10% sample.
solved in the monolingual case), and the possibil-
ity that questions may have no answers in the for-
eign corpus. This is particuarly true for the cluster-
ing approach?although the clustering system finds
more answers with the English corpus, the baseline
system answers slightly more unique questions (57
vs 66). The 1032 answers found by the clusters
in the French corpus came from just 56 questions
(compared to 29 unique questions answered by the
baseline). This suggests that the translations found
by the clustering can be more useful than those of
Moses on this task?for example, it may find an
equivalence between a rare French term and a com-
mon related English term, where machine transla-
tion may only find a more literal translation.
Despite this, we see the clusters have learnt to
687
paraphrase a variety of relations between languages
with high accuracy, suggesting that there is much
potential for the use of unsupervised clusters in
cross-lingual semantic applications. Some examples
answers are given in Table 2. Most of the errors are
caused by a small number of questions.
7 Translation Reranking Experiments
Ultimately, we would like to be able to translate
using semantic parsing with cross-lingual clusters.
As a step towards this, we investigated whether we
could rerank the output of a machine translation sys-
tem, on the basis of whether the semantic parse of
the source sentence is consistent with that of candi-
date translations.
We sample French sentences where we can pro-
duce a semantic parse (i.e. we can extract a predicate
between named entities that maps to a cross-lingual
cluster). These sentences are translated to English
using Moses, taking the 50-best list, and semantic
parses are produced for each of these. If the seman-
tic parse for the 1-best translation does not match the
source semantic parse, we take the parse from the
50-best list that most closely matches it?otherwise
we discard the sentence from our evaluation, as our
semantics agrees with the machine-translation.
To ensure that the evaluation focuses on the clus-
ters, we try to exclude several other factors that
might affect the results. The coverage of our CCG
parsing and semantic analysis drops significantly on
noisy translated sentences, and potentially acts as a
language model by failing to produce any semantic
parse on ungrammatical output sentences. We there-
fore only consider sentences where we can produce
a semantic parse for the 1-best machine translation
output. We also try to avoid penalizing the machine-
translation system for failing to translate named en-
tities correctly, so we do not attempt to rerank sen-
tences where the entities from the source sentence
are not present in the 1-best translation.
Human annotators were shown the source sen-
tence, the 1-best translation, and the translation cho-
sen by the reranker (the translations were shown in
a random order). To focus the evaluation on the se-
mantic relations we are modelling, we ask the anno-
tators which sentence best preserves the meaning be-
tween the named entities that have different relations
Percentage of
translations preferred
1-best Moses translation 5%
Cluster-based Reranker 39%
No preference 56%
Table 5: Human preference judgements for the transla-
tion reranking experiment, based on a sample of 87 sen-
tences. Results show the percentage of sentences for
which the annotators preferred the original translation,
the reranked translation, or neither. As discussed in the
text, results where annotators had no preference were typ-
ically due to syntactic parse errors.
in the semantic parse. This avoids our system being
penalized for choosing a translation that is worse in
aspects other than the relations it is modelling. An
example is shown in Table 4. The data was anno-
tated jointly by two fluent bilingual speakers, who
reported high agreement on this task.
Results are shown in Table 5, and are highly en-
couraging, with the original Moses output being pre-
ferred to the reranked translation in only 5% of cases
where our model makes a positive prediction.
Inspecting the results, we see that many of the
cases where the annotators had no preference were
caused by syntactic parse errors. For example, if
the 1-best translation is correct, but a prepositional
phrase is incorrectly attached, it will appear to have
an incorrect semantics. A similar translation in the
50-best list may be correctly parsed, and conse-
quently selected by our reranker. However, a human
will have no preference between these translations.
Incorporating K-Best parsing into our pipeline may
help mitigate against such cases.
This preliminary experiment suggests that there is
potential for future improvements in machine trans-
lation using cross-lingual distributional semantics.
The system only attempts to rerank a very small
proportion of sentences, but we believe the cover-
age could be greatly improved by including relations
between common nouns (rather than just named-
entities)?future work should explore this.
8 Related Work
Our work builds on recent progress in monolingual
distributional semantics (Poon and Domingos, 2009;
Yao et al, 2011; Lewis and Steedman, 2013) by
688
Source Le Princess Elizabeth arrive a` Dunkerque le 3 aou?t 1999
Machine translation 1-best Le Princess Elizabeth is to manage to Dunkirk on 3 August 1999
Reranked translation The Princess Elizabeth arrives at Dunkirk on 3 August 1999
Table 4: Example sentence that is reranked by our clusters. Human evaluators were asked which translation best
preserved the meaning between Princess Elizabeth and Dunkirk.
clustering typed predicates into those which are se-
mantically equivalent. We also show how to boot-
strap semantic information about entities from the
Wikipedia markup, and believe this makes it an in-
teresting corpus for future work on monolingual dis-
tributional semantics.
Cross-language Latent Relational Analysis (Duc
et al, 2011) is perhaps the most similar previous
work to ours, which moves the work of Turney
(2005) into a multilingual setting. Duc et al (2011)
aim to compute, for example, that the ?latent rela-
tion? between (Obama, US) in an English corpus is
similar to that between (Cameron, UK) in a foreign
corpus. This is solved by finding all textual patterns
between the two entity-pairs, and computing their
overall similarity. Like us, they compute similarity
between expressions in different languages based on
named-entity arguments and clustering (unlike us,
they also rely on machine translation for comput-
ing similarity). A key difference is that their sys-
tem aims to understand the overall relation between
an entity-pair based on many observations, whereas
our approach attempts to understand each sentence
individually (as is required for tasks such as transla-
tion).
Various recent papers have explored the rela-
tionship between translation and monolingual para-
phrases ?for example Bannard and Callison-Burch
(2005) create paraphrases by pivoting through a for-
eign translation, and Callison-Burch et al (2006)
show that including monolingual paraphrases im-
proves the quality of translation by reducing spar-
sity. The success of these approaches depends on the
many-to-many relationship between equivalent ex-
pressions in different languages. Our approach aims
to model this relationship explicitly by clustering all
equivalent paraphrases in different languages.
Current state-of-the-art machine translation sys-
tems circumvent the problem of full semantic in-
terpretation, by using phrase-based models learnt
from large parallel corpora (Brown et al, 1993). Al-
though this approach has been very successful, it has
significant limitations?for example, when translat-
ing between languages with very different word-
orders (Birch et al, 2009), or with little parallel text.
Semantic machine translation aims to map the
source language to an interlingual semantic rep-
resentation, and then generate the target language
sentence from this. Jones et al (2012) show how
this can be done on a small dataset using hyper-
edge replacement grammars. A major obstacle to
this is designing a suitable meaning representation,
which involves choosing a set of primitive concepts
which are abstract enough to be capable of express-
ing meaning in any language (Dorr et al, 2004).
A recent proposal for this is the Abstract Meaning
Representation (Banarescu et al, 2013), which uses
English verbs as a set of predicates. This is a less ab-
stract form of semantic interpretation than our pro-
posal, as semantically equivalent paraphrases may
be given a different representation. Such an ap-
proach also relies on annotating large amounts of
text with the semantic representation?whereas our
unsupervised approach offers a way to build such an
interlingua using only a method for extracting pred-
icates from sentences.
Whilst almost all recent work on machine-
translation has relied on parallel text, there have
been several interesting approaches that do not.
Rapp (1999) learns to translate words based on small
seed bilingual dictionary. Klementiev et al (2012a)
exploit a variety of interesting indirect sources of
information to learn a lexicon?for example as-
suming that equivalent Wikipedia articles in differ-
ent languages will use semantically similar words.
The Polylingual Topic Model (Mimno et al, 2009)
makes use of similar intuitions. Whilst we exploit
equivalent Wikipedia articles for entity linking, we
do not require aligned articles. Incorporating such
techniques into our model would be a natural next
689
step, allowing us to learn a more complete lexicon.
To our knowledge, ours is the first approach to learn
to translate semantic relations, rather than words and
phrases.
Several other recent papers have learnt cross-
lingual word clusters, and used these to improve
cross-lingual tasks such as document-classification
(Klementiev et al, 2012b), parsing (Ta?ckstro?m et
al., 2012) and semantic role labelling (Kozhevnikov
and Titov, 2013) in resource-poor languages. Cross-
lingual word clusters are learnt by aligning mono-
lingual clusters on the basis of parallel text?in
language-pairs where parallel text is available, this
offers an interesting complement to our method of
clustering based on named entities.
9 Conclusions and Future Work
We have demonstated that our previous work on
monolingual distributional semantics can simply be
extended to learn a language-independent semantics
of relations from unlabelled text, and that this se-
mantics is powerful enough to aid applications such
as question answering and translation reranking.
There is much potential for future extensions to
address the limitations of the process described here.
As we use a flat clustering of relations, we are
only able to model synonyms and not hypernyms.
More sophisticated clustering techniques, such as
those used by Berant et al (2011), seem to offer
a way to address this. Our system clusters rela-
tions with similar named-entity arguments, but this
means it does not cluster relations whose arguments
are rarely named entities. However, using cross-
lingual clusters of common nouns, such as those
from Ta?ckstro?m et al (2012), it should be possible to
cluster relations that take semantically similar com-
mon noun arguments. Embedding cluster-identifiers
in a logical form allows us to also model logical op-
erators, such as negation and quantifiers, which may
help to improve the translation of these. It would
also be interesting to experiment with more diverse
languages types.
Acknowledgements
We thank the anonymous reviewers for their helpful
comments, and Eva Hasler for help training Moses.
This work was funded by ERC Advanced Fellow-
ship 249520 GRAMPLUS and IP EC-FP7-270273
Xperience.
References
Kisuh Ahn, Beatrix Alex, Johan Bos, Tiphaine Dalmas,
Jochen L Leidner, and Matthew B Smillie. 2004.
Cross-lingual question answering with QED. In Work-
ing Notes, CLEF Cross-Language Evaluation Forum,
pages 335?342.
Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract Meaning Representation
for sembanking. In Proceedings of the 7th Linguistic
Annotation Workshop and Interoperability with Dis-
course, Sofia, Bulgaria, August. Association for Com-
putational Linguistics.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL?05), pages 597?604,
Ann Arbor, Michigan, June. Association for Compu-
tational Linguistics.
Stephen Beale, Sergei Nirenburg, and Kavi Mahesh.
1995. Semantic analysis in the Mikrokosmos machine
translation project. In Proceedings of the 2nd Sym-
posium on Natural Language Processing, pages 297?
307.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies - Volume 1, HLT ?11, pages 610?
619. Association for Computational Linguistics.
C. Biemann. 2006. Chinese whispers: an efficient graph
clustering algorithm and its application to natural lan-
guage processing problems. In Proceedings of the
First Workshop on Graph Based Methods for Natural
Language Processing, pages 73?80. Association for
Computational Linguistics.
Alexandra Birch, Phil Blunsom, and Miles Osborne.
2009. A Quantitative Analysis of Reordering Phenom-
ena. In Proceedings of the Fourth Workshop on Sta-
tistical Machine Translation, pages 197?205, Athens,
Greece, March. Association for Computational Lin-
guistics.
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
dirichlet alocation. the Journal of machine Learning
research, 3:993?1022.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring hu-
690
man knowledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management of
data, SIGMOD ?08, pages 1247?1250, New York, NY,
USA. ACM.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Comput. Linguist., 19(2):263?311, June.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared
task on multilingual dependency parsing. In Proceed-
ings of the Tenth Conference on Computational Nat-
ural Language Learning, pages 149?164. Association
for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine transla-
tion using paraphrases. In Proceedings of the Human
Language Technology Conference of the NAACL, Main
Conference, pages 17?24, New York City, USA, June.
Association for Computational Linguistics.
Marie Candito, Beno??t Crabbe?, Pascal Denis, et al 2010.
Statistical french dependency parsing: treebank con-
version and first results. In Proceedings of the Seventh
International Conference on Language Resources and
Evaluation (LREC 2010), pages 1840?1847.
Stephen Clark and James R. Curran. 2004. Parsing the
WSJ using CCG and log-linear models. In Proceed-
ings of the 42nd Annual Meeting on Association for
Computational Linguistics, ACL ?04. Association for
Computational Linguistics.
S. Clark, M. Steedman, and J.R. Curran. 2004. Object-
extraction and question-parsing using CCG. In Pro-
ceedings of the EMNLP Conference, pages 111?118.
Pascal Denis, Beno??t Sagot, et al 2009. Coupling an
annotated corpus and a morphosyntactic lexicon for
state-of-the-art pos tagging with less human effort. In
PACLIC, pages 110?119.
Bonnie J Dorr, Eduard H Hovy, and Lori S Levin. 2004.
Machine translation: Interlingual methods.
Nguyen Tuan Duc, Danushka Bollegala, and Mitsuru
Ishizuka. 2011. Cross-language latent relational
search: Mapping knowledge across languages. Asso-
ciation for the Advancement of Artificial Intelligence,
pages 1237?1242.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information
extraction. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
EMNLP ?11, pages 1535?1545. Association for Com-
putational Linguistics.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness using wikipedia-based ex-
plicit semantic analysis. In IJCAI, volume 7, pages
1606?1611.
Bevan Jones, Jacob Andreas, Daniel Bauer, Karl Moritz
Hermann, and Kevin Knight. 2012. Semantics-
based machine translation with hyperedge replacement
grammars. Proc. COLING, 2012.
Alexandre Klementiev, Ann Irvine, Chris Callison-
Burch, and David Yarowsky. 2012a. Toward statis-
tical machine translation without parallel corpora. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Linguis-
tics, EACL ?12, pages 130?140. Association for Com-
putational Linguistics.
Alexandre Klementiev, Ivan Titov, and Binod Bhattarai.
2012b. Inducing crosslingual distributed representa-
tions of words. In Proceedings of the International
Conference on Computational Linguistics (COLING),
Bombay, India, December.
Philipp Koehn and Christof Monz. 2006. Manual and au-
tomatic evaluation of machine translation between Eu-
ropean languages. In Proceedings on the Workshop on
Statistical Machine Translation, pages 102?121, New
York City, June. Association for Computational Lin-
guistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions, ACL ?07,
pages 177?180. Association for Computational Lin-
guistics.
Philipp Koehn. 2005. Europarl: A parallel corpus for sta-
tistical machine translation. In MT summit, volume 5.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-geffet. 2010. Directional distributional
similarity for lexical inference. Nat. Lang. Eng.,
16(4):359?389, October.
Mikhail Kozhevnikov and Ivan Titov. 2013. Crosslin-
gual transfer of semantic role models. In To Appear in
Proceedings of the 51th Annual Meeting of the Asso-
ciation for Computational Linguistics, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Mike Lewis and Mark Steedman. 2013. Combined Dis-
tributional and Logical Semantics. Transactions of
the Association for Computational Linguistics, 1:179?
192.
Dekang Lin and Patrick Pantel. 2001. DIRT - Discovery
of Inference Rules from Text. In In Proceedings of the
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining, pages 323?328.
Xiao Ling and Daniel S Weld. 2012. Fine-grained entity
recognition. In Proceedings of the 26th Conference on
Artificial Intelligence (AAAI).
691
Paul McNamee, James Mayfield, Dawn Lawrie, Dou-
glas W Oard, and David Doermann. 2011. Cross-
language entity linking. Proc. IJCNLP2011.
David Mimno, Hanna M Wallach, Jason Naradowsky,
David A Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing: Volume 2-Volume 2, pages 880?
889. Association for Computational Linguistics.
M. Mintz, S. Bills, R. Snow, and D. Jurafsky. 2009. Dis-
tant supervision for relation extraction without labeled
data. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP: Volume 2-Volume 2, pages 1003?
1011. Association for Computational Linguistics.
Teruko Mitamura, Eric H Nyberg, and Jaime G Car-
bonell. 1991. An efficient interlingua translation sys-
tem for multi-lingual document production.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. Maltparser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing: Volume 1 - Volume 1, EMNLP ?09,
pages 1?10. Association for Computational Linguis-
tics.
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated english and german cor-
pora. In Proceedings of the 37th annual meeting of the
Association for Computational Linguistics on Compu-
tational Linguistics, ACL ?99, pages 519?526. Asso-
ciation for Computational Linguistics.
Philip Resnik and Noah A Smith. 2003. The web
as a parallel corpus. Computational Linguistics,
29(3):349?380.
Stefan Schoenmackers, Oren Etzioni, Daniel S. Weld,
and Jesse Davis. 2010. Learning first-order horn
clauses from web text. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ?10, pages 1088?1098.
Association for Computational Linguistics.
Mark Steedman. 2012. Taking Scope: The Natural Se-
mantics of Quantifiers. MIT Press.
Oscar Ta?ckstro?m, Ryan McDonald, and Jakob Uszkoreit.
2012. Cross-lingual word clusters for direct transfer of
linguistic structure. In Proceedings of the 2012 Con-
ference of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language
Technologies, NAACL HLT ?12, pages 477?487. As-
sociation for Computational Linguistics.
Peter D. Turney. 2005. Measuring semantic similarity
by latent relational analysis. In Proceedings of the
19th international joint conference on Artificial intel-
ligence, IJCAI?05, pages 1136?1141, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Bernard Vauquois. 1968. A survey of formal grammars
and algorithms for recognition and transformation in
machine translation. In IFIP Congress, volume 68,
pages 254?260.
Hila Weisman, Jonathan Berant, Idan Szpektor, and
Ido Dagan. 2012. Learning verb inference rules
from linguistically-motivated evidence. In Proceed-
ings of the 2012 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning, EMNLP-CoNLL
?12, pages 194?204. Association for Computational
Linguistics.
Limin Yao, Aria Haghighi, Sebastian Riedel, and Andrew
McCallum. 2011. Structured relation discovery using
generative models. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP ?11, pages 1456?1466. Association for
Computational Linguistics.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2012. Unsupervised relation discovery with sense dis-
ambiguation. In ACL (1), pages 712?720.
692
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 990?1000,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
A
?
CCG Parsing with a Supertag-factored Model
Mike Lewis
School of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
mike.lewis@ed.ac.uk
Mark Steedman
School of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
steedman@inf.ed.ac.uk
Abstract
We introduce a new CCG parsing model
which is factored on lexical category as-
signments. Parsing is then simply a de-
terministic search for the most probable
category sequence that supports a CCG
derivation. The parser is extremely simple,
with a tiny feature set, no POS tagger, and
no statistical model of the derivation or
dependencies. Formulating the model in
this way allows a highly effective heuris-
tic for A
?
parsing, which makes parsing
extremely fast. Compared to the standard
C&C CCG parser, our model is more ac-
curate out-of-domain, is four times faster,
has higher coverage, and is greatly simpli-
fied. We also show that using our parser
improves the performance of a state-of-
the-art question answering system.
1 Introduction
CCG is a strongly lexicalized grammatical formal-
ism, in which the vast majority of the decisions
made during interpretation involve choosing the
correct definitions of words. We explore the ef-
fect of modelling this explicitly in a parser, by
only using a probabilistic model of lexical cate-
gories (based on a local context window), rather
than modelling the derivation or dependencies.
Existing state-of-the-art CCG parsers use com-
plex pipelines of POS-tagging, supertagging and
parsing?each with its own feature sets and pa-
rameters (and sources of error)?together with fur-
ther parameters governing their integration (Clark
and Curran, 2007). We show that much simpler
models can achieve high performance. Our model
predicts lexical categories based on a tiny fea-
ture set of word embeddings, capitalization, and 2-
character suffixes?with no parsing model beyond
a small set of CCG combinators, and no POS-
tagger. Simpler models are easier to implement,
replicate and extend.
Another goal of our model is to parse CCG
optimally and efficiently, without using excessive
pruning. CCG?s large set of lexical categories,
and generalized notion of constituency, mean that
sentences can have a huge number of potential
parses. Fast existing CCG parsers rely on aggres-
sive pruning?for example, the C&C parser uses
a supertagger to dramatically cut the search space
considered by the parser. Even the loosest beam
setting for their supertagger discards the correct
parse for 20% of sentences. The structure of our
model allows us to introduce a simple but power-
ful heuristic for A
?
parsing, meaning it can parse
almost 50 sentences per second exactly, with no
beam-search or pruning. Adding very mild prun-
ing increases the speed to 186 sentences per sec-
ond with minimal loss of accuracy.
Our approach faces two obvious challenges.
Firstly, categories are assigned based on a local
window, which may not contain the necessary con-
text for resolving some attachment decisions. For
example, in I saw a squirrel 2 weeks ago with a
nut, the model cannot make an informed decision
on whether to assign with an adverbial or adnomi-
nal preposition category, as the crucial words saw
and squirrel fall outside the local context window.
Secondly, even if the supertagger makes all lexical
category decisions correctly, then the parser can
still make erroneous decisions. One example is
in coordination-scope ambiguities, such as clever
boys and girls, where the two interpretations use
the same assignment of categories.
We hypothesise that such decisions are rela-
tively rare, and are challenging for any parsing
model, so a weak model is unlikely to result in
substantially lower accuracy. Our implementation
of this model
1
, which we call EASYCCG, has high
1
Available from https://github.com/
mikelewis0/easyccg
990
accuracy?suggesting that most parsing decisions
can be made accurately based on a local context
window.
Of course, there are many parsing decisions that
can only be made accurately with more complex
models. However, exploring the power and lim-
itations of simpler models may help focus future
research on the more challenging cases.
2 Background
2.1 Combinatory Categorial Grammar
CCG (Steedman, 2000) is a strongly lexicalized
grammatical formalism. Words have categories
representing their syntactic role, which are either
atomic, or functions from one category to another.
Phrase-structure grammars have a relatively
small number of lexical categories types (e.g.
POS-tags), and a large set of rules used to build
a syntactic analysis of a complete sentence (e.g.
an adjective and noun can combine into a noun).
In contrast, CCG parsing has many lexical cate-
gory types (we use 425), but a small set of combi-
natory rule types (we use 10 binary and 13 unary
rule schemata). This means that, aside from the
lexicon, the grammar is small enough to be hand-
coded?which allows us, in this paper, to confine
the entire statistical model to the lexicon.
CCG?s generalized notion of constituency
means that many derivations are possible for
a given a set of lexical categories. However,
most of these derivations will be semantically
equivalent?for example, deriving the same de-
pendency structures?in which case the actual
choice of derivation is unimportant. Such ambi-
guity is often called spurious.
2.2 Existing CCG Parsing Models
The seminal C&C parser is by far the most pop-
ular choice of CCG parser (Clark and Curran,
2007). It showed that it was possible to parse to
an expressive linguistic formalism with high speed
and accuracy. The performance of the parser has
enabled large-scale logic-based distributional re-
search (Harrington, 2010; Lewis and Steedman,
2013a; Lewis and Steedman, 2013b; Reddy et al.,
2014), and it is a key component of Boxer (Bos,
2008).
The C&C parser uses CKY chart parsing, with a
log-linear model to rank parses. The vast number
of possible parses means that computing the com-
plete chart is impractical. To resolve this prob-
lem, a supertagger is first run over the sentence to
prune the set of lexical categories considered by
the parser for each word. The initial beam out-
puts an average of just 1.2 categories per word,
rather than the 425 possible categories?making
the standard CKY parsing algorithm very efficient.
If the parser fails to find any analysis of the com-
plete sentence with this set of supertags, the su-
pertagger re-analyses the sentence with a more re-
laxed beam (adaptive supertagging).
2.3 A
?
Parsing
Klein and Manning (2003a) introduce A
?
parsing
for PCFGs. The parser maintains a chart and an
agenda, which is a priority queue of items to add to
the chart. The agenda is sorted based on the items?
inside probability, and a heuristic upper-bound on
the outside probability?to give an upper bound
on the probability of the complete parse. The chart
is then expanded in best-first order, until a com-
plete parse for the sentence is found.
Klein and Manning calculate an upper bound on
the outside probability of a span based on a sum-
mary of the context. For example, the summary
for the SX heuristic is the category of the span, and
the number of words in the sentence before and af-
ter the span. The value of the heuristic is the prob-
ability of the best possible sentence meeting these
restrictions. These probabilities are pre-computed
for every non-terminal symbol and for every pos-
sible number of preceding and succeeding words,
leading to large look-up tables.
Auli and Lopez (2011b) find that A
?
CCG pars-
ing with this heuristic is very slow. However,
they achieve a modest 15% speed improvement
over CKY when A
?
is combined with adaptive su-
pertagging. One reason is that the heuristic esti-
mate is rather coarse, as it deals with the best pos-
sible outside context, rather than the actual sen-
tence. We introduce a new heuristic which gives a
tighter upper bound on the outside probability.
3 Model
3.1 Lexical Category Model
As input, our parser takes a distribution over all
CCG lexical categories for each word in the sen-
tence. These distributions are assigned using
Lewis and Steedman (2014)?s semi-supervised su-
pertagging model. The supertagger is a unigram
log-linear classifier that uses features of the ?3
word context window surrounding a word. The
991
key feature is word embeddings, initialized with
the 50-dimensional embeddings trained in Turian
et al. (2010), and fine-tuned during supervised
training. The model also uses 2-character suffixes
and capitalization features.
The use of word embeddings, which are trained
on a large unlabelled corpus, allows the supertag-
ger to generalize well to words not present in the
labelled data. It does not use a POS-tagger, which
avoids problems caused by POS-tagging errors.
Our methods could be applied to any supertag-
ging model, but we find empirically that this
model gives higher performance than the C&C su-
pertagger.
3.2 Parsing Model
Let a CCG parse y of a sentence S be a list of
lexical categories c
1
. . . c
n
and a derivation. If we
assume all derivations licensed by our grammar
are equally likely, and that lexical category assign-
ments are conditionally independent given the sen-
tence, we can compute the optimal parse y? as:
y? = argmax
y
?
n
i=1
p(c
i
|S)
As discussed in Section 2.1, many derivations
are possible given a sequence of lexical categories,
some of which may be semantically distinct. How-
ever, our model will assign all of these an equal
score, as they use the same sequence of lexical
categories. Therefore we extend our model with
a simple deterministic heuristic for ranking parses
that use the same lexical categories. Given a set of
derivations with equal probability, we output the
one maximizing the sums of the length of all arcs
in the corresponding dependency tree.
The effect of this heuristic is to prefer non-
local attachments in cases of ambiguity, which
we found worked better on development data than
favouring local attachments. In cases of spurious
ambiguity, all parses will have the same value of
this heuristic, so one is chosen arbitrarily. For
example, one of the parses in Figures 1a and 1b
would be selected over the parse in Figure 1c.
Of course, we could use any function of the
parses in place of this heuristic, for example a
head-dependency model. However, one aim of
this paper is to demonstrate that an extremely sim-
ple parsing model can achieve high performance,
so we leave more sophisticated alternatives to fu-
ture work.
a house in Paris in France
NP (NP\NP)/NP NP (NP\NP)/NP NP
> >
NP\NP NP\NP
<
NP
<
NP
(a) A standard derivation of a house in Paris in France, with a
dependency from in France to house
a house in Paris in France
NP (NP\NP)/NP NP (NP\NP)/NP NP
> >
NP\NP NP\NP
>B
NP\NP
<
NP
(b) A derivation of a house in Paris in France, which is spu-
riously equivalent to Figure 1a. A composition combinator is
used to compose the predicates in Paris and in France, creating
a constituent which creates dependencies to its argument from
both in Paris and in France.
a house in Paris in France
NP (NP\NP)/NP NP (NP\NP)/NP NP
>
NP\NP
<
NP
>
NP\NP
<
NP
(c) A derivation of a house in Paris in France, which yields
different dependencies to Figures 1a and 1b: here, there is a
dependency from in France to Paris, not house.
Figure 1: Three CCG parses of a house in Paris
in France, given the same set of supertags. The
first two are spuriously equivalent, but the third is
semantically distinct.
3.3 A
?
Search
For parsing, we use an A
?
search for the most-
probable complete CCG derivation of a sentence.
A key advantage of A
?
parsing over CKY parsing
is that it does not require us to prune the search
space first with a supertagger, allowing the parser
to consider the complete distribution of 425 cate-
gories for each word (in contrast to an average of
3.57 categories per word considered by the C&C
parser?s most relaxed beam). This is possible be-
cause A
?
only searches for the Viterbi parse of
a sentence, rather than building a complete chart
with every possible category per word (another al-
ternative, used by Hockenmaier (2003), is to use a
highly aggressive beam search in the parser).
In A
?
parsing, items on the agenda are sorted by
their cost; the product of their inside probability
and an upper bound on their outside probability.
992
For a span w
i
. . . w
j
with lexical categories
c
i
. . . c
j
in a sentence S = w
1
. . . w
n
, the inside
probability is simply:
?
j
k=i
p(c
k
|S)
The factorization of our model lets us give the
following upper-bound on the outside probability:
h(w
i
. . . w
j
) =
?
k<i
k=1
max
c
k
p(c
k
|S)?
?
k?n
k=j+1
max
c
k
p(c
k
|S)
This heuristic assumes that all words outside the
span will take their highest-probability supertag.
Because the model is factored on lexical cate-
gories, this estimate is clearly an upper bound.
As supertagging is over 90% accurate, the upper
bound will often be exact, and in Section 4.3 we
show empirically that it is extremely efficient. The
values of the heuristic can be computed once for
each sentence and cached.
To implement the preference for non-local at-
tachment described in Section 3.2, if two agenda
items have the same cost, the one with the longer
dependencies is preferred.
Intuitively, the parser first attempts to find a
parse for the sentence using the 1-best category for
each word, by building as complete a chart as pos-
sible. If it fails to find a parse for the complete
sentence, it adds one more supertag to the chart
(choosing the most probable tag not already in the
chart), and tries again. This strategy allows the
parser to consider an unbounded number of cate-
gories for each word, as it does not build a com-
plete chart with all supertags.
3.4 Grammar
Here, we describe the set of combinators and
unary rules in the EASYCCG grammar. Because
we do not have any probabilistic model of the
derivation, all rules can apply with equal probabil-
ity. This means that some care needs to be taken
in designing the grammar to ensure that all the
rules are generally applicable. We also try to limit
spurious ambiguity, and build derivations which
are compatible with the C&C parser?s scripts for
extracting dependencies (for evaluation). We de-
scribe the grammar in detail, to ensure replicabil-
ity.
Our parser uses the following binary combi-
nators from Steedman (2012): forward applica-
tion, backward application, forward composition,
backward crossed composition, generalized for-
ward composition, generalized backward crossed
composition. These combinators are posited to
be linguistically universal. The generalized rules
Initial Result Usage
N NP Bare
noun
phrases
NP S/(S\NP ) Type
NP (S\NP )/((S\NP )/NP ) raising
PP (S\NP )/((S\NP )/PP )
S
pss
\NP NP\NP
S
ng
\NP NP\NP Reduced
S
adj
\NP NP\NP relative
S
to
\NP NP\NP clauses
S
to
\NP N\N
S
dcl
/NP NP\NP
S
pss
\NP S/S VP
S
ng
\NP S/S Sentence
S
to
\NP S/S Modifiers
Table 1: Set of unary rules used by the parser.
are generalized to degree 2. Following Steedman
(2000) and Clark and Curran (2007), backward
composition is blocked where the argument of the
right-hand category is anN orNP . The unhelpful
[nb] feature is ignored.
As in the C&C parser, we add a special Con-
junction rule:
Y X
>
X \X
Where Y ? {conj, comma, semicolon}. We
block conjunctions where the right-hand category
is type-raised, punctuation, N , or NP\NP . This
rule (and the restrictions) could be removed by
changing CCGBank to analyse conjunctions with
(X\X)/X categories.
We also add syntagmatic rules for removing any
punctuation to the right, and for removing open-
brackets and open-quotes to the left
The grammar also contains 13 unary rules,
listed in Table 1. These rules were chosen based
on their frequency in the training data, and their
clear semantic interpretations.
Following Clark and Curran (2007), we also add
a (switchable) constraint that only category com-
binations that have combined in the training data
may combine in the test data. We found that this
was necessary for evaluation, as the C&C conver-
sion tool for extracting predicate-argument depen-
dencies had relatively low coverage on the CCG
derivations produced by our parser. While this
restriction is theoretically inelegant, we found it
did increase parsing speed without lowering lexi-
993
cal category accuracy.
We also use Eisner Normal Form Constraints
(Eisner, 1996), and Hockenmaier and Bisk?s
(2010) Constraint 5, which automatically rule out
certain spuriously equivalent derivations, improv-
ing parsing speed.
We add a hard constraint that the root category
of the sentence must be a declarative sentence, a
question, or a noun-phrase.
This grammar is smaller and cleaner than that
used by the C&C parser, which uses 32 unary
rules (some of which are semantically dubious,
such as S[dcl]? NP\NP ), and non-standard bi-
nary combinators such as merging two NP s into
an NP . The C&C parser also has a large num-
ber of special case rules for handling punctua-
tion. Our smaller grammar reduces the grammar
constant, eases implementation, and simplifies the
job of building downstream semantic parsers such
as those of Bos (2008) or Lewis and Steedman
(2013a) (which must implement semantic analogs
of each syntactic rule).
3.5 Extracting Dependency Structures
The parsing model defined in Section 3.2 re-
quires us to compute unlabelled dependency trees
from CCG derivations (to prefer non-local attach-
ments). It is simple to extract an unlabelled depen-
dency tree from a CCG parse, by defining one ar-
gument of each binary rule instantiation to be the
head. For forward application and (generalized)
forward composition, we define the head to be the
left argument, unless the left argument is an endo-
centric head-passing modifier category X/X . We
do the inverse for the corresponding ?backward?
combinators. For punctuation rules, the head is the
argument which is not punctuation, and the head
of a Conjunction rule is the right-hand argument.
The standard CCG parsing evaluation uses a
different concept of dependencies, correspond-
ing to the predicate-argument structure defined by
CCGBank. These dependencies capture a deeper
information?for example by assigning both boy
and girl as subjects of talk in a boy and a girl
talked. We extract these dependencies using
the generate program supplied with the C&C
parser.
3.6 Pruning
Our parsing model is able to efficiently and op-
timally search for the best parse. However,
we found that over 80% of the run-time of our
pipeline was spent during supertagging. Naively,
the log-linear model needs to output a probability
for each of the 425 categories. This is expensive
both in terms of the number of dot products re-
quired, and the cost of building the initial priority-
queue for the A
?
parsing agenda. It is also largely
unnecessary?for example, periods at the end of
sentences always have the same category, but our
supertagger calculates a distribution over all pos-
sible categories.
Note that the motivation for introducing prun-
ing here is fundamentally different from for the
C&C pipeline. The C&C supertagger prunes the
the categories so that the parser can build the com-
plete set of derivations given those categories. In
contrast, our parser can efficiently search large (or
infinite) spaces of categories, but pruning is help-
ful for making supertagging itself more efficient,
and for building the initial agenda.
We therefore implemented the following strate-
gies to improve efficiency:
? Only allowing at most 50 categories per
word. The C&C parser takes on average 1.27
tags per word (and an average of 3.57 at its
loosest beam setting), so this restriction is a
very mild one. Nevertheless, it considerably
reduces the potential size of the agenda.
? Using a variable-width beam ? which prunes
categories less likely than ? times the prob-
ability of the best category. We set ? =
0.00001, which is two orders-of-magnitude
smaller than the equivalent C&C beam.
Again, this heuristic is useful for reducing the
length of the agenda.
? Using a tag dictionary of possible categories
for each word, so that weights are only cal-
culated for a subset of the categories. Unlike
the other methods, this approach does affect
the probabilities which are calculated, as the
normalizing constant is only computed for a
subset of the categories. However, the proba-
bility mass contained in the pruned categories
is small, and it only slightly decreases pars-
ing accuracy. To build the tag dictionary, we
parsed 42 million sentences of Wikipedia us-
ing our parser, and for all words occurring at
least 500 times, we stored the set of observed
word-category combinations. When parsing
new sentences, these words are only allowed
to occur with one of these categories.
994
Supertagger Parser CCGBank Wikipedia Bioinfer
F1 COV F1 Time F1 COV F1 F1 COV F1
(cov) (all) (cov) (all) (cov) (all)
C&C C&C 85.47 99.63 85.30 54s 81.19 99.0 80.64 76.08 97.2 74.88
EASYCCG EASYCCG 83.37 99.96 83.37 13s 81.75 100 81.75 77.24 100 77.24
EASYCCG C&C 86.14 99.96 86.11 69s 82.46 100 82.46 78.00 99.8 77.88
Table 2: Parsing F1-scores for labelled dependencies across a range of domains. F1 (cov) refers to
results on sentences which the parser is able to parse, and F1 (all) gives results over all sentences. For
the EASYCCG results, scores are only over parses where the C&C dependency extraction script was
successful, which was 99.3% on CCGBank, 99.5% on Wikipedia, and 100% on Bioinfer.
4 Experiments
4.1 Experimental Setup
We trained our model on Sections 02-21 of CCG-
Bank (Hockenmaier and Steedman, 2007), using
Section 00 for development. For testing, we used
Section 23 of CCGBank, a Wikipedia corpus an-
notated by Honnibal and Curran (2009), and the
Bioinfer corpus of biomedical abstracts (Pyysalo
et al., 2007). The latter two are out-of-domain, so
are more challenging for the parsers.
We compare the performance of our model
against both the C&C parser, and the system de-
scribed in Lewis and Steedman (2014). This
model uses the same supertagger as used in EASY-
CCG, but uses the C&C parser for parsing, using
adaptive supertagging with the default values.
All timing experiments used the same 1.8Ghz
AMD machine.
4.2 Parsing Accuracy
Results are shown in Table 2. Our parser per-
forms competitively with a much more complex
parsing model, and outperforms the C&C pipeline
on both out-of-domain datasets. This result con-
firms our hypothesis that the majority of parsing
decisions can be made accurately with a simple
tagging model and a deterministic parser.
We see that the combination of the EASYCCG
supertagger and the C&C parser achieves the best
accuracy across all domains. This result shows
that, unsurprisingly, there is some value to hav-
ing a statistical model of the dependencies that the
parser is evaluated on. However, the difference is
not large, particularly out-of-domain, considering
that a sophisticated and complex statistical parser
is being compared with a deterministic one. Our
parser is also far faster than this baseline.
It is interesting that the performance gap is
Speed (sentences/second)
System Tagger Parser Total
C&C 343 52 45
EASYCCG tagger +
C&C parser
299 58 49
EASYCCG baseline 56 222 45
+Tag Dictionary 185 217 99
+Max 50 tags/word 238 345 141
+?=0.00001 299 493 186
EASYCCG ? null
heuristic
300 221 127
Table 3: Effect of our optimizations of parsing
speed.
much lower on out-of-domain datasets (2.8 points
in domain, but only 0.65-0.75 out-of-domain),
suggesting that much of the C&C parser?s depen-
dency model is domain specific, and does not gen-
eralize well to other domains.
We also briefly experimented using the C&C
supertagger (with a beam of ? = 10
?5
) with the
EASYCCG parser. Performance was much worse,
with an F-score of 79.63% on the 97.8% of sen-
tences it parsed on CCGBank Section 23. This
shows that our model is reliant on the accuracy of
the supertagger.
4.3 Parsing Speed
CCG parsers have been used in distributional
approaches to semantics (Lewis and Steedman,
2013a; Lewis and Steedman, 2013b), which bene-
fit from large corpora. However, even though the
C&C parser is relatively fast, it will still take over
40 CPU-days to parse the Gigaword corpus on our
hardware, which is slow enough to be an obstacle
to scaling distributional semantics to larger cor-
995
0 20 40 60 80 100
10
20
30
40
50
Sentence Length
A
v
e
r
a
g
e
P
a
r
s
e
T
i
m
e
(
m
s
)
Figure 2: Average parse times in milliseconds, by
sentence length.
pora such as ClueWeb. Therefore, it is important
to be able to parse sentences at a high speed.
We measured parsing times on Section 23 of
CCGBank (after developing against Section 00),
using the optimizations described in Section 4.3.
We also experimented with the null heuristic,
which always estimates the outside probability as
being 1.0. Times exclude the time taken to load
models.
Results are shown in Table 3. The best EASY-
CCG model is roughly four times faster than the
C&C parser
2
. Adding the tag dictionary caused
accuracy to drop slightly from 83.46 to 83.37, and
meant the parser failed to parse a single sentence
in the test set (?Among its provisions :?) but other
changes did not affect accuracy. The pruning in
the supertagger improves parsing speed, by limit-
ing the length of the priority queue it builds for the
agenda. Of course, we could use a backoff model
to ensure full coverage (analogously to adaptive
supertagging), but we leave that to future work.
Using our A
?
heuristic doubles the speed of pars-
ing (excluding supertagging).
To better understand the properties of our
model, we also investigate how parsing time varies
with sentence length. Unlike the cubic CKY al-
gorithm typically used by chart parsers, our A
?
search potentially takes exponential time in the
sentence length. For this experiment, we used the
Sections 02-21 of CCGBank. Sentences were di-
vided into bins of width 10, and we calculated the
average parsing time for sentences in each bin.
Results are shown in Figure 2, and demon-
2
It is worth noting that the C&C parser code is written in
highly-optimized C++, compared to our simple Java imple-
mentation. It seems likely that our parser could be made sub-
stantially faster with a similar level of engineering effort.
strate that while parsing is highly efficient for sen-
tences of up to 50 words (over 95% of CCGBank),
it scales super-linearly with long sentences. In
fact, Section 00 contains a sentence of 249 words,
which took 37 seconds to parse (3 times longer
than the other 1912 sentences put together). In
practice, this scaling is unlikely to be problematic,
as long sentences are typically filtered when pro-
cessing large corpora.
4.4 Semantic Parsing
A major motivation for CCG parsing is to exploit
its transparent interface to the semantics, allowing
syntactic parsers to do much of the work of seman-
tic parsers. Therefore, perhaps the most relevant
measure of the performance of a CCG parser is its
effect on the accuracy of downstream applications.
We experimented with a supervised version
of Reddy et al. (2014)?s model for question-
answering on Freebase (i.e. without using Reddy
et al.?s lexicon derived from unlabelled text), us-
ing the WEBQUESTIONS dataset (Berant et al.,
2013)
3
. The model learns to map CCG parses to
database queries. We compare the performance of
the QA system using both our parser and C&C,
taking the 10-best parses from each parser for
each sentence. Syntactic question parsing models
were trained from the combination of 10 copies
of Rimell and Clark (2008)?s question dataset and
one copy of the CCGBank
The accuracy of Reddy et al. (2014)?s model
varies significantly between iterations of the train-
ing data. Rather than tune the number of iterations,
we instead measure the accuracy after each iter-
ation. We experimented with the models? 1-best
answers, and the oracle accuracy of their 100 best
answers. The oracle accuracy gives a better indi-
cation of the performance of the parser, by miti-
gating errors caused by the semantic component.
Results are shown in Figure 3, and demonstrate
that using EASYCCG can lead to better down-
stream performance than the C&C parser. The im-
provement is particularly large on oracle accuracy,
increasing the upper bound on the performance of
the semantic parser by around 4 points.
5 Related Work
CCG parsing has been the subject of much re-
search. We have already described the C&C pars-
3
Using the Business, Film and People domains, with 1115
questions for training and 570 for testing.
996
2 4 6 8 10
40
42
44
46
48
50
Iteration
F
1
-
s
c
o
r
e
C&C
EASYCCG
2 4 6 8 10
54
56
58
60
62
64
Iteration
1
0
0
-
b
e
s
t
O
r
a
c
l
e
F
1
-
s
c
o
r
e
C&C
EASYCCG
Figure 3: Question Answering accuracy per iteration of Reddy et al. (2014)?s supervised model.
ing model. Kummerfeld et al. (2010) showed that
the speed of the C&C parser can be improved
with domain-specific self-training?similar im-
provements may be possible applying this tech-
nique to our model. Auli and Lopez (2011a)
have achieved the best CCG parsing accuracy, by
allowing the parser and supertagger to perform
joint inference (though there is a significant speed
penalty). Auli and Lopez (2011b) were the first to
use A
?
parsing for CCG, but their system is both
much slower and less accurate than ours (due to a
different model and a different A
?
heuristic). Kr-
ishnamurthy and Mitchell (2014) show how CCG
parsing can be improved by jointly modelling the
syntax and semantics. Fowler and Penn (2010)
apply the Petrov parser to CCG, making a small
improvement in accuracy over the C&C parser,
at the cost of a 300-fold speed decrease. Zhang
and Clark (2011) and Xu et al. (2014) explored
shift-reduce CCG parsing, but despite the use of a
linear-time algorithm, parsing speed in practice is
significantly slower than the C&C parser.
Parsers based on supertagging models have pre-
viously been applied to other strongly lexical-
ized formalisms, such as to LTAG (Bangalore and
Joshi, 1999) and to HPSG (Ninomiya et al., 2006).
A major contribution of our work over these is
showing that factoring models on lexical cate-
gories allows fast and exact A
?
parsing, without
the need for beam search. Our parsing approach
could be applied to any strongly lexicalized for-
malism.
Our work fits into a tradition of attempting to
simplify complex models without sacrificing per-
formance. Klein and Manning (2003b) showed
that unlexicalized parsers were only slightly less
accurate than their lexicalized counterparts. Col-
lobert et al. (2011) showed how a range of NLP
tagging tasks could be performed at high accu-
racy using a small feature set based on vector-
space word embeddings. However, the extension
of this work to phrase-structure parsing (Collobert,
2011) required a more complex model, and did not
match the performance of traditional parsing tech-
niques. We achieve state-of-the-art results using
the same feature set and a simpler model by ex-
ploiting CCG?s lexicalized nature, which makes it
more natural to delegate parsing decisions to a tag-
ging model.
Other parsing research has focused on build-
ing fast parsers for web-scale processing, typically
using dependency grammars (e.g. Nivre (2003)).
CCG has some advantages over dependency gram-
mars, such as supporting surface-compositional
semantics. The fastest dependency parsers use
an easy-first strategy, in which edges are added
greedily in order of their score, with O(nlog(n))
complexity (Goldberg and Elhadad, 2010; Tratz
and Hovy, 2011). This strategy is reminiscent of
our A
?
search, which expands the chart in a best-
first order. A
?
has higher asymptotic complexity,
but finds a globally optimal solution.
6 Future Work
We believe that our model opens several interest-
ing directions for future research.
One interesting angle would be to increase the
amount of information in CCGBank?s lexical en-
tries, to further reduce the search space for the
parser. For example, PP categories could be dis-
tinguished with the relevant preposition as a fea-
ture; punctuation and coordination could be given
more detailed categories to avoid needing their
own combinators, and slashes could be extended
997
with Baldridge and Kruijff (2003)?s multi-modal
extensions to limit over-generation. Honnibal and
Curran (2009) show how unary rules can be lexi-
calized in CCG. Such improvements may improve
both the speed and accuracy of our model.
Because our parser is factored on a unigram tag-
ging model, it can be trained from isolated anno-
tated words, and does not require annotated parse
trees or full sentences. Reducing the requirements
for training data eases the task for human annota-
tors. It may also make the model more amenable
to semi-supervised approaches to CCG parsing,
which have typically focused on extending the lex-
icon (Thomforde and Steedman, 2011; Deoskar et
al., 2014). Finally, it may make it easier to convert
other annotated resources, such as UCCA (Abend
and Rappoport, 2013) or AMR (Banarescu et al.,
2013), to CCG training data?as only specific
words need to be converted, rather than full sen-
tences.
Our model is weak at certain kinds of deci-
sions, e.g. coordination-scope ambiguities or non-
local attachments. Incorporating specific models
for such decisions may improve accuracy, while
still allowing fast and exact search?for example,
we intend to try including Coppola et al. (2011)?s
model for prepositional phrase attachment.
7 Conclusions
We have shown that a simple, principled, deter-
ministic parser combined with a tagging model
can parse an expressive linguistic formalism with
high speed and accuracy. Although accuracy
is not state-of-the-art on CCGBank, our model
gives excellent performance on two out-of-domain
datasets, and improves the accuracy of a question-
answering system. We have shown that this model
allows an efficient heuristic for A
?
parsing, which
makes parsing extremely fast, and may enable
logic-based distributional semantics to scale to
larger corpora. Our methods are directly applica-
ble to other lexicalized formalisms, such as LTAG,
LFG and HPSG.
Acknowledgments
We would like to thank Tejaswini Deoskar, Bharat
Ram Ambati, Michael Roth and the anonymous
reviewers for helpful feedback on an earlier ver-
sion of this paper, and Siva Reddy for running the
semantic parsing experiments.
References
Omri Abend and Ari Rappoport. 2013. Universal con-
ceptual cognitive annotation (ucca). In Proceedings
of ACL.
Michael Auli and Adam Lopez. 2011a. A compari-
son of loopy belief propagation and dual decompo-
sition for integrated CCG supertagging and parsing.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies-Volume 1, pages 470?480.
Association for Computational Linguistics.
Michael Auli and Adam Lopez. 2011b. Efficient CCG
parsing: A* versus adaptive supertagging. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 1577?1585.
Association for Computational Linguistics.
Jason Baldridge and Geert-Jan M Kruijff. 2003. Multi-
modal combinatory categorial grammar. In Pro-
ceedings of the tenth conference on European chap-
ter of the Association for Computational Linguistics-
Volume 1, pages 211?218. Association for Compu-
tational Linguistics.
Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract Meaning Representation
for sembanking. In Proceedings of the 7th Linguis-
tic Annotation Workshop and Interoperability with
Discourse, Sofia, Bulgaria, August. Association for
Computational Linguistics.
Srinivas Bangalore and Aravind K Joshi. 1999. Su-
pertagging: An approach to almost parsing. Com-
putational linguistics, 25(2):237?265.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on freebase from
question-answer pairs. In Proceedings of EMNLP.
Johan Bos. 2008. Wide-coverage semantic analy-
sis with boxer. In Johan Bos and Rodolfo Del-
monte, editors, Semantics in Text Processing. STEP
2008 Conference Proceedings, Research in Compu-
tational Semantics, pages 277?286. College Publi-
cations.
Stephen Clark and James R Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493?552.
Ronan Collobert, Jason Weston, L?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493?2537.
Ronan Collobert. 2011. Deep learning for efficient dis-
criminative parsing. In International Conference on
Artificial Intelligence and Statistics, number EPFL-
CONF-192374.
998
Gregory F Coppola, Alexandra Birch, Tejaswini De-
oskar, and Mark Steedman. 2011. Simple semi-
supervised learning for prepositional phrase attach-
ment. In Proceedings of the 12th International Con-
ference on Parsing Technologies, pages 129?139.
Association for Computational Linguistics.
Tejaswini Deoskar, Christos Christodoulopoulos,
Alexandra Birch, and Mark Steedman. 2014.
Generalizing a Strongly Lexicalized Parser using
Unlabeled Data. In Proceedings of the 14th Con-
ference of the European Chapter of the Association
for Computational Linguistics. Association for
Computational Linguistics.
Jason Eisner. 1996. Efficient normal-form parsing for
combinatory categorial grammar. In Proceedings of
the 34th annual meeting on Association for Com-
putational Linguistics, pages 79?86. Association for
Computational Linguistics.
Timothy AD Fowler and Gerald Penn. 2010. Accu-
rate context-free parsing with combinatory catego-
rial grammar. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 335?344. Association for Computa-
tional Linguistics.
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 742?750. Association for Computa-
tional Linguistics.
Brian Harrington. 2010. A semantic network ap-
proach to measuring relatedness. In Proceedings
of the 23rd International Conference on Compu-
tational Linguistics: Posters, COLING ?10, pages
356?364. Association for Computational Linguis-
tics.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: a corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355?396.
Julia Hockenmaier. 2003. Data and models for statis-
tical parsing with combinatory categorial grammar.
Matthew Honnibal and James R Curran. 2009. Fully
lexicalising CCGbank with hat categories. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing: Volume 3-
Volume 3, pages 1212?1221. Association for Com-
putational Linguistics.
Dan Klein and Christopher D Manning. 2003a. A*
parsing: fast exact viterbi parse selection. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 40?47. Association for Computa-
tional Linguistics.
Dan Klein and Christopher D Manning. 2003b. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423?430. Asso-
ciation for Computational Linguistics.
Jayant Krishnamurthy and Tom M Mitchell. 2014.
Joint syntactic and semantic parsing with combina-
tory categorial grammar. June.
Jonathan K. Kummerfeld, Jessika Roesner, Tim Daw-
born, James Haggerty, James R. Curran, and
Stephen Clark. 2010. Faster parsing by supertag-
ger adaptation. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, ACL ?10, pages 345?355. Association for
Computational Linguistics.
Mike Lewis and Mark Steedman. 2013a. Combined
Distributional and Logical Semantics. Transactions
of the Association for Computational Linguistics,
1:179?192.
Mike Lewis and Mark Steedman. 2013b. Unsuper-
vised induction of cross-lingual semantic relations.
In Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing, pages
681?692, Seattle, Washington, USA, October. Asso-
ciation for Computational Linguistics.
Mike Lewis and Mark Steedman. 2014. Improved
CCG parsing with Semi-supervised Supertagging.
Transactions of the Association for Computational
Linguistics (to appear).
Takashi Ninomiya, Takuya Matsuzaki, Yoshimasa Tsu-
ruoka, Yusuke Miyao, and Jun?ichi Tsujii. 2006.
Extremely lexicalized models for accurate and fast
hpsg parsing. In Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language
Processing, EMNLP ?06, pages 155?163, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
8th International Workshop on Parsing Technologies
(IWPT.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bj?rne, Jorma Boberg, Jouni J?rvinen, and Tapio
Salakoski. 2007. Bioinfer: a corpus for information
extraction in the biomedical domain. BMC bioinfor-
matics, 8(1):50.
Siva Reddy, Mirella Lapata, and Mark Steedman.
2014. Large-scale Semantic Parsing without
Question-Answer Pairs. Transactions of the Asso-
ciation for Computational Linguistics (to appear).
Laura Rimell and Stephen Clark. 2008. Adapt-
ing a lexicalized-grammar parser to contrasting do-
mains. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 475?484. Association for Computational Lin-
guistics.
999
Mark Steedman. 2000. The Syntactic Process. MIT
Press.
Mark Steedman. 2012. Taking Scope: The Natural
Semantics of Quantifiers. MIT Press.
Emily Thomforde and Mark Steedman. 2011. Semi-
supervised CCG lexicon extension. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, pages 1246?1256. Asso-
ciation for Computational Linguistics.
Stephen Tratz and Eduard Hovy. 2011. A fast, effec-
tive, non-projective, semantically-enriched parser.
In Proceedings of EMNLP.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384?394. Association for
Computational Linguistics.
Wenduan Xu, Stephen Clark, and Yue Zhang. 2014.
Shift-reduce ccg parsing with a dependency model.
In Proceedings of the 52nd Annual Meeting of
the Association for Computational Linguistics (ACL
2014). Association for Computational Linguistics,
June.
Yue Zhang and Stephen Clark. 2011. Shift-reduce
CCG parsing. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies-Volume 1,
pages 683?692. Association for Computational Lin-
guistics.
1000
Transactions of the Association for Computational Linguistics, 1 (2013) 179?192. Action Editor: Johan Bos.
Submitted 1/2013; Revised 3/2013; Published 5/2013. c?2013 Association for Computational Linguistics.
Combined Distributional and Logical Semantics
Mike Lewis
School of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
mike.lewis@ed.ac.uk
Mark Steedman
School of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
steedman@inf.ed.ac.uk
Abstract
We introduce a new approach to semantics
which combines the benefits of distributional
and formal logical semantics. Distributional
models have been successful in modelling the
meanings of content words, but logical se-
mantics is necessary to adequately represent
many function words. We follow formal se-
mantics in mapping language to logical rep-
resentations, but differ in that the relational
constants used are induced by offline distri-
butional clustering at the level of predicate-
argument structure. Our clustering algorithm
is highly scalable, allowing us to run on cor-
pora the size of Gigaword. Different senses of
a word are disambiguated based on their in-
duced types. We outperform a variety of ex-
isting approaches on a wide-coverage question
answering task, and demonstrate the ability to
make complex multi-sentence inferences in-
volving quantifiers on the FraCaS suite.
1 Introduction
Mapping natural language to meaning representa-
tions is a central challenge of NLP. There has been
much recent progress in unsupervised distributional
semantics, in which the meaning of a word is in-
duced based on its usage in large corpora. This ap-
proach is useful for a range of key applications in-
cluding question answering and relation extraction
(Lin and Pantel, 2001; Poon and Domingos, 2009;
Yao et al, 2011). Because such a semantics can be
automically induced, it escapes the limitation of de-
pending on relations from hand-built training data,
knowledge bases or ontologies, which have proved
of limited use in capturing the huge variety of mean-
ings that can be expressed in language.
However, distributional semantics has largely de-
veloped in isolation from the formal semantics liter-
ature. Whilst distributional semantics has been ef-
fective in modelling the meanings of content words
such as nouns and verbs, it is less clear that it can be
applied to the meanings of function words. Semantic
operators, such as determiners, negation, conjunc-
tions, modals, tense, mood, aspect, and plurals are
ubiquitous in natural language, and are crucial for
high performance on many practical applications?
but current distributional models struggle to capture
even simple examples. Conversely, computational
models of formal semantics have shown low recall
on practical applications, stemming from their re-
liance on ontologies such as WordNet (Miller, 1995)
to model the meanings of content words (Bobrow et
al., 2007; Bos and Markert, 2005).
For example, consider what is needed to answer
a question like Did Google buy YouTube? from the
following sentences:
1. Google purchased YouTube
2. Google?s acquisition of YouTube
3. Google acquired every company
4. YouTube may be sold to Google
5. Google will buy YouTube or Microsoft
6. Google didn?t takeover YouTube
All of these require knowledge of lexical seman-
tics (e.g. that buy and purchase are synonyms), but
some also need interpretation of quantifiers, nega-
tives, modals and disjunction. It seems unlikely that
179
distributional or formal approaches can accomplish
the task alone.
We propose a method for mapping natural lan-
guage to first-order logic representations capable of
capturing the meanings of function words such as
every, not and or, but which also uses distributional
statistics to model the meaning of content words.
Our approach differs from standard formal seman-
tics in that the non-logical symbols used in the log-
ical form are cluster identifiers. Where standard se-
mantic formalisms would map the verb write to a
write? symbol, we map it to a cluster identifier such
as relation37, which the noun author may also map
to. This mapping is learnt by offline clustering.
Unlike previous distributional approaches, we
perform clustering at the level of predicate-argument
structure, rather than syntactic dependency struc-
ture. This means that we abstract away from many
syntactic differences that are not present in the se-
mantics, such as conjunctions, passives, relative
clauses, and long-range dependencies. This signifi-
cantly reduces sparsity, so we have fewer predicates
to cluster and more observations for each.
Of course, many practical inferences rely heavily
on background knowledge about the world?such
knowledge falls outside the scope of this work.
2 Background
Our approach is based on Combinatory Categorial
Grammar (CCG; Steedman, 2000), a strongly lexi-
calised theory of language in which lexical entries
for words contain all language-specific information.
The lexical entry for each word contains a syntactic
category, which determines which other categories
the word may combine with, and a semantic inter-
pretation, which defines the compositional seman-
tics. For example, the lexicon may contain the entry:
write ` (S\NP)/NP : ?y?x.write?(x,y)
Crucially, there is a transparent interface between
the syntactic category and the semantics. For ex-
ample the transitive verb entry above defines the
verb syntactically as a function mapping two noun-
phrases to a sentence, and semantically as a bi-
nary relation between its two argument entities.
This means that it is relatively straightforward to
deterministically map parser output to a logical
form, as in the Boxer system (Bos, 2008). This
Every dog barks
NP?/N N S\NP
? p?q.?x[p(x) =? q(x)] ?x.dog?(x) ?x.bark?(x)
>NP??q.?x[dog?(x) =? q(x)]
>S
?x[dog?(x) =? bark?(x)]
Figure 1: A standard logical form derivation using CCG.
The NP? notation means that the subject is type-raised,
and taking the verb-phrase as an argument?so is an ab-
breviation of S/(S\NP). This is necessary in part to sup-
port a correct semantics for quantifiers.
Input Sentence
Shakespeare wrote Macbeth
?
Intial semantic analysis
writearg0,arg1(shakespeare, macbeth)
?
Entity Typing
writearg0:PER,arg1:BOOK(shakespeare:PER,
macbeth:BOOK)
?
Distributional semantic analysis
relation37(shakespeare:PER, macbeth:BOOK)
Figure 2: Layers used in our model.
form of semantics captures the underlying predicate-
argument structure, but fails to license many impor-
tant inferences?as, for example, write and author
do not map to the same predicate.
In addition to the lexicon, there is a small set of
binary combinators and unary rules, which have a
syntactic and semantic interpretation. Figure 1 gives
an example CCG derivation.
3 Overview of Approach
We attempt to learn a CCG lexicon which maps
equivalent words onto the same logical form?for
example learning entries such as:
author ` N/PP[o f ] : ?x?y.relation37(x,y)
write ` (S\NP)/NP : ?x?y.relation37(x,y)
The only change to the standard CCG derivation is
that the symbols used in the logical form are arbi-
trary relation identifiers. We learn these by first map-
ping to a deterministic logical form (using predicates
180
such as author? and write?), using a process simi-
lar to Boxer, and then clustering predicates based on
their arguments. This lexicon can then be used to
parse new sentences, and integrates seamlessly with
CCG theories of formal semantics.
Typing predicates?for example, determining that
writing is a relation between people and books?
has become standard in relation clustering (Schoen-
mackers et al, 2010; Berant et al, 2011; Yao et
al., 2012). We demonstate how to build a typing
model into the CCG derivation, by subcategorizing
all terms representing entities in the logical form
with a more detailed type. These types are also in-
duced from text, as explained in Section 5, but for
convenience we describe them with human-readable
labels, such as PER, LOC and BOOK.
A key advantage of typing is that it allows us to
model ambiguous predicates. Following Berant et
al. (2011), we assume that different type signatures
of the same predicate have different meanings, but
given a type signature a predicate is unambiguous.
For example a different lexical entry for the verb
born is used in the contexts Obama was born in
Hawaii and Obama was born in 1961, reflecting a
distinction in the semantics that is not obvious in the
syntax1. Typing also greatly improves the efficiency
of clustering, as we only need to compare predicates
with the same type during clustering (for example,
we do not have to consider clustering a predicate
between people and places with predicates between
people and dates).
In this work, we focus on inducing binary rela-
tions. Many existing approaches have shown how
to produce good clusterings of (non-event) nouns
(Brown et al, 1992), any of which could be sim-
ply integrated into our semantics?but relation clus-
tering remains an open problem (see Section 9).
N-ary relations are binarized, by creating a bi-
nary relation between each pair of arguments. For
example, for the sentence Russia sold Alaska to
the United States, the system creates three binary
relations? corresponding to sellToSomeone(Russia,
Alaska), buyFromSomeone(US, Alaska), sellSome-
thingTo(Russia, US). This transformation does not
1Whilst this assumption is very useful, it does not always hold?
for example, the genitive in Shakespeare?s book is ambigu-
ous between ownership and authorship relations even given the
types of the arguments.
exactly preserve meaning, but still captures the most
important relations. Note that this allows us to
compare semantic relations across different syntac-
tic types?for example, both transitive verbs and
argument-taking nouns can be seen as expressing bi-
nary semantic relations between entities.
Figure 2 shows the layers used in our model.
4 Initial Semantic Analysis
The initial semantic analysis maps parser output
onto a logical form, in a similar way to Boxer. The
semantic formalism is based on Steedman (2012).
The first step is syntactic parsing. We use the
C&C parser (Clark and Curran, 2004), trained on
CCGBank (Hockenmaier and Steedman, 2007), us-
ing the refined version of Honnibal et al (2010)
which brings the syntax closer to the predicate-
argument structure. An automatic post-processing
step makes a number of minor changes to the parser
output, which converts the grammar into one more
suitable for our semantics. PP (prepositional phrase)
and PR (phrasal verb complement) categories are
sub-categorised with the relevant preposition. Noun
compounds with the same MUC named-entity type
(Chinchor and Robinson, 1997) are merged into a
single non-compositional node2 (we otherwise ig-
nore named-entity types). All argument NPs and
PPs are type-raised, allowing us to represent quanti-
fiers. All prepositional phrases are treated as core ar-
guments (i.e. given the category PP, not adjunct cat-
egories like (N\N)/NP or ((S\NP)\(S\NP))/NP),
as it is difficult for the parser to distinguish argu-
ments and adjuncts.
Initial semantic lexical entries for almost all
words can be generated automatically from the
syntactic category and POS tag (obtained from
the parser), as the syntactic category captures the
underlying predicate-argument structure. We use
a Davidsonian-style representation of arguments
(Davidson, 1967), which we binarize by creating a
separate predicate for each pair of arguments of a
word. These predicates are labelled with the lemma
of the head word and a Propbank-style argument key
(Kingsbury and Palmer, 2002), e.g. arg0, argIn. We
distinguish noun and verb predicates based on POS
2For example, this allows us to give Barack Obama the seman-
tics ?x.barack obama(x) instead of ?x.barack(x)? obama(x),
which is more convenient for collecting distributional statistics.
181
Word Category Semantics
Automatic author N/PP[o f ] ?x?y.authorarg0,argOf (y,x)
write (S\NP)/NP ?x?y.writearg0,arg1(y,x)
Manual every NP?/N ? p?q.?x[p(x)? q(x)]
not (S\NP)/(S\NP) ? p?x.?p(x)
Figure 3: Example initial lexical entries
tag?so, for example, we have different predicates
for effect as a noun or verb.
This algorithm can be overridden with man-
ual lexical entries for specific closed-class function
words. Whilst it may be possible to learn these
from data, our approach is pragmatic as there are
relatively few such words, and the complex logical
forms required would be difficult to induce from dis-
tributional statistics. We add a small number of lexi-
cal entries for words such as negatives (no, not etc.),
and quantifiers (numbers, each, every, all, etc.).
Some example initial lexical entries are shown in
Figure 3.
5 Entity Typing Model
Our entity-typing model assigns types to nouns,
which is useful for disambiguating polysemous
predicates. Our approach is similar to O?Seaghdha
(2010) in that we aim to cluster entities based on
the noun and unary predicates applied to them (it
is simple to convert from the binary predicates
to unary predicates). For example, we want the
pair (bornargIn, 1961) to map to a DAT type, and
(bornargIn, Hawaii) to map to a LOC type. This is
non-trivial, as both the predicates and arguments can
be ambiguous between multiple types?but topic
models offer a good solution (described below).
5.1 Topic Model
We assume that the type of each argument of a pred-
icate depends only on the predicate and argument,
although Ritter et al (2010) demonstrate an advan-
tage of modelling the joint probability of the types
of multiple arguments of the same predicate. We use
the standard Latent Dirichlet Allocation model (Blei
et al, 2003), which performs comparably to more
complex models proposed in O?Seaghdha (2010).
In topic-modelling terminology, we construct a
document for each unary predicate (e.g. bornargIn),
based on all of its argument entities (words). We as-
sume that these arguments are drawn from a small
number of types (topics), such as PER, DAT or
LOC3. Each type j has a multinomial distribution
? j over arguments (for example, a LOC type is more
likely to generate Hawaii than 1961). Each unary
predicate i has a multinomial distribution ?i over
topics, so the bornargIn predicate will normally gen-
erate a DAT or LOC type. Sparse Dirichlet priors
? and ? on the multinomials bias the distributions
to be peaky. The parameters are estimated by Gibbs
sampling, using the Mallet implementation (McCal-
lum, 2002).
The generative story to create the data is:
For every type k:
Draw the p(arg|k) distribution ?k from Dir(? )
For every unary predicate i:
Draw the p(type|i) distribution ?i from Dir(?)
For every argument j:
Draw a type zi j from Mult(?i)
Draw an argument wi j from Mult(??i)
5.2 Typing in Logical Form
In the logical form, all constants and variables repre-
senting entities x can be assigned a distribution over
types px(t) using the type model. An initial type
distribution is applied in the lexicon, using the ?
distributions for the types of nouns, and the ?i dis-
tributions for the type of arguments of binary predi-
cates (inverted using Bayes? rule). Then at each ? -
reduction in the derivation, we update probabilities
of the types to be the product of the type distribu-
tions of the terms being reduced. If two terms x and
3Types are induced from the text, but we give human-readable
labels here for convenience.
182
file a suit
(S\NP)/NP NP?
?y :
{ DOC=0.5LEGAL=0.4CLOTHES=0.01...
}
?x :
{PER = 0.7ORG = 0.2...
}
. f ilearg0,arg1(x,y) ? p.?y :
{CLOTHES = 0.6LEGAL = 0.3DOC=0.001...
}
[suit ?(y)? p(y)]
<S\NP
?x :
{ PER = 0.7ORG = 0.2...
}
?y :
{ LEGAL = 0.94CLOTHES = 0.05DOC = 0.004...
}
)[suit ?(y)? f ilearg0,arg1(x,y)]
Figure 4: Using the type model for disambiguation in the derivation of file a suit. Type distributions are shown after
the variable declarations. Both suit and the object of file are lexically ambiguous between different types, but after the
? -reduction only one interpretation is likely. If the verb were wear, a different interpretation would be preferred.
y combine to a term z:
pz(t) = px(t)py(t)? t ? px(t ?)py(t ?)
For example, in wore a suit and file a suit, the vari-
able representing suit may be lexically ambiguous
between CLOTHES and LEGAL types, but the vari-
ables representing the objects of wear and f ile will
have preferences that allow us to choose the correct
type when the terms combine. Figure 4 shows an
example derivation using the type model for disam-
biguation4.
6 Distributional Relation Clustering
Model
The typed binary predicates can be grouped
into clusters, each of which represents a dis-
tinct semantic relation. Note that because we
cluster typed predicates, bornarg0:PER,argIn:LOC and
bornarg0:PER,argIn:DAT can be clustered separately.
6.1 Corpus statistics
Typed binary predicates are clustered based on the
expected number of times they hold between each
argument-pair in the corpus. This means we cre-
ate a single vector of argument-pair counts for each
predicate (not a separate vector for each argument).
For example, the vector for the typed predicate
writearg0:PER,arg1:BOOK may contain non-zero counts
for entity-pairs such as (Shakespeare, Macbeth),
(Dickens, Oliver Twist) and (Rowling, Harry Potter).
4Our implementation follows Steedman (2012) in using Gener-
alized Skolem Terms rather than existential quantifiers, in order
to capture quantifier scope alternations monotonically, but we
omit these from the example to avoid introducing new notation.
The entity-pair counts for authorarg0:PER,argOf :BOOK
may be similar, on the assumption that both are sam-
ples from the same underlying semantic relation.
To find the expected number of occurrences of
argument-pairs for typed binary predicates in a cor-
pus, we first apply the type model to the derivation
of each sentence, as described in Section 5.2. This
outputs untyped binary predicates, with distributions
over the types of their arguments. The type of the
predicate must match the type of its arguments, so
the type distribution of a binary predicate is simply
the joint distribution of the two argument type dis-
tributions.
For example, if the arguments in a
bornarg0,argIn(obama,hawaii) derivation have the
respective type distributions (PER=0.9, LOC=0.1)
and (LOC=0.7, DAT=0.3), the distribution over bi-
nary typed predicates is (bornarg0:PER,argIn:LOC=0.63,
bornarg0:PER,argIn:DAT =0.27, etc.) The expected
counts for (obama,hawaii) in the vectors for
bornarg0:PER,argIn:LOC and bornarg0:PER,argIn:DAT are
then incremented by these probabilities.
6.2 Clustering
Many algorithms have been proposed for cluster-
ing predicates based on their arguments (Poon and
Domingos, 2009; Yao et al, 2012). The number of
relations in the corpus is unbounded, so the cluster-
ing algorithm should be non-parametric. It is also
important that it remains tractable for very large
numbers of predicates and arguments, in order to
give us a greater coverage of language than can be
achieved by hand-built ontologies.
We cluster the typed predicate vectors using the
Chinese Whispers algorithm (Biemann, 2006)?
183
although somewhat ad-hoc, it is both non-parametric
and highly scalable5. This has previously been used
for noun-clustering by Fountain and Lapata (2011),
who argue it is a cognitively plausible model for
language acquisition. The collection of predicates
and arguments is converted into a graph with one
node per predicate, and edge weights representing
the similarity between predicates. Predicates with
different types have zero-similarity, and otherwise
similarity is computed as the cosine-similarity of the
tf-idf vectors of argument-pairs. We prune nodes oc-
curring fewer than 20 times, edges with weights less
than 10?3, and a short list of stop predicates.
The algorithm proceeds as follows:
1. Each predicate p is assigned to a different se-
mantic relation rp
2. Iterate over the predicates p in a random order
3. Set rp = argmaxr ?p? 1r=rp? sim(p, p
?), where
sim(p, p?) is the distributional similarity be-
tween p and p?, and 1r=r? is 1 iff r=r? and 0
otherwise.
4. Repeat (2.) to convergence.
7 Semantic Parsing using Relation
Clusters
The final phase is to use our relation clusters in the
lexical entries of the CCG semantic derivation. This
is slightly complicated by the fact that our predi-
cates are lexically ambiguous between all the pos-
sible types they could take, and hence the relations
they could express. For example, the system can-
not tell whether born in is expressing a birthplace
or birthdate relation until later in the derivation,
when it combines with its arguments. However, all
the possible logical forms are identical except for
the symbols used, which means we can produce a
packed logical form capturing the full distribution
over logical forms. To do this, we make the predi-
cate a function from argument types to relations.
For each word, we first take the lexical semantic
definition produced by the algorithm in Section 4.
For binary predicates in this definition (which will
5We also experimented with a Dirichlet Process Mixture Model
(Neal, 2000), but even with the efficient A* search algorithms
introduced by Daume? III (2007), the cost of inference was found
to be prohibitively high when run at large scale.
be untyped), we perform a deterministic lookup in
the cluster model learned in Section 6, using all pos-
sible corresponding typed predicates. This allows us
to represent the binary predicates as packed predi-
cates: functions from argument types to relations.
For example, if the clustering maps
bornarg0:PER,argIn:LOC to rel49 (?birthplace?)
and bornarg0:PER,argIn:DAT to rel53 (?birthdate?), our
lexicon contains the following packed lexical entry
(type-distributions on the variables are suppressed):
born ` (S\NP)/PP[in] :
?y?x.
{
(x : PER,y :LOC)?rel49
(x : PER,y :DAT)?rel53
}
(x,y)
The distributions over argument types then imply
a distribution over relations. For example, if the
packed-predicate for bornarg0,argIn is applied to ar-
guments Obama and Hawaii, with respective type
distributions (PER=0.9, LOC=0.1) and (LOC=0.7,
DAT=0.3)6, the distribution over relations will be
(rel49=0.63, rel53=0.27, etc.).
If 1961 has a type-distribution (LOC=0.1,
DAT=0.9), the output packed-logical form for
Obama was born in Hawaii in 1961 will be:
?
?
?
rel49=0.63
rel53=0.27
...
?
?
?(ob,hw)?
?
?
?
rel49=0.09
rel53=0.81
...
?
?
?(ob,1961)
The probability of a given logical form can be read
from this packed logical form.
8 Experiments
Our approach aims to offer a strong model of both
formal and lexical semantics. We perform two eval-
uations, aiming to target each of these separately, but
using the same semantic representations in each.
We train our system on Gigaword (Graff et al,
2003), which contains around 4 billion words of
newswire. The type-model is trained using 15
types7, and 5,000 iterations of Gibbs sampling (us-
ing the distributions from the final sample). Table 1
6These distributions are composed from the type-distributions
for both the predicate and argument, as explained in Section 5
7This number was chosen by examination of models trained
with different numbers of types. The algorithm produces se-
mantically coherent clusters for much larger numbers of types,
but many of these are fine-grained categories of people, which
introduces sparsity in the relation clustering.
184
Type Top Words
1 suspect, assailant, fugitive, accomplice
2 author, singer, actress, actor, dad
5 city, area, country, region, town, capital
8 subsidiary, automaker, airline, Co., GM
10 musical, thriller, sequel, special
Table 1: Most probable terms in some clusters induced
by the Type Model.
shows some example types. The relation clustering
uses only proper nouns, to improve precision (spar-
sity problems are partly offset by the large input cor-
pus). Aside from parsing, the pipeline takes around
a day to run using 12 cores.
8.1 Question Answering Experiments
As yet, there is no standard way of evaluating lexical
semantics. Existing tasks like Recognising Textual
Entailment (RTE; Dagan et al, 2006) rely heavily on
background knowledge, which is beyond the scope
of this work. Intrinsic evaluations of entailment rela-
tions have low inter-annotator agreement (Szpektor
et al, 2007), due to the difficulty of evaluating rela-
tions out of context.
Our evaluation is based on that performed by
Poon and Domingos (2009). We automatically con-
struct a set of questions by sampling from text,
and then evaluate how many answers can be found
in a different corpus. From dependency-parsed
newswire, we sample either Xnsub j? verbdob j? Y, Xnsub j?
verbpob j? Y or Xnsub j? be dob j? nounpob j? Y patterns,
where X and Y are proper nouns and the verb is
not on a list of stop verbs, and deterministically con-
vert these to questions. For example, from Google
bought YouTube we create the questions What did
Google buy? and What bought YouTube?. The task
is to find proper-noun answers to these questions in
a different corpus, which are then evaluated by hu-
man annotators based on the sentence the answer
was retrieved from8. Systems can return multiple
8Common nouns are filtered automatically. To focus on evalu-
ating the semantics, annotators ignored garbled sentences due
to errors pre-processing the corpus (these are excluded from
the results). We also automatically exclude weekday and
month answers, which are overwhelmingly syntax errors for
all systems?e.g. treating Tuesday as an object in Obama an-
nounced Tuesday that...
answers to the same question (e.g. What did Google
buy? may have many valid answers), and all of
these contribute to the result. As none of the systems
model tense or temporal semantics, annotators were
instructed to annotate answers as correct if they were
true at any time. This approach means we evaluate
on relations in proportion to corpus frequency. We
sample 1000 questions from the New York Times
subset of Gigaword from 2010, and search for an-
swers in the New York Times from 2009.
We evaluate the following approaches:
? CCG-Baseline The logical form produced by
our CCG derivation, without the clustering.
? CCG-WordNet The CCG logical form, plus
WordNet as a model of lexical semantics.
? CCG-Distributional The logical form includ-
ing the type model and clusters.
? Relational LDA An LDA based model for
clustering dependency paths (Yao et al, 2011).
We train on New York Times subset of Giga-
word9, using their setup of 50 iterations with
100 relation types.
? Reverb A sophisticated Open Information Ex-
traction system (Fader et al, 2011).
Unsupervised Semantic Parsing (USP; Poon and
Domingos, 2009; USP; Poon and Domingos, 2010;
USP; Titov and Klementiev, 2011) would be another
obvious baseline. However, memory requirements
mean it is not possible to run at this scale (our system
is trained on 4 orders of magnitude more data than
the USP evaluation). Yao et al (2011) found it had
comparable performance to Relational LDA.
For the CCG models, rather than performing full
first-order inference on a large corpus, we simply
test whether the question predicate subsumes a can-
didate answer predicate, and whether the arguments
match10. In the case of CCG-Distributional, we cal-
culate the probability that the two packed-predicates
9This is around 35% of Gigaword, and was the largest scale
possible on our resources.
10We do this as it is much more efficient than full first-order
theorem-proving. We could in principle make additional in-
ferences with theorem-proving, such as answering What did
Google buy? from Google bought the largest video website and
YouTube is the largest video website.
185
System Answers Correct
Relational-LDA 7046 11.6%
Reverb 180 89.4%
CCG-Baseline 203 95.8%
CCG-WordNet 211 94.8%
CCG-Distributional@250 250 94.1%
CCG-Distributional@500 500 82.0%
Table 2: Results on wide-coverage Question Answer-
ing task. CCG-Distributional ranks question/answer pairs
by confidence?@250 means we evaluate the top 250 of
these. It is not possible to give a recall figure, as the total
number of correct answers in the corpus is unknown.
are in the same cluster, marginalizing over their ar-
gument types. Answers are ranked by this proba-
bility. For CCG-WordNet, we check if the ques-
tion predicate is a hypernym of the candidate answer
predicate (using any WordNet sense of either term).
Results are shown in Table 2. Relational-LDA in-
duces many meaningful clusters, but predicates must
be assigned to one of 100 relations, so results are
dominated by large, noisy clusters (it is not possi-
ble to take the N-best answers as the cluster assign-
ments do not have a confidence score). The CCG-
Baseline errors are mainly caused by parser errors,
or relations in the scope of non-factive operators.
CCG-WordNet adds few answers to CCG-Baseline,
reflecting the limitations of hand-built ontologies.
CCG-Distributional substantially improves recall
over other approaches whilst retaining good preci-
sion, demonstrating that we have learnt a powerful
model of lexical semantics. Table 3 shows some
correctly answered questions. The system improves
over the baseline by mapping expressions such as
merge with and acquisition of to the same relation
cluster. Many of the errors are caused by conflating
predicates where the entailment only holds in one
direction, such as was elected to with ran for. Hier-
archical clustering could be used to address this.
8.2 Experiments on the FraCaS Suite
We are also interested in evaluating our approach
as a model of formal semantics?demonstrating that
it is possible to integrate the formal semantics of
Steedman (2012) with our distributional clusters.
The FraCaS suite (Cooper et al, 1996)11 contains
a hand-built set of entailment problems designed to
be challenging in terms of formal semantics. We
use Section 1, which contains 74 problems requiring
an understanding of quantifiers12. They do not re-
quire any knowledge of lexical semantics, meaning
we can evaluate the formal component of our system
in isolation. However, we use the same representa-
tions as in our previous experiment, even though the
clusters provide no benefit on this task. Figure 5
gives an example problem.
The only previous work we are aware of on
this dataset is by MacCartney and Manning (2007).
This approach learns the monotonicity properties
of words from a hand-built training set, and uses
this to transform a sentence into a polarity anno-
tated string. The system then aims to transform the
premise string into a hypothesis. Positively polar-
ized words can be replaced with less specific ones
(e.g. by deleting adjuncts), whereas negatively po-
larized words can be replaced with more specific
ones (e.g. by adding adjuncts). Whilst this is high-
precision and often useful, this logic is unable to per-
form inferences with multiple premise sentences (in
contrast to our first-order logic).
Development consists of adding entries to our lex-
icon for quantifiers. For simplicity, we treat multi-
word quantifiers like at least a few, as being multi-
word expressions?although a more compositional
analysis may be possible. Following MacCartney
and Manning (2007), we do not use held-out data?
each problem is designed to test a different issue, so
it is not possible to generalize from one subset of the
suite to another. As we are interested in evaluating
the semantics, not the parser, we manually supply
gold-standard lexical categories for sentences with
parser errors (any syntactic mistake causes incorrect
semantics). Our derivations produce a distribution
over logical forms?we license the inference if it
holds in any interpretation with non-zero probabil-
ity. We use the Prover9 (McCune, 2005) theorem
prover for inference, returning yes if the premise im-
plies the hypothesis, no if it implies the negation of
the hypothesis, and unknown otherwise.
Results are shown in Table 4. Our system im-
11We use the version converted to machine readable format by
MacCartney and Manning (2007)
12Excluding 6 problems without a defined solution.
186
Question Answer Sentence
What did Delta merge with? Northwest The 747 freighters came with Delta?s acquisition of Northwest
What spoke with Hu Jintao? Obama Obama conveyed his respect for the Dalai Lama to China?s
president Hu Jintao during their first meeting. . .
What arrived in Colorado? Zazi Zazi flew back to Colorado. . .
What ran for Congress? Young . . . Young was elected to Congress in 1972
Table 3: Example questions correctly answered by CCG-Distributional.
Premises: Every European has the right to live in Europe.
Every European is a person.
Every person who has the right to live in Europe can travel freely within Europe.
Hypothesis: Every European can travel freely within Europe
Solution: Yes
Figure 5: Example problem from the FraCaS suite.
System Single Multiple
Premise Premises
MacCartney&Manning 07 84% -
MacCartney&Manning 08 98% -
CCG-Dist (parser syntax) 70% 50%
CCG-Dist (gold syntax) 89% 80%
Table 4: Accuracy on Section 1 of the FraCaS suite.
Problems are divided into those with one premise sen-
tence (44) and those with multiple premises (30).
proves on previous work by making multi-sentence
inferences. Causes of errors include missing a dis-
tinct lexical entry for plural the, only taking existen-
tial interpretations of bare plurals, failing to inter-
pret mass-noun determiners such as a lot of, and not
providing a good semantics for non-monotone de-
terminers such as most. We believe these problems
will be surmountable with more work. Almost all er-
rors are due to incorrectly predicting unknown ? the
system makes just one error on yes or no predictions
(with or without gold syntax). This suggests that
making first-order logic inferences in applications
will not harm precision. We are less robust than
MacCartney and Manning (2007) to syntax errors
but, conversely, we are able to attempt more of the
problems (i.e. those with multi-sentence premises).
Other approaches based on distributional semantics
seem unable to tackle any of these problems, as they
do not represent quantifiers or negation.
9 Related Work
Much work on semantics has taken place in a su-
pervised setting?for example the GeoQuery (Zelle
and Mooney, 1996) and ATIS (Dahl et al, 1994) se-
mantic parsing tasks. This approach makes sense for
generating queries for a specific database, but means
the semantic representations do not generalize to
other datasets. There have been several attempts
to annotate larger corpora with semantics?such as
Ontonotes (Hovy et al, 2006) or the Groningen
Meaning Bank (Basile et al, 2012). These typically
map words onto senses in ontologies such as Word-
Net, VerbNet (Kipper et al, 2000) and FrameNet
(Baker et al, 1998). However, limitations of these
ontologies mean that they do not support inferences
such as X is the author of Y ? X wrote Y.
Given the difficulty of annotating large amounts
of text with semantics, various approaches have at-
tempted to learn meaning without annotated text.
Distant Supervision approaches leverage existing
knowledge bases, such as Freebase (Bollacker et al,
2008), to learn semantics (Mintz et al, 2009; Krish-
namurthy and Mitchell, 2012). Dependency-based
Compositional Semantics (Liang et al, 2011) learns
the meaning of questions by using their answers as
denotations?but this appears to be specific to ques-
tion parsing. Such approaches can only learn the
pre-specified relations in the knowledge base.
The approaches discussed so far in this section
have all attempted to map language onto some pre-
187
specified set of relations. Various attempts have
been made to instead induce relations from text by
clustering predicates based on their arguments. For
example, Yao et al (2011) propose a series of LDA-
based models which cluster relations between en-
tities based on a variety of lexical, syntactic and
semantic features. Unsupervised Semantic Pars-
ing (Poon and Domingos, 2009) recursively clusters
fragments of dependency trees based on their argu-
ments. Although USP is an elegant model, it is too
computationally expensive to run on large corpora.
It is also based on frame semantics, so does not clus-
ter equivalent predicates with different frames. To
our knowledge, our work is the first such approach
to be integrated within a linguistic theory supporting
formal semantics for logical operators.
Vector space models represent words by vectors
based on co-occurrence counts. Recent work has
tackled the problem of composing these matrices
to build up the semantics of phrases or sentences
(Mitchell and Lapata, 2008). Another strand (Co-
ecke et al, 2010; Grefenstette et al, 2011) has
shown how to represent meanings as tensors, whose
order depends on the syntactic category, allowing
an elegant correspondence between syntactic and
semantic types. Socher et al (2012) train a com-
position function using a neural network?however
their method requires annotated data. It is also not
obvious how to represent logical relations such as
quantification in vector-space models. Baroni et al
(2012) make progress towards this by learning a
classifier that can recognise entailments such as all
dogs =? some dogs, but this remains some way
from the power of first-order theorem proving of the
kind required by the problem in Figure 5.
An alternative strand of research has attempted
to build computational models of linguistic theories
based on formal compositional semantics, such as
the CCG-based Boxer (Bos, 2008) and the LFG-
based XLE (Bobrow et al, 2007). Such approaches
convert parser output into formal semantic repre-
sentations, and have demonstrated some ability to
model complex phenomena such as negation. For
lexical semantics, they typically compile lexical re-
sources such as VerbNet and WordNet into inference
rules?but still achieve only low recall on open-
domain tasks, such as RTE, mostly due to the low
coverage of such resources. Garrette et al (2011)
use distributional statistics to determine the proba-
bility that a WordNet-derived inference rule is valid
in a given context. Our approach differs in that
we learn inference rules not present in WordNet.
Our lexical semantics is integrated into the lexicon,
rather than being implemented as additional infer-
ence rules, meaning that inference is more efficient,
as equivalent statements have the same logical form.
Natural Logic (MacCartney and Manning, 2007)
offers an interesting alternative to symbolic logics,
and has been shown to be able to capture complex
logical inferences by simply identifying the scope of
negation in text. This approach achieves similar pre-
cision and much higher recall than Boxer on the RTE
task. Their approach also suffers from such limita-
tions as only being able to make inferences between
two sentences. It is also sensitive to word order, so
cannot make inferences such as Shakespeare wrote
Macbeth =? Macbeth was written by Shakespeare.
10 Conclusions and Future Work
This is the first work we are aware of that combines a
distributionally induced lexicon with formal seman-
tics. Experiments suggest our approach compares
favourably with existing work in both areas.
Many potential areas for improvement remain.
Hierachical clustering would allow us to capture
hypernym relations, rather than the synonyms cap-
tured by our flat clustering. There is much potential
for integrating existing hand-built resources, such
as Ontonotes and WordNet, to improve the accu-
racy of clustering. There are cases where the ex-
isting CCGBank grammar does not match the re-
quired predicate-argument structure?for example
in the case of light verbs. It may be possible to re-
bank CCGBank, in a way similar to Honnibal et al
(2010), to improve it on this point.
Acknowledgements
We thank Christos Christodoulopoulos, Tejaswini
Deoskar, Mark Granroth-Wilding, Ewan Klein, Ka-
trin Erk, Johan Bos and the anonymous reviewers
for their helpful comments, and Limin Yao for shar-
ing code. This work was funded by ERC Advanced
Fellowship 249520 GRAMPLUS and IP EC-FP7-
270273 Xperience.
188
References
C.F. Baker, C.J. Fillmore, and J.B. Lowe. 1998. The
berkeley framenet project. In Proceedings of the
36th Annual Meeting of the Association for Computa-
tional Linguistics and 17th International Conference
on Computational Linguistics-Volume 1, pages 86?90.
Association for Computational Linguistics.
M. Baroni, R. Bernardi, N.Q. Do, and C. Shan. 2012.
Entailment above the word level in distributional se-
mantics. In Proceedings of EACL, pages 23?32. Cite-
seer.
V. Basile, J. Bos, K. Evang, and N. Venhuizen. 2012.
Developing a large semantically annotated corpus. In
Proceedings of the Eighth International Conference
on Language Resources and Evaluation (LREC12). To
appear.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies - Volume 1, HLT ?11, pages 610?
619. Association for Computational Linguistics.
C. Biemann. 2006. Chinese whispers: an efficient graph
clustering algorithm and its application to natural lan-
guage processing problems. In Proceedings of the
First Workshop on Graph Based Methods for Natural
Language Processing, pages 73?80. Association for
Computational Linguistics.
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
dirichlet alocation. the Journal of machine Learning
research, 3:993?1022.
D. G. Bobrow, C. Condoravdi, R. Crouch, V. De Paiva,
L. Karttunen, T. H. King, R. Nairn, L. Price, and
A. Zaenen. 2007. Precision-focused textual inference.
In Proceedings of the ACL-PASCAL Workshop on Tex-
tual Entailment and Paraphrasing, RTE ?07, pages
16?21. Association for Computational Linguistics.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring hu-
man knowledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management of
data, SIGMOD ?08, pages 1247?1250, New York, NY,
USA. ACM.
J. Bos and K. Markert. 2005. Recognising textual en-
tailment with logical inference. In Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Processing,
pages 628?635. Association for Computational Lin-
guistics.
Johan Bos. 2008. Wide-coverage semantic analysis with
boxer. In Johan Bos and Rodolfo Delmonte, editors,
Semantics in Text Processing. STEP 2008 Conference
Proceedings, Research in Computational Semantics,
pages 277?286. College Publications.
P.F. Brown, P.V. Desouza, R.L. Mercer, V.J.D. Pietra, and
J.C. Lai. 1992. Class-based n-gram models of natural
language. Computational linguistics, 18(4):467?479.
N. Chinchor and P. Robinson. 1997. Muc-7 named entity
task definition. In Proceedings of the 7th Conference
on Message Understanding.
Stephen Clark and James R. Curran. 2004. Parsing the
WSJ using CCG and log-linear models. In Proceed-
ings of the 42nd Annual Meeting on Association for
Computational Linguistics, ACL ?04. Association for
Computational Linguistics.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark.
2010. Mathematical foundations for a compositional
distributional model of meaning. Linguistic Analysis:
A Festschrift for Joachim Lambek, 36(1-4):345?384.
Robin Cooper, Dick Crouch, Jan Van Eijck, Chris Fox,
Johan Van Genabith, Jan Jaspars, Hans Kamp, David
Milward, Manfred Pinkal, Massimo Poesio, et al
1996. Using the framework. FraCaS Deliverable D,
16.
Ido Dagan, O. Glickman, and B. Magnini. 2006. The
PASCAL recognising textual entailment challenge.
Machine Learning Challenges. Evaluating Predictive
Uncertainty, Visual Object Classification, and Recog-
nising Tectual Entailment, pages 177?190.
D.A. Dahl, M. Bates, M. Brown, W. Fisher, K. Hunicke-
Smith, D. Pallett, C. Pao, A. Rudnicky, and
E. Shriberg. 1994. Expanding the scope of the ATIS
task: The ATIS-3 corpus. In Proceedings of the work-
shop on Human Language Technology, pages 43?48.
Association for Computational Linguistics.
Hal Daume? III. 2007. Fast search for dirichlet process
mixture models. In Proceedings of the Eleventh In-
ternational Conference on Artificial Intelligence and
Statistics (AIStats), San Juan, Puerto Rico.
D. Davidson. 1967. 6. the logical form of action sen-
tences. Essays on actions and events, 1(9):105?149.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information
extraction. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
EMNLP ?11, pages 1535?1545. Association for Com-
putational Linguistics.
T. Fountain and M. Lapata. 2011. Incremental models of
natural language category acquisition. In Proceedings
of the 32st Annual Conference of the Cognitive Science
Society.
D. Garrette, K. Erk, and R. Mooney. 2011. Integrating
logical representations with probabilistic information
using markov logic. In Proceedings of the Ninth In-
ternational Conference on Computational Semantics,
189
pages 105?114. Association for Computational Lin-
guistics.
D. Graff, J. Kong, K. Chen, and K. Maeda. 2003. English
gigaword. Linguistic Data Consortium, Philadelphia.
Edward Grefenstette, Mehrnoosh Sadrzadeh, Stephen
Clark, Bob Coecke, and Stephen Pulman. 2011. Con-
crete sentence spaces for compositional distributional
models of meaning. Computational Semantics IWCS
2011, page 125.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: a corpus of CCG derivations and dependency
structures extracted from the penn treebank. Compu-
tational Linguistics, 33(3):355?396.
M. Honnibal, J.R. Curran, and J. Bos. 2010. Rebanking
CCGbank for improved NP interpretation. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 207?215. Associa-
tion for Computational Linguistics.
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2006. Ontonotes: the 90% solution.
In Proceedings of the Human Language Technology
Conference of the NAACL, Companion Volume: Short
Papers, pages 57?60. Association for Computational
Linguistics.
P. Kingsbury and M. Palmer. 2002. From treebank to
propbank. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), pages 1989?1993. Citeseer.
K. Kipper, H.T. Dang, and M. Palmer. 2000. Class-based
construction of a verb lexicon. In Proceedings of the
National Conference on Artificial Intelligence, pages
691?696. Menlo Park, CA; Cambridge, MA; London;
AAAI Press; MIT Press; 1999.
Jayant Krishnamurthy and Tom M. Mitchell. 2012.
Weakly supervised training of semantic parsers. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, EMNLP-
CoNLL ?12, pages 754?765. Association for Compu-
tational Linguistics.
P. Liang, M.I. Jordan, and D. Klein. 2011. Learning
dependency-based compositional semantics. In Proc.
Association for Computational Linguistics (ACL).
Dekang Lin and Patrick Pantel. 2001. DIRT - Discovery
of Inference Rules from Text. In In Proceedings of the
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining, pages 323?328.
Bill MacCartney and Christopher D. Manning. 2007.
Natural logic for textual inference. In Proceedings
of the ACL-PASCAL Workshop on Textual Entailment
and Paraphrasing, RTE ?07, pages 193?200. Associa-
tion for Computational Linguistics.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
W. McCune. 2005. Prover9 and Mace4.
http://cs.unm.edu/?mccune/mace4/.
G.A. Miller. 1995. Wordnet: a lexical database for en-
glish. Communications of the ACM, 38(11):39?41.
M. Mintz, S. Bills, R. Snow, and D. Jurafsky. 2009. Dis-
tant supervision for relation extraction without labeled
data. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP: Volume 2-Volume 2, pages 1003?
1011. Association for Computational Linguistics.
J. Mitchell and M. Lapata. 2008. Vector-based models of
semantic composition. proceedings of ACL-08: HLT,
pages 236?244.
R.M. Neal. 2000. Markov chain sampling methods for
dirichlet process mixture models. Journal of compu-
tational and graphical statistics, 9(2):249?265.
D.O. O?Seaghdha. 2010. Latent variable models of se-
lectional preference. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, pages 435?444. Association for Compu-
tational Linguistics.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing: Volume 1 - Volume 1, EMNLP ?09,
pages 1?10. Association for Computational Linguis-
tics.
Hoifung Poon and Pedro Domingos. 2010. Unsuper-
vised ontology induction from text. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics, ACL ?10, pages 296?305. As-
sociation for Computational Linguistics.
A. Ritter, O. Etzioni, et al 2010. A latent dirichlet alo-
cation method for selectional preferences. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 424?434. Associa-
tion for Computational Linguistics.
Stefan Schoenmackers, Oren Etzioni, Daniel S. Weld,
and Jesse Davis. 2010. Learning first-order horn
clauses from web text. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ?10, pages 1088?1098.
Association for Computational Linguistics.
R. Socher, B. Huval, C.D. Manning, and A.Y. Ng. 2012.
Semantic compositionality through recursive matrix-
vector spaces. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 1201?1211. Association for
Computational Linguistics.
190
Mark Steedman. 2000. The Syntactic Process. MIT
Press.
Mark Steedman. 2012. Taking Scope: The Natural Se-
mantics of Quantifiers. MIT Press.
Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007.
Instance-based evaluation of entailment rule acquisi-
tion. In In Proceedings of ACL 2007, volume 45, page
456.
Ivan Titov and Alexandre Klementiev. 2011. A bayesian
model for unsupervised semantic parsing. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 1445?1455, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Limin Yao, Aria Haghighi, Sebastian Riedel, and Andrew
McCallum. 2011. Structured relation discovery using
generative models. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP ?11, pages 1456?1466. Association for
Computational Linguistics.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2012. Unsupervised relation discovery with sense dis-
ambiguation. In ACL (1), pages 712?720.
J.M. Zelle and R.J. Mooney. 1996. Learning to parse
database queries using inductive logic programming.
In Proceedings of the National Conference on Artifi-
cial Intelligence, pages 1050?1055.
191
192
Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 28?32,
Baltimore, Maryland USA, June 26 2014.
c
?2014 Association for Computational Linguistics
Combining Formal and Distributional Models of Temporal and
Intensional Semantics
Mike Lewis
School of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
mike.lewis@ed.ac.uk
Mark Steedman
School of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
steedman@inf.ed.ac.uk
Abstract
We outline a vision for computational se-
mantics in which formal compositional
semantics is combined with a powerful,
structured lexical semantics derived from
distributional statistics. We consider how
existing work (Lewis and Steedman, 2013)
could be extended with a much richer
lexical semantics using recent techniques
for modelling processes (Scaria et al.,
2013)?for example, learning that visit-
ing events start with arriving and end with
leaving. We show how to closely inte-
grate this information with theories of for-
mal semantics, allowing complex compo-
sitional inferences such as is visiting?has
arrived in but will leave, which requires
interpreting both the function and content
words. This will allow machine reading
systems to understand not just what has
happened, but when.
1 Combined Distributional and Logical
Semantics
Distributional semantics aims to induce the mean-
ing of language from unlabelled text. Traditional
approaches to distributional semantics have repre-
sented semantics in vector spaces (Baroni et al.,
2013). Words are assigned vectors based on col-
locations in large corpora, and then these vectors
a composed into vectors representing longer utter-
ances. However, so far there is relatively limited
empirical evidence that composed vectors provide
useful representations for whole sentences, and it
is unclear how to represent logical operators (such
as universal quantifiers) in a vector space. While
future breakthroughs may overcome these limita-
tions, there are already well developed solutions in
the formal semantics literature using logical rep-
resentations. On the other hand, standard for-
mal semantic approaches such as Bos (2008) have
found that hand-built ontologies such as Word-
Net (Miller, 1995) provide an insufficient model
of lexical semantics, leading to low recall on appli-
cations. The complementary strengths and weak-
nesses of formal and distributional semantics mo-
tivate combining them into a single model.
In Lewis and Steedman (2013), we proposed
a solution to these problems which uses CCG
(Steedman, 2012) as a model of formal semantics,
making it straightforward to build wide-coverage
logical forms. Hand built representations are
added for a small number of function words such
as negatives and quantifiers?but the lexical se-
mantics is represented by first clustering predi-
cates (based on their usage in large corpora), and
then using the cluster-identifiers as symbols in the
logical form. For example, the induced CCG lexi-
con might contain entries such as the following
1
:
write ` (S\NP)/NP
: ?y?x?e.rel43(x, y, e)
author `N/PP
of
: ?y?x?e.rel43(x, y, e)
Equivalent sentences like Shakespeare wrote
Macbeth and Shakespeare is the author of
Macbeth can then both be mapped to a
rel43(shakespeare,macbeth) logical form, us-
ing derivations such as:
Shakespeare wrote Macbeth
NP (S\NP)/NP NP
shakespeare ?y?x?e.rel43(x, y, e) macbeth
>
S\NP
?x?e.rel43(x,macbeth, e)
<
S
?e.rel43(shakespeare,macbeth, e)
This approach interacts seamlessly with stan-
dard formal semantics?for example modelling
negation by mapping Francis Bacon didn?t write
Macbeth to ?rel43(francis bacon,macbeth).
Their method has shown good performance on a
dataset of multi-sentence textual inference prob-
lems involving quantifiers, by using first-order the-
1
The e variables are Davidsonian event variables.
28
orem proving. Ambiguity is handled by a proba-
bilistic model, based on the types of the nouns.
Beltagy et al. (2013) use an alternative approach
with similar goals, in which every word instance
expresses a unique semantic primitive, but is con-
nected to the meanings of other word instances us-
ing distributionally-derived probabilistic inference
rules. This approach risks requiring very large
number of inference rules, which may make infer-
ence inefficient. Our approach avoid this problem
by attempting to fully represent lexical semantics
in the lexicon.
2 Proposal
We propose how our previous model could be ex-
tended to make more sophisticated inferences. We
will demonstrate how many interesting problems
in semantics could be solved with a system based
on three components:
? A CCG syntactic parse for modelling com-
position. Using CCG allows us to handle in-
teresting forms of composition, such as co-
ordination, extraction, questions, right node
raising, etc. CCG also has both a developed
theory of operator semantics and a transpar-
ent interface to the underlying predicate ar-
gument structure.
? A small hand built lexicon for words
with complex semantics?such as negatives,
quantifiers, modals, and implicative verbs.
? A rich model of lexical semantics de-
rived from distributionally-induced entail-
ment graphs (Berant et al., 2011), extended
with subcategories of entailment relations in
a similar way to Scaria et al. (2013). We show
how such graphs can be converted into a CCG
lexicon.
2.1 Directional Inference
A major limitation of our previous model is
that it uses a flat clustering to model the
meaning of content words. This method en-
ables them to model synonymy relations be-
tween words, but not relations where the en-
tailment only holds in one direction?for ex-
ample, conquers?invades, but not vice-versa.
This problem can be addressed using the en-
tailment graph framework introduced by Berant
et al. (2011), which learns globally consistent
graphs over predicates in which directed edges
indicate entailment relations. Exactly the same
methods can be used to build entailment graphs
over the predicates derived from a CCG parse:
1
attack
arg0,arg1
2
invade
arg0,arg1
invasion
poss,of
3
conquer
arg0,arg1
annex
arg0,arg1
4
bomb
arg0,arg1
The graph can then be converted to a CCG lexi-
con by making the semantics of a word be the con-
junction of all the relation identifiers it implies in
the graph. For example, the above graph is equiv-
alent to the following lexicon:
attack ` (S\NP)/NP
: ?x?y?e.rel1(x, y, e)
bomb ` (S\NP)/NP
: ?x?y?e.rel1(x, y, e)?rel4(x, y, e)
invade ` (S\NP)/NP
: ?x?y?e.rel1(x, y, e)?rel2(x, y, e)
conquer` (S\NP)/NP
: ?x?y?e.rel1(x, y, e) ?
rel2(x, y, e) ? rel3(x, y, e)
This lexicon supports the correct infer-
ences, such as conquers?attacks and didn?t
invade?didn?t conquer.
2.2 Temporal Semantics
One case where combining formal and distribu-
tional semantics may be particularly helpful is in
giving a detailed model of temporal semantics. A
rich understanding of time would allow us to un-
derstand when events took place, or when states
were true. Most existing work ignores tense, and
would treat the expressions used to be president
and is president either as equivalent or completely
unrelated. Failing to model tense would lead to in-
correct inferences when answering questions such
as Who is the president of the USA?
Another motivation for considering a detailed
model of temporal semantics is that understanding
the time of events should improve the quality of
the distributional clustering. It has recently been
shown that such information is extremely useful
for learning equivalences between predicates, by
determining which sentences describe the same
29
events using date-stamped text and simple tense
heuristics (Zhang and Weld, 2013). Such meth-
ods escape common problems with traditional ap-
proaches to distributional similarity, such as con-
flating causes with effects, and may prove very
useful for building entailment graphs.
Temporal information is conveyed by both by
auxiliary verbs such as will or used to, and in
the semantics of content words. For example, the
statement John is visiting Baltimore licences en-
tailments such as John has arrived in Baltimore
and John will leave Baltimore, which can only be
understood through both knowledge of tense and
lexical semantic relations.
The requisite information about lexical seman-
tics could be represented by labelling edges in the
entailment graphs, along the lines of Scaria et al.
(2013). Instead of edges simply representing en-
tailment, they should represent different kinds of
lexical relations, such as precondition or conse-
quence. Building such graphs requires training
classifiers that predict fine-grained semantic rela-
tions between predicates, and defining transitivity
properties of the relations (e.g. a precondition of a
precondition is a precondition). For example, the
system might learn the following graph:
1
visit
arg0,arg1
3
leave
arg0,arg1
exit
arg0,arg1
depart
arg0,from
2
arrive
arg0,in
reach
arg0,arg1
initiated by terminated by
By defining a simple mapping between edge la-
bels and logical forms, this graph can be converted
to CCG lexical entries such as:
visit ` (S\NP)/NP
: ?y?x?e.rel1(x, y, e) ?
?e
?
[rel2(x, y, e
?
)? before(e, e
?
)]?
?e
??
[rel3(x, y, e
??
) ? after(e, e
??
)]
arrive ` (S\NP)/PP
in
: ?y?x?e.rel2(x, y, e)
leave ` (S\NP)/NP
: ?y?x?e.rel3(x, y, e)
These lexical entries could be complemented
with hand-built interpretations for a small set of
common auxiliary verbs:
has ` (S\NP)/(S
b
\NP)
: ?p?x?e.before(r, e) ? p(x, e)
will ` (S\NP)/(S
b
\NP)
: ?p?x?e.after(r, e) ? p(x, e)
is ` (S\NP)/(S
ng
\NP)
: ?p?x?e.during(r, e) ? p(x, e)
used ` (S\NP)/(S
to
\NP)
: ?p?x?e.before(r, e) ? p(x, e) ?
??e
?
[during(r) ? p(x, e
?
)]
Here, r is the reference time (e.g. the time that
the news article was written). It is easy to verify
that such a lexicon supports inferences such as is
visiting?will leave, has visited?has arrived in,
or used to be president?is not president.
The model described here only discusses tense,
not aspect?so does not distinguish between John
arrived in Baltimore and John has arrived in Bal-
timore (the latter says that the consequences of his
arrival still hold?i.e. that he is still in Baltimore).
Going further, we could implement the much more
detailed proposal of Moens and Steedman (1988).
Building this model would require distinguishing
states from events?for example, the semantics of
arrive, visit and leave could all be expressed in
terms of the times that an is in state holds.
2.3 Intensional Semantics
Similar work could be done by subcatego-
rizing edges in the graph with other lexi-
cal relations. For example, we could ex-
tend the graph with goal relations between
words, such as between set out for and ar-
rive in, search and find, or invade and conquer:
1
set out
arg0,for
head
arg0,to
2
arrive
arg0,in
reach
arg0,arg1
goal
The corresponding lexicon contains entries such
as:
set out ` (S\NP)/PP
for
: ?y?x?e.rel1(x, y, e) ?
?e
?
[goal(e, e
?
) ? rel2(x, y, e
?
)]
The modal logic  operator is used to mark that
the goal event is a hypothetical proposition, that
is not asserted to be true in the real world?so
Columbus set out for India6?Columbus reached
India. The same mechanism allows us to handle
Montague (1973)?s example that John seeks a uni-
corn does not imply the existence of a unicorn.
Just as temporal information can be expressed
by auxiliary verbs, relations such as goals can
30
Columbus failed to reach India
<
S
dcl
/(S
dcl
\NP ) (S
dcl
\NP )/(S
to
\NP ) (S
to
\NP )/(S
b
\NP ) S
b
\NP
?p.p(Columbus) ?p?x?e.  ?e
?
[p(x, e
?
) ? goal(e
?
, e)] ? ??e
??
[p(x, e
??
)] ?p?x?e.p(x, e) ?x?e.rel2(x, India, e)
>
S
to
\NP
?x?e.rel2(x, India, e)
>
S
dcl
\NP
?x?e.  ?e
?
[rel2(x, India, e
?
) ? goal(e
?
, e)] ? ??e
??
[rel2(x, India, e
??
)]
>
S
dcl
?e.  ?e
?
[rel2(Columbus, India, e
?
) ? goal(e
?
, e)] ? ??e
??
[rel2(Columbus, India, e
??
)]
Figure 1: Output from our system for the sentence Columbus failed to reach India
be expressed using implicative verbs like try or
fail. As the semantics of implicative verbs is of-
ten complex (Karttunen, 1971), we propose hand-
coding their lexical entries:
try ` (S\NP)/(S
to
\NP)
: ?p?x?e.?e
?
[goal(e, e
?
)?p(x, e
?
)]
fail ` (S\NP)/(S
to
\NP)
: ?p?x?e.?e
?
[goal(e, e
?
)?p(x, e
?
)]?
??e
??
[goal(e, e
??
) ? p(x, e
??
)]
The  operator is used to assert that the comple-
ment of try is a hypothetical proposition (so try to
reach 6?reach). Our semantics for fail is the same
as that for try, except that it asserts that the goal
event did not occur in the real world.
These lexical entries allow us to make complex
compositional inferences, for example Columbus
failed to reach India now entails Columbus set
out for India, Columbus tried to reach India and
Columbus didn?t arrive in India.
Again, we expect that the improved model of
formal semantics should increase the quality of
the entailment graphs, by allowing us to only clus-
ter predicates based on their real-world arguments
(ignoring hypothetical events).
3 Conclusion
We have argued that several promising recent
threads of research in semantics can be combined
into a single model. The model we have described
would enable wide-coverage mapping of open-
domain text onto rich logical forms that model
complex aspects of semantics such as negation,
quantification, modality and tense?whilst also
using a robust distributional model of lexical se-
mantics that captures the structure of events. Con-
sidering these interwined issues would allow com-
plex compositional inferences which are far be-
yond the current state of the art, and would give
a more powerful model for natural language un-
derstanding.
Acknowledgements
We thank Omri Abend, Michael Roth and the
anonymous reviewers for their helpful comments.
This work was funded by ERC Advanced Fellow-
ship 249520 GRAMPLUS and IP EC-FP7-270273
Xperience.
References
M. Baroni, R. Bernardi, and R. Zamparelli. 2013.
Frege in space: A program for compositional dis-
tributional semantics. Linguistic Issues in Language
Technologies.
Islam Beltagy, Cuong Chau, Gemma Boleda, Dan Gar-
rette, Katrin Erk, and Raymond Mooney. 2013.
Montague meets markov: Deep semantics with
probabilistic logical form. pages 11?21, June.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies - Volume 1, HLT ?11, pages
610?619. Association for Computational Linguis-
tics.
Johan Bos. 2008. Wide-coverage semantic analy-
sis with boxer. In Johan Bos and Rodolfo Del-
monte, editors, Semantics in Text Processing. STEP
2008 Conference Proceedings, Research in Compu-
tational Semantics, pages 277?286. College Publi-
cations.
L. Karttunen. 1971. The Logic of English Predi-
cate Complement Constructions. Linguistics Club
Bloomington, Ind: IU Linguistics Club. Indiana
University Linguistics Club.
Mike Lewis and Mark Steedman. 2013. Combined
Distributional and Logical Semantics. Transactions
of the Association for Computational Linguistics,
1:179?192.
G.A. Miller. 1995. Wordnet: a lexical database for
english. Communications of the ACM, 38(11):39?
41.
31
Marc Moens and Mark Steedman. 1988. Temporal on-
tology and temporal reference. Computational lin-
guistics, 14(2):15?28.
Richard Montague. 1973. The proper treatment of
quantification in ordinary english. In Approaches to
natural language, pages 221?242. Springer.
Aju Thalappillil Scaria, Jonathan Berant, Mengqiu
Wang, Peter Clark, Justin Lewis, Brittany Harding,
and Christopher D. Manning. 2013. Learning bi-
ological processes with global constraints. In Pro-
ceedings of EMNLP.
Mark Steedman. 2012. Taking Scope: The Natural
Semantics of Quantifiers. MIT Press.
Congle Zhang and Daniel S Weld. 2013. Harvest-
ing parallel news streams to generate paraphrases of
event relations.
32
