Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 209?216, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Minimum Sample Risk Methods for Language Modeling1  
Jianfeng Gao  
Microsoft Research Asia 
jfgao@microsoft.com 
Hao Yu,  Wei Yuan 
 Shanghai Jiaotong Univ., China 
Peng Xu 
John Hopkins Univ., U.S.A. 
xp@clsp.jhu.edu 
                                                     
1
  The work was done while the second, third and fourth authors were visiting Microsoft Research Asia. Thanks to Hisami Suzuki for 
her valuable comments. 
Abstract 
This paper proposes a new discriminative 
training method, called minimum sample risk 
(MSR), of estimating parameters of language 
models for text input. While most existing 
discriminative training methods use a loss 
function that can be optimized easily but 
approaches only approximately to the objec-
tive of minimum error rate, MSR minimizes 
the training error directly using a heuristic 
training procedure. Evaluations on the task 
of Japanese text input show that MSR can 
handle a large number of features and train-
ing samples; it significantly outperforms a 
regular trigram model trained using maxi-
mum likelihood estimation, and it also out-
performs the two widely applied discrimi-
native methods, the boosting and the per-
ceptron algorithms, by a small but statisti-
cally significant margin. 
1 Introduction 
Language modeling (LM) is fundamental to a wide 
range of applications, such as speech recognition 
and Asian language text input (Jelinek 1997; Gao et 
al. 2002). The traditional approach uses a paramet-
ric model with maximum likelihood estimation (MLE), 
usually with smoothing methods to deal with data 
sparseness problems. This approach is optimal 
under the assumption that the true distribution of 
data on which the parametric model is based is 
known. Unfortunately, such an assumption rarely 
holds in realistic applications. 
An alternative approach to LM is based on the 
framework of discriminative training, which uses a 
much weaker assumption that training and test 
data are generated from the same distribution but 
the form of the distribution is unknown. Unlike the 
traditional approach that maximizes the function 
(i.e. likelihood of training data) that is loosely as-
sociated with error rate, discriminative training 
methods aim to directly minimize the error rate on 
training data even if they reduce the likelihood. So, 
they potentially lead to better solutions. However, 
the error rate of a finite set of training samples is 
usually a step function of model parameters, and 
cannot be easily minimized. To address this prob-
lem, previous research has concentrated on the 
development of a loss function that approximates 
the exact error rate and can be easily optimized. 
Though these methods (e.g. the boosting method) 
have theoretically appealing properties, such as 
convergence and bounded generalization error, we 
argue that the approximated loss function may 
prevent them from attaining the original objective 
of minimizing the error rate. 
In this paper we present a new estimation pro-
cedure for LM, called minimum sample risk (MSR). It 
differs from most existing discriminative training 
methods in that instead of searching on an ap-
proximated loss function, MSR employs a simple 
heuristic training algorithm that minimizes the 
error rate on training samples directly. MSR oper-
ates like a multidimensional function optimization 
algorithm: first, it selects a subset of features that 
are the most effective among all candidate features. 
The parameters of the model are then optimized 
iteratively: in each iteration, only the parameter of 
one feature is adjusted. Both feature selection and 
parameter optimization are based on the criterion 
of minimizing the error on training samples. Our 
evaluation on the task of Japanese text input shows 
that MSR achieves more than 20% error rate reduc-
tion over MLE on two newswire data sets, and it 
also outperforms the other two widely applied 
discriminative methods, the boosting method and 
the perceptron algorithm, by a small but statisti-
cally significant margin. 
Although it has not been proved in theory that 
MSR is always robust, our experiments of cross- 
domain LM adaptation show that it is. MSR can 
effectively adapt a model trained on one domain to 
209
different domains. It outperforms the traditional 
LM adaptation method significantly, and achieves 
at least comparable or slightly better results to the 
boosting method and the perceptron algorithm. 
2 IME Task and LM 
This paper studies LM on the task of Asian lan-
guage (e.g. Chinese or Japanese) text input. This is 
the standard method of inputting Chinese or 
Japanese text by converting the input phonetic 
symbols into the appropriate word string. In this 
paper we call the task IME, which stands for input 
method editor, based on the name of the commonly 
used Windows-based application. 
Performance on IME is measured in terms of the 
character error rate (CER), which is the number of 
characters wrongly converted from the phonetic 
string divided by the number of characters in the 
correct transcript. Current IME systems make 
about 5-15% CER in conversion of real data in a 
wide variety of domains (e.g. Gao et al 2002).  
Similar to speech recognition, IME is viewed as 
a Bayes decision problem. Let A be the input pho-
netic string. An IME system?s task is to choose the 
most likely word string W* among those candidates 
that could be converted from A: 
)|()(maxarg)|(maxarg
(A))(
* WAPWPAWPW
WAW GENGEN ??
==  (1) 
where GEN(A) denotes the candidate set given A. 
Unlike speech recognition, however, there is no 
acoustic ambiguity since the phonetic string is 
inputted by users. Moreover, if we do not take into 
account typing errors, it is reasonable to assume a  
unique mapping from W and A in IME, i.e. P(A|W) 
= 1. So the decision of Equation (1) depends solely 
upon P(W), making IME a more direct evaluation 
test bed for LM than speech recognition. Another 
advantage is that it is easy to convert W to A (for 
Chinese and Japanese), which enables us to obtain 
a large number of training data for discriminative 
learning, as described later.  
The values of P(W) in Equation (1) are tradi-
tionally calculated by MLE: the optimal model 
parameters ?* are chosen in such a way that 
P(W|?*) is maximized on training data. The argu-
ments in favor of MLE are based on the assumption 
that the form of the underlying distributions is 
known, and that only the values of the parameters 
characterizing those distributions are unknown. In 
using MLE for LM, one always assumes a multi-
nomial distribution of language. For example, a 
trigram model makes the assumption that the next 
word is predicted depending only on two preced-
ing words. However, there are many cases in 
natural language where words over an arbitrary 
distance can be related. MLE is therefore not opti-
mal because the assumed model form is incorrect. 
What are the best estimators when the model is 
known to be false then? In IME, we can tackle this 
question empirically. Best IME systems achieve the 
least CER. Therefore, the best estimators are those 
which minimize the expected error rate on unseen 
test data. Since the distribution of test data is un-
known, we can approximately minimize the error 
rate on some given training data (Vapnik 1999). 
Toward this end, we have developed a very simple 
heuristic training procedure called minimum sample 
risk, as presented in the next section. 
3 Minimum Sample Risk 
3.1 Problem Definition 
We follow the general framework of linear dis-
criminant models described in (Duda et al 2001). In 
the rest of the paper we use the following notation, 
adapted from Collins (2002). 
? Training data is a set of example input/output 
pairs. In LM for IME, training samples are repre-
sented as {Ai, WiR}, for i = 1?M, where each Ai is an 
input phonetic string and WiR is the reference tran-
script of Ai. 
? We assume some way of generating a set of 
candidate word strings given A, denoted by 
GEN(A).  In our experiments, GEN(A) consists of 
top N word strings converted from A using a base-
line IME system that uses only a word trigram 
model. 
? We assume a set of D+1 features fd(W), for d = 
0?D. The features could be arbitrary functions that 
map W to real values. Using vector notation, we 
have f(W)??D+1, where f(W) = [f0(W), f1(W), ?, 
fD(W)]T. Without loss of generality, f0(W) is called 
the base feature, and is defined in our case as the 
log probability that the word trigram model as-
signs to W. Other features (fd(W), for d = 1?D) are 
defined as the counts of word n-grams (n = 1 and 2 
in our experiments) in W. 
? Finally, the parameters of the model form a 
vector of D+1 dimensions, each for one feature 
function, ? = [?0, ?1, ?, ?D]. The score of a word 
string W can be written as  
210
)(),( WWScore ?f? = ?
=
=
D
d
dd Wf?
0
)( . (2)
The decision rule of Equation (1) is rewritten as 
),(maxarg),(
(A)
* ??
GEN
WScoreAW
W?
= . (3)
Equation (3) views IME as a ranking problem, 
where the model gives the ranking score, not 
probabilities. We therefore do not evaluate the 
model via perplexity. 
Now, assume that we can measure the number 
of conversion errors in W by comparing it with a 
reference transcript WR using an error function 
Er(WR,W) (i.e.  the string edit distance function in 
our case). We call the sum of error counts over the 
training samples sample risk. Our goal is to mini-
mize the sample risk while searching for the pa-
rameters as defined in Equation (4), hence the name 
minimum sample risk (MSR). Wi* in Equation (4) is 
determined by Equation (3), 
?
=
=
Mi
ii
R
i
def
MSR AWW
...1
* )),(,Er(minarg ??
?
. (4)
We first present the basic MSR training algorithm, 
and then the two improvements we made. 
3.2 Training Algorithm 
The MSR training algorithm is cast as a multidi-
mensional function optimization approach (Press 
et al 1992): taking the feature vector as a set of 
directions; the first direction (i.e. feature) is selected 
and the objective function (i.e. sample risk) is 
minimized along that direction using a line search; 
then from there along the second direction to its 
minimum, and so on, cycling through the whole set 
of directions as many times as necessary, until the 
objective function stops decreasing.  
This simple method can work properly under 
two assumptions. First, there exists an implemen-
tation of line search that optimizes the function 
along one direction efficiently. Second, the number 
of candidate features is not too large, and these 
features are not highly correlated. However, nei-
ther of the assumptions holds in our case. First of 
all, Er(.) in Equation (4) is a step function of ?, thus 
cannot be optimized directly by regular gradient- 
based procedures ? a grid search has to be used 
instead. However, there are problems with simple 
grid search: using a large grid could miss the op-
timal solution whereas using a fine-grained grid 
would lead to a very slow algorithm. Secondly, in 
the case of LM, there are millions of candidate 
features, some of which are highly correlated. We 
address these issues respectively in the next two 
subsections. 
3.3 Grid Line Search 
Our implementation of a grid search is a modified 
version of that proposed in (Och 2003). The modi-
fications are made to deal with the efficiency issue 
due to the fact that there is a very large number of 
features and training samples in our task, compared 
to only 8 features used in (Och 2003). Unlike a 
simple grid search where the intervals between any 
two adjacent grids are equal and fixed, we deter-
mine for each feature a sequence of grids with 
differently sized intervals, each corresponding to a 
different value of sample risk. 
As shown in Equation (4), the loss function (i.e. 
sample risk) over all training samples is the sum of 
the loss function (i.e. Er(.)) of each training sample. 
Therefore, in what follows, we begin with a discus-
sion on minimizing Er(.) of a training sample using 
the line search.  
Let ? be the current model parameter vector, 
and fd be the selected feature. The line search aims to 
find the optimal parameter ?d* so as to minimize 
Er(.). For a training sample (A, WR), the score of each 
candidate word string W?GEN(A), as in Equation 
(2), can be decomposed into two terms: 
)()()(),(
'0'
'' WfWfWWScore dd
D
ddd
dd ?? +== ?
??=
?f? , 
where the first term on the right hand side does not 
change with ?d. Note that if several candidate word 
strings have the same feature value fd(W), their 
relative rank will remain the same for any ?d. Since 
fd(W) takes integer values in our case (fd(W) is the 
count of a particular n-gram in W), we can group the 
candidates using fd(W) so that candidates in each 
group have the same value of fd(W). In each group, 
we define the candidate with the highest value of  
? ??=D ddd dd Wf'0' '' )(?  
as the active candidate of the group because no 
matter what value ?d takes, only this candidate 
could be selected according to Equation (3). 
Now, we reduce GEN(A) to a much smaller list 
of active candidates. We can find a set of intervals 
for ?d, within each of which a particular active 
candidate will be selected as W*. We can compute 
the Er(.) value of that candidate as the Er(.) value for 
the corresponding interval. As a result, for each 
211
training sample, we obtain a sequence of intervals 
and their corresponding Er(.) values. The optimal 
value ?d* can then be found by traversing the se-
quence and taking the midpoint of the interval with 
the lowest Er(.) value.  
305
306
307
308
309
310
311
312
0. 85 0. 9 0. 95 1 1.05 1.1 1. 15 1. 2lambda
SR
(.)
sample risk
smoothed sample risk
 
Figure 1. Examples of line search.  
This process can be extended to the whole 
training set as follows. By merging the sequence of 
intervals of each training sample in the training set, 
we obtain a global sequence of intervals as well as 
their corresponding sample risk. We can then find 
the optimal value ?d* as well as the minimal sample 
risk by traversing the global interval sequence. An 
example is shown in Figure 1. 
The line search can be unstable, however. In 
some cases when some of the intervals are very 
narrow (e.g. the interval A in Figure 1), moving the 
optimal value ?d* slightly can lead to much larger 
sample risk. Intuitively, we prefer a stable solution 
which is also known as a robust solution (with even 
slightly higher sample risk, e.g. the interval B in 
Figure 1). Following Quirk et al (2004), we evaluate 
each interval in the sequence by its corresponding 
smoothed sample risk. Let ? be the midpoint of an 
interval and SR(?) be the corresponding sample risk 
of the interval. The smoothed sample risk of the 
interval is defined as 
???? d
b
b
 )SR(? +?   
where b is a smoothing factor whose value is de-
termined empirically  (0.06 in our experiments). As 
shown in Figure 1, a more stable interval B is se-
lected according to the smoothed sample risk. 
In addition to reducing GEN(A) to an active 
candidate list described above, the efficiency of the 
line search can be further improved. We find that 
the line search only needs to traverse a small subset 
of training samples because the distribution of 
features among training samples are very sparse. 
Therefore, we built an inverted index that lists for 
each feature all training samples that contain it. As 
will be shown in Section 4.2, the line search is very 
efficient even for a large training set with millions of 
candidate features. 
3.4 Feature Subset Selection 
This section describes our method of selecting 
among millions of features a small subset of highly 
effective features for MSR learning. Reducing the 
number of features is essential for two reasons: to 
reduce computational complexity and to ensure the 
generalization property of the linear model. A large 
number of features lead to a large number of pa-
rameters of the resulting linear model, as described 
in Section 3.1. For a limited number of training 
samples, keeping the number of features suffi-
ciently small should lead to a simpler model that is 
less likely to overfit to the training data. 
The first step of our feature selection algorithm 
treats the features independently. The effectiveness 
of a feature is measured in terms of the reduction of 
the sample risk on top of the base feature f0. For-
mally, let SR(f0) be the sample risk of using the base 
feature only, and SR(f0 + ?dfd) be the sample risk of 
using both f0 and fd and the parameter ?d that has 
been optimized using the line search. Then the 
effectiveness of fd, denoted by E(fd), is given by 
))SR()(SR(max
)SR()SR(
)(
00
...1,
00
ii
Dif
dd
d fff
fff
fE
i
?
?
+?
+?=
=
, (5)
where the denominator is a normalization term to 
ensure that E(f) ? [0, 1]. 
The feature selection procedure can be stated as 
follows: The value of E(.) is computed according to 
Equation (5) for each of the candidate features. 
Features are then ranked in the order of descending 
values of E(.). The top l features are selected to form 
the feature vector in the linear model. 
Treating features independently has the ad-
vantage of computational simplicity, but may not 
be effective for features with high correlation. For 
instance, although two features may carry rich 
discriminative information when treated sepa-
rately, there may be very little gain if they are com-
bined in a feature vector, because of the high cor-
relation between them. Therefore, in what follows, 
we describe a technique of incorporating correla-
tion information in the feature selection criterion.  
Let xmd, m = 1?M and d = 1?D, be a Boolean 
value: xmd = 1 if the sample risk reduction of using 
the d-th feature on the m-th training sample, com-
B 
A
212
puted by Equation (5), is larger than zero, and 0 
otherwise. The cross correlation coefficient be-
tween two features fi and fj is estimated as 
??
?
==
==
M
m mj
M
m mi
M
m mjmi
xx
xx
jiC
1
2
1
2
1),( . (6)
It can be shown that C(i, j) ? [0, 1]. Now, similar to  
(Theodoridis and Koutroumbas 2003), the feature 
selection procedure consists of the following steps, 
where fi denotes any selected feature and fj denotes 
any candidate feature to be selected. 
Step 1. For each of the candidate features (fd, for d = 
1?D), compute the value of E(f) according to 
Equation (5). Rank them in a descending order and 
choose the one with the highest E(.) value. Let us 
denote this feature as f1. 
Step 2. To select the second feature, compute the 
cross correlation coefficient between the selected 
feature f1 and each of the remaining M-1 features, 
according to Equation (6). 
Step 3. Select the second feature f according to { } ),1()1()(maxarg*
...2
jCfEj j
Dj
?? ??=
=
 
where ? is the weight that determines the relative 
importance we give to the two terms. The value of 
? is optimized on held-out data (0.8 in our experi-
ments). This means that for the selection of the 
second feature, we take into account not only its 
impact of reducing the sample risk but also the 
correlation with the previously selected feature. It 
is expected that choosing features with less corre-
lation gives better sample risk minimization. 
Step 4. Select k-th features, k = 3?K, according to 
??
?
??
?
?
??= ??
=
1
1
),(
1
1
)(maxarg*
k
i
j
j
jiC
k
fEj
??  (7)
That is, we select the next feature by taking into 
account its average correlation with all previously 
selected features. The optimal number of features, l, 
is determined on held-out data. 
Similarly to the case of line search, we need to 
deal with the efficiency issue in the feature selec-
tion method. As shown in Equation (7), the esti-
mates of E(.) and C(.) need to be computed. Let D 
and K (K << D) be the number of all candidate 
features and the number of features in the resulting 
model, respectively. According to the feature se-
lection method described above, we need to esti-
mate E(.) for each of the D candidate features only 
once in Step 1. This is not very costly due to the 
efficiency of our line search algorithm. Unlike the 
case of E(.), O(K?D) estimates of C(.) are required in 
Step 4. This is computationally expensive even for a 
medium-sized K. Therefore, every time a new fea-
ture is selected (in Step 4), we only estimate the 
value of C(.) between each of the selected features 
and each of the top N remaining features with the 
highest value of E(.). This reduces the number of 
estimates of C(.) to O(K?N). In our experiments we 
set N = 1000, much smaller than D. This reduces the 
computational cost significantly without producing 
any noticeable quality loss in the resulting model. 
The MSR algorithm used in our experiments is 
summarized in Figure 2. It consists of feature se-
lection (line 2) and optimization (lines 3 - 5) steps. 
1 Set ?0 = 1 and ?d = 0 for d=1?D 
2 Rank all features and select the top K features, using 
the feature selection method described in Section 3.4
3 For t = 1?T (T= total number of iterations) 
4 For each k = 1?K  
5    Update the parameter of fk using line search.  
Figure 2: The MSR algorithm 
4 Evaluation 
4.1 Settings 
We evaluated MSR on the task of Japanese IME. 
Two newspaper corpora are used as training and 
test data: Nikkei and Yomiuri Newspapers. Both 
corpora have been pre-word-segmented using a 
lexicon containing 167,107 entries. A 5,000-sentence 
subset of the Yomiuri Newspaper corpus  was used 
as held-out data (e.g. to determine learning rate, 
number of iterations and features etc.). We tested 
our models on another  5,000-sentence subset of the 
Yomiuri Newspaper corpus.  
We used an 80,000-sentence subset of the Nikkei 
Newspaper corpus as the training set. For each A, 
we produced a word lattice using the baseline 
system described in (Gao et al 2002), which uses a 
word trigram model trained via MLE on anther 
400,000-sentence subset of the Nikkei Newspaper 
corpus. The two subsets do not overlap so as to 
simulate the case where unseen phonetic symbol 
strings are converted by the baseline system. For 
efficiency, we kept for each training sample the 
best 20 hypotheses in its candidate conversion set 
GEN(A) for discriminative training. The oracle best 
hypothesis, which gives the minimum number of 
errors, was used as the reference transcript of A. 
213
4.2 Results 
We used unigrams and bigrams that occurred more 
than once in the training set as features. We did not 
use trigram features because they did not result in a 
significant improvement in our pilot study. The 
total number of candidate features we used was 
around 860,000.  
Our main experimental results are shown in 
Table 1. Row 1 is our baseline result using the word 
trigram model. Notice that the result is much better 
than the state-of-the-art performance currently 
available in the marketplace (e.g. Gao et al 2002), 
presumably due to the large amount of training 
data we used, and to the similarity between the 
training and the test data. Row 2 is the result of the 
model trained using the MSR algorithm described 
in Section 3. We also compared the MSR algorithm 
to two of the state-of-the-art discriminative training 
methods: Boosting in Row 3 is an implementation 
of the improved algorithm for the boosting loss 
function proposed in (Collins 2000), and Percep-
tron in Row 4 is an implementation of the averaged 
perceptron algorithm described in (Collins 2002).  
We see that all discriminative training methods 
outperform MLE significantly (p-value < 0.01). In 
particular, MSR outperforms MLE by more than 
20% CER reduction. Notice that we used only uni-
gram and bigram features that have been included 
in the baseline trigram model, so the improvement 
is solely attributed to the high performance of MSR. 
We also find that MSR outperforms the perceptron 
and boosting methods by a small but statistically 
significant margin. 
The MSR algorithm is also very efficient: using a 
subset of 20,000 features, it takes less than 20 min-
utes to converge on an XEON(TM) MP 1.90GHz 
machine. It is as efficient as the perceptron algo-
rithm and slightly faster than the boosting method. 
4.3 Robustness Issues 
Most theorems that justify the robustness of dis-
criminative training algorithms concern two ques-
tions. First, is there a guarantee that a given algo-
rithm converges even if the training samples are 
not linearly separable? This is called the convergence 
problem. Second, how well is the training error 
reduction preserved when the algorithm is applied 
to unseen test samples? This is called the generali-
zation problem. Though we currently cannot give a 
theoretical justification, we present empirical evi-
dence here for the robustness of the MSR approach. 
As Vapnik (1999) pointed out, the most robust 
linear models are the ones that achieve the least 
training errors with the least number of features. 
Therefore, the robustness of the MSR algorithm are 
mainly affected by the feature selection method. To 
verify this, we created four different subsets of 
features using different settings of the feature se-
lection method described in Section 3.4. We se-
lected different numbers of features (i.e. 500 and 
2000) with and without taking into account the 
correlation between features (i.e. ? in Equation (7) 
is set to 0.8 and 1, respectively). For each of the four 
feature subsets, we used the MSR algorithm to 
generate a set of models. The CER curves of these 
models on training and test data sets are shown in 
Figures 3 and 4, respectively.  
2.08
2.10
2.12
2.14
2.16
2.18
2.20
2.22
2.24
2.26
2.28
1 250 500 750 1000 1250 1500 1750 2000
# of rounds
C
E
R
(%
)
MSR(?=1)-2000
MSR(?=1)-500
MSR(?=0.8)-2000
MSR(?=0.8)-500
 
Figure 3. Training error curves of the MSR algorithm 
2.94
2.99
3.04
3.09
3.14
3.19
3.24
1 250 500 750 1000 1250 1500 1750 2000
# of rounds
C
E
R
(%
)
MSR(?=1)-2000
MSR(?=1)-500
MSR(?=0.8)-2000
MSR(?=0.8)-500
 
Figure 4. Test error curves of the MSR algorithm 
The results reveal several facts. First, the con-
vergence properties of MSR are shown in Figure 3 
where in all cases, training errors drop consistently 
with more iterations. Secondly, as expected, using 
more features leads to overfitting, For example, 
MSR(? =1)-2000 makes fewer errors than MSR(? 
=1)-500 on training data but more errors on test 
data. Finally, taking into account the correlation 
between features (e.g. ? = 0.8 in Equation (7)) re-
 Model CER (%) % over MLE 
1. MLE  3.70 -- 
2. MSR (K=2000) 2.95 20.9 
3. Boosting  3.06 18.0 
4. Perceptron 3.07 17.8 
Table 1. Comparison of CER results. 
214
sults in a better subset of features that lead to not 
only fewer training errors, as shown in Figure 3, 
but also better generalization properties (fewer test 
errors), as shown in Figure 4. 
4.4 Domain Adaptation Results  
Though MSR achieves impressive performance in 
CER reduction over the comparison methods, as 
described in Section 4.2, the experiments are all 
performed using newspaper text for both training 
and testing, which is not a realistic scenario if we 
are to deploy the model in an application. This 
section reports the results of additional experi-
ments in which we adapt a model trained on one 
domain to a different domain, i.e., in a so-called 
cross-domain LM adaptation paradigm. See (Su-
zuki and Gao 2005) for a detailed report. 
The data sets we used stem from five distinct 
sources of text. The Nikkei newspaper corpus de-
scribed in Section 4.1 was used as the background 
domain, on which the word trigram model was 
trained. We used four adaptation domains: Yomi-
uri (newspaper corpus), TuneUp (balanced corpus 
containing newspapers and other sources of text), 
Encarta (encyclopedia) and Shincho (collection of 
novels). For each of the four domains, we used an 
72,000-sentence subset as adaptation training data, 
a 5,000-sentence subset as held-out data and an-
other 5,000-sentence subset as test data. Similarly, 
all corpora have been word-segmented, and we 
kept for each training sample, in the four adapta-
tion domains, the best 20 hypotheses in its candi-
date conversion set for discriminative training.  
We compared MSR with three other LM adap-
tation methods:  
Baseline is the background word trigram model, 
as described in Section 4.1. 
MAP (maximum a posteriori) is a traditional LM 
adaptation method where the parameters of the 
background model are adjusted in such a way that 
maximizes the likelihood of the adaptation data. 
Our implementation takes the form of linear in-
terpolation as P(wi|h) = ?Pb(wi|h) + (1-?)Pa(wi|h), 
where Pb is the probability of the background 
model, Pa is the probability trained on adaptation 
data using MLE and the history h corresponds to 
two preceding words (i.e. Pb and Pa are trigram 
probabilities). ? is the interpolation weight opti-
mized on held-out data.  
Perceptron, Boosting and MSR are the three 
discriminative methods described in the previous 
sections.  For each of them, the base feature was 
Model Yomiuri TuneUp Encarta Shincho 
Baseline 3.70 5.81 10.24 12.18 
MAP  3.69 5.47 7.98 10.76 
MSR  2.73 5.15 7.40 10.16 
Boosting  2.78 5.33 7.53 10.25 
Perceptron 2.78 5.20 7.44 10.18 
Table 2. CER(%) results on four adaptation test sets . 
derived from the word trigram model trained on 
the background data, and other n-gram features (i.e. 
fd, d = 1?D in Equation (2)) were trained on adap-
tation data. That is, the parameters of the back-
ground model are adjusted in such a way that 
minimizes the errors on adaptation data made by 
background model. 
Results are summarized in Table 2. First of all, 
in all four adaptation domains, discriminative 
methods outperform MAP significantly. Secondly, 
the improvement margins of discriminative 
methods over MAP correspond to the similarities 
between background domain and adaptation do-
mains. When the two domains are very similar to 
the background domain (such as Yomiuri), dis-
criminative methods outperform MAP by a large 
margin. However, the margin is smaller when the 
two domains are substantially different (such as 
Encarta and Shincho). The phenomenon is attrib-
uted to the underlying difference between the two 
adaptation methods: MAP aims to improve the 
likelihood of a distribution, so if the adaptation 
domain is very similar to the background domain, 
the difference between the two underlying distri-
butions is so small that MAP cannot adjust the 
model effectively. However, discriminative meth-
ods do not have this limitation for they aim to 
reduce errors directly. Finally, we find that in most 
adaptation test sets, MSR achieves slightly better 
CER results than the two competing discriminative 
methods. Specifically, the improvements of MSR 
are statistically significant over the boosting 
method in three out of four domains, and over the 
perceptron algorithm in the Yomiuri domain. The 
results demonstrate again that MSR is robust. 
5 Related Work 
Discriminative models have recently been proved 
to be more effective than generative models in 
some NLP tasks, e.g., parsing (Collins 2000), POS 
tagging (Collins 2002) and LM for speech recogni-
tion (Roark et al 2004). In particular, the linear 
models, though simple and non-probabilistic in 
nature, are preferred to their probabilistic coun-
215
terpart such as logistic regression. One of the rea-
sons, as pointed out by Ng and Jordan (2002), is 
that the parameters of a discriminative model can 
be fit either to maximize the conditional likelihood 
on training data, or to minimize the training errors. 
Since the latter optimizes the objective function that 
the system is graded on, it is viewed as being more 
truly in the spirit of discriminative learning. 
The MSR method shares the same motivation: to 
minimize the errors directly as much as possible. 
Because the error function on a finite data set is a 
step function, and cannot be optimized easily, 
previous research approximates the error function 
by loss functions that are suitable for optimization 
(e.g. Collins 2000; Freund et al 1998; Juang et al 
1997; Duda et al 2001). MSR uses an alternative 
approach. It is a simple heuristic training proce-
dure to minimize training errors directly without 
applying any approximated loss function. 
MSR shares many similarities with previous 
methods. The basic training algorithm described in 
Section 3.2 follows the general framework of multi- 
dimensional optimization (e.g., Press et al 1992). 
The line search is an extension of that described in 
(Och 2003; Quirk et al 2005. The extension lies in 
the way of handling large number of features and 
training samples. Previous algorithms were used to 
optimize linear models with less than 10 features. 
The feature selection method described in Section 
3.4 is a particular implementation of the feature 
selection methods described in (e.g., Theodoridis 
and Koutroumbas 2003). The major difference 
between the MSR and other methods is that it es-
timates the effectiveness of each feature in terms of 
its expected training error reduction while previ-
ous methods used metrics that are loosely coupled 
with reducing training errors. The way of dealing 
with feature correlations in feature selection in 
Equation (7), was suggested by Finette et al (1983). 
6 Conclusion and Future Work 
We show that MSR is a very successful discrimina-
tive training algorithm for LM. Our experiments 
suggest that it leads to significantly better conver-
sion performance on the IME task than either the 
MLE method or the two widely applied discrimi-
native methods, the boosting and perceptron 
methods. However, due to the lack of theoretical 
underpinnings, we are unable to prove that MSR 
will always succeed. This forms one area of our 
future work. 
One of the most interesting properties of MSR is 
that it can optimize any objective function (whether 
its gradient is computable or not), such as error rate 
in IME or speech, BLEU score in MT, precision and 
recall in IR (Gao et al 2005). In particular, MSR can 
be performed on large-scale training set with mil-
lions of candidate features. Thus, another area of 
our future work is to test MSR on wider varieties of 
NLP tasks such as parsing and tagging. 
References 
Collins, Michael. 2002. Discriminative training methods 
for Hidden Markov Models: theory and experiments 
with the perceptron algorithm. In EMNLP 2002. 
Collins, Michael. 2000. Discriminative reranking for 
natural language parsing. In ICML 2000. 
Duda, Richard O, Hart, Peter E. and Stork, David G. 2001. 
Pattern classification. John Wiley & Sons, Inc. 
Finette S., Blerer A., Swindel W. 1983. Breast tissue clas-
sification using diagnostic ultrasound and pattern rec-
ognition techniques: I. Methods of pattern recognition. 
Ultrasonic Imaging, Vol. 5, pp. 55-70. 
Freund, Y, R. Iyer, R. E. Schapire, and Y. Singer. 1998. An 
efficient boosting algorithm for combining preferences. 
In ICML?98.  
Gao, Jianfeng, Hisami Suzuki and Yang Wen. 2002. 
Exploiting headword dependency and predictive clus-
tering for language modeling. In EMNLP 2002. 
Gao, J, H. Qin, X. Xiao and J.-Y. Nie. 2005. Linear dis-
criminative model for information retrieval. In SIGIR.  
Jelinek, Fred. 1997. Statistical methods for speech recognition. 
MIT Press, Cambridge, Mass. 
Juang, B.-H., W.Chou and C.-H. Lee. 1997. Minimum 
classification error rate methods for speech recognition. 
IEEE Tran. Speech and Audio Processing 5-3: 257-265.  
Ng, A. N. and M. I. Jordan. 2002. On discriminative vs. 
generative classifiers: a comparison of logistic regres-
sion and na?ve Bayes. In NIPS 2002: 841-848. 
Och, Franz Josef. 2003. Minimum error rate training in 
statistical machine translation. In ACL 2003 
Press, W. H., S. A. Teukolsky, W. T. Vetterling and B. P. 
Flannery. 1992. Numerical Recipes In C: The Art of Scien-
tific Computing. New York: Cambridge Univ. Press. 
Quirk, Chris, Arul Menezes, and Colin Cherry. 2005. 
Dependency treelet translation: syntactically informed 
phrasal SMT. In ACL 2005: 271-279.  
Roark, Brian, Murat Saraclar and Michael Collins. 2004. 
Corrective language modeling for large vocabulary ASR 
with the perceptron algorithm. In ICASSP 2004. 
Suzuki, Hisami and Jianfeng Gao. 2005. A comparative 
study on language model adaptation using new 
evaluation metrics. In HLT/EMNLP 2005. 
Theodoridis, Sergios and Konstantinos Koutroumbas. 
2003. Pattern Recognition. Elsevier. 
Vapnik, V. N. 1999. The nature of statistical learning theory. 
Springer-Verlag, New York. 
216
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 121?124,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Reducing SMT Rule Table with Monolingual Key Phrase
Zhongjun He? Yao Meng? Yajuan Lj ? Hao Yu? Qun Liu?
? Fujitsu R&D Center CO., LTD, Beijing, China
{hezhongjun, mengyao, yu}@cn.fujitsu.com
? Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China
{lvyajuan, liuqun}@ict.ac.cn
Abstract
This paper presents an effective approach
to discard most entries of the rule table for
statistical machine translation. The rule ta-
ble is filtered by monolingual key phrases,
which are extracted from source text us-
ing a technique based on term extraction.
Experiments show that 78% of the rule ta-
ble is reduced without worsening trans-
lation performance. In most cases, our
approach results in measurable improve-
ments in BLEU score.
1 Introduction
In statistical machine translation (SMT) commu-
nity, the state-of-the-art method is to use rules that
contain hierarchical structures to model transla-
tion, such as the hierarchical phrase-based model
(Chiang, 2005). Rules are more powerful than
conventional phrase pairs because they contain
structural information for capturing long distance
reorderings. However, hierarchical translation
systems often suffer from a large rule table (the
collection of rules), which makes decoding slow
and memory-consuming.
In the training procedure of SMT systems, nu-
merous rules are extracted from the bilingual cor-
pus. During decoding, however, many of them are
rarely used. One of the reasons is that these rules
have low quality. The rule quality are usually eval-
uated by the conditional translation probabilities,
which focus on the correspondence between the
source and target phrases, while ignore the quality
of phrases in a monolingual corpus.
In this paper, we address the problem of reduc-
ing the rule table with the information of mono-
lingual corpus. We use C-value, a measurement
of automatic term recognition, to score source
phrases. A source phrase is regarded as a key
phrase if its score greater than a threshold. Note
that a source phrase is either a flat phrase consists
of words, or a hierarchical phrase consists of both
words and variables. For rule table reduction, the
rule whose source-side is not key phrase is dis-
carded.
Our approach is different from the previous re-
search. Johnson et al (2007) reduced the phrase
table based on the significance testing of phrase
pair co-occurrence in bilingual corpus. The ba-
sic difference is that they used statistical infor-
mation of bilingual corpus while we use that of
monolingual corpus. Shen et al (2008) pro-
posed a string-to-dependency model, which re-
stricted the target-side of a rule by dependency
structures. Their approach greatly reduced the rule
table, however, caused a slight decrease of trans-
lation quality. They obtained improvements by
incorporating an additional dependency language
model. Different from their research, we restrict
rules on the source-side. Furthermore, the system
complexity is not increased because no additional
model is introduced.
The hierarchical phrase-based model (Chiang,
2005) is used to build a translation system. Exper-
iments show that our approach discards 78% of the
rule table without worsening the translation qual-
ity.
2 Monolingual Phrase Scoring
2.1 Frequency
The basic metrics for phrase scoring is the fre-
quency that a phrase appears in a monolingual cor-
pus. The more frequent a source phrase appears in
a corpus, the greater possibility the rule that con-
tains the source phrase may be used.
However, one limitation of this metrics is that if
we filter the rule table by the source phrase with
lower frequency, most long phrase pairs will be
discarded. Because the longer the phrase is, the
less possibility it appears. However, long phrases
121
are very helpful for reducing ambiguity since they
contains more information than short phrases.
Another limitation is that the frequency metrics
focuses on a phrase appearing by itself while ig-
nores it appears as a substring of longer phrases.
It is therefore inadequate for hierarchical phrases.
We use an example for illustration. Considering
the following three rules (the subscripts indicate
word alignments):
R
1
:
An Automatic Evaluation Method for Localization Oriented 
Lexicalised EBMT System 
 
Jianmin Yao+, Ming Zhou++, Tiejun Zhao+, Hao Yu+, Sheng Li+ 
  +School of Computer Science and Technology
Harbin Institute of Technology,  
Harbin, China, 150001 
{james, tjzhao, yu, shengli}@mtlab.hit.edu.cn
++Natural Language Computing Group 
Microsoft Research Asia 
Beijing, China, 100080 
Mingzhou@microsoft.com 
 
Abstract  
To help developing a localization oriented 
EBMT system, an automatic machine 
translation evaluation method is 
implemented which adopts edit distance, 
cosine correlation and Dice coefficient as 
criteria. Experiment shows that the 
evaluation method distinguishes well 
between ?good? translations and ?bad? ones. 
To prove that the method is consistent with 
human evaluation, 6 MT systems are scored 
and compared. Theoretical analysis is made 
to validate the experimental results. 
Correlation coefficient and significance tests 
at 0.01 level are made to ensure the 
reliability of the results. Linear regression 
equations are calculated to map the 
automatic scoring results to human scorings. 
Introduction 
Machine translation evaluation has always been 
a key and open problem. Various evaluation 
methods exist to answer either of the two 
questions (Bohan 2000): (1) How can you tell if 
a machine translation system is ?good?? And (2) 
How can you tell which of two machine 
translation systems is ?better?? Since manual 
evaluation is time consuming and inconsistent, 
automatic methods are broadly studied and 
implemented using different heuristics. Jones 
(2000) utilises linguistic information such as 
balance of parse trees, N-grams, semantic 
co-occurrence and so on as indicators of 
translation quality. Brew C (1994) compares 
human rankings and automatic measures to 
decide the translation quality, whose criteria 
involve word frequency, POS tagging 
distribution and other text features. Another type 
of evaluation method involves comparison of the 
translation result with human translations. 
Yokoyama (2001) proposed a two-way MT 
based evaluation method, which compares 
output Japanese sentences with the original 
Japanese sentence for the word identification, 
the correctness of the modification, the syntactic 
dependency and the parataxis. Yasuda (2001) 
evaluates the translation output by measuring the 
similarity between the translation output and 
translation answer candidates from a parallel 
corpus. Akiba (2001) uses multiple edit 
distances to automatically rank machine 
translation output by translation examples. 
Another path of machine translation evaluation 
is based on test suites. Yu (1993) designs a test 
suite consisting of sentences with various test 
points. Guessoum (2001) proposes a 
semi-automatic evaluation method of the 
grammatical coverage machine translation 
systems via a database of unfolded grammatical 
structures. Koh (2001) describes their test suite 
constructed on the basis of fine-grained 
classification of linguistic phenomena. 
There are many other valuable reports on 
automatic evaluation. All the evaluation 
methods show the wisdom of authors in their 
utilisation of available tools and resources for 
automatic evaluation tasks. For our 
localization-oriented lexicalised EBMT system 
an automatic evaluation module is implemented. 
Some string similarity criteria are taken as 
heuristics. Experimental results show that this 
method is useful in quality feedback in 
development of the EBMT system. Six machine 
translation systems are utilised to test the 
consistency between the automatic method and 
human evaluation. To avoid stochastic errors, 
significance test and linear correlation are 
calculated. Compared with previous works, ours 
is special in the following ways: 1) It is 
developed for localisation-oriented EBMT, 
which demands higher translation quality. 2) 
Statistical measures are introduced to verify the 
significance of the experiments. Linear 
regression provides a bridge over human and 
automatic scoring for systems. 
The paper is organised as follows: First the 
localization-oriented lexicalised EBMT system 
is introduced as the background of evaluation 
task. Second the automatic evaluation method is 
further described. Both theoretical and 
implementation of the evaluation method are 
fully discussed. Then six systems are evaluated 
both manually and with our automatic method. 
Consistency between the two methods is 
analysed. At last before the conclusion, linear 
correlation and significance test validate the 
result and exclude the possibility of random 
consistency. 
1 EBMT Evaluation Solution 
1.1 EBMT System Setup 
From Figure 1 you can get a general overview of 
our EBMT system. 
 
Input sentence 
Transfer 
<Phrase Alignment>
Translation result 
Resources 
(Bilingual and 
monolingual) 
Example Base 
(Software 
manual) Match 
Recombine 
Figure 1. Flowchart of the EBMT System 
The EBMT system is developed for 
localization purpose, which demands the 
translation to be restricted in style and 
expression. This makes it rational to take string 
similarity as criterion for translation quality 
evaluation. The solution is useful because in 
localization, an example based machine 
translation system helps only if it outputs the 
very high quality translation results. 
1.2 Evaluation Criteria 
The criteria we utilise for evaluation include edit 
distance, dice coefficient and cosine correlation 
between (the vectors or word bag sets of) the 
machine translation and the gold standard 
translation. Followed is a detailed description of 
the three criteria. 
The edit distance between two strings s1 
and s2, is defined as the minimum number of 
operations to become the same 
(Levenshtein1965). It gives an indication of how 
`close' or ?similar? two strings are. Denote the 
length of a sentence s as |s|. A two-dimensional 
matrix, m[0...|s1|,0...|s2|] is used to hold the edit 
distance values. The algorithm is as follows 
(Wagner 1974): 
Step 1 Initialization: 
For i=0 to |s1| 
m[i, 0] = i//initializing the columns  
For j=1 to |s2| 
m[0, j] = j //initializing the rows 
Step 2 Iteration: 
For i=1 to |s1| 
For j=1 to |s2| 
 if(s1[i] = s2[j]) 
 { 
    d=m[i-1,j-1] 
 }//equality 
 else 
 { 
d=m[i-1,j-1]+1 
 }//substring 
 m[i, j]=min(m[i-1,j]+1,m[i,j-1]+1,d) 
End For 
End For 
Step 3: Result: 
Return m[i,j] 
Figure 2. Algorithm for Edit Distance 
The time complexity of this algorithm is 
O(|s1|*|s2|). If s1 and s2 have a `similar' length, 
about `n' say, this complexity is O(n2). 
Taking into account the lengths of 
translations, the edit distance is normalised as 
21
)2,1(d2
tDistancenormal_edi  
ss
ss
+
?=
     (1) 
Cosine correlation between the vectors of 
two sentences is often used to compute the 
similarity in information retrieval between a 
document and a query (Manning 1999). In our 
task, it is a similarity criterion defined as 
follows: 
?= ?=
?
?=
?
= n
1i
n
1i
2w2i2w1i
n
1i
2i)(w1i
s2)cos(s1,
w
    (2) 
Where 
w1i = weight of ith term in vector of sentence 
s1, 
w2i = weight of ith term in vector for sentence 
s2, 
n = number of words in sum vector of s1 and s2. 
The cosine correlation reaches maximum value 
of 1 when the two strings s1 and s2 are the same, 
while if none of the elements co-occurs in both 
vectors, the cosine value will reach its minimum 
of 0. 
Another criterion we utilised is the Dice 
coefficient of element sets of strings s1 and s2, 
21
21
2)2,1(
ss
ss
ssDice +?=
I
   (3) 
The Dice coefficient demonstrates the 
intuitive that good translation tends to have 
more common words with standard than bad 
ones. This is especially true for example based 
machine translation for localization purpose. 
1.3 Relationship Among Similarity Criteria 
In this section we analyse the relationship 
between the criteria so that we have a better 
understanding of the experiment results. 
If weight of all words are 1, i.e. each word has 
the uniform importance to translation quality, 
the cosine value becomes very similar to the 
Dice coefficient criterion. if we assume 
??
?=
                                                    else        0 
rsboth vectoin  occurs ith word  theiff     1 
bi
 
??
?=
                                                    else        0 
s1 ofin vector  occurs ith word  theiff     1 
1ib
 
??
?=
                                                    else        0 
s2 ofin vector  occurs ith word  theiff     1 
2ib
 
then 
?= ?=
?
?=
?
= n
1i
n
1i
2w2i2w1i
n
1i
2i)(w1i
s2)cos(s1,
w
 
?= ?=
?
?== n
i
n
i
ibib
n
i
bi
1 1
2221
1
21
21
1 1
21
1
ss
ss
n
i
n
i
ibib
n
i
bi
?=?= ?=
?
?== I
  
Similar to (3), this is also a calculation of the 
number of words in common The Dice 
coefficient and cosine function have common 
characteristics. Especially when two strings are 
of the same length, we have 
)2,1(
21
21
2
1
21
11
21
21
21
1 1
21
1)2,1cos(
                 ssDice
ss
ss
s
ss
ss
ss
ss
ss
n
i
n
i
ibib
n
i
bi
ss
=+?==
?
=
?
=
?= ?=
?
?==
II
II
 
The above equation holds if and only if |s1| 
== |s2|. The experimental results will clearly 
demonstrate the correspondence between cosine 
correlation and Dice coefficient. The two values 
become more similar as the lengths of the two 
strings draw nearer. They become the same 
when the two sentences are of the same length. 
The (normalized) edit distance evaluation 
has a somewhat different variance from the other 
two values. Edit distance cares not only how 
many words there are in common, but also takes 
into account the factor of word order adjustment. 
For example, take two strings of s1 and s2 
composed of words, 
s1 = w1 w2 w3 w4 
s2 = w1 w3 w2 w4 
Then, 
1
44
4221
21
2)2,1( =+?=+?= ss
ss
ssDice
I
 
1
44
4
1 1
2221
1)2,1cos( =
?
=
?
=
?
=
?
?
==
n
i
n
i
ibib
n
i
bi
ss
 
5.0
44
22
21
)2,1(d2 tDistancenormal_edi
2s2)ce(s1,editDistan
=+
?=+
?=
=
ss
ss
 
Edit distance and the other two criteria have 
their respective good aspects and shortcomings. 
So they can complement each other in the 
evaluation work.  
In the EBMT development, we sort the 
translations by a combination of the three factors, 
i.e. first by Dice coefficient in descending order, 
then by cosine correlation in descending order, 
last by normalized edit distance in ascending 
order. This method makes a simple combination 
of the three factors, while no more complexity 
arises from this combination. 
2 Experiments and Results 
2.1 Experimental Setup 
Our evaluation method is designed to help in 
developing the EBMT system. It is supposed to 
sort the translations by quality. Experiments 
show that it works well sorting the sentences by 
order of it?s being good or bad translations. In 
order to justify the effectiveness of the 
evaluation method, we also design experiments 
to compare the automatic evaluation with human 
evaluation. The result shows good compatibility 
between the automatic and human evaluation 
results. Followed are details of the experimental 
setup and results. 
In order to evaluate the performance of our 
EBMT system, a sample from a bilingual corpus 
of Microsoft Software Manual is taken as the 
standard test set. Denote the source sentences in 
the test set as set S, and the target T. Sentences 
in S are fed into the EBMT system. We denote 
the output translation set as R. Every sentence ti 
in T is compared with the corresponding 
sentence ri in R. Evaluation results are got via 
the functions cosine(ti, ri), Dice(ti, ri), and 
normalized edit distance normal_editDistance(ti, 
ri). As discussed in the previous section, good 
translations tend to have higher values of cosine 
correlation, Dice coefficient and lower edit 
distance. After sorting the translations by these 
values, we will see clearly which sentences are 
translated with high quality and which are not. 
Knowledge engineers can obtain much help 
finding the weakness of the EBMT system. 
Some sample sentences and evaluation 
results are attached in the Appendix. In our 
experience, with Dice as example, the 
translations scored above 0.7 are fairly good 
translations with only some minor faults; those 
between 0.5 and 0.7 are faulty ones with some 
good points; while those scored under 0.4 are 
usually very bad translations. From these 
examples, we can see that the three criteria 
really help sorting the good translation from 
those bad ones. This greatly aids the developers 
to find out the key faults in sentence types and 
grammar points. 
2.2 Comparison with Human Evaluation 
In the above descriptions, we have presented our 
theoretical analysis and experimental results of 
our string similarity based evaluation method. 
The evaluation has gained the following 
achievements: 1) It helps distinguishing ?good? 
translations from ?bad? ones in developing the 
EBMT system; 2) The scores give us a clear 
view of the quality of the translations in 
localization based EBMT. In this section we will 
make a direct comparison between human 
evaluation and our automatic machine 
evaluation to test the effectiveness of the string 
similarity evaluation method. To tackle this 
problem, we carry out another experiment, in 
which human scoring of systems are compared 
with the machine scoring. 
The human scoring is carried out with a test 
suite of High School English. Six undergraduate 
students are asked to score the translations 
independent from each other. The average of 
their scoring is taken as human scoring result. 
The method is similar to ALPAC scoring system. 
We score the translations with a 6-point scale 
system. The best translations are scored 1. If it?s 
not so perfect, with small errors, the translation 
gets a score of 0.8. If a fatal error occurs in the 
translation but it?s still understandable, a point 
of 0.6 is scored. The worst translation gets 0 
Table 1. Human Evaluation of 6 Machine Translation Systems 
System# #1 #2 #3 #4 #5 #6 
Error5 5 5% 1 1% 2 2% 4 4% 9 9% 7 7% 
Error4 4 4% 6 6% 4 4% 7 7% 18 18% 21 21%
Error3 7 7% 14 14% 21 21% 23 23% 23 23% 26 26%
Error2 14 14% 15 14% 21 21% 19 19% 18 18% 17 17%
Error1 15 14% 17 17% 33 32% 16 16% 15 15% 8 8% 
Perfect 57 56% 49 48% 21 21% 33 32% 19 19% 23 23%
Good% 70% 65% 43% 48% 34% 31% 
Score 81 78 69 68 55 54 
point of score. Table 1 shows the manual 
evaluation results for 6 general-purpose machine 
translation systems available to us. In table 1, 
Error5 means the worst translation. Error4 to 
Error1 are better when the numbering becomes 
smaller. A translation is labelled ?Perfect? when 
it?s a translation without any fault in it. 
?Good%? is the sum of percent of ?Error1? and 
?Perfect?. Because ?Error1? translations refer to 
those have small imperfections. ?Score? is the 
weighted sum of scores of the 6 kinds of 
translations. E.g. for machine translation system 
MTS1, the score is calculated as follows: 
811578.0156.014                         
4.072.0405)1(
=?+?+?
+?+?+?=MTSscore
 
In table 2, the human scorings and automatic 
scorings of the 6 machine translation systems are 
listed. The translations of system #1 are taken as 
standard for automatic evaluations, i.e. all 
scorings are made on the basis of the result of 
system #1. In principle this will introduce some 
errors, but we suppose it not so great as to 
invalidate the automatic evaluation result. This 
is also why the scorings of system #1 are 100. 
The last row labele AutoAver is the average of 
automatic evaluations. 
Table 2. Scoring of 6 MT Systems 
System# #1 #2 #3 #4 #5 #6 
Human 100 78 69 68 55 54 
Dice 100 70 57 65 48 56 
Cosine 100 75 64 72 55 63 
Edistance 100 78 69 75 63 68 
AutoAver 100 74 63 71 55 62 
Figure 3 presents the scorings of Dice 
coefficient, cosine correlation, edit distance and 
the average of the three automatic criterions in a 
chart, we can clearly see the consistency among 
these parameters. 
4 0
5 0
6 0
7 0
8 0
9 0
1 0 0
1 2 3 4 5 6
D i c e C o s i n e
E d i t D A u t o A v e r
 
Figure 3. Automatic Scoring of 6 MT Systems 
In Figure 3, the numbers on X-axis are the 
numbering of machine translation systems, 
while the Y-axis denotes the evaluation scores. 
40
50
60
70
80
90
100
1 2 3 4 5 6
Human Automatic
 
Figure 4. Scoring of 6 MT Systems 
The human and automatic average scoring 
is shown in Figure 4. The Automatic data refers 
to the average of Dice, cosine correlation and 
edit distance scorings. On the whole, human and 
automatic evaluations tend to present similar 
scores for a specific system, e.g. 78/74 for 
system #2, while 69/63 for system #3. 
3 Result Analysis 
The experimental results and the charts have 
shown some intuitionistic relationship among 
the automatic criteria of Dice coefficient, cosine 
value, edit distance and the human evaluation 
result. A more solid analysis is made in this 
section to verify this relationship. Statistical 
analysis is a useful tool to 1) find the 
relationship between data sets and 2) decide 
whether the relationship is significant enough or 
just for random errors.  
The measure of linear correlation is a way 
of assessing the degree to which a linear 
relationship between two variables is implied by 
observed data. The correlation coefficient 
between variable X and Y is defined as 
YXss
YXCOVYXr ),(),( =
   (7) 
where 
COV(X,Y) is the covariance defined by 
? ???= ))((11),( YYXXnYXCOV ii  (8) 
The symbol meanings are as follows: 
sX: sample standard deviation of variable X 
sY: sample standard deviation of variable Y 
n: sample size 
Xi (Yi) : the ith component of variable X (Y) 
X (Y ): the sample mean of variable X (Y) 
From its definition, we know that the correlation 
coefficient is scale-independent and 11 ??? r . 
After we get the correlation coefficient r, a 
significance test at the level 01.0=?  is made 
to verify whether the correlation is real or just 
due to random errors. Linear regression is used 
to construct a model that specifies the linear 
relationship between the variables X and Y. A 
scatter diagram and regression line will be 
presented for an intuitionistic view of the 
relationship. The results are presented in the 
graphs below. In the graphs, the human 
evaluation results are placed on the X axis, while 
the automatic results are on the Y axis. 
Correlation coefficient and the linear regression 
equation are shown below the graphs. Taking 
into the sample size and the correlation 
coefficient, the significance level is also 
calculated for the statistical analysis. 
 
Figure 5. Human (X) and AutoAver (Y) 
Y=8.0+0.89X, P < 0.01 
r = 0.96, P < 0.01 
 
Figure 6. Human (X) and Dice (Y) 
Y=6.9+1.03X, P < 0.01 
r = 0.96, P < 0.01 
 
Figure 7. Human (X) and Cosine (Y) 
Y=9.3+0.88X, P < 0.01 
r = 0.96, P < 0.01 
 
Figure 8. Human (X) and Edistance (Y) 
Y=23.3+0.74X, P < 0.01 
r = 0.95, P < 0.01 
It is a property of r that it has a value 
domain of [-1,+1]. A positive r implies that the 
X and Y tend to increase/decrease together. A 
minus r implies a tendency for Y to decrease as 
X increases and vice versa. When there is no 
particular relation between X and Y, r tends to 
have a value close to zero. From the above 
analysis, we can see that the Dice coefficient, 
cosine, and average of the automatic values are 
highly correlated with the human evaluation 
results with r=0.96. P < 0.01 shows the two 
variables are strongly correlated with a 
significance level beyond the 99%. While P < 
0.01 for the linear regression equation has the 
same meaning. 
Conclusion 
Our evaluation method is designed for the 
localization oriented EBMT system. This is why 
we take string similarity criteria as basis of the 
evaluation. In our approach, we take edit 
distance, dice coefficient and cosine correlation 
between the machine translation results and the 
standard translation as evaluation criteria. A 
theoretical analysis is first made so that we can 
know clearly the goodness and shortcomings of 
the three factors. The evaluation has been used 
in our development to distinguish bad 
translations from good ones. Significance test at 
0.01 level is made to ensure the reliability of the 
results. Linear regression and correlation 
coefficient are calculated to map the automatic 
scoring results to human scorings. 
Acknowledgements 
This work was done while the author visited 
Microsoft Research Asia. Our thanks go to Wei 
Wang, Jinxia Huang, and Professor Changning 
Huang at Microsoft Research Asia and Jing 
Zhang, Wujiu Huang at Harbin Institute of 
Technology. Their help has contributed much to 
this paper. 
References  
A. Guessoum, R. Zantout, Semi-Automatic 
Evaluation of the Grammatical Coverage of 
Machine Translation Systems, MT Summit? 
conference, Santiago de Compostela, 2001 
Brew C, Thompson H.S, Automatic Evaluation of 
Computer Generated Text: A Progress Report on 
the TextEval Project, Proceedings of the Human 
Language Technology Workshop, 108-113, 1994. 
Christopher D. Manning, Hinrich Schutze, 
Foundations of Statistical Natural Language 
Processing, the MIT Press, 1999, 530-572 
Douglas A. Jones, Gregory M. Rusk, 2000, Toward a 
Scoring Function for Quality-Driven Machine 
Translation, Proceedings of COLING-2000. 
Keiji Yasuda, Fumiaki Sugaya, etc, An Automatic 
Evaluation Method of Translation Quality Using 
Translation Answer Candidates Queried from a 
Parallel Corpus, MT Summit? conference, Santiago 
de Compostela, 2001 
Language and Machines. Computers in Translation 
and Linguistics, (ALPAC report, 1966). National 
Academy of Sciences, 1966 
Niamh Bohan, Elisabeth Breidt, Martin Volk, 2000, 
Evaluating Translation Quality as Input to Product 
Development, 2nd International Conference on 
Language Resources and Evaluation, Athens, 2000. 
Shoichi Yokoyama, Hideki Kashioka, etc., An 
Automatic Evaluation Method for Machine 
Translation using Two-way MT, 8th MT Summit 
conference, Santiago de Compostela, 2001 
Sungryong Koh, Jinee Maeng, etc, A Test Suite for 
Evaluation of English-to-Korean Machine 
Translation Systems, MT Summit? conference, 
Santiago de Compostela, 2001 
Shiwen Yu, Automatic Evaluation of Quality for 
Machine Translation Systems, Machine Translation, 
8: 117-126, 1993, Kluwer Academic Publishers, 
printed in the Netherlands. 
Wagner A.R.  and Fischer M., The string-to-stirng 
correction problem, Journal of the ACM, Vol. 21, 
No. 1, 168-173 
V.I. Levenshtein, Binary codes capable of correcting 
deletions, insertions and reversals. Doklady 
Akademii Nauk SSSR 163(4) 845-848, 1965 
Yasuhiro Akiba, Kenji Imamura, and Eiichiro Sumita, 
Using Multiple Edit Distances to Automatically 
Rank Machine Translation Output, MT Summit? 
conference, Santiago de Compostela, 2001 
Appendix: Automatic Evaluation Results 
cosine      Dice  edistance* `  standard translation&EBMT translation 
0.27273     0.27273      44/6=7     ???????MAPI?? 
                   ????extendedmapi? 
0.43301     0.42857     28/6=4     ???????? 
                             ??mail?? 
0.53452     0.53333 30/7=4      ???????? 
                               ??role??? 
0.62994     0.625     32/4=8      ????????? 
                               ??????? 
0.7     0.7      80/16=5      ???????????????????? 
                               ???????????????????? 
0.72058     0.72      50/11=4      ???????????? 
                               ????????????? 
0.78335     0.78261 46/3=15      ???????????? 
                         ??????????? 
0.81786     0.81633 98/20=4      ?????????????????????????? 
                             ??????????????????????? 
0.8528     0.84211 76/12=6      ?????????????????????? 
                               ???????????????? 
0.86772     0.86486 37/2=18      ?????????? 
                               ????????: 
0.875      0.875  32/1=32   ???????? 
                               ???????? 
0.90889     0.90476 42/2=21      ??????????... 
                               ????????... 
 
*Notes: The data presented in ?edistance? is the reciprocal of the normalized edit distance: the numerator is |s1 + s2| in bytes ; the 
denominator is the edit distance in Chinese characters or English words. 
Subcategorization Acquisition and Evaluation for Chinese Verbs 
Xiwu Han, Tiejun Zhao, Haoliang Qi, Hao Yu 
Department of Computer Science,  
Harbin Institute of Technology, 150001 Harbin, China 
{hxw, tjzhao, qhl, yh}@mtlab.hit.edu.cn 
 
Abstract 
This paper describes the technology and an ex-
periment of subcategorization acquisition for 
Chinese verbs. The SCF hypotheses are gener-
ated by means of linguistic heuristic information 
and filtered via statistical methods. Evaluation 
on the acquisition of 20 multi-pattern verbs 
shows that our experiment achieved the similar 
precision and recall with former researches. Be-
sides, simple application of the acquired lexicon 
to a PCFG parser indicates great potentialities of 
subcategorization information in the fields of 
NLP. 
Credits 
This research is sponsored by National Natural 
Science Foundation (Grant No. 60373101 and 
603750 19), and High-Tech Research and Devel-
opment Program (Grant No. 2002AA117010-09). 
Introduction 
Since (Brent 1991) there have been a consider-
able amount of researches focusing on verb lexi-
cons with respective subcategorization informa-
tion specified both in the field of traditional lin-
guistics and that of computational linguistics. As 
for the former, subcategory theories illustrating 
the syntactic behaviors of verbal predicates are 
now much more systemically improved, e.g. 
(Korhonen 2001). And for auto-acquisition and 
relevant application, researchers have made great 
achievements not only in English, e.g. (Briscoe 
and Carroll 1997), (Korhonen 2003), but also in 
many other languages, such as Germany (Schulte 
im Walde 2002), Czech (Sarkar and Zeman 
2000), and Portuguese (Gamallo et. al 2002). 
However, relevant theoretical researches on 
Chinese verbs are generally limited to case gram-
mar, valency, some semantic computation theo-
ries, and a few papers on manual acquisition or 
prescriptive designment of syntactic patterns. 
Due to irrelevant initial motivations, syntactic 
and semantic generalizabilities of the consequent 
outputs are not in such a harmony that satisfies 
the description granularity for SCF (Han and 
Zhao 2004). The only auto-acquisition work for 
Chinese SCF made by (Han and Zhao 2004) de-
scribes the predefinition of 152 general frames 
for all verbs in Chinese, but that experiment is 
not based on real corpus. After observing and 
analyzing quantity of subcategory phenomena in 
real Chinese corpus in the People?s Daily 
(Jan.~June, 1998), we removed from Han & 
Zhao?s predefinition 15 SCFs that are actually 
similar derivants of others, and then with this 
foundation and linguistic rules from (Zhao 2002) 
as heuristic information we generated SCF hy-
potheses from the corpus of People?s Daily 
(Jan.~June, 1998), and statistically filtered the 
hypotheses into a Chinese verb SCF lexicon. As 
far as we know, this is the first attempt of Chi-
nese SCF auto-acquisition based on real corpus. 
In the rest of this paper, the second section de-
scribes a comprehensive system that builds verb 
SCF lexicons from large real corpus, the respec-
tive operating principles, and the knowledge 
coded in our SCF. The third section analyzed the 
acquired lexicon with two experiments: one 
evaluated the acquisition results of 20 verbs with 
multi syntactic patterns against manual gold 
standard; the other checked the performance of 
the lexicon when applied in a PCFG parser. The 
forth section compares and contrasts this research 
with related works done by others. And at last, 
Section 5 concludes our present achievements, 
disadvantages and possible future focuses. 
 
1   SCF Acquisition 
1.1 The Acquisition Method 
There are generally 4 steps in the process of our 
auto-acquisition experiment. First, the corpus is 
processed with a cascaded HMM parser; second, 
every possible local patterns for verbs are ab-
stracted; and then, the verb patterns are classified 
into SCF hypotheses according to the predefined 
set; at last, hypotheses are filtered statistically 
and the respective frequencies are also recorded. 
The actual application program consists of 6 
parts as shown in the following paragraphs. 
a. Segmenting and tagging: The raw cor-
pus is segmented into words and tagged 
with POS by the comprehensive seg-
menting and tagging processor devel-
oped by MTLAB of Computer 
Department in Harbin Institute of Tech-
nology. The advantage of the POS defi-
nition is that it describes some subsets of 
nouns and verbs in Chinese. 
b. Parsing: The tagged sentences are parsed 
with a cascaded HMM parser1, devel-
oped by MTLAB of HIT, but only the 
intermediate parsing results are used. 
The training set of the parser is 20,000 
sentences in the Chinese Tree Bank2 of 
(Zhao 2002). 
c. Error-driven correction: Some key errors 
occurring in the former two parts are 
corrected according to manually ob-
tained error-driven rules, which are gen-
erally about words or POS in the corpus. 
d. Pattern abstraction: Verbs with largest 
governing ranges are regarded as predi-
cates, then local patterns, previous 
phrases and respective syntactic tags are 
abstracted, and isolated parts are com-
bined, generalized or omitted according 
to basic phrase rules in (Zhao 2002). 
e. Hypothesis generation: Based on lin-
guistic restraining rules, e.g. no more 
than two NP?s occurring in a series and 
no more than three in one pattern, and 
no PP TP MP occurring with NP before 
any predicates (Han and Zhao 2004), the 
patterns are coordinated and classified 
into the predefined SCF groups. In this 
part, about 5% unclassifiable patterns 
are removed. 
                                                           
1 When evaluated on auto-tagged open corpus, the parser?s 
phrase precision if 62.3%, and phrase recall is 60.9% (Meng, 
2003). 
2 A sample of the tree bank or relevant introduction could be 
found at http://mtlab.hit.edu.cn. 
f. Hypothesis filtering: According to the 
statistical reliability of each type of the 
SCF hypotheses and the linguistic prin-
ciple that arguments occur more fre-
quently with predicates than adjuncts do, 
the hypotheses are filtered by means of 
statistical methods, in this paper which 
are binomial hypotheses testing (BHT) 
and maximum likelihood estimation 
(MLE). 
 
Table 1: An Example of Auto-acquisition 
 
No Actions Results 
a) Input ????????????
???????? 
b) Tag and 
parse 
BNP[BMP[?/m ?/q ]?/ng ]
? /p NDE[ ? ? ? /r ?
/usde ]BVP[ ? ? /vg ?
/vq ]BVP[ ? ? /vg ?
/ut ]NP[??/nc ?/usde ??
/ng ]?/wj 
c) Correct 
errors 
BNP[BMP[?/m ?/q ]?/ng ]
?/p NDE[???/r ?/usde ?
?/vg ?/vq ]BVP[??/vg ?
/LE ]NP[??/nc ?/usde ??
/ng ]?/wj 
d) Abstract 
patterns 
BNP PP BVP[vg LE ] NP 
e) Generate 
hypothesis
NP v NP ?01000? 
f) Filter hy-
potheses 
NP v NP {01111}3 
 
In Table 1, for example, when acquiring SCF 
information for ???? (prove) and a related sen-
tence in the corpus is a), our tagger and parser 
will return b), and error-driven correction will 
return c) with errors of NDE and the 1st BVP cor-
rected4. Since the governing range of ???? is 
larger than that of ???? (ask), the other verb in 
this sentence, the program abstracts its local pat-
tern BVP[vg LE] and previous phrase BNP, gen-
                                                           
3  {01000} projects to the Chinese syntactic mor-
phemes {?????????}, 1 means the SCF 
may occur with the respective morpheme, while 0 
may not (Han & Zhao, 2004). 
4 Note that not all errors in this example have been corrected, 
but this doesn?t affect further procession. Also, for defini-
tions of NDE and BVP see (Zhao, 2002). 
eralizes BNP and NDE as NP, combines the sec-
ond NP with isolated part ??/p? into PP, and 
returns d). Then the hypothesis generator returns 
e) as the possible SCF in which the verb may 
occurs. Actually in the corpus there are 621 hy-
pothesis tokens generated, and among them 92 
ones are of same arguments with e), and thus e) 
can pass the hypothesis testing (See also Section 
1.2), so we obtain one SCF for ???? as f). 
1.2 Filtering Methods 
In researches of subcategorization acquisition, 
statistical methods for hypothesis filtering mainly 
include the BHT, the Log Likelihood Ratio 
(LLR), the T-test and the MLE, and the most 
popular one is the BHT. Since (Brent 1993) be-
gan to use the method, most researchers have 
agreed that the BHT results in better precision 
and recall with SCF hypotheses of high, medium 
and low frequencies. Only (Korhonen 2001) re-
ports 11.9% total performance of the MLE better 
than the BHT. Therefore, we applied the two sta-
tistical methods in our present experiment. This 
subsection chiefly illustrates the expressions of 
our methods and definitions of parameters in 
them, while performance comparison of the two 
will be introduced in Section 3. 
When applying the BHT method, it is nec-
essary to determine the probability of the primi-
tive event. As for SCF acquisition, the co-
occurrence of one predefined SCF scfi with one 
verb v is the relevant primitive event, and the 
concerned probability is p(v|scfi) here. However, 
the aim of filtering is to rule out those unreliable 
hypotheses, so it is the probability that one primi-
tive event doesn't occur that is often used for 
SCF hypothesis testing, i.e. the error probability: 
pe(v|scfi) = 1 p(v|scfi). (Brent 1993) estimated pe 
according to the acquisition system?s perform-
ance, while (Briscoe and Carroll 1997) calculated 
pe from the distribution of SCF types in ANLT 
and SCF tokens in Susanne as shown in the fol-
lowing equation. 
 
Brent?s method mainly depends on the related 
corpus and processing program, which may 
cause intolerable errors. Briscoe and Carroll?s 
method draws on both linguistic and statistical 
information thus leading to comparatively stable 
estimation, and therefore has been used by many 
latter researches, e.g. (Korhonen 2001). But there 
is no MRD proper for Chinese SCF description 
so we estimated pe from the 1,775 common verbs 
and SCF tokens in the related corpus of 43,000 
sentences used by (Han and Zhao 2004). We 
formed the equation as follows: 
 
Then the number of all hypotheses about verb 
vj is recorded as n, and the number of those for 
scfi as m. According to Bernoulli theory, the 
probability P that an event with probability p ex-
actly happens m times out of n such trials is:  
 
And the probability that the event happens m or 
more times is: 
 
In turn, P(m+, n, pe) is the probability that scfi 
wrongly occurs m or more times with a verb that 
doesn't match it. Therefore, a threshold of 0.05 
on this probability will yield a 95% confidence 
that a high enough proportion of hypotheses for 
scfi have been observed for the verb legitimately 
to be assigned scfi (Korhonen 2001). 
The MLE method is closely related to the general 
performance of the concerned SCF acquisition 
system. First, we randomly draw from the ap-
plied corpus a training set, which is large enough 
so as to ensure similar SCF frequency distribu-
tion. Then, the frequency of scfi occurring with a 
verb vj is recorded and used to estimate the actual 
probability p(scfi| vj). Thirdly, an empirical 
threshold is determined, such that it ensures 
maximum value of F measure on the training set. 
Finally, the threshold is used to filter out those 
SCF hypotheses with low frequencies from the 
total set. 
2    Experimental Evaluation 
2.1   Acquisition Performance 
 
Using the previously described theory and tech-
nology we have acquired an SCF lexicon for 
3,558 common Chinese verbs from the corpus of 
People?s Daily (Jan.~June, 1998). In the lexicon 
the minimum number of SCF tokens for a verb is 
30, and the maximum is 20,000. In order to check 
the acquisition performance of the used system, 
we evaluated a part of the lexicon against a man-
ual gold standard. The testing set includes 20 
verbs of multi syntactic patterns, and for each 
verb there are 503~2,000 SCF tokens with the 
total number of 18,316 (See Table 2). Table 3 
gives the evaluation results for different filtering 
methods, including non-filtering 5 , BHT, and 
MLE with thresholds of 0.001, 0.005, 0.008 and 
0.01. We calculated the type precision and recall 
by the following expressions as (Korhonen 2001) 
did:  
 
In here, true positives are correct SCF types 
proposed by the system, false positives are incor-
rect SCF types proposed by system, and false 
negatives are correct SCF types not proposed by 
the system. 
 
Table 2: Verbs in the Testing Set6 
 
Verbs English Tokens Verbs English Tokens
? Read 503 ?? Hope 620 
?? Find 529 ? See 645 
?? Reckon 543 ?? Invest 679 
? Pull 544 ?? Know 722 
?? Report 612 ? Send 800 
?? Develop 1,006 ?? Set up 1,186
?? Behave 1,007 ?? Insist 1,200
?? Decide 1,038 ? Think 1,200
?? End 1,140 ?? Require 1,200
?? Begin 1142 ? Write 2,000
 
According to Table 3, all other filtering meth-
ods outperform non-filtering, and MLE is better 
than BHT. Among the four MLE thresholds, 
0.008 achieves the best comprehensive perform-
ance but its F-measure is only 0.74 larger than 
that of 0.01 while its precision drops by 2.4 per-
cent. Hence, we chose 0.01 as the threshold for 
the whole experiment with purpose to meet the 
practical requirement of high precision and to 
avoid possible over-fit phenomena. Finally, with 
a confidence of 95% we can estimate the general 
performance of the acquisition system with preci-
sion of  60.6% +/- 2.39%, and recall of 51.3%+/-
2.45%. 
                                                           
5 Non-filtering means filtering with a zero threshold or not 
filtering at all. This method is used as baseline here.  
6 The English meanings given here are not intended to cover 
the whole semantic range of the respective verbs, on the 
contrary they are just for readers? reference. 
 
Table 3: System Performance for Different 
 Filtering Methods 
 
          Measures
Methods Precision Recall F-measure
Non-filtering 37.43% 85.9% 52.14 
BHT 50% 57.2% 53.36 
0.001 39.2% 85.9% 53.83 
0.005 40.3% 83.33% 54.33 
0.008 58.2% 54.5% 56.3 MLE
0.01 60.6% 51.3% 55.56 
 
2.2    Task-oriented Evaluation 
In order to further analyze the practicability of 
the previously described technology, we per-
formed a simple task-oriented evaluation, apply-
ing the acquired SCF lexicon in a PCFG parser 
helping to choose from the n-best parsing results. 
The concerned parser was trained from 10,000 
manually parsed Chinese sentences7. In this ex-
periment there are 664 verbs and their SCF in-
formation involved. The open testing set consists 
of 1,500 sentences, for each of which the PCFG 
parser outputs 5-best parsing results. Then SCF 
hypotheses are generated for each result by 
means of the formerly mentioned technology. 
Finally, the maximum likelihood between hy-
potheses and those SCF types for the related verb 
in the lexicon is calculated in the following way:  
 
where i ? 5, hi is one of the hypotheses generated 
for the parsing results, and scfj is the jth SCF type 
for the concerned verb. This calculation keeps the 
likelihood between 0 and 1. The parsing result 
                                                           
7 These sentences and the testing corpus mentioned latter are 
all taken from the Chinese Tree Bank developed by MTLAB 
of HIT, and a sample may be downloaded at 
http://mtlab.hit.edu.cn. 
with maximum likelyhood is then regarded as the 
final choice. When two or more hypotheses hold 
the same likelihood, the one with larger or largest 
PCFG probability will be chosen.  
Table 4 shows the phrase-based and sentence-
based evaluation results for the parser without 
and with SCF heuristic information. There are 
three cased included: a) The output is one-best; b) 
The output is 5-best and the best evaluation result 
is recorded; c) The 5-best output is checked again 
for the best syntactic tree by means of SCF in-
formation. The phrased-based evaluation follows 
the popular method for evaluating a parser, while 
the sentence-based depends on the intersection of 
the parsed trees and those in the gold standard. 
Since the PCFG parser output at least one syntac-
tic tree for every sentence in our testing corpus, 
the sentence-based precision and recall are equal 
to each other. 
 
Table 4: Parsing Evaluation 
 
Phrase-based Sentence-
based 
Parsing  
Methods 
Precision Recall Precision  
= Recall 
One-best 57.5% 55% 13.64% 
5-best 65.28% 64.59% 26.2% 
With SCF 62.86% 62.1% 21.66% 
 
Table 4 shows that SCF information remarka-
bly improved the performance of the PCFG 
parser: the phrase-based precision increased by 
5.36% and recall by 7.1%, while the sentence-
based precision and recall both increased by 
8.04%. However, this doesn?t reach the upper 
limit of the 5-best. The possible reasons are: a) 
the our present SCF lexicon remains to be im-
proved; b) our method of applying SCF informa-
tion to the parser is too simple, e.g. probabilities 
of PCFG parsing results haven?t been exploited 
thoroughly. 
 
3 Related Works 
As far as we know, this is the first attempt to 
automatically acquire SCF information from real 
Chinese corpus and the first trial to apply SCF 
lexicon to a Chinese parser. Our research draws a 
lot on related works from international researches, 
and for the purpose of crosslingual processing, 
our research is kept in consistency with SCF 
conventions as much as possible. 
Due to linguistic differences, nevertheless, not 
all theories, methods or experiences could adapt 
to Chinese. Generally, there are four aspects that 
our research differs from those of other lan-
guages. First, the SCF formalization of most 
former researches follows the Levin style, in 
which most SCFs omit NP before predicates, 
while Chinese SCFs need to depict arguments 
occurring before verbs. Second, except (Sarkar 
and Zeman 2000), most former researches are 
based on manual SCF predefinition, while our 
predefined SCF set is statistically acquired (See 
Han and Zhao 2004). Third, involved parsers of 
former researches are mostly better than Chinese 
parsers to some degree. Forth, our SCF informa-
tion also includes 5 syntactic morphemes (See 
also Section 1.1). 
Meanwhile, the basic purpose for Chinese SCF 
acquisition is also to determine the subcategory 
features for a verb via its argument distributions 
and then apply the lexicon to NLP tasks. There-
fore, under similar cases the respective evalua-
tions are comparable. And Table 5 gives the 
comparison between our research and the best 
English results without semantic backoff 8  in 
(Korhonen 2001). 
 
Table 5: Performance Comparison Between 
Chinese and English Researches 
 
                Filtering 
Measures    Non BHT MLE
Ours 37.43% 50% 58.2%Precision Korhonen 24.3% 50.3% 74.8%
Ours 85.9% 57.2% 54.5%Recall Korhonen 83.5% 56.6% 57.8%
Ours 52.14 53.36 56.3F-
measure Korhonen 37.6 53.3 65.2
 
The comparison shows that our nonfiltering re-
sult is better than Korhonen?s, both BHT results 
are similar, while our MLE result is much worse 
                                                           
8 Semantic backoff is a method of generating SCF hypothe-
ses according to the semantic classification of the concerned 
verb. Note that this paper doesn?t involve verb meanings for 
generating hypotheses. Besides, though the evaluation for 
English SCF acquisition is the best, it?s not the newest. For 
the newest, please refer to (Korhonen 2003), in which the 
precision is 71.8% and recall is 34.5%. 
than Korhonen?s. That means our hypothesis 
generator performs well but our filtering method 
remains to be improved. According to the analy-
sis of relevant corpus, we found the main cause 
might be that low frequency SCF types account 
for 32% in our corpus while those in (Korhonen 
2001) sum to nearly 21%. 
Further more, (Briscoe and Carroll 1997) ap-
plied their acquired English SCF lexicon to an 
intermediate parser, and reported a 7% improve-
ment of both phrase-based precision and recall. 
Our application of SCF lexicon to a PCFG parser 
leads to 5.36% improvement for phrase-based 
precision, 7.1% for recall, and 8.04% for sen-
tence-based precision and recall. 
 
4    Conclusion 
 
This paper for the first time describes a largescale 
experiment of automatically acquiring SCF lexi-
con from real Chinese corpus. Perfor mance eva-
luation shows that our technology and acquiring 
program have achieved similar performance 
compared with former researches of other lan-
guages. And the application of the acquired lexi-
con to a PCFG parser indicates great potentiali-
ties of SCF information in the field of NLP. 
However, there is still a large gap between 
Chinese subcategorization works and those of o-
ther languages. Our future work will focus on the 
optimization of linguistic heuristic information 
and filtering methods, the application of semantic 
backoff, and the exploitation of SCF lexicon for 
other NLP tasks. 
References  
Brent, M. R. 1991. Automatic acquisition of subcate-
gorization frames from untagged text. In Proceed-
ings of the 29th Annual Meeting of the Association 
for Computational Linguistics, Berkeley, CA. 209-
214. 
Brent, M. 1993. From Grammar to Lexicon: un-
supervised learning of lexical syntax. Compu-
tational Linguistics 19.3. 243-262. 
Briscoe, Ted and John Carroll, 1997. Automatic ex-
traction of subcategorization from corpora. In Pro-
ceedings of the 5th ACL Conference on Applied 
Natural Language Processing, Washington, DC. 
Dorr, B. J. Gina-Anne Levow, Dekang Lin, and Scott 
Thomas, 2000. Chinese-English Semantic Resource 
Construction, 2nd International Conference on 
Language Resources and Evaluation (LREC2000), 
Athens, Greece, pp. 757--760. 
Gamallo, P., Agustini, A. and Lopes Gabriel P., 2002. 
Using Co-Composition for Acquiring Syntactic 
and Semantic Subcategorisation, ACL-02.  
Han, Xiwu, Tiejun Zhao, 2004. FML-Based SCF Pre-
definition Learning for Chinese Verbs. Interna-
tional Joint Conference of NLP 2004. 
Jin, Guangjin, 2001. Semantic Computations for Mod-
ern Chinese Verbs. Beijing University Press, Bei-
jing. (in Chinese) 
Korhonen, Anna, 2001. Subcategorization Acquistion, 
Dissertation for Ph.D, Trinity Hall University of 
Cambridge. 29-77. 
Korhonen, Anna, 2003. Clustering Polysemic Sub-
categorization Frame Distributions Semantically. 
Proceedings of the 41st Annual Meeting of the As-
sociation for Computational Linguistics, pp. 64-71. 
Meng, Yao, 2003. Research on Global Chinese Pars-
ing Model and Algorithm Based on Maximum En-
tropy. Dissertation for Ph.D. Computer Department, 
HIT. 33-34. 
Sabine Shulte im Walde, 2002. Inducing German Se-
mantic Verb Classes from Purely Syntactic Sub-
categorization Information. Proceedings of the 40st 
ACL, pp. 223-230. 
Sarkar, A. and Zeman, D. 2000. Automatic Ex-
traction of Subcategorization Frames for 
Czech. In Proceedings of the 19th Interna-
tional Conference on Computational Linguis-
tics, aarbrucken, Germany. 
Zhan Weidong, 2000. Valence Based Chinese Seman-
tic Dictionary, Language and Character Applica-
tions, Volume 1. (in Chinese) 
Zhao Tiejun, 2002. Knowledge Engineering Report 
for MTS2000.  
 
A Hybrid Chinese Language Model based on a Combination of 
Ontology with Statistical Method 
Dequan Zheng, Tiejun Zhao, Sheng Li and Hao Yu 
MOE-MS Key Laboratory of Natural Language Proceessing and Speech 
Harbin Institute of Technology 
Harbin, China, 150001 
{dqzheng, tjzhao, lisheng, yu}@mtlab.hit.edu.cn 
  
Abstract 
In this paper, we present a hybrid Chi-
nese language model based on a com-
bination of ontology with statistical 
method. In this study, we determined 
the structure of such a Chinese lan-
guage model. This structure is firstly 
comprised of an ontology description 
framework for Chinese words and a 
representation of Chinese lingual on-
tology knowledge. Subsequently, a 
Chinese lingual ontology knowledge 
bank is automatically acquired by de-
termining, for each word, its co-
occurrence with semantic, pragmatics, 
and syntactic information from the 
training corpus and the usage of Chi-
nese words will be gotten from lingual 
ontology knowledge bank for a actual 
document. To evaluate the performance 
of this language model, we completed 
two groups of experiments on texts re-
ordering for Chinese information re-
trieval and texts similarity computing. 
Compared with previous works, the 
proposed method improved the preci-
sion of nature language processing. 
1 Introduction 
Language modeling is a description of natural 
language and a good language model can help to 
improve the performance of the natural language 
processing. 
Traditional statistical language model 
(SLM) is fundamental to many natural language 
applications like automatic speech recognitionP[1]P, 
statistical machine translationP[2]P, and information 
retrievalP[3]P. Different statistical models have 
been proposed in the past, but n-gram models (in 
particular, bi-gram and tri-gram models) still 
dominate SLM research. After that, other ap-
proaches were put forward, such as the 
combination of statistical-based approach and 
rule-based approachP[4,5]P, self-adaptive language 
modelsP[6]P, topic-based model P[7]P and cache-based 
model P[8]P. But when the models are applied, the 
crucial disadvantages are that they can?t repre-
sent and process the semantic information of a 
natural language, so they can?t adapt well to the 
environment with changeful topics. 
Ontology was recognized as a conceptual 
modeling tool, which can descript an informa-
tion system in the semantic level and knowledge 
level. After it was first introduced in the field of 
Artificial IntelligenceP[9]P, it was closed combined 
with natural language processing and are widely 
applied in many field such as knowledge engi-
neering, digital library, information retrieval, 
semantic Web, and etc.  
In this paper, combining with the character-
istic of ontology and statistical method, we pre-
sent a hybrid Chinese language model. In this 
study, we determined the structure of Chinese 
language model and evaluate its performance 
with two groups of experiments on texts reorder-
ing for Chinese information retrieval and texts 
similarity computing. 
The rest of this paper is organized as fol-
lows. In section 2, we describe the Chinese lan-
guage model. In section 3, we evaluate the 
language model by several experiments about 
natural language processing. In section 4, we 
present the conclusion and some future work. 
2 The language model description 
Traditional SLM is make use to estimate the 
likelihood (or probability) of a word string, in 
13
this study, we determined the structure of Chi-
nese language model, first, we gave the ontology 
description framework of Chinese word and the 
representation of Chinese lingual ontology 
knowledge, and then, automatically acquired the 
usage of a word with its co-occurrence of con-
text in using semantic, pragmatics, syntactic, etc 
from the corpus to act as Chinese lingual ontol-
ogy knowledge bank. In actual document, the 
usage of lingual knowledge will be gotten from 
lingual ontology knowledge bank. 
2.1   Ontology description framework 
Traditional ontology mainly emphasizes the 
interrelations between essential concept, domain 
ontology is a public concept set of this do-
main P[10]P. We make use of this to present Chinese 
lingual ontology knowledge bank. 
In practical application, ontology can be 
figured in many waysP[11]P, natural languages, 
frameworks, semantic webs, logical languages, 
etc. Presently, popular models, such as Ontolin-
gua, CycL and Loom, are all based on logical 
language. Though logical language has a strong 
expression, its deduction is very difficult to lin-
gual knowledge. Semantic web and natural lan-
guage are non-formal, which have disadvantages 
in grammar and expression. 
For a Chinese word, we provided a frame-
work structure that can be understood by com-
puter combined with WordNet, HowNet and 
Chinese Thesaurus. This framework includes a 
Chinese word in concept, part of speech (POS), 
semantic, synonyms, English translation. Fig-
ure1 shows the ontology description framework 
of a Chinese word. 
 
 
 
 
 
 
 
Fig. 1. Ontology description framework 
2.2   Lingual ontology knowledge representation 
A word is the basic factor that composes the 
natural language, to acquire lingual ontology 
knowledge, we need to know POS, means and 
semantic of a word in a sentence. For example, 
for a Chinese sentence, the POS, means and 
Semantic label of ??? in HowNet are shown in 
table 1. For the Chinese sentence ??????
??????, after words segmented, POS tag-
ging and semantic tagging, we get a characteris-
tic string. They are shown in table 2. 
Table 1. the usage of ??? in Chinese sentence 
Chinese Sentence POS Means Semantic Num 
??? Verb Weave 525(weave|?? ) 
?? ? Verb Buy 348(buy|? ) 
Table 2. Segmentation, POS and Semantic tagging 
Items Results (???? acts as keyword) 
Chinese sentence ?????????? 
Words segmenta-
tion 
??  ??  ?  ??  ??  ? 
POS tagging ?? nd/ ?? Keyword/? vg/ ?? nd/ ??
vg/ ?wj/ 
Semantic label 
tagging 
?? nd/021243 ?? Keyword/070366?  
vg/017545 ?? nd/021243 ?? vg/092317 ?
wj/-1 
Characteristic string nd/021243 ?? Keyword/070366  vg/017545 
nd/021243 vg/092317 
Explanation of 
Semantic label 
021243 represents ????, 070366 represents 
???, 092317 represents ? ?? ?, ?-1? repre-
sents not to be defined or exist this semantic in 
HowNet. 
 
In order to use and express easily, we gave 
a description for ontology knowledge of every 
Chinese word, which learned from corpus, to be 
shown as expression 1. All of them composed 
the Chinese lingual ontology knowledge bank. 
( ) ( ) ( )???????? == UU
n
r
rrr
m
l
lll CLPOSSemCLPOSSemontologyKeyWord
11
,,,,,,,,
 
Where, KeyWord(ontology) is the ontology 
description of a Chinese word, ( )iii CLPOSSem ,,,  is 
the left co-occurrence knowledge of a Chinese 
word got from its context and ( )iii CLPOSSem ,,,  is 
the right co-occurrence knowledge. Symbol 
?? ? represents the aggregate of all the co-
occurrence with the KeyWord. ( )iii CLPOSSem ,,,  denotes the multi-grams 
from context of a Chinese word, which is com-
posed of semantic information SemBi B, part of 
speech POS Bi B, the position L from the word 
KeyWord to its co-occurrence, the average dis-
tance lC  from the word to its left (or right) i-th 
word. 
( )( )LPOSSemKeyword ii ,,,  denotes a seman-
tic relation pair between the keyword and its co-
occurrence in current context. 
The multi-grams of a Chinese word in con-
text, including the co-occurrence and their posi-
tion will act as the composition of lingual 
ontology knowledge too. In figure 2, the charac-
teristic string WB1 B, WB2 B, ?, WBi B represents POS and 
semantic label, Keyword is keyword itself, l or r 
Keywords  <?>
Concept             <?> 
Part of Speech  <?> 
Ontology   Semantic            <?> 
                    Synonym           <?> 
E-translation     <?> 
14
is the position of word that is left or right co-
occurrence with keyword. 
 
Fig. 2. Co-occurrence and the position information 
2.3   Lingual ontology knowledge acquisition 
According to the course that human being ac-
quires and accumulates knowledge, we propose 
a measurable description for Chinese lingual 
ontology knowledge through automatically 
learning typical corpus. In this approach, we will 
acquire the usage of a Chinese word in semantic, 
pragmatic and syntactic in all documents. We 
combine with the multi-grams in context includ-
ing its co-occurrence, POS, semantic, synonym, 
position. In practical application, we will proc-
ess every Chinese keyword that has the same 
grammar expression, semantic representation 
and syntactic structure with Chinese lingual on-
tology knowledge bank. 
2.3.1   Algorithm of automatic acquisition 
Step 1: corpus pre-processing.  
For any Chinese document DBi B in the docu-
ment set {D}, we treat the sentence that includes 
keyword as a processing unit. First, we have a 
Chinese word segmentation, POS tagging, Se-
mantic label tagging based on HowNet, and then, 
confirm a word to act as the keyword for acquir-
ing its co-occurrence knowledge. We wipe off 
the word that can do little contribution to the 
lingual ontology knowledge, such as preposition, 
conjunction, auxiliary word and etc. 
Step 2: Unify the keyword. 
Making use of the ontology description of 
Chinese word, we make the synonym into uni-
form one. 
Step 3: Calculate the co-occurrence distance. 
In our proposal, first, we treat the sentence 
that includes keyword as a processing unit and 
make POS tagging, semantic label tagging, then, 
we get Characteristic string. We take the key-
word as the center, define the left and right dis-
tance factor B Bl B and B Br B to be shown at formula 1. 
ml
B
??
???
??
?
=
2
11
2
11               
nr
B
??
???
??
?
=
2
11
2
11     (1) 
Where, m and n represent the left and right 
number of word that centered with the keyword. 
In this way, we try to get the language intuition, 
in a word, if the co-occurrence is nearer to the 
keyword, we will get more the co-occurrence 
distant. Final, we respectively get the left-side 
and right-side co-occurrence distant from key-
word to its co-occurrence to be shown as for-
mula 2. 
l
i
li BC
1
2
1 ???
???
?=  (i=1,?,m) 
r
j
rj BC
1
2
1 ???
???
?=  (j=1,?,n)       (2) 
Step4: Calculate the average co-occurrence 
distance. 
For a keyword, in the current sentence of 
document DBi,B we regard the keyword and its co-
occurrence (SemBi B, POS Bi B, L) as semantic relation 
pair, and CBjB is their co-occurrence distance. We 
calculate the average of CBjB that appear in corpus 
and act as the average co-occurrence distance 
lC  
between the keyword and its co-occurrence 
(SemBi B, POS Bi B, L). 
When all of documents are learned, all of 
keyword and their co-occurrence information ( )iii CLPOSSem ,,,  compose the Chinese lingual 
ontology knowledge bank. 
Step 5: Rebuild the index. 
In order to improve the processing speed, 
for acquired lingual ontology knowledge bank, 
we first build an index according to Chinese 
word, and then, we respectively make a sorting 
according to the semantic label SemBi B for every 
Chinese word. 
2.3.2 Lingual ontology knowledge application 
In practical application, we will respectively get 
different evaluation of a document from the lin-
gual ontology knowledge bank. For the natural 
language processing, e.g. documents similarity 
computing, text re-ranking for information re-
trieval, information filtering, the general proc-
essing is as follow. 
Step 1: Pre-processing and unify the key-
word. 
The processing is the same as Step 1 and 
Step 2 in section 2.3.1. 
Step 2: Fetch the average co-occurrence 
distance from lingual ontology knowledge bank. 
We regard a sentence including keyword in 
document D as a processing unit. First, we make 
POS tagging, semantic label tagging and get 
Characteristic string, and then, for every key-
word, if it has the same semantic relation pair as 
lingual ontology knowledge bank, i.e. the key-
word and its co-occurrence (SemBi B, POS Bi B, L) in 
practical document is the same one as lingual 
15
ontology knowledge bank, we add up all the 
average co-occurrence distance 
lC  from Chinese 
lingual ontology knowledge bank acquired in 
section 2.3.1. 
Step 3: Get the evaluation value of a docu-
ment. 
Repeat Step 2 until all keywords be proc-
essed and the accumulation of the average co-
occurrence distance 
lC will act as the evaluation 
value of current document. 
3 Evaluation of language model 
We completed two groups of experiments on 
text re-ranking for information retrieval, text 
similarity computing to verify the performance 
of lingual ontology knowledge. 
3.1   Texts reordering 
Information retrieval is used to retrieve relevant 
documents from a large document set for a user 
query, where the user query can be a simple de-
scription by natural. As a general rule, users 
hope more to acquire relevant information from 
the top ranking documents, so they concern 
more on the precision of top ranking documents 
than the recall. 
We use the Chinese document set CIRB011 
(132,173 documents) and CIRB020 (249,508 
documents) from NTCIR3 CLIR dataset and 
select 36 topics from 50 search topics (see 
http://research.nii.ac.jp/ntcir-ws3/work-en.html 
for more information) to evaluate our method. 
We use the same method to retrieve documents 
mentioned by Yang LingpengP[12]P, i.e. we use 
vector space model to retrieve documents, use 
cosine to calculate the similarity between docu-
ment and user query. We respectively use bi-
grams and words as indexing unitsP[13,14]P, the av-
erage precision of top N ranking documents acts 
as the normal results. In this paper, we used a 
Chinese dictionary that contains about 85,000 
items to segment Chinese document and query. 
To measure the effectiveness of informa-
tion retrieval, we use the same two kinds of 
relevant measures: relax-relevant and rigid-
relevantP[14,15]P. A document is rigid-relevant if it?s 
highly relevant or relevant with user query, and 
a document is relax-relevant if it is high relevant 
or relevant or partially relevant with user query. 
We also use PreAt10 and PreAt100 to represent 
the precision of top 10 ranking documents and 
top 100 ranking documents. 
3.1.1   Strategy of texts reordering 
First, we get some keywords to every topic by 
query description. For example, 
Title: ????? (The birth of a cloned 
calf) 
Description: ????????????
?????????????? (Find Arti-
cles relating to the birth of cloned calves using 
the technique called somatic cell nuclear transfer) 
We extract ???, ???, ??, ???
?? as feature word in this topic. 
Second, acquire lingual ontology knowl-
edge every topic by their feature words. In this 
proposal, we arrange 300 Chinese texts of this 
topic as learning corpus to get lingual ontology 
knowledge bank. 
Third, get the evaluation value of every text 
about this topic, i.e. respectively add up all the 
average co-occurrence distance lC  to the same 
semantic relation pairs in every text from lingual 
ontology knowledge bank.  
If a text has several keywords, repeat step3 
to acquire every evaluation value to these key-
words, and then, add up each evaluation value to 
act as the text evaluation value. 
Final, we reorder the initial retrieval texts 
according to the every text evaluation value of 
every topic. 
3.1.2   Experimental results and analysis 
We calculate the evaluation value of every text 
in each topic to reorder the initial relevant 
documents. 
Table 3 lists the normal results and our re-
sults based on bi-gram indexing, our results are 
acquired based on Chinese lingual ontology 
knowledge to enhance the effectiveness. 
PreAt10 is the average precision of 36 topics in 
precision of top 10 ranking documents, while 
PreAt100 is top 100 ranking documents. 
Table 4 lists the normal results and our re-
sults based on word indexing. Ratio displays an 
increase ratio of our result compared with nor-
mal result. 
Table 3. Precision (bi-gram as indexing unit) 
Items Normal Our method Ratio 
PreAt10 (Relax) 0.3704 0.4389 18.49% 
PreAt100 (Relax) 0.1941 0.2239 15.35% 
PreA10 (Rigid) 0.2625 0.3083 17.45% 
PreAt100 (Rigid) 0.1312 0.1478 12.65% 
16
Table 4. Precision (word as indexing unit) 
Items Normal Our method Ratio 
PreAt10 (Relax) 0.3829 0.4481 17.03% 
PreAt100 (Relax) 0.2022 0.2306 14.05% 
PreAt10 (Rigid) 0.2745 0.3169 15.45% 
PreAt100 (Rigid) 0.1405 0.1573 11.96% 
In table 3, it is shown that compared with 
bi-grams as indexing units, our method respec-
tively increases 18.49% in relax relevant meas-
ure and 17.45% in rigid in PreAt10. In PreAt100 
level, our method respectively increases 15.35% 
in relax relevant and 12.65% in rigid relevant 
measure. Figure 3 displays the PreAt10 values 
of each topic in relax relevant measure based on 
bi-gram indexing where one denotes the preci-
sion enhanced with our method, another denotes 
the normal precision. It is shown the precision of 
each topic is all improved by using our method. 
 
Fig. 3. PreAt10 of all topics in relax judgment 
In table 4, using words as indexing units, 
our method respectively increases 17.03% in 
relax relevant measure and 15.45% in rigid in 
PreAt10. In PreAt100 level, our method respec-
tively increases 14.05% in relax relevant meas-
ure and 11.96% in rigid. 
In our experiments, compared with the two 
Chinese indexing units: bi-gram and words, our 
method increases the average precision of all 
queries in top 10 and top 100 measure levels for 
about 17.1% and 13.5%. What lies behind our 
method is that for each topic, we manually select 
some Chinese corpus to acquire the lingual on-
tology knowledge, and can help us to focus on 
relevant documents. Our experiment also shows 
improper extract and corpus may decrease the 
precision of top documents. So our method de-
pends on right keywords in texts, queries and the 
corpus. 
3.2   Text similarity computing 
Text similarity is a measure for the matching 
degree between two or more texts, the more high 
the similarity degree is, the more the meaning of 
text expressing is closer, vice versa. Some pro-
posal methods include Vector Space Model P[16]P, 
Ontology-based P[17]P, Distributional Semantics 
model P[18]P. 
3.2.1   Strategy of similarity computation 
First, for two Chinese texts DBiB and DBjB, we re-
spectively extract k same feature words, if the 
same feature words in the two texts is less than k, 
we don?t compare their similarity. 
Second, acquire lingual ontology knowl-
edge every text by their feature words. 
Third, get the evaluation value of every text, 
i.e. respectively add up all the average co-
occurrence distance 
lC  to the same semantic 
relation pairs in two texts. 
Final, compute the similarity ratio of every 
two text DBi B and DBj B. The similarity ratio equals to 
the ratio of the similarity evaluation value of 
text DBi B and DBj B, if the ratio is in the threshold ?, 
then we think that text DBi B is similar to text DBj B. 
3.2.2   Experimental results and analysis 
We download four classes of text for testing 
from Sina, Yahoo, Sohu and Tom, which in-
clude 71 current affairs news, 68 sports news, 69 
IT news, 74 education news. 
For the test of current affairs texts, accord-
ing to the strategy of similarity computation, we 
choose five words as feature word. They are ??
?, ??, ??, ??, ???. In the texts, the 
word ???, ??? are all replaced by word ??
?? and other classes are similar. The testing 
result is shown in table 5.  
Table 5. Testing results for text similarity 
0.95<?<1.05 0.85<?<1.15 Items 
Precision Recall FB1 B-measure Precision Recall FB1 B-measure 
Current affairs news 97.14% 97.14% 97.14% 94.60% 100% 97.23% 
Sports News 88.57% 91.18% 89.86% 84.62% 97.06% 90.41% 
IT news 93.75% 96.77% 95.24% 91.18% 100% 95.39% 
Education news 94.74% 97.30% 96.00% 90.24 100% 94.87% 
General results 93.57% 95.62% 94.58% 90.07% 99.27% 94.42% 
 
17
We analyzed all the experimental results to 
find that the results for current affairs texts are 
the best, while the sports texts are lower than 
others. We think it is mainly because some 
sports terms are unprofessional for the lower 
sports texts recognition, such as ????, ??, 
???. Other feature words are more fixed and 
more concentrated. 
4 Conclusion 
In this paper, we presented a hybrid Chinese 
language model based on a combination of on-
tology with statistical method. We discuss the 
modeling and evaluate its performance. In the 
test about texts reordering, our experiences show 
that our method can increase the performance of 
Chinese information retrieval about 17.1% and 
13.5% at top 10 and top 100 documents measure 
level. In another test about texts similarity com-
puting, F1-measure is above 95%. 
On the other hand, in the current disposal 
of our information processing, we only make 
use of some characteristics ontology and use 
some co-occurrence information, such as seman-
tics, POS, context, position, distance, and etc. 
For the further research and experiment, we will 
be on the following: (1) Research on the charac-
teristics of relations between semantics and 
combine with some mature natural language 
processing techniques. (2) Research traditional 
ontology representation to keep up with interna-
tional stand. (3) Apply our key techniques to 
English information retrieval and cross-lingual 
information retrieval systems and study a 
general approach. 
References 
1. Jelinek, F. 1990. Self-organized language model-
ing for speech recognition. In Readings in Speech 
Recognition,A. Waibel and K. F. Lee, eds. Mor-
gan-Kaufmann, San Mateo, CA,1990, 450-506. 
2.  Brown, P., Pietra, S. D., Pietra, V. D., and Mercer, 
R. 1993. The mathematics of statistical machine-
translation: Parameter estimation. Computational 
Linguistics 19, 2 (1993), 269-311. 
3.  Croft, W. B. and Lafferty, J. (EDS.) 2003. Lan-
guage Modeling for Information Retrieval. Kluwer 
Academic,Amsterdam. 
4. Wang Xiaolong, Wang Kaizhu. 1994. Speech in-
put by sentence, Chinese Journal of Computers, 
17(2): 96-103 
5.  Zhou Ming, Huang Changning, Zhang Min, Bai 
Shuanhu, and Wu Sheng. 1994. A Chinese parsing 
model based on corpus, rules and statistics, Com-
puter research and development, 31(2):40-49 
6. R DeMori, M Federico. 1999. Language model 
adaptation. In: Keith Pointing ed. Computational 
Models of Speech Pattern Processing. NATO ASI 
Series. Berlin: Springer Verlag, 102-111 
7. R Kuhn , R D Mori. 1990. A cache-based natural 
language model for speech reproduction. IEEE 
Trans on Pattern Analysis and Machine Intelli-
gence, PAM2-12(6), 570-583 
8.  Daniel Gildea, Thomas Hofmannl. 1999. Topic-
based language models using EM1. In : Proceed-
ing of the 6th European Conf on Speech Commu-
nication and Technology, Budapest, Hungary: 
ESCA, 2167-2170 
9.  Neches R., Fikes R., Finin T., Gruber T., Patil R., 
Senator T., and Swartout W. R.. 1991. Enabling 
Technology for Knowledge Sharing. AI Magazine, 
12(3) :16~36 
10. Gruber, T. R. 1993. Toward principles for the 
design of ontologies used for knowledge sharing. 
International Workshop on Formal Ontology, Pa-
dova, Italy 
11. Uschold M. 1996. Building Ontologies-Towards 
A Unified Methodology. In expert systems 96 
12. Yang Lingpeng, Ji Donghong, TangLi. 2004. 
Document Re-ranking Based on Automatically 
Acquired Key Terms in Chinese Information Re-
trieval. In Proceedings of the COLING'2004, pp. 
480-486 
13. Kwok, K.L. 1997. Comparing Representation in 
Chinese Information Retrieval. In Proceeding of 
the ACM SIGIR-97, pp. 34-4 
14. Nie, J.Y., Gao, J., Zhang, J., Zhou, M. 2000. On 
the Use of Words and N-grams for Chinese Infor-
mation Retrieval. In Proceedings of the IRAL-
2000, pp. 141-148 
15. Robertson, S.E. and Walker, S. 2001. Microsoft 
Cambridge at TREC-9: Filtering track: In Pro-
ceeding of the TREC 2000, pages 361-369 
16. Salton, G., Buckley, C. Term weighting ap-
proaches in automatic text retrieval. Information 
Processing and Management, 1988, 24(5), 
pp.513?523 
17. Vladimir Oleshchuk, Asle Pedersen. Ontology 
Based Semantic Similarity Comparison of Docu-
ments, 14th International Workshop on Database 
and Expert Systems Applications, September, 
2003, pp.735-738 
18. Besancon, R., Rajman, M., Chappelier, J. C. Tex-
tual similarities based on a distributional approach, 
Tenth International Workshop on Database and 
Expert Systems Applications, 1-3 Sept. 1999, 
pp.180-184 
 
18
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 213?216,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Chinese Term Extraction Using Different Types of Relevance 
 
 
Yuhang Yang1, Tiejun Zhao1, Qin Lu2, Dequan Zheng1 and Hao Yu1
1School of Computer Science and Technology,  
Harbin Institute of Technology, Harbin 150001, China 
{yhyang,tjzhao,dqzheng,yu}@mtlab.hit.edu.cn 
2Department of Computing,  
The Hong Kong Polytechnic University, Hong Kong, China 
csluqin@comp.polyu.edu.hk 
 
  
 
Abstract 
This paper presents a new term extraction ap-
proach using relevance between term candi-
dates calculated by a link analysis based 
method. Different types of relevance are used 
separately or jointly for term verification. The 
proposed approach requires no prior domain 
knowledge and no adaptation for new domains. 
Consequently, the method can be used in any 
domain corpus and it is especially useful for 
resource-limited domains. Evaluations con-
ducted on two different domains for Chinese 
term extraction show significant improve-
ments over existing techniques and also verify 
the efficiency and relative domain independent 
nature of the approach. 
1 Introduction 
Terms are the lexical units to represent the most 
fundamental knowledge of a domain. Term ex-
traction is an essential task in domain knowledge 
acquisition which can be used for lexicon update, 
domain ontology construction, etc. Term extrac-
tion involves two steps. The first step extracts 
candidates by unithood calculation to qualify a 
string as a valid term. The second step verifies 
them through termhood measures (Kageura and 
Umino, 1996) to validate their domain specificity.  
Many previous studies are conducted on term 
candidate extraction. Other tasks such as named 
entity recognition, meaningful word extraction 
and unknown word detection, use techniques 
similar to that for term candidate extraction. But, 
their focuses are not on domain specificity. This 
study focuses on the verification of candidates by 
termhood calculation.  
Relevance between term candidates and docu-
ments is the most popular feature used for term 
verification such as TF-IDF (Salton and McGill, 
1983; Frank, 1999) and Inter-Domain Entropy 
(Chang, 2005), which are all based on the hy-
pothesis that ?if a candidate occurs frequently in 
a few documents of a domain, it is likely a term?. 
Limited distribution information of term candi-
dates in different documents often limits the abil-
ity of such algorithms to distinguish terms from 
non-terms. There are also attempts to use prior 
domain specific knowledge and annotated cor-
pora for term verification. TV_ConSem (Ji and 
Lu, 2007) calculates the percentage of context 
words in a domain lexicon using both frequency 
information and semantic information. However, 
this technique requires a domain lexicon whose 
size and quality have great impact on the per-
formance of the algorithm. Some supervised 
learning approaches have been applied to pro-
tein/gene name recognition (Zhou et al, 2005) 
and Chinese new word identification (Li et al, 
2004) using SVM classifiers (Vapnik, 1995) 
which also require large domain corpora and an-
notations. The latest work by Yang (2008) ap-
plied the relevance between term candidates and 
sentences by using the link analysis approach 
based on the HITS algorithm to achieve better 
performance. 
In this work, a new feature on the relevance 
between different term candidates is integrated 
with other features to validate their domain 
specificity. The relevance between candidate 
terms may be useful to identify domain specific 
terms based on two assumptions. First, terms are 
more likely to occur with other terms in order to 
express domain information. Second, term can-
didates extracted from domain corpora are likely 
213
to be domain specific. Previous work by (e.g. Ji 
and Lu, 2007) uses similar information by com-
paring the context to an existing large domain 
lexicon. In this study, the relevance between 
term candidates are iteratively calculated by 
graphs using link analysis algorithm to avoid the 
dependency on prior domain knowledge.  
The rest of the paper is organized as follows. 
Section 2 describes the proposed algorithms. 
Section 3 explains the experiments and the per-
formance evaluation. Section 4 concludes and 
presents the future plans. 
2 Methodology 
This study assumes the availability of term can-
didates since the focus is on term verification by 
termhood calculation. Three types of relevance 
are first calculated including (1) the term candi-
date relevance, CC; (2) the candidate to sentence 
relevance, CS; and the candidates to document 
relevance, CD. Terms are then verified by using 
different types of relevance. 
2.1 Relevance between Term Candidates 
Based on the assumptions that term candidates 
are likely to be used together in order to repre-
sent a particular domain concept, relevance of 
term candidates can be represented by graphs in 
a domain corpus. In this study, CC is defined as 
their co-occurrence in the same sentence of the 
domain corpus. For each document, a graph of 
term candidates is first constructed. In the graph, 
a node is a term candidate. If two term candi-
dates TC1 and TC2 occur in the same sentence, 
two directional links between TC1 to TC2 are 
given to indicate their mutually related. Candi-
dates with overlapped substrings are not removed 
which means long terms can be linked to their 
components if the components are also candi-
dates.  
After graph construction, the term candidate 
relevance, CC, is then iteratively calculated using 
the PageRank algorithm (Page et al 1998) origi-
nally proposed for information retrieval. PageR-
ank assumes that the more a node is connected to 
other nodes, it is more likely to be a salient node. 
The algorithm assigns the significance score to 
each node according to the number of nodes link-
ing to it as well as the significance of the nodes. 
The PageRank calculation PR of a node A is 
shown as follows:  
)
)(
)(
...
)(
)(
)(
)(
()1()(
2
2
1
1
t
t
BC
BPR
BC
BPR
BC
BPR
ddAPR ++++?=
(1) 
where B1, B2,?, Bt are all nodes linked to node A; 
C(Bi) is the number of outgoing links from node 
Bi; d is the factor to avoid loop trap in the 
graphic structure. d is set to 0.85 as suggested in 
(Page et al, 1998). Initially, all PR weights are 
set to 1. The weight score of each node are ob-
tained by (1), iteratively. The significance of 
each term candidate in the domain specific cor-
pus is then derived based on the significance of 
other candidates it co-occurred with. The CC 
weight of term candidate TCi is given by its PR 
value after k iterations, a parameter to be deter-
mined experimentally. 
2.2 Relevance between Term Candidates 
and Sentences 
A domain specific term is more likely to be con-
tained in domain relevant sentences. Relevance 
between term candidate and sentences, referred 
to as CS, is calculated using the TV_HITS (Term 
Verification ? HITS) algorithm proposed in 
(Yang et al, 2008) based on  Hyperlink-Induced 
Topic Search (HITS) algorithm (Kleinberg, 
1997). In TV_HITS, a good hub in the domain 
corpus is a sentence that contains many good 
authorities; a good authority is a term candidate 
that is contained in many good hubs.  
In TV_HITS, a node p can either be a sentence 
or a term candidate. If a term candidate TC is 
contained in a sentence Sen of the domain corpus, 
there is a directional link from Sen to TC. 
TV_HITS then makes use of the relationship be-
tween candidates and sentences via an iterative 
process to update CS weight for each TC.  
Let VA(w(p1)A, w(p2)A,?, w(pn)A) denote the 
authority vector and VH(w(p1)H, w(p2)H,?, w(pn)H) 
denote the hub vector. VA and VH are initialized 
to (1, 1,?, 1). Given weights VA and VH with a 
directional link p?q, w(q)A and w(p)H are up-
dated by using the I operation(an in-pointer to a 
node) and the O operation(an out-pointer to a 
node) shown as follows. The CS weight of term 
candidate TCi is given by its w(q)A value after 
iteration. 
I operation:          (2) ?
??
=
Eqp
HA w(p)w(q)
O operation:         (3) ?
??
=
Eqp
AH w(q)w(p)
2.3 Relevance between Term Candidates 
and Documents 
The relevance between term candidates and 
documents is used in many term extraction algo-
214
rithms. The relevance is measured by the TF-IDF 
value according to the following equations: 
)IDF(TC)TF(TC)TFIDF(TC iii ?=      (4) 
)
)(
log()(
i
i TCDF
D
TCIDF =             (5) 
where TF(TCi) is the number of times term can-
didate TCi occurs in the domain corpus, DF(TCi) 
is the number of documents in which TCi occurs 
at least once, |D| is the total number of docu-
ments in the corpus, IDF(TCi) is the inverse 
document frequency which can be calculated 
from the document frequency. 
2.4 Combination of Relevance 
To evaluate the effective of the different types of 
relevance, they are combined in different ways in 
the evaluation. Term candidates are then ranked 
according to the corresponding termhood values 
Th(TC) and the top ranked candidates are con-
sidered terms.  
For each document Dj in the domain corpus 
where a term candidate TCi occurs, there is CCij 
weight and a CSij weight. When features CC and 
CS are used separately, termhood ThCC(TCi) and 
ThCS(TCi) are calculated by averaging CCij and 
CSij, respectively. Termhood of different combi-
nations are given in formula (6) to (9). R(TCi) 
denotes the ranking position of TCi.  
)(TCR)(TCR
)(TCTh
iCSiCC
iCSCC
11 +=+    (6) 
)log()()(
Cj
ijiCDCC DF
D
CCTCTh ?=+     (7) 
)log()()(
Cj
ijiCDCS DF
D
CSTCTh ?=+     (8) 
)(TCR)(TCR
TCTh
iCDCSiCDCC
iCDCSCC
++
++ += 11)( (9) 
3 Performance Evaluation 
3.1 Data Preparation 
To evaluate the performance of the proposed 
relevance measures for Chinese in different do-
mains, experiments are conducted on two sepa-
rate domain corpora CorpusIT and CorpusLegal., 
respectively. CorpusIT includes academic papers 
of 6.64M in size from Chinese IT journals be-
tween 1998 and 2000. CorpusLegal includes the 
complete set of official Chinese constitutional 
law articles and Economics/Finance law articles 
of 1.04M in size (http://www.law-lib.com/).  
For comparison to previous work, all term 
candidates are extracted from the same domain 
corpora using the delimiter based algorithm 
TCE_DI (Term Candidate Extraction ? Delimiter 
Identification) which is efficient according to 
(Yang et al, 2008). In TCE_DI, term delimiters 
are identified first. Words between delimiters are 
then taken as term candidates. 
The performances are evaluated in terms of 
precision (P), recall (R) and F-value (F). Since 
the corpora are relatively large, sampling is used 
for evaluation based on fixed interval of 1 in 
each 10 ranked results. The verification of all the 
sampled data is carried out manually by two ex-
perts independently. To evaluate the recall, a set 
of correct terms which are manually verified 
from the extracted terms by different methods is 
constructed as the standard answer. The answer 
set is certainly not complete. But it is useful as a 
performance indication for comparison since it is 
fair to all algorithms. 
3.2 Evaluation on Term Extraction 
For comparison, three reference algorithms are 
used in the evaluation. The first algorithm is 
TV_LinkA which takes CS and CD into consid-
eration and performs well (Yang et al, 2008). 
The second one is a supervised learning ap-
proach based on a SVM classifier, SVMlight 
(Joachims, 1999). Internal and external features 
are used by SVMlight. The third algorithm is the 
popular used TF-IDF algorithm. All the refer-
ence algorithms require no training except 
SVMlight. Two training sets containing thousands 
of positive and negative examples from IT do-
main and legal domain are constructed for the 
SVM classifier. The training and testing sets are 
not overlapped. 
Table 1 and Table 2 show the performance of 
the proposed algorithms using different features 
for IT domain and legal domain, respectively. 
The algorithm using CD alone is the same as the 
TF-IDF algorithm. The algorithm using CS and 
CD is the TV_LinkA algorithm.  
Algorithms Precision 
(%) 
Recall 
(%) 
F-value 
(%) 
SVM 63.6 49.5 55.6 
CC 47.1 36.5 41.2 
CS 65.6 51 57.4 
CD(TF-IDF) 64.8 50.4 56.7 
CC+CS 80.4 62.5 70.3 
CC+CD 49 38.1 42.9 
CS+CD 
(TV_LinkA) 
75.4 58.6 66 
CC+CS+CD 82.8 64.4 72.4 
Table 1. Performance on IT Domain 
215
 Algorithms Precision 
(%) 
Recall 
(%) 
F-value 
(%) 
SVM 60.1 54.2 57.3 
CC 45.2 40.3 42.6 
CS 70.5 40.1 51.1 
CD(TF-IDF) 59.4 52.9 56 
CC+CS 64.2 49.9 56.1 
CC+CD 48.4 43.1 45.6 
CS+CD 
(TV_LinkA) 
67.4 60.1 63.5 
CC+CS+CD 70.2 62.6 66.2 
Table 2. Performance on Legal Domain 
Table 1 and Table 2 show that the proposed 
algorithms achieve similar performance on both 
domains. The proposed algorithm using all three 
features (CC+CS+CD) performs the best. The 
results confirm that the proposed approach are 
quite stable across domains and the relevance 
between candidates are efficient for improving 
performance of term extraction in different do-
mains. The algorithm using CC only does not 
achieve good performance. Neither does CC+CS. 
The main reason is that the term candidates used 
in the experiments are extracted using the 
TCE_DI algorithm which can extract candidates 
with low statistical significance. TCE_DI pro-
vides a better compromise between recall and 
precision. CC alone is vulnerable to noisy candi-
dates since it relies on the relevance between 
candidates themselves. However, as an addi-
tional feature to the combined use of CS and CD 
(TV_LinkA), improvement of over 10% on F-
value is obtained for the IT domain, and 5% for 
the legal domain. This is because the noise data 
are eliminated by CS and CD, and CC help to 
identify additional terms that may not be statisti-
cally significant.  
4 Conclusion and Future Work 
In conclusion, this paper exploits the relevance 
between term candidates as an additional feature 
for term extraction approach. The proposed ap-
proach requires no prior domain knowledge and 
no adaptation for new domains. Experiments for 
term extraction are conducted on IT domain and 
legal domain, respectively. Evaluations indicate 
that the proposed algorithm using different types 
of relevance achieves the best performance in 
both domains without training.  
In this work, only co-occurrence in a sentence 
is used as the relevance between term candidates. 
Other features such as syntactic relations can 
also be exploited. The performance may be fur-
ther improved by using more efficient combina-
tion strategies. It would also be interesting to 
apply this approach to other languages such as 
English. 
Acknowledgement: The project is partially sup-
ported by the Hong Kong Polytechnic University 
(PolyU CRG G-U297) 
References 
Chang Jing-Shin. 2005. Domain Specific Word Ex-
traction from Hierarchical Web Documents: A 
First Step toward Building Lexicon Trees from 
Web Corpora. In Proc of the 4th SIGHAN Work-
shop on Chinese Language Learning: 64-71. 
Eibe Frank, Gordon. W. Paynter, Ian H. Witten, Carl 
Gutwin, and Craig G. Nevill-Manning. 1999. Do-
main-specific Keyphrase Extraction. In Proc.of 
16th Int. Joint Conf. on AI,  IJCAI-99: 668-673. 
Joachims T. 2000. Estimating the Generalization Per-
formance of a SVM Efficiently. In Proc. of the Int 
Conf. on Machine Learning, Morgan Kaufman, 
2000. 
Kageura K., and B. Umino. 1996. Methods of auto-
matic term recognition: a review. Term 3(2):259-
289. 
Kleinberg J. 1997. Authoritative sources in a hyper-
linked environment. In Proc. of the 9th ACM-SIAM 
Symposium on Discrete Algorithms: 668-677. New 
Orleans, America, January 1997. 
Ji Luning, and Qin Lu. 2007. Chinese Term Extrac-
tion Using Window-Based Contextual Information. 
In Proc. of CICLing 2007, LNCS 4394: 62 ? 74. 
Li Hongqiao, Chang-Ning Huang, Jianfeng Gao, and 
Xiaozhong Fan. The Use of SVM for Chinese New 
Word Identification. In Proc. of the 1st Int.Joint 
Conf. on NLP (IJCNLP2004): 723-732. Hainan Is-
land, China, March 2004. 
Salton, G., and McGill, M.J. (1983). Introduction to 
Modern Information Retrieval. McGraw-Hill. 
S. Brin, L. Page. The anatomy of a large-scale hyper-
textual web search engine. The 7th Int. World Wide 
Web Conf, Brisbane, Australia, April 1998, 107-
117. 
Vladimir N. Vapnik. 1995. The Nature of Statistical 
Learning Theory. Springer, 1995. 
Yang Yuhang, Qin Lu, Tiejun Zhao. (2008). Chinese 
Term Extraction Using Minimal Resources. The 
22nd Int. Conf. on Computational Linguistics (Col-
ing 2008). Manchester, Aug., 2008, 1033-1040. 
Zhou GD, Shen D, Zhang J, Su J, and Tan SH. 2005. 
Recognition of Protein/Gene Names from Text us-
ing an Ensemble of Classifiers. BMC Bioinformat-
ics 2005, 6(Suppl 1):S7. 
216
Statistics Based Hybrid Approach to Chinese Base Phrase Identification 
Tie-jun ZHAO, Mu-yun YANG~ Fang LIU, Jian-min YAO, Hao YU 
Department of Computer Science and Engineering, Harbin Institute of Technology 
{tjzaho, )may, flu.fang, james, yu} @mtlab.hit.edu.en 
ABSTRACT 
This paper extends the base noun 
phrase(BNP) identification i to a research 
on Chinese base phrase identification. ARer 
briefly introducing some basic concepts on 
Chinese base phrase, this paper presents a
statistics based hybrid model for identifying 
7 types of Chinese base phrases in view. 
Experiments how the efficiency of the 
proposed method in simplifying sentence 
structure. Significance of the research es in 
it provides a solid foundation for the 
Chinese parser. 
Keywords: Chinese base phrase 
identification, parsing, statistical model 
1 Introduction 
Decomposing syntactic analysis into 
several phases o as to decrease its difficulty 
is a new stream in NIP research. The 
successful POS tagging has encouraged 
researchers to explore further possibility for 
resolving sub-problems in parsing(Zhou, et 
al, 1999). The typical examples are the 
recognition of BaseNP in English and 
Chinese. 
In English BNP (base noun phrase) is 
defined as simple and non-nesting noun 
phrases, i.e. noun phrases that do not contain 
other noun phrase descendants (Church, 
1988). After that researches on BNP 
identification reports promising results for 
such task in English. Observing that the 
Chinese BNP is different form English, 
(Zhao & Huang, 1999) puts forward the 
definition of Chinese BNP in terms of 
combination of determinative modifier and 
head noun. According to them a BNP in 
Chinese can be recursively defined as: 
BaseNP ::= Determinative modifier + 
Noun I Nominalized verb(NIO 
Determinative modifier ::= Adjective I
Differentiable Adjective(DA) I Verb I Noun I 
Location I String l Numeral + Classifier 
Inspired by these researches, we extend 
the concept of BNP to Base Phrase in 
Chinese. It is based on such knowledge that 
there are many structures, not only NP, in 
which the trivial components closely attach 
to their central words and constitute a basic 
phrase in a Chinese sentence. Obviously, 
resolving all these base phrases will greatly 
benefit Chinese parser by reliving it from 
some pre-processing (though non-trivial) 
and enable it focus on the most subtle 
syntactic structures. 
Since the whole system of Chinese base 
phrase is still under discussing, this paper 
just presents some tentative research 
achievements on statistics based hybrid 
model to Chinese base phrase identification. 
For the 7 types we considered at present, our 
algorithm turns out promising results and 
smoothes the way for a better Chinese 
parser. 
2 Statistics Based Hybrid Approach to 
Chinese Base Phrase Identification 
2.1 Concepts and Defmitions 
In addition to BNP, constituents of 
many local structure in Chinese centers 
around a core word with certain fixed POS 
sequences. Therefore their identification is
slightly different from parsing in that it 
bears relatively simple phenomenon. Like 
BNP identification, identification of these 
phenomena before parsing will provide a 
simpler sequence for parser, and thus 
deserves a separate r search. 
CutTenfly, we are considering 7 Chinese 
base phrases in our research, namely base 
adjective phrase(BADJP), base adverbial 
phrase (BADVP), base noun phrase (BNP), 
73 
base temporal phrase (BTN), base location 
phrase (BNS), base verb phrase (BVP) and 
base quantity phrase (BMP) Though 
theoretically definitions for these base 
phrases are still unavailable, Appendix I lists 
the preliminary illustrations for them in 
BNF format (necessary account for POS 
annotation can also be found).. 
To frame the identification of Chinese 
base phrases, we fm'ther develop the 
following concepts: 
Definition 1: Chinese based phrases are 
recognized as atomic parts of a sentence 
beyond words that posses certain functions 
and meanings. A base phrase may consist of 
words or other base phrases, but its 
constituents, in turn, should not contain any 
base phrases. 
Definition 2: Base phrase tag is the 
token representing the syntactic function of 
the phrase. At present, base tag either falls in 
one of the 7 Chinese base phrases we are 
considering or not: 
Phrase-Tag ::= BADJP I BADVP I BNP I 
Br  r l Bm I BrP I BMP I lVULL 
Definition 3: Boundary tag denotes the 
possible relative position of a word to a base 
phrase. A boundary tag for a gfven word is 
either L( left boundary of a base phrase), 
R( right boundary of a ), I(inside a base 
phrase) or O(outside the base phrase). 
2.2 Duple Based HMM Parser 
Based on above definitions, we could, 
in view of Wojciech's proposal \[Wojeieeh and 
Thorsten, 1998\], interpret the parsing of 
Chinese base phrases as the following: 
Suppose the input as a sequence of POS 
annotations T= (to, ....... t , , ) .  The task is to 
find RC, a most possible sequence of duples 
formed by base phrase tags and boundary 
tags, among the POS sequence T.
RC = (<ro, co > ........ <rn, Cn>), 
in whil~h ri ( l  <i< =n )indicates the boundary 
tags, ci represents he base phrase tags. 
To go along with the POS tagger 
developed previously by us, we first think of 
preserving HMM (hidden Markov Model) 
for parsing Chinese base phrases. Thus the 
following formula is usually?at hand: 
RC = arg max p(RC I T) 
= arg max p(RC)*  p (T IRC)  
p(T) 
For a given sequence of T, this formula 
can be transformed into: 
RC = arg max p(RC IT) 
= arg max p(RC)*p(T  \ [RC)  
Essentially this model could be 
established through bigram or tri-gram 
statistical training by a annotated corpus. In 
practice, we just build our model from 
l O, O00 manual annotated sentences with 
common bi-gram training: 
p(RC p(RC , IRC  ,_,) 
i=1 
p(T  I RC ) = 1FI p (T i  I RC i) 
i= l  
In realization, a Viterbi algorithm is 
adopted to search the best path. An open test 
on additional 1000 sentences i  performed to 
check its accuracy. Results are shown in 
Tablel(note precision is calculated by 
word'k 
Precision 
for R 
Close 85.7% Test 
Open 82.4% 
Test 
Precision Precision for Both for .C RandC 
87.5% 79.0% 
85.1% 74.7% 
Table 1. Results for Duple Based HMM 
2.3 Triple Based MM Exploiting 
Linguistic Information 
Although results shown in Table 1 i s  
encouraging enough for research purposes, 
it is still lies a long way for practical 
Chinese parser we are aiming at. Reasons 
for errors may be account by too 
coarse-grained information provided by RC. 
Observing the fact that the Chinese base 
phrase occurs more frequently with some 
fixed patterns, i.e. some frozen POS chains, 
we decide to improved our previous model 
by emphasizing the contribution given by 
POS information. 
Adding t denoting POS in the duple (r, 
74 
c), we develop a triple in the form of (t,r,e) 
for the calculation of a node. Naturally, the 
new model is changed into a MM (Markov 
model) as: 
TRC = arg max p(TRC ) 
= arg max I~  p(TRC i I TRC i - 1) 
To train this model, we still using a 
bi-gram model. Applying the same corpus 
and tests described above, we got the 
performance of triple based MM identifier 
for Chinese base phrases (see Table 2). 
Precision Precision Precision 
for R ~rC 
89.2% 91 .5% 84.6% 
88.4% 89.9% 83% 
Close 
Open 
for Both 
R and C 
Table 2. Result for Triple Based MM 
2.4 Further  Improvement Through TBED 
Learning 
Like other statistical models, the above 
model, whether duple based or triple based, 
both seem to reach an accuracy ceiling after 
enlarging training set to 12, 000 or so. To 
cover the remaining accuracy, we apply the 
transformation-based error driven (TBED) 
learning strategy described in \[Brill, 1992\] 
to acquired esired rules. 
In our module, some initial rules are 
first designed as compensation of statistical 
model. Applying these rules will cause new 
mistakes as well as make correct 
identifications. Then the module will 
compare the processed texts with training 
sentences, generate new rules according to 
pre-defmed actions and update its rule bank 
after evaluation (see Fig 1.). 
I I Compare and Rules Passing 
Generate New Rules - - - - !~ Evaluation I 
Tt 
TextTraining \] TextPr?eessed \]
Identifier 
'T 
Input Text 
Figure 1. TBED Learning Module 
The dotted line in fig 2. will stop 
functioning if pre-set accuracy is reached by 
the identifier for the Chinese base phrase. 
Evaluation of new rules is based on an 
greedy algorithm: only rule with max 
contribution (max correction and rain error) 
will be added. Design of rule generation 
(pre-defined actions) is similar to those 
described in \[Brill, 1992\]. 
Table 3 shows a significant 
improvement after applying rules obtained 
through TBED learner. It is also the final 
performance of the proposed Chinese base 
phrase identification model. 
Precision Precision Precision for Both for R for C Rand C 
91.2% 92.8% 89% 
90.4% 91.1% 87.1% 
Close 
open 
Table 3. Results after TBED Module 
3 Conclusions and Discussions 
We have accomplished preliminary 
expedments on identification of various 
types of base phrases defined in this paper. 
The data shown in last seetion prove that our 
method generates atisfactory results for 
75 
Chinese base phrase identification. The 
overall process of our method is outlined the 
following figure. 
Input Chinese Sentences 
after Sengmentation and 
POS tagging 
~ Converted into Nodes 
to Be Parsed 
Triple Based Bi-gram 
MM with Viterbi 
Algorithm 
TBED Based 
Correction 
T" ' Output 
\ 
Fig 2. Processing ofChinese Based Phrase Identification 
However, the 7 types Chinese base 
phrases we have proposed are far l~om 
perfection. Even what we have proposed for 
the 7 phrases is still under test. Further 
improvement will focus on two aspects: one 
is to discuss and add new base phrase for a 
broader coverage; the other is to define, 
theoretically or empirically, the Chinese 
base phrases with more strict constraints. Of 
course, new techniques to improved the 
accuracy of statistical model are the constant 
aim of our research. 
To sum up, Chinese base phrase 
identification will reduce complexity of a 
Chinese parser. The successful idemifieation 
of the 7 base phrases clearly simplifies the 
structure of the sentence. We expect hat the 
research described in this paper will lay a 
solid foundation for a high-accuracy 
Chinese parser. 
22(2): pp141-146 
\[Zhou, et al 1999\] Zhou Qiang, Sun 
Mao-Song, Huang Chang-Ning, Chunk 
parsing scheme for Chinese sentences, 
Chinese J. Computer, 22(11): pp1159-1165 
Reference 
\[Church, 1988\] K. Church, A stochastic 
parts program and noun phrase parser for 
unrestricted text, In: Proc. of Second 
Conference on Applied Natural Language 
Processing, 1988 
\[Wojciech and Thorsten, 1998\] Wojciech Skut and 
Thorsten Brants, Chunk Tagger, Statistical 
Recongnition of Noun Phrases, In ESSLLI-98 
Workshop on Automated Acquisition of Syntax 
and Parsing, Saarbrvcken, 1998. 
\[Zhao & Huang, 1999\] Zhao Jun and Huang 
Chang-Ning, The model for Chinese baseNP 
structure analysis, Chinese J. Computer, 
76 
Appendix Illustration of 7 Chinese Base 
Phrases in BNF 
The patterns listed here are far from 
complete (even for the 7 phrases 
themselves). Theoretical definition is 
beyond this paper and what we provide here 
is actually stage results of expert 
observation and linguistic abstraction. 
BADJP ::= d++a \[ d+BADJP \] a + I a+BADJP 
\[ BADVP+a I BADVP+BADJP 
BADVP ::= a+usdi(:~) I d+usdi I vg+usdi I 
BADJP+usdi IBADVP+usdi IBMP+usdi 
BMP ::= m + \[ m*+q* \[ m+q+m \[ d+m+q \] 
f+m+q \[ r+m+q I BMP ? 
BNP ::= a+n I a+usde(~)+n I a+usde+BNP I 
a+BNP \]b+n \] b+usde+n I b+usde+BNP I 
b+BNP I d+usde+n I f+n I f+usde+n I f+BNP 
1 m+n I m+BNP I n+ I n+usde+n I
n+usde+BNP I n+usde+BMP I n+BNP I q+n 
I q+BNP I r+a+n I r+m+n I r+n I r+usde+n I 
r+usde+BNP \[ r+BNP I s+n I s+usde+n \[
s+usde+BNP I t+nl t+usde+n \[ t+usde+BNP 
I vg +usde+n I vg+usde+BNP I BADJP+n 
BADJP+usde+n \] BADJP+usde+BNP 
BADJP+BNP \[ BMP+n \[ BMP+usde+n 
BMP+usde+BNP \[ BMP+BNP \[ BNP+n 
BNP+usde+n \[ BNP+usde+BNP 
BNP+usde+BMP \[ BNP+BNP 
BNS+usde+n \[ BNS+usde+BNP 
BNS+BNP I BTN+usde+n 
BTN+usde+BNP \[ BVP+usde+n 
BVP+usde+BNP 
BNS ::= a+nd I m+nd I n+s I r+nd I 
n+usde+f I n+usde+nd I n+usde+s I 
n+usde+BNS I nd + I r+usde+nd \[ r+usde+s I 
s+usde+nd I s+usde+BNS I BNP BNS I 
BNS + 
BTN ::= a+t I m+t I r+t I t+ I t+usd~t  I 
BMP+t I BTN+t I BNP+usde+t 
BVP ::= a+vg I d+vg I vg+d+a I vg+d+vq I 
vg+d+vb I vg+usdf(~)+a I vg+usdf+d I 
vg+usdf+vq \[ vg+usdf+u I vg+usdf+BADJP I 
vg+ut I vg+vb I vg+ut+vq I vq+vg I vq+BVP 
\] vz+vg I vz+BVP I BADJP+vg I 
BADVP+vg \[ BADVP+BVP I BVP+ut I 
BVP+vq I BVP+BVP 
Symbol 
a 
d 
Part-Of-Speech 
Adjective 
Adverb 
TemporaYspacial 
position word 
Examples 
~(beaut i fu l ) ,  ~( romant ic )  
~(very), ~(s t i l l )  
~(in), _k(on), ~N(between) 
m numeral --(one), ~(two), -~(three) 
n noun ~ ~ (people), ~ ~I~  (tomato), 
"bl-~JL(computer) 
nd Name of place ~(Be i j ing) ,  I I~(Harb in ) ,  
~.\]t~(New York) 
q classifier \]\]~(flock), +(NULL) 
r pronoun '~'~(you), ~(I, me), ~(he, him) 
s location oun I~.(around), ~:gb(outside) 
t time noun ~;~(yesterday), --L~ (July) 
ut tense auxiliary ~,T,~c_(NULL) 
vb Complemental verb ~,~_t(NULL) 
vg common verb ~ll~(know), ~( long  for) 
vq directional verb ~,T  ~i~(NULL) 
vz modal verb ~I ~(can), )~(shou ld )  
Table for POS symbols used in Appendix 
77 
Automatic Information Transfer Between English And Chinese 
Jianmin Yao, Hao Yu, Tiejun Zhao  
School of Computer Science and Technology 
Harbin Institute of Technology 
Harbin, China, 150001 
james@mtlab.hit.edu.cn 
Xiaohong Li 
Department of Foreign Studies 
Harbin Institute of Technology 
Harbin, China, 150001 
goodtreeyale@yahoo.com.cn 
 
Abstract  
The translation choice and transfer modules 
in an English Chinese machine translation 
system are introduced. The translation 
choice is realized on basis of a grammar tree 
and takes the context as a word bag, with the 
lexicon and POS tag information as context 
features. The Bayes minimal error 
probability is taken as the evaluation 
function of the candidate translation. The 
rule-based transfer and generation module 
takes the parsing tree as the input and 
operates on the information of POS tag, 
semantics or even the lexicon. 
Introduction 
Machine translation is urgently needed to get 
away with the language barrier between 
different nations. The task of machine 
translation is to realize mapping from one 
language to another. At present there are three 
main methods for machine translation systems 
[Zhao 2000]: 1) pattern/rule based systems: 
production rules compose the main body of the 
knowledge base. The rules or patterns are often 
manually written or automatically acquired from 
training corpus; 2) example based method. The 
knowledge base is a bilingual corpus of source 
slices S? and their translations T? Given a source 
slice of input S, match S with the source slices 
and choose the most similar as the translation or 
get the translation from it. 3) Statistics based 
method: it is a method based on monolingual 
language model and bilingual language model. 
The probabilities are acquired from large-scale 
(bilingual) corpora.  
Machine translation is more than a 
manipulation of one natural language (e.g. 
Chinese). Not only the grammatical and 
semantic characteristics of the source language 
must be considered, but also those of the target 
language. To sum up, the characteristics of 
bilingual translation is the essence of a machine 
translation system.  
A machine translation system usually 
includes 3 sub-systems [Zhao 1999] ? (1) 
Analysis: to analyse the source language 
sentence and generate a syntactic tree with 
syntactic functional tags; (2) Transfer: map a 
source parsing tree into a target language parsing 
tree; (3) Generation: generate the target 
language sentence according to the target 
language syntactic tree.  
The MTS2000 system developed in Harbin 
Institute of Technology is a bi-directional 
machine translation system based on a 
combination of stochastic and rule-based 
methods. Figure 1 shows the flow of the system.  
 Input English Sentence  
 Morphology Analysis  
 
Syntactic Analysis  
 
Word Translation Choice  
 
Transfer and Generation  
 
 
 
Output Chinese Sentence 
Figure 1 Flowchart of MTS2000 System  
Analysis and transfer are separated in the 
architecture of the MTS2000 system. This 
modularisation is helpful to the integration of 
stochastic method and the rule based method. 
New techniques are easier to be integrated into 
the modularised system. Two modules 
implement the transfer step and the generation 
step after analysis of the source sentence. The 
specific task of transfer and generation is to 
produce a target language sentence given the 
source language syntactic tree. In details, given 
an English syntactic tree (e.g. S[PP[ In/IN 
BNP[our/PRP$ workshop/NN]] BNP[ there/EX] 
VP[ is/VBZ NP[ no/DT NP[ NN[ machine/NN 
tool/NN] SBAR[ but/CC VP[ is/VBZ 
made/VBN PP[ in/IN BNP[ China/NNP ]]]]]]]]), 
using knowledge sources such as grammatical 
features, simple semantic features, construct a 
Chinese syntactic tree, whose terminal nodes 
compromise in sequence the Chinese translation.  
The input sentence are analysed using the 
morphology analyser, part-of-speech tagger, and 
syntactic analyser. After these steps, a syntactic 
parsing tree is obtained which has multiple 
levels with functional tags [Meng 2000]. 
Followed is the parser flow: 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2. Parser based on Hybrid Methods 
At present, our English parser is able to 
generate syntactic tree ble 
way. The English parsin
information about relatio
in the source sentence
information of the nodes
of transfer and generatio
the nodes is the starting
generation. After syntact
transfer and generation in
choice of ambiguous 
adjustment and inserti
functional words. Trans
implemented using two
word translation choice, 
transfer and translation m
1 Parsing Based Tran
First we will give a f
translation choice in 
[Manning 1999]: Suppose the source sentence to 
be translated to be ES. In the sentence the 
ambiguous word EW has M target translations 
CW1, CW2, ... CWM. And the translations 
occurs in a specific context C with probabilities 
P(CW1 | C)?P(CW2 | C), ... P(CWM|C)?From 
the Bayes minimum error probability formula, 
we get: 
CW = argmax[P(CWk|C)] 
= argmax[logP(CWk) + logP(C|CWk) ] (1) 
Generally when the condition fulfills 
P(CW1|C)>P(CW2|C)>...>P(CWM|C), we may 
choose CW1 as the translation for EW. From the 
Na?ve Bayes formula? 
P(C|CWk) = P({vj | vj in C}|CWk) 
 = ?Vj in C P(vj|sk)              (2) 
So formula (1) can be rewritten as: 
CW = argmax[P(CWk|C)] 
Input Sentence Statistics Knowledge = argmax[logP(CWk)+?Vj in ClogP(vj|CWk)] (3) 
Where P(CWk) denotes the probability that 
CWk occurs in the corpus; P (vj| CWk) denotes 
the probability that the context feature vj 
co-occurs with translation CWk? 
A general algorithm of supervised word 
sense disambiguation is as follows: 
1. comment: Training 
2. for all senses sk of w do 
3.    for all words vj in the vocabulary do 
4.       P(vj|sk) = C(vj, sk)/C(vj) 
5.    end 
6. end 
7. for all senses sk of w do 
8.    P(sk) = C(sk)/C(w) 
9. end 
POS Tagger Manual Rule 
Base 
PPA Resolution 
Layered Parsing eein comparative usa
g 
nsh
, a
, is
n. 
 p
ic 
clu
w
on
fer 
 m
the
od
sla
orm
mParsing Trtree, with the basic 
ip among the nodes 
lso with semantic 
 input to the module 
The information of 
oint of transfer and 
parsing, the task of 
des word translation 
ords, word order 
/deletion of some 
and generation are 
odules: one is for 
 other for structure 
ification. 
tion Choice 
al description for 
achine translation 
10. comment: Disambiguation 
11. for all sense sk of w do 
12.    Score(sk) = logP(sk) 
13.    for all words vj in the context window c do
14.       score(sk) = score(sk) + logP(vj|sk) 
15.    end 
16. end 
17. choose s? = argmaxskscore(sk) 
Figure 5. Bayesian disambiguation 
From the above formal description we can 
see that the key to the stochastic word 
translation is to select proper context and context 
features Vj. Present methods often define a word 
window of some size, i.e. to suppose only words 
within the window contributes to the translation 
choice of the ambiguous word. For example, 
[Huang 1997] uses a word window of length 6 
words for word sense disambiguation; [Xun 
1998] define a moveable window of length 4 
words; [Ng 1997] uses a word window with 
offset ?2. But two problems exist for this 
method: (1) some words that are informative to 
sense disambiguation may not be covered by the 
window; (2) some words that are covered by the 
word window really contribute nothing to the 
sense choice, but only bring noise information. 
After a broad investigation for large-scale 
ambiguous words, we choose the context 
according to the correlation of the context words 
with the ambiguous word, but not only the 
distance from the word. 
From the above analysis, we choose the 
translation choice method based on syntactic 
analysis. Place the module of translation choice 
between the parser and the generator; acquire a 
context set for the ambiguous word. When 
choosing the translation, we may take the 
context set as a word bag, i.e. the grammatical 
context as word bag. No single word is 
considered but only that lexical and 
part-of-speech information are taken as context 
features. Bayes minimum error probability is 
taken as evaluation function for word translation 
choice. 
In this paper, grammatical context is 
considered for word translation choice. The 
structure related features of the ambiguous 
words are taken into account for fully use of the 
parsing result. It has the characteristics below: (1) 
The window size is not defined by human but on 
basis of the grammatical structure of the 
sentence, so we can acquire more efficiently the 
useful context features; (2) The unrelated 
context features in sentence structure are filtered 
out for translation choice; (3) The features are 
based on the structure relationship, but not 100% 
right parsing result. From the above 
characteristics, we can see the method is really 
practical. 
2 Rule Based Transfer & Generation 
For MTS2000, structural transfer is to start from 
the syntactic parsing tree and construct the 
Chinese syntactic tree. While the generation of 
Chinese is to generate a word link from the 
Chinese tree and build the translation sentence 
[Yao 2001]. This module has adopted the 
rule-based knowledge representation method. 
The design of the rule system is highly related to 
the performance of the machine translation 
system. 
The rule description language of the 
machine translation system is in the form of 
production rules, i.e. a rule composed of a 
conditional part and an operational part. The 
conditional part is a scan window of variable 
length, which uses the context constraint 
conditions such as phrases or some linguistic 
features. The operational part generates the 
corresponding translation or some corresponding 
generation features in the operational part. If the 
conditions are met, the operations will be 
performed. The representation of the rule system 
has shown a characteristic of the system, that is 
the integration of transfer and generation. The 
rule description language is similar to natural 
language and consistent with human habits. 
Multiple description methods are implemented. 
The conditional part of the rules is 
composed of node numbers and ?+? symbols 
that is used to link the nodes. The operation part 
consists of corresponding conditional parts and 
translations and also, if necessary, some action 
functions. 
For example, the rule to combine an 
adjective and a noun to generate a noun phrase is 
as follows:  
0:Cate=A + 1:Cate=N 
->0:* + 1:* + _NodeUf(N?0?1) 
in which, ?*? stands for corresponding 
translation of the nodes, _NodeUf() is a function 
that combines the nodes to generate a new node. 
The new translation is generated at the same 
time with the combination of nodes. 
In general, the English Chinese machine 
translation system has the following features in 
the transfer and generation phase: 
1) The grammatical and semantic features are 
described by a string composed of frame 
name and values linked with ?=?; 
2) The conditions may be operated by ?and?, 
?or? and ?not?; 
3) Nodes in the same level of the sentence may 
be scanned and tested arbitrarily; 
4) The action functions and test functions can 
generate corresponding features for feature 
transmission and test. 
The rules are organized into various levels. 
All the rules are put in the knowledge base with 
part-of-speech as the entry feature. The rules 
have different priorities, which decide their 
sequence in rule matching. In general, the more 
specific the rule, the higher is its priority. The 
more general the rule, the lower is its priority. 
The levels of the rules help resolve rule 
collision. 
Conclusion 
The system prototype has been implemented and 
large-scale development and refinement are 
under progress. From our knowledge of the 
system, knowledge acquisition and rule base 
organization is the bottleneck for MTS2000 
system and similar natural language processing 
systems. The knowledge acquisition for word 
translation choice needs large-scale word 
aligned bilingual corpus. We are making 
research on new word translation methods on 
basis of our 60,000-sentence aligned bilingual 
corpus. The transfer and generation knowledge 
base are facing much knowledge collision and 
redundancy problem. The organization 
technique of knowledge base is also an 
important issue in the project. 
References  
Tie-Jun Zhao, En-Dong Xun, Bin Chen, Xiao-Hu 
Liu,Sheng Li, Research on Word Sense 
Disambiguation based on Target Language 
Statistics, Applied Fundamental and Engineering 
Journal, 1999?7?1??101-110 
Meng Yao, Zhao Tiejun, Yu Hao, Li Sheng, A 
Decision Tree Based Corpus Approach to English 
Base Noun Phrase Identification, Proceedings 
International conference on East-Asian Language 
Processing and Internet Information Technology, 
Shenyang, 2000: 5-10 
Christopher D. Manning, Hinrich Sch ? tze, 
Foundation of Statistical Natural Language 
Processing. The MIT Press. pp229-262. 1999. 
Chang-Ning Huang, Juan-Zi Li, A language model 
for word sense disambiguation, 10th anniversary 
for Chinese Linguistic Society, October, 1997, 
Fuzhou  
En-Dong Xun, Sheng Li, Tie-Jun Zhao, Bi-gram 
co-occurrence based stochastic method for word 
sense disambiguation, High Technologies, 1998, 
10(8): 21-25  
Hwee Tou Ng. Exemplar-Based Word Sense 
Disambiguation: Some Recent Improvements. In 
Proceedings of the Second Conference on 
Empirical Methods in Natural Language 
Processing (EMNLP-2), August 1997  
Tie-Jun Zhao etc, Principle of Machine Translation, 
Press of Harbin Institute of Technology, 2000. 
Jian-Min Yao, Jing Zhang, Hao Yu, Tie-Jun 
Zhao,Sheng Li, Transfer from an English parsing 
tree to a Chinese syntactic tree, Joint Conference of 
the Society of Computational Linguistics, 2001, 
Taiyuan.-138.  
  
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 427?434, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Chinese Named Entity Recognition Based on Multiple Features 
 
 
Youzheng Wu, Jun Zhao, Bo Xu Hao Yu 
National Laboratory of Pattern Recognition Fujitsu R&D Center Co., Ltd 
Institute of Automation, CAS Beijing 100016, China 
Beijing, 100080, China yu@frdc.fujitsu.com 
(yzwu,jzhao,bxu)@nlpr.ia.ac.cn  
 
 
 
 
Abstract 
This paper proposes a hybrid Chinese 
named entity recognition model based on 
multiple features. It differentiates from 
most of the previous approaches mainly 
as follows. Firstly, the proposed Hybrid 
Model integrates coarse particle feature 
(POS Model) with fine particle feature 
(Word Model), so that it can overcome 
the disadvantages of each other. Secondly, 
in order to reduce the searching space and 
improve the efficiency, we introduce heu-
ristic human knowledge into statistical 
model, which could increase the perform-
ance of NER significantly. Thirdly, we 
use three sub-models to respectively de-
scribe three kinds of transliterated person 
name, that is, Japanese, Russian and 
Euramerican person name, which can im-
prove the performance of PN recognition. 
From the experimental results on People's 
Daily testing data, we can conclude that 
our Hybrid Model is better than the mod-
els which only use one kind of features. 
And the experiments on MET-2 testing 
data also confirm the above conclusion, 
which show that our algorithm has consis-
tence on different testing data. 
1 Introduction 
Named Entity Recognition (NER) is one of the key 
techniques in the fields of Information Extraction, 
Question Answering, Parsing, Metadata Tagging in 
Semantic Web, etc. In MET-2 held in conjunction 
with the Seventh Message Understanding Confer-
ence (MUC-7), the task of NER is defined as rec-
ognizing seven sub-categories entities: person (PN), 
location (LN), organization (ON), time, date, cur-
rency and percentage. As for Chinese NEs, we fur-
ther divide PN into five sub-classes, that is, 
Chinese PN (CPN), Japanese PN (JPN), Russian 
PN (RPN), Euramerican PN (EPN) and abbrevi-
ated PN (APN) like "???/Mr. Wu". Similarly, 
LN is split into common LN (LN) like "???
/Zhongguancun" and abbreviated LN (ALN) such 
as "?/Beijing", "?/Shanghai". The recognition of 
time (TM) and numbers (NM) is comparatively 
simpler and can be implemented via finite state 
automata. Therefore, our research focuses on the 
recognition of CPN, JPN, RPN, EPN, APN, LN, 
ALN and ON. 
Compared to English NER, Chinese NER is 
more difficult. We think that the main differences 
between Chinese NER and English NER lie in: (1) 
Unlike English, Chinese lacks the capitalization 
information which can play very important roles in 
identifying named entities. (2) There is no space 
between words in Chinese, so we have to segment 
the text before NER. Consequently, the errors in 
word segmentation will affect the result of NER. 
In this paper, we proposes a hybrid Chinese 
NER model based on multiple features which em-
phasizes on (1) combining fine particle features 
(Word Model) with coarse particle features (POS 
Model); (2) integrating human knowledge into sta-
tistical model; (3) and using diverse sub-models 
for different kinds of entities. Especially, we divide 
transliterated person name into three sub-classes 
according to their characters set, that is, JPN, RPN 
and EPN. In order to deduce the complexity of the 
model and the searching space, we divide the rec-
427
ognition process into two steps: (1) word segmen-
tation and POS tagging; (2) named entity recogni-
tion based on the first step. 
Trained on the NEs labeled corpus of five-
month People's Daily corpus and tested on one-
month People's Daily corpus, the Hybrid Model 
achieves the following performance. The precision 
and the recall of PN (including CPN, JPN, RPN, 
EPN, AP N), LN (including ALN) and ON are re-
spectively (94.06%, 95.21%), (93.98%, 93.48%), 
and (84.69%, 86.86%). From the experimental re-
sults on People's Daily testing data, we can con-
clude that our Hybrid Model is better than other 
models which only use one kind of features. And 
the experiments on MET-2 testing data also con-
firm the above conclusion, which show that our 
algorithm has consistence on different testing data. 
2 Related Work 
On the impelling of international evaluations like 
MUC, CoNLL, IEER and ACE, the researches on 
English NER have achieved impressive results. For 
example, the best English NER system[Chinchor. 
1998] in MUC7 achieved 95% precision and 92% 
recall. However, Chinese NER is far from mature. 
For example, the performance (precision, recall) of 
the best Chinese NER system in MET-2 is (66%, 
92%), (89%, 91%), (89%, 88%) for PN, LN and 
ON respectively.  
Recently, approaches for NER are a shift away 
from handcrafted rules[Grishman, et al 1995] 
[Krupka, et al 1998][Black et al 1998] towards 
machine learning algorithms, i.e. unsupervised 
model like DL-CoTrain, CoBoost[Collins, 1999, 
2002], supervised learning like Error-driven [Ab-
erdeen, et al 1995], Decision Tree [Sekine, et al 
1998], HMM[Bikel, et al 1997] and Maximum 
Entropy[Borthwick, et al 1999][Mikheev, et 
al.1998].  
Similarly, the models for Chinese NER can also 
be divided into two categories: Individual Model 
and Integrated Model.  
Individual Model[Chen, et al 1998][Sun, et al 
1994][Zheng, et al 2000] consists of several sub-
models, each of them deals with a kind of entities. 
For example, the recognition of PN may be statis-
tical-based model, while LN and ON may be rule-
based model like [Chen, et al 1998]. Integrated 
Model[Sun, et al 2002] [Zhang, et al 2003][Yu, et 
al. 1998][Chua, et al 2002] deals with all kinds of 
entities in a unified statistical framework. Most of 
these integrated models can be viewed as a HMM 
model. The differences among them are the defini-
tion of state and the features used in entity model 
and context model.  
In fact, a NER model recognizes named entities 
through mining the intrinsic features in the entities 
and the contextual features around the entities. 
Most of existing approaches employ either coarse 
particle features, like POS and ROLE[Zhang, et al 
2003], or fine particle features like word. The data 
sparseness problem is serious if only using fine 
particle features, and coarse particle features will 
lose much important information though without 
serious data sparseness problem. Our idea is that 
coarse particle features should be integrated into 
fine particle features to overcome the disadvan-
tages of them. However, most systems do not com-
bine them and especially ignore the impact of POS. 
Inspired by the algorithms of identifying 
BaseNP and Chunk[Xun, et al 2000], we propose 
a hybrid NER model which emphasizes on com-
bining coarse particle features (POS Model) with 
fine particle features (Word Model). Though the 
Hybrid Model can overcome the disadvantages of 
the Word Model and the POS Model, there are still 
some problems in such a framework. Data sparse-
ness still exists and very large searching space in 
decoding will influence efficiency. Our idea is that 
heuristic human knowledge can not only improve 
the time efficiency, but also solve the data sparse-
ness problem to some extent by restricting the gen-
eration of entity candidates. So we intend to 
incorporate human knowledge into the statistical 
model to improve efficiency and effectivity of the 
Hybrid Model.  
Similarly, for capturing intrinsic features in dif-
ferent types of entities, we design several sub-
models for each kind of entities. For example, we 
divide transliterated person name into three sub-
classes according to their characters sets, that is, 
JPN, RPN and EPN. 
3 Chinese NER with Multiple Features 
Chinese NEs have very distinct word features in 
their composition and contextual information. For 
example, about 365 highest frequently used sur-
names cover 99% Chinese surnames[Sun, et al 
1994]. Similarly the characters used for transliter-
ated names are also limited. LNs and ONs often 
428
end with the specific words like "?/province" and 
"??/company". However, data sparseness is very 
serious when using word features. So we try to 
introduce coarse particle feature to overcome the 
data sparseness problem. POS features are simplest 
and easy to obtain. Therefore, our hybrid model 
combines word feature with POS feature to recog-
nize Chinese NEs. 
Given a word/pos sequence as equation (1): 
nnii twtwtwTW //// 11 LL=                    (1) 
where n is the number of words and ti is the POS 
of word wi. The task of Chinese NE identification 
is to find the optimal sequence WC*/ TC* by split-
ting, combining and classifying the sequence of (1). 
mmii21 tc/wctc/wctc/wc*TC/*WC LL=     (2) 
where [ ]ljji wwwc += L , [ ]ljji tttc += L , nm ? . 
Note that the definition of words in {wi} set is 
that each kind of NEs (including PN, APN, LN, 
ALN, ON, TM, NM) is defined as a word and all 
the other words in the vocabulary are also defined 
as individual words. Consequently, {wi} set has 
|V|+7 words, where |V| is the size of vocabulary. 
The size of {ti} set is 48 which include PKU POS 
tagging set1 and each kind of NEs. 
Obviously, we could obtain the optimal se-
quence WC*/TC* through the following three 
models: the Word Model, the POS Model and the 
Hybrid Model.  
The Word Model employs word features for 
NER, which is introduced by [Sun, et al 2002]. 
The POS Model employs POS features for NER. 
This paper proposes a Hybrid Model which com-
bines word features with POS features.  
We will describe these models in detail in fol-
lowing section. 
3.1 The Hybrid Model 
For the convenience of description, we take apart 
equation (1) into two components: word sequence 
as equation (3) and POS sequence as (4).  
ni21 wwwwW LL=                                     (3) 
ni21 ttttT LL=                                          (4) 
The Word Model estimates the probability of 
generating a NE from the viewpoint of word se-
quence, which can be expressed in equation (5).  
                                                          
1 http://icl.pku.edu.cn/nlp-tools/catetkset.html 
( ) ( )WC|WPWCPargmax*WC wc=                  (5) 
The POS Model estimates the probability of 
generating a NE from the viewpoint of POS se-
quence, which can be expressed in equation (6). 
( ) ( )TC|TPTCPargmax*TC TC=                      (6) 
Our proposed Hybrid Model combines the Word 
Model with the POS Model, which can be ex-
pressed in the equation (7). 
( )
( ) ( )
( ) ( ) ( )
( ) ( )
( ) ( ) ( ) ( ) ( ) ?]TCPTC|T[PWCPWC|WPargmax
W,TWC,TC,Pargmax
T,WPW,TWC,TC,Pargmax
W,T|WC,TCPargmax
*TC*,WC
TCWC,
TCWC,
TCWC,
TCWC,
?
=
=
=
  (7) 
where factor ? > 0 is to balance the Word Model 
and the POS Model. 
Therefore, the Hybrid Model consists of four 
sub-models: word context model P(WC), POS con-
text model P(TC), word entity model P(W|WC) 
and POS entity model P(T|TC). 
3.2 Context Model 
The word context model and the POS context 
model estimate the probability of generating a 
word or a POS given previous context. P(WC) and 
P(TC) can be estimated according to (8) and (9) 
respectively.  
( ) ( )?
=
=
m
1i
1i2ii wcwc|wcPWCP                       (8) 
( ) ( )?
=
=
m
1i
1i2ii tctc|tcPTCP                             (9) 
3.3 Word Entity Model  
Different types of NEs have different structures 
and intrinsic characteristics. Therefore, a single 
model can't capture all types of entities. Typical, 
character-based model is more appropriate for PNs, 
whereas, word-based model is more competent for 
LNs and ONs. Especially, we divided transliterated 
PN into three categories such as JPN, RPN and 
EPN.  
For the sake of estimating the probability of 
generating a NE, we define 19 sub-classes shown 
as Table 1 according to their position in NEs. 
 
429
Tag Description 
Sur Surname of CPN 
Dgb First character of Given Name of CPN 
Dge Last character of Give Name of CPN 
Bfn First character of EPN 
Mfn Middle character of EPN 
Efn Last character of EPN 
RBfn First character of RPN 
RMfn Middle character of RPN 
REfn Last character of RPN 
JBfn surname of JPN 
JMfn Middle character of JPN 
JEfn Last character of JPN 
Bol First word of LN 
Mol Middle word of LN 
Eol Last word of LN 
Aloc Single character LN 
Boo First word of ON 
Moo Middle word of ON 
Eoo Last word of ON 
Table 1 Sub-classes in Entity Model 
3.3.1 Word Entity Model for PN 
For the class of PN (including CPN, APN, JPN, 
RPN and EPN), the word entity model is a charac-
ter-based trigram model which can be expressed in 
equation (10). ( )
( ) ( )( )
( )( )1kiik
1liil1i
ik1i
ik1i
wcwc
1k
2l
wcwcwc
2k
wcwc
iwcwc
w,ENe|wP
w,MNe|wPBNe|wP
ENeMNeMNeBNe|wwP
wc|wwP
?
?
?
??
???
?
???
?=
??
=
? 448476
LL
L
       (10) 
where, BNe, MNe and ENe denotes the first, mid-
dle and last characters respectively. 
The word entity models for PN are estimated 
with Chinese, Japanese, Russian and Euramerican 
names lists which contain 15.6 million, 0.15 mil-
lion, 0.44 million, 0.4 million entities respectively. 
3.3.2 Word Entity Model for LN and ON 
For the class of LN and ON, the word entity model 
is a word-based trigram model. The model can be 
expressed by (11). 
( )
( ) ( )
( )( ) ( )
( )( ) ( )ikwcwcwcwc
wcwcwc
1k
2l
1liil
wcendwcstartwcwc
2k
wcwcwc
iendwcstartwc
wc|wwPwc,ENe|wcP
wc|wwPwc,MNe|wcP
wc|w..wPBNe|wcP
ENeMNeMNeBNe|wcwcwcP
wc|wwP
denikstartik1kiik
ilendilstartil
1i1i1i1i
ikil1i
ii
?
L
L
448476
LLL
L
?
?
=
???
?
???
?=
=
 (11) 
The word entity models and the POS entity 
model for LN and ON are estimated with LN and 
ON names lists which respectively contain 0.44 
mil-lion and 3.2 million entities. 
3.3.3 Word Entity Model for ALN 
For the class of ALN, we use word-based bi-gram 
model. The entity model for ALN can be expressed 
by equation (12). 
( ) ( )
)LocA(C
ocAL,wC
ocAL|wP ii =                           (12) 
where wi is the ALN which includes single and 
multiple characters ALN. 
3.4 POS Entity Model 
But for the class of PN, it's very difficult to obtain 
the corpus to train POS Entity Model. For the sake 
of simplification, we use word entity model shown 
in equation (10) to replace the POS entity model. 
For the class of LN and ON, POS entity model 
can be expressed by equation (13). ( )
( ) ( )
( )( ) ( )
( )( ) ( )iktctctctc
tctctc
1k
2l
1liil
tcendwcstarttctc
2k
tctctc
iendtcstarttc
tc|ttPtc,ENe|tcP
tc|ttPtc,MNe|tcP
tc|t..tPBNe|tcP
ENeMNeMNeBNe|tctctcP
tc|ttP
denikstartik1kiik
ilendilstartil
1i1i1i1i
ikil1i
ii
?
L
L
448476
LLL
L
?
?
=
???
?
???
?=
=
    (13) 
While for the class of ALN, POS entity model is 
shown as equation (14). 
( ) ( )
)ocAL(C
ocAL,tiCocAL|tP i =                               (14) 
4 Heuristic Human Knowledge 
In this section, we will introduce heuristic human 
knowledge that is used for Chinese NER and the 
430
method of how to incorporate them into statistical 
model which are shown as follows. 
1. CPN surname list (including 476 items) and 
JPN surnames list (including 9189 items): Only 
those characters in the surname list can trigger per-
son name recognition. 
2. RPN and EPN characters lists: Only those 
consecutive characters in the transliterated charac-
ter list form a candidate transliterated name. 
3. Entity Length Restriction: Person name can-
not span any punctuation and the length of CN 
cannot exceed 8 characters while the length of TN 
is unrestrained. 
4. Location keyword list (including 607 items):  
If the word belongs to the list, 2~6 words before 
the salient word are accepted as candidate LNs. 
5. General word list (such as verbs and preposi-
tions): Words in the list usually is followed by a 
location name, such as "?/at", "?/go". If the cur-
rent word is in the list, 2~6 words following it are 
accepted as candidate LNs. 
6. ALN name list (including 407 items): If the 
current word belongs to the list, we accept it as a 
candidate ALN. 
7. Organization keyword list (including 3129 
items): If the current word is in organization key-
word list, 2~6 words before keywords are accepted 
as the candidate ONs. 
8. An organization name template list: We 
mainly use organization name templates to recog-
nize the missed nested ONs in the statistical model. 
Some of these templates are as follows: 
ON-->LN D* OrgKeyWord 
ON-->PN D* OrgKeyWord 
ON-->ON OrgKeyWord 
D and OrgKeyWord denote words in the middle 
of ONs and ONs keywords. D* means repeating 
zero or more times. 
5 Back-off Model to Smooth 
Data sparseness problem still exists. As some pa-
rameters were never observed in training corpus, 
the model will back off to a less powerful model. 
The escape probability[Black, et al 1998] was ad-
opted to smooth the statistical model shown as (15). 
00N11N2N1N
1N1NN1N1N
^
p)W(p)WWW(p
)WWW(p)WWW(p
???
?
+++
+=
LL
LL  (15) 
where NN e1? = , Ni0,e)e1(?
N
1ik
kii <<=
+=
? , and ei 
is the escape probability which can be estimated by 
equation (16). 
)WWW(f
)WWW(q
e
1N21
1N21
N L
L=                           (16) 
q(w1w2?wN-1) in (16) denotes the number of dif-
ferent symbol wN that have directly followed the 
word sequence w1w2?wN-1. 
6 Experiments 
In this chapter, we will conduct experiments to 
answer the following questions.  
Will the Hybrid Model be more effective than 
the Word Model and the POS Model? To answer 
this question, we will compare the performances of 
models with different parameter ? and find the best 
value of ? in equation (7). 
Will the conclusion from different testing sets be 
consistent? To answer this question, we evaluate 
models on the MET-2 test data and compare the 
performances of the Word Model, the POS Model 
and the Hybrid Model. 
Will the performance be improved significantly 
after combining human knowledge? To answer this 
question, we compare two models with and with-
out human knowledge.  
In our evaluation, only NEs with correct 
boundaries and correct categories are considered as 
the correct recognition. We conduct evaluations in 
terms of precision, recall and F-Measure. Note that 
PNs in experiments includes all kinds of PNs and 
LNs include ALNs. 
6.1 Will the Hybrid Model be More Effective 
Than the Word Model and POS Model? 
The parameter ? in equation (7) denotes the balanc-
ing factor of the Word Model and the POS Model. 
The larger ?, the larger contribution of the POS 
Model. The smaller ?, the larger contribution of the 
Word Model. So the task of this experiment is to 
find the best value of ?. In this experiment, the 
training corpus is from five-month's People's Daily 
tagged with NER tags and the testing set is from 
one-month's People's Daily. 
With the change of ?, the performances of rec-
ognizing PNs are shown in Fig.1.  
Note that the left, middle and right point in ab-
scissa respectively denote the performance of the 
431
Word Model, the Hybrid Model and the POS 
Model. 
0 1.6 3.2 4.8 6.4 8 9.6
0.88
0.89
0.9
0.91
0.92
0.93
0.94
0.95
0.96
Lamda
%
Precision
Recall
F?Measure
 
Fig.1 Performance of Recognizing LNs Impacted 
by ? 
From Fig.1, we can find that the performances 
of recognizing PNs are improved with the increas-
ing of ? in the beginning stage but decline in the 
ending. This experiment shows that the Word 
Model and the POS Model can overcome their dis-
advantages, and it is a feasible approach to inte-
grate the Word Model and the POS Model in order 
to improve the performance PNs recognition.  
With the change of ?, the performances of rec-
ognizing LNs are shown in Fig.2. 
0 1.6 3.2 4.8 6.4 8 9.6
0.9
0.91
0.92
0.93
0.94
Lamda
%
Precision
Recall
F?Measure
 
Fig.2 Performance of Recognizing LNs Impacted 
by ? 
As the Fig.2 shows, the precision and recall of 
LNs are improved with the increasing of ? and de-
creased in the later stage. This phenomenon also 
proves that the Hybrid Model is better for recog-
nizing LN than either the Word Model or the POS 
Model. 
Similarly, with the change of ?, the perform-
ances of recognizing ONs are shown in Fig.3. 
 
0 1.6 3.2 4.8 6.4 8 9.6
0.7
0.75
0.8
0.85
Lamda
%
Precision
Recall
F?Measure
 
Fig.3 Performance of Recognizing LNs Impacted 
by ? 
Comparing Fig.3 with Fig.1 and Fig.2, we find 
that the POS Model has different impact on recog-
nizing ONs from that on recognizing PNs and LNs. 
Especially, the POS Model has obvious side-effect 
on the recall. We speculate that the reasons may be 
that the probability of generating POS sequence by 
POS entity model is lower than that by POS con-
text model. 
According to Fig.1~Fig.3, we choose the best 
value ? = 2.8. And the performances of different 
models are shown in Table 2 in detail. 
 P(%) R(%) F(%) 
PN 94.06 95.21 94.63 
LN 93.98 93.48 93.73 
Hybrid 
Model 
(?= 2.8) 
ON 84.69 86.86 85.76 
 
PN 88.24 90.11 89.16 
LN 91.50 93.17 92.32 
Word 
Model 
ON 78.85 88.77 83.52 
    
PN 93.44 95.11 94.27 
LN 89.97 92.20 91.07 
POS 
Model 
ON 80.90 69.29  74.65 
Table 2 Performance of the Hybrid Model, the 
Word Model and the POS Model 
From Table 2, we find that the F-Measures of 
the Hybrid Model for PN, LN, ON are improved 
by 5.4%, 1.4%, 2.2% respectively in comparison 
with the Word Model, and these F-Measures are 
improved by 0.4%, 2.7%, 11.1% respectively in 
comparison with the POS Model. 
432
Conclusion 1: The experimental results validate 
our idea that the Hybrid Model can improve the 
performance of both the Word Model and the POS 
Model. However, the improvements for PN, LN 
and ON are different. That is, the POS Model has 
obvious side-effect on the recall of ON recognition 
at all times, while the recalls for PN and ON rec-
ognition are improved in the beginning but de-
creased in the ending with the increasing of ?. 
6.2 Will the Conclusion from Different Test-
ing Sets be Consistent? 
We also conduct experiments on the MET-2 test-
ing corpus to validate our conclusion from Exp.1, 
that is, the Hybrid Model could achieve better per-
formance than either the Word Model or the POS 
Model alone. The experimental results (F-Measure) 
on MET-2 are shown in Table 3. 
Model Word Model 
Hybrid 
Model 
POS 
Model 
PN 75.21% 80.77% 76.61% 
LN 89.78% 90.95% 89.81% 
ON 76.30% 80.21% 76.83% 
Table 3 F-Measure on MET-2 test corpus  
Comparing Table 3 with Table 2, we find that 
the performances of models on MET-2 are not as 
good as that on People Daily's testing data. The 
main reason lies in that the NE definitions in Peo-
ple Daily's corpus are different from that in MET-2. 
However, Table 3 can still validate our conclude 1, 
that is, the Hybrid Model is better than both the 
Word Model and the POS Model. For example, the 
F-Measures of the Hybrid Model for PN, LN and 
ON are improved by 5.6%, 1.2% and 3.9% respec-
tively in comparison with the Word Model, and 
these F-Measures are improved by 4.2%, 3.1% and 
3.4% respectively in comparison with the POS 
Model. 
Conclusion 2: Though the performances of the 
Hybrid Model on MET-2 are not as good as that 
on People's Daily corpus, the experimental results 
also support conclusion 1, i.e. the Hybrid Model 
which combining the Word Model with the POS 
Model can achieve better performance than either 
the Word Model or the POS Model. 
6.3 Will the Performance be Improved Sig-
nificantly after Incorporating Human 
Knowledge?  
One of our ideas in this paper is that human 
knowledge can not only reduce the search space, 
but also improve the performance through avoiding 
generating the noise NEs. This experiment will be 
conducted to validate this idea. Table 4 shows the 
performances of models with and without human 
knowledge.  
 P(%) R(%) F(%) 
PN 91.81 70.65 79.85 
LN 79.47 88.83 83.89 Model I 
ON 64.95 80.63 71.95 
 
PN 94.06 95.21 94.63 
LN 93.98 93.48 93.73 Model II 
ON 84.69 86.86 85.76 
Table 4 Performances Impacted by Human Know-
ledge 
From Table 4, we find that F-Measure of model 
with human knowledge (Model II) is improved by 
14.8%?9.8%?13.8% for PN, LN and ON respec-
tively compared with that of the model without 
human knowledge (Model I). 
Conclusion 3: From this experiment, we learn 
that human knowledge can not only reduce the 
search space, but also significantly improve the 
performance of pure statistical model. 
7 Conclusion 
In this paper, we propose a hybrid Chinese NER 
model which combines multiple features. The main 
contributions are as follows: ? The proposed Hy-
brid Model emphasizes on integrating coarse parti-
cle feature (POS Model) with fine particle feature 
(Word Model), so that it can overcome the disad-
vantages of each other; ? In order to reduce the 
search space and improve the efficiency of model, 
we incorporate heuristic human knowledge into 
statistical model, which could increase the per-
formance of NER significantly; ? For capturing 
intrinsic features in different types of entities, we 
design several sub-models for different entities. 
Especially, we divide transliterated person name 
into three sub-classes according to their characters 
set, that is, CPN JPN, RPN and EPN. 
There is a lack of effective recognition strategy 
for abbreviated ONs such as ????(Kunming 
Machine Tool Co.,Ltd), ? ? ? ? (Phoenix 
Photonics Ltd) in this paper. And most of mis-
433
recognized ONs in current system belong to them. 
So in the future work, we will be focusing more on 
recognizing abbreviated ONs. 
8 Acknowledgements 
This research is carried out as part of the coopera-
tive project with Fujitsu R&D Center Co., Ltd. We 
would like to thank Yingju Xia, Fumihito Nisino 
for helpful feedback in the process of developing 
and implementing. This work was supported by the 
Natural Sciences Foundation of China under grant 
No. 60372016 and 60272041, the Natural Science 
Foundation of Beijing under grant No. 4052027. 
References  
N.A. Chinchor: Overview of MUC-7/MET-2. In: Pro-
ceedings of the Seventh Message Understanding 
Conference (MUC-7), April. (1998). 
Youzheng Wu, Jun Zhao, Bo Xu: Chinese Named En-
tity Recognition Combining Statistical Model with 
Human Knowledge. In: The Workshop attached with 
41st ACL for Multilingual and Mix-language Named 
Entity Recognition, Sappora, Japan. (2003) 65-72. 
Endong Xun, Changning Huang, Ming Zhou: A Unified 
Statistical Model for the Identification of English 
BaseNP. In: Proceedings of ACL-2000, Hong Kong. 
(2000). 
Jian Sun, Jianfeng Gao, Lei Zhang, Ming Zhou, 
Changning Huang: Chinese Named Entity Identifica-
tion Using Class-based Language Model. In: 
COLING 2002. Taipei, August 24-25. (2002). 
Huaping Zhang, Qun Liu, Hongkui Yu, Xueqi Cheng, 
Shuo Bai: Chinese Named Entity Recognition Using 
Role Model. In: the International Journal of Compu-
tational Linguistics and Chinese Language Process-
ing, vol.8, No.2. (2003) 29-60. 
D.M. Bikel, Scott Miller, Richard Schwartz, Ralph 
Weischedel: Nymble: a High-Performance Learning 
Name-finder. In: Fifth Conference on Applied Natu-
ral Language Processing, (published by ACL). (1997) 
194-201. 
Borthwick .A: A Maximum Entropy Approach to 
Named Entity Recognition. PhD Dissertation. (1999). 
Mikheev A., Grover C. and Moens M: Description of 
the LTG System Used for MUC-7. In: Proceedings of 
7th Message Understanding Conference (MUC-7), 
1998. 
Sekine S., Grishman R. and Shinou H: A decision tree 
method for finding and classifying names in Japanese 
texts. In: Proceedings of the Sixth Workshop on Very 
Large Corpora, Canada, 1998. 
Aberdeen, John, et al MITRE: Description of the 
ALEMBIC System Used for MUC-6. In: Proceedings 
of the Sixth Message Understanding Conference 
(MUC-6), November. (1995) 141-155. 
Ralph Grishman and Beth Sundheim: Design of the 
MUC-6 evaluation. In: 6th Message Understanding 
Conference, Columbia, MD. (1995) 
Krupka, G. R. and Hausman, K. IsoQuest: Inc.: Descrip-
tion of the NetOwl TM Extractor System as Used for 
MUC-7. In Proceedings of the MUC-7, 1998. 
Black, W.J.; Rinaldi, F, Mowart, D: FACILE: Descrip-
tion of the NE System Used for MUC-7. In Proceed-
ings of the MUC-7, 1998. 
Michael Collins, Yoram Singer: Unsupervised models 
for named entity classification. In Proceedings of 
EMNLP. (1999) 
Michael Collins: Ranking Algorithms for Named Entity 
Extraction: Boosting and the Voted Perceptron. In: 
Proceeding of ACL-2002. (2002) 489-496. 
S.Y.Yu, et al Description of the Kent Ridge Digital 
Labs System Used for MUC-7. In: Proceedings of the 
Seventh Message Understanding Conference, 1998. 
H.H. Chen, et al Description of the NTU System Used 
for MET2. In: Proceedings of the Seventh Message 
Understanding Conference. 
Tat-Seng Chua, et al Learning Pattern Rules for Chi-
nese Named Entity Extraction. In: Proceedings of 
AAAI'02. (2002) 
Maosong Sun, et al Identifying Chinese Names in Un-
restricted Texts. Journal of Chinese Information 
Processing. (1994). 
Jiahen Zheng, Xin Li, Hongye Tan: The Research of 
Chinese Names Recognition Methods Based on Cor-
pus. In: Journal of Chinese Information Processing. 
Vol.14 No.1. (2000). 
CoNLL. http://cnts.uia.ac.be/conll2004/ 
IEER. http://www.nist.gov/speech/tests/ie-er/er99/er99. 
htm 
ACE. http://www.itl.nist.gov/iad/894.01/tests/ace/ 
 
434
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 542 ? 552, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
A Lexicon-Constrained Character Model for Chinese 
Morphological Analysis 
Yao Meng, Hao Yu, and Fumihito Nishino 
Fujitsu R&D Center Co., Ltd, Room B1003, Eagle Run Plaza, No. 26 Xiaoyun Road,  
Chaoyang District, Bejing, 100016, P. R. China 
{Mengyao, Yu, Nishino}@frdc.fujitsu.com 
Abstract. This paper proposes a lexicon-constrained character model that com-
bines both word and character features to solve complicated issues in Chinese 
morphological analysis. A Chinese character-based model constrained by a 
lexicon is built to acquire word building rules. Each character in a Chinese sen-
tence is assigned a tag by the proposed model. The word segmentation and part-
of-speech tagging results are then generated based on the character tags. The 
proposed method solves such problems as unknown word identification, data 
sparseness, and estimation bias in an integrated, unified framework. Preliminary 
experiments indicate that the proposed method outperforms the best SIGHAN 
word segmentation systems in the open track on 3 out of the 4 test corpora. Ad-
ditionally, our method can be conveniently integrated with any other Chinese 
morphological systems as a post-processing module leading to significant im-
provement in performance. 
1   Introduction 
Chinese morphological analysis is a fundamental problem that has been studied ex-
tensively [1], [2], [3], [4], [5], [6], [7], [8]. Researchers make use of word or character 
features to cope with this problem. However, neither of them seems completely satis-
factory. 
In general, a simple word-based approach can achieve about 90% accuracy for 
segmentation with a medium-size dictionary. However, since no dictionary includes 
every Chinese word, the unknown word (or Out Of Vocabulary, OOV) problem [9], 
[10] can severely affect the performance of word-based approaches. Furthermore, 
word-based models have an estimation bias when faced with segmentation candidates 
with different numbers of words. For example, in the standard hidden Markov model, 
the best result, ?
=
?
==
n
i
iiii
TT
tttptwpWTpT
1
11
* )...|()|(maxarg)|( maxarg , is related to the number of 
the words in the segmentation candidates. As such, a candidate with fewer words is 
preferred over those with more words in the selection process. Therefore, most word-
based models are likely to fail when a combinational ambiguity1 sequence is separated 
into multiple words.  
                                                          
1
  A typical segmentation ambiguity, it refers to a situation in which the same Chinese sequence 
may be one word or several words in different contexts. 
 A Lexicon-Constrained Character Model for Chinese Morphological Analysis 543 
Compared with Chinese words, Chinese characters are relatively less unambigu-
ous. The Chinese character set is very limited. Therefore, unknown characters occur 
rarely in a sentence. The grammatical advantages of characters have inspired re-
searchers to adopt character features in Chinese morphology and parsing [5], [6], [11], 
[12]. However, it is difficult to incorporate necessary word features, such as the form 
of a Chinese word and its fixed part-of-speech tags, in most character-based ap-
proaches. For this reason, character-based approaches have not achieved satisfactory 
performance in large-scale open tests.  
In this paper, we propose a lexicon-constrained character model to combine the 
merits of both approaches. We explore how to capture the Chinese word building 
rules using a statistical method, which reflects the regularities in the word formation 
process. First, a character hidden Markov method assigns the candidate tags to each 
character. Next, a large-size word list combined with linguistic information is used to 
filter out erroneous candidates. Finally, segmentation and part-of-speech tagging for 
the sentence are provided based on the character tags.  
The proposed model solves the problems of unknown word detection, word seg-
mentation and part-of-speech tagging using both word and character features. Addi-
tionally, our module is a post-processing module, which can be coupled to any exist-
ing Chinese morphological system; and it can readily recall some of the unknown 
words omitted by the system, and as a result, significantly improves the overall per-
formance. Evaluations of the proposed system on SIGHAN open test sets indicate that 
our method outperforms the best bakeoff results on 3 test sets, and ranks 2nd in the 4th 
test set [9].  
2   A Lexicon-Constrained Character Model for Chinese 
Morphology 
2.1   An Elementary Model to Describe Chinese Word Building Rules  
It is recognized that there are some regularities in the process of forming words from 
Chinese characters. This in general can be captured by word building rules. In this 
paper, we explore a statistical model to acquire such rules. The following are some 
definitions used in the proposed model. 
[Def. 1] character position feature 
We use four notations to denote the position of a character in a Chinese word. ?F? 
means the first character of the word, ?L? the last character, ?M? is a character within 
it and ?S? the word itself.  
[Def. 2] character tag set 
It is the product of the set of character position features and the set of part-of-
speech tags.  
Character tag set ={xy| setwordx  POS ? , },,,{ LMFSy ? }, where, x denotes one 
part-of-speech (POS) tag and y a character position feature. Together they are used to 
define the rules of Chinese word formation.  
[Def. 3] character tagging 
Given a Chinese sentence; character tagging is the process for assigning a character 
tag to each character in the sentence.  
544 Y. Meng, H. Yu, and F. Nishino 
Word building rules are acquired based on the relation between the character and 
the corresponding character tag. Word segmentation and part-of-speech tagging can 
be achieved easily based on the result of character tagging. For example, a character 
with ?xS? is a single character word with the part-of-speech tag ?x?; a character se-
quence starting with ?xF? and ending with ?xL? is a multiple character word with the 
part-of-speech tag ?x?.   
The elementary model adopts the character bi-gram hidden Markov model. In hid-
den Markov model, given the sentence,
nn ccccs 121 ...: ? , and character tagging result 
nn xyxyxyxyt 121 ...: ? , the probability of result t of s is estimated as: 
?
=
??
?=
ni
iiiii xycpxyxyxypstp
,1
12 )|() |()|(  (1) 
The best character tagging result for the sentences is given by equation (2):  
?
=
??
?=
ni
iiiii
t
xycpxyxyxypt
,1
12
* )|()|(maxarg  (2) 
We used the People's Daily Corpus of 1998 [13] to train this model. Also we 
adopted a 100,000-word dictionary listing all valid part-of-speech tags for each Chi-
nese word in the training phase to solve the data sparseness problem. The training data 
are converted into character tagging data through the following steps: a single charac-
ter word with ?x? is converted into the character marked with tag ?xS?; a two-character 
word with ?x? is converted into a first character with ?xF? and a second character with 
?xL?; a word with more than two characters with ?x? are converted into a first character 
with ?xF?, middle characters with ?xM? and last character with ?xL?. We adopt the POS 
tag set from the People's Daily Corpus, which consists of 46 tags. Taking into account 
of the four position features, the final character tag set is comprised of 184 tags. 
The emitted probability and transition probability of the model are estimated by the 
maximum likelihood method. The emitted probability is counted by the training Cor-
pus and the dictionary, where the Chinese words in the dictionary are counted one 
time. The transition probability is trained from the training Corpus only. 
2.2   An Improved Character-Based Model Using Lexicon Constraints 
We tested the above model based on the SIGHAN open test set [9]. The average pre-
cision for word segmentation was more than 88%. This means that most of the word 
building rules in Chinese have been obtained by the elementary model. However, the 
performance was relatively inferior to other word segmentation systems. It indicated 
that the model needed more features to learn word building rules. In error analysis, we 
found that the elementary model was so flexible that it produced many pseudo-words 
and invalid part-of-speech tags. In practice, a Chinese word is a stable sequence of 
Chinese characters, whose formation and part-of-speech tags are fixed by long-term 
usage. It seemed that only character position and meaning cannot describe a word 
building rule effectively.  
We also observed that word segmentation systems based on a simple dictionary 
matching algorithm and a few linguistic rules could achieve about 90% accuracy [14]. 
This suggested that a lexicon may have contribution to word building rules. Thus, we 
tried to incorporate a lexicon to the model to improve the performance. 
 A Lexicon-Constrained Character Model for Chinese Morphological Analysis 545 
The major errors in the elementary model were pseudo words and invalid part-of-
speech (POS) tags. We proposed two constraints based on the lexicon to deal with 
these errors: 
1. If a possible word produced from the elementary model is in the word-
dictionary, the character tag of the characters forming this word should be 
consistent with the part-of-speech tag of the word in the dictionary.  
2. If a possible word produced is not in the dictionary, it must include one or 
more single characters, and none of which may be subsumed by any word in 
the dictionary in the current context. 
The first constraint eliminates invalid character tags. For example, the character  
?? ? has six character tags: ?aF? (first in adjective) , ?dF? (first in adverb), ?nF? (first 
in noun), ?nrF? (first in person name), ?tF? (first in time), and ?vF? (first in verb). The 
character ??? has five character tags: ?dL?, ?nL?, ?nrL?, ?tL?, and ?vL?. The combina-
tion of the two characters produces the possible word ????, which includes five 
possible word part-of-speech tags: ?d?, ?n?, ?nr?, ?t?, and ?v? based on these character 
tags. But ???? is a word in the dictionary, which only has two valid part-of-speech 
tags, namely, ?time? and ?person name?. Obviously, the part-of-speech tags: ?d?, ?n? 
and ?v? of ???? are invalid. Accordingly, the tags ?aF?, ?dF?, ?nF? , ?vF? on ??? and 
the tags ?dL?, ?nL?, ?vL? on ??? are also invalid. So they should be pruned from the 
candidates of the character tagging.  
The second constraint prunes pseudo words in the elementary model. Many studies 
in dictionary-based segmentation treat unknown words as sequences of single charac-
ters [1], [14]. The second constraint ensures that the new word produced by the ele-
mentary model must have one or more ?unattached? single characters (not subsumed 
by any other words). For example, the sequence ?????? (program error) will 
combine the pseudo word ???? because of the tag ?nF? on ??? and the tag ?nL? on 
???. The second constraint will prune ???? since ???? (program) and ???? 
(error) are already in the dictionary and there is no ?unattached? single character in it. 
Accordingly, the tag ?nF? on ??? and the tag ?nL? on ??? will be deleted from the 
candidates of character tagging.  
The following experiments show the lexicon-based constraints are very effective in 
eliminating error cases. The elementary model faces an average of 9.3 character tags 
for each character. The constraints will prune 70% of these error tags from it. As a 
result, the performance of character tagging is improved.  
It is worth noting that the lexicon in the elementary model cannot distort the prob-
ability of the character tagging results in the model. The pruned cases are invalid 
cases which cannot occur in the training data because all the words and POS tags in 
the training data are valid. Thus, the model built from the training data is not affected 
by the pruning process.  
2.3   Case Study 
In this subsection, we illustrate the advantages of the proposed method for Chinese 
morphology with an example.  
546 Y. Meng, H. Yu, and F. Nishino 
Example: ?????????????? 
(Xiaoming will analyze the program errors tomorrow).  
Where, ???? is an unknown word (person name), and the sequence ???? is a 
combinational ambiguity (either ???? (put up with) or ???+ ??? (will)). Here is 
how our approach works. 
Step 1: List all the character tags for each character. Figure 1 shows the character 
tags in the sequence ?????? . 
?
aF dF nF nrF nM nrM nsM qM vM aL dL vL aS 
?
aF dF nF nrF vF tF nM lM tM aL dL nrL aS 
?
aF dF nF nrF vF tF nM lM tM aL dL nrL aS 
?
nF tF nrM dL nL nrL tL vL
 
Fig. 1. Candidates for the sequence ?????? 
In this step we are able to find possible unknown words based on character position 
features. For example, the character tags in ?????? combine four possible un-
known words: ????, ?????, ????? , and ??????.   
Step 2: Prune the invalid candidates using constraints. 
The first constraint prunes some invalid character tags. For example, ???? can be 
either an adverb (d) or a personal name (nr); ???? is a time (t) word. The other part-
of-speech tags of these two words will be deleted. With the second constraint, we can 
delete ????? because ???? and ???? are words in the dictionary. However, ??
?? , ?????, and ?????? will be kept because ??? is a ?unattached? single 
character. The remaining candidates are shown in figure 2.  
?
aF dF nF nrF nM nrM nsM qM vM aL dL vL aS
?
 dF  nrF   nM lM tM    aS
?
     tF nM lM tM  dL nrL aS
?
nF tF nrM   nrL tL  
 
Fig. 2. Remaining Candidates for the sequence ?????? 
Step 3: Choose the best character tagging result based on the proposed character 
hidden Markov model.  
The best character tagging result is chosen using equation 2 in Section 2.1. The 
ambiguities in segmentation and word POS tagging are solved in the character tag-
ging process.  
Consider the combinational ambiguity ???? in the following 2 candidates: 
Candidate 1: ???/nr ??/t ?/d ?/d ??/n ??/n ??/v ??/v? 
Candidate 2: ???/nr ??/t ??/v ??/n ??/n ??/v ??/v? 
 A Lexicon-Constrained Character Model for Chinese Morphological Analysis 547 
In word-based linear model, the erroneous candidate 2 will be prior to the correct 
candidate 1 since the model counts 9 nodes in candidate 1 but 8 nodes in candidate 2. 
However, there is no such bias in the character model because the number of charac-
ters does not change. The combinational ambiguity ???? will be denoted as ??/dS 
?/dS? or ??/vF ?/vL?. The number of nodes in all candidates of character tagging is 
the same.  
At last, the correct result ??/nrF ?/nrL ?/tF ?/tL ?/dS ?/dS?/nF ?/nL ?/nF 
?/nL ?/vF ?/vL ?/vF ?/vL? is selected, and the corresponding morphological result 
is: ???/nr ??/t ?/d ?/d ??/n ??/n ?? /v ??/v ?.  
The above steps show the proposed approach solves the various issues related to 
Chinese morphology by a concise character tagging process where word building is 
revealed. 
3   Experiments and Discussion 
We evaluated the proposed character method using the SIGHAN Backoff data, i.e. the 
one-month People's Daily Corpus of 1998, and the first version of Penn Chinese Tree-
bank [15]. We compared our approach against two state-of-the-art systems: one is 
based on a bi-gram word segmentation model [7], and the other based on a word-
based hidden Markov model [3]. For simplicity, we only considered three kinds of 
unknown words (personal name, location name, and organization name) in the all 
methods.  
The same corpus and word-dictionary were used to train the above three systems. 
The training data set was the 5-month People's Daily Corpus of 1998, which con-
tained approximately 6,300,000 words and 46 word part-of-speech tags. The system 
dictionary contained 100,000 words and the valid part-of-speech tag(s) of each word.  
On average, there were 1.3 part-of-speech tags for a word in the dictionary.  
In the following, chr-HMM refers to the proposed elementary model; chr-
HMM+Dic refers to the character model improved by integrating linguistic informa-
tion. W-Bigram is the word-based bi-gram system, and W-HMM is the word-based 
hidden Markov system. 
3.1   Morphological Experimental Results 
We examined the performance of our model in comparison against W-Bigram and W-
HMM. Table 1 compares the segmentation performance of our model against that of 
other models. Table 2 shows the accuracy in unknown word identification. Table 3 
illustrates the performance of the part-of-speech tagging. The experiments in Table 1 
and Table 2 were examined using the SIGHAN open test corpora. The experiments in 
Table 3 were performed again on the one-month People's Daily Corpus (PD corpus) 
and 4,000 sentences in the Penn Chinese Treebank (Penn CTB). We only examined 4 
major word categories in the Penn Chinese Treebank due to inconsistency in the part-
of-speech tag sets between the two corpora. The 4 major word categories were: noun 
(shown as NN, NR in Penn CTB; n, nr, ns, nz in PD corpus), verb (VV in Penn CTB; 
v, vd, vn in PD corpus), adjective (JJ in Penn CTB; a, ad, an in PD corpus) and adverb 
(AD in Penn CTB; d in PD corpus). 
548 Y. Meng, H. Yu, and F. Nishino 
Segmentation and word POS tagging performance is measured in precision (P%), 
recall (R%) and F-score (F). Unknown words (NW) are those words not found in our 
word-dictionary, which include named entities and other new words. The unknown 
word rate (NW-Rate), the precision on unknown words (NW-Precision) and recall on 
total unknown words (NW-Recall) are given by:   
NW-Rate= identifiedNW  of  #  total
rdsunknown wo of  #
 NW-Precision = identifiedNW  of # total
rdsunknown wo  validof #
 
NW-Recall = data in testingNW  of # total
rdunknown wo  validof #
 
Table 1 shows that the above three systems achieve similar performances on the 
PK testing corpus. All of them were trained by the People's Daily corpus. For this 
reason, their performances were similar when the testing data had similar styles. But 
for other texts, the proposed character model performed much better than the word-
based models in both recall and precision. This indicated that our approach performed 
better for unseen data.  
Table 2 shows that our method for unknown word identification also outperforms 
the word-based method. We notice that word-based approaches and character-based 
approaches have similar precision on unknown word identification, however word-
based approaches have much lower recall than character-based ones. The main reason 
for this is that word-based systems focus only on unknown words with proper word 
structures, but cannot recognize newly generated words, rare words, and other new 
words unlisted in the dictionary. A very high proportion of these types of unknown 
word in the SIGHAN testing data affects the recall of the word-based methods on 
unknown words. The experiments reveal that our method could effectively identify all 
kinds of new words. This is because our model has defined word building rules for all 
kinds of words. 
Without a widely recognized testing standard, it is very hard to evaluate the per-
formance on part-of-speech tagging. The results in Penn Chinese Treebank was better 
than that in the People's Daily Corpus since we examined all 42 POS tags in the Peo-
ple's Daily Corpus, but we only tested four major POS tags in Penn Chinese Tree-
bank. Our approach is better than the word-based method for two test data sets. How-
ever, we could not conclude that our method was superior to the word-based method 
because of the limited testing approaches and testing data. A thorough empirical com-
parison among different approaches should be investigated in the future.  
Table 1. Comparison of word segmentation based on SIGHAN open test sets 
 PK CTB HK AS 
 R%/ P% F R%/ P% F R%/ P% F R%/ P% F 
Chr-HMM 91.9/91.8 91.8 86.9/87.3 87.1 87.7/86.7 87.2 89.9/89.1 89.5 
Chr-HMM+Dic 95.9/96.7 96.3 92.7/93.5 93.1 91.1/91.9 91.5 92.3/93.9 93.1 
W-Bigram 94.7/95.4 95.1 87.4/86.8 87.1 88.7/83.7 86.3 87.9/85.1 86.5 
W-HMM 94.6/95.1 94.9 88.6/89.2 88.9 90.7/89.1 89.9 90.7/87.2 89.0 
Rank 1 in SIG 96.3/95.6 96.0 91.6/90.7 91.2 95.8/95.4 95.6 91.5/89.4 90.5 
Rank 2 in SIG 96.3/94.3 95.3 91.1/89.1 90.1 90.9/86.3 88.6 89.2/85.3 87.3 
 A Lexicon-Constrained Character Model for Chinese Morphological Analysis 549 
Table 2. Accuracy of unknown word identification for SIGHAN open test sets 
 PK CTB HK AS 
Chr-HMM UWR% P% R% UWR% P% R% UWR% P% R% UWR% P% R% 
Chr-HMM+Dic 2.3 56.2 54.8 10.4 68.8 64.4 9.7 61.4 58.4 8 65.4 62.9 
W-Bigram 2.3 54.7 53.6 10.4 53.9 23.8 9.7 53.0 29.6 8 64.6 35.3 
W-HMM 2.3 58.1 51.3 10.4 68.3 37.2 9.7 62.3 40.7 8 68.4 41.1 
Table 3. Comparison of word part-of-speech tagging 
 People Daily Penn CTB 
 P% R% F-score P% R% F-score 
Chr-HMM 82.4% 82.5% 82.5 89.7% 88.5% 89.1 
Chr-HMM+Dic 89.3 87.8 88.6 92.5 91.5 92.0 
W-HMM 86.2% 85.4% 85.7 91.1% 90.8% 91.0 
From Table 1 and Table 3, we notice that chr-HMM achieved 88% accuracy in 
word segmentation and 80% in part-of-speech tagging without a word-dictionary. 
Chr-HMM is a state-of-the-art Chinese morphology system without a word-
dictionary. Its performance is comparable to some dictionary-based approaches (e.g., 
forward-maximum). This result indicates that our model has effectively captured most 
of the Chinese word building rules.  
The results also show that chr-HMM+Dic outperformed the best SIGHAN word 
segmentation system on 3 out of the 4 SIGHAN open track test corpora, and achieved 
top 2 in the case of HK testing corpus.  
3.2   Incorporation with Other Systems  
The advantage of the proposed model is proficiency in describing word building rules 
and since many existing NLP application systems are weak in identifying new words, 
it is intuitive to integrate our model to existing systems and serves as a post-
processing subsystem. In this subsection, we show how existing word segmentation 
systems could be improved using chr-HMM.  
Given a segmentation result, we assume that unidentified new words may be a se-
quence of unattached characters. That is, all multiple-character words in the given 
result are considered correct, while single words, which might include unidentified 
new words will be rechecked by the chr-HMM. The entire process involves 3 steps: 
1. Only character tags that are consistent with the position of the character in the 
word are listed for multi-character words.  
2. The unattached characters are tagged with all possible character tags. In this 
way, the original segmentation result is converted into a group of character 
tagging candidates.  
3. We then input these character tagging candidates into the chr-HMM to select 
the best one.  
550 Y. Meng, H. Yu, and F. Nishino 
Consider an original result:  
?? [?  ?  ?  ?  ?] ??  [?  ? ] (Jordan bounced back strongly from the 
bottom yesterday) 
The parts in brackets are the sequence of single characters where the new words 
may appear. The chr-HMM will list all possible character tags for these ?unattached? 
characters. The parts outside the brackets are multiple-character words identified by 
the original system. They are assumed correct and maintain also positional informa-
tion. Only the character tags, which are consistent with the positions of the character 
in the word are listed. The character tagging candidates for the above sample is given 
in Figure 3: 
?  ? ?  
vL ? nL nsL ? 
 
tL vL nM nrL vM
  
? nL lM nsF nL 
 
nM ?
nrF nrL tL nM vF nrF nM vF vL vF vL
nF nL nL aM pS nF vF nF nL aF nL
aF aL tS tS dS nS nS aF aL aS vS
? ? ? ? ? ? ? ? ? ? ?
 
Fig. 3. Character tagging candidates for rechecking 
Chr-HMM is then applied to the character tagging candidates and the best charac-
ter tagging selected based on the probability of the candidates is output as the result. 
In this example, the result is: ??? (Jordan) ?? (yesterday) ? (from) ?? (earth) 
?? (strongly) ?? (bound)?. The three missing new words in the original system 
are identified by this post-processing subsystem.  
We re-assigned the word segmentation results for all participants who have given 
permission to release data from the SIGHAN site (available for download from 
http://www.sighan.org/bakeoff2003 ). Table 4 enlists the performance of SIGHAN 
open test with and without chr-HMM. The participant numbers correspond to the sites 
listed in [9].  
Table 4. Comparison of results with and without chr-HMM 
Corpus Site  R% P% F 
AS 03 Before After 
89.2 
90.8 
85.3 
92.0 
87.2 
91.4 
01 Before After 
88.7 
90.1 
87.6 
91.8 
88.1 
90.9 
03 Before After 
85.3 
86.4 
80.6 
87.8 
82.9 
87.1 CTB 
10 Before After 
91.1 
91.0 
89.1 
93.5 
90.1 
92.3 
HK 03 Before After 
90.9 
89.4 
86.3 
91.0 
88.6 
90.2 
03 Before After 
94.1 
94.4 
91.1 
95.3 
92.5 
94.9 PK 
10 Before After 
96.3 
95.6 
95.6 
97.7 
95.9 
96.7 
 A Lexicon-Constrained Character Model for Chinese Morphological Analysis 551 
From Table 4, it is obvious that word segmentation precision increases signifi-
cantly, and at the same time, the corresponding recall remains the same or slightly 
declined. This implies that the chr-HMM retains the correct words by the original 
system and concurrently decreases significantly its errors.  
4   Related Work  
Although character features are very important in Chinese morphology, research in 
character-based approach is unpopular.  Chooi-Ling Goh et al [16], Jianfeng Gao et 
al. [8] and Huaping Zhang [3] adopted character information to handle unknown 
words; X. Luo [11], Yao Meng [12] and Shengfen Luo [17] each presented character-
based parsing models for Chinese parsing or new-word extraction. T. Nakagawa used 
word-level information and character-level information for word segmentation [6]. 
Hwee Tou Ng et al [5] investigated word-based and character-based approaches and 
proposed a maximum entropy character-based POS analyzer. Although the character 
tags proposed in this paper are essentially similar to some of the previous work men-
tioned above, here our focus is to integrate various word features with the character-
based model in such a way that the probability of the model is undistorted. The pro-
posed model is effective in acquiring word building rules. To our knowledge, our 
work is the first character-based approach, which outperforms the word-based ap-
proaches for SIGHAN open test. Also, our approach is versatile and can be easily 
integrated with existing morphological systems to achieve improved performance.  
5   Conclusion and Future Works  
A lexicon-constrained character model is proposed to capture word building rules 
using word features and character features. The combination of word and character 
features improves the performance of word segmentation and part-of-speech tagging. 
The proposed model can solve complicated issues in Chinese morphological analysis. 
The Chinese morphological analysis is generalized into a process of specific character 
tagging and word filtering. A lexicon supervises the character-based model to elimi-
nate invalid character tagging candidates.  
Our system outperformed the best SIGHAN word segmentation system in 3 out of 
the 4 SIGHAN open test sets. To our knowledge, our work is the first character-based 
approach, which performs better than word-based approaches for SIGHAN open test. 
In addition, the proposed method is versatile and can be easily integrated to any exist-
ing Chinese morphological system as a post-processing subsystem leading to en-
hanced performance.  
In this paper, we focused on word features in character-based mode, and adopted 
HMM as the statistical model to identify the rules. Other statistical models, such as 
maximum entropy, boosting, support vector machine, etc., may also be suitable for 
this application. They are worth investigating. The data sparseness problem is practi-
cally non-existent in the character-based model for the Chinese character set is lim-
ited. However, odd characters are occasionally found in Chinese personal or place 
names. Some rules using named entity identification technique may help smoothen 
552 Y. Meng, H. Yu, and F. Nishino 
this. In a broader view, the word building rules proposed in our model is simple 
enough for linguistic studies to better understand for example formation of Chinese 
words or even the Chinese language itself. 
References 
1. Andi Wu. Chinese Word Segmentation in MSR-NLP. In Proc. of SIGHAN Workshop, 
Sapporo, Japan, (2003) 127-175 
2. GuoDong Zhou and Jian Su. A Chinese Efficient Analyzer Integrating Word Segmenta-
tion, Part-Of-Speech Tagging, Partial Parsing and Full Parsing. In Proc. Of SIGHAN 
Workshop, Sapporo, Japan, (2003) 78-83 
3. Huaping Zhang, Hong-Kui Yu et al. HHMM-based Chinese Lexical Analyzer ICTCLAS. 
In Proc. Of SIGHAN Workshop, Sapporo, Japan, (2003) 184-187 
4. Nianwen Xue and Libin Shen. Chinese Word Segmentation as LMR Tagging. In Proc. Of 
SIGHAN Workshop, Sapporo, Japan, (2003) 176-179 
5. Hwee Tou Ng, Low, Jin Kiat. Chinese Part-of-Speech Tagging: One-at-a-Time or All-at-
Once? Word-Based or Character-Based? In Proc. of EMNLP, Barcelona, Spain, (2004) 
277-284 
6. Tetsuji Nakagawa. Chinese and Japanese Word Segmentation Using Word-level and 
Character-level Information, In Proc. of the 20th COLING, Geneva, Switzerland, (2004) 
466-472 
7. Guohong Fu and Kang-Kwong Luke. A Two-stage Statistical Word Segmentation System 
for Chinese. In Proc. Of SIGHAN Workshop, Sapporo, Japan, (2003) 156-157 
8. Jianfeng Gao, Andi Wu, Chang-Ning Huang et al Adaptive Chinese Word Segmentation. 
In Proc. of 42nd ACL. Barcelona, Spain, (2004) 462-469  
9. Richard Sproat and Thomas Emerson. The First International Chinese Word Segmentation 
Bakeoff. In Proc. Of SIGHAN Workshop, Sapporo, Japan, (2003) 133-143 
10. X. Luo. A Maximum Entropy Chinese Character-based Parser. In Proc. of EMNLP. Sap-
poro, Japan, (2003) 192-199 
11. Honglan Jin, Kam-Fai Wong, ?A Chinese Dictionary Construction Algorithm for Informa-
tion Retrieval?, ACM Transactions on Asian Language Information Processing, 1(4):281-
296, Dec. 2002. 
12. Yao Meng, Hao Yu and Fumihito Nishino. 2004. Chinese New Word Identification Based 
on Character Parsing Model. In Proc. of 1st  IJCNLP, Hainan, China, (2004) 489-496 
13. Shiwen Yu, Huiming Duan, etal. ?????????????????. ?????
?v(5), (2002) 49-64, 58-65 
14. Maosong Sun and Benjamin K. T? Sou. Ambiguity Resolution in Chinese Word Segmenta-
tion. In Proc. of 10th Pacific Asia Conference on Language, Information & Computation, 
(1995) 121-126 
15. Nianwen Xue, Fu-Dong Chiou and Martha Palmer. Building a Large-scale Annotated 
Chinese Corpus. In Proc. of the 19th COLING. Taibei, Taiwan, (2002)  
16. Chooi-Ling GOH, Masayuki Asahara, Yuji Matsumoto. Chinese Unknown Word Identifi-
cation Using Character-based Tagging and Chunking. In Proc. of the 41st ACL, Interac-
tive Poster/Demo Sessions, Sapporo, Japan, (2003) 197-200 
17. Shengfen Luo, Maosong Sun. 2003, Two-character Chinese Word Extraction Based on 
Hybrid of Internal and Contextual Measure, In Proc. of the 2nd SIGHAN Workshop, Sap-
poro, Japan, (2003) 20-30 
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 1004 ? 1016, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Web-Based Terminology Translation Mining 
Gaolin Fang, Hao Yu, and Fumihito Nishino 
Fujitsu Research and Development Center, Co., LTD. Beijing 100016, China  
{glfang, yu, nishino}@frdc.fujitsu.com 
Abstract. Mining terminology translation from a large amount of Web data can 
be applied in many fields such as reading/writing assistant, machine translation 
and cross-language information retrieval. How to find more comprehensive re-
sults from the Web and obtain the boundary of candidate translations, and how 
to remove irrelevant noises and rank the remained candidates are the challeng-
ing issues. In this paper, after reviewing and analyzing all possible methods of 
acquiring translations, a feasible statistics-based method is proposed to mine 
terminology translation from the Web. In the proposed method, on the basis of 
an analysis of different forms of term translation distributions, character-based 
string frequency estimation is presented to construct term translation candidates 
for exploring more translations and their boundaries, and then sort-based subset 
deletion and mutual information methods are respectively proposed to deal with 
subset redundancy information and prefix/suffix redundancy information 
formed in the process of estimation. Extensive experiments on two test sets of 
401 and 3511 English terms validate that our system has better performance.  
1   Introduction 
The goal of Web-based terminology translation mining is to mine the translations of 
terminologies or proper nouns which cannot be looked up in the dictionary from the 
Web using a statistical method, and then construct an application system for read-
ing/writing assistant (e.g. Mont Blanc????, ???). Translators and technical 
researchers cannot yet obtain an accurate translation after many lookup efforts when 
they encounter terminology or proper noun during translating or writing foreign lan-
guage. According to Web statistics by Google, 76.59% of Web pages are English. In 
China, statistical results by China Internet Network Information Center in July 2004 
show that the number of Internet users has reached 94 million, and nearly 87.4% of 
users have educational backgrounds beyond high school. These users can smoothly read 
general English pages, but some terminologies in the Web hamper them to exactly un-
derstand the whole content. Some skilled users perhaps resort to a Web search engine, 
but they cannot obtain effective information from a large amount of retrieved irrelevant 
pages and redundancy information. Thus, it is necessary to provide a system to auto-
matically mine translation knowledge of terms or proper nouns using abundant Web 
information so as to help users accurately read or write foreign language.  
The system of Web-based terminology translation mining has many applications. 
1) Reading/writing assistant, as one part of computer-assisted language learning 
(CALL) used in the E-learning. During reading or writing, users often meet terms 
 Web-Based Terminology Translation Mining 1005 
whose translations cannot be found in the dictionary, but this system can help them 
mine native and accurate translations from the Web. 2) The tool for constructing bi-
lingual dictionary. The system can not only provide translation candidates for compil-
ing bilingual lexicon, but also evaluate or rescore the candidate list of the dictionary. 
The constructed dictionary can be further applied in cross-language information re-
trieval (CLIR) and machine translation. 3) As one of the typical application paradigms 
of the combination of CLIR and Web mining. 
There are some issues that need to be solved using Web information to mine termi-
nology translation: 1) How to find more comprehensive results, i.e. mining all possi-
ble forms of annotation pairs in the Web. 2) How to obtain the boundary of candidate 
translations, especially for the language without the boundary mark such as Chinese 
and Japanese. Because we don?t know the translation is at left or right, and what is 
between the pair, and where is the candidate endpoint? 3) How to remove the noises 
formed in the statistics and rank the remained candidates.  
On the basis of reviewing all possible methods of acquiring translations, a feasible 
statistics-based method is proposed to mine terminology translation from the Web. In 
the proposed method, after analyzing different forms of term translation distributions, 
character-based string frequency estimation is employed to construct term candidate 
translations for exploring more translations and their boundaries, and then the candi-
date noises formed in the process of statistics are defined as two categories: subset 
redundancy information and prefix/suffix redundancy information. Sort-based subset 
deletion and mutual information methods are respectively proposed to deal with two 
redundancy information. Experiments on two test sets of 401 and 3511 English terms 
show that our system has better performance. In all reported literatures, our experi-
ment is the first time for the extensive research on Web-based terminology translation 
mining on the largest scale. 
2   Related Work 
Automatic acquisition of bilingual word pairs or translations has been extensively 
researched in the literature. The methods of acquiring translations are usually summa-
rized as four categories: 1) acquiring translation from parallel corpora, 2) acquiring 
translation from a combination of translations of constituent words, 3) acquiring 
translation from bilingual annotation in the Web, and 4) acquiring translation from 
non-parallel corpora.  
1)  Acquiring translation from parallel corpora. 
Acquiring bilingual lexicon or translations from parallel corpora (including sentence 
alignment and paragraph alignment) is to utilize statistics information such as co-
occurrence, position, and length between source word and translation equivalence in 
parallel texts as an evaluation criterion to obtain one-to-one map word pairs. Many 
previous researches focused on extracting bilingual lexicon from parallel corpora, and 
readers can refer to the reviews [1], [2] for the details. However, due to the restriction 
of current available parallel corpora of different languages, together with the fact that 
corpus annotation requires a lot of manpower and resources, researchers have at-
tempted to extract translations from non-parallel corpus or Web data. As opposed to 
extracting from parallel corpora, there are no corresponding units in non-parallel 
1006 G. Fang, H. Yu, and F. Nishino 
corpora so that statistics information such as co-occurrence, position and length be-
come unreliable. New statistical clues have to be proposed to build the relationship 
for acquiring translation pairs from non-parallel corpora, which is more difficult to 
handle than in parallel corpora.  
2) Acquiring translation from a combination of translations of constituent words. 
Grefenstette [3] employed an example-based approach to obtain compound word 
translations. His method first combined possible translations of each constituent, and 
then searched them in WWW, where the retrieved number was viewed as an evalua-
tion criterion. Experiments on a set of 724 German words and a set of 1140 Spanish 
terms showed that the accuracies of English translations were about 87% and 86%, 
respectively.  
Cao and Li [4] proposed a dictionary-based translation combination method to col-
lect translation candidates of English base noun phrases, and then employed a naive 
Bayesian classifier and TF-IDF vector constructed with EM algorithm as evaluation 
criterions for translation selection. In an experiment with 1000 English base noun 
phrases, the coverage of acquiring translations was 91.4%, and the accuracy of top 3 
choices was 79.8%. The system was further improved in the literature [5]. 
Navigli et al [6] proposed an ontology learning method for acquiring terminology 
translations from English to Italian. His method was based on bilingual lexicon and 
semantic relation between the constituents of source language derived from ontology 
learning, where disambiguated terms dramatically reduced the number of alternative 
translations and their combinations. This system can automatically extract the transla-
tions of 405 complex terms in the tourism domain. 
Using the translation combination of each constituent to acquire the translation of a 
multiword term is very suitable for translation acquisitions of base noun phrases. 
However, terminologies and technical terms often consist of unknown words, and 
their translations are seldom the combination of each constituent. Thus, the result of 
direct combination is not very desirable for terminology translation acquisition.  
3)  Acquiring translation from bilingual annotation in the Web. 
Nagata et al [7] proposed an empirical function of the byte distance between Japa-
nese and English terms as an evaluation criterion to extract the translation of Japanese 
word, and their results could be used as a Japanese-English dictionary. Preliminary 
experiments on the 50 word pairs showed that an accuracy of top 50 candidates 
reached 56%. The reasons for such experimental results have two aspects: first, the 
system didn?t further deal with candidate noises for mining useful knowledge; second, 
this system only handled top 100 Web pages retrieved from search engine. In fact, 
previous 100 Web pages seldom contain effective bilingual annotation information 
only directly using keyword search rather than imposing other restrictions. Thus, this 
problem should be further researched for practical applications. Since his research 
focused on finding English translation given a Japanese term, the segmentation of 
Japanese could be avoided. However, our problem is to find Chinese equivalent using 
English term, so we have to cope with how to obtain the correct boundary of Chinese 
translations. Therefore, the issue and the proposed method in this paper are distinctly 
different with Nagata?s.  
 Web-Based Terminology Translation Mining 1007 
4)  Acquiring translation from non-parallel corpora. 
Acquiring translation from non-parallel corpora is based on the clue that the context 
of the source term is very similar to that of the target translation in a large amount of 
corpora. In 1995, Rapp [8] assumed that there is a correlation between the patterns of 
word co-occurrence in non-parallel texts of different languages, and then proposed a 
matrix permutation method to match these patterns. However, computational limita-
tion hampered further extension of this method. In 1996, Tannaka and Iwasaki [9] 
demonstrated how to extract lexical translation candidates from non-aligned corpora 
using the similar idea. In 1999, this method was developed and improved by Rapp 
[10]. Rather than computing the co-occurrence relation matrix between one word and 
all words, the matrix between one word and a small base lexicon are estimated. Ex-
periments on 100 German words indicated that an accuracy of top 1 English transla-
tion was 72%, and top 10 was 89%. This system was only suitable for the situation of 
one word to one word, and didn?t further research on the translation acquisition from 
multiword to multiword. 
In 1995, Fung [11] proposed a ?context heterogeneity? method to compute the 
measure similarity between word and its translation for finding translation candidates. 
In the experiment with 58 English words, an accuracy of 50% is obtained in the top 
10 Chinese word candidates. Based on this work, Fung presented the word relation 
matrix to find the translation pair in 1997 [12]. This method respectively computed 
the correlation vectors between source word and seed word, target word and seed 
word. In 19 Japanese term test set, the accuracy of English translations reached 30%. 
In 1998, the method was improved to extend to non-parallel, comparable texts for 
translation acquisition [13]. This system use TF/IDF as the feature, and different 
measure functions as the similarity computation between the candidate pair. However, 
the system was restricted to the assumption that there are no missing translations and 
all translations are included in the candidate word list.  
Shahzad et al [14] first extracted the sentence corpora that are likely to contain the 
target translation using bilingual dictionary and transformation table. And then, the 
heuristics method was employed to obtain the correct candidate by analyzing the 
relations of source compound nouns and using partial context information. Experi-
ments on the 10 compound nouns showed that the average accuracy and recall were 
respectively 34% and 60%. 
As shown from the current situation of translation acquisition from non-parallel 
corpora, all experiments above are basically performed on small-scaled word set, and 
their results are very inspiring but difficult to put into practical use. Furthermore, most 
experimental methods are only suitable for one word translation, i.e. the word number 
ratio of translation pair is on a basis of 1:1. Thus, there are many issues to be further 
researched before it is used to explore new translation in the application area.  
From the review above, we know that Method 1 requires a large number of parallel 
corpora, and Method 2 and Method 4 have some limitations when they are applied to 
acquire the terminology translation, and Method 3 makes the best of mass Web re-
sources and is a feasible approach. When people use Asia language such as Chinese, 
Japanese, and Korean to write, especially scientific article or technical paper, they 
often annotate the associated English meaning after the terminology. With the devel-
opment of Web and the open of accessible electronic documents, digital library, and 
1008 G. Fang, H. Yu, and F. Nishino 
scientific articles, these resources will become more and more abundant. Thus, 
Method 3 is a feasible way to solve the terminology translation acquisition, which is 
also validated by the following experiments. 
3   The Framework of the Terminology Translation Mining System 
The Web-based terminology  translation  mining  system  is  depicted  in Fig. 1 as 
follows: 
 
Query  
?Mont Blanc?
WWW 
(bilingual annotation)
Rank & sort 
candidates
String frequency 
estimation 
Result 
????? 
Web page 
download 
module
Candidate noises 
& solutions 
HTML  
analysis 
Web page collection 
Terminology translation mining 
 
Fig. 1.  The Web-based terminology translation mining system 
The system consists of two parts: Web page collection and terminology translation 
mining. Web page collection includes download module and HTML analysis module. 
The function of download module is to collect these Web pages with terms? associ-
ated bilingual annotations, and then the pages are inputted into HTML analysis mod-
ule. In HTML analysis, Web pages are built as a tree structure from which possible 
features for the bilingual pair and text information in the HTML page are simultane-
ously extracted. 
Terminology translation mining includes string frequency estimation, candidate 
noises and their solutions, and rank & sort candidates. Translation candidates are 
constructed through string frequency estimation module, and then we analyze their 
noises and propose the corresponding methods to handle them. At last, the approach 
combining the possible features such as frequency, distribution, length proportion, 
distance, keywords and key symbols is employed to rank these candidates. 
In Web pages, there are a variety of bilingual annotation forms. Correctly exploring 
all kinds of forms can make the mining system extract the comprehensive translation 
results. After analyzing a large amount of Web page examples, we summarize transla-
tion distribution forms as the following six categories: 1) Direct annotation 2) Sepa-
rate annotation 3) Subset form 4) Table form 5) List form 6) Explanation form. Direct 
annotation is the most widely used form in the Web, where English meaning often 
 Web-Based Terminology Translation Mining 1009 
follows after Chinese terminology, and some have symbol marks such as bracket 
parentheses and bracket, and some have nothing, e.g. ????Mont Blanc?. Separate 
annotation is referred to as the case that there are some Chinese words or English 
letters between the translation pair, e.g. ?????,???universal life insurance?. 
Subset form is that the extracted translation pair is a subset of existing bilingual pair, 
for example, during searching the term ?Mont Blanc?, the term pair ????????
(Chamonix Mont Blanc)? also provides the valid information. Table or list form is the 
Web page in the form of table or list. Explanation form is the explanation and illustra-
tion for technical terms.  
 
Fig. 2. The examples of translation distribution forms, (a) Direct annotation, some has no mark 
(a1), and some have some symbol marks (a2, a3) (b) Separate annotation, there are English 
letters (b1) or some Chinese words (b2, b3) between the translation pair  (c) Subset form  (d) 
Table form  (e) List form  (f) Explanation form 
4   Statistics Based Translation Finding 
4.1   Character-Based String Frequency Estimation 
All kinds of possible translation forms of terminologies in the Web can be effectively 
and comprehensively mined through character-based string frequency estimation. The 
proposed method with Chinese character as the basic unit of statistics can not only 
obtain the correct boundary of the translation candidate, but also conveniently explore 
these Chinese candidate terminologies that usually consist of unknown words or un-
known compound words. 
String frequency information is one of the important clues during extracting candi-
date translations. Its estimation method has a direct influence on the system perform-
ance efficiency. The method combing hash index and binary search is employed to 
(a1) (a2) (a3) 
(b1) (b2) (b3) 
(c) (d) (e) (f) 
1010 G. Fang, H. Yu, and F. Nishino 
construct the index for all translation candidates. The definition of hash function is 
calculated according to 6763 Chinese characters in GB2312 system with a one-to-one 
map. Hash function is formulized as:  
otherwise
c
c
cc
cc
Y 215
176215
6763
5)161()176(94
)161()176(94
0
0
10
10
>
??
??
??
?
??+?
?+?
=  ,                    (1) 
where 10 ,cc  are respectively the unsigned encoding values of the first, second bytes 
of first Chinese character of candidate items. All strings are partitioned into different 
blocks in terms of the first Chinese character with the hash function above, where the 
strings with the same first character are sorted by lexicographic order, and the strings 
with non-Chinese character as the first position are indexed to the value of 6763. 
Here, GB2312 is employed as our statistics standard. Other encoding system is con-
verted to the corresponding characters in GB2312, and the characters will be omitted 
if there is no counterpart. The reasons for this strategy are as follows: 1) terminology 
seldom consists of rare words out of GB2312, 2) the index space is dramatically re-
duced using GB2312 rather than the Unicode encoding so as to quicken the estimation 
speed. 
The terminology to be looked up is inputted into search engine, and the relevant 
Web pages with this term?s associated bilingual annotation are collected. Web pages 
are transformed into text through HTML analysis module. The term position is lo-
cated as the center point through keyword search, and then string frequency and dis-
tribution estimation is performed in a window of 100 bytes. In Web pages, terminol-
ogies are often written as different forms because of the effect of noise. For example, 
the term ?Mont Blanc? may be written as ?MONT BLANC?, ?Mont-Blanc?, ?Mont 
??Blanc?, and ?MontBlanc?. For finding different forms of keywords in the Web, the 
fuzzy string matching approach is proposed. This method takes 26 English letters in 
the keyword as effective matching symbols, while ignoring the blank space and other 
symbols. In the matched text, only these English letters are viewed as effective items 
for comparison. Using this method can effectively locate different forms of terms and 
therefore obtain comprehensive translation candidates.  
The process of string frequency estimation is described as follows. In the windows 
with keyword as the center, each character is built as a beginning index, and then the 
string candidates are constructed with the increase of the string in the form of one 
Chinese character unit. Since terminology translation usually consists of unknown 
words or compound words, character is employed as the basic unit of statistics rather 
than word so as to explore these unknown term translations as more as possible. 
String candidates are indexed in the database with hash and binary search method, if 
there exists the same item as the inputted candidate, its frequency is increased by 1, 
otherwise, this candidate is added to the database at this position. After handling one 
Web page, the distribution information is also estimated at the same time. In the pro-
gramming implementation, the table of stop words and some heuristic rules of the 
beginning and end with respect to the keyword position are constructed to accelerate 
the statistics process.  
 Web-Based Terminology Translation Mining 1011 
4.2   Translation Noises and Their Solutions 
All possible forms of terminology translations can be comprehensively mined after 
character-based string frequency estimation. However, there are many irrelevant items 
and redundancy noises formed in the process of mining. These noises are defined as 
the following two categories. 
1) Subset redundancy information. The characteristic of this kind information is 
that this item is a subset of one item, but its frequency is lower than that item. For 
example: ?Mont Blanc???(38) ??(27) ??(11)?, where ????, ???? belong 
to subset redundancy information. They should be removed. 
2) Prefix/suffix redundancy information. The characteristic of this kind information 
is that this item is the prefix or suffix of one item, but its frequency is greater than that 
item. For example: 1. ?Mont Blanc ??(16) ???(9) ???(8)?, 2. ?Credit Rating
??(12) ????(10)?, 3. ?Knowledge Portal ????(33) ??????(30)?. 
In Example 1, the item ???? is suffix redundancy information and should be re-
moved. In Example 2, the item ???? is prefix redundancy information and should 
also be removed. In Example 3, the term ?????? is in accord with the definition 
of suffix redundancy information, but this term is a correct candidate. Thus, the prob-
lem of prefix/suffix redundancy information is so complex that we need an evaluation 
method to decide to retain or drop this candidate.  
 
Fig. 3. The description of the sort-based subset deletion algorithm 
4.2.1   Sort-Based Subset Deletion Method 
Aiming at subset redundancy information, we propose sort-based subset deletion 
method to handle it. Because subset redundancy information is an intermediate of 
estimating terminology translations, its information is basically contained by the 
1. Sort by entropy value 
2. Sort by boundary[*] for the same entropy 
3. Sort by length and lexical sort for the same entropy and boundary 
4. int nNum = 0;    //record the number of remained candidates 
5. for(int i=0; i<m_nDataNum; i++)  { 
6.  int nIsSubString = FALSE; 
7.   if(nNum == 0)    //for the first item to be remained 
8. Judge whether to remain this item using boundary and length proportion 
information; 
9.   else  { 
10.         for(int j=0; j< nNum; j++)  { 
11. Judge if the ith candidate is a subset of the jth, and doesn?t emerge in 
the isolated form, if yes   
12. {    nIsSubString = TRUE;    break;    } 
13.            } 
14.  } 
15.  if(!nIsSubString)  { 
16.         Move the ith candidate information to nNum position, and save; 
17.         The saved number nNum++; 
18.  } 
19. } 
20. m_nDataNum = nNum; //Save the total number. 
[*]Note: refer to the case that the string has the distinct left and right boundary in the Web 
1012 G. Fang, H. Yu, and F. Nishino 
longer string candidate with higher frequency. Therefore, this problem can be well 
solved by first sorting and then judging if this item is a subset of the preceding candi-
dates. The detailed algorithm is described in Fig. 3. 
4.2.2   Mutual Information Based Method 
Prefix/suffix redundancy information is very complicated to deal with. In some cases, 
previous candidate is a correct translation and should be retained, while in other cases, 
it is a noise and should be deleted. In this paper, mutual information based method is 
proposed to decide if the candidate should be retained or deleted. 
The concept of information entropy is first proposed by Shannon in 1948. Entropy 
is a measure of uncertainty of a random variable, and defined as:  
 ??=
=
k
i
ii xpxpXH
1
2 )(log)()( ,                                       (2) 
where )( ixp  is a probability function of a random variable X=xi. 
Mutual information is a concept of information theory, and is a measure of the 
amount of information that one random variable contains about another variable. The 
mutual information of two events X and Y is defined as: 
 ),()()(),( YXHYHXHYXI ?+= ,                                (3) 
where H(X) and H(Y) are respectively the entropies of the random variables of X and 
Y, and H(X,Y) is the co-occurrence entropy of X and Y.  
Mutual information reflects a closeness degree of the combination of X and Y. If 
there is no interesting relationship between X and Y, I(X,Y)=0, that is, X and Y are 
independent each other. If there is a genuine association between X and Y, the co-
occurrence of XY will be bigger than the random individual occurrence chance of X 
or Y, and consequently I>>0. In this case, the possibility as a fixed compound phrase 
of XY becomes very big. Small mutual information hints that the combination of X 
and Y is very loose, and therefore there is a great possibility of a boundary between 
two words X, Y.  
String frequency estimation is performed on different Web pages. In each Web 
page there is more than one occurrence for a candidate translation. Mapping this esti-
mation process to the entropy calculation, we define Nnxp ii /)( = , where ni denotes 
the number of a translation candidate in one Web page, and N represents the total 
number of this candidate. We define k as the number of the estimated Web pages. The 
calculation of entropy is rewritten as:  
  Nnn
NN
n
N
nXH
k
i
ii
k
i
ii
2
1
2
1
2 loglog
1log)( +??=??=
==
.                  (4) 
Through this formula, the candidate entropy can be computed directly rather than 
after counting all Web data. Therefore, it can reduce the time of statistics.  
Entropy can not only reflect the frequency information N, but also the distribution 
information in different Webs. The higher the frequency is, and the larger the entropy 
is. If the distribution is more uniform, this entropy value will become bigger. This is 
also in accord with our intuition.  
 Web-Based Terminology Translation Mining 1013 
Given two candidate patterns of 1t , 2t  in the set of translation candidates, 
)()( 21 tCtC > , where C denotes the frequency of estimation. For suffix redundancy 
information, )( 21 tsufft = ; for prefix redundancy information, )( 21 tpreft = . Accord-
ing to the definition of mutual information, )()()()( 21212 tHttHtHtI ??+= .  
The mutual information based method for prefix/suffix redundancy information is 
described as follows. First, judge if the condition of 95.0)(/)( 11 ?? tCttC
i
i  or 
95.0)(/)( 11 ?? tCttC
i
i  is satisfied, where the candidates itt1  represent the items that 
do not contained each other in the windows of 10 candidates after the candidate 1t . If 
the condition is met, then delete 1t . In an example of ?Dendritic Cell ??(62) ??
???(40) ????(15) ?????(4)?, because (40+15+4)/62=0.952>0.95, the 
candidate ???? is deleted. If prefix/suffix redundancy information don?t satisfy the 
condition above, then judge the condition of )()( 21 tItI <? , if yes, then delete 1t , 
otherwise retain it. The value of ? is determined by the experiments, and the following 
experimental results demonstrate that ?=0.85 is the best parameter. 
5   Experiments 
Our experimental database consists of two sets of 401 English-Chinese term pairs and 
3511 English-Chinese term pairs in the financial domain. There is no intersection 
between two sets. Each terminology often consists of 1-6 English words, and the 
associated translation contains 2-8 Chinese characters. In the test set of 401 terms, 
there are more than one Chinese translation for one English term, and only one Chi-
nese translation for 3511 term pairs. The top n accuracy is defined as the percentage 
of terms whose top n translations include correct translation in the term pairs.  
55
60
65
70
75
80
85
90
95
100
Top1 Top3 Top5 Top10 Top30
The calculated number
A
c c
u
ra
c y
?=0.9
?=0.85
?=0.82
?=0.8
?=0.7
 
Fig. 4.  The relationship between the parameter ? and the accuracy 
For testing in what condition, mutual information based method is the best to deal 
with the prefix/suffix redundancy information. The parameter of ? is respectively set 
1014 G. Fang, H. Yu, and F. Nishino 
to 0.7, 0.8, 0.82, 0.85, and 0.9 in the experiment on the test set of 401 terms. Experi-
mental results are shown in Fig. 4. From the figure, we know that ?=0.85 is the  
best parameter. 
60
65
70
75
80
85
90
95
100
50 100 150 200 250 300
The number of Web pages
A
c c
u r
a c
y
N=1
N=3
N=5
 
Fig. 5.  The relationship between the number of Web pages and the accuracy 
A second experiment is to analyze the number of Web pages influencing the term 
translation accuracy. The experiments are respectively performed on 50, 100, 150, 
200, 250, and 300 Web pages retrieved from the Web. Experimental results are illus-
trated in Fig. 5, where N=1, 3, 5 represent the results of top 1, top 3, and top 5. As 
seen from the figure, the result of using 200 Web pages is best. When the Web pages 
increase more than 200 Web pages, the performance isn?t improved distinctly, while 
the computation cost grows. In the case of 200 Web pages, the Chinese translation 
accuracy of top 1 is 71.8%, and top 3 is 94.5%, and top 5 is 97% on the test set of 401 
English terms (see Table 1). 
Table 1.  Experimental results on a test set of 401 terms 
Candidates Top30 Top10 Top5 Top3 Top1 
Accuracy 99.5% 99% 97% 94.5% 71.8% 
Using the previous trained parameters, we perform term translation mining ex-
periments in the test set of 3511 terms. Experimental results are listed in Table 2. 
From this table, the accuracy of top 3 is 83.6%. Experiments also validate that the 
accuracy of top 30 is nearly equal to the coverage of translations (the percentage of 
term translations found by our system). This is because there is no change on the 
accuracy when increasing the candidate number after top 30.  
Table 2.  Experimental results on a test set of 3511 terms 
Candidates Top30 Top10 Top5 Top3 Top1 
Accuracy 95.4% 93.8% 89.1% 83.6% 56.4% 
 Web-Based Terminology Translation Mining 1015 
6   Conclusions 
In this paper, after reviewing and analyzing all possible methods of acquiring trans-
lations, a statistics-based method is proposed to mine terminology translation from 
the Web. In the proposed method, character-based string frequency estimation is 
first presented to construct term translation candidates, and then sort-based subset 
deletion and mutual information methods are respectively proposed to deal with two 
redundancy information formed in the process of estimation. Experiments on two 
vocabularies of 401 and 3511 English terms show that our system has better per-
formance, about 94.5% and 83.6% in the top 3 Chinese candidates. The contribu-
tions of this paper focus on the following two aspects: 1) On the basis of reviewing 
all possible methods of acquiring translations and analyzing different forms of term 
translation distribution, a statistics-based method is proposed to mine terminology 
translation from the Web. 2) The candidate noises are defined as two categories: 
subset redundancy information and prefix/suffix redundancy information. Sort-
based subset deletion and mutual information methods are respectively proposed to 
deal with two redundancy information.  
References 
1. Somers, H.: Bilingual Parallel Corpora and Language Engineering. Proc. Anglo-Indian 
Workshop "Language Engineering for South-Asian languages", (2001) 
2. V?ronis, J.: Parallel Text Processing - Alignment and Use of Translation Corpora. The 
Netherlands: Kluwer Academic Publishers, (2000) 
3. Grefenstette, G.: The WWW as a Resource for Example-Based MT Tasks. Proc. ASLIB 
Translating and the Computer 21 Conference, (1999) 
4. Cao, Y., Li, H.: Base Noun Phrase Translation Using Web Data and the EM Algorithm. 
Proc. 19th Int?l Conf. Computational Linguistics, (2002) 127-133 
5. Li, H., Cao, Y., Li, C.: Using Bilingual Web Data to Mine and Rank Translations. IEEE 
Intelligent Systems. 4 (2003) 54-59 
6. Navigli, R., Velardi, P., Gangemi, A.: Ontology Learning and Its Application to Auto-
mated Terminology Translation. IEEE Intelligent Systems. 1 (2003) 22-31 
7. Nagata, M., Saito, T., Suzuki, K.: Using the Web as a Bilingual Dictionary.  
Proc. ACL 2001 Workshop Data-Driven Methods in Machine Translation, (2001)  
95?102 
8. Rapp, R.: Identifying Word Translations in Nonparallel Texts. Proc. 33th Annual Meeting 
of the Association for Computational Linguistics, (1995) 320-322 
9. Tanaka, K., Iwasaki, H.: Extraction of Lexical Translation from Non-Aligned Corpora, 
Proc. 16th Int?l Conf. Computational Linguistics, (1996) 580-585 
10. Rapp, R.: Automatic Identification of Word Translations from Unrelated English and 
German Corpora. Proc. 37th Annual Meeting Assoc. Computational Linguistics, (1999) 
519-526 
11. Fung, P.: Compiling Bilingual Lexicon Entries from a Non-Parallel English-Chinese Cor-
pus. Proc. Third Annual Workshop on Very Large Corpora, (1995) 173-183 
 
1016 G. Fang, H. Yu, and F. Nishino 
12. Fung, P.: Finding Terminology Translations from Nonparallel Corpora. Proc. Fifth Annual 
Workshop on Very Large Corpora (WVLC'97), (1997) 192-202 
13. Fung P., Yee, L.P.: An IR Approach for Translation New Words from Nonparallel, Com-
parable Texts. Proc. 17th Int?l Conf. Computational Linguistics and 36th Annual Meeting 
of the Association for Computational Linguistics, (1998) 414-420 
14. Shahzad, I., Ohtake, K., Masuyama, S., Yamamoto, K.: Identifying Translations of Com-
pound Nouns Using Non-Aligned Corpora. Proc. Workshop on Multilingual Information 
Processing and Asian Language Processing, (1999) 108-113 
Product Named Entity Recognition Based on Hierarchical Hidden
Markov Model?
Feifan Liu, Jun Zhao, Bibo Lv, Bo Xu
National Laboratory of Pattern Recognition
Institute of Automation Chinese Academy of Sciences
Beijing P.O. Box 2728, 100080
{ffliu,jzhao,bblv,xubo}@nlpr.ia.ac.cn
Hao Yu
FUJITSU R&D
Xiao Yun Road No.26
Chao Yang District, Beijing, 100016
yu@frdc.fujitsu.com
Abstract
A hierarchical hidden Markov model
(HHMM) based approach of product
named entity recognition (NER) from
Chinese free text is presented in this pa-
per. Characteristics and challenges in
product NER is also investigated and
analyzed deliberately compared with
general NER. Within a unified statis-
tical framework, the approach we pro-
posed is able to make probabilistically
reasonable decisions to a global opti-
mization by leveraging diverse range
of linguistic features and knowledge
sources. Experimental results show that
our approach performs quite well in two
different domains.
1 Introduction
Named entity recognition(NER) plays a sig-
nificantly important role in information extrac-
tion(IE) and many other applications. Previous
study on NER is mainly focused either on the
proper name identification of person(PER), lo-
cation(LOC), organization(ORG), time(TIM) and
numeral(NUM) expressions almost in news do-
main, which can be viewed as general NER, or
other named entity (NE) recognition in specific
domain such as biology.
As far as we know, however, there is little prior
research work conducted by far on product named
0This work was supported by the Natural Sciences Foun-
dation of China(60372016,60272041) and the Natural Sci-
ence Foundation of Beijing(4052027).
entity recognition which can be crucial and valu-
able in many business IE applications, especially
with the increasing research interest in Market
Intelligence Management(MIM), Enterprise Con-
tent Management (ECM) [Pierre 2002] and etc.
This paper describes a prototype system for
product named entity recognition, ProNER, in
which a HHMM-based approach is employed.
Within a unified statistical framework, the ap-
proach based on a mixture model is able to make
probabilistically reasonable decisions to a global
optimization by exploiting diverse range of lin-
guistic features and knowledge sources. Experi-
mental results show that ProNER performs quite
well in two different domains.
2 Related Work
Up to now not much work has been done on
product named entity recognition, nor systematic
analysis of characteristics for this task. [Pierre
2002] developed an English NER system capable
of identifying product names in product views. It
employed a simple Boolean classifier for identi-
fying product name, which was constructed from
the list of product names. The method is sim-
ilar to token matching and has a limitation for
product NER applications. [Bick et al 2004] rec-
ognized named entities including product names
based on constraint grammar based parser for
Danish. This rule-based approach is highly de-
pendent on the performance of Danish parser and
suffers from its weakness in system portability.
[C. Niu et al 2003] presented a bootstrapping ap-
proach for English named entity recognition us-
ing successive learners of parsing-based decision
40
System Statistical Model Linguistic Feature Combinative Points
[Zhang et al 2003] HMM semantic role, tokens pattern rules
[Sun et al 2002] class-based LM word form, NE category cue words list
[Tsai et al 2004] ME model tokens knowledge representation
Table 1: Comparison between several Chinese NER systems1
list and HMM, and promising experiment results
(F-measure: 69.8%) on product NE (correspond-
ing to our PRO) were obtained. Its main advan-
tage lies in that manual annotation of a sizable
training corpus can be avoided, but it suffers from
two problems, one is that it is difficult to find suf-
ficient concept-based seeds needed in bootstrap-
ping for the coverage of the variations of PRO
subcategories, another it is highly dependent on
parser performance as well.
Research on product NER is still at its early
stage, especially in Chinese free text collec-
tions. However, considerable amount of work
has been done in the last decade on the gen-
eral NER task and biological NER task. The
typical machine learning approaches for English
NE are transformation-based learning[Aberdeen
et al 1995], hidden Markov model[Bikel et
al. 1997], maximum entropy model[Borthwick,
1999], support vector machine learning[Eunji Yi
et al 2004], unsupervised model[Collins et al
1999]and etc.
For Chinese NER, the prevailing methodology
applied recently also lie in machine learning com-
bining other knowledge base or heuristic rules,
which can be compared on the whole in three as-
pects showed in Table 1.
In short, the trend in NER is to adopt a statis-
tical framework which try to exploit some knowl-
edge base as well as different level of text features
within and outside NEs. Further those ideas, we
present a hybrid approach based on HHMM [S.
Fine et al 1998] which will be described in de-
tail.
3 Problem Statements and Analysis
3.1 Task Definition
3.1.1 Definition of Product Named Entity
In our study, only three kinds of prod-
uct named entities are considered, namely
1Note: LM(language model); ME(maximum entropy).
Brand Name(BRA), Product Type(TYP), Product
Name(PRO), and BRA and TYP are often embed-
ded in PRO. In the following two examples, there
are two BRA NEs, one TYP NE and one PRO
NE all of which belong to the family of product
named entities.
Exam 1: ??(Benq)/BRA ??(brand)?
? ? ? ?(market shares)? ?(steadily)?
?(ascend)b
Exam 2: ? ?(corporation)? ?(will)?
?(deliver) [Canon/BRA 334?(ten thou-
sand)? ?(pixels)? ?(digital)? ?(camera)
Pro90IS/TYP]/PROb
Brand Name refer to proper name of product
trademark such as ???(Benq)? in Exam 1.
Product Type is a kind of product named en-
tities indicating version or series information of
product, which can consist of numbers, English
characters, or other symbols such as ?+? and ?-
? etc.In our study, two principles should be fol-
lowed.
(1) Chinese characters are not considered to
be TYP, nor subpart of TYP although some of
them can contain version or series information.
For instance, in ?2005????(happy new
year)?(version)??(cell phone)?, here ???
??(happy new year)?(version)?should not be
considered as a TYP.
(2) Numbers are essential elements in prod-
uct type entity. For instance, in ?PowerShot
??(series)??(digital)??(camera)?, ?Pow-
erShot? is not considered as a TYP, however,
in ?PowerShot S10 ??(digital)??(camera)?,
?PowerShot S10? can make up of a TYP.
Product Name, as showed above in Exam 2, is
a kind of product named entities expressing self-
contained proper name for some specified product
in real world compared to BRA and TYP which
only express one attribute of product. i.e. a PRO
NE must be assigned with distinctly discrimina-
tive information which can not shared with other
general product-related expressions.
41
(1) Product-related expressions which are em-
bedded with either BRA or TYP can be qual-
ified to be a PRO entity. e.g. ?BenQ?
?(flash)?(disk)? is a PRO entity, but the gen-
eral product-related expression ???(flash)?
?(market)??(investigation)? cannot make up
of a PRO entity.
(2) Product-related expressions indicating
some specific version or series information which
is unique for a BRA can also be considered as a
PRO entity. e.g. ?DIGITAL IXUS??(series)?
?(digital)? ?(camera)? is a PRO because
?DIGITAL IXUS? series is unique for Canon
product, but ?? ?(intelligent)?(version)?
?(cell phone)? is not a PRO because the at-
tribute of ?intelligent version? can be assigned to
any cell phone product.
3.1.2 Product Named Entity Recognition
Product named entity recognition involves the
identification of product-related proper names
in free text and their classification into differ-
ent kinds of product named entities, referring to
PRO, TYP and BRA in this paper.In comparison
with general NER, nested product NEs should be
tagged separately rather than being tagged just as
a single item, shown as Figure 1.
3.2 Challenges for Product Named Entity
Recognition
?For general named entities, there are some
cues which are very useful for entity recogni-
tion, such as ???(city), ????(Inc.), and etc. In
comparison, product named entities have no such
named conventions and cues, resulting in higher
boundary ambiguities and more complex NE can-
didate triggering difficulties.
?In comparison with general NER, more chal-
lenges in product NER result from miscellaneous
classification ambiguities. Many entities with
identical form can be a kind of general named en-
tity, a kind of product named entity, or just com-
mon words.
?In comparison with general named entities,
product named entities show more flexible vari-
ant forms. The same entity can be expressed in
several different forms due to spelling variation,
word permutation and etc. This also compounds
the difficulties in product named entity recogni-
tion.
?In comparison with general named entities,
it is more frequent that product named entities are
nested as Figure 1 illustrates. More efforts have
to be made to identify such named entities sepa-
rately.
3.3 Our Solutions
We adopt the following strategies in triggering
and disambiguating process respectively.
(1) As to product NER, it?s pivotal to control
the triggering candidates efficiently for the bal-
ance between precision and recall. Here we use
the knowledge base such as brand word list, and
other heuristic information which can be easily
acquired.
(2)After triggering candidates, we try to em-
ploy a statistical model to make the most of
multi-level context information mentioned above
in disambiguation. We choose hierarchical hid-
den Markov model (HHMM) [S. Fine et al 1998]
for its more powerful ability to model the multi-
plicity of length scales and recursive nature of se-
quences.
42
4 Hybrid Approach for Product NE
Recognition
4.1 Overall Workflow of ProNER
?Preprocessing: Segment, POS tagging and
general NER is primarily conducted using our off-
shelf SegNer2.0 toolkit on input text.
?Generating Product NE Candidates: First,
BRA or ORG and TYP are triggered by brand
word list and some word features respectively.
Here we categorize the triggering word features
into six classes: alphabet string, alphanumeric
string, digits, alphabet string with fullwidth, dig-
its with fullwidth and other symbols except Chi-
nese characters. Then PRO are triggered by BRA
and TYP candidates as well as some clue words
indicating type information to some extent such
as ???(version), ????(series). In this step the
model structure(topology) of HHMM[S. Fine et
al. 1998] is dynamically constructed, and some
conjunction words or punctuations and specified
maximum length of product NE are used to con-
trol it.
?Disambiguating Candidates: In this mod-
ule, boundary and classification ambiguities be-
tween candidates are resolved simultaneously.
And Viterbi algorithm is applied for most-likely
state sequences based on the HHMM topology.
4.2 Integration with Heuristic Information
To get more efficient control in triggering process
above, we try to integrate some heuristic informa-
tion. The heuristic rules we used are as domain-
independent as possible in order that they can
be integrated with statistical model systematically
rather than just some tricks on it.
(1) Stop Word List:
Common English words, English brand word,
and some punctuations are extracted automati-
cally from training set to make up of stop word
list for TYP; by co-occurrence statistics between
ORG and its contexts, some words are extracted
from the contexts to make up of stop word list
for PRO in order to overcome the case that brand
word is prone to bind its surroundings to be a
PRO.
(2) Constrain Rules:
Rule 1: For the highly frequent pattern ??
?+?????(number + English quantifier
ES PS5IS2IS1
IS0
ES PS1 PS2 PS4PS3ES
0.2 0.5
0.3
0.7  0.3 0.5 0.30.7
0.2
0.3
Figure 2 Structure of Hierarchical Hidden
Markov Model (HHMM)
word), all the corresponding TYP candidates trig-
gered by categorized word features(CWF) should
be removed.
Rule 2: Product NE candidates in which some
binate symbols don?t match each other should be
removed.
Rule 3: Unreasonable symbols such as ?-? or
?:? should not occur in the beginning or end of
product NE candidates.
4.3 HHMM for product NER application
By HHMM [S. Fine et al 1998] the product
NER can be formulated as a tagging problem us-
ing Viterbi algorithm. Unlike traditional HMM
in POS tagging, here the topology of HHMM is
not fixed and internal states can be also a similar
stochastic model on themselves, called internal
states compared to production states which will
emit only observations.
Our HHMM structure actually consists of three
level approximately illustrated as figure 2 in
which IS denotes internal state, PS denotes pro-
duction state and ES denote end state at ev-
ery level. For our application, an input se-
quence from our SegNer2.0 toolkit can be formal-
ized as w1/t1w2/t2 . . . wi/ti . . . wn/tn, among
which wi and ti is the ith word and its part-of-
speech, n is the number of words. The POS
tag set here is the combination of tag set from
Peking University(PKU-POS) and our general
NE categories(GNEC) including PER(person),
LOC(location), ORG(organization), TIM(time ex-
pression), NUM(numeric expression). Therefore
we can construct our HHMM model by the state
set {S} consisting of {GNEC}, {BRA, PRO,
TYP}, and {V} as well as the observation set {O}
consisting of {V} which is the word set from
training data. That is to say, the word forms
43
in {V} which are not included in NEs are also
viewed as production states.
In our model, only PRO are internal state which
may activate other production states such as BRA
and TYP resulting in recursive HMM. In consis-
tence with S. Fine?s work, qdi (1? d ? D) is used
to indicate the ith state in the dth level of hierar-
chy. So, the product NER problem is to find the
most-likely state activation sequence Q*, a multi-
scale list of states, based on the dynamic topol-
ogy of HHMM given a observation sequence W
= w1w2 . . . wi . . . wn, formulated as follows based
on Bayes rule (P (W )=1).
Q?= argmax
Q
P (Q|W )= argmax
Q
P (Q)P (W |Q)
(1)
From the root node of HHMM, activity flows
to all other nodes at different levels according to
their transition probability. For description conve-
nience, we take the kth level as example(activated
by the mth state at the k-1th level).
P (Q) ?= p(qk1 |qk?1m )
? ?? ?
vertical transition
horizontal transition
? ?? ?
p(qk2 |qk1 )
|qk|
?
j=3
p(qkj |qkj?1, qkj?2)
(2)
P (W |Q)=
?
???????
???????
?=
|qkPS |?
j=1
p([wqkj ?begin...wqkj ?end]|q
k
j )
if qkj /? {IS}
activate other states recursively
if qkj ? {IS}
(3)
Where |qk| is the number of all states and |qkPS |
is the number of production states in the kth level;
wqkj ?begin...wqkj ?end indicates the word sequence
corresponding to the state qkj .
(1) In equation (3), if qkj ? {{GNEC},{V}},
p([wqkj ?begin...wqkj ?end]|q
k
j )=1, because we as-
sume that the general NER results from the pre-
ceding toolkit are correct;
(2) If qkj = PRO, production states in the
(k+1)th level will be activated by this internal
state through equation (2),(3) and go back when
arriving at an end state, thus hierarchical compu-
tation is implemented;
(3) If qkj =BRA, we assign equation (3) a con-
stant value in that BRA candidates consist of only
a single brand word in our method. In addition
brand word can also generate ORG candidates,
thus we can assign equation (3) as follows.
p([wqkj ?begin...wqkj ?end]|q
k
j = BRA) = 0.5 (4)
(4) If qkj = TY P , categorized word fea-
tures(CWFs) defined in section 4.1 are applied,
i.e. the words associated with the current state are
replaced with their CWFs (WC) acting as obser-
vations. Then we can compute the emission prob-
ability of this TYP production state as the follow-
ing equation, among which |qkj | is the length of
observation sequence associated with the current
state.
p([wqkj ?begin...wqkj ?end]|q
k
j = TY P )
?=p(wc1|begin)p(end|wc|qkj |)
|qkj |?
m=2
p(wcm|wcm?1)
All the parameters in every level of HHMM can
be acquired using maximum likelihood method
with smoothing from training data.
4.4 Mixture of Two Hierarchical Hidden
Markov Models
Now we have implemented a simple HHMM
for product NER. Note that in the above
model(HHMM-1), we exploit both internal and
external features of product NEs only at lev-
els of simply semantic classification and just
word form. To achieve our motivation in sec-
tion 3.3, we construct another HHMM(HHMM-
2) for exploiting multi-level contexts by mixing
with HHMM-1.
In HHMM-2, the difference from HHMM-1
lies in the state set SII and observation set OII .
Because the input text will be processed by seg-
ment, POS tagging and general NER, as a alterna-
tive, we can also take T=t1t2 . . . ti . . . tn as obser-
vation sequence, i.e. OII={PKU-POS}. Accord-
ingly, SII= {{PKU-POS}, {GNEC}, BRA, TYP,
44
Data Sets PRO BRA TYP PER LOC ORG
DataSetPRO1.2 12,432 5,047 10,606 424 1,733 4,798
OpenTestSet 1800 803 1364 39 207 614
CloseTestSet 1553 513 1296 55 248 619
Table 2: Overview of Data Sets
PRO}, among which PRO is internal state. Sim-
ilarly, the problem is formulated as follows with
HHMM-2.
Q?II = argmax
QII
P (QII |T )
= argmax
QII
P (QII)P (T |QII) (5)
The description and computation of HHMM-2
is similar to HHMM-1 and is omitted here.
We can see that besides making use of semantic
classification of NEs in common, HHMM-1 and
HHMM-2 exploit word form and part-of-speech
(POS) features respectively. Word form features
make the model more discriminative, while POS
features result in robustness. Intuitively, the mix-
ture of these two models is desirable for higher
performance in product NER by balancing the ro-
bustness and discrimination which can be formu-
lated in logarithmic form as follows.
(Q?, Q?II)
= argmax
Q,QII
{log(P (Q)) + log(P (W |Q))
+ ?[log(P (QII)) + log(P (T |QII))]} (6)
Where ? is a tuning parameter for adjusting the
weight of two models.
5 Experiments and analysis
5.1 Data Set Preparation
A large number of web pages in mobile phone
and digital domain are compiled into text collec-
tions, DataSetPRO, on which multi-level process-
ing were performed. Our final version, DataSet-
PRO1.2, consists of 1500 web pages, roughly
1,000,000 Chinese characters. Randomly se-
lected 140 texts (digital 70, mobile phone 70) are
separated from DataSetPRO1.2 as our OpenTest-
Set, the rest as TrainingSet, from which 160 texts
are extracted as CloseTestSet. Table 2 illustrates
the overview of them.
5.2 Experiments
Due to various and flexible forms of product NEs,
though some boundaries of recognized NEs are
inconsistent with manual annotation, they are also
reasonable. So soft evaluation is also applied
in our experiments to make the evaluation more
reasonable. The main idea is that a discount
score will be given to recognized NEs with wrong
boundary but correct detection and classification.
However, strict evaluation only score completely
correct ones.
All the results is conducted on OpenTestSet un-
less it is particularly specified. Also, the evalu-
ation scores used below are obtained mainly by
45
Digital Domain (??8)
Product NEs Close Test Open TestPrecision Recall F-measure Precision Recall F-measure
PRO 0.864 0.799 0.830 0.762 0.744 0.753
TYP 0.903 0.906 0.905 0.828 0.944 0.882
BRA 0.824 0.702 0.758 0.723 0.705 0.714
Mobile Phone Domain (??8)
Product NEs Close Test Open TestPrecision Recall F-measure Precision Recall F-measure
PRO 0.917 0.935 0.926 0.799 0.856 0.827
TYP 0.959 0.976 0.967 0.842 0.886 0.864
BRA 0.911 0.741 0.818 0.893 0.701 0.785
Table 3: Experimental Results in Digital and Mobile Phone Domain
soft metrics, and strict scores are also given for
comparison in experiment 3.
1. Evaluation on the Influence of ? in the Mix-
ture Model.
In the mixture model denoted as equation (6),
the ? value reflects the different contribution of
two individual models to the overall system per-
formance. The larger ?, the more contribution
made by HHMM-2. Figure 3, 4, 5 illustrate the
varying curves of recognition performance with
the ? value on PRO, TYP, BRA respectively.
Note that, if ? equal to 1 then two models
are mixed with equivalent weight. We can see
that, as ? goes up, the F-measures of PRO and
TYP increase obviously firstly, and begin to go
down slightly after a period of growing flat. It
can be explained that HHMM-2 mainly exploits
part-of-speech and general NER features which
can relieve the sparseness problem to some ex-
tent, which is more serious in HHMM-1 due to
using lower level of contextual information such
as word form. However, as ? becomes larger,
the problem of imprecise modeling in HHMM-
2 will be more salient and begin to illustrate a
side-effect in the mixture model. Whereas, the
influence of ? on BRA is negligible because its
candidates are triggered by the relatively reliable
knowledge base and its sub-model in HHMM is
assigned a constant as shown in equation(4).
Summings-up:
(1) Mixture with HHMM-2 can make up the
weakness of HHMM-1.
(2) HHMM-2 can make more contributions
to the mixture model under the conditions that
limited annotated data is available at present. In
our system, ? is assigned to 8 based on above ex-
perimental results.
2. Evaluation on the portability of ProNER in
two domains.
First, we can see from Table 3 that ProNER
have achieved fairly high performance in both
digital and mobile phone domain. This can val-
idate to some extent the portability of our sys-
tem?which is consistent with our initial motiva-
tion.
Second, the results also show that our system
performs slightly better in mobile phone domain
for both close test and open test. This can be ex-
plained that there are more challenging ambigui-
ties in digital domain due to more complex prod-
uct taxonomy and more flexible variants of prod-
uct NEs.
Summings-up: The results provide promising
evidence on the portability of our system to dif-
ferent domains though there are some differences
between them.
3. Evaluation on the efficiency of the mixture
model and the improvement of the triggering
control with heuristics.
In table 4, ?1? denotes HHMM-1; ?2? denotes
HHMM-2; ?+? means the mixture model; ?*?
means integrating with heuristics mentioned in
section 4.2.
The results reveal that the mixture model out-
performs each individual model with both soft
and strict metrics. Also, the results show that
heuristic information can increase the F-measure
of PRO and TYP by 10 points or so for both indi-
46
HHMM
BRA TYP PRO
strict
score
soft
score
strict
score
soft
score
strict
score
soft
score
1 0.68 0.72 0.57 0.66 0.52 0.61
1* 0.70 0.74 0.70 0.80 0.63 0.72
2 0.67 0.73 0.66 0.74 0.61 0.68
2* 0.70 0.74 0.76 0.85 0.70 0.76
1+2 0.70 0.75 0.67 0.77 0.67 0.72
1+2* 0.72 0.76 0.76 0.87 0.75 0.80
Table 4: Improvement results (F-measure) with
heuristics and model mixture
vidual model and the mixture model. Addition-
ally we can see that HHMM-2 performs better
on the whole than HHMM-1, which is consistent
with experiment 1 that heavier weights should be
assigned to HHMM-2 in the mixture model.
Summings-up:
(1) Either HHMM-1 or HHMM-2 can not
perform quite well independently, but systemat-
ical integration of them can achieve obvious per-
formance improvement due to the leverage of di-
verse levels of linguistic features by their efficient
interaction.
(2) Heuristic information can highly enhance
the performance for both individual model and the
mixture model.
6 Conclusions and Future Work
This paper presented a hierarchical HMM (hidden
Markov model) based approach of product named
entity recognition from Chinese free text. By uni-
fying some heuristic rules into a statistical frame-
work based on a mixture model of HHMM, the
approach we proposed can leverage diverse range
of linguistic features and knowledge sources to
make probabilistically reasonable decisions for a
global optimization. The prototype system we
built achieved the overall F-measure of 79.7%,
86.9%, 75.8% corresponding to PRO, TYP, BRA
respectively, which also provide experimental ev-
idence to some extent on its portability to differ-
ent domains.
Our future work will focus on the following:
(1) Using long dependency information;
(2) Integrating segment, POS tagging, general
NER and product NER to avoid error spread.
References
John M. Pierre. (2002) Mining Knowledge from Text
Collections Using Automatically Generated Meta-
data. In: Procs of Fourth International Conference
on Practical Aspects of Knowledge Management.
Michael Collins and Yoram Singer. (1999) Unsuper-
vised Models for Named Entity Classification. In:
Proc. of EMNLP/VLC-99.
Eunji Yi, Gary Geunbae Lee, and Soo-Jun Park.
(2004) SVM-based Biological Named Entity
Recognition using Minimum Edit-Distance Feature
Boosted by Virtual Examples. In: Proceedings of
the First International Joint Conference on Natural
Language Processing (IJCNLP-04).
Bick, Eckhard (2004) A Named Entity Recognizer for
Danish. In: Proc. of 4th International Conf. on Lan-
guage Resources and Evaluation,pp:305-308.
Jian Sun, Jianfeng Gao, Lei Zhang, Ming Zhou,
Changning Huang. (2002) Chinese Named Entity
Identification Using Class-based Language Model.
In: COLING 2002. Taipei, Taiwan.
Huaping Zhang, Qun Liu, Hongkui Yu, Xueqi Cheng,
Shuo Bai. Chinese Named Entity Recognition Us-
ing Role Model. Special Iissue ?Word Formation
and Chinese Language processing? of the Inter-
national Journal of Computational Linguistics and
Chinese Language Processing, 8(2),2003, pp:29-60
Aberdeen, John et al (1995)MITRE: Description of
the ALEMBIC System Used for MUC-6. Proc. of
MUC-6, pp. 141-155
D.M. Bikel, S. Miller, R. Schwartz, R. Weischedel.
(1997) Nymble: a High-Performance Learning
Name-finder. In: Fifth Conference on Applied Nat-
ural Language Processing, pp 194-201.
Borthwick. A. (1999) A Maximum Entropy Approach
to Named Entity Recognition. PhD Dissertation.
Tzong-Han Tsai, S.H. Wu, C.W. Lee, Cheng-Wei
Shih, and Wen-Lian Hsu. (2004) Mencius: A Chi-
nese Named Entity Recognizer Using the Maxi-
mum Entropy-based Hybrid Model. International
Journal of Computational Linguistics and Chinese
Language Processing, Vol. 9, No 1.
Cheng Niu, W. Li, J.h. Ding and R.K. Srihari. (2003) A
Bootstrapping Approach to Named Entity Classifi-
cation Using Successive Learners. In: Proceedings
of the 41st ACL, Sapporo, Japan, pp:335-342.
S. Fine, Y. Singer, N. Tishby. (1998) The Hierarchical
Hidden Markov Model: Analysis and Applications.
Machine Learning. 32(1), pp:41-62
47
Dimensionality Reduction with Multilingual Resource 
YingJu Xia                      Hao Yu                        Gang Zou 
Fujitsu Research & Development Center Co.,LTD. 
13F Tower A, Ocean International Center, No.56 Dong Si Huan Zhong Rd, Chaoyang District, 
Beijing, China, 100025 
{yjxia,yu,zougang}@cn.fujitsu.com 
 
 
Abstract 
Query and document representation is a 
key problem for information retrieval and 
filtering. The vector space model (VSM) 
has been widely used in this domain. But 
the VSM suffers from high dimensionality. 
The vectors built from documents always 
have high dimensionality and contain too 
much noise. In this paper, we present a 
novel method that reduces the dimensional-
ity using multilingual resource. We intro-
duce a new metric called TC to measure the 
term consistency constraints. We deduce a 
TC matrix from the multilingual corpus and 
then use this matrix together with the term-
by-document matrix to do the Latent Se-
mantic Indexing (LSI). By adopting differ-
ent TC threshold, we can truncate the TC 
matrix into small size and thus lower the 
computational cost of LSI. The experimen-
tal results show that this dimensionality re-
duction method improves the retrieval per-
formance significantly. 
1 Introduction 
1.1 Basic concepts 
The vast amount of electronic information that is 
available today requires effective techniques for 
accessing relevant information from it. The meth-
odologies developed in information retrieval aim at 
devising effective means to extract relevant docu-
ments in a collection when a user query is given. In 
information retrieval and filtering, Query and 
document representation is a key problem and 
many techniques have been developed. Among 
these techniques, the vector space model (VSM) 
proposed by Salton (1971; 1983) has been widely 
used. In the VSM, a document is represented by a 
vector of terms. The cosine of the angle between 
two document vectors indicates the similarity be-
tween the corresponding documents. A smaller 
angle corresponds to a larger cosine value and in-
dicates higher document similarity. A query, which 
describes the information need, is encoded as a 
vector as well. Retrieval of documents that satisfy 
the information need is achieved by finding the 
documents most similar to the query, or equiva-
lently, the document vectors closest to the query 
vector. There are several advantages to this ap-
proach beyond its mathematical simplicity. Above 
all, it is efficient to compute and store the word 
counts. This is one reason that why VSM is widely 
used for query and document representation. But 
this method has problem that the vectors built from 
documents always have high dimensionality and 
contain too much noise. The high dimensionality 
causes high computational and memory require-
ments while noise in the vectors degrades the sys-
tem performance. 
1.2 Related works 
To address these problems, many dimensionality 
reduction techniques have been applied to query 
and document representation. Among these tech-
niques, Latent Semantic Indexing (LSI) (Deer-
wester et al, 1990; Hofmann, 1999; Ding, 2000; 
Jiang and Littman, 2000; Ando, 2001; Kokiopou-
lou and Saad, 2004; Lee et al, 2006) is a well-
known approach. LSI constructs a smaller docu-
ment matrix that retains only the most important 
information from the original by using the Singular 
Value Decomposition (SVD). Many modifications 
have been made to this approach (Hofmann, 1999; 
Ding, 2000; Jiang and Littman, 2000; Kokiopoulou 
613
and Saad, 2004; Sun et al, 2004; Husbands et al, 
2005). Among them, IRR (Ando and Lee, 2001) is 
a subspace-projection method that counteracts ten-
dency to ignore minority-class documents. This is 
done by repeatedly rescaling vectors to amplify the 
presence of documents poorly represented in pre-
vious iterations. 
 In concept indexing (CI) (Karypis and Han, 
2000) method, the original set of documents is first 
clustered into k similar groups, and then for each 
group, the centroid vector (i.e., the vector obtained 
by averaging the documents in the group) is used 
as one of the k axes of the lower dimensional space. 
The key motivation behind this dimensionality re-
duction approach is the view that each centroid 
vector represents a concept present in the collec-
tion, and the lower dimensional representation ex-
presses each document as a function of these con-
cepts. George and Han (2000) extend concept in-
dexing in the context of supervised dimensionality 
reduction. To capture the concept, phrase also has 
been used as indexing entries (Mao and Chu, 2002).  
The LPI method (Isbell and Viola, 1999) tries to 
discover the local structure and obtains a compact 
document representation subspace that best detects 
the essential semantic structure. The LPI uses Lo-
cality Preserving Projections (LPP) (Xiaofei He 
and Partha, 2003) to learn a semantic space for 
document representation. Xiaofei He et al, (2004) 
try to get sets of highly-related words, queries and 
documents are represented by their distance to 
these sets. These algorithms have successfully re-
duced the dimensionality and improve the retrieval 
performance but at the mean time they led to a 
high computational complexity.  
1.3 Our method 
In this study, we propose a novel method that re-
duces the dimensionality using multilingual re-
source. We first introduce a new metric called TC 
to measure the term consistency constraints. We 
use this metric to deduce a TC matrix from the 
multilingual corpus. Then we combine this matrix 
to the term-by-document matrix and do the Latent 
Semantic Indexing. By adopting different TC 
threshold, we can truncate the TC matrix into small 
size and thus lower the computational cost of LSI.  
The remainder of this paper is organized as fol-
lows. Section 2 describes the dimensionality reduc-
tion method using multilingual resource. Section 3 
shows the experimental results to evaluate the di-
mensionality reduction method. Finally, we pro-
vide conclusions and remarks of future work in 
Section 4. 
2 Dimensionality reduction using multi-
lingual resource 
2.1 Motivation 
As mentioned above, the queries and documents 
are represented by vectors of terms. The weight of 
each term indicates its contribution to the vectors. 
Many weighting schemes have been proposed. The 
simplest form is to use the term-frequency (TF) as 
the term weight. In this condition, a document can 
be represented as a vector ),...,,( 21 ntftftfd =
r
, where 
is the frequency of the ith term in the document. 
A widely used refinement to this model is to 
weight each term based on its inverse document 
frequency (IDF) in the documents collection. This 
is commonly done by multiplying the frequency of 
each term i by , where N is the total 
number of documents in the collection, and is 
the number of documents that contain the ith term. 
This leads to the TF-IDF representation of the 
documents. Although the TF-IDF weighting 
scheme has many variants (Buckley, 1985; Berry 
et al, 1999; Robertson et al, 1999), the idea is the 
same one that uses the statistical information such 
as TF and IDF to calculate the term weight of 
vectors.  
itf
)/log( idfN
idf
This kind of statistical information is independ-
ence with languages. For example, in one language, 
say La, we have a vocabulary Va = {w1a, w2a, ?, 
wna} and a documents collection Da = {d1a, d2a,?, 
dma }. If this documents collection has a parallel 
corpus in language Lb, say, Db = {d1b, d2b,?, dmb } 
and a vocabulary Vb = {w1b, w2b, ?, wnb}. When 
we put a query Qka = {qk1a, qk2a ,?, qkla } (qkia ?Va) 
into an information retrieval system. The informa-
tion retrieval system will converts the query Qka 
and the documents in the collection Da into vectors. 
By calculating the similarity between query Qka 
and each document dia, the system selects the 
documents whose similarity is higher than a 
threshold as the results Rka. If we translate the 
query Qka into language Lb and get query Qkb, when 
putting the Qkb into the same information retrieval 
system, we get the retrieval results Rkb. Since the 
Qka and Qkb contain the same content and only ex-
pressed in different languages. We expect that Rka 
614
and Rkb will contain the same content. If this as-
sumption holds, the vocabulary which is used to 
build queries and documents vectors should have 
high representative ability. Since the weight of 
each term in the vector is calculated by the statisti-
cal information such as TF and IDF. If the vocabu-
lary Va and Vb have high representative ability, 
their statistical information will be consistent as 
well. This is the main motivation of our dimen-
sionality reduction method. 
2.2 Dimensionality reduction method 
The most straightforward way to measure the 
word?s representability in multilingual resource is 
to calculate the TF and IDF of each word in differ-
ent languages. But this method has one problem 
that the TF-IDF scheme is dedicated for each sin-
gle document, the same word will have different 
weight in different documents. It is impractical to 
impose the consistency constraint to every docu-
ment. Even we can do that, this method still has the 
drawback that it is very difficult to port to another 
documents collection. To address this problem, we 
consider the whole documents collection as one 
single document. In this condition, the IDF will be 
a fixed number. 
We introduce a new metric to measure the term 
consistency called TC. Figure 1 and Figure 2 illus-
trate the basic idea. In these figures, the curve La 
shows the word logarithmic frequency in the 
documents collection of language La, the curve Lb 
shows the corresponding translation?s logarithmic 
frequency in the documents collection of language 
Lb. TCi and TCj are the term consistency of wi and 
wj respectively.  
Figure 1 shows the TC in normal condition that 
the average word frequency in language a is proxi-
mate to that of language b. In this case, the TC is 
defined as below: 
))log(/)log(),log(/)min(log()( ai
b
i
b
i
a
i
b
i ffffwTC =       (1) 
Here fia  is the frequency of wia in language a. fib  
is the frequency of the wia?s translation in language 
b. In multilingual case, the TC(wi) will be defined 
as below: 
))(...),(min()( ni
b
ii wTCwTCwTC =   (
In the case that 
2) 
the average word frequency in 
lan
to calculate the TC of wi as below: 
 (3) 
Here H is distance between the moving average 
guage a is different with that of language b, we 
will first calculate the moving average as shown in 
the Figure 2. After that, we use the moving average 
)))/(log()log(),log(/))min((log()( HfffHfwTC ai
b
i
b
i
a
i
b
i ++=
     
and the original one. 
words
Fr
eq
ue
nc
y
Language a
Language b
w i w j
TC i
TC j
 
Figure 1. TC in normal condition 
words
Fr
eq
ue
nc
y
Language a
Language b
Moving average
w i w j
TC i
TC j
 
Figure 2. TC in shift condition 
Once we get the guage a, 
we present i diag(TC , 
?  
do = 
B
): 
TC of every word in lan
t in a diagonal matrix T =tt? 1
TC2, ? , TCt), TC1 ? TC2 ? ?  ? TCt. 
When applying the TC matrix tT in informa-
tion retrieval, we combine T  into the t
t
tt? erm-by-
cument matrix dtA ? . Where dtA ? [aij] and the 
aij is the weight of term i in ument j. We get a 
new matrix dtttdt ATB ??? = . Then following the 
classical LSI dt?  by a low-rank ap-
proximation derived from its truncated Singular 
Value Decomposition (SVD
T
ndnnntdt VUB ???? ?=  
Here IUUT = , IVV T = ,
doc
, we replace 
),...,,( 21 ndiag ???=?  
== 0...... 121? =??? + nr? r?? ? ? .  
ain pro m of LSI is that it usually led to
a high computational complexity sinc he matrix 
matri
The m ble  
e t
dt?  usually in 10B 3-105 dimensional space. To 
lower the computational cost, we truncate the TC 
x ttT ?  according to different TC threshold 
and get a new matrix ),...,,(? 21 ttt TCTCTCdiagT =? , 
0......21 1 ===???? r TCTCTCTCTC . Then 
??
+ tr
we get AT ??= . Since r is small than t, the drrrdrB ?
615
computational cost on the matrix wil
. Note that 
b). To  this one-to-many phe-
no
stem to evaluate 
eduction method presented in 
Section 2. The term weight in the term-by-
do
comes from Chinese Linguistic 
eseldc.org/
drB ?? l lower 
than tB ? the matrix drB ??  is deduced 
from the TC matrix ttT ?  which is sorted by word 
representative ability. It will contain less noise and 
outperform the original matrix dtA ? . The experi-
mental results have shown the effective of this 
method.  
For one word w
d
i
a in languag , there are al-
ways several translations in language L
e La
b, say (wi1b, 
wi2b,?, wik  handle
menon, we calculate the co-occurrence of wia 
and each translation and select the highest one as 
the translation of wia.  
3 Experiments 
We adopt a VSM based IR sy
the dimensionality r
cument matrix is calculated by the TF-IDF 
weighting scheme. 
3.1 Training and test corpora 
The training corpus 
Data Consortium (http://www.chin , ab-
?2004-863-
 (?2003-863-006?). It is 
a C
breviate as CLDC). Its code number is 
009?. This parallel corpus contains parallel texts in 
Chinese, English and Japanese. It is aligned to sen-
tence level. The sentence alignment is manually 
verified and the sampling examination shows the 
accuracy reaches 99.9%.  
The experiments are conducted on two test cor-
pora. The first one is the information retrieval test 
corpus gotten from CLDC
hinese IR corpus and contains 20 topics for test. 
Each topic has key words and description and nar-
rative. The second one is the Reuters 2001 data 
(http://about.reuters.com/researchandstandards/cor
pus/ ). This corpus is a collection of about 810,000 
Reuters English news stories from August 20, 1996 
to August 19, 1997. It was used by the TREC-10
Filtering Tracks (Robertson and Soboroff, 2002). 
In TREC-10, 84 Reuters categories were used to 
simulate user profiles.  
The evaluate measure is a version of van 
Rijsbergen(1979)?s F measure with ?=1(we de-
note it as F1). 
3.2 Experimental results 
The table1 and table2 show the experimental re-
sults conducted on Chinese and English test Cor-
pus respectively. In these tables, we compare our 
method with basic LSI and LPI (Xiaofei et.al, 
2004). In the table1, the ?C-E? means the TC ma-
trix gotten from Chinese-English training collec-
tion (deduced from the trilingual training corpus). 
The ?C-J? means that the TC matrix gotten from 
Chinese-Japanese training collection, and so force 
the ?C-E-J?. All the TC matrices have been normal-
ized to range from 0 to 1. The threshold ? is used 
to truncate the TC matrix into small size. Bigger ? 
corresponds to smaller truncated TC matrix. Note 
that here ? is discrete since for some ?, the size of 
truncated matrix is very similar. For example, 
when ? = 0.85 and ? = 0.9, the size of truncated TC 
matrices are the same one.  
LSI: 0.3785,  LPI: 0.405 
? C-E C-J C-E-J 
0.3 0.404 0.4014 0.4124 
0.4 0.4098 0.406 0.4185 
0.45 0.4159 0.4185 0.4226 
0.5 0.4204 0.4124 0.4105 
0.55 0.4061 0.4027 0.3997 
0.6 0.3913 0.3992 0.396 
0.8 0.3856 0.3867 0.3842 
0.85 0.3744 0.3754 0.3768 
Table1.F1 measure of Chinese test corpus 
LSI: 0.3416,  LPI: 0.3556 
? E-C E-J E-C-J 
0.3 0.356 0.3478 0.3578 
0.4 0.3578 0.3596 0.3702 
0.45 0.3698 0.3651 0.3734 
0.5 0.3636 0.3575 0.363  
0.55 0.3523 0.3564 0.3477 
0.6 0.3422 0.3448 0.3458 
0.8 0.3406 0.3397 0.3378 
0.85 0.3304 0.3261 0.3278 
Table2. F1 measure of English test corpus 
 
From the experimental results, we can see that 
our method make great enhancement to the basic 
LSI method. And our method also outperforms the 
LPI method in both test corpora. Comparing the 
performance on different training collection, we 
can find that the difference is subtle. In Chinese 
test corpus, the TC matrix gotten from C-E-J train-
ing collection get the best performance (F1=0.4226) 
at ?=0.45 while the C-E test collection get 0.4204 
616
at ?=0.5 and the C-J test collection get 0.4185 at 
?=0.45. For the English test corpus, the trilingual 
training collection also gets the best performance. 
But the difference between bilingual and trilingual 
training collection is also subtle (E-C-J: F1=0.3734, 
E-C: F1=0.3698, E-J: F1=0.3651). In the English 
test corpus, all the training collection get the best 
performance at ?=0.45.  
As mentioned before, the bigger ? means the 
smaller size of the truncated TC matrix. While 
small size of the truncated TC matrix means low 
computational cost and high system speed. This is 
one of the advantages of our method over the tradi-
tional LSI method. We conducted some experi-
ments to test the system speed on different thresh-
old ?. We use the number of documents per sec-
ond (docs/s) to denote this kind of system speed. 
The experiment is conducted on the personal com-
puter with a Pentium (R) 4 processor @2.8GHz, 
256 KB cache and 512 MB memory. Table 3 
shows the experimental results that the ? vs. sys-
tem speed and Figure 3 illustrates the F1 measure 
vs. the system speed.  
Baseline(LSI): 566.5 docs/s 
? C-E C-J C-E-J 
0.3 1039.3 1034.4 1355.0 
0.4 1148.4 1188.9 1372.5 
0.45 1290.5 1246.9 1391.3 
0.5 1323.9 1323.3 1469.6 
0.55 1393.3 1392.6 1563.8 
0.6 1413.3 1508.8 1590.1 
0.8 1513.1 1555.6 1660.5 
0.85 1641.1 1778.2 1773.5 
Table 3. ? vs. system speed 
0.37
0.38
0.39
0.4
0.41
0.42
0.43
1000 1200 1400 1600 1800
docs/s
F 
M
ea
su
re
C-E
C-J
C-E-J
 
Figure 3. F1 measure vs. system speed 
4 Conclusions 
In this paper, we present a novel method that re-
duces the dimensionality using multilingual re-
source. We deduce a TC matrix from the multilin-
gual corpus and then truncate it to small size ac-
cording to different TC threshold. Then we use the 
truncated matrix together with the term-by-
document matrix to do the LSI analysis. Since the 
truncated TC matrix is sorted by word representa-
tive ability. It will contain less noise than the origi-
nal term-by-document matrix. The experimental 
results have shown the effectiveness of this method. 
In the future, we will try to find the optimal 
truncate threshold ? automatically. And since it 
is more difficult to get the parallel corpora than 
comparable corpora, we will explore using com-
parable corpora to do the dimensionality reduc-
tion. 
Acknowledgement 
This research was carried out through financial 
support provided under the NEDO International 
Joint Research Grant Program (NEDO Grant). 
References 
Ando R. K., ?Latent Semantic Space: Iterative Scaling 
improves precision of inter-document similarity 
measurement?, in Proc. of the 23th International 
ACM SIGIR, Athens, Greece, 2000. 
Ando R. K., and Lee L., ?Iterative Residual Rescaling: 
An Analysis and Generalization of LSI?, in Proc. of 
the 24th International ACM SIGIR, New Orleans, 
LA, 2001. 
Arampatzis A., Beney J., Koster C.H.A., and T.P. van 
der Weide. KUN on the TREC9 Filtering Track: In-
crementality, decay, and theshold optimization for 
adaptive filtering systems. The ninth Text Retrieval 
Conference, November 9-12, 2000 Gaithersburg, MD, 
Avi Arampatzis and Andre van Hameren The Score-
Distributional Threshold Optimization for Adaptive 
Binary Classification Tasks , SIGIR?01, September 
9-12,2001, New Orleans, Louisiana,USA. 285-293 
Berry M., Drmac Z., and Jessup E.. Matrices, vector 
spaces, and information retrieval. SIAM Review, 
41(2):pp335-362, 1999. 
Bingham E. and Mannila H., ?Random Projection in 
dimensionality reduction: applications to image and 
text data?, Proc. Of the seventh ACM SIGKDD In-
ternational Conference on Knowledge Discovery and 
Data Mining, p. 245-250,2001. 
Buckley C.. Implementation of the SMART information 
retrieval system. Technical Report TR85-686, De-
partment of Computer Science, Cornell University, 
617
Ithaca, NY 14853, May 1985. Source code available 
at ftp://ftp.cs.cornell.edu/pub/smart. 
C.H. Lee, H.C. Yang, and S.M. Ma, ?A Novel Multi-
Language Text Categorization System Using Latent 
Semantic Indexing?, The First International Confer-
ence on Innovative Computing, Information and 
Control (ICICIC-06), Beijing, China, 2006. 
C.J. van Rijsbergen. Information Retrieval, chapter 7. 
Butterworths, 2 edition, 1979. 
Deerwester S. C., Dumais S. T., Landauer T. K., Furnas 
G. W., and harshman R. A., ?Indexing by Latent Se-
mantic Analysis?, Journal of the American Society of 
Information Science, 41(6):391-407, 1990. 
Ding C. H.. A probabilistic model for dimensionality 
reduction in information retrieval and filtering. In 
Proc. of 1st SIAM Computational Information Re-
trieval Workshop, October 2000. 
George Karypis, Eui-Hong (Sam) Han, Fast supervise d 
dimensionality reduction algorithm with applications 
to document categorization & retrieval ,Proceedings 
of the ninth international conference on Information 
and knowledge management, November 2000 
Hofmann T., ?Probabilistic Latent Semantic Indexing?, 
in Proc. of the 22th International ACM SIGIR, 
Berkeley, California, 1999. 
Husbands, P., Simon, H., and Ding, C. Term norm dis-
tribution and its effects on latent semantic indexing, 
Information Processing and Management: an Interna-
tional Journal, v.41 n.4, p.777-787, July 2005 
Isbell C. L. and Viola P., ?Restructuring Sparse High 
Dimensional Data for Effective Retrieval?, Advances 
in Neural Information Systems, 1999. 
Jiang F. and Littman M.L., Approximate dimension 
equalization in vector-based information retrieval. 
Proc. 17th Int'l Conf. Machine Learning, 2000. 
Karypis G. and Han E.H.. Concept indexing: A fast di-
mensionality reduction algorithm with applications to 
document retrieval & categorization. Technical Re-
port TR-00-016, Department of Computer Science, 
University of USA 
Kokiopoulou E., Saad Y., Polynomial filtering in latent 
semantic indexing for information re-
trieval ,Proceedings of the 27th annual international 
ACM SIGIR conference on Research and develop-
ment in information retrieval SIGIR '04,July 2004 
Mao W. and Chu W.W.. Free-text medical document 
retrieval via phrase-based vector space model. In 
Proceedings of AMIA Annual Symp 2002. 
Minnesota, Minneapolis, 2000. Available on the WWW 
at URL http://www.cs.umn.edu/~karypis. 
Robertson SE, Walker S, Beaulieu M,Okapi at TREC-7: 
automatic ad hoc, filtering, VLC and interactive 
track- Proceedings of the seventh Text Retrieval 
Conference, TREC-7, pp. 253-264 ,1999 
Robertson, S., & Soboroff, I., The TREC-10 Filtering 
track final report. Proceeding of the Tenth Text RE-
trieval Conference (TREC-10) pp. 26-37. National 
Institute of Standards and Technology, special publi-
cation 500-250., 2002 
Salton, G, the SMART Retrieval System ? Experiments 
in Automatic Document Processing. Prentice-Hall, 
Englewood. Cliffs, New Jersey,1971. 
Salton, G., Dynamic Information and Library process-
ing. Prentice-Hall, Englewood Cliffs, New Jer-
sey,1983. 
Salton, G and McGill. M.J., Introduction to Modern 
Information retrieval. McGraw Hill, New York,1983. 
Sun, J.T. , Chen , Z. , Zeng , H.J. , Lu, Y.C. , Shi, C.Y. 
and Ma, W.Y. ,?Supervised Latent Semantic Index-
ing for Document Categorization? , In Proceedings of 
the Fourth IEEE International Conference on Data 
Mining 2004 
Xiaofei He and Partha Niyogi, ?Locality Preserving 
Projections?,in Advances in Neural Information 
Processing Systems 16, Vancouver, Canada, 2003. 
Xiaofei He, Deng Cai, Haifeng Liu, Wei-Ying Ma, Lo-
cality preserving indexing for document representa-
tion, Proceedings of the 27th annual international 
ACM SIGIR conference on Research and develop-
ment in information retrieval SIGIR '04, July 2004 
Zhai C., Jansen P., Roma N., Stoica E., and Evans D.A.. 
Optimization in CLARIT adaptive filtering. In pro-
ceeding of the Eight Text Retrieval Conference 1999, 
253-258. 
Zhang Y., and Callan J.. Maximum likelihood Estima-
tion for Filtering Thresholds. SIGIR?01, September 
9-12,2001, New Orleans, Louisiana,USA. 294-302 
618
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 199?206,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Chinese-English Term Translation Mining Based on 
Semantic Prediction  
 
 
Gaolin Fang, Hao Yu, and Fumihito Nishino 
Fujitsu Research and Development Center, Co., LTD. Beijing 100016, China  
{glfang, yu, nishino}@cn.fujitsu.com
 
  
 
Abstract 
Using abundant Web resources to mine 
Chinese term translations can be applied 
in many fields such as reading/writing as-
sistant, machine translation and cross-
language information retrieval. In mining 
English translations of Chinese terms, 
how to obtain effective Web pages and 
evaluate translation candidates are two 
challenging issues. In this paper, the ap-
proach based on semantic prediction is 
first proposed to obtain effective Web 
pages. The proposed method predicts 
possible English meanings according to 
each constituent unit of Chinese term, and 
expands these English items using 
semantically relevant knowledge for 
searching. The refined related terms are 
extracted from top retrieved documents 
through feedback learning to construct a 
new query expansion for acquiring more 
effective Web pages. For obtaining a cor-
rect translation list, a translation 
evaluation method in the weighted sum of 
multi-features is presented to rank these 
candidates estimated from effective Web 
pages. Experimental results demonstrate 
that the proposed method has good per-
formance in Chinese-English term trans-
lation acquisition, and achieves 82.9% 
accuracy. 
1 Introduction 
The goal of Web-based Chinese-English (C-E) 
term translation mining is to acquire translations 
of terms or proper nouns which cannot be looked 
up in the dictionary from the Web using a statis-
tical method, and then construct an application 
system for reading/writing assistant (e.g., ???
?The Romance of Three Kingdoms). During 
translating or writing foreign language articles, 
people usually encounter terms, but they cannot 
obtain native translations after many lookup ef-
forts. Some skilled users perhaps resort to a Web 
search engine, but a large amount of retrieved 
irrelevant pages and redundant information ham-
per them to acquire effective information. Thus, 
it is necessary to provide a system to automati-
cally mine translation knowledge of terms using 
abundant Web information so as to help users 
accurately read or write foreign language articles.  
The system of Web-based term translation 
mining has many applications. 1) Read-
ing/writing assistant. 2) The construction tool of 
bilingual or multilingual dictionary for machine 
translation. The system can not only provide 
translation candidates for compiling a lexicon, 
but also rescore the candidate list of the diction-
ary. We can also use English as a medium lan-
guage to build a lexicon translation bridge 
between two languages with few bilingual anno-
tations (e.g., Japanese and Chinese). 3) Provide 
the translations of unknown queries in cross-
language information retrieval (CLIR). 4) As one 
of the typical application paradigms of the com-
bination of CLIR and Web mining. 
Automatic acquisition of bilingual translations 
has been extensively researched in the literature. 
The methods of acquiring translations are usually 
summarized as the following six categories. 1) 
Acquiring translations from parallel corpora. To 
reduce the workload of manual annotations, re-
searchers have proposed different methods to 
automatically collect parallel corpora of different 
language versions from the Web (Kilgarriff, 
2003). 2) Acquiring translations from non-
parallel corpora (Fung, 1997; Rapp, 1999). It is 
based on the clue that the context of source term 
is very similar to that of target translation in a 
large amount of corpora. 3) Acquiring transla-
tions from a combination of translations of con-
stituent words (Li et al, 2003). 4) Acquiring 
translations using cognate matching (Gey, 2004) 
199
or transliteration (Seo et al, 2004). This method 
is very suitable for the translation between two 
languages with some intrinsic relationships, e.g., 
acquiring translations from Japanese to Chinese 
or from Korean to English. 5) Acquiring transla-
tions using anchor text information (Lu et al, 
2004). 6) Acquiring translations from the Web. 
When people use Asia language (Chinese, Japa-
nese, and Korean) to write, they often annotate 
associated English meanings after terms. With 
the development of Web and the open of accessi-
ble electronic documents, digital library, and sci-
entific articles, these resources will become more 
and more abundant. Thus, acquiring term transla-
tions from the Web is a feasible and effective 
way. Nagata et al (2001) proposed an empirical 
function of the byte distance between Japanese 
and English terms as an evaluation criterion to 
extract translations of Japanese words, and the 
results could be used as a Japanese-English dic-
tionary.   
Cheng et al (2004) utilized the Web as the 
corpus source to translate English unknown que-
ries for CLIR. They proposed context-vector and 
chi-square methods to determine Chinese transla-
tions for unknown query terms via mining of top 
100 search-result pages from Web search engines.  
Zhang and Vines (2004) proposed using a Web 
search engine to obtain translations of Chinese 
out-of-vocabulary terms from the Web to im-
prove CLIR performance. The method used Chi-
nese as query items, and retrieved previous 100 
document snippets by Google, and then estimated 
possible translations using co-occurrence infor-
mation.  
From the review above, we know that previous 
related researches didn?t concern the issue how to 
obtain effective Web pages with bilingual 
annotations, and they mainly utilized the 
frequency feature as the clue to mine the 
translation. In fact, previous 100 Web results 
seldom contain effective English equivalents. 
Apart from the frequency information, there are 
some other features such as distribution, length 
ratio, distance, keywords, key symbols and 
boundary information which have very important 
impacts on term translation mining. In this paper, 
the approach based on semantic prediction is 
proposed to obtain effective Web pages; for 
acquiring a correct translation list, the evaluation 
strategy in the weighted sum of multi-features is 
employed to rank the candidates.  
The remainder of this paper is organized as 
follows. In Section 2, we give an overview of the 
system. Section 3 proposes effective Web page 
collection. In Section 4, we introduce translation 
candidate construction and noise solution. Sec-
tion 5 presents candidate evaluation based on 
multi-features. Section 6 shows experimental 
results. The conclusion is drawn in the last sec-
tion. 
2 System Overview 
The C-E term translation mining system based on 
semantic prediction is illustrated in Figure 1. 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1. The Chinese-English term translation min-
ing system based on semantic prediction 
 
The system consists of two parts: Web page 
handling and term translation mining. Web page 
handling includes effective Web page collection 
and HTML analysis. The function of effective 
Web page collection is to collect these Web 
pages with bilingual annotations using semantic 
prediction, and then these pages are inputted into 
HTML analysis module, where possible features 
and text information are extracted. Term transla-
tion mining includes candidate unit construction, 
candidate noise solution, and rank&sort candi-
dates. Translation candidates are formed through 
candidate unit construction module, and then we 
analyze their noises and propose the correspond-
ing methods to handle them. At last, the approach 
using multi-features is employed to rank these 
candidates. 
Correctly exploring all kinds of bilingual anno-
tation forms on the Web can make a mining sys-
tem extract comprehensive translation results. 
After analyzing a large amount of Web page ex-
amples, translation distribution forms is summa-
rized as six categories in Figure 2: 1) Direct 
annotation (a). some have nothing (a1), and some 
have symbol marks (a2, a3) between the pair; 2) 
Separate annotation. There are English letters (b1) 
or some Chinese words (b2, b3) between the pair; 
3) Subset form (c); 4) Table form (d); 5) List 
form (e); and 6) Explanation form (f).  
 
 
Query  
????? 
WWW
Features
1. Frequency 
2. Distribution 
3. Distance 
4. Length ratio
5. Key symbols
and boundary 
Rank & sort 
candidates
Candidate unit 
construction Result 
?Mont Blanc?
Effective 
 Web page 
collection 
HTML 
analysis 
Candidate noise 
solution 
200
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2. The examples of translation distribution 
forms 
3 Effective Web page collection 
For mining the English translations of Chinese 
terms and proper names, we must obtain effective 
Web pages, that is, collecting these Web pages 
that contain not only Chinese characters but also 
the corresponding English equivalents. However, 
in a general Web search engine, when you input a 
Chinese technical term, the number of retrieved 
relevant Web pages is very large. It is infeasible 
to download all the Web pages because of a huge 
time-consuming process. If only the 100 abstracts 
of Web pages are used for the translation estima-
tion just as in the previous work, effective Eng-
lish equivalent words are seldom contained for 
most Chinese terms in our experiments, for ex-
ample: ?????, ????, ?????, ??
??. In this paper, a feasible method based on 
semantic prediction is proposed to automatically 
acquire effective Web pages. In the proposed 
method, possible English meanings of every con-
stituent unit of a Chinese term are predicted and 
further expanded by using semantically relevant 
knowledge, and these expansion units with the 
original query are inputted to search bilingual 
Web pages. In the retrieved top-20 Web pages, 
feedback learning is employed to extract more 
semantically-relevant terms by frequency and 
average length. The refined expansion terms, to-
gether with the original query, are once more sent 
to retrieve effective relevant Web pages. 
3.1 Term expansion  
Term expansion is to use predictive semantically-
relevant terms of target language as the expan-
sion of queries, and therefore resolve the issue 
that top retrieved Web pages seldom contain ef-
fective English annotations. Our idea is based on 
the assumption that the meanings of Chinese 
technical terms aren?t exactly known just through 
their constituent characters and words, but the 
closely related semantics and vocabulary infor-
mation may be inferred and predicted. For exam-
ple, the corresponding unit translations of a term 
?????? are respectively: three(?), country, 
nation(?), act, practice(?), and meaning, jus-
tice(?). As seen from these English translations, 
we have a general impression of ?things about 
three countries?. After expanding, the query item 
for the example above becomes "????"+ 
(three | country | nation | act | practice | meaning | 
justice). The whole procedure consists of three 
steps: unit segmentation, item translation knowl-
edge base construction, and expansion knowl-
edge base evaluation. 
Unit segmentation. Getting the constituent 
units of a technical term is a segmentation proce-
dure. Because most Chinese terms consist of out-
of-vocabulary words or meaningless characters, 
the performance using general word segmenta-
tion programs is not very desirable. In this paper, 
a segmentation method is employed to handle 
term segmentation so that possible meaningful 
constituent units are found. In the inner structure 
of proper nouns or terms, the rightmost unit usu-
ally contains a headword to reflect the major 
meaning of the term. Sometimes, the modifier 
starts from the leftmost point of a term to form a 
multi-character unit. As a result, forward maxi-
mum matching and backward maximum match-
ing are respectively conducted on the term, and 
all the overlapped segmented units are added to 
candidate items. For example, for the term 
?abcd?, forward segmented units are ?ab cd?, 
backward are ?a bcd?, so ?ab cd a bcd? will be 
viewed as our segmented items. 
Item translation knowledge base construc-
tion. Because the segmented units of a technical 
term or proper name often consist of abbreviation 
items with shorter length, limited translations 
provided by general dictionaries often cannot 
satisfy the demand of translation prediction. Here, 
a semantic expansion based method is proposed 
to construct item translation knowledge base. In 
this method, we only keep these nouns or adjec-
tive items consisting of 1-3 characters in the dic-
tionary. If an item length is greater than two 
characters and contains any item in the knowl-
edge base, its translation will be added as transla-
tion candidates of this item. For example, the 
Chinese term ????? can be segmented into 
the units ???? and ???, where ??? has only 
two English meanings ?section, thigh? in the dic-
tionary. However, we can derive its meaning us-
(a1) (a2) (a3)
(b1) (b2) (b3)
(c) (d) (e) (f)
201
ing the longer word including this item such as 
???, ???. Thus, their respective translations 
?stock, stockholder? are added into the knowl-
edge base list of ??? (see Figure 3).  
 
 
 
 
 
 
Figure 3. An expansion example in the dictionary 
knowledge base 
Expansion knowledge base evaluation. To 
avoid over-expanding of translations for one item, 
using the retrieved number from the Web as our 
scoring criterion is employed to remove irrele-
vant expansion items and rank those possible 
candidates. For example, ??? and its expansion 
translation ?stock? are combined as a new query 
?? stock ????. It is sent to a general search 
engine like Google to obtain the count number, 
where only the co-occurrence of ? ? ? and 
?stock? excluding the word ???? is counted. 
The retrieved number is about 316000. If the oc-
currence number of an item is lower than a cer-
tain threshold (100), the evaluated translation 
will not be added to the item in the knowledge 
base. Those expanded candidates for the item in 
the dictionary are sorted through their retrieved 
number. 
3.2 Feedback learning 
Though pseudo-relevance feedback (PRF) has 
been successfully used in the information re-
trieval (IR), whether PRF in single-language IR 
or pre-translation PRF and post-translation PRF 
in CLIR, the feedback results are from source 
language to source language or target language to 
target language, that is, the language of feedback 
units is same as the retrieval language. Our novel 
is that the input language (Chinese) is different 
from the feedback target language (English), that 
is, realizing the feedback from source language to 
target language, and this feedback technique is 
also first applied to the term mining field. 
After the expansion of semantic prediction, the 
predicted meaning of an item has some devia-
tions with its actual sense, so the retrieved docu-
ments are perhaps not our expected results. In 
this paper, a PRF technique is employed to ac-
quire more accurate, semantically relevant terms. 
At first, we collect top-20 documents from search 
results after term expansion, and then select 
target language units from these documents, 
get language units from these documents, which 
are highly related with the original query in 
source language. However, how to effectively 
select these units is a challenging issue. In the 
literature, researchers have proposed different 
methods such as Rocchio?s method or Robert-
son?s probabilistic method to solve this problem. 
After some experimental comparisons, a simple 
evaluation method using term frequency and av-
erage length is presented in this paper. The 
evaluation method is defined as follows: 
1)(
1)()(
+?
+= ttftw , where N
tsDt
N
i i?
=? =1 ),()(  (1) 
?(t) represents the average length between the 
source word s and the target candidate t. If the 
greater that the average length is, the relevance 
degree between source terms and candidates will 
become lower. The purpose of adding ?(t) to 1 
is to avoid the divide overflow in the case that the 
average length is equal to zero. Di(s,t) denotes the 
byte distance between source words and target 
candidates, and N represents the total number of 
candidate occurrences in the estimated Web 
pages. This evaluation method is very suitable for 
the discrimination of these words with lower, but 
same term frequencies. In the ranked candidates 
after PRF feedback, top-5 candidates are selected 
as our refined expansion items. In the previous 
example, the refined expansion items are: King-
doms, Three, Romance, Chinese, Traditional. 
These refined expansion terms, together with the 
original query, "????"+(Kingdoms | Three | 
Romance | Chinese | Traditional) are once more 
sent to retrieve relevant results, which are viewed 
as effective Web pages used in the process of the 
following estimation. 
4 Translation candidate construction and 
noise solution 
The goal of translation candidate construction is 
to construct and mine all kinds of possible trans-
lation forms of terms from the Web, and effec-
tively estimate their feature information such as 
frequency and distribution. In the transferred text, 
we locate the position of a query keyword, and 
then obtain a 100-byte window with keyword as 
the center. In this window, each English word is 
built as a beginning index, and then string candi-
dates are constructed with the increase of string 
in the form of one English word unit. String can-
didates are indexed in the database with hash and 
binary search method. If there exists the same 
item as the inputted candidate, its frequency is 
increased by 1, otherwise, this candidate is added 
?
?? ?? 
202
to this position of the database. After handling 
one Web page, the distribution information is 
also estimated at the same time. In the program-
ming implementation, the table of stop words and 
some heuristic rules of the beginning and end 
with respect to the keyword position are em-
ployed to accelerate the statistics process. 
The aim of noise solution is to remove these ir-
relevant items and redundant information formed 
in the process of mining. These noises are de-
fined as the following two categories.  
1) Subset redundancy. The characteristic is 
that this item is a subset of one item, but its fre-
quency is lower than that item. For example, ??
???License plate number (6), License plate 
(5)?, where the candidate ?License plate? belongs 
to subset redundancy. They should be removed.   
2) Affix redundancy. The characteristic is that 
this item is the prefix or suffix of one item, but its 
frequency is greater than that item. For example, 
1. ?????: Three Kingdoms (30), Romance 
of the Three Kingdoms (22), The Romance of 
Three Kingdoms (7)?, 2. ???? : Blue Chip 
(35), Blue Chip Economic Indicators (10)?. In 
Example 1, the item ?Three Kingdoms? is suffix 
redundancy and should be removed. In Example 
2, the term ?Blue Chip? is in accord with the 
definition of prefix redundancy information, but 
this term is a correct translation candidate. Thus, 
the problem of affix redundancy information is 
so complex that we need an evaluation method to 
decide to retain or drop the candidate.  
To deal with subset redundancy and affix 
redundancy information, sort-based subset 
deletion and mutual information methods are 
respectively proposed. More details refer to our 
previous paper (Fang et al, 2005). 
5 Candidate evaluation based on multi-
features 
5.1 Possible features for translation pairs 
Through analyzing mass Web pages, we obtain 
the following possible features that have impor-
tant influences on term translation mining. They 
include: 1) candidate frequency and its distribu-
tion in different Web pages, 2) length ratio be-
tween source terms and target candidates (S-T), 3) 
distance between S-T, and 4) keywords, key 
symbols and boundary information between S-T. 
1) Candidate frequency and its distribution  
Translation candidate frequency is the most 
important feature and is the basis of decision-
making. Only the terms whose frequencies are 
greater than a certain threshold are further con-
sidered as candidates in our system. Distribution 
feature reflects the occurrence information of one 
candidate in different Webs. If the distribution is 
very uniform, this candidate will more possibly 
become as the translation equivalent with a 
greater weight. This is also in accord with our 
intuition. For example, the translation candidates 
of the term ?????? include ?put option? and 
?short put?, and their frequencies are both 5. 
However, their distributions are ?1, 1, 1, 1, 1? 
and ?2, 2, 1?. The distribution of ?put option? is 
more uniform, so it will become as a translation 
candidate of ?????? with a greater weight.  
2) Length ratio between S-T 
The length ratio between S-T should satisfy 
certain constraints. Only the word number of a 
candidate falls within a certain range, the possi-
bility of becoming a translation is great.  
To estimate the length ratio relation between 
S-T, we conduct the statistics on the database 
with 5800 term translation pairs. For example, 
when Chinese term has three characters, i.e. W=3, 
the probability of English translations with two 
words is largest, about P(E=2 |W =3)= 78%, and 
there is nearly no occurrence out of the range of 
1-4. Thus, different weights can be impacted on 
different candidates by using statistical distribu-
tion information of length ratio. The weight con-
tributing to the evaluation function is set 
according to these estimated probabilities in the 
experiments. 
3) Distance between S-T 
Intuitively, if the distance between S-T is 
longer, the probability of being a translation pair 
will become smaller. Using this knowledge we 
can alleviate the effect of some noises through 
impacting different weights when we collect pos-
sible correct candidates far from the source term.  
To estimate the distance between S-T, experi-
ments are carried on 5800*200 pages with 5800 
term pairs, and statistical results are depicted as 
the histogram of distances in Figure 4. 
0
2000
4000
6000
8000
10000
12000
14000
-100 -75 -50 -25 0 25 50 75 100
 
Figure 4. The histogram of distances between S-T 
203
 
In the figure, negative value represents that 
English translation located in front of the Chinese 
term, and positive value represents English trans-
lation is behind the Chinese term. As shown from 
the figure, we know that most candidates are dis-
tributed in the range of -60-60 bytes, and few 
occurrences are out of this range. The numbers of 
translations appearing in front of the term and 
after the term are nearly equal. The curve looks 
like Gaussian probability distribution, so Gaus-
sian models are proposed to model it. By the 
curve fitting, the parameters of Gaussian models 
are obtained, i.e. u=1 and sigma=2. Thus, the 
contribution probability of distance to the ranking 
function is formulized as 
8/)1),(( 2
22
1),( ??= jiDD ejip
?
, where D(i,j) repre-
sents the byte distance between the source term i 
and the candidate j.  
4) Keywords, key symbols and boundary in-
formation between S-T 
Some Chinese keywords or capital English ab-
breviation letters between S-T can provide an 
important clue for the acquisition of possible cor-
rect translations. These Chinese keywords in-
clude the words such as ????, ????, 
????, ?????, ????, ???, 
???, ???, ????, ????, ?
????. The punctuations between S-T can also 
provide very strong constraints, for example, 
when the marks ?? ?( ) [ ]? exist, the probabil-
ity of being a translation pair will greatly increase. 
Thus, correctly judging these cases can not only 
make translation finding results more compre-
hensive, but also increase the possibility that this 
candidate is as one of correct translations. 
Boundary information refers to the fact that the 
context of candidates on the Web has distinct 
mark information, for example, the position of 
transition from continuous Chinese to English, 
the place with bracket ellipsis and independent 
units in the HTML text. 
5.2 Candidate evaluation method 
After translation noise handling, we evaluate 
candidate translations so that possible candidates 
get higher scores. The method in the weighted 
sum of multi-features including: candidate fre-
quency, distribution, length ratio, distance, key-
words, key symbols and boundary information 
between S-T, is proposed to rank the candidates. 
The evaluation method is formulized as follows:  
? ?
++=
=
N
i
j DL wjijiptsptScore
1
1 )),(),(([),()( ??  
)]),(),((max2 wjijipD
j
?? + , 121 =+ ??    (2) 
In the equation, Score(t) is proportional to 
),( tspL , N and ),( jipD . If the bigger these com-
ponent values are, the more they contribute to the 
whole evaluation formula, and correspondingly 
the candidate has higher score. The length ratio 
relation ),( tspL  reflects the proportion relation 
between S-T as a whole, so its weight will be 
impacted on the Score(t) in the macro-view. The 
weights are trained through a large amount of 
technical terms and proper nouns, where each 
relation corresponds to one probability. N de-
notes the total number of Web pages that contain 
candidates, and partly reflects the distribution 
information of candidates in different Web pages. 
If the greater N is, the greater Score(t) will be-
come. The distance relation ),( jipD  is defined as 
the distance contribution probability of the jth 
source-candidate pair on the ith Web pages, 
which is impacted on every word pair emerged 
on the Web in the point of micro-view. Its calcu-
lation formula is defined in Section 5.1. The 
weights of 1?  and 2?  represent the proportion of 
term frequency and term distribution, and 1?  de-
notes the weight of the total number of one can-
didate occurrences, and 2?  represents the weight 
of counting the nearest distance occurrence for 
each Web page. wji ),(?  is the contribution prob-
ability of keywords, key symbols and boundary 
information. If there are predefined keywords, 
key symbols, and boundary information between 
S-T, i.e., 1),( =ji? , then the evaluation formula 
will give a reward w, otherwise, 0),( =ji?  indi-
cate that there is no impact on the whole equation. 
6 Experiments 
Our experimental data consist of two sets: 400 C-
E term pairs and 3511 C-E term pairs in the fi-
nancial domain. There is no intersection between 
the two sets. Each term often consists of 2-8 Chi-
nese characters, and the associated translation 
contains 2-5 English words. In the test set of 400 
terms, there are more than one English translation 
for every Chinese term, and only one English 
translation for 3511 term pairs. In the test sets, 
Chinese terms are inputted to our system on 
batch, and their corresponding translations are 
viewed as a criterion to evaluate these mined 
candidates. The top n accuracy is defined as the 
204
percentage of terms whose top n translations in-
clude correct translations in the term pairs. A se-
ries of experiments are conducted on the two test 
sets.  
Experiments on the number of feedback 
pages: To obtain the best parameter of feedback 
Web pages that influence the whole system accu-
racy, we perform the experiments on the test set 
of 400 terms. The number of feedback Web 
pages is respectively set to 0, 10, 20, 30, and 40. 
N=1, 3, 5 represent the accuracies of top 1, 3, and 
5. From the feedback pages, previous 5 semanti-
cally-relevant terms are extracted to construct a 
new query expansion for retrieving more effec-
tive Web pages. Translation candidates are mined 
from these effective pages, whose accuracy 
curves are depicted in Figure 5. 
60
65
70
75
80
85
90
95
100
0 10 20 30 40
The number of feedback Web pages
Ac
cu
rac
y
N=1
N=3
N=5
 
Figure 5.  The number of feedback Web pages 
 
As seen from the figure above, when the num-
ber of feedback Web pages is 20, the accuracy 
reaches the best. Thus, the feedback parameter in 
our experiments is set to 20. 
Experiments on the parameter 1? : In the 
candidate evaluation method using multi-features, 
the parameter of 1?  need be chosen through the 
experiments. To obtain the best parameter, the 
experiments are set as follows. The accuracy of 
top 5 candidates is viewed as a performance cri-
terion. The parameters are respectively set from 0 
to 1 with the increase of 0.1 step. The results are 
listed in Figure 6. As seen from the figure, 
1? =0.4 is best parameter, and therefore 2? =0.6. 
In the following experiments, the parameters are 
all set to this value. 
80
85
90
95
100
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Parameter
Ac
cu
rac
y
Figure 6.  The relation between the parameter 1?  and 
the accuracy 
Experiments on the test set of 400 terms us-
ing different methods: The methods respec-
tively without prediction(NP), with prediction(P), 
with prediction and feedback(PF) only using term 
frequency (TM), and with prediction and feed-
back using multi-features(PF+MF) are employed 
on the test set of 400 terms. The results are listed 
in Table 1. As seen from this table, if there is no 
semantic prediction, the obtained translations 
from Web pages are about 48% in the top 30 
candidates. This is because general search en-
gines will retrieve more relevant Chinese Web 
pages rather than those effective pages including 
English meanings. Thus, the semantic prediction 
method is employed. Experiments demonstrate 
the method with semantic prediction distinctly 
improves the accuracy, about 36.8%. To further 
improve the performance, the feedback learning 
technique is proposed, and it increases the aver-
age accuracy of 6.5%. Though TM is very effec-
tive in mining the term translation, the multi-
feature method fully utilizes the context of can-
didates, and therefore obtains more accurate re-
sults, about 92.8% in the top 5 candidates. 
 
Table 1. The term translation results using different 
methods 
 Top30 Top10 Top5 Top3 Top1
NP 48.0 47.5 46.0 44.0 28.0
P 84.8 83.3 82.3 79.3 60.8
PF+TM 91.3 90.8 90.3 88.3 71.0
PF+MF 95.0 94.5 92.8 91.5 78.8
 
Experiments on a large vocabulary: To vali-
date our system performance, experiments are 
carried on a large vocabulary of 3511 terms using 
different methods. One method is to use term 
frequency (TM) as an evaluation criterion, and 
the other method is to use multi-features (MF) as 
an evaluation criterion. Experimental results are 
shown as follows. 
 
Table 2. The term translation results on a large vo-
cabulary 
 Top30 Top10 Top5 Top3 Top1
TM 82.5 81.2 78.3 73.5 49.4
MF 89.1 88.4 86.0 82.9 58.2
 
From Table 2, we know the accuracy with top 
5 candidates is about 86.0%. The method using 
multi-features is better than that of using term 
frequency, and improves an average accuracy of 
7.94% 
Some examples of acquiring English transla-
tions of Chinese terms are provided in Table 3. 
1?
205
Only top 3 English translations are listed for each 
Chinese term.  
 
Table 3.  Some C-E mining examples  
Chinese 
terms 
The list of English translations  
(Top 3) 
???? 
The Three Kingdoms 
The Romance of the Three Kingdoms
The Romance of Three Kingdoms 
???? 
Merit student 
"Three Goods" student 
Excellent League member 
??? 
Blue Chip 
Blue Chips 
Blue chip stocks 
??? 
Mont Blanc 
Mont-Blanc 
Chamonix Mont-Blanc 
????? 
Burmuda Triangle 
Bermuda Triangle 
The Bermuda Triangle 
??? 
License plate number 
Vehicle plate number 
Vehicle identification no 
 
7 Conclusions  
In this paper, the method based on semantic 
prediction is first proposed to acquire effective 
Web pages. The proposed method predicts 
possible meanings according to each constituent 
unit of Chinese term, and expands these items for 
searching using semantically relevant knowledge, 
and then the refined related terms are extracted 
from top retrieved documents through feedback 
learning to construct a new query expansion for 
acquiring more effective Web pages. For obtain-
ing a correct translation list, the translation 
evaluation method using multi-features is pre-
sented to rank these candidates. Experimental 
results show that this method has good perform-
ance in Chinese-English translation acquisition, 
about 82.9% accuracy in the top 3 candidates. 
References  
P.J. Cheng, J.W. Teng, R.C. Chen, et al 2004. Trans-
lating unknown queries with web corpora for 
cross-language information retrieval, Proc. ACM 
SIGIR, pp. 146-153. 
G.L. Fang, H. Yu, and F. Nishino. 2005. Web-Based 
Terminology Translation Mining, Proc. IJCNLP, 
pp. 1004-1016. 
P. Fung. 1997. Finding terminology translations from 
nonparallel corpora, Proc. Fifth Annual Work-
shop on Very Large Corpora (WVLC'97), pp. 
192-202. 
F.C. Gey. 2004. Chinese and Korean topic search of 
Japanese news collections, In Working Notes of 
the Fourth NTCIR Workshop Meeting, Cross-
Lingual Information Retrieval Task, pp. 214-218. 
A. Kilgarriff and G. Grefenstette. 2003. Introduction 
to the special issue on the Web as corpus, Com-
putational Linguistics, 29(3): 333-348.  
H. Li, Y. Cao, and C. Li. 2003.Using bilingual web 
data to mine and rank translations, IEEE Intelli-
gent Systems, 18(4): 54-59. 
W.H. Lu, L.F. Chien, and H.J. Lee. 2004. Anchor text 
mining for translation of Web queries: A transi-
tive translation approach, ACM Trans. Informa-
tion System, 22(2): 242-269. 
M. Nagata, T. Saito, and K. Suzuki. 2001. Using the 
web as a bilingual dictionary, Proc. ACL 2001 
Workshop Data-Driven Methods in Machine 
Translation, pp. 95-102. 
R. Rapp. 1999. Automatic identification of word 
translations from unrelated English and German 
corpora, Proc. 37th Annual Meeting Assoc. Com-
putational Linguistics, pp. 519-526. 
H.C. Seo, S.B. Kim, H.G. Lim and H.C. Rim. 2004. 
KUNLP system for NTCIR-4 Korean-English 
cross language information retrieval, In Working 
Notes of the Fourth NTCIR Workshop Meeting, 
Cross-Lingual Information Retrieval Task, pp. 
103-109. 
Y. Zhang and P. Vines. 2004. Using the web for 
automated translation extraction in cross-
language information retrieval, Proc. ACM 
SIGIR, pp. 162-169. 
206
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 827?834,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Infrastructure for standardization of Asian language resources
Tokunaga Takenobu
Tokyo Inst. of Tech.
Virach Sornlertlamvanich
TCL, NICT
Thatsanee Charoenporn
TCL, NICT
Nicoletta Calzolari
ILC/CNR
Monica Monachini
ILC/CNR
Claudia Soria
ILC/CNR
Chu-Ren Huang
Academia Sinica
Xia YingJu
Fujitsu R&D Center
Yu Hao
Fujitsu R&D Center
Laurent Prevot
Academia Sinica
Shirai Kiyoaki
JAIST
Abstract
As an area of great linguistic and cul-
tural diversity, Asian language resources
have received much less attention than
their western counterparts. Creating a
common standard for Asian language re-
sources that is compatible with an interna-
tional standard has at least three strong ad-
vantages: to increase the competitive edge
of Asian countries, to bring Asian coun-
tries to closer to their western counter-
parts, and to bring more cohesion among
Asian countries. To achieve this goal, we
have launched a two year project to create
a common standard for Asian language re-
sources. The project is comprised of four
research items, (1) building a description
framework of lexical entries, (2) building
sample lexicons, (3) building an upper-
layer ontology and (4) evaluating the pro-
posed framework through an application.
This paper outlines the project in terms of
its aim and approach.
1 Introduction
There is a long history of creating a standard
for western language resources. The human
language technology (HLT) society in Europe
has been particularly zealous for the standardiza-
tion, making a series of attempts such as EA-
GLES1, PAROLE/SIMPLE (Lenci et al, 2000),
ISLE/MILE (Calzolari et al, 2003) and LIRICS2.
These continuous efforts has been crystallized as
activities in ISO-TC37/SC4 which aims to make
an international standard for language resources.
1http://www.ilc.cnr.it/Eagles96/home.html
2lirics.loria.fr/documents.html
(1) Description 
framework of lexical 
entries
(2) Sample lexicons
(4) Evaluation 
through application
(3) Upper layer 
ontologyrefinement
description classification
refinement
evaluationevaluation
Figure 1: Relations among research items
On the other hand, since Asia has great lin-
guistic and cultural diversity, Asian language re-
sources have received much less attention than
their western counterparts. Creating a common
standard for Asian language resources that is com-
patible with an international standard has at least
three strong advantages: to increase the competi-
tive edge of Asian countries, to bring Asian coun-
tries to closer to their western counterparts, and to
bring more cohesion among Asian countries.
To achieve this goal, we have launched a two
year project to create a common standard for
Asian language resources. The project is com-
prised of the following four research items.
(1) building a description framework of lexical
entries
(2) building sample lexicons
(3) building an upper-layer ontology
(4) evaluating the proposed framework through
an application
Figure 1 illustrates the relations among these re-
search items.
Our main aim is the research item (1), building
a description framework of lexical entries which
827
fits with as many Asian languages as possible, and
contributing to the ISO-TC37/SC4 activities. As
a starting point, we employ an existing descrip-
tion framework, the MILE framework (Bertagna
et al, 2004a), to describe several lexical entries of
several Asian languages. Through building sam-
ple lexicons (research item (2)), we will find prob-
lems of the existing framework, and extend it so
as to fit with Asian languages. In this extension,
we need to be careful in keeping consistency with
the existing framework. We start with Chinese,
Japanese and Thai as target Asian languages and
plan to expand the coverage of languages. The re-
search items (2) and (3) also comprise the similar
feedback loop. Through building sample lexicons,
we refine an upper-layer ontology. An application
built in the research item (4) is dedicated to evalu-
ating the proposed framework. We plan to build an
information retrieval system using a lexicon built
by extending the sample lexicon.
In what follows, section 2 briefly reviews the
MILE framework which is a basis of our de-
scription framework. Since the MILE framework
is originally designed for European languages, it
does not always fit with Asian languages. We ex-
emplify some of the problems in section 3 and sug-
gest some directions to solve them. We expect
that further problems will come into clear view
through building sample lexicons. Section 4 de-
scribes a criteria to choose lexical entries in sam-
ple lexicons. Section 5 describes an approach
to build an upper-layer ontology which can be
sharable among languages. Section 6 describes
an application through which we evaluate the pro-
posed framework.
2 The MILE framework for
interoperability of lexicons
The ISLE (International Standards for Language
Engineering) Computational Lexicon Working
Group has consensually defined the MILE (Mul-
tilingual ISLE Lexical Entry) as a standardized
infrastructure to develop multilingual lexical re-
sources for HLT applications, with particular at-
tention toMachine Translation (MT) and Crosslin-
gual Information Retrieval (CLIR) application
systems.
The MILE is a general architecture devised
for the encoding of multilingual lexical informa-
tion, a meta-entry acting as a common representa-
tional layer for multilingual lexicons, by allowing
integration and interoperability between different
monolingual lexicons3.
This formal and standardized framework to en-
code MILE-conformant lexical entries is provided
to lexicon and application developers by the over-
all MILE Lexical Model (MLM). As concerns
the horizontal organization, the MLM consists of
two independent, but interlinked primary compo-
nents, the monolingual and the multilingual mod-
ules. The monolingual component, on the vertical
dimension, is organized over three different repre-
sentational layers which allow to describe differ-
ent dimensions of lexical entries, namely the mor-
phological, syntactic and semantic layers. More-
over, an intermediate module allows to define
mechanisms of linkage and mapping between the
syntactic and semantic layers. Within each layer, a
basic linguistic information unit is identified; basic
units are separated but still interlinked each other
across the different layers.
Within each of the MLM layers, different types
of lexical object are distinguished :
? the MILE Lexical Classes (MLC) represent
the main building blocks which formalize
the basic lexical notions. They can be seen
as a set of structural elements organized in
a layered fashion: they constitute an on-
tology of lexical objects as an abstraction
over different lexical models and architec-
tures. These elements are the backbone of
the structural model. In the MLM a defini-
tion of the classes is provided together with
their attributes and the way they relate to each
other. Classes represent notions like Inflec-
tionalParadigm, SyntacticFunction, Syntac-
ticPhrase, Predicate, Argument,
? the MILE Data Categories (MDC) which
constitute the attributes and values to adorn
the structural classes and allow concrete en-
tries to be instantiated. MDC can belong to
a shared repository or be user-defined. ?NP?
and ?VP? are data category instances of the
class SyntacticPhrase, whereas and ?subj?
and ?obj? are data category instances of the
class SyntacticFunction.
? lexical operations, which are special lexical
entities allowing the user to define multilin-
3MILE is based on the experience derived from exist-
ing computational lexicons (e.g. LE-PAROLE, SIMPLE, Eu-
roWordNet, etc.).
828
gual conditions and perform operations on
lexical entries.
Originally, in order to meet expectations placed
upon lexicons as critical resources for content pro-
cessing in the Semantic Web, the MILE syntactic
and semantic lexical objects have been formalized
in RDF(S), thus providing a web-based means to
implement the MILE architecture and allowing for
encoding individual lexical entries as instances of
the model (Ide et al, 2003; Bertagna et al, 2004b).
In the framework of our project, by situating our
work in the context of W3C standards and relying
on standardized technologies underlying this com-
munity, the original RDF schema for ISLE lexi-
cal entries has been made compliant to OWL. The
whole data model has been formalized in OWL by
using Prote?ge? 3.2 beta and has been extended to
cover the morphological component as well (see
Figure 2). Prote?ge? 3.2 beta has been also used as
a tool to instantiate the lexical entries of our sam-
ple monolingual lexicons, thus ensuring adherence
to the model, encoding coherence and inter- and
intra-lexicon consistency.
3 Existing problems with the MILE
framework for Asian languages
In this section, we will explain some problematic
phenomena of Asian languages and discuss pos-
sible extensions of the MILE framework to solve
them.
Inflection The MILE provides the powerful
framework to describe the information about in-
flection. InflectedForm class is devoted to de-
scribe inflected forms of a word, while Inflec-
tionalParadigm to define general inflection rules.
However, there is no inflection in several Asian
languages, such as Chinese and Thai. For these
languages, we do not use the Inflected Form and
Inflectional Paradigm.
Classifier Many Asian languages, such as
Japanese, Chinese, Thai and Korean, do not dis-
tinguish singularity and plurality of nouns, but use
classifiers to denote the number of objects. The
followings are examples of classifiers of Japanese.
? inu
(dog)
ni
(two)
hiki
(CL)
? ? ? two dogs
? hon
(book)
go
(five)
satsu
(CL)
? ? ? five books
?CL? stands for a classifier. They always follow
cardinal numbers in Japanese. Note that differ-
ent classifiers are used for different nouns. In the
above examples, classifier ?hiki? is used to count
noun ?inu (dog)?, while ?satsu? for ?hon (book)?.
The classifier is determined based on the semantic
type of the noun.
In the Thai language, classifiers are used in var-
ious situations (Sornlertlamvanich et al, 1994).
The classifier plays an important role in construc-
tion with noun to express ordinal, pronoun, for in-
stance. The classifier phrase is syntactically gener-
ated according to a specific pattern. Here are some
usages of classifiers and their syntactic patterns.
? Enumeration
(Noun/Verb)-(cardinal number)-(CL)
e.g. nakrian
(student)
3 khon
(CL)
? ? ? three students
? Ordinal
(Noun)-(CL)-/thi:/-(cardinal number)
e.g. kaew
(glass)
bai
(CL)
thi: 4
(4th)
? ? ? the 4th glass
? Determination
(Noun)-(CL)-(Determiner)
e.g. kruangkhidlek
(calculator)
kruang
(CL)
nii
(this)
? ? ? this calculator
Classifiers could be dealt as a class of the part-
of-speech. However, since classifiers depend on
the semantic type of nouns, we need to refer to
semantic features in the morphological layer, and
vice versa. Some mechanism to link between fea-
tures beyond layers needs to be introduced into the
current MILE framework.
Orthographic variants Many Chinese words
have orthographic variants. For instance, the con-
cept of rising can be represented by either char-
acter variants of sheng1: ? or ?. However,
the free variants become non-free in certain com-
pound forms. For instance, only? allowed for?
? ?liter?, and only? is allowed for?? ?to sub-
lime?. The interaction of lemmas and orthographic
variations is not yet represented in MILE.
Reduplication as a derivational process In
some Asian languages, reduplication of words de-
rives another word, and the derived word often has
a different part-of-speech. Here are some exam-
ples of reduplication in Chinese. Man4 ? ?to be
slow? is a state verb, while a reduplicated form
829
Inflectional
Paradigm
Lexical Entry SyntacticUnit
Form Lemmatized Form Stem
Inflected Form
Combiner
Calculator Mrophfeat
Operation Argument
Morph
DataCats
0..*
0..* 0..*
0..*
0..*
0..1
0..*
0..*
1..*
<LemmatizedForm rdf:ID="LFstar">
  <hasInflectedForm>
    <InflectedForm rdf:ID="stars">
      <hasMorphoFeat>
<MorphoFeat rdf:ID="pl">
  <number rdf:datatype="http://www.w3c.org/
2001/ XMLSchema#string">
    plural
  </number>
</MorphoFeat>
      </hasMorphoFeat>
    </InflectedForm>
  </hasInflectedForm>
  <hasInflectedForm>
    <InflectedForm rdf:ID="star">
      <hasMorphoFeat>
<MorphoFeat rdf:ID="sg">
  <number rdf:datatype="http://www.w3c.org/
2001/ XMLSchema#string">
    singular
  </number>
</MorphoFeat>
      </hasMorphoFeat>
    </InflectedForm>
  </hasInflectedForm>
</LemmatiedForm>
Figure 2: Formalization of the morphological layer and excerpt of a sample RDF instantiation
man4-man4 ?? is an adverb. Another example
of reduplication involves verbal aspect. Kan4 ?
?to look? is an activity verb, while the reduplica-
tive form kan4-kan4 ??, refers to the tentative
aspect, introducing either stage-like sub-division
or the event or tentativeness of the action of the
agent. This morphological process is not provided
for in the current MILE standard.
There are also various usages of reduplication in
Thai. Some words reduplicate themselves to add a
specific aspect to the original meaning. The redu-
plication can be grouped into 3 types according to
the tonal sound change of the original word.
? Word reduplication without sound change
e.g. /dek-dek/ ? ? ? (N) children, (ADV) child-
ishly, (ADJ) childish
/sa:w-sa:w/ ? ? ? (N) women
? Word reduplication with high tone on the first
word
e.g. /dam4-dam/ ? ? ? (ADJ) extremely black
/bo:i4-bo:i/ ? ? ? (ADV) really often
? Triple word reduplication with high tone on
the second word
e.g. /dern-dern4-dern/ ?? (V) intensively walk
/norn-norn4-norn/??(V) intensively sleep
In fact, only the reduplication of the same sound
is accepted in the written text, and a special sym-
bol, namely /mai-yamok/ is attached to the origi-
nal word to represent the reduplication. The redu-
plication occurs in many parts-of-speech, such as
noun, verb, adverb, classifier, adjective, preposi-
tion. Furthermore, various aspects can be added
to the original meaning of the word by reduplica-
tion, such as pluralization, emphasis, generaliza-
tion, and so on. These aspects should be instanti-
ated as features.
Change of parts-of-speech by affixes Af-
fixes change parts-of-speech of words in
Thai (Charoenporn et al, 1997). There are
three prefixes changing the part-of-speech of the
original word, namely /ka:n/, /khwa:m/, /ya:ng/.
They are used in the following cases.
? Nominalization
/ka:n/ is used to prefix an action verb and
/khwa:m/ is used to prefix a state verb
in nominalization such as /ka:n-tham-nga:n/
(working), /khwa:m-suk/ (happiness).
? Adverbialization
An adverb can be derived by using /ya:ng/ to
prefix a state verb such as /ya:ng-di:/ (well).
Note that these prefixes are also words, and form
multi-word expressions with the original word.
This phenomenon is similar to derivation which
is not handled in the current MILE framework.
Derivation is traditionally considered as a different
phenomenon from inflection, and current MILE
focuses on inflection. The MILE framework is al-
ready being extended to treat such linguistic phe-
nomenon, since it is important to European lan-
guages as well. It would be handled in either the
morphological layer or syntactic layer.
830
Function Type Function types of predicates
(verbs, adjectives etc.) might be handled in a
partially different way for Japanese. In the syn-
tactic layer of the MILE framework, Function-
Type class is prepared to denote subcategorization
frames of predicates, and they have function types
such as ?subj? and ?obj?. For example, the verb
?eat? has two FunctionType data categories of
?subj? and ?obj?. Function types basically stand
for positions of case filler nouns. In Japanese,
cases are usually marked by postpositions and case
filler positions themselves do not provide much in-
formation on case marking. For example, both of
the following sentences mean the same, ?She eats
a pizza.?
? kanojo
(she)
ga
(NOM)
piza
(pizza)
wo
(ACC)
taberu
(eat)
? piza
(pizza)
wo
(ACC)
kanojo
(she)
ga
(NOM)
taberu
(eat)
?Ga? and ?wo? are postpositions which mark
nominative and accusative cases respectively.
Note that two case filler nouns ?she? and ?pizza?
can be exchanged. That is, the number of slots is
important, but their order is not.
For Japanese, we might use the set of post-
positions as values of FunctionType instead of
conventional function types such as ?subj? and
?obj?. It might be an user defined data category or
language dependent data category. Furthermore,
it is preferable to prepare the mapping between
Japanese postpositions and conventional function
types. This is interesting because it seems more
a terminological difference, but the model can be
applied also to Japanese.
4 Building sample lexicons
4.1 Swadesh list and basic lexicon
The issue involved in defining a basic lexicon for a
given language is more complicated than one may
think (Zhang et al, 2004). The naive approach of
simply taking the most frequent words in a lan-
guage is flawed in many ways. First, all frequency
counts are corpus-based and hence inherit the bias
of corpus sampling. For instance, since it is eas-
ier to sample written formal texts, words used pre-
dominantly in informal contexts are usually under-
represented. Second, frequency of content words
is topic-dependent and may vary from corpus to
corpus. Last, and most crucially, frequency of a
word does not correlate to its conceptual necessity,
which should be an important, if not only, criteria
for core lexicon. The definition of a cross-lingual
basic lexicon is even more complicated. The first
issue involves determination of cross-lingual lexi-
cal equivalencies. That is, how to determine that
word a (and not a?) in language A really is word b
in language B. The second issue involves the deter-
mination of what is a basic word in a multilingual
context. In this case, not even the frequency of-
fers an easy answer since lexical frequency may
vary greatly among different languages. The third
issue involves lexical gaps. That is, if there is a
word that meets all criteria of being a basic word
in language A, yet it does not exist in language D
(though it may exist in languages B, and C). Is this
word still qualified to be included in the multilin-
gual basic lexicon?
It is clear not all the above issues can be un-
equivocally solved with the time frame of our
project. Fortunately, there is an empirical core lex-
icon that we can adopt as a starting point. The
Swadesh list was proposed by the historical lin-
guist Morris Swadesh (Swadesh, 1952), and has
been widely used by field and historical linguists
for languages over the world. The Swadesh list
was first proposed as lexico-statistical metrics.
That is, these are words that can be reliably ex-
pected to occur in all historical languages and can
be used as the metrics for quantifying language
variations and language distance. The Swadesh
list is also widely used by field linguists when
they encounter a new language, since almost all
of these terms can be expected to occur in any
language. Note that the Swadesh list consists of
terms that embody human direct experience, with
culture-specific terms avoided. Swadesh started
with a 215 items list, before cutting back to 200
items and then to 100 items. A standard list of
207 items is arrived at by unifying the 200 items
list and the 100 items list. We take the 207 terms
from the Swadesh list as the core of our basic lex-
icon. Inclusion of the Swadesh list also gives us
the possibility of covering many Asian languages
in which we do not have the resources to make a
full and fully annotated lexicon. For some of these
languages, a Swadesh lexicon for reference is pro-
vided by a collaborator.
4.2 Aligning multilingual lexical entries
Since our goal is to build a multilingual sample
lexicon, it is required to align words in several
831
Asian languages. In this subsection, we propose
a simple method to align words in different lan-
guages. The basic idea for multilingual alignment
is an intermediary by English. That is, first we
prepare word pairs between English and other lan-
guages, then combine them together to make cor-
respondence among words in several languages.
The multilingual alignment method currently we
consider is as follows:
1. Preparing the set of frequent words of each
language
Suppose that {Jw
i
}, {Cw
i
}, {Tw
i
} is the
set of frequent words of Japanese, Chinese
and Thai, respectively. Now we try to con-
struct a multilingual lexicon for these three
languages, however, our multilingual align-
ment method can be easily extended to han-
dle more languages.
2. Obtaining English translations
A word Xw
i
is translated into a set of En-
glish words EXw
ij
by referring to the bilin-
gual dictionary, where X denotes one of our
languages, J , C or T . We can obtain map-
pings as in (1).
Jw
1
: EJw
11
, EJw
12
, ? ? ?
Jw
2
: EJw
21
, EJw
22
, ? ? ?
...
Cw
1
: ECw
11
, ECw
12
, ? ? ?
Cw
2
: ECw
21
, ECw
22
, ? ? ?
...
Tw
1
: ETw
11
, ETw
12
, ? ? ?
Tw
2
: ETw
21
, ETw
22
, ? ? ?
...
(1)
Notice that this procedure is automatically
done and ambiguities would be left at this
stage.
3. Generating new mapping
From mappings in (1), a new mapping is gen-
erated by inverting the key. That is, in the
new mapping, a key is an English word Ew
i
and a correspondence for each key is sets
of translations XEw
ij
for 3 languages, as
shown in (2):
Ew
1
: (JEw
11
, JEw
12
, ? ? ?)
(CEw
11
, CEw
12
, ? ? ?)
(TEw
11
, TEw
12
, ? ? ?)
Ew
2
: (JEw
21
, JEw
22
, ? ? ?)
(CEw
21
, CEw
22
, ? ? ?)
(TEw
21
, TEw
22
, ? ? ?)
...
(2)
Notice that at this stage, correspondence be-
tween different languages is very loose, since
they are aligned on the basis of sharing only
a single English word.
4. Refinement of alignment
Groups of English words are constructed by
referring to the WordNet synset information.
For example, suppose that Ew
i
and Ew
j
be-
long to the same synset S
k
. We will make a
new alignment by making an intersection of
{XEw
i
} and {XEw
j
} as shown in (3).
Ew
i
: (JEw
i1
, ??) (CEw
i1
, ??) (TEw
i1
, ??)
Ew
j
: (JEw
j1
, ??)(CEw
j1
, ??)(TEw
j1
, ??)
? intersection
S
k
: (JEw?
k1
, ??)(CEw?
k1
, ??)(TEw?
k1
, ??)
(3)
In (3), the key is a synset S
k
, which is sup-
posed to be a conjunction of Ew
i
and Ew
j
,
and the counterpart is the intersection of set
of translations for each language. This oper-
ation would reduce the number of words of
each language. That means, we can expect
that the correspondence among words of dif-
ferent languages becomes more precise. This
new word alignment based on a synset is a
final result.
To evaluate the performance of this method,
we conducted a preliminary experiment using the
Swadesh list. Given the Swadesh list of Chi-
nese, Italian, Japanese and Thai as a gold stan-
dard, we tried to replicate these lists from the En-
glish Swadesh list and bilingual dictionaries be-
tween English and these languages. In this experi-
ment, we did not perform the refinement step with
WordNet. From 207 words in the Swadesh list,
we dropped 4 words (?at?, ?in?, ?with? and ?and?)
due to their too many ambiguities in translation.
As a result, we obtained 181 word groups
aligned across 5 languages (Chinese, English, Ital-
ian, Japanese and Thai) for 203 words. An
aligned word group was judged ?correct? when the
words of each language include only words in the
Swadesh list of that language. It was judged ?par-
tially correct? when the words of a language also
include the words which are not in the Swadesh
list. Based on the correct instances, we obtain
0.497 for precision and 0.443 for recall. These fig-
ures go up to 0.912 for precision and 0.813 for re-
call when based on the partially correct instances.
This is quite a promising result.
832
5 Upper-layer ontology
The empirical success of the Swadesh list poses
an interesting question that has not been explored
before. That is, does the Swadesh list instantiates a
shared, fundamental human conceptual structure?
And if there is such as a structure, can we discover
it?
In the project these fundamental issues are as-
sociated with our quest for cross-lingual interop-
erability. We must make sure that the items of
the basic lexicon are given the same interpreta-
tion. One measure taken to ensure this consists in
constructing an upper-ontology based on the ba-
sic lexicon. Our preliminary work of mapping the
Swadesh list items to SUMO (Suggested Upper
Merged Ontology) (Niles and Pease, 2001) has al-
ready been completed. We are in the process of
mapping the list to DOLCE (Descriptive Ontology
for Linguistic and Cognitive Engineering) (Ma-
solo et al, 2003). After the initial mapping, we
carry on the work to restructure the mapped nodes
to form a genuine conceptual ontology based on
the language universal basic lexical items. How-
ever one important observation that we have made
so far is that the success of the Swadesh list is
partly due to its underspecification and to the lib-
erty it gives to compilers of the list in a new lan-
guage. If this idea of underspecification is essen-
tial for basic lexicon for human languages, then we
must resolve this apparent dilemma of specifying
them in a formal ontology that requires fully spec-
ified categories. For the time being, genuine ambi-
guities resulted in the introduction of each disam-
biguated sense in the ontology. We are currently
investigating another solution that allows the in-
clusion of underspecified elements in the ontology
without threatening its coherence. More specifi-
cally we introduce a underspecified relation in the
structure for linking the underspecified meaning
to the different specified meaning. The specified
meanings are included in the taxonomic hierarchy
in a traditional manner, while a hierarchy of un-
derspecified meanings can be derived thanks to the
new relation. An underspecified node only inherits
from the most specific common mother of its fully
specified terms. Such distinction avoids the clas-
sical misuse of the subsumption relation for rep-
resenting multiple meanings. This method does
not reflect a dubious collapse of the linguistic and
conceptual levels but the treatment of such under-
specifications as truly conceptual. Moreover we
Internet
Query
Local 
DB
User interest
 model
Topic
Feedback
Search
engine
Crawler
Retrieval
results
Figure 3: The system architecture
hope this proposal will provide a knowledge rep-
resentation framework for the multilingual align-
ment method presented in the previous section.
Finally, our ontology will not only play the role
of a structured interlingual index. It will also serve
as a common conceptual base for lexical expan-
sion, as well as for comparative studies of the lex-
ical differences of different languages.
6 Evaluation through an application
To evaluate the proposed framework, we are build-
ing an information retrieval system. Figure 3
shows the system architecture.
A user can input a topic to retrieve the docu-
ments related to that topic. A topic can consist
of keywords, website URL?s and documents which
describe the topic. From the topic information, the
system builds a user interest model. The system
then uses a search engine and a crawler to search
for information related to this topic in WWW and
stores the results in the local database. Generally,
the search results include many noises. To filter
out these noises, we build a query from the user
interest model and then use this query to retrieve
documents in the local database. Those documents
similar to the query are considered as more related
to the topic and the user?s interest, and are returned
to the user. When the user obtains these retrieval
results, he can evaluate these documents and give
the feedback to the system, which is used for the
further refinement of the user interest model.
Language resources can contribute to improv-
ing the system performance in various ways.
Query expansion is a well-known technique which
expands user?s query terms into a set of similar and
related terms by referring to ontologies. Our sys-
tem is based on the vector space model (VSM) and
traditional query expansion can be applicable us-
ing the ontology.
There has been less research on using lexical in-
833
formation for information retrieval systems. One
possibility we are considering is query expansion
by using predicate-argument structures of terms.
Suppose a user inputs two keywords, ?hockey?
and ?ticket? as a query. The conventional query
expansion technique expands these keywords to
a set of similar words based on an ontology. By
referring to predicate-argument structures in the
lexicon, we can derive actions and events as well
which take these words as arguments. In the above
example, by referring to the predicate-argument
structure of ?buy? or ?sell?, and knowing that
these verbs can take ?ticket? in their object role,
we can add ?buy? and ?sell? to the user?s query.
This new type of expansion requires rich lexical
information such as predicate argument structures,
and the information retrieval system would be a
good touchstone of the lexical information.
7 Concluding remarks
This paper outlined a new project for creating a
common standard for Asian language resources
in cooperation with other initiatives. We start
with three Asian languages, Chinese, Japanese
and Thai, on top of the existing framework which
was designed mainly for European languages.
We plan to distribute our draft to HLT soci-
eties of other Asian languages, requesting for
their feedback through various networks, such
as the Asian language resource committee net-
work under Asian Federation of Natural Language
Processing (AFNLP)4, and Asian Language Re-
source Network project5. We believe our ef-
forts contribute to international activities like ISO-
TC37/SC46 (Francopoulo et al, 2006) and to the
revision of the ISO Data Category Registry (ISO
12620), making it possible to come close to the
ideal international standard of language resources.
Acknowledgment
This research was carried out through financial
support provided under the NEDO International
Joint Research Grant Program (NEDO Grant).
References
F. Bertagna, A. Lenci, M. Monachini, and N. Calzo-
lari. 2004a. Content interoperability of lexical re-
sources, open issues and ?MILE? perspectives. In
4http://www.afnlp.org/
5http://www.language-resource.net/
6http://www.tc37sc4.org/
Proceedings of the 4th International Conference on
Language Resources and Evaluation (LREC2004),
pages 131?134.
F. Bertagna, A. Lenci, M. Monachini, and N. Calzo-
lari. 2004b. The MILE lexical classes: Data cat-
egories for content interoperability among lexicons.
In A Registry of Linguistic Data Categories within
an Integrated Language Resources Repository Area
? LREC2004 Satellite Workshop, page 8.
N. Calzolari, F. Bertagna, A. Lenci, and M. Mona-
chini. 2003. Standards and best practice for mul-
tilingual computational lexicons. MILE (the mul-
tilingual ISLE lexical entry). ISLE Deliverable
D2.2&3.2.
T. Charoenporn, V. Sornlertlamvanich, and H. Isahara.
1997. Building a large Thai text corpus ? part-
of-speech tagged corpus: ORCHID?. In Proceed-
ings of the Natural LanguageProcessing Pacific Rim
Symposium.
G. Francopoulo, G. Monte, N. Calzolari, M. Mona-
chini, N. Bel, M. Pet, and C. Soria. 2006. Lex-
ical markup framework (LMF). In Proceedings of
LREC2006 (forthcoming).
N. Ide, A. Lenci, and N. Calzolari. 2003. RDF in-
stantiation of ISLE/MILE lexical entries. In Pro-
ceedings of the ACL 2003 Workshop on Linguistic
Annotation: Getting the Model Right, pages 25?34.
A. Lenci, N. Bel, F. Busa, N. Calzolari, E. Gola,
M. Monachini, A. Ogonowsky, I. Peters, W. Peters,
N. Ruimy, M. Villegas, and A. Zampolli. 2000.
SIMPLE: A general framework for the development
of multilingual lexicons. International Journal of
Lexicography, Special Issue, Dictionaries, Thesauri
and Lexical-Semantic Relations, XIII(4):249?263.
C. Masolo, A. Borgo, S.; Gangemi, N. Guarino, and
A. Oltramari. 2003. Wonderweb deliverable d18
?ontology library (final)?. Technical report, Labo-
ratory for Applied Ontology, ISTC-CNR.
I. Niles and A Pease. 2001. Towards a standard upper
ontology. In Proceedings of the 2nd International
Conference on Formal Ontology in Information Sys-
tems (FOIS-2001).
V. Sornlertlamvanich, W. Pantachat, and S. Mek-
navin. 1994. Classifier assignment by corpus-
based approach. In Proceedings of the 15th Inter-
national Conference on Computational Linguistics
(COLING-94), pages 556?561.
M. Swadesh. 1952. Lexico-statistical dating of pre-
historic ethnic contacts: With special reference to
north American Indians and Eskimos. In Proceed-
ings of the American Philo-sophical Society, vol-
ume 96, pages 452?463.
H. Zhang, C. Huang, and S. Yu. 2004. Distributional
consistency: A general method for defining a core
lexicon. In Proceedings of the 4th International
Conference on Language Resources and Evaluation
(LREC2004), pages 1119?1222.
834
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 653?661,
Beijing, August 2010
Structure-Aware Review Mining and Summarization
Fangtao Li1, Chao Han1, Minlie Huang1, Xiaoyan Zhu1,
Ying-Ju Xia2, Shu Zhang2 and Hao Yu2
1State Key Laboratory of Intelligent Technology and Systems?
1Tsinghua National Laboratory for Information Science and Technology?
1Department of Computer Science and Technology, Tsinghua University
2Fujitsu Research and Development Center
fangtao06@gmail.com; zxy_dcs@tsinghua.edu.cn
Abstract
In this paper, we focus on object feature 1
1 Introduction
based review summarization. Different from 
most of previous work with linguistic rules or 
statistical methods, we formulate the review
mining task as a joint structure tagging prob-
lem. We propose a new machine learning 
framework based on Conditional Random 
Fields (CRFs). It can employ rich features to 
jointly extract positive opinions, negative opi-
nions and object features for review sentences.
The linguistic structure can be naturally inte-
grated into model representation. Besides li-
near-chain structure, we also investigate con-
junction structure and syntactic tree structure
in this framework. Through extensive experi-
ments on movie review and product review 
data sets, we show that structure-aware mod-
els outperform many state-of-the-art ap-
proaches to review mining.
With the rapid expansion of e-commerce, people 
are more likely to express their opinions and 
hands-on experiences on products or services
they have purchased. These reviews are impor-
tant for both business organizations and personal 
costumers. Companies can decide on their strat-
egies for marketing and products improvement. 
Customers can make a better decision when pur-
1 Note that there are two meanings for word ?feature?. 
We use ?object feature? to represent the target entity,
which the opinion expressed on, and use ?feature? as
the input for machine learning methods.
chasing products or services. Unfortunately, 
reading through all customer reviews is difficult, 
especially for popular items, the number of re-
views can be up to hundreds or even thousands. 
Therefore, it is necessary to provide coherent 
and concise summaries for these reviews.
Figure 1. Feature based Review Summarization
Inspired by previous work (Hu and Liu, 2004; 
Jin and Ho, 2009), we aim to provide object fea-
ture based review summarization. Figure 1 
shows a summary example for movie ?Gone 
with the wind?. The object (movie) features, 
such as ?movie?, ?actor?, with their correspond-
ing positive opinions and negative opinions, are 
listed in a structured way. The opinions are 
ranked by their frequencies. This provides a con-
cise view for reviews. To accomplish this goal, 
we need to do three tasks:  1), extract all the ob-
ject features and opinions; 2), determine the sen-
timent polarities for opinions; 3), for each object 
feature, determine the relevant opinions, i.e. ob-
ject feature-opinion pairs.
For the first two tasks, most previous studies
employ linguistic rules or statistical methods (Hu 
and Liu, 2004; Popescu and Etzioni 2005). They 
mainly use unsupervised learning methods,
which lack an effective way to address infre-
quent object features and opinions. They are also
hard to incorporate rich overlapping features.
Gone With The Wind:
Movie:
     Positive: great, good, amazing, ? , breathtaking
     Negative: bad, boring, waste time, ? , mistake
Actor: 
     Positive: charming , brilliant , great, ? , smart 
     Negative: poor, fail, dirty, ? , lame
Music:
     Positive: great, beautiful, very good, ? , top
     Negative: annoying, noise, too long, ? , unnecessary 
    ? ?
653
Actually, there are many useful features, which 
have not been fully exploited for review mining.
Meanwhile, most of previous methods extract 
object features, opinions, and determine the po-
larities for opinions separately. In fact, the object 
features, positive opinions and negative opinions
correlate with each other. 
In this paper, we formulate the first two tasks,
i.e. object feature, opinion extraction and opi-
nion polarity detection, as a joint structure tag-
ging problem, and propose a new machine learn-
ing framework based on Conditional Random 
Fields (CRFs). For each sentence in reviews, we 
employ CRFs to jointly extract object features,
positive opinions and negative opinions, which 
appear in the review sentence. This framework
can naturally encode the linguistic structure. Be-
sides the neighbor context with linear-chain 
CRFs, we propose to use Skip-chain CRFs and 
Tree CRFs to utilize the conjunction structure
and syntactic tree structure. We also propose a
new unified model, Skip-Tree CRFs to integrate 
these structures. Here, ?structure-aware? refers 
to the output structure, which model the relation-
ship among output labels. This is significantly 
different from the previous input structure me-
thods, which consider the linguistic structure as 
heuristic rules (Ding and Liu, 2007) or input fea-
tures for classification (Wilson et al 2009). Our 
proposed framework has the following advan-
tages: First, it can employ rich features for re-
view mining. We will analyze the effect of fea-
tures for review mining in this framework.
Second, the framework can utilize the relation-
ship among object features, positive opinions 
and negative opinions. It jointly extracts these 
three types of expressions in a unified way.
Third, the linguistic structure information can be 
naturally integrated into model representation,
which provides more semantic dependency for 
output labels. Through extensive experiments on 
movie review and product review, we show our 
proposed framework is effective for review min-
ing.
The rest of this paper is organized as follows: 
In Section 2, we review related work. We de-
scribe our structure aware review mining me-
thods in Section 3. Section 4 demonstrates the 
process of summary generation. In Section 5, we 
present and discuss the experiment results. Sec-
tion 6 is the conclusion and future work.
2 Related Work
Object feature based review summary has been 
studied in several papers. Zhuang et al (2006) 
summarized movie reviews by extracting object 
feature keywords and opinion keywords. Object 
feature-opinion pairs were identified by using a 
dependency grammar graph. However, it used a
manually annotated list of keywords to recognize 
movie features and opinions, and thus the system 
capability is limited. Hu and Liu (2004) pro-
posed a statistical approach to capture object 
features using association rules. They only con-
sidered adjective as opinions, and the polarities 
of opinions are recognized with WordNet expan-
sion to manually selected opinion seeds. Popescu 
and Etzioni (2005) proposed a relaxation labe-
ling approach to utilize linguistic rules for opi-
nion polarity detection. However, most of these 
studies focus on unsupervised methods, which
are hard to integrate various features. Some stu-
dies (Breck et al 2007; Wilson et al 2009; Ko-
bayashi et al 2007) have used classification 
based methods to integrate various features. But 
these methods separately extract object features
and opinions, which ignore the correlation 
among output labels, i.e. object features and opi-
nions. Qiu et al (2009) exploit the relations of 
opinions and object features by adding some lin-
guistic rules. However, they didn?t care the opi-
nion polarity. Our framework can not only em-
ploy various features, but also exploit the corre-
lations among the three types of expressions, i.e.
object features, positive opinions, and negative 
opinions, in a unified framework. Recently, Jin 
and Ho (2009) propose to use Lexicalized HMM
for review mining. Lexicalized HMM is a va-
riant of HMM. It is a generative model, which is 
hard to integrate rich, overlapping features. It 
may encounter sparse data problem, especially 
when simultaneously integrating multiple fea-
tures. Our framework is based on Conditional 
Random Fields (CRFs). CRFs is a discriminative 
model, which can easily integrate various fea-
tures.
These are some studies on opinion mining with 
Conditional Random Fields. For example, with 
CRFs, Zhao et al(2008) and McDonald et al 
(2007) performed sentiment classification in sen-
tence and document level; Breck et al(2007) 
identified opinion expressions from newswire 
documents; Choi et al (2005) determined opi-
654
nion holders to opinions also from newswire da-
ta. None of previous work focuses on jointly ex-
tracting object features, positive opinions and 
negative opinions simultaneously from review 
data. More importantly, we also show how to 
encode the linguistic structure, such as conjunc-
tion structure and syntactic tree structure, into 
model representation in our framework. This is 
significantly different from most of previous 
studies, which consider the structure information 
as heuristic rules (Hu and Liu, 2004) or input 
features (Wilson et al 2009).
Recently, there are some studies on joint sen-
timent/topic extraction (Mei et al 2007; Titov 
and McDonald, 2008; Snyder and Barzilay, 
2007). These methods represent reviews as sev-
eral coarse-grained topics, which can be consi-
dered as clusters of object features. They are
hard to indentify the low-frequency object fea-
tures and opinions. While in this paper, we will 
extract all the present object features and corres-
ponding opinions with their polarities. Besides, 
the joint sentiment/topic methods are mainly
based on review document for topic extraction.
In our framework, we focus on sentence-level
review extraction.
3 Structure Aware Review Mining
3.1 Problem Definition
To produce review summaries, we need to first 
finish two tasks: identifying object features, opi-
nions, and determining the polarities for opi-
nions. In this paper, we formulate these two 
tasks as a joint structure tagging problem. We
first describe some related definitions:
Definition (Object Feature): is defined as whole 
target expression that the subjective expressions 
have been commented on. Object features can be 
products, services or their elements and proper-
ties, such as ?character?, ?movie?, ?director? for 
movie review, and ?battery?, ?battery life?,
?memory card? for product review.
Definition (Review Opinion): is defined as the 
whole subjective expression on object features.
For example, in sentence ?The camera is easy to 
use?, ?easy to use? is a review opinion. ?opinion? 
is used for short.
Definition (Opinion Polarity): is defined as the 
sentiment category for review opinion. In this 
paper, we consider two types of polarities: posi-
tive opinion and negative opinion. For example,
?easy to use? belongs to positive opinion.
For our review mining task, we need to 
represent three types of expressions: object fea-
tures, positive opinions, and negative opinions. 
These expressions may be words, or whole
phrases. We use BIO encoding for tag represen-
tation, where the non-opinion and neutral opi-
nion words are represented as ?O?. With Nega-
tion (N), which is only one word, such as ?not?,
?don?t?, as an independent tag, there are totally 8 
tags, as shown in Table 1. The following is an 
example to denote the tags:
The/O camera/FB comes/O with/O a/O piti-
ful/CB 32mb/FB compact/FI flash/FI card/FI ./O
FB Feature Beginning CB Negative Beginning
FI Feature Inside CI Negative Inside
PB Positive Beginning N Negation Word 
PI Positive Inside O Other 
Table 1. Basic Tag Set for Review Mining
3.2 Structure Aware Model
In this section, we describe how to encode dif-
ferent linguistic structure into model representa-
tion based on our CRFs framework.
3.2.1 Using Linear CRFs.
For each sentence in a review, our task is to ex-
tract all the object features, positive opinions and 
negative opinions. This task can be modeled as a 
classification problem. Traditional classification 
tools, e.g. Maximum Entropy model (Berger et 
al, 1996), can be employed, where each word or 
phrase will be treated as an instance. However, 
they independently consider each word or 
phrase, and ignore the dependency relationship 
among them.
Actually, the context information plays an im-
portant role for review mining. For example, 
given two continuous words with same part of 
speech, if the previous word is a positive opi-
nion, the next word is more likely a positive opi-
nion. Another example is that if the previous 
word is an adjective, and it is an opinion, the 
next noun word is more likely an object feature.
To this end, we formulate the review mining 
task as a joint structure tagging problem, and 
propose a general framework based on Condi-
tional Random Fields (CRFs) (Lafferty et al, 
2001) which are able to model the dependencies 
655
y1 yn-1y3y2 yn
x1 xn-1x3x2 xn
(a) Linear-chain  CRFs
y4
x1 xn-1x3x2 xnxn-2
?
x4
y1 yn-2y3 yn
y2 yn-1
(c) Tree-CRFs
y4
x1 xn-1x3x2 xnxn-2
?
x4
y1 yn-2y3 yn
y2 yn-1
(d) Skip-Tree CRFs
(b) Skip-chain  CRFs
Figure 2 CRFs models
between nodes. (See Section 3.2.5 for more 
about CRFs)
In this section, we propose to use linear-chain
CRFs to model the sequential dependencies be-
tween continuous words, as discussed above. It 
views each word in the sentence as a node, and 
adjacent nodes are connected by an edge. The 
graphical representation is shown in Figure 2(a).
Linear CRFs can make use of dependency rela-
tionship among adjacent words.
3.2.2 Leveraging Conjunction Structure
We observe that the conjunctions play important 
roles on review mining: If the words or phrases 
are connected by conjunction ?and?, they mostly 
belong to the same opinion polarity. If the words 
or phrases are connected by conjunction ?but?, 
they mostly belong to different opinion polarity,
as reported in (Hatzivassiloglou and McKeown,
1997; Ding and Liu, 2007). For example, ?This
phone has a very cool and useful feature ? the
speakerphone?, if we only detect ?cool?, it is 
hard to determine its opinion polarity. But if we 
see ?cool? is connected with ?useful? by con-
junction ?and?, we can easily acquire the polari-
ty of ?cool? as positive. This conjunction struc-
ture not only helps to determine the opinions, but 
also helps to recognize object features. For ex-
ample, ?I like the special effects and music in 
this movie?, with word ?music? and conjunction
?and?, we can easily detect that ?special effects? 
as an object feature.
To model the long distance dependency with 
conjunctions, we use Skip-chain CRFs model to 
detect object features and opinions. The graphi-
cal representation of a Skip-chain CRFs, given in 
Figure 2(b), consists of two types of edges: li-
near-edge (

to 

) and skip-edge (

to 

). 
The linear-edge is described as linear CRFs. The 
skip-edge is imported as follows:
We first identify the conjunctions in the re-
view sentence, with a collected conjunction set,
including ?and?, ?but?, ?or?, ?however?, ?al-
though? etc. For each conjunction, we extract its 
connected two text sequences. The nearest two 
words with same part of speech from the two 
text sequences are connected with the skip-edge. 
Here, we just consider the noun, adjective, and 
adverb. For example, in ?good pictures and 
beautiful music?, there are two skip-edges: one 
connects two adjective words ?good? and ?beau-
tiful?; the other connects two nouns ?pictures? 
and ?music?. We also employ the general senti-
ment lexicons, SentiWordNet (Esuli and Sebas-
tiani, 2006), to connect opinions. Two nearest 
opinion words, detected by sentiment lexicon,
from two sequences, will also be connected by 
skip-edge. If the nearest distance exceeds the 
threshold, this skip edge will be discarded. Here,
we consider the threshold as nine.
Skip-chain CRFs improve the performance of 
review mining, because it naturally encodes the 
conjunction structure into model representation 
with skip-edges.
3.2.3 Leveraging Syntactic Tree Structure
Besides the conjunction structure, the syntactic 
tree structure also helps for review mining. The
tree denotes the syntactic relationship among 
words. In a syntactic dependency representation, 
each node is a surface word. For example, the 
corresponding dependency tree (Klein and Man-
ning, 2003) for the sentence, ?I really like this 
long movie?, is shown in Figure 3.
y1 yn-1y3y2 yn
x1 xn-1x3x2 xn
656
like
longthis
really movieI
nsubj dobjadvmod
det amod
Figure 3. Syntactic Dependency Tree Representation
In linear-chain structure and skip-chain structure, 
?like? and ?movie? have no direct edge, but in 
syntactic tree, ?movie? is directly connected 
with ?like?, and their relationship ?dobj? is also 
included, which shows ?movie? is an objective 
of ?like?. It can provide deeper syntactic depen-
dencies for object features, positive opinions and 
negative opinions. Therefore, it is important to 
consider the syntactic structure in the review 
mining task. 
In this section, we propose to use Tree CRFs to
model the syntactic tree structure for review 
mining. The representation of a Tree CRFs is 
shown in Figure 2(c). The syntactic tree structure 
is encoded into our model representation. Each 
node is corresponding to a word in the depen-
dency tree. The edge is corresponding to depen-
dency tree edge. Tree CRFs can make use of de-
pendency relationship in syntactic tree structure
to boost the performance.
3.2.4 Integrating Conjunction Structure and 
Syntactic Tree Structure
Conjunction structure provides the semantic re-
lations correlated with conjunctions. Syntactic 
tree structure provides dependency relation in 
the syntactic tree. They represent different se-
mantic dependencies. It is interesting to consider 
these two dependencies in a unified model. We 
propose Skip-Tree CRFs, to combine these two 
structure information. The graphical representa-
tion of a Skip-Tree CRFs, given in Figure 2(d),
consists of two types of edges: tree edges and 
conjunction skip-edges. We hope to simulta-
neously model the dependency in conjunction 
structure and syntactic tree structure.
We also notice that there is a relationship 
?conj? in syntactic dependency tree. However, 
we find that it only connects two head words for 
a few coordinating conjunction, such as ?and", 
?or", ?but?. Our designed conjunction skip-edge
provides more information for joint structure 
tagging. We analyze more conjunctions to con-
nect not only two head words, but also the words 
with same part of speech. We also connect the 
words with sentiment lexicon. We will show that 
the skip-tree CRFs, which combine the two 
structures, is effective in the experiment section.
3.2.5 Conditional Random Fields
A CRFs is an undirected graphical model G of 
the conditional distribution (	|
). Y are the 
random variables over the labels of the nodes 
that are globally conditioned on X, which are the 
random variables of the observations. The condi-
tional probability is defined as: 
P
(
	 
|


)
=  
1
(
)
    



(, 	|, 
)
,
+   



(, 	|, 
)
Coling 2010: Poster Volume, pages 383?390,
Beijing, August 2010
Learning Phrase Boundaries
for Hierarchical Phrase-based Translation
Zhongjun HE Yao MENG Hao YU
Fujitsu R&D Center CO., LTD.
{hezhongjun, mengyao, yu}@cn.fujitsu.com
Abstract
Hierarchical phrase-based models pro-
vide a powerful mechanism to capture
non-local phrase reorderings for statis-
tical machine translation (SMT). How-
ever, many phrase reorderings are arbi-
trary because the models are weak on de-
termining phrase boundaries for pattern-
matching. This paper presents a novel
approach to learn phrase boundaries di-
rectly from word-aligned corpus without
using any syntactical information. We use
phrase boundaries, which indicate the be-
ginning/ending of phrase reordering, as
soft constraints for decoding. Experi-
mental results and analysis show that the
approach yields significant improvements
over the baseline on large-scale Chinese-
to-English translation.
1 Introduction
The hierarchial phrase-based (HPB) model (Chi-
ang, 2005) outperformed previous phrase-based
models (Koehn et al, 2003; Och and Ney, 2004)
by utilizing hierarchical phrases consisting of both
words and variables. Thus the HPB model has
generalization ability: a translation rule learned
from a phrase pair can be used for other phrase
pairs with the same pattern, e.g. reordering infor-
mation of a short span can be applied for a large
span during decoding. Therefore, the model cap-
tures both short and long distance phrase reorder-
ings.
However, one shortcoming of the HPB model is
that it is difficult to determine phrase boundaries
for pattern-matching. Therefore, during decod-
ing, a rule may be applied for all possible source
phrases with the same pattern. However, incorrect
pattern-matching will cause wrong translation.
Consider the following rule that is used to trans-
late the Chinese sentence in Figure 1 into English:
X ? ?XL de XR, XR in XL? (1)
The rule translates the Chinese word ?de? into
English word ?in?, and swaps the left sub-phrase
covered by XL and the right sub-phrase covered
by XR on the target side. However, XL may
pattern-match 5 spans on the left side of ?de? and
XR may pattern-match 3 spans on the right side.
Therefore, the rule produces 15 different deriva-
tions. However, 14 of them are incorrect.
The correct derivation Sc is shown in Figure 2,
while one of the wrong derivations Si is shown in
Figure 3. We observe that the basic difference be-
tween Sc and Si is the phrase boundary matched
by ?XR?. In Sc, XR matches the span [7, 9] and
moves it as a whole unit. While in Si, XR matches
the span [7, 8] and left the last word [9, 9] be trans-
lated separately. Similarly, other incorrect deriva-
tions are caused by inadequate pattern-matching
of XL and/or XR.
Previous research showed that phrases should
be constrained to some extent for improving trans-
lation quality. Most of the existing approaches uti-
lized syntactic information to constrain phrases to
respect syntactic boundaries. Chiang (2005) in-
troduced a constituent feature to reward phrases
that match a syntactic tree but did not yield signif-
icant improvement. Marton and Resnik (2008) re-
vised this method by distinguishing different con-
stituent syntactic types, and defined features for
each type to count whether a phrase matches or
crosses the syntactic boundary. This led to a sub-
stantial improvements. Gimpel and Smith (2008)
presented rich contextual features on the source
side including constituent syntactical features for
phrase-based translation. Cherry (2008) utilized
a dependency tree as a soft constraint to detect
syntactic cohesion violations for a phrase-based
383
?1
ta
?2
jiang
??3
chengwei
??4
yindu
????5
youshiyilai
?6
de
??7
shouwei
?8
n?
??9
zongtong
She1 will2 become3 the4 first5 female6 president7 in8 India?s9 history10
X[5,5]
X[4,5]
X[3,5]
X[2,5]
X[1,5]
X[7,7]
X[7,8]
X[7,9]
Figure 1: An example of Chinese-English translation. The rule X ? ?XL de XR, XR in XL?
pattern-matches 5 and 3 spans on the left and right of the Chinese word ?de?, respectively.
Sc ? ????? X, She will become X?
? ????? X[4,5]? X[7,9], She will become X[7,9] in X[4,5]?
? ????? ??????? ?? ??????,
She will become the first female president in India?s history?
Figure 2: The correct derivation with adequate pattern-matching of XR.
Si ? ????? X ??, She will become X president?
? ????? X[4,5]? X[7,8]??, She will become X[7,8] in X[4,5] president?
? ????? ??????? ?? ???? ???,
She will become the first female in India?s history president?
Figure 3: A wrong derivation with inadequate pattern-matching of XR.
system. Xiong et al (2009) presented a syntax-
driven bracketing model to predict whether two
phrases are translated together or not, using syn-
tactic features learned from training corpus. Al-
though these approaches differ from each other,
the main basic idea is the utilization of syntactic
information.
In this paper, we present a novel approach to
learn phrase boundaries for hierarchical phrase-
based translation. A phrase boundary indicates the
beginning or ending of a phrase reordering. Moti-
vated by Ng and Low (2004) that built a classifier
to predict word boundaries for word segmenta-
tion, we build a classifier to predict phrase bound-
aries. We classify each source word into one of the
4 boundary tags: ?b? indicates the beginning of a
phrase, ?m? indicates a word appears in the mid-
dle of a phrase, ?e? indicates the end of a phrase,
?s? indicates a single-word phrase.
We use phrase boundaries as soft constraints for
decoding. To do this, we incorporate our classifier
as a feature into the HPB model and propose an
efficient decoding algorithm.
Compared to the previous work, out approach
has the following advantages:
? Our approach maintains the strength of the
phrase-based models since it does not re-
quire any syntactical information. There-
fore, phrases do not need to respect syntactic
boundaries.
? The training instances are directly learned
from a word-aligned bilingual corpus, rather
than from manually annotated corpus.
384
? The decoder outputs phrase segmentation in-
formation as a byproduct, in addition to
translation result.
We evaluate our approach on large-scale
Chinese-to-English translation. Experimental re-
sults and analysis show that using phrase bound-
aries as soft constraints achieves significant im-
provements over the baseline system.
2 Previous Work
2.1 Learning Word Boundaries
In some languages, such as Chinese, words are not
demarcated. Therefore, it is a preliminary task to
determine word boundaries for a sentence, which
is the so-called word segmentation.
Ng and Low (2004) regarded word segmen-
tation as a classification problem. They labelled
each Chinese character with one of 4 possible
boundary tags: ?b?, ?m?, ?e? respectively indi-
cates the begin, the middle and the end of a word,
and ?s? indicates a single-character word. Their
segmenter was built within a maximum entropy
framework and trained on manually segmented
sentences.
Learning phrase boundaries is analogous to
word boundaries. The basic difference is that
the unit for learning word boundaries is charac-
ter while the unit for learning phrase boundaries
is word. In this paper, we adopt the boundary
tags presented by Ng and Low (2004) and build a
classifier to predict phrase boundaries within max-
imum entropy framework. We train it directly on a
word-aligned bilingual corpus, without any man-
ually annotation and syntactical information.
2.2 The Hierarchical Phrase-based Model
We built a hierarchical phrase-based MT system
(Chiang, 2007) based on weighted SCFG. The
translation knowledge is represented by rewriting
rules:
X ? ??, ?,?? (2)
where X is a non-terminal, ? and ? are source and
target strings, respectively. Both of them contain
words and possibly co-indexed non-terminals. ?
describes a one-to-one correspondence between
non-terminals in ? and ?.
Chiang (2007) used the standard log-linear
framework (Och and Ney, 2002) to combine var-
ious features:
Pr(e|f) ?
?
i
?ihi(?, ?) (3)
where hi(?, ?) is a feature function and ?i is
the weight of hi. Analogous to the previous
phrase-based model, Chiang defined the follow-
ing features: translation probabilities p(?|?) and
p(?|?), lexical weights pw(?|?) and pw(?|?),
word penalty, rule penalty, and a target n-gram
language model.
In this paper, we integrate a phrase boundary
classifier as an additional feature into the log-
linear model to provide soft constraint for pattern-
matching during decoding. The feature weights
are optimized by MERT algorithm (Och, 2003).
3 Learning Phrase Boundaries
We build a phrase boundary classifier (PBC)
within a maximum entropy framework. The PBC
predicts a boundary tag for each source word, con-
sidering contextual features:
Ptag(t|fj , F J1 ) =
exp(
?
i ?ihi(t, fj , F J1 ))?
t exp(
?
i ?ihi(t, fj , F J1 )
(4)
where, t ? {b, m, e, s}, fj is the jth word in
source sentence F J1 , hi is a feature function and
?i is the weight of hi.
To build PBC, we first present a method to rec-
ognize phrase boundaries and extract training ex-
amples from word-aligned bilingual corpus, then
we define contextual feature functions.
3.1 Phrase Boundary
During decoding, intuitively, words within a
phrase should be translated or moved together.
Therefore, a phrase boundary should indicate re-
ordering information. We assign one of the
boundary tags (b,m, e, s) to each word in source
sentences. Thus the word with tag b, e or s is a
phrase boundary. One question is that how to as-
sign boundary tag to a word? In this paper, we
recognize the largest source span which has the
monotone translation. Then we assign boundary
385
?? ?? ?
jointly held by
(a)
?? ??
a short visit
(b)
Figure 4: Illustration for monotone span (a) and
PM span (b).
tags to each word in the source span, according to
their position.
To do this, we first introduce some notations.
Given a bilingual sentence (F J1 , EI1) together with
word alignment matrix A, we use L(Aj) and
H(Aj) to represent the lowest and highest tar-
get word position which links to the source word
fj , respectively. Since the word alignment for fj
maybe ?one-to-many?, all the corresponding tar-
get words will appear in the span [L(Aj),H(Aj)].
we define a source span [j1, j2] (1 ? j1 ? j2 ?
J) a monotone span, iff:
1. ?(j, i) ? A, j1 ? j ? j2 ? L(Aj1) ? i ?
H(Aj2)
2. ?k1, k2 ? [j1, j2], k1 ? k2 ? H(Ak1) ?
L(Ak2)
The first condition indicates that
(F j2j1 , E
H(Aj2 )
L(Aj1 )
) is a phrase pair as described
previously in phrase-based SMT models. While
the second condition indicates that the lower
target bound linked to a source word cannot be
lower than any target word position linked to the
previous source word. Therefore, a monotone
span does not contain crossed links or internal
reorderings.
Considering that word alignments could be
very noisy and complex in real-world data, we de-
fine pseudo-monotone (PM) span by loosening the
second condition:
?k1, k2 ? [j1, j2], k1 ? k2 ? L(Ak1) ? L(Ak2)
(5)
This condition allows crossed links to some ex-
tent by loosening the bound of Ak1 from upper
to lower. Figure 4 (a) shows an example of
monotone span, in which the translation is mono-
tone. While Figure 4 (b) is not a monotone span
because there is a cross link between the upper
bound of ???? and the lower bound of ????
on the target side. However, it is a PM span ac-
cording to the definition. Note that in some cases,
a source word may not be contained in any phrase
pair, therefore we consider a single word span as
a PM span, specificly.
An interesting feature of PM span is that if two
PM spans are consecutive on both source side and
their corresponding target side, the two PM spans
can be combined as a larger PM span. Formally,
(F jj1 , E
i
i1)
?
(F j2j+1, Ei2i+1) = (F
j2
j1 , E
i2
i1 ) (6)
where [j1, j] and [j+1, j2] are PM spans, [i1, i]
and [i + 1, i2] are the target spans corresponding
to [j1, j] and [j+1, j2], respectively. For example,
Figure 4 (a) shows a PM phrase pair that consists
of two small PM pairs ???, jointly? and ???
?, held by?.
In this paper, we are interested in phrase re-
ordering boundaries for a source sentence. We de-
fine translation span (TS) the largest possible PM
span. A TS may consist of one or more PM spans.
According to our definition, cross links may ap-
pear within PM spans but do not appear between
PM spans within a TS. Therefore, TS is the largest
possible span that will be translated as a unit and
phrase reorderings may occur between TSs during
decoding.
To obtain phrase boundary examples from
word-aligned bilingual sentences, we first find all
possible TSs and then assign boundary tags to
each word. For a TS [j1, j2] (j1 < j2) that contain
more than two words, we assign ?b? to the first
word fj1 and ?e? to the last word fj2 , and ?m? to
the middle words fj (j1 < j < j2). For a single
word span TS [j, j], we assign ?s? to the word fj .
Figure 5 shows an example of labelling source
words with boundary tags. The source sentence is
segmented into 4 TSs. Using the phrase boundary
information to guide decoding, the decoder will
produce the correct derivation and translation as
shown in Figure 2.
386
??
?
?
?
?
?
?
?
??
?
??
?
?
TAG b m e b e s b m e
She
will
become
the first
female
president
in
India?s
history
Figure 5: Illustration for labelling the source
words with boundary tags. The solid boxes
present word alignments. The bordered boxes are
TSs.
3.2 Feature Definition
The features we used for the PS model are anal-
ogous to (Ng and Low, 2004). For a word W0,
we define the following contextual features with a
window of ?n?:
? The word feature Wn, which denotes the left
(right) n words of the current word W0;
? The part-of-speech (POS) feature Pn, which
denotes the POS tag of the word Wn.
For example, the tag of the word ??? (be-
come)? in Figure 5 is ?e?, indicating that it is
the end of a phrase. If we set the context window
n = 2, the features of the word ??? (become)?
are:
? W?2=? W?1=? W0=? ? W1=? ?
W2=????
? P?2=r P?1=d P0=v P1=ns P2=l
We collect TSs from bilingual sentences to-
gether with the contextual features and used a
MaxEnt toolkit (Zhang, 2004) to train a PBC.
? ? ??
b 0.78 0.10 1.2e-5
m 6.4e-8 0.75 5.4e-5
e 2.1e-8 0.11 0.87
s 0.22 0.04 0.13
Table 1: The TPM for a source sentence. The
highest probability of each word is in bold.
4 Phrase Boundary Constrained
Decoding
Give a source sentence, we can assign boundary
tags to each word by running the PBC. During
decoding, a rule is prohibited to pattern-match
across phrase boundaries. By doing this, the PBC
is integrated as a hard constraint. However, this
method will invalidate a large number of rules and
the decoder suffers from a risk that there are not
enough rules to cover the source sentence.
Alternatively, inspired by previous approaches,
we integrate the phrase boundary classifier as a
soft constraint by incorporating it as a feature into
the HPB model:
hpbc(F J1 ) = log(
J?
j=1
Ptag(t|fj , F J1 )) (7)
To perform translation, for each word fj in
a source sentence F J1 , we first compute all tag
probabilities Ptag(t|fj), where t ? (b,m, e, s),
j ? [1, J ], according to Equation 4. Therefore, we
build a 4? J tag-word probability matrix (TPM).
TPM [i, j] indicates the probability of the word
fj labelled with the tag ti. Table 1 shows the
TPM for a source text ??????.
Then we select rule options from the rule ta-
ble that can be used for translating the source text.
Since each rule option (f? , e?, a) 1 can be regarded
as a bilingual sentence with word alignments, thus
we find all TS in f? and assign an initial tag (IT)
for each source word. This procedure is analogous
to label phrase boundary tags for a word-aligned
bilingual sentence. For example, the following
rules are used for translating the Chinese sentence
in Table 1:
1We keep word alignments of a rule when it is extracted
from bilingual sentence.
387
X ? ??bX?1 , She X1? (8)
X1 ? ??b??e, will become? (9)
Since both the source sides of these two rules
are PM spans according to the word alignments,
the IT sequences for rule (8) and (9) are ?b *?2
and ?b e?, respectively. According to Table 1,
the initial hpbc score for these two rules can be
computed as follows:
h(7)pbc = log(Ptag(b|?)) = log(TPM [1, 1]) (10)
h(8)pbc = log(Ptag(b|?)) + log(Ptag(e|??))
= log(TPM [1, 2]) + log(TPM [3, 3]) (11)
Note that to keep the tag sequence valid, e.g.
?m? follows ?b? rather than ?s?, the ITs maybe
updated during decoding. The tag-updating
should be consistent with the definition of TS as
described in Section 3.1. Specifically, when the
non-terminal symbol X is derived from its cov-
ered span f(X), the boundary tags should be up-
dated.
When a tag of word fj is updated from tk1 to
tk2 , the PBC score should also be updated accord-
ing to TPM:
?PBC = log(TPM [k2, j])? log(TPM [k1, j])
(12)
The following is a derivation of the source sen-
tence in Table 1:
S ? ??bX?1 , She X1?
? ??b?b?m??e, She will become?
When X1 is derived, the tag of its left boundary
word ??? is updated from ?b? to ?m?. The reason
is that after derivation, the combined span forms
a larger PM span and the left boundary of f(X1)
should be updated.
As a result, the hpbc score is recomputed:
hpbc(F 31 ) = h
(7)
pbc + h
(8)
pbc +?PBC (13)
where,
?PBC = log(TPM [2, 2])? log(TPM [1, 2])
(14)
2We use ?*? as a tag of the non-terminal symbol ?X1?
since it has not been derived.
The decoding algorithm is efficient since the
computing of the PBC score is a procedure of
table-lookup.
5 Experiments
5.1 Experimental Setup
Our experiments were on Chinese-to-English
translation. The training corpus (77M+81M) we
used are from LDC 3. The evaluation metric is
BLEU (Papineni et al, 2002), as calculated by
mteval-v11b.pl with case-insensitive matching of
n-grams, where n = 4.
To obtain word alignments, we first ran
GIZA++ (Och and Ney, 2002) in both translation
directions and then refined it by ?grow-diag-final?
method (Koehn et al, 2003).
For the language model, we used the SRI Lan-
guage Modeling Toolkit (Stolcke, 2002) to train
two 4-gram models on xinhua portion of Giga-
Word corpus and the English side of the training
corpus.
The NIST MT03 test set is used to tune the fea-
ture weights of the log-linear model by MERT
(Och, 2003). We tested our system on the NIST
MT06 and MT08 test sets.
5.2 Results
The results are shown in Table 2. We tested vari-
ous settings of the context window. It is observed
that the small values of n (n = 1, 2) drop the
BLEU score, suggesting that perhaps there are not
enough contextual information. With more con-
textual information is used, the BLEU scores are
improved over all test sets. When n = 3, the most
significant improvements are obtained on MT06G
and MT08. The improvements over the baseline
are statistically significant at p < 0.01 by using
the significant test method described in (Koehn,
2004). While for MT06N, the optimized context
window size is n = 4 but the improvement is
not statistically significant. In most cases, with
n larger than 3, we do not obtain further improve-
ments because of the data sparseness for training
3LDC2002E18, LDC2002L27, LDC2002T01,
LDC2003E07, LDC2003E14, LDC2004T07, LDC2005E83,
LDC2005T06, LDC2005T10, LDC2005T34, LDC2006E24,
LDC2006E26, LDC2006E34, LDC2006E86, LDC2006E92,
LDC2006E93, LDC2004T08(HK News, HK Hansards).
388
System MT06G MT06N MT08
baseline 14.66 34.42 26.29
+PBC (n=1) 13.78 33.20 24.58
+PBC (n=2) 14.34 34.21 25.87
+PBC (n=3) 15.19* 34.63 27.25*
+PBC (n=4) 14.76 34.73 26.70
Table 2: Results on the test sets with different con-
text window (n) of the phrase boundary classifier.
The largest BLEU score on each test set is in bold.
MT06G: MT06 GALE set. MT06N: MT06 NIST
set. *: significantly better than the baseline at
p < 0.01.
the classifier.
6 Discussion
The experimental results show that the phrase
boundary constrained method improves the BLEU
score over the baseline system. Furthermore, we
are interested in how the PBC affects the transla-
tion results? We compared the outputs generated
by the baseline and ?+PBC (n = 3)? system and
found some interesting translations. For example,
the translations of a source sentence of NIST08
are as follows 4:
? Src: ?b1 ??m2 ?m3 ??m4 ??e5 ???b6
?m7 ??e8 ??b9??m10??e11
? Ref: US1 Treasury-Secretary2 Arrives-in3
China4 for-a-Visit-with5 Environment6 and7
Exchange-Rate8 as9 Focus10,11
? HPB: US1 Treasury2 in-environmental-
protection6 and7 visit5 China4 is9 key11
to-the-concern-of10 the-exchange-rate8
? +PBC: US1 Treasury2 arrived-in3 China4
for-a-visit5 environmental-protection6 and7
exchange-rate8 is9 concerned-about10 the-
key11
In the example, both ???? and ???? in the
source sentence are the concern of the ?visit?.
Therefore, the source span [6, 8] indicates a co-
hesive phrase, which should be translated as a
4The co-indexes of the words on the source and target
sentence indicate word alignments.
whole unit. However, the baseline translates the
spans [6, 7] and [8, 8] separately. It moves [6, 7]
before ?visit China? and [8, 8] after ?concern?.
This makes an mistake on phrase reordering. We
observe that the ?+PBC? system produces a bet-
ter translation. After incorporating the PBC as
a soft constraint, the system assigns a boundary
tag to each source word and segments the source
sentence into three TSs. According to our defi-
nition, TSs are encouraged as pseudo-monotone
translation unit during decoding. As a result, the
?+PBC? system discourages some arbitrary re-
ordering rules and produces more fluent transla-
tion.
7 Conclusion and Future Work
This paper presented a phrase boundary con-
strained method for hierarchical phrase-based
translation. A phrase boundary indicates begin
or end of a phrase reordering. We built a phrase
boundary classifier within a maximum entropy
framework and learned phrase boundary exam-
ples directly from word-aligned bilingual corpus.
We proposed an efficient decoding method to in-
tegrate the PBC into the decoder as a soft con-
straint. Experiments and analysis show that the
phrase boundary constrained method achieves sig-
nificant improvements over the baseline system.
The most advantage of the PBC is that it han-
dles both syntactic and non-syntactic phrases. In
the future, We would like to try different meth-
ods to determine more informative phrase bound-
aries, e.g. Xiong et al (2010) proposed a method
to learn translation boundaries from a hierarchical
tree that decomposed from word alignments using
a shift-reduce algorithm. In addition, we will try
more features as described in (Chiang et al, 2008;
Chiang et al, 2009), e.g. the length of the phrases
that covered by non-terminals.
References
Cherry, Colin. 2008. Cohesive phrase-based decoding
for statistical machine translation. In Proceedings
of the 46rd Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, page 72?80.
Chiang, David, Yuval Marton, and Philip Resnik.
389
2008. Online large-margin training of syntactic and
structural translation features. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, page 224?233.
Chiang, David, Wei Wang, and Kevin Knight. 2009.
11,001 new features for statistical machine trans-
lation. In Proceedings of Human Language Tech-
nologies: the 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, page 218?226.
Chiang, David. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 263?270.
Chiang, David. 2007. Hierarchical phrase-based
translation. Computational Linguistics, pages
33(2):201?228.
Gimpel, Kevin and Noah A. Smith. 2008. Rich
source-side context for statistical machine transla-
tion. In In Proceedings of the ACL-2008 Workshop
on Statistical Machine Translation (WMT-2008),
pages 9?17.
Koehn, Philipp, Franz J. Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL 2003, pages 127?133.
Koehn, Philipp. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
the 2004 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 388?
395.
Marton, Yuval and Philip Resnik. 2008. Soft syntac-
tic constraints for hierarchical phrased-based trans-
lation. In Proceedings of the 46rd Annual Meeting
of the Association for Computational Linguistics:
Human Language Technologies, pages 1003?1011.
Ng, Hweetou and Jinkiat Low. 2004. Chinese part-
of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based? In Proceedings of
the 2004 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP 2004), pages
277?284.
Och, Franz Josef and Hermann Ney. 2002. Dis-
criminative training and maximum entropy models
for statistical machine translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics, pages 295?302.
Och, Franz Josef and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. 30:417?449.
Och, Franz Josef. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics, pages 160?167.
Papineni, K., S. Roukos, T. Ward, and W.-J. Zhu.
2002. Bleu: a method for automatic evaluation of
machine translation. In Proceedings of the 40th An-
nual Meeting of the Association for Computational
Linguistics, pages 311?318.
Stolcke, Andreas. 2002. SRILM ? An extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken language Process-
ing, volume 2, pages 901?904.
Xiong, Deyi, Min Zhang, Aiti Aw, and Haizhou Li.
2009. A syntax-driven bracketing model for phrase-
based translation. In ACL-IJCNLP 2009, page
315?323.
Xiong, Deyi, Min Zhang, and Haizhou Li. 2010.
Learning translation boundaries for phrase-based
decoding. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the ACL, page 136?144.
Zhang, Le. 2004. Maximum entropy model-
ing toolkit for python and c++. available at
http://homepages.inf.ed.ac.uk/s0450736/maxent too-
lkit.html.
390
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 555?563,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Maximum Entropy Based Phrase Reordering
for Hierarchical Phrase-based Translation
Zhongjun He Yao Meng Hao Yu
Fujitsu R&D Center CO., LTD.
15/F, Tower A, Ocean International Center, 56 Dongsihuan Zhong Rd.
Chaoyang District, Beijing, 100025, China
{hezhongjun, mengyao, yu}@cn.fujitsu.com
Abstract
Hierarchical phrase-based (HPB) translation
provides a powerful mechanism to capture
both short and long distance phrase reorder-
ings. However, the phrase reorderings lack of
contextual information in conventional HPB
systems. This paper proposes a context-
dependent phrase reordering approach that
uses the maximum entropy (MaxEnt) model
to help the HPB decoder select appropriate re-
ordering patterns. We classify translation rules
into several reordering patterns, and build a
MaxEnt model for each pattern based on var-
ious contextual features. We integrate the
MaxEnt models into the HPB model. Ex-
perimental results show that our approach
achieves significant improvements over a stan-
dard HPB system on large-scale translation
tasks. On Chinese-to-English translation,
the absolute improvements in BLEU (case-
insensitive) range from 1.2 to 2.1.
1 Introduction
The hierarchical phrase-based (HPB) model (Chi-
ang, 2005; Chiang, 2007) has been widely adopted
in statistical machine translation (SMT). It utilizes
synchronous context free grammar (SCFG) rules
to perform translation. Typically, there are three
types of rules (see Table 1): phrasal rule, a phrase
pair consisting of consecutive words; hierarchical
rule, a hierarchical phrase pair consisting of both
words and variables; and glue rule, which is used to
merge phrases serially. Phrasal rule captures short
distance reorderings within phrases, while hierar-
chical rule captures long distance reorderings be-
Type Constituent ExamplesWord Variable
PR
?
- X ? ???, one of?
HR
? ?
X ? ?X, ofX?
GR -
?
S ? ?SX, SX?
Table 1: A classification of grammar rules for the HPB
model. PR = phrasal rule, HR = hierarchical rule, GR =
glue rule.
tween phrases. Therefore, the HPB model outper-
forms conventional phrase-based models on phrase
reorderings.
However, HPB translation suffers from a limita-
tion, in that the phrase reorderings lack of contex-
tual information, such as the surrounding words of
a phrase and the content of sub-phrases that rep-
resented by variables. Consider the following two
hierarchical rules in translating a Chinese sentence
into English:
X ? ?X1  X2, X1 ?s X2? (1)
X ? ?X1  X2, X2X1? (2)
? ?d  !
with Russia ?s talks
talks with Russia
Both pattern-match the source sentence, but pro-
duce quite different phrase reorderings. The first
rule generates a monotone translation, while the sec-
ond rule swaps the source phrases covered by X1
and X2 on the target side. During decoding, the first
555
rule is more likely to be used, as it occurs more fre-
quently in a training corpus. However, the exam-
ple is not a noun possessive case because the sub-
phrase covered by X1 is not a noun but a preposi-
tional phrase. Thus, without considering informa-
tion of sub-phrases, the decoder may make errors on
phrase reordering.
Contextual information has been widely used to
improve translation performance. It is helpful to re-
duce ambiguity, thus guide the decoder to choose
correct translation for a source text. Several re-
searchers observed that word sense disambiguation
improves translation quality on lexical translation
(Carpuat and Wu, 2007; Chan et al, 2007). These
methods utilized contextual features to determine
the correct meaning of a source word, thus help an
SMT system choose an appropriate target transla-
tion.
Zens and Ney (2006) and Xiong et al (2006)
utilized contextual information to improve phrase
reordering. They addressed phrase reordering as
a two-class classification problem that translating
neighboring phrases serially or inversely. They built
a maximum entropy (MaxEnt) classifier based on
boundary words to predict the order of neighboring
phrases.
He et al (2008) presented a lexicalized rule selec-
tion model to improve both lexical translation and
phrase reordering for HPB translation. They built
a MaxEnt model for each ambiguous source side
based on contextual features. The method was also
successfully applied to improve syntax-based SMT
translation (Liu et al, 2008), using more sophisti-
cated syntactical features. Shen et al (2008) inte-
grated various contextual and linguistic features into
an HPB system, using surrounding words and de-
pendency information for building context and de-
pendency language models, respectively.
In this paper, we focus on improving phrase re-
ordering for HPB translation. We classify SCFG
rules into several reordering patterns consisting of
two variables X and F (or E) 1, such as X1FX2
and X2EX1. We treat phrase reordering as a classi-
fication problem and build a MaxEnt model for each
source reordering pattern based on various contex-
1We use F and E to represent source and target words, re-
spectively.
tual features. We propose a method to integrate the
MaxEnt models into an HPB system. Specifically:
? For hierarchical rules, we classify the source-
side and the target-side into 7 and 17 reordering
patterns, respectively. Target reordering pat-
terns are treated as possible labels. We then
build a classifier for each source pattern to pre-
dict phrase reorderings. This is different from
He et al (2008), in which they built a clas-
sifier for each ambiguous hierarchical source-
side. Therefore, the training examples for each
MaxEnt model is small and the model maybe
unstable. Here, we classify source hierarchical
phrases into 7 reordering patterns according to
the arrangement of words and variables. We
can obtain sufficient samples for each MaxEnt
model from large-scale bilingual corpus.
? For glue rules, we extend the HPB model by
using bracketing transduction grammar (BTG)
(Wu, 1996) instead of the monotone glue rule.
By doing this, there are two options for the de-
coder to merge phrases: serial or inverse. We
then build a classifier for glue rules to predict
reorderings of neighboring phrases, analogous
to Xiong et al (2006).
? We integrate the MaxEnt based phrase reorder-
ing models as features into the HPB model
(Chiang, 2005). The feature weights can be
tuned together with other feature functions by
MERT algorithm (Och, 2003).
Experimental results show that the presented method
achieves significant improvement over the baseline.
On Chinese-to-English translation tasks of NIST
evluation, improvements in BLEU (case-insensitive)
are 1.2 on MT06 GALE set, 1.8 on MT06 NIST set,
and 2.1 on MT08.
The rest of the paper is structured as follows: Sec-
tion 2 describes the MaxEnt based phrase reorder-
ing method. Section 3 integrates the MaxEnt mod-
els into the translation model. In Section 4, we re-
port experimental results. We analyze the presented
method and experimental results in Section 5 and
conclude in Section 6.
556
Source phrase Target phrase
X and
X ? with X
between X and
Figure 1: A source hierarchical phrase and its corre-
sponding target translation.
2 MaxEnt based Phrase Reordering
We regard phrase reordering as a pattern classifica-
tion problem. A reordering pattern indicates an ar-
rangement of words and variables. Although there
are a large amount of hierarchical rules may be ex-
tracted from bilingual corpus, these rules can be
classified into several reordering patterns (Section
2.1). In addition, we extend the HPB model with
BTG, that adding an inverted glue rule to merge
phrases inversely (Section 2.2). Therefore, the glue
rules are classified into two patterns: serial or in-
verse. We then build a MaxEnt phrase reordering
(MEPR) classifier for each source reordering pattern
(Section 2.3). In Section 2.4, we describe contextual
features.
2.1 Reordering Pattern Classification for
Hierarchical Rule
Hierarchical rule, consisting of both words and vari-
ables, is of great importance for the HPB model.
During decoding, words are used for lexical trans-
lation, and variables capture phrase reordering. We
may learn millions of hierarchical rules from a bilin-
gual corpus. Although these rules are different from
each other, they can be classified into several re-
ordering patterns according to the arrangement of
variables and words.
In this paper, we follow the constraint as de-
scribed in (Chiang, 2005) that a hierarchical rule
can have at most two variables and they cannot be
adjacent on the source side. We use ?X? to rep-
resent the variable, and ?F ? and ?E? to represent
word strings in source and target language, respec-
tively. Therefore, in a hierarchical rule, E is the lex-
ical translation of F , while the order of X and E
contains phrase reordering information.
For the hierarchical rule that contains one vari-
able (see Figure 1 for example), both the source and
the target phrases can be classified into three pat-
Source pattern Target pattern
XF XE
FX EX
FXF EXE
Table 2: A classification of the source side and the target
side for the hierarchical rule that contains one variable.
Source pattern Target pattern
X1EX2
X2EX1
X1X2E
X2X1E
EX1X2
X1FX2 EX2X1
X1FX2F X1EX2E
FX1FX2 X2EX1E
FX1FX2F EX1X2E
EX2X1E
EX1EX2
EX2EX1
EX1EX2E
EX2EX1E
Table 3: A classification of the source side and the target
side for the hierarchical rule that contains two variables.
terns (Table 2). To reduce the complexity of clas-
sification, we do not distinguish the order of word
strings. For example, we consider ?e1Xe2? and
?e2Xe1? as the same pattern ?EXE?, because the
target words are determined by lexical translation of
source words. Our focus is the order between X and
E. During decoding the phrases covered by X are
dynamically changed and the contextual information
of these phrases is ignored for pattern-matching of
hierarchical rules.
Analogously, for the hierarchical rule that con-
tains two variables, the source phrases are classified
into 4 patterns, while the target phrases are classified
into 14 patterns, as shown in Table 3. The pattern
number on the source side is less than that on the
target side, because on the source side, ?X1? always
appears before ?X2?, and they cannot be adjacent.
557
2.2 Reordering Pattern Classification for Glue
Rule
The HPB model used glue rule to combine phrases
serially. The reason is that in some cases, there are
no valid translation rules that cover a source span.
Therefore, the glue rule provides a default monotone
combination of phrases in order to complete a trans-
lation. This is not sufficient because in certain cases,
the order of phrases may be inverted on the target-
side.
In this paper, we extend the glue rule with BTG
(Wu, 1996), which consists of three types of rules:
X ? ?f? , e?? (3)
X ? ?X1X2, X1X2? (4)
X ? ?X1X2, X2X1? (5)
Rule 3 is a phrasal rule that translates a source
phrase f? into a target phrase e?. Rule 4 merges two
consecutive phrases in monotone order, while Rule
5 merges them in inverted order. During decod-
ing, the decoder first uses Rule 3 to produce phrase
translation, and then iteratively uses Rule 4 and 5 to
merge two neighboring phrases into a larger phrase
until the whole sentence is covered.
We replace the original glue rules in the HPB
model with BTG rules (see Table 4). We believe
that the extended HPB model can benefit from BTG
in the following aspects:
? In the HPB model, as we mentioned, hierarchi-
cal rules are constrained in that nonterminals
cannot be adjacent on the source side, i.e., the
source side cannot contain ?X1X2?. One rea-
son is that it will heavily increase the rule table
size. The other reason is that it can cause a spu-
rious ambiguity problem (Chiang, 2005). The
inverted glue rule in BTG, however, can solve
this problem.
? In the HPB model, only a monotone glue rule
is provided to merge phrases serially. In the ex-
tended HPB model, the combination of phrases
is classified into two types: monotone and in-
verse.
Analogous to Xiong et al (2006), to perform
context-dependent phrase reordering, we build a
Glue Rule Extended Glue Rule
S ? ?X,X? S ? ?X,X?
S ? ?SX, SX? X ? ?X1X2, X1X2?
- X ? ?X1X2, X2X1?
Table 4: Extending the glue rules in the HPB model with
BTG.
MaxEnt based classifier for glue rules to predict the
order of two neighboring phrases. In this paper, we
utilize more contextual features.
2.3 The MaxEnt based Phrase Reordering
Classifier
As described above, we classified phrase reorderings
into several patterns. Therefore, phrase reordering
can be regarded as a classification problem: for each
source reordering pattern, we treat the correspond-
ing target reordering patterns as labels.
We build a general classification model within the
MaxEnt framework:
Pme(T? |T?, ?, ?) =
exp(?i ?ihi(?, ?, f(X), e(X))
?
T? exp(
?
i ?ihi(?, ?, f(X), e(X))
(6)
where, ? and ? are the source and target side, re-
spectively. T?/T? is the reordering pattern of ?/?.
f(X) and e(X) are the phrases that covered by X
one the source and target side, respectively. Given
a source phrase, the model predicts a target reorder-
ing pattern, considering various contextual features
(Section 2.4).
According to the classification of reordering pat-
terns, there are 3 kinds of classifiers:
? P hr1me includes 3 classifiers for the hierarchical
rules that contain 1 variable. Each of the clas-
sifier has 3 labels;
? P hr2me includes 4 classifiers for the hierarchical
rules that contain 2 variables. Each of the clas-
sifier has 14 labels;
? P grme includes 1 classifier for the glue rules. The
classifier has 2 labels that predict a monotone
or inverse order for two neighboring phrases.
This classifier is analogous to (Xiong et al,
2006).
558
There are 8 classifiers in total. This is much fewer
than the classifiers in He et al (2008), in which a
classifier was built for each ambiguous hierarchical
source side. In this way, a classifier may face the
risk that there are not enough samples for training a
stable MaxEnt model. While our approach is more
generic, rather than training a MaxEnt model for a
specific hierarchical source side, we train a model
for a source reordering pattern. Thus, we reduce the
number of classifiers and can extract large training
examples for each classifier.
2.4 Feature definition
For a reordering pattern pair ?T?, T??, we design
three feature functions for phrase reordering classi-
fiers:
? Source lexical feature, including boundary
words and neighboring words. Boundary
words are the left and right word of the source
phrases covered by f(X), while neighboring
words are the words that immediately to the left
and right of a source phrase f(?);
? Part-of-Speech (POS) feature, POS tags of the
boundary and neighboring words on the source
side.
? Target lexical feature, the boundary words of
the target phrases covered by e(X).
These features can be extracted together with
translation rules from bilingual corpus. However,
since the hierarchical rule does not allow for adja-
cent variables on the source side, we extract features
for P grme by using the method described in Xiong et
al. (2006). We train the classifiers with a MaxEnt
trainer (Zhang, 2004).
3 Integrating the MEPR Classifier into the
HPB Model
The HPB model is built within the standard log-
linear framework (Och and Ney, 2002):
Pr(e|f) ?
?
i
?ihi(?, ?) (7)
where hi(?, ?) is a feature function and ?i is the
weight of hi. The HPB model has the following fea-
tures: translation probabilities p(?|?) and p(?|?),
lexical weights pw(?|?) and pw(?|?), word penalty,
phrase penalty, glue rule penalty, and a target n-
gram language model.
To integrate the MEPR classifiers into the transla-
tion model, the features of the log-linear model are
changed as follows:
? We add the MEPR classifier as a feature func-
tion to predict reordering pattern:
hme(T? |T?) =
?
Pme(T? |T?, ?, ?) (8)
During decoding, we first classify each source
phrase into one of the 8 source reordering pat-
terns and then use the corresponding MEPR
classifier to predict the possible target reorder-
ing pattern. Therefore, the contextual informa-
tion guides the decoder to perform phrase re-
ordering.
? We split the ?glue rule penalty? into two fea-
tures: monotone glue rule number and inverted
glue rule number. These features reflect pref-
erence of the decoder for using monotone or
inverted glue rules.
The advantage of our extension method is that the
weights of the new features can be tuned together
with the other features by MERT algorithm (Och,
2003).
We utilize a standard CKY algorithm for decod-
ing. Given a source sentence, the decoder searches
the best derivation from the bottom to top. For a
source span [j1, j2], the decoder uses three kinds of
rules: translation rules produce lexical translation
and phrase reordering (for hierarchical rules), mono-
tone rule merges any neighboring sub-spans [j1, k]
and [k + 1, j2] serially, and inverted rule swap them.
Note that when the decoder uses the monotone and
inverted glue rule to combine sub-spans, it merges
phrases that do not contain variables. Because the
CKY algorithm guarantees that the sub spans [j1, k]
and [k + 1, j2] have been translated before [j1, j2].
559
4 Experiments
We carried out experiments on four systems:
? HPB: replication of the Hiero system (Chiang,
2005);
? HPB+MEHR: HPB with MaxEnt based classi-
fier for hierarchical rules, as described in Sec-
tion 2.1;
? HPB+MEGR: HPB with MaxEnt based classi-
fier for glue rules, as described in Section 2.2;
? HPB+MER: HPB with MaxEnt based classifier
for both hierarchical and glue rules.
All systems were tuned on NIST MT03 and tested
on MT06 and MT08. The evaluation metric was
BLEU (Papineni et al, 2002) with case-insensitive
matching of n-grams, where n = 4.
We evaluated our approach on Chinese-to-
English translation. The training data contained
77M Chinese words and 81M English words.
These data come from 17 corpora: LDC2002E18,
LDC2002L27, LDC2002T01, LDC2003E07,
LDC2003E14, LDC2004T07, LDC2005E83,
LDC2005T06, LDC2005T10, LDC2005T34,
LDC2006E24, LDC2006E26, LDC2006E34,
LDC2006E86, LDC2006E92, LDC2006E93,
LDC2004T08 (HK News, HK Hansards).
To obtain word alignments, we first ran GIZA++
(Och and Ney, 2000) in both translation directions
and then refined the results using the ?grow-diag-
final? method (Koehn et al, 2003). For the lan-
guage model, we used the SRI Language Modeling
Toolkit (Stolcke, 2002) to train two 4-gram models
on the Xinhua portion of the GigaWord corpus and
the English side of the training corpus.
4.1 Statistical Information of Rules
Hierarchical Rules
We extracted 162M translation rules from the train-
ing corpus. Among them, there were 127M hi-
erarchical rules, which contained 85M hierarchical
source phrases. We classified these source phrases
into 7 patterns as described in Section 2.1. Table
5 shows the statistical information. We observed
that the most frequent source pattern is ?FXF ?,
Source Pattern Percentage (%)
XF 9.7
FX 9.7
FXF 46.1
X1FX2 3.7
X1FX2F 11.9
FX1FX2 11.8
FX1FX2F 7.1
Table 5: Statistical information of reordering pattern clas-
sification for hierarchical source phrases.
# Source
Target (%) FX XF FXF
EX 82.8 7 4.6
XE 6.4 82.4 2.9
EXE 10.8 10.6 92.5
Table 6: Percentage of target reordering pattern for each
source pattern containing one variable.
which accounted for 46.1% of the total. Interest-
ingly, ?X1FX2?, accounting for 3.7%, was the least
frequent pattern. Table 6 and Table 7 show the
distributions of reordering patterns for hierarchical
source phrases that contain one and two variables,
respectively. From both the tables, we observed
that for Chinese-to-English translation, the most fre-
quent ?reordering? pattern for a source phrase is
monotone translation (bold font in the tables).
Glue Rules
To train a MaxEnt classifier for glue rules, we ex-
tracted 65.8M reordering (monotone and inverse)
instances from the training data, using the algo-
rithm described in Xiong et al (2006). There were
63M monotone instances, accounting for 95.7%. Al-
though instances of inverse reordering accounted for
4.3%, they are important for phrase reordering.
4.2 Results
Table 8 shows the BLEU scores and decoding speed
of the four systems on MT06 (GALE set and NIST
set) and MT08. From the table, we made the follow-
ing observations:
560
# Source
Target (%) FX1FX2 FX1FX2F X1FX2 X1FX2F
EX1EX2 78.1 3.6 4.6 1.2
EX1EX2E 2.1 75.9 0.1 1.6
EX1X2 6.8 0.1 2.8 0.1
EX1X2E 1.8 11.2 0.1 2
EX2EX1 2.8 1.4 2 1.2
EX2EX1E 1.4 2.3 0.7 1.1
EX2X1 0.9 0.1 2.2 0.2
EX2X1E 1 1.1 0.9 1.0
X1EX2 1.9 0.1 71.2 3.3
X1EX2E 0.7 2.1 6 78.4
X1X2E 0.1 0.1 2.8 5.9
X2EX1 0.9 0.4 1.6 0.7
X2EX1E 1.5 1.5 2.6 2.4
X2X1E 0.1 0.04 2.2 0.8
Table 7: Percentage of target reordering pattern for each source pattern containing two variables.
System Test Data Speed06G 06N 08
HPB 14.19 33.93 25.85 8.7
HPB+MEHR 14.76 34.95 26.56 3.2
HPB+MEGR 15.09 35.72 27.34 2.7
HPB+MER 15.42 35.80 27.94 1.7
Table 8: BLEU percentage scores and translation speed (words/second) on test data. G=GALE set, N=NIST set. All
improvements are statistically significant (p < 0.01). Note that MT06G has one reference for each source sentence,
while the MT06N and MT08 have four references.
? The HPB+MEHR system achieved significant
improvements on all test sets compared to the
HPB system. The absolute increases in BLEU
scores ranging from 0.6 (on 06G) to 1.0 (on
06N) percentage points. This indicates that the
ME based reordering for hierarchical rules im-
proves translation performance.
? The HPB+MEGR system achieved significant
improvements over the HPB system. The ab-
solute increases in BLEU scores ranging from
0.9 (on 06G) to 1.8 (on 06N) percentage points.
The HPB+MEGR system overcomes the short-
coming of the HPB system by using both
monotone glue rule and inverted glue rule,
which merging phrases serially and inversely,
respectively. Furthermore, the HPB+MEGR
system outperformed the HPB+MEHR system.
? The HPB+MER system achieved the best per-
formances on all test sets, with absolute in-
creases of BLEU scores ranging from 1.2 (on
06G) to 2.1 (on 08). The system combin-
ing with ME based reordering for both hier-
archical and glue rules, outperformed both the
HPB+MEHR and HPB+MEGR systems.
? In addition, we found that the decoder takes
more time after adding the MEPR models (the
speed column of Table 8). The average transla-
tion speed of HPB+MER (1.7 words/second) is
about 5 times slower than the HPB system (8.7
words/second). One reason is that the MEPR
models utilized contextual information to com-
pute classification scores. Another reason is
that adding inverted glue rules increases search
space.
561
5 Analysis
Experiments showed that the presented approach
achieved significant gains on BLEU scores. Further-
more, we sought to explore what would happen af-
ter integrating the MEPR classifiers into the transla-
tion model. We compared the outputs of HPB and
HPB+MER and observed that the translation perfor-
mance are improved on phrase reordering. For ex-
ample, the translations of a source sentence in MT08
are as follows 2:
? Src: ?I1 ?2 ??3 .4 m?5 ??6
?7 ?m8 J?9 4010 ?11 ??12 13 
?14 Oy15
? Ref: At the end4 of last3 month3, the
South1 Korean1 government2 began5 a plan15
to provide9 400,00010 tonnes11 of rice12 as
aid14 to North8 Korea8
? HPB: South Korean government late last
month to start with 400,000 tons of rice aid to
the DPRK
? HPB+MER: Start at the end of last month,
South Korean government plans to provide
400,000 tons of rice in aid to the DPRK
The most obvious error that the baseline system
makes is the order of the time expression ???
., the end of last month?, which should be either
at the beginning or the end on target side. However,
the baseline produced a monotone translation by us-
ing the rule ??I ? X1, South Korean govern-
ment X1?. The HPB+MER system, however, moved
the time expression to the beginning of the sentence
by using the rule ??I ? X1, X1 South Ko-
rean government?. The reason is that the MaxEnt
phrase reordering classifier uses the contextual fea-
tures (e.g. the boundary words) of the phrase cov-
ered by X1 to predict the phrase reordering as X1E
for the source phrase FX1.
2The co-indexes of the words in the source and reference
sentence indicate word alignments.
6 Conclusions and Future Work
In this paper, we have proposed a MaxEnt based
phrase reordering approach to help the HPB decoder
select reordering patterns. We classified hierarchical
rules into 7 reordering patterns on the source side
and 17 reordering patterns on the target side. In ad-
dition, we introduced BTG to enhance the reorder-
ing of neighboring phrases and classified the glue
rules into two patterns. We trained a MaxEnt clas-
sifier for each reordering pattern and integrated it
into a standard HPB system. Experimental results
showed that the proposed approach achieved signif-
icant improvements over the baseline. The absolute
improvements in BLEU range from 1.2 to 2.1.
MaxEnt based phrase reordering provides a mech-
anism to incorporate various features into the trans-
lation model. In this paper, we only use a few fea-
ture sets based on standard contextual word and POS
tags. We believe that additional features will fur-
ther improve translation performance. Such features
could include syntactical features (Chiang et al,
2009). In the future, we will carry out experiments
on deeper features and evaluate the effects of differ-
ent feature sets.
References
Marine Carpuat and Dekai Wu. 2007. Improving statisti-
cal machine translation using word sense disambigua-
tion. In Proceedings of EMNLP-CoNLL 2007, pages
61?72.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Proceedings of the 45th Annual
Meeting of the Association for Computational Linguis-
tics, pages 33?40.
David Chiang, Wei Wang, and Kevin Knight. 2009.
11,001 new features for statistical machine transla-
tion. In Proceedings of the North American Chapter
of the Association for Computational Linguistics - Hu-
man Language Technologies 2009 Conference, page
218?226.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics, pages 263?270.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, pages 33(2):201?
228.
Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Im-
proving statistical machine translation using lexical-
562
ized rule selection. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics,
pages 321?328.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
the 2003 Conference of the North American Chapter of
the Association for Computational Linguistics on Hu-
man Language Technology, pages 48?54.
Qun Liu, Zhongjun He, Yang Liu, and Shouxun Lin.
2008. Maximum entropy based rule selection model
for syntax-based statistical machine translation. In
Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, page
89?97.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics, pages 440?447.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics, pages 295?302.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics, pages 160?167.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics,
pages 311?318.
Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2008. Effective use of linguis-
tic and contextual information for statistical machine
translation. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 72?80.
Andreas Stolcke. 2002. SRILM ? An extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken language Processing,
volume 2, pages 901?904.
Dekai Wu. 1996. A polynomial-time algorithm for sta-
tistical machine translation. In Proceedings of the
Thirty-Fourth Annual Meeting of the Association for
Computational Linguistics.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Max-
imum entropy based phrase reordering model for sta-
tistical machine translation. In Proceedings of the 44th
Annual Meeting of the Association for Computational
Linguistics, pages 521?528.
Richard Zens and Hermann Ney. 2006. Discriminative
reordering models for statistical machine translation.
In Proceedings of the Workshop on Statistical Machine
Translation, pages 55?63.
Le Zhang. 2004. Maximum entropy model-
ing toolkit for python and c++. available at
http://homepages.inf.ed.ac.uk/s0450736/maxent too-
lkit.html.
563
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1119?1128,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Function-based question classification for general QA
Fan Bu, Xingwei Zhu, Yu Hao and Xiaoyan Zhu
State Key Laboratory of Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Sci. and Tech., Tsinghua University
buf08@mails.tsinghua.edu.cn
etzhu192@hotmail.com
haoyu@mail.tsinghua.edu.cn
zxy-dcs@tsinghua.edu.cn
Abstract
In contrast with the booming increase of inter-
net data, state-of-art QA (question answering)
systems, otherwise, concerned data from spe-
cific domains or resources such as search en-
gine snippets, online forums and Wikipedia in
a somewhat isolated way. Users may welcome
a more general QA system for its capability
to answer questions of various sources, inte-
grated from existed specialized sub-QA en-
gines. In this framework, question classifica-
tion is the primary task.
However, the current paradigms of question
classification were focused on some speci-
fied type of questions, i.e. factoid questions,
which are inappropriate for the general QA.
In this paper, we propose a new question clas-
sification paradigm, which includes a ques-
tion taxonomy suitable to the general QA and
a question classifier based on MLN (Markov
logic network), where rule-based methods and
statistical methods are unified into a single
framework in a fuzzy discriminative learning
approach. Experiments show that our method
outperforms traditional question classification
approaches.
1 Introduction
During a long period of time, researches on question
answering are mainly focused on finding short and
concise answers from plain text for factoid questions
driven by annual trackes such as CLEF, TREC and
NTCIR. However, people usually ask more complex
questions in real world which cannot be handled by
these QA systems tailored to factoid questions.
During recent years, social collaborative applica-
tions begin to flourish, such asWikipedia, Facebook,
Yahoo! Answers and etc. A large amount of semi-
structured data, which has been accumulated from
these services, becomes new sources for question
answering. Previous researches show that different
sources are suitable for answering different ques-
tions. For example, the answers for factoid questions
can be extracted from webpages with high accuracy,
definition questions can be answered by correspond-
ing articles in wikipedia(Ye et al, 2009) while com-
munity question answering services provide com-
prehensive answers for complex questions(Jeon et
al., 2005). It will greatly enhance the overall per-
formance if we can classify questions into several
types, distribute each type of questions to suitable
sources and trigger corresponding strategy to sum-
marize returned answers.
Question classification (QC) in factoid QA is to
provide constraints on answer types that allows fur-
ther processing to pinpoint and verify the answer
(Li and Roth, 2004). Usually, questions are classi-
fied into a fine grained content-based taxonomy(e.g.
UIUC taxonomy (Li and Roth, 2002)). We can-
not use these taxonomies directly. To guide ques-
tion distribution and answer summarization, ques-
tions are classified according to their functions in-
stead of contents.
Motivated by related work on user goal classi-
fication(Broder, 2002; Rose and Levinson, 2004) ,
we propose a function-based question classification
category tailored to general QA. The category con-
tain six types, namely Fact, List, Reason, Solution,
Definition and Navigation. We will introduced this
1119
category in detail in Section 2.
To classify questions effectively, we unify rule-
based methods and statistical methods into a single
framework. Each question is splited into functional
words and content words. We generate strict pat-
terns from functional words and soft patterns from
content words. Each strict pattern is a regular ex-
pression while each soft pattern is a bi-gram clus-
ter. Given a question, we will evaluate its matching
degree to each patterns. The matching degree is ei-
ther 0 or 1 for strict pattern and between 0 and 1 for
soft pattern. Finally, Markov logic network (MLN)
(Richardson and Domingos, 2006) is used to com-
bine and evaluate all the patterns.
The classical MLN maximize the probability of
an assignment of truth values by evaluating the
weights of each formula. However, the real world
is full of uncertainty and is unnatural to be repre-
sented by a set of boolean values. In this paper,
we propose fuzzy discriminative weight learning of
Markov logic network. This method takes degrees
of confidence of each evidence predicates into ac-
count thus can model the matching degrees between
questions and soft patterns.
The remainder of this paper is organized as fol-
lows: In the next section we review related work
on question classification, query classification and
Markov logic network. Section 2 gives a detailed
introduction to our new taxonomy for general QA.
Section 4 introduces fuzzy discriminative weight
learning of MLN and our methodology to extract
strict and soft patterns. In Section 5 we compare our
method with previous methods on Chinese question
data from Baidu Zhidao and Sina iAsk. In the last
section we conclude this work.
Although we build patterns and do experiments
on Chinese questions, our method does not take ad-
vantage of the particularity of Chinese language and
thus can be easily implemented on other languages.
2 Related Work
Many question taxonomies have been proposed in
QA community. Lehnert (1977) developed the sys-
tem QUALM based on thirteen conceptual cate-
gories which are based on a theory of memory repre-
sentation. On the contrary, the taxonomy proposed
by Graesser et al (1992) has foundations both in the-
ory and in empirical research. Both of these tax-
onomies are for open-domain question answering.
With the booming of internet, researches on
question answering are becoming more practical.
Most taxonomies proposed are focused on factoid
questions, such as UIUC taxonomy (Li and Roth,
2002). UIUC taxonomy contains 6 coarse classes
(Abbreviation, Entity, Description, Human, Lo-
cation and Numeric Value) and 50 fine classes.
All coarse classes are factoid oriented except De-
scription. To classify questions effectively, Re-
searchers have proposed features of different levels,
such as lexical features, syntactic features (Nguyen
et al, 2007; Moschitti et al, 2007) and semantic fea-
tures (Moschitti et al, 2007; Li and Roth, 2004).
Zhang and Lee (2003) compared five machine learn-
ing methods and found SVM outperformed the oth-
ers.
In information retrieval community, researchers
have described frameworks for understanding goals
of user searches. Generally, web queries are classi-
fied into four types: Navigational, Informational,
Transactional (Broder, 2002) and Resource (Rose
and Levinson, 2004). Lee et al (2005) automatically
classify Navigational and Informational queries
based on past user-click behavior and anchor-link
distribution. Jansen and Booth (2010) investigate
the correspondence between three user intents and
eighteen topics. The result shows that user intents
distributed unevenly among different topics.
Inspired by Rose and Levinson (2004)?s work in
user goals classification, Liu et al (2008) describe
a three-layers cQA oriented question taxonomy and
use it to determine the expected best answer types
and summarize answers. Other than Navigational,
Informational and Transactional, the first layer
contains a new Social category which represents the
questions that do not intend to get an answer but to
elicit interaction with other people. Informational
contains two subcategories Constant and Dynamic.
Dynamic is further divided into Opinion, Context-
Dependent and Open.
Markov logic network (MLN) (Richardson and
Domingos, 2006) is a general model combining
first-order logic and probabilistic graphical models
in a single representation. Illustratively, MLN is a
first-order knowledge base with a weight attached
to each formula. The weights can be learnt ei-
1120
TYPE DESCRIPTION EXAMPLES
1. Fact People ask these questions for general facts.
The expected answer will be a short phrase.
Who is the president
of United States?
2. List People ask these questions for a list of an-
swers. Each answer will be a single phrase
or a phrase with explanations or comments.
List Nobel price
winners in 1990s.
Which movie star do
you like best?
3. Reason People ask these questions for opinions or ex-
planations. A good answer summary should
contain a variety of opinions or comprehen-
sive explanations. Sentence-level summariza-
tion can be employed.
Is it good to drink
milk while fasting?
What do you think of
Avatar?
4. Solution People ask these questions for problem shoot-
ing. The sentences in an answer usually have
logical order thus the summary task cannot be
performed on sentence level.
What should I do
during an earthquake?
How to make pizzas?
5. Definition People ask these questions for description of
concepts. Usually these information can be
found in Wikipedia. If the answer is a too
long, we should summarize it into a shorter
one.
Who is Lady Gaga?
What does the Matrix
tell about?
6. Navigation People ask these questions for finding web-
sites or resources. Sometimes the websites are
given by name and the resources are given di-
rectly.
Where can I download
the beta version of
StarCraft 2?
Table 1: Question Taxonomy for general QA
ther generatively (Richardson and Domingos, 2006)
or discriminatively (Singla and Domingos, 2005).
Huynh and Mooney (2008) applies ?
1
-norm regu-
larized MLE to select candidate formulas generated
by a first-order logic induction system and prevent
overfitting. MLN has been introduced to NLP and
IE tasks such as semantic parsing (Poon et al, 2009)
and entity relation extraction (Zhu et al, 2009).
3 A Question Taxonomy
We suggest a function-based taxonomy tailored to
general QA systems by two principles. First, ques-
tions can be distributed into suitable QA subsys-
tems according to their types. Second, we can
employ suitable answer summarization strategy for
each question type. The taxonomy is shown in Tab.
1.
At first glance, classifying questions onto this tax-
onomy seems a solved problem for English ques-
tions because of interrogative words. In most cases,
a question starting with ?Why? is for reason and
?How? is for solution. But it is not always the case
for other languages. From table 2 we can see two
questions in Chinese share same function word ??
??? but have different types.
In fact, even in English, only using interroga-
tive words is not enough for function-based ques-
tion classification. Sometimes the question content
is crucial. For example, for question ?Who is the
current president of U.S. ??, the answer is ?Barak
Obama? and the type is Fact. But for question ?Who
is Barak Obama??, it will be better if we return the
first paragraph from the corresponding Wiki article
instead of a short phrase ?current president of U.S.?.
Therefore the question type will be Definition.
Compared to Wendy Lehnert?s or Arthur
Graesser?s taxonomy, our taxonomy is more prac-
tical on providing useful information for question
1121
Question ?????????
How to cook Kung Pao Chicken?
Type Solution
Question ???????????
What do you think of Avatar?
Type Reason
Table 2: Two Chinese questions share same function
words but have different types
extraction and summarization. Compared to ours,
The UIUC taxonomy is too much focused on factoid
questions. Apart from Description, all coarse types
in UIUC can be mapped into Fact. The cQA
taxonomy proposed in Liu et al (2008) has similar
goal with ours. But it is hard to automatically
classify questions into that taxonomy, especially for
types Constant, Dynamic and Social. Actually the
author did not give implementation in the paper as
well. To examine reasonableness of our taxonomy,
we select and manually annotate 5800 frequent
asked questions from Baidu Zhidao (see Section
5.1). The distribution of six types is shown in Fig.
1. 98.5 percent of questions can be categorized
into our taxonomy. The proportion of each type is
between 7.5% and 23.8%.
The type Navigation was originally proposed in
IR community and did not cause too much concerns
in previous QA researches. But from Fig. 1 we
can see that navigational questions take a substan-
tial proportion in cQA data.
Moreover, we can further develop subtypes for
each type. For example, most categories in UIUC
Reason18.1%
Fact14.4%Solution19.7%Navigation14.8%
List23.8% Definition7.5% Other1.5%
Figure 1: Distribution of six types in Baidu Zhidao data
taxonomy can be regarded as refinement to Fact and
Navigation can be refined into Resource and Web-
site. We will not have further discussion on this is-
sue.
4 Methodology
Many efforts have been made to take advantage of
grammatical , semantic and lexical features in ques-
tion classification. Zhang and Lee (2003) proposed
a SVM based system which used tree kernel to in-
corporate syntactic features.
In this section, we propose a new question clas-
sification methodology which combines rule-based
methods and statistical methods by Markov logic
network. We do not use semantic and syntactic fea-
tures for two reasons. First, the questions posted on
online communities are casually written which can-
not be accurately parsed by NLP tools, especially for
Chinese. Second, the semantic and syntactic pars-
ing are time consuming thus unpractical to be used
in real systems.
We will briefly introduce MLN and fuzzy dis-
criminative learning in section 4.1. The construction
of strict patterns and soft patterns will be shown in
4.2 and 4.3. In section 4.4 we will give details on
MLN construction, inference and learning.
4.1 Markov Logic Network
A first-order knowledge base contains a set of for-
mulas constructed from logic operators and symbols
for predicates, constants, variables and functions.
An atomic formula or atom is a predicate symbol.
Formulas are recursively constructed from atomic
formulas using logical operators. The grounding
of a predicate (formula) is a replacement of all of
its arguments (variables) by constants. A possible
world is an assignment of truth values to all possible
groundings of all predicates.
In first-order KB, if a possible world violates
even one formula, it has zero probability. Markov
logic is a probabilistic extension and softens the hard
constraints by assigning a weight to each formula.
When a possible world violates one formula in the
KB, it is less probable. The higher the weight, the
greater the difference in log probability between a
world that satisfies the formula and a world does
not. Formally, Markov logic network is defined as
1122
follows:
Definition 1 (Richardson & Domingos 2004) A
Markov logic network L is a set of pairs (?
?
, ?
?
),
where ?
?
is a formula in first-order logic and ?
?
is a
real number. Together with a finite set of constants
C = {?
1
, ?
2
, ..., ?
???
}, it defines a Markov network
?
?,?
as follows:
1. ?
?,?
contains one binary node for each pos-
sible grounding of each predicate appearing in
L. The value of the node is 1 if the ground pred-
icate is true, and 0 otherwise.
2. ?
?,?
contains one feature for each possible
grounding of each formula ?
?
in L. The value
of this feature is 1 if the ground formula is true,
and 0 otherwise. The weight of the feature is
the ?
?
associated with ?
?
in L.
There is an edge between two nodes of ?
?,?
iff
the corresponding grounding predicates appear to-
gether in at least one grounding of one formula in
?. An MLN can be regarded as a template for con-
structing Markov networks. From Definition 1 and
the definition of Markov networks, the probability
distribution over possible worlds ? specified by the
ground Markov network ?
?,?
is given by
? (? = ?) =
1
?
exp
(
?
?
?=1
?
?
?
?
(?)
)
MLN weights can be learnt genera-
tively(Richardson and Domingos, 2006) or
discriminatively(Singla and Domingos, 2005). In
discriminative weight learning, ground atom set ?
is partitioned into a set of evidence atoms ? and
a set of query atoms ? . The goal is to correctly
predict the latter given the former. In this paper, we
propose fuzzy discriminative weight learning which
can take the prior confidence of each evidence atom
into account.
Formally, we denote the ground formula set by
? . Suppose each evidence atom ? is given with a
prior confidence ?
?
? [0, 1], we define a confidence
function ? : ? ? [0, 1] as follows. For each ground
atom ?, if ? ? ? then we have ?(?) = ?
?
, else
?(?) = 1. For each ground non-atomic formulas, ?
is defined on standard fuzzy operators, which are
?(??) = 1? ?(?)
?(?
1
? ?
2
) = min(?(?
1
), ?(?
2
))
?(?
1
? ?
2
) = max(?(?
1
), ?(?
2
))
We redefined the conditional likelihood of ?
given ? as
? (???) =
1
?
?
exp
?
?
?
???
?
?
?
?
?
(?, ?)
?
?
=
1
?
?
exp
?
?
?
???
?
?
?
?
?
?
(?, ?)
?
?
Where ?
?
is the set of ground formulas involving
query atoms, ?
?
is the set of formulas with at least
one grounding involving a query atom and ??
?
(?, ?)
is the sum of confidence of the groundings of the i th
formula involving query atoms. The gradient of the
conditional log-likelihood (CLL) is
?
??
?
log?
?
(???)
= ?
?
?
(?, ?)?
?
?
?
?
?
(?
?
??)?
?
?
(?, ?
?
)
= ?
?
?
(?, ?)? ?
?
[?
?
?
(?, ?)] (1)
By fuzzy discriminative learning we can incorpo-
rate evidences of different confidence levels into one
learning framework. Fuzzy discriminative learn-
ing will reduce to traditional discriminative learning
when all prior confidences equal to 1.
4.2 Strict Patterns
In our question classification task, we find function
words are much more discriminative and less sparse
than content words. Therefore, we extract strict pat-
terns from function words and soft patterns from
content words. The definition of content and func-
tion words may vary with languages. In this paper,
nouns, verbs, adjectives, adverbs, numerals and pro-
nouns are regarded as content words and the rest are
function words.
The outline of strict pattern extraction is shown
in Alg. 1. In line 3, we build template ??? by re-
moving punctuations and replacing each character
in each content word by a single dot. In line 4, we
generate patterns from the template as follows. First
we generate n-grams(n is between 2 and ? ) from
1123
Algorithm 1: Strict Pattern Extraction
Input: Question Set ? = {?
1
, ?
2
...?
?
},
Parameters ? and ?
Output: Pattern Set ?
Initialize Pattern Set ? ;1
for each Question ?
?
do2
String ???=ReplaceContentWords(?
?
,?.?);3
Pattern Set ?
?
?
=GeneratePatterns(???,? );4
for each Pattern ? in ?
?
?
do5
if ? in ? then6
UpdateTypeFreq(?,? );7
else8
Add ? to ? ;9
Merge similar patterns in ? ;10
Sort ? by Information Gain on type11
frequencies;
return top ? Patterns in ? ;12
??? during which each dot is treated as a character
of zero length. For coverage concern, if a gener-
ated n-gram ? is not start(end) with dot, we build
another n-gram ?? by adding a dot before(behind) ?
and add both ? and ?? into n-gram set. Then for each
n-gram, we replace each consecutive dot sequence
by ?.*? and the n-gram is transformed into a regular
expression. A example is shown in Tab. 3. Although
generated without exhaustively enumerating all pos-
sible word combinations, these regular expressions
can capture most long range dependencies between
function words.
Each pattern consists of a regular expression as
well as its frequency in each type of questions. Still
Question ???????????
Can I launch online banking services
on internet?
Template ?..??....?
Patterns .*?.*? .*?.*?.*
(?=4) .*??.* .*??.*?
.*??.*?.* .*?.*??.*
.*?.*??.*?.* ?.*?.*
?.*??.* ?.*??.*?
.*?.*?.*
Table 3: Strict patterns generated from a question
from Alg. 1, in line 5-9, if a pattern ? in question ?
?
with type ? is found in ? , we just update the fre-
quency of ? in ?, else ? is added to ? with only
freq. ? equals to 1. In line 10, we merge similar
patterns in ? . two patterns ?
1
and ?
2
are similar iff
?q?QmatchP(q,p1) ? matchP(q,p2), in which
matchP is defined in Section 4.4.
Since a large number of patterns are generated,
it is unpractical to evaluate all of them by Markov
logic network. We sort patterns by information gain
and only choose top? ?good? patterns in line 11-12
of Alg. 1. A ?good? pattern should be discriminative
and of wide coverage. The information gain IG of a
pattern ? is defined as
IG(?) = ? (?)
?
?
?=1
? (?
?
??) log? (?
?
??)+
? (?)
?
?
?=1
? (?
?
??) log? (?
?
??)?
?
?
?=1
? (?
?
) log? (?
?
)
in which ? is the number of question types, ? (?
?
) is
the probability of a question having type ?
?
, ? (?)(or
? (?)) is the probability of a question matching(or
not matching) pattern ?. ? (?
?
??)(or ? (?
?
??)) is
the probability of a question having type ?
?
given
the condition that the question matches(or does not
match) pattern ?. These probabilities can be approx-
imately calculated by type and pattern frequencies
on training data. From the definition we can see
that information gain is suitable for pattern selec-
tion. The more questions a pattern ? matches and
the more unevenly the matched questions distribute
among questions types, the higher IG(?) will be.
4.3 Soft Patterns
Apart from function words, content words are also
important in function-based question classification.
Content words usually contain topic information
which can be a good complement to function words.
Previous research on query classification(Jansen and
Booth, 2010) shows that user intents distribute un-
evenly among topics. Moreover, questions given by
users may be incomplete and contain not function
words. For these questions, we can only predict the
question types from topic information.
Compared with function words, content words
distribute much more sparsely among questions.
1124
When we represent topic information by content
words (or bi-grams), since the training set are small
and less frequent words (or bi-grams) are filtered
to prevent over-fitting, those features would be too
sparse to predict further unseen questions.
To solve this problem, we build soft patterns on
question set. Each question is represented by a
weighted vector of content bi-grams in which the
weight is bi-gram frequency. Cosine similarity is
used to compute the similarity between vectors.
Then we cluster question vectors using a simple
single-pass clustering algorithm(Frakes and Yates,
1992). That is, for each question, we compute its
similarity with each centroid of existing cluster. If
the similarity with nearest cluster is greater than
a minimum similarity threshold ?
1
, we assign this
question to that cluster, else a new cluster is created
for this question.
Each cluster is defined as a soft pattern. Unlike
strict patterns, a question can match a soft pattern
to some extent. In this paper, the degree of match-
ing is defined as the cosine similarity between ques-
tion and centroid of cluster. Soft patterns are flexible
and could alleviate the sparseness of content words.
Also, soft patterns can be pre-filtered by information
gain described in 4.2 if necessary.
4.4 Implementation
Currently, we model patterns into MLN as follows.
The main query predicate is Type(q,t), which
is true iff question q has type t. For strict pat-
terns, the evidence predicate MatchP(q,p) is true
iff question q is matched by strict pattern p. The
confidence of MatchP(q,p) is 1 for each pair of
(q,p). For soft patterns, the evidence predicate
MatchC(q,c) is true iff the similarity of question
q and the cluster c is greater than a minimum simi-
larity requirement ?
2
. If MatchC(q,c) is false, its
confidence is 1, else is the similarity between q and
c.
We represent the relationship between patterns
and types by a group of formulas below.
MatchP(q,+p)?Type(q,+t)
?
?
?
?=?
?Type(q,t?)
The ?+p, +t? notation signifies that the MLN con-
tains an instance of this formula for each (pattern,
type) pair. For the sake of efficacy, for each pattern-
type pair (p,t), if the proportion of type t in ques-
tions matching p is less than a minimum require-
ment ?, we remove corresponding formula from
MLN.
Similarly, we incorporate soft patterns by
MatchC(q,+c)?Type(q,+t)
?
?
?
?=?
?Type(q,t?)
Our weight learner use ?
1
-regularization (Huynh
and Mooney, 2008) to select formulas and prevent
overfitting. A good property of ?
1
-regularization is
its tendency to force parameters to exact zero by
strongly penalizing small terms (Lee et al, 2006).
After training, we can simply remove the formulas
with zero weights.
Formally, to learn weight for each formula, we
iteratively solve ?
1
-norm regularized optimization
problem:
? : ?
?
= argmax
?
log?
?
(???)? ????
1
where ?.?
1
is ?
1
-norm and parameter ? controls the
penalization of non-zero weights. We implement the
Orthant-Wise Limited-memory Quasi-Newton algo-
rithm(Andrew and Gao, 2007) to solve this opti-
mization.
Since we do not model relations among questions,
the derived markov network ?
?,?
can be broken up
into separated subgraphs by questions and the gradi-
ent of CLL(Eq. 1) can be computed locally on each
subgraph as
?
??
?
log?
?
(???)
=
?
?
(
?
?
?
(?
?
, ?
?
)??
?
[?
?
?
(?
?
, ?
?
)]
)
(2)
in which ?
?
and ?
?
are the evidence and query atoms
involving question ?. Eq. 2 can be computed fast
without approximation.
We initialize formula weights to the same posi-
tive value ?. Iteration started from uniform prior
can always converge to a better local maximum than
gaussian prior in our task.
5 Experiments
5.1 Data Preparation
To the best of our knowledge, there is not general
QA system(the system which can potentially answer
1125
all kinds of questions utilizing data from heteroge-
neous sources) released at present. Alteratively, we
test our methodology on cQA data based on obser-
vation that questions on cQA services are of var-
ious length, domain independent and wrote infor-
mally(even with grammar mistakes). General QA
systems will meet these challenges as well.
In our experiments, both training and test data
are from Chinese cQA services Baidu Zhidao and
Sina iAsk. To build training set, we randomly select
5800 frequent-asked questions from Baidu Zhidao.
A question is frequent-asked if it is lexically simi-
lar to at least five other questions. Then we ask 10
native-speakers to annotate these questions accord-
ing to question title and question description. If an
annotator cannot judge type from question title, he
can view the question description. If type can be
judged from the description, the question title will
be replaced by a sentence selected from it. If not,
this question will be labeled as Other.
Each question is annotated by two people. If a
question is labeled different types, another annotator
will judge it and make final decision. If this annota-
tor cannot judge the type, this question will also be
labeled as Other. As a result, disagreements show
up on eighteen percents of questions. After the third
annotator?s judgment, the distribution of each type
is shown in Fig. 1.
To examine the generalization capabilities, the
test data is composed of 700 questions randomly se-
lected from Baidu Zhidao and 700 questions from
Sina iAsk. The annotation process on test data is as
same as the one on training data.
5.2 Methods Compared and Results
We compare four methods listed as follows.
SVM with bi-grams. We extract bi-grams from
questions on training data as features. After filtering
the ones appearing only once, we collect 5700 bi-
grams. LIBSVM(Chang and Lin, 2001)is used as
the multi-class SVM classifier. All parameters are
adjusted to maximize the accuracy on test data. We
denote this method as ?SB?;
MLN with bi-grams. To compare MLN and
SVM, we treat bi-grams as strict patterns. If a ques-
tion contain a bi-gram, it matches the corresponding
pattern. We set ? = 0.01, ? = 0.3 and ? = 0.3.
As a result, 5700 bi-grams are represented by 10485
formulas. We denote this method as ?MB?;
MLNwith strict patterns and bi-grams. We ask
two native-speakers to write strict patterns for each
type. The pattern writers can view training data for
reference and write any Java-style regular expres-
sions. Then we carefully choose 50 most reliable
patterns. To overcome the low coverage, We also
use the method described in Sec. 4.2 to automati-
cally extract strict patterns from training set. We first
select top 3000 patterns by information gain, merge
these patterns with hand-crafted ones and combine
similar patterns. Then we represent these patterns
by formulas and learn the weight of each formula by
MLN. After removing the formula with low weights,
we finally retain 2462 patterns represented by 3879
formulas. To incorporate content information, we
extract bi-grams from questions with function words
removed and remove the ones with frequency lower
than two. With bi-grams added, we get 8173 formu-
las in total. All parameters here are the same as in
?MB?. We denote this method as ?MSB?;
MLN with strict patterns and soft patterns. To
incorporate content information, We cluster ques-
tions on training data with similarity threshold ?
1
=
0.4 and get 2588 clusters(soft patterns) which are
represented by 3491 formulas. We these soft pat-
terns with strict patterns extracted in ?MSB?, which
add up to 7370 formulas. We set ?
2
= 0.02 and the
other parameters as same as in ?MB?. We denote
this method as ?MSS?;
We separate test set into easy set and difficult set.
A question is classified into easy set iff it contains
function-words. As a result, the easy set contains
1253 questions. We measure the accuracy of these
four methods on easy data and the whole test data.
The results are shown in Tab 4. From the results we
can see that all methods perform better on easy ques-
tions and MLN outperforms SVM using same bi-
gram features. Although MSS is inferior to MSB on
F. num Easy data All data
SB NA 0.724 0.685
MB 10485 0.722 0.692
MSB 8173 0.754 0.714
MSS 7370 0.752 0.717
Table 4: Experimental results on Chinese cQA data
1126
F L S R D N
Prec. 0.63 0.65 0.83 0.76 0.69 0.55
Recall 0.55 0.74 0.86 0.76 0.44 0.58
F
1
0.59 0.69 0.84 0.76 0.54 0.56
Table 5: Precision, recall and F-score on each type
easy questions, it shows better overall performance
and uses less formulas.
We further investigate the performance on each
type. The precision, recall and F
1
-score of each type
by method MSS are shown in Tab. 5. From the re-
sults we can see that the performance on Solution
and Reason are significantly better than the others.
It is because the strict patterns for this two types are
simple and effective. A handful of patterns could
cover a wide range of questions with high precision.
It is difficult to distinguish Fact from List because
strict patterns for these two types are partly overlap
each other. Sometimes we need content information
to determine whether the answer is unique. Since
List appears more frequently than Fact on training
set, MLN tend to misclassify Fact toListwhich lead
to low recall of the former and low precision of the
latter. The recall of Definition is very low because
many definition questions on test set are short and
only consists of content words(e.g. a noun phrase).
This shortage could be remedied by building strict
patterns on POStagging sequence.
fraction lines, college entrance exam
???????????????...
Fact: 56.4% List: 33.3% Solu.: 5.5%
lose weight, summer, fast
???????????????...
Reas.: 53.8% Solu.: 42.3% List: 3.8%
TV series, interesting, recent
???????????????...
List: 84.0% Fact: 8.0% Navi.: 2.0%
converter, format, 3gp
??????3gp?mp4????...
Navi.: 75% List: 18.8% Solu.: 6.2%
Table 6: Selected soft patterns on training data
5.3 Case Study on Soft Patterns
To give an intuitive illustration of soft patterns, we
show some of them clustered on training data in Tab.
6. For each soft pattern, we list five most frequent
bi-grams and its distribution on each type(only top 3
frequent types are listed).
From the results we can see that soft patterns are
consistent with our ordinary intuitions. For exam-
ple, if user ask a questions about ?TV series?, he is
likely to ask for recommendation of recent TV series
and the question have a great chance to be List. If
user ask questions about ?lose weight?, he probably
ask something like ?How can I lose weight fast?? or
?Why my diet does not work?? . Thus the type is
likely to be Solution or Reason.
6 Conclusion and Future Work
We have proposed a new question taxonomy tai-
lored to general QA on heterogeneous sources.
This taxonomy provide indispensable information
for question distribution and answer summarization.
We build strict patterns and soft patterns to repre-
sent the information in function words and content
words. Also, fuzzy discriminative weight learning
is proposed for unifying strict and soft patterns into
Markov logic network.
Currently, we have not done anything fancy on the
structure of MLN. We just showed that under uni-
form prior and L1 regularization, the performance
of MLN is comparable to SVM. To give full play
to the advantages of MLN, future work will focus
on fast structure learning. Also, since questions on
online communities are classified into categories by
topic, we plan to perform joint question type infer-
ence on function-based taxonomy as well as topic-
based taxonomy by Markov logic. The model will
not only capture the relation between patterns and
types but also the relation between types in different
taxonomy.
Acknowledgment
This work was supported mainly by Canada?s IDRC
Research Chair in Information Technology program,
Project Number: 104519-006. It is also supported
by the Chinese Natural Science Foundation grant
No. 60973104.
1127
References
G. Andrew and J. Gao. 2008. Scalable training of L1-
regularized log-linear models. In Proc. of ICML 2007,
pp. 33-40.
A. Broder. 2002. A taxonomy of Web search. SIGIR
Forum, 36(2), 2002.
C.C. Chang and C.J. Lin. 2001. LIBSVM: a library
for support vector machines. Software available at
http://www.csie.ntu.edu.tw/ cjlin/libsvm.
W.B. Frakes and R. Baeza-Yates, editors. 1992. In-
formation Retrieval: Data Structures and Algorithms.
Prentice-Hall, 1992.
A.C. Graesser, N.K. Person and J.D. Huber. 1992. Mech-
anisms that generate questions. Questions and Infor-
mation Systems, pp. 167-187), Hillsdale, N.J.: Erl-
baum.
T.N. Huynh and R.J. Mooney. 2008. Discriminative
Structure and Parameter Learning for Markov Logic
Networks. In Proc. of ICML 2008, pp. 416-423.
B.J. Jansen and D. Booth. 2010. Classifying web queries
by topic and user intent. In Proc. of the 28th interna-
tional conference on human factors in computing sys-
tems, pp. 4285-4290.
J. Jeon, W.B. Croft and J.H. Lee. 2005. Finding similar
questions in large question and answer archives. In
Proc. of ACM CIKM 2005,pp. 76-83.
S. Lee, V. Ganapathi and D. Koller. 2005. Effi-
cient structure learning of Markov networks using ?
1
-
regularization.. Advances in Neural Information Pro-
cessing Systems 18.
U. Lee, Z. Liu and J. Cho. 2005. Automatic identification
of user goals in Web search. In Proc. of WWW 2005.
W. Lehnert. 1977. Human and computational question
answering. Cognitive Science, vol. 1, 1977, pp. 47-63.
X. Li and D. Roth. 2002. Learning question classifiers.
In Proc. of COLING 2002, pp. 556-562.
X. Li and D. Roth. 2004. Learning question classifiers:
the role of semantic information. Natural Language
Engineering.
Y. Liu, S. Li, Y. Cao, C.Y. Lin, D. Han and Y. Yu.
2008. Understanding and summarizing answers in
community-based question answering services. In
Proc. of COLING 2008.
A. Moschitti, S. Quarteroni, R. Basili and S. Manand-
har. 2007. Exploiting Syntactic and Shallow Semantic
Kernels for Question/Answer Classification. In Proc.
of ACL 2007.
M.L. Nguyen, T.T. Nguyen and A. Shimazu. 2007. Sub-
tree Mining for Question Classification Problem. In
Proc. of IJCAI 2007.
H. Poon and P. Domingos. 2009. Unsupervised semantic
parsing. In Proc. of EMNLP 2009, pp. 1-10
D.E. Rose and D. Levinson. 2004. Understanding user
goals in web search. In Proc. of WWW 2004.
M. Richardson and P. Domingos. 2006. Markov logic
networks. Machine Learning 62:107-136.
P. Singla and P. Domingos. 2005. Discriminative Train-
ing of Markov Logic Networks. In Proc. of AAAI
2005.
S. Ye, T.S. Chua and J. Lu. 2009. Summarizing Defini-
tion from Wikipedia. In Proc. of ACL 2009.
D. Zhang and W.S. Lee. 2003. Question classification
using support vector machines. In Proc. of ACM SI-
GIR 2003, pp. 26-32.
J. Zhu , Z. Nie, X. Liu, B. Zhang and J.R. Wen. 2009.
StatSnowball: a Statistical Approach to Extracting En-
tity Relationships. In Proc. of WWW 2009, pp. 101-
110
1128
