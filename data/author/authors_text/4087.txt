An Approach for Combining Content-based and Collaborative Filters 
Qing Li 
Dept. of Computer Sciences 
Kumoh National Institute of Technology
Kumi, kyungpook, 730-701,South Korea
liqing@se.Kumoh.ac.kr 
 Byeong Man Kim 
Dept. of Computer Sciences  
Kumoh National Institute of Technology
Kumi, kyungpook, 730-701,South Korea
bmkim@se.Kumoh.ac.kr 
 
 
Abstract 
In this work, we apply a clustering tech-
nique to integrate the contents of items 
into the item-based collaborative filtering 
framework. The group rating information 
that is obtained from the clustering result 
provides a way to introduce content in-
formation into collaborative recommenda-
tion and solves the cold start problem. 
Extensive experiments have been con-
ducted on MovieLens data to analyze the 
characteristics of our technique. The re-
sults show that our approach contributes 
to the improvement of prediction quality 
of the item-based collaborative filtering, 
especially for the cold start problem. 
1 Introduction 
There are two dominant research paradigms of in-
formation filtering: content-based and collabora-
tive filtering. Content-based filtering selects the 
right information for users by comparing represen-
tations of searching information to representations 
of contents of user profiles which express interests 
of users. Content-based information filtering has 
proven to be effective in locating textual items 
relevant to a topic using techniques, such as Boo-
lean queries (Anick et al, 1990; Lee et al, 1993; 
Verhoeff et al, 1961), vector-space queries (Salton 
and Buckley, 1998), probabilistic model (Robert-
son and Sparck, 1976), neural network (Kim and 
Raghavan, 2000) and fuzzy set model (Ogawa et 
al., 1991). However, content-based filtering has 
some limitations: 
 
? It is hard for content-based filtering to pro-
vide serendipitous recommendations, be-
cause all the information is selected and 
recommended based on the content. 
? It is hard for novices to use content-based 
systems effectively. 
Collaborative filtering is the technique of using 
peer opinions to predict the interests of others. A 
target user is matched against the database to dis-
cover neighbors, who have historically had similar 
interests to target user. Items that neighbors like 
are then recommended to the target user. The Tap-
estry text filtering system, developed by Nichols 
and others at the Xerox Palo Alto Research Center 
(PARC), applied collaborative filtering (Douglas, 
1993; Harman, 1994). The GroupLens project at 
the University of Minnesota is a popular collabora-
tive system. Collaborative systems have been 
widely used in so many areas, such as Ringo sys-
tem recommends music albums (Upendar and Patti, 
1995), MovieLens system recommends movies, 
Jeter system recommends jokes (Gupta et al, 1999) 
and Flycasting recommends online radio (Hauver, 
2001). 
Collaborative filtering system overcomes some 
limitations of content-based filtering. The system 
can suggest items (the things to be recommended, 
such as books, music etc.) to users and recommen-
dations are based on the ratings of items, instead of 
the contents of the items, which can improve the 
quality of recommendations. Although collabora-
tive filtering has been successfully used in both 
research and practice, there still remain some chal-
lenges for it as an efficient information filtering. 
This work was supported by Korea Research Foundation 
Grant (KRF-2002-041-D00459). 
? Cold start problem, where recommendations 
are required for items that no user has yet 
rated.  
? Although collaborative filtering can improve 
the quality of recommendations based on the 
user ratings, it completely denies any infor-
mation that can be extracted from contents. 
It is obvious that the content-based filtering 
does not suffer the above problems. So it is a natu-
ral way to combine them in order to achieve a bet-
ter performance of filtering, and take the 
advantages of each. 
The rest of the paper is organized as follows. 
The next section provides a brief describing of re-
lated work. In section 3, we present the detail algo-
rithmic components of our approach, and look into 
the methods of grouping items, calculating the 
similarities between items and solving the cold 
start problem. Section 4 describes our experimental 
work. It provides details of our data sets, evalua-
tion metrics, results of our experiment and discus-
sion of the results. The final section provides some 
concluding remarks. 
2 Related work 
Proposed approaches to hybrid system, which 
combines content-based and collaborative filters 
together, can be categorized into two groups.  
One group is the linear combination of results 
of collaborative and content-based filtering, such 
as systems that are described by Claypool (1999) 
and Wasfi (1999). ProfBuilder (Wasfi, 1999) rec-
ommends web pages using both content-based and 
collaborative filters, and each creates a recommen-
dation list without combining them to make a 
combined prediction. Claypool (1999) describes a 
hybrid approach for an online newspaper domain, 
combining the two predictions using an adaptive 
weighted average: as the number of users access-
ing an item increases, the weight of the collabora-
tive component tends to increase. But how to 
decide the weights of collaborative and content-
based components is unclearly given by the author. 
The other group is the sequential combination 
of content-based filtering and collaborative filter-
ing. In this system, firstly, content-based filtering 
algorithm is applied to find users, who share simi-
lar interests. Secondly, collaborative algorithm is 
applied to make predictions, such as RAAP 
(Delgado et al, 1998) and Fab filtering systems 
(Balabanovic and Shoham, 1990). RAAP is a con-
tent-based collaborative information filtering for 
helping the user to classify domain specific infor-
mation found in the WWW, and also recommends 
these URLs to other users with similar interests. To 
decide the similar interests of users is using scal-
able Pearson correlation algorithm based on web 
page category. Fab system, which uses content-
based techniques instead of user ratings to create 
profiles of users. So the quality of predictions is 
fully depended on the content-based techniques, 
inaccurate profiles result in inaccurate correlations 
with other users and thus make poor predictions.  
As for collaborative recommendation, there are 
two ways to calculate the similarity for clique rec-
ommendation ? item-based and user-based. Sarwar 
(Sarwar et al 2001) has proved that item-based 
collaborative filtering is better than user-based col-
laborative filtering at precision and computation 
complexity. 
Figure1. Overview of the our approach 
3 Overview of our approach 
In this paper, we suggest a technique that intro-
duces the contents of items into the item-based 
collaborative filtering to improve its prediction 
quality and solve the cold start problem. Shortly, 
we call the technique ICHM (Item-based Cluster-
ing Hybrid Method). 
In ICHM, we integrate the item information and 
user ratings to calculate the item-item similarity. 
Figure 1 shows this procedure. The detail proce-
dure of our approach is described as follows: 
? Apply clustering algorithm to group the 
items, then use the result, which is repre-
sented by the fuzzy set, to create a group-
rating matrix. 
? Compute the similarity: firstly, calculate the 
similarity of group-rating matrix using ad-
justed-cosine algorithm, then calculate the 
similarity of item-rating matrix using Pear-
son correlation-based algorithm. At last, the 
Rating Data 
             
 + 
Item
rating
Collaborative 
filter Group
rating
Group
rater
Clustering Itemcontent
Item 
group 
vector
total similarity is the linear combination of 
the above two. 
? Make a prediction for an item by perform-
ing a weighted average of deviations from 
the neighbour?s mean. 
3.1  Group rating 
The goal of grouping ratings is to group the items 
into several cliques and provides content-based 
information for collaborative similarity calculation. 
    Each item has it?s own attribute features, such as 
movie item, which may have actor, actress, direc-
tor, genre, and synopsis as its attribute features. 
Thus, we can group the items based on them. 
The algorithm that is applied for grouping rat-
ings is derived from K-means Clustering Algo-
rithm (Han and Kamber, 2000). The difference is 
that we apply the fuzzy set theory to represent the 
affiliation between object and cluster. As shown in 
Figure 2, firstly, items are grouped into a given 
number of clusters. After completion of grouping, 
the probability of one object j (here one object 
means one item) to be assigned to a certain cluster 
is calculated as follows. 
( , )Pr ( , ) 1-                                (1)
( , )
CS j ko j k
MaxCS i k
=  
where Pr ( , )o j k means the probability of object j  to 
be assigned to cluster k ; The ( , )CS j k  means the 
function to calculate the counter-similarity be-
tween object j  and cluster k ;  ( , )Max CS i k means 
the maximum counter-similarity between an object 
and cluster k . 
 
Input : the number of clusters k  and item attributes  
Output: a set of k clusters that minimizes the squared-
error criterion, and the probability of each item to be 
assigned to each cluster center, which are represented as 
a fuzzy set. 
(1) Arbitrarily choose k  objects as the initial cluster 
centers 
(2) Repeat (a) and (b) until no change 
(a) (Re) assign each object to the cluster to which the 
object is the most similar, based on the mean value of 
the objects in the cluster 
(b) Update the cluster means, i.e., calculate the mean 
value of the objects of each cluster; 
(3) Compute the probability between objects and each 
cluster center. 
Figure 2.  Algorithm for grouping ratings 
The counter-similarity ( , )CS j k  can be calcu-
lated by Euclidean distance or Cosine method.  
3.2 Similarity computation 
As we can see, after grouping the items, we get a 
new rating matrix. We can use the item-based col-
laborative algorithm to calculate the similarity and 
make the predictions for users. 
There are many ways to compute the similarity. 
In our approach, we use two of them, and make a 
linear combination of their results. 
3.2.1 Pearson correlation-based similarity 
The most common measure for calculating the 
similarity is the Pearson correlation algorithm. 
Pearson correlation measures the degree to which a 
linear relationship exists between two variables. 
The Pearson correlation coefficient is derived from 
a linear regression model, which relies on a set of 
assumptions regarding the data, namely that the 
relationship must be linear, and the errors must be 
independent and have a probability distribution 
with mean 0 and constant variance for every set-
ting of the independent variable (McClave and 
Dietrich, 1998). 
, ,1
2 2
, ,1 1
( )( )cov( , )( , )      (2)
( ) ( )
m
u k k u l lu
m m
k l u k k u l lu i
R R R Rk lsim k l
R R R R? ?
=
= =
? ?= =
? ?
?
? ?  
where ( , )sim k l  means the similarity between item 
k  and l ; m  means the total number of users, who 
rated on both item k  and l ; kR , lR  are the average 
ratings of item k  and l , respectively;  
,u kR , ,u lR mean the rating of user u on item k  and l  
respectively. 
3.2.2 Adjust cosine similarity 
Cosine similarity once has been used to calculate 
the similarity of users but it has one shortcoming. 
The difference in rating scale between different 
users will result in a quite different similarity. For 
instance, if Bob only rates score 4 on the best 
movie, he never rates 5 on any movie; and he rates 
1 on the bad movie, instead of the standard level 
score 2. But Oliver always rates according to the 
standard level. He rates score 5 on the best movie, 
and 2 on the bad movie. If we use traditional co-
sine similarity, both of them are quite different. 
The adjusted cosine similarity (Sarwar et al, 2001) 
was provided to offset this drawback. 
, ,1
2 2
, ,1 1
( )( )
 ( , )               (3)
( ) ( )
m
u k u u l uu
m m
u k u u l uu u
R R R R
sim k l
R R R R
=
= =
? ?=
? ?
?
? ?  
where ( , )sim k l  means the similarity between item 
k  and l ; m  means the total number of users, who 
rates on both item k  and l ; uR  are the average rat-
ings of user u ; ,u kR , ,u lR mean the rating of user u on 
item k  and l  respectively. 
3.2.3 Linear combination of similarity 
Due to difference in value range between item-
rating matrix and group-rating matrix, we use dif-
ferent methods to calculate the similarity. As for 
item-ratings matrix, the rating value is integer; As 
for group-rating matrix, it is the real value ranging 
from 0 to 1. The natural way is to enlarge the con-
tinuous data range from [0 1] to [1 5] or reduce the 
discrete data range from [1 5] to [0 1] and then ap-
ply Pearson correlation-based algorithm or ad-
justed cosine algorithm to calculate similarity. We 
call this enlarged ICHM. We also propose another 
method: firstly, use Pearson correlation-based al-
gorithm to calculate the similarity from item-rating 
matrix, and then calculate the similarity from 
group-rating matrix by adjusted cosine algorithm, 
at last, the total user similarity is linear combina-
tion of the above two, we call this combination 
ICHM. 
 ( , ) ( , ) (1- ) ( , )            (4)item groupsim k l sim k l c sim k l c= ? + ?  
where ( , )sim k l  means the similarity between item 
k and l ; c  means the combination coefficient; 
( , )itemsim k l means that the similarity between item 
k and l , which is calculated from item-rating ma-
trix; ( , )groupsim k l means that the similarity between 
item k and l , which is calculated from group-
rating matrix. 
3.3 Collaborative prediction 
Prediction for an item is then computed by per-
forming a weighted average of deviations from the 
neighbour?s mean. Here we use top N  rule to se-
lect the nearest N  neighbours based on the simi-
larities of items. The general formula for a 
prediction on item k of user u (Resnick et al, 1994) 
is: 
,1
,
1
( ) ( , )
                          (5)
( , )
n
u i ii
u k k n
i
R R sim k i
P R
sim k i
=
=
? ?= + ? ?  
where ,u kP  represents the predication for the user 
u on item k ; n  means the total neighbours of item 
k ; ,u iR means the user u  rating on item i ; kR  is the 
average ratings on item k ; ( , )sim k i  means the simi-
larity between item k  and its? neighbour i ; iR  
means the average ratings on item i .  
3.4 Cold start problem 
In traditional collaborative filtering approach, it is 
hard for pure collaborative filtering to recommend 
a new item to user since no user made any rating 
on this new item. However, in our approach, based 
on the information from group-rating matrix, we 
can make predictions for the new item. In our ex-
periment, it shows a good recommendation per-
formance for the new items. In Equation 5, kR  is 
the average rating of all ratings on item k . As for 
the new item, no user makes any rating on it, kR  
should be the zero. Since kR  is the standard base-
line of user ratings and it is zero, it is unreasonable 
for us to apply Equation 5 to new item. Therefore, 
for a new item, we use the neighborsR , the average rat-
ing of all ratings on the new item?s nearest 
neighbour instead of kR , which is inferred by the 
group-rating matrix. 
3.5 A scenario of our approach 
z Users:  
         Number of users: three 
         User name: Tom, Jack, and Oliver 
z Items: 
         Item category: movie 
Number of items: five 
Title of items: Gone with the Wind, Pearl 
Harbour, Swordfish, Hero, The Sound of Music 
z Ratings: 1~5 integer score 
Too  bad:1  Bad:2  Common:3  Good:4  too good:5 
Table 1: Item-rating 
 Tom  Jack  Olive
Gone with the Wind 5 3  
Swordfish 5 2 4 
Pearl Harbour 2 5 
Hero 4 2  
The Sound of Music    
Table 2. Group-rating 
 Cluster1  Cluster2 
Gone with the Wind 98% 0.13% 
Swordfish 100% 0.02% 
Pearl Harbour 1.0% 95% 
Hero 95% 1.2% 
The Sound of Music 0.12% 98% 
The following is a procedure of our approach. 
? Based on the item contents, such as movie 
genre, director, actor, actress, even synopsis, 
we apply clustering algorithm to group the 
items. Here, we use fuzzy set to represent 
the clustering result. Assume the result is as 
follows: Cluster 1: {Gone with the Wind 
(98%), Swordfish (100%), Pearl Harbour 
(1.0%), Hero (95%), The Sound of Music 
(0.12%)}, Cluster 2: {Gone with the Wind 
(0.13%), Swordfish (0.02%), Pearl Harbour 
(95%), Hero (1.2%), The Sound of Music 
(98%)}, the number in the parenthesis fol-
lowing the movie name means the probabil-
ity of the movie belonging to the cluster. 
? We use group-rating engine to make a 
group-rating matrix. As Table 2 shows. 
Then combine the group-rating matrix and 
item-rating matrix to form a new rating ma-
trix.  
? Now, we can calculate the similarity be-
tween items based on this new unified rating 
data matrix. The similarity between items 
consists of two parts. The first part calcu-
lates the similarity based on user ratings, us-
ing the Pearson correlation-based algorithm. 
The second part calculates the similarity 
based on the clustering result by using ad-
justed cosine algorithm. The total similarity 
between items is the linear combination of 
them. For example, when we calculate the 
similarity between Gone with the Wind and 
Swordfish, firstly, itemsim(G,S) and groupsim(G,S)  
are calculated based on Equation 2 and 3 
separately. 
item 2 2 2 2
(5-4) (5-3.5)+(3-4) (2-3.5)sim(G,S) = 1
(5-4) +(3-4) (5-3.5) +(3.5-2)
? ? =?  
group
2 2 2 2
  sim(G,S) =
(0.98-0.59) (1-0.59)+(0.013-0.39) (0.002-0.39)
(0.98-0.59) +(0.013-0.39) (1-0.59) +(0.002-0.39)
0.9999
? ?
=
 
Secondly, sim(G,S) is calculated based on 
Formula 4, here the combination coefficient 
is 0.4. 
sim(G,S)=1 (1-0.4)+0.9999 0.4=0.9999 ? ?  
? Then, predictions for items are calculated by 
performing a weighted average of deviations 
from the neighbour?s mean.  
In the example, we can observe, the item - The 
Sound of Music, which no one make any rating on, 
can be treated as a new item. In traditional item-
based collaborative method, which makes predic-
tion only based on item-based matrix (Table 1), it 
is impossible to make predictions on this item. 
However, in our approach, we can make prediction 
for users, based on group rating (Table 2). 
From the description of our approach, we can 
observe that this approach can fully realize the 
strengths of content-based filtering, mitigating the 
effects of the new user problem. In addition, when 
calculating the similarity, our approach considers 
the information not only from personal tastes but 
also from the contents, which provides a latent 
ability for better prediction and makes serendipi-
tous recommendation. 
3.6 UCHM 
UCHM 
 Movie 1 Movie 2 Movie 3 Cluster 1 Cluster 2
User 1 ? ? ? ? ? 
User 2 ? ? ? ? ? 
User 3 ? ? ? ? ? 
ICHM 
ICHM User 1 User 2 User 3 Cluster 1 Cluster 2
Movie 1 ? ? ? ? ? 
Movie 2 ? ? ? ? ? 
Movie 3 ? ? ? ? ? 
Figure 3. UCHM & ICHM  
 
Clustering technique not only can be applied to 
item-based collaborative recommenders but also 
can be applied to user-based collaborative recom-
menders. Shortly we call the late one UCHM 
(User-based Clustering Hybrid Method)  
In UCHM, clustering is based on the attributes 
of user profiles and clustering result is treated as 
items. However, in ICHM, clustering is based on 
the attributes of items and clustering result is 
treated as users, as Figure 3 shows. 
In Combination UCHM, we apply Equation 2 to 
calculate the similarity in user-rating matrix, and 
User-rating   Matrix Group-rating Matrix   
Group-rating   Matrix   Item-rating Matrix 
Equation 3 to calculate the similarity in group-
rating matrix.  Then make a linear combination of 
them. When we apply Equation 2 and 3 to UCHM, 
k  and l  mean the user and u means the item, in-
stead the original meaning. 
As for UCHM, clustering is based on the user 
profiles. User profiles indicate the information 
needs or preferences on items that users are inter-
ested in. A user profile can consist of several pro-
file vectors and each profile vector represents an 
aspect of his preferences, such as movie genre, 
director, actor, actress and synopsis. The profile 
vectors are automatically constructed from rating 
data by the following simple equation. 
( )   /                                           8A m n=  
where, n is the number of items whose ranking 
value is lager than a given threshold, m is the num-
ber of items containing attribute A among n items 
and its ranking is larger than threshold. In our ex-
periment, we set the value of the threshold as 3.  
For example, in Section 3.5, Tom makes ratings on 
four movies, and three of them lager than the 
threshold 3. From the genre information, we know 
Gone with the Wind belongs to love genre, sword-
fish and Hero belong to action genre. So Tom?s 
profile is as follows. Tom {love (1/3), action (2/3)}. 
4 Experimental evaluations 
4.1   Data set 
Currently, we perform experiment on a subset of 
movie rating data collected from the MovieLens 
web-based recommender. The data set contained 
100,000 ratings from 943 users and 1,682 movies, 
with each user rating at least 20 items. We divide 
data set into a training set and a test data set.  
4.2 Evaluation metrics 
MAE (Mean Absolute Error) has widely been used 
in evaluating the accuracy of a recommender sys-
tem by comparing the numerical recommendation 
scores against the actual user ratings in the test 
data. The MAE is calculated by summing these 
absolute errors of the corresponding rating-
prediction pairs and then computing the average.  
, ,1                               (7)
n
u i u iu
P R
MAE
n
= ?= ?  
where ,u iP  means the user u prediction on item i ; 
,u iR  means the user u  rating on item i  in the test 
data; n is the number of rating-prediction pairs be-
tween the test data and the prediction result. The 
lower the MAE, the more accurate. 
4.3 Behaviours of our method  
0.735
0.745
0.755
0.765
0.775
0 5 10 20 30 40 50 60 70
No. of Clusters
MA
E
ICHM UCHM
 
Figure 4. Sensitivity of the cluster size 
We implement group-rating method described in 
section 3.1 and test them on MovieLens data with 
the different number of clusters. Figure 4 shows 
the experimental results. It can be observed that the 
number of clusters does affect the quality of pre-
diction, no matter in UCHM or ICHM. 
Figure 5. Coefficient 
In order to find the optimal combination coeffi-
cient c in the Equation 4, we conducted a series of 
experiments by changing combination coefficient 
from 0 to 1 with a constant step 0.1. Figure 5 
shows that when the coefficient arrives at 0.4, an 
optimal recommendation performance is achieved. 
0.73
0.735
0.74
0.745
0.75
0.755
0.76
10 20 30 40 50 60NO.of Neigbors
MA
E
Cosine Angle Euclidean Distance
Figure 6. Grouping items 
As described in Section 3.2, our grouping rat-
ings method needs to calculate similarity between 
0.73
0.74
0.75
0.76
0.77
0.78
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Coefficient
MA
E
ICHM UCHM
objects and clusters. So, we try two methods ? one 
is Euclidean distance and the other cosine angle. It 
can be observed in Figure 6 that the approach of 
cosine angle method has a trend to show better per-
formance than the Euclidean Distance method, but 
the difference is negligible. 
Figure 7. Comparison 
From the Figure 7, it can be observed that the 
performance of combination ICHM is the best, and 
the second is the enlarged ICHM, which is fol-
lowed by the item-based collaborative method, the 
last is UCHM (User-based Clustering Hybrid 
Method) which applies the clustering technique 
described in Section 3 to user-based collaborative 
filtering, where user profiles are clustered instead 
of item contents.  
We also can observe that the size of neighbour-
hood does affect the quality of prediction (Her-
locker et al, 1999). The performance improves as 
we increase the neighbourhood size from 10 to 30, 
then tends to be flat. 
1
2
3
4
5
1 2 3 4 5 6 7 8 9 10 11
Movie items
Ra
ti
ng
s
Real Value Predict Value
 
Figure 8.  Cold start problem 
Table 3. MAE of new item 
 10 20 30 40 50 100 
MAE 0.743 0.755 0.812 0.732 0.762 0.757
As for cold start problem, we choose the items 
from the training data set and delete all the ratings 
of those items, thus we can treat them as new items. 
First, we randomly selected item No.946. In the 
test data, user No.946 has 11 ratings, which is de-
scribed by bar real value in Figure 8. We can ob-
serve that the prediction for a new item can 
partially reflect the user preference. To generalize 
the observation, we randomly select the number of 
items from 10 to 50 with the step of 10 and 100 
from the test data, and delete all the ratings of 
those items and treat them as new items. Table 3 
shows that ICHM can solve the cold start problem. 
0.72
0.73
0.74
0.75
MA
E
Synopsis Genre
 
Figure 9. Item attribute 
When we apply clustering method to movie 
items, we use the item attribute ? movie genre. 
However, our approach can consider more dimen-
sion of item attribute, such as actor, actress, and 
director, even the synopsis. In order to observe the 
effect of the high dimension item attributes, we 
collect the 100 movie synopsis from Internet 
Movie Database (http://www.imdb.com) to provide 
attribute information for clustering movies. In our 
experiment, it shows that the correct attributes of 
movies can further improve the performance of 
recommender system, as Figure 9 shows. 
4.4    Our method versus the classic one 
Although some hybrid recommender systems have 
already exited, it is hard to make an evaluation 
among them. Some systems (Delgado et al, 1998) 
use Boolean value (relevant or irrelevant) to repre-
sent user preferences, while others use numeric 
value. The same evaluation metrics cannot make a 
fair comparison. Further more, the quality of some 
systems depends on the time, in which system pa-
rameters are changed with user feedback (Claypool 
et al, 1999), and Claypool does not clearly de-
scribe how to change the weight with time passed. 
However, we can make a simple concept compari-
son. In Fab system, the similarity for prediction is 
only based on the user profiles. As for UCHM, 
which groups the content information of user pro-
files and uses user-based collaborative algorithm 
instead of ICHM, the impact of combination coef-
ficient can be observed in Figure 5. In UCHM, 
when the value of coefficient equals to 1, it de-
scribes condition that Fab applied, which means 
the similarity between users is only calculated 
from the group-rating matrix. In that condition, the 
MAE shows the worst quality of recommendation.  
0.735
0.745
0.755
0.765
0.775
10 20 30 40 50 60 70 80 90 100
No. of neighbors
MA
E
Combination ICHM
Item-based Collaborative
Enlarged ICHM
Combination UCHM
5 Conclusions 
We apply clustering technique to the item content 
information to complement the user rating infor-
mation, which improves the correctness of collabo-
rative similarity, and solves the cold start problem. 
Our work indicates that the correct application of 
the item information can improve the 
recommendation performance. 
References 
Anick, P. G., Brennan, J. D., Flynn, R. A., Hanssen, D. 
R., Alvey, B. and Robbins, J.M.. 1990. A Direct Ma-
nipulation Interface for Boolean Information Re-
trieval via Natural Language Query, In Proc. ACM-
SIGIR Conf., pp.135-150.  
Balabanovic, M. and Shoham, Y.. 1997. Fab: Content-
Based, Collaborative Recommendation, Communica-
tions of the ACM, 40(3), pp.66-72. 
Claypool, M., Gokhale, A., Miranda, T., Murnikov, P., 
Netes, D. and Sartin, M.. 1999. Combining content-
based and collaborative filters in an online newspa-
per , In Proc. ACM-SIGIR Workshop on Recom-
mender Systems: Algorithms and Evaluation. 
Delgado, J., Ishii, N. and Ura, T.. 1998. Content-based 
Collaborative Information Filtering: Actively Learn-
ing to Classify and Recommend Documents, In Proc. 
Second Int. Workshop, CIA'98, pp.206-215. 
Douglas B. Terry. 1993. A tour through tapestry, In 
Proc. ACM Conf. on Organizational Computing Sys-
tems (COOCS). pp.21?30. 
Gupta, D., Digiovanni, M., Narita, H. and Goldberg, K.. 
1999.  Jester 2.0: A New Linear-Time Collaborative 
Filtering Algorithm Applied to Jokes, In Proc. ACM-
SIGIR Workshop on Recommender Systems: Algo-
rithms and Evaluation. 
Han, J., and Kamber, M.. 2000. Data mining: Concepts 
and Techniques. New York: Morgan-Kaufman. 
Harman D.. 1994. Overview of TREC-3, In Proc.TREC-
3, pp.1-19. 
Hauver, D. B.. 2001. Flycasting: Using Collaborative 
Filtering to Generate a Play list for Online Radio, In 
Int. Conf. on Web Delivery of Music. 
Herlocker, J., Konstan, J., Borchers A., and Riedl, J.. 
1999. An algorithmic framework for performing col-
laborative Filtering, In Proc. ACM-SIGIR Conf., 
1999, pp. 230-237. 
Kim, M. and Raghavan, V.V.. 2000. Adaptive concept-
based retrieval using a neural network, In Proc. Of 
ACM-SIGIR Workshop on Mathematical/Formal 
Methods in IR. 
McClave, J. T. and Dietrich, F. H.. 1998. Statistics. San 
Francisco: Ellen Publishing Company. 
Lee, J.H., Kim, M.H. and Lee, Y.H.. 1993.  Ranking 
documents in thesaurus-based Boolean retrieval sys-
tems, Information Processing and Management, 30(1), 
pp.79-91. 
Oard, D.W. and Marchionini, G.. 1996. A conceptual 
framework for text filtering, Technical Report EE-
TR-96-25, CAR-TR-830, CS-TR3643. 
Ogawa, Y., Morita, T. and Kobayashi, K.. 1991. A fuzzy 
document retrieval system using the keyword connec-
tion matrix and a learning method, Fuzzy sets and 
Systems, 1991, pp.39, pp.163-179. 
O'Conner, M. and Herlocker, J.. 1999. Clustering items 
for collaborative filtering, In Proc. ACM-SIGIR 
Workshop on Recommender Systems. 
Resnick, P., Iacovou, N., Suchak, M., Bergstorm, P. and 
Riedl, J.. 1994. GroupLens: An open architecture for 
collaborative filtering of Netnews, In Proc. ACM 
Conf. on Computer-Supported Cooperative Work. 
pp.175-186. 
Ricardo Baeza-Yates, Berthier Riberio-Neto. 1999. 
Modern Information Retrieval. New York:Addison-
Wesley Publishers. 
Robertson S. E. and Sparck Jones K.. 1976. Relevance 
weighting of search terms, J. of the American Society 
for Information Science, 1976, pp.27, pp.129-146. 
Salton, G. and Buckley, C.. 1988. Term-weight ap-
proaches in automatic retrieval, Information Proc-
essing and Management, 24(5), 1988, pp.513-523. 
Sarwar, B. M., Karypis, G., Konstan, J. A. and Riedl, J.. 
2001. Item-based Collaborative Filtering Recom-
mendation Algorithms, In Proc. Tenth Int. WWW 
Conf. 2001, pp. 285-295. 
Upendra, S. and Patti, M.. 1995. Social Information 
Filtering: Algorithms for Automating "Word of 
Mouth", In Proc. ACM CHI'95 Conf. on Human Fac-
tors in Computing Systems. pp.210?217. 
Verhoeff, J., Goffman, W. and Belzer, J.. 1961. Ineffi-
ciency of the use of the boolean functions for infor-
mation retrieval systems, Communications of the 
ACM, 4, pp.557--558, pp.594. 
Wasfi, A. M. A.. 1999. Collecting User Access Patterns 
for Building user Profiles and Collaborative Filter-
ing, In Int. Conf. on Intelligent User Interfaces. 
pp.57- 64. 
 
Extraction of User Preferences from a Few Positive Documents 
Byeong Man Kim, Qing Li 
Dept. of Computer Sciences 
Kumoh National Institute of Technology 
Kumi, kyungpook, 730-701,South Korea 
(Bmkim, liqing)@se.Kumoh.ac.kr
Jong-Wan Kim 
School of Computer & Information  
Taegu University 
Kyungsan-City, Kyungpook, South Korea
jwkim@biho.taegu.ac.kr 
 
 
Abstract 
In this work, we propose a new method 
for extracting user preferences from a few 
documents that might interest users. For 
this end, we first extract candidate terms 
and choose a number of terms called ini-
tial representative keywords (IRKs) from 
them through fuzzy inference. Then, by 
expanding IRKs and reweighting them us-
ing term co-occurrence similarity, the fi-
nal representative keywords are extracted. 
Performance of our approach is heavily 
influenced by effectiveness of selection 
method for IRKs so we choose fuzzy in-
ference because it is more effective in 
handling the uncertainty inherent in se-
lecting representative keywords of docu-
ments. The problem addressed in this 
paper can be viewed as the one of finding 
a representative vector of documents in 
the linear text classification literature. So, 
to show the usefulness of our approach, 
we compare it with two famous methods - 
Rocchio and Widrow-Hoff - on the 
Reuters-21578 collection. The results 
show that our approach outperforms the 
other approaches. 
1 Introduction 
Agent technology is able to provide increasingly 
more services for individuals, groups, and organi-
zations. Agents, which have been developed for 
Internet, have addressed many tasks such as infor-
mation finding, filtering and presentation, contract 
negotiation, and electronic commerce (Soltysiak 
and Crabtree, 2000). Most of them rely on the 
knowledge of the user. The inclusion of user in-
formation becomes a key area.  
A user model that represents some aspects of a 
user?s information needs or preferences can be use-
ful in any information system design, and in the 
case of information filtering (Kim et al, 2000). 
User models can be constructed by hand, or 
learned automatically based on feedback provided 
by the users. Some systems require users to explic-
itly specify their profiles, often as a set of key-
words or categories. But it is difficult for a user to 
exactly and correctly specify their information 
needs. The machine learning techniques offer the 
potential to automatic construction and continuous 
refinement of user model.  
The research systems adopting the machine 
learning techniques have been applied feedback 
techniques that explicitly provide relevance judg-
ments on documents. Studies have shown that such 
explicit feedback from the user is clearly useful 
(Goldberg, 1992; Yan and Garcia-Molina, 1995), 
but, in practice, many users are unwilling to pro-
vide relevance judgments on documents (Pazzani, 
M., Billsus, 1997; Baeza-Yates and  Ribeiro-Neto, 
1999) . Users may have problems to decide about 
some documents.  An alternative is to use implicit 
feedback where document relevance is inferred 
from user?s behavior, which has received increased 
attention in recent years (Nichols, 1997; Konstan et 
al., 1997; Kim, 2000)   
This paper focuses upon the extraction of user 
preferences from a few documents that might in-
terest a user. It does not consider how to provide 
relevance judgment on documents, i.e. it assumes 
This work was supported by grant No. 2000-1-51200-008-2
 from the Korea Science & Engineering Foundation 
that relevant documents are given explicitly or im-
plicitly. Our approach is based on the vector space 
model (Baeza-Yates and Ribeiro-Neto, 1999), 
where text-based documents are represented as 
vectors of term weights. So, the problem addressed 
in this paper is how to extract representative key-
words from documents provided by a user and 
what weights should be assigned to these keywords. 
We present a new technique to solve this problem. 
The proposed method is composed of two parts, 
one is to select initial representative keywords 
(IRKs) and the other is to automatically expand 
and reweight IRKs. For the first part, we can con-
sider feature selection methods (Yang and Peder-
sen, 1997) that focus on performance improvement 
and dimensionality reduction of document classifi-
ers for a huge amount of documents covering vari-
ous categories. However, since this kind of 
methods select features using information of other 
categories and negative document sets as well as 
positive ones, it is impossible to apply these to the 
target problem in this paper that extract feature 
keywords from only few positive documents in the 
same category. As alternatives, we can consider 
the Rocchio algorithm and Widrow-Hoff algorithm 
used as a training algorithm for linear text classi-
fier since these algorithms can extract keywords 
and assign weights to them effectively with only 
positive document sets. However, here, a new 
technique that adopts fuzzy inference to extract or 
generate IRKs from a few example documents (the 
set of documents judged relevant by the users) is 
suggested since the existing algorithms did not 
show good results as we expected.  
For the second part, we can choose one of 
query term expansion and term weight modifica-
tion methods based on vector model (Xu and Croft , 
1996; Mitra et al,1998; Baeza-Yates and  Ribeiro-
Neto, 1999). Instead, we take a new approach 
where the term co-occurrence similarity is intro-
duced as a measure of similarity between the dis-
tributions within the feedbacked documents of a 
given term and the initial query. With this similar-
ity and the document frequency in feedbacked 
documents, the weight of the term in the new query 
was calculated.  
In the next section, Rocchio and Widrow-Hoff 
algorithms are reviewed. Section 3 presents a 
method for user?s preference extraction. The ex-
periments to test the proposed method will be out-
lined in Section 4. Finally, conclusion is followed. 
2 Background 
To extract a user?s preference from example docu-
ments is the same problem as finding their 
representative vector in linear text classifiers. A 
variety of algorithms for training linear classifiers 
have been suggested. Among them, here, we only 
review two widely used algorithms, Rocchio algo-
rithm and Widrow-Hoff algorithm, for comparing 
with our method.  
The Rocchio algorithm (David et al, 1996) is a 
batch algorithm. So, it produces a new weight vec-
tor w  from an existing weight vector oldw  by ana-
lyzing the entire set of training data at once. The 
j th?  component of w   is : 
c
Ci
ji
c
Ci
ji
joldj nn
x
n
x
ww ?
?
?
?
+= ??
,,
, ???               (1) 
where, ,i jx  means j th?  component of i th?  docu-
ment vector ix  and  n  is the number of training 
documents. C is the set of positive training docu-
ments, and cn is the number of positive training 
documents. The parameter ?? , and ?  control the 
relative impact of the original weight vector, the 
positive examples, and the negative examples, re-
spectively. However, in our experiments, ? = 0, 
? =1, and ?  = 0 because only positive examples 
are given in our application. Neither original 
weight vector nor negative examples is given.  
The Widrow-Hoff algorithm (David et al, 1996) 
is an online algorithm where one training example 
is presented at a time. It updates its current weight 
vector based on the example and then discards the 
example, retaining only the new weight vector. A 
new weight vector wi+1 is computed from an old 
weight vector iw  and a training document ix  with 
class label iy . The class label iy  is 1 if a training 
document ix  is in the set of positive or relevant 
training documents, otherwise 0. In our application, 
iy  is always 1 because we deal with only positive 
examples. The initial weight vector w1 is typically 
set to zero vector, w1 = (0, ... 0). 
1, , ,2 ( )i j i j i i i i jw w w x y x?+ = ? ? ?               (2) 
where,  ?   is the learning rate which controls how 
quickly the weight vector w is allowed to change 
and ii xw ? is the cosine value of the two vectors. 
3 Extraction of user preferences 
User preferences are extracted from a few example 
documents through two steps: a) the first step gen-
erates a set of keywords called IRKs (Initial Repre-
sentative Keywords) which corresponds to the 
initial user query in the relevance feedback tech-
niques of IR and b) these IRKs are expanded and 
reweighted by a relevance feedback technique. 
It is very important to select IRKs reflecting 
user?s preferences well from example or training 
documents (set of documents judged relevant by 
the user) because we have to calculate term co-
occurrences similarity between these IRKs and 
candidate terms within each example document. 
Three factors of a term (term frequency, document 
frequency within positive examples, and IDF) are 
used to calculate the importance of a specific term.  
Since these factors essentially have inexact and 
uncertain characteristics, we combine them by 
fuzzy inference instead of a simple equation.  
The IRKs are selected based on the selection 
criteria that each example document has at least 
one or more IRKs. After selecting the IRKs, we 
perform term modification process based on the 
term co-occurrence similarity between these IRKs 
and candidate terms. The Rocchio and Widrow-
Hoff algorithms do not consider the term co-
occurrence relationship within training documents. 
But, we regard the term co-occurrence relationship 
as the key factor to calculate the importance of 
terms under the assumption that the IRKs reflect 
user?s preferences well.  
3.1 Calculation of the Representativeness of 
Terms through Fuzzy Inference 
The given positive examples are transformed into 
the set of candidate terms through eliminating 
stopwords and stemming by Porter?s algorithm. 
The TF, DF, and IDF of each term are calculated 
based on this set and used as inputs of fuzzy infer-
ence. From now on, we will explain these three 
input variables. The TF (Term Frequency) is the 
term frequency of a specific term not in a docu-
ment but in a set of documents, which is calculated 
by dividing total occurrences of the term in a set of 
documents by the number of documents in the set 
containing the term. It needs to be normalized for 
being used in fuzzy inference. The following 
shows the normalized term frequency (NTF). 
max
i
i
i
j
j
j
TF
DF
NTF
TF
DF
= ? ?? ?? ?? ?
              (3) 
where, iTF  is the frequency of term ti in the exam-
ple documents, iDF  is the number of documents 
having term ti in the example document, 
[ ]j jMax x means the maximum value of vari-
able jx . 
The DF (Document Frequency) represents the 
frequency of documents having a specific term 
within the example documents. The normalized 
document frequency, NDF, is defined in equation 
(4), where iDF is the number of documents having 
term ti in the example documents. 
     
max
i
i
j j
DF
NDF
DF
=                        (4) 
The IDF (Inverse Document Frequency) repre-
sents the inverse document frequency of a specific 
term over an entire document collection not exam-
ple documents. The normalized inverse document 
frequency, NIDF, is defined as follows: 
,    log
max
i
i i
j j i
IDF NNIDF IDF
IDF n
= =              (5) 
where, N is the total number of documents and in  
is the number of documents containing term ti . 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1. Fuzzy input/output variables 
0 0.2 0.4 0.6 0.8 1 
Z S M XX L
1
TW 
11 0.2 0.7
L S L
10.80.61 0.1 0.3
M 
1
NTF=S NDF, NIDF 
1
(a) Input variable    
(b) Output variable   
Z: zero       
S: small       
M: middle    
L : large       
X: x large     
XX:xx 
larger    
 
S
Figure 1 shows the membership functions of the 
input/output variables - 3 inputs (NTF, NDF, NIDF) 
and 1 output (TW) - used in our method. As you 
can see in Figure 1(a), NTF variable has 
{ S(Small), L(Large) }, and NDF and NIDF vari-
ables have { S(Small), M(Middle), L(Large) } as 
linguistic labels (or terms). The fuzzy output vari-
able, TW (Term Weight) which represents the im-
portance of a term, has six linguistic labels as 
shown in Figure 1(b). 
The 18 fuzzy rules are involved to infer the 
term weight (TW). The rules are constructed based 
on the intuition that the important or representative 
terms may occur across many positive example 
documents but not in general documents, i.e., their 
NDF and NIDF are very high. As shown in Table 1, 
the TW of a term is Z in most cases regardless of 
its NDF and NTF if its NIDF is S, because such 
term may occur frequently in any document and 
thus its NDF and NTF can be high. When NDF of 
a term is high and its NIDF is also high, the term is 
considered as a representative keyword and then 
the output value is between X and XX. The other 
rules were set similarly. 
Table 1. Fuzzy inference rules 
NIDF 
NDF 
S M L NIDF 
NDF      
S M L 
S Z Z S S Z S M
M Z M L M Z L X
L S L X L S X XX
NTF = S NTF = L
We can get the term weight TW through the 
following procedure. But, the output is in the form 
of fuzzy set and thus has to be converted to the 
crisp value. In this paper, the center of gravity 
method is used to defuzzify the output  (Lee, 1990). 
? Apply the NTF, NDF, and NIDF fuzzy val-
ues to the antecedent portions of 18 fuzzy 
rules. 
? Find the minimum value among the mem-
bership degrees of three input fuzzy values. 
? Classify every 18 membership degree into 6 
groups according to the fuzzy output vari-
able TW. 
? Calculate the maximum output value for 
each group and then generate 6 output val-
ues. 
3.2 Selection of Initial Representative Key-
words 
After calculation of the term weights of candidate 
terms through fuzzy inference, some candidate 
terms are selected as IRKs based on their weights 
with the constraint that each example document 
should contain at least one or more IRKs. The al-
gorithm for selection of IRKs is given in Figure 2. 
Let us consider the following example to under-
stand our selection procedure. 
i) An example document set, DS, is composed of 
documents d1, d2, d3, d4, d5, and d6. Each 
document contains the following terms:  
d1 = {a, b, f}, d2 = {a, c, d}, d3 = {d, e, f}, 
d4 = {d, f},  d5 = {b, c, e},  d6 = {e, f} 
ii) A candidate term set, TS, is composed of {(a, 
0.9), (b, 0.8), (c, 0.7), (d, 0.6), (e, 0.5), (f, 0.4)}, 
where (ti, TWi) represents that TWi is the term 
weight of term ti. 
If we apply the algorithm in Figure 2 to this ex-
ample, then temporary variables in line 2, 3 and 4 
are initialized. The statement block from line 5 to 
line 14 is executed repeatedly until at least one or 
more IRKs are extracted from every example 
document in DS. Let us assume that the documents 
in the example document set are processed in se-
quence. After the first loop of the statement block 
from line 5 to line 14 is executed, the output value 
of ITS contains only term ?a?. There is no change 
in ITS after the second loop of the block because 
term ?a? has already been included in ITS. After 
d3, the third loop of the block, is processed, a term 
?d? is newly added to ITS. So, there is {a, d} in 
ITS. After d4, d5, and d6 are sequentially proc-
essed, none, term ?b?, and term ?e? are added to 
ITS, respectively. Therefore the algorithm return 
ITS having a set of terms {a, b, d, e}. We can find 
the algorithm in Figure 2 works well according to 
our constraint. 
 
Input: DS (Example Documents Set) 
           TS (Candidate Terms Set) 
 1] Procedure get_ITS(DS, TS) 
 2] ITS: Initial Representative Terms Set, initialized to empty. 
 3] TS': Temporary Terms Set, initialized to TS. 
 4] d, t: Document and Term element respectively. 
 5] Repeat 
 6]   Select a document element as d from DS.   
 7]   Repeat 
 8]      Select the highest element as t in TS' 
              according to the weight.  
 9]      If t appears in d and not member in ITS 
             Then Add t to ITS.   
10]     Remove t from TS. 
11]   Until t appears in d.  
12]   Remove d from DS. 
13]   Assign TS to TS'. 
14] Until DS is empty.    
15] Return ITS. 
Figure 2. The algorithm for selection of initial rep-
resentative terms 
3.3 Automatic Expansion and Reweighting of  
IRKs 
After the IRKs are selected, additional terms are 
selected to be expanded in the order of their 
weights calculated by the method in Section 3.1. 
Let us assume that 5 terms are used to represent a 
user's preference and the number of IRKs is 3. 
Then, 2 terms with highest weights except IRKs 
are selected additionally. The IRKs and these terms 
constitute the final representative keywords 
(FRKs) and are reweighted by considering the co-
occurrence similarity with IRKs.  For this end, the 
relevance degrees of the FRKs in every document 
are calculated with the equation (6). Each positive 
example document represents user?s preferable 
content. In other words, each document tends to 
contain general or specific or partial contents. We 
regard the IRKs as the essential terms of the given 
positive examples. So, the possibility that the re-
lated terms, e.g., synonym, collocated terms and so 
on, occurred together with these IRKs in the same 
document set increases.  
1
)(
log1 1
2
+
? ?
?= =
n
tfkf
RD
n
j
ikjk
pik              (6) 
where, RDik is the relevance degree between IRKs 
and candidate term ti in document dk, kfjk is the fre-
quency of initial representative keyword j in 
document dk, tfik is the frequency of candidate term 
ti in document dk, n is the number of IRKs, p is a 
control parameter. In our experiments, p is set to 
10. The RDik is treated as 0 if it has negative value. 
For example, let K be a set of IRKs consisting 
of k1, k2 and k3 terms and their frequencies in 
document d1 be 4, 3, and 1, respectively. Also, let 
the frequency of term t1 be 2. Then, its relevance 
degree is calculated as follows: 
1
3
)1(12log1
222
1011 +?++?=RD =  0.762 
As shown in the above equation, RDik is in-
versely proportional to the sum of term frequency 
difference between initial representative term and 
candidate term. So, the higher is the value of Rd, 
the more similar the co-occurrence is, that is, the 
equation reflects the co-occurrence similarity be-
tween initial representative terms and a candidate 
term appropriately. After calculating the relevance 
degree of a candidate term, the weight of the term 
in the set of example documents is determined by 
the following equation: 
iikik
n
k
ikikri
IDFTFw
RDww
?=
? ?=
=1
)(
                (7) 
where, wri is the weight of term ti in the document 
set, wik is the weight of term ti in document dk, TFik 
is the frequency of term ti in document dk, IDFi is 
the inverse document frequency of term ti, and n is 
the number of example documents. 
The equation (7) is a modification of the Roc-
chio's in Section 2. Different from that equation, 
we additionally use the term relevance degree be-
tween initial representative terms and a candidate 
term. Let us assume that the IDF value of the can-
didate term t1 is 1.0 and it occurs 3, 2, and 1 within 
document d1, d2 and d3, respectively. If the rele-
vance degrees for three documents are also as-
sumed to 0.3, 0.5, and 0.7, respectively, then the 
weight of candidate term ti is calculated as below. 
82.1))7.00.1()5.00.2()4.00.3((1 =?+?+?=rw  
Finally, the weights of the FRKs are calculated 
by the following equation: 
rikii www +=                              (8) 
where, wki, is  the initial weight of term ti. Instead 
of using the weight obtained by fuzzy inference, 
the initial weight wki of term ti is recalculated by 
the equation (9), if the term is in IRKs and other-
wise 0. The equation is the one introduced to as-
sign a weight to an initial query term in IR systems 
based on the vector space model (Baeza-Yates and  
Ribeiro-Neto, 1999). 
???
?
???
?????
?
???
? ?+=
ijj
i
ki n
N
freq
freq
w log
max
5.0
5.0        (9) 
where, freqi is the frequency of initial representa-
tive keyword ti, ni is the frequency of documents in 
which ti appear, and N is the total number of docu-
ments. 
Let K = {t1, t3, t4} be the set of IRKs, WK = 
{3.0, 2.0, 1.0} be the set of their weights calculated 
by the equation (9), T = {t1, t2, t3, t4, t5} be the 
set of FRKs, and WT = {5.0, 4.0, 3.0, 2.0, 1.0} be 
their weights through the equation (7). Then, we 
can get the final weights of FRKs, {8.0,4.0, 5.0, 
3.0, 1.0}. 
4 Experiments 
We used Reuters-21578 data as an experimental 
document set. This collection has five different sets 
of contents related categories. They are 
EXCHANGES, ORGS, PEOPLE, PLACES and 
TOPICS. Some of the categories set have up to 265 
categories, but some of them have just 39 catego-
ries. We chose the TOPICS categories set which 
has 135 categories. We divided the documents ac-
cording to the ?ModeApte? split. There are 9603 
training documents and 3299 test documents. 
Among the 135 categories, we first chose only 90 
ones that have at least one training example and 
one testing example. Then, we finally selected 21 
categories that have from 10 to 30 training docu-
ments. The 3019 documents of those categories are 
used as testing documents. The document fre-
quency information from 7770 training documents 
in 90 categories is used to calculate IDF values of 
terms. We did not consider negative documents 
under the assumption that only positive documents 
coincident with users? preferences were given im-
plicitly or explicitly . 
Documents are ranked by the cosine similarity 
and the following F-measure (Baeza-Yates and 
Ribeiro-Neto, 1999), which is a weighted combina-
tion of recall and precision and popularly used for 
performance evaluation. Since the maximum value 
for F can be interpreted as the best possible com-
promise between recall and precision, we use this 
maximum value.  
)/(2
11
2
jjjj
jj
j RPRP
RP
F +=
+
=          (10) 
where, Rj and Pj are the recall and precision for the 
j?th document in the ranking and Fj is their har-
monic mean.  
First, our method was compared to the Rocchio 
and Widrow-Hoff algorithms. To see the effect of 
the number of FRKs, we made experiments by 
varying it from 5 to 30 in increment 5 and for the 
case that all terms are used.  Table 2 shows the 
overall or summary result of the proposed method 
compared to the two existing algorithms for 
21categories. The result shows that our method is 
better than the others in all cases, especially when 
10 terms are used to represent user preferences. 
Table 3 shows the detail result in that case, i.e. the 
F-values and the performance improvement ratios 
when 10 terms are used. The proposed method has 
achieved about 20% over Rocchio algorithm and 
10% over Widrow-Hoff algorithm on the average.  
When 5 terms are used to represent user prefer-
ences, 19 categories among 21 categories are used 
because ?strategic-metal? and ?pet-chem? catego-
ries do not satisfy the constraint in Section 3.2, i.e., 
5 terms are too few to cover all training documents. 
Table 2. Performance of 21 categories in the 
REUTERS corpus and comparison with two exist-
ing algorithms. 
 Our Rocchio W.H. 
5 0.582 0.511 0.566 
10 0.594 0.496 0.540 
15 0.571 0.490 0.529 
20 0.552 0.489 0.522 
25 0.545 0.491 0.493 
30 0.541 0.495 0.500 
All 0.490 0.467 0.483 
It is not clear which component of our method 
mainly contributes to such improvement since our 
method consists of two main components - one is 
for extracting IRKs, the other for expanding and 
reweighting of IRKs. To analyze our method, we 
made several variants of the proposed method and 
did experiments with them. The variants are named 
by the sequence of the following symbols.  
IF, IR, IW: mean that IRKs are selected based on 
the weight obtained by the method in Section 3.1, 
the Rocchio algorithm, and the Widrow-Hoff algo-
rithm, respectively. 
RC, RR, RW: mean that terms are reweighted by 
the method in Section 3.3, the Rocchio algorithm, 
and the Widrow-Hoff algorithm, respectively. 
EC, EF, ER, EW: mean that expanded terms are 
selected based on the weight obtained by applying 
the method in Section 3.3, the method in Section 
3.1, the Rocchio algorithm, and the Widrow-Hoff 
algorithm, respectively. 
For example, the proposed method in Section 3 is 
named as IF_EF_RC, which means IRKs, and ex-
panded terms are selected based on the weight cal-
culated by the method in Section 3.1 and then 
reweighted by the method in Section 3.3. For an-
other example, the method called by IF_RC_EC 
means that IRKs are selected based on the weight 
obtained by the method in Section 3.1 and then all 
terms are reweighed by the method in Section 3.3 
before expanded terms are selected. 
In the proposed method, fuzzy inference tech-
nique is used to extract IRKs. So, we tried two 
variants, IR_ER_RC and IW_EW_RC, where the 
Rocchio and Widrow-Hoff algorithms are used 
respectively to calculate the representativeness (or 
weights) of terms instead of the method in Section 
3.1, and then IRKs and expanded terms are se-
lected based on these weights. The variants all use 
the reweighting scheme in Section 3.3. Table 4 
shows that other keyword extraction algorithms do 
not show any benefit over the fuzzy inference ap-
proach. We can also observe that when one of the 
existing algorithms is combined with the second 
component of our method, the performance im-
provement over the case that the algorithm solely 
is used is negligible. 
The method to extract IRKs reflecting user?s 
preference directly affects the result of the term 
reweighting process because the process is based 
on the term co-occurrence similarity with the IRKs. 
If the terms that are far from user?s preference are 
extracted as IRKs, then some terms that actually 
are improper in representing user?s information 
needs may be assigned with high weights during 
the reweighting process and then the final vector 
generated from the results may be disqualified 
from representing user?s preferences. So, we can 
know that our fuzzy inference technique is effec-
tive to extract IRKs from the results in Table 4. 
To demonstrate the usefulness of the second 
part of our method, i.e., the expansion and re-
weighting technique, we also tried the 5 variants of 
our method (IF_RC_EC, IF_RR_ER, IF_RW_EW, 
IF_EF_RR, IF_EF_RW). Table 5 shows the all 
variants are not better than the original though they 
outperform Rocchio and Widrow-Hoff algorithms. 
5 Conclusions 
In this study, we apply fuzzy inference technique 
and term reweighting scheme based on the term 
co-occurrence similarity to the problem that extract 
important keywords representing contents of 
documents presented by users. We have conducted 
extensive experiments on the Reuters-21578 col-
lection. The results show that our method outper-
forms two well-known training algorithms for 
linear text classifiers. Moreover, some variants of 
our method have been explored to analyze the 
characteristics of our method. Though this paper 
only describes how to extract user preferences 
from example documents, the technique will be 
applicable to several areas such as query modifica-
tion in IR, user profile modification in information 
filtering, text summarization and so forth directly 
or with some modifications.  
Since only positive examples are considered in 
our method, the method is not applicable to a 
document set containing negative examples. For 
covering negative examples, it needs to modify the 
fuzzy inference rules with considering additional 
input variables.  The proposed method was also 
designed for a small set of documents. So, we 
could not achieve performance improvement as 
described in this paper when our method is applied 
to a large set of documents. However, such a prob-
lem will be alleviated if clustering techniques are 
used together as in (Alberto et al, 2001; Lam and 
Ho, 1998; Ugur et al, 2000). 
Table 3. The detail result when 10 terms are used 
for user preferences 
 Our Rocchio W.H. 
lumber 0.7273 0.4444 0.6667 
dmk 0.4 0.4444 0.4 
sunseed 0.5714 0.3333 0.3333 
lei 1 0.8 1 
soy-meal 0.6667 0.5143 0.5185 
fuel 0.4615 0.4615 0.4615 
heat 0.75 0.75 0.75 
soy-oil 0.3704 0.2692 0.32 
lead 0.5625 0.5 0.5 
strategic- 0.13333 0.1053 0.1408 
hog 0.8 0.6 0.8 
orange 0.9091 0.9091 0.8571 
housing 0.5714 0.6667 0.5714 
tin 0.96 0.7857 0.9231 
rapeseed 0.6154 0.5714 0.6154 
wpi 0.5714 0.5882 0.5882 
pet-chem 0.3704 0.2727 0.2759 
silver 0.381 0.4 0.5 
zinc 0.8966 0.6667 0.6842 
retail 0.1667 0.0548 0.0548 
sorghum 0.5882 0.2727 0.3871 
Average 0.5940 0.4957 0.5404 
Table 4. The performance of our method and its 
two variants that use Rocchio and Widrow-Hoff 
algorithms instead of fuzzy inference, respectively. 
Table 5. The performance of our method and its 
five variants that use different reweighting and ex-
panding approaches. 
References 
Alberto Diaz Esteban, Manuel J. Mana Lopez, Manuel 
de Buenaga Rodriguez, Jose Ma Gomez Hidalgo and 
Pablo Gervas Gomez-Navarro. 2001. Using linear 
classifiers in the integration of user modeling and 
text content analysis in the personalization of a web-
based Spanish news servic, In Proceedings. of the 
Workshop on Machine Learning, Information Re-
trieval and User Modeling, 8th International Confer-
ence on User Modeling. 
Baeza-Yates, R. and Ribeiro-Neto B.. 1999.  Modern 
Information Retrieval, ACM Press, USA. 
David D. Lewis, Robert E. Schapire , James P. Callan 
and Ron Papka. 1996. Training algorithms for linear 
text classifiler, In Proc. of SIGIR-96, 19th ACM In-
ternational Conference on Research and Develop-
ment in Information Retrieval. 
Goldberg D., Nichols D., Oki B. M., and Terry D.. 1992. 
Using collaborative filtering to weave an information 
tapestr, Communication of the ACM, 35(12), p61-70. 
Kim, J., Oard, D.W., and Romanik, K.. 2000. User 
modeling for information filtering based on implicit 
feedback, In Proceedings. of ISKO-France. 
Konstan J. A. , Miller B. N., Maltz D., Herlocker J. L., 
Gordon L.R. and Riedl J.. 1997. GroupLens: Apply-
ing collaborative filtering to Usenet News, Commu-
nication of the ACM, 40(3), p 77-87. 
Lam K. and Ho C.. 1998. Using a generalized instance 
set for automatic text categorization, In 21th Ann. Int. 
ACM SIGIR Conference on Research and Develop-
ment in Information Retrieval, p81-89. 
Lee C.C.. 1990. Fuzzy logic in control systems: fuzzy 
logic controller-part I, IEEE Trans. On Systems, 
Man, and Cybernetics, 20 (2) , p408-418. 
Mitra, M., Singhal, A., and Buckley, C.,. 1998. Improv-
ing Automatic Query Expansion, In Proceedings of 
the 21st Annual International ACM SIGIR Confer-
ence on Research and Development in Information 
Retrieval, p206-214, 1998. 
Nichols D. M.. 1997. Implicit ratings and filteri?, In 
Proceedings of the 5th DELOS Workshop on Filter-
ing and Collaborative Filtering, p10-12. 
Pazzani, M. and  Billsus, D.. 1997. Learning and revis-
ing user profiles: the identification of interesting Web 
site, Machine Learning, 1997. 
Seo, Y. and Zhang, B.. 2001. Personalized Web Docu-
ment Filtering Using Reinforcement Learning, Ap-
plied Artificial Intelligence. 
Soltysiak, S. J. and Crabtree, I. B.. 2000.  Automatic 
Learning of User Profiles?Towards the Personaliza-
tion of Agent Services, BT Technology Journal, 16 
(3), p110?117. 
Ugur ?etintemel, Franklin Michael J. and Lee Giles C.. 
2000 . Self-Adaptive User Profiles for Large-Scale 
Data Delivery, ICDE, p622-633. 
Xu Jinix and Croft W. B.. 1996. Query Expansion Us-
ing Local and Global Document Analysis, In Pro-
ceeding of ACM SIGIR International Conference on 
Research and Development in Information Retrieval, 
p4-11. 
Yan T. W. and Garcia-Molin H.. 1995. SIFT- A tool for 
wide-area information dissemination, In Proceedings 
of the 1995 USENIX Technical Conference, p177-
186. 
Yang, Y. and  Pedersen, J..  1997. A comparative study 
on feature selection in text categorization, In Pro-
ceedings of the 14th International Conference on 
Machine Learning, p412-420. 
 
 IF_EF_RC IR_ER_RC IW_EW_RC
5 0.582 0.509 0.571 
10 0.594 0.505 0.528 
15 0.571 0.502 0.537 
20 0.552 0.491 0.526 
25 0.545 0.487 0.518 
30 0.541 0.497 0.510 
All 0.490 0.478 0.490 
 
IF_EF
_RC 
IF_RC
_EC 
IF_RR
_ER 
IF_RW
_EW 
IF_EF
_RR 
IF_EF
_RW
5 0.582 0.571 0.546 0.580 0.545 0.570
10 0.594 0.520 0.498 0.549 0.551 0.561
15 0.571 0.514 0.491 0.508 0.518 0.517
20 0.552 0.513 0.495 0.533 0.497 0.538
25 0.545 0.509 0.498 0.503 0.491 0.521
30 0.541 0.515 0.506 0.512 0.498 0.511
All 0.490 0.488 0.478 0.494 0.465 0.483
