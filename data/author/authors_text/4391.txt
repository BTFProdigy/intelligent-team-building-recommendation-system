Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora 2006, pages 5?12,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Semi-Automatic Method for  
Annotating a Biomedical Proposition Bank 
 
Wen-Chi Chou1, Richard Tzong-Han Tsai1,2, Ying-Shan Su1, 
Wei Ku1,3, Ting-Yi Sung1 and Wen-Lian Hsu1 
1Institute of Information Science, Academia Sinica, Taiwan, ROC. 
2Dept. of Computer Science and Information Engineering, National Taiwan University, Taiwan, ROC. 
3Institute of Molecular Medicine, National Taiwan University, Taiwan, ROC. 
{jacky957,thtsai,qnn,wilmaku,tsung,hsu}@iis.sinica.edu.tw 
 
  
 
Abstract 
In this paper, we present a semi-
automatic approach for annotating se-
mantic information in biomedical texts. 
The information is used to construct  
a biomedical proposition bank called 
BioProp. Like PropBank in the newswire 
domain, BioProp contains annotations of 
predicate argument structures and seman-
tic roles in a treebank schema. To con-
struct BioProp, a semantic role labeling 
(SRL) system trained on PropBank is 
used to annotate BioProp. Incorrect tag-
ging results are then corrected by human 
annotators. To suit the needs in the bio-
medical domain, we modify the Prop-
Bank annotation guidelines and charac-
terize semantic roles as components of 
biological events. The method can sub-
stantially reduce annotation efforts, and 
we introduce a measure of an upper 
bound for the saving of annotation efforts. 
Thus far, the method has been applied 
experimentally to a 4,389-sentence tree-
bank corpus for the construction of Bio-
Prop. Inter-annotator agreement meas-
ured by kappa statistic reaches .95 for 
combined decision of role identification 
and classification when all argument la-
bels are considered. In addition, we show 
that, when trained on BioProp, our bio-
medical SRL system called BIOSMILE 
achieves an F-score of 87%. 
1 Introduction 
The volume of biomedical literature available on 
the Web has grown enormously in recent years, a 
trend that will probably continue indefinitely. 
Thus, the ability to process literature automati-
cally would be invaluable for both the design and 
interpretation of large-scale experiments. To this 
end, several information extraction (IE) systems 
using natural language processing techniques 
have been developed for use in the biomedical 
field. Currently, the focus of IE is shifting from 
the extraction of nominal information, such as 
named entities (NEs) to verbal information that 
represents the relations between NEs, e.g., events 
and function (Tateisi et al, 2004; Wattarujeekrit 
et al, 2004). In the IE of relations, the roles of 
NEs participating in a relation must be identified 
along with a verb of interest. This task involves 
identifying main roles, such as agents and objects, 
and adjunct roles (ArgM), such as location, man-
ner, timing, condition, and extent. This identifi-
cation task is called semantic role labeling (SRL). 
The corresponding roles of the verb (predicate) 
are called predicate arguments, and the whole 
proposition is known as a predicate argument 
structure (PAS). 
To develop an automatic SRL system for the 
biomedical domain, it is necessary to train the 
system with an annotated corpus, called proposi-
tion bank (Palmer et al, 2005). This corpus con-
tains annotations of semantic PAS?s superim-
posed on the Penn Treebank (PTB) (Marcus et 
al., 1993; Marcus et al, 1994). However, the 
process of manually annotating the PAS?s to 
construct a proposition bank is quite time-
consuming. In addition, due to the complexity of 
proposition bank annotation, inconsistent annota-
tion may occur frequently and further complicate 
5
the annotation task. In spite of the above difficul-
ties, there are proposition banks in the newswire 
domain that are adequate for training SRL sys-
tems (Xue and Palmer, 2004; Palmer et al, 2005). 
In addition, according to the CoNLL-2005 
shared task (Carreras and M?rquez, 2005), the 
performance of SRL systems in general does not 
decline significantly when tagging out-of-domain 
corpora. For example, when SRL systems trained 
on the Wall Street Journal (WSJ) corpus were 
used to tag the Brown corpus, the performance 
only dropped by 15%, on average. In comparison 
to annotating from scratch, annotation efforts 
based on the results of an available SRL system 
are much reduced. Thus, we plan to use a news-
wire SRL system to tag a biomedical corpus and 
then manually revise the tagging results. This 
semi-automatic procedure could expedite the 
construction of a biomedical proposition bank for 
use in training a biomedical SRL system in the 
future. 
2 The Biomedical Proposition Bank - 
BioProp 
As proposition banks are semantically annotated 
versions of a Penn-style treebank, they provide 
consistent semantic role labels across different 
syntactic realizations of the same verb. The an-
notation captures predicate-argument structures 
based on the sense tags of polysemous verbs 
(called framesets) and semantic role labels for 
each argument of the verb. Figure 1 shows the 
annotation of semantic roles, exemplified by the 
following sentence: ?IL4 and IL13 receptors ac-
tivate STAT6, STAT3 and STAT5 proteins in 
normal human B cells.? The chosen predicate is 
the word ?activate?; its arguments and their as-
sociated word groups are illustrated in the figure. 
 
IL4 and IL 13 
 receptors 
activate STAT6, STAT3 
and  
STAT5 proteins 
the human 
B cells 
in 
NP 
Arg0 predicate AM-LOC Arg1 
NP 
NP-SBJ VP 
VP PP 
Figure 1. A treebank annotated with semantic 
role labels 
Since proposition banks are annotated on top 
of a Penn-style treebank, we selected a biomedi-
cal corpus that has a Penn-style treebank as our 
corpus. We chose the GENIA corpus (Kim et al, 
2003), a collection of MEDLINE abstracts se-
lected from the search results with the following 
keywords: human, blood cells, and transcription 
factors. In the GENIA corpus, the abstracts are 
encoded in XML format, where each abstract 
also contains a MEDLINE UID, and the title and 
content of the abstract. The text of the title and 
content is segmented into sentences, in which 
biological terms are annotated with their seman-
tic classes. The GENIA corpus is also annotated 
with part-of-speech (POS) tags (Tateisi and Tsu-
jii, 2004), and co-references are added to part of 
the GENIA corpus by the MedCo project at the 
Institute for Infocomm Research, Singapore 
(Yang et al, 2004). 
The Penn-style treebank for GENIA, created 
by Tateisi et al (2005), currently contains 500 
abstracts. The annotation scheme of the GENIA 
Treebank (GTB), which basically follows the 
Penn Treebank II (PTB) scheme (Bies et al, 
1995), is encoded in XML. However, in contrast 
to the WSJ corpus, GENIA lacks a proposition 
bank. We therefore use its 500 abstracts with 
GTB as our corpus. To develop our biomedical 
proposition bank, BioProp, we add the proposi-
tion bank annotation on top of the GTB annota-
tion. 
In the following, we report on the selection of 
biomedical verbs, and explain the difference be-
tween their meaning in PropBank (Palmer et al, 
2005), developed by the University of Pennsyl-
vania, and their meaning in BioProp (a biomedi-
cal proposition bank). We then introduce Bio-
Prop?s annotation scheme, including how we 
modify a verb?s framesets and how we define 
framesets for biomedical verbs not defined in 
VerbNet (Kipper et al, 2000; Kipper et al, 2002). 
2.1 Selection of Biomedical Verbs 
We selected 30 verbs according to their fre-
quency of use or importance in biomedical texts. 
Since our targets in IE are the relations of NEs, 
only sentences containing protein or gene names 
are used to count each verb?s frequency. Verbs 
that have general usage are filtered out in order 
to ensure the focus is on biomedical verbs. Some 
verbs that do not have a high frequency, but play 
important roles in describing biomedical rela-
tions, such as ?phosphorylate? and ?transacti-
vate?, are also selected. The selected verbs are 
listed in Table 1. 
6
 Predicate Frameset Example 
express  
(VerbNet) 
Arg0: agent  
Arg1: theme 
Arg2: recipient or destina-
tion 
[Some legislatorsArg0][expressedpredicate] [concern that a gas-tax 
increase would take too long and possibly damage chances of a 
major gas-tax-increasing ballot initiative that voters will consider 
next JuneArg1 ]. 
translate 
(VerbNet) 
Arg0: causer of transfor-
mation  
Arg1: thing changing   
Arg2: end state 
Arg3: start state 
But some cosmetics-industry executives wonder whether [tech-
niques honed in packaged goodsArg1] [willAM-MOD] [translatepredicate] 
[to the cosmetics businessArg2]. 
express  
(BioProp) 
Arg0: causer of expression 
Arg1: thing expressing 
[B lymphocytes and macrophagesArg0] [expresspredicate] [closely 
related immunoglobulin G ( IgG ) Fc receptors ( Fc gamma RII ) 
that differ only in the structures of their cytoplasmic domainsArg1]. 
Table 2. Framesets and examples of ?express? and ?translate? 
 
Type Verb list 
1 encode, interact, phosphorylate,  transactivate 
2 express, modulate 
3 bind 
4 
activate, affect, alter, associate, block, 
decrease differentiate, encode, enhance, 
increase, induce, inhibit, mediate, mu-
tate, prevent, promote, reduce, regulate, 
repress, signal, stimulate, suppress, 
transform, trigger 
Table 1. Selected biomedical verbs and their 
types 
2.2 Framesets of Biomedical Verbs 
Annotation of BioProp is mainly based on 
Levin?s verb classes, as defined in the VerbNet 
lexicon (Kipper et al, 2000). In VerbNet, the 
arguments of each verb are represented at the 
semantic level, and thus have associated seman-
tic roles. However, since some verbs may have 
different usages in biomedical and newswire 
texts, it is necessary to customize the framesets 
of biomedical verbs. The 30 verbs in Table 1 are 
categorized into four types according to the de-
gree of difference in usage: (1) verbs that do not 
appear in VerbNet due to their low frequency in 
the newswire domain; (2) verbs that do appear in 
VerbNet, but whose biomedical meanings and 
framesets are undefined; (3) verbs that do appear 
in VerbNet, but whose primary newswire and 
biomedical usage differ; (4) verbs that have the 
same usage in both domains. 
Verbs of the first type play important roles in 
biomedical texts, but rarely appear in newswire 
texts and thus are not defined in VerbNet. For 
example, ?phosphorylate? increasingly appears 
in the fast-growing PubMed abstracts that report 
experimental results on phosphorylated events; 
therefore, it is included in our verb list. However, 
since VerbNet does not define the frameset for 
?phosphorylate?, we must define it after analyz-
ing all the sentences in our corpus that contain 
the verb. Other type 1 verbs may correspond to 
verbs in VerbNet; in such cases, we can borrow 
the VerbNet definitions and framesets. For ex-
ample, ?transactivate? is not found in VerbNet, 
but we can adopt the frameset of ?activate? for 
this verb. 
Verbs of the second type appear in VerbNet, 
but have unique biomedical meanings that are 
undefined. Therefore, the framesets correspond-
ing to their biomedical meanings must be added. 
In most cases, we can adopt framesets from 
VerbNet synonyms. For example, ?express? is 
defined as ?say? and ?send very quickly? in 
VerbNet. However, in the biomedical domain, its 
usage is very similar to ?translate?. Thus, we can 
use the frameset of ?translate? for ?express?. Ta-
ble 2 shows the framesets and corresponding ex-
amples of ?express? in the newswire domain and 
biomedical domain, as well as that of ?translate? 
in VerbNet.  
Verbs of the third type also appear in VerbNet. 
Although the newswire and biological senses are 
defined therein, their primary newswire sense is 
not the same as their primary biomedical sense. 
?Bind,? for example, is common in the newswire 
domain, and it usually means ?to tie? or ?restrain 
with bonds.? However, in the biomedical domain, 
its intransitive use- ?attach or stick to?- is far 
more common. For example, a Google search for 
the phrase ?glue binds to? only returned 21 re-
sults, while the same search replacing ?glue? 
with ?protein? yields 197,000 hits. For such 
verbs, we only need select the appropriate alter-
native meanings and corresponding framesets. 
Lastly, for verbs of the fourth type, we can di-
7
rectly adopt the newswire definitions and frame-
sets, since they are identical.  
2.3 Distribution of Selected Verbs 
There is a significant difference between the oc-
currence of the 30 selected verbs in biomedical 
texts and their occurrence in newswire texts. The 
verbs appearing in verb phrases constitute only 
1,297 PAS?s, i.e., 1% of all PAS?s, in PropBank 
(shown in Figure 2), compared to 2,382 PAS?s, 
i.e., 16% of all PAS?s, in BioProp (shown in 
Figure 3). Furthermore, some biomedical verbs 
have very few PAS?s in PropBank, as shown in 
Table 3. The above observations indicate that it 
is necessary to annotate a biomedical proposition 
bank for training a biomedical SRL system. 
 
Figure 2. The percentage of the 30 biomedical 
verbs and other verbs in PropBank 
 
Figure 3. The percentage of the 30 biomedical 
verbs and other verbs in BioProp 
3  Annotation of BioProp 
3.1 Annotation Process 
After choosing 30 verbs as predicates, we 
adopted a semi-automatic method to annotate 
BioProp. The annotation process consists of the 
following steps: (1) identifying predicate candi-
dates; (2) automatically annotating the biomedi-
cal semantic roles with our WSJ SRL system; (3) 
transforming the automatic tagging results into 
WordFreak (Morton and LaCivita, 2003) format; 
and (4) manually correcting the annotation re-
sults with the WordFreak annotation tool. We 
now describe these steps in detail: 
  
Verbs # in BioProp Ratio(%) 
# in 
PropBank Ratio(%) 
induce 290 1.89 16 0.01 
bind 252 1.64 0 0 
activate 235 1.53 2 0 
express 194 1.26 53 0.03 
inhibit 184 1.20 6 0 
increase 166 1.08 396 0.24 
regulate 122 0.79 23 0.01 
mediate 104 0.68 1 0 
stimulate 93 0.61 11 0.01 
associate 82 0.53 51 0.03 
encode 79 0.51 0 0 
affect 60 0.39 119 0.07 
enhance 60 0.39 28 0.02 
block 58 0.38 71 0.04 
reduce 55 0.36 241 0.14 
decrease 54 0.35 16 0.01 
suppress 38 0.25 4 0 
interact 36 0.23 0 0 
alter 27 0.18 17 0.01 
transactivate 24 0.16 0 0 
modulate 22 0.14 1 0 
phosphorylate 21 0.14 0 0 
transform 21 0.14 22 0.01 
differentiate 21 0.14 2 0 
repress 17 0.11 1 0 
prevent 15 0.10 92 0.05 
promote 14 0.09 52 0.03 
trigger 14 0.09 40 0.02 
mutate 14 0.09 1 0 
signal 10 0.07 31 0.02 
Table 3. The number and percentage of PAS?s 
for each verb in BioProp and PropBank 
1. Each word with a VB POS tag in a verb 
phrase that matches any lexical variant of 
the 30 verbs is treated as a predicate candi-
date. The automatically selected targets are 
then double-checked by human annotators. 
As a result, 2,382 predicates were identified 
in BioProp.  
2. Sentences containing the above 2,382 
predicates were extracted and labeled 
automatically by our WSJ SRL system. In 
total, 7,764 arguments were identified. 
3. In this step, sentences with PAS annota-
tions are transformed into WordFreak for-
mat (an XML format), which allows anno-
tators to view a sentence in a tree-like fash-
ion. In addition, users can customize the tag 
set of arguments. Other linguistic informa-
tion can also be integrated and displayed in 
8
WordFreak, which is a convenient annota-
tion tool. 
4. In the last step, annotators check the pre-
dicted semantic roles using WordFreak and 
then correct or add semantic roles if the 
predicted arguments are incorrect or miss-
ing, respectively. Three biologists with suf-
ficient biological knowledge in our labora-
tory performed the annotation task after re-
ceiving computational linguistic training 
for approximately three months.   
Figure 4 illustrates an example of BioProp an-
notation displayed in WordFreak format, using 
the frameset of ?phophorylate? listed in Table 4.  
This annotation process can be used to con-
struct a domain-specific corpus when a general-
purpose tagging system is available.  In our ex-
perience, this semi-automatic annotation scheme 
saves annotation efforts and improves the anno-
tation consistency. 
 
Predicate Frameset 
phosphorylate  
 
Arg0: causer of phosphorylation 
Arg1: thing being phosphorylated 
Arg2: end state  
Arg3: start state 
Table 4. The frameset of ?phosphorylate? 
3.2 Inter-annotation Agreement 
We conducted preliminary consistency tests on 
2,382 instances of biomedical propositions. The 
inter-annotation agreement was measured by the 
kappa statistic (Siegel and Castellan, 1988), the 
definition of which is based on the probability of 
inter-annotation agreement, denoted by P(A), and 
the agreement expected by chance, denoted by 
P(E). The kappa statistics for inter-annotation 
agreement were .94 for semantic role identifica-
tion and .95 for semantic role classification when 
ArgM labels were included for evaluation. When 
ArgM labels were omitted, kappa statistics 
were .94 and .98 for identification and classifica-
tion, respectively. We also calculated the results 
of combined decisions, i.e., identification and 
classification. (See Table 5.) 
3.3 Annotation Efforts 
Since we employ a WSJ SRL system that labels 
semantic roles automatically, human annotators 
can quickly browse and determine correct tag-
ging results; thus, they do not have to examine  
 
Figure 4. An example of BioProp displayed with 
WordFreak 
 
  P(A) P(E) Kappa 
score 
role identification .97 .52 .94 
role classification .96 .18 .95 including ArgM 
combined decision .96 .18 .95 
role identification .97 .26 .94 
role classification .99 .28 .98 excluding ArgM 
combined decision .99 .28 .98 
Table 5. Inter-annotator agreement 
all tags during the annotation process, as in the 
full manual annotation approach. Only incor-
rectly predicted tags need to be modified, and 
missed tags need to be added. Therefore, annota-
tion efforts can be substantially reduced. To 
quantify the reduction in annotation efforts, we 
define the saving of annotation effort, ?, as: 
)1(
nodes missed of# incorrect  of # correct  of #
nodes  labeled correctly  of #
nodes all of#
nodes  labeled correctly  of #
++
<
=?
 
In Equation (1), since the number of nodes 
that need to be examined is usually unknown, we 
9
use an easy approximation to obtain an upper 
bound for ?. This is based on the extremely op-
timistic assumption that annotators should be 
able to recover a missed or incorrect label by 
only checking one node. However, in reality, this 
would be impossible. In our annotation process, 
the upper bound of ? for BioProp is given by: 
%46
40975
18932
15316668218932
18932
==
++
<? , 
which means that, at most, the annotation effort 
could be reduced by 46%. 
A more accurate tagging system is preferred 
because the more accurate the tagging system, 
the higher the upper bound ? will be.  
4 Disambiguation of Argument Annota-
tion 
During the annotation process, we encountered a 
number of problems resulting from different us-
age of vocabulary and writing styles in general 
English and the biomedical domain. In this sec-
tion, we describe three major problems and pro-
pose our solutions. 
4.1 Cue Words for Role Classification 
PropBank annotation guidelines provide a list of 
words that can help annotators decide an argu-
ment?s type. Similarly, we add some rules to our 
BioProp annotation guideline. For example, ?in 
vivo? and ?in vitro? are used frequently in bio-
medical literature; however, they seldom appear 
in general English articles. According to their 
meanings, we classify them as location argument 
(AM-LOC).  
In addition, some words occur frequently in 
both general English and in biomedical domains 
but have different meanings/usages. For instance, 
?development? is often tagged as Arg0 or Arg1 
in general English, as shown by the following 
sentence: 
 
Despite the strong case for stocks, however, most 
pros warn that [individualsArg0] shouldn't try to 
[profitpredicate] [from short-term developmentsArg1].  
 
However, in the biomedical domain, ?devel-
opment? always means the stage of a disease, 
cell, etc. Therefore, we tag it as temporal argu-
ment (AM-TMP), as shown in the following sen-
tence: 
 
[Rhom-2 mRNAArg1] is [expressedpredicate] [in 
early mouse developmentAM-TMP] [in central 
nervous system, lung, kidney, liver, and spleen 
but only very low levels occur in thymusAM-LOC]. 
4.2 Additional Argument Types 
In PropBank, the negative argument (AM-NEG) 
usually contains explicit negative words such as 
?not?. However, in the biomedical domain, re-
searchers usually express negative meaning im-
plicitly by using ?fail?, ?unable?, ?inability?, 
?neither?, ?nor?, ?failure?, etc. Take ?fail? as an 
example. It is tagged as a verb in general English, 
as shown in the following sentence: 
 
But [the new pactArg1] will force huge debt on the 
new firm and [couldAM-MOD] [stillAM-TMP] [failpredi-
cate] [to thwart rival suitor McCaw CellularArg2]. 
 
Negative results are important in the biomedi-
cal domain. Thus, for annotation purposes, we 
create additional negation tag (AM-NEG1) that 
does not exist in PropBank. The following sen-
tence is an example showing the use of AM-
NEG1: 
  
[TheyArg0] [failAM-NEG1] to [inducepredicate] [mRNA 
of TNF-alphaArg1] [after 3 h of culture AM-TMP]. 
  
In this example, if we do not introduce the 
AM-NEG1, ?fail? is considered as a verb like in 
PropBank, not as a negative argument, and it will 
not be included in the proposition for the predi-
cate ?induce?. Thus, BioProp requires the ?AM-
NEG1? tag to precisely express the correspond-
ing proposition. 
4.3 Essentiality of Biomedical Knowledge 
Since PAS?s contain more semantic information, 
proposition bank annotators require more domain 
knowledge than annotators of other corpora. In 
BioProp, many ambiguous expressions require 
biomedical knowledge to correctly annotate them, 
as exemplified by the following sentence in Bio-
Prop: 
 
In the cell types tested, the LS mutations indi-
cated an apparent requirement not only for the 
intact NF-kappa B and SP1-binding sites but also 
for [several regions between -201 and -130Arg1] 
[notAM-NEG] [previouslyAM-MNR] [associatedpredi-
cate][with viral infectivityArg2]. 
 
Annotators without biomedical knowledge 
may consider [between -201 and -130] as extent 
argument (AM-EXT), because the PropBank 
guidelines define numerical adjuncts as AM-
10
EXT. However, it means a segment of DNA. It is 
an appositive of [several regions]; therefore, it 
should be annotated as part of Arg1 in this case. 
5 Effect of Training Corpora on SRL 
Systems 
To examine the possibility that BioProp can im-
prove the training of SRL systems used for 
automatic tagging of biomedical texts, we com-
pare the performance of systems trained on Bio-
Prop and PropBank in different domains. We 
construct a new SRL system (called a BIOmedi-
cal SeMantIc roLe labEler, BIOSMILE) that is 
trained on BioProp and employs all the features 
used in our WSJ SRL system (Tsai et al, 2006).  
As with POS tagging, chunking, and named 
entity recognition, SRL can also be formulated as 
a sentence tagging problem. A sentence can be 
represented by a sequence of words, a sequence 
of phrases, or a parsing tree; the basic units of a 
sentence in these representations are words, 
phrases, and constituents, respectively. Hacioglu 
et al (2004) showed that tagging phrase-by-
phrase (P-by-P) is better than word-by-word (W-
by-W). However, Punyakanok et al (2004) 
showed that constituent-by-constituent (C-by-C) 
tagging is better than P-by-P. Therefore, we use 
C-by-C tagging for SRL in our BIOSMILE. 
SRL can be divided into two steps. First, we 
identify all the predicates. This can be easily ac-
complished by finding all instances of verbs of 
interest and checking their part-of-speech (POS) 
tags. Second, we label all arguments correspond-
ing to each predicate. This is a difficult problem, 
since the number of arguments and their posi-
tions vary according to a verb?s voice (ac-
tive/passive) and sense, along with many other 
factors.  
In BIOSMILE, we employ the maximum en-
tropy (ME) model for argument classification. 
We use Zhang?s MaxEnt toolkit 
(http://www.nlplab.cn/zhangle/maxent_toolkit.ht
ml) and the L-BFGS (Nocedal and Wright, 1999) 
method of parameter estimation for our ME 
model. Table 6 shows the features we employ in 
BIOSMILE and our WSJ SRL system. 
To compare the effects of using biomedical 
training data versus using general English data, 
we train BIOSMILE on 30 randomly selected 
training sets from BioProp (g1,.., g30), and WSJ 
SRL system on 30 from PropBank (w1,.., w30), 
each of which has 1,200 training PAS?s. 
BASIC FEATURES 
z Predicate ? The predicate lemma 
z Path ? The syntactic path through the parsing tree 
from the parse constituent being classified to the 
predicate 
z Constituent type 
z Position ? Whether the phrase is located before or af-
ter the predicate 
z Voice ? passive: If the predicate has a POS tag VBN, 
and its chunk is not a VP, or it is preceded by a form 
of ?to be? or ?to get? within its chunk; otherwise, it is 
active 
z Head word ? Calculated using the head word table 
described by Collins (1999) 
z Head POS ? The POS of the Head Word 
z Sub-categorization ? The phrase structure rule that 
expands the predicate?s parent node in the parsing 
tree 
z First and last Word and their POS tags 
z Level ? The level in the parsing tree 
PREDICATE FEATURES 
z Predicate?s verb class 
z Predicate POS tag 
z Predicate frequency 
z Predicate?s context POS 
z Number of predicates 
FULL PARSING FEATURES 
z Parent?s, left sibling?s, and right sibling?s paths, 
constituent types, positions, head words and head 
POS tags 
z Head of PP parent ? If the parent is a PP, then the 
head of this PP is also used as a feature 
COMBINATION FEATURES 
z Predicate distance combination 
z Predicate phrase type combination 
z Head word and predicate combination 
z Voice position combination 
OTHERS 
z Syntactic frame of predicate/NP 
z Headword suffixes of lengths 2, 3, and 4 
z Number of words in the phrase 
z Context words & POS tags 
Table 6. The features used in our argument clas-
sification model 
 We then test both systems on 30 400-PAS test 
sets from BioProp, with g1 and w1 being tested on 
test set 1, g2 and w2 on set 2, and so on. Then we 
generate the scores for g1-g30 and w1-w30, and 
compare their averages. 
Table 7 shows the experimental results. When 
tested on the biomedical corpus, BIOSMILE out-
performs the WSJ SRL system by 22.9%. This 
result is statistically significant as expected. 
 
Training Test Precision Recall F-score 
PropBank BioProp 74.78 56.25 64.20 
BioProp BioProp 88.65 85.61 87.10 
Table 7. Performance comparison of SRL sys-
tems trained on BioProp and PropBank 
11
6 Conclusion & Future Work 
The primary contribution of this study is the an-
notation of a biomedical proposition bank that 
incorporates the following features. First, the 
choice of 30 representative biomedical verbs is 
made according to their frequency and impor-
tance in the biomedical domain. Second, since 
some of the verbs have different usages and oth-
ers do not appear in the WSJ proposition bank, 
we redefine their framesets and add some new 
argument types. Third, the annotation guidelines 
in PropBank are slightly modified to suit the 
needs of the biomedical domain. Fourth, using 
appropriate argument types, framesets and anno-
tation guidelines, we construct a biomedical 
proposition bank, BioProp, on top of the popular 
biomedical GENIA Treebank. Finally, we em-
ploy a semi-automatic annotation approach that 
uses an SRL system trained on the WSJ Prop-
Bank. Incorrect tagging results are then corrected 
by human annotators. This approach reduces an-
notation efforts significantly. For example, in 
BioProp, the annotation efforts can be reduced 
by, at most, 46%. In addition, trained on BioProp, 
BIOSMILE?s F-score increases by 22.9% com-
pared to the SRL system trained on the PropBank. 
In our future work, we will investigate more 
biomedical verbs. Besides, since there are few 
biomedical treebanks, we plan to integrate full 
parsers in order to annotate syntactic and seman-
tic information simultaneously. It will then be 
possible to apply the SRL techniques more ex-
tensively to biomedical relation extraction. 
References 
Ann Bies, Mark Ferguson, Karen Katz, and Robert 
MacIntyre. 1995. Bracketing Guidelines for Tree-
bank II Style Penn Treebank Project. Technical re-
port, University of Pennsylvania. 
Xavier Carreras and Llu?s M?rquez. 2005. Introduc-
tion to the CoNLL-2005 Shared Task: Semantic 
Role Labeling. In Proceedings of CoNLL-2005. 
Michael Collins. 1999. Head-driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis, 
University of Pennsylvania. 
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, 
James H. Martin, and Daniel Jurafsky. 2004. Se-
mantic Role Labeling by Tagging Syntactic 
Chunks. In Proceedings of CoNLL-2004. 
Jin-Dong Kim, Tomoko Ohta, Yuka Teteisi, and Jun'-
ichi Tsujii. 2003. GENIA corpus?a semantically 
annotated corpus for bio-textmining. Bioinformat-
ics, 19(Suppl. 1): i180-i182. 
Karin Kipper, Hoa Trang Dang, and Martha Palmer. 
2000. Class-based construction of a verb lexicon. 
In Proceedings of AAAI-2000. 
Karin Kipper, Martha Palmer, and Owen Rambow. 
2002. Extending PropBank with VerbNet semantic 
predicates. In Proceedings of AMTA-2002. 
Mitchell Marcus, Grace Kim, Mary Ann 
Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark 
Ferguson, Karen Katz, and Britta Schasberger. 
1994. The Penn Treebank: Annotating predicate 
argument structure. In Proceedings of ARPA Hu-
man Language Technology Workshop. 
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann 
Marcinkiewicz. 1993. Building a large annotated 
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2): 313-330. 
Thomas Morton and Jeremy LaCivita. 2003. Word-
Freak: an open tool for linguistic annotation. In 
Proceedings of HLT/NAACL-2003. 
Jorge Nocedal and Stephen J Wright. 1999. Numeri-
cal Optimization, Springer. 
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 
2005. The Proposition Bank: An Annotated Corpus 
of Semantic Roles. Computational Linguistics, 
31(1). 
Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav 
Zimak. 2004. Semantic Role Labeling via Integer 
Linear Programming Inference. In Proceedings of 
COLING-2004. 
Sidney Siegel and N. John Castellan. 1988. Non-
parametric Statistics for the Behavioral Sciences. 
New York, McGraw-Hill. 
Richard Tzong-Han Tsai, Wen-Chi Chou, Yu-Chun 
Lin, Cheng-Lung Sung, Wei Ku, Ying-Shan Su, 
Ting-Yi Sung, and Wen-Lian Hsu. 2006. BIOS-
MILE: Adapting Semantic Role Labeling for Bio-
medical Verbs: An Exponential Model Coupled 
with Automatically Generated Template Features. 
In Proceedings of BioNLP'06. 
Yuka Tateisi, Tomoko Ohta, and Jun-ichi Tsujii. 2004. 
Annotation of Predicate-argument Structure of Mo-
lecular Biology Text. In Proceedings of the 
IJCNLP-04 workshop on Beyond Shallow Analyses. 
Yuka Tateisi and Jun-ichi Tsujii. 2004. Part-of-
Speech Annotation of Biology Research Abstracts. 
In Proceedings of the 4th International Conference 
on Language Resource and Evaluation. 
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and 
Jun-ichi Tsujii. 2005. Syntax Annotation for the 
GENIA corpus. In Proceedings of IJCNLP-2005. 
Tuangthong Wattarujeekrit, Parantu K Shah, and 
Nigel Collier1. 2004. PASBio: predicate-argument 
structures for event extraction in molecular biology. 
BMC Bioinformatics, 5(155). 
Nianwen Xue and Martha Palmer. 2004. Calibrating 
Features for Semantic Role Labeling. In Proceed-
ings of the EMNLP-2004. 
Xiaofeng Yang, Guodong Zhou, Jian Su, and Chew 
Lim Tan. 2004. Improving Noun Phrase Corefer-
ence Resolution by Matching Strings. In Proceed-
ings of 1st International Joint Conference on Natu-
ral Language Processing: 226-233. 
  
12
Proceedings of the BioNLP Workshop on Linking Natural Language Processing and Biology at HLT-NAACL 06, pages 57?64,
New York City, June 2006. c?2006 Association for Computational Linguistics
BIOSMILE: Adapting Semantic Role Labeling for Biomedical Verbs: 
An Exponential Model Coupled with 
Automatically Generated Template Features 
 
 
Richard Tzong-Han Tsai1,2, Wen-Chi Chou1, Yu-Chun Lin1,2, Cheng-Lung Sung1,  
Wei Ku1,3, Ying-Shan Su1,4, Ting-Yi Sung1 and Wen-Lian Hsu1 
1Institute of Information Science, Academia Sinica 
2Dept. of Computer Science and Information Engineering, National Taiwan University 
3Institute of Molecular Medicine, National Taiwan University  
4Dept. of Biochemical Science and Technology, National Taiwan University 
{thtsai,jacky957,sbb,clsung,wilmaku,qnn,tsung,hsu}@iis.sinica.edu.tw 
 
 
 
 
Abstract 
In this paper, we construct a biomedical 
semantic role labeling (SRL) system that 
can be used to facilitate relation extraction. 
First, we construct a proposition bank on 
top of the popular biomedical GENIA 
treebank following the PropBank annota-
tion scheme. We only annotate the predi-
cate-argument structures (PAS?s) of thirty 
frequently used biomedical predicates and 
their corresponding arguments. Second, 
we use our proposition bank to train a 
biomedical SRL system, which uses a 
maximum entropy (ME) model. Thirdly, 
we automatically generate argument-type 
templates which can be used to improve 
classification of biomedical argument 
types. Our experimental results show that 
a newswire SRL system that achieves an 
F-score of 86.29% in the newswire do-
main can maintain an F-score of 64.64% 
when ported to the biomedical domain. 
By using our annotated biomedical corpus, 
we can increase that F-score by 22.9%. 
Adding automatically generated template 
features further increases overall F-score 
by 0.47% and adjunct arguments (AM) F-
score by 1.57%, respectively. 
1 Introduction 
The volume of biomedical literature available has 
experienced unprecedented growth in recent years. 
The ability to automatically process this literature 
would be an invaluable tool for both the design and 
interpretation of large-scale experiments. To this 
end, more and more information extraction (IE) 
systems using natural language processing (NLP) 
have been developed for use in the biomedical 
field. A key IE task in the biomedical field is ex-
traction of relations, such as protein-protein and 
gene-gene interactions. 
Currently, most biomedical relation-extraction 
systems fall under one of the following three ap-
proaches: cooccurence-based (Leroy et al, 2005), 
pattern-based (Huang et al, 2004), and machine-
learning-based. All three, however, share the same 
limitation when extracting relations from complex 
natural language. They only extract the relation 
targets (e.g., proteins, genes) and the verbs repre-
senting those relations, overlooking the many ad-
verbial and prepositional phrases and words that 
describe location, manner, timing, condition, and 
extent. The information in such phrases may be 
important for precise definition and clarification of 
complex biological relations. 
The above problem can be tackled by using se-
mantic role labeling (SRL) because it not only rec-
ognizes main roles, such as agents and objects, but 
also extracts adjunct roles such as location, manner, 
57
timing, condition, and extent. The goal of SRL is 
to group sequences of words together and classify 
them with semantic labels. In the newswire domain, 
Morarescu et al (2005) have demonstrated that 
full-parsing and SRL can improve the performance 
of relation extraction, resulting in an F-score in-
crease of 15% (from 67% to 82%). This significant 
result leads us to surmise that SRL may also have 
potential for relation extraction in the biomedical 
domain. Unfortunately, no SRL system for the 
biomedical domain exists. 
In this paper, we aim to build such a biomedical 
SRL system. To achieve this goal we roughly im-
plement the following three steps as proposed by 
Wattarujeekrit et al, (2004): (1) create semantic 
roles for each biomedical verb; (2) construct a 
biomedical corpus annotated with verbs and their 
corresponding semantic roles (following defini-
tions created in (1) as a reference resource;) (3) 
build an automatic semantic interpretation model 
using the annotated text as a training corpus for 
machine learning. In the first step, we adopt the 
definitions found in PropBank (Palmer et al, 2005), 
defining our own framesets for verbs not in Prop-
Bank, such as ?phosphorylate?. In the second step, 
we first use an SRL system (Tsai et al, 2005) 
trained on the Wall Street Journal (WSJ) to auto-
matically tag our corpus. We then have the results 
double-checked by human annotators. Finally, we 
add automatically-generated template features to 
our SRL system to identify adjunct (modifier) ar-
guments, especially those highly relevant to the 
biomedical domain. 
2 Biomedical Proposition Bank  
As proposition banks are semantically annotated 
versions of a Penn-style treebank, they provide 
consistent semantic role labels across different syn-
tactic realizations of the same verb (Palmer et al, 
2005). The annotation captures predicate-argument 
structures based on the sense tags of polysemous 
verbs (called framesets) and semantic role labels 
for each argument of the verb. Figure 1 shows the 
annotation of semantic roles, exemplified by the 
following sentence: ?IL4 and IL13 receptors acti-
vate STAT6, STAT3 and STAT5 proteins in the 
human B cells.? The chosen predicate is the word 
?activate?; its arguments and their associated word 
groups are illustrated in the figure. 
 
 
Figure 1. A Treebank Annotated with Semantic 
Role Labels 
Since proposition banks are annotated on top of 
a Penn-style treebank, we selected a biomedical 
corpus that has a Penn-style treebank as our corpus. 
We chose the GENIA corpus (Kim et al, 2003), a 
collection of MEDLINE abstracts selected from 
the search results with the following keywords: 
human, blood cells, and transcription factors. In the 
GENIA corpus, the abstracts are encoded in XML 
format, where each abstract also contains a 
MEDLINE UID, and the title and content of the 
abstract. The text of the title and content is seg-
mented into sentences, in which biological terms 
are annotated with their semantic classes. The 
GENIA corpus is also annotated with part-of-
speech (POS) tags (Tateisi et al, 2004), and co-
references (Yang et al, 2004). 
The Penn-style treebank for GENIA, created by 
Tateisi et al (2005), currently contains 500 ab-
stracts. The annotation scheme of the GENIA 
Treebank (GTB), which basically follows the Penn 
Treebank II (PTB) scheme (Bies et al, 1995), is 
encoded in XML. However, in contrast to the WSJ 
corpus, GENIA lacks a proposition bank. We 
therefore use its 500 abstracts with GTB as our 
corpus. To develop our biomedical proposition 
bank, BioProp, we add the proposition bank anno-
tation on top of the GTB annotation. 
2.1 Important Argument Types 
In the biomedical domain, relations are often de-
pendent upon locative and temporal factors 
(Kholodenko, 2006). Therefore, locative (AM-
LOC) and temporal modifiers (AM-TMP) are par-
ticularly important as they tell us where and when 
biomedical events take place. Additionally, nega-
58
tive modifiers (AM-NEG) are also vital to cor-
rectly extracting relations. Without AM-NEG, we 
may interpret a negative relation as a positive one 
or vice versa. In total, we use thirteen modifiers in 
our biomedical proposition bank. 
2.2 Verb Selection 
We select 30 frequently used verbs from the mo-
lecular biology domain given in Table 1. 
express trigger encode 
associate repress enhance 
interact signal increase 
suppress activate induce 
prevent alter Inhibit 
modulate affect Mediate 
phosphorylate bind Mutated 
transactivate block Reduce 
transform decrease Regulate 
differentiated promote Stimulate 
Table 1. 30 Frequently Biomedical Verbs 
Let us examine a representative verb, ?activate?. 
Its most frequent usage in molecular biology is the 
same as that in newswire. Generally speaking, ?ac-
tivate? means, ?to start a process? or ?to turn on.? 
Many instances of this verb express the action of 
waking genes, proteins, or cells up. The following 
sentence shows a typical usage of the verb ?acti-
vate.?  
[NF-kappaB
 Arg1
] is [not
 AM-NEG
] [activated
predicate
] [upon tetra-
cycline removal
AM-TMP
] [in the NIH3T3 cell line
AM-LOC
]. 
3 Semantic Role Labeling on BioProp 
In this section, we introduce our BIOmedical Se-
MantIc roLe labEler, BIOSMILE. Like POS tag-
ging, chunking, and named entity recognition, SRL 
can be formulated as a sentence tagging problem. 
A sentence can be represented by a sequence of 
words, a sequence of phrases, or a parsing tree; the 
basic units of a sentence are words, phrases, and 
constituents arranged in the above representations, 
respectively. Hacioglu et al (2004) showed that 
tagging phrase by phrase (P-by-P) is better than 
word by word (W-by-W). Punyakanok et al, (2004) 
further showed that constituent-by-constituent (C-
by-C) tagging is better than P-by-P. Therefore, we 
choose C-by-C tagging for SRL. The gold standard 
SRL corpus, PropBank, was designed as an addi-
tional layer of annotation on top of the syntactic 
structures of the Penn Treebank. 
SRL can be broken into two steps. First, we 
must identify all the predicates. This can be easily 
accomplished by finding all instances of verbs of 
interest and checking their POS?s. 
Second, for each predicate, we need to label all 
arguments corresponding to the predicate. It is a 
complicated problem since the number of argu-
ments and their positions vary depending on a 
verb?s voice (active/passive) and sense, along with 
many other factors.  
In this section, we first describe the maximum 
entropy model used for argument classification. 
Then, we illustrate basic features as well as spe-
cialized features such as biomedical named entities 
and argument templates.  
3.1 Maximum Entropy Model 
The maximum entropy model (ME) is a flexible 
statistical model that assigns an outcome for each 
instance based on the instance?s history, which is 
all the conditioning data that enables one to assign 
probabilities to the space of all outcomes. In SRL, 
a history can be viewed as all the information re-
lated to the current token that is derivable from the 
training corpus. ME computes the probability, 
p(o|h), for any o from the space of all possible out-
comes, O, and for every h from the space of all 
possible histories, H. 
The computation of p(o|h) in ME depends on a 
set of binary features, which are helpful in making 
predictions about the outcome. For instance, the 
node in question ends in ?cell?, it is likely to be 
AM-LOC. Formally, we can represent this feature 
as follows: 
??
??
?
=
=
=
otherwise :0
LOC-AMo and    
 true)(s_in_cellde_endcurrent_no if :1
),(
h
ohf
Here, current_node_ends_in_cell(h) is a binary 
function that returns a true value if the current 
node in the history, h, ends in ?cell?. Given a set of 
features and a training corpus, the ME estimation 
process produces a model in which every feature f i 
has a weight ?i. Following Bies et al (1995), we 
can compute the conditional probability as: 
?=
i
ohf
i
i
hZ
hop ),(
)(
1
)|( ?  
??=
o i
ohf
i
ihZ ),()( ?  
59
The probability is calculated by multiplying the 
weights of the active features (i.e., those of f i (h,o) 
= 1).  ?i is estimated by a procedure called Gener-
alized Iterative Scaling (GIS) (Darroch et al, 
1972). The ME estimation technique guarantees 
that, for every feature, f i, the expected value of ?i 
equals the empirical expectation of ?i in the train-
ing corpus. We use Zhang?s MaxEnt toolkit and 
the L-BFGS (Nocedal et al, 1999) method of pa-
rameter estimation for our ME model. 
BASIC FEATURES 
z Predicate ? The predicate lemma 
z Path ? The syntactic path through the parsing tree from 
the parse constituent be-ing classified to the predicate 
z Constituent type 
z Position ? Whether the phrase is located before or after 
the predicate 
z Voice ? passive: if the predicate has a POS tag VBN, 
and its chunk is not a VP, or it is preceded by a form of 
?to be? or ?to get? within its chunk; otherwise, it is ac-
tive 
z Head word ? calculated using the head word table de-
scribed by (Collins, 1999) 
z Head POS ? The POS of the Head Word 
z Sub-categorization ? The phrase structure rule that ex-
pands the predicate?s parent node in the parsing tree 
z First and last Word and their POS tags 
z Level ? The level in the parsing tree 
PREDICATE FEATURES 
z Predicate?s verb class 
z Predicate POS tag 
z Predicate frequency 
z Predicate?s context POS 
z Number of predicates 
FULL PARSING FEATURES 
z Parent?s, left sibling?s, and right sibling?s paths, con-
stituent types, positions, head words and head POS 
tags 
z Head of PP parent ? If the parent is a PP, then the head 
of this PP is also used as a feature 
COMBINATION FEATURES 
z Predicate distance combination 
z Predicate phrase type combination 
z Head word and predicate combination 
z Voice position combination 
OTHERS 
z Syntactic frame of predicate/NP 
z Headword suffixes of lengths 2, 3, and 4 
z Number of words in the phrase 
z Context words & POS tags 
Table 2. The Features Used in the Baseline Argu-
ment Classification Model 
3.2 Basic Features 
Table 2 shows the features that are used in our 
baseline argument classification model. Their ef-
fectiveness has been previously shown by (Pradhan 
et al, 2004; Surdeanu et al, 2003; Xue et al, 
2004). Detailed descriptions of these features can 
be found in (Tsai et al, 2005). 
3.3 Named Entity Features 
In the newswire domain, Surdeanu et al (2003) 
used named entity (NE) features that indicate 
whether a constituent contains NEs, such as per-
sonal names, organization names, location names, 
time expressions, and quantities of money. Using 
these NE features, they increased their system?s F-
score by 2.12%. However, because NEs in the 
biomedical domain are quite different from news-
wire NEs, we create bio-specific NE features using 
the five primary NE categories found in the 
GENIA ontology1: protein, nucleotide, other or-
ganic compounds, source and others. Table 3 illus-
trates the definitions of these five categories. When 
a constituent exactly matches an NE, the corre-
sponding NE feature is enabled.  
 NE Definition 
Protein Proteins include protein groups, families, molecules, complexes, and substructures.  
Nucleotide A nucleic acid molecule or the compounds that consist of nucleic acids. 
Other organic 
compounds 
Organic compounds exclude protein and 
nucleotide. 
Source 
Sources are biological locations where 
substances are found and their reactions 
take place.  
Others 
The terms that are not categorized as 
sources or substances may be marked up, 
with 
Table 3. Five GENIA Ontology NE Categories 
3.4 Biomedical Template Features 
Although a few NEs tend to belong almost exclu-
sively to certain argument types (such as ??cell? 
being mainly AM-LOC), this information alone is 
not sufficient for argument-type classification. For 
one, most NEs appear in a variety of argument 
types. For another, many appear in more than one 
constituent (node in a parsing tree) in the same 
sentence. Take the sentence ?IL4 and IL13 recep-
tors activate STAT6, STAT3 and STAT5 proteins 
in the human B cells,? for example. The NE ?the 
human B cells? is found in two constituents (?the 
                                                          
1 http://www-tsujii.is.s.u-tokyo.ac.jp/~genia/topics/Corpus/ 
genia-ontology.html  
60
human B cells? and ?in the human B cells?) as 
shown in figure 1. Yet only ?in the human B cells? 
is an AM-LOC because here ?human B cells? is 
preceded by the preposition ?in? and the deter-
miner ?the?. Another way to express this would be 
as a template?<prep> the <cell>.? We believe 
such templates composed of NEs, real words, and 
POS tags may be helpful in identifying constitu-
ents? argument types. In this section, we first de-
scribe our template generation algorithm, and then 
explain how we use the generated templates to im-
prove SRL performance. 
Template Generation (TG) 
Our template generation (TG) algorithm extracts 
general patterns for all argument types using the 
local alignment algorithm. We begin by pairing all 
arguments belonging to the same type according to 
their similarity. Closely matching pairs are then 
aligned word by word and a template that fits both 
is created. Each slot in the template is given con-
straint information in the form of either a word, NE 
type, or POS. The hierarchy of this constraint in-
formation is word > NE type > POS. If the argu-
ments share nothing in common for a given slot, 
the TG algorithm will put a wildcard in that posi-
tion. Figure 2 shows an aligned pair arguments. 
For this pair, the TG algorithm generated the tem-
plate ?AP-1 CC PTN? (PTN: protein name) be-
cause in the first position, both arguments have 
?AP-1;? in the second position, they have the same 
POS ?CC;? and in the third position, they share a 
common NE type, ?PTN.? The complete TG algo-
rithm is described in Algorithm 1. 
AP-1/PTN/NN and/O/CC NF-AT/PTN/NN 
AP-1/PTN/NN or/O/CC NFIL-2A/PTN/NN 
Figure 2. Aligned Argument Pair 
Applying Generated Templates 
The generated templates may match exactly or par-
tially with constituents. According to our observa-
tions, the former is more useful for argument 
classification. For example, constituents that per-
fectly match the template ?IN a * <cell>? are 
overwhelmingly AM-LOCs. Therefore, we only 
accept exact template matches. That is, if a con-
stituent exactly matches a template t, then the fea-
ture corresponding to t will be enabled. 
Algorithm 1 Template Generation 
Input: Sentences set S = {s1, . . . , sn}, 
Output: A set of template T = {t1, . . . , tk}. 
 
1: T = {}; 
2: for each sentence si from s1 to sn-1 do 
3:    for each sentence sj from si to sn do 
4:        perform alignment on si and sj, then 
5:          pair arguments according to similarity; 
6:        generate common template t from argument pairs; 
7:        T?T?t; 
8:    end; 
9: end; 
10: return T; 
4 Experiments 
4.1 Datasets 
In this paper, we extracted all our datasets from 
two corpora, the Wall Street Journal (WSJ) corpus 
and the BioProp, which respectively represent the 
newswire and biomedical domains. The Wall 
Street Journal corpus has 39,892 sentences, and 
950,028 words. It contains full-parsing information, 
first annotated by Marcus et al (1997), and is the 
most famous treebank (WSJ treebank). In addition 
to these syntactic structures, it was also annotated 
with predicate-argument structures (WSJ proposi-
tion bank) by Palmer et al (2005).  
In biomedical domain, there is one available 
treebank for GENIA, created by Yuka Tateshi et al 
(2005), who has so far added full-parsing informa-
tion to 500 abstracts. In contrast to WSJ, however, 
GENIA lacks any proposition bank. 
Since predicate-argument annotation is essential 
for training and evaluating statistical SRL systems, 
to make up for GENIA?s lack of a proposition 
bank, we constructed BioProp. Two biologists with 
masters degrees in our laboratory undertook the 
annotation task after receiving computational lin-
guistic training for approximately three months.  
We adopted a semi-automatic strategy to anno-
tate BioProp. First, we used the PropBank to train 
a statistical SRL system which achieves an F-score 
of over 86% on section 24 of the PropBank. Next, 
we used this SRL system to annotate the GENIA 
treebank automatically. Table 4 shows the amounts 
of all adjunct argument types (AMs) in BioProp. 
The detail description of can be found in (Babko-
Malaya, 2005).  
 
61
Type Description # Type Description # 
NEG negation 
marker 
103 ADV general  
purpose 
307
LOC location 389 PNC purpose 3
TMP time 145 CAU cause 15
MNR manner 489 DIR direction 22
EXT extent 23 DIS discourse 
connectives 
179
   MOD modal verb 121
Table 4. Subtypes of the AM Modifier Tag 
4.2 Experiment Design 
Experiment 1: Portability 
Ideally, an SRL system should be adaptable to the 
task of information extraction in various domains 
with minimal effort. That is, we should be able to 
port it from one domain to another. In this experi-
ment, we evaluate the cross-domain portability of 
our SRL system. We use Sections 2 to 21 of the 
PropBank to train our SRL system. Then, we use 
our system to annotate Section 24 of the PropBank 
(denoted by Exp 1a) and all of BioProp (denoted 
by Exp 1b). 
Experiment 2: The Necessity of BioProp 
To compare the effects of using biomedical train-
ing data vs. using newswire data, we train our SRL 
system on 30 randomly selected training sets from 
BioProp (g1,.., g30) and 30 from PropBank (w1,.., 
w30), each having 1200 training PAS?s. We then 
test our system on 30 400-PAS test sets from Bio-
Prop, with g1 and w1 being tested on test set 1, g2 
and w2 on set 2, and so on. Then we add up the 
scores for w1-w30 and g1-g30, and compare their 
averages. 
Experiment 3: The Effect of Using Biomedical-
Specific Features 
In order to improve SRL performance, we add do-
main specific features. In Experiment 3, we inves-
tigate the effects of adding biomedical NE features 
and argument template features composed of 
words, NEs, and POSs. The dataset selection pro-
cedure is the same as in Experiment 2. 
5 Results and Discussion 
All experimental results are summarized in Table 5. 
For argument classification, we report the preci-
sion (P), recall (R) and F-scores (F). The details 
are illustrated in the following paragraphs. 
Configuration Training Test P R F 
Exp 1a PropBank PropBank 90.47 82.48 86.29
Exp 1b PropBank BioProp 75.28 56.64 64.64
Exp 2a PropBank BioProp 74.78 56.25 64.20
Exp 2b BioProp BioProp 88.65 85.61 87.10
Exp 3a BioProp BioProp 88.67 85.59 87.11
Exp 3b BioProp BioProp 89.13 86.07 87.57
Table 5. Summary of All Experiments 
Exp 1a Exp 1b Role 
P R F P R F 
+/-(%)
Overall 90.47 82.48 86.29 75.28 56.64 64.64 -21.65
ArgX 91.46 86.39 88.85 78.92 67.82 72.95 -15.90
Arg0 86.36 78.01 81.97 85.56 64.41 73.49   -8.48
Arg1 95.52 92.11 93.78 82.56 75.75 79.01 -14.77
Arg2 87.19 84.53 85.84 32.76 31.59 32.16 -53.68
AM 86.76 70.02 77.50 62.70 32.98 43.22 -34.28
-ADV 73.44 52.32 61.11 39.27 26.34 31.53 -29.58
-DIS 81.71 48.18 60.62 67.12 48.18 56.09 -4.53
-LOC 89.19 57.02 69.57 68.54 2.67 5.14 -64.43
-MNR 67.93 57.86 62.49 46.55 22.97 30.76 -31.73
-MOD 99.42 92.5 95.84 99.05 88.01 93.2 -2.64
-NEG 100 91.21 95.40 99.61 80.13 88.81 -6.59
-TMP 88.15 72.83 79.76 70.97 60.36 65.24 -14.52
Table 6. Performance of Exp 1a and Exp 1b 
Experiment 1 
Table 6 shows the results of Experiment 1. The 
SRL system trained on the WSJ corpus obtains an 
F-score of 64.64% when used in the biomedical 
domain. Compared to traditional rule-based or 
template-based approaches, our approach suffers 
acceptable decrease in overall performance when 
recognizing ArgX arguments. However, Table 6 
also shows significant decreases in F-scores from 
other argument types. AM-LOC drops 64.43% and 
AM-MNR falls 31.73%. This may be due to the 
fact that the head words in PropBank are quite dif-
ferent from those in BioProp. Therefore, to achieve 
better performance, we believe it will be necessary 
to annotate biomedical corpora for training bio-
medical SRL systems. 
Experiment 2 
Table 7 shows the results of Experiment 2. When 
tested on BioProp, BIOSMILE (Exp 2b) outper-
forms the newswire SRL system (Exp 2a) by 
22.9% since the two systems are trained on differ-
ent domains. This result is statistically significant. 
Furthermore, Table 7 shows that BIOSMILE 
outperforms the newswire SRL system in most 
62
argument types, especially Arg0, Arg2, AM-ADV, 
AM-LOC, AM-MNR.  
Exp 2a Exp 2b Role 
P R F P R F 
+/-(%)
Overall 74.78 56.25 64.20 88.65 85.61 87.10 22.90
ArgX 78.40 67.32 72.44 91.96 89.73 90.83 18.39
Arg0 85.55 64.40 73.48 92.24 90.59 91.41 17.93
Arg1 81.41 75.11 78.13 92.54 90.49 91.50 13.37
Arg2 34.42 31.56 32.93 86.89 81.35 84.03 51.10
AM 61.96 32.38 42.53 81.27 76.72 78.93 36.40
-ADV 36.00 23.26 28.26 64.02 52.12 57.46 29.20
-DIS 69.55 51.29 59.04 82.71 75.60 79.00 19.96
-LOC 75.51 3.23 6.20 80.05 85.00 82.45 76.25
-MNR 44.67 21.66 29.17 83.44 82.23 82.83 53.66
-MOD 99.38 88.89 93.84 98.00 95.28 96.62 2.78
-NEG 99.80 79.55 88.53 97.82 94.81 96.29 7.76
-TMP 67.95 60.40 63.95 80.96 61.82 70.11 6.16
Table 7. Performance of Exp 2a and Exp 2b 
The performance of Arg0 and Arg2 in our sys-
tem increases considerably because biomedical 
verbs can be successfully identified by BIOSMILE 
but not by the newswire SRL system. For AM-
LOC, the newswire SRL system scored as low as 
76.25% lower than BIOSMILE. This is likely due 
to the reason that in the biomedical domain, many 
biomedical nouns, e.g., organisms and cells, func-
tion as locations, while in the newswire domain, 
they do not. In newswire, the word ?cell? seldom 
appears. However, in biomedical texts, cells repre-
sent the location of many biological reactions, and, 
therefore, if a constituent node on a parsing tree 
contains ?cell?, this node is very likely an AM-
LOC. If we use only newswire texts, the SRL sys-
tem will not learn to recognize this pattern. In the 
biomedical domain, arguments of manner (AM-
MNR) usually describe how to conduct an experi-
ment or how an interaction arises or occurs, while 
in newswire they are extremely broad in scope. 
Without adequate biomedical domain training cor-
pora, systems will easily confuse adverbs of man-
ner (AM-MNR), which are differentiated from 
general adverbials in semantic role labeling, with 
general adverbials (AM-ADV). In addition, the 
performance of the referential arguments of Arg0, 
Arg1, and Arg2 increases significantly. 
Experiment 3 
Table 8 shows the results of Experiment 3. The 
performance does not significantly improve after 
adding NE features. We originally expected that 
NE features would improve recognition of AM 
arguments such as AM-LOC. However, they failed 
to ameliorate the results since in the biomedical 
domain most NEs are just matched parts of a con-
stituent. This results in fewer exact matches. Fur-
thermore, in matched cases, NE information alone 
is insufficient to distinguish argument types. For 
example, even if a constituent exactly matches a 
protein name, we still cannot be sure whether it 
belongs to the subject (Arg0) or object (Arg1). 
Therefore, NE features were not as effective as we 
had expected. 
NE (Exp 3a) Template (Exp 3b) Role 
P R F P R F 
+/-(%)
Overall 88.67 85.59 87.11 89.13 86.07 87.57 0.46
ArgX 91.99 89.70 90.83 91.89 89.73 90.80 -0.03
Arg0 92.41 90.57 91.48 92.19 90.59 91.38 -0.1
Arg1 92.47 90.45 91.45 92.42 90.44 91.42 -0.03
Arg2 86.93 81.3 84.02 87.08 81.66 84.28 0.26
AM 81.30 76.75 78.96 82.96 78.18 80.50 1.54
-ADV 64.11 52.23 57.56 65.66 55.60 60.21 2.65
-DIS 82.51 75.42 78.81 83.00 75.79 79.23 0.42
-LOC 80.07 85.09 82.50 84.24 85.48 84.86 2.36
-MNR 83.50 82.19 82.84 84.56 84.14 84.35 1.51
-MOD 98.14 95.28 96.69 98.00 95.28 96.62 -0.07
-NEG 97.66 94.81 96.21 97.82 94.81 96.29 0.08
-TMP 81.14 62.06 70.33 83.10 63.95 72.28 1.95
Table 8. Performance of Exp 3a and Exp 3b 
6 Conclusions and Future Work 
In Experiment 3b, we used the argument templates 
as features. Since ArgX?s F-score is close to 90%, 
adding the template features does not improve its 
score. However, AM?s F-score increases by 1.54%. 
For AM-ADV, AM-LOC, and AM-TMP, the in-
crease is greater because the automatically gener-
ated templates effectively extract these AMs.  
In Figure 3, we compare the performance of ar-
gument classification models with and without ar-
gument template features. The overall F-score 
improves only slightly. However, the F-scores of 
main adjunct arguments increase significantly. 
The contribution of this paper is threefold. First, 
we construct a biomedical proposition bank, Bio-
Prop, on top of the popular biomedical GENIA 
treebank following the PropBank annotation 
scheme. We employ semi-automatic annotation 
using an SRL system trained on PropBank, thereby 
significantly reducing annotation effort. Second, 
we create BIOSMILE, a biomedical SRL system, 
which uses BioProp as its training corpus. Thirdly, 
we develop a method to automatically generate 
templates that can boost overall performance, es-
63
pecially on location, manner, adverb, and temporal 
arguments. In the future, we will expand BioProp 
to include more verbs and will also integrate an 
automatic parser into BIOSMILE. 
 
Figure 3. Improvement of Template Features 
Overall and on Several Adjunct Types 
Acknowledgement 
We would like to thank Dr. Nianwen Xue for his 
instruction of using the WordFreak annotation tool. 
This research was supported in part by the National 
Science Council under grant NSC94-2752-E-001-
001 and the thematic program of Academia Sinica 
under grant AS94B003. Editing services were pro-
vided by Dorion Berg. 
References  
Babko-Malaya, O. (2005). Propbank Annotation 
Guidelines. 
Bies, A., Ferguson, M., Katz, K., MacIntyre, R., 
Tredinnick, V., Kim, G., et al (1995). Bracketing 
Guidelines for Treebank II Style Penn Treebank 
Project  
Collins, M. J. (1999). Head-driven Statistical Models 
for Natural Language Parsing. Unpublished Ph.D. 
thesis, University of Pennsylvania. 
Darroch, J. N., & Ratcliff, D. (1972). Generalized 
Iterative Scaling for Log-Linear Models. The Annals 
of Mathematical Statistics. 
Hacioglu, K., Pradhan, S., Ward, W., Martin, J. H., & 
Jurafsky, D. (2004). Semantic Role Labeling by 
Tagging Syntactic Chunks. Paper presented at the 
CONLL-04. 
Huang, M., Zhu, X., Hao, Y., Payan, D. G., Qu, K., & 
Li, M. (2004). Discovering patterns to extract 
protein-protein interactions from full texts. 
Bioinformatics, 20(18), 3604-3612. 
Kholodenko, B. N. (2006). Cell-signalling dynamics in 
time and space. Nat Rev Mol Cell Biol, 7(3), 165-176. 
Kim, J. D., Ohta, T., Tateisi, Y., & Tsujii, J. (2003). 
GENIA corpus--semantically annotated corpus for 
bio-textmining. Bioinformatics, 19 Suppl 1, i180-182. 
Leroy, G., Chen, H., & Genescene. (2005). An 
ontology-enhanced integration of linguistic and co-
occurrence based relations in biomedical texts. 
Journal of the American Society for Information 
Science and Technology, 56(5), 457-468. 
Marcus, M. P., Santorini, B., & Marcinkiewicz, M. A. 
(1997). Building a large annotated corpus of English: 
the Penn Treebank. Computational Linguistics, 19. 
Morarescu, P., Bejan, C., & Harabagiu, S. (2005). 
Shallow Semantics for Relation Extraction. Paper 
presented at the IJCAI-05. 
Nocedal, J., & Wright, S. J. (1999). Numerical 
Optimization: Springer. 
Palmer, M., Gildea, D., & Kingsbury, P. (2005). The 
proposition bank: an annotated corpus of semantic 
roles. Computational Linguistics, 31(1). 
Pradhan, S., Hacioglu, K., Kruglery, V., Ward, W., 
Martin, J. H., & Jurafsky, D. (2004). Support vector 
learning for semantic argument classification. 
Journal of Machine Learning  
Punyakanok, V., Roth, D., Yih, W., & Zimak, D. (2004). 
Semantic Role Labeling via Integer Linear 
Programming Inference. Paper presented at the 
COLING-04. 
Surdeanu, M., Harabagiu, S. M., Williams, J., & 
Aarseth, P. (2003). Using Predicate-Argument 
Structures for Information Extraction. Paper 
presented at the ACL-03. 
Tateisi, Y., & Tsujii, J. (2004). Part-of-Speech 
Annotation of Biology Research Abstracts. Paper 
presented at the LREC-04. 
Tateisi, Y., Yakushiji, A., Ohta, T., & Tsujii, J. (2005). 
Syntax Annotation for the GENIA corpus. 
Tsai, T.-H., Wu, C.-W., Lin, Y.-C., & Hsu, W.-L. 
(2005). Exploiting Full Parsing Information to Label 
Semantic Roles Using an Ensemble of ME and SVM 
via Integer Linear Programming. . Paper presented at 
the CoNLL-05. 
Wattarujeekrit, T., Shah, P. K., & Collier, N. (2004). 
PASBio: predicate-argument structures for event 
extraction in molecular biology. BMC Bioinformatics, 
5, 155. 
Xue, N., & Palmer, M. (2004). Calibrating Features for 
Semantic Role Labeling. Paper presented at the 
EMNLP-04. 
Yang, X., Zhou, G., Su, J., & Tan., C. (2004). 
Improving Noun Phrase Coreference Resolution by 
Matching Strings. Paper presented at the IJCNLP-04. 
64
