Developing an Arabic Treebank: Methods, Guidelines, Procedures, and Tools 
Mohamed MAAMOURI 
LDC, University of Pennsylvania  
3600 Market Street, Suite 810 
Philadelphia, PA 19104, USA 
maamouri@ldc.upenn.edu 
Ann BIES 
LDC, University of Pennsylvania  
3600 Market Street, Suite 810 
Philadelphia, PA 19104, USA 
bies@ldc.upenn.edu 
 
Abstract 
In this paper we address the following 
questions from our experience of the last two 
and a half years in developing a large-scale 
corpus of Arabic text annotated for 
morphological information, part-of-speech, 
English gloss, and syntactic structure:  (a) 
How did we ?leapfrog? through the stumbling 
blocks of both methodology and training in 
setting up the Penn Arabic Treebank (ATB) 
annotation? (b) How did we reconcile the 
Penn Treebank annotation principles and 
practices with the Modern Standard Arabic 
(MSA) traditional and more recent 
grammatical concepts? (c) What are the 
current issues and nagging problems? (d) 
What has been achieved and what are our 
future expectations? 
1 Introduction 
Treebanks are language resources that provide 
annotations of natural languages at various levels 
of structure: at the word level, the phrase level, and 
the sentence level. Treebanks have become 
crucially important for the development of data-
driven approaches to natural language processing 
(NLP), human language technologies, automatic 
content extraction (topic extraction and/or 
grammar extraction), cross-lingual information 
retrieval, information detection, and other forms of 
linguistic research in general. 
The Penn Arabic Treebank began in the fall of 
2001 and has now completed two full releases of 
data: (1) Arabic Treebank: Part 1 v 2.0, LDC 
Catalog No. LDC2003T06, roughly 166K words of 
written Modern Standard Arabic newswire from 
the Agence France Presse corpus; and (2) Arabic 
Treebank: Part 2 v 2.0, LDC Catalog No. 
LDC2004T02, roughly 144K words from Al-Hayat 
distributed by Ummah Arabic News Text.  New 
features of annotation in the UMAAH (UMmah 
Arabic Al-Hayat) corpus include complete 
vocalization (including case endings), lemma IDs, 
and more specific part-of-speech tags for verbs and 
particles.  Arabic Treebank: Part 3 is currently 
underway, and consists of text from An-Nahar. 
(Maamouri and Cieri, 2002) 
The ATB corpora are annotated for 
morphological information, part-of-speech, 
English gloss (all in the ?part-of-speech? phase of 
annotation), and for syntactic structure (Treebank 
II style). (Marcus, et al, 1993), (Marcus, et al, 
1994)  
In addition to the usual issues involved with the 
complex annotation of data, we have come to 
terms with a number of issues that are specific to a 
highly inflected language with a rich history of 
traditional grammar. 
2 Issues of methodology and training with 
Modern Standard Arabic 
2.1 Defining the specificities of ?Modern 
Standard Arabic? 
Modern Standard Arabic (MSA), the natural 
language under investigation, is not natively 
spoken by Arabs, who acquire it only through 
formal schooling.  MSA is the only form of written 
communication in the whole of the Arab world.  
Thus, there exists a living writing and reading 
community of MSA.  However, the level of MSA 
acquisition by its members is far from being 
homogeneous, and their linguistic knowledge, even 
at the highest levels of education, very unequal.  
This problem is going to have its impact on our 
corpus annotation training, routine, and results.  As 
in other Semitic languages, inflection in MSA is 
mostly carried by case endings, which are 
represented by vocalic diacritics appended in 
word-final position.  One must specify here that 
the MSA material form used in the corpus data we 
use consists of a graphic representation in which 
short vowel markers and other pertinent signs like 
the ?shaddah? (consonantal germination) are left 
out, as is typical in most written Arabic, especially 
news writing.  However, this deficient graphic 
representation does not indicate a deficient 
language system.  The reader reads the text and 
interprets its meaning by ?virtually providing? the 
missing grammatical information that leads to its 
acceptable interpretation. 
2.2 How important is the missing 
information? 
Our description and analysis of MSA linguistic 
structures is first done in terms of individual words 
and then expanded to syntactic functions.  Each 
corpus token is labeled in terms of its category and 
also in terms of its functions.  It is marked 
morphologically and syntactically, and other 
relevant relationship features also intervene such as 
concord, agreement and adjacency.  This 
redundancy decreases the importance of the 
absence of most vocalic features. 
2.3 The issue of vocalization 
The corpus for our annotation in the ATB 
requires that annotators complement the data by 
mentally supplying morphological information 
before choosing the automatic analysis, which 
amounts to a pre-requisite ?manual/human? 
intervention and which takes effect even before the 
annotation process begins.  Since no automatic 
vocalization of unvocalized MSA newswire data is 
provided prior to annotation, vocalization becomes 
the responsibility of annotators at both layers of 
annotation.  The part-of-speech (POS) annotators 
provide a first interpretation of the text/data and a 
vocalized output is created for the syntactic 
treebank (TB) annotators, who then engage in the 
responsibility of either validating the interpretation 
under their scrutiny or challenging it and providing 
another interpretation.  This can have drastic 
consequences as in the case of the so-called 
?Arabic deverbals? where the same bare graphemic 
structure can be two nouns in an ?idhafa 
(annexation or construct state) situation? with a 
genitive case ending on the second noun or a 
?virtual? verb or verbal function with a noun 
complement in the accusative to indicate a direct 
object.  In Example 1, genitive case is assigned 
under the noun interpretation, while accusative 
case is assigned by the same graphemic form of the 
word in its more verbal function (Badawi, et al, 
2004, cf. Section 2.10, pp. 237-246). 
Example 11 
Neutral form:  <xbArh Al+nb>        ?????? ?????  
Idhafa:  <ixbAruhu Al+naba>i         ?????????? ???????  
      his receipt (of) the news [news genitive] 
Verbal:  <ixbAruhu Al+naba>a         ?????????? ??????? 
      his telling the news [news accusative] 
These are sometimes difficult decisions to make, 
and annotators? agreement in this case is always at 
                                                     
1  For the transliteration system of all our Arabic 
corpora, we use Tim Buckwalter?s code, at 
http://www.ldc.upenn.edu/myl/morph/buckwalter.html 
its lowest.  Vocalization decisions have a non-
trivial impact on the overall annotation routine in 
terms of both accuracy and speed. 
Vocalization is a difficult problem, and we did 
not have the tools to address it when the project 
began.  We originally decided to treat our first 
corpus, AFP, by having annotators supply word-
internal lexical identity vocalization only, because 
that is how people normally read Arabic ? taking 
the normal risks taken by all readers, with the 
assumption that any interpretation of the case or 
mood chosen would be acceptable as the 
interpretation of an educated native speaker 
annotator.  In our second corpus, UMAAH, we 
decided that it would improve annotation and the 
overall usefulness of the corpus to vocalize the 
texts, by putting the necessary rules of syntax and 
vocalization at the POS level of annotation ? our 
annotators added case endings to nouns and voice 
to verbs, in addition to the word-internal lexical 
identity vocalization.  For our third corpus, 
ANNAHAR (currently in production), we have 
decided to fully vocalize the text, adding the final 
missing piece, mood endings for verbs. In 
conclusion, vocalization is a nagging but necessary 
?nuisance? because while its presence just 
enhances the linguistic analysis of the targeted 
corpus, its absence could be turned into an issue of 
quality of annotation and of grammatical 
credibility among Arab and non-Arab users. 
3 Reconciling Treebank annotation with 
traditional grammar concepts in Arabic 
The question we had to face in the early stages 
of ATB was how to develop a Treebank 
methodology ? an analysis of all the targeted 
syntactic structures ? for MSA represented by 
unvocalized written text data.  Since all Arabic 
readers ? Arabs and foreigners ? go through the 
process of virtually providing/inserting the 
required grammatical rules which allow them to 
reach an interpretation of the text and consequent 
understanding, and since all our recruited 
annotators are highly educated native Arabic 
speakers, we accepted going through our first 
corpus annotation with that premise. Our 
conclusion was that the two-level annotation was 
possible, but we noticed that because of the extra 
time taken hesitating about case markings at the 
TB level, TB annotation was more difficult and 
more time-consuming.  This led to including all 
possible/potential case endings in the POS 
alternatives provided by the morphological 
analyzer.  Our choice was to make the two 
annotation passes equal in difficulty by transferring 
the vocalization difficulty to the POS level.  We 
also thought that it is better to localize that 
difficulty at the initial level of annotation and to try 
to find the best solution to it.  So far, we are happy 
with that choice.  We are aware of the need to have 
a full and correct vocalization for our ATB, and we 
are also aware that there will never be an existing 
extensive vocalized corpus ? except for the 
Koranic text ? that we could totally trust.  The 
challenge was and still is to find annotators with a 
very high level of grammatical knowledge in 
MSA, and that is a tall order here and even in the 
Arab region. 
So, having made the change from unvocalized 
text in the ?AFP Corpus? to fully vocalized text 
now for the ?ANNAHAR Corpus,? we still need to 
ask ourselves the question of what is better: (a) an 
annotated corpus in which the ATB end users are 
left with the task of providing case endings to 
read/understand or (b) an annotated ATB corpus 
displaying case endings with a higher percentage 
of errors due to a significantly more complex 
annotation task? 
3.1 Training annotators, ATB annotation 
characteristics and speed 
The two main factors which affect annotation 
speed in our ATB experience are both related to 
the specific ?stumbling blocks? of the Arabic 
language. 
1.  The first factor which affects annotation 
accuracy and consistency pertains to the 
annotators? educational background (their 
linguistic ?mindset?) and more specifically to their 
knowledge ? often confused and not clear ? of 
traditional MSA grammar.  Some of the important 
obstacles to POS training come from the confusing 
overlap, which exists between the morphological 
categories as defined for Western language 
description and the MSA traditional grammatical 
framework.  The traditional Arabic framework 
recognizes three major morphological categories 
only, namely NOUN, VERB, and PARTICLE. 
This creates an important overlap which leads to 
mistakes/errors and consequent mismatches 
between the POS and syntactic categories.  We 
have noticed the following problems in our POS 
training: (a) the difficulty that annotators have in 
identifying ADJECTIVES as against NOUNS in a 
consistent way; (b) problems with defining the 
boundaries of the NOUN category presenting 
additional difficulties coming from the fact that the 
NOUN includes adjectives, adverbials, and 
prepositions, which could be formally nouns in 
particular functions (e.g., from fawq ??? NOUN to 
fawqa ????? PREP ?above? and fawqu ????? ADV 
etc.).  In this case, the NOUN category then 
overlaps with the adverbs and prepositions of 
Western languages, and this is a problem for our 
annotators who are linguistically savvy and have 
an advanced  knowledge of English and, most 
times, a third Western language. (c) Particles are 
very often indeterminate, and their category also 
overlaps with prepositions, conjunctions, 
negatives, etc. 
2.  The second factor which affects annotation 
accuracy and speed is the behemoth of 
grammatical tests.  Because of the frequency of 
obvious weaknesses among very literate and 
educated native speakers in their knowledge of the 
rules of ?<iErAb? (i.e., case ending marking), it 
became necessary to test the grammatical 
knowledge of each new potential annotator, and to 
continue occasional annotation testing at intervals 
in order to maintain consistency. 
While we have been able to take care of the first 
factor so far, the second one seems to be a very 
persistent problem because of the difficulty level 
encountered by Arab and foreign annotators alike 
in reaching a consistent and agreed upon use of 
case-ending annotation. 
4 Tools and procedures 
4.1 Lexicon and morphological analyzer 
The Penn Arabic Treebank uses a level of 
annotation more accurately described as 
morphological analysis than as part-of-speech 
tagging.  The automatic Arabic morphological 
analysis and part-of-speech tagging was performed 
with the Buckwalter Arabic Morphological 
Analyzer, an open-source software package 
distributed by the Linguistic Data Consortium 
(LDC catalog number LDC2002L49). 
The analyzer consists primarily of three Arabic-
English lexicon files: prefixes (299 entries), 
suffixes (618 entries), and stems (82158 entries 
representing 38600 lemmas).  The lexicons are 
supplemented by three morphological 
compatibility tables used for controlling prefix-
stem combinations (1648 entries), stem-suffix 
combinations (1285 entries), and prefix-suffix 
combinations (598 entries). 
The Arabic Treebank: Part 2 corpus contains 
125,698 Arabic-only word tokens (prior to the 
separation of clitics), of which 124,740 (99.24%) 
were provided with an acceptable morphological 
analysis and POS tag by the morphological parser, 
and 958 (0.76%) were items that the morphological 
parser failed to analyze correctly. 
 
Items with solution      124740   99.24% 
Items with no solution           958     0.76% 
Total                    125698 100.00% 
Table 1. Buckwalter lexicon coverage, UMAAH 
 
The ANNAHAR coverage statistics after POS 1 
(dated January 2004) are as follows:  
The ANNAHAR Corpus contains 340,281 
tokens, of which 47,246 are punctuation, numbers, 
and Latin strings, and 293,035 are Arabic word 
tokens.  
 
Punctuation, Numbers, Latin strings 47,246
Arabic Word Tokens 293,035
TOTAL 340,281
Table 2. Token distribution, ANNAHAR 
 
Of the 293,035 Arabic word tokens, 289,722 
(98.87%) were provided with an accurate 
morphological analysis and POS tag by the 
Buckwalter Arabic Morphological Analyzer.  
3,313 (1.13%) Arabic word tokens were judged to 
be incorrectly analyzed, and were flagged with a 
comment describing the nature of the inaccuracy.  
(Note that 204 of the 3,313 tokens for which no 
correct analysis was found were typos in the 
original text). 
 
Accurately analyzed 
Arabic Word Tokens 
289,722 98.87% 
Commented Arabic Word 
Tokens/ items with no 
solution 
3,313 1.13% 
TOTAL 293,035 100.00% 
Table 3. Lexicon coverage, ANNAHAR 
 
 
COMMENTS ON ITEMS WITH NO SOLUTION 
(no comment)  1741 52.55% 
MISC comment  566 17.08% 
ADJ    250 7.55% 
NOUN   233 7.03% 
TYPO   204 6.16% 
PASSIVE_FORM  110 3.32% 
DIALECTAL_FORM 68 2.05% 
VERB   37 1.12% 
FOREIGN WORD  34 1.03% 
IMPERATIVE  24 0.73% 
ADV    9 0.27% 
GRAMMAR_PROBLEM 9 0.27% 
NOUN_SHOULD_BE_ADJ 7 0.21% 
A_NAME   6 0.18% 
NUMERICAL  6 0.18% 
ABBREV   5 0.15% 
INTERR_PARTICLE 4 0.12% 
TOTAL   3313 100.00% 
Table 4. Distribution of items with no solution, 
     ANNAHAR 
 
4.2 Parsing engine 
In order to improve the speed and accuracy of 
the hand annotation, we automatically pre-parse 
the data after POS annotation and before TB 
annotation using Dan Bikel's parsing engine 
(Bikel, 2002).  Automatically pre-parsing the data 
allows the TB annotators to concentrate on the task 
of correcting a given parse and providing 
information about syntactic function (subject, 
direct object, adverbial, etc.). 
The parsing engine is capable of implementing a 
variety of generative, PCFG-style models 
(probabilistic context free grammar), including that 
of Mike Collins.  As such, in English, it gets 
results that are as good if not slightly better than 
the Collins parser.  Currently, this means that, for 
Section 00 of the WSJ of the English Penn 
Treebank (the development test set), the parsing 
engine gets a recall of 89.90 and a precision of 
90.15 on sentences of length <= 40 words.  The 
Arabic version of this parsing engine currently 
brackets AFP data with recall of 75.6 and precision 
of 77.4 on sentences of 40 words or less, and we 
are in the process of analyzing and improving the 
parser results. 
4.3 Annotation procedure 
Our annotation procedure is to use the automatic 
tools we have available to provide an initial pass 
through the data.  Annotators then correct the 
automatic output. 
First, Tim Buckwalter?s lexicon and 
morphological analyzer is used to generate a 
candidate list of ?POS tags? for each word (in the 
case of Arabic, these are compound tags assigned 
to each morphological segment for the word).  The 
POS annotation task is to select the correct POS 
tag from the list of alternatives provided.  Once 
POS is done, clitics are automatically separated 
based on the POS selection in order to create the 
segmentation necessary for treebanking.  Then, the 
data is automatically parsed using Dan Bikel?s 
parsing engine for Arabic.  Treebank annotators 
correct the automatic parse and add semantic role 
information, empty categories and their 
coreference, and complete the parse.  After that is 
done, we check for inconsistencies between the 
treebank and POS annotation.  Many of the 
inconsistencies are corrected manually by 
annotators or automatically by script if reliably 
safe and possible to do so.  
4.4 POS annotation quality control 
Five files with a total of 853 words (and a 
varying number of POS choices per word) were 
each tagged independently by five annotators for a 
quality control comparison of POS annotators.  Out 
of the total of 853 words, 128 show some 
disagreement.  All five annotators agreed on 85% 
of the words; the pairwise agreement is at least 
92.2%. 
For 82 out of the 128 words with some 
disagreement, four annotators agreed and only one 
disagreed.  Of those, 55 are items with ?no match? 
having been chosen from among the POS choices, 
due to one annotator?s definition of good-enough 
match differing from all of the others?.  The 
annotators have since reached agreement on which 
cases are truly ?no match,? and thus the rate of this 
disagreement should fall markedly in future POS 
files, raising the rate of overall agreement. 
5 Specifications for the Penn Arabic 
Treebank annotation guidelines 
5.1 Morphological analysis/Part-of-Speech 
The guidelines for the POS annotators are 
relatively straightforward, since the task essentially 
involves choosing the correct analysis from the list 
of alternatives provided by the morphological 
analyzer and adding the correct case ending.  The 
difficulties encountered by annotators in assigning 
POS and case endings are somewhat discussed 
above and will be reviewed by Tim Buckwalter in 
a separate presentation at COLING 2004.  
5.2 Syntactic analysis 
For the most part, our syntactic/predicate-
argument annotation of newswire Arabic follows 
the bracketing guidelines for the Penn English 
Treebank where possible. (Bies, et al 1995)  Our 
updated Arabic Treebank Guidelines is available 
on-line from the Linguistic Data Consortium at: 
http://www.ldc.upenn.edu/Catalog/docs/LDC2004
T02/ 
Some points where the Penn Arabic Treebank 
differs from the Penn English Treebank: 
? Arabic subjects are analyzed as VP 
internal, following the verb. 
? Matrix clause (S) coordination is 
possible and frequent. 
? The function of NP objects of transitive 
verbs is directly shown as NP-OBJ. 
We are also informed by on-going efforts to 
share data and reconcile annotations with the 
Prague Arabic Dependency Treebank (two Prague-
Penn Arabic Treebanking Workshops took place in 
2002 and 2003).  Some points where the Penn 
Arabic Treebank differs from the Prague Arabic 
Dependency Treebank: 
? Specific adverbial functions (LOC, 
TMP, etc.) are shown on the adverbial 
(PP, ADVP, clausal) modification of 
predicates. 
? The argument/adjunct distinction within 
NP is shown for noun phrases and 
clauses.  
? Empty categories (pro-drop subjects and 
traces of syntactic movement) are 
inserted. 
? Apposition is distinguished from other 
modification of nouns only for proper 
names. 
In spite of the considerable differences in word 
order between Modern Standard Arabic and 
English, we found that for the most part, it was 
relatively straightforward to adapt the guidelines 
for the Penn English Treebank to our Arabic 
Treebank.  In the interest of speed in starting 
annotation and of using existing tools to the 
greatest extent possible, we chose to adapt as much 
as possible from the English Treebank guidelines. 
There exists a long-standing, extensive, and 
highly valued paradigm of traditional grammar in 
Classical Arabic.  We chose to adapt the 
constituency approach from the Penn English 
Treebank rather than keeping to a strict and 
difficult adherence to a traditional Arabic grammar 
approach for several reasons: 
? Compatibility with existing treebanks, 
processing software and tools, 
? We thought it would be easier and more 
efficient to teach annotators, who come 
trained in Arabic grammar, to use our 
constituency approach than to teach 
computational linguists an old and 
complex Arabic-specific syntactic 
terminology.  
Nonetheless, it was important to adhere to an 
approach that did not strongly conflict with the 
traditional approach, in order to ease the cognitive 
load on our annotators, and also in order to be 
taken seriously by modern Arabic grammarians.  
Since there has been little work done on large data 
corpora in Arabic under any of the current 
syntactic theories in spite of the theoretical 
syntactic work being done (Mohamed, 2000), we 
have been working out solutions to Arabic syntax 
by combining the Penn Treebank constituency 
approach with pertinent insights from traditional 
grammar as well as modern theoretical syntax. 
For example, we analyze the underlying basic 
sentence structure as verb-initial, following the 
traditional grammar approach.  However, since the 
verb is actually not the first element in many 
sentences in the data, we adopt a topicalization 
structure for arguments that are fronted before the 
verb (as in Example 2, where the subject is 
fronted) and allow adverbials and conjunctions to 
appear freely before the verb (as in Example 3, 
where a prepositional phrase is pre-verbal).   
 
Example 2  
 
(S (NP-TPC-1 Huquwq+u  ??????? 
(NP Al+<inosAn+i  ??????????? )) 
(VP ta+qaE+u     ?????? 
(NP-SBJ-1 *T*)
(PP Dimona   ?????? 
(NP <ihotimAm+i+nA   ??????????? )
)))
  
??????? ??????????? ?????? ?????? ??????????? 
human rights exist within our concern 
 
 
Example 3 
 
(S (PP min  ??? 
(NP jih+ap+K   ??????   
>uxoraY  ?????? )) 
(VP ka$af+at     ???????  
(NP-SBJ maSAdir+u    ???????? 
miSoriy~+ap+N    ??????????? 
muT~aliE+ap+N  ???????????  )) 
(NP-OBJ Haqiyqata  ????????? 
(NP Al->amri  ??????? )))
 
 ???????? ???????? ??????????? ??????????? ???????? ??????  ??? ??????  ?????? 
from another side, well-informed Egyptian 
sources revealed the truth of the matter 
 
For many structures, the traditional approach and 
the treebank approach come together very easily.  
The traditional ?equational sentence,? for example, 
is a sentence that consists of a subject and a 
predicate without an overt verb (kAna or ?to be? 
does not appear overtly in the present tense).  This 
is quite satisfactorily represented in the same way 
that small clauses are shown in the Penn English 
Treebank, as in Example 4, since traditional 
grammar does not have a verb here, and we do not 
want to commit to the location of any potential 
verb phrase in these sentences. 
 
Example 4  
 
(S (NP-SBJ Al-mas>alatu ??????????? ) 
(ADJP-PRD basiyTatuN  ?????????)) 
  
???????????? ???????? 
the question is simple 
 
5.3 Current issues and nagging problems 
In a number of structures, however, the 
traditional grammar view does not line up 
immediately with the structural view that is 
necessary for annotation.  Often these are 
structures that are known to be problematic in a 
more general sense for either traditional grammar 
or theoretical syntax, or both.  We take both views 
into account and reconcile them in the best way 
that we can. 
5.3.1 Clitics 
The prevalence of cliticization in Arabic 
sentences of determiners, prepositions, 
conjunctions, and pronouns led to a necessary 
difference in tokenization between the POS files 
and the TB files.  Such cliticized constituents are 
written together with their host constituents in the 
text (e.g., Al+<inosAn+i  ???????????  ?the person? and 
??????????  bi+qirA?ati ?with reading?).  Clitics that 
play a role in the syntactic structure are split off 
into separate tokens (e.g., object pronouns 
cliticized to verbs, subject pronouns cliticized to 
complementizers, cliticized prepositions, etc.), so 
that their syntactic roles can be annotated in the 
tree.  Clitics that do not affect the structure are not 
separated (e.g., determiners).  Since the word 
boundaries necessary to separate the clitics are 
taken from the POS tags, and since it is not 
possible to show the syntactic structure unless the 
clitics are separated, correct POS tagging is 
extremely important in order to be able to properly 
separate clitics prior to the syntactic annotation. 
In the example below, both the conjunction wa 
?and? and the direct object hA ?it/them/her? are 
cliticized to the verb and also serve syntactic 
functions independent of the verb (sentential 
coordination and direct object). 
 
Example 5 
 
??????????? 
wasatu$AhiduwnahA 
wa/CONJ+sa/FUT+tu/IV2MP+$Ahid/VERB_IMP
ERFECT+uwna/IVSUFF_SUBJ:MP_MOOD:I+h
A/IVSUFF_DO:3FS 
and + will + you [masc.pl.] + 
watch/observe/witness + it/them/her 
 
The rest of the verbal inflections are also 
regarded as clitics in traditional grammar terms.  
However, for our purposes they do not require 
independent segmentation as they do not serve 
independent syntactic functions.  The subject 
inflection, for example, appears readily with full 
noun phrase subject in the sentence as well 
(although in this example, the subject is pro-
dropped).  The direct object pronoun clitic, in 
contrast, is in complementary distribution with full 
noun phrase direct objects.  Topicalized direct 
objects can appear with resumptive pronouns in the 
post-verbal direct object position.  However, 
resumptive pronouns in this structure should not be 
seen as problematic full noun phrases, as they are 
parasitic on the trace of movement ? and in fact 
they are taken to be evidence of the topicalization 
movement, since resumptive pronouns are 
common in relative clauses and with other 
topicalizations. 
Thus, we regard the cliticized object pronoun as 
carrying the full syntactic function of direct object.  
As such, we segment it as a separate token and 
represent it as a noun phrase constituent that is a 
sister to the verb (as shown in Example 6 below). 
 
Example 6 
 
(S wa-    -? 
(VP sa+tu+$Ahid+uwna-   ???????????? 
(NP-SBJ *)
(NP-OBJ ?hA      ??  )))
??????????? 
and you will observe her 
 
5.3.2 Gerunds (Masdar) and participials 
The question of the dual noun/verb nature of 
gerunds and participles in Arabic is certainly no 
less complex than for English or other languages.  
We have chosen to follow the Penn English 
Treebank practice to represent the more purely 
nominal masdar as noun phrases (NP) and the 
masdar that function more verbally as clauses (as 
S-NOM when in nominal positions).  In Example 
7, the masdar behaves like a noun in assigning 
genitive case.   
 
Example 7 
(PP bi-  -?? 
(NP qirA?ati        ???????? 
(NP kitAbi       ??????  
(NP Al-naHwi ??????? )))) 
 
??????????? ?????? ?????? 
with the reading of the book of syntax  
[book genitive] 
 
 
In Example 8, in contrast, the masdar functions 
more verbally, in assigning accusative case. 
 
 
Example 8 
 
(PP bi-     -?? 
(S-NOM (VP qirA?ati ????????) 
(NP-SBJ fATimata ???????? -) 
(NP-OBJ Al-kitAba  ????????  
                                                   ))))
 
 ???????????  ????????  ???????  
with Fatma?s reading the book  
[book accusative] 
 
This annotation scheme to allow for both the 
nominal and verbal functions of masdar is easily 
accepted and applied by annotators for the most 
part.  However, there are situations where the 
functions and behaviors of the masdar are in 
disagreement.  For example, a masdar can take a 
determiner ?Al-? (the behavior of a noun) and at 
the same time assign accusative case (the behavior 
of a verb). 
 
Example 9 
 
(PP bi     -?? 
(S-NOM
(VP Al+mukal~afi    ?????????? 
(NP-SBJ *)
(NP-OBJ <injAza   ??????? 
(NP Al+qarAri ?????????
Al+mawEuwdi
?????????? )))))
 
?????????????? ??????? ????????? ????????? 
with the (person in) charge of completion (of) 
the promised report [completion accusative] 
 
In this type of construction, the annotators must 
choose which behaviors to give precedence 
(accusative case assignment trumps determiners, 
for example).  However, it also brings up the issues 
and problems of assigning case ending and the 
annotators? knowledge of Arabic grammar and the 
rules of ?<iErAb.?  These examples are complex 
grammatically, and finding the right answer (even 
in strictly traditional grammar terms) is often 
difficult. 
This kind of ambiguity and decision-making 
necessarily slows annotation speed and reduces 
accuracy.  We are continuing our discussions and 
investigations into the best solutions for such 
issues. 
6 Future work 
Annotation for the Arabic Treebank is on-going, 
currently on a corpus of An-Nahar newswire 
(350K words).  We continue efforts to improve 
annotation accuracy, consistency and speed, both 
for POS and TB annotation.   
Conclusion 
In designing our annotation system for Arabic, 
we relied on traditional Arabic grammar, previous 
grammatical theories of Modern Standard Arabic 
and modern approaches, and especially the Penn 
Treebank approach to syntactic annotation, which 
we believe is generalizable to the development of 
other languages.  We also benefited from the 
existence at LDC of a rich experience in linguistic 
annotation.  We were innovative with respect to 
traditional grammar when necessary and when we 
were sure that other syntactic approaches 
accounted for the data.  Our goal is for the Arabic 
Treebank to be of high quality and to have 
credibility with regards to the attitudes and respect 
for correctness known to be present in the Arabic 
world as well as with respect to the NLP and wider 
linguistic communities.  The creation and use of 
efficient tools such as an automated morphological 
analyzer and an automated parsing engine ease and 
speed the annotation process.  These tools helped 
significantly in the successful creation of a process 
to analyze Arabic text grammatically and allowed 
the ATB team to publish the first significant 
database of morphologically and syntactically 
annotated Arabic news text in the world within one 
year.  Not only is this an important achievement 
for Arabic for which we are proud, but it also 
represents significant methodological progress in 
treebank annotation as our first data release was 
realized in significantly less time.  Half a million 
MSA words will be treebanked by end of 2004, 
and our choice of MSA corpora will be diversified 
to be representative of the current MSA writing 
practices in the Arab region and the world.  In spite 
of the above, we are fully aware of the humbling 
nature of the task and we fully understand and 
recognize that failures and errors may certainly be 
found in our work. The devil is in the details, and 
we remain committed to ironing out all mistakes.  
We count on the feedback of our users and readers 
to complete our work.  
8 Acknowledgements 
We gratefully acknowledge the tools and support 
provided to this project by Tim Buckwalter, Dan 
Bikel and Hubert Jin.  Our sincere thanks go to all 
of the annotators who have contributed their 
invaluable time and effort to Arabic part-of-speech 
and treebank annotation, and more especially to 
our dedicated treebank annotators, Wigdan El 
Mekki and Tasneem Ghandour. 
References  
Elsaid Badawi, M. G. Carter and Adrian Gully, 
2004. Modern Written Arabic: A Comprehensive 
Grammar.  Routledge: New York. 
Daniel M. Bikel, 2002. Design of a multi-lingual, 
parallel-processing statistical parsing engine. 
Proceedings of the Human Language 
Technology Workshop. 
Bracketing Guidelines for Treebank II Style, 1995. 
Eds: Ann Bies, Mark Ferguson, Karen Katz, 
Robert MacIntyre, Penn Treebank Project, 
University of Pennsylvania, CIS Technical 
Report MS-CIS-95-06. 
Mohamed Maamouri and Christopher Cieri, 2002. 
Resources for Arabic Natural Language 
Processing at the Linguistic Data Consortium. 
Proceedings of the International Symposium on 
Processing of Arabic.  Facult? des Lettres, 
University of Manouba, Tunisia. 
M. Marcus, G. Kim, M. Marcinkiewicz, R. 
MacIntyre, A. Bies, M. Ferguson, K. Katz & B. 
Schasberger, 1994. The Penn Treebank: 
Annotating predicate argument structure. 
Proceedings of the Human Language 
Technology Workshop, San Francisco. 
M. Marcus, B. Santorini and M.A. Marcinkiewicz, 
1993. Building a large annotated corpus of 
English: the Penn Treebank. Computational 
Linguistics. 
Mohamed A. Mohamed, 2000. Word Order, 
Agreement and Pronominalization in Standard 
and Palestinian Arabic. CILT 181. John 
Benjamins: Philadelphia. 
Zdenek ?abokrtsk? and Otakar Smr?, 2003. Arabic 
Syntactic Trees: from Constituency to 
Dependency. EACL 2003 Conferenceompanion. 
Association for Computational Linguistics, 
Hungary.  
 
 
Integrated Annotation for Biomedical Information Extraction
Seth Kulick and Ann Bies and Mark Liberman and Mark Mandel
and Ryan McDonald and Martha Palmer and Andrew Schein and Lyle Ungar
University of Pennsylvania
Philadelphia, PA 19104
 
skulick,bies,myl  @linc.cis.upenn.edu,
mamandel@unagi.cis.upenn.edu,
 
ryantm,mpalmer,ais,ungar  @cis.upenn.edu
Scott Winters and Pete White
Division of Oncology,
Children?s Hospital of Philadelphia
Philadelphia, Pa 19104
 
winters,white  @genome.chop.edu
Abstract
We describe an approach to two areas of
biomedical information extraction, drug devel-
opment and cancer genomics. We have devel-
oped a framework which includes corpus anno-
tation integrated at multiple levels: a Treebank
containing syntactic structure, a Propbank con-
taining predicate-argument structure, and an-
notation of entities and relations among the en-
tities. Crucial to this approach is the proper
characterization of entities as relation compo-
nents, which allows the integration of the entity
annotation with the syntactic structure while
retaining the capacity to annotate and extract
more complex events. We are training statis-
tical taggers using this annotation for such ex-
traction as well as using them for improving the
annotation process.
1 Introduction
Work over the last few years in literature data mining
for biology has progressed from linguistically unsophisti-
cated models to the adaptation of Natural Language Pro-
cessing (NLP) techniques that use full parsers (Park et
al., 2001; Yakushiji et al, 2001) and coreference to ex-
tract relations that span multiple sentences (Pustejovsky
et al, 2002; Hahn et al, 2002) (For an overview, see
(Hirschman et al, 2002)). In this work we describe an ap-
proach to two areas of biomedical information extraction,
drug development and cancer genomics, that is based on
developing a corpus that integrates different levels of se-
mantic and syntactic annotation. This corpus will be a
resource for training machine learning algorithms useful
for information extraction and retrieval and other data-
mining applications. We are currently annotating only
abstracts, although in the future we plan to expand this to
full-text articles. We also plan to make publicly available
the corpus and associated statistical taggers.
We are collaborating with researchers in the Division
of Oncology at The Children?s Hospital of Philadelphia,
with the goal of automatically mining the corpus of can-
cer literature for those associations that link specified
variations in individual genes with known malignancies.
In particular we are interested in extracting three entities
(Gene, Variation Event, and Malignancy) in the follow-
ing relationship: Gene X with genomic Variation Event
Y is correlated with Malignancy Z. For example, WT1 is
deleted in Wilms Tumor #5. Such statements found in the
literature represent individual gene-variation-malignancy
observables. A collection of such observables serves
two important functions. First, it summarizes known
relationships between genes, variation events, and ma-
lignancies in the cancer literature. As such, it can be
used to augment information available from curated pub-
lic databases, as well as serve as an independent test for
accuracy and completeness of such repositories. Second,
it allows inferences to be made about gene, variation, and
malignancy associations that may not be explicitly stated
in the literature, both at the fact and entity instance lev-
els. Such inferences provide testable hypotheses and thus
future research targets.
The other major area of focus, in collaboration with
researchers in the Knowledge Integration and Discov-
ery Systems group at GlaxoSmithKline (GSK), is the ex-
traction of information about enzymes, focusing initially
on compounds that affect the activity of the cytochrome
P450 (CYP) family of proteins. For example, the goal is
to see a phrase like
Amiodarone weakly inhibited CYP2C9,
CYP2D6, and CYP3A4-mediated activities
                                            Association for Computational Linguistics.
                   Linking Biological Literature, Ontologies and Databases, pp. 61-68.
                                                HLT-NAACL 2004 Workshop: Biolink 2004,
with Ki values of 45.1?271.6 
and extract the facts
amiodarone inhibits CYP2C9 with
Ki=45.1-271.6
amiodarone inhibits CYP2D6 with
Ki=45.1-271.6
amiodarone inhibits CYP3A4 with
Ki=45.1-271.6
Previous work at GSK has used search algorithms that
are based on pattern matching rules filling template slots.
The rules rely on identifying the relevant passages by first
identifying compound names and then associating them
with a limited number of relational terms such as inhibit
or inactivate. This is similar to other work in biomedical
extraction projects (Hirschman et al, 2002).
Creating good pattern-action rules for an IE problem is
far from simple. There are many complexities in the dif-
ferent ways that a relation can be expressed in language,
such as syntactic alternations and the heavy use of co-
ordination. While sufficiently complex patterns can deal
with these issues, it requires a good amount of time and
effort to build such hand-crafted rules, particularly since
such rules are developed for each specific problem. A
corpus that is annotated with sufficient syntactic and se-
mantic structure offers the promise of training taggers for
quicker and easier information extraction.
The corpus that we are developing for the two differ-
ent application demands consists of three levels of anno-
tation: the entities and relations among the entities for the
oncology or CYP domain, syntactic structure (Treebank),
and predicate-argument structure (Propbank). This is a
novel approach from the point-of-view of NLP since pre-
vious efforts at Treebanking and Propbanking have been
independent of the special status of any entities, and pre-
vious efforts at entity annotation have been independent
of corresponding layers of syntactic and semantic struc-
ture. The decomposition of larger entities into compo-
nents of a relation, worthwhile by itself on conceptual
grounds for entity definition, also allows the component
entities to be mapped to the syntactic structure. These
entities can be viewed as semantic types associated with
syntactic constituents, and so our expectation is that au-
tomated analyses of these related levels will interact in a
mutually reinforcing and beneficial way for development
of statistical taggers. Development of such statistical tag-
gers is proceeding in parallel with the annotation effort,
and these taggers help in the annotation process, as well
as being steps towards automatic extraction.
In this paper we focus on the aspects of this project
that have been developed and are in production, while
also trying to give enough of the overall vision to place
the work that has been done in context. Section 2 dis-
cusses some of the main issues around the development
of the guidelines for entity annotation, for both the on-
cology and inhibition domains. Section 3 first discusses
the overall plan for the different levels of annotation, and
then focuses on the integration of the two levels currently
in production, entity annotation and syntactic structure.
Section 4 describes the flow of the annotation process,
including the development of the statistical taggers men-
tioned above. Section 5 is the conclusion.
2 Guidelines for Entity Annotation
Annotation has been proceeding for both the oncology
and the inhibition domains. Here we give a summary of
the main features of the annotation guidelines that have
been developed. We have been influenced in this by pre-
vious work in annotation for biomedical information ex-
traction (Ohta et al, 2002; Gaizauskas et al, 2003). How-
ever, we differ in the domains we are annotating and the
design philosophy for the entity guidelines. For exam-
ple, we have been concentrating on explicit concepts for
entities like genes rather than developing a wide-range
ontology for the various physical instantiations.
2.1 Oncology Domain
Gene Entity For the sake of this project the defini-
tion for ?Gene Entity? has two significant characteristics.
First, ?Gene? refers to a composite entity as opposed to
the strict biological definition. As has been noted by oth-
ers, there are often ambiguities in the usage of the en-
tity names. For example, it is sometimes unclear as to
whether it is the gene or protein being referenced, or the
same name might refer to the gene or the protein at dif-
ferent locations in the same document. Our approach to
this problem is influenced by the named entity annota-
tion in the Automatic Content Extraction (ACE) project
(Consortium, 2002), in which ?geopolitical? entities can
have different roles, such as ?location? or ?organization?.
Analogously, we consider a ?gene? to be a composite en-
tity that can have different roles throughout a document.
Standardization of ?Gene? references between different
texts and between gene synonyms is handled by exter-
nally referencing each instance to a standard ontology
(Ashburner et al, 2000).
In the context of this project, ?Gene? refers to a con-
ceptual entity as opposed to the specific manifestation of
a gene (i.e. an allele or nucleotide sequence). Therefore,
we consider genes to be abstract concepts identifying ge-
nomic regions often associated with a function, such as
MYC or TrkB; we do not consider actual instances of
such genes within the gene-entity domain. Since we are
interested in the association between Gene-entities and
malignancies, for this project genes are of interest to us
when they have an associated variation event. Therefore,
the combination of Gene entities and Variation events
provides us with an evoked entity representing the spe-
cific instance of a gene.
Variation Events as Relations Variations comprise a
relationship between the following entities: Type (e.g.
point mutation, translocation, or inversion), Location
(e.g. codon 14, 1p36.1, or base pair 278), Original-State
(e.g. Alanine), and Altered-State (e.g. Thymine). These
four components represent the key elements necessary
to describe any genomic variation event. Variations are
often underspecified in the literature, frequently having
only two or three of these specifications. Characterizing
individual variations as a relation among such compo-
nents provides us with a great deal of flexibility: 1) it al-
lows us to capture the complete variation event even when
specific components are broadly spaced in the text, often
spanning multiple sentences or even paragraphs; 2) it pro-
vides us with a convenient means of tracking anaphora
between detailed descriptions (e.g. a point mutation at
codon 14 and summary references (e.g. this variation);
and 3) it provides a single structure capable of capturing
the breadth of variation specifications (e.g. A-  T point
mutation at base pair 47, A48-  G or t(11;14)(q13;32)).
Malignancy The guidelines for malignancy annotation
are under development. We are planning to define it in a
manner analogous to variation, whereby a Malignancy is
composed of various attribute types (such as developmen-
tal stage, behavior, topographic site, and morphology).
2.2 CYP Domain
In the CYP Inhibition annotation task we are tagging
three types of entities:
1. CYP450 enzymes (cyp)
2. other substances (subst)
3. quantitative measurements (quant)
Each category has its own questions and uncertain-
ties. Names like CYP2C19 and cytochrome P450 en-
zymes proclaim their membership, but there are many
aliases and synonyms that do not proclaim themselves,
such as 17,20-lyase. We are compiling a list of such
names.
Other substances is a potentially huge and vaguely-
delimited set, which in the current corpus includes grape-
fruit juice and red wine as well as more obviously bio-
chemical entities like polyunsaturated fatty acids and ery-
thromycin. The quantitative measurements we are di-
rectly interested in are those directly related to inhibition,
such as IC50 and K(i). We tag the name of the measure-
ment, the numerical value, and the unit. For example, in
the phrase ...was inhibited by troleandomycin (ED50 = 1
microM), ED50 is the name, 1 the value, and microM the
unit. We are also tagging other measurements, since it
is easy to do and may provide valuable information for
future IE work.
3 Integrated Annotation
As has been noted in the literature on biomedical IE (e.g.,
(Pustejovsky et al, 2002; Yakushiji et al, 2001)), the
same relation can take a number of syntactic forms. For
example, the family of words based on inhibit occurs
commonly in MEDLINE abstracts about CYP enzymes
(as in the example in the introduction) in patterns like A
inhibited B, A inhibited the catalytic activity of B, inhibi-
tion of B by A, etc.
Such alternations have led to the use of pattern-
matching rules (often hand-written) to match all the rele-
vant configurations and fill in template slots based on the
resulting pattern matches. As discussed in the introduc-
tion, dealing with such complications in patterns can take
much time and effort.
Our approach instead is to build an annotated corpus
in which the predicate-argument information is annotated
on top of the parsing annotations in the Treebank, the re-
sulting corpus being called a ?proposition bank? or Prop-
bank. This newly annotated corpus is then used for train-
ing processors that will automatically extract such struc-
tures from new examples.
In a Propbank for biomedical text, the types of in-
hibit examples listed above would consistently have their
compounds labeled as Arg0 and their enzymes labeled as
Arg1, for nominalized forms such as A is an inhibitor of
B, A caused inhibition of B, inhibition of B by A, as well
the standard A inhibits B. We would also be able to la-
bel adjuncts consistently, such as the with prepositional
phrase in CYP3A4 activity was decreased by L, S and F
with IC(50) values of about 200 mM. In accordance with
other Calibratable verbs such as rise, fall, decline, etc.,
this phrase would be labeled as an Arg2-EXTENT, re-
gardless of its syntactic role.
A Propbank has been built on top of the Penn Tree-
bank, and has been used to train ?semantic taggers?, for
extracting argument roles for the predicates of interest,
regardless of the particular syntactic context.1
Such semantic taggers have been developed by using
machine learning techniques trained on the Penn Prop-
bank (Surdeanu et al, 2003; Gildea and Palmer, 2002;
Kingsbury and Palmer, 2002). However, the Penn Tree-
bank and Propbank involve the annotation of Wall Street
Journal text. This text, being a financial domain, differs
in significant ways from the biomedical text, and so it is
1The Penn Propbank is complemented by NYU?s Nom-
bank project (Meyers, October 2003), which includes tagging
of nominal predicate structure. This is particular relevant for
the biomedical domain, given the heavy use of nominals such
mutation and inhibition.
necessary for this approach to have a corpus of biomed-
ical texts such as MEDLINE articles annotated for both
syntactic structure (Treebanking) and shallow semantic
structure (Propbanking).
In this project, the syntactic and semantic annotation is
being done on a corpus which is also being annotated for
entities, as described in Section 2. Since semantic tag-
gers of the sort described above result in semantic roles
assigned to syntactic tree constituents, it is desirable to
have the entities correspond to syntactic constituents so
that the semantic roles are assigned to entities. The en-
tity information can function as type information and be
taken advantage of by learning algorithms to help charac-
terize the properties of the terms filling specified roles in
a given predicate.
This integration of these three different annotation lev-
els, including the entities, is being done for the first time2,
and we discuss here three main challenges to this corre-
spondence between entities and constituents: (1) entities
that are large enough to cut across multiple constituents,
(2) entities within prenominal modifiers, and (3) coordi-
nation.3
Relations and Large Entities One major area of con-
cern is the possibility of entities that contain more than
one syntactic constituent and do not match any node in
the syntax tree. For example, as discussed in Section 2, a
variation event includes material on a variation?s type, lo-
cation, and state, and can cut not only across constituents,
but even sentences and paragraphs. A simple example is
point mutations at codon 12, containing both the nominal
(the type of mutation) and following NP (the location).
Note that while in isolation this could also be considered
one syntactic constituent, the NP and PP together, the ac-
tual context is ...point mutations at codon 12 in duode-
nal lavage fluid.... Since all PPs are attached at the same
level, at codon 12 and in duodenal lavage fluid are sis-
ters, and so there is no constituent consisting of just point
mutations at codon 12.
Casting the variation event as a relation between dif-
ferent component entities allows the component entities
to correspond to tree constituents, while retaining the ca-
pacity to annotate and search for more complex events.
In this case, one component entity point mutations cor-
2An influential precursor to this integration is the system de-
scribed in (Miller et al, 1996). Our work is in much the same
spirit, although the representation of the predicate-argument
structure via Propbank and the linkage to the entities is quite
different, as well as of course the domain of annotation.
3There are cases where the entities are so minimal that they
are contained within a NP, not including the determiner, such as
CpG site in the NP a CpG site. entities. We are not as concerned
about these cases since we expect that such entity information
properly contained within a base NP can be associated with the
full base NP.
responds to a (base) NP node, and at codon 12 is corre-
sponds to the PP node that is the NP?s sister. At the same
time, the relation annotation contains the information re-
lating these two constituents.
Similarly, while the malignancy entity definition is cur-
rently under development, as mentioned in Section 2.1, a
guiding principle is that it will also be treated as a relation
and broken down into component entities. While this also
has conceptual benefits for the annotation guidelines, it
has the fortunate effect of making such otherwise syntax-
unfriendly malignancies as colorectal adenomas contain-
ing early cancer and acute myelomonocytic leukemia in
remission amenable for mapping the component parts to
syntactic nodes.
Entities within Prenominal Modifiers While we are
for the most part following the Penn Treebank guide-
lines (Bies et al, 1995), we are modifying them in two
important aspects. One concerns the prenominal mod-
ifiers, which in the Penn Treebank were left flat, with
no structure, but in this biomedical domain contain much
of the information - e.g., cancer-associated autoimmune
antigen. Not only would this have had no annotation
for structure, but even more bizarrely, cancer-associated
would have been a single token in the Penn Treebank,
thus making it impossible to capture the information as
to what is associated with what. We have developed new
guidelines to assign structure to prenominal entities such
as breast cancer, as well as changed the tokenization
guidelines to break up tokens such as cancer-associated.
Coordination We have also modified the treebank an-
notation to account for the well-known problem of enti-
ties that are discontinuous within a coordination structure
- e.g., K- and H-ras, where the entities are K-ras and H-
ras. Our annotation tool allows for discontinuous entities,
so that both K-ras and H-ras are annotated as genes.
Under standard Penn Treebank guidelines for tokeniza-
tion and syntactic structure, this would receive the flat
structure
NP
K- and H-ras
in which there is no way to directly associate the entity
K-ras with a constituent node.
We have modified the treebank guidelines so that K-ras
and H-ras are both constituents, with the ras part of K-ras
represented with an empty category co-indexed with ras
in H-ras:4.
4This is related to the approach to coordination in the GE-
NIA project.
NP
NP
K - NX-1
*P*
and NP
H - NX-1
ras
4 Annotation Process
We are currently annotating MEDLINE abstracts for both
the oncology and CYP domains. The flowchart for the
annotation process is shown in Figure 1. Tokenization,
POS-tagging, entity annotation (both domains), and tree-
banking are in full production. Propbank annotation and
the merging of the entities and treebanking remain to be
integrated into the current workflow. The table in Fig-
ure 2 shows the number of abstracts completed for each
annotation area.
The annotation sequence begins with tokenization and
part-of-speech annotating. While both aspects are simi-
lar to those used for the Penn Treebank, there are some
differences, partly alluded to in Section 3. Tokens are
somewhat more fine-grained than in the Penn Treebank,
so that H-ras, e.g., would consist of three tokens: H, -,
and ras.
Tokenized and part-of-speech annotated files are then
sent to the entity annotators, either for oncology or CYP,
depending on which domain the abstract has been chosen
for. The entities described in Section 2 are annotated at
this step. We are using WordFreak, a Java-based linguis-
tic annotation tool5, for annotation of tokenization, POS,
and entities. Figure 3 is a screen shot of the oncology do-
main annotation, here showing a variation relation being
created out of component entities for type and location.
In parallel with the entity annotation, a file is tree-
banked - i.e., annotated for its syntactic structure. Note
that this is done independently of the entity annotation.
This is because the treebanking guidelines are relatively
stable (once they were adjusted for the biomedical do-
main as described in Section 3), while the entity defini-
tions can require a significant period of study before sta-
bilizing, and with the parallel treatment the treebanking
can proceed without waiting for the entity annotation.
However, this does mean that to produce the desired
integrated annotation, the entity and treebanking annota-
tions need to be merged into one representation. The con-
sideration of the issues described in Section 3 has been
carried out for the purpose of allowing this integration
of the treebanking and entity annotation. This has been
completed for some pilot documents, but the full merging
remains to be integrated into the workflow system.
5http://www.sf.net/projects/wordfreak
As mentioned in the introduction, statistical taggers
are being developed in parallel with the annotation effort.
While such taggers are part of the final goal of the project,
providing the building blocks for extracting entities and
relations, they are also useful in the annotation process
itself, so that the annotators only need to perform correc-
tion of automatically tagged data, instead of starting from
scratch.
Until recently (Feb. 10), the part-of-speech annotation
was done by hand-correcting the results of tagging the
data with a part-of-speech tagger trained on a modified
form of the Penn Treebank.6 The tagger is a maximum-
entropy model utilizing the opennlp package available
at http://www.sf.net/projects/opennlp . It
has now been retrained using 315 files (122 from the
oncology domain, 193 from the cyp domain). Figure 4
shows the improvement of the new vs. the old POS tag-
ger on the same 294 files that have been hand-corrected.
These results are based on testing files that have already
been tokenized, and thus are an evaluation only of the
POS tagger and not the tokenizer. While not directly
comparable to results such as (Tateisi and Tsujii, 2004),
due to the different tag sets and tokenization, they are in
the same general range.7
The oncology and cyp entity annotation, as well as the
treebanking are still being done fully manually, although
that will change in the near future. Initial results for a tag-
ger to identify the various components of a variation re-
lation are promising, although not yet integrated into an-
notation process. The tagger is based on the implementa-
tion of Conditional Random Fields (Lafferty et al, 2001)
in the Mallet toolkit (McCallum, 2002). Briefly, Condi-
tional Random Fields are log-linear models that rely on
weighted features to make predictions on the input. Fea-
tures used by our system include standard pattern match-
ing and word features as well as some expert-created reg-
ular expression features8. Using 10-fold cross-validation
on 264 labelled abstracts containing 551 types, 1064 lo-
6Roughly, Penn Treebank tokens were split at hyphens, with
the individual components then sent through a Penn Treebank-
trained POS tagger, to create training data for another POS tag-
ger. For example (JJ York-based) is treated as (NNP
York) (HYPH -) (JJ based). While this works rea-
sonably well for tokenization, the POS tagger suffered severely
from being trained on a corpus with such different properties.
7The tokenizer has also been retrained and the new tokenizer
is being used for annotation, although although we do not have
the evaluation results here.
8e.g., chr|chromosome [1-9]|1[0-9]|2[0-
2]|X|Y p|q
Merged Entity/
Treebank Annotation
Tokenization
Entity Annotation
POS Annotation
Treebank/Propbank
Annotation
Figure 1: Annotation Flow
Annotation Task Start Date Annotated Documents
Part-of-Speech Tagging 8/22/03 422
Entity Tagging 9/12/03 414
Treebanking 1/8/04 127
Figure 2: Current Annotation Production Results
Figure 3: Relation Annotation in WordFreak
Tagger Training Material Token Instances
Old Sections 00-15 Penn Treebank 773832
New 315 abstracts 103159
Tagger Overall Accuracy Number Token Instances Accuracy on Accuracy on
Unseen in Training Data Unseen Seen
Old 88.53% 14542 58.80% 95.53%
New 97.33% 4096 85.05% 98.02%
(Testing Material: 294 abstracts from the oncology domain, with 76324 token instances.)
Figure 4: Evaluation of Part-of-Speech Taggers
cations and 557 states, we obtained the following results:
Entity Precision Recall F-measure
Type 0.80 0.72 0.76
Location 0.85 0.73 0.79
State 0.90 0.80 0.85
Overall 0.86 0.75 0.80
An entity is considered correctly identified if and only
if it matches the human labeling by both category (type,
location or state) and span (from position a to position b).
At this stage we have not distinguished between initial
and final states.
While it is difficult to compare taggers that tag
different types of entities (e.g., (Friedman et al, 2001;
Gaizauskas et al, 2003)), CRFs have been utilized for
state-of-the-art results in NP-chunking and gene and
protein tagging (Sha and Pereira, 2003; McDonald
and Pereira, 2004) Currently, we are beginning to
investigate methods to identify relations over the varia-
tion components that are extracted using the entity tagger.
5 Conclusion
We have described here an integrated annotation ap-
proach for two areas of biomedical information extrac-
tion. We discussed several issues that have arisen for this
integration of annotation layers. Much effort has been
spent on the entity definitions and how they relate to the
higher-level concepts which are desired for extraction.
There are promising initial results for training taggers to
extract these entities.
Next steps in the project include: (1) continued anno-
tation of the layers we are currently doing, (2) integra-
tion of the level of predicate-argument annotation, and
(3) further development of the statistical taggers, includ-
ing taggers for identifying relations over their component
entities.
Acknowledgements
The project described in this paper is based at the In-
stitute for Research in Cognitive Science at the Uni-
versity of Pennsylvania and is supported by grant EIA-
0205448 from the National Science Foundation?s Infor-
mation Technology Research (ITR) program.
We would like to thank Aravind Joshi, Jeremy
Lacivita, Paula Matuszek, Tom Morton, and Fernando
Pereira for their comments.
References
M. Ashburner, C.A. Ball, J.A. Blake, D. Botstein, H. But-
ler, J.M. Cherry, A.P. Davis, K. Dolinski, S.S. Dwight,
J.T. Eppig, M.A. Harris, D.P. Hill, L. Issel-Tarver,
A. Kasarskis, S. Lewis, J.C. Matese, J.E. Richardson,
M. Ringwald, G.M. Rubin, and G. Sherlock. 2000.
Gene ontology: Tool for the unification of biology.
Nature Genetics, 25(1):25?29.
Ann Bies, Mark Ferguson, Karen Katz, and Robert Mac-
Intyre. 1995. Bracketing guidelines for Treebank II
Style, Penn Treebank Project. Tech report MS-CIS-
95-06, University of Pennsylvania, Philadelphia, PA.
Linguistic Data Consortium. 2002. Entity de-
tection and tracking - phase 1 - EDT and
metonymy annotation guidelines version 2.5
20021205. http://www.ldc.upenn.edu/Projects/ACE
/PHASE2/Annotation/.
Carol Friedman, Pauline Kra, Hong Yu, Michael
Krauthammer, and Andrey Rzhetsky. 2001. Genies: a
natural-language processing system for the extraction
of molecular pathways from journal articles. ISMB
(Supplement of Bioinformatics), pages 74?82.
R. Gaizauskas, G. Demetriou, P. Artymiuk, and P. Wil-
lett. 2003. Bioinformatics applications of information
extraction from journal articles. Journal of Bioinfor-
matics, 19(1):135?143.
Daniel Gildea and Martha Palmer. 2002. The Necessity
of Syntactic Parsing for Predicate Argument Recogni-
tion. In Proc. of ACL-2002.
U. Hahn, M. Romacker, and S. Schulz. 2002. Creating
knowledge repositories from biomedical reports: The
MEDSYNDIKATE text mining system. In Proceed-
ings of the Pacific Rim Symposium on Biocomputing,
pages 338?349.
Lynette Hirschman, Jong C. Park, Junichi Tsuji, Limsoon
Wong, and Cathy H. Wu. 2002. Accomplishments and
challenges in literature data mining for biology. Bioin-
formatics Review, 18(12):1553?1561.
Paul Kingsbury and Martha Palmer. 2002. From Tree-
bank to Propbank. In Proceedings of the 3rd Interna-
tional Conference on Language Resources and Evalu-
ation (LREC2002), Las Palmas, Spain.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc.
18th International Conf. on Machine Learning, pages
282?289. Morgan Kaufmann, San Francisco, CA.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Ryan McDonald and Fernando Pereira. 2004. Identify-
ing gene and protein mentions in text using conditional
random fields. In A Critical Assessment of Text Min-
ing Methods in Molecular Biology workshop. To be
presented.
Adam Meyers. October, 2003. Nombank. Talk at Auto-
matic Content Extraction (ACE) PI Meeting, Alexan-
dria, VA.
Scott Miller, David Stallard, Robert Bobrow, and Richard
Schwartz. 1996. A fully statistical approach to
natural language interfaces. In Aravind Joshi and
Martha Palmer, editors, Proceedings of the Thirty-
Fourth Annual Meeting of the Association for Compu-
tational Linguistics, pages 55?61, San Francisco. Mor-
gan Kaufmann Publishers.
Tomoko Ohta, Yuka Tateisi, Jin-Dong Kim, and Jun?ici
Tsuji. 2002. The GENIA corpus: An annotated corpus
in molecular biology domain. In Proceedings of the
10th International Conference on Intelligent Systems
for Molecular Biology.
J. Park, H. Kim, and J. Kim. 2001. Bidirectional in-
cremental parsing for automatic pathway identification
with combinatory categorial grammar. In Proceedings
of the Pacific Rim Symposium on Biocomputing, pages
396?407.
J. Pustejovsky, J. Castano, and J. Zhang. 2002. Robust
relational parsing over biomedical literature: Extract-
ing inhibit relations. In Proceedings of the Pacific Rim
Symposium on Biocomputing, pages 362?373.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In Proceeds of Human
Language Technology-NAACL 2003.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using predicate-argument struc-
tures for information extraction. In Proceedings of
ACL 2003, Sapporo, Japan.
Yuka Tateisi and Jun-ichi Tsujii. 2004. Part-of-speech
annotation of biology research abstracts. In Proceed-
ings of LREC04. To be presented.
A. Yakushiji, Y. Tateisi, Y. Miyao, and J. Tsujii. 2001.
Event extraction from biomedical papers using a full
parser. In Proceedings of the Pacific Rim Symposium
on Biocomputing, pages 408?419.
Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 21?28,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Parallel Entity and Treebank Annotation
Ann Bies
Linguistic Data Consortium
3600 Market Street, 810
Philadelphia, PA 19104
bies@ldc.upenn.edu
Seth Kulick
Institute for Research
in Cognitive Science
3401 Walnut Street
Suite 400A
Philadelphia, PA 19104
skulick@linc.cis.upenn.edu
Mark Mandel
Linguistic Data Consortium
3600 Market Street, 810
Philadelphia, PA 19104
mamandel@ldc.upenn.edu
Abstract
We describe a parallel annotation ap-
proach for PubMed abstracts. It includes
both entity/relation annotation and a tree-
bank containing syntactic structure, with a
goal of mapping entities to constituents in
the treebank. Crucial to this approach is a
modification of the Penn Treebank guide-
lines and the characterization of entities as
relation components, which allows the in-
tegration of the entity annotation with the
syntactic structure while retaining the ca-
pacity to annotate and extract more com-
plex events.
1 Introduction
A great deal of annotation effort for many different
corpora has been devoted to annotation for entities
and syntactic structure (treebanks). However, pre-
vious efforts at treebanking have largely been inde-
pendent of the constituency of entities, and previous
efforts at entity annotation have likewise been inde-
pendent of corresponding layers of syntactic struc-
ture. We describe here a corpus being developed
for biomedical information extraction with levels of
both entity annotation and treebank annotation, with
a goal that entities can be mapped to constituents in
the treebank.
We are collaborating with researchers in the Di-
vision of Oncology at The Children?s Hospital of
Philadelphia, for the purpose of automatically min-
ing the corpus of cancer literature for those as-
sociations that link specified variations in individ-
ual genes with known malignancies. In particular,
we are interested in extracting three entities (Gene,
Variation event, and Malignancy) in the following
relationship: Gene X with genomic Variation event
Y is correlated with Malignancy Z. For example,
WT1 is deleted in Wilms Tumor #5. In addition, Vari-
ation events are themselves relations, consisting of
entities representing different aspects of a Variation
event.
Mapping entities to treebank constituents is a de-
sirable goal since the entities can then be viewed
as semantic types associated with syntactic con-
stituents, and we expect that automated analyses of
these related levels will interact in a mutually rein-
forcing and beneficial way for development of sta-
tistical taggers.
In this paper we describe aspects of the entity
and treebank annotation that allow this mapping
to be largely successful. Potentially large enti-
ties that would otherwise cut across syntactic con-
stituents are decomposed into components of a re-
lation. While this is worthwhile by itself on con-
ceptual grounds for entity definition, and was in fact
not done for reasons of mapping to syntactic con-
stituents, it makes such a mapping easier. The tree-
bank annotation has been modified from the Penn
Treebank guidelines in various ways, such as greater
structure for prenominal modifiers. Again, while
this would have been done regardless of the map-
ping of entities, it does make such a mapping more
successful.
Previous work on integrating syntactic structure
with entity information, as well as relation infor-
21
mation, is described in (Miller et al, 2000). Our
work is in much the same spirit, although we do
not integrate relation annotation into the syntactic
trees. PubMed abstracts are quite different from the
newswire sources used in that earlier work, with sev-
eral consequences discussed throughout, such as the
use of discontinuous entities.
Section 2 discusses some of the main issues
around the development of the guidelines for en-
tity annotation, and Section 3 discusses some of the
changes that have been made for the treebank guide-
lines. Section 4 describes the annotation workflow
and the resulting merged representation. Section
5 evaluates the mapping between entities and con-
stituents, and Section 6 is the conclusion.
2 Guidelines for Entity Annotation
Here we give a summary of the main features of
our annotation guidelines. We have been influenced
in this by the annotation guidelines for the Auto-
matic Content Extraction (ACE) project (Consor-
tium, 2004).1 However, our source materials are
medical abstracts from PubMed2, and important dif-
ferences between the domains have required sig-
nificant changes and additions to many definitions,
guidelines, and procedures.
Most obviously, the vocabulary is very different.
Many of the tokens in our source texts are chemical
terms with a complex productive morphology, and a
certain number are unique in PubMed. Many oth-
ers are strings of notation, like S37F, often contain-
ing relevant entity references that must be isolated
(S, 37, and F). And even apart from these, we are
looking at a very different dialect of English from
that used by the Wall Street Journal and the Asso-
ciated Press. Annotation of English newswire re-
quires native English competency; entity annotation
of biomedical English requires a background in bi-
ology as well.
The entity instances in the text are also qualita-
tively different. Instead of individual pieces of the
physical or social universe ? Emanuel Sosa, the Eif-
fel Tower, the man in the yellow hat ? we have ab-
1Another source of influence is previous work in annota-
tion for biomedical information extraction, such as (Ohta et al,
2002). Space prevents adequate discussion of here of the differ-
ences.
2http://www.ncbi.nlm.nih.gov/entrez/
stractions, categories that are not to be confused with
their instantiations: neuroblastoma, K-ras (a gene),
codon 42.3 We are not currently annotating pronom-
inal or other forms of coreference.
2.1 Entities Annotated
2.1.1 Gene Entity
For the sake of this project the definition for
?Gene Entity? has two significant characteristics.
First, as just mentioned, ?Gene? refers to a concep-
tual entity as opposed to the specific manifestation
of a gene (e.g., not the ?K-ras? in some specific cell
in some individual, but an abstraction that cannot be
pointed to).
Second, ?Gene? refers to a composite entity as op-
posed to the strict biological definition. There are
often ambiguities in the usage of the entity names. I
is sometimes unclear as to whether the gene or pro-
tein is being referenced, and the same name can refer
to the gene or the protein at different locations in the
same document. In a similar way as the ACE project
allows ?geopolitical? entities to have different roles,
such as ?location? or ?organization?, we consider a
?Gene? to be a composite entity that can have differ-
ent roles throughout a document. Therefore, Gene
entity mentions can have types Gene-generic, Gene-
protein, and Gene-RNA.
2.1.2 Variation Events as Relations
As mentioned in the introduction, Variation
events are relations between entities representing
different aspects of a Variation; specifically, a Vari-
ation is a relationship between two or more of
the following entities: Type (e.g., point mutation,
translocation, or inversion), Location (e.g., codon
14, 1p36.1, or base pair 278), Original-State and
Altered-State (e.g., Thymine).
The entities as such are independent and uncon-
nected. We add a level of relation to annotate the
associations between them: For example, the text
fragment a single nucleotide substitution at codon
249, predicting a serine to cysteine amino acid sub-
stitution (S249C) contains the entities:
Variation-type substitution
Variation-location codon 249
3This domain shows no such clear distinction between Name
and Nominal mentions as in the texts covered by ACE.
22
Variation-state-original serine
Variation-state-altered cysteine
These entities are annotated individually but are also
collected into a single Variation relation.
It is also possible for a Variation relation to arise
from a more compact collection of entities. For ex-
ample, the text S249C consists of three entities col-
lected into a Variation relation:
Variation-location 249
Variation-state-original S
Variation-state-altered C
These four components represent the key ele-
ments necessary to describe any genomic variation
event. Variations are often underspecified in the lit-
erature. For example, the first relation above has
all four components while the second is missing
the Variation-type. Characterizing individual Varia-
tions as relations among such components provides
us with a great deal of flexibility.
The ?Gene? entities are analogous to the ACE
geopolitical entity, in that the second part of the en-
tity names (?-RNA?, ?-generic?,?-protein?) disam-
biguates the metonymy of the ?Gene?. The subtypes
of the Variation entities, in contrast, indicate differ-
ent kinds of entities in their own right, which can
also function as components of a Variation relation.
2.1.3 Malignancy
The Malignancy annotation guidelines were un-
der development during the annotation of the corpus
described here. While they have since been more
completely defined, they are not included as part of
the annotated files discussed here, and so are not fur-
ther discussed in this paper.
2.2 Discontinuous Entities
We have introduced a mechanism we call ?chain-
ing? to annotate discontinuous entities, which may
be more common in abstracts than in full text be-
cause of the pressure to reduce word count. For ex-
ample, in K- and N-ras there are two entities, K-ras
and N-ras, of which only the second is a solid block
of text. Our entity annotators are allowed to change
the tokenization if necessary to isolate the compo-
nents of K-ras:
text K- and N-ras
original tokenization [K-][and][N-ras]
Multiple Tokens
Entity Single Non-
Type Tokens chains Chains
Gene-generic 104 6 0
Gene-protein 921 349 6
Gene-RNA 1987 156 36
Var-
location 95 445 125
Var-
state-orig 151 5 0
Var-
state-altered 162 10 0
Var-type 235 271 1
Table 1: Entity Instances
modified tokenization
[K][-][and][N][-][ras]
entity annotation
1. K- ... ras (chain with separated to-
kens)
2. N-ras (contiguous tokens)
2.3 Entity Frequencies
Table 1 shows the number of instances of each of the
entity types in the 318 abstracts, discussed further
in Section 4, that have been both entity annotated
and treebanked. We separate the entities into single-
token and multiple-token categories since it is only
the multiple-token categories that raise an issue for
mapping constituents.
3 Treebank Annotation
The Penn Treebank II guidelines (Bies et al, 1995)
were followed as closely as possible, but the nature
of the biomedical corpus has made some changes
necessary or desirable. We have also taken this
opportunity to address several long-standing issues
with the original set of guidelines, with regard to NP
structure in particular. This has resulted in the intro-
duction of one new node label for sub-NP nominal
substrings (NML). One additional empty category
(*P*) has been introduced in order to improve the
match-up of chained entity categories with treebank
nodes. It is used as a placeholder to represent dis-
tributed modification in nominals and does not rep-
resent the trace of movement.
23
3.1 Tokenization/Part-of-Speech
We have also adopted several changes in word-level
tokenization, leading to a number of part-of-speech
and structural differences as well. Many hyphenated
words are now treated as separate tokens (New York
- based would be four tokens, for example). These
hyphens now have the part-of-speech tag HYPH. If
the separated prefix is a morphological unit that does
not exist as a free-standing word, it has the part-of-
speech tag AFX. With chemical names and scien-
tific notation in the biomedical corpus in particular,
spaces and punctuation may occur within a single
?token?, which will have a single POS tag.
3.2 Right-Branching Default
We assume a default binary right-branching struc-
ture under any NP and NML node. Each daughter
of the phrase (whether a single token or itself a con-
stituent node) is assumed to have scope over every-
thing to its right. This means that every daughter
also forms a constituent with everything to its right.
This assumption makes the annotation process for
multi-token nominals less complex and the resulting
trees more legible, but still allows us to readily de-
rive constituent nodes not explicitly represented. For
example, in
(NP (JJ primary) (NN liver)
(NN cancer))
we assume that ?liver cancer? is a constituent, and
that ?primary? has scope over it.
So, although we do not show the intermediate
nodes explicitly in our annotation, our assumed
structure for this NP could be derived as
(NP (JJ primary)
(newnode (NN liver)
(newnode (NN cancer))))
As discussed in Section 5, entities sometimes map
to such implicit constituents, and a node needs to
be added to make the constituent explicit so the the
entity can be mapped to it.
3.3 New Node Level for Non-Right-Branching:
NML
We use the NML node label to mark nominal sub-
constituents that do not follow the default binary
right-branching structure. Any two or more non-
final elements that form a constituent are bound to-
gether by NML.
(NP (NML (NN human)
(NN liver)
(NN tumor))
(NN analysis))
3.4 New Empty Category for Distributed
Readings within NP: *P*
As discussed in Section 2.2, discontinuous enti-
ties are annotated using the ?chaining? mechanism.
Analogously, we have introduced a placeholder,
*P*, for distributed material in the treebank. It is
used exclusively in coordinated nominal structures,
placed in coordinated elements that are missing ei-
ther a distributed head or a distributed premodifier.
In K- and N-ras, the coordinated premodifier K- is
missing the distributed head ras, so the placeholder
*P* is inserted after K- and coindexed with ras:
(NP (NP (NN K) (HYPH -)
(NML-1 (-NONE- *P*)))
(CC and)
(NP (NN N) (HYPH - )
(NML-1 (NN ras))))
This creates constituent nodes K-ras and N-ras
that align with the entities being represented by
chaining.4
4 Annotation Process
The annotation process comprises the following
steps: Paragraph and sentence annotation (includ-
ing the delimitation of irrelevant text such as au-
thor names); tokenization; entity annotation; part-
of-speech (POS) annotation; treebanking; merged
representation.
Entity annotation precedes POS annotation, since
the entity annotators often have to correct the tok-
enization, which affects the POS labels. For exam-
ple, nephro- and hepatocarcinoma refers to two en-
tities, nephrocarcinoma and hepatocarcinoma, and
so the entity annotator would split hepatocarcinoma
into two tokens, for chaining nephro and carcinoma
4In spite of the apparent similarity between *P* and right
node raising structures (*RNR*), they are not interchangeable
as the shared element often occurs to the left rather than the
right (e.g., codon 12 or 13 in Section 5.3).
24
(see Section 2.2). Since the entity annotators are not
qualified for POS annotation, doing POS annotation
after entity annotation allows the POS annotators to
annotate any such tokenization changes.
Treebank annotation uses the same tokenization
as for the corresponding entity file. Continuing the
above example, the treebank file would have sepa-
rate tokens for hepato and carcinoma. Note that this
would be the case even if we did not have the goal
of mapping entities to constituents. It arises from the
more minimal requirement of maintaining identical
tokenization in the treebank and entity files, and so
leads to changes in treebank annotation such as dis-
cussed in Section 3.4.
All of the annotation steps except entity annota-
tion use automated taggers (or a parser in the case of
treebanking),5 producing annotation that then gets
hand-corrected.
The use of the parser for producing a parse for
correction by the treebankers include a somewhat
unusual feature that arises from our parallel entity
and treebank annotation. The parser that we are us-
ing, (Bikel, 2004),6 allows prebracketing of parts
of the parser input, so that the parser will respect
the prebracketing. We use this ability to prebracket
entities, which can also help to disambiguate the
constituencies for prenominal modifiers, which can
often be unclear for annotators without a medical
background. For example, the input to the parser
might contain something like:
...(NN activation)
(IN of)
(PRP$ its)
(* (NN tyrosine)
(NN kinase) )
(NN activity)...
indicating by the (* ) that tyrosine kinase should
be a constituent. (It is a Gene-protein.)
Our first release of data, PennBioIE Release 0.9
(http://bioie.ldc.upenn.edu/
publications), contains 1157 oncology
PubMed abstracts, all annotated for entities and
POS, of which 318 have also been treebanked. The
website also contains full documentation for the
5Entity taggers have been developed (McDonald et al,
2004) but have not yet been integrated into the project.
6Available at http://www.cis.upenn.edu/~dbikel
/software.html#stat-parser
;sentence 4 Span:331..605
;In the present study, we screened for
;the K-ras exon 2 point mutations in a
;group of 87 gynecological neoplasms
;(82 endometrial carcinomas, four
;carcinomas of the uterine cervix and
;one uterine carcinosarcoma) using the
;non-isotopic PCR-SSCP-direct
;sequencing techniques.
;[373..378]:gene-rna:"K-ras"
;[379..385]:variation-location:"exon 2"
;[386..401]:variation-type:
"point mutations"
(SENT
(S
(PP (IN:[331..333] In)
(NP (DT:[334..337] the)
(JJ:[338..345] present)
(NN:[346..351] study)))
(,:[351..352] ,)
(NP-SBJ (PRP:[353..355] we))
(VP (VBD:[356..364] screened)
(PP-CLR (IN:[365..368] for)
(NP (DT:[369..372] the)
(NN:[373..378] K-ras)
(NML (NN:[379..383] exon)
(CD:[384..385] 2))
(NN:[386..391] point)
(NNS:[392..401] mutations)))
(PP (IN:[402..404] in)
(NP
(NP (DT:[405..406] a)
(NN:[408..413] group))
(PP (IN:[414..416] of)
(NP (CD:[417..419] 87)
(JJ:[420..433]
gynecological)
(NNS:[434..443]
neoplasms)
[...]
Figure 1: Example .mrg file
various annotation guidelines mentioned in this
paper.
4.1 Example of Merged Output
The 318 files that have been both treebanked and en-
tity annotated are also available in a merged ?.mrg?
format. The treebank and entity annotations are both
stand-off, referring to character spans in the same
source file, and we take advantage of this so that the
merged representation relates the entities and con-
stituents by these spans. Figure 1 shows a fragment
of one such .mrg file.
This .mrg file excerpt shows the text of sen-
tence 4 in the file, which spans the character offsets
331..605. Each entity is listed by span (which can in-
25
clude several tokens), entity type, and the text of the
entity. The treebank part is the same basic format as
the .mrg files from the Penn Treebank, except that
each terminal has the format
(POSTag:[from..to] terminal)
where [from..to] is that terminal?s span in the
source file.
The first entity listed, K-ras, is a Gene-RNA entity
with span [373..378], which corresponds to the
single token:
(NN:[373..378] K-ras)
The second entity, exon 2, is a Variation-location
with span [379..385], which corresponds to the
two tokens:
(NN:[379..383] exon)
(CD:[384..385] 2)
The third entity, point mutations, is a Variation-type
with span [386..401], which corresponds to the
two tokens:
(NN:[386..391] point)
(NNS:[392..401] mutations)
By including the terminal span information in the
treebank, we make explicit how the tokens that make
up the entities are treated in the treebank representa-
tion.
5 Entity-Constituent Mapping
One of our goals for the release of the corpus is to
allow users to choose how they wish to handle the
integration of the entity and treebank information.
By providing the corresponding spans for both as-
pects of the annotation, we provide the raw material
for any integrated approach.
We therefore do not attempt to force the entities
and constituents to line up perfectly. However, given
the parallel annotation just illustrated, we can an-
alyze how close we come to the ideal of the en-
tities behaving as semantic types on syntactic con-
stituents.
5.1 Mapping Categories
Leaving aside chains for the moment, we categorize
each entity/treebank mapping in one of three ways:
Exact match There is a node in the tree that yields
exactly the entity. For example, the entity exon 2 in
Figure 1
;[379..385]:variation-location:
"exon 2"
corresponds exactly to the NML node in Figure 1
(NML (NN:[379..383] exon)
(CD:[384..385] 2))
Missing node There is no node in the tree that
yields exactly that entity, but it is possible to add a
node to the tree that would yield the entity. A com-
mon reason for this is that the default right branch-
ing treebank annotation (Section 3.2) does not make
explicit the required node.
For example, the entity point mutations in Figure
1
;[386..401]:variation-type:
"point mutations"
does not correspond to a node in the relevant part of
the tree:
(NP (DT:[369..372] the)
(NN:[373..378] K-ras)
(NML (NN:[379..383] exon)
(CD:[384..385] 2))
(NN:[386..391] point)
(NNS:[392..401] mutations))
However, it is possible to insert a node into the tree
to yield exactly the entity:
(NP (DT:[369..372] the)
(NN:[373..378] K-ras)
(NML (NN:[379..383] exon)
(CD:[384..385] 2))
(newnode (NN:[386..391] point)
(NNS:[392..401]
mutations)))
Note that this node corresponds exactly to the im-
plicit constituency assumed by the right branching
rule. For our own internal research purposes we have
generated a version of the treebank with such nodes
added, although they are not in the current release.
Crossing The most troublesome case, in which the
entity does not match a node in the tree and also cuts
across constituent boundaries, so it is not even pos-
sible to add a node yielding the entity. Typically this
26
Exact Miss- Cross-
Entity Type Total Match ing ing
Gene-generic 6 4 1 1
Gene-protein 349 236 103 10
Gene-RNA 156 115 35 6
Var-
location 445 348 68 29
Var-
state-orig 5 3 1 1
Var-
state-altered 10 8 0 2
Var-type 271 123 142 6
Total 1242 837 350 55
Table 2: Matching Status of Non-Chained Multiple
Token Instances
is due to an entity containing text corresponding to
a prepositional phrase. For example, the sentence
One ER showed a G-to-T mutation in
the second position of codon 12
has the entity
[1280..1307]:variation-location:
"second position
of codon 12"
The relevant part of the corresponding tree is
(PP-LOC (IN:[1272..1274] in)
(NP
(NP (DT:[1276..1279] the)
(JJ:[1280..1286] second)
(NN:[1287..1295] position))
(PP (IN:[1296..1298] of)
(NP (NN:[1299..1304] codon)
(CD:[1305..1307] 12)))))
Due to the inclusion of the determiner in the NP
the second position, while it is absent from the en-
tity definition which does include the following PP,
it is not possible to add a node to the tree yielding
exactly second position of codon 12.7 It is possible
7The inclusion of the PP in an entity can be a problem for
the constituent mapping even aside from the determiner issue.
It is possible for the PP, such as of codon 12, to be followed by
another PP, such as in K-ras. Since all PPs are attached at the
same level, of codon 12 and in K-ras are sisters, and so, even
if the determiner was included in the entity name, there is no
constituent consisting of just the second position of codon 12.
However, in that case it is then possible to add a node yield-
ing the NP and first PP. A similar issue sometimes arises when
attempting to relate Propbank arguments to tree constituents.
Exact Not Exact
Entity Type Total Match Match
Gene-generic 0 0 0
Gene-protein 6 4 2
Gene-RNA 36 29 7
Var-
location 125 103 22
Var-
state-orig 0 0 0
Var-
state-altered 0 0 0
Var-type 1 0 1
Total 168 136 32
Table 3: Matching Status of Chained Multiple Token
Instances
to relax the requirements on exact match to include
the determiner.8
However, one of our initial goals in this investi-
gation was to determine whether this sort of limited
crossing is indeed a major source of the mapping
mismatches.
5.2 Overall Mapping Results
Table 2 is a breakdown of how well the (non-chain)
entities can be mapped to constituents. Here we are
concerned only with entities that consist of multiple
tokens, since single-token entities can of course map
directly to the relevant token.
The number of crossing cases is relatively small.
One reason for this is the use of relations for break-
ing potentially large entities into component parts,
since the component entities either already map to
an entity or can easily be made to do so by mak-
ing implicit constituents explicit to disambiguate the
tree structure. The crossing cases tend to be ones in
which the entities are in a sense a bit too ?big?, such
as including a prepositional phrase.9
8Another alternative would be to modify the treatment of
noun phrases and determiners in the treebank annotation to be
more akin to DPs. However, this has proved to be an impractical
addition to the annotation process.
9As discussed in Section 4, we are prebracketing entities in
the parses prepared for the treebankers to correct. There are two
possibilities for how the entities can therefore ever cross tree-
bank constituents: (1) the treebank annotation was done before
we started doing such prebracketing, so the treebank annotator
was not aware of the entities, or (2) the prebracketing was in-
27
5.3 Chained Entities
Table 3 shows the matching status of multiple token
instances that are also chains (and so were not in-
cluded in Table 2). The presence of chains is mostly
localized to certain entity types, and the mapping is
mostly successful. Variation-location contains many
of the chains due to the occurrences of phrases such
as codon 12 or 13, which map exactly to the corre-
sponding use of the *P* placeholder, such as:
(NP (NP
(NML-1 (NN codon))
(CD 12))
(CC or)
(NP
(NML-1 (-NONE- *P*))
(CD 13)))
Cases that do not map exactly are ones in which
the syntactic context does not permit the use of
the placeholder *P*. For example, the text spe-
cific codons (12, 13, and 61), has three discontin-
uous entities (codons..12, codons..13, codons..61),
but the parenthetical context does not permit using
the placeholder *P*:
(NP (JJ specific) (NNS codons)
(PRN (-LRB- -LRB-)
(NP (NP (CD 12))
(, ,)
(NP (CD 13))
(, ,) (CC and)
(NP (CD 61)))
(-RRB- -RRB-)))
and so this example contains three mismatches.
6 Conclusion
We have described here parallel syntactic and entity
annotation and how changes in the guidelines facil-
itate a mapping between entities and syntactic con-
stituents. Our main purpose in this paper has been to
investigate the success of this mapping. As Tables 2
and 3 show, once we make explicit the implicit right-
branching binary structure, only 6.2%10 of the enti-
ties cannot be mapped directly to a node in the tree.
It also appears likely that a significant percentage of
even the non-matching cases can match as well, with
a slight relaxation of the matching requirement (e.g.,
allowing entities to have an optional determiner).
deed done, but the treebank annotator could not abide by the
resulting tree and modified the parser output accordingly.
101410 total multiple token entities, both chained and non-
chained, with 87 cases that cannot be mapped (55 crossing, 32
chained non-exact match).
We view this in part as a successful experiment
illustrating how both linguistic content and entity
annotation can be enhanced by their interaction.
We expect this enhancement to be useful both for
biomedical information extraction in particular and
more generally for the development of statistical
systems that can take into account different levels
of annotation in a mutually beneficial way.
Acknowledgements
The project described in this paper is based at the In-
stitute for Research in Cognitive Science at the Uni-
versity of Pennsylvania and is supported by grant
EIA-0205448 from the National Science Founda-
tion?s Information Technology Research (ITR) pro-
gram. We would like to thank Yang Jin, Mark Liber-
man, Eric Pancoast, Colin Warner, Peter White, and
Scott Winters for their comments and assistance, as
well as the invaluable feedback of all the annotators
listed at http://bioie.ldc.upenn.edu/
index.jsp?page=aboutus.html.
References
Ann Bies, Mark Ferguson, Karen Katz, and Robert Mac-
Intyre. 1995. Bracketing guidelines for Treebank II
Style, Penn Treebank Project. Tech report MS-CIS-
95-06, University of Pennsylvania, Philadelphia, PA.
Daniel M. Bikel. 2004. On the Parameter Space of Lex-
icalized Statistical Parsing Models. Ph.D. thesis, De-
partment of Computer and Information Sciences, Uni-
versity of Pennsylvania.
Linguistic Data Consortium. 2004. Anno-
tation guidelines for entity detection and
tracking (edt), version 4.2.6 200400401.
http://www.ldc.upenn.edu/Projects/ACE/docs/
EnglishEDTV4-2-6.PDF.
Ryan McDonald, Scott Winters, Mark Mandel, Yang Jin,
Pete White, and Fernando Pereira. 2004. An entity
tagger for recognizing acquired genomic variations in
cancer literature. Bioinformatics, 22(20):3249?3251.
Scott Miller, Heidi Fox, Lance Ramshaw, and Ralph
Weischedel. 2000. A novel use of statistical parsing to
extract information from text. In 6th Applied Natural
Language Processing Conference.
Tomoko Ohta, Yuka Tateisi, Jin-Dong Kim, and Jun?ici
Tsuji. 2002. The GENIA corpus: An annotated cor-
pus in molecular biology domain. In Proceedings of
the 10th International Conference on Intelligent Sys-
tems for Molecular Biology.
28
Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora 2006, pages 70?77,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Issues in Synchronizing the English Treebank and PropBank 
 
Olga Babko-Malayaa, Ann Biesa, Ann Taylorb, Szuting Yia, Martha Palmerc,  
Mitch Marcusa, Seth Kulicka and Libin Shena 
aUniversity of Pennsylvania, bUniversity of York, cUniversity of Colorado 
{malayao,bies}@ldc.upenn.edu, {szuting,mitch,skulick,libin}@linc.cis.upenn.edu,
at9@york.ac.uk, Martha.Palmer@colorado.edu
 
Abstract 
The PropBank primarily adds semantic 
role labels to the syntactic constituents in 
the parsed trees of the Treebank. The 
goal is for automatic semantic role label-
ing to be able to use the domain of local-
ity of a predicate in order to find its ar-
guments. In principle, this is exactly what 
is wanted, but in practice the PropBank 
annotators often make choices that do not 
actually conform to the Treebank parses. 
As a result, the syntactic features ex-
tracted by automatic semantic role label-
ing systems are often inconsistent and 
contradictory. This paper discusses in de-
tail the types of mismatches between the 
syntactic bracketing and the semantic 
role labeling that can be found, and our 
plans for reconciling them. 
1 Introduction 
The PropBank corpus annotates the entire Penn 
Treebank with predicate argument structures by 
adding semantic role labels to the syntactic 
constituents of the Penn Treebank.  
Theoretically, it is straightforward for PropBank 
annotators to locate possible arguments based on 
the syntactic structure given by a parse tree, and 
mark the located constituent with its argument 
label. We would expect a one-to-one mapping 
between syntactic constituents and semantic 
arguments. However, in practice, PropBank 
annotators often make choices that do not 
actually conform to the Penn Treebank parses. 
The discrepancies between the PropBank and 
the Penn Treebank obstruct the study of the syn-
tax and semantics interface and pose an immedi-
ate problem to an automatic semantic role label-
ing system. A semantic role labeling system is 
trained on many syntactic features extracted from 
the parse trees, and the discrepancies make the 
training data inconsistent and contradictory. In 
this paper we discuss in detail the types of mis-
matches between the syntactic bracketing and the 
semantic role labeling that can be found, and our 
plans for reconciling them. We also investigate 
the sources of the disagreements, which types of 
disagreements can be resolved automatically, 
which types require manual adjudication, and for 
which types an agreement between syntactic and 
semantic representations cannot be reached. 
1.1 Treebank  
The Penn Treebank annotates text for syntactic 
structure, including syntactic argument structure 
and rough semantic information. Treebank anno-
tation involves two tasks: part-of-speech tagging 
and syntactic annotation. 
The first task is to provide a part-of-speech tag 
for every token. Particularly relevant for Prop-
Bank work, verbs in any form (active, passive, 
gerund, infinitive, etc.) are marked with a verbal 
part of speech (VBP, VBN, VBG, VB, etc.). 
(Marcus, et al 1993; Santorini 1990) 
The syntactic annotation task consists of 
marking constituent boundaries, inserting empty 
categories (traces of movement, PRO, pro), 
showing the relationships between constituents 
(argument/adjunct structures), and specifying a 
particular subset of adverbial roles. (Marcus, et 
al. 1994; Bies, et al 1995) 
Constituent boundaries are shown through 
syntactic node labels in the trees. In the simplest 
case, a node will contain an entire constituent, 
complete with any associated arguments or 
modifiers. However, in structures involving syn-
tactic movement, sub-constituents may be dis-
placed. In these cases, Treebank annotation 
represents the original position with a trace and 
shows the relationship as co-indexing. In (1) be-
low, for example, the direct object of entail is 
shown with the trace *T*, which is coindexed to 
the WHNP node of the question word what. 
 
(1) (SBARQ (WHNP-1 (WP What ))
(SQ (VBZ does )
(NP-SBJ (JJ industrial )
(NN emigration ))
(VP (VB entail)
(NP *T*-1)))
(. ?))
70
In (2), the relative clause modifying a journal-
ist has been separated from that NP by the prepo-
sitional phrase to al Riyadh, which is an argu-
ment of the verb sent. The position where the 
relative clause originated or ?belongs? is shown 
by the trace *ICH*, which is coindexed to the 
SBAR node containing the relative clause con-
stituent. 
 
(2)(S (NP-SBJ You)  
(VP sent
(NP (NP a journalist)
(SBAR *ICH*-2))
(PP-DIR to
(NP al Riyadh))
(SBAR-2
(WHNP-3 who)
(S (NP-SBJ *T*-3)
(VP served
(NP (NP the name)
(PP of
(NP Lebanon)))
(ADVP-MNR
magnificently))))))
 
Empty subjects which are not traces of move-
ment, such as PRO and pro, are shown as * (see 
the null subject of the infinite clause in (4) be-
low). These null subjects are coindexed with a 
governing NP if the syntax allows. The null sub-
ject of an infinitive clause complement to a noun 
is, however, not coindexed with another node in 
the tree in the syntax. This coindexing is shown 
as a semantic coindexing in the PropBank anno-
tation. 
The distinction between syntactic arguments 
and adjuncts of the verb or verb phrase is made 
through the use of functional dashtags rather than 
with a structural difference. Both arguments and 
adjuncts are children of the VP node. No distinc-
tion is made between VP-level modification and 
S-level modification. All constituents that appear 
before the verb are children of S and sisters of 
VP; all constituents that appear after the verb are 
children of VP.  
Syntactic arguments of the verb are NP-SBJ, 
NP (no dashtag), SBAR (either ?NOM-SBJ or no 
dashtag), S (either ?NOM-SBJ or no dashtag),  
-DTV, -CLR (closely/clearly related), -DIR with 
directional verbs. 
Adjuncts or modifiers of the verb or sentence 
are any constituent with any other adverbial 
dashtag, PP (no dashtag), ADVP (no dashtag). 
Adverbial constituents are marked with a more 
specific functional dashtag if they belong to one 
of the more specific types in the annotation sys-
tem (temporal ?TMP, locative ?LOC, manner  
?MNR, purpose ?PRP, etc.). 
Inside NPs, the argument/adjunct distinction is 
shown structurally. Argument constituents (S and 
SBAR only) are children of NP, sister to the head 
noun. Adjunct constituents are sister to the NP 
that contains the head noun, child of the NP that 
contains both:  
 
(NP (NP head)
(PP adjunct)) 
1.2 PropBank   
PropBank is an annotation of predicate-argument 
structures on top of syntactically parsed, or Tree-
banked, structures. (Palmer, et al 2005; Babko-
Malaya, 2005). More specifically, PropBank 
annotation involves three tasks: argument 
labeling, annotation of modifiers, and creating 
co-reference chains for empty categories.  
The first goal is to provide consistent argu-
ment labels across different syntactic realizations 
of the same verb, as in   
 
(3) [ARG0 John] broke [ARG1 the window]   
 [ARG1 The window] broke.  
 
As this example shows, semantic arguments 
are tagged with numbered argument labels, such 
as Arg0, Arg1, Arg2, where these labels are de-
fined on a verb by verb basis.  
The second task of the PropBank annotation 
involves assigning functional tags to all modifi-
ers of the verb, such as MNR (manner), LOC 
(locative), TMP (temporal), DIS (discourse con-
nectives), PRP (purpose) or DIR (direction) and 
others. 
And, finally, PropBank annotation involves 
finding antecedents for ?empty? arguments of the 
verbs, as in (4). The subject of the verb leave in 
this example is represented as an empty category 
[*] in Treebank. In PropBank, all empty catego-
ries which could be co-referred with a NP within 
the same sentence are linked in ?co-reference? 
chains:  
 
(4) I made a decision [*] to leave 
 
Rel:    leave,   
Arg0: [*] -> I 
 
As the following sections show, all three tasks 
of PropBank annotation result in structures 
which differ in certain respects from the corre-
sponding Treebank structures. Section 2 presents 
71
our approach to reconciling the differences be-
tween Treebank and PropBank with respect to 
the third task, which links empty categories with 
their antecedents. Section 3 introduces mis-
matches between syntactic constituency in Tree-
bank and PropBank. Mismatches between modi-
fier labels are not addressed in this paper and are 
left for future work. 
2 Coreference and syntactic chains  
PropBank chains include all syntactic chains 
(represented in the Treebank) plus other cases of 
nominal semantic coreference, including those  
in which the coreferring NP is not a syntactic 
antecedent. For example, according to PropBank 
guidelines, if a trace is coindexed with a NP in 
Treebank, then the chain should be reconstructed: 
 
(5) What-1 do you like [*T*-1]? 
 
Original PropBank annotation: 
Rel: like 
Arg0: you 
Arg1: [*T*] -> What 
 
Such chains usually include traces of A and A? 
movement and PRO for subject and object con-
trol. On the other hand, not all instances of PROs 
have syntactic antecedents. As the following ex-
ample illustrates, subjects of infinitival verbs and 
gerunds might have antecedents within the same 
sentence, which cannot be linked as a syntactic 
chain. 
 
(6) On the issue of abortion , Marshall Coleman 
wants  to take away your right  [*] to choose 
and give it to the politicians .  
 
ARG0:          [*] -> your 
REL:           choose 
 
Given that the goal of PropBank is to find all 
semantic arguments of the verbs, the links be-
tween empty categories and their coreferring NPs 
are important, independent of whether they are 
syntactically coindexed or not. In order to recon-
cile the differences between Treebank and Prop-
Bank annotations, we decided to revise Prop-
Bank annotation and view it as a 3 stage process. 
First, PropBank annotators should not recon-
struct syntactic chains, but rather tag empty cate-
gories as arguments. For example, under the new 
approach annotators would simply tag the trace 
as the Arg1 argument in (7): 
(7) What-1 do you like [*T*-1]? 
 
Revised PropBank annotation: 
Rel: like 
Arg0: you 
Arg1: [*T*]  
  
As the second stage, syntactic chains will be re-
constructed automatically, based on the 
coindexation provided by Treebank (note that the 
trace is coindexed with the NP What in (7)). And, 
finally, coreference annotation will be done on 
top of the resulting resource, with the goal of 
finding antecedents for the remaining empty 
categories, including empty subjects of infinitival 
verbs and gerunds.   
One of the advantages of this approach is that 
it allows us to distinguish different types of 
chains, such as syntactic chains (i.e., chains 
which are derived as the result of syntactic 
movement, or control coreference), direct 
coreference chains (as illustrated by the example 
in (6)), and semantic type links for other ?indi-
rect? types of links between an empty category 
and its antecedent.  
Syntactic chains are annotated in Treebank, 
and are reconstructed automatically in PropBank. 
The annotation of direct coreference chains is 
done manually on top of Treebank, and is re-
stricted to empty categories that are not 
coindexed with any NP in Treebank. And, finally, 
as we show next, a semantic type link is used for 
relative clauses and a coindex link for verbs of 
saying. 
A semantic type link is used when the antece-
dent and the empty category do not refer to the 
same entity, but do have a certain kind of rela-
tionship. For example, consider the relative 
clause in (8):  
 
(8) Answers that we?d like to have 
 
Treebank annotation: 
(NP (NP answers)
(SBAR (WHNP-6 which)
(S (NP-SBJ-3 we)
(VP 'd
(VP like
(S (NP-SBJ *-3)
(VP to
(VP have
(NP *T*-6)
))))))))
 
In Treebank, the object of the verb have is a trace, 
which is coindexed with the relative pronoun. In 
72
the original PropBank annotation, a further link 
is provided, which specifies the relative pronoun 
as being of ?semantic type? answers.  
 
(9) Original PropBank annotation: 
Arg1:    [NP *T*-6] -> which -> answers 
 rel:         have 
 Arg0:     [NP-SBJ *-3] -> we 
 
This additional link between which and answers 
is important for many applications that make use 
of preferences for semantic types of verb argu-
ments, such as Word Sense Disambiguation 
(Chen & Palmer 2005). In the new annotation 
scheme, annotators will first label traces as ar-
guments: 
 
(10) Revised PropBank annotation (stage 1): 
Rel:  have 
Arg1: [*T*-6]  
Arg0: [NP-SBJ *-3] 
 
As the next stage, the trace [*T*-6] will be 
linked to the relative pronoun automatically (in 
addition to the chain [NP-SBJ *-3] -> we being 
automatically reconstructed). As the third stage, 
PropBank annotators will link which to answers. 
However, this chain will be labeled as a ?seman-
tic type? to distinguish it from direct coreference 
chains and to indicate that there is no identity 
relation between the coindexed elements. 
Verbs of saying illustrate another case of links 
rather than coreference chains. In many sen-
tences with direct speech, the clause which intro-
duces a verb of saying is ?embedded? into the 
utterance. Syntactically this presents a problem 
for both Treebank and Propbank annotation. In 
Treebank, the original annotation style required a 
trace coindexed to the highest S node as the ar-
gument of the verb of saying, indicating syntactic 
movement. 
 
(11) Among other things, they said  [*T*-1] , Mr. 
Azoff would develop musical acts for a new 
record label . 
 
Treebank annotation: 
(S-1 (PP Among
(NP other things))
(PRN ,
(S (NP-SBJ they)
(VP said
(SBAR 0
(S *T*-1))))
,)
(NP-SBJ Mr. Azoff)
(VP would
(VP develop
(NP (NP musical acts)
(PP for
(NP a new record
label)))))
.)
In PropBank, the different pieces of the utterance, 
including the trace under the verb said, were 
concatenated 
 
(12) Original PropBank annotation: 
ARG1:      [ Among other things] [ Mr. 
Azoff] [ would develop musical acts for a 
new record label] [ [*T*-1]] 
ARG0:       they 
rel:        said 
 
Under the new approach, in stage one, Tree-
bank annotation will introduce not a trace of the 
S clause, but rather *?*, an empty category indi-
cating ellipsis. In stage three, PropBank annota-
tors will link this null element to the S node, but 
the resulting chain will not be viewed as  ?direct? 
coreference. A special tag will be used for this 
link, in order to distinguish it from other types of 
chains. 
 
(13) Revised PropBank  annotation: 
ARG1:      [*?*] (-> S) 
ARG0:       they 
rel:        said 
3 Differences in syntactic constituency  
3.1 Extractions of mismatches between 
PropBank and Treebank 
In order to make the necessary changes to both 
the Treebank and the PropBank, we have to first 
find all instances of mismatches. We have used 
two methods to do this: 1) examining the argu-
ment locations; 2) examining the discontinuous 
arguments. 
 
Argument Locations  In a parse tree which ex-
presses the syntactic structure of a sentence, a 
semantic argument occupies specific syntactic 
locations: it appears in a subject position, a verb 
complement location or an adjunct location. 
Relative to the predicate, its argument is either a 
sister node, or a sister node of the predicate?s 
ancestor. We extracted cases of PropBank argu-
ments which do not attach to the predicate spine, 
and filtered out VP coordination cases. For ex-
ample, the following case is a problematic one 
because the argument PP node is embedded too 
73
deeply in an NP node and hence it cannot find a 
connection with the main predicate verb lifted. 
This is an example of a PropBank annotation 
error. 
 
(14) (VP (VBD[rel] lifted) 
(NP us) )
(NP-EXT
(NP a good 12-inches)
(PP-LOC[ARGM-LOC] above
(NP the water level))))
 
However, the following case is not problem-
atic because we consider the ArgM PP to be a 
sister node of the predicate verb given the VP 
coordination structure:  
 
(15) (VP (VP (VB[rel] buy)  
(NP the basket of ? )
(PP in whichever market ?))
(CC and)
(VP (VBP sell)
(NP them)
(PP[ARGM] in the more
expensive market)))
 
Discontinuous Arguments happen when Prop-
Bank annotators need to concatenate several 
Treebank constituents to form an argument.  Dis-
continuous arguments often represent different 
opinions between PropBank and Treebank anno-
tators regarding the interpretations of the sen-
tence structure. 
For example, in the following case, the Prop-
Bank concatenates the NP and the PP to be the 
Arg1. In this case, the disagreement on PP at-
tachment is simply a Treebank annotation error. 
 
(16) The region lacks necessary mechanisms for 
handling the aid and accounting items. 
 
Treebank annotation: 
(VP lacks
(NP necessary mechanisms)
(PP for
(NP handing the aid?)))
 
PropBank annotation: 
REL: lacks 
Arg1: [NP necessary mechanisms][PP for 
handling the aid and accounting items] 
 
All of these examples have been classified into 
the following categories: (1) attachment ambi-
guities, (2) different policy decisions, and (3) 
cases where one-to-one mapping cannot be pre-
served. 
3.2 Attachment ambiguities  
Many cases of mismatches between Treebank 
and PropBank constituents are the result of am-
biguous interpretations. The most common ex-
amples are cases of modifier attachment ambi-
guities, including PP attachment. In cases of am-
biguous interpretations, we are trying to separate 
cases which can be resolved automatically from 
those which require manual adjudication. 
 
PP-Attachment  The most typical case of PP 
attachment annotation disagreement is shown in 
(17).  
 
(17) She wrote a letter for Mary. 
 
Treebank annotation: 
(VP wrote
(NP (NP a letter)
(PP for
(NP Mary))))
 
PropBank annotation: 
REL: write 
Arg1: a letter 
Arg2: for Mary 
 
In (17), the PP ?for Mary? is attached to the 
verb in PropBank and to the NP in Treebank. 
This disagreement may have been influenced by 
the set of roles of the verb ?write?, which in-
cludes a beneficiary as its argument.  
 
(18) Frameset write:  Arg0: writer 
   Arg1: thing written 
   Arg2: beneficiary 
 
Examples of this type cannot be automatically 
resolved and require manual adjudication. 
Adverb Attachment  Some cases of modifier 
attachment ambiguities, on the other hand, could 
be automatically resolved. Many cases of mis-
matches are of the type shown in (19), where a 
directional adverbial follows the verb. In Tree-
bank, this adverbial is analyzed as part of an 
ADVP which is the argument of the verb in 
question. However, in PropBank, it is annotated 
as a separate ArgM-DIR.  
(19) Everything is going back to Korea or Japan. 
 
 
74
Treebank annotation:  
(S (NP-SBJ (NN Everything) )
(VP (VBZ is)
(VP (VBG[rel] going)
(ADVP-DIR
(RB[ARGM-DIR] back)
(PP[ARG2] (TO to)
(NP (NNP Korea)
(CC and)
(NNP Japan)
))))) (. .))
 
Original PropBank annotation: 
Rel: going 
ArgM-DIR: back 
Arg2: to Korea or Japan 
 
For examples of this type, we have decided to 
automatically reconcile PropBank annotations to 
be consistent with Treebank, as shown in (20). 
 
(20) Revised PropBank annotation: 
Rel:  going 
Arg2: back to Korea or Japan 
3.3 Sentential complements 
Another area of significant mismatch between 
Treebank and PropBank annotation involves sen-
tential complements, both infinitival clauses and 
small clauses. In general, Treebank annotation 
allows many more verbs to take sentential com-
plements than PropBank annotation. 
For example, the Treebank annotation of the 
sentence in (21) gives the verb keep a sentential 
complement which has their markets active un-
der the S as the subject of the complement clause. 
PropBank annotation, on the other hand, does not 
mark the clause but rather labels each subcon-
stituent as a separate argument. 
 
(21)  ?keep their markets active 
 
Treebank annotation: 
(VP keep
(S (NP-SBJ their markets)
(ADJP-PRD active)))
 
PropBank annotation: 
REL: keep 
Arg1: their markets 
Arg2: active 
 
In Propbank, an important criterion for decid-
ing whether a verb takes an S argument, or de-
composes it into two arguments (usually tagged 
as Arg1 and Arg2) is based on the semantic in-
terpretation of the argument, e.g. whether the 
argument can be interpreted as an event or pro-
position. 
For example, causative verbs (e.g. make, get), 
verbs of perception (see, hear), and intensional 
verbs (want, need, believe), among others, are 
analyzed as taking an S clause, which is inter-
preted as an event in the case of causative verbs 
and verbs of perception, and as a proposition in 
the case of intensional verbs. On the other hand, 
?label? verbs (name, call, entitle, label, etc.), do 
not select for an event or proposition and are 
analyzed as having 3 arguments: Arg0, Arg1, 
and Arg2. 
Treebank criteria for distinguishing arguments, 
on the other hand, were based on syntactic 
considerations, which did not always match with 
Propbank. For example, in Treebank, evidence of 
the syntactic category of argument that a verb 
can take is used as part of the decision process 
about whether to allow the verb to take a small 
clause. Verbs that take finite or non-finite (verbal) 
clausal arguments, are also treated as taking 
small clauses. The verb find takes a finite clausal 
complement as in We found that the book was 
important and also a non-finite clausal comple-
ment as in We found the book to be important. 
Therefore, find is also treated as taking a small 
clause complement as in We found the book 
important.  
 
(22) (S (NP-SBJ We) 
(VP found
(S (NP-SBJ the book)
(ADJP-PRD important))))
 
The obligatory nature of the secondary predi-
cate in this construction also informed the deci-
sion to use a small clause with a verb like find. In 
(22), for example, important is an obligatory part 
of the sentence, and removing it makes the sen-
tence ungrammatical with this sense of find (?We 
found the book? can only be grammatical with a 
different sense of find, essentially ?We located 
the book?). 
With verbs that take infinitival clausal com-
plements, however, the distinction between a 
single S argument and an NP object together 
with an S argument is more difficult to make. 
The original Treebank policy was to follow the 
criteria and the list of verbs taking both an NP 
object and an infinitival S argument given in 
Quirk, et al (1985).  
Resultative constructions are frequently a 
source of mismatch between Treebank annota-
75
tion as a small clause and PropBank annotation 
with Arg1 and Arg2. Treebank treated a number 
of resultative as small clauses, although certain 
verbs received resultative structure annotation, 
such as the one in (23). 
 
(23) (S (NP-SBJ They) 
(VP painted
(NP-1 the apartment)
(S-CLR (NP-SBJ *-1)
(ADJP-PRD orange))))
 
In all the mismatches in the area of sentential 
complementation, Treebank policy tends to 
overgeneralize S-clauses, whereas Propbank 
leans toward breaking down clauses into separate 
arguments.  
This type of mismatch is being resolved on a 
verb-by-verb basis. Propbank will reanalyze 
some of the verbs (like consider and find), which 
have been analyzed as having 3 arguments, as 
taking an S argument. Treebank, on the other 
hand, will change the analysis of label verbs like 
call, from a small clause analysis to a structure 
with two complements. 
Our proposed structure for label verbs, for ex-
ample, is in (24). 
 
(24) (S (NP-SBJ[Arg0] his parents) 
(VP (VBD called)
(NP-1[Arg1] him)
(S-CLR[Arg2]
(NP-SBJ *-1)
(NP-PRD John))))
 
This structure will accommodate both Treebank 
and PropBank requirements for label verbs. 
4 Where Syntax and Semantics do not 
match  
Finally, there are some examples where the dif-
ferences seem to be impossible to resolve with-
out sacrificing some important features of Prop-
Bank or Treebank annotation. 
4.1 Phrasal verbs   
PropBank has around 550 phrasal verbs like 
keep up, touch on, used to and others, which are 
analyzed as separate predicates in PropBank. 
These verbs have their own set of semantic roles, 
which is different from the set of roles of the cor-
responding ?non-phrasal? verbs, and therefore 
they require a separate PropBank entry. In Tree-
bank, on the other hand, phrasal verbs are not 
distinguished. If the second part of the phrasal 
verb is labeled as a verb+particle combination in 
the Treebank, the PropBank annotators concate-
nate it with the verb as the REL. If Treebank la-
bels the second part of the ?phrasal verb? as part 
of a prepositional phrase, there is no way to re-
solve the inconsistency.   
 
(25) But Japanese institutional investors are used 
to quarterly or semiannual payments on their in-
vestments, so ?  
 
Treebank annotation: 
(VBN used)
(PP (TO to)
(NP quarterly or ?
on their investments))
 
PropBank annotation: 
      Arg1: quarterly or ? on their investments 
 Rel: used to (?used to? is a separate predi-
cate in PropBank) 
4.2 Conjunction  
In PropBank, conjoined NPs and clauses are 
usually analyzed as one argument, parallel to 
Treebank. For example, in John and Mary came, 
the NP John and Mary is a constituent in Tree-
bank and it is also marked as Arg0 in PropBank. 
However, there are a few cases where one of the 
conjuncts is modified, and PropBank policy is to 
mark these modifiers as ArgMs. For example, in 
the following NP, the temporal ArgM now modi-
fies a verb, but it only applies to the second con-
junct.  
 
(26) 
(NP (NNP Richard)
(NNP Thornburgh) )
(, ,)
(SBAR
(WHNP-164 (WP who))
(S
(NP-SBJ-1 (-NONE- *T*-164))
(VP
(VBD went)
(PRT (RP on) )
(S
(NP-SBJ (-NONE- *-1))
(VP (TO to)
(VP (VB[rel] become)
(NP-PRD
(NP[ARG2]
(NP (NN governor))
(PP (IN of)
(NP
(NNP
Pennsylvania))))
76
(CC and)
(PRN (, ,)
(ADVP-TMP (RB now))
(, ,) )
(NP[ARG2] (NNP U.S.)
(NNP Attorney)
(NNP General))
)))))))
 
In PropBank, cases like this can be decom-
posed into two propositions: 
 
(27) Prop1:      rel: become    
                Arg1: attorney general         
                Arg0: [-NONE- *-1]                       
         
   Prop2: rel:  become    
  ArgM-TMP: now   
  Arg0: [-NONE- *-1] 
Arg1: a governor               
 
In Treebank, the conjoined NP is necessarily 
analyzed as one constituent. In order to maintain 
the one-to-one mapping between PropBank and 
Treebank, PropBank annotation would have to 
be revised in order to allow the sentence to have 
one proposition with a conjoined phrase as an 
argument. Fortunately, these types of cases do 
not occur frequently in the corpus. 
4.3 Gapping 
Another place where the one-to-one mapping 
is difficult to preserve is with gapping construc-
tions. Treebank annotation does not annotate the 
gap, given that gaps might correspond to differ-
ent syntactic categories or may not even be a 
constituent. The policy of Treebank, therefore, is 
simply to provide a coindexation link between 
the corresponding constituents:  
 
(28) Mary-1 likes chocolates-2 and  
 Jane=1 ? flowers=2 
 
This policy obviously presents a problem for 
one-to-one mapping, since Propbank annotators 
tag Jane and flowers as the arguments of an im-
plied second likes relation, which is not present 
in the sentence. 
5 Summary 
In this paper we have considered several types 
of mismatches between the annotations of the 
English Treebank and the PropBank: coreference 
and syntactic chains, differences in syntactic 
constituency, and cases in which syntax and se-
mantics do not match. We have found that for the 
most part, such mismatches arise because Tree-
bank decisions are based primarily  on syntactic 
considerations while PropBank decisions give 
more weight  to semantic representation.. 
In order to reconcile these differences we have 
revised the annotation policies of both the Prop-
Bank and Treebank in appropriate ways. A 
fourth source of mismatches is simply annotation 
error in either the Treebank or PropBank. Look-
ing at the mismatches in general has allowed us 
to find these errors, and will facilitate their cor-
rection.  
References 
Olga Babko-Malaya. 2005. PropBank Annotation 
Guidelines. http://www.cis.upenn.edu/~mpalmer/ 
project_pages/PBguidelines.pdf 
Ann Bies, Mark Ferguson, Karen Katz, Robert Mac-
Intyre. 1995. Bracketing Guidelines for Treebank 
II Style. Penn Treebank Project, University of 
Pennsylvania, Department of Computer and Infor-
mation Science Technical Report MS-CIS-95-06. 
Jinying Chen and Martha Palmer. 2005. Towards Ro-
bust High Performance Word Sense Disambigua-
tion of English Verbs Using Rich Linguistic Fea-
tures. In Proceedings of the 2nd International Joint 
Conference on Natural Language Processing, 
IJCNLP2005, pp. 933-944. Oct. 11-13, Jeju Island, 
Republic of Korea. 
M. Marcus, G. Kim, M. Marcinkiewicz, R. MacIn-
tyre, A. Bies, M. Ferguson, K. Katz & B. Schas-
berger, 1994. The Penn Treebank: Annotating 
predicate argument structure. Proceedings of the 
Human Language Technology Workshop, San 
Francisco. 
M. Marcus, B. Santorini and M.A. Marcinkiewicz, 
1993. Building a large annotated corpus of English: 
the Penn Treebank. Computational Linguistics. 
Martha Palmer, Dan Gildea, and Paul Kingsbury. 
2005. The proposition bank: An annotated corpus 
of semantic roles. Computational Linguistics, 
31(1). 
R. Quirk, S. Greenbaum, G. Leech and J. Svartvik. 
1985. A Comprehensive Grammar of the English 
Language. Longman, London. 
B. Santorini. 1990. Part-of-speech tagging guidelines 
for the Penn Treebank Project. University of Penn-
sylvania, Department of Computer and Information 
Science Technical Report MS-CIS-90-47. 
77
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 661?664,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
A Treebank Query System Based on an Extracted Tree Grammar
Seth Kulick and Ann Bies
Linguistic Data Consortium
University of Pennsylvania
3600 Market St., Suite 810
Philadelphia, PA 19104
{skulick,bies}@ldc.upenn.edu
Abstract
Recent work has proposed the use of an ex-
tracted tree grammar as the basis for treebank
analysis and search queries, in which queries
are stated over the elementary trees, which are
small chunks of syntactic structure. However,
this work was lacking in two crucial ways.
First, it did not allow for including lexical
properties of tokens in the search. Second,
it did not allow for using the derivation tree
in the search, describing how the elementary
trees are connected together. In this work we
describe an implementation that overcomes
these problems.
1 Introduction
(Kulick and Bies, 2009) describe the need for tree-
bank search that compares two sets of trees over the
same tokens. Their motivation is the problem of
comparing different annotations of the same data,
such as with inter-annotator agreement evaluation
during corpus construction. The typical need is to
recognize which annotation decisions the annotators
are disagreeing on. This is similar to the problem of
determining where the gold trees and parser output
differ, which can also be viewed as two annotations
of the same data.
As they point out, for this purpose it would be use-
ful to be able to state queries in a way that relates to
the decisions that annotators actually make, or that
a parser mimics. They provide examples suggesting
that (parent, head, sister) relations as in e.g. (Collins,
2003) are not sufficient, and that what is needed is
the ability to state queries in terms of small chunks
of syntactic structure.
Their solution is to use an extracted tree gram-
mar, inspired by Tree Adjoining Grammar (Joshi
and Schabes, 1997). The ?elementary trees? of the
TAG-like grammar become the objects on which
queries can be stated. They demonstrate how the
?lexicalization? property of the grammar, in which
each elementary tree is associated with one or more
token, allows for the the queries to be carried out in
parallel across the two sets of trees.
However, the work was lacking in two crucial
ways. First, it did not allow for including lexical
properties of a token, such as its Part-of-Speech tag,
together with the elementary tree search. This made
it impossible to formulate such queries as ?find all
ADVP elementary trees for which the head of the
tree is a NOUN NUM?. Even more seriously, there
was no way to search over the ?derivation tree?,
which encodes how the extracted elementary trees
combine together to create the original tree. This
made it impossible to carry out searches such as
?find all verb frames with a PP-LOC modifying it?,
and in general to search for the crucial question of
where annotators disagree on attachment decisions.
In this paper we describe how we have solved
these two problems.
2 Tree Extraction
Following (Kulick and Bies, 2009), we draw our ex-
amples from the Arabic Treebank1 For our gram-
1Part 3, v3.1 - Linguistic Data Consortium LDC2008E22.
Also, we use the Buckwalter Arabic transliteration scheme
http://www.qamus.org/transliteration.htm.
661
SVP
PV
tHTmt
crashed
I

?

?

m2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 305?314,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Using Supertags and Encoded Annotation Principles for Improved
Dependency to Phrase Structure Conversion
Seth Kulick and Ann Bies and Justin Mott
Linguistic Data Consortium
University of Pennsylvania
Philadelphia, PA 19104
{skulick,bies,jmott}@ldc.upenn.edu
Abstract
We investigate the problem of automatically
converting from a dependency representa-
tion to a phrase structure representation, a
key aspect of understanding the relationship
between these two representations for NLP
work. We implement a new approach to this
problem, based on a small number of su-
pertags, along with an encoding of some of
the underlying principles of the Penn Tree-
bank guidelines. The resulting system signifi-
cantly outperforms previous work in such au-
tomatic conversion. We also achieve compara-
ble results to a system using a phrase-structure
parser for the conversion. A comparison with
our system using either the part-of-speech tags
or the supertags provides some indication of
what the parser is contributing.
1 Introduction and Motivation
Recent years have seen a significant increase in
interest in dependency treebanks and dependency
parsing. Since the standard training and test set for
English parsing is a phrase structure (PS) treebank,
the Penn Treebank (PTB) (Marcus et al, 1993; Mar-
cus et al, 1994), the usual approach is to convert this
to a dependency structure (DS) treebank, by means
of various heuristics for identifying heads in a PS
tree. The resulting DS representation is then used
for training and parsing, with results reported on the
DS representation.
Our goal in this paper is to go in the reverse di-
rection, from the DS to PS representation, by find-
ing a minimal DS representation from which we can
use an approximate version of the principles of the
PTB guidelines to reconstruct the PS. Work in this
conversion direction is somewhat less studied (Xia
et al, 2009; Xia and Palmer, 2001), but it is still
an important topic for a number of reasons. First,
because both DS and PS treebanks are of current in-
terest, there is an increasing effort made to create
multi-representational treebank resources with both
DS and PS available from the beginning, without a
loss of information in either direction (Xia et al,
2009). Second, it is sometimes the case that it is
convenient to do annotation in a dependency repre-
sentation (e.g., if the annotators are already famil-
iar with such a representation), though the treebank
will in final form be either phrase-structure or multi-
representational (Xia et al, 2009).
However, our concern is somewhat different. We
are specifically interested in experimenting with de-
pendency parsing of Arabic as a step in the annota-
tion of the Arabic Treebank, which is a phrase struc-
ture treebank (Maamouri et al, 2011). Although we
currently use a phrase structure parser in this annota-
tion pipeline, there are advantages to the flexibility
of being able to experiment with advances in pars-
ing technology for dependency parsing. We would
like to parse with a dependency representation of the
data, and then convert the parser output to a phrase
structure representation so that it can feed into the
annotation pipeline. Therefore, in order to make use
of dependency parsers, we need a conversion from
dependency to phrase structure with very high accu-
racy, which is the goal of this paper.
While one of our underlying concerns is DS to
PS conversion for Arabic, we are first focusing on
305
a conversion routine for the English PTB because it
is so well-established and the results are easier to
interpret. The intent is then to transfer this conver-
sion algorithm work to the Arabic treebank as well.
We expect this to be successful because the ATB has
some fundamental similarities to the PTB in spite of
the language difference (Maamouri and Bies, 2004).
As mentioned above, one goal in our DS to PS
conversion work is to base it on a minimal DS rep-
resentation. By ?minimal?, we mean that it does
not include information that is redundant, together
with our conversion code, with the implicit informa-
tion in the dependency structure itself. As discussed
more in Section 2.1, we aim to make our dependency
representation simpler than ?hybrid? representations
such as Johansson and Nugues (2007). The rea-
son for our interest in this minimal representation
is parsing. We do not want to require the parser to
recover such a complex dependency representations,
when it is, in fact, unnecessary, as we believe our ap-
proach shows. The benefit of this approach can only
be seen when this line of work is extended to ex-
periments with parsing and Arabic conversion. The
work described here is just the first step in this pro-
cess.
A conversion scheme, such as ours, necessarily
relies on some details of the annotation content in
the DS and PS representations, and so our algorithm
is not an algorithm designed to take as input any ar-
bitrary DS representation. However, the fundamen-
tals of our dependency representation are not radi-
cally different than others - e.g. we make an auxil-
iary verb the child of the main verb, instead of the
other way, but such choices can be adjusted for in
the conversion.
To evaluate the success of this conversion algo-
rithm, we follow the same evaluation procedure as
Xia et al (2009) and Xia and Palmer (2001). We
convert the PTB to a DS, and then use our algorithm
to convert the DS back to a PS representation. The
original PS and the converted-from-DS PS are then
compared, in exactly the same way as parser output
is compared with the original (gold) tree. We will
show that our results in this area are a significant
improvement above previous efforts.
A key aspect of this work is that our DS-to-PS
conversion encodes many of the properties of the
PTB annotation guidelines (Bies et al, 1995), both
globally and for specific XP projections. The PTB
guidelines are built upon broad decisions about PS
representation that provide an overall framework
and cohesion for the details of the PS trees. To
implement these underlying principles of the guide-
lines, we defined a set of 30 ?supertags? that indi-
cate how a lexical item can project in the syntac-
tic structure, allowing us to specify these principles.
We describe these as supertags because of a concep-
tual similarity to the supertagging work in the Tree
Adjoining Grammar (TAG) tradition (Bangalore and
Joshi, 2010), although ours is far smaller than a typ-
ical supertag set, and indeed is actually smaller than
the PTB POS tag set.
Our DS-to-PS code is based on this set of su-
pertags, and can be run using either the supertags
created from the gold POS tags, or using the POS
tags, together with the dependency structure to first
(imperfectly) derive the supertags, and then proceed
with the conversion. This choice of starting point al-
lows us to measure the impact of POS tag complex-
ities on the DS-to-PS conversion, which provides an
interesting insight on what a phrase structure parser
contributes in addition to this sort of automated DS-
to-PS conversion, as discussed in Section 4.
We have chosen this approach of encoding under-
lying principles of the PTB guidelines for two rea-
sons. First, these principles are non-statistical, and
thus we felt it would let us tease apart the contri-
bution of the frequency information relating, e.g.,
heads, on the one hand, and the basic notions of
phrase structure on the other. The second reason is
that it was quite easy to implement these principles.
We did not attempt a complete examination of every
possible rule in Bies et al (1995), but rather just se-
lected the most obvious ones. As we will see in Sec-
tion 4.2, our results indeed are sometimes hurt by
such lack of thoroughness, although in future work
we will make this more complete.
2 Overview and Example
Figures 1-4 provide a running example of the four
steps in the process. Figure 1 is the original tree
from the Penn Treebank. Figures 2 and 3 illustrate
the two-step process of creating the dependency rep-
resentation, and Figure 4 shows the conversion back
to phrase structure.
306
SADVP
RB
Aside
PP
IN
from
NP
NNP
GM
NP-SBJ
JJ
other
NN
car
NNS
makers
VP
VBD
posted
NP
ADJP
RB
generally
VBN
mixed
NNS
results
Figure 1: Penn Treebank tree
postedVP
S
AsideADVP
fromPP
GMNP-OBJ
makersNP-SBJother car
resultsNP-OBJ
generally
mixedADJP
Figure 2: Tree Insertion Grammar decomposition of Figure
1
VBD/P VP
posted
RB/P ADVP
Aside
IN/P PP
from
NNP/P NP-OBJ
GM
NNS/P NP-SBJ
makers
JJ/P ADJP
other
NN/P PRENOM
car
NNS/P NP-OBJ
results
VBN/P ADJP
mixed
RB/P ADVP
generally
Figure 3: Dependency representation derived from TIG de-
composition in Figure 2
S
ADVP
RB
Aside
PP
IN
from
NP-OBJ
NNP
GM
NP-SBJ
JJ
other
NN
car
NNS
makers
VP
VBD
posted
NP-OBJ
ADJP
RB
generally
VBN
mixed
NNS
results
Figure 4: Conversion of dependency representation in Fig-
ure 3 back to phrase structure.
2.1 Creation of Dependency Representation
The creation of the dependency representation is
similar in basic aspects to many other approaches, in
that we utilize some basic assumptions about head
relations to decompose the full tree into smaller
units. However, we first decompose the original
trees into a Tree Insertion Grammar representation
(Chiang, 2003), utilizing tree substitution and sister
adjunction. We refer the reader to Chiang (2003) for
details of these operations, and instead focus on the
fact that the TIG derivation tree in Figure 2 parti-
tions the phrase structure representation in Figure 1
into smaller units, called elementary trees. We leave
out the POS tags in Figure 2 to avoid clutter.
The creation of the dependency representation is
structurally a simple rewrite of the TIG derivation,
taking the word associated with each elementary tree
and using it as a node in the dependency tree. In
this way, the dependency representation in Figure 3
follows immediately from Figure 2.
However, in addition, we utilize the TIG deriva-
tion tree and the structures of the elementary trees to
create a supertag (in the sense discussed in Section
1) for each word. For example, aside heads an ele-
mentary tree that projects to ADVP, so it is assigned
the supertag P ADVP in Figure 3, meaning that it
projects to ADVP. We label each node in Figure 3
with both its POS tag and supertag, so in this case
the node for aside has RB/P ADVP.
There are two typical cases that are not so
straightforward. The first concerns elementary trees
with more than one level of projection, such as that
for the verb, posted, which has two levels of pro-
jection, S and VP. In such cases we base the supertag
only on the immediate parent of the word. For ex-
ample, in this case the supertag for posted is P VP,
rather than P S. As will be seen in Section 3.2, our
perspective is that the local context of the depen-
dency tree will provide the necessary disambigua-
tion as to what node is above the VP.
307
Projection Type Supertag
NP P NP
ADJP P ADJP
ADVP P ADVP
PP P PP, P WHPP
S,SINV,SQ P VP
QP,NP,QP-NP,QP-ADJP P QP
WHNP P WHNP
default P WHADVP, P INTJ, P PRT, P LST
none P AUX, P PRENOM, P DET, P COMMA, P PERIOD, P CC, P COMP,
P POS, P PRP$, P BACKDQUOTE, P DQUOTE, P COLON, P DOLLAR,
P LRB, P RB, P PDT, P SYM, P FW, P POUND
Table 1: 30 supertags handled by 14 projection types. The ambiguity in some, such as P VP projecting as S, SINV,
SQ is handled by an examination of the dependency structure.
The second non-straightforward case1 is that of
degenerate elementary trees, in which the ?tree?
is just the word itself, as for other, car, and
generally. In such cases we default the supertag
based on the original POS tag, and in some cases, the
tree configuration. For example, a word with the JJ
tag, such as other, would get the supertag P ADJP,
with the RB tag such as generally the supertag
P ADVP. We assign prenominal nouns such as car
here the tag P PRENOM.
Generating supertags in this way is a convenient
way to correct some of the POS tag errors in the PTB
(Manning, 2011). For example, if that has the (in-
correct) tag DT in the complementizer position, it
still receives the new POS tag P COMP.
This procedure results in a set of 30 supertags, and
Table 1 shows how they are partitioned into 14 pro-
jection types. These supertags and projection types
are the basis of our DS-to-PS conversion, as dis-
cussed further in Section 2.2.
We note here a brief comparison with earlier work
on ?hybrid? representations, which encode a PS rep-
resentation inside a DS one, in order to convert from
the latter to the former. (Hall and Nivre, 2008; Jo-
han Hall and Nilsson, 2007; Johansson and Nugues,
2007). Our goal is very different. Instead of en-
1There are other details not discussed here. For example, we
do not automatically assign a P NP supertag to the head child
of an NP, since such a head can legitimately be, e.g, a JJ, in
which case we make the supertag P ADJP, on the reasoning that
it would be encoding ?too much? to treat it as P NP. Instead, we
rely on the DS and such labels as SBJ or OBJ to determine when
to project it as NP in the converted PS.
coding the phrase structure in the dependency tree
via complex tags such as SBARQ in Johansson and
Nugues (2007), we use a minimal representation and
rely on our encoding of the general principles of
PTB phrase structure to carry much of the weight.
While supertags such as P VP may appear to encode
some of the structure, their primary role is as an in-
termediate link between the POS tags and the phrase
structure conversion. The created supertags are not
in fact necessary for this conversion. As we will see
in the following sections, we convert from DS to PS
using either just the original POS tags, or with our
created supertags.
We also include five labels in the dependency rep-
resentation: SBJ, OBJ, PRN, COORD CONJ, APP.
The example dependency tree in Figure 3 includes
instances of the SBJ and OBJ labels, in italics on
the node instead of the edges, for convenience. The
SBJ label is of course already a function tag in the
PTB. We process the PTB when creating the TIG
decomposition to add an OBJ tag, as well basing the
PRN label on the occurrence of the PRN node. We
also use heuristics to identify cases of coordination
and apposition, resulting in the COORD CONJ and
APP tags. The reasons for including these labels is
that they prove useful in the conversion to phrase
structure, as illustrated in some of the examples be-
low.
Before moving on to the dependency-to-phrase-
stucture conversion, we end this section with a com-
ment on the role of function tags and empty cate-
gories. The PTB makes use of function tags to in-
308
dicate certain syntactic and semantic information,
and of empty categories (and co-indexing) for a
more complete and accurate syntactic representa-
tion. There is some overlap between the five la-
bels we use, as just described, and the PTB func-
tion tags, but in general we do not encode the full
range of function tags in our representation, saving
this for future work. More significantly, we also
do not include empty categories and associated co-
indexing, which has the consequence that the depen-
dency trees are projective.
The reason we have not included these aspects in
our representation and conversion yet is that we are
focused here first on the evaluation for comparison
with previous work, and the basis for this previous
work is the usual evalb program (Sekine and Collins,
2008), which ignores function tags and empty cate-
gories. We return to this issue in the conclusion.
2.2 From Dependency to Phrase Structure
There are two key aspects to the conversion from de-
pendency to phrase structure. (1) We encode general
conventions about annotation that are used through-
out the annotation guidelines for the PTB. A com-
mon example is that of the ?single-word? rule, in
which a constituent consisting of just a single word
is reduced to just that word, without the constituent
bracketing, in many cases. (2) We use the set of su-
pertags as the basis for defining projection-specific
rules for how to attach children on the left or right of
the head, in many cases utilizing the supertag names
that we include to determine the specific attachment.
For example, the leaf GM in Figure 3 has the su-
pertag P NP (with the label OBJ), so heading a NP
projection, (NP GM). Its parent node, from, has
the supertag P PP, indicating that it heads a PP pro-
jection, and so attaches the (NP GM) as a sister of
from. It does not reduce it down as a single word,
because the encoding of the PP projection specifies
that it does not do so for children on its right.
A more substantial case is that of the NP other
car makers. Here the head noun, makers,
has the supertag P NP, and so projects as an NP.
Its first child, other, has the supertag P ADJP,
and so projects as an ADJP, resulting in (ADJP
other). The second child, car, has the supertag
P PRENOM (prenominal), and so does not project
at all. When the NP projection for makers is as-
sembled, it applies the ?single-word? constraint to
children on its left (as encoded in the definition
of the NP projection), thus stripping the ADJP off
of other, resulting in the desired flat NP other
car makers. Likewise, the ADVP projection for
generally is stripped off before it is attached as
a left sister of the ADJP projection mixed. The
encoding of a VP projection specifies that it must
project above VP if it is the root of the tree, and so
the VP projection for posted projects to S (by de-
fault).
In this way we can see that encoding some of
the general characteristics of the annotation guide-
lines allows the particular details of the PTB phrase-
structure representation to be created from the less-
specific dependency representation.
3 Some Further Examples
3.1 QP Projection or Reduction
As mentioned in Section 2.2, the ?single word? con-
vention is implemented in the conversion to PS, as
was the case with other in the previous section.
The projection associated with P QP has a slight
twist to this principle, because of the nature of some
of the financialspeak in the PTB. In particular, the
dollar sign is treated as a displaced word and is
therefore not counted, in a QP constituent, as a token
for purposes of the ?single token? rule.
For example, (1abc) in Figure 5 illustrates a case
where the QP structure projects to an NP node as
well. (1a) is the original PTB PS tree, and (1b) is
the DS representation. Note that billion heads
the about $ 9 billion subtree, with the su-
pertag P QP and the label OBJ.2 Because it has more
than one child in addition to the $, it is converted to
phrase structure as a QP under an NP, implying the
empty *U*, although we do not actually put it in.
In contrast, (2abc) is a case in which the QP node
is not generated. 100 is the head of the phrase $ 100
*U* in the PTB PS (a), as shown in the dependency
structure (b). However, because it only has one child
in addition to the $, no additional QP node is cre-
ated in the phrase structure representation in (c). We
stress that the presence of the QP in (1a) and its ab-
2A good case can be made that in fact $ should be the daugh-
ter of to in the dependency tree, although we have not imple-
mented this as such.
309
(1) (A)
PP
TO
to
NP
QP
IN
about
$
$
CD
9
CD
billion
-NONE-
*U*
(B)
P PP
to
P QP-OBJ
billion
P PP
about
P DOLLAR
$
P QP
9
(C)
PP
P PP
to
NP
QP
P PP
about
P DOLLAR
$
P QP
9
P QP
billion
(2) (A)
PP
IN
for
NP
$
$
CD
100
-NONE-
*U*
(B)
P PP
for
P QP-OBJ
100
P DOLLAR
$
(C)
PP
P PP
for
NP-OBJ
P DOLLAR
$
P QP
100
Figure 5: Examples of handling of QP in dependency to phrase-structure conversion.
sence in (2a) is correct annotation, consistent with
the annotation guidelines.
3.2 Refinement of VP Projections
As mentioned above, instead of having separate su-
pertags for S, SINV, SQ, SBAR, SBARQ, we use
only the P VP supertag and let the context determine
the specifics of the projection. Sentences (3ab) in
Figure 6 illustrate how the SBJ label is used to treat
the P VP supertag as indicating projection to SINV
(or SQ) instead of S. The determination is based on
the children of the P VP node. For example, if there
is a child with the P AUX supertag which is before a
child with the SBJ label, which in turn is before the
P VP node itself, then the latter is treated as project-
ing to either SINV or SQ, depending on the some
additional factors, primarily whether there is a WH
word among the children. In this example, there is
no WH word, so it becomes a SINV.3 We note here
that we also include a simple listing of verbs that
take complements of certain types - such as verbs of
saying, etc., that take SBAR complements, so that a
VP will project not just to S, but SBAR, even if the
complement is missing.
3.3 Coordination
We represent coordination in the dependency in
one of the standard ways, by making the follow-
ing conjuncts be children of the head word of
3This is not a fully precise implementation of the condi-
tions distinguishing SQ and SINV projections, in that it does
not properly check for whether the clause is a question.
(3) (A)
P VP
absorbed
P AUX
had
P NP-SBJ
cost
P DET
the
P VP
been
(B)
SINV
VBD
had
NP-SBJ
DT
the
NN
cost
VP
VBN
been
VP
VBN
absorbed
Figure 6: (3ab) shows that the local context of the P VP
supertag in the dependency tree results in a SINV struc-
ture in the converted phrase structure tree (3b).
the first conjunct. For example, a dependency
representation of ...turn down the volume
and close the curtains is shown in (4a) in
Figure 7. The conjunct close the curtains
is converted as a VP projection projecting to S. How-
ever, when the projection for turn is assembled, the
code checks if the conjuncts are missing subjects,
and if so, reduces the configuration to standard VP
coordination, as in (4b). The COORD label is used
to identify such structures for examination.
4 Results of Dependency to Phrase
Structure Conversion
To evaluate the correctness of conversion from de-
pendency to phrase structure, we follow the same
strategy as Xia and Palmer (2001) and Xia et al
(2009). We convert the phrase structure trees in the
PTB to dependency structure and convert the depen-
dency back to phrase structure. We then compare
the original PTB trees with the newly-created phrase
310
(4) (A)
P VP
turn
P PRT
down
P NP
volume
P DET
the
P CC
and
P VP-COORD
close
P NP-OBJ
curtains
P DET
the
(B)
VP
VP
turn PRT
down
NP-OBJ
the volume
and VP
close NP-OBJ
the curtains
Figure 7: (4a) is the dependency representation of a coordination structure, and the resulting phrase structure (4b)
shows that the conversion treated it as VP coordination, due to the absence of a subject.
Sec System rec prec f
00 Xia & Palmer ?01 86.2 88.7 87.5
Xia et al ?09 91.8 89.2 90.5
USE-POS-UNLABEL 96.6 97.4 97.0
USE-POS 94.6 95.4 95.0
USE-SUPER 95.9 97.0 96.4
22 Xia et al ?09 90.7 88.1 89.4
USE-POS 95.0 95.5 95.3
USE-SUPER 96.4 97.1 96.7
23 Wang & Zong ?10 95.9 96.3 96.1
USE-POS 94.8 95.7 95.3
USE-SUPER 96.2 97.3 96.7
24 USE-POS 94.0 94.7 94.4
USE-SUPER 95.9 97.1 96.5
Table 2: Results of dependency to phrase structure con-
version. For our system, the results are presented in two
ways, using either the gold part-of-speech tags (USE-
POS) or our gold supertags (USE-SUPER). For purposes
of comparison with Xia and Palmer (2001) and Xia et
al. (2009), we also present the results for Section 00 us-
ing part-of-speech tags, but with an unlabeled evaluation
(USE-POS-UNLABEL).
structure trees, using the standard evalb scoring code
(Sekine and Collins, 2008). Xia and Palmer (2001)
defined three different algorithms for the conversion,
utilizing different heuristics for how to build projec-
tion chains, and where to attach dependent subtrees.
They reported results for their system for Section 00
of the PTB, and we include in Table 2 only their
highest scoring algorithm. The system of Xia et al
(2009) uses conversion rules learned from Section
19, and then tested on Sections 00 and Section 22.
We developed the algorithm using Section 24, and
we also report results for Sections 00, 22, and 23, for
comparison with previous work. We ran our system
in two ways. In one we use the ?gold? supertags
that were created as described in Section 2.1 (USE-
SUPER), based on the TIG decomposition of the
original tree. In the other (USE-POS) we use the
gold POS tags, and not the supertags. Because our
DS-to-PS algorithm is based on using the supertags
to guide the conversion, the USE-POS runs work
by using a few straightforward heuristics to guess
the correct supertag from the POS tag and the de-
pendency structure. For example, if a word x has
the POS tag ?TO? and the word y to its immediate
right is its parent in the dependency tree and y has
one of the verbal POS tags, then x receives the su-
pertag P AUX, and otherwise P PP. Any word with
the POS tag JJ, JJR, or JJS, receives the supertag
P ADJP, and so on. The results for Xia and Palmer
(2001) and Xia et al (2009) were reported using an
unlabeled version of evalb, so to compare properly
we also report our results for Section 00 using an
unlabeled evaluation of the run using the POS tags
(USE-POS-UNLABEL), while all the other results
use a labeled evaluation.
We also compare our system with that of Wang
and Zong (2010). Unlike the three other systems
(including ours), this was not based on an automatic
conversion from a gold dependency tree to phrase
structure, but rather used the gold dependency tree
as additional input for a phrase structure parser (the
Berkeley parser).
4.1 Analysis
While our system was developed using Section 24,
the f-measure results for USE-SUPER are virtually
identical across all four sections (96.4, 96.7, 96.7,
96.5). Interestingly, there is more variation in the
311
USE-POS results (95.0, 95.3, 95.3, 94.4). We take
this to be an indication of a difference in the sec-
tions as to the utility of the POS tags to ?bootstrap?
the syntactic structure. As just mentioned above, the
USE-POS runs work by using heuristics to approxi-
mate the gold supertags from the POS tags.
The supertags, because they are partially derived
from the phrase structure, can obscure a discon-
nect between a POS tag and the syntactic structure
it projects. For example, the word according
in the structure (PP (VBG according) (PP
(TO to) ...)) receives the gold supertag P PP,
a more explicit representation of the word?s role in
the structure than the ambiguous VBG. This is why
the USE-POS score is lower than the USE-SUPER
score, since the POS tag and dependency structure
do not always, at least with our simple heuristics,
lead to the gold supertag. For example, in the USE-
POS run, according receives the incorrect su-
pertag P VP, leading to an incorrect structure, while
in the USE-SUPER run, it is able to use P PP, lead-
ing to the correct structure.
However, even with the lower performance of
USE-POS, it is well above the results reported in Xia
et al (2009) for Section 22, and even more so with
the unlabeled evaluation of Section 00 compared to
Xia and Palmer (2001) and Xia et al (2009). The
comparison with Wang and Zong (2010) for Section
23 (they did not report results for any other section)
shows something very different, however. Their re-
sult, using a gold dependency tree together with the
Berkeley parser, is above our USE-POS version and
below our USE-SUPER version.
Our interpretation of this is that it provides an
indication of what the parser is providing on top
of the gold dependency structure, which is roughly
the same information that we have encoded in our
DS to PS code. However, because the Wang and
Zong (2010) system performs better than our USE-
POS version, it is likely learning some of the non-
straightforward cases of how USE-POS tags can
bootstrap the syntactic structure that our USE-POS
version is missing. However, any conclusions must
be tentative since our dependency structures are not
necessarily the same as theirs and so it is not an ex-
act comparison.
Error type count
problem with PTB annotation 8
ambiguous ADVP placement 3
incorrect use of ?single token rule? 3
FRAG/X 2
multiple levels of recursion 2
other 5
Table 3: Analysis of errors in first 50 sentences of USE-
SUPER run for Section 24
4.2 Errors from Dependency Structure with
Supertags to Phrase Structure
We stressed in the introduction that we are interested
in understanding better the relationship between the
DS and PS representations. Identifying areas where
the conversion from DS did not result in a perfect
(evalb score) PS is therefore of particular interest.
For this analysis, we used our dev section, 24,
with the run USE-SUPER. We use this run because
we are interested in cases where, even with the gold
supertags, there was still a problem with the conver-
sion to the PS. We examined the first 50 sentences in
the section, with a total of 23 errors. We recognize
that this is a very small sample. An eyeball exam-
ination of other sentences does not reveal anything
significantly different than what we present here as
far as the sorts of errors, although we have only per-
formed a rigorous analysis of these 23 errors, which
is why we limit our discussion here to these cases.
Table 3 shows a breakdown of these 23 errors.
Note that by ?error? here we mean a difference be-
tween the reconstructed PS structure, and the PTB
gold PS structure, causing the score for Section 24,
USE-SUPER (last row) in Table 2 to be less than
perfect.
The most common ?error? is that in which the
PTB annotation is itself in error, while our algo-
rithm actually creates a correct phrase structure, in
the sense that it is consistent with the PTB guide-
lines. Three of these eight annotation problems are
of the same type, in which a NP is headed by a word
with the RB tag. An example is shown in (5) in
which (5a) shows (a fragment of) the original tree
in the PTB, and (5b) is the resulting DS, with (5c)
the reconstructed PS tree. The word here receives
the supertag P ADVP, thus resulting in a different re-
312
constructed PS, with an ADVP. There is a mismatch
between the POS tag and the node label in the origi-
nal tree (5a), and in fact in this case the node label in
the PTB tree should have been ADVP-LOC, instead
of NP-LOC.
(5) (A)
VP
VBD
premiered
NP-LOC
RB
here
(B)
P VP
premiered
P ADVP
here
(C)
VP
VBD
premiered
ADVP
RB
here
(6) (A)
S
NP-SBJ
-NONE-
VP
ADVP-MNR
RB
frantically
VBG
selling
NP
NNS
bonds
(B)
P VP
selling
P ADVP
frantically
P NP
bonds
(C)
S
ADVP
P ADVP
frantically
VP
VBG
selling
NP
NNS
bonds
An example of the ?ambiguous ADVP place-
ment? error is shown in (6), in which the PTB tree
has the adverb frantically inside the VP, infor-
mation which is not available in the DS (6b). Our
conversion code has to choose as to where to put
such ADVPs, and it puts them outside the VP, as in
(6c), which is sometimes correct, but not in this case.
5 Conclusion and Future Work
In this work we have described an approach to auto-
matically converting DS to PS with significantly im-
proved accuracy over previous efforts, and compara-
ble results to that of using a phrase structure parser
guided by the dependency structure.
Following the motivation discussed in Section 1,
the next step is straightforward - to adapt the al-
gorithm to work on conversion from a dependency
representation of the Arabic Treebank to the phrase
structure representation necessary for the annotation
pipeline. Following this, we will then experiment
with parsing the Arabic dependency representation,
converting to phrase structure, and evaluating the re-
sulting phrase structure representation as usual for
parsing evaluation. We will also experiment with
dependency parsing for the PTB dependency repre-
sentation discussed in this paper. Habash and Roth
(2009) discuss an already-existing dependency rep-
resentation of parts of the ATB and it will be inter-
esting to compare the conversion accuracy using the
different dependency representations, although we
expect that there will not be any major differences
in the representations.
One other aspect of future work is to implement
the algorithm in Wang and Zong (2010), using our
own dependency representation, since this would al-
low a precise investigation of what the phrase struc-
ture parser is contributing as compared to our auto-
matic conversion. We note that this work also ex-
perimented with dependency parsing, and then auto-
matically converting the results to PS, a further basis
of comparison.
Finally, we would like to stress that while we have
used evalb for scoring the converting PS because it
is the standard evaluation for PS work, it is a very
insufficient standard for this work. As discussed at
the end of Section 2, we have not included all the
function tags or empty categories in our representa-
tion, a significant omission. We would like to ex-
pand our dependency representation to allow all the
function tags and empty categories to be included
in the converted PS. Our plan is to take our anal-
ogy to TAG more seriously (e.g., (Joshi and Ram-
bow, 2003)) and use a label akin to adjunction to en-
code leftward (non-projective) movement in the tree,
also using an appropriate dependency parser as well
(Shen and Joshi, 2008).
Acknowledgements
This work was supported in part by the Defense Ad-
vanced Research Projects Agency, GALE Program
Grant No. HR0011-06-1-0003. The views, opinions
and/or findings contained in this article/presentation
are those of the author/presenter and should not be
interpreted as representing the official views or poli-
cies, either expressed or implied, of the Defense Ad-
vanced Research Projects Agency or the Department
of Defense. We would also like to thank Mohamed
Maamouri, Colin Warner, Aravind Joshi, and Mitch
Marcus for valuable conversations and feedback.
313
References
Srinivas Bangalore and Aravind K. Joshi, editors. 2010.
Supertagging: Using Complex Lexical Descriptions in
Natural Language Processing. MIT Press.
Ann Bies, Mark Ferguson, Karen Katz, and Robert Mac-
Intyre. 1995. Bracketing guidelines for Treebank II-
style Penn Treebank project. Technical Report MS-
CIS-95-06, University of Pennsylvania.
David Chiang. 2003. Statistical parsing with an auto-
matically extracted Tree Adjoining Grammar. In Data
Oriented Parsing. CSLI.
Nizar Habash and Ryan Roth. 2009. CATiB: The
Columbia Arabic Treebank. In Proceedings of the
ACL-IJCNLP 2009 Conference Short Papers, pages
221?224, Suntec, Singapore, August. Association for
Computational Linguistics.
Johan Hall and Joakim Nivre. 2008. A dependency-
driven parser for German dependency and con-
stituency representations. In Proceedings of the Work-
shop on Parsing German, pages 47?54, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Joakim Nivre Johan Hall and Jens Nilsson. 2007. Hy-
brid constituency-dependency parser for Swedish. In
Proceedings of NODALIDA, Tartu, Estonia.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for english. In
Proceedings of NODALIDA, Tartu, Estonia.
Aravind Joshi and Owen Rambow. 2003. A formal-
ism for dependency grammar based on Tree Adjoin-
ing Grammar. In Proceedings of the Conference on
Meaning-Text Theory, Paris, France.
Mohamed Maamouri and Ann Bies. 2004. Developing
an arabic treebank: Methods, guidelines, procedures,
and tools. In Ali Farghaly and Karine Megerdoomian,
editors, COLING 2004 Computational Approaches to
Arabic Script-based Languages, pages 2?9, Geneva,
Switzerland, August 28th. COLING.
Mohamed Maamouri, Ann Bies, and Seth Kulick. 2011.
Upgrading and enhancing the Penn Arabic Treebank.
In Joseph Olive, Caitlin Christianson, and John Mc-
Cary, editors, Handbook of Natural Language Pro-
cessing and Machine Translation: DARPA Global Au-
tonomous Language Exploitation. Springer.
Christopher Manning. 2011. Part-of-speech tagging
from 97% to 100%: Is it time for some linguistics?
In Alexander Gelbukh, editor, Computational Linguis-
tics and Intelligent Text Processing, 12th International
Conference, CICLing 2011, Proceedings, Part I. Lec-
ture Notes in Computer Science 6608.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
linguistics, 19:313?330.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The Penn Tree-
bank: Annotating predicate argument structure. In
Proceedings of HLT.
Satoshi Sekine and Michael Collins. 2008. Evalb.
http://nlp.cs.nyu.edu/evalb/.
Libin Shen and Aravind Joshi. 2008. LTAG dependency
parsing with bidirectional incremental construction.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages 495?
504, Honolulu, Hawaii, October. Association for Com-
putational Linguistics.
Zhiguo Wang and Chengqing Zong. 2010. Phrase struc-
ture parsing with dependency structure. In COLING
2010: Posters, pages 1292?1300, Beijing, China, Au-
gust.
Fei Xia and Martha Palmer. 2001. Converting depen-
dency structures to phrase structures. In HLT-2001.
Fei Xia, Owen Rambow, Rajesh Bhatt, Martha Palmer,
and Dipti Misra Sharma. 2009. Towards a multi-
representational treebank. In Proceedings of the Work-
shop on Treebanks and Linguistic Theories.
314
Proceedings of NAACL-HLT 2013, pages 550?555,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Using Derivation Trees for Informative Treebank Inter-Annotator
Agreement Evaluation
Seth Kulick and Ann Bies and Justin Mott
and Mohamed Maamouri
Linguistic Data Consortium
University of Pennsylvania
{skulick,bies,jmott,maamouri}
@ldc.upenn.edu
Beatrice Santorini and
Anthony Kroch
Department of Linguistics
University of Pennsylvania
{beatrice,kroch}
@ling.upenn.edu
Abstract
This paper discusses the extension of a sys-
tem developed for automatic discovery of tree-
bank annotation inconsistencies over an entire
corpus to the particular case of evaluation of
inter-annotator agreement. This system makes
for a more informative IAA evaluation than
other systems because it pinpoints the incon-
sistencies and groups them by their structural
types. We evaluate the system on two corpora
- (1) a corpus of English web text, and (2) a
corpus of Modern British English.
1 Introduction
This paper discusses the extension of a system de-
veloped for automatic discovery of treebank annota-
tion inconsistencies over an entire corpus to the par-
ticular case of evaluation of inter-annotator agree-
ment (IAA). In IAA, two or more annotators anno-
tate the same sentences, and a comparison identi-
fies areas in which the annotators might need more
training, or the annotation guidelines some refine-
ment. Unlike other IAA evaluation systems, this
system application results in a precise pinpointing of
inconsistencies and the grouping of inconsistencies
by their structural types, making for a more infor-
mative IAA evaluation.
Treebank annotation, consisting of syntactic
structure with words as the terminals, is by its na-
ture more complex and therefore more prone to error
than many other annotation tasks. However, high an-
notation consistency is crucial to providing reliable
training and testing data for parsers and linguistic
research. Error detection is therefore an important
area of research, and the importance of work such as
Dickinson and Meurers (2003) is that errors and an-
notation inconsistencies might be automatically dis-
covered, and once discovered, be targeted for subse-
quent quality control.
A recent approach to this problem (Kulick et al,
2011; Kulick et al, 2012) (which we will call the
KBM system) improves upon Dickinson and Meur-
ers (2003) by decomposing the full syntactic tree
into smaller units, using ideas from Tree Adjoining
Grammar (TAG) (Joshi and Schabes, 1997). This al-
lows the comparison to be based on small syntactic
units instead of string n-grams, improving the detec-
tion of inconsistent annotation.
The KBM system, like that of Dickinson and
Meurers (2003) before it, is based on the notion of
comparing identical strings. In the general case, this
is a problematic assumption, since annotation in-
consistencies are missed because of superficial word
differences between strings which one would want
to compare.1 However, this limitation is not present
for IAA evaluation, since the strings to compare are,
by definition, identical.2 The same is also true of
parser evaluation, since the parser output and the
gold standard are based on the same sentences.
We therefore take the logical step of applying the
KBM system developed for automatic discovery of
annotation inconsistency to the special case of IAA.3
1Boyd et al (2007) and other current work tackles this prob-
lem. However, that is not the focus of this paper.
2Aside from possible tokenization differences by annotators.
3In this paper, we do not yet apply the system to parser eval-
uation, although it is conceptually the same problem as IAA
evaluation. We wanted to first refine the system using annota-
tor input for the IAA application before applying it to parser
550
(1) a. NP-SBJ
NP
NP
The word
NP
renaissance
NP
-LRB- NP
Rinascimento
PP
in NP
Italian
-RRB-
b. NP-SBJ
The word renaissance PRN
-LRB- FRAG
NP
Rinascimento
PP
in NP
Italian
-RRB-
Figure 1: Two example trees showing a difference in IAA
To our knowledge, this work is the first to utilize
such a general system for this special case.
The advantages of the KBM system play out
somewhat differently in the context of IAA evalu-
ation than in the more general case. In this con-
text, the comparison of word sequences based on
syntactic units allows for a precise pinpointing of
differences. The system also retains the ability to
group inconsistencies together by their structural
type, which we have found to be useful for the more
general case. Together, these two properties make
for a useful and informative system for IAA evalua-
tion.
In Section 2 we describe the basic working of our
system. In Section 3 we discuss in more detail the
advantages of this approach. In Section 4 we evalu-
ate the system on two treebanks, a corpus of English
web text and a corpus of Modern British English.
Section 5 discusses future work.
2 System Overview
The basic idea of the KBM system is to detect word
sequences that are annotated in inconsistent ways by
evaluation.
wordNP
The RinascimentoNP -RRB--LRB-renaissance
NP
renaissanceNPb1The -RRB--LRB- Rinascimento
FRAGword PRN
a1
a2
a3
a4
a5
a8
b2
b3 b5
b4 b8
inPPa6 ItalianNP a7
inPP b6 ItalianNPb7
NP
A
A
A A A A
A
A
MM MM M
A
(2) a.
b.
Figure 2: E-trees and derivation trees corresponding to
(1ab)
comparing local syntactic units. Following Dickin-
son and Meurers (2003), we refer to sequences ex-
amined for inconsistent annotation as nuclei. The
sentence excerpts (1ab) in Figure 1, from the test
corpora used in this work, illustrate an inconsistency
in the annotation of corresponding strings. We fo-
cus here on the difference in the annotation of the
nucleus The word renaissance, which in (1a) is an-
notated as an appositive structure, while in (1b) it is
flat.
Following the TAG approach, KBM decomposes
the full phrase structure into smaller chunks called
elementary trees (henceforth, e-trees). The relation-
ship of the e-trees underlying a full phrase struc-
ture to each other is recorded in a derivation tree,
in which each node is an e-tree, related to its par-
ent node by a composition operation, as shown in
(2ab).4
KBM uses two composition operations, each with
left and right variants, shown in Figure 3: (1) ad-
4The decomposition is based on head-finding heuristics,
with the result here that word is the head of (1a), while renais-
sance is the head of (1b), as reflected in their respective deriva-
tion trees (2a) and (2b). We omit the POS tags in (1ab) and
(2ab) to avoid clutter.
551
wo ro wo wo ro
dNPTheRinaschNmswot wo
-BLhebN1hBFeRinaschNms
ro t ro
-BLheRinaschNmswot wo
dNPThebN1hBFeRinaschNmsro
tro
woro ro wo wo
Figure 3: Composition operations (left and right)
junction, which attaches one tree to a target node in
another tree by creating a copy of the target node,
and (2) sister adjunction, which attaches one tree as
a sister to a target node in another tree. Each arc in
Figure 2 is labeled by an ?M? for adjunction and ?A?
for sister-adjunction. 5
The system uses the tree decomposition and re-
sulting derivation tree for the comparison of differ-
ent instances of the same nucleus. The full deriva-
tion tree for a sentence is not used, but rather only
that slice of it having e-trees with words that are in
the nucleus being examined, which we call a deriva-
tion tree fragment. That is, for a given nucleus with
a set of instances, we compare the derivation frag-
ments for each instance.
For example, for the nucleus The word renais-
sance, the derivation tree fragment for the instance
in (1a) consists of the e-trees a1, a2, a3 (and their
arcs) in (2a), and likewise the derivation tree from
the instance in (1b) consists of the e-trees b1, b2, b3
in (2b). These derivation fragments have a differ-
ent structure, and so the two instances of The word
renaissance are recognized as inconsistent.
Two important aspects of the overall system re-
quire mention here: (1) Nuclei are identified by us-
ing sequences that occur as a constituent anywhere
5KBM is based on a variant of Spinal TAG (Shen et al,
2008), and uses sister adjunction without substitution. Space
prohibits full discussion, but multiple adjunction to a single
node (e.g., a4, a6, a8 to a5 in (2a)) does not create multiple
levels of recursion, while a special specification handles the ex-
tra NP recursion for the apposition with a2, a3, and a5. For
reasons of space, we also leave aside a precise comparison to
Tree Insertion Grammar (Chiang, 2003) and Spinal TAG (Shen
et al, 2008).
in the corpus, even if other instances of the same
sequence are not constituents. Both instances of
The word renaissance are compared, because the
sequence occurs at least once as a constituent. (2)
We partition each comparison of the instances of a
nucleus by the lowest nonterminal in the derivation
tree fragment that covers the sequence. The two in-
stances of The word renaissance are compared be-
cause the lowest nonterminal is an NP in both in-
stances.
3 Advantages of this approach
As Kulick et al (2012) stressed, using derivation
tree fragments allows the comparison to abstract
away from interference by irrelevant modifiers, an
issue with Dickinson and Meurers (2003). However,
in the context of IAA, this advantage of KBM plays
out in a different way, in that it allows for a pre-
cise pinpointing of the inconsistencies. For IAA,
the concern is not whether an inconsistent annota-
tion will be reported, since at some level higher in
the tree every difference will be found, even if the
context is the entire tree. KBM, however, will find
the inconsistencies in a more informative way, for
example reporting just The word renaissance, not
some larger unit. Likewise, it reports Rinascimento
in Italian as an inconsistently annotated sequence.6
A critical desirable property of KBM that carries
over from the more general case is that it allows for
different nuclei to be grouped together in the sys-
tem?s output if they have the same annotation in-
consistency type. As in Kulick et al (2011), each
nucleus found to be inconsistent is categorized by
an inconsistency type, which is simply the collec-
tion of different derivation tree fragments used for
the comparison of its instances, including POS tags
but not the words. For example, the inconsistency
type of the nucleus The word renaissance in (1ab) is
the pair of derivation tree fragments (a1,a2,a3) and
(b1,b2,b3) from (2ab), with the POS tags. This nu-
6Note however that it does not report -LRB- Rinascimento
in Italian -RRB- which is also a constituent, and so might be
expected to be compared. The lowest nonterminal above this
substring in the two derivation trees in Figure 2 is the NP in a5
and the FRAG in b5, thus exempting them from comparison. It
is exactly this sort of case that motivated the ?external check?
discussed in Kulick et al (2012), which we have not yet imple-
mented for IAA.
552
Inconsistency type # Found # Accurate
Function tags only 53 53
POS tags only 18 13
Structural 129 122
Table 1: Inconsistency types found for system evaluation
cleus is then reported together with other nuclei that
use the same derivation fragments. In this case, it
therefore also reports the nucleus The term renais-
sance, which appears elsewhere in the corpus with
the two annotations from the different annotators as
in (3):
(3) a. NP
NP
The term
NP
renaissance
b. NP
The term renaissance
KBM reports The word renaissance and The term
renaissance together because they are inconsistently
annotated in exactly the same way, in spite of the dif-
ference in words. This grouping together of incon-
sistencies based on structural characteristics of the
inconsistency is critically important for understand-
ing the nature of the annotation inconsistencies.
It is the combination of these two characteristics -
(1) pinpointing of errors and (2) grouping by struc-
ture - that makes the system so useful for IAA. This
is an improvement over alternatives such as using
evalb (Sekine and Collins, 2008) for IAA. No other
system to our knowledge groups inconsistencies by
structural type, as KBM does. The use of the deriva-
tion tree fragments greatly lessens the multiple re-
porting of a single annotation difference, which is
a difficulty for using evalb (Manning and Schuetze,
1999, p. 436) or Dickinson and Meurers (2003).
4 Evaluation
4.1 English web text
We applied our approach to pre-release subset of
(Bies et al, 2012), dually annotated and used for
annotator training, from which the examples in Sec-
tions 2 and 3 are taken. It is a small section of the
corpus, with 4,270 words dually annotated.
For this work, we also took the further step of
characterizing the inconsistency types themselves,
allowing for an even higher-level view of the incon-
sistencies found. In addition to grouping together
different strings as having the same inconsistent an-
notation, the types can also be grouped together for
comparison at a higher level. For this IAA sample,
we separated the inconsistency types into the three
groups in Table 1, with the derivation tree fragments
differing (1) only on function tags, (2) only on POS
tags7, and (3) on structural differences. We man-
ually examined each inconsistency group to deter-
mine if it was an actual inconsistency found, or a
spurious false positive. As shown in Table 1, the pre-
cision of the reported inconsistencies is very high.
It is in fact even higher than it appears, because
the seven (out of 129) instances incorrectly listed
as structural problems were actually either POS or
function tag inconsistencies, that were discovered
by the system only by a difference in the derivation
tree fragment, and so were categorized as structural
problems instead of POS or function tag inconsis-
tencies. 8
Because of the small size of the corpus, there
are relatively few nuclei grouped into inconsistency
types. The 129 structural inconsistency types in-
clude 130 nuclei, with the only inconsistency type
with more than one nucleus being the type with The
word renaissance and The term renaissance, as dis-
cussed above. There is more grouping together in
the ?POS tags only? case (37 nuclei included in
the 18 inconsistency types), and the ?function tags
only? case (56 nuclei included in the 53 inconsis-
tency types).
4.2 Modern British English corpus
We also applied our approach to a supplemental sec-
tion (Kroch and Santorini, in preparation) to a cor-
pus of modern British English (Kroch et al, 2010),
part of a series of corpora used for research into lan-
guage change. The annotation style is similar to that
of the Penn Treebank, although with some differ-
ences. In this case, because neither the function tags
nor part-of-speech tags were part of the IAA work,
7As mentioned in footnote 4, although POS tags were left
out of Figure 2 for readability, they are included in the actual e-
trees. This allows POS differences in a similar syntactic context
to be naturally captured within the overall KBM framework.
8A small percentage of inconsistencies are the result of lin-
guistic ambiguities and not an error by one of the annotators.
553
we do not separate out the inconsistency types, as
done in Section 4.1.
The supplement section consisted of 82,701
words dually annotated. The larger size, as com-
pared with the corpus in Section 4.1, results in some
differences in the system output. Because of the
larger size, there are more substantial cases of dif-
ferent nuclei grouped together as the same inconsis-
tency type than in Section 4.1. The first inconsis-
tency type (sorted by number of nuclei) has 88 nu-
clei, and the second has 37 nuclei. In total, there are
1,532 inconsistency types found, consisting of 2,194
nuclei in total. We manually examined the first 20
inconsistency types (sorted by number of nuclei),
consisting in total of 375 nuclei. All were found to
be true instances of inconsistent annotation.
(4) a. NP
the ADJP
only true
thing
b. NP
the only true thing
(5) a. NP
their ADJP
only actual
argument
b. NP
their only actual argument
The trees in (4) and (5) show two of the 88 nu-
clei grouped into the first inconsistency type. As
with The word renaissance and The term renais-
sance in the English web corpus, nuclei with similar
(although not identical) words are often grouped into
the same inconsistency type. To repeat the point,
this is not because of any search for similarity of
the words in the nuclei. It arises from the fact that
the nuclei are annotated inconstantly in the same
way. Of course not all nuclei in an inconsistency
type have the same words. Nuclei found in this in-
consistency type include only true and only actual
as shown above, and also nuclei such as new En-
glish, greatest possible, thin square, only necessary.
Taken together, they clearly indicate an issue with
the annotation of multi-word adjective phrases.9
9Note that the inconsistencies discussed throughout this pa-
per are not taken from the the published corpora. These results
are only from internal annotator training files.
5 Future work
There are several ways in which we plan to improve
the current approach. As mentioned above, there is
a certain class of inconsistencies which KBM will
not pinpoint precisely, which requires adopting the
?external check? from Kulick et al (2012). The ab-
straction on inconsistency types described in Sec-
tion 4 can also be taken further. For example, one
might want to examine in particular inconsistency
types that arise from PP attachment or that have to
do with the PRN function tag.
One main area for future work is the application
of this work to parser evaluation as well as IAA. For
this area, there is some connection to the work of
Goldberg and Elhadad (2010) and Dickinson (2010),
which are both concerned with examining depen-
dency structures of more than one edge. The con-
nection is that those works are focused on depen-
dency representations, and ithe KBM system does
phrase structure analysis using a TAG-like deriva-
tion tree, which strongly resembles a dependency
tree (Rambow and Joshi, 1997). There is much in
this area of common concern that is worth examin-
ing further.
Acknowledgments
This material is based upon work supported by
the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-11-C-0145.
The content does not necessarily reflect the position
or the policy of the Government, and no official en-
dorsement should be inferred. This applies to the
first four authors. The first, fifth, and sixth authors
were supported in part by National Science Foun-
dation Grant # BCS-114749. We would also like
to thank Colin Warner, Aravind Joshi, Mitch Mar-
cus, and the computational linguistics group at the
University of Pennsylvania for helpful conversations
and feedback.
554
References
Ann Bies, Justin Mott, Colin Warner, and Seth Kulick.
2012. English Web Treebank. LDC2012T13. Lin-
guistic Data Consortium.
Adriane Boyd, Markus Dickinson, and Detmar Meurers.
2007. Increasing the recall of corpus annotation er-
ror detection. In Proceedings of the Sixth Workshop
on Treebanks and Linguistic Theories (TLT 2007),
Bergen, Norway.
David Chiang. 2003. Statistical parsing with an auto-
matically extracted Tree Adjoining Grammar. In Data
Oriented Parsing. CSLI.
Markus Dickinson and Detmar Meurers. 2003. Detect-
ing inconsistencies in treebanks. In Proceedings of the
Second Workshop on Treebanks and Linguistic The-
ories (TLT 2003), Sweden. Treebanks and Linguistic
Theories.
Markus Dickinson. 2010. Detecting errors in
automatically-parsed dependency relations. In Pro-
ceedings of the 48th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 729?738,
Uppsala, Sweden, July. Association for Computational
Linguistics.
Yoav Goldberg and Michael Elhadad. 2010. Inspecting
the structural biases of dependency parsing algorithms.
In Proceedings of the Fourteenth Conference on Com-
putational Natural Language Learning, pages 234?
242, Uppsala, Sweden, July. Association for Compu-
tational Linguistics.
A.K. Joshi and Y. Schabes. 1997. Tree-adjoining gram-
mars. In G. Rozenberg and A. Salomaa, editors,
Handbook of Formal Languages, Volume 3: Beyond
Words, pages 69?124. Springer, New York.
Anthony Kroch and Beatrice Santorini. in preparation.
Supplement to the Penn Parsed Corpus of Modern
British English.
Anthony Kroch, Beatrice Santorini, and Ariel Dier-
tani. 2010. Penn Parsed Corpus of Mod-
ern British English. http://www.ling.upenn.edu/hist-
corpora/PPCMBE-RELEASE-1/index.html.
Seth Kulick, Ann Bies, and Justin Mott. 2011. Using
derivation trees for treebank error detection. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 693?698, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Seth Kulick, Ann Bies, and Justin Mott. 2012. Further
developments in treebank error detection using deriva-
tion trees. In LREC 2012: 8th International Confer-
ence on Language Resources and Evaluation, Istanbul.
Christopher Manning and Hinrich Schuetze. 1999.
Foundations of Statistical Natural Language Process-
ing. MIT Press.
Owen Rambow and Aravind Joshi. 1997. A formal
look at dependency grammars and phrase-structure
grammars, with special consideration of word-order
phenomena. In L. Wanner, editor, Recent Trends
in Meaning-Text Theory, pages 167?190. John Ben-
jamins, Amsterdam and Philadelphia.
Satoshi Sekine and Michael Collins. 2008. Evalb.
http://nlp.cs.nyu.edu/evalb/.
Libin Shen, Lucas Champollion, and Aravind Joshi.
2008. LTAG-spinal and the Treebank: A new re-
source for incremental, dependency and semantic pars-
ing. Language Resources and Evaluation, 42(1):1?19.
555
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 693?698,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Using Derivation Trees for Treebank Error Detection
Seth Kulick and Ann Bies and Justin Mott
Linguistic Data Consortium
University of Pennsylvania
3600 Market Street, Suite 810
Philadelphia, PA 19104
{skulick,bies,jmott}@ldc.upenn.edu
Abstract
This work introduces a new approach to
checking treebank consistency. Derivation
trees based on a variant of Tree Adjoining
Grammar are used to compare the annotation
of word sequences based on their structural
similarity. This overcomes the problems of
earlier approaches based on using strings of
words rather than tree structure to identify the
appropriate contexts for comparison. We re-
port on the result of applying this approach
to the Penn Arabic Treebank and how this ap-
proach leads to high precision of error detec-
tion.
1 Introduction
The internal consistency of the annotation in a tree-
bank is crucial in order to provide reliable training
and testing data for parsers and linguistic research.
Treebank annotation, consisting of syntactic struc-
ture with words as the terminals, is by its nature
more complex and thus more prone to error than
other annotation tasks, such as part-of-speech tag-
ging. Recent work has therefore focused on the im-
portance of detecting errors in the treebank (Green
and Manning, 2010), and methods for finding such
errors automatically, e.g. (Dickinson and Meur-
ers, 2003b; Boyd et al, 2007; Kato and Matsubara,
2010).
We present here a new approach to this problem
that builds upon Dickinson and Meurers (2003b), by
integrating the perspective on treebank consistency
checking and search in Kulick and Bies (2010). The
approach in Dickinson andMeurers (2003b) has cer-
tain limitations and complications that are inher-
ent in examining only strings of words. To over-
come these problems, we recast the search as one of
searching for inconsistently-used elementary trees in
a Tree Adjoining Grammar-based form of the tree-
bank. This allows consistency checking to be based
on structural locality instead of n-grams, resulting in
improved precision of finding inconsistent treebank
annotation, allowing for the correction of such in-
consistencies in future work.
2 Background and Motivation
2.1 Previous Work - DECCA
The basic idea behind the work in (Dickinson and
Meurers, 2003a; Dickinson and Meurers, 2003b) is
that strings occurring more than once in a corpus
may occur with different ?labels? (taken to be con-
stituent node labels), and such differences in labels
might be the manifestation of an annotation error.
Adopting their terminology, a ?variation nucleus? is
the string of words with a difference in the annota-
tion (label), while a ?variation n-gram? is a larger
string containing the variation nucleus.
(1) a. (NP the (ADJP most
important) points)
b. (NP the most important points)
For example, suppose the pair of phrases in (1)
are taken from two different sentences in a cor-
pus. The ?variation nucleus? is the string most
important, and the larger surrounding n-gram
is the string the most important points.
This is an example of error in the corpus, since the
second annotation is incorrect, and this difference
manifests itself by the nucleus having in (a) the label
ADJP but in (b) the default label NIL (meaning for
their system that the nucleus has no covering node).
Dickinson and Meurers (2003b) propose a ?non-
693
fringe heuristic?, which considers two variation nu-
clei to have a comparable context if they are prop-
erly contained within the same variation n-gram -
i.e., there is at least one word of the n-gram on both
sides of the nucleus. For the the pair in (1), the two
instances of the variation nucleus satisfy the non-
fringe heuristic because they are properly contained
within the identical variation n-gram (with the and
points on either side). See Dickinson and Meur-
ers (2003b) for details. This work forms the basis
for the DECCA system.1
2.2 Motivation for Our Approach
(2) a. NP
qmp
summit
NP
$rm
Sharm
NP
Al$yx
the Sheikh
b. NP
qmp
summit
NP
$rm
Sharm
Al$yx
the Sheikh
c. NP
qmp
summit
NP
NP
$rm
Sharm
Al$yx
the Sheikh
NP
( mSr
Egypt
)
We motivate our approach by illustrating the lim-
itations of the DECCA approach. Consider the trees
(2a) and (2b), taken from two instances of the three-
word sequence qmp $rm Al$yx in the Arabic
Treebank.2 There is no need to look at any surround-
ing annotation to conclude that there is an incon-
sistency in the annotation of this sequence.3 How-
ever, based on (2ab), the DECCA system would not
even identify the three-word sequence qmp $rm
Al$yx as a nucleus to compare, because both in-
stances have a NP covering node, and so are consid-
ered to have the same label. (The same is true for
the two-word subsequence $rm Al$yx.)
Instead of doing the natural comparison of the
1http://www.decca.osu.edu/.
2In Section 4 we give the details of the corpus. We use the
Buckwalter Arabic transliteration scheme (Buckwalter, 2004).
3While the nature of the inconsistency is not the issue here,
(b) is the correct annotation.
inconsistent structures for the identical word se-
quences as in (2ab), the DECCA approach would
instead focus on the single word Al$yx, which has
a NP label in (2a), while it has the default label
NIL in (2b). However, whether it is reported as a
variation depends on the irrelevant fact of whether
the word to the right of Al$yx is the same in both
instances, thus allowing it to pass the non-fringe
heuristic (since it already has the same word, $rm,
on the left).
Consider now the two trees (2bc). There is an
additional NP level in (2c) because of the adjunct
( mSr ), causing qmp $rm Al$yx to have no
covering node, and so have the default label NIL,
and therefore categorized as a variation compared to
(2b). However, this is a spurious difference, since
the label difference is caused only by the irrelevant
presence of an adjunct, and it is clear, without look-
ing at any further structure, that the annotation of
qmp $rm Al$yx is identical in (2bc). In this case
the ?non-fringe heuristic? serves to avoid report-
ing such spurious differences, since if qmp $rm
Al$yx did not have an open parenthesis on the right
in (b), and qmp did not have the same word to its
immediate left in both (b) and (c), the two instances
would not be surrounded by the same larger varia-
tion n-gram, and so would not pass the non-fringe
heuristic.
This reliance on irrelevant material arises from us-
ing on a single node label to characterize a struc-
tural annotation and the surrounding word context
to overcome the resulting complications. Our ap-
proach instead directly compares the annotations of
interest.
3 Using Derivation Tree Fragments
We utilize ideas from the long line of Tree Adjoining
Grammar-based research (Joshi and Schabes, 1997),
based on working with small ?elementary trees? (ab-
breviated ?etrees? in the rest of this paper) that are
the ?building blocks? of the full trees of a treebank.
This decomposition of the full tree into etrees also
results in a ?derivation tree? that records how the el-
ementary trees relate to each other.
We illustrate the basics of TAG-based deriva-
tion we are using with examples based on the
trees in (2). Our grammar is a TAG variant with
694
qmp
summit
#c1
S:1.2
NP
NP^
#c2
M:1,right
#c4 NP
mSr
Egypt
qmp
summit
#a1
S:1.2
NP
NP^
$rm
Sharm
#a2
S:1.2
NP
NP^
Al$yx
The Sheikh
NP
#a3
qmp
summit
#b1
S:1.2
NP
NP^
$rm
Sharm
#b2
NP
Al$yx
The Sheikh
    For (2a)                    For (2b)                For (2c) 
A:1.1,left#b3
NP
Al$yx
The Sheikh
A:1.1,left
#c3
$rm
Sharm
Figure 1: Etrees and Derivation Trees for (2abc).
tree-substitution, sister-adjunction, and Chomsky-
adjunction (Chiang, 2003). Sister adjunction at-
taches a tree (or single node) as a sister to another
node, and Chomsky-adjunction forms a recursive
structure as well, duplicating a node. As typically
done, we use head rules to decompose a full tree and
extract the etrees. The three derivation trees, corre-
sponding to (2abc), are shown in Figure 1.
Consider first the derivation tree for (2a). It has
three etrees, numbered a1, a2, a3, which are the
nodes in the derivation tree which show how the
three etrees connect to each other. This derivation
tree consists of just tree substitutions. The ? sym-
bol at node NP? in a1 indicates that it is a sub-
stitution node, and the S:1.2 above a2 indicates
that it substitutes into node at Gorn address 1.2 in
tree a1 (i.e., the substitution node), and likewise
for a3 substituting into a2. The derivation tree for
(2b) also has three etrees, although the structure
is different. Because the lower NP is flat in (2b),
the rightmost noun, Al$yx, is taken as the head
of the etree b2, with the degenerate tree for $rm
sister-adjoining to the left of Al$yx, as indicated
by the A:1.1,left. The derivation tree for (2c)
is identical to that of (2b), except that it has the
additional tree c4 for the adjunct mSr, which right
Chomsky-adjoins to the root of c2, as indicated by
the M:1,right.4
4We leave out the irrelevant (here) details of the parentheses
This tree decomposition and resulting derivation
tree provide us with the tool for comparing nuclei
without the interfering effects from words not in the
nucleus. We are interested not in the derivation tree
for an entire sentence, but rather only that slice of
it having etrees with words that are in the nucleus
being examined, which we call the derivation tree
fragment. That is, for a given nucleus being exam-
ined, we partition its instances based on the covering
node in the full tree, and within each set of instances
we compare the derivation tree fragments for each
instance. These derivation tree fragments are the
relevant structures to compare for inconsistent an-
notation, and are computed separately for each in-
stance of each nucleus from the full derivation tree
that each instance is part of.5
For example, for comparing our three instances
of qmp $rm Al$yx, the three derivation tree frag-
ments would be the structures consisting of (a1, a2,
a3), (b1, b2, b3) and (c1, c2, c3), along with their
connecting Gorn addresses and attachment types.
This indicates that the instances (2ab) have differ-
ent internal structures (without the need to look at a
surrounding context), while the instances (2bc) have
identical internal structures (allowing us to abstract
away from the interfering effects of adjunction).
Space prevents full discussion here, but the etrees
and derivation trees as just described require refine-
ment to be truly appropriate for comparing nuclei.
The reason is that etrees might encode more infor-
mation than is relevant for many comparisons of nu-
clei. For example, a verb might appear in a corpus
with different labels for its objects, such as NP or
SBAR, etc., and this would lead to its having dif-
ferent etrees, differing in their node label for the
substitution node. If the nucleus under compari-
son includes the verb but not any words from the
complement, the inclusion of the different substi-
tution nodes would cause irrelevant differences for
that particular nucleus comparison.
We solve these problems by mapping down the
in the derivation tree.
5A related approach is taken by Kato and Matsubara (2010),
who compare partial parse trees for different instances of the
same sequence of words in a corpus, resulting in rules based on
a synchronous Tree Substitution Grammar (Eisner, 2003). We
suspect that there are some major differences between our ap-
proaches regarding such issues as the representation of adjuncts,
but we leave such a comparison for future work.
695
System nuclei n-grams instances
DECCA 24,319 1,158,342 2,966,274
Us 54,496 not used 605,906
Table 1: Data examined by the two systems for the ATB
System nuclei non-duplicate types of
found nuclei found inconsistency
DECCA 4,140 unknown unknown
Us-internal 9,984 4,272 1,911
Table 2: Annotation inconsistencies reported for the ATB
representation of the etrees in a derivation tree frag-
ment to form a ?reduced? derivation tree fragment.
These reductions are (automatically) done for each
nucleus comparison in a way that is appropriate for
that particular nucleus comparison. A particular
etree may be reduced in one way for one nucleus,
and then a different way for a different nucleus. This
is done for each etree in a derivation tree fragment.
4 Results on Test Corpus
Green and Manning (2010) discuss annotation con-
sistency in the Penn Arabic Treebank (ATB), and for
our test corpus we follow their discussion and use
the same data set, the training section of three parts
of the ATB (Maamouri et al, 2008a; Maamouri et
al., 2009; Maamouri et al, 2008b). Their work is
ideal for us, since they used the DECCA algorithm
for the consistency evaluation. They did not use the
?non-fringe? heuristic, but instead manually exam-
ined a sample of 100 nuclei to determine whether
they were annotation errors.
4.1 Inconsistencies Reported
The corpus consists of 598,000 tokens. Table 1 com-
pares token manipulation by the two systems. The
DECCA system6 identified 24,319 distinct variation
nuclei, while our system had 54,496. DECCA ex-
amined 1,158,342 n-grams, consisting of 2,966,274
6We worked at first with version 0.2 of the software. How-
ever this software does not implement the non-fringe heuristic
and does not make available the actual instances of the nuclei
that were found. We therefore re-implemented the algorithm
to make these features available, being careful to exactly match
our output against the released DECCA system as far as the nu-
clei and n-grams found.
instances (i.e., different corpus positions of the n-
grams), while our system examined 605,906 in-
stances of the 54,496 nuclei. For our system, the
number of nuclei increases and the variation n-
grams are eliminated. This is because all nuclei with
more than one instance are evaluated, in order to
search for constituents that have the same root but
different internal structure.
The number of reported inconsistencies is shown
in Table 2. DECCA identified 4,140 nuclei as likely
errors - i.e., contained in larger n-grams, satisfying
the non-fringe heuristic. Our system identified 9,984
nuclei as having inconsistent annotation - i.e., with
at least two instances with different derivation tree
fragments.
4.2 Eliminating Duplicate Nuclei
Some of these 9,984 nuclei are however redundant,
due to nuclei contained within larger nuclei, such as
$rm Al$yx inside qmp $rm Al$yx in (2abc).
Eliminating such duplicates is not just a simple mat-
ter of string inclusion, since the larger nucleus can
sometimes reveal different annotation inconsisten-
cies than just those in the smaller substring nucleus,
and also a single nucleus string can be included in
different larger nuclei. We cannot discuss here the
full details of our solution, but it basically consists
of two steps.
First, as a result of the analysis described so far,
for each nucleus we have a mapping of each instance
of that nucleus to a derivation tree fragment. Sec-
ond, we test for each possible redundancy (meaning
string inclusion) whether there is a true structural re-
dundancy by testing for an isomorphism between the
mappings for two nuclei. For this test corpus, elimi-
nating such duplicates leaves 4,272 nuclei as having
inconsistent annotation. It is unknown how many
of the DECCA nuclei are duplicates, although many
certainly are. For example, qmp $rm Al$yx and
$rm Al$yx are reported as separate results.
4.3 Grouping Inconsistencies by Structure
Across all variation nuclei, there are only a finite
number of derivation tree fragments and thus ways
in which such fragments indicate an annotation in-
consistency. We categorize each annotation incon-
sistency by the inconsistency type, which is simply
a set of numbers representing the different derivation
696
tree fragments. We can then present the results not
by listing each nucleus string, but instead by the in-
consistency types, with each type having some num-
ber of nuclei associated with it.
For example, instances of $rm Al$yx might
have just the derivation tree fragments (a2, a3) and
(b2, b3) in Figure 1, and the numbers representing
this pair is the ?inconsistency type? for this (nucleus,
internal context) inconsistency. There are nine other
nuclei reported as having an inconsistency based on
the exact same derivation tree fragments (abstracting
only away from the particular lexical items), and so
all these nuclei are grouped together as having the
same ?inconsistency type?. This grouping results in
the 4,272 non-duplicate nuclei found being grouped
into 1,911 inconsistency types.
4.4 Precision and Recall
The grouping of internal checking results by incon-
sistency types is a qualitative improvement in con-
sistency reporting, with a high precision.7 By view-
ing inconsistencies by structural annotation types,
we can examine large numbers of nuclei at a time.
Of the first 10 different types of derivation tree in-
consistencies, which include 266 different nuclei, all
10 appear to real cases of annotation inconsistency,
and the same seems to hold for each of the nuclei in
those 10 types, although we have not checked every
single nucleus. For comparison, we chose a sample
of 100 nuclei output by DECCA on this same data,
and by our judgment the DECCA precision is about
74%, including 15 duplicates.
Measuring recall is tricky, even using the errors
identified in Green and Manning (2010) as ?gold?
errors. One factor is that a system might report a
variation nucleus, but still not report all the relevant
instances of that nucleus. For example, while both
systems report $rm Al$yx as a sequence with in-
consistent annotation, DECCA only reports the two
instances that pass the ?non-fringe heuristic?, while
our system lists 132 instances of $rm Al$yx, parti-
tioning them into the two derivation tree fragments.
We will be carrying out a careful accounting of the
recall evaluation in future work.
7
?Precision? here means the percentage of reported varia-
tions that are actually annotation errors.
5 Future Work
While we continue the evaluation work, our pri-
mary concern now is to use the reported inconsistent
derivation tree fragments to correct the annotation
inconsistencies in the actual data, and then evaluate
the effect of the corpus corrections on parsing. Our
system groups all instances of a nucleus into differ-
ent derivation tree fragments, and it would be easy
enough for an annotator to specify which is correct
(or perhaps instead derive this automatically based
on frequencies).
However, because the derivation trees and etrees
are somewhat abstracted from the actual trees in the
treebank, it can be challenging to automatically cor-
rect the structure in every location to reflect the cor-
rect derivation tree fragment. This is because of de-
tails concerning the surrounding structure and the
interaction with annotation style guidelines such as
having only one level of recursive modification or
differences in constituent bracketing depending on
whether a constituent is a ?single-word? or not. We
are focusing on accounting for these issues in cur-
rent work to allow such automatic correction.
Acknowledgments
We thank the computational linguistics group at the
University of Pennsylvania for helpful feedback on
a presentation of an earlier version of this work.
We also thank Spence Green and Chris Manning
for supplying the data used in their analysis of the
Penn Arabic Treebank. This work was supported
in part by the Defense Advanced Research Projects
Agency, GALE Program Grant No. HR0011-06-
1-0003 (all authors) and by the GALE program,
DARPA/CMO Contract No. HR0011-06-C-0022
(first author). The content of this paper does not
necessarily reflect the position or the policy of the
Government, and no official endorsement should be
inferred.
References
Adriane Boyd, Markus Dickinson, and Detmar Meurers.
2007. Increasing the recall of corpus annotation er-
ror detection. In Proceedings of the Sixth Workshop
on Treebanks and Linguistic Theories (TLT 2007),
Bergen, Norway.
697
Tim Buckwalter. 2004. Buckwalter Arabic morphologi-
cal analyzer version 2.0. Linguistic Data Consortium
LDC2004L02.
David Chiang. 2003. Statistical parsing with an auto-
matically extracted tree adjoining grammar. In Data
Oriented Parsing. CSLI.
Markus Dickinson and Detmar Meurers. 2003a. Detect-
ing errors in part-of-speech annotation. In Proceed-
ings of the 10th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL-03), pages 107?114, Budapest, Hungary.
Markus Dickinson and Detmar Meurers. 2003b. Detect-
ing inconsistencies in treebanks. In Proceedings of the
Second Workshop on Treebanks and Linguistic The-
ories (TLT 2003), Sweden. Treebanks and Linguistic
Theories.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In The Companion Vol-
ume to the Proceedings of 41st Annual Meeting of
the Association for Computational Linguistics, pages
205?208, Sapporo, Japan, July. Association for Com-
putational Linguistics.
Spence Green and Christopher D. Manning. 2010. Bet-
ter Arabic parsing: Baselines, evaluations, and anal-
ysis. In Proceedings of the 23rd International Con-
ference on Computational Linguistics (Coling 2010),
pages 394?402, Beijing, China, August. Coling 2010
Organizing Committee.
A.K. Joshi and Y. Schabes. 1997. Tree-adjoining gram-
mars. In G. Rozenberg and A. Salomaa, editors,
Handbook of Formal Languages, Volume 3: Beyond
Words, pages 69?124. Springer, New York.
Yoshihide Kato and Shigeki Matsubara. 2010. Correct-
ing errors in a treebank based on synchronous tree sub-
stitution grammar. In Proceedings of the ACL 2010
Conference Short Papers, pages 74?79, Uppsala, Swe-
den, July. Association for Computational Linguistics.
Seth Kulick and Ann Bies. 2010. A TAG-derived
database for treebank search and parser analysis. In
TAG+10: The 10th International Conference on Tree
Adjoining Grammars and Related Formalisms, Yale.
Mohamed Maamouri, Ann Bies, Seth Kulick, Fatma
Gaddeche, Wigdan Mekki, Sondos Krouna, and
Basma Bouziri. 2008a. Arabic treebank part 1 - v4.0.
Linguistic Data Consortium LDC2008E61, December
4.
Mohamed Maamouri, Ann Bies, Seth Kulick, Fatma
Gaddeche, Wigdan Mekki, Sondos Krouna, and
Basma Bouziri. 2008b. Arabic treebank part 3 - v3.0.
Linguistic Data Consortium LDC2008E22, August 20.
Mohamed Maamouri, Ann Bies, Seth Kulick, Fatma
Gaddeche, Wigdan Mekki, Sondos Krouna, and
Basma Bouziri. 2009. Arabic treebank part 2- v3.0.
Linguistic Data Consortium LDC2008E62, January
20.
698
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 668?673,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Parser Evaluation Using Derivation Trees:
A Complement to evalb
Seth Kulick and Ann Bies and Justin Mott
Linguistic Data Consortium, University of Pennsylvania, Philadelphia, PA 19104
{skulick,bies,jmott}@ldc.upenn.edu
Anthony Kroch and Mark Liberman and Beatrice Santorini
Department of Linguistics, University of Pennsylvania, Philadelphia, PA 19104
{kroch,myl,beatrice}@ling.upenn.edu
Abstract
This paper introduces a new technique for
phrase-structure parser analysis, catego-
rizing possible treebank structures by inte-
grating regular expressions into derivation
trees. We analyze the performance of the
Berkeley parser on OntoNotes WSJ and
the English Web Treebank. This provides
some insight into the evalb scores, and
the problem of domain adaptation with the
web data. We also analyze a ?test-on-
train? dataset, showing a wide variance in
how the parser is generalizing from differ-
ent structures in the training material.
1 Introduction
Phrase-structure parsing is usually evaluated using
evalb (Sekine and Collins, 2008), which provides
a score based on matching brackets. While this
metric serves a valuable purpose in pushing parser
research forward, it has limited utility for under-
standing what sorts of errors a parser is making.
This is the case even if the score is broken down
by brackets (NP, VP, etc.), because the brackets
can represent different types of structures. We
would also like to have answers to such questions
as ?How does the parser do on non-recursive NPs,
separate from NPs resulting from modification?
On PP attachment?? etc.
Answering such questions is the goal of this
work, which combines two strands of research.
First, inspired by the tradition of Tree Adjoin-
ing Grammar-based research (Joshi and Schabes,
1997; Bangalore and Joshi, 2010), we use a de-
composition of the full trees into ?elementary
trees? (henceforth ?etrees?), with a derivation tree
that records how the etrees relate to each other,
as in Kulick et al (2011). In particular, we use
the ?spinal? structure approach of (Shen et al,
2008; Shen and Joshi, 2008), where etrees are con-
strained to be unary-branching.
Second, we use a set of regular expressions
(henceforth ?regexes?) that categorize the possible
structures in the treebank. These are best thought
of as an extension of head-finding rules, which not
only find a head but simultaneously identify each
parent/children relation as one of a limited number
of types of structures (right-modification, etc.).
The crucial step is that we integrate these
regexes into the spinal etrees. The derivation trees
provide elements of a dependency analysis, which
allow us to calculate scores for head identification
and attachment for different projections (e.g., PP).
The regexes allow us to also provide scores based
on spans of different construction types. Together
these two aspects break down the evalb brackets
into more meaningful categories, and the simulta-
neous head and span scoring allows us to separate
these aspects in the analysis.
After describing in more detail the basic frame-
work, we show some aspects of the resulting anal-
ysis of the performance of the Berkeley parser
(Petrov et al, 2008) on three datasets: (a)
OntoNotes WSJ sections 2-21 (Weischedel et al,
2011)
1
, (b) OntoNotes WSJ section 22, and (c)
the ?Answers? section of the English Web Tree-
bank (Bies et al, 2012). We trained the parser on
sections 2-21, and so (a) is ?test-on-train?. These
three results together show how the parser is gen-
eralizing from the training data, and what aspects
of the ?domain adaptation? problem to the web
material are particularly important.
2
2 Framework for analyzing parsing
performance
We first describe the use of the regexes in tree de-
composition, and then give some examples of in-
1
We refer only to the WSJ treebank portion of OntoNotes,
which is roughly a subset of the Penn Treebank (Marcus et
al., 1999) with annotation revisions including the addition of
NML nodes.
2
We parse (c) while training on (a) to follow the procedure
in Petrov and McDonald (2012)
668
corporating these regexes into the derivation trees.
2.1 Use of regular expressions
Decomposing the original phrase-structure tree
into the smaller components requires some
method of determining the ?head? of a nonter-
minal, from among its children nodes, similar to
parsing work such as Collins (1999). As described
above, we are also interested in the type of lin-
guistic construction represented by that one-level
structure, each of which instantiates one of a few
types - recursive coordination, simple head-and-
sister, etc. We address both tasks together with the
regexes. In contrast to the sort of head rules in
(Collins, 1999), these refer as little as possible to
specific POS tags. Instead of explicitly listing the
POS tags of possible heads, the heads are in most
cases determined by their location in the structure.
Sample regexes are shown in Figure 1. There
are 49 regexes used.
3
Regexes ADJP-t and
ADVP-t in (a) identify their terminal head to
be the rightmost terminal, possibly preceded by
some number of terminals or nonterminals, rely-
ing on a mapping that maps all terminals (except
CC, which is mapped to CONJ) to TAG and all
nonterminals (except CONJP and NML) to NT.
Structures with a CONJ/CONJP/NML child do
not match this rule and are handled by different
regexes, which are all mutually exclusive. In some
cases, we need to search for particular nonterminal
heads, such as with the (b) regexes S-vp and SQ-
vp, which identify the rightmost VP among the
children of a S or SQ as the head. (c) NP-modr
is a regex for a recursive NP with a right modifier.
In this case, the NP on the left is identified as the
head. (d) VP-crd is also a regex for a recursive
structure, in this case for VP coordination, pick-
ing out the leftmost conjunct as the head of the
structure. The regex names roughly describe their
purpose - ?mod? for right-modification, ?crd? for
coordination, etc. The suffix ?-t? is for the simple
non-recursive case in which the head is a terminal.
2.2 Regexes in the derivation trees
The names of these regexes are incorporated into
the etrees themselves, as labels of the nontermi-
nals. This allows an etree to contain information
3
Some among the 49 are duplicates, used for different
nonterminals, as with (a) and (b) in Figure 1. We derived
the regexes via an iterative process of inspection of tree de-
composition on dataset (a), together with taking advantage of
the treebanking experience from some of the co-authors.
(a)ADJP-t,ADVP-t:
?(TAG|NT|NML)
*
(head:TAG) (NT)
*
$
(b)S-vp, SQ-vp: ?([? ]+)
*
(head:VP)$
(c)NP-modr:
?(head:NP)(SBAR|S|VP|ADJP|PP|ADVP|NP)+$
(d)VP-crd: ?(head:VP) (VP)
*
CONJ VP$
Figure 1: Some sample regexes
such as ?this node represents right modification?.
For example, Figure 2 shows the derivation tree
resulting from the decomposition of the tree in
Figure 4. Each structure within a circle is one
etree, and the derivation as a whole indicates how
these etrees are combined. Here we indicate with
arrows that point to the relevant regex. For ex-
ample, the PP-t etree #a6 points to the NP-modr
regex, which consists of the NP-t together with
the PP-t. The nonterminals of the spinal etrees are
the names of the regexes, with the simpler non-
terminal labels trivially derivable from the regex
names.
4
The tree in Figure 5 is the parser output corre-
sponding to the gold tree in Figure 4, and in this
case gets the PP-t attachment wrong, while every-
thing else is the same as the gold.
5
This is reflected
in the derivation tree in Figure 3, in which the NP-
modr regex is absent, with the NP-t and PP-t etrees
#b5 and #b6 both pointing to the VP-t regex in
#b3. We show in Section 2.3 how this derivation
tree representation is used to score this attachment
error directly, rather than obscuring it as an NP
bracket error as evalb would do.
2.3 Scoring
We decompose both the gold and parser output
trees into derivation trees with spinal etrees, and
score based on the regexes projected by each word.
There is a match for a regex if the corresponding
words in gold/parser files project to that regex, a
precision error if the parser file does but the gold
does not, and a recall error if the gold does but the
parser file does not.
For example, comparing the trees in Figures 4
and 5 via their derivation trees in Figures 2 and
Figures 3, the word ?trip? has a match for the regex
NP-t, but a recall error for NP-modr. The word
4
We do not have space here to discuss the data structure
in complete detail, but multiple regex names at a node, such a
VP-aux and VP-t at tree a3 in Figure 2, indicate multiple VP
nonterminals.
5
We leave function tags aside for future work. The gold
tree is shown without the SBJ function tag.
669
#a1 TheyNP-t tripNP-modr  NP-t toPP-t#a6
#a3
#a4themake
VP-auxVP-tS-vp#a2will #a7FloridaNP-t
#a5
Figure 2: Derivation Tree for Figure 4
#b1TheyNP-t toPP-t#b6tripNP-t#b4themake
VP-auxVP-t#b3 S-vp#b2will #b7FloridaNP-t
#b5
Figure 3: Derivation Tree for Figure 5)
S
NP
They
VP
will VP
make NP
NP
the trip
PP
to NP
Florida
S
NP
They
VP
will VP
make NP
the trip
PP
to NP
Florida
1
Figure 4: Gold tree
S
NP
They
VP
will VP
make NP
NP
the trip
PP
to NP
Florida
S
NP
They
VP
will VP
make NP
the trip
PP
to NP
Florida
1
Figure 5: Parser output tree
Corpus tokens brackets coverage % evalb
2-21 g 650877 578597 571243 98.7
p 575744 569480 98.9 93.8
22 g 32092 24819 24532 98.8
p 24801 24528 98.9 90.1
Ans g 53960 48492 47348 97.6
p 48750 47423 97.3 80.8
Table 1: Corpus information for gold(g) and
parsed(p) sections of each corpus
?make? has a match for the regexes VP-t, VP-
aux, and S-vp, and so on. Summing such scores
over the corresponding gold/parser trees gives us
F-scores for each regex.
There are two modifications/extensions to these
F-scores that we also use:
(1) For each regex match, we score whether it
matches based on the span as well. For exam-
ple, ?make? is a match for VP-t in Figures 2
and 3, and is also a match for the span as well,
since in both derivation trees it includes the words
?make. . .Florida?. It is this matching for span as
well as head that allows us to compare our results
to evalb. We call the match just for the head the ?F-
h? score and the match that also includes the span
information the ?F-s? score. The F-s score roughly
corresponds to the evalb score. However, the F-
s score is for separate syntactic constructions (in-
cluding also head identification), although we can
also sum it over all the structures, as done later in
Figure 6. The simultaneous F-h and F-s scores let
us identify constructions where the parser has the
head projection correct, but gets the span wrong.
(2) Since the derivation tree is really a depen-
dency tree with more complex nodes (Rambow
and Joshi, 1997; Kulick et al, 2012), we can also
score each regex for attachment.
6
For example,
while ?to? is a match for PP-t, its attachment is
not, since in Figure 2 it is a child of the ?trip? etree
(#a5) and in Figure 3 it is a child of the ?make?
etree (#b3). Therefore our analysis results in an
attachment score for every regex.
2.4 Comparison with previous work
This work is in the same basic line of research
as the inter-annotator agreement analysis work in
Kulick et al (2013). However, that work did
not utilize regexes, and focused on comparing se-
quences of identical strings. The current work
scores on general categories of structures, without
6
A regex intermediate in a etree, such as VP-t above, is
considered to have a default null attachment. Also, the at-
tachment score is not relevant for regexes that already express
a recursive structure, such as NP-modr. In Figure 2, NP-t in
etree #a5 is considered as having the attachment to #a3.
670
Sections 2-21 (Ontonotes) Section 22 (Ontonotes) Answers (English Web Treebank)
regex %gold F-h F-s att spanR %gold F-h F-s att spanR %gold F-h F-s att spanR
NP-t 30.7 98.9 97.6 96.5 99.6 31.1 98.0 95.8 94.4 99.6 28.5 95.4 91.5 90.9 99.3
VP-t 13.5 98.8 94.5 98.4 95.8 13.4 98.1 91.7 97.3 93.7 16.0 96.7 81.7 96.1 85.4
PP-t 12.2 99.2 91.0 90.5 92.0 12.1 98.7 86.4 86.1 88.2 8.4 96.4 80.5 80.7 84.7
S-vp 12.2 97.9 92.8 96.8 96.3 11.9 96.5 89.1 95.4 95.0 14.2 94.1 72.9 88.0 84.1
NP-modr 8.6 88.4 80.3 - 91.5 8.5 82.9 71.8 - 87.9 4.4 69.0 54.2 - 80.5
VP-aux 5.5 97.9 94.0 - 96.1 5.0 96.5 91.0 - 94.6 6.2 94.4 81.7 - 86.7
SBAR-s 3.7 96.1 91.1 91.8 95.3 3.5 94.3 87.2 86.4 93.5 4.0 84.8 68.2 81.9 81.9
ADVP-t 2.7 95.2 93.3 93.9 98.6 3.0 89.6 84.5 88.0 95.9 4.5 84.0 78.2 80.3 96.8
NML-t 2.3 91.6 90.3 97.6 99.8 2.6 85.6 82.2 93.5 99.8 0.7 42.1 37.7 88.8 100.0
ADJP-t 1.9 94.6 88.4 95.5 94.6 1.8 86.8 77.0 93.6 90.7 2.5 84.7 67.0 88.1 84.2
QP-t 1.0 95.3 93.8 98.3 99.6 1.2 91.0 89.0 97.1 100.0 0.2 57.7 57.7 94.4 100.0
NP-crd 0.8 80.3 73.7 - 92.4 0.6 68.6 58.4 - 86.1 0.5 55.3 47.8 - 88.1
VP-crd 0.4 84.3 82.8 - 98.2 0.4 75.3 73.5 - 97.6 0.8 65.5 58.3 - 89.8
S-crd 0.3 83.7 83.2 - 99.6 0.4 70.9 68.6 - 96.7 0.8 68.5 63.0 - 93.4
SQ-v 0.1 88.3 82.0 93.3 97.8 0.1 66.7 66.7 88.9 100.0 0.9 81.9 72.4 93.4 95.8
FRAG-nt 0.1 49.9 48.6 95.4 97.9 0.1 28.6 28.6 100.0 100.0 0.8 22.7 21.3 96.3 96.3
Table 2: Scores for the most frequent categories of brackets in the three datasets of corpora, as determined
by the regexes. % gold is the frequency of this regex type compared to all the brackets in the gold. F-h
is the score based on matching heads, F-s also incorporates the span information, att is the attachment
accuracy for words that match in F-h, and spanR is the span-right accuracy for words that match in F-h.
the reliance on sequences of individual strings.
7
3 Analysis of parsing results
We worked with the three datasets as described
in the introduction. We trained the parser on sec-
tions 2-21 of OntoNotes WSJ, and parsed the three
datasets with the gold tags, since at present we
wish to analyze the parser performance in isola-
tion from Part-of-Speech tagging errors. Table 1
shows the sizes of the three corpora in terms of
tokens and brackets, for both the gold and parsed
versions, with the evalb scores for the parsed ver-
sions. The score is lower for Answers, as also
found by Petrov and McDonald (2012).
To facilitate comparison of our analysis with
evalb, we used corpora versions with the same
bracket deletion (empty yields and most punctua-
tion) as evalb. We ran the gold and parsed versions
through our regex decomposition and derivation
tree creation. Table 1 shows the number and per-
centage of brackets handled by our regexes. The
high coverage (%) reinforces the point that there is
a limited number of core structures in the treebank.
In the results below in Table 2 and Figure 6 we
combine the nonterminals that are not covered by
one of the regexes with the simple non-recursive
regex case for that nonterminal.
8
7
In future work we will compare our approach to that
of Kummerfeld et al (2012), who also move beyond evalb
scores in an effort to provide more meaningful error analysis.
8
We also combine a few other non-recursive regexes to-
gether with NP-t, such as the special one for possessives.
We present the results in two ways. Table 2 lists
the most frequent categories in the three datasets,
with their percentage of the overall number of
brackets (%gold), their score based just on the
head identification (F-h), their score based on head
identification and (left and right) span (F-s), and
the attachment (att) and span-right (spanR) scores
for those that match based on the head.
9
The two graphs in Figure 6 show the cumu-
lative results based on F-h and F-s, respectively.
These show the cumulative score in order of the
frequency of categories. For example, for sections
2-21, the score for NP-t is shown first, with 30.7%
of the brackets, and then together with the VP-t
category, they cover 45.2% of the brackets, etc.
10
The benefit of the approach described here is that
now we can see the contribution to the evalb score
of the particular types of constructions, and within
those constructions, how well the parser is doing
at getting the same head projection, but failing or
9
The score for the left edge is almost always very high for
every category, and we just list here the right edge score. The
attachment score does not apply to the recursive categories,
as mentioned above.
10
The final F-s value is lower than the evalb score - e.g.
92.5 for sections 2-21 (the rightmost point in the graph for
sections 2-21 in the F-s graph in Figure 6) compared to the
93.8 evalb score. Space prevents full explanation, but there
are two reasons for this. One is that there are cases in which
bracket spans match, but the head, as found by our regexes, is
different in the gold and parser trees. The other cases is when
brackets match, and may even have the same head, but their
regex is different. In future work we will provide a full ac-
counting of such cases, but they do not affect the main aspects
of the analysis.
671
F-scores by head identification
cumulative % of all brackets
0 5 10 20 30 40 50 60 70 80 90 100
89
.2
91
.2
93
.2
95
.2
97
.2
99
.0
2-21
1 2
3
4
5
6
7
8
9
10
11
14
13
15
12
22
1 2
3
4
5
6
7
8
9
10
11
14
13
15
12
answers
1
2
4
3
6
8
5
7
10
12
13
14
15
9
11
 1:NP-t     2:VP-t
 3:PP-t     4:S-vp
 5:NP-modr  6:VP-aux
 7:SBAR-s   8:ADVP-t
 9:NML-t   10:ADJP-t
11:QP-t    12:SQ-vp
13:S-crd   14:VP-crd
15:FRAG-nt
F-scores by head identification and span
cumulative % of all backets
0 5 10 20 30 40 50 60 70 80 90 100
78
.0
82
.0
86
.0
90
.0
94
.0
97
.6
2-21
1
2
3
4
5
6
7
8
9
10
11
14
13
15
12
22
1
2
3
4
5
6
7
8
9
10
11
14
13
15
12
answers
1
2
4
3
6
8
5
7
10 12
13
14
15
9
11
 1:NP-t     2:VP-t
 3:PP-t     4:S-vp
 5:NP-modr  6:VP-aux
 7:SBAR-s   8:ADVP-t
 9:NML-t   10:ADJP-t
11:QP-t    12:SQ-vp
13:S-crd   14:VP-crd
15:FRAG-nt
Figure 6: Cumulative scores based on F-h (left) and F-s (right). These graphs are both cumulative in
exactly the same way, in that each point represents the total percentage of brackets accounted for so far.
So for the 2-21 line, point 1, meaning the NP non-recursive regex, accounts for 30.7% of the brackets,
point 2, meaning the VP non-recursive regex, accounts for another 13.5%, so 44.2% cumulatively, etc.
not on the spans.
3.1 Analysis and future work
As this is work-in-progress, the analysis is not yet
complete. We highlight a few points here.
(1) The high performance on the OntoNotes WSJ
material is in large part due to the score on the
non-recursive regexes of NP-t, VP-t, S-vp, and the
auxiliaries (points 1, 2, 4, 6 in the graphs). Critical
to this is the fact that the parser does well on deter-
mining the right edge of verbal structures, which
affects the F-s score for VP-t (non-recursive), VP-
aux, and S-vp. The spanR score for VP-t is 95.8
for Sections 2-21 and 93.7 for Section 22.
(2) We wouldn?t expect the test-on-training evalb
score to be 100%, since it has to back off from
the training data, but the results for the different
categories vary widely, with e.g., the NP-modr F-
h score much lower than other frequent regexes.
This variance from the test-on-training dataset car-
ries over almost exactly to Section 22.
(3) The different distribution of structures in
Answers hurts performance. For example, the
mediocre performance of the parser on SQ-vp
barely affects the score with OntoNotes, but has
a larger negative effect with Answers, due to its
increased frequency in the latter.
(4) While the different distribution of construc-
tions is a problem for Answers, more critical is
the poor performance of the parser on determin-
ing the right edge of verbal constructions. This is
only 85.4 for VP-t in Answers, compared to the
OntoNotes results mentioned in (1). Since this af-
fects the F-s scores for VP-t, VP-aux, and S-vp,
the negative effect is large. Preliminary investi-
gation shows that this is due in part to incorrect
PP and SBAR placement (the PP-t and SBAR-s
attachment scores (80.7 and 81.9) are worse for
Answers compared to Section 22 (86.1 and 86.4)),
and coordinated S-clauses with no conjunction.
In sum, there is a wealth of information from
this new type of analysis that we will use in our on-
going work to better understand what the parser is
learning and how it works on different genres.
Acknowledgments
This material is based upon work supported by Na-
tional Science Foundation Grant # BCS-114749
(first, fourth, and sixth authors) and by the Defense
Advanced Research Projects Agency (DARPA)
under Contract No. HR0011-11-C-0145 (first,
second, and third authors). The content does not
necessarily reflect the position or the policy of the
Government, and no official endorsement should
be inferred.
672
References
Srinivas Bangalore and Aravind K. Joshi, editors.
2010. Supertagging: Using Complex Lexical De-
scriptions in Natural Language Processing. MIT
Press.
Ann Bies, Justin Mott, Colin Warner, and Seth Kulick.
2012. English Web Treebank. LDC2012T13. Lin-
guistic Data Consortium.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
Department of Computer and Information Sciences,
University of Pennsylvania.
A.K. Joshi and Y. Schabes. 1997. Tree-adjoining
grammars. In G. Rozenberg and A. Salomaa, ed-
itors, Handbook of Formal Languages, Volume 3:
Beyond Words, pages 69?124. Springer, New York.
Seth Kulick, Ann Bies, and Justin Mott. 2011. Using
derivation trees for treebank error detection. Asso-
ciation for Computational Linguistics.
Seth Kulick, Ann Bies, and Justin Mott. 2012. Using
supertags and encoded annotation principles for im-
proved dependency to phrase structure conversion.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 305?314, Montr?eal, Canada, June. Associa-
tion for Computational Linguistics.
Seth Kulick, Ann Bies, Justin Mott, Mohamed
Maamouri, Beatrice Santorini, and Anthony Kroch.
2013. Using derivation trees for informative tree-
bank inter-annotator agreement evaluation. In Pro-
ceedings of the 2013 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
550?555, Atlanta, Georgia, June. Association for
Computational Linguistics.
Jonathan K. Kummerfeld, David Hall, James R. Cur-
ran, and Dan Klein. 2012. Parser showdown at the
wall street corral: An empirical investigation of error
types in parser output. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 1048?1059, Jeju Island,
Korea, July. Association for Computational Linguis-
tics.
Mitchell P. Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor. 1999. Treebank-3.
LDC99T42, Linguistic Data Consortium, Philadel-
phia.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 shared task on parsing the web. In Pro-
ceedings of the First Workshop on Syntactic Analysis
of Non-Canonical Language.
Slav Petrov, Leon Barrett, Romain Thibaux, and
Dan Klein. 2008. The Berkeley Parser.
https://code.google.com/p/berkeleyparser/.
Owen Rambow and Aravind Joshi. 1997. A formal
look at dependency grammars and phrase-structure
grammars, with special consideration of word-order
phenomena. In L. Wanner, editor, Recent Trends
in Meaning-Text Theory, pages 167?190. John Ben-
jamins, Amsterdam and Philadelphia.
Satoshi Sekine and Michael Collins. 2008. Evalb.
http://nlp.cs.nyu.edu/evalb/.
Libin Shen and Aravind Joshi. 2008. LTAG depen-
dency parsing with bidirectional incremental con-
struction. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 495?504, Honolulu, Hawaii, October. As-
sociation for Computational Linguistics.
Libin Shen, Lucas Champollion, and Aravind Joshi.
2008. LTAG-spinal and the Treebank: A new
resource for incremental, dependency and seman-
tic parsing. Language Resources and Evaluation,
42(1):1?19.
Ralph Weischedel, Martha Palmer, Mitchell Marcus,
Eduard Hovy, Sameer Pradhan, Lance Ramshaw,
Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle
Franchini, Mohammed El-Bachouti, Robert Belvin,
and Ann Houston. 2011. OntoNotes 4.0. Linguistic
Data Consortium LDC2011T03.
673
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 1?10,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Automatic Correction and Extension of Morphological Annotations
Ramy Eskander, Nizar Habash
Center for Computational Learning Systems, Columbia University
{reskander,habash}@ccls.columbia.edu
Ann Bies, Seth Kulick, Mohamed Maamouri
Linguistic Data Consortium, University of Pennsylvania
{bies,skulick,maamouri}@ldc.upenn.edu
Abstract
For languages with complex morpholo-
gies, limited resources and tools, and/or
lack of standard grammars, developing an-
notated resources can be a challenging
task. Annotated resources developed un-
der time/money constraints for such lan-
guages tend to tradeoff depth of represen-
tation with degree of noise. We present
two methods for automatic correction and
extension of morphological annotations,
and demonstrate their success on three di-
vergent Egyptian Arabic corpora.
1 Introduction
Annotated corpora are essential for most research
in natural language processing (NLP). For exam-
ple, the development of treebanks, such as the
Penn Treebank and the Penn Arabic Treebank,
has been essential in pushing research on part-
of-speech (POS) tagging and parsing of English
and Arabic (Marcus et al, 1993; Maamouri et al,
2004). The creation of such resources tends to be
quite expensive and time consuming: guidelines
need to be developed, annotators hired, trained,
and regularly evaluated for quality control. For
languages with complex morphologies, limited re-
sources and tools, and/or lack of standard gram-
mars, such as any of the Dialectal Arabic (DA)
varieties, developing annotated resources can be a
challenging task. As a result, annotated resources
developed under time/money constraints for such
languages tend to tradeoff depth of representation
with degree of noise. In the extremes, we find rich
morphological representations that may be noisy
and inconsistent or simple by highly consistent
and reliable annotations that have limited usabil-
ity. Furthermore, such resources are often devel-
oped by different research groups leading to many
inconstancies that make pooling these resources
not a very easy task.
In this paper, we describe two general tech-
niques to address the limitations of the two types
of annotations: corrections of rich noisy annota-
tions and extensions of clean but shallow ones.
We present our work on Egyptian Arabic, an im-
portant Arabic dialect with limited resources, and
rich and ambiguous morphology. Resulting from
this effort is the largest Egyptian Arabic corpus
annotated in one common representation by pool-
ing resources from three very different sources:
a non-final, pre-release version of the ARZ1 cor-
pora from the Linguistic Data Consortium (LDC)
(Maamouri et al, 2012g), the LDC?s CallHome
Egypt transcripts (Gadalla et al, 1997) and CMU?s
Egyptian Arabic corpus (CMUEAC) (Mohamed et
al., 2012).
Although the paper focuses on Arabic, the ba-
sic problem is relevant to other languages, espe-
cially spontaneously written colloquial language
forms such as those used in social media. The
general solutions we propose are language inde-
pendent given availability of specific language re-
sources.
Next we discuss some related work and rel-
evant linguistic facts (Sections 2 and 3, respec-
tively). Section 4 presents our annotation cor-
rection technique; and Section 5 presents out an-
notation extension technique. Finally, Section 6
presents some statistics on the Egyptian Arabic
corpus annotated in one unified representation re-
sulting from our correction and extension work.
2 Related Work
Much work has been done on automatic spelling
correction. Both supervised and unsupervised ap-
proaches have been used employing a variety of
1ARZ is the language code for Egyptian Ara-
bic, http://www-01.sil.org/iso639-3/
documentation.asp?id=arz
1
tools, resources, and heuristics, e.g., morpholog-
ical analyzers, language models, annotated data
and edit-distance measures, respectively (Kukich,
1992; Oflazer, 1996; Shaalan et al, 2003; Hassan
et al, 2008; Kolak and Resnik, 2002; Magdy and
Darwish, 2006). Our work is different from these
approaches in that it extends beyond spelling of
word forms to deeper annotations. However, we
use some of these techniques to correct not just
the words, but also malformed POS tags.
A number of efforts exist on treebank en-
richment for many languages including Arabic
(Palmer et al, 2008; Hovy et al, 2006; Alkuh-
lani and Habash, 2011; Alkuhlani et al, 2013).
Our morphological extension effort is similar to
Alkuhlani et al (2013)?s work except that they
start with tokenizations, reduced POS tags and de-
pendency trees and extend them to full morpho-
logical information.
There has been a lot of work on Arabic POS tag-
ging and morphological disambiguation (Habash
and Rambow, 2005; Smith et al, 2005; Hajic? et
al., 2005; Habash, 2010; Habash et al, 2013).
The work by Habash et al (2013) uses one of the
resources we improve on in this paper. In their
work, they simply attempt to ?synchronize? un-
known/malformed annotations with the morpho-
logical analyzer they use, thus forcing a reading on
the word to make the unknown/malformed annota-
tion usable. In our work, we address the cleaning
issue directly. We intend to make these automatic
corrections and extensions available in the future
so that they can be used in future disambiguation
tools.
Maamouri et al (2009) described a set of man-
ual and automatic techniques used to improve on
the quality of the Penn Arabic Treebank. Their
work is most similar to ours except in the follow-
ing aspects: we work only on morphology and for
dialectal Arabic, whereas their work is primarily
on syntax and standard Arabic. Furthermore, the
challenge of malformed tags is not a major prob-
lem for them, while it is a core problem for us.
Furthermore, we work with data that has partial
annotations that we extend, while their work was
for very rich syntax/morphology annotations.
3 Linguistic Facts
The Arabic language is a collection of variants,
most prominent amongst which is Modern Stan-
dard Arabic (MSA), the official language of the
media and education. The other variants, the Ara-
bic dialects, are the day-to-day native vernaculars
spoken in the Arab World. While MSA is the of-
ficial language, it is not the native language of any
modern day Arabic speakers. Their differences
from MSA are comparable to the differences be-
tween Romance languages and Latin.2
Egyptian Arabic poses many challenges for
NLP. Arabic in general is a morphologically com-
plex language which includes rich inflectional
morphology, expressed both templatically and af-
fixationally, and several classes of attachable cl-
itics. For example, the Egyptian Arabic word
A??J.

J?J
?? wi+ha+yi-ktib-uw+hA
3 ?and they will
write it? has two proclitics (+? wi+ ?and? and + ?
ha+ ?will?), one prefix -?


yi- ?3rd person?, one
suffix ?- -uw ?masculine plural? and one pronom-
inal enclitic A?+ +hA ?it/her?. The word is consid-
ered an inflected form of the lemma katab ?write
[lit. he wrote]?. An important challenge for NLP
work on dialectal Arabic in general is the lack of
an orthographic standard. Egyptian Arabic writ-
ers are often inconsistent even in their own writ-
ing (Habash et al, 2012a), e.g., the future particle
h Ha appears as a separate word or as a proclitic
+h/+? Ha+/ha+, reflecting different pronuncia-
tions. Arabic orthography in general drops dia-
critical marks that mark short vowels and gemi-
nation. However in analyses, we want these dia-
critics to be indicated. Moreover, some letters in
Arabic (in general) are often spelled inconsistently
which leads to an increase in both sparsity (multi-
ple forms of the same word) and ambiguity (same
form corresponding to multiple words), e.g., vari-
ants of Hamzated Alif,

@ ? or @ A?, are often writ-
ten without their Hamza (Z ?): @ A; and the Alif-
Maqsura (or dotless Ya) ? ? and the regular dotted
Ya ?


y are often used interchangeably in word fi-
nal position (El Kholy and Habash, 2010). For the
purposes of normalizing the representations used
in computational models, we follow the work of
Habash et al (2012a) who devised a conventional
orthography for dialectal Arabic (CODA) for use
in computational processing of Arabic dialects..
An analysis of an Egyptian word for our work
consists of a surface form that may not be in
2Habash and Rambow (2006) reported that a state-of-the-
art MSA morphological analyzer has only 60% coverage of
Levantine Arabic verb forms.
3Arabic orthographic transliteration is presented in the
Habash-Soudi-Buckwalter scheme (Habash et al, 2007):
@ H.
H H h. h p X
	
XP 	P ? ? ?
	
? ?
	
? ?
	
?
	
?

? ? ? ?
	
? ? ? ?


A b t ? j H x d?r z s ? S D T D? ? ? f q k l m n hw y
in addition to ? Z, ?

@, A? @, A?

@, w? ?', y? Z?', ~ ?, ? ?.
2
CODA (henceforth, RAW), a fully diacritized
CODA form (henceforth, DIAC), a morpheme
split form (henceforth, MORPH), which may
slightly differ from the allomorphic DIAC surface
forms, a POS tag for each morpheme and stem,
and a lemma (henceforth LEM). For instance,
the Egyptian Arabic example used above has the
following analysis:
RAW whyktbuwhA
DIAC wiHayiktibuwhA
MORPH wi+Ha+yi+ktib+uwA+hA
POS CONJ+FUT_PART+IV3P+IV
+IVSUFF_SUBJ:3P+IVSUFF_DO:3FS
LEM katab
The morphological analyzers we use in the pa-
per, CALIMA (Habash et al, 2012b) and SAMA
(Graff et al, 2009), both generate the different lev-
els of representation discussed above.
4 Automatic Morphological Correction
In this section, we present the effort on auto-
matic morphological correction of rich noisy an-
notations. We next describe the data set we work
with and the problems it has. This is followed by
a discussion of our approach and results including
an error analysis.
4.1 Data
We use a non-final, pre-release version of six man-
ually annotated Egyptian Arabic corpora devel-
oped by the LDC, and labeled as ?ARZ?, parts one
through six. The published versions of these cor-
pora (Maamouri et al, 2012a-f) do not include the
annotation errors discussed in this paper. Rather,
in the official releases of the data from the LDC,
such problematic cases with an unknown POS tag
sequence (as in the example at the end of Sec-
tion 4.2) were caught and given a NO_FUNC POS
tag instead, in order to allow syntactic annotation
of the data to proceed, and in order to meet data
publication deadlines. The combined corpus con-
sists of about 274K words. The annotations are
very detailed contextually selected morphological
analyses that include for each RAW word its LEM,
POS, MORPH and DIAC as described earlier. The
LDC used the CALIMA4 Egyptian Arabic mor-
phological analyzer (Habash et al, 2012b) to pro-
vide the annotators with sets of analyses to se-
lect from.5 CALIMA?s non-lexical morphologi-
4Columbia Arabic Language and dIalect Morphological
Analyzer
5SAMA, the Standard Arabic Morphological Analyzer
(Graff et al, 2009), was used to provide the annotators with
cal coverage (i.e. model of affixes and stem POS
combinations) is almost complete; and its lexical
entries are of high precision. However, CALIMA
lacks some lexical items, i.e., its lexical recall is
not perfect ? Habash et al (2012b) report coverage
of 84% for basic CALIMA and 92% for CALIMA
extended with SAMA (Graff et al, 2009) (hence-
forth, CALIMA+SAMA or simply the analyzer).6
Many missing entries are a result of spelling vari-
ants that are not modeled in CALIMA. In cases
when CALIMA fails to provide analyses or the
annotators disagree with all the provided analy-
ses, the annotators enter the information manually
or copy and modify CALIMA provided analyses,
which sometimes introduces errors.
For the purpose of this work, we consider
all analyses in the corpus that are in the CAL-
IMA+SAMA morphological analyzer to be cor-
rect. We will not attempt to modify them. Al-
most 30% of the corpus analyses are not in the
analyzer, i.e. analyzer out-of-vocabulary (OOV).
We discuss next the general patterns of these anal-
yses. We refer to the original corpus analyses as
the ?Baseline? analyses.
4.2 Patterns of OOV Analyses in Baseline
About 3.3% of all OOV analyses (and 1% of all
corpus words) are tagged as TYPOs.7 We do not
address these cases in this paper.
Over half of the POS OOVs (56%) in the
pre-release data involve a different category of
a nominal (NOUN/NOUN_PROP/ADJ). This is
a well known issue even in MSA. The rest
of the cases involve incorrect feature combina-
tions such as giving the unaccusative verb
	
Y
	
?
	
J

K @
Aitnaf?i? ?be performed? the POS PV_PASS
(passive perfective).8 Another example is assign-
ing the feminine singular pronoun ?


X diy the
POS DEM_PRON instead of DEM_PRON_FS.
Or the imperative verb @? 	?? @ AilguwA ?cancel [you
plural]? the POS CV+CVSUFF_SUBJ:2MS (for
?you masculine singular?) instead of the correct
CV+CVSUFF_SUBJ:2MP. A tiny percentage of
all POS tags in the corpus (0.02%) include case-
related variation (e.g. CONJ vs Conj); these add
to type sparsity, but are trivial to handle.
analyses for the MSA tokens.
6In our work, we distinguish between morphological anal-
ysis, which refers to producing the various readings of a word
out of context, and morphological tagging (or disambigua-
tion), which identifies the appropriate analysis in context.
7The rate of TYPO words in the ARZ data is almost 18
times the rate in the MSA PATB data sets.
8The inflected verb Aitnaf?i? is the passive voice of the
verb with the lemma naf?a? or the active voice of the verb
with the lemma Aitnaf?i?.
3
Among LEMs and DIACs, there is consider-
able variation in the Arabic spelling, particularly
involving the spelling of Alif/Hamza forms, the
Egyptian long vowels /e:/ and /o:/ and often re-
quiring adjustment to conform to CODA guide-
lines.9 The following are some examples. Specific
CODA cases include spelling ?Y? kidah ?as such?
as @Y? kdA or spelling ?


?

? qawiy [pronounced
/awi/] ?very? as ?


?@ Awy. The preposition ?J

	
? fiyh
?in it? is incorrectly spelled as fiyuh (allomorphic
form is incorrect). The word I
K. bayt ?house? is
spelled biyt (long vowel spelling error). And fi-
nally the interjection

B l? ?no!? is spelled as (the
implausible form) Z? la?.
Among LEMs, over 63% of the errors is due
to inconsistency in assigning lemmas of punctu-
ation and digit, a trivial challenge. 29% of the
cases are spelling errors such as those discussed
above. The remaining 10% are due to not follow-
ing the specific format guidelines of lemmas (e.g.,
must be singular, uncliticized, and with a sense id
number). Among DIACs, almost all of the mis-
matches are non-CODA-compliant spelling varia-
tions. One third is Alif/Hamza forms, and another
quarter is long vowel spelling. One eighth involves
diacritic choice.
Combinations of these error types occur,
of course. One extreme case is the pro-
gressive particle prefix bi, which should be
tagged as bi/PROG_PART, but appears addi-
tionally as b/PROG_PART, ba/PROG_PART,
bi/PART_PROG, bi/PRO_PART, and
bi/FUT_PART.
Example For the rest of this section, we con-
sider the example word @??g.

AJ
k Hy?jlwA ?and
they will postpone?. Figure 1 contrasts an erro-
neous analysis in the pre-release data with a cor-
rected version of it. There are multiple problems
in this example. First, the POS tag is both in-
ternally inconsistent and is inconsistent with the
MORPH choice. The POS has a singular subject
prefix (IV3MS) and a plural subject suffix (IV-
SUFF_SUBJ:P); and the plural subject suffix is
written using the morpheme (+uh), which corre-
sponds to a direct object enclitic. The two mor-
phemes, +uh and +uwA, are homophonous, which
is the most likely cause for this error. Second, the
future marker (Ha+) is written in a non-CODA-
9LDC annotators were not asked to comply with CODA
guidelines during the annotation task. Therefore, multiple
spelling variants for OOV Egyptian Arabic words were to be
expected.
compliant way (ha+) in the analysis. And finally,
the lemma is malformed, containing multiple ex-
tra sense id digits. It is important to point out
that there are multiple ways to correct the anal-
ysis. For example, it can be Ha+yi+?aj?il+uh
FUT_PART+IV3MS+IV+IVSUFF_DO:3MS ?he
will postpone it?.10
4.3 Approach
Our target is to provide correct morphological
analyses for the OOV annotations in the pre-
release version of the ARZ corpus. Since not
all of the OOV annotations are wrong in prin-
ciple, we do not force map them all to CAL-
IMA+SAMA in-vocabulary variants, especially
for open class categories, where we know CAL-
IMA+SAMA may be deficient. As such, our gen-
eral solution focuses on correcting closed classes
(some stems and all of the affixes) by mapping
them to in-vocabulary variants. We also use a set
of language-specific preprocessing corrections for
common orthographic variations (for all open and
closed classes). An important tool we use through-
out to rank choices and break ties is modified Lev-
enshtein edit distance.11
Next, we present the four steps of our correction
process: annotation preprocessing, morpheme-
POS correction, lemma correction and surface
DIAC generation.
Annotation Preprocessing When first reading
the pre-release annotations, we perform a prepro-
cessing step that includes a set of deterministic
corrections for common non-CODA-compliant or-
thographic variations and errors, and POS tagging
typos. The corrections apply to the POS tags, lem-
mas, morphemes and surface forms. Examples of
these corrections include the following: reorder-
ing diacritics, e.g., saji?l? saj?il; removing du-
plicate diacritics, e.g., saj?iil? saj?il; adjusting
Alif-Hamza forms to match the diacritics that fol-
10Since our approach currently considers words out of con-
text, such a correction is not preferred because it requires
more character edits (see Figure 2). We acknowledge this
to be a limitation and plan to address it in the future.
11The Levenshtein edit distance is defined as the minimum
number of single-character edits (insertion, deletion and sub-
stitution) required to change one string into the other. For
Arabic words and morphemes, we modify the cost of sub-
stitutions involving two phonologically or orthographically
similar letters to count as half edits. We acquire the list of
such letter substitutions from Eskander et al (2013), who re-
port them as the most frequent source of errors in Egyptian
Arabic orthography. We map all diacritic-only morphemes
to empty morphemes in both ways at a cost of half edit also.
For POS tag edit distance, we use the standard definition of
Levenshtein edit distance. Edit cost is an area where a lot of
tuning could be done and we plan to explore it in the future.
4
RAW ??g. AJ
? hyAjlw
Analysis Incorrect Annotation Correct Annotation
DIAC hayi?aj?iluh Hayi?aj?iluwA
MORPH ha+yi+?aj?il+uh Ha+yi+?aj?il+uwA
POS FUT_PART+IV3MS+IV+IVSUFF_SUBJ:P FUT_PART+IV3P+IV+IVSUFF_SUBJ:P
LEM ?aj?ill1 ?aj?il_1
Figure 1: An incorrect annotation example with a possible correction.
low them, e.g., A?aSl? ?aSl; and POS tag capital-
ization, e.g., Fut_Part? FUT_PART.
Morpheme-POS Correction For morpheme
correction purposes, we define an abstract rep-
resentation that combines all the closed-class
morphemes and POS tags. For open-class
stems, we simply use the POS tag. For exam-
ple, the abstract morpheme representation for
the correct version of the word in Figure 1 is
Ha/FUT_PART+yi/IV3P+IV+uwA/IVSUFF_SUBJ:P.
We will refer to this representation as the inflec-
tional morph-tag (IMT).
We build two models for this task. First, we
build an IMT language model from the CAL-
IMA+SAMA databases. This models all possible
inflections in the analyzer without the open class
stems. This model includes 304K sequences. Sec-
ond, we construct a map from all the seen IMTs
in the ARZ corpus to all the in-vocabulary IMTs
in the IMT language model. The mapping in-
cludes a cost that is based on the edit distance dis-
cussed earlier. Figure 2 shows the top mappings
for the IMTs in our example. Both models are im-
plemented as finite state machines using the ATT
FSM toolkit (Mohri et al, 1998).
The input, possibly incorrect, IMT is con-
verted into an FSM that is then composed
with the mapping transducer and the language
model automaton to generate a cost-ranked list
of mappings. The output for our example is
listed in Figure 3. We then replace the input
POS and MORPH with the top ranked correction:
Ha/FUT_PART+yi/IV3MS+IV+uh/IVSUFF_SUBJ:P
at a cost of 4.0. The open class stem is not modi-
fied.
Lemma Correction We generate a map that in-
cludes all the possible lemmas for every possi-
ble stem morpheme in CALIMA+SAMA. For a
given ARZ word analysis, if the stem morpheme
is in CALIMA+SAMA, then we pick the lemma
from its corresponding lemma set. When there is
more than one possible lemma, we pick the lemma
that is closest to the provided pre-release ARZ
Base IMT
Morpheme
Mapped IMT
Morphemes Cost
ha/FUT_PART Ha/FUT_PART 0.5
sa/FUT_PART 1.0
yi/IV3MS
yi/IV3MS 0.0
ya/IV3MS 1.0
y/IV3MS 1.0
yu/IV3MS 1.0
yi/IV3P 2.0
IV
IV 0.0
PV 1.0
CV 1.0
uh/IVSUFF_SUBJ:P uwA/IVSUFF_SUBJ:P 1.5
na/IVSUFF_SUBJ:FP 3.0
Figure 2: Top mappings for the IMT morphemes
ha/FUT_PART, yi/IV3P, IV and uh/IVSUFF_SUBJ:P
Input: ha/FUT_PART+yi/IV3P+IV+uh/IVSUFF_SUBJ:P
FSM Output Cost
Ha/FUT_PART+yi/IV3P+IV+uwA/IVSUFF_SUBJ:P 4.0
Ha/FUT_PART+y/IV3P+IV+uwA/IVSUFF_SUBJ:P 5.0
Ha/FUT_PART+ti/IV2P+IV+uwA/IVSUFF_SUBJ:P 6.0
Ha/FUT_PART+yi/IV3MS+IV+uh/IVSUFF_DO:3MS 6.5
Ha/FUT_PART+yi/IV3MS+IV+kuw/IVSUFF_DO:2P 7.0
Ha/FUT_PART+yi/IV3MS+IV+nA/IVSUFF_DO:1P 7.0
Ha/FUT_PART+tu/IV2P+IV+uwA/IVSUFF_SUBJ:P 7.0
sa/FUT_PART+ya/IV3FP+IV+na/IVSUFF_SUBJ:FP 7.0
sa/FUT_PART+yu/IV3FP+IV+na/IVSUFF_SUBJ:FP 7.0
Ha/FUT_PART+yi/IV3MS+IV+kum/IVSUFF_DO:2P 7.5
Figure 3: Top corrections for the input
ha/FUT_PART+yi/IV3P+IV+uh/IVSUFF_SUBJ:P
lemma, based on their string edit distance as de-
fined earlier. If the stem morpheme is not in CAL-
IMA+SAMA (e.g., open class), then we keep the
ARZ lemma as it is.
In our example, the stem morpheme ?aj?il/IV
is paired in CALIMA+SAMA with the lemma
?aj?il_1. Accordingly, ?aj?il_1 replaces the in-
put pre-release ARZ lemma.
Surface DIAC Generation After correcting the
morphemes and POS tags in the input word,
we use them to generate a new surface DIAC
form. For all the closed-class morphemes and
in-vocabulary open-class stems, we use CAL-
IMA+SAMA to identify all the MORPH+POS to
DIAC mappings. For open-class stems that are
5
OOVs, we use their corresponding DIAC form in
the input word.12 This may lead to many possible
sequences. We rank them by their edit distance
(defined above) to the surface DIAC of the input
word.
In our example, this process is rather trivial:
every morpheme is paired with only one surface
DIAC in the morphological analyzer. The surface
DIACs corresponding to Ha/FUT_PART, yi/IV3P,
?aj?il/IV and uwA/IVSUFF_SUBJ:P are Ha, yi,
?aj?il and uwA, respectively. The final combined
surface is Hayi?aj?iluwA.
A more interesting example is the word A 	JJ
??
?alay+nA ?upon us? which has the analysis
?ala?/PREP+nA/PRON_1P. The MORPH stem
?ala? has two DIAC forms: ?ala? and ?alay. The
second form is only used when an enclitic is
present. It is selected in this example because it
has a smaller edit distance to the full word input
DIAC form than the surface stem ?ala?. In the
future, we plan to use more sophisticated genera-
tion and detokenization techniques (El Kholy and
Habash, 2010).
4.4 Results and Error Analysis
Results We conducted a manual evaluation for
1,000 words from the internal, pre-release ARZ
after applying the automatic correction process.
This set is a blind test set, i.e., not used as part
of the development. The results are listed in Ta-
ble 1 for the lemmas, POS tags, diacritized mor-
phemes and diacritized surface forms, in addition
to the complete morphological analyses (token-
based), where the correction output is compared
to the pre-release ARZ annotations (the baseline).
The results are listed for different subsets of
the data. The first row lists the results consider-
ing the complete 1,000 words, where all the in-
vocabulary words are considered correct. This is
only intended to give an overall estimate of the
correctness of the set. The second row lists the re-
sults for CALIMA+SAMA OOV words only. The
third row is the same as the second, but exclud-
ing punctuations, digits and typos. Focusing on
the last row, we see that we achieve between 58%
and 24% error reduction on different features, and
reach almost 40% error reduction on all features
combined.
Error Analysis For POS, 99.7% of all the cor-
rect cases in the Baseline were not changed. Only
12Since the surface DIAC splits are not provided, we deter-
mine the exact boundary of the surface DIAC stem by mini-
mizing the edit distance between the prefixing/suffixing mor-
phemes and the full input surface DIAC form.
one case was changed and it was caused by an er-
ror in the input MORPH splits. Of the erroneous
cases in the Baseline, 40% were not changed.
Among the attempted changes, 71% successfully
fixed the baseline problem. Almost all of the failed
changes are due to implausible null pronouns in
the Baseline that were not handled in the cur-
rent implementation, which only considered cor-
rect null pronouns. We plan to address these in
the future. Among the errors that were not ad-
dressed, the most common case involves nominal
form (41%) followed by hard features to resolve
and open class passive-voice inconsistency (each
27%).
Regarding lemmas, 93.9% of all correct base-
line lemmas remained correct. In the rest, over-
correction attempts resulting from matching the
OOV lemma to the wrong in-vocabulary lemma
backfired. Around 8.7% of the erroneous baseline
lemmas were not modified and 1.6% were mod-
ified incorrectly. The rest, 92.8%, were success-
fully fixed. Almost all of the system errors result-
ing from changes involve over correction by map-
ping to incorrect INV lemma forms.
Finally, as for diacritized forms, 96.9% of the
correct baseline DIACs remained correct; the rest
fell victim to over-correction. Among incorrect
baseline cases, 43% remained unchanged; and
45% were fixed; 4% were over-corrected and 8%
only partially corrected. Remaining DIAC errors
are mostly in open classes where the analyzer re-
call problems cannot help.
5 Automatic Morphological Extension
In this section, we present the general technique
we use to extend shallow annotations. We discuss
the data sets, the approach and evaluation results
next.
5.1 Data
We conduct our experiments on two differ-
ent Egyptian Arabic corpora: the CALLHOME
Egypt (CHE) corpus (Gadalla et al, 1997) and
Carnegie Mellon University Egyptian Arabic cor-
pus (CMUEAC) (Mohamed et al, 2012).
CHE The CHE corpus contains 140 telephone
conversation transcripts of about 179K words.
Each word is represented by its phonological form
and undiacritized Arabic script orthography. The
orthography used is quite similar to the CODA
standard we use. Being a transcript corpus, it is
quite clean and free of spelling variations. We use
a technique described in more detail in Habash et
6
POS
LEM POS MORPH DIAC +MORPH All
All words
Baseline 79.8% 93.2% 92.2% 91.1% 87.3% 72.7%
System 95.7% 95.5% 93.8% 93.6% 91.5% 90.0%
Analyzer OOV
Baseline 47.1% 82.4% 79.7% 76.8% 66.8% 28.4%
System 88.9% 88.42% 83.9% 83.4% 77.9% 73.9%
Analyzer OOV, no Baseline 71.3% 82.5% 74.1% 69.7% 59.0% 43.0%
Punc/Digit/Typos System 88.0% 87.3% 80.5% 79.7% 71.3% 65.3%
Table 1: Accuracy of the automatic morphological correction of internal, pre-release ARZ data.
al. (2012b) to combine the phonological form and
undiacritized Arabic script into diacritized Arabic
script, i.e. DIAC. For example, the undiacritized
word ? 	JJ
? ?ynh ?his eye? is combined with its pro-
nunciation /?e:nu/ producing the diacritized form
?aynuh.
CMUEAC The CMUEAC corpus includes
about 23K words that are only annotated for
morph splits. The corpus text includes sponta-
neously written Egyptian Arabic text collected off
the web. To use the same example as above, the
word ? 	JJ
? ?ynh ?his eye? is segmented as ?yn+h in-
dicating that there is a base word plus an enclitic.
5.2 Approach
Our approach to morphological extension is to au-
tomatically annotate the corpus using a very rich
morphological tagger, and then use the limited
manual annotations to adjust the morphological
choice. We use a morphological tagger, MADA-
ARZ (Morphological Analysis and Disambigua-
tion for Egyptian Arabic) (Habash et al, 2013).
MADA-ARZ produces, for each input word, a
contextually ranked list of analyses specifying all
the morphological interpretations of that word as
provided by the CALIMA+SAMA morphological
analyzer.
CHE In the case of CHE, we select the first
choice from the ranked list of analyses whose
DIAC matches the diacritized word in CHE. For
example, for the word ? 	JJ
? ?ynh MADA-ARZ
generates 45 different morphological analyses
with different lemmas, POS, orthographies and
diacritics: ?ayn+uh ?his eye?, ?ay?in+a~ ?sam-
ple? and ?ay?in+uh ?he appointed him?. The
diacritized word ?ayn+uh allows us to select the
following full analysis:
Metric CHE CMUEAC
LEM 97.2 82.0
POS 95.2 79.6
MORPH 96.8 77.6
DIAC 97.2 78.4
POS+MORPH 92.8 74.0
All 92.8 72.0
Table 2: Accuracy of automatic morphological ex-
tension of CHE and CMUEAC.
RAW Eynh
DIAC Eaynuh
MORPH Eayn+uh
POS NOUN+POSS_PRON_3MS
LEM Eayn_1
Although this example may not require the full
power of a tagger, but just the out-of-context an-
alyzer, other cases involving POS ambiguity un-
realized through diacritization necessitate the use
of a tagger, e.g., the word I.

KA? kAtib can be
an ADJ meaning ?writing? or a NOUN meaning
?writer/author?.
CMUEAC In the case of CMUEAC, we se-
lect the first choice from the ranked list of anal-
yses whose undiacritized MORPH splits match the
word tokenization. In the case of the word ? 	JJ
?
?yn+h, the tokenization cannot distinguish be-
tween the noun reading ?ayn+uh ?his eye? and
the verbal reading ?ay?in+uh ?he appointed him?.
MADA-ARZ effectively selects in such cases.
We expect the performance on CMUEAC to
be worse than CHE given the difference in the
amount of information between the two corpora.
5.3 Results and Error Analysis
We evaluate the accuracy of the morphological
extension process on both CHE and CMUEAC
using two 300 word samples that were manu-
ally enriched. Table 2 presents the accuracies of
the assigned LEMs, POS tags, DIAC forms and
7
MORPHs, in addition to the complete morpholog-
ical analysis. All results are token-based.
CHE CHE analyses have high accuracies rang-
ing between 95.2% and 97.2% for the different
analysis features, with the complete analysis hav-
ing an accuracy of 92.8%. One third of the er-
rors is due to gold diacritization errors in the
CHE corpus. 28% of the errors are due to wrong
verbal features (person, number and gender) for
forms that are not distinguishable in DIAC, e.g.,
I.

J? katabt ?I/you wrote? and I.

J?

K tiktib ?you
write/she writes?. The rest of the errors are be-
cause of failure in assigning the correct POS tags
for nouns, particles and verbs with percentages of
22%, 11% and 6%, respectively.
CMUEAC CMUEAC analyses have much
lower accuracies compared to CHE, ranging be-
tween 77.6% and 82.0% for different features,
with the complete analysis accuracy at 72.0%. The
CMUEAC is much harder to extend for two rea-
sons: the text, being naturally occurring, con-
tains a lot of orthographic noise; and tokeniza-
tion information is not sufficient to disambiguate
many analyses. For CMUEAC, a quarter of the
errors is due to gold tokenization errors in the
original CMUEAC corpus. Another quarter of
the errors results from MADA-ARZ assigning an
MSA analysis instead of an Egyptian Arabic anal-
ysis.13 Failure to assign the correct POS tags for
particles, verbs and nouns represents 14%, 10%
and 7% of the errors, respectively. Other errors
are because of wrong verbal features (13%) and
wrong diacritization (6%).
As expected, relatively richer annotations (i.e.,
diacritics) are easier to extend to full morpholog-
ical information that relatively poorer annotations
(i.e., tokenization). Of course, the tradeoff is still
there as tokenizations are much easier and cheaper
to annotate. We plan to explore the question of
what would be an optimal set of poor annotations
that can help us extend to the full morphology at
high accuracy in the future.
6 Egyptian Corpus
After applying morphological corrections to pre-
release ARZ and morphological extensions to
CHE and CMUEAC, we have now three big cor-
pora that are automatically adjusted to include
the same rich morphological information, that is:
13MADA-ARZ is trained on a combination of MSA and
Egyptian Arabic text and as such may select an MSA analysis
in cases that are ambiguous.
lemma, POS tag, diacritized morphemes, and dia-
critized surface. We combine the three resources
together in one morphologically rich corpus that
contains about 46K sentences and 447K words,
representing 61K unique lemmas. We intend to
make these automatic corrections and extensions
available in the future to provide extensive sup-
port for Egyptian Arabic processing for different
purposes.
7 Conclusion and Future Work
We presented two methods for automatic correc-
tion and extension of morphological annotations
and demonstrated their success on three different
Egyptian Arabic corpora, which now have annota-
tions that are automatically adjusted to include the
same rich morphological information although at
different degrees of quality that correspond to the
amount of initial information.
We presented two methods for automatic cor-
rection and extension of morphological annota-
tions and demonstrated their success on three dif-
ferent Egyptian Arabic corpora, which now have
annotations that are automatically adjusted to in-
clude the same rich morphological information al-
though at different degrees of quality that corre-
spond to the amount of initial information.
In the future, we plan to study how to optimize
the amount of basic information to annotate man-
ually in order to maximize the benefit of auto-
matic extensions. We also plan to provide feed-
back to the annotation process to reduce the per-
centage of errors generated by the annotators, per-
haps through a tighter integration of the correc-
tion/extension techniques with the annotation pro-
cess. We also plan on using the cleaned up corpus
to extend the existing analyzer for Egyptian Ara-
bic.
Acknowledgment
This paper is based upon work supported by
the Defense Advanced Research Projects Agency
(DARPA) under contracts No. HR0011-12-C-
0014 and HR0011-11-C-0145. Any opinions,
findings and conclusions or recommendations ex-
pressed in this paper are those of the authors and
do not necessarily reflect the views of DARPA. We
also would like to thank Emad Mohamed and Ke-
mal Oflazer for providing us with the CMUEAC
corpus. We thank Ryan Roth for help with
MADA-ARZ. Finally, we thank Owen Rambow,
Mona Diab and Warren Churchill for helpful dis-
cussions.
8
References
Sarah Alkuhlani and Nizar Habash. 2011. A Corpus
for Modeling Morpho-Syntactic Agreement in Ara-
bic: Gender, Number and Rationality. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics (ACL?11), Portland,
Oregon, USA.
Sarah Alkuhlani, Nizar Habash, and Ryan Roth. 2013.
Automatic morphological enrichment of a morpho-
logically underspecified treebank. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 460?470, At-
lanta, Georgia, June. Association for Computational
Linguistics.
Ahmed El Kholy and Nizar Habash. 2010. Techniques
for Arabic Morphological Detokenization and Or-
thographic Denormalization. In Proceedings of the
seventh International Conference on Language Re-
sources and Evaluation (LREC), Valletta, Malta.
Ramy Eskander, Nizar Habash, Owen Rambow, and
Nadi Tomeh. 2013. Processing spontaneous orthog-
raphy. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 585?595, Atlanta, Georgia, June.
Association for Computational Linguistics.
Hassan Gadalla, Hanaa Kilany, Howaida Arram,
Ashraf Yacoub, Alaa El-Habashi, Amr Shalaby,
Krisjanis Karins, Everett Rowson, Robert MacIn-
tyre, Paul Kingsbury, David Graff, and Cynthia
McLemore. 1997. CALLHOME Egyptian Ara-
bic Transcripts. In Linguistic Data Consortium,
Philadelphia.
David Graff, Mohamed Maamouri, Basma Bouziri,
Sondos Krouna, Seth Kulick, and Tim Buckwal-
ter. 2009. Standard Arabic Morphological Analyzer
(SAMA) Version 3.1. Linguistic Data Consortium
LDC2009E73.
Nizar Habash and Owen Rambow. 2005. Arabic Tok-
enization, Part-of-Speech Tagging and Morphologi-
cal Disambiguation in One Fell Swoop. In Proceed-
ings of the 43rd Annual Meeting of the Association
for Computational Linguistics (ACL?05), pages 573?
580, Ann Arbor, Michigan.
Nizar Habash and Owen Rambow. 2006. MAGEAD:
A Morphological Analyzer and Generator for the
Arabic Dialects. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 681?688, Sydney, Aus-
tralia.
Nizar Habash, Abdelhadi Soudi, and Tim Buckwalter.
2007. On Arabic Transliteration. In A. van den
Bosch and A. Soudi, editors, Arabic Computa-
tional Morphology: Knowledge-based and Empiri-
cal Methods. Springer.
Nizar Habash, Mona Diab, and Owen Rabmow. 2012a.
Conventional Orthography for Dialectal Arabic. In
Proceedings of the Language Resources and Evalu-
ation Conference (LREC), Istanbul.
Nizar Habash, Ramy Eskander, and Abdelati Hawwari.
2012b. A Morphological Analyzer for Egyptian
Arabic. In NAACL-HLT 2012 Workshop on Com-
putational Morphology and Phonology (SIGMOR-
PHON2012), pages 1?9, Montr?al, Canada.
Nizar Habash, Ryan Roth, Owen Rambow, Ramy Es-
kander, and Nadi Tomeh. 2013. Morphological
Analysis and Disambiguation for Dialectal Arabic.
In Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-HLT), Atlanta, GA.
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publish-
ers.
Jan Hajic?, Otakar Smr?, Tim Buckwalter, and Hubert
Jin. 2005. Feature-based tagger of approximations
of functional Arabic morphology. In Proceedings of
the Workshop on Treebanks and Linguistic Theories
(TLT), Barcelona, Spain.
Ahmed Hassan, Sara Noeman, and Hany Hassan.
2008. Language Independent Text Correction us-
ing Finite State Automata. In Proceedings of the In-
ternational Joint Conference on Natural Language
Processing (IJCNLP 2008).
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: The 90% Solution. In NAACL ?06: Pro-
ceedings of the Human Language Technology Con-
ference of the NAACL, Companion Volume: Short
Papers on XX, pages 57?60, Morristown, NJ, USA.
Okan Kolak and Philip Resnik. 2002. OCR error cor-
rection using a noisy channel model. In Proceed-
ings of the second international conference on Hu-
man Language Technology Research.
Karen Kukich. 1992. Techniques for Automatically
Correcting Words in Text. ACM Computing Sur-
veys, 24(4).
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The Penn Arabic Treebank:
Building a Large-Scale Annotated Arabic Corpus.
In NEMLAR Conference on Arabic Language Re-
sources and Tools, pages 102?109, Cairo, Egypt.
Mohamed Maamouri, Ann Bies, and Seth Kulick.
2009. Creating a methodology for large-scale cor-
rection of treebank annotation: The case of the ara-
bic treebank. In MEDAR Second International Con-
ference on Arabic Language Resources and Tools,
Egypt. Citeseer.
Mohamed Maamouri, Ann Bies, Seth Kulick, Sondos
Krouna, Dalila Tabassi, and Michael Ciul. 2012a.
Egyptian Arabic Treebank DF Part 1 V2.0. LDC
catalog number LDC2012E93.
Mohamed Maamouri, Ann Bies, Seth Kulick, Sondos
Krouna, Dalila Tabassi, and Michael Ciul. 2012b.
Egyptian Arabic Treebank DF Part 2 V2.0. LDC
catalog number LDC2012E98.
9
Mohamed Maamouri, Ann Bies, Seth Kulick, Sondos
Krouna, Dalila Tabassi, and Michael Ciul. 2012c.
Egyptian Arabic Treebank DF Part 3 V2.0. LDC
catalog number LDC2012E89.
Mohamed Maamouri, Ann Bies, Seth Kulick, Sondos
Krouna, Dalila Tabassi, and Michael Ciul. 2012d.
Egyptian Arabic Treebank DF Part 4 V2.0. LDC
catalog number LDC2012E99.
Mohamed Maamouri, Ann Bies, Seth Kulick, Sondos
Krouna, Dalila Tabassi, and Michael Ciul. 2012e.
Egyptian Arabic Treebank DF Part 5 V2.0. LDC
catalog number LDC2012E107.
Mohamed Maamouri, Ann Bies, Seth Kulick, Sondos
Krouna, Dalila Tabassi, and Michael Ciul. 2012f.
Egyptian Arabic Treebank DF Part 6 V2.0. LDC
catalog number LDC2012E125.
Mohamed Maamouri, Sondos Krouna, Dalila Tabessi,
Nadia Hamrouni, and Nizar Habash. 2012g. Egyp-
tian Arabic Morphological Annotation Guidelines.
Walid Magdy and Kareem Darwish. 2006. Ara-
bic OCR Error Correction Using Character Segment
Correction, Language Modeling, and Shallow Mor-
phology. In Proceedings of 2006 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2006), pages 408?414, Sydney, Austrailia.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn treebank. Computa-
tional Linguistics, 19(2):313?330, June.
Emad Mohamed, Behrang Mohit, and Kemal Oflazer.
2012. Annotating and Learning Morphological Seg-
mentation of Egyptian Colloquial Arabic. In Pro-
ceedings of the Language Resources and Evaluation
Conference (LREC), Istanbul.
Mehryar Mohri, Fernando C. N. Pereira, and Michael
Riley. 1998. A rational design for a weighted finite-
state transducer library. In D. Wood and S. Yu, ed-
itors, Automata Implementation, Lecture Notes in
Computer Science 1436, pages 144?58. Springer.
Kemal Oflazer. 1996. Error-tolerant finite-state recog-
nition with applications to morphological analysis
and spelling correction. Computational Linguistics,
22:73?90.
Martha Palmer, Olga Babko-Malaya, Ann Bies, Mona
Diab, Mohamed Maamouri, Aous Mansouri, and
Wajdi Zaghouani. 2008. A Pilot Arabic Prop-
bank. In Proceedings of LREC, Marrakech, Mo-
rocco, May.
Khaled Shaalan, Amin Allam, and Abdallah Gomah.
2003. Towards Automatic Spell Checking for Ara-
bic. In Conference on Language Engineering,
ELSE, Cairo, Egypt.
Noah Smith, David Smith, and Roy Tromble. 2005.
Context-Based Morphological Disambiguation with
Random Fields. In Proceedings of the 2005 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP05), pages 475?482, Vancou-
ver, Canada.
10
Proceedings of the 2nd Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 21?25,
Baltimore, Maryland, USA, June 22-27, 2014.
c?2014 Association for Computational Linguistics
Inter-Annotator Agreement for ERE Annotation
Seth Kulick and Ann Bies and Justin Mott
Linguistic Data Consortium, University of Pennsylvania, Philadelphia, PA 19104
{skulick,bies,jmott}@ldc.upenn.edu
Abstract
This paper describes a system for inter-
annotator agreement analysis of ERE an-
notation, focusing on entity mentions and
how the higher-order annotations such as
EVENTS are dependent on those entity
mentions. The goal of this approach is to
provide both (1) quantitative scores for the
various levels of annotation, and (2) infor-
mation about the types of annotation in-
consistencies that might exist. While pri-
marily designed for inter-annotator agree-
ment, it can also be considered a system
for evaluation of ERE annotation.
1 Introduction
In this paper we describe a system for analyz-
ing dually human-annotated files of Entities, Re-
lations, and Events (ERE) annotation for consis-
tency between the two files. This is an important
aspect of training new annotators, to evaluate the
consistency of their annotation with a ?gold? file,
or to evaluate the agreement between two anno-
tators. We refer to both cases here as the task of
?inter-annotator agreement? (IAA).
The light ERE annotation task was defined as
part of the DARPA DEFT program (LDC, 2014a;
LDC, 2014b; LDC, 2014c) as a simpler version
of tasks like ACE (Doddington et al., 2004) to al-
low quick annotation of a simplified ontology of
entities, relations, and events, along with iden-
tity coreference. The ENTITIES consist of co-
referenced entity mentions, which refer to a span
of text in the source file. The entity mentions are
also used as part of the annotation of RELATIONS
and EVENTS, as a stand in for the whole ENTITY.
The ACE program had a scoring metric de-
scribed in (Doddington et al., 2004). However,
our emphasis for IAA evaluation is somewhat dif-
ferent than that of scoring annotation files for ac-
curacy with regard to a gold standard. The IAA
system aims to produce output to help an annota-
tion manager understand the sorts of errors occur-
ring, and the general range of possible problems.
Nevertheless, the approach to IAA evaluation de-
scribed here can be used for scoring as well. This
approach is inspired by the IAA work for tree-
banks in Kulick et al. (2013).
Because the entity mentions in ERE are the fun-
damental units used for the ENTITY, EVENT and
RELATION annotations, they are also the funda-
mental units upon which the IAA evaluation is
based. The description of the system therefore be-
gins with a focus on the evaluation of the consis-
tency of the entity mention annotations. We derive
a mapping between the entity mentions between
the two files (henceforth called File A and File
B). We then move on to ENTITIES, RELATIONS,
and EVENTS, pointing out the differences between
them for purposes of evaluation, but also their sim-
ilarities.
1
This is a first towards a more accurate use of
the full ENTITIES in the comparison and scoring
of ENTITIES and EVENTS annotations. Work to
expand in this direction is in progress. When a
more complete system is in place it will be more
appropriate to report corpus-based results.
2 Entity Mentions
There are two main aspects to the system?s han-
dling of entity mentions. First we describe the
mapping of entity mentions between the two an-
notators. As in Doddington et al. (2004), the pos-
sibility of overlapping mentions can make this a
complex problem. Second, we describe how our
system?s output categorizes possible errors.
1
This short paper focuses on the design of the IAA sys-
tem, rather than reporting on the results for a specific dataset.
The IAA system has been run on dually annotated ERE data,
however, which was the source for the examples in this paper.
21
m-502
m-892m-398
SOUTH OF IRANm-463A'smentionsB's mentions THE EASTTHE EAST     AND    SOUTH OF     IRAN
Figure 1: Case of ambiguous Entity Mention map-
ping disambiguated by another unambiguous map-
ping
2.1 Mapping
As mentioned in the introduction, our system de-
rives a mapping between the entity mentions in
Files A and B, as the basis for all further eval-
uation of the ERE annotations. Entity mentions
in Files A and B which have exactly the same lo-
cation (offset and length) are trivially mapped to
each other. We refer to these as ?exact? matches.
The remaining cases fall into two categories.
One is the case of when an entity mention in one
file overlaps with one and only one entity men-
tion in the other file. We refer to these as the ?un-
ambiguous? overlapping matches. It is also pos-
sible for an entity mention in one file to overlap
with more than one entity mention in the other file.
We refer to these as the ?ambiguous? overlapping
matches, and these patterns can get quite complex
if multiple ambiguous overlapping matches are in-
volved.
2.1.1 Disambiguation by separate
unambiguous mapping
Here an ambiguous overlapping is disambiguated
by the presence of an unambiguous mapping, and
the choice for mapping the ambiguous case is de-
cided by the desire to maximize the number of
mapped entity mentions.
Figure 1 shows such a case. File A has two en-
tity mentions annotations (m-502 and m-463) and
File B has two entity mention annotations (m-398
and m-892). These all refer to the same span of
text, so m-502 (THE EAST) and m-463 (SOUTH
OF IRAN) both overlap with m-398 in File B
(THE EAST AND SOUTH OF IRAN). m-463 in
addition overlaps with m-892 (IRAN).
We approach the mapping from the perspective
of File A. If we assign the mapping for m-463 to
be m-398, it will leave m-502 without a match,
since m-398 will already be used in the mapping.
Therefore, we assign m-502 and m-398 to map to
m-905
m-788
TALIBAN     MILITIA  m-892A'smentionsB's mentions THE NOW-OUSTED TALIBAN     MILITIA
Figure 2: Case of Entity Mention mapping re-
solved by maximum overlap
each other, while m-463 and m-892 are mapped to
each other. The goal is to match as many mentions
as possible, which this accomplishes.
2.1.2 Disambiguation by maximum overlap
The other case is shown in Figure 2. Here there are
two mentions in File A, m-892 (TALIBAN MILI-
TIA) and m-905 (TALIBAN), both overlapping
with one mention in File B, m-788 (THE NOW-
OUSTED TALIBAN MILITIA), so it is not pos-
sible to have a matching of all the mentions. We
choose the mapping with greatest overlap, in terms
of characters, and so m-892 and m-788 are taken
to match, while m-905 is left without a match.
For such cases of disambiguation by maximum
overlap, it may be possible that a different match-
ing, the one with less overlap, might be a better
fit for one of the higher levels of annotation. This
issue will be resolved in the future by using ENTI-
TIES rather than ENTITY MENTIONS as the units
to compare for the RELATION and EVENT levels.
2.2 Categorization of annotation
inconsistencies
Our system produces an entity mention report that
lists the number of exact matches, the number of
overlap matches, and for Files A and B how many
entity mentions each had that did not have a corre-
sponding match in the other annotator?s file.
Entity mentions can overlap in different ways,
some of which are more ?serious? than other. We
categorize each overlapping entity mention based
on the nature of the edge differences in the non-
exact match, such as the presence or absence of a
determiner or punctuation, or other material.
In addition, both exact and overlap mentions
can match based on location, but be different as
far as the entity mention level (NAMed, NOMi-
nal, and PROnominal). The software also outputs
all such mismatches for each match.
22
SUPPORTERS IN PAKISTAN
m-333 m-1724
m-1620m-3763
ENTITYA's
ENTITYB's
A's ENTITY and B's ENTITY are a "complete" matchSUPPORTERS SUPPORTERSSUPPORTERS IN PAKISTAN
Figure 3: Complete match between File A and File
B ENTITIES despite overlapping mentionsA's ENTITY and B's ENTITY are an "incomplete" match
AL-QAEDAm-437 m-840m-2580m-424 AL-QAEDA NETWORK AL-QAEDAm-593
A's ENTITYB's ENTITY 0AL-QAEDA AL-QAEDA
Figure 4: Incomplete match between File A and
File B ENTITIES, because File B does not have a
mention corresponding to m-593 in File A
3 Entities
An ENTITY is a group of coreferenced entity men-
tions. We use the entity mention mapping dis-
cussed in Section 2 to categorize matches between
the ENTITIES as follows:
Complete match: This means that for some EN-
TITY x in File A and ENTITY y in File B, there
is a 1-1 correspondence between the mentions of
these two ENTITIES. For purposes of this catego-
rization, we do not distinguish between exact and
overlap mapping but include both as correspond-
ing mention instances, because this distinction was
already reported as part of the mention mapping.
Figure 3 shows an example of a complete
match. File A has two mentions, m-333 (SUP-
PORTERS) and m-1724 (another instance of SUP-
PORTERS). These are co-referenced together to
form a single ENTITY. In File B there are
two mentions, m-3763 (SUPPORTERS IN PAK-
ISTAN) an m-1620 (another instance of SUP-
PORTERS IN PAKISTAN). It was determined by
the algorithm for entity mention mapping in Sec-
tion 2.1 that m-333 and m-3763 are mapped to
each other, as are m-1724 and m-1620, although
each pair of mentions is an overlapping match, not
an exact match. At the ENTITY level of corefer-
ences mentions, there is a 1-1 mapping between
the mentions of A?s ENTITY and B?s ENTITY.
Therefore these two ENTITIES are categorized as
having a complete mapping between them.
Incomplete match: This means that for some EN-
TITY x in file A and ENTITY y in file B, there may
be some mentions that are part of x in A that have
no match in File B, but all the mentions that are
part of x map to mentions that are part of EN-
TITY y in File B, and vice-versa. Figure 4 shows
an example of an incomplete match. File A has
three entity mentions, m-437 (AL-QAEDA), m-
593 (AL-QAEDA NETWORK), and m-840 (AL-
QAEDA again), coreferenced together as a single
ENTITY. File B has two entity mentions, m-424
(AL-QAEDA) and m-2580 (AL-QAEDA again),
coreferenced together as a single ENTITY. While
m-437 maps to m-424 and m-840 maps to m-2580,
m-593 does not have a match in File B, causing
this to be categorized as an incomplete match.
No match: It is possible that some ENTITIES may
not map to an ENTITY in the other file, if the con-
ditions for neither type of match exist. For exam-
ple, if in Figure 4 m-593 mapped to a mention in
File B that was part of a different ENTITY than m-
424 and m-2580, then there would not be even an
incomplete match between the two ENTITIES.
Similar to the mentions, ENTITIES as a whole
can match as complete or incomplete, but still dif-
fer on the entity type (ORGanization, PERson,
etc.). We output such type mismatches as separate
information for the ENTITY matching.
4 Relations
A RELATION is defined as having:
1) Two RELATION arguments, each of which is an
ENTITY.
2) An optional ?trigger?, a span of text.
3) A type and subtype. (e.g., ?Physical.Located?)
For this preliminary stage of the system, we
match RELATIONS in a similar way as we do
the ENTITIES, by matching the corresponding en-
tity mentions, as stand-ins for the ENTITY argu-
ments for the RELATION. We use the previously-
established mapping of mentions as basis of the
RELATION mapping.
2
We report four types of RELATION matching:
3
1) exact match - This is the same as the complete
2
This is a stricter mapping requirement than is ultimately
necessary, and future work will adjust the basis of RELATION
mapping to be full ENTITIES.
3
Because of space reasons and because RELATIONS are
so similar to EVENTS, we do not show here an illustration of
RELATION mapping.
23
match for ENTITIES, except in addition checking
for a trigger match and type/subtype.
2) types different - a match for the arguments, al-
though the type or subtypes of the RELATIONS do
not match. (The triggers may or may not be differ-
ent for this case.)
3) triggers different - a match for the arguments
and type/subtype, although with different triggers.
4) no match - the arguments for a RELATION in
one file do not map to arguments for any one sin-
gle RELATION in the other file.
5 Events
The structure of an EVENT is similar to that of a
RELATION. Its components are:
1) One or more EVENT arguments. Each EVENT
argument is an ENTITY or a date.
2) An obligatory trigger argument.
3) A type and subtype (e.g., ?Life.MARRY?)
In contrast to RELATIONS, the trigger argument
is obligatory. There must be at least one ENTITY
argument (or a date argument) in order for the
EVENT to qualify for annotation, although it does
not need to be exactly two, as with RELATIONS.
The mapping between EVENTS works essen-
tially as for ENTITIES and RELATIONS, once again
based on the already-established mapping of the
entity mentions.
4
There are two slight twists, how-
ever. It is possible for the only EVENT argument
to be a date, which is not an entity mention, and so
we must also establish a mapping for EVENT date
arguments, as we did for the entity mentions. Be-
cause the trigger is obligatory, we treat it with the
same level of importance as the arguments, and es-
tablish a mapping between EVENT triggers as well.
We report three types of EVENT matching:
5
1) exact match - all arguments match, as does the
trigger, as well as the type/subtype.
2) types different - a match for the arguments
and trigger, although the type or subtypes of the
EVENTS do not match.
3) no match - either the arguments for a EVENT in
4
As with relations, this is a stricter mapping than neces-
sary, and future work will adjust to use ENTITIES as EVENT
arguments.
5
Currently, if an EVENT argument does not map to any
mention in the other file, we consider the EVENT to be a ?no
match?. In the future we will modify this (and likewise for
RELATIONS) to be more forgiving, along the lines of the ?in-
complete match? for ENTITIES.
JULY 30, 2008 m-489
m-255 m-268
POLICEm-515 triggeragent
agent
person
JULY 30, 2008
MEXICO CITYplacedate m-502
m-292 THE POLICE
APPREHENDED
APPREHENDEDMEXICO CITY A DRUG TRAFFICKERpersontriggerplacedate
A's
EVENTB's
EVENT A DRUG TRAFFICKER
Figure 5: EVENT match
one file do not map to arguments for any one single
EVENT in the other file, or the triggers do not map.
Figure 5 shows an example of an exact match
for two EVENTS, one each in File A and B. All
of the arguments in one EVENT map to an argu-
ment in the other EVENT, as does the trigger. Note
that the argument m-502 (an entity mention, PO-
LICE) in File A maps to argument m-255 (an en-
tity mention, THE POLICE) in File B as an over-
lap match, although the EVENTS are considered an
exact match.
6 Future work
We did these comparisons based on the lowest en-
tity mention level in order to develop a prelimi-
nary system. However, the arguments for EVENTS
and RELATIONS are ENTITIES, not entity men-
tions, and the system be adjusted to do the correct
comparison. Work to adjust the system in this di-
rection is in progress. When the full system is in
place in this way, we will report results as well. In
future work we will be developing a quantitative
scoring metric based on the work described here.
Acknowledgments
This material is based on research sponsored by
Air Force Research Laboratory and Defense Ad-
vance Research Projects Agency under agreement
number FA8750-13-2-0045. The U.S. Govern-
ment is authorized to reproduce and distribute
reprints for Governmental purposes notwithstand-
ing any copyright notation thereon. The views and
conclusions contained herein are those of the au-
thors and should not be interpreted as necessar-
ily representing the official policies or endorse-
ments, either expressed or implied, of Air Force
Research Laboratory and Defense Advanced Re-
search Projects Agency or the U.S. Government.
24
References
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. Automatic content extraction
(ACE) program - task definitions and performance
measures. In LREC 2004: 4th International Confer-
ence on Language Resources and Evaluation.
Seth Kulick, Ann Bies, Justin Mott, Mohamed
Maamouri, Beatrice Santorini, and Anthony Kroch.
2013. Using derivation trees for informative tree-
bank inter-annotator agreement evaluation. In Pro-
ceedings of the 2013 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
550?555, Atlanta, Georgia, June. Association for
Computational Linguistics.
LDC. 2014a. DEFT ERE Annotation Guidelines: En-
tities v1.6. Technical report, Linguistic Data Con-
sortium.
LDC. 2014b. DEFT ERE Annotation Guidelines:
Events v1.3. Technical report, Linguistic Data Con-
sortium.
LDC. 2014c. DEFT ERE Annotation Guidelines: Re-
lations v1.3. Technical report, Linguistic Data Con-
sortium.
25
Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 93?103,
October 25, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Transliteration of Arabizi into Arabic Orthography: Developing a 
Parallel Annotated Arabizi-Arabic Script SMS/Chat Corpus 
 
Ann Bies, Zhiyi Song, Mohamed Maamouri, Stephen Grimes, Haejoong Lee,  
Jonathan Wright, Stephanie Strassel, Nizar Habash?, Ramy Eskander?, Owen Rambow? 
Linguistic Data Consortium, University of Pennsylvania 
{bies,zhiyi,maamouri,sgrimes,haejoong, 
jdwright,strassel}@ldc.upenn.edu 
?Computer Science Department, New York University Abu Dhabi 
?
nizar.habash@nyu.edu 
?Center for Computational Learning Systems, Columbia University 
?
{reskander,rambow}@ccls.columbia.edu 
 
  
 
Abstract 
This paper describes the process of creating a 
novel resource, a parallel Arabizi-Arabic 
script corpus of SMS/Chat data.  The lan-
guage used in social media expresses many 
differences from other written genres: its vo-
cabulary is informal with intentional devia-
tions from standard orthography such as re-
peated letters for emphasis; typos and non-
standard abbreviations are common; and non-
linguistic content is written out, such as 
laughter, sound representations, and emoti-
cons.  This situation is exacerbated in the 
case of Arabic social media for two reasons.  
First, Arabic dialects, commonly used in so-
cial media, are quite different from Modern 
Standard Arabic phonologically, morphologi-
cally and lexically, and most importantly, 
they lack standard orthographies. Second, 
Arabic speakers in social media as well as 
discussion forums, SMS messaging and 
online chat often use a non-standard romani-
zation called Arabizi.  In the context of natu-
ral language processing of social media Ara-
bic, transliterating from Arabizi of various 
dialects to Arabic script is a necessary step, 
since many of the existing state-of-the-art re-
sources for Arabic dialect processing expect 
Arabic script input.  The corpus described in 
this paper is expected to support Arabic NLP 
by providing this resource. 
1 Introduction 
The language used in social media expresses 
many differences from other written genres: its 
vocabulary is informal with intentional devia-
tions from standard orthography such as repeated 
letters for emphasis; typos and non-standard ab-
breviations are common; and non-linguistic con-
tent is written out, such as laughter, sound repre-
sentations, and emoticons. 
This situation is exacerbated in the case of Ar-
abic social media for two reasons.  First, Arabic 
dialects, commonly used in social media, are 
quite different from Modern Standard Arabic 
(MSA) phonologically, morphologically and lex-
ically, and most importantly, they lack standard 
orthographies (Maamouri et.al. 2014). Second, 
Arabic speakers in social media as well as dis-
cussion forums, Short Messaging System (SMS) 
text messaging and online chat often use a non-
standard romanization called ?Arabizi? (Dar-
wish, 2013).  Social media communication in 
Arabic takes place using a variety of orthogra-
phies and writing systems, including Arabic 
script, Arabizi, and a mixture of the two.  Alt-
hough not all social media communication uses 
Arabizi, the use of Arabizi is prevalent enough to 
pose a challenge for Arabic NLP research. 
In the context of natural language processing 
of social media Arabic, transliterating from 
Arabizi of various dialects to Arabic script is a 
necessary step, since many of the existing state-
of-the-art resources for Arabic dialect processing 
and annotation expect Arabic script input (e.g., 
Salloum and Habash, 2011; Habash et al. 2012c; 
Pasha et al., 2014). 
To our knowledge, there are no naturally oc-
curring parallel texts of Arabizi and Arabic 
script.  In this paper, we describe the process of 
creating such a novel resource at the Linguistic 
Data Consortium (LDC).  We believe this corpus 
will be essential for developing robust tools for 
converting Arabizi into Arabic script. 
93
The rest of this paper describes the collection 
of Egyptian SMS and Chat data and the creation 
of a parallel text corpus of Arabizi and Arabic 
script for the DARPA BOLT program.1  After 
reviewing the history and features in Arabizi 
(Section 2) and related work on Arabizi (Section 
3), in Section 4, we describe our approach to col-
lecting the Egyptian SMS and Chat data and the 
annotation and transliteration methodology of the 
Arabizi SMS and Chat into Arabic script, while 
in Section 5, we discuss the annotation results, 
along with issues and challenges we encountered 
in annotation. 
2 Arabizi and Egyptian Arabic Dialect 
2.1 What is Arabizi? 
Arabizi is a non-standard romanization of Arabic 
script that is widely adopted for communication 
over the Internet (World Wide Web, email) or 
for sending messages (instant messaging and 
mobile phone text messaging) when the actual 
Arabic script alphabet is either unavailable for 
technical reasons or otherwise more difficult to 
use.  The use of Arabizi is attributed to different 
reasons, from lack of good input methods on 
some mobile devices to writers? unfamiliarity 
with Arabic keyboard.  In some cases, writing in 
Arabizi makes it easier to code switch to English 
or French, which is something educated Arabic 
speakers often do.  Arabizi is used by speakers of 
a variety of Arabic dialects. 
Because of the informal nature of this system, 
there is no single ?correct? encoding, so some 
character usage overlaps.  Most of the encoding 
in the system makes use of the Latin character 
(as used in English and French) that best approx-
imates phonetically the Arabic letter that one 
wants to express (for example, either b or p cor-
responds to ?).  This may sometimes vary due to 
regional variations in the pronunciation of the 
Arabic letter (e.g., j is used to represent ? in the 
Levantine dialect, while in Egyptian dialect g is 
used) or due to differences in the most common 
non-Arabic second language (e.g., sh corre-
sponds to ? in the previously English dominated 
Middle East Arab countries, while ch shows a 
predominantly French influence as found in 
North Africa and Lebanon).  Those letters that do 
not have a close phonetic approximate in the Lat-
in script are often expressed using numerals or 
other characters, so that the numeral graphically 
                                                 
1 http://www.darpa.mil/Our_Work/I2O/Programs/Broad_Op 
erational_Language_Translation_%28BOLT%29.aspx 
approximates the Arabic letter that one wants to 
express (e.g., the numeral 3 represents ? because 
it looks like a mirror reflection of the letter). 
Due to the use of Latin characters and also 
frequent code switching in social media Arabizi, 
it can be difficult to distinguish between Arabic 
words written in Arabizi and entirely unrelated 
foreign language words (Darwish 2013).  For 
example, mesh can be the English word, or 
Arabizi for ?? ?not?.  However, in context these 
cases can be clearly labeled as either Arabic or a 
foreign word.  An additional complication is that 
many words of foreign origin have become Ara-
bic words (?borrowings?).  Examples include 
banadoora ?????? ?tomato? and mobile ?????? 
?mobile phone?.  It is a well-known practical and 
theoretical problem to distinguish borrowings 
(foreign words that have become part of a lan-
guage and are incorporated fully into the mor-
phological and syntactic system of the host lan-
guage) from actual code switching (a bilingual 
writer switches entirely to a different language, 
even if for only a single word).  Code switching 
is easy to identify if we find an extended passage 
in the foreign language which respects that lan-
guage?s syntax and morphology, such as Bas eh 
ra2yak I have the mask.  The problem arises 
when single foreign words appear without Arabic 
morphological marking: it is unclear if the writer 
switched to the foreign language for one word or 
whether he or she simply is using an Arabic 
word of foreign origin.  In the case of banadoora 
?????? ?tomato?, there is little doubt that this has 
become a fully Arabic word and the writer is not 
code switching into Italian; this is also signaled 
by the fact that a likely Arabizi spelling (such as 
banadoora) is not in fact the Italian orthography 
(pomodoro).  However, the case is less clear cut 
with mobile ?????? ?mobile phone?: even if it is a 
borrowing (clearly much more recent than bana-
doora ?????? ?tomato?), a writer will likely spell 
the word with the English orthography as mobile 
rather than write, say, mubail.  More research is 
needed on this issue.  However, because of the 
difficulty of establishing the difference between 
code switching and borrowing, we do not attempt 
to make this distinction in this annotation 
scheme. 
2.2 Egyptian Arabic Dialect 
Arabizi is used to write in multiple dialects of 
Arabic, and differences between the dialects 
themselves have an effect on the spellings cho-
sen by individual writers using Arabizi.  Because 
Egyptian Arabic is the dialect of the corpus cre-
94
ated for this project, we will briefly discuss some 
of the most relevant features of Egyptian Arabic 
with respect to Arabizi transliteration.  For a 
more extended discussion of the differences be-
tween MSA and Egyptian Arabic, see Habash et 
al. (2012a) and Maamouri et al. (2014). 
Phonologically, Egyptian Arabic is character-
ized by the following features, compared with 
MSA: 
(a) The loss of the interdentals /?/ and /?/ 
which are replaced by /d/ or /z/ and /t/ or /s/ 
respectively, thus giving those two original 
consonants a heavier load. Examples in-
clude  ??? /zakar/ ?to mention?, ???  /daba?/ 
?to slaughter?,  ???  /talg/ ?ice?,  ???  /taman/ 
?price?, and  ???  /sibit/ ?to stay in place, 
become immobile?. 
(b) The exclusion of /q/ and /?/ from the conso-
nantal system, being replaced by the /?/ and 
/g/, e.g., ???  /?u?n/ ?cotton?, and  ???  
/gamal/ ?camel?. 
At the level of morphology and syntax, the 
structures of Egyptian Arabic closely resemble 
the overall structures of MSA with relatively mi-
nor differences to speak of.  Finally, the Egyptian 
Arabic lexicon shows some significant elements 
of semantic differentiation. 
The most important morphological difference 
between Egyptian Arabic and MSA is in the use 
of some Egyptian clitics and affixes that do not 
exist in MSA.  For instance, Egyptian Arabic has 
the future proclitics h+ and ?+ as opposed to the 
standard equivalent s+. 
Lexically, there are lexical differences be-
tween Egyptian Arabic and MSA where no ety-
mological connection or no cognate spelling is 
available.  For example, the Egyptian Arabic ??  
/bu??/ ?look? is ???? /?unZur/ in MSA. 
3 Related Work 
Arabizi-Arabic Script Transliteration  Previ-
ous efforts on automatic transliterations from 
Arabizi to Arabic script include work by Chalabi 
and Gerges (2012), Darwish (2013) and Al-
Badrashiny et al. (2014).  All of these approaches 
rely on a model for character-to-character map-
ping that is used to generate a lattice of multiple 
alternative words which are then selected among 
using a language model.  The training data used 
by Darwish (2013) is publicly available but it is 
quite limited (2,200 word pairs).  The work we 
are describing here can help substantially im-
prove the quality of such system.  We use the 
system of Al-Badrashiny et al. (2014) in this pa-
per as part of the automatic transliteration step 
because they target the same conventional or-
thography of dialectal Arabic (CODA) (Habash 
et al., 2012a, 2012b), which we also target.  
There are several commercial products that con-
vert Arabizi to Arabic script, namely: Microsoft 
Maren, 2  Google Ta3reeb, 3  Basis Arabic chat 
translator4 and Yamli.5  Since these products are 
for commercial purposes, there is little infor-
mation available about their approaches, and 
whatever resources they use are not publicly 
available for research purposes.  Furthermore, as 
Al-Badrashiny et al. (2014) point out, Maren, 
Ta3reeb and Yamli are primarily intended as in-
put method support, not full text transliteration.  
As a result, their users? goal is to produce Arabic 
script text not Arabizi text, which affects the 
form of the romanization they utilize as an in-
termediate step.  The differences between such 
?functional romanization? and real Arabizi in-
clude that the users of these systems will use less 
or no code switching to English, and may em-
ploy character sequences that help them arrive at 
the target Arabic script form faster, which other-
wise they would not write if they were targeting 
Arabizi (Al-Badrashiny et al., 2014). 
Name Transliteration  There has been some 
work on machine transliteration by Knight and 
Graehl (1997).  Al-Onaizan and Knight (2002) 
introduced an approach for machine translitera-
tion of Arabic names. Freeman et al. (2006) also 
introduced a system for name matching between 
English and Arabic.  Although the general goal 
of transliterating from one script to another is 
shared between these efforts and ours, we are 
considering a more general form of the problem 
in that we do not restrict ourselves to names. 
Code Switching  There is some work on code 
switching between Modern Standard Arabic 
(MSA) and dialectal Arabic (DA).  Zaidan and 
Callison-Burch (2011) were interested in this 
problem at the inter-sentence level.  They 
crawled a large dataset of MSA-DA news com-
mentaries, and used Amazon Mechanical Turk to 
annotate the dataset at the sentence level.  
Elfardy et al. (2013) presented a system, AIDA, 
that tags each word in a sentence as either DA or 
MSA based on the context.  Lui et al. (2014) 
proposed a system for language identification in 
                                                 
2 http://www.getmaren.com 
3 http://www.google.com/ta3reeb 
4 http://www.basistech.com/arabic-chat-translator-
transforms-social-media-analysis/ 
5 http://www.yamli.com/ 
95
multilingual documents using a generative mix-
ture model that is based on supervised topic 
modeling algorithms.  Darwish (2013) and Voss 
et al. (2014) deal with exactly the problem of 
classifying tokens in Arabizi as Arabic or not.  
More specifically, Voss et al. (2014) deal with 
Moroccan Arabic, and with both French and 
English, meaning they do a three-way classifica-
tion.  Darwish (2013)'s data is more focused on 
Egyptian and Levantine Arabic and code switch-
ing with English. 
Processing Social Media Text  Finally, while 
English NLP for social media has attracted con-
siderable attention recently (Clark and Araki, 
2011; Gimpel et al., 2011; Gouws et al., 2011; 
Ritter et al., 2011; Derczynski et al., 2013), there 
has not been much work on Arabic yet.  Darwish 
et al. (2012) discuss NLP problems in retrieving 
Arabic microblogs (tweets).  They discuss many 
of the same issues we do, notably the problems 
arising from the use of dialectal Arabic such as 
the lack of a standard orthography.  Eskander et 
al. (2013) described a method for normalizing 
spontaneous orthography into CODA. 
4 Corpus Creation 
This work was prepared as part of the DARPA 
Broad Operational Language Translation 
(BOLT) program which aims at developing tech-
nology that enables English speakers to retrieve 
and understand information from informal for-
eign language sources including chat, text mes-
saging and spoken conversations. LDC collects 
and annotates informal linguistic data of English, 
Chinese and Arabic, with Egyptian Arabic being 
the representative of the Arabic language family.  
 
 
Egyptian Arabic has the advantage over all other 
dialects of Arabic of being the language of the 
largest linguistic community in the Arab region, 
and also of having a rich level of internet com-
munication.  
4.1 SMS and Chat Collection 
In BOLT Phase 2, LDC collected large volumes 
of naturally occurring informal text (SMS) and 
chat messages from individual users in English, 
Chinese and Egyptian Arabic (Song et al., 2014).  
Altogether we recruited 46 Egyptian Arabic par-
ticipants, and of those 26 contributed data.  To 
protect privacy, participation was completely 
anonymous, and demographic information was 
not collected.  Participants completed a brief lan-
guage test to verify that they were native Egyp-
tian Arabic speakers.  On average, each partici-
pant contributed 48K words.  The Egyptian Ara-
bic SMS and Chat collection consisted of 2,140 
conversations in a total of 475K words after 
manual auditing by native speakers of Egyptian 
Arabic to exclude inappropriate messages and 
messages that were not Egyptian Arabic.  96% of 
the collection came from the personal SMS or 
Chat archives of participants, while 4% was col-
lected through LDC?s platform, which paired 
participants and captured their live text messag-
ing (Song et al., 2014).  A subset of the collec-
tion was then partitioned into training and eval 
datasets.   
Table 1 shows the distribution of Arabic script 
vs. Arabizi in the training dataset.  The conversa-
tions that contain Arabizi were then further anno-
tated and transliterated to create the Arabizi-
Arabic script parallel corpus, which consists of 
 
 
 Total Arabic 
script only 
Arabizi 
only 
Mix of Arabizi and Arabic script 
Arabizi Arabic script 
Conversations 1,503 233 987 283 
Messages 101,292 18,757 74,820 3,237 4,478 
Sentence units 94,010 17,448 69,639 3,017 3,906 
Words 408,485 80,785 293,900 10,244 23,556 
Table 1. Arabic SMS and Chat Training Dataset 
 
1270 conversations. 6   All conversations in the 
training dataset were also translated into English 
to provide Arabic-English parallel training data. 
                                                 
6 In order to form single, coherent units (Sentence units) of 
an appropriate size for downstream annotation tasks using 
this data, messages that were split mid-sentence (often mid-
Not surprisingly, most Egyptian conversations 
in our collection contain at least some Arabizi; 
                                                                          
word) due to SMS messaging character limits were rejoined, 
and very long messages (especially common in chat) were 
split into two or more units, usually no longer than 3-4 sen-
tences. 
96
only 15% of conversations are entirely written in 
Arabic script, while 66% are entirely Arabizi.  
The remaining 19% contain a mixture of the two 
at the conversation level.  Most of the mixed 
conversations were mixed in the sense that one 
side of the conversation was in Arabizi and the 
other side was in Arabic script, or in the sense 
that at least one of the sides switched between 
the two forms in mid-conversation.  Only rarely 
are individual messages in mixed scripts.  The 
annotation for this project was performed on the 
Arabizi tokens only.  Arabic script tokens were 
not touched and were kept in their original 
forms.  
The use of Arabizi is predominant in the SMS 
and Chat Egyptian collection, in addition to the 
presence of other typical cross-linguistic text ef-
fects in social media data.  For example, the use 
of emoticons and emoji is frequent.  We also ob-
served the frequent use of written out representa-
tions of speech effects, including representations 
of laughter (e.g., hahaha), filled pauses (e.g., 
um), and other sounds (e.g., hmmm).  When these 
representations are written in Arabizi, many of 
them are indistinguishable from the same repre-
sentations in English SMS data.  Neologisms are 
also frequently part of SMS/Chat in Egyptian  
 
Arabic, as they are in other languages.  English 
words use Arabic morphology or determiners, as 
in el anniversary ?the anniversary?.  Sometimes 
English words are spelled in a way that is closer 
phonetically to the way an Egyptian speaker 
would pronounce them, for example lozar for 
?loser?, or beace for ?peace?. 
The adoption of Arabizi for SMS and online 
chat may also go some way to explaining the 
high frequency of code mixing in the Egyptian 
Arabic collection.  While the auditing process 
eliminated messages that were entirely in a non-
target language, many of the acceptable messag-
es contain a mixture of Egyptian Arabic and 
English. 
4.2 Annotation Methodology 
All of the Arabizi conversations, including the 
conversations containing mixtures of Arabizi and 
Arabic script were then annotated and translit-
erated: 
1. Annotation on the Arabizi source text to 
flag certain features 
2. Correction and normalization of the trans-
literation according to CODA conventions 
 
 
 
Figure 1. Arabizi Annotation and Transliteration Tool 
 
The annotators were presented with the source 
conversations in their original Arabizi form as 
well as the transliteration output from an auto-
matic Arabization system, and used a web-based 
tool developed by LDC (see Figure 1) to perform 
the two annotation tasks, which allowed annota-
tors perform both annotation and transliteration 
token by token, sentence by sentence and review 
the corrected transliteration in full context.  The 
GUI shows the full conversation in both the orig-
inal Arabizi and the resulting Arabic script trans-
literation for each sentence.  Annotators must 
97
annotate each sentence in order, and the annota-
tion is displayed in three columns.  The first col-
umn shows the annotation of flag features on the 
source tokens, the second column is the working 
panel where annotators correct the automatic 
transliteration and retokenize, and the third col-
umn displays the final corrected and retokenized 
result. 
Annotation was performed according to anno-
tation guidelines developed at the Linguistic Da-
ta Consortium specifically for this task (LDC, 
2014). 
4.3 Automatic Transliteration 
To speed up the annotation process, we utilized 
an automatic Arabizi-to-Arabic script translitera-
tion system (Al-Badrashiny et al., 2014) which 
was developed using a small vocabulary of 2,200 
words from Darwish (2013) and an additional 
6,300 Arabic-English proper name pairs (Buck-
walter, 2004).  The system has an accuracy of 
69.4%.  We estimate that using this still allowed 
us to cut down the amount of time needed to type 
in the Arabic script version of the Arabizi by 
two-thirds.  This system did not identify Foreign 
words or Names and transliterated all of the 
words.  In one quarter of the errors, the provided 
answer was plausible but not CODA-compliant 
(Al-Badrashiny et al., 2014). 
4.4 Annotation on Arabizi Source Text to 
Flag Features 
This annotation was performed only on sentences 
containing Arabizi words, with the goal of tag-
ging any words in the source Arabizi sentences 
that would be kept the same in the output of an 
English translation with the following flags: 
 
? Punctuation (not including emoticons) 
o Eh ?!//Punct  
o Ma32ula ?!//Punct 
o Ebsty ?//Punct  
 
? Sound effects, such as laughs (?haha? or 
variations), filled pauses, and other sounds 
(?mmmm? or ?shh? or ?um? etc.) 
o hahhhahhah//Sound akeed 3arfa :p da 
enty t3rafy ablia :pp 
o Hahahahaahha//Sound Tb ana ta7t fel 
ahwaa 
o Wala Ana haha//Sound 
o Mmmm//Sound okay 
 
? Foreign language words and numbers.  All 
cases of code switching and all cases of bor-
rowings which are rendered in Arabizi us-
ing standard English orthography are 
marked as ?Foreign?. 
o ana kont mt25er fe t2demm l pro-
jects//Foreign 
o oltilik okay//Foreign ya Babyy//Foreign 
balashhabal!!!! 
o zakrty ll sat//Foreign 
o Bat3at el whatsapp//Foreign 
o La la la merci//Foreign gedan bs la2 
o We 9//Foreign galaeeb dandash lel ban-
at 
 
? Names, mainly person names 
o Youmna//Name 7atigi?? 
 
4.5 Correction and Normalization of the 
Transliteration According to CODA 
Conventions 
The goal of this task was to correct all spelling in 
the Arabic script transliteration to CODA stand-
ards (Habash et al., 2012a, 2012b).  This meant 
that annotators were required to confirm both (1) 
that the word was transliterated into Arabic script 
correctly and also (2) that the transliterated word 
conformed to CODA standards.  The automatic 
transliteration was provided to the annotators, 
and manually corrected by annotators as needed. 
Correcting spelling to a single standard (CO-
DA), however, necessarily included some degree 
of normalization of the orthography, as the anno-
tators had to correct from a variety of dialect 
spellings to a single CODA-compliant spelling 
for each word.  Because the goal was to reach a 
consistent representation of each word, ortho-
graphic normalization was almost the inevitable 
effect of correcting the automatic transliteration.  
This consistent representation will allow down-
stream annotation tasks to take better advantage 
of the SMS/Chat data.  For example, more con-
sistent spelling of Egyptian Arabic words will 
lead to better coverage from the CALIMA mor-
phological analyzer and therefore improve the 
manual annotation task for morphological anno-
tation, as in Maamouri et al. (2014). 
 
Modern Standard Arabic (MSA) cognates and 
Egyptian Arabic sound changes 
Annotators were instructed to use MSA or-
thography if the word was a cognate of an MSA 
98
root, including for those consonants that have 
undergone sound changes in Egyptian Arabic.7 
? use mqfwl ?????  and not ma>fwl ?????  for 
?locked? 
? use HAfZ ???? and not HAfz ???? for the 
name (a proper noun)  
 
Long vowels 
Annotators were instructed to reinstate miss-
ing long vowels, even when they were written as 
short vowels in the Arabizi source, and to correct 
long vowels if they were included incorrectly. 
? use sAEap ???? and not saEap  ???  for 
?hour? 
? use qAlt   ????  and not qlt ??? for ?(she) 
said? 
 
Consonantal ambiguities 
Many consonants are ambiguous when written 
in Arabizi, and many of the same consonants are 
also difficult for the automatic transliteration 
script.  Annotators were instructed to correct any 
errors of this type.   
? S vs. s/ ?  vs. ? 
o use SAyg ????  and not  sAyg  ????  for 
?jeweler? 
? D vs. Z/ ?  vs. ? 
o use DAbT ????  and not  ZAbT ???? for 
?officer? 
o use Zlmp  ????  and not Dlmp  ????  for 
?darkness? 
? Dotted ya vs. Alif Maqsura/ ? vs. ?.  Alt-
hough the dotted ya/ ? and Alif Maqsura/ ? 
are often used interchangeably in Egyptian 
Arabic writing conventions, it was neces-
sary to make the distinction between the 
two for this task. 
o use Ely ???  and not ElY  ???  for ?Ali? 
(the proper name)  
? Taa marbouta.  In Arabizi and so also in the 
Arabic script transliteration, the taa mar-
bouta/ ? may be written for both nominal fi-
nal -h/ ? and verbal final -t/ ?, but for dif-
ferent reasons. 
o mdrsp Ely  ???  ?????  ?Ali?s school? 
o mdrsth  ??????  ?his school? 
 
Morphological ambiguities 
Spelling variation and informal usage can 
combine to create morphological ambiguities as 
well.  For example, the third person masculine 
                                                 
7 Both Arabic script and the Buckwalter transliteration 
(http://www.qamus.org/transliteration.htm) are shown for 
the transliterated examples in this paper. 
singular pronoun and the third person plural ver-
bal suffix can be ambiguous in informal texts.  
For example: 
? use byHbwA bED  ???  ?????? and not byHbh 
bED  ???  ?????  for ?(They) loved each oth-
er? 
? use byEmlwA  ???????  and not byEmlh  ??????  
for ?(They) did? or ?(They) worked? 
In addition, because final -h is sometimes re-
placed in speech by final /-uw/, it was occasion-
ally necessary to correct cases of overuse of the 
third person plural verbal suffix (-wA) to the 
pronoun -h as well. 
 
Merging and splitting tokens written with in-
correct word boundaries 
Annotators were instructed to correct any 
word that was incorrectly segmented.  The anno-
tation tool allowed both the merging and splitting 
of tokens. 
Clitics were corrected to be attached when 
necessary according to (MSA) standard writing 
conventions.  These include single letter proclit-
ics (both verbal and nominal) and the negation 
suffix -$, as well as pronominal clitics such as 
possessive pronouns and direct object pronouns.  
For example, 
? use fAlbyt  ??????    and not  
fAl  byt  ???  ??? or  flbyt  ?????   for ?in the 
house? 
? use EAlsTH ?????? and not  
EAl sTH ??? ??? or ElsTH ????? for ?on the 
roof? 
The conjunction w- / -? is always attached to 
its following word. 
? use wkAn  ????  and not w kAn  ??? ?    for 
?and was? 
? use wrAHt  ????? and not w  rAHt ????  ?  
for ?and (she) left? 
Words that were incorrectly segmented in the 
Arabizi source were also merged.  For example, 
? use msHwrp ?????? and not  
ms Hwrp ???? ??  for ?bewitched 
(fem.sing.)? 
? use $ErhA ????? and not $Er hA  ?? ???   for 
?her hair? 
Particles that are not attached in standard 
MSA written forms were corrected as necessary 
by the splitting function of the tool.  For exam-
ple,  
? use yA Emry  ???? ?? and not yAEmry  
??????  for ?Hey, dear!? 
? use lA trwH  ????  ? and not lAtrwH  ?????  
for ?Do not go? 
99
 
Abbreviations in Arabizi 
Three abbreviations in Arabizi received spe-
cial treatment: msa, isa, 7ma.  These three abbre-
viations only were expanded out to their full 
form using Arabic words in the corrected Arabic 
script transliteration. 
? msa: use mA $A' All~h  ? ???  ?? for ?As 
God wills? 
? isa: use <n $A' All~h  ? ???  ?? for ?God 
willing? 
? 7ma: use AlHmd ll~h for     ??? ??  ?Thank 
God, Praised be the Lord? 
All other Arabic abbreviations were not ex-
panded, and were transliterated simply letter for 
letter.  When the abbreviation was in English or 
another foreign language, it was kept as is in the 
transliteration, using both consonants and semi-
vowels to represent it. 
? use Awkyh  ????  for ?OK? (note that this is 
an abbreviation in English, but not in Egyp-
tian Arabic) 
 
Correcting Arabic typos 
Annotators were instructed to correct typos in 
the transliterated Arabic words, including typos 
in proper names.  However, typos and non-
standard spellings in the transliteration of a for-
eign words were kept as is and not corrected. 
? Ramafan  ?????  should be corrected to 
rmDAn  ?????  for ?Ramadan? 
? babyy  ????  since it is the English word ?ba-
by? it should not be corrected 
 
Flagged tokens in the correction task 
Tokens flagged during task 1 as Sound and 
Foreign were transliterated into Arabic script but 
were not corrected during task 2.  Note that even 
when a whole phrase or sentence appeared in 
English, the transliteration was not corrected. 
? ks  ??  for ?kiss? 
? Dd yA hAf fAn  ??? ???  ??  ??  for ?did you 
have fun? 
The transliteration of proper names was cor-
rected in the same way as all other words. 
Emoticons and emoji were replaced in the 
transliteration with #.  Emoticons refer to a set of 
numbers or letters or punctuation marks used to 
express feelings or mood.  Emoji refers to a spe-
cial set of images used in messages.  Both Emot-
icons and Emoji are frequent in SMS/Chat data. 
5 Discussion 
Annotation and transliteration were performed 
on all sentence units that contain Arabizi.  Sen-
tence units that contain only Arabic script were 
ignored and untouched during annotation.  In 
total, we reviewed 1270 conversations, among 
which over 42.6K sentence units (more than 
300K words) were deemed to be containing 
Arabizi and hence annotated and transliterated. 
The corpus files are in xml format.  All con-
versations have six layers: source, annotation on 
the source Arabizi tokens, automatic translitera-
tion via 3ARRIB, manual correction of the au-
tomatic transliteration, re-tokenized corrected 
transliteration, and human translation.  See Ap-
pendix A for examples of the file format. 
Each conversation was annotated by one anno-
tator, with 10 percent of the data being reviewed 
by a second annotator as a QC procedure.  Twen-
ty six conversations (roughly 3400 words) were 
also annotated dually by blind assignment to 
gauge inter-annotator agreement. 
As we noted earlier, code switching is fre-
quent in the SMS and Chat Arabizi data.  There 
were about 23K words flagged as foreign words.  
Written out speech effects in this type of data are 
also prevalent, and 6610 tokens were flagged as 
Sounds (laughter, filled pause, etc.).  Annotators 
most often agreed with each other in the detec-
tion and flagging of tokens as Foreign, Name, 
Sound or Punctuation, with over 98% agreement 
for all flags. 
The transliteration annotation was more diffi-
cult than the flagging annotation, because apply-
ing CODA requires linguistic knowledge of Ara-
bic.  Annotators went through several rounds of 
training and practice and only those who passed 
a test were allowed to work on the task.  In an 
analysis of inter-annotator agreement in the dual-
ly annotated files, the overall agreement between 
the two annotators was 86.4%.  We analyzed all 
the disagreements and classified them in four 
high level categories: 
? CODA  60% of the disagreements were related 
to CODA decisions that did not carefully follow 
the guidelines.  Two-fifths of these cases were 
related to Alif/Ya spelling (mostly Alif Hamza-
tion, rules of hamza support) and about one-fifth 
involved the spelling of common dialectal words.  
An additional one-third were due to non-CODA 
root, pattern or affix spelling.  Only one-tenth of 
the cases were because of split or merge deci-
sions.  These issues suggest that additional train-
ing may be needed.  Additionally, since some of 
100
the CODA errors may be easy to detect and cor-
rect using available tools for morphological 
analysis of Egyptian Arabic (such as the CALI-
MA-ARZ analyzer), we will consider integrating 
such support in the annotation interface in the 
future.  
? Task  In 23% of the overall disagreements, the 
annotators did not follow the task guidelines for 
handling punctuation, sounds, emoticons, names 
or foreign words.  Examples include disagree-
ment on whether a question mark should be split 
or kept attached, or whether a non-Arabic word 
should be corrected or not.  Many of these cases 
can also be caught as part of the interface; we 
will consider the necessary extensions in the fu-
ture. 
? Ambiguity  In 12% of the cases, the annota-
tors? disagreement reflected a different reading 
of the Arabizi resulting in a different lemma or 
inflectional feature.  These differences are una-
voidable and reflect the natural ambiguity in the 
task. 
? Typos  Finally, in less than 5% of the cases, 
the disagreement was a result of a typographical 
error unrelated to any of the above issues.  
Among the cases that were easy to adjudicate, 
one of the two annotators was correct 60% more 
than the other.  This is consistent with the obser-
vation that more training may be needed to fill in 
some of the knowledge gaps or increase the an-
notator?s attention to detail. 
6 Conclusion 
This is the first Arabizi-Arabic script parallel 
corpus that supports research on transliteration 
from Arabizi to Arabic script.  We expect to 
make this corpus available through the Linguistic 
Data Consortium in the near future. 
This work focuses on the novel challenges of 
developing a corpus like this, and points out the 
close interaction between the orthographic form 
of written informal genres of Arabic and the spe-
cific features of individual Arabic dialects.  The 
use of Arabizi and the use of Egyptian Arabic in 
this corpus come together to present a host of 
spelling ambiguities and multiplied forms that 
were resolved in this corpus by the use of CODA 
for Egyptian Arabic.  Developing a similar cor-
pus and transliteration for other Arabic dialects 
would be a rich area for future work. 
We believe this corpus will be essential for 
NLP work on Arabic dialects and informal gen-
res.  In fact, this corpus has recently been used in 
development by Eskander et al. (2014). 
Acknowledgements 
This material is based upon work supported by 
the Defense Advanced Research Projects Agency 
(DARPA) under Contract No. HR0011-11-C-
0145. The content does not necessarily reflect the 
position or the policy of the Government, and no 
official endorsement should be inferred. 
Nizar Habash performed most of his contribu-
tion to this paper while he was at the Center for 
Computational Learning Systems at Columbia 
University. 
References 
Mohamed Al-Badrashiny, Ramy Eskander, Nizar Ha-
bash, and Owen Rambow. 2014. Automatic Trans-
literation of Romanized Dialectal Arabic. In Pro-
ceedings of the Conference on Computational Nat-
ural Language Learning (CONLL), Baltimore, 
Maryland, 2014. 
Tim Buckwalter. 2004. Buckwalter Arabic Morpho-
logical Analyzer Version 2.0. LDC catalog number 
LDC2004L02, ISBN 1-58563-324-0.  
Achraf Chalabi and Hany Gerges. 2012. Romanized 
Arabic Transliteration. In Proceedings of the Sec- 
ond Workshop on Advances in Text Input Methods 
(WTIM 2012).  
Eleanor Clark and Kenji Araki. 2011. Text normaliza-
tion in social media: Progress, problems and ap- 
plications for a pre-processing system of casual 
English. Procedia - Social and Behavioral Scienc-
es, 27(0):2 ? 11. 
Kareem Darwish, Walid Magdy, and Ahmed Mourad. 
2012. Language processing for arabic microblog 
re- trieval. In Proceedings of the 21st ACM Inter-
national Conference on Information and 
Knowledge Management, CIKM ?12, pages 2427?
2430, New York, NY, USA. ACM.  
Kareem Darwish. 2013. Arabizi Detection and Con- 
version to Arabic. CoRR, arXiv:1306.6755 [cs.CL]. 
Leon Derczynski, Alan Ritter, Sam Clark, and Kalina 
Bontcheva. 2013. Twitter part-of-speech tagging 
for all: Overcoming sparse and noisy data. In Pro-
ceedings of the International Conference Recent 
Advances in Natural Language Processing RANLP 
2013, pages 198?206, Hissar, Bulgaria, September. 
INCOMA Ltd. Shoumen, Bulgaria.  
Heba Elfardy, Mohamed Al-Badrashiny, and Mona 
Diab. 2013. Code Switch Point Detection in Ara-
bic. In Proceedings of the 18th International Con-
ference on Application of Natural Language to In-
formation Systems (NLDB2013), MediaCity, UK, 
June.  
Ramy Eskander, Mohamed Al-Badrashiny, Nizar Ha-
bash and Owen Rambow. 2014. Foreign Words 
101
and the Automatic Processing of Arabic Social 
Media Text Written in Roman Script. In Arabic 
Natural Language Processing Workshop, EMNLP, 
Doha, Qatar. 
Ramy Eskander, Nizar Habash, Owen Rambow, and 
Nadi Tomeh. 2013. Processing Spontaneous Or- 
thography. In Proceedings of the 2013 Conference 
of the North American Chapter of the Association 
for Computational Linguistics: Human Language 
Technologies (NAACL-HLT), Atlanta, GA.  
Andrew T. Freeman, Sherri L. Condon and Christo-
pher M. Ackerman. 2006. Cross Linguistic Name 
Matching in English and Arabic: A ?One to Many 
Mapping? Extension of the Levenshtein Edit Dis-
tance Algorithm. In Proceedings of HLT-NAACL, 
New York, NY. 
Kevin Gimpel, Nathan Schneider, Brendan O?Connor, 
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Mi-
chael Heilman, Dani Yogatama, Jeffrey Flanigan, 
and Noah A. Smith. 2011. Part-of-speech tagging 
for twitter: Annotation, features, and experiments. 
In Proceedings of ACL-HLT ?11.  
Stephan Gouws, Donald Metzler, Congxing Cai, and 
Eduard Hovy. 2011. Contextual bearing on linguis-
tic variation in social media. In Proceedings of the 
Workshop on Languages in Social Media, LSM 
?11, pages 20?29, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics. 
Nizar Habash, Mona Diab, and Owen Rambow 
(2012a).Conventional Orthography for Dialectal 
Arabic: Principles and Guidelines ? Egyptian Ara-
bic. Technical Report CCLS-12-02, Columbia 
University Center for Computational Learning Sys-
tems.  
Nizar Habash, Mona Diab, and Owen Rabmow. 
2012b. Conventional Orthography for Dialectal 
Arabic. In Proceedings of the Language Resources 
and Evaluation Conference (LREC), Istanbul.  
Nizar Habash, Ramy Eskander, and Abdelati Haw-
wari. 2012c. A Morphological Analyzer for Egyp-
tian Arabic. In Proceedings of the Twelfth Meeting 
of the Special Interest Group on Computational 
Morphology and Phonology, pages 1?9, Montr?al, 
Canada.  
Kevin Knight and Jonathan Graehl. 1997. Machine 
Transliteration. In Proceedings of the Conference 
of the Association for Computational Linguistics 
(ACL). 
Linguistic Data Consortium. 2014. BOLT Program: 
Romanized Arabic (Arabizi) to Arabic Translitera-
tion and Normalization Guidelines, Version 3.1. 
Linguistic Data Consortium, April 21, 2014.  
Marco Lui, Jey Han Lau, and Timothy Baldwin. 
2014. Automatic detection and language identifica-
tion of multilingual documents. In Proceedings of 
the Language Resources and Evaluation Confer-
ence (LREC), Reykjavik, Iceland.  
Mohamed Maamouri, Ann Bies, Seth Kulick, Michael 
Ciul, Nizar Habash and Ramy Eskander. 2014. De-
veloping a dialectal Egyptian Arabic Treebank: 
Impact of Morphology and Syntax on Annotation 
and Tool Development. In Proceedings of the Lan-
guage Resources and Evaluation Conference 
(LREC), Reykjavik, Iceland. 
Yaser Al-Onaizan and Kevin Knight. 2002. Machine 
Transliteration of Names in Arabic Text. In Pro-
ceedings of ACL Workshop on Computational Ap-
proaches to Semitic Languages. 
Arfath Pasha, Mohamed Al-Badrashiny, Mona Diab, 
Ahmed El Kholy, Ramy Eskander, Nizar Habash, 
Manoj Pooleery, Owen Rambow, and Ryan M. 
Roth. 2014. MADAMIRA: A Fast, Comprehensive 
Tool for Morphological Analysis and Disambigua-
tion of Arabic. In Proceedings of the Language Re-
sources and Evaluation Conference (LREC), Rey-
kjavik, Iceland.  
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni. 
2011. Named entity recognition in tweets: An ex-
perimental study. In Proceedings of the Conference 
on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?11. 
Wael Salloum and Nizar Habash. 2011. Dialectal to 
Standard Arabic Paraphrasing to Improve Arabic- 
English Statistical Machine Translation. In Pro- 
ceedings of the First Workshop on Algorithms and 
Resources for Modelling of Dialects and Language 
Varieties, pages 10?21, Edinburgh, Scotland.  
Zhiyi Song, Stephanie Strassel, Haejoong Lee, Kevin 
Walker, Jonathan Wright, Jennifer Garland, Dana 
Fore, Brian Gainor, Preston Cabe, Thomas Thom-
as, Brendan Callahan, Ann Sawyer. Collecting 
Natural SMS and Chat Conversations in Multiple 
Languages: The BOLT Phase 2 Corpus. In Pro-
ceedings of the Language Resources and Evalua-
tion Conference (LREC) 2014, Reykjavik, Iceland. 
Clare Voss, Stephen Tratz, Jamal Laoudi, and Dou- 
glas Briesch. 2014. Finding romanized Arabic dia-
lect in code-mixed tweets. In Proceedings of the 
Ninth International Conference on Language Re-
sources and Evaluation (LREC?14), Reykjavik, 
Iceland. 
Omar F Zaidan and Chris Callison-Burch. 2011. The 
arabic online commentary dataset: an annotated da-
taset of informal arabic with high dialectal content. 
In Proceedings of ACL, pages 37?41. 
102
Appendix A: File Format Examples 
 
 
 
Example 1: 
 
<su id="s1582"> 
  <source>marwan ? ana walahi knt gaya today :/</source> 
   <annotated_arabizi> 
        <token id="t0" tag="name">marwan</token> 
       <token id="t1" tag="punctuation">?</token> 
       <token id="t2">ana</token> 
      <token id="t3">walahi</token> 
   <token id="t4">knt</token> 
        <token id="t5">gaya</token> 
       <token id="t6" tag="foreign">today</token> 
        <token id="t7">:/</token> 
     </annotated_arabizi> 
    <auto_transliteration> :/ ???? ???? ??? ?? ???  ?????? </auto_transliteration> 
<corrected_transliteration> # ????  ???? ??? ?? ???  ?????? </corrected_transliteration> 
<retokenized_transliteration> # ???? ???? ??? ?? ???  ?????? </retokenized_transliteration> 
     <translation lang="eng">Marwan? I swear I was coming today :/</translation> 
     <messages> 
<message id="m2377" time="2013-10-01 22:03:34 UTC" participant="139360">marwan ? ana 
walahi knt gaya today :/</message> 
     </messages> 
  </su> 
 
Example 2: 
 
<su id="s3"> 
<source>W sha3rak ma2sersh:D haha</source> 
<annotated_arabizi> 
<token id="t0">W</token> 
<token id="t1">sha3rak</token> 
<token id="t2">ma2sersh:D</token> 
<token id="t3" tag="sound">haha</token> 
</annotated_arabizi> 
<auto_transliteration> ?? # [-]????? ???? [+]? </auto_transliteration> 
<corrected_transliteration> ?? #[-]????[-]?? ???? [+]? </corrected_transliteration> 
<retokenized_transliteration> ?? # ???? ?? ????? </retokenized_transliteration> 
<translation lang="eng">And your hair did not become short? :D Haha</translation> 
<messages> 
<message id="m0004" medium="IM" time="2012-12-22 15:36:31 UTC" participant="138112">W 
sha3rak ma2sersh:D haha</message> 
</messages> 
</su> 
 
 
 
103
