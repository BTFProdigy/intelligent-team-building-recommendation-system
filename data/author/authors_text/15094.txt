Proceedings of the ACL-08: HLT Student Research Workshop (Companion Volume), pages 49?54,
Columbus, June 2008. c?2008 Association for Computational Linguistics
An Unsupervised Vector Approach to Biomedical Term Disambiguation:
Integrating UMLS and Medline
Bridget T. McInnes
Computer Science Department
University of Minnesota Twin Cities
Minneapolis, MN 55155, USA
bthomson@cs.umn.edu
Abstract
This paper introduces an unsupervised vector
approach to disambiguate words in biomedi-
cal text that can be applied to all-word dis-
ambiguation. We explore using contextual
information from the Unified Medical Lan-
guage System (UMLS) to describe the pos-
sible senses of a word. We experiment with
automatically creating individualized stoplists
to help reduce the noise in our dataset. We
compare our results to SenseClusters and
Humphrey et al (2006) using the NLM-WSD
dataset and with SenseClusters using con-
flated data from the 2005 Medline Baseline.
1 Introduction
Some words have multiple senses. For example, the
word cold could refer to a viral infection or the tem-
perature. As humans, we find it easy to determine
the appropriate sense (concept) given the context in
which the word is used. For a computer, though, this
is a difficult problem which negatively impacts the
accuracy of biomedical applications such as medical
coding and indexing. The goal of our research is to
explore using information from biomedical knowl-
edge sources such as the Unified Medical Language
System (UMLS) and Medline to help distinguish be-
tween different possible concepts of a word.
In the UMLS, concepts associated with words
and terms are enumerated via Concept Unique Iden-
tifiers (CUIs). For example, two possible senses
of cold are ?C0009264: Cold Temperature? and
?C0009443: Common Cold? in the UMLS release
2008AA. The UMLS is also encoded with differ-
ent semantic and syntactic structures. Some such
information includes related concepts and semantic
types. A semantic type (ST) is a broad subject cat-
egorization assigned to a CUI. For example, the ST
of ?C0009264: Cold Temperature? is ?Idea or Con-
cept? while the ST for ?C0009443: Common Cold?
is ?Disease or Syndrome?. Currently, there exists
approximately 1.5 million CUIs and 135 STs in the
UMLS. Medline is an online database that contains
11 million references biomedical articles.
In this paper, we introduce an unsupervised vector
approach to disambiguate words in biomedical text
using contextual information from the UMLS and
Medline. We compare our approach to Humphrey et
al. (2006) and SenseClusters. The ability to make
disambiguation decisions for words that have the
same ST differentiates SenseClusters and our ap-
proach from Humphrey et al?s (2006). For exam-
ple, the word weight in the UMLS has two possible
CUIs, ?C0005912: Body Weight? and ?C0699807:
Weight?, each having the ST ?Quantitative Con-
cept?. Humphrey et al?s (2006) approach relies on
the concepts having different STs therefore is unable
to disambiguate between these two concepts.
Currently, most word sense disambiguation ap-
proaches focus on lexical sample disambiguation
which only attempts to disambiguate a predefined
set of words. This type of disambiguation is not
practical for large scale systems. All-words dis-
ambiguation approaches disambiguate all ambigu-
ous words in a running text making them practi-
cal for large scale systems. Unlike SenseClusters,
Humphrey, et al (2006) and our approach can be
49
used to perform all-words disambiguation.
In the following sections, we first discuss related
work. We then discuss our approach, experiments
and results. Lastly, we discuss our conclusions and
future work.
2 Related Work
There has been previous work on word sense dis-
ambiguation in the biomedical domain. Leroy and
Rindflesch (2005) introduce a supervised approach
that uses the UMLS STs and their semantic relations
of the words surrounding the target word as features
into a Naive Bayes classifier. Joshi et al (2005) in-
troduce a supervised approach that uses unigrams
and bigrams surrounding the target word as features
into a Support Vector Machine. A unigram is a sin-
gle content word that occurs in a window of context
around the target word. A bigram is an ordered pair
of content words that occur in a window of context
around the target word. McInnes et al (2007) in-
troduce a supervised approach that uses CUIs of the
words surrounding the target word as features into a
Naive Bayes classifier.
Humphrey et al (2006) introduce an unsupervised
vector approach using Journal Descriptor (JD) In-
dexing (JDI) which is a ranking algorithm that as-
signs JDs to journal titles in MEDLINE. The authors
apply the JDI algorithm to STs with the assumption
that each possible concept has a distinct ST. In this
approach, an ST vector is created for each ST by ex-
tracting associated words from the UMLS. A target
word vector is created using the words surrounding
the target word. The JDI algorithm is used to obtain
a score for each word-JD and ST-JD pair using the
target word and ST vectors. These pairs are used to
create a word-ST table using the cosine coefficient
between the scores. The cosine scores for the STs of
each word surrounding the target word are averaged
and the concept associated with the ST that has the
highest average is assigned to the target word.
3 Vector Approaches
Patwardhan and Pedersen (2006) introduce a vector
measure to determine the relatedness between pairs
of concepts. In this measure, a co-occurrence matrix
of all words in a given corpus is created containing
how often they occur in the same window of con-
text with each other. A gloss vector is then created
for each concept containing the word vector for each
word in the concepts definition (or gloss). The co-
sine between the two gloss vectors is computed to
determine the concepts relatedness.
SenseClusters 1 is an unsupervised knowledge-
lean word sense disambiguation package The pack-
age uses clustering algorithms to group similar in-
stances of target words and label them with the ap-
propriate sense. The clustering algorithms include
Agglomerative, Graph partitional-based, Partitional
biased agglomerative and Direct k-way clustering.
The clustering can be done in either vector space
where the vectors are clustered directly or similar-
ity space where vectors are clustered by finding the
pair-wise similarities among the contexts. The fea-
ture options available are first and second-order co-
occurrence, unigram and bigram vectors. First-order
vectors are highly frequent words, unigrams or bi-
grams that co-occur in the same window of context
as the target word. Second-order vectors are highly
frequent words that occur with the words in their re-
spective first order vector.
We compare our approach to SenseClusters v0.95
using direct k-way clustering with the I2 clustering
criterion function and cluster in vector space. We ex-
periment with first-order unigrams and second-order
bigrams with a Log Likelihood Ratio greater than
3.84 and the exact and gap cluster stopping param-
eters (Purandare and Pedersen, 2004; Kulkarni and
Pedersen, 2005).
4 Our Approach
Our approach has three stages: i) we create a the
feature vector for the target word (instance vector)
and each of its possible concepts (concept vectors)
using SenseClusters, ii) we calculate the cosine be-
tween the instance vector and each of the concept
vectors, and iii) we assign the concept whose con-
cept vector is the closest to the instance vector to the
target word.
To create the the instance vector, we use the words
that occur in the same abstract as the target word as
features. To create the concept vector, we explore
four different context descriptions of a possible con-
cept to use as features. Since each possible concept
1http://senseclusters.sourceforge.net/
50
has a corresponding CUI in the UMLS, we explore
using: i) the words in the concept?s CUI definition,
ii) the words in the definition of the concept?s ST
definition, iii) the words in both the CUI and ST
definitions, and iv) the words in the CUI definition
unless one does not exist then the words in its ST
definition.
We explore using the same feature vector param-
eters as in the SenseCluster experiments: i) first-
order unigrams, and ii) second-order bigram. We
also explore using a more judicious approach to de-
termine which words to include in the feature vec-
tors. One of the problems with an unsupervised vec-
tor approach is its susceptibility to noise. A word
frequently seen in a majority of instances may not
be useful in distinguishing between different con-
cepts. To alleviate this problem, we create an in-
dividualized stoplist for each target word using the
inverse document frequency (IDF). We calculate the
IDF score for each word surrounding the target word
by taking the log of the number of documents in the
training data divided by the number of documents
the term has occurred in the dataset. We then ex-
tract those words that obtain an IDF score under the
threshold of one and add them to our basic stoplist
to be used when determining the appropriate sense
for that specific target word.
5 Data
5.1 Training Data
We use the abstracts from the 2005 Medline Base-
line as training data. The data contains 14,792,864
citations from the 2005 Medline repository. The
baseline contains 2,043,918 unique tokens and
295,585 unique concepts.
5.2 NLM-WSD Test Dataset
We use the National Library of Medicine?s Word
Sense Disambiguation (NLM-WSD) dataset devel-
oped by (Weeber et al, 2001) as our test set. This
dataset contains 100 instances of 50 ambiguous
words from 1998 MEDLINE abstracts. Each in-
stance of a target word was manually disambiguated
by 11 human evaluators who assigned the word a
CUI or ?None? if none of the CUIs described the
concept. (Humphrey et al, 2006) evaluate their ap-
proach using a subset of 13 out of the 50 words
whose majority sense is less than 65% and whose
possible concepts do not have the same ST. Instances
tagged as ?None? were removed from the dataset.
We evaluate our approach using these same words
and instances.
5.3 Conflate Test Dataset
To test our algorithm on a larger biomedical dataset,
we are creating our own dataset by conflating two
or more unambiguous words from the 2005 Med-
line Baseline. We determine which words to conflate
based on the following criteria: i) the words have a
single concept in the UMLS, ii) the words occur ap-
proximately the same number of times in the corpus,
and iii) the words do not co-occur together.
We create our dataset using name-conflate 2 to
extract instances containing the conflate words from
the 2005 Medline Baseline. Table 4 shows our cur-
rent set of conflated words with their corresponding
number of test (test) and training (train) instances.
We refer to the conflated words as their pseudowords
throughout the paper.
6 Experimental Results
In this section, we report the results of our ex-
periments. First, we compare the results of using
the IDF stoplist over a basic stoplist. Second, we
compare the results of using the different context
descriptions. Third, we compare our approach to
SenseClusters and Humphrey et al (2006) using the
NLM-WSD dataset. Lastly, we compare our ap-
proach to SenseClusters using the conflated dataset.
In the following tables, CUI refers to the CUI def-
inition of the possible concept as context, ST refers
to using the ST definition of the possible concept as
context, CUI+ST refers to using both definitions as
context, and CUI?ST refers to using the CUI defi-
nition unless if one doesn?t exist then using ST def-
inition. Maj. refers to the ?majority sense? baseline
which is accuracy that would be achieved by assign-
ing every instance of the target word with the most
frequent sense as assigned by the human evaluators.
6.1 Stoplist Results
Table 2 shows the overall accuracy of our approach
using the basic stoplist and the IDF stoplist on the
2http://www.d.umn.edu/ tpederse/namedata.html
51
target word Unigram BigramCUI ST CUI+ST CUI?ST CUI ST CUI+ST CUI?STadjustment 44.57 31.61 46.74 44.57 47.83 38.04 27.17 47.83blood pressure 39.39 34.34 41.41 38.38 43.43 27.27 47.47 38.38degree 3.13 70.31 70.31 70.31 3.13 48.44 48.44 48.44evaluation 50.51 50.51 53.54 51.52 50.51 54.55 52.53 51.52growth 63.64 51.52 42.42 63.64 63.64 51.52 48.48 63.64immunosuppression 50.51 46.46 50.51 50.51 43.43 57.58 48.48 43.43mosaic 0 33.33 27.08 37.50 0 28.13 22.92 22.92nutrition 28.41 34.09 35.23 25.00 38.64 39.77 36.36 37.50radiation 57.73 44.78 58.76 57.73 60.82 28.36 60.82 60.82repair 74.63 25.00 41.79 37.31 76.12 54.69 44.78 41.79scale 32.81 48.00 42.19 51.56 0 18.00 95.31 96.88sensitivity 6.00 50.56 48.00 48.00 8.00 44.94 18.00 18.00white 48.31 38.61 46.07 49.44 44.94 38.16 43.82 49.44
average 38.43 43.01 46.46 48.11 36.96 40.73 45.74 47.74
Table 1: Accuracy of Our Approach using Different Context Descriptions
NLM-WSD dataset using each of the different con-
text descriptions described above. The results show
an approximately a 2% higher accuracy over using
the basic stoplist. The exception is when using the
CUI context description; the accuracy decreased by
approximately 2% when using the unigram feature
set and approximately 1% when using the bigram
feature set.context Basic stoplist IDF stoplist
unigram bigram unigram bigram
CUI 41.02 37.68 38.43 36.96ST 42.74 37.14 43.01 40.73CUI+ST 44.13 42.71 46.46 45.74CUI?ST 46.61 45.58 48.11 47.74
Table 2: Accuracy of IDF stoplist on the NLM-WSD
dataset
6.1.1 Context Results
Table 1 shows the results of our approach using
the CUI and ST definitions as context for the possi-
ble concepts on the NLM-WSD dataset and Table 4
shows similar results using the conflate dataset.
On the NLM-WSD dataset, the results show a
large difference in accuracy between the contexts on
a word by word basis making it difficult to deter-
mine which of the context description performs the
best. The unigram results show that CUI?ST and
CUI+ST obtain the highest accuracy for five words,
and CUI and ST obtain the highest accuracy for one
word. The bigram results show that CUI?ST and
CUI obtains the highest accuracy for two words,
ST obtains the highest accuracy for four words, and
CUI+ST obtains the highest accuracy for one word.
The overall results show that using unigrams with
the context description CUI?ST obtains the high-
est overall accuracy.
On the conflated dataset, the pseudowords a a,
a o, d d and e e have a corresponding CUI defini-
tion for each of their possible concepts therefore the
accuracy for CUI and CUI? would be the same for
these datasets and is not reported. The pseudowords
a a i, x p p and d a m e do not have a CUI defini-
tions for each of their possible concepts. The results
show that CUI obtained the highest accuracy for six
out of the seven datasets and CUI?ST obtained the
highest accuracy for one. These experiments were
run using the unigram feature.
6.2 NLM-WSD Results
Table 3 shows the accuracy of the results obtained
by our unsupervised vector approach using the
CUI?ST context description, SenseClusters, and
the results reported by Humphrey et al (2006).
As seen with the context description results, there
exists a large difference in accuracy on a word by
word basis between the approaches. The results
show that Humphrey et al (2006) report a higher
overall accuracy compared to SenseClusters and our
approach. Although, Humphrey et al (2006) per-
formed better for 5 out of the 13 words where as
SenseClusters performed better for 9. The unigram
feature set with gap cluster stopping returned the
highest overall accuracy for SenseClusters. The
number of clusters for all of the gap cluster stopping
experiments were two except for growth which re-
turned one. For our approach, the unigram feature
set returned the highest overall accuracy.
52
target word senses Maj. Humphrey SenseClusters Our Approachet al 2006 exact cluster stopping gap cluster stopping CUI?STunigram bigram unigram bigram unigram bigramadjustment 3 66.67 76.67 49.46 38.71 55.91 45.16 44.57 47.83blood pressure 3 54.00 41.79 40.00 46.00 51.00 54.00 38.38 38.38degree 2 96.92 97.73 53.85 55.38 53.85 55.38 70.31 48.44evaluation 2 50.00 59.70 66.00 50.00 66.00 50.00 51.52 51.52growth 2 63.00 70.15 66.00 52.00 66.00 63.00 63.64 63.64immunosuppression 2 59.00 74.63 67.00 80.00 67.00 80.00 50.51 43.43mosaic 2 53.61 67.69 72.22 58.57 61.86 50.52 37.50 22.92nutrition 2 50.56 35.48 40.45 47.19 44.94 41.57 25.00 37.50radiation 2 62.24 78.79 69.39 56.12 69.39 56.12 57.73 60.82repair 2 76.47 86.36 86.76 73.53 86.76 73.53 37.31 41.79scale 2 100.0 60.47 100.0 100.0 100.0 100.0 51.56 96.88sensitivity 2 96.08 82.86 41.18 41.18 52.94 54.90 48.00 18.00white 2 54.44 55.00 80.00 53.33 80.00 53.33 49.44 49.44
average 67.92 68.26 64.02 57.85 65.82 59.81 48.11 47.74
Table 3: Accuracy of Approaches using the NLM-WSD Dataset
target word pseudo- test train Maj. Sense Our Approachword Clusters CUI ST CUI+ST CUI?STactin-antigens a a 33193 298723 63.44 91.30 53.95 44.81 54.17angiotensin II-olgomycin a o 5256 47294 93.97 56.76 16.62 20.68 17.73dehydrogenase-diastolic d d 22606 203441 58.57 95.85 45.78 43.94 45.70endogenous-extracellular matrix e e 19820 178364 79.92 71.21 74.34 65.37 73.37allogenic-arginine-ischemic a a i 22915 206224 57.16 69.03 47.68 24.60 33.77 32.07X chromosome-peptide-plasmid x p p 46102 414904 74.61 66.21 20.04 31.60 42.89 42.98diacetate-apamin-meatus-enterocyte d a m e 1358 12212 25.95 74.23 28.87 24.08 26.07 22.68
Table 4: Accuracy of Approaches using the Conflate Dataset
6.3 Conflate Results
Table 4 shows the accuracy of the results obtained by
our approach and SenseClusters. The results show
that SenseClusters returns a higher accuracy than
our approach except for the e e dataset.
7 Discussion
We report the results for four experiments in this pa-
per: i) the results of using the IDF stoplist over a ba-
sic stoplist, ii) the results of our approach using dif-
ferent context descriptions of the possible concepts
of a target word, iii) the results of our approach com-
pared to SenseClusters and Humphrey et al (2006)
using the NLM-WSD dataset, and iv) the results of
our approach compared to SenseClusters using the
conflated dataset.
The results of using an individualized IDF stoplist
for each target word show an improvement over us-
ing the basic stoplist. The results of our approach
using different context descriptions show that for the
NLM-WSD dataset the large differences in accuracy
makes it unclear which of the context descriptions
performed the best. On the conflated dataset, adding
the ST definition to the context description improved
the accuracy of only one pseudoword. When com-
paring our approach to Humphrey et al (2006) and
SenseClusters, our approach did not return a higher
accuracy.
When analyzing the data, we found that there does
not exist a CUI definition for a large number of pos-
sible concepts. Table 5 shows the number of words
in the CUI and ST definitions for each concept in the
NLM-WSD dataset. Only four target words have a
CUI definition for each possible concept. We also
found the concept definitions vary widely in length.
The CUI definitions in the UMLS come from a va-
riety of sources and there may exist more than one
definition per source. Unlike CUI definitions, there
does exist an ST definition for each possible con-
cept. The ST definitions come from the same source
and are approximately the same length but they are
a broad categorization. We believe this makes them
too coarse grained to provide descriptive enough in-
formation about their associated concepts.
This can also be seen when analyzing the con-
flate datasets. The conflate dataset d a m e is miss-
ing two definition which is a contributing factor to
its low accuracy for CUI. Adding the ST definition
53
target word CUI Definition ST Definitionc1 c2 c3 c1 c2 c3adjustment 41 9 48 31 19 10blood pressure 26 18 0 20 31 22degree 0 0 15 23evaluation 54 0 33 17growth 91 91 20 19immunosuppression 130 41 30 20mosaic 0 38 0 10 10 23nutrition 152 152 0 10 31 30radiation 71 207 14 30repair 0 51 30 20scale 0 10 144 47 23 8sensitivity 0 0 0 25 50 22white 0 60 15 28
Table 5: Number of words in CUI and ST Definitions of
Possible the Concepts in the NLM-WSD Dataset
though did not provide enough distinctive informa-
tion to distinguish between the possible concepts.
8 Conclusions and Future Work
This paper introduces an unsupervised vector ap-
proach to disambiguate words in biomedical text us-
ing contextual information from the UMLS. Our ap-
proach makes disambiguation decisions for words
that have the same ST unlike Humphrey et al
(2006). We believe that our approach shows promise
and leads us to our goal of exploring the use of
biomedical knowledge sources.
In the future, we would also like to increase the
size of our conflated dataset and possibly create a
biomedical all-words disambiguation test set to test
our approach. Unlike SenseClusters, our approach
can be used to perform all-words disambiguation.
For example, given the sentence: His weight has
fluctuated during the past month. We first create
a instance vector containing fluctuated, past and
months for the word weight and a concept vector
for each of its possible concepts, ?C0005912: Body
Weight? and ?C0699807: Quantitative Concept? us-
ing their context descriptions. We then calculate the
cosine between the instance vector and each of the
two concept vectors. The concept whose vector has
the smallest cosine score is assigned to weight. We
then repeat this process for fluctuated, past and
months.
We also plan to explore using different contex-
tual information to improve the accuracy of our
approach. We are currently exploring using co-
occurrence and relational information about the pos-
sible CUIs in the UMLS. Our IDF stoplist exper-
iments show promise, we are planning to explore
other measures to determine which words to include
in the stoplist as well as a way to automatically de-
termine the threshold.
Acknowledgments
The author thanks Ted Pedersen, John Carlis and
Siddharth Patwardhan for their comments.
Our experiments were conducted using
CuiTools v0.15, which is freely available from
http://cuitools.sourceforge.net.
References
S.M. Humphrey, W.J. Rogers, H. Kilicoglu, D. Demner-
Fushman, and T.C. Rindflesch. 2006. Word sense dis-
ambiguation by selecting the best semantic type based
on journal descriptor indexing: Preliminary experi-
ment. Journal of the American Society for Information
Science and Technolology, 57(1):96?113.
M. Joshi, T. Pedersen, and R. Maclin. 2005. A compar-
ative study of support vectors machines applied to the
supervised word sense disambiguation problem in the
medical domain. In Proceedings of 2nd Indian Inter-
national Conference on AI, pages 3449?3468, Dec.
A. Kulkarni and T. Pedersen. 2005. SenseClusters: un-
supervised clustering and labeling of similar contexts.
In Proceedings of the ACL 2005 on Interactive poster
and demonstration sessions, pages 105?108, June.
G. Leroy and T.C. Rindflesch. 2005. Effects of in-
formation and machine learning algorithms on word
sense disambiguation with small datasets. Interna-
tional Journal of Medical Info., 74(7-8):573?85.
B. McInnes, T. Pedersen, and J. Carlis. 2007. Using umls
concept unique identifiers (cuis) for word sense disam-
biguation in the biomedical domain. In Proceedings of
the Annual Symposium of the American Medical Infor-
matics Association, pages 533?37, Chicago, IL, Nov.
S. Patwardhan and T. Pedersen. 2006. Using WordNet-
based Context Vectors to Estimate the Semantic Relat-
edness of Concepts. In Proceedings of the EACL 2006
Workshop Making Sense of Sense - Bringing Computa-
tional Linguistics and Psycholinguistics Together, vol-
ume 1501, pages 1?8, Trento, Italy, April.
A. Purandare and T. Pedersen. 2004. Word sense dis-
crimination by clustering contexts in vector and sim-
ilarity spaces. In Proceedings of the Conference on
CoNLL, pages 41?48.
M. Weeber, J.G. Mork, and A.R. Aronson. 2001. Devel-
oping a test collection for biomedical word sense dis-
ambiguation. In Proceedings of the American Medical
Informatics Association Symposium, pages 746?750.
54
BioNLP 2007: Biological, translational, and clinical language processing, pages 9?16,
Prague, June 2007. c?2007 Association for Computational Linguistics
Determining the Syntactic Structure of Medical Terms in Clinical Notes
Bridget T. McInnes
Dept. of Computer Science
and Engineering
University of Minnesota
Minneapolis, MN, 55455
bthomson@cs.umn.edu
Ted Pedersen
Dept. of Computer Science
University of Minnesota Duluth
Duluth, MN, 55812
tpederse@d.umn.edu
Serguei V. Pakhomov
Dept. of Pharmaceutical Care
and Health Systems Center
for Health Informatics
University of Minnesota
Minneapolis, MN, 55455
pakh0002@umn.edu
Abstract
This paper demonstrates a method for de-
termining the syntactic structure of medi-
cal terms. We use a model-fitting method
based on the Log Likelihood Ratio to clas-
sify three-word medical terms as right or
left-branching. We validate this method by
computing the agreement between the clas-
sification produced by the method and man-
ually annotated classifications. The results
show an agreement of 75% - 83%. This
method may be used effectively to enable
a wide range of applications that depend
on the semantic interpretation of medical
terms including automatic mapping of terms
to standardized vocabularies and induction
of terminologies from unstructured medical
text.
1 Introduction
Most medical concepts are expressed via a domain
specific terminology that can either be explicitly
agreed upon or extracted empirically from domain
specific text. Regardless of how it is constructed,
a terminology serves as a foundation for informa-
tion encoding, processing and exchange in a special-
ized sub-language such as medicine. Concepts in the
medical domain are encoded through a variety of lin-
guistic forms, the most typical and widely accepted
is the noun phrase (NP). In some even further spe-
cialized subdomains within medicine, such as nurs-
ing and surgery, an argument can be made that some
concepts are represented by an entire predication
rather than encapsulated within a single nominal-
ized expression. For example, in order to describe
someone?s ability to lift objects 5 pounds or heav-
ier above their head, it may be necessary to use a
term consisting of a predicate such as [LIFT] and a
set of arguments corresponding to various thematic
roles such as <PATIENT> and <PATH> (Ruggieri
et al, 2004). In this paper, we address typical med-
ical terms encoded as noun phrases (NPs) that are
often structurally ambiguous, as in Example 1, and
discuss a case for extending the proposed method to
non-nominalized terms as well.
small1 bowel2 obstruction3 (1)
The NP in Example 1 can have at least two interpre-
tations depending on the syntactic analysis:
[[small1 bowel2] obstruction3] (2)
[small1 [bowel2 obstruction3]] (3)
The term in Example 2 denotes an obstruction in
the small bowel, which is a diagnosable disorder;
whereas, the term in Example 3 refers to a small un-
specified obstruction in the bowel.
Unlike the truly ambiguous general English cases
such as the classical ?American History Professor?
where the appropriate interpretation depends on the
context, medical terms, such as in Example 1, tend
to have only one appropriate interpretation. The
context, in this case, is the discourse domain of
medicine. From the standpoint of the English lan-
guage, the interpretation that follows from Example
3 is certainly plausible, but unlikely in the context
of a medical term. The syntax of a term only shows
9
what interpretations are possible without restricting
them to any particular one. From the syntactic anal-
ysis, we know that the term in Example 1 has the po-
tential for being ambiguous; however, we also know
that it does have an intended interpretation by virtue
of being an entry term in a standardized terminology
with a unique identifier anchoring its meaning. What
we do not know is which syntactic structure gen-
erated that interpretation. Being able to determine
the structure consistent with the intended interpreta-
tion of a clinical term can improve the analysis of
unrestricted medical text and subsequently improve
the accuracy of Natural Language Processing (NLP)
tasks that depend on semantic interpretation.
To address this problem, we propose to use a
model-fitting method which utilizes an existing sta-
tistical measure, the Log Likelihood Ratio. We val-
idate the application of this method on a corpus
of manually annotated noun-phrase-based medical
terms. First, we present previous work on structural
ambiguity resolution. Second, we describe the Log
Likelihood Ratio and then its application to deter-
mining the structure of medical terms. Third, we
describe the training corpus and discuss the compi-
lation of a test set of medical terms and human ex-
pert annotation of those terms. Last, we present the
results of a preliminary validation of the method and
discuss several possible future directions.
2 Previous Work
The problem of resolving structural ambiguity has
been previously addressed in the computational lin-
guistics literature. There are multiple approaches
ranging from purely statistical (Ratnaparkhi, 1998),
to hybrid approaches that take into account the lexi-
cal semantics of the verb (Hindle and Rooth, 1993),
to corpus-based, which is the approach discussed
in this paper. (Marcus, 1980) presents an early ex-
ample of a corpus-based approach to syntactic am-
biguity resolution. One type of structural ambigu-
ity that has received much attention has to do with
nominal compounds as seen in the work of (Resnik,
1993), (Resnik and Hearst, 1993), (Pustejovsky et
al., 1993), and (Lauer, 1995).
(Lauer, 1995) points out that the existing ap-
proaches to resolving the ambiguity of noun phrases
fall roughly into two camps: adjacency and de-
pendency. The proponents of the adjacency model
((Liberman and Sproat, 1992), (Resnik, 1993) and
(Pustejovsky et al, 1993)) argue that, given a three
word noun phrase XYZ, there are two possible an-
alyzes [[XY]Z] and [X[YZ]]. The correct analysis
is chosen based on the ?acceptability? of the adja-
cent bigrams A[XY] and A[YZ]. If A[XY] is more
acceptable than A[YZ], then the left-branching anal-
ysis [[XY]Z] is preferred.
(Lauer and Dras, 1994) and (Lauer, 1995) address
the issue of structural ambiguity by developing a de-
pendency model where instead of computing the ac-
ceptability of A[YZ] one would compute the accept-
ability of A[XZ]. (Lauer, 1995) argues that the de-
pendency model is not only more intuitive than the
adjacency model, but also yields better results. (La-
pata and Keller, 2004) results also support this as-
sertion.
The difference between the approaches within the
two models is the computation of acceptability. Pro-
posals for computing acceptability (or preference)
include raw frequency counts ((Evans and Zhai,
1996) and (Lapata and Keller, 2004)), Latent Se-
mantic Indexing ((Buckeridge and Sutcliffe, 2002))
and statistical measures of association ((Lapata et
al., 1999) and (Nakov and Hearst, 2005)).
One of the main problems with using frequency
counts or statistical methods for structural ambigu-
ity resolution is the sparseness of data; however,
(Resnik and Hearst, 1993) used conceptual associa-
tions (associations between groups of terms deemed
to form conceptual units) in order to alleviate this
problem. (Lapata and Keller, 2004) use the doc-
ument counts returned by WWW search engines.
(Nakov and Hearst, 2005) use the ?2 measure based
on statistics obtained from WWW search engines to
compute values to determine acceptability of a syn-
tactic analysis for nominal compounds. This method
is tested using a set of general English nominal com-
pounds developed by (Lauer, 1995) as well as a set
of nominal compounds extracted from MEDLINE
abstracts.
The novel contribution of our study is in demon-
strating and validating a corpus-based method for
determining the syntactic structure of medical terms
that relies on using the statistical measure of asso-
ciation, the Log Likelihood Ratio, described in the
following section.
10
3 Log Likelihood Ratio
The Log Likelihood Ratio (G2) is a ?goodness of
fit? statistic first proposed by (Wilks, 1938) to test if
a given piece of data is a sample from a set of data
with a specific distribution described by a hypothe-
sized model. It was later applied by (Dunning, 1993)
as a way to determine if a sequence of N words (N-
gram) came from an independently distributed sam-
ple.
(Pedersen et al, 1996) pointed out that there ex-
ists theoretical assumptions underlying the G2 mea-
sure that were being violated therefore making them
unreliable for significance testing. (Moore, 2004)
provided additional evidence that although G2 may
not be useful for determining the significance of an
event, its near equivalence to mutual information
makes it an appropriate measure of word associa-
tion. (McInnes, 2004) applied G2 to the task of ex-
tracting three and four word collocations from raw
text.
G2, formally defined for trigrams in Equation 4,
compares the observed frequency counts with the
counts that would be expected if the words in the
trigram (3-gram; a sequence of three words) corre-
sponded to the hypothesized model.
G2 = 2 ?
?
x,y,z
nxyz ? log(
nxyz
mxyz
) (4)
The parameter nxyz is the observed frequency of
the trigram where x, y, and z respectively represent
the occurrence of the first, second and third words
in the trigram. The variable mxyz is the expected
frequency of the trigram which is calculated based
on the hypothesized model. This calculation varies
depending on the model used. Often the hypothe-
sized model used is the independence model which
assumes that the words in the trigram occur together
by chance. The calculation of the expected values
based on this model is as follows:
mxyz = nx++ ? n+y+ ? n++z/n+++ (5)
The parameter, n+++, is the total number of tri-
grams that exist in the training data, and nx++,
n+y+, and n++z are the individual marginal counts
of seeing words x, y, and z in their respective posi-
tions in a trigram. A G2 score reflects the degree to
which the observed and expected values diverge. A
G2 score of zero implies that the observed values are
equal to the expected and the trigram is represented
perfectly by the hypothesized model. Hence, we
would say that the data ?fits? the model. Therefore,
the higher the G2 score, the less likely the words
in the trigram are represented by the hypothesized
model.
4 Methods
4.1 Applying Log Likelihood to Structural
Disambiguation
The independence model is the only hypothesized
model used for bigrams (2-gram; a sequence of
two words). As the number of words in an N-
gram grows, the number of hypothesized models
also grows. The expected values for a trigram can
be based on four models. The first model is the
independence model discussed above. The second
is the model based on the probability that the first
word and the second word in the trigram are depen-
dent and independent of the third word. The third
model is based on the probability that the second
and third words are dependent and independent of
the first word. The last model is based on the prob-
ability that the first and third words are dependent
and independent of the second word. Table 1 shows
the different models for the trigram XYZ.
Table 1: Models for the trigram XYZ
Model 1 P(XYZ) / P(X) P(Y) P(Z)
Model 2 P(XYZ) / P(XY) P(Z)
Model 3 P(XYZ) / P(X) / P(YZ)
Model 4 P(XYZ) / P(XZ) P(Y)
Slightly different formulas are used to calculate
the expected values for the different hypothesized
models. The expected values for Model 1 (the in-
dependence model) are given above in Equation 5.
The calculation of expected values for Model 2, 3, 4
are seen in Equations 6, 7, 8 respectively.
mxyz = nxy+ ? n++z/n+++ (6)
mxyz = nx++ ? n+yz/n+++ (7)
mxyz = nx+z ? n+y+/n+++ (8)
The parameter nxy+ is the number of times words
x and y occur in their respective positions, n+yz is
11
the number of times words y and z occur in their
respective positions and nx+z is the number of times
that words x and z occur in their respective positions
in the trigram.
The hypothesized models result in different ex-
pected values which results in a different G2 score.
A G2 score of zero implies that the data are perfectly
represented by the hypothesized model and the ob-
served values are equal to the expected. Therefore,
the model that returns the lowest score for a given
trigram is the model that best represents the struc-
ture of that trigram, and hence, best ?fits? the trigram.
For example, Table 2 shows the scores returned for
each of the four hypothesized models for the trigram
?small bowel obstruction?.
Table 2: Example for the term ?small bowel obstruc-
tion?
Model G2 score Model G2 score
Model 1 11,635.45 Model 2 5,169.81
Model 3 8,532.90 Model 4 7,249.90
The smallest G2 score is returned by Model 2
which is based on the first and second words be-
ing dependent and independent of the third. Based
on the data, Model 2 best represents or ?fits? the tri-
gram, ?small bowel obstruction?. In this particular
case that happens to be the correct analysis.
The frequency counts and G2 scores for each
model were obtained using the N-gram Statistics
Package 1 (Banerjee and Pedersen, 2003).
4.2 Data
The data for this study was collected from two
sources: the Mayo Clinic clinical notes and
SNOMED-CT terminology (Stearns et al, 2001).
4.2.1 Clinical Notes
The corpus used in this study consists of over
100,000 clinical notes covering a variety of ma-
jor medical specialties at the Mayo Clinic. These
notes document each patient-physician contact and
are typically dictated over the telephone. They range
in length from a few lines to several pages of text
and represent a quasi-spontaneous discourse where
the dictations are made partly from notes and partly
1http://www.d.umn.edu/ tpederse/nsp.html
from memory. At the Mayo Clinic, the dictations
are transcribed by trained personnel and are stored
in the patient?s chart electronically.
4.2.2 SNOMED-CT
SNOMED-CT (Systematized Nomenclature of
Medicine, Clinical Terminology) is an ontologi-
cal resource produced by the College of American
Pathologists and distributed as part of the Unified
Medical Language System2 (UMLS) Metathesaurus
maintained by the National Library of Medicine.
SNOMED-CT is the single largest source of clini-
cal terms in the UMLS and as such lends itself well
to the analysis of terms found in clinical reports.
SNOMED-CT is used for many applications in-
cluding indexing electronic medical records, ICU
monitoring, clinical decision support, clinical trials,
computerized physician order entry, disease surveil-
lance, image indexing and consumer health informa-
tion services. The version of SNOMED-CT used in
this study consists of more than 361,800 unique con-
cepts with over 975,000 descriptions (entry terms)
(SNOMED-CT Fact Sheet, 2004).
4.3 Testset of Three Word Terms
We used SNOMED-CT to compile a list of terms
in order to develop a test set to validate the G2
method. The test set was created by extracting all
trigrams from the corpus of clinical notes and all
three word terms found in SNOMED-CT. The inter-
section of the SNOMED-CT terms and the trigrams
found in the clinical notes was further restricted to
include only simple noun phrases that consist of a
head noun modified with a set of other nominal or
adjectival elements including adjectives and present
and past participles. Adverbial modification of ad-
jectives was also permitted (e.g. ?partially edentu-
lous maxilla?). Noun phrases with nested prepo-
sitional phrases such as ?fear of flying? as well as
three word terms that are not noun phrases such as
?does not eat? or ?unable to walk? were excluded
from the test set. The resulting test set contains 710
items.
The intended interpretation of each three word
term (trigram) was determined by arriving at a
2Unified Medical Language System is a compendium of
over 130 controlled medical vocabularies encompassing over
one million concepts.
12
consensus between two medical index experts
(kappa=0.704). These experts have over ten years of
experience with classifying medical diagnoses and
are highly qualified to carry out the task of deter-
mining the intended syntactic structure of a clinical
term.
Table 3: Four Types of Syntactic Structures of Tri-
gram Terms
left-branching ((XY)Z):
[[urinary tract] infection]
[[right sided] weakness]
right-branching (X(YZ)):
[chronic [back pain]]
[low [blood pressure]]
non-branching ((X)(Y)(Z)):
[[follicular][thyroid][carcinoma]]
[[serum][dioxin][level]]
monolithic (XYZ):
[difficulty finding words]
[serous otitis media]
In the process of annotating the test set of tri-
grams, four types of terms emerged (Table 3). The
first two types are left and right-branching where the
left-branching phrases contain a left-adjoining group
that modifies the head of the noun phrase. The right-
branching phrases contain a right-adjoining group
that forms the kernel or the head of the noun phrase
and is modified by the remaining word on the left.
The non-branching type is where the phrase contains
a head noun that is independently modified by the
other two words. For example, in ?follicular thyroid
carcinoma?, the experts felt that ?carcinoma? was
modified by both ?follicular? and ?thyroid? indepen-
dently, where the former denotes the type of cancer
and the latter denotes its location. This intuition is
reflected in some formal medical classification sys-
tems such as the Hospital International Classifica-
tion of Disease Adaptation (HICDA) where cancers
are typically classified with at least two categories -
one for location and one for the type of malignancy.
This type of pattern is rare. We were able to iden-
tify only six examples out of the 710 terms. The
monolithic type captures the intuition that the terms
function as a collocation and are not decomposable
into subunits. For example, ?leg length discrepancy?
denotes a specific disorder where one leg is of a dif-
ferent length from the other. Various combinations
of subunits within this term result in nonsensical ex-
pressions.
Table 4: Distribution of term types in the test set
Type Count %total
Left-branching 251 35.5
Right-branching 378 53.4
Non-branching 6 0.8
Monolithic 73 10.3
Total 708 100
Finally, there were two terms for which no con-
sensus could be reached: ?heart irregularly irregu-
lar? and ?subacute combined degeneration?. These
cases were excluded from the final set. Table 4
shows the distribution of the four types of terms in
the test set.
5 Evaluation
We hypothesize that general English typically has
a specific syntactic structure in the medical domain,
which provides a single semantic interpretation. The
patterns observed in the set of 710 medical terms
described in the previous section suggest that the
G2 method offers an intuitive way to determine the
structure of a term that underlies its syntactic struc-
ture.
Table 5: G2 Model Descriptions
left-branching Model 2 [ [XY] Z ]
right-branching Model 3 [ X [YZ] ]
The left and right-branching patterns roughly cor-
respond to Models 2 and 3 in Table 5. Models 1
and 4 do not really correspond to any of the pat-
terns we were able to identify in the set of terms.
Model 1 would represent a term where words are
completely independent of each other, which is an
unlikely scenario given that we are working with
terms whose composition is dependent by definition.
This is not to say that in other applications (e.g.,
syntactic parsing) this model would not be relevant.
Model 4 suggests dependence between the outer
edges of a term and their independence from the
13
Figure 1: Comparison of the results with two base-
lines: L-branching and R-branching assumptions
middle word, which is not motivated from the stand-
point of a traditional context free grammar which
prohibits branch crossing. However, this model may
be welcome in a dependency grammar paradigm.
One of the goals of this study is to test an ap-
plication of the G2 method trained on a corpus of
medical data to distinguish between left and right-
branching patterns. The method ought to suggest
the most likely analysis for an NP-based medical
term based on the empirical distribution of the term
and its components. As part of the evaluation, we
compute the G2 scores for each of the terms in the
test set, and picked the model with the lowest score
to represent the structural pattern of the term. We
compared these results with manually identified pat-
terns. At this preliminary stage, we cast the problem
of identifying the structure of a three word medical
term as a binary classification task where a term is
considered to be either left or right-branching, ef-
fectively forcing all terms to either be represented
by either Model 2 or Model 3.
6 Results and Discussion
In order to validate the G2 method for determin-
ing the structure of medical terms, we calculated
the agreement between human experts? interpreta-
tion of the syntactic structure of the terms and the
interpretation suggested by the G2 method. The
agreement was computed as the ratio of match-
ing interpretations to the total number of terms be-
ing interpreted. We used two baselines, one estab-
lished by assuming that each term is left-branching
and the other by assuming that each term is right-
branching. As is clear from Table 4, the left-
branching baseline is 35.5% and the right-branching
baseline is 53.4% meaning that if we simply as-
sign left-branching pattern to each three word term,
we would agree with human experts 35.5% of the
time. The G2 method correctly identifies 185 tri-
grams as being left-branching (Model 2) and 345 tri-
grams as being right-branching (Model 3). There are
116 right-branching trigrams incorrectly identified
as left-branching, and 62 left-branching trigrams in-
correctly identified as right- branching. Thus the
method and the human experts agreed on 530 (75%)
terms out of 708 (kappa=0.473), which is better than
both baselines (Figure 1). We did not find any over-
lap between the terms that human experts annotated
as non-branching and the terms whose corpus dis-
tribution can be represented by Model 4 ([[XZ]Y]).
This is not surprising as this pattern is very rare.
Most of the terms are represented by either Model 2
(left-branching) or Model 3 (right-branching). The
monolithic terms that the human experts felt were
not decomposable constitute 10% of all terms and
may be handled through some other mechanism
such as collocation extraction or dictionary lookup.
Excluding monolithic terms from testing results in
83.5% overall agreement (kappa=0.664).
We observed that 53% of the terms in our test
set are right-branching while only 35% are left-
branching. (Resnik, 1993) found between 64% and
67% of nominal compounds to be left-branching and
used that finding to establish a baseline for his exper-
iments with structural ambiguity resolution. (Nakov
and Hearst, 2005) also report a similar percentage
(66.8%) of left-branching noun compounds. Our
test set is not limited to nominal compounds, which
may account for the fact that a slight majority of the
terms are found to be right-branching as adjectival
modification in English is typically located to the
left of the head noun. This may also help explain
the fact that the method tends to have higher agree-
ment within the set of right-branching terms (85%)
vs. left-branching (62%).
We also observed that many of the terms marked
as monolithic by the experts are of Latin origin such
as the term in Example 9 or describe the functional
14
status of a patient such as the term in Example 10.
erythema1 ab2 igne3 (9)
difficulty1 swallowing2 solids3 (10)
Example 10 merits further discussion as it illus-
trates another potential application of the method
in the domain of functional status terminology. As
was mentioned in the introduction, functional status
terms may be be represented as a predication with
a set of arguments. Such view of functional status
terminology lends itself well to a frame-based repre-
sentation of functional status terms in the context of
a database such as FrameNet 3 or PropBank4. One of
the challenging issues in representing functional sta-
tus terminology in terms of frames is the distinction
between the core predicate and the frame elements
(Ruggieri et al, 2004). It is not always clear what
lexical material should be part of the core predicate
and what lexical material should be part of one or
more arguments. Consider the term in Example 10
which represents a nominalized form of a predica-
tion. Conceivably, we could analyze this term as a
frame shown in Example 11 where the predication
consists of a predicate [DIFFICULTY] and two ar-
guments. Alternatively, Example 12 presents a dif-
ferent analysis where the predicate is a specific kind
of difficulty with a single argument.
[P:DIFFICULTY]
[ARG1:SWALLOWING<ACTIVITY>]
[ARG2:SOLIDS<PATIENT>]
(11)
[P:SWALLOWING DIFFICULTY]
[ARG1: SOLIDS<PATIENT>]
(12)
The analysis dictates the shape of the frames
and how the frames would fit into a network of
frames. The G2 method identifies Example 10 as
left-branching (Model 2), which suggests that it
would be possible to have a parent DIFFICULTY
frame and a child CLIMBING DIFFICULTY that
would inherit form its parent. An example where
this is not possible is the term ?difficulty staying
asleep? where it would probably be nonsensical or at
least impractical to have a predicate such as [STAY-
ING DIFFICULTY]. It would be more intuitive to
3http://www.icsi.berkeley.edu/framenet/
4http://www.cis.upenn.edu/ ace/
assign this term to the DIFFICULTY frame with
a frame element whose lexical content is ?staying
asleep?. The method appropriately identifies the
term ?difficulty staying asleep? as right-branching
(Model 3) where the words ?staying asleep? are
grouped together. This is an example based on in-
formal observations; however, it does suggest a util-
ity in constructing frame-based representation of at
least some clinical terms.
7 Limitations
The main limitation of the G2 method is the expo-
nential growth in the number of models to be evalu-
ated with the growth in the length of the term. This
limitation can be partly alleviated by either only con-
sidering adjacent models and limiting the length to
5-6 words, or using a forward or backward sequen-
tial search proposed by (Pedersen et al, 1997) for
the problem of selecting models for the Word Sense
Disambiguation task.
8 Conclusions and Future Work
This paper presented a simple but effective method
based on G2 to determine the internal structure of
three-word noun phrase medical terms. The abil-
ity to determine the syntactic structure that gives
rise to a particular semantic interpretation of a med-
ical term may enable accurate mapping of unstruc-
tured medical text to standardized terminologies and
nomenclatures. Future directions to improve the ac-
curacy of our method include determining how other
measures of association, such as dice coefficient and
?2, perform on this task. We feel that there is a pos-
sibility that no single measure performs best over all
types of terms. In that case, we plan to investigate in-
corporating the different measures into an ensemble-
based algorithm.
We believe the model-fitting method is not lim-
ited to structural ambiguity resolution. This method
could be applied to automatic term extraction and
automatic text indexing of terms from a standard-
ized vocabulary. More broadly, the principles of us-
ing distributional characteristics of word sequences
derived from large corpora may be applied to unsu-
pervised syntactic parsing.
15
Acknowledgments
We thank Barbara Abbott, Debra Albrecht and
Pauline Funk for their contribution to annotating the
test set and discussing aspects of medical terms.
This research was supported in part by the
NLM Training Grant in Medical Informatics (T15
LM07041-19). Ted Pedersen?s participation in this
project was supported by the NSF Faculty Early Ca-
reer Development Award (#0092784).
References
S. Banerjee and T. Pedersen. 2003. The design, imple-
mentation, and use of the Ngram Statistic Package. In
Proc. of the Fourth International Conference on Intel-
ligent Text Processing and Computational Linguistics,
Mexico City, February.
A.M. Buckeridge and R.F.E. Sutcliffe. 2002. Disam-
biguating noun compounds with latent semantic index-
ing. International Conference On Computational Lin-
guistics, pages 1?7.
T. Dunning. 1993. Accurate methods for the statistics of
surprise and coincidence. Computational Linguistics,
19(1):61?74.
D.A. Evans and C. Zhai. 1996. Noun-phrase analysis in
unrestricted text for information retrieval. Proc. of the
34th conference of ACL, pages 17?24.
D. Hindle and M. Rooth. 1993. Structural Ambigu-
ity and Lexical Relations. Computational Linguistics,
19(1):103?120.
M. Lapata and F. Keller. 2004. The Web as a Base-
line: Evaluaing the Performance of Unsupervised
Web-based Models for a Range of NLP Tasks. Proc.
of HLT-NAACL, pages 121?128.
M. Lapata, S. McDonald, and F. Keller. 1999. Determi-
nants of Adjective-Noun Plausibility. Proc. of the 9th
Conference of the European Chapter of ACL, 30:36.
M. Lauer and M. Dras. 1994. A Probabilistic Model of
Compound Nouns. Proc. of the 7th Australian Joint
Conference on AI.
M. Lauer. 1995. Corpus Statistics Meet the Noun Com-
pound: Some Empirical Results. Proc. of the 33rd An-
nual Meeting of ACL, pages 47?55.
M. Liberman and R. Sproat. 1992. The stress and struc-
ture of modified noun phrases in English. Lexical Mat-
ters, CSLI Lecture Notes, 24:131?181.
M.P. Marcus. 1980. Theory of Syntactic Recognition
for Natural Languages. MIT Press Cambridge, MA,
USA.
B.T. McInnes. 2004. Extending the log-likelihood ratio
to improve collocation identification. Master?s thesis,
University of Minnesota.
R. Moore. 2004. On log-likelihood-ratios and the sig-
nificance of rare events. In Dekang Lin and Dekai
Wu, editors, Proc. of EMNLP 2004, pages 333?340,
Barcelona, Spain, July. Association for Computational
Linguistics.
P. Nakov and M. Hearst. 2005. Search engine statistics
beyond the n-gram: Application to noun compound
bracketing. In Proceedings of the Ninth Conference on
Computational Natural Language Learning (CoNLL-
2005), pages 17?24, Ann Arbor, Michigan, June. As-
sociation for Computational Linguistics.
T. Pedersen, M. Kayaalp, and R. Bruce. 1996. Signifi-
cant lexical relationships. In Howard Shrobe and Ted
Senator, editors, Proc. of the Thirteenth National Con-
ference on Artificial Intelligence and the Eighth Inno-
vative Applications of Artificial Intelligence Confer-
ence, Vol. 2, pages 455?460, Menlo Park, California.
AAAI Press.
T. Pedersen, R. Bruce, and J. Wiebe. 1997. Sequen-
tial model selection for word sense disambiguation. In
Proc. of the Fifth Conference on Applied Natural Lan-
guage Processing, pages 388?395, Washington, DC,
April.
J. Pustejovsky, P. Anick, and S. Bergler. 1993. Lexi-
cal semantic techniques for corpus analysis. Compu-
tational Linguistics, 19(2):331?358.
A. Ratnaparkhi. 1998. Maximum Entropy Models for
Natural Lnaguage Ambiguity Resolution. Ph.D. thesis,
University of Pennsylvania.
P. Resnik and M. Hearst. 1993. Structural Ambiguity
and Conceptual Relations. Proc. of the Workshop on
Very Large Corpora: Academic and Industrial Per-
spectives, June, 22(1993):58?64.
P.S. Resnik. 1993. Selection and Information: A Class-
Based Approach to Lexical Relationships. Ph.D. the-
sis, University of Pennsylvania.
A.P. Ruggieri, S. Pakhomov, and C.G. Chute. 2004. A
Corpus Driven Approach Applying the ?Frame Se-
mantic? Method for Modeling Functional Status Ter-
minology. Proc. of MedInfo, 11(Pt 1):434?438.
M.Q. Stearns, C. Price, KA Spackman, and AY Wang.
2001. SNOMED clinical terms: overview of the de-
velopment process and project status. Proc AMIA
Symp, pages 662?6.
S. S. Wilks. 1938. The large-sample distribution of the
likelihood ratio for testing composite hypotheses. The
Annals of Mathematical Statistics, 9(1):60?62, March.
16
The Duluth Word Alignment System
Bridget Thomson McInnes
Department of Computer Science
University of Minnesota
Duluth, MN 55812
bthomson@d.umn.edu
Ted Pedersen
Department of Computer Science
University of Minnesota
Duluth, MN 55812
tpederse@umn.edu
Abstract
The Duluth Word Alignment System partici-
pated in the 2003 HLT-NAACL Workshop on
Parallel Text shared task on word alignment for
both English?French and Romanian?English.
It is a Perl implementation of IBM Model 2.
We used approximately 50,000 aligned sen-
tences as training data for each language pair,
and found the results for Romanian?English to
be somewhat better. We also varied the Model
2 distortion parameters among the values 2, 4,
and 6, but did not observe any significant dif-
ferences in performance as a result.
1 Introduction
Word alignment is a crucial part of any Machine Transla-
tion system, since it is the process of determining which
words in a given source and target language sentence
pair are translations of each other. This is a token level
task, meaning that each word (token) in the source text
is aligned with its corresponding translation in the target
text.
The Duluth Word Alignment System is a Perl imple-
mentation of IBM Model 2 (Brown et al, 1993). It learns
a probabilistic model from sentence aligned parallel text
that can then be used to align the words in another such
text (that was not a part of the training process).
A parallel text consists of a source language text and
its translation into some target language. If we have de-
termined which sentences are translations of each other
then the text is said to be sentence aligned, where we call
a source and target language sentence that are translations
of each other a sentence pair.
(Brown et al, 1993) introduced five statistical transla-
tion models (IBM Models 1 ? 5). In general a statistical
machine translation system is composed of three com-
ponents: a language model, a translation model, and a
decoder (Brown et al, 1988).
The language model tells how probable a given sen-
tence is in the source language, the translation model in-
dicates how likely it is that a particular target sentence is
a translation of a given source sentence, and the decoder
is what actually takes a source sentence as input and pro-
duces its translation as output. Our focus is on translation
models, since that is where word alignment is carried out.
The IBM Models start very simply and grow steadily
more complex. IBM Model 1 is based solely on the prob-
ability that a given word in the source language translates
as a particular word in the target language. Thus, a word
in the first position of the source sentence is just as likely
to translate to a word in the target sentence that is in the
first position versus one at the last position. IBM Model
2 augments these translation probabilities by taking into
account how likely it is for words at particular positions
in a sentence pair to be alignments of each other.
This paper continues with a more detailed description
of IBM Model 2. It goes on to present the implementa-
tion details of the Duluth Word Alignment System. Then
we describe the data and the parameters that were used
during the training and testing stages of the shared task
on word alignment. Finally, we discuss our experimental
results and briefly outline our future plans.
2 IBM Model 2
Model 2 is trained with sentence aligned parallel corpora.
However, our goal is learn a model that can perform word
alignment, and there are no examples of word alignments
given in the training data. Thus, we must cast the train-
ing process as a missing data problem, where we learn
about word alignments from corpora where only sentence
(but not word) alignments are available. As is common
with missing data problems, we use the Expectation?
Maximization (EM) Algorithm (Dempster et al, 1977)
to estimate the probabilities of word alignments in this
model.
The objective of Model 2 is to estimate the probability
that a given sentence pair is aligned a certain way. This
is represented by   	
 , where  is the source sen-
tence,

 is the target sentence, and  is the proposed word
alignment for the sentence pair. However, since this prob-
ability can?t be estimated directly from the training data,
we must reformulate it so we can use the EM algorithm.
From Bayes Rule we arrive at:
    


  
   


  
   
 (1)
where   
    is the probability of a proposed align-
ment of the words in the target sentence to the words in
the given source sentence. To estimate a probability for
a particular alignment, we must estimate the numerator
and then divide it by the sum of the probabilities of all
possible alignments given the source sentence.
While clear in principle, there are usually a huge num-
ber of possible word alignments between a source and
target sentence, so we can?t simply estimate this for ev-
ery possible alignment. Model 2 incorporates a distortion
factor to limit the number of possible alignments that are
considered. This factor defines the number of positions
a source word may move when it is translated into the
target sentence. For example, given a distortion factor of
two, a source word could align with a word up to two
positions to the left or right of the corresponding target
word?s position.
Model 2 is based on the probability of a source and tar-
get word being translations of each other, and the proba-
bility that words at particular source and target positions
are translations of each other (without regard to what
those words are). Thus, the numerator in Equation 1 is
estimated as follows:
  
   


	


  


 
	
  

      (2)
The translation probability, 
  
       , is the likelihood
that 
  , the target word at position  , is the translation
of a given source word  that occurs at position   . The
alignment probability,         , is the likelihood that
position   in the source sentence can align to a given
position  in the target sentence, where  and  are the
given lengths of the source and target sentences.
The denominator in Equation 1 is the sum of all the
probabilities of all the possible alignments of a sentence
pair. This can be estimated by taking the product of the
sums of the translational and positional alignment proba-
bilities.


  
   


	


 


  


 

Proceedings of the NAACL HLT 2013 Demonstration Session, pages 28?31,
Atlanta, Georgia, 10-12 June 2013. c?2013 Association for Computational Linguistics
UMLS::Similarity: Measuring the Relatedness
and Similarity of Biomedical Concepts
Bridget T. McInnes? & Ying Liu
Minnesota Supercomputing Institute
University of Minnesota
Minneapolis, MN 55455
Ted Pedersen
Department of Computer Science
University of Minnesota
Duluth, MN 55812
Genevieve B. Melton
Institute for Health Informatics
University of Minnesota
Minneapolis, MN 55455
Serguei V. Pakhomov
College of Pharmacy
University of Minnesota
Minneapolis, MN 55455
Abstract
UMLS::Similarity is freely available open
source software that allows a user to mea-
sure the semantic similarity or relatedness of
biomedical terms found in the Unified Medi-
cal Language System (UMLS). It is written in
Perl and can be used via a command line in-
terface, an API, or a Web interface.
1 Introduction
UMLS::Similarity1 implements a number of seman-
tic similarity and relatedness measures that are based
on the structure and content of the Unified Medical
Language System. The UMLS is a data warehouse
that provides a unified view of many medical termi-
nologies, ontologies and other lexical resources, and
is also freely available from the National Library of
Medicine.2
Measures of semantic similarity quantify the de-
gree to which two terms are similar based on their
proximity in an is-a hierarchy. These measures are
often based on the distance between the two con-
cepts and their common ancestor. For example, lung
disease and Goodpasture?s Syndrome share the con-
cept disease as a common ancestor. Or in general
English, scalpel and switchblade would be consid-
ered very similar since both are nearby descendents
of the concept knife.
However, concepts that are not technically similar
can still be very closely related. For example, Good-
pasture?s Syndrome and Doxycycline are not similar
?Contact author : bthomson@umn.edu.
1http://umls-similarity.sourceforge.net
2http://www.nlm.nih.gov/research/umls/
since they do not have a nearby common ancestor,
but they are very closely related since Doxycycline
is a possible treatment for Goodpasture?s Syndrome.
A more general example might be elbow and arm,
while they are not similar, an elbow is a part-of an
arm and is therefore very closely related. Measures
of relatedness quantify these types of relationships
by using information beyond that which is found
in an is-a hierarchy, which the UMLS contains in
abundance.
2 Related Work
Measures of semantic similarity and relatedness
have been used in a number of different biomedi-
cal and clinical applications. Early work relied on
the Gene Ontology (GO)3, which is a hierarchy of
terms used to describe genomic information. For
example, (Lord et al, 2003) measured the similar-
ity of gene sequence data and used this in an appli-
cation for conducting semantic searches of textual
resources. (Guo et al, 2006) used semantic simi-
larity measures to identify direct and indirect pro-
tein interactions within human regulatory pathways.
(Ne?ve?ol et al, 2006) used semantic similarity mea-
sures based on MeSH (Medical Subject Headings)4
to evaluate automatic indexing of biomedical arti-
cles by measuring the similarity between their rec-
ommended terms and the gold standard index terms.
UMLS::Similarity was first released in 2009, and
since that time has been used in various different
applications. (Sahay and Ram, 2010) used it in a
3http://www.geneontology.org/
4http://www.ncbi.nlm.nih.gov/mesh
28
health information search and recommendation sys-
tem. (Zhang et al, 2011) used the measures to
identify redundancy within clinical records, while
(Mathur and Dinakarpandian, 2011) used them to
help identify similar diseases. UMLS::Similarity
has also enabled the development and evaluation
of new measures by allowing them to be compared
to existing methods, e.g., (Pivovarov and Elhadad,
2012). Finally, UMLS::Similarity can serve as a
building block in other NLP systems, for exam-
ple UMLS::SenseRelate (McInnes et al, 2011) is a
word sense disambiguation system for medical text
based on semantic similarity and relatedness.
3 UMLS::Similarity
UMLS::Similarity is a descendent of Word-
Net::Similarity (Pedersen et al, 2004), which
implements various measures of similarity and
relatedness for WordNet.5 However, the structure,
nature, and size of the UMLS is quite different from
WordNet, and the adaptations from WordNet were
not always straightforward. One very significant
difference, for example, is that the UMLS is stored
in a MySQL database while WordNet has its own
customized storage format. As a result, the core
of UMLS::Similarity is different and offers a
great deal of functionality specific to the UMLS.
Table 1 lists the measures currently provided in
UMLS::Similarity (as of version 1.27).
The Web interface provides a subset of the func-
tionality offered by the API and command line inter-
face, and allows a user to utilize UMLS::Similarity
without requiring the installation of the UMLS
(which is an admittedly time?consuming process).
4 Unified Medical Language System
The UMLS is a data warehouse that includes over
100 different biomedical and clinical data resources.
One of the largest individual sources is the System-
atized Nomenclature of Medicine?Clinical Terms
(SNOMED CT), a comprehensive terminology cre-
ated for the electronic exchange of clinical health in-
formation. Perhaps the most fine?grained source is
the Foundational Model of Anatomy (FMA), an on-
tology created for biomedical and clinical research.
One of the most popular sources is MeSH (MSH), a
5http://wordnet.princeton.edu/
Table 1: UMLS::Similarity Measures
Type Citation Name
Similarity
(Rada et al, 1989) path
(Caviedes and Cimino, 2004) cdist
(Wu and Palmer, 1994) wup
(Leacock and Chodorow, 1998) lch
(Nguyen and Al-Mubaid, 2006) nam
(Zhong et al, 2002) zhong
(Resnik, 1995) res
(Lin, 1998) lin
(Jiang and Conrath, 1997) jcn
Relatedness
(Banerjee and Pedersen, 2003) lesk
(Patwardhan and Pedersen, 2006) vector
terminology that is used for indexing medical jour-
nal articles in PubMed.
These many different resources are semi-
automatically combined into the Metathesaurus,
which provides a unified view of nearly 3,000,000
different concepts. This is very important since the
same concept can exist in multiple different sources.
For example, the concept Autonomic nerve exists in
both SNOMED CT and FMA. The Metathesaurus
assigns synonymous concepts from multiple sources
a single Concept Unique Identifier (CUI). Thus
both Autonomic nerve concepts in SNOMED CT
and FMA are assigned the same CUI (C0206250).
These shared CUIs essentially merge multiple
sources into a single resource in the Metathesaurus.
Some sources in the Metathesaurus contain addi-
tional information about the concept such as syn-
onyms, definitions,6 and related concepts. Paren-
t/child (PAR/CHD) and broader/narrower (RB/RN)
are the main types of hierarchical relations between
concepts in the Metathesaurus. Parent/child rela-
tions are already defined in the sources before they
are integrated into the UMLS, whereas broader/-
narrower relations are added by the UMLS edi-
tors. For example, Splanchnic nerve has an is-a
relation with Autonomic nerve in FMA. This re-
lation is carried forward in the Metathesaurus by
creating a parent/child relation between the CUIs
C0037991 [Splanchnic nerve] and C0206250 [Au-
tonomic nerve].
6However, not all concepts in the UMLS have a definition.
29
Table 2: Similarity scores for finger and arm
Source Relations CUIs path cdist wup lch nam zhong res lin jcn
FMA PAR/CHD 82,071 0.14 0.14 0.69 1.84 0.15 0.06 0.82 0.34 0.35
SNOMED CT PAR/CHD 321,357 0.20 0.20 0.73 2.45 0.15 0.16 2.16 0.62 0.48
MSH PAR/CHD 26,685 0.25 0.25 0.76 2.30 0.18 0.19 2.03 0.68 0.55
5 Demonstration System
The UMLS::Similarity Web interface7 allows a user
to enter two terms or UMLS CUIs as input in term
boxes. The user can choose to calculate similarity or
relatedness by clicking on the Calculate Similarity
or Calculate Relatedness button. The user can also
choose which UMLS sources and relations should
be used in the calculation. For example, if the terms
finger and arm are entered and the Compute Simi-
larity button is pressed, the following is output:
View D e f i n i t i o n s
View S h o r t e s t Pa th
R e s u l t s :
The s i m i l a r i t y o f f i n g e r
( C0016129 ) and arm ( C0446516 )
u s i n g Pa th Length ( p a t h ) i s
0 . 2 5 .
Using :
SAB : : i n c l u d e MSH
REL : : i n c l u d e PAR/CHD
The Results show the terms and their assigned
CUIs. If a term has multiple possible CUIs associ-
ated with it, UMLS::Similarity returns the CUI pair
that obtained the highest similarity score. In this
case, finger was assigned CUI C0016129 and arm
assigned CUI C0449516 and the resulting similarity
score for the path measure using the MeSH hierar-
chy was 0.25.
Additionally, the paths between the concepts and
their definitions are shown. The View Definitions
and View Shortest Path buttons show the definition
and shortest path between the concepts in a sepa-
rate window. In the example above, the shortest path
between finger (C0016129) and arm (C0446516) is
C0016129 (Finger, NOS) => C0018563 (Hand,
NOS) => C1140618 (Extremity, Upper) =>
7http://atlas.ahc.umn.edu/
C0446516 (Upper arm), and one of the definitions
shown for arm (C0446516) is The superior part
of the upper extremity between the shoulder and
the elbow.
SAB :: include and REL :: include are config-
uration parameters that define the sources and rela-
tions used to find the paths between the two CUIs
when measuring similarity. In the example above,
similarity was calculated using PAR/CHD relations
in the MeSH hierarchy.
All similarity measures default to the use of
MeSH as the source (SAB) with PAR/CHD rela-
tions. While these are reasonable defaults, for many
use cases these should be changed. Table 2 shows
the similarity scores returned for each measure us-
ing different sources. It also shows the number of
CUIs connected via PAR/CHD relations per source.
A similar view is displayed when pressing the
Compute Relatedness button:
View D e f i n i t i o n s
View S h o r t e s t Pa th
R e s u l t s :
The r e l a t e d n e s s o f f i n g e r
( C0016129 ) and arm ( C0446516 )
u s i n g Vec to r Measure ( v e c t o r )
i s 0 . 5 5 1 3 .
Using :
SABDEF : : i n c l u d e
UMLS ALL
RELDEF : : i n c l u d e
CUI /PAR/CHD/RB/RN
Relatedness measures differ from similarity in
their use of the SABDEF and RELDEF parameters.
SABDEF :: include andRELDEF :: include define
the source(s) and relation(s) used to extract defini-
tions for the relatedness measures. In this example,
the definitions come from any source in the UMLS
and include not only the definition of the concept but
30
Table 3: Relatedness scores for finger and arm
Source Relations lesk vector
UMLS ALLCUI/PAR/CHD/RB/RN10,607 0.55
UMLS ALLCUI 39 0.05
also the definition of its PAR/CHD and RB/RN rela-
tions. Table 3 shows the relatedness scores returned
for each of the relatedness measures using just the
concept?s definition (CUI) from all of the sources in
the UMLS (UMLS ALL) and when the definitions
are extended to include the definitions of the con-
cept?s PAR/CHD and RB/RN relations.
6 Acknowledgments
This work was supported by the National Insti-
tute of Health, National Library of Medicine Grant
#R01LM009623-01. It was carried out in part using
computing resources at the University of Minnesota
Supercomputing Institute.
The results reported here are based on the
2012AA version of the UMLS and were computed
using version 1.23 of UMLS::Similarity and version
1.27 of UMLS::Interface.
References
S. Banerjee and T. Pedersen. 2003. Extended gloss over-
laps as a measure of semantic relatedness. In Proceed-
ings of the Eighteenth International Joint Conference
on Artificial Intelligence, pages 805?810, Acapulco,
August.
J.E. Caviedes and J.J. Cimino. 2004. Towards the devel-
opment of a conceptual distance metric for the umls.
Journal of Biomedical Informatics, 37(2):77?85.
X. Guo, R. Liu, C.D. Shriver, H. Hu, and M.N. Lieb-
man. 2006. Assessing semantic similarity measures
for the characterization of human regulatory pathways.
Bioinformatics, 22(8):967?973.
J. Jiang and D. Conrath. 1997. Semantic similarity based
on corpus statistics and lexical taxonomy. In Proceed-
ings on Intl Conf on Research in CL, pages pp. 19?33.
C. Leacock and M. Chodorow. 1998. Combining local
context and WordNet similarity for word sense iden-
tification. WordNet: An electronic lexical database,
49(2):265?283.
D. Lin. 1998. An information-theoretic definition of
similarity. In Intl Conf ML Proc., pages 296?304.
PW Lord, RD Stevens, A. Brass, and CA Goble. 2003.
Semantic similarity measures as tools for exploring the
gene ontology. In Pacific Symposium on Biocomput-
ing, volume 8, pages 601?612.
S. Mathur and D. Dinakarpandian. 2011. Finding dis-
ease similarity based on implicit semantic similarity.
Journal of Biomedical Informatics, 45(2):363?371.
B.T. McInnes, T. Pedersen, Y. Liu, S. Pakhomov, and
G. Melton. 2011. Knowledge-based method for deter-
mining the meaning of ambiguous biomedical terms
using information content measures of similarity. In
Proceedings of the Annual Symposium of the Ameri-
canMedical Informatics Association, pages 895 ? 904,
Washington, DC.
A. Ne?ve?ol, K. Zeng, and O. Bodenreider. 2006. Besides
Precision & Recall: ExploringAlternative Approaches
to Evaluating an Automatic Indexing Tool for MED-
LINE. In AMIA Annu Symp Proc., page 589.
H.A. Nguyen and H. Al-Mubaid. 2006. New ontology-
based semantic similarity measure for the biomedical
domain. In Proc of the IEEE Intl Conf on Granular
Computing, pages 623?628.
S. Patwardhan and T. Pedersen. 2006. Using WordNet-
based Context Vectors to Estimate the Semantic Relat-
edness of Concepts. In Proc of the EACL 2006 Work-
shop Making Sense of Sense, pages 1?8.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
WordNet::Similarity - Measuring the Relatedness of
Concepts. In The Annual Meeting of the HLT and
NAACL: Demonstration Papers, pages 38?41.
R. Pivovarov and N. Elhadad. 2012. A hybrid
knowledge-based and data-driven approach to iden-
tifying semantically similar concepts. Journal of
Biomedical Informatics, 45(3):471?481.
R. Rada, H. Mili, E. Bicknell, and M. Blettner. 1989.
Development and application of a metric on semantic
nets. IEEE Transactions on Systems, Man, and Cyber-
netics, 19(1):17?30.
P. Resnik. 1995. Using information content to evaluate
semantic similarity in a taxonomy. In Proceedings of
the 14th Intl Joint Conf on AI, pages 448?453.
S. Sahay and A. Ram. 2010. Socio-semantic health in-
formation access. In Proceedings of the AAAI Spring
Symposium on AI and Health Communication.
Z. Wu and M. Palmer. 1994. Verbs semantics and lexical
selection. In Proceedings of the 32nd Meeting of ACL,
pages 133?138, Las Cruces, NM, June.
R. Zhang, S. Pakhomov, B.T. McInnes, and G.B. Melton.
2011. Evaluating measures of redundancy in clinical
texts. In AMIA Annual Symposium Proceedings, vol-
ume 2011, page 1612.
J. Zhong, H. Zhu, J. Li, and Y. Yu. 2002. Concep-
tual graph matching for semantic search. Proceedings
of the 10th International Conference on Conceptual
Structures, pages 92?106.
31
Proceedings of the NAACL HLT 2010 Second Louhi Workshop on Text and Data Mining of Health Documents, pages 46?52,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Automated Identification of Synonyms in Biomedical  
Acronym Sense Inventories  
?
Genevieve B. Melton SungRim Moon 
Institute for Health Informatics & Dept of Surgery Institute for Health Informatics 
University of Minnesota University of Minnesota 
Minneapolis, MN 55455 USA Minneapolis, MN 55455 USA 
gmelton@umn.edu moonx086@umn.edu 
  
Bridget McInnes  Serguei Pakhomov 
College of Pharmacy College of Pharmacy 
University of Minnesota University of Minnesota 
Minneapolis, MN 55455 USA Minneapolis, MN 55455 USA 
bthomson@umn.edu  pakh0002@umn.edu 
?
  
Abstract 
Acronyms are increasingly prevalent in bio-
medical text, and the task of acronym disam-
biguation is fundamentally important for 
biomedical natural language processing sys-
tems. Several groups have generated sense in-
ventories of acronym long form expansions 
from the biomedical literature. Long form 
sense inventories, however, may contain con-
ceptually redundant expansions that negative-
ly affect their quality. Our approach to 
improving sense inventories consists of map-
ping long form expansions to concepts in the 
Unified Medical Language System (UMLS) 
with subsequent application of a semantic si-
milarity algorithm based upon conceptual 
overlap. We evaluated this approach on a ref-
erence standard developed for ten acronyms. 
A total of 119 of 155 (78%) long forms 
mapped to concepts in the UMLS. Our ap-
proach identified synonymous long forms 
with a sensitivity of 70.2% and a positive pre-
dictive value of 96.3%. Although further re-
finements are needed, this study demonstrates 
the potential value of using automated tech-
niques to merge synonymous biomedical 
acronym long forms to improve the quality of 
biomedical acronym sense inventories. 
1 Introduction 
Acronyms and abbreviations are increasingly used 
in biomedical text. This is in large part due to the 
expansive growth of the biomedical literature esti-
mated to be close to one million articles annually 
(Stead et al 2005). Ambiguous acronyms represent 
a challenge to both human readers and compute-
rized processing systems for resolving the 
acronym?s meaning within a particular context. For 
any given acronym, there are often multiple possi-
ble long form expansions. Techniques to determine 
the context-specific meaning or sense of an ambi-
guous acronym are fundamentally important for 
biomedical natural language processing and can 
assist with important tasks such as information re-
trieval and information extraction (Friedman 
2000). 
Acronym ambiguity resolution represents a spe-
cial case of word sense disambiguation (WSD) 
with unique challenges. In particular, there are in-
creasing numbers of new acronyms (i.e., short 
forms) as well as increasing numbers of new 
senses (i.e., long forms) for existing acronyms 
within biomedical text. Acronyms in biomedicine 
also range from those that are common, to those 
that are infrequent which appear to be created in an 
ad hoc fashion resulting essentially in neologisms 
distinct to small sets of biomedical discourse.  
Sense inventories are important tools that can 
assist in the task of disambiguation of acronyms 
and abbreviations. The relative formal nature of 
biomedical literature discourse lends itself well to 
building these inventories because long forms are 
typically contained within the text itself, providing 
a ?definition? on its first mention in an article, next 
to a parenthetical expression containing the short 
form or vice versa (Schwartz and Hearst 2003). In 
contrast, clinical documents are less structured and 
46
typically lack expanded long forms for acronyms 
and abbreviations, leaving sense inventories based 
on documents in the clinical domain not as well 
developed as the sense inventories developed from 
the biomedical literature (Pakhomov et al 2005).  
Compilation of sense inventories for acronyms 
in clinical documents typically relies on vocabula-
ries contained in the Unified Medical Language 
System (UMLS) as well as other resources such as 
ADAM (Zhou et al 2006). However, with the ad-
vantage of using rich and diverse resources like 
ADAM and the UMLS comes the challenge of 
having to identify and merge synonymous long 
form expansions which can occur for a given short 
form. Having synonymous long forms in a sense 
inventory for a given acronym poses a problem for 
automated acronym disambiguation because the 
sense inventory dictates that the disambiguation 
algorithm must be able to distinguish between se-
mantically equivalent senses. This is an important 
problem to address because effective identification 
of synonymous long forms allows for a clean sense 
inventory, and it creates the ability for long form 
expansions to be combined while preserving the 
variety of expression occurring in natural lan-
guage. By automating the merging of synonymous 
expansions and building a high quality sense in-
ventory, the task of acronym disambiguation will 
be improved resulting in better biomedical NLP 
system performance.  
Our approach to reducing multiple synonymous 
variants of the same long form for a set of ten bio-
medical acronyms is based on mapping sense in-
ventories for biomedical acronyms to the UMLS 
and using a semantic similarity algorithm based on 
conceptual overlap. This study is an exploratory 
evaluation of this approach on a manually created 
reference standard.  
2 Background  
2.1 Similarity measures in biomedicine 
The area of semantic similarity in biomedicine 
is a major area within biomedical NLP and know-
ledge representation research. Semantic similarity 
aids NLP systems, improves the performance of 
information retrieval tasks, and helps to reveal im-
portant latent relationships between biomedical 
concepts. Several investigators have studied con-
ceptual similarity and have used relationships in 
controlled biomedical terminologies, empiric sta-
tistical data from biomedical text, and other know-
ledge sources (Lee et al 2008; Caviedes and 
Cimino 2004). However, most of these techniques 
focus on generating measures between a single pair 
of concepts and do not deal directly with the task 
of comparing two groups of concepts.  
Patient similarity represents an important ana-
logous problem that deals with sets of concepts. 
The approach used by Melton et al (2006) was to 
represent each patient case as a set of nodes within 
a controlled biomedical terminology (SNOMED 
CT). The investigators then applied several meas-
ures to ascertain similarity between patient cases. 
These measures ranged from techniques indepen-
dent of the controlled terminology (i.e. set overlap 
or Hamming distance) to methods heavily reliant 
upon the controlled terminology based upon path 
traversal between pair of nodes using defined rela-
tionships (either IS-A relationships or other seman-
tic relationships) within the terminology.  
2.2 Lesk algorithm for measuring similarity 
using sets of definitional words 
A variety of techniques have been used for the 
general problem of WSD that range from highly 
labor intensive that depend upon human data tag-
ging (i.e. supervised learning) to unsupervised ap-
proaches that are completely automated and rely 
upon non-human sources of information, such as 
context and other semantic features of the sur-
rounding text or definitional data.  
The Lesk algorithm (Lesk 1986) is one example 
of an unsupervised method that uses dictionary 
information to perform WSD. This algorithm uses 
the observation that words co-occurring in a sen-
tence refer to the same topic and that dictionary 
definition words will have topically related senses, 
as well. The classic form of this algorithm returns a 
measure of word overlap. Lesk depends upon find-
ing common words between dictionary definitions. 
One shortcoming of Lesk, however, it that it can 
perform worse for words with terse, few word de-
finitions.  
As a modification of Lesk, researchers have 
proposed using WordNet (Felbaum 1998) to en-
hance its performance. WordNet has additional 
semantic information that can aid in the task of 
disambiguation, such as relationships between the 
term of interest and other terms. Banerjee and Pe-
47
dersen (2002) demonstrated that modifications to 
Lesk improved performance significantly with the 
addition of semantic relationship information. 
2.3 Biomedical literature sense inventories 
A number of acronym and abbreviation sense in-
ventories have been developed from the biomedi-
cal literature using a variety of approaches. Chang 
et al (2002) developed the Stanford biomedical 
abbreviation server1 using titles and abstracts from 
MEDLINE, lexical heuristic rules, and supervised 
logistic regression to align text and extract short 
form/long form pairs that matched well with 
acronym short form letters. Similarly, Adar (2004) 
developed the Simple and Robust Abbreviation 
Dictionary (SaRAD)2. This inventory, in addition 
to providing the abbreviation and definition, also 
clusters long forms using an N-gram approach 
along with classification rules to disambiguate de-
finitions. This resource, while analogous with re-
spect to its goal of merging and aligning long form 
expansions, is not freely available. Adar measured 
a normalized similarity between N-gram sets and 
then clustered long forms to create a clustered 
sense inventory resource. 
One of the most comprehensive biomedical 
acronym and abbreviation databases is ADAM 
(Zhou et al 2006) an open source database3 that 
we used for this study. Once identified, short 
form/long form pairs were filtered statistically with 
a rule of length ratio and an empirically-based cut-
off value.  This sense inventory is based on  
MEDLINE titles and abstracts from 2006 and con-
sists of over 59 thousand abbreviation/long form 
pairs. The authors report high precision with 
ADAM (97%) and up to 33% novel abbreviations 
not contained within the UMLS or Stanford Ab-
breviation dictionary.  
2.4 MetaMap resource for automated map-
ping to the UMLS 
An important resource for mapping words and 
phrases to the UMLS Metathesaurus is MetaMap. 
This resource was developed at the National Li-
brary of Medicine (Aronson 2001) to map text of 
biomedical abstracts to the UMLS. MetaMap uses 
                                                          
1 http://abbreviation.stanford.edu 
2 http://www.hpl.hp.com/shl/projects/abbrev.html 
3 http://arrowsmith.psych.uic.edu 
a knowledge intensive approach that relies upon 
computational linguistic, statistical, and symbol-
ic/lexical techniques. While MetaMap was initially 
developed to help with indexing of biomedical lite-
rature, it has been applied and expanded success-
fully to a number of diverse applications including 
clinical text.  
With each mapping, an evaluation function 
based upon centrality, variation, coverage, and co-
hesiveness generates a score for a given mapping 
from 0 to 1000 (strongest match). A cut-off score 
of 900 or greater is considered to represent a good 
conceptual match for MetaMap and was used in 
this study as the threshold to select valid mappings. 
3  Methods  
Ten randomly selected acronyms with between 10 
to 20 long forms were selected from the ADAM 
resource database for this pilot study.  
3.1 Long form mappings to UMLS 
Each acronym long-form was mapped to the 
UMLS with MetaMap using two settings. First, 
MetaMap was run with its default setting on each 
long form expansion. Second, MetaMap was run in 
its ?browse mode? (options ?-zogm?) which allows 
for term processing, overmatches, concept gaps, 
and ignores word order. 
Processing each long form with MetaMap then 
resulted in a set of Concept Unique Identifiers 
(CUIs) representing the long form. Each CUI with 
a score over 900 was included in the overall set of 
CUIs for a particular long form expansion. For a 
given pair of long form expansions the two sets of 
CUIs that each long form mapped to were com-
pared for concept overlap, in an analogous fashion 
to the Lesk algorithm. The overlap between con-
cept sets was calculated between each pair of long 
form expansions and expressed as a ratio: 
 
 ????????????? ????????? ???????????????? ?????????? ????????????? .   
 
For this study, an overlap of 50% or greater was 
considered to indicate a potential synonymous pair. 
Now let us assume that we have two concept 
sets: The first one is {A, B} and the second one is 
{A, B, C}, with each CUI having a score over 900. 
In this example, the overlap of concepts for the 
first concept set between it and the other is 100%, 
and for the second that is 66.7%. Because overlaps 
48
are greater than 50%, they are a potential syn-
onymous pair, and the overlap ratio is calculated as 
?????
????? ?
?
? = 1 (100%). 
3.2 Expert-derived reference standard 
Two physicians were asked to judge the similarity 
between each pair combination of long forms ex-
pansions on a continuous scale for our initial refer-
ence standard. Physicians were instructed to rate 
pairs of long forms for conceptual similarity. Long 
forms were presented on a large LCD touch-screen 
display (Hewlett-Packard TouchSmart 22? desk-
top) along with a continuous scale for the physi-
cians to rate long form pairs as dissimilar (far left 
screen) or highly similar (far right screen). The 
rating was measured on a scale from 1 to 1500 pix-
els representing the maximum width of the touch 
sensitive area of the display (along the x-
coordinate). Inter-rater agreement was assessed 
using Pearson correlation.  
Expert scores were then averaged and plotted 
on a histogram to visualize expert ratings. We sub-
sequently used a univariate clustering approach 
based on the R implementation of the Partitioning 
Around Medoids (PAM) method to estimate a cut-
off point between similar and dissimilar terms 
based on the vector of the average responses by the 
two physicians. The responses were clustered into 
two and three clusters based on an informal obser-
vation of the distribution of responses on the histo-
gram showing evidence of at least a bimodal and 
possibly a trimodal distribution.  
As a quality measure, a third physician manual-
ly reviewed the mean similarity ratings of the first 
two physicians to assess whether their similarity 
judgments represented the degree of synonymy 
between long form expansions necessary to war-
rant merging the long form expansions. This re-
view was done using a binary scale (0=not 
synonymous, 1=synonymous). 
3.3 Evaluation of automated methods  
Long form pair determinations based on the map-
pings to the UMLS were compared to our refer-
ence standard as described in Section 3.2. We 
calculated overall results of all long form pair 
comparisons and on all long form pairs that 
mapped to the UMLS with MetaMap. Performance 
is reported as sensitivity, specificity, and positive 
predictive value.  
4 Results 
A total of 10 random acronyms were used in this 
study. All long forms for these 10 acronyms were 
from the sense inventory ADAM (Zhou et al, 
2006). This resulted in a total of 155 long form 
expansions (median 16.5 per acronym, range 11-
19) (Table 1).  
Acronym N of LF  
expansions 
LF expansions 
mapped by MetaMap 
Total 155 119 (78%) 
ALT 13 9 (70%) 
CK 14 9 (64%) 
CSF 11 7 (74%) 
CTA 19 14 (74%) 
MN 19 17 (89%) 
NG 17 15 (88%) 
PCR 17 8 (47%) 
PET 17 15 (88%) 
RV 16 14 (88%) 
TTP 12 11(92%) 
Table 1. Number of acronym long forms in 
ADAM and mapping to the UMLS 
4.1 Long form mappings to UMLS 
The default mode of MetaMap resulted in 119 
(78%) long forms with mappings to the UMLS 
with MetaMap (Table 1). Use of MetaMap?s 
browse mode did not increase the total number of 
mapped long forms but did change some of the 
mapped concepts returned by MetaMap (not de-
picted).  
 
Acronym N pairs Pearson r 
Total 1125 0.78* 
ALT 78 0.79* 
CK 91 0.77* 
CSF 55 0.80* 
CTA 136 0.92* 
MN 171 0.69* 
NG 136 0.68* 
PCR 136 0.89* 
PET 136 0.78* 
RV 120 0.67* 
TTP 66 0.76* 
Table 2. Pearson correlation coefficient for ratings over-
all and for individual acronyms. *p<0.0001 
 
49
 
Figure 1. Two-way and three-way clustering solution of 
expert ratings of long form pairs. 
4.2 Expert-derived reference standard  
For the 1125 total comparison pairs, two raters as-
sessed similarity between long form pairs on a con-
tinuous scale. The overall mean correlation 
between the two raters was 0.78 (standard devia-
tion 0.08). Pearson correlation coefficients for each 
acronym are depicted in Table 2. 
 
Two-way and three-way clustering demonstrat-
ed an empirically determined ?cutoff? of 525 pix-
els from the left of the screen. This separation 
point between clusters (designated as ?low cutoff?) 
was evident on both the two-way and three-way 
clustering approaches using the PAM method to 
estimate a cut-off point between similar and dissi-
milar terms based on the vector of the average res-
ponses by the two physicians (Figure 1). Intuitively 
this low cutoff includes manual ratings indicative 
of moderate to low similarity (as 525 pixels along 
a 1500 pixel-wide scale is approximately one-third 
of the way from the left ?dissimilar? edge of the 
touch-sensitive screen). To isolate terms that were 
rated as highly similar, we also created an arbitrary 
?high cutoff? of 1200 pixels. 
 
 
Figure 2. Examples of terms originally rated as highly 
similar but not synonymous by the curating physician. 
 
Expert curation of the ratings by the third phy-
sician demonstrated that conceptual similarity rat-
ings were sometimes not equivalent to synonymy 
that would warrant the collapse of long form pairs. 
Of 1125 total pairs of long forms, 70 (6%) origi-
CTA: 
   ?CT hepatic arteriography?     ?CT angiography? 
MN:    
   ?median nerve?         ?motor neuron?  
RV:    
    ?rabies virus?           ?rotavirus? 
    ?right ventricular free wall?    ?right ventricle?                 
TTP:  
    ?thiamine triphosphate?          ?thymidine triphosphate? 
Default Mode: MetaMap Browse Mode: MetaMap 
All LF Mapped LF only All LF Mapped LF only 
High Cutoff 
Sensitivity  21.6% 39.6% 23.8% 43.8% 
Specificity  98.1% 96.8% 99.4% 99.0% 
PPV  48.7% 48.7% 77.8% 77.8% 
NPV  93.6% 95.5% 93.9% 95.9% 
Expert Curation 
Sensitivity  34.3% 64.9% 37.1% 70.2% 
Specificity  98.6% 97.7% 99.9% 99.8% 
PPV 61.5% 61.5% 96.3% 96.3% 
NPV  95.8% 98.0% 96.0% 98.3% 
 
Table 3. Performance of automated techniques for merging biomedical long form senses  
for all long forms and for long forms that mapped to the UMLS only.  
PPV, positive predictive value; NPV, negative predictive value. 
50
nally classified as similar were re-classified as 
conceptually different by the third physician. Sev-
eral examples of long form pairs that were origi-
nally rated as highly similar but were judged as not 
synonymous are contained in Figure 2. 
4.3 Evaluation of automated methods 
The performance of our algorithm is shown in Ta-
ble 3 using MetaMap in the default mode and 
browse mode and then applying our reference 
standard using the ?low cutoff?, ?high cutoff?, and 
expert curation (Table 3). Performance is reported 
for all 155 long forms (All LF) and for the subset 
of 119 long forms that mapped to the UMLS 
(Mapped LF only).  Compared to the ?low cutoff? 
reference standard, the ?high cutoff? and expert 
curation were positively associated with more con-
sistent performance. The browse mode identified 
fewer potential terms to merge and had higher ac-
curacy than the default MetaMap mode.   
5 Conclusions  
The results of this pilot study are promising and 
demonstrate high positive predictive value and 
moderate sensitivity for our algorithm, which indi-
cates to us that this technique with some additional 
modifications has value. We found that mapping 
long form expansions to a controlled terminology 
to not be straightforward. Although approximately 
80% of long forms mapped, another 20% were not 
converted to UMLS concepts. Because each long 
form resulted in multiple paired comparisons, a 
20% loss of mappings resulted globally in a 40% 
loss in overall system performance. While long 
form expansions were entered into MetaMap using 
a partially normalized representation of the long 
form, it is possible that additional normalization 
will improve our mapping. 
An important observation from our expert-
derived reference standard was that terms judged 
by physicians as semantically highly similar may 
not necessarily be synonymous (Figure 2). While 
semantic similarity is analogous, there may be 
some fundamentally different cognitive determina-
tions between similarity and synonymy for human 
raters.  
The current technique that we present compares 
sets of mapped concepts in an analogous fashion to 
the Lesk algorithm and other measures of similari-
ty between groups of concepts previously reported. 
This study did not utilize features of the controlled 
terminology nor statistical information about the 
text to help improve performance. Despite the lack 
of additional refinement to the presented tech-
niques, we found a flat overlap measure to be 
moderately effective in our evaluation. 
6 Future Work 
There are several lines of investigation that we will 
pursue as an extension of this study. The most ob-
vious would be to use semantic similarity measures 
between pairs of concepts that capitalize upon fea-
tures and relationships in the controlled terminolo-
gy. We can also expand upon the type of similarity 
measures for the overall long form comparison 
which requires a measure of similarity between 
groups of concepts. In addition, an empiric weight-
ing scheme based on statistical information of 
common senses may be helpful for concept map-
pings to place more or less emphasis on important 
or less important concepts. We plan to determine 
the impact of automatically reduced sense invento-
ries on the evaluation of WSD algorithms used for 
medical acronym disambiguation.   
Finally, we would like to utilize this work to 
help improve the contents of a sense inventory that 
we are currently developing for acronyms and ab-
breviations. This sense inventory is primarily 
based on clinical documents but incorporates in-
formation from a number of diverse sources in-
cluding ADAM, the UMLS, and a standard 
medical dictionary with abbreviations and acro-
nyms.  
Acknowledgments 
This work was supported by the University of 
Minnesota Institute for Health Informatics and De-
partment of Surgery and by the National Library of 
Medicine (#R01 LM009623-01). We would like to 
thank Fairview Health Services for ongoing sup-
port of this research. 
References  
Eytan Adar (2004) SaRAD: A simple and robust ab-
breviation dictionary. Bioinformatics 20:527?33. 
Alan R Aronson (2001) Effective mapping of biomedi-
cal text to the UMLS Metathesaurus: the MetaMap 
program. Proc AMIA Symp. 2001:17-21. 
51
Satanjeev Banerjee, Ted Pedersen. 2002. An Adapted 
Lesk Algorithm for Word Sense Disambiguation Us-
ing WordNet, Proceedings of the Third International 
Conference on Computational Linguistics and Intel-
ligent Text Processing, p.136-145, February 17-23. 
Jorge E. Caviedes JE, James J Cimino. (2004) Towards 
the development of a conceptual distance metric for 
the UMLS. J Biomed Inform. Apr;37(2):77?85. 
Jeffrey T Chang, Hinrich Schutze, Russ B Altman 
(2001) Creating an online dictionary of abbreviations 
from Medline. J Am Med Inform Assoc 9:612?20. 
Christiane Fellbaum, editor. 1998. WordNet: An elec-
tronic lexical database. MIT Press. 
Carol Friedman. 2000. A broad-coverage natural lan-
guage processing system. Proc AMIA Symp., 270?
274. 
Wei-Nchih Lee, Nigam Shah, Karanjot Sundlass, Mark 
Musen (2008) Comparison of Ontology-based Se-
mantic-Similarity Measures. AMIA Annu Symp 
Proc. 2008. 384?388. 
Michael E. Lesk. 1986. Automatic sense disambiguation 
using machine readable dictionaries: How to tell a 
pine cone from a ice cream cone. In Proceedings of 
SIGDOC ?86. 
Genevieve B. Melton, Simon Parsons, Frances P. Mor-
rison, Adam S. Rothschild, Marianthi Markatou, 
George Hripcsak. 2006. Inter-patient distance metrics 
using SNOMED CT defining relationships, Journal 
of Biomedical Informatics, 39(6), 697-705.  
Serguei Pakhomov, Ted Pedersen, Christopher G. 
Chute. 2005. Abbreviation and Acronym Disambigu-
ation in Clinical Discourse. American Medical In-
formatics Association Annual Symposium, 589-593. 
Ariel S Schwartz and Marti A. Hearst. 2003. A Simple 
Algorithm for Identifying Abbreviation Definitions 
in Biomedical Text. Pacific Symposium on Biocom-
puting p451-462. 
William W Stead, Brian J Kelly, Robert M Kolodner. 
2005. Achievable steps toward building a National 
Health Information infrastructure in the United 
States. J. Am. Med. Inform. Assoc., 12, 113?120. 
Wei Zhou, Vetle I Torvik, Neil R Smalheiser (2006) 
ADAM: Another database of abbreviations in Med-
line. Bioinformatics 22:2813? 8. 
52
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 145?153,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Using Second-order Vectors in a
Knowledge-based Method for Acronym Disambiguation
Bridget T. McInnes?
College of Pharmacy
University of Minnesota
Minneapolis, MN 55455
Ted Pedersen
Department of Computer Science
University of Minnesota
Duluth, MN 55812
Ying Liu
College of Pharmacy
University of Minnesota
Minneapolis, MN 55455
Serguei V. Pakhomov
College of Pharmacy
University of Minnesota
Minneapolis, MN 55455
Genevieve B. Melton
Institute for Health Informatics
University of Minnesota
Minneapolis, MN 55455
Abstract
In this paper, we introduce a knowledge-based
method to disambiguate biomedical acronyms
using second-order co-occurrence vectors. We
create these vectors using information about a
long-form obtained from the Unified Medical
Language System and Medline. We evaluate
this method on a dataset of 18 acronyms found
in biomedical text. Our method achieves an
overall accuracy of 89%. The results show
that using second-order features provide a dis-
tinct representation of the long-form and po-
tentially enhances automated disambiguation.
1 Introduction
Word Sense Disambiguation (WSD) is the task
of automatically identifying the appropriate sense of
a word with multiple senses. For example, the word
culture could refer to anthropological culture
(e.g., the culture of the Mayan civilization), or a
laboratory culture (e.g., cell culture).
Acronym disambiguation is the task of automat-
ically identifying the contextually appropriate long-
form of an ambiguous acronym. For example, the
acronym MS could refer to the disease Multiple Scle-
rosis, the drug Morphine Sulfate, or the state Missis-
sippi, among others. Acronym disambiguation can
be viewed as a special case of WSD, although, un-
like terms, acronyms tend to be complete phrases
or expressions, therefore collocation features are
not as easily identified. For example, the feature
rate when disambiguating the term interest, as in
?Contact author : bthomson@umn.edu.
interest rate, may not be available. Acronyms also
tend to be noun phrases, therefore syntactic features
do not provide relevant information for the purposes
of disambiguation.
Identifying the correct long-form of an acronym
is important not only for the retrieval of information
but the understanding of the information by the re-
cipient. In general English, Park and Byrd (2001)
note that acronym disambiguation is not widely
studied because acronyms are not as prevalent in lit-
erature and newspaper articles as they are in specific
domains such as government, law, and biomedicine.
In the biomedical sublanguage domain, acronym
disambiguation is an extensively studied problem.
Pakhomov (2002) note acronyms in biomedical lit-
erature tend to be used much more frequently than in
news media or general English literature, and tend
to be highly ambiguous. For example, the Uni-
fied Medical Language System (UMLS), which in-
cludes one of the largest terminology resources in
the biomedical domain, contains 11 possible long-
forms of the acronym MS in addition to the four
examples used above. Liu et al (2001) show that
33% of acronyms are ambiguous in the UMLS. In a
subsequent study, Liu et al (2002a) found that 80%
of all acronyms found in Medline, a large repository
of abstracts from biomedical journals, are ambigu-
ous. Wren and Garner (2002) found that there exist
174,000 unique acronyms in the Medline abstracts
in which 36% of them are ambiguous. The authors
also estimated that the number of unique acronyms
is increasing at a rate of 11,000 per year.
Supervised and semi-supervised methods have
been used successfully for acronym disambiguation
145
but are limited in scope due to the need for sufficient
training data. Liu et al (2004) state that an acronym
could have approximately 16 possible long-forms in
Medline but could not obtain a sufficient number of
instances for each of the acronym-long-form pairs
for their experiments. Stevenson et al (2009) cite
a similar problem indicating that acronym disam-
biguation methods that do not require training data,
regardless if it is created manually or automatically,
are needed.
In this paper, we introduce a novel knowledge-
based method to disambiguate acronyms using
second-order co-occurrence vectors. This method
does not rely on training data, and therefore, is not
limited to disambiguating only commonly occurring
possible long-forms. These vectors are created us-
ing the first-order features obtained from the UMLS
about the acronym?s long-forms and second-order
features obtained from Medline. We show that us-
ing second-order features provide a distinct repre-
sentation of the long-form for the purposes of dis-
ambiguation and obtains a significantly higher dis-
ambiguation accuracy than using first order features.
2 Unified Medical Language System
The Unified Medical Language System (UMLS) is
a data warehouse that stores a number of distinct
biomedical and clinical resources. One such re-
source, used in this work, is the Metathesaurus.
The Metathesaurus contains biomedical and clin-
ical concepts from over 100 disparate terminol-
ogy sources that have been semi-automatically in-
tegrated into a single resource containing a wide
range of biomedical and clinical information. For
example, it contains the Systematized Nomencla-
ture of Medicine?Clinical Terms (SNOMED CT),
which is a comprehensive clinical terminology cre-
ated for the electronic exchange of clinical health
information, the Foundational Model of Anatomy
(FMA), which is an ontology of anatomical concepts
created specifically for biomedical and clinical re-
search, and MEDLINEPLUS, which is a terminol-
ogy source containing health related concepts cre-
ated specifically for consumers of health services.
The concepts in these sources can overlap. For
example, the concept Autonomic nerve exists in both
SNOMED CT and FMA. The Metathesaurus assigns
the synonymous concepts from the various sources
a Concept Unique Identifiers (CUIs). Thus both
the Autonomic nerve concepts in SNOMED CT and
FMA are assigned the same CUI (C0206250). This
allows multiple sources in the Metathesaurus to be
treated as a single resource.
Some sources in the Metathesaurus contain ad-
ditional information about the concept such as a
concept?s synonyms, its definition and its related
concepts. There are two main types of relations
in the Metathesaurus that we use: the parent/child
and broader/narrower relations. A parent/child re-
lation is a hierarchical relation between two con-
cepts that has been explicitly defined in one of the
sources. For example, the concept Splanchnic nerve
has an is-a relation with the concept Autonomic
nerve in FMA. This relation is carried forward to
the CUI level creating a parent/child relations be-
tween the CUIs C0037991 (Splanchnic nerve) and
C0206250 (Autonomic nerve) in the Metathesaurus.
A broader/narrower relation is a hierarchical relation
that does not explicitly come from a source but is
created by the UMLS editors. We use the entire
UMLS including the RB/RN and PAR/CHD rela-
tions in this work.
3 Medline
Medline (Medical Literature Analysis and Retrieval
System Online) is a bibliographic database contain-
ing over 18.5 million citations to journal articles
in the biomedical domain which is maintained by
the National Library of Medicine (NLM). The 2010
Medline Baseline, used in this study, encompasses
approximately 5,200 journals starting from 1948 and
is 73 Gigabytes; containing 2,612,767 unique uni-
grams and 55,286,187 unique bigrams. The majority
of the publications are scholarly journals but a small
number of newspapers, and magazines are included.
4 Acronym Disambiguation
Existing acronym disambiguation methods can be
classified into two categories: form-based and
context-based methods. Form-based methods, such
as the methods proposed by Taghva and Gilbreth
(1999), Pustejovsky et al (2001), Schwartz and
Hearst (2003) and Nadeau and Turney (2005), dis-
ambiguate the acronym by comparing its letters di-
146
rectly to the initial letters in the possible long-forms
and, therefore, would have difficulties in distin-
guishing between acronyms with similar long-forms
(e.g., RA referring to Refractory anemia or Rheuma-
toid arthritis).
In contrast, context-based methods disambiguate
between acronyms based on the context in which the
acronym is used with the assumption that the context
surrounding the acronym would be different for each
of the possible long-forms. In the remainder of this
section, we discuss these types of methods in more
detail.
4.1 Context-based Acronym Disambiguation
Methods
Liu et al (2001) and Liu et al (2002b) introduce
a semi-supervised method in which training and
test data are automatically created by extracting ab-
stracts from Medline that contain the acronym?s
long-forms. The authors use collocations and a bag-
of-words approach to train a Naive Bayes algorithm
and report an accuracy of 97%. This method be-
gins to treat acronym disambiguation as more of a
WSD problem by looking at the context in which
the acronym exists to determine its long-form, rather
than the long-form itself. In a subsequent study, Liu
et al (2004) explore using additional features and
machine learning algorithms and report an accuracy
of 99% using the Naive Bayes.
Joshi (2006) expands on Liu, et als work. They
evaluate additional machine learning algorithms us-
ing unigrams, bigrams and trigrams as features.
They found that given their feature set, SVMs ob-
tain the highest accuracy (97%).
Stevenson et al (2009) re-recreate this dataset us-
ing the method described in Liu et al (2001) to auto-
matically create training data for their method which
uses a mixture of linguistics features (e.g., colloca-
tions, unigrams, bigrams and trigrams) in combina-
tion with the biomedical features CUIs and Medi-
cal Subject Headings, which are terms manually as-
signed to Medline abstracts for indexing purposes.
The authors evaluate the Naive Bayes, SVM and
Vector Space Model (VSM) described by Agirre and
Martinez (2004), and report that VSM obtained the
highest accuracy (99%).
Pakhomov (2002) also developed a semi-
supervised method in which training data was
automatically created by first identifying the long-
form found in the text of clinical reports, replacing
the long-form with the acronym to use as training
data. A maximum entropy model trained and tested
on a corpus of 10,000 clinical notes achieved an
accuracy of 89%. In a subsequent study, Pakhomov
et al (2005) evaluate obtaining training data from
three sources: Medline, clinical records and the
world wide web finding using a combination of
instances from clinical records and the web obtained
the highest accuracy.
Joshi et al (2006) compare using the Naive
Bayes, Decision trees and SVM on ambiguous
acronyms found in clinical reports. The authors
use the part-of-speech, the unigrams and the bi-
grams of the context surrounding the acronym as
features. They evaluate their method on 7,738
manually disambiguated instances of 15 ambiguous
acronyms obtaining an accuracy of over 90% for
each acronym.
5 Word Sense Disambiguation
Many knowledge-based WSD methods have been
developed to disambiguate terms which are closely
related to the work presented in this paper. Lesk
(1986) proposes a definition overlap method in
which the appropriate sense of an ambiguous term
was determined based on the overlap between its
definition in a machine readable dictionary (MRD).
Ide and Ve?ronis (1998) note that this work provided
a basis for most future MRD disambiguation meth-
ods; including the one presented in this paper.
Banerjee and Pedersen (2002) use the Lesk?s
overlap method to determine the relatedness be-
tween two concepts (synsets) in WordNet. They ex-
tend the method to not only include the definition
(gloss) of the two synsets in the overlap but also the
glosses of related synsets.
Wilks et al (1990) expand upon Lesk?s method by
calculating the number of times the words in the def-
inition co-occur with the ambiguous words. In their
method, a vector is created using the co-occurrence
information for the ambiguous word and each of its
possible senses. The similarity is then calculated be-
tween the ambiguous word?s vector and each of the
sense vectors. The sense whose vector is most simi-
lar is assigned to the ambiguous word.
147
0
.3
0 0 0 0 0 0disphosphoric
glucose
fructose
phosphoric
esters
changed
effect
0 0 0 0 0
glycolyte
en
zym
es
co
m
bined
decreases
intensity
acid
0
m
etabolites
FEATURES
0 0 0 0 .2 0acid 0 0 0 .1 0 0
0 0 0 0 .5 0 0esters 0 0 0 0 0 0
0 .1 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0
fructose
0 0 0 0 0 0
0 0 0 0 0 0 0
diphosphate
0 0 0 0 0 0isomer
0 0 0 0 0 0 0prevalent 0 0 0 0 0 0
0 .1 0 .3 .5 .2 02nd order vector forFructose Diphosphate 0 0 0 .1 0 0
Ex
te
nd
ed
 D
ef
in
iti
on

fo
r F
ru
ct
os
e 
Di
ph
os
ph
at
e
Figure 1: 2nd Order Vector for Fructose Diphosphate (FDP)
Patwardhan and Pedersen (2006) introduce a vec-
tor measure to determine the relatedness between
pairs of concepts. In this measure, a second order
co-occurrence vector is created for each concept us-
ing the words in each of the concepts definition and
calculating the cosine between the two vectors. This
method has been used in the task of WSD by calcu-
lating the relatedness between each possible sense
of the ambiguous word and its surrounding context.
The context whose sum is the most similar is as-
signed to the ambiguous word.
Second-order co-occurrence vectors were first in-
troduced by Schu?tze (1992) for the task of word
sense discrimination and later extended by Puran-
dare and Pedersen (2004). As noted by Pedersen
(2010), disambiguation requires a sense-inventory
in which the long-forms are known ahead of time,
where as in discrimination this information is not
known a priori.
6 Method
In our method, a second-order co-occurrence vec-
tor is created for each possible long-form of the
acronym, and the acronym itself. The appropriate
long-form of the acronym is then determined by
computing a cosine between the vector represent-
ing the ambiguous acronym and each of the vectors
representing the long-forms. The long-form whose
vector has the smallest angle between it and the
acronym vector is chosen as the most likely long-
form of the acronym.
To create a second-order vector for a long-form,
we first obtain a textual description of the long-form
in the UMLS, which we refer to as the extended defi-
nition. Each long-form, from our evaluation set, was
mapped to a concept in the UMLS, therefore, we use
the long-form?s definition plus the definition of its
parent/children and narrow/broader relations and the
terms in the long-form.
We include the definition of the related concepts
because not all concepts in the UMLS have a defini-
tion. In our evaluation dataset, not a single acronym
has a definition for each possible long-form. On
average, each extended definition contains approx-
imately 453 words. A short example of the extended
definition for the acronym FDP when referring to
148
fructose diphosphate is: ? Diphosphoric acid esters
of fructose. The fructose diphosphate isomer is most
prevalent. fructose diphosphate.?
After the extended definition is obtained, we cre-
ate the second-order vector by first creating a word
by word co-occurrence matrix in which the rows
represent the content words in the long-forms, ex-
tended definition, and the columns represent words
that co-occur in Medline abstracts with the words in
the definition. Each cell in this matrix contains the
Log Likelihood Ratio (Dunning (1993)) of the word
found in the row and the word in the column. Sec-
ond, each word in the long-forms, extended defini-
tion is replaced by its corresponding vector, as given
in the co-occurrence matrix. The centroid of these
vectors constitutes the second order co-occurrence
vector used to represent the long-form.
For example, given the example corpus contain-
ing two instances: 1) The metabolites, glucose fruc-
tose and their phosphoric acid esters are changed
due to the effect of glycolytic enzymes, and 2)
The phosphoric acid combined with metabolites de-
creases the intensity. Figure 1 shows how the
second-order co-occurrence vector is created for the
long-form fructose diphosphate using the extended
definition and features from our given corpus above.
The second-order co-occurrence vector for the
ambiguous acronym is created in a similar fashion,
only rather than using words in the extended defini-
tion, we use the words surrounding the acronym in
the instance.
Vector methods are subject to noise introduced by
features that do not distinguish between the differ-
ent long-forms of the acronym. To reduce this type
of noise, we select the features to use in the second
order co-occurrence vectors based on the following
criteria: 1) second order feature cannot be a stop-
word, and 2) second order feature must occur at least
twice in the feature extraction dataset and not occur
more than 150 times. We also experiment with the
location of the second-order feature with respect to
the first-order feature by varying the window size of
zero, four, six and ten words to the right and the left
of the first-order feature. The experiments in this
paper were conducted using CuiTools v0.15. 1
Our method is different from other context-based
1http://cuitools.sourceforge.net
acronym disambiguation methods discussed in the
related work because it does not require annotated
training data for each acronym that needs to be dis-
ambiguated. Our method differs from the method
proposed by Wilks et al (1990) in two fundamen-
tal aspects: 1) using the extended definition of
the possible long-forms of an acronym, and 2) using
second-order vectors to represent the instance con-
taining the acronym and each of the acronym?s pos-
sible long-forms.
7 Data
7.1 Acronym Dataset
We evaluated our method on the ?Abbrev? dataset 2
made available by Stevenson et al (2009). The
acronyms and long-forms in the data were initially
presented by Liu et al (2001). Stevenson et al
(2009) automatically re-created this dataset by iden-
tifying the acronyms and long-forms in Medline ab-
stracts and replacing the long-form in the abstract
with its acronym. Each abstract contains approxi-
mately 216 words. The dataset consists of three sub-
sets containing 100 instances, 200 instances and 300
instances of the ambiguous acronym referred to as
Abbrev.100, Abbrev.200, Abbrev.300, respectively.
The acronyms long-forms were manually mapped to
concepts in the UMLS by Stevenson, et al
A sufficient number of instances were not found
for each of the 21 ambiguous acronyms by Steven-
son et al (2009). For example, ?ASP? only con-
tained 71 instances and therefore not included in any
of the subsets. ?ANA? and ?FDP? only contained
just over 100 instances and therefore, are only in-
cluded in the Abbrev.100 subset. ?ACE?, ?ASP?
and ?CSF? were also excluded because several of
the acronyms? long-forms did not occur frequently
enough in Medline to create a balanced dataset.
We evaluate our method on the same subsets that
Stevenson et al (2009) used to evaluate their super-
vised method. The average number of long-forms
per acronym is 2.6 and the average majority sense
across all subsets is 70%.
7.2 Feature Extraction Dataset
We use abstracts from Medline, containing ambigu-
ous acronym or long-form, to create the second-
2http://nlp.shef.ac.uk/BioWSD/downloads/corpora
149
order co-occurrence vectors for our method as de-
scribed in Section 6. Table 1 shows the number of
Medline abstracts extracted for the acronyms.
Acronyms # Abstracts Acronym # Abstracts
ANA 3,267 APC 11,192
BPD 3,260 BSA 10,500
CAT 44,703 CML 8,777
CMV 13,733 DIP 2,912
EMG 16,779 FDP 1,677
LAM 1,572 MAC 6,528
MCP 2,826 PCA 11,044
PCP 5,996 PEG 10,416
PVC 2,780 RSV 5,091
Table 1: Feature Extraction Data for Acronyms
8 Results
Table 2 compares the majority sense baseline and the
first-order baseline with the results obtained using
our method on the Acronym Datasets (Abbrev.100,
Abbrev.200 and Abbrev.300) using a window size
of zero, four, six and ten. Differences between the
means of disambiguation accuracy produced by var-
ious approaches were tested for statistical signifi-
cance using the pair-wise Student?s t-tests with the
significance threshold set to 0.01.
Window Abbrev
Size 100 200 300
Maj. Sense Baseline 0.70 0.70 0.70
1-order Baseline 0.57 0.61 0.61
Our Method
0 0.83 0.83 0.81
4 0.86 0.87 0.86
6 0.88 0.90 0.89
10 0.88 0.90 0.89
Table 2: Overall Disambiguation Results
The majority sense baseline is often used to evalu-
ate supervised learning algorithms and indicates the
accuracy that would be achieved by assigning the
most frequent sense (long-form) to every instance.
The results in Table 2 demonstrate that our method is
significantly more accurate than the majority sense
baseline (p ? 0.01).
We compare the results using second-order vec-
tors to first-order vectors. Table 2 shows that ac-
curacy of the second-order results is significantly
higher than the first-order results (p ? 0.01).
The results in Table 2 also show that, as the win-
dow size grows from zero to six, the accuracy of the
system increases and plateaus at a window size of
ten. There is no statistically significant difference
between using a window size of six and ten but there
is a significant difference between a window size of
zero and six, as well as four and six (p ? 0.01).
Acronym # Long Abbrev Abbrev Abbrev
forms 100 200 300
ANA 3 0.84
APC 3 0.88 0.87 0.87
BPD 3 0.96 0.95 0.95
BSA 2 0.95 0.93 0.92
CAT 2 0.88 0.87 0.87
CML 2 0.81 0.84 0.83
CMV 2 0.98 0.98 0.98
DIP 2 0.98 0.98
EMG 2 0.88 0.89 0.88
FDP 4 0.65
LAM 2 0.86 0.87 0.88
MAC 4 0.94 0.95 0.95
MCP 4 0.73 0.67 0.68
PCA 4 0.78 0.79 0.79
PCP 2 0.97 0.96 0.96
PEG 2 0.89 0.89 0.88
PVC 2 0.95 0.95
RSV 2 0.97 0.98 0.98
Table 3: Individual Results using a Window Size of 6.
9 Error Analysis
Table 3 shows the results obtained by our method for
the individual acronyms using a window size of six,
and the number of possible long-forms per acronym.
Of the 18 acronyms, three obtain an accuracy below
80 percent: FDP, MCP and PCA.
FPD has four possible long-forms: Fructose
Diphosphate (E1), Formycin Diphosphate (E2), Fib-
rinogen Degradation Product (E3) and Flexor Dig-
itorum Profundus (E4). The confusion matrix in
Table 4 shows that the method was unable to dis-
tinguish between the two long-forms, E1 and E2,
which are both diphosphates, nor E2 and E3.
Long-Form E1 E2 E3 E4
E1: Fructose Diphosphate
E2: Formycin Diphosphate 5 2 11 19
E3: Fibrinogen Degradation Product 4
E4: Flexor Digitorum Profundus 59
Table 4: FDP Confusion Matrix
MCP also has four possible long-forms: Multicat-
alytic Protease (E1), Metoclopramide (E2), Mono-
cyte Chemoattractant Protein (E3) and Membrane
150
Cofactor Protein (E4). The confusion matrix in Ta-
ble 5 shows that the method was not able to distin-
guish between E3 and E4, which are both proteins,
and E1, which is a protease (an enzyme that breaks
down a protein).
Long-Form E1 E2 E3 E4
E1: Multicatalytic Protease 1 5 6 1
E2: Metoclopramide 15
E3: Monocyte Chemoattractant Protein 1 3 44 11
E4: Membrane Cofactor Protein 13
Table 5: MCP Confusion Matrix
PCA has four possible long-forms: Passive Cu-
taneous Anaphylaxis (E1), Patient Controlled Anal-
gesia (E2), Principal Component Analysis (E3), and
Posterior Cerebral Artery (E4). The confusion ma-
trix in Table 6 shows that the method was not able
to distinguish between E2 and E3. Analyzing the
extended definitions of the concepts showed that E2
includes the definition to the concept Pain Manage-
ment. The words in this definition overlap with
many of the words used in E3s extended definition.
Long-Form E1 E2 E3 E4
E1:Passive Cutaneous Anaphylaxis 18 6 1
E2:Patient Controlled Analgesia 5 15
E3:Principal Component Analysis 48
E4:Posterior Cerebral Artery 7
Table 6: PCA Confusion Matrix
10 Comparison with Previous Work
Of the previously developed methods, Liu et al
(2004) and Stevenson et al (2009) evaluated their
semi-supervised methods on the same dataset as we
used for the current study. A direct comparison
can not be made between our method and Liu et al
(2004) because we do not have an exact duplication
of the dataset that they use. Their results are com-
parable to Stevenson et al (2009) with both report-
ing results in the high 90s. Our results are directly
comparable to Stevenson et al (2009) who report
an overall accuracy of 98%, 98% and 99% on the
Abbrev.100, Abbrev.200 and Abbrev.300 datasets
respectively. This is approximately 10 percentage
points higher than our results.
The advantage of the methods proposed by
Stevenson et al (2009) and Liu et al (2004) is that
they are semi-supervised which have been shown to
obtain higher accuracies than methods that do not
use statistical machine learning algorithms. The dis-
advantage is that sufficient training data are required
for each possible acronym-long-form pair. Liu et
al. (2004) state that an acronym could have approxi-
mately 16 possible long-forms in Medline but a suf-
ficient number of instances for each of the acronym-
long-form pairs were not found in Medline and,
therefore, evaluated their method on 15 out of the
original 34 acronyms. Stevenson et al (2009) cite
a similar problem in re-creating this dataset. This
shows the limitation to these methods is that a suffi-
cient number of training examples can not be ob-
tained for each acronym that needs to be disam-
biguated. The method proposed in the paper does
not have this limitation and can be used to disam-
biguate any acronym in Medline.
11 Discussion
In this paper, we presented a novel method to disam-
biguate acronyms in biomedical text using second-
order features extracted from the UMLS and Med-
line. The results show that using second-order fea-
tures provide a distinct representation of the long-
form that is useful for disambiguation.
We believe that this is because biomedical text
contains technical terminology that has a rich source
of co-occurrence information associated with them
due to their compositionality. Using second-order
information works reasonably well because when
the terms in the extended definition are broken up
into their individual words, information is not being
lost. For example, the term Patient Controlled Anal-
gesia can be understood by taking the union of the
meanings of the three terms and coming up with an
appropriate definition of the term (patient has con-
trol over their analgesia).
We evaluated various window sizes to extract the
second-order co-occurrence information from, and
found using locally occurring words obtains a higher
accuracy. This is consistent with the finding reported
by Choueka and Lusignan (1985) who conducted an
experiment to determine what size window is needed
for humans to determine the appropriate sense of an
ambiguous word.
The amount of data used to extract the second-
151
order features for each ambiguous acronym varied
depending on its occurrence in Medline. Table 1 in
Section 7.2 shows the number of abstracts in Med-
line used for each acronym. We compared the accu-
racy obtained by our method using a window size of
six on the Abbrev.100 dataset with the number of ab-
stracts in the feature extraction data. We found that
the accuracy was not correlated with the amount of
data used (r = 0.07). This confirms that it is not the
quantity but the content of the contextual informa-
tion that determines the accuracy of disambiguation.
We compared using second-order features and
first-order features showing that the second-order re-
sults obtained a significantly higher accuracy. We
believe that this is because the definitions of the pos-
sible concepts are too sparse to provide enough in-
formation to distinguish between them. This find-
ing coincides to that of Purandare and Pedersen
(2004) and Pedersen (2010) who found that with
large amounts of data, first-order vectors perform
better than second-order vectors, but second-order
vectors are a good option when large amounts of
data are not available.
The results of the error analysis indicate that
for some acronyms using the extended definition
does not provide sufficient information to make
finer grained distinctions between the long-forms.
This result also indicates that, although many long-
forms of acronyms can be considered coarse-grained
senses, this is not always the case. For example, the
analysis of MCP showed that two of its possible
long-forms are proteins which are difficult to differ-
entiate from given the context.
The results of the error analysis also show that
indicative collocation features for acronyms are not
easily identified because acronyms tend to be com-
plete phrases. For example, two of the possible
long-forms of DF are Fructose Diphosphate and
Formycin Diphosphate.
Two main limitations of this work must be men-
tioned to facilitate the interpretation of the results.
The first is the small number of acronyms and the
small number of long-forms per acronym in the
dataset; however, the acronyms in this dataset are
representative of the kinds of acronyms one would
expect to see in biomedical text. The second limita-
tion is that the dataset contains only those acronyms
whose long-forms were found in Medline abstracts.
The main goal of this paper was to determine if the
context found in the long-forms, extended definition
was distinct enough to distinguish between them us-
ing second-order vectors. For this purpose, we feel
that the dataset was sufficient although a more ex-
tensive dataset may be needed in the future for im-
proved coverage.
12 Future Work
In the future, we plan to explore three different
avenues. The first avenue is to look at obtaining
contextual descriptions of the possible long-forms
from resources other than the UMLS such as the
MetaMapped Medline baseline and WordNet. The
second avenue is limiting the features that are used
in the instance vectors. The first-order features in
the instance vector contain the words from the entire
abstract. As previously mentioned, vector methods
are subject to noise, therefore, in the future we plan
to explore using only those words that are co-located
next to the ambiguous acronym. The third avenue is
expanding the vector to allow for terms. Currently,
we use word vectors, in the future, we plan to extend
the method to use terms, as identified by the UMLS,
as features rather than single words.
We also plan to test our approach in the clinical
domain. We believe that acronym disambiguation
may be more difficult in this domain due to the in-
crease amount of long-forms as seen in the datasets
used by Joshi et al (2006) and Pakhomov (2002).
13 Conclusions
Our study constitutes a significant step forward in
the area of automatic acronym ambiguity resolu-
tion, as it will enable the incorporation of scalable
acronym disambiguation into NLP systems used for
indexing and retrieval of documents in specialized
domains such as medicine. The advantage of our
method over previous methods is that it does not re-
quire manually annotated training for each acronym
to be disambiguated while still obtaining an overall
accuracy of 89%.
Acknowledgments
This work was supported by the National Insti-
tute of Health, National Library of Medicine Grant
#R01LM009623-01.
152
References
E. Agirre and D. Martinez. 2004. The Basque Country
University system: English and Basque tasks. In Pro-
ceedings of the 3rd ACL workshop on the Evaluation
of Systems for the Semantic Analysis of Text (SENSE-
VAL), pages 44?48.
S. Banerjee and T. Pedersen. 2002. An adapted lesk al-
gorithm for word sense disambiguation using Word-
Net. In Proceedings of the 3rd International Confer-
ence on Intelligent Text Processing and Computational
Linguistics, pages 136?145.
Y. Choueka and S. Lusignan. 1985. Disambiguation
by short contexts. Computers and the Humanities,
19(3):147?157.
T. Dunning. 1993. Accurate methods for the statistics of
surprise and coincidence. Computational Linguistics,
19(1):61?74.
N. Ide and J. Ve?ronis. 1998. Introduction to the special
issue on word sense disambiguation: the state of the
art. Computational Linguistics, 24(1):2?40.
M. Joshi, S. Pakhomov, T. Pedersen, and C.G. Chute.
2006. A comparative study of supervised learning as
applied to acronym expansion in clinical reports. In
Proceedings of the Annual Symposium of AMIA, pages
399?403.
M. Joshi. 2006. Kernel Methods for Word Sense Disam-
biguation and Abbreviation Expansion. Master?s the-
sis, University of Minnesota.
M. Lesk. 1986. Automatic sense disambiguation using
machine readable dictionaries: how to tell a pine cone
from an ice cream cone. Proceedings of the 5th Annual
International Conference on Systems Documentation,
pages 24?26.
H. Liu, YA. Lussier, and C. Friedman. 2001. Disam-
biguating ambiguous biomedical terms in biomedical
narrative text: an unsupervised method. Journal of
Biomedical Informatics, 34(4):249?261.
H. Liu, A.R. Aronson, and C. Friedman. 2002a. A study
of abbreviations in MEDLINE abstracts. In Proceed-
ings of the Annual Symposium of AMIA, pages 464?
468.
H. Liu, S.B. Johnson, and C. Friedman. 2002b. Au-
tomatic resolution of ambiguous terms based on ma-
chine learning and conceptual relations in the UMLS.
JAMIA, 9(6):621?636.
H. Liu, V. Teller, and C. Friedman. 2004. A multi-
aspect comparison study of supervised word sense dis-
ambiguation. JAMIA, 11(4):320?331.
D. Nadeau and P. Turney. 2005. A supervised learning
approach to acronym identification. In Proceedings
of the 18th Canadian Conference on Artificial Intelli-
gence, pages 319?329.
S. Pakhomov, T. Pedersen, and C.G. Chute. 2005. Ab-
breviation and acronym disambiguation in clinical dis-
course. In Proceedings of the Annual Symposium of
AMIA, pages 589?593.
S. Pakhomov. 2002. Semi-supervised maximum en-
tropy based approach to acronym and abbreviation
normalization in medical texts. In Proceedings of
the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 160?167.
Y. Park and R.J. Byrd. 2001. Hybrid text mining for find-
ing abbreviations and their definitions. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 126?133.
S. Patwardhan and T. Pedersen. 2006. Using WordNet-
based context vectors to estimate the semantic related-
ness of concepts. In Proceedings of the EACL 2006
Workshop Making Sense of Sense - Bringing Com-
putational Linguistics and Psycholinguistics Together,
pages 1?8.
T. Pedersen. 2010. The effect of different context repre-
sentations on word sense discrimination in biomedical
texts. In Proceedings of the 1st ACM International IHI
Symposium, pages 56?65.
A. Purandare and T. Pedersen. 2004. Word sense dis-
crimination by clustering contexts in vector and sim-
ilarity spaces. In Proceedings of the Conference on
Computational Natural Language Learning (CoNLL),
pages 41?48.
J. Pustejovsky, J. Castano, B. Cochran, M. Kotecki,
M. Morrell, and A. Rumshisky. 2001. Extraction and
disambiguation of acronym-meaning pairs in medline.
Unpublished manuscript.
H. Schu?tze. 1992. Dimensions of meaning. In Proceed-
ings of the 1992 ACM/IEEE Conference on Supercom-
puting, pages 787?796.
A.S. Schwartz and M.A. Hearst. 2003. A simple
algorithm for identifying abbreviation definitions in
biomedical text. In Proceedings of the Pacific Sym-
posium on Biocomputing (PSB), pages 451?462.
M. Stevenson, Y. Guo, A. Al Amri, and R. Gaizauskas.
2009. Disambiguation of biomedical abbreviations.
In Proceedings of the ACL BioNLP Workshop, pages
71?79.
K. Taghva and J. Gilbreth. 1999. Recognizing acronyms
and their definitions. ISRI UNLV, 1:191?198.
Y. Wilks, D. Fass, C.M. Guo, J.E. McDonald, T. Plate,
and B.M. Slator. 1990. Providing machine tractable
dictionary tools. Machine Translation, 5(2):99?154.
J.D. Wren and H.R. Garner. 2002. Heuristics for iden-
tification of acronym-definition patterns within text:
towards an automated construction of comprehensive
acronym-definition dictionaries. Methods of Informa-
tion in Medicine, 41(5):426?434.
153
Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 131?133,
Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational Linguistics
The Ngram Statistics Package (Text::NSP) - A Flexible Tool for Identifying
Ngrams, Collocations, and Word Associations
Ted Pedersen?
Department of Computer Science
University of Minnesota
Duluth, MN 55812
Satanjeev Banerjee
Twitter, Inc.
795 Folsom Street
San Francisco, CA 94107
Bridget T. McInnes
College of Pharmacy
University of Minnesota
Minneapolis, MN 55455
Saiyam Kohli
SDL Language Weaver, Inc.
6060 Center Drive, Suite 150
Los Angeles, CA 90045
Mahesh Joshi
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
Ying Liu
College of Pharmacy
University of Minnesota
Minneapolis, MN 55455
Abstract
The Ngram Statistics Package (Text::NSP)
is freely available open-source software that
identifies ngrams, collocations and word as-
sociations in text. It is implemented in Perl
and takes advantage of regular expressions to
provide very flexible tokenization and to allow
for the identification of non-adjacent ngrams.
It includes a wide range of measures of associ-
ation that can be used to identify collocations.
1 Introduction
The identification of multiword expressions is a key
problem in Natural Language Processing. Despite
years of research, there is still no single best way
to proceed. As such, the availability of flexible and
easy to use toolkits remains important. Text::NSP
is one such package, and includes programs for
counting ngrams (count.pl, huge-count.pl), measur-
ing the association between the words that make up
an ngram (statistic.pl), and for measuring correlation
between the rankings of ngrams created by differ-
ent measures (rank.pl). It is also able to identify n-
th order co-occurrences (kocos.pl) and pre?specified
compound words in text (find-compounds.pl).
This paper briefly describes each component of
NSP. Additional details can be found in (Banerjee
and Pedersen, 2003) or in the software itself, which
is freely available from CPAN 1 or Sourceforge 2.
?Contact author : tpederse@d.umn.edu. Note that authors
Banerjee, McInnes, Kohli and Joshi contributed to Text::NSP
while they were at the University of Minnesota, Duluth.
1http://search.cpan.org/dist/Text-NSP/
2http://sourceforge.net/projects/ngram/
2 count.pl
The program count.pl takes any number of plain
text files or directories of such files and counts the
total number of ngrams as well their marginal to-
tals. It provides the ability to define what a token
may be using regular expressions (via the --token
option). An ngram is an ordered sequence of n to-
kens, and under this scheme tokens may be almost
anything, including space separated strings, charac-
ters, etc. Also, ngrams may be made up of nonadja-
cent tokens due to the --window option that allows
users to specify the number of tokens within which
an ngram must occur.
Counting is done using hashes in Perl which are
memory intensive. As a result, NSP also provides
the huge-count.pl program and various other huge-
*.pl utilities that carry out count.pl functionality us-
ing hard drive space rather than memory. This can
scale to much larger amounts of text, although usu-
ally taking more time in the process.
By default count.pl treats ngrams as ordered se-
quences of tokens; dog house is distinct from house
dog. However, it may be that order does not always
matter, and a user may simply want to know if two
words co-occur. In this case the combig.pl program
adjusts counts from count.pl to reflect an unordered
count, where dog house and house dog are consid-
ered the same. Finally, find-compounds.pl allows a
user to specify a file of already known multiword ex-
pressions (like place names, idioms, etc.) and then
identify all occurrences of those in a corpus before
running count.pl
131
3 statistic.pl
The core of NSP is a wide range of measures of
association that can be used to identify interest-
ing ngrams, particularly bigrams and trigrams. The
measures are organized into families that share com-
mon characteristics (which are described in detail in
the source code documentation). This allows for an
object oriented implementation that promotes inher-
itance of common functionality among these mea-
sures. Note that all of the Mutual Information mea-
sures are supported for trigrams, and that the Log-
likelihood ratio is supported for 4-grams. The mea-
sures in the package are shown grouped by family
in Table 1, where the name by which the measure is
known in NSP is in parentheses.
Table 1: Measures of Association in NSP
Mutual Information (MI)
(ll) Log-likelihood Ratio (Dunning, 1993)
(tmi) true MI (Church and Hanks, 1990)
(pmi) Pointwise MI (Church and Hanks, 1990)
(ps) Poisson-Stirling (Church, 2000)
Fisher?s Exact Test (Pedersen et al, 1996)
(leftFisher) left tailed
(rightFisher) right tailed
(twotailed) two tailed
Chi-squared
(phi) Phi Coefficient (Church, 1991)
(tscore) T-score (Church et al, 1991)
(x2) Pearson?s Chi-Squared (Dunning, 1993)
Dice
(dice) Dice Coefficient (Smadja, 1993)
(jaccard) Jaccard Measure
(odds) Odds Ratio (Blaheta and Johnson, 2001)
3.1 rank.pl
One natural experiment is to compare the output of
statistic.pl for the same input using different mea-
sures of association. rank.pl takes as input the out-
put from statistic.pl for two different measures, and
computes Spearman?s Rank Correlation Coefficient
between them. In general, measures within the same
family correlate more closely with each other than
with measures from a different family. As an ex-
ample tmi and ll as well as dice and jaccard differ
by only constant terms and therefore produce identi-
cal rankings. It is often worthwhile to conduct ex-
ploratory studies with multiple measures, and the
rank correlation can help recognize when two mea-
sures are very similar or different.
4 kocos.pl
In effect kocos.pl builds a word network by finding
all the n-th order co-occurrences for a given literal
or regular expression. This can be viewed somewhat
recursively, where the 3-rd order co-occurrences of
a given target word are all the tokens that occur with
the 2-nd order co-occurrences, which are all the to-
kens that occur with the 1-st order (immediate) co-
occurrences of the target. kocos.pl outputs chains of
the form king -> george -> washington,
where washington is a second order co-occurrence
(of king) since both king and washington are first
order co-occurrences of george. kocos.pl takes as
input the output from count.pl, combig.pl, or statis-
tic.pl.
5 API
In addition to command line support, Test::NSP of-
fers an extensive API for Perl programmers. All of
the measures described in Table 1 can be included
in Perl programs as object?oriented method calls
(Kohli, 2006), and it is also easy to add new mea-
sures or modify existing measures within a program.
6 Development History of Text::NSP
The Ngram Statistics Package was originally imple-
mented by Satanjeev Banerjee in 2000-2002 (Baner-
jee and Pedersen, 2003). Amruta Purandare in-
corporated NSP into SenseClusters (Purandare and
Pedersen, 2004) and added huge-count.pl, com-
big.pl and kocos.pl in 2002-2004. Bridget McInnes
added the log-likelihood ratio for longer ngrams
in 2003-2004 (McInnes, 2004). Saiyam Kohli
rewrote the measures of association to use object-
oriented methods in 2004-2006, and also added
numerous new measures for bigrams and trigams
(Kohli, 2006). Mahesh Joshi improved cross plat-
form support and created an NSP wrapper for Gate
in 2005-2006. Ying Liu wrote find-compounds.pl
and rewrote huge-count.pl in 2010-2011.
132
References
S. Banerjee and T. Pedersen. 2003. The design, imple-
mentation, and use of the Ngram Statistics Package.
In Proceedings of the Fourth International Conference
on Intelligent Text Processing and Computational Lin-
guistics, pages 370?381, Mexico City, February.
D. Blaheta and M. Johnson. 2001. Unsupervised learn-
ing of multi-word verbs. In ACL/EACL Workshop on
Collocations, pages 54?60, Toulouse, France.
K. Church and P. Hanks. 1990. Word association norms,
mutual information and lexicography. Computational
Linguistics, pages 22?29.
K. Church, W. Gale, P. Hanks, and D. Hindle. 1991. Us-
ing statistics in lexical analysis. In U. Zernik, editor,
Lexical Acquisition: Exploiting On-Line Resources to
Build a Lexicon. Lawrence Erlbaum Associates, Hills-
dale, NJ.
K. Church. 1991. Concordances for parallel text. In
Seventh Annual Conference of the UW Centre for New
OED and Text Research, Oxford, England.
K. Church. 2000. Empirical estimates of adaptation:
The chance of two noriegas is closer to p/2 than p2.
In Proceedings of the 18th International Conference
on Computational Linguistics (COLING-2000), pages
180?186, Saarbru?cken, Germany.
T. Dunning. 1993. Accurate methods for the statistics of
surprise and coincidence. Computational Linguistics,
19(1):61?74.
S. Kohli. 2006. Introducing an object oriented design to
the ngram statistics package. Master?s thesis, Univer-
sity of Minnesota, Duluth, July.
B. McInnes. 2004. Extending the log-likelihood ratio
to improve collocation identification. Master?s thesis,
University of Minnesota, Duluth, December.
T. Pedersen, M. Kayaalp, and R. Bruce. 1996. Signifi-
cant lexical relationships. In Proceedings of the Thir-
teenth National Conference on Artificial Intelligence,
pages 455?460, Portland, OR, August.
A. Purandare and T. Pedersen. 2004. Word sense
discrimination by clustering contexts in vector and
similarity spaces. In Proceedings of the Conference
on Computational Natural Language Learning, pages
41?48, Boston, MA.
F. Smadja. 1993. Retrieving collocations from text:
Xtract. Computational Linguistics, 19(1):143?177.
133
