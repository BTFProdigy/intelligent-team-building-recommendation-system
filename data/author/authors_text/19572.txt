Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 376?386, Dublin, Ireland, August 23-29 2014.
A Supervised Learning Approach Towards Profiling the Preservation of
Authorial Style in Literary Translations
Gerard Lynch
Centre for Applied Data Analytics Research
University College Dublin
Ireland
firstname.lastname@ucd.ie
Abstract
Recently there has been growing interest in the application of approaches from the text classi-
fication literature to fine-grained problems of textual stylometry. This paper seeks to answer a
question which has concerned the translation studies community: how does a literary transla-
tor?s style vary across their translations of different authors? This study focuses on the works
of Constance Garnett, one of the most prolific English-language translators of Russian literature,
and uses supervised learning approaches to analyse her translations of three well-known Rus-
sian authors, Ivan Turgenev, Fyodor Dosteyevsky and Anton Chekhov. This analysis seeks to
identify common linguistic patterns which hold for all of the translations from the same author.
Based on the experimental results, it is ascertained that both document-level metrics and n-gram
features prove useful for distinguishing between authorial contributions in our translation corpus
and their individual efficacy increases further when these two feature types are combined, result-
ing in classification accuracy of greater than 90 % on the task of predicting the original author
of a textual segment using a Support Vector Machine classifier. The ratio of nouns and pronouns
to total tokens are identified as distinguishing features in the document metrics space, along with
occurrences of common adverbs and reporting verbs from the collection of n-gram features.
1 Introduction
The application of supervised learning technologies to textual data from the humanities in order to shed
light on stylometric questions has become more popular of late. In particular, these approaches have been
applied to questions from the field of translation studies, which concern the notion of translationese
1
detection in Italian and other languages, (Baroni and Bernardini, 2006; Ilisei et al., 2010; Ilisei and
Inkpen, 2011; Popescu, 2011; Koppel and Ordan, 2011; Lembersky et al., 2011). Work has also been
carried out on source language detection from translation corpora, (van Halteren, 2008; Lynch and Vogel,
2012) and translation direction detection in parallel MT training corpora, (Kurokawa et al., 2009), which
can have applications in the domain of machine translation where the direction of bilingual translation
corpora has been shown to impact on the accuracy of automated translations using such corpora
2
.
This work seeks to apply these methods to the task of identifying authorial style within a corpus of
translations by the same translator. Venuti (1995) mentions the concept of the translator?s invisibility,
that the measure of the best translator is that their style is not distinguishable in the translation, that
their main concern and focus is to deliver the original text in a faithful manner. Of course, this task is
often subject to their own vocabulary choices and as was often the case, cultural or personal bias of the
translator or the regime or government in which they were operating. Identifying the former case will
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are
added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1
The subset or dialect of language which consists solely of translations from another language.
2
Translating FR-EN, a smaller bilingual corpus of French translated to English provides similar qualitative results (BLEU
score) to a larger corpus consisting of English translated to French.
376
be the focus of this work, as choices of vocabulary or sentence construction can be isolated through the
application of machine learning methods, although the latter is also a highly interesting question, albeit
a more complex one to tackle using the methods at hand.
3
2 Previous work
Baroni and Bernardini (2006) were among the first to apply advanced machine learning techniques to
questions of textual stylometry, although use of linguistic features and metrics was already established
in studies such as Borin and Pruetz (2001) who worked on POS distributions in translated Swedish and
work by Mikhailov and Villikka (2001) who examined translated Finnish using statistical methods and
metrics from authorship attribution. Baroni and Bernardini (2006) investigated a corpus of translated and
original text from an Italian current affairs journal using a Support Vector Machine classifier, managing
ca. 87% accuracy in distinguishing the two textual classes. Their study also investigated the performance
of humans on such a task and found that the machine learning algorithm was more consistent although it
was outperformed by one of the expert human analysts. Ilisei et al. (2010) used textual features such as
type-token ratio and readability scores in their work on detecting translated text in Spanish and obtained
comparable accuracy to Baroni and Bernardini (2006) who mostly used mixed POS and word n-grams.
Popescu (2011) employed a different approach using a feature set consisting of n-grams of characters,
and maintained reasonable accuracy in classifying translated literary works from originals.
Koppel and Ordan (2011) concerned themselves with the concept of dialects of translationese and
whether translations from the same source language were more similar to one another than translations
from different source languages and to what extent genre affected translationese. In their experiments
on the Europarl corpus and a three source-language corpus from the International Herald Tribune, they
found that training on one corpus and testing on another reported low accuracy, indicating genre effects,
coupled with the fact that training on a corpus of translations from one source language and testing
on a corpus translation from another source language obtained poorer results than using a corpus of
translations from several source languages.
van Halteren (2008) investigated the predictability of source language from a corpus of Europarl trans-
lations and predicted source language with an accuracy of over 90%, using multiple translations of a
source text in different languages. Distinguishing features from the Europarl corpus included phrases
such as a certain number in texts of French origin, framework conditions in texts of a German origin
and various features that were particular to the nature of the corpus as a collection of parliamentary
speeches.
4
More recently, Lynch and Vogel (2012) revisited the source language detection task with a
focus on literary translations, and obtained classification accuracy of ca. 80% on a corpus of translations
into English from Russian, German and French using a feature set containing a combination of ratios of
parts of speech and POS n-grams. Texts translated from French had a higher ratio of nouns to total words
than the other two categories, and the frequency of contractions such as it?s and that?s varied between the
subcorpora.
Focusing on the stylistic variation of individual translators from the point of view of researchers in
translation studies, Baker (2000) defined frameworks for performing stylistic analyses of translator?s us-
ing quantitative methods. Her own examples examined translators of Portuguese and Arabic and focused
on the translation of common verbs, such as say and tell. She found that the frequency of these verbs
was a distinguishing metric between translators but was careful to mention that these features might vary
depending on the corpora in question. Winters (2007) profiled translator style in two translations of F.
Scott Fitzgeralds The Beautiful and the Damned, focusing on modal particles and speech act reporting
verbs as a distinguishing aspect of translatorial style. Vajn (2009) applied textual metrics such as type-
token ratio and relative vocabulary richness to two translations of Plato?s Republic to investigate the
variation between two translations by Benjamin Jowett and Robin Waterfield and developed a theory of
co-authorship to explain the complementary stylistic effect of authorial and translatorial style.
3
See Li et al. (2011) and Wang and Li (2012) for examples of studies of translation from Chinese and English which take
the cultural background of translators into account when discussing distinguishable features.
4
German native speakers addressed the congregation in a different manner to English native speakers, for example.
377
Ongoing work in translation studies and digital humanities have examined the question of translatorial
vs. authorial style using computational analyses. Burrows (2002) investigated the stylistic properties of
several English translations of Roman poet Juvenal using his own Delta metric developed for authorship
attribution and the frequencies of common words, Lucic and Blake (2011) investigated two translations
of German author Rainer Maria Rilke in English using the Stanford Lexical Parser and found differing
patterns of syntactic structure such as negation modifiers and adverbial modifiers.
5
Recently, Forsyth and Lam (2013) analysed two parallel English translations of the French-language
correspondence of Theo and Vincent Van Gogh using k-nearest neighbour classifiers and a feature set
consisting of the sixty-nine most frequent words and found that a distinct authorial style for each of
the brothers was preserved in both translations, with translatorial style also proving distinguishable,
albeit to a lesser extent than its authorial counterpart. Lynch (2013) investigated two English translators
of Henrik Ibsen?s dramas using machine learning methods and found that document metrics and n-
gram features similar to those used in this current study proved accurate in distinguishing authorship of
parallel translations of the same source, and also that document metrics such as average sentence length
distributions learned from translations of different works by the same author could be used to classify
the author of a parallel translation, indicating that the translators? styles were learnable across a diverse
corpus of works by the same author.
Rybicki (2006) used Burrow?s Delta to investigate the stylistic nature of character idiolects in dramatic
translation, focusing on Polish drama, and found that the translated idiolects tended to cluster in similar
patterns
6
to the idiolects in the original text. Lynch and Vogel (2009) worked on a similar topic, the
clustering of character idiolects in English and German translations of Henrik Ibsen?s plays using the ?
2
metric. Rybicki and Heydel (2013) used Burrow?s Delta, and dendrogram clustering to investigate the
case of a Polish translation of Virginia Woolf?s Night and Day and found that the method identified the
point in the novel where one translator had taken over from another
7
Rybicki (2012) had previously used
these techniques to distinguish translatorial style in a large corpus of Polish translations and concluded
that such style was not to be captured using the methods at hand, which consisted of using Burrow?s Delta
metric with five thousand of the most frequent words. Although the metric performed well at clustering
translations by author, it failed to cluster translations by translator, leading the author to conclude that as
Venuti (1995) had claimed, the best translators are in fact invisible.
Although these studies are generally of an exploratory nature and often seek to draw conclusions
about particular literary works and figures, the methodologies used are general to textual stylometry
and have been successfully applied to emerging tasks in computational linguistics such as MT quality
estimation, (Felice and Specia, 2012), personality detection (Mairesse and Walker, 2008), sentiment
analysis (Gamon, 2004), fraud detection (Goel and Gangolly, 2012) and many other studies where textual
analyses are pertinent.
3 Motivation and background to study
In this study, the translations of a literary translator of a number of different authors are examined in order
to measure the extent to which authorial style is preserved by the translator in question. This analysis
encompasses features represented by n-grams of words or POS tags and also stylometric metrics based
on whole texts, such as type-token ratio, lexical richness and readability scores. Previous work (Rybicki
and Heydel, 2013; Burrows, 2002; Rybicki, 2012; Koppel and Ordan, 2011) focused on lists of highly
frequent words in their analysis of translations. By using supervised learning techniques, it is possible to
investigate exactly which words are discriminating between author?s idiolects in translation, regardless
of frequency, together with abstract representations of word types and textual metrics, which present an
alternative overview of the data in question.
This study examines the translations of British translator Constance Garnett (1861-1946) from the
Russian originals written by Fyodor Dosteyevsky, Ivan Turgenev and Anton Chekhov. Moser (1988) and
5
not and nearly.
6
Villians with villians, heroes with heroes and female and male characters formed separate clusters
7
The original translator passed away before she could finish the translation, hence the completion by another party.
378
Remnick (2005) write about Garnett?s
8
life, describing her early days as a student of Latin and Greek
in Cambridge, marriage to publisher and literary figure Edward Garnett and her chance introduction
to Russian literature by the chance meeting with a young revolutionary in London. Along with the
three aforementioned characters, she also translated works by Leo Tolstoy and Nikolai Gogol, Alexander
Ostrovsky and Alexander Herzen, seventy works in all.
According to Moser (1988), her reputation was firmly established with her translations of Turgenev
and thereafter Garnett was more or less responsible for igniting the English language-world?s love affair
with Dosteyevsky. Her translations were not without criticism however, Moser (1988) mentioning that
Edmund Wilson believed she caused Russian authors to sound more or less the same, a claim echoed later
by Joseph Brodsky who remarked that the average Western English-language reader cannot distinguish
Tolstoy?s voice from Dosteyevsky?s, as they are in fact reading Constance Garnett?s own voice.
Indeed, Remnick (2005) describes Garnett?s translation style and mentions how she translated at break-
neck speed, often skipping over sections which she did not understand. He also mentions Vladimir
Nabokov?s disdain for Garnett?s translations, who was known to scribble vitriolic notes in the mar-
gins of Garnett translations during his tenure as a professor at Cornell and Wellesley in the United
States. Remnick notes that children?s book author Kornei Churnosky praised her translations of Tur-
genev and Chekhov but was less than pleased with her rendering of Dosteyevsky, complaining that she
had smoothed over the erratic and challenging original text of that particular author. Thus, this work fo-
cuses on these claims of distinguishability in particular, for it is exactly these characteristics that can, in
principle, be investigated using supervised learning techniques: Is it the case that one can automatically
distinguish Garnett?s renderings of Dosteyevsky from her translations of Turgenev, and if so, based on
which textual characteristics, word distributions or individual word frequencies?
4 Corpus and methodology
The corpus was limited in these experiments to works by Dosteyevsky, Turgenev and Chekhov as these
were the three authors translated by Garnett for which the most public domain text was available. Texts
were downloaded from Project Gutenberg.
9
The final corpus consisted of eight works by Turgenev, seven
works by Dosteyevsky and eleven collections of short stories by Chekhov. A selection of random text
was made from each work matching the size of the smallest possible size of a work by each author and
this selection was then divided into chunks of ten kilobytes each. The resulting corpus contains 942
segments from the three authors, 330 from Chekhov, 192 from Turgenev and 420 from Dosteyevsky.
TagHelperTools was used to create the n-gram tokens, (Ros?e et al., 2008) and calculate nineteen docu-
ment statistics using TreeTagger, (Schmid, 1994) to tag texts for parts-of-speech. Weka, (Frank et al.,
2005) was used for the supervised learning experiments, the SMO implemenation of a Support Vector
Machine classifier along with the Naive Bayes and Simple Logistic Regression algorithms were used in
the experiments.
The eighteen document level metrics used in the experiments are listed in Table 2. These were in-
fluenced by features used by Ilisei et al. (2010) in work which examined the problem of translationese
detection in Spanish text. The two readability metrics employed are the Coleman-Liau Index, (Coleman
and Liau, 1975) and the Automated Readability Index, (Smith and Senter, 1967). The n-gram features are
calculated using TagHelper tools and the frequency of these features were reduced to a binary variable
detailing the occurrence or non-occurrence of each feature in each segment.
5 Experiments
5.1 Document-level metrics
Experiments were carried out using different feature sets on the corpus described in Section 4. The
experiments seek to classify the original author of a translated textual segment. The SVM classifier
managed to achieve 87% accuracy when averaged using ten-fold cross validation on the whole corpus
8
(nee Black)
9
www.gutenberg.org
379
Work Author Work Author
The Bishop & O. Stories Chekhov The Cook?s Wedding Chekhov
The Chorus Girl Chekhov The Darling Chekhov
The Duel Chekhov The Horse-Stealers Chekhov
The School Master Chekhov The Party Chekhov
The Wife Chekhov The Witch Chekhov
Love & O. Stories Chekhov A Raw Youth Dosteyevsky
Brothers Karamasov Dosteyevsky Crime & Punishment Dosteyevsky
The Insulted and The Injured Dosteyevsky The Possessed Dosteyevsky
White Nights Dosteyevsky Five Stories Dosteyevsky
A House of Gentlefolk Turgenev Fathers & Children Turgenev
On The Eve Turgenev Knock,Knock,Knock Turgenev
Rudin Turgenev Smoke Turgenev
The Torrents of Spring Turgenev The Jew Turgenev
Table 1: Literary works in study
Feature Desc. Feature Desc.
nounratio nouns vs. total words avgwordlength average word length
pnounratio pronouns vs. total words prepratio prepositions vs total words
lexrich lemmas vs. total words grammlex closed vs. open class
complextotal >1 verb: total sent. simple complex > 1 verb : <= 1 verb
simpletotal <= 1 verb : total sent. avgsent average sentence length
infoload open-class : total words dmarkratio discourse markers : total words
CLI readability metric fverbratio finite verbs : total words
conjratio conjunctions : total words ARI readability metric
numratio numerals : total words typetoken word types : total words
Table 2: Document-level metrics used
using document-level features only. This result suggests that the authorial style of the three authors in
question has indeed been preserved in translation.
Examining the features ranked by information gain in Table 3, it is clear that the ratio of nouns to total
words and the ratio of pronouns to total words are highly distinguishing between the original authors.
Ratio of prepositions to total words and the type-token ratio also feature in more elevated positions on
the list than readability scores and sentence length measures.
5.2 N-gram features
The next set of experiments concerned the use of n-gram features, namely word unigram and POS bi-
grams. For the word features, all noun features were removed as these, while providing clues to the
identity of the author of a translation, are arguably not universal features of authorial style
10
. Verb
features were not removed in such a fashion, however it may be argued that these also contain topical
information and should be treated with caution. The remaining features were ranked by efficacy using
the information gain metric and ten-fold cross validation and a subset of one hundred features were used
for the classification experiments.
The SVM classifier in Weka with a linear kernel obtained 89.5% accuracy using a dataset of 100
words. The Simple Logistic regression classifier obtained 91.5% accuracy using the same feature set.
This feature set was obtained by ranking the total list of word unigrams using information gain over ten-
fold cross validation and removing the noun features as mentioned above. These high accuracy scores
10
There is interest in lexical variation in translation, (Kenny, 2001) but this work focuses on stylistic features such as verbs
and closed-class words as they are less prone to bias from the themes or topics in a text
380
obtained further reinforce the results obtained by using the document-level metrics, that a distinct textual
style is learnable from the translations by Garnett of Dosteyevsky, Tolstoy and Turgenev. A number of
these features and their relative frequencies are displayed in Table 6.
Feature Rank. Feature Rank.
nounratio 1 avgwordlength 2
pnounratio 3 prepratio 4
typetoken 5 lexrich 6
simpletotal 7 simplecomplex 8
complextotal 9 grammlex 10
avgsent 11 infoload 12
cli 13 fverbratio 14
numratio 15 ari 16
conjratio 17 dmarkratio 18
Table 3: Metrics ranked using information gain and ten-fold cross validation
Feature set Algorithm. Accuracy
18 doc metrics SVM 87%
18 doc metrics Naive Bayes 74.2%
18 doc metrics Naive Bayes 87.89%
100 words SVM 89.5%
100 words SimpLog 91.5%
1021 POS bigrams SVM 83 %
1021 POS bigrams SimpLog 78.98%
1021 POS bigrams Naive Bayes 80%
1153 mix SVM 95%
1153 mix SVM 94.6 %
1153 mix SimpLog 95%
Table 4: Accuracy overview
Using the 1021 unique POS bigrams which are present in the corpus as features, 83% classification
accuracy was obtained using the SVM classifier, with Naive Bayes and Simple Logistic Regression
managing 80% and 78.98% respectively.
5.3 Combined feature sets
Combining the feature sets from each of the experiments above, accuracy is improved. SVM obtains
95% accuracy, Naive Bayes and Simple Logistic Regression manage 94.6% and 95% respectively. This
combined set contains 1153 features, 1021 POS bigrams, one hundred words and eighteen document
level features. Ranking these features using ten-fold cross validation and Information Gain, the ranking
displayed in Table 5 is obtained. Word unigrams and document-level features dominate the top fifty
ranked features, with a number of POS-bigrams also occurring in the list.
6 Discussion
Tables 6 and 7 reflect the individual characteristics of each of the three authorial subcorpora examined
here. The translations of Turgenev are distinguished by the higher average frequencies of the verbs
observed, repeated and replied. Taking the value of the document-level metrics into account, Turgenev
is to some extent unremarkable by these measures, although his works report higher average values for
the two readability metrics, CLI and ARI, than the other two authors. The translations of Dosteyevsky
distinguish themselves by the higher frequencies of adverbial forms such as almost and perhaps, which
381
Feature Rank. Feature Rank. Feature Rank Feature Rank
prepratio 1 pnounratio 2 nounratio 3 almost 4
avgwordlength 5 observed 6 simplecomplex 7 complextotal 8
simpletotal 9 replied 10 repeated 11 near 12
smell 13 perhaps 14 avgsent 15 big 16
cried 17 added 18 sigh 19 rather 20
however 21 dark 22 purpose 23 sighed 24
certain 25 typetoken 26 lexrich 27 fact 28
few 29 eat 30 certainly 31 slowly 32
moment 33 cli 34 black 35 remarked 36
BOL VBG 37 simply 38 ll 39 contrary 40
idea 41 quite 42 drank 43 CC NNS 44
FW NNP 45 NNP RB 46 ah 47 high 48
ate 49 believe 50 slightly 51 infoload 52
Table 5: Mixed feature set ranked using information gain and ten-fold cross validation
Author almost near observed perhaps repeated replied smell
Chekhov 0.000247 0.000574 0.000041 0.000289 0.000168 0.000013 0.000226
Dosteyevsky 0.000958 0.000244 0.000223 0.001107 0.000168 0.000042 0.000026
Turgenev 0.000741 0.000497 0.000508 0.000437 0.000592 0.000373 0.000070
Author added big cry dark however rather sigh
Chekhov 0.000091 0.000569 0.000361 0.000875 0.000109 0.000146 0.000757
Dosteyevsky 0.000335 0.000131 0.000339 0.000285 0.000374 0.000446 0.000230
Turgenev 0.000530 0.000171 0.000198 0.000565 0.000462 0.000538 0.000483
Author certain certainly feat fact few sighed slowly
Chekhov 0.000225 0.000114 0.003798 0.000482 0.000108 0.000240 0.000199
Dosteyevsky 0.000763 0.000323 0.003169 0.000909 0.000203 0.000022 0.000077
Turgenev 0.000576 0.000322 0.004093 0.000507 0.000446 0.000105 0.000309
Author I?ll black contrary idea moment remarked simply
Chekhov 0.014557 0.000450 0.000035 0.000312 0.000378 0.000001 0.000263
Dosteyevsky 0.013233 0.000170 0.000205 0.000806 0.000999 0.000014 0.000701
Turgenev 0.016007 0.000351 0.000093 0.000421 0.000355 0.000140 0.000342
Table 6: Relative frequencies for distinguishing words by author: Max values in bold
reflect uncertainty, but also adverbial forms such as certain, certainly and simply. They report a high
average word length, and both a lower ratio of nouns to total words and lexical richness measure than the
other two texts. They are not particular distinguished by their frequencies of verbal usage. The Chekhov
translations are distinguishable by higher frequencies of near and smell, coupled with a lower average
sentence length
11
and lower ratios of pronouns and prepositions to total words respectively. The three
sentence type metrics are also distinctive. Perhaps the genre of the corpus has an effect here, as all of
the included works by Chekhov are short stories while contributions from the other authors are primarily
novels and novellas. Temporal variation or development of translatorial style may also play a role in any
distinction, Garnett first began translating Turgenev in the late 19th century, followed by Dosteyevsky
and Chekhov in the early 20th century, and it is probable that her knowledge of Russian and own writing
style in English may have evolved over these years.
Reporting verbs
12
have been examined by Winters (2007), Mikhailov and Villikka (2001) and Baker
(2000) in their work on finding distinguishing features of parallel translations of the same text. Here they
11
Just over fifteen words, compared with over eighteen words for the other two authors.
12
Observed, repeated, replied can be considered part of this category.
382
Author Chekhov Turgenev Dosteyevsky
Attribute Mean StdDev Mean StdDev Mean StdDev
grammlex 0.6553 0.0419 0.6388 0.0518 0.6849 0.0664
infoload 0.4482 0.0176 0.455 0.0237 0.4509 0.027
avgsent 15.8881 5.6812 18.6987 5.2877 18.1451 6.6252
nounratio 0.1759 0.0176 0.1723 0.0239 0.1522 0.0287
fverbratio 0.0903 0.0083 0.0942 0.0093 0.0943 0.01
pnounratio 0.1047 0.017 0.1184 0.0176 0.1278 0.0224
prepratio 0.0423 0.0071 0.0336 0.0057 0.0354 0.0068
conjratio 0.0913 0.0116 0.0867 0.011 0.0917 0.0148
numratio 0.0065 0.0025 0.0048 0.0021 0.0065 0.0036
typetoken 0.2954 0.0219 0.3007 0.0297 0.2758 0.031
avgwordlength 12.4996 0.6329 12.4417 0.7341 13.4928 1.0065
cli 3.8567 3.3722 5.4318 3.3074 5.0254 4.0703
ari 5.8797 0.9478 6.1201 1.1031 5.9542 1.2986
lexrich 0.2567 0.021 0.2586 0.0271 0.2372 0.0303
simplecomplex 2.0079 1.3585 1.278 0.526 1.3952 0.6163
dmarkratio 0.0011 0.0008 0.0015 0.0008 0.0012 0.0009
complextotal 3.0075 1.3584 2.278 0.526 2.3951 0.6162
simpletotal 1.7428 0.5121 1.9679 0.6037 1.9226 0.6496
Table 7: Mean and standard deviation per author: document metrics
occur as distinguishing features of authorial idiolects within works by the same translator. Of course, the
efficacy of these features may be increased in these experiments as a result of eliminating noun features,
although this was done in an attempt to mitigate the effect of topic based classification of the works of
a particular author, and focus on features which represent deeper stylistic patterns. Further analyses of
these phenomena must consult the nature of the source text, investigating to what degree of accuracy can
the original works of each author be distinguished from one another.
7 Conclusions and Future Directions
This study has demonstrated the efficacy of supervised learning techniques as applied to the task of
distinguishing authorial style in a literary corpus translated from Russian to English by a single translator.
Both document metrics and n-gram features perform very well for this task, obtaining accuracies of over
80% using feature sets from each category. Combined feature sets improved performance, resulting in
95% classification accuracy between the three authors in question. Highly ranked features included the
ratio of nouns to total words, the ratio of pronouns to total words and the ratios of prepositions to total
words, also adverbs and reporting verbs such as almost, observed, replied and repeated and near. These
results imply that in this case there is indeed a clear preservation of the individual authorial style by the
translator in question, which to some extent refutes the claims of stylistic similarity or sameness across
this particular translator?s canon.
13
, and supports the theory of a translator?s invisibility as claimed by
Venuti (1995). One aspect of the problem not focused on in this study is the relationship between the
source and target text, and it is of interest in future work to investigate to what degree the stylistic
shifts in translator?s style reflect the original source text, or does the translator in fact create their own
defined idiolect for a particular author? Further work may investigate how Garnett?s style is distinct from
another translator, there is evidence of stylistic differences existing between authors, and also between
translators, with different features proving discriminating in both cases, as found in studies by Forsyth
and Lam (2013) and Lynch (2013).
Future work on this topic will encompass a wider range of translators and languages in order to inves-
13
Comments by Vladimir Nabokov and others as refered to by Remnick (2005).
383
tigate more general patterns in translated literature. Results using relatively shallow linguistic features
such as POS n-grams and word class distributions have proven themselves useful in distinguishing au-
thorial variation in a translator?s style, however it is also of interest to apply deeper linguistic processing
to these texts in order to investigate more fine-grained elements of authorial and translatorial style within
text. Examples of technologies which could be applied include semantic role labeling, (Swier and Steven-
son, 2004) deep syntactic parsing, (Lucic and Blake, 2011), and LDA for detecting levels of metaphor
(Heintz et al., 2013), in order to obtain a clearer picture of the stylistic structure of such documents.
Acknowledgements
The Centre for Applied Data Analytics Research is an Enterprise Ireland and IDA initiative. Many thanks
to Dr. Daniel Isemann at Universit?at Leipzig for comments on an early draft of this work and to Prof.
Carl Vogel at Trinity College Dublin who provided guidance, inspiration and extensive comments on
previous studies in this space.
References
M. Baker. 2000. Towards a methodology for investigating the style of a literary translator. Target, 12(2):241?266.
M. Baroni and S. Bernardini. 2006. A new approach to the study of translationese: Machine-learning the differ-
ence between original and translated text. Literary and Linguistic Computing, 21(3):259.
L. Borin and K. Pruetz. 2001. Through a glass darkly: Part-of-speech distribution in original and translated text.
Language and Computers, 37(1):30?44.
J. Burrows. 2002. The Englishing of Juvenal: computational stylistics and translated texts. Style, 36(4):677?699.
Meri Coleman and TL Liau. 1975. A computer readability formula designed for machine scoring. Journal of
Applied Psychology, 60(2):283.
Mariano Felice and Lucia Specia. 2012. Linguistic features for quality estimation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages 96?103. Association for Computational Linguistics.
Richard S. Forsyth and Phoenix W. Y. Lam. 2013. Found in translation: To what extent is authorial discriminability
preserved by translators? Literary and Linguistic Computing.
E. Frank, M.A. Hall, G. Holmes, R. Kirkby, B. Pfahringer, and I.H. Witten. 2005. Weka: A machine learn-
ing workbench for data mining. Data Mining and Knowledge Discovery Handbook: A Complete Guide for
Practitioners and Researchers, pages 1305?1314.
Michael Gamon. 2004. Sentiment classification on customer feedback data: noisy data, large feature vectors, and
the role of linguistic analysis. In Proceedings of the 20th international conference on Computational Linguistics,
page 841. Association for Computational Linguistics.
Sunita Goel and Jagdish Gangolly. 2012. Beyond the numbers: Mining the annual reports for hidden cues indica-
tive of financial statement fraud. Intelligent Systems in Accounting, Finance and Management, 19(2):75?89.
Ilana Heintz, Ryan Gabbard, Mahesh Srinivasan, David Barner, Donald S Black, Marjorie Freedman, and Ralph
Weischedel. 2013. Automatic extraction of linguistic metaphor with lda topic modeling. Meta4NLP 2013,
page 58.
I. Ilisei and D. Inkpen. 2011. Translationese traits in romanian newspapers: A machine learning approach.
International Journal of Computational Linguistics and Applications.
I. Ilisei, D. Inkpen, G. Corpas Pastor, and R. Mitkov. 2010. Identification of Translationese: A Machine Learning
Approach. Computational Linguistics and Intelligent Text Processing, pages 503?511.
Dorothy Kenny. 2001. Lexis and creativity in translation: a corpus-based study. St Jerome Pub.
M. Koppel and N. Ordan. 2011. Translationese and its dialects. 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Technologies, Portland, OR, USA.
D. Kurokawa, C. Goutte, and P. Isabelle. 2009. Automatic Detection of Translated Text and its Impact on Machine
Translation. In Proceedings of the XII MT Summit,Ottawa, Ontario, Canada. AMTA.
384
G. Lembersky, N. Ordan, and S. Wintner. 2011. Language Models for Machine Translation: Original vs. Trans-
lated Texts. Empirical Methods in Natural Language Processing, Edinburgh, Scotland, 2011.
D. Li, C. Zhang, and K. Liu. 2011. Translation style and ideology: a corpus-assisted analysis of two english
translations of hongloumeng. Literary and Linguistic Computing, 26(2):153.
Ana Lucic and Catherine Blake. 2011. Comparing the similarities and differences between two translations. In
Digital Humanities 2011, page 174. ALLC.
Gerard Lynch and Carl Vogel. 2009. Chasing the ghosts of ibsen: A computational stylistic analysis of drama
in translation. In Digital Humanities 2009: University of Maryland, College Park, MD, USA, page 192.
ALLC/ACH.
Gerard Lynch and Carl Vogel. 2012. Towards the automatic detection of the source language of a literary trans-
lation. In Martin Kay and Christian Boitet, editors, COLING (Posters), pages 775?784. Indian Institute of
Technology Bombay.
Gerard Lynch. 2013. Identifying Translation Effects in English Natural Language Text. Ph.D. thesis, Trinity
College Dublin.
F. Mairesse and M. Walker. 2008. Trainable generation of big-five personality styles through data-driven parameter
estimation. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL),
pages 165?173.
M. Mikhailov and M. Villikka. 2001. Is there such a thing as a translators style? In Proceedings of Corpus
Linguistics 2001, Lancaster, UK, pages 378?385.
Charles A Moser. 1988. Translation: The achievement of constance garnett. The American Scholar, pages 431?
438.
M. Popescu. 2011. Studying translationese at the character level. In Proceedings of the 8th International Confer-
ence on Recent Advances in Natural Language Processing (RANLP?2011). Hissar, Bulgaria.
David Remnick. 2005. The translation wars. The New Yorker, 7:98?109.
Carolyn Ros?e, Yi-Chia Wang, Yue Cui, Jaime Arguello, Karsten Stegmann, Armin Weinberger, and Frank Fischer.
2008. Analyzing collaborative learning processes automatically: Exploiting the advances of computational
linguistics in computer-supported collaborative learning. International journal of computer-supported collabo-
rative learning, 3(3):237?271.
Jan Rybicki and Magda Heydel. 2013. The stylistics and stylometry of collaborative translation: Woolfs night and
day in polish. Literary and Linguistic Computing, 28(4):708?717.
J. Rybicki. 2006. Burrowing into Translation: Character Idiolects in Henryk Sienkiewicz?s Trilogy and its Two
English Translations. Literary and Linguistic Computing, 21(1):91?103.
J. Rybicki. 2012. The great mystery of the (almost) invisible translator. Quantitative Methods in Corpus-Based
Translation Studies: A Practical Guide to Descriptive Translation Research, page 231.
H. Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of international
conference on new methods in language processing, volume 12, pages 44?49. Manchester, UK.
EA Smith and RJ Senter. 1967. Automated readability index. AMRL-TR. Aerospace Medical Research Laborato-
ries (6570th), page 1.
Robert S Swier and Suzanne Stevenson. 2004. Unsupervised semantic role labelling. In Proceedings of EMNLP,
volume 95, page 102.
Dominik Vajn. 2009. Two-dimensional theory of style in translations: an investigation into the style of literary
translations. Ph.D. thesis, University of Birmingham.
H. van Halteren. 2008. Source Language Markers in EUROPARL Translations. In Proceedings of the 22nd
International Conference on Computational Linguistics (Coling 2008), pages 937?944. Coling 2008 Organizing
Committee.
L. Venuti. 1995. The translator?s invisibility: A history of translation. Routledge.
385
Q. Wang and D. Li. 2012. Looking for translator?s fingerprints: a corpus-based study on Chinese translations of
Ulysses. Literary and Linguistic Computing.
Marion Winters. 2007. F. scott fitzgerald?s die sch?onen und verdammten: A corpus-based study of speech-act
report verbs as a feature of translators? style. Meta: Journal des traducteurs, 52(3).
386
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 257?262,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
A Naive Bayes classifier for automatic correction of preposition
and determiner errors in ESL text
Gerard Lynch, Erwan Moreau and Carl Vogel
Centre for Next Generation Localisation
Integrated Language Technology Group
School of Computer Science and Statistics
Trinity College Dublin, Ireland
gplynch,moreaue,vogel@scss.tcd.ie
Abstract
This is the report for the CNGL ILT team en-
try to the HOO 2012 shared task. A Naive-
Bayes-based classifier was used in the task
which involved error detection and correction
in ESL exam scripts. The features we use in-
clude n-grams of words and POS tags together
with features based on the external Google N-
Grams corpus. Our system placed 11th out
of 14 teams for the detection and recognition
tasks and 11th out of 13 teams for the correc-
tion task based on F-score for both preposition
and determiner errors.
1 Introduction
The HOO 2012 shared task seeks to apply compu-
tational methods to the correction of certain types
of errors in non-native English texts. The previous
year?s task, (Dale and Kilgarriff, 2011), focused on
a larger scale of errors and a corpus of academic ar-
ticles. This year?s task focuses on six error types in a
corpus of non-native speaker text. The scope of the
errors is as follows:1
Error Code Description Example
RT Replace Preposition When I arrived at London
MT Missing preposition I gave it John
UT Unnecessary preposition I told to John that
RD Replace determiner Have the nice day
MD Missing determiner I have car
UD Unnecessary determiner There was a lot of the traffic
Table 1: Error types for HOO 2012 Shared Task
In Section 2, we give a brief summary of the data
for the shared task and in Section 3 we explain the
1http://correcttext.org/hoo2012/
errortypes.html last verified, May 10, 2012
individual steps in the system. Section 4 details the
different configurations for each of the runs submit-
ted and finally, Section 5 presents the results.
2 Training data
The training data for this shared task has been pro-
vided by Cambridge University Press and consists of
scripts from students sitting the Cambridge ESOL
First Certificate in English (FCE) exams. The top-
ics of the texts are comparable as they have been
drawn from two consecutive exam years. The data is
provided in XML format and contains 1000 original
exam scripts, together with a standoff file containing
edits of the type described in Section 1 above, also
in XML format. These edits consist of offset infor-
mation, edit type information and before and after
text for correction. The results for the shared task
were presented in this format.
The test data consists of 100 exam scripts drawn
from a new corpus of exam scripts.
Some extra metadata is present in the source files,
including information about the student?s mother
tongue and the age-range of the student, however the
mother tongue data is not present in the test set.
3 Approach
The approach we have chosen for this task involves
the use of supervised machine-learning algorithms
in a four-part classification task.
3.1 Overview of the system
The first part of the task involves identification of
edits in the training data, perhaps the most challeng-
257
ing given the large imbalance of edits vs non-edits
in the data.
The next step concerns classification of edits into
the six types described above, and the final task
involves correction of edits, replacing or adding
prepositions and determiners, and possibly in some
cases removal of same.
There is a fourth step involved which reassesses
the classification and correction based on some sim-
ple heuristics, using POS tags of the head word of
each instance. If the headword is not a preposition
and the system has marked a replace preposition er-
ror at that position, this error will be removed from
the system. Likewise when the headword is not a
determiner and a replace determiner error has been
marked. If the replacement suggested is the same
as the original text (in some cases this occurs), the
edit is also removed. Another case for removal in
this fashion includes an error type involving a miss-
ing determiner error where the head word is neither
a noun or an adjective. In some cases the system
reported and corrected an error suggesting the same
text as was originally there, i.e no change. These
cases are also removed from the end result.
3.2 Classification
We utilise the freely available Weka machine learn-
ing toolkit (Hall et al, 2009), and the algorithm used
for classification in each step is Naive Bayes.
3.2.1 Representing the data
We represent each word in the training data as a
vector of features. There are 39 basic features used
in the detection process, and 42 in the classification
and training step. The first 7 features contain in-
formation which is not used for classification but is
used to create the edit structures, such as start offset,
end offset, native language, age group and source
filename and part information. These features in-
clude the current word plus the four preceding and
following words, POS and spell-checked versions of
each, together with bigrams of the two following and
two preceding words with spell-checked and POS
versions for these. Information on speaker age and
native language is also included although native lan-
guage information is not present in the test set.
3.2.2 Additional processing
All tokens have been lower-cased and punctuation
has been removed. POS information for each token
has been added. The open-source POS tagger from
the OpenNLP tools package (OpenNLP, 2012) has
been used to this end. Spell correction facility has
been provided using the basic spellchecker in the
Lucene information retrieval API(Gospodnetic and
Hatcher, 2005) and the top match string as provided
by this spell correcting software is used in addition
to each feature. The basic maximum entropy model
for English is used for the POS tagger.
We had also planned to include features based
on the Google Books n-gram corpus, (Michel et al,
2011) which is freely available on the web, but un-
fortunately did not get to include them in the ver-
sion submitted due to errors which were found in the
scripts for generating the features late in the process.
Nevertheless, we describe these features in Section
3.3 and present some cross-validation results from
the training data for the detection step in Section 5.1.
3.3 Google N-grams Features
3.3.1 Motivation
The Google Books N-Grams2 is a collection of
datasets which consist of all the sequences of words
(n-grams) extracted from millions of books (Michel
et al, 2011). The ?English Million? dataset contains
more more than 500 millions distinct n-grams3, from
size 1 to 5. for every n-gram, its frequency, page
frequency (number of pages containing it) and book
frequency (number of books containing it) are pro-
vided.
In this Shared Task, we aim to use the Google N-
grams as a reference corpus to help detecting the
errors in the input. The intuition is the following:
if an error occurs, comparing the frequency of the
input n-grams against the frequency of other possi-
bilities in the Google N-grams data might provide
useful indication on the location/type of the error.
For example, given the input ?I had to go in a li-
brary?, The Google N-grams contain only 36,716
occurrences of the trigram ?go in a?, but 244,098
occurrences of ?go to a?, which indicates that the
latter is more likely.
2http://books.google.com/ngrams/datasets
3The least frequent n-grams were discarded.
258
However there are several difficulties in using
such a dataset:
? Technical limitations. Extracting information
from the dataset can take a lot of time because
of the size of the data, thus the range of ap-
proaches is restricted by efficiency constraints.
? Quality of the data. The Google N-grams were
extracted automatically using OCR, which
means that the dataset can contain errors or un-
expected data (for example, the English dataset
contains a significant number of non-English
words).
This is why the Google N-grams must be used
cautiously, and only as an indication among others.
3.3.2 Method
Our goal is to add features extracted from the
Google N-grams dataset to the features described
above, and feed the supervised classification process
with these. Before computing the features, a list L
of ?target expressions? is extracted from the train-
ing data, which contains all the words or sequences
of words (determiners and prepositions) which oc-
cur in a correction. Then, given an input sentence
A1 . . . Am and a position n in this sentence, two
types of information are extracted from the Google
data:
? Specific indications of whether an error exists
at this position:
1. No change: the frequency of the input se-
quence An?1An and An?1AnAn+1 ;
2. Unnecessary word(s): the frequency of the
sequence An?1An+1 if A ? L;
3. Missing word(s): the frequency of the se-
quence XAn (resp. An?1XAn for tri-
grams) for any target expression X ? L;
4. Replacement: if A ? L, the frequency of
XAn+1 (resp. An?1XAn+1 for trigrams)
for any target expression X ? L;
? Generic indications taking the context into ac-
count: for length N from 1 to 5 in a window
An?4 . . . An+4, 16 combinations are computed
based only on the fact the n-grams appear in the
Google data; for example, one of these combi-
nations is the normalized sum for the 4 5-grams
in this window of 0 or 1 (the n-gram occurs or
does not).
Additionally, several variants are considered:
? bigrams or trigrams for ?specific? features;
? binary values for ?specific? features: 1 if the
n-gram appears, 0 otherwise;
? keep only the ?generic? features and the first
three features.
4 Run configurations
Ten runs were submitted to the organisers based on
different configurations. Modification of the data
was carried out using both instance reduction and
feature selection techniques. The system facilitated
the use of different training data for each of the three
main classification steps.
4.1 Least frequent words filter
Before classification, the data is preprocessed by re-
placing all the least frequent words with a default
value (actually treated as missing values by the clas-
sifier). This is intended to help the classifier focus
on the most relevant indications and to prevent over-
specification of the classification model.
4.2 Instance reduction filters
4.2.1 POSTrigrams filter
The POS trigrams filter works as follows: during
the training stage, the sequences of POS tags for the
words current-1.current.current+1 are extracted for
each instance, together with its corresponding class.
Every POS trigram is then associated with the fol-
lowing ratio:
Frequency of true instances
Frequency of false instances
Then, when predicting the class, the filter is applied
before running the classifier: the sequences of tri-
grams are extracted for each instance, and are com-
pared against the corresponding ratio observed dur-
ing the training stage; the instance is filtered out if
the ratio is lower than some threshold N%. In Table
259
Run Detection Classification Correction
0 R1 Normal Normal
1 R20 Normal Normal
2 Full F12 Normal
3 R10 Normal Normal
4 R30 Normal Normal
5 F12 F12 Normal
6 R4new Normal Normal
7 R4 + F12 F12 Normal
8 R4 Normal Normal
9 R2 Normal Normal
Table 2: Run configurations
2, the label RN refers to the percentage (N) used as
cut-off in the experiments.
This filter is intended to reduce the impact of the
fact that the classes are strongly unbalanced. It per-
mits discarding a high number of false instances,
while removing only a small number of true in-
stances. However, as a side effect, it can cause the
classifier to miss some clues which were in the dis-
carded instances.
4.2.2 CurrentPlusOrMinusOne filter
The current plusorminus one filter works as fol-
lows: A list of all current.current+1 word bigrams
is made from the error instances in the training data,
along with all current-1.current bigrams. The non-
error instances in the training data are then filtered
based on whether an instance contains an occur-
rence of any current.current+1 or current-1.current
bigram in the list.
4.3 Feature selection filters
4.3.1 F12
During preliminary experiments, selecting a sub-
set of 12 features produced classification accuracy
gains in the detection and classification steps of the
process using ten-fold cross validation on the train-
ing set. These twelve features were: current, cur-
rent+1.current+2, current-1.current-2, currentSC,
currentPOS, current-1, current-2, current+1, cur-
rent+2, current+1SC, and current-1SC. The SC
postfix refers to the spell-corrected token, with POS
referring to the part-of-speech tag. The F12 config-
uration filter removes all other features except these.
5 Results
Table 3 displays the results for both preposition and
determiner errors which were obtained by the sys-
tem on the preliminary test set before teams sub-
mitted their revisions. Table 4 refers to the results
obtained by the system after the revised errors were
removed/edited.
Task Rank Run Precision Recall F-Score
Detection 11 9 5.33 25.61 8.82
Recognition 11 9 4.18 20.09 6.92
Correction 11 9 2.66 12.8 4.41
Table 3: Overall results on original data: TC
Task Rank Run Precision Recall F-Score
Detection 11 8 6.56 26.0 10.48
Recognition 11 8 4.91 19.45 7.84
Correction 11 8 3.09 12.26 4.94
Table 4: Overall results on revised data: TC
5.1 Some detailed results (detection)
The results reported here were obtained on the train-
ing data only, using 5-fold cross-validation, and only
for the detection task. We have studied various set-
tings for the parameters; figure 1 shows a global
overview of the performance depending on several
parameters (we show only a few different values in
order to keep the graph readable).
The results show that the Google features con-
tribute positively to the performance, but only
slightly: the F1 score is 0.6% better on average. This
overview also hides the fact that some combinations
of values work better together; for instance, contrary
to the fact that not filtering the POS trigrams per-
Run3 Recall Precision F
Detection 9.05 7.42 8.15
Correction 4.19 3.44 3.78
Recognition 9.05 7.42 8.15
Run8 Recall Precision F
Detection 22.51 5.44 8.76
Correction 11.25 2.72 4.38
Recognition 22.51 5.44 8.76
Run9 Recall Precision F
Detection 25.61 5.33 8.82
Correction 12.80 2.66 4.41
Recognition 20.09 4.18 6.92
Table 5: Top results on original test data
260
Figure 1: Average F-score depending on several parameters.
10
11
12
13
14
15
16
mea
n of 
f1
20
50
100
500
1000
POS?trigrams.0POS?trigrams.1
POS?trigrams.10
POS?trigrams.3 2?3?binary2?binary3 i r
none
window0
window2
window4
factor(minFreq) filter googleFeatures attributes
forms better on average, the best performances are
obtained when filtering, as shown in figure 2.
Figure 2: F-score (%) w.r.t POS trigrams filter threshold.
Parameters: window 2, Google features with bigrams and
trigrams.
0 2 4 6 8 10
0
5
10
15
20
filter threshold
f1 sc
ore
min. frequency 20min. frequency 50min. frequency 100min. frequency 500min. frequency 1000
? Minimum frequency4 (preprocessing, see 4.1).
4Remark: the values used as ?minimum frequencies? re-
ported in this paper can seem unusually high. This is due to
the fact that, for technical reasons, the thresholds were applied
globally to the data after it had been formatted as individual in-
stances, each instance containing a context window of 9 words.
As a consequence a threshold of N means that a given word
must occur at least N/9 times in the original input data.
As shown in Figure 2, using a high threshold
helps the classifier build a better model.
? POS trigrams filter (see 4.2.1.) Even if not fil-
tering at all performs better on average, the best
cases are obtained with a low threshold. Addi-
tionally, this parameter can be used to balance
between recall and precision (when one wants
to favor one or the other).
? Size of the context window. Results can show
important differences depending on the size
of the window, but no best configuration was
found in general for this parameter.
? Google features (see 3.3.2.) The Google fea-
tures help slightly in general, and are used in
the best cases that we have obtained. How-
ever there is no significantly better approach
between using the original frequencies, simpli-
fying these to binary values, or even not using
the list of target expressions.
6 Conclusions
The task of automated error correction is a difficult
one, with the best-performing systems managing ap-
prox. 40 % F-score for the detection, recognition
and correction (Dale et al, 2012). There are several
areas where our system?s performance might be im-
proved. The spellcheck dictionary which was used
261
was a general one and this resulted in many spelling
corrections which were out of context. A more tai-
lored dictionary employing contextual awareness in-
formation could be beneficial for the preprocessing
step.
Multi-word corrections were not supported by the
system due to how the instances were constructed
and these cases were simply ignored, to the detri-
ment of the results.
In the basic feature set, the majority of features
were based on word unigrams, however more n-
gram features could improve results as these were
found to perform well during classification.
There were many different ways to exploit the
Google N-Grams features and it may be the case
that better combinations of features can be found for
each of the classification steps.
Finally, very little time was spent tuning the
datasets for the classification and correction step as
opposed to the detection phase, this is another part of
the system where fine-tuning parameters could im-
prove performance.
Acknowledgments
This material is based upon works supported by
the Science Foundation Ireland under Grant No.[SFI
07/CE/I 1142.].
References
Robert Dale and Adam Kilgarriff. 2011. Helping Our
Own: The HOO 2011 pilot shared task. In Proceed-
ings of the 13th European Workshop on Natural Lan-
guage Generation, Dublin, Ireland.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. HOO 2012: A Report on the Preposition and
Determiner Error Correction Shared Task. In Pro-
ceedings of the Seventh Workshop on Innovative Use
of NLP for Building Educational Applications, Mon-
treal, Canada.
O. Gospodnetic and E. Hatcher. 2005. Lucene. Man-
ning.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I.H. Witten. 2009. The WEKA data min-
ing software: an update. ACM SIGKDD Explorations
Newsletter, 11(1):10?18.
J.B. Michel, Y.K. Shen, A.P. Aiden, A. Veres, M.K.
Gray, J.P. Pickett, D. Hoiberg, D. Clancy, P. Norvig,
J. Orwant, et al 2011. Quantitative analysis of
culture using millions of digitized books. Science,
331(6014):176.
OpenNLP. 2012. Website: http://opennlp. apache. org.
262
Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 73?78,
Baltimore, Maryland, USA. June 27, 2014.
c?2014 Association for Computational Linguistics
Linguistically Informed Tweet Categorization for Online Reputation
Management
Gerard Lynch and P
?
adraig Cunningham
Centre for Applied Data Analytics Research
(CeADAR)
University College Dublin
Belfield Office Park
Dublin 4, Ireland
firstname.lastname@ucd.ie
Abstract
Determining relevant content automati-
cally is a challenging task for any ag-
gregation system. In the business intel-
ligence domain, particularly in the appli-
cation area of Online Reputation Manage-
ment, it may be desirable to label tweets
as either customer comments which de-
serve rapid attention or tweets from in-
dustry experts or sources regarding the
higher-level operations of a particular en-
tity. We present an approach using a com-
bination of linguistic and Twitter-specific
features to represent tweets and examine
the efficacy of these in distinguishing be-
tween tweets which have been labelled
using Amazon?s Mechanical Turk crowd-
sourcing platform. Features such as part-
of-speech tags and function words prove
highly effective at discriminating between
the two categories of tweet related to sev-
eral distinct entity types, with Twitter-
related metrics such as the presence of
hashtags, retweets and user mentions also
adding to classification accuracy. Accu-
racy of 86% is reported using an SVM
classifier and a mixed set of the aforemen-
tioned features on a corpus of tweets re-
lated to seven business entities.
1 Motivation
Online Reputation Management (ORM) is a grow-
ing field of interest in the domain of business in-
telligence. Companies and individuals alike are
highly interested in monitoring the opinions of
others across social and traditional media and this
information can have considerable business value
for corporate entities in particular.
1.1 Challenges
There are a number of challenges in creating an
end-to-end software solution for such purposes,
and several shared tasks have already been estab-
lished to tackle these issues
1
. The most recent
RepLab evaluation was concerned with four tasks
related to ORM, filtering, polarity for reputation,
topic detection and priority assignment. Based
on these evaluations, it is clear that although the
state of the art of topic-based filtering of tweets is
relatively accomplished (Perez-Tellez et al., 2011;
Yerva et al., 2011; Spina et al., 2013), other as-
pects of the task such as sentiment analysis and
prioritisation of tweets based on content are less
trivial and require further analysis.
Whether Twitter mentions of entities are ac-
tual customer comments or in fact represent the
views of traditional media or industry experts and
sources is an important distinction for ORM sys-
tems. With this study we investigate the degree to
which this task can be automated using supervised
learning methods.
2 Related Work
2.1 Studies on Twitter data
While the majority of research in the computa-
tional sciences on Twitter data has focused on is-
sues such as topic detection (Cataldi et al., 2010),
event detection, (Weng and Lee, 2011; Sakaki
et al., 2010), sentiment analysis, (Kouloumpis et
al., 2011), and other tasks based primarily on the
topical and/or semantic content of tweets, there
is a growing body of work which investigates
more subtle forms of information represented in
tweets, such as reputation and trustworthiness,
(O?Donovan et al., 2012), authorship attribution
(Layton et al., 2010; Bhargava et al., 2013) and
Twitter spam detection, (Benevenuto et al., 2010).
1
See (Amig?o et al., 2012) and (Amig?o et al., 2013) for
details of the RepLab series
73
These studies combine Twitter-specific and textual
features such as retweet counts, tweet lengths and
hashtag frequency, together with sentence-length,
character n-grams and punctuation counts.
2.2 Studies on non-Twitter data
The textual features used in our work such
as n-grams of words and parts-of-speech have
been used for gender-based language classifica-
tion (Koppel et al., 2002), social profiling and per-
sonality type detection (Mairesse et al., 2007), na-
tive language detection from L2 text, (Brooke and
Hirst, 2012) translation source language detection,
(van Halteren, 2008; Lynch and Vogel, 2012) and
translation quality detection, (Vogel et al., 2013).
3 Experimental setup and corpus
Tweets were gathered between June 2013 and Jan-
uary 2014 using the twitter4j Java library. A lan-
guage detector was used to filter only English-
language tweets.
2
The criteria for inclusion were
that the entity name was present in the tweet. The
entities focused on in this study had relatively un-
ambigious business names, so no complex filtering
was necessary.
3.1 Pilot study
A smaller pilot study was carried out before the
main study in order to examine response quality
and accuracy of instruction. Two hundred sam-
ple tweets concerning two airlines
3
were anno-
tated using Amazon?s Mechanical Turk system by
fourteen Master annotators. After annotation, we
selected the subset (72%) of tweets for which both
annotators agreed on the category to train the clas-
sifier. During the pilot study, the tweets were
pre-processed
4
to remove @ and # symbols and
punctuation to treat account names and hashtags
as words. Hyperlinks representations were main-
tained within the tweets. The Twitter-specific met-
rics were not employed in the pilot study.
3.2 Full study
In the full study, 2454 tweets concerning seven
business entities
5
were tagged by forty annota-
tors as to whether they corresponded to one of the
2
A small amount of non-English tweets were found in the
dataset, these were assigned to the Other category.
3
Aer Lingus and Ryanair
4
This was not done in the full study, these symbols were
counted and used as features.
5
Aer Lingus, Ryanair, Bank of Ireland, C & C Group,
Permanent TSB, Glanbia, Greencore
three categories described in Section 1.1. For 57%
of the tweets, annotators agreed on the categories
with disagreement in the remaining 43%. The dis-
puted tweets were annotated again by two anno-
tators. From this batch, a similar proportion were
agreed on. For the non-agreed tweets in the sec-
ond round, a majority category vote was reached
by combining the four annotations over the first
and second rounds. After this process, roughly
two hundred tweets remained as ambiguous (each
having two annotations for one of two particular
categories) and these were removed from the cor-
pus used in the experiments.
3.3 Category breakdown
Table 5 displays the number of tweets for which
no majority category agreement was reached. The
majority disagreement class across all entities are
texts which have been labelled as both business
operations and other. For the airline entities, a
large proportion of tweets were annotated as both
customer comment and other, this appeared to be
a categorical issue which may have required clar-
ification in the instructions. The smallest cate-
gory for tied agreement is customer comment and
business operations, it appears that the distinc-
tion between these categories was clearer based
on the data provided to annotators. 2078 tweets
were used in the final experiments. The classes
were somewhat imbalanced for the final corpus,
the business operations category was the largest,
with 1184 examples, customer comments con-
tained 585 examples and the other category con-
tained 309 examples.
3.4 Feature types
The features used for classification purposes can
be divided into the following two categories:
1. Twitter-specific:
? Tweet is a retweet or not
? Tweet contains a mention
? Tweet contains a hashtag or a link
? Weight measure (See Fig 3)
? Retweet account for a tweet.
2. Linguistic: The linguistic features are based
on the textual content of the tweet repre-
sented as word unigrams, word bigrams and
part-of-speech bigrams.
74
We used TagHelperTools, (Ros?e et al., 2008) for
textual feature creation which utilises the Stanford
NLP toolkit for NLP annotation and returns for-
matted representations of textual features which
can be employed in the Weka toolkit which imple-
ments various machine learning algorithms. All
linguistic feature frequencies were binarised in our
representations
6
.
4 Results
4.1 Pilot study
Using the Naive Bayes classifier in the Weka
toolkit and a feature set consisting of 130 word
tokens, 80% classification accuracy was obtained
using ten-fold cross validation on the full set of
tweets . Table 1 shows the top word features when
ranked using 10-fold cross validation and the in-
formation gain metric for classification power over
the three classes. Using the top 50 ranked POS-
bigram features alone, 74% classification accuracy
was obtained using the Naive Bayes classifier. Ta-
ble 2 shows the top twenty features, again ranked
by information gain.
Combining the fifty POS-bigrams and the 130
word features, we obtained 84% classification ac-
curacy using the Naive Bayes classifier. Accuracy
was improved by removing all noun features from
the dataset and using the top seventy five features
from the remaining set ranked with information
gain, resulting in 86.6% accuracy using the SVM
classifier with a linear kernel. Table 3 displays the
top twenty combined features.
Rank Feature Rank Feature
1 http 11 investors
2 flight 12 would
3 talks 13 by
4 for 14 says
5 strike 15 profit
6 an 16 cabin
7 you 17 crew
8 I 18 via
9 that 19 at
10 action 20 since
Table 1: Top 20 ranked word features for pilot
study
6
1 if feature is present in a tweet, otherwise 0.
Rank Feature Rank Feature
1 NNP EOL 11 VB PRP
2 VBD JJ 12 NN NNS
3 NNP VBD 13 IN PRP$
4 NNP NN 14 BOL CD
5 BOL PRP 15 BOL JJS
6 VBD NNP 16 IN VBN
7 NNP CC 17 PRP$ JJ
8 TO NNP 18 PRP MD
9 NN RB 19 PRP$ VBG
10 RB JJ 20 CC VBP
Table 2: Top 20 ranked POS bigram features for
pilot study
Rank Feature Rank Feature
1 http 11 TO NNP
2 NNP EOL 12 RB JJ
3 NNP VBD 13 that
4 VBD JJ 14 tells
5 NNP NN 15 way
6 BOL PRP 16 I
7 VBD NNP 17 would
8 NNP CC 18 you
9 for 19 NN RB
10 an 20 BOL JJS
Table 3: Top 20 ranked combined features for pilot
study
4.2 Full study
4.2.1 Results
Using the SMO classifier, Weka?s support vec-
tor machine implementation using a linear kernel,
a hybrid feature set containing linguistic, custom
and Twitter-specific features obtained 72% clas-
sification accuracy for the three categories. F-
measures were highest for the business operations
class, and lowest for the other class, which con-
tained the most diversity. Examining Figure 2, it
is clear that f-measures for the other class are al-
most zero. This indicates that tweets given this
category may not be homogeneous enough to cat-
egorise using the features defined in Table 7.
4.3 Two classes
After the removal of the other class from the
experiment, the same feature set obtained 86%
classification accuracy between the two remain-
ing classes. The distinguishing features consisted
predominantly of pronouns (I, me, my), part-of-
75
Entity BO CC Other
Aer Lingus 174 138 44
Ryanair 58 212 52
AIB 69 29 43
BOI 208 85 40
C&C 45 14 15
Glanbia 276 39 46
Greencore 37 4 13
Kerry Group 158 10 36
Permanent TSB 160 54 20
Table 4: Tweets per entity by category: Majority
agreement
Entity CC+BO O-CC O-BO
Aer Lingus 4 24 15
Ryanair 7 30 8
AIB 4 5 11
BOI 9 5 16
C&C 0 1 3
Glanbia 7 4 19
Greencore 0 0 2
Kerry Group 5 2 12
Permanent TSB 3 6 10
Table 5: Tweets per entity by category: Tied
agreement
speech bigrams including pairs of plural nouns,
lines beginning with prepositions and function
words (so, just, new, it). Business operations
tweets were more likely to mention a user account
or be a retweet, personal pronouns were more
commonplace in customer comments and as ob-
served in the pilot study, customer comments were
more likely to begin with a preposition and busi-
ness operations tweets were more likely to contain
noun-noun compounds and pairs of coordinating
conjunctions and nouns.
4.4 Features
Hashtags were slightly more common in business
operations tweets, however the number of hash-
tags was not counted, simply whether at least one
was present. Hashtags as a proportion of words
might be a useful feature for further studies. Func-
tion words and POS tags were highly discrimina-
tory, indicating that this classifier may be applica-
ble to different topic areas. Weight (See Figure 3)
was a distinguishing feature, with business opera-
tions tweets having higher weight scores, reflect-
Figure 1: F-scores by category for pilot study
Figure 2: F-scores by category for full study
ing the tendency for these tweets to originate from
Twitter accounts linked to news sources or influ-
ential industry experts.
5 Results per sub-category
To investigate whether the entity domain had a
bearing on the results, we separated the data into
three subsets, airlines, banks and food industry
concerns. We performed the same feature selec-
tion as in previous experiments, calculating each
feature type separately, removing proper nouns,
hashtags and account names from the word n-
grams, then combining and ranking the features
using ten-fold cross validation and information
gain. The SVM classifier reported similar results
to the main study on the three class problem for
each sub-domain, and for the two class problem
results ranged between 86-87% accuracy, similar
Number of followers
Number following
(retweets)
Figure 3: Twitter weight metric
76
to the results on the mixed set
7
. Thus, we be-
lieve that the individual subdomains do not war-
rant different classifiers for the problem, indeed
examining the top 20-ranked features for each sub-
domain, there is a large degree of overlap, as seen
in bold and italics in Table 6.
Banks Airlines Food
@ @ @
my NNP NNP PRP VBP
i i i
me BOL IN BOL IN
PRP VBP PRP VBP VB PRP
account DT NN BOL PRP
NNP VBZ IN PRP HASHASH
VB PRP the you
IN PRP new me
you PRP VBD know
BOL RB NNP VBZ my
RB JJ IN DT i know
NNP NNP you PRP CC
PRP VBD BOL PRP used
my bank ISRT BOL CC
DT NN it NNP CD
NN PRP me NN NNP
VBD PRP my CC PRP
BOL IN RB RB ISRT
i?m so CC NNP
Table 6: Top twenty ranked features by Informa-
tion Gain for three domains
6 Conclusions and future directions
6.1 Classification results
We found that accurate categorization of our pre-
defined tweet types was possible using shallow
linguistic features. This was aided by Twitter spe-
cific metrics but these did not add significantly to
the classification accuracy
8
. The lower score (72-
73%) in the three class categorization problem is
due to the linguistic diversity of the other tweet
category.
6.2 Annotation and Mechanical Turk
We found the definition of categorization criteria
to be an important and challenging step when us-
ing Mechanical Turk for annotation. The high de-
gree of annotator disagreement reflected this, how-
ever it is important to note that in many cases,
tweets fit equally into two or more of our defined
categories. The use of extra annotations
9
allowed
for agreement to be reached in the majority of
7
The food subset was highly imbalanced however, con-
taining only 43 customer comments and 313 business opera-
tions tweets, the other two subsets were relatively balanced.
8
ca. 2% decrease in accuracy on removal.
9
over the initial two annotators
cases, however employing more evaluations could
have also resulted in deadlock. Examples of am-
biguous tweets included: Cheap marketing tactics.
Well, if it ain?t broke, why fix it! RT @Ryanair?s
summer ?14 schedule is now on sale! where a
Twitter user has retweeted an official announce-
ment and added their own comment.
Another possible pitfall is that as Mechanical
Turk is a US-based service and requires workers to
have a US bank account in order to perform work,
Turkers tend to be US-based, and therefore an an-
notation task concerning non-US business entities
is perhaps more difficult without sufficient back-
ground awareness of the entities in question.
Future experiments will apply the methodology
developed here to a larger dataset of tweets, one
candidate would be the dataset used in the RepLab
2013 evaluation series which contains 2,200 an-
notated tweets for 61 business entities in four do-
mains.
Acknowledgments
The authors are grateful to Enterprise Ireland and
the IDA for funding this research and CeADAR
through their Technology Centre Programme.
Rank Feature Rank Feature
1 @ 26 NNP PRP
2 i 27 NN PRP
3 PRP VBP 28 VBP PRP
4 my 29 when
5 BOL IN 30 if
6 me 31 don?t
7 you 32 PRP MD
8 NNP NNP 33 they
9 IN PRP 34 like
10 VB PRP 35 PRP VB
11 PRP VBD 36 got
12 WEIGHT 37 CC NNP
13 so 38 but
14 NNP VBZ 39 RB IN
15 BOL PRP 40 RT
16 RB JJ 41 with
17 DT NN 42 PRP IN
18 BOL RB 43 a
19 it 44 NNS RB
20 PRP RB 45 CC PRP
21 RB RB 46 VBD PRP
22 IN DT 47 VBD DT
23 i?m 48 no
24 just 49 the
25 get 50 PRP$ NN
Table 7: Top 50 ranked mixed features for main
study
77
References
Enrique Amig?o, Adolfo Corujo, Julio Gonzalo, Edgar
Meij, and Maarten de Rijke. 2012. Overview
of replab 2012: Evaluating online reputation man-
agement systems. In CLEF (Online Working
Notes/Labs/Workshop).
Enrique Amig?o, Jorge Carrillo de Albornoz, Irina
Chugur, Adolfo Corujo, Julio Gonzalo, Tamara
Mart??n, Edgar Meij, Maarten de Rijke, and Dami-
ano Spina. 2013. Overview of replab 2013:
Evaluating online reputation monitoring systems.
In Information Access Evaluation. Multilinguality,
Multimodality, and Visualization, pages 333?352.
Springer.
Fabr?cio Benevenuto, Gabriel Magno, Tiago Ro-
drigues, and Virg?lio Almeida. 2010. Detect-
ing spammers on twitter. In Collaboration, elec-
tronic messaging, anti-abuse and spam conference
(CEAS), volume 6.
Mudit Bhargava, Pulkit Mehndiratta, and Krishna
Asawa. 2013. Stylometric analysis for authorship
attribution on twitter. In Big Data Analytics, pages
37?47. Springer International Publishing.
Julian Brooke and Graeme Hirst. 2012. Measuring
interlanguage: Native language identification with
l1-influence metrics. In LREC, pages 779?784.
Mario Cataldi, Luigi Di Caro, and Claudio Schifanella.
2010. Emerging topic detection on twitter based on
temporal and social terms evaluation. In Proceed-
ings of the Tenth International Workshop on Multi-
media Data Mining, page 4. ACM.
Moshe Koppel, Shlomo Argamon, and Anat Rachel
Shimoni. 2002. Automatically categorizing writ-
ten texts by author gender. Literary and Linguistic
Computing, 17(4):401?412.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the omg! In ICWSM.
Robert Layton, Paul Watters, and Richard Dazeley.
2010. Authorship attribution for twitter in 140 char-
acters or less. In Cybercrime and Trustworthy Com-
puting Workshop (CTC), 2010 Second, pages 1?8.
IEEE.
Gerard Lynch and Carl Vogel. 2012. Towards the au-
tomatic detection of the source language of a literary
translation. In COLING (Posters), pages 775?784.
Franc?ois Mairesse, Marilyn A Walker, Matthias R
Mehl, and Roger K Moore. 2007. Using linguis-
tic cues for the automatic recognition of personality
in conversation and text. J. Artif. Intell. Res.(JAIR),
30:457?500.
John O?Donovan, Byungkyu Kang, Greg Meyer, To-
bias Hollerer, and Sibel Adalii. 2012. Credibility in
context: An analysis of feature distributions in twit-
ter. In Privacy, Security, Risk and Trust (PASSAT),
2012 International Conference on and 2012 Inter-
national Confernece on Social Computing (Social-
Com), pages 293?301. IEEE.
Fernando Perez-Tellez, David Pinto, John Cardiff, and
Paolo Rosso. 2011. On the difficulty of cluster-
ing microblog texts for online reputation manage-
ment. In Proceedings of the 2nd Workshop on Com-
putational Approaches to Subjectivity and Sentiment
Analysis, pages 146?152. Association for Computa-
tional Linguistics.
Carolyn Ros?e, Yi-Chia Wang, Yue Cui, Jaime Ar-
guello, Karsten Stegmann, Armin Weinberger, and
Frank Fischer. 2008. Analyzing collaborative
learning processes automatically: Exploiting the ad-
vances of computational linguistics in computer-
supported collaborative learning. International
journal of computer-supported collaborative learn-
ing, 3(3):237?271.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes twitter users: real-time
event detection by social sensors. In Proceedings
of the 19th international conference on World wide
web, pages 851?860. ACM.
Damiano Spina, Julio Gonzalo, and Enrique Amig?o.
2013. Discovering filter keywords for company
name disambiguation in twitter. Expert Systems with
Applications.
Hans van Halteren. 2008. Source language mark-
ers in europarl translations. In Proceedings of the
22nd International Conference on Computational
Linguistics-Volume 1, pages 937?944. Association
for Computational Linguistics.
Carl Vogel, Ger Lynch, Erwan Moreau, Liliana Ma-
mani Sanchez, and Phil Ritchie. 2013. Found in
translation: Computational discovery of translation
effects. Translation Spaces, 2(1):81?104.
Jianshu Weng and Bu-Sung Lee. 2011. Event detec-
tion in twitter. In ICWSM.
Surender Reddy Yerva, Zolt?an Mikl?os, and Karl
Aberer. 2011. What have fruits to do with technol-
ogy?: the case of orange, blackberry and apple. In
Proceedings of the International Conference on Web
Intelligence, Mining and Semantics, page 48. ACM.
78
Proceedings of Third Workshop on Semantic Web and Information Extraction, pages 41?50,
Dublin, Ireland, 24 August, 2014.
Towards a robust framework for the semantic representation of temporal
expressions in cultural legacy data
Daniel Isemann
Natural Language Processing Group
Department of Computer Science
Leipzig University
lastname@informatik.
uni-leipzig.de
Gerard Lynch
Centre for Applied
Data Analytics Research
University College Dublin
Clonskeagh, Dublin 4
Ireland
firstname.lastname@ucd.ie
Raffaella Lanino
Documentation and
Digitisation
National Gallery of Ireland
Dublin
rlanino@ngi.ie
Abstract
Date and time descriptors play an important role in cultural record keeping. As part of digi-
tal access and information retrieval on heritage databases it is becoming increasingly important
that date descriptors are not matched as strings but that their semantics are properly understood
and interpreted by man and machine alike. This paper describes a prototype system designed
to resolve temporal expressions from English language cultural heritage records to ISO 8601
compatible date expressions. The architecture we advocate calls for a two stage resolution with
a ?semantic layer? between the input and ISO 8601 output. The system is inspired by a similar
system for German language records and was tested on real world data from the National Gallery
of Ireland in Dublin. Results from an evaluation with two senior art and metadata experts from
the gallery are reported.
1 Introduction
Preserving a memory of past events has been central to human culture for millennia and may even be
seen as a defining element of cultural life in general. The practice of specifying locations in time for this
purpose transcends cultural boundaries. The earliest precursors of the Chinese lunisolar calendar can be
traced back to the second millennium before Christ. In ancient Attica the ?eponymous archon? lent his
name to the year he ruled in and a similar system was employed by republican Romans. The introduction
of the Julian calendar and its Georgian reform, although haphazardly adopted, has eventually led to a
widely accepted standard for locating events in time (although alternative calendars exist and thrive to
this day). The advent of the computer age has brought with it stricter requirements for such standards, for
instance that of unambiguous machine readability. A number of such standards for encoding the meaning
or extension of temporal expressions have emerged in recent years (ISO 8601, TimeML, VRA core).
However, legacy records in the field of cultural heritage still abound with natural language descriptions
of dates and date ranges that are not expressed in a standardised form, such as "around 1660", "late
15th century", "1720-30 (?)". The non-standard nature of such expressions is compounded by inherent
uncertainty about the dates which is expressed through uncertainty markers such as "around", "(?)" or
similar. While human experts have little difficulty interpreting such expressions these are not amenable
to machine-based processing and thus are not directly useful for querying databases based on dates, for
instance. The latter purpose is much better served by date ranges with a clear beginning and end.
We argue for conceptually splitting the process of ?translating?, ?converting? or otherwise associating
informal descriptions of dates with concrete date ranges with an unambiguous beginning and end. A first
step should capture the semantics of the original expression including possible uncertainty markers with
as little loss in meaning as possible.
1
The target of this step should be a language independent ontology
or semantic standard, such as the VRA core 4.0 date element. A second step should then map from a
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1
What we mean by minimising ?loss in meaning? is essentially that in a first step uncertainty markers should be kept (albeit
in a standardized form) rather then resolved to a date range (i.e. ?c.1888?? ?c(1888)? instead of ?c.1888?? ?1878-1898?).
41
representation in the semantic standard to a date range with concrete beginning and end according to
user, institution or context specific preferences, using intelligent defaults in the absence of preferences.
In this paper we describe a prototype system designed to resolve temporal expressions from English
language cultural heritage records to ISO 8601 compatible date expressions. The system is inspired by
a similar system for German language records and was tested on real life data from the National Gallery
of Ireland in Dublin and evaluated by two senior art and metadata experts from the gallery. The default
rules for converting the ?meaning? of date expressions to date ranges were found to be superior to the
heuristics currently configured by the National Gallery in their collection management system.
2 Background
The National Gallery of Ireland (NGI) has developed a set of in-house standards for cataloguing date
expressions related to the creation of artworks (Appendix A and B). These standards complement the
editorial guidelines outlined in the NGI house style guide for works of art in the collection and they must
be applied when entering the data into the relevant field on TMS
2
(The Museum System), the collection
management system used by the Gallery. These guidelines have been created based on best practice
standards for cataloguing date expressions. There are several authoritative resources that institutions can
consult to draft their own in house cataloguing standards, including date format and epoch descriptors:
AAT
3
(Art & Architecture Thesaurus), CDWA
4
(Categories for the Description of Works of Art) or the
AAE Style Guide
5
(Association of Art Editors Style Guide), to mention just a few.
As shown in Appendix A and B, the NGI standards cover a diverse set of date expressions, from
specific dates to more generic ones, giving the opportunity to enter into the system a range of years,
decades or centuries. The date values are expressed as four digit years. More specific dates related to
other events connected to the creation of the art work (for example for published volumes or different
print editions), are recorded in the ?Historical Dates field? where the required date can be selected from
a pop up calendar and the type of date can be selected from a drop down list (for example ?Published?).
The Date label on TMS consists of three main fields: Date, which displays the actual date or range of
dates related to the creation of the art work and which appears on the main object record screen as part
of the basic object tombstone information; Begin Date and End Date, which represent the earliest and
the latest possible years from a range of dates during which the artwork was created (Fig. 1). The Begin
Date and End Date are not displayed in the Date label on the data entry screen of a record, as they are
used for indexing and searching purposes only. Through the Simple Search and the Advanced Search
functionality in the system it is possible to retrieve records with a range of dates, by either searching for
earliest date, latest date or a certain time between these dates, the resulting records being drawn from the
values recorded in the Begin and End Date.
(a) ?Dates Assitant? for entering dates. (b) Panel for date queries.
Figure 1: The ?Dates Assistant? for entering dates into the database and a selection panel for date queries
currently in use in the collection management system of the National Gallery of Ireland.
2
http://www.gallerysystems.com/tms (last accessed 13/07/2014).
3
http://www.getty.edu/research/tools/vocabularies/guidelines/aat_4_2_appendix_b_
dates.html (last accessed 13/07/2014).
4
http://www.getty.edu/research/publications/electronic_publications/cdwa/
14creation.html#CREATION-DATE (last accessed 13/07/2014).
5
http://www.artedit.org/styleguide.htm (last accessed 13/07/2014).
42
The Begin and End Dates can be inserted automatically by the system either by pressing the ?Calc?
button or by accepting a suggestion for both Begin Date and End Date which is updated automatically ev-
ery time a new value is inserted in the Date field. The suggestions can be accepted or modified manually
and then saved. A date expression can also be suggested by the system when entering the relevant years
directly into Begin and End Date. In this case the ?Calc? button prompts a pop up window with different
date expressions based on the years inserted as beginning and end. For example, by entering ?1575? and
?1578? in the Begin and End Date, the suggestion box for the Date field will list the following options:
?1575-1578?; ?c.1576?; ?late 16th century?. When date ranges include two specific years (for example in
the case of ?YYYY/YYYY? or ?YYYY-YYYY?) the two year values are automatically suggested in the
Begin and End Date fields. When a single year is inserted in the Date field, the Begin and End Date are
automatically filled with that same year value.
Through the configuration menu it is possible to specify the range of years to be ?suggested? in the
Begin and End Date when entering a particular date expression in the Date field. By default this applies
for the circa label (in the NGI case the range is 5 years before and after the specified date) and decades.
Although the automatic suggestions for Begin and End Date are configurable through the back end of
the system, manual input is still necessary for accuracy when entering certain date expressions. Centuries
for example (in all their formats, from ?xxth century? to ?early/mid/late xxth century?) are not recognized
by Begin and End Date, which in these cases need to be filled in manually. However the process works
in reverse: when inserting the correct earliest and latest year that indicate a century span, the suggestion
box for the Date field displays different options, including the correct ?xxth century? format.
On the other hand, in the case of decades, the relevant Begin Date and End Date are correctly suggested
when inserting the ?YYYYs? format in the Date field, while, when entering the relevant years indicating
the time span of a decade in the Begin and End date, the options listed as suggestions for the Date field do
not include the correct format, giving instead the option of selecting ?YYYY-YYYY? as an alternative.
Similarly the Date field does not distinguish between years separated by an ?or?, a dash or a hyphen
when displaying the suggestions based on years inserted in Begin and End Date: when two different
years are inserted in the Begin and End Date, the only relevant option listed by the system is the range
of years separated by a hyphen. However, when entering the same date expressions in the Date field
whether separated by ?or?, dash or hyphen, the correct values are inserted in the Begin and End Date.
As the Date field is a free-text field on TMS, the process of manually entering date values, especially
the ones that indicate uncertainty and include a prefix and non-numerical values, gives more room for
error. In addition to this, not every date expression inserted in the Date field is recognised by the Begin
and End Dates, in which case these also have to be entered manually.
At the same time the automatic suggestions given for the Date field when entering Begin Date and End
seem to be more comprehensive and work better and they are helpful in giving the opportunity to select
the correct option without having to manually enter the data, thus reducing the possibility of error. In the
case of the NGI some configuration is further needed to make the most of the automated system already
in place. In particular it would be useful to include in the provided suggestions for the Begin and End
Date, those date expressions that are not currently recognised by the system.
3 Methodology and data set
The development of our system is inspired by an earlier system of temporal expression resolution for
German language date expressions, an auxiliary part of a research project concerned with information
retrieval on digital repositories of works of art, (Isemann and Ahmad, 2014). The approach was an
iterative development cycle of successively resolving ever more complex date and time descriptors and
mapping them to unambiguous time spans in ISO 8601 format. The data used were German date entries
in a commercially available digital collection of 40,000 works of art.
6
Example expressions from this
data set are: ?1707-1712?, ?1734/39?, ?1790-3?, ?12./13. Jh.?, ?1. Drittel 16. Jh.?, ?1420-1375 v. Chr.?.
These examples represent date ranges that have a fairly well defined beginning and end. One may
perhaps argue whether the 13th century should include the year 1300 or not, but in general the intended
6
The DVD collection ?40000 Meisterwerke?, published by Directmedia Publishing.
43
boundaries are reasonably clear. The following examples, however, are compounded by the fact that they
contain uncertainty markers which leave the precise date range that should be assigned to them up to
context and interpretation: ?um 1568?, ?1642 (?)?, ?Vor 1650?, ?ab 1486?, ?nach 1776-77?.
For the experiments presented here, we obtained a similar although much smaller English language
data set from the National Gallery of Ireland. The data consisted of 939 records from the NGI database,
comprising date expressions such as ?1791 to 1794?, ?1870/72?, ?1740s?, ?18th century?, ?1st February
1751?, ??c.1893?, ?after 1752?, ?late 16th century?, ?mid-1930s?. Unlike in the German data set, most
date expressions in the NGI data are already associated with a ?Begin Date? and ?End Date? either calcu-
lated by the NGI collection management system or manually entered by NGI staff (compare Section 2).
These date ranges sanctioned by art experts present a valuable additional resource which may serve as
training data for statistical learning or as a benchmark to compare against.
In contrast to the German language system we are conceptually using a two stage approach in which
we first attempt to represent the intended meaning of a date expression (?intension?) and only then map
it to a date range for search and retrieval (one might call this range the ?extension? of a date expression).
For the representation of date expression semantics (intension) we have chosen the VRA core 4.0 set of
metadata elements and here in particular the ?date? element.
7
VRA core is a set of categories defined and
maintained by the Data Standards Committee of the Visual Resources Association.
8
The latest version
4.0 dates from 2007. The standard has been used for semantic annotation (cf. Hollink et al. (2003) which
use VRA core 3.0) and defines mappings to other metadata schemata, such as Dublin Core,
9
CDWA,
10
CCO
11
(Cataloging Cultural Objects) and its own predecessors (VRA core 2.0 and 3.0). As value ranges
the standard recommends widely used thesauri (AAT
12
) or controlled vocabularies (ULAN
13
) or in the
case of dates the ISO 8601 standard. Structurally, the standard prescribes that ?date? elements have a
?type? attribute (such as ?creation?, ?design?, ?alteration?) and may have an ?earliestDate? and ?latestDate?
subelement, both of which should only take ISO 8601 compatible values and can be modified by a
boolean ?circa? attribute.
The semantic representation is the point of departure for the resolution of a date expression to a con-
crete date range. This leaves room for interpretation, especially in cases where a ?circa? flag is present.
Ideally this mapping should be governed by preferences at the user and/or institution level (similar to the
guidelines presented in Appendices A and B).
While the interpretation of these dates may vary on a case-by-case basis and even experts may disagree,
we believe that certain default rules will allow at least a rough approximation of the intended time range in
many cases. Analysing the data we noticed that mentions of years are not uniformly distributed in terms
of the digit they end on. Figure 2 shows the relative frequency of year end digits for the German data set
(red line) in expressions involving a ?circa? flag (German: ?um?). Assuming a uniform distribution of
years the frequencies should be 801.1 throughout. It is statistically extremely unlikely that the observed
deviation from a uniform distribution is due to chance variation (chi squared test, 9 degrees of freedom,
p < 0.001). As it appears equally unlikely that artists over the centuries have had a particular propensity
to be more productive in years ending in 0 and 5, we believe that the natural explanation is that art
historians documenting temporality tend to gravitate to "round" numbers in cases of greater uncertainty.
As an upshot we would like to suggest that all else being equal approximate dates involving years should
be seen as less certain if they end in 0 or 5 than if they end in other digits. Accordingly we add ?10
years to years ending in ?0?, ?5 to years ending in ?5? and ?1 to years ending in other digits. Table 1
shows a number of the resolutions our system can perform.
7
http://www.loc.gov/standards/vracore/ (last accessed 13/07/2014).
8
http://www.vraweb.org (last accessed 13/07/2013)
9
http://dublincore.org (last accessed 13/07/2014).
10
Categories for the Description of Works of Art, cf. Section 2
11
http://vraweb.org/ccoweb/cco/intro.html (last accessed 13/07/2014).
12
http://www.getty.edu/research/tools/vocabularies/aat (last accessed 13/07/2014).
13
http://www.getty.edu/research/tools/vocabularies/ulan (last accessed 13/07/2014).
44
Figure 2: Distribution of year end digits in German expressions with an uncertainty marker (?um?, red
line) and frequencies of ?00? endings compared to other multiples of 10 (blue line).
Date expression Date resolution
1889 +1889/+1889
1522/1523 +1522/+1523
1791 to 1794 +1791/+1794
1870/72 +1870/+1872
1740s +1740/+1749
18th century +1700/+1799
1st February 1751 +1751/+1751
(a) Dates with well defined scope.
Date expression Date semantics Date resolution
c.1824 c(+1824)/c(+1824) +1823/+1825
c.1795 c(+1795)/c(+1795) +1790/1800
c.1890 c(+1890)/c(+1890) +1880/+1900
?c.1893 c(+1893)/c(+1893) +1892/+1894
after 1752 +1752/null +1752/+1762
late 16th century +1566/+1599 +1566/+1599
mid-1930s +1933/+1936 +1933/+1936
(b) Dates with fuzzy scope.
Table 1: Date expressions from the National Gallery data set with default resolutions from our sys-
tem. For the case of date expressions containing uncertainty or ?fuzziness? we also show the semantic
layer (b). Here ?c(?)? represents a positive circa attribute in the VRA core earliestDate and latestDate
subelements. Note, that not all expressions which may informally appear vague involve a circa attribute
and that we assign a latest date by default for cases such as ?after 1752?, contrary to the VRA core
recommendation (which we adopt as semantic representation for such cases).
4 Experiments
We implemented a rule-based date expression resolver for the expressions in the English language Na-
tional Gallery data set (achieving nearly complete coverage) with the set of heuristics outlined in the
previous section (cf. Table 1). Two art history and meta data experts from the National Gallery agreed
to participate in an evaluation of the output of our resolution system compared against the current date
range entry in the National Gallery database. The entries in the NGI database are not a direct feature of
the collection management system, but rather of how the system is currently used.
We observed that our system output agreed with the NGI entries in about half of the cases (58%). In
order not to burden our volunteers? time too much we did not evaluate on the complete data set, but on
a randomly extracted subset in which we only included cases where our system output differed from
the existing gallery records. We used a random number generator in Java to extract records until we
reached a limit of 50 cases in which the two date interpretations were different. This limit was reached
after selecting a total of 104 entries. The 50 non-trivial cases were compiled into a list comprised of the
original date expression and a choice of two different date ranges each, one from the NGI records and
one from our system. The order of the choices was randomized independently for each individual record.
The two evaluation participants were given this list together with a short introductory text outlining
the background and purpose of the evaluation. They were then instructed to select which of the two
date range alternatives they felt best captured the meaning of the date expression or indicate that they
had no preference. Introductory paragraphs in the evaluation stressed that while individual context may
sometimes enter into such a decision, they should think of the given date expressions as generic examples.
5 Results
Of the 100 individual decisions made by our two experts (50 each) exactly half (50) were in favour of
our system?s default recommendation, less than a third were in favour of the existing database entry (29)
45
and just over one in five (21) had no particular preference (Table 2).
Annotator A
Our System Gallery Records No Preference Total Annotator B
Our System 8 0 3 11
Annotator B Gallery Records 19 2 6 27
No Preference 12 0 0 12
Total Annotator A 39 2 9
Table 2: Agreement of preferences expressed by our evaluators in 50 test cases.
While this may be seen as an encouraging result for our date range recommender system it has to be
said that in their overall preference our two evaluators were leaning different ways. While one over-
whelmingly agreed with our system recommendations (preferring the NGI alternative in just two cases
with nine ties), the other was leaning towards the NGI records (preferring our system in just eleven cases
with twelve ties). Overall the two evaluators agreed in ten of the 50 cases (Cohen?s kappa = -0.048).
We believe that the reason for the differing opinions between our two evaluators may be that one
of them is working closely with the NGI database and is therefore very familiar with the status quo,
including certain agreed in-house standards. The other evaluator, who was leaning towards the rules
implemented in our system, is from the curatorial department and concerned with absolute and relative
dating of works of art in a more theoretical way. A more thorough evaluation is needed in order to
determine if the more flexible rules we are advocating would be appreciated by an expert user community.
6 Related Work
The resolution of temporal expressions is an important topic in the information extraction and semantic
web community and employing these methods on cultural heritage texts in particular has been the focus
of research spanning these fields and the emergent discipline of digital humanities.
Context-free grammars (CFG) for the resolution of temporal expressions have been employed by An-
geli et al. (2012) and Kauppinen et al. (2010). Angeli et al. (2012) attempt to learn a probabilistic CFG for
time and date expressions and at the same time an expectation maximation framework for the resolution
of pragmatic ambiguity in time expressions (e.g. ?Friday? may refer to last or next Friday, ?last Friday?
may refer to the previous Friday or the Friday two weeks ago etc.). For training their system they employ
the TempEval-2 Task A dataset.
14
Despite the relatively small training set (1052 time expressions) they
report comparable performance of their system with leading rule-based temporal resolvers.
Kauppinen et al. (2010) employ fuzzy sets towards the representation and querying of temporally
disputable periodic expressions from cultural heritage such as ?late-Roman-era?, ?Middle Ages? or ?be-
ginning of the 1st century BC?, which can vary due to subjectivity or lack of hard records. They define
a date span with a fuzzy beginning and end which encompasses the widest possible bounds for a tem-
poral period and then a more concise beginning and end which encompasses more constrained bounds.
Queries are matched against the fuzzy set using a bespoke querying model which finds the level of over-
lap between the query and the fuzzy set. They test their theories on a set of records from the Ancient
Milan
15
project, representing fuzzy date ranges as four RDF triples, one for each of the date points. They
represent definite temporal expressions such as First half of the 1st Century BC in Backus-Naur form.
Research into frameworks for temporal expression extraction in the computational sciences, (Chang
and Manning (2012), Str?tgen and Gertz (2010), Sun et al. (2013)) has tended to focus on domains
such as clinical texts and newswire for developing temporal expression resolution systems. We believe,
however, that there is a clear and present need for systems and frameworks which can extract structured
information from cultural heritage text, particularly in the domain of fine art image catalogues. These
methodologies can enable the development of smarter retrieval systems for catalogues of cultural history
14
Cf. http://timeml.org/tempeval2 (last accessed 13/07/2014).
15
http://www.csai.disco.unimib.it/CSAI/space/CuRM/projects+and+research/Milano+
Antica (last accessed 13/07/2014).
46
data. Grandi and Mandreoli (2001), Grandi (2002) describe work on representing a geographical history
resource, il Dizionario geografico, fisico e storico della Toscana
16
created by cultural historian Emanuele
Repetti in the early 19th century. They focus on the resolution of temporal expressions? indeterminancy
and varying granularity in Italian temporal expressions, such as around X, circa. X, near the end of the
X century and others. They represent such indeterminacy using a four category classification of date
expressions and a probabilistic approach from the TSQL2 standard, (Snodgrass et al. (1994)). Lilis and
others (2005) use multidimensional RDF in their representation of cultural artifacts in a museum setting.
Smith (2002) focuses on detecting events in unstructured historical text with dates forming the main
focus of his study. The author investigates the co-occurrence of place names and dates in 19th century text
and extracts a geo-located list of events from the text. He mentions that 98% of numerical tokens in the
texts refer to dates, although in different text genres, date information may be more vaguely expressed.
Furthermore, he finds that certain dates are expressed as a calendar day and others refer merely to the year
an event occurred. These expressions can prove problematic for traditional date processing algorithms,
and often a more complex mapping is required to convert these textual representations to a computational
formalism such as the CIDOC specification. Chang and Manning (2012) focus on generic temporal
expressions with their SUTIME parser, which represents date and temporal information extracted from
text using the TIMEX3 tag format from the TimeML (Boguraev and Ando (2005)) standard.
An emerging trend in date resolution literature encompasses the big data paradigm. Blamey et al.
(2013) develop a probabilistic approach toward modelling everyday natural language date expressions
17
using textual data from image descriptions and EXIF
18
data from uploaded photos on the flickr website.
7 Conclusion and Future Work
We have presented and tested a system specifically designed for the resolution of date expressions in
cultural heritage legacy records and we have argued for a ?semantic layer? between the literal expressions
and the date range resolution. Our evaluation, although small scale, suggests that such a system may
potentially be able to improve even records which already incorporate date resolutions, if slightly more
complex rules than are contained in the current system or data entry guidelines are implemented.
A number of possible lines of future work suggest themselves. In order to arrive at an explicit local
grammar for ?heritage dates? (cf. Kauppinen et al. (2010) and Angeli et al. (2012)), we have created a
context-free grammar, that accepts roughly the same input as our current rule set. Initial examination
suggests that the non-lexical part of the grammar can cover both English and German language data
given appropriate lexicons. The grammar phrases can be mapped to representations in the semantic layer
thereby in effect creating a system which could process multilingual input and produce consistent output.
A further extension to the system would involve the processing of semantically more complex temporal
period expressions, such as ?Victorian?, ?Edwardian?, ?Gr?nderzeit?, ?Gilded Age? or ?Renaissance?.
Examples tied to the reign of a monarch tend towards a more defined scope however wider-ranging
and more culturally-disputed periods such as ?the Renaissance? tend to attract a less precise beginning
and end-date than the former examples and may require a more complex set of semantics. Data-driven
approaches could be employed to model the temporal boundaries for temporal expressions of a more
vague nature. Angeli et al. (2012) demonstrate that this may be feasible even on relatively small datasets.
Examples which could benefit from an ontological augmentation involving events and periods include
the practice of dating works of art with implicit reference to such periods based on a believed or pre-
viously confirmed date for a major event such as a battle or war. One example of this practice could
be an artwork dated ?after 1453?, with the date actually representing the current dating of the fall of
Constantinople. As historical information is updated or revised, the corresponding date range estimating
the temporal origin of a work could be resolved based on updated information for the reference event.
Similar suggestions were made in (Isemann and Ahmad, 2009). Perhaps the practically most relevant
example of this kind could be cross-referencing the lifespan of an artist associated with the production
16
A geographical, physical and historical dictionary of Tuscany
17
Their work focuses on UK-specific cultural expressions such as Bonfire Night, first day of summer, Christmas holidays.
18
Timestamps saved by digital cameras.
47
of a work with the temporal expression for its creation: if the expression says ?after 1756? but we have
concrete knowledge that the artist died in 1758, this can be used to add bounds to the creation event.
Acknowledgements
The authors would like to thank Dr Adriaan Waiboer, curator for Northern European Art at the National
Gallery of Ireland, for valuable suggestions and his help in organising the expert evaluation.
References
Gabor Angeli, Christopher Manning, and Daniel Jurafsky. 2012. Parsing time: Learning to interpret time expres-
sions. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Compu-
tational Linguistics: Human Language Technologies, pages 446?455, Montr?al, Canada, June. Association for
Computational Linguistics.
Ben Blamey, Tom Crick, and Giles Oatley. 2013. ?The First Day of Summer?: Parsing temporal expressions
with distributed semantics. In Research and Development in Intelligent Systems XXX, pages 389?402. Springer
International Publishing.
Branimir Boguraev and Rie Kubota Ando. 2005. Timeml-compliant text analysis for temporal reasoning. In
Proceedings of the 19th international joint conference on Artificial intelligence, pages 997?1003. Morgan Kauf-
mann Publishers Inc.
Angel X Chang and Christopher Manning. 2012. Sutime: A library for recognizing and normalizing time expres-
sions. In LREC, pages 3735?3740.
Fabio Grandi and Federica Mandreoli. 2001. The "XML/Repetti" Project: Encoding and Manipulation of Tempo-
ral Information in Historical Text Sources. In ICHIM (2), pages 243?252. Citeseer.
Fabio Grandi. 2002. Xml representation and management of temporal information for web-based cultural heritage
applications. Data Science Journal, 1(1):68?83.
L. Hollink, A. Th. Schreiber, B. Wielemaker, and B. Wielinga. 2003. Semantic annotation of image collections.
In Proceedings of the KCAP?03 Workshop on Knowledge Markup and Semantic Annotation, Florida, USA,
October.
Daniel Isemann and Khurshid Ahmad. 2009. Navigating cultural heritage in style. Sketching an ontological
representation of metadata: the example of artistic period and style. Museum Ireland, 19:149?155.
Daniel Isemann and Khurshid Ahmad. 2014. Ontological access to images of fine art. Journal on Computing and
Cultural Heritage (JOCCH), 7(1):3.
Tomi Kauppinen, Glauco Mantegari, Panu Paakkarinen, Heini Kuittinen, Eero Hyv?nen, and Stefania Bandini.
2010. Determining relevance of imprecise temporal intervals for cultural heritage information retrieval. Inter-
national journal of human-computer studies, 68(9):549?560.
Pantelis Lilis et al. 2005. A metadata model for representing time-dependent information in cultural collections.
In MTSR, First online metadata and semantics research conference, Conference Proceedings. Citeseer.
David A. Smith. 2002. Detecting and browsing events in unstructured text. In Proceedings of the 25th Annual
International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ?02, pages
73?80, New York, NY, USA. ACM.
Richard Thomas Snodgrass, Ilsoo Ahn, Gad Ariav, Don S Batory, James Clifford, Curtis E Dyreson, Ramez
Elmasri, Fabio Grandi, Christian S Jensen, Wolfgang K?fer, et al. 1994. Tsql2 language specification. Sigmod
Record, 23(1):65?86.
Jannik Str?tgen and Michael Gertz. 2010. Heideltime: High quality rule-based extraction and normalization of
temporal expressions. In Proceedings of the 5th International Workshop on Semantic Evaluation, SemEval ?10,
pages 321?324, Stroudsburg, PA, USA. Association for Computational Linguistics.
Weiyi Sun, Anna Rumshisky, and Ozlem Uzuner. 2013. Evaluating temporal relations in clinical text: 2012 i2b2
challenge. Journal of the American Medical Informatics Association, 20(5):806?813.
48
A NGI definitions/explanations of date expressions
Figure 3: Rules for interpreting object date descriptions in NGI records.
49
B NGI guidelines for entering dates and date ranges
Display Search Search Rule
Date Date Date
Begin End
Single date 1855 1855 1855
Begin date and end 1630-1633 1630 1633 Separate the 2 dates with a dash, no
date (begun 1630, spaces between
finished 1633) Include all 4 digits for both years
One of 2 years 1631 or 1633 1631 1633 Use the word ?or?.
(work was done in
either 1631 or 1633) Include all 4 digits for both years
Range of dates 1745/1748 1745 1748 Separate the 2 dates with a slash, with
(work was done no spaces between
sometime between Include all 4 digits for both years
1745 and 1748)
Decades 1930s 1930 1939 no apostrophe before ?s?
early 1930s 1930 1934 ?early? and ?late? all in lower case
late 1930s 1935 1939
Circa c.1900 1895 1905 Use c.
c.1600 or 1610 1595 1610 Don?t use ?about?, ?circa?, ?ca? or ?c?
c.1510-1520 1505 1520 No space between c. and date
c.1510-c.1520 1505 1525
Before 1686-before 1770 1686 1770 Use ?before?
before 1686-1750 no begin 1750 Don?t use ?prior to?
date
After 1823-after 1941 1823 no end Use ?after?
date Don?t use ?post?
after 1822-1900 1822 1900
Uncertainty ?1750 1750 1750 Place a question mark before the
doubtful element without a space.
Do not use ?probably?
More precise dates November 1900 1900 1900
21 January 1890 1890 1890 Dates given in full should be entered as
day month year without punctuation or
ordinal abbreviation such as rd, th, nd
Table 3: Data Entry Conventions when entering Object Dates and Search Dates into TMS (excerpt).
50
