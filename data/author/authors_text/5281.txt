Example-based Speech Intention Understanding and
Its Application to In-Car Spoken Dialogue System
Shigeki Matsubara? Shinichi Kimura? Nobuo Kawaguchi?
Yukiko Yamaguchi? and Yasuyoshi Inagaki?
?Information Technology Center, Nagoya University
?Graduate School of Engineering, Nagoya University
CIAIR, Nagoya University
Furo-cho, Chikusa-ku, Nagoya, 464-8601, Japan
matubara@itc.nagoya-u.ac.jp
Abstract
This paper proposes a method of speech inten-
tion understanding based on dialogue examples.
The method uses a spoken dialogue corpus with
intention tags to regard the intention of each in-
put utterance as that of the sentence to which it
is the most similar in the corpus. The degree of
similarity is calculated according to the degree
of correspondence in morphemes and dependen-
cies between sentences, and it is weighted by
the dialogue context information. An exper-
iment on inference of utterance intentions us-
ing a large-scale in-car spoken dialogue corpus
of CIAIR has shown 68.9% accuracy. Further-
more, we have developed a prototype system of
in-car spoken dialogue processing for a restau-
rant retrieval task based on our method, and
confirmed the feasiblity of the system.
1 Introduction
In order to interact with a user naturally and
smoothly, it is necessary for a spoken dialogue
system to understand the intentions of utter-
ances of the user exactly. As a method of speech
intention understanding, Kimura et al has pro-
posed a rule-based approach (Kimura et al,
1998). They have defined 52 kinds of utterance
intentions, and constructed rules for inferring
the intention from each utterance by taking ac-
count of the intentions of the last utterances, a
verb, an aspect of the input utterance, and so
on. The huge work for constructing the rules,
however, cannot help depending on a lot of
hands, and it is also difficult to modify the rules.
On the other hand, a technique for tagging di-
alogue acts has been proposed so far (Araki et
al., 2001). For the purpose of concretely deter-
mining the operations to be done by the system,
the intention to be inferred should be more de-
tailed than the level of dialogue act tags such as
?yes-no question? and ?wh question?.
This paper proposes a method of understand-
ing speeches intentions based on a lot of dia-
logue examples. The method uses the corpus in
which the utterance intention has given to each
sentence in advance. We have defined the ut-
terance intention tags by extending an annota-
tion scheme of dialogue act tags, called JDTAG
(JDRI, 2000), and arrived at 78 kinds of tags
presently. To detail an intention even on the
level peculiar to the task enables us to describe
the intention linking directly to operations of
the system.
In the technique for the intention inference,
the degree of similarity of each input utter-
ance with every sentence in a corpus is calcu-
lated. The calculation is based on the degree of
morphologic correspondence and that of depen-
dency correspondence. Furthermore, the degree
of similarity is weighted by using dialogue con-
text information. The intention of the utterance
to which the maximum score is given in the cor-
pus, will be accepted as that of the input utter-
ance. Our method using dialogue examples has
the advantage that it is not necessary to con-
struct rules for inferring the intention of every
utterance and that the system can also robustly
cope with the diversity of utterances.
An experiment on intention inference has
been made by using a large-scale corpus of spo-
ken dialogues. The experimental result, provid-
ing 68.9% accuracy, has shown our method to
be feasible and effective. Furthermore, we have
developed, based on our method, a prototype
system of in-car spoken dialogue processing for
a restaurant retrieval task, and confirmed the
feasiblity of the system.
Chikaku-ni chushajo-wa aru-ka-na
(Is there a parking lot nearby?)
Kono chikaku-ni firstfood aru?
(Is there a first food shop near here?
Mosburger-ga gozai-masu-ga
(Mosburger is near here.)
Spoken dialogue 
corpus with
intention tags
?????????
???????????
??????????
????????
???????????
?????????????
Dependency and morpheme analysis
System?s
speech
Intensions
probability
Calculation
of similarity
weighting
Utterance intension: ?parking lot question?
Context information
User?s
speech
Figure 1: Flow of the intention inference pro-
cessing
2 Outline of Example-based
Approach
Intentions of a speaker would appear in the vari-
ous types of phenomenon relevant to utterances,
such as phonemes, morphemes, keywords, sen-
tential structures, and contexts. An example-
based approach is expected to be effective for
developing the system which can respond to the
human?s complicated and diverse speeches. A
dialogue corpus, in which a tag showing an ut-
terance intention is given to each sentence, is
used for our approach. In the below, the outline
of our method is explained by using an inference
example.
Figure 1 shows the flow of our intention
inference processing for an input utterance
?Chikaku-ni chushajo-wa aru-ka-na ? (Is there
a parking lot nearby?)?. First, morphological
analysis and dependency analysis to the utter-
ance are carried out.
Then, the degree of similarity of each input
utterance with sentences in the corpus can be
calculated by using the degree of correspon-
dence since the information on both morphol-
ogy and dependency are given to all sentences
in the corpus in advance. In order to raise the
accuracy of the intention inference, moreover,
the context information is taken into consid-
eration. That is, according to the occurrence
probability of a sequence of intentions learned
from a dialogue corpus with the intention tags,
the degree of similarity with each utterance is
weighted based on the intentions of the last ut-
terances. Consequently, if the utterance whose
degree of similarity with the input utterance is
the maximum is ?sono chikaku-ni chushajo ari-
masu-ka? (Is there a parking lot near there?)?,
the intention of the input utterance is regarded
as ?parking lot question?.
3 Similarity and its Calculation
This section describes a technique for calculat-
ing the degree of similarity between sentences
using the information on both dependency and
morphology.
3.1 Degree of Similarity between
Sentences
In order to calculate the degree of similarity be-
tween two sentences, it can be considered to
make use of morphology and dependency infor-
mation. The calculation based on only mor-
phemes means that the similarity of only sur-
face words is taken into consideration, and thus
the result of similarity calculation may become
large even if they are not so similar from a struc-
tural point of view. On the other hand, the cal-
culation based on only dependency relations has
the problem that it is difficult to express the lex-
ical meanings for the whole sentence, in partic-
ular, in the case of spoken language. By using
both the information on morphology and de-
pendency, it can be expected to carry out more
reliable calculation.
Formula (1) defines the degree of similarity
between utterances as the convex combination
? of the degree of similarity on dependency, ?d,
and that on morpheme, ?m.
? = ??d + (1 ? ?)?m (1)
?d : the degree of similarity in dependency
?m: the degree of similarity in morphology
? : the weight coefficient (0 ? ? ? 1)
Section 3.2 and 3.3 explain ?d and ?m, re-
spectively.
3.2 Dependency Similarity
Generally speaking, a Japanese dependency re-
lation means the modification relation between
a bunsetsu and a bunsetsu. For example,
a spoken sentence ?kono chikaku-ni washoku-
no mise aru? (Is there a Japanese restau-
rant near here?)? consists of five bunsetsus of
?kono (here)?, ?chikaku-ni (near)?, ?washoku-
no (Japanese-style food)?, ?mise (a restau-
rant)?, ?aru (being)?, and there exist some de-
pendencies such that ?mise? modifies ?aru?. In
the case of this instance, the modifying bun-
setsu ?mise? and the modified bunsetsu ?aru?
are called dependent and head, respectively. It
is said that these two bunsetsus are in a depen-
dency relation. Likewise, ?kono?, ?chikaku-ni?
and ?washoku-no? modify ?chikaku-ni?, ?aru?
and ?mise?, respectively. In the following of this
paper, a dependency relation is expressed as the
order pair of bunsetsus like (mise, aru), (kono,
chikaku-ni).
A dependency relation expresses a part of
syntactic and semantic characteristics of the
sentence, and can be strongly in relation to the
intentional content. That is, it can be expected
that two utterances whose dependency relations
are similar each other have a high possibility
that the intentions are also so.
A formula (2) defines the degree of similar-
ity in Japanese dependency, ?D, between two
utterances SA and SB as the degree of corre-
spondence between them.
?d =
2CD
DA + DB
(2)
DA: the number of dependencies in SA
DB: the number of dependencies in SB
CD : the number of dependencies in corre-
spondence
Here, when the basic forms of independent
words in a head bunsetsu and in a dependent
bunsetsu correspond with each other, these de-
pendency relations are considered to be in cor-
respondence. For example, two dependencies
(chikaku-ni, aru) and (chikaku-ni ari-masu-ka)
correspond with each other because the inde-
pendent words of the head bunsetsu and the de-
pendent bunsetsu are ?chikaku? and ?aru?, re-
spectively. Moreover, each word class is given
to nouns and proper nouns characteristic of a
dialogue task. If a word which constitutes each
dependency belongs to the same class, these de-
pendencies are also considered to be in corre-
spondence.
3.3 Morpheme Similarity
A formula (3) defines the degree of similarity in
morpheme ?m between two sentences SA and
????????????? ??????????????
(Is there a Japanese restaurant near here?)
????
Japanese dependency
4 dependencies
common dependencies: 3
?d = 0.86
Japanese morpheme
common morphemes: 6
?m = 0.80
?
= 0.82
Degree of Similarity
If?= 0.4,
= 0.4*0.86+0.6*0.80
7 morphemes
User?s utterance unit: Si
????????????????????? ?????????? ????
(Is there a European restaurant nearby?)
Example of utterance: Se
3 dependencies 8 morphemes
Figure 2: Example of similarity calculation
SB.
?m =
2CM
MA + MB
(3)
MA: the number of morphemes in SA
MB: the number of morphemes in SB
CM : the number of morphemes in correspon-
dence
In our research, if a word class is given to
nouns and proper nouns characteristic of a di-
alogue task and two morphemes belong to the
same class, these morphemes are also consid-
ered to be in correspondence. In order to ex-
tract the intention of the sentence more simi-
lar as the whole sentence, not only independent
words and keywords but also all the morphemes
such as noun and particle are used for the cal-
culation on correspondence.
3.4 Calculation Example
Figure 2 shows an example of the calculation
of the degree of similarity between an input ut-
terance Si ?kono chikaku-ni washoku-no mise
aru? (Is there a Japanese restaurant near
here?)? and an example sentence in a corpus,
Se, ?chikaku-ni yoshoku-no mise ari-masu-ka (Is
there a European restaurant located nearby?)?,
when a weight coefficient ? = 0.4. The num-
ber of the dependencies of Si and Se is 4 and
3, respectively, and that of dependencies in cor-
respondence is 2, i.e., (chikaku, aru) and (mise,
aru). Moreover, since ?washoku (Japanese-style
food)? and ?yoshoku? (European-style food)
belong to the same word class, the dependencies
(washoku, aru) and (yoshoku, aru) also corre-
spond with each other. Therefore, the degree
of similarity in dependency ?d comes to 0.86
by the formula (2). Since the number of mor-
phemes of Si and Se are 7 and 8, respectively,
and that of morphemes in correspondence is 6,
i.e., ?chikaku?, ?ni?, ?no?, ?mise?, ?aru(i)? and
?wa(yo)shoku?. Therefore, ?m comes to 0.80
by a formula (3). As mentioned above, ? us-
ing both morphemes and dependencies comes
to 0.82 by a formula (1).
4 Utilizing Context Information
In many cases, the intention of a user?s utter-
ance occurs in dependence on the intentions of
the previous utterances of the user or those of
the person to which the user is speaking. There-
fore, an input utterance might also receive the
influence in the contents of the speeches before
it. For example, the user usually returns the
answer to it after the system makes a question,
and furthermore, may ask the system a ques-
tion after its response. Then, in our technique,
the degree of similarity ?, which has been ex-
plained in Section 3, is weighted based on the
intentions of the utterances until it results in a
user?s utterance. That is, we consider the oc-
currence of a utterance intention In at a certain
time n to be dependent on the intentions of the
last N ? 1 utterances. Then, the conditional
occurrence probability P (In|I
n?1
n?N+1) is defined
as a formula (4).
P (In|I
n?1
n?N+1) =
C(Inn?N+1)
C(In?1n?N+1)
(4)
Here, we write a sequence of utterance in-
tentions In?N+1 ? ? ?In as Inn?N+1, call it in-
tentions N-gram, and write the number of
appearances of them in a dialogue corpus as
C(Inn?N+1). Moreover, we call the conditional
occurrence probability of the formula (4), in-
tentions N-gram probability.
The weight assignment based on the inten-
tions sequences is accomplished by reducing the
value of the degree of similarity when the in-
tentions N-gram probability is smaller than a
threshold. That is, a formula (5) defines the de-
gree of similarity ? using the weight assignment
by intentions N-gram probability.
Search
Condition search
Parking search
Nearness question
Shop question
Business hours question
Distance question
Time question
Rank question
Menu price question
Number of car question
Parking price question Parking question
???intention tag
???dialogue act tag
???conditional tag
leafYes-no question Wh question ???
Unknown information
Unknown information
Figure 3: Decision tree of intention tag (a part)
? =
{
?? (P (In|I
n?1
n?N+1) ? ?)
? (otherwise)
(5)
?: weight coefficient (0 ? ? ? 1)
?: the degree of similarity
?: threshold
A typical example of the effect of using inten-
tions N-gram is shown below. For an input ut-
terance ?chikaku-ni chushajo-wa ari-masu-ka?
(Is there a parking lot located nearby?)?, the
degree of similarity with a utterance with a
tag ?parking lot question? which intends to
ask whether a parking lot is located around
the searched store, and a utterance with a tag
?parking lot search? which intends to search a
parking lot located nearby, becomes the maxi-
mum. However, if the input utterance has oc-
curred after the response intending that there
is no parking lot around the store, the system
can recognize its intention not to be ?parking
lot question? from the intentions N-gram prob-
abilities learned from the corpus, As a result,
the system can arrive at a correct utterance in-
tention ?parking lot search?.
5 Evaluation
In order to evaluate the effectiveness of our
method, we have made an experiment on ut-
terance intention inference.
5.1 Experimental Data
An in-car speech dialogue corpus which has
been constructed at CIAIR (Kawaguchi et al,
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
weight coefficient ?
a
c
c
u
r
a
c
y
recall precision
Figure 4: Relation between the weight coeffi-
cient ? and the accuracy (? = 0.3)
2001), was used as a corpus with intention tags,
and analyzed based on Japanese dependency
grammar (Matsubara et al, 2002). That is,
the intention tags were assigned manually into
all sentences in 412 dialogues about restaurant
search recorded on the corpus. The intentions
2-gram probability was learned from the sen-
tences of 174 dialogues in them. The standard
for assigning the intention tags was established
by extending the decision tree proposed as a di-
alogue tag scheme (JDRI, 2000). Consequently,
78 kinds of intention tags were prepared in all
(38 kinds are for driver utterances). The inten-
tion tag which should be given to each utter-
ance can be defined by following the extended
decision tree. A part of intention tags and the
sentence examples is shown in Table 1, and a
part of the decision tree for driver?s utterances
is done in Figure 3 1.
A word class database (Murao et al, 2001),
which has been constructed based on the cor-
pus, was used for calculating the rates of cor-
respondence in morphemes and dependencies.
Moreover, Chasen (Matsumoto et al, 99) was
used for the morphological analysis.
5.2 Experiment
5.2.1 Outline of Experiment
We have divided 1,609 driver?s utterances of
238 dialogues, which is not used for learning
the intentions 2-gram probability, into 10 pieces
equally, and evaluated by cross validation. That
is, the inference of the intentions of all 1,609 sen-
1In Figure 3, the description in condition branches is
omitted.
????
????
???
????
????
????
????
???
? ??? ??? ??? ??? ??? ??? ??? ??? ??? ?
????????????????????
?
?
?
?
?
?
?
?
???????????????????
?????????????????
Figure 5: Relation between the weight coeffi-
cient ? and the accuracy
tences was performed, and the recall and preci-
sion were calculated. The experiments based on
the following four methods of calculating the de-
gree of similarity were made, and their results
were compared.
1. Calculation using only morphemes
2. Calculation using only denpendencies
3. Calculation using both morphemes and
denpendencies (With changing the value of
the weight coefficient ?)
4. Calculation using intentions 2-gram prob-
abilities in addition to the condition of 3.
(With changing the value of the weight co-
efficient ? and as ? = 0)
5.2.2 Experimental Result
The experimental result is shown in Figure 4.
63.7% as the recall and 48.2% as the precision
were obtained by the inference based on the
above method 1 (i.e. ? = 0), and 62.6% and
58.6% were done in the method 2 (i.e. ? = 1.0).
On the other hand , in the experiment on the
method 3, the precision became the maximum
by ? = 0.2, providing 61.0%, and the recall by
? = 0.3 was 67.2%. The result shows our tech-
nique of using both information on morphology
and dependency to be effective.
When ? ? 0.3, the precision of the method
3 became lower than that of 1. This is because
the user speaks with driving a car (Kawaguchi
et al, 2000) and therefore there are much com-
paratively short utterances in the in-car speech
corpus. Since there is a few dependencies per
Table 1: Intention tags and their utterance examples
intention tag utterance example
search Is there a Japanese restaurant near here?
request Guide me to McDonald?s.
parking lot question Is there a parking lot?
distance question How far is it from here?
nearness question Which is near here?
restaurant menu question Are Chinese noodles on the menu?
Morphological & 
Intension
Intension
Action
Dependency analysis
Shop
information
database
Search
Response
inference
generation
In-car
spoken
dialogue
corpus with
intension tags Calculation
Intensions 2-gram
probabilityWeighting
Dictionary &
parsing rules
Intension-action
transfer rules
Context
stack
Decision
Analysis
Results
User?s Utterance
System?s utterance
Figure 6: Configuration of the prototype system
one utterance, a lot of sentences in the corpus
tend to have the maximum value in inference
using dependency information.
Next, the experimental result of the inference
using weight assignment by intentions 2-gram
probabilities, when considering as ? = 0.3, is
shown in Figure 5. At ? = 0.8, the maximum
values in both precision and recall were provided
(i.e., the precision is 68.9%). This shows our
technique of learning the context information
from the spoken dialogue corpus to be effective.
6 In-car Spoken Dialogue System
In order to confirm our technique for automat-
ically inferring the intentions of the user?s ut-
terances to be feasible and effective for task-
oriented spoken dialogue processing, a proto-
type system for restaurant retrieval has been
developed. This section describes the outline of
the system and its evaluation.
6.1 Implementation of the System
The configuration of the system is shown in Fig-
ure 6.
Table 2: Comparison between the results on in-
ferred intentions and those on given intentions
Inferred Given
Intentions num. rate num. rate
Correct 31 51.7% 42 70.0%
Partially corr. 5 8.3% 4 6.7%
Incorrect 7 11.7% 2 3.3%
No action 17 28.3% 12 20.0%
1. Morphological and dependency anal-
ysis: For the purpose of example-based
speech understanding, the morphological
and dependency analyses are given to each
user?s utterance by referring the dictionary
and parsing rules. Morphological analy-
sis is executed by Chasen (Matsumoto et
al., 99). Dependency parsing is done based
on a statistical approach (Matsubara et al,
2002).
2. Intentions inference: As section 3 and
4 explain, the intention of the user?s ut-
terance is inferred according to the degree
of similarity of it with each sentence in a
corpus, and the intentions 2-gram proba-
bilities.
3. Action: The transfer rules from the
user?s intentions to the system?s actions
have been made so that the system can
work as the user intends. We have al-
ready made the rules for all of 78 kinds
of intentions. The system decides the ac-
tions based on the rules, and executes
them. After that, it revises the context
stack. For example, if a user?s utterance
is ?kono chikaku-ni washoku-no mise ari-
masu-ka (Is there a Japanese restaurant
near here?)?, its intention is ?search?. In-
ferring it, the system retrieves the shop
information database by utilizing the key-
words such as ?washoku (Japanese restau-
rant)? and ?chikaku (near)?.
4. Response generation: The system re-
sponds based on templates which include
the name of shop, the number of shops, and
so on, as the slots.
6.2 Evaluation of the System
In order to confirm that by understanding the
user?s intention correctly the system can behave
appropriately, we have made an experiment on
the system. We used 1609 of driver?s utterances
in Section 5.2.1 as the learning data, and the
intentions 2-gram probabilities learned by 174
of dialogues in Section 5.1. Furthermore, 60 of
driver?s utterances which are not included in the
learning data were used for the test. We have
compared the results of the actions based on the
inferred intentions with those based on the given
correct intentions. The results have been classi-
fied into four groups: correct, partially correct,
incorrect, and no action.
The experimental result is shown in Table
2. The correct rate including partial correct-
ness provides 76.7% for the giving intentions
and 60.0% for the inferred intentions. We have
confirmed that the system could work appropri-
ately if correct intentions are inferred.
The causes that the system based on given
intentions did not behave appropriately for 14
utterances, have been investigated. 6 utterances
are due to the failure of keywords processing,
and 8 utterances are due to that they are out of
the system?s expectation. It is expected for the
improvement of the transfer rules to be effective
for the former. For the latter, it is considered
to turn the responses such as ?I cannot answer
the question. If the questions are about ? ? ?, I
can do that.?
7 Concluding Remarks
This paper has proposed the example-based
method for inferring speaker?s intention. The
intention of each input utterance is regarded as
that of the most similar utterance in the cor-
pus. The degree of similarity is calculated based
on the degrees of correspondence in both mor-
phemes and dependencies, taking account of the
effects of a sequence of the previous utterance?s
intentions. The experimental result using 1,609
driver?s utterances of CIAIR in-car speech cor-
pus has shown the feasibility of example-based
speech intention understanding. Furthermore,
we have developed a prototype system of in-car
spoken dialogue processing for a restaurant re-
trieval task based on our method.
Acknowledgement: The authors would like
to thank Dr. Hiroya Murao, Sanyo Electric Co.
LTD. for his helpful advice. This work is par-
tially supported by the Grand-in-Aid for COE
Research of the Ministry of Education, Sci-
ence, Sports and Culture, Japan. and Kayamori
Foundation of Information Science Advance-
ment.
References
Araki, M., Kimura, Y., Nishimoto, T. and Ni-
imi, Y.: Development of a Machine Learnable
Discourse Tagging Tool, Proc. of 2nd SIGdial
Workshop on Discourse and Dialogue, pp. 20?
25 (2001).
The Japanese Discouse Research Initiative
JDRI: Japanese Dialogue Corpus of Multi-
level Annotation, Proc. of 1st SIGdial Work-
shop on Discourse and Dialogue (2000).
Kawaguchi, N., Matsubara, S., Iwa, H., Ka-
jita, S, Takeda, K., Itakura, F. and Inagaki,
Y.: Construction of Speech Corpus in Mov-
ing Car Environment, Proc. of ICSLP-2000,
Vol. III, pp. 362?365 (2000).
Kawaguchi, N., Matsubara, S., Takeda, K.
and Itakura, F.: Multimedia Data Collec-
tion of In-car Speech Communication, Proc.
of Eurospeech-2001, pp. 2027?2030 (2001).
Kimura, H., Tokuhisa, M., Mera, K., Kai, K.
and Okada, N.: Comprehension of Intentions
and Planning for Response in Dialogue, Tech-
nical Report of IEICE, TL98-15, pp:25?32
(1998). (In Japanese)
Matsubara, S., Murase, T., Kawaguchi, N. and
Inagaki, Y.: Stochastic Dependency Parsing
of Spontaneous Japanese Spoken Language,
Proc. of COLING-2002 (2002).
Matsumoto, Y., Kitauchi, A., Yamashita, T.
and Hirano, Y.: Japanese Morphological
Analysis System Chasen version 2.0 Man-
ual, NAIST Techinical Report, NAIST-IS-
TR99009 (1999).
Murao, H., Kawaguchi, N., Matsubara, S. and
Inagaki, Y.: Example-based Query Genera-
tion for Spontaneous Speech, Proc. of ASRU-
2000 (2001).
Stochastic Dependency Parsing of
Spontaneous Japanese Spoken Language
Shigeki Matsubara? Takahisa Murase? Nobuo Kawaguchi?
and Yasuyoshi Inagaki?
?Information Technology Center/CIAIR, Nagoya University
?Graduate School of Engineering, Nagoya University
Furo-cho, Chikusa-ku, Nagoya, 464-8601, Japan
matubara@itc.nagoya-u.ac.jp
Abstract
This paper describes the characteristic features
of dependency structures of Japanese spoken
language by investigating a spoken dialogue cor-
pus, and proposes a stochastic approach to de-
pendency parsing. The method can robustly
cope with inversion phenomena and bunsetsus
which don?t have the head bunsetsu by relax-
ing the syntactic dependency constraints. The
method acquires in advance the probabilities
of dependencies from a spoken dialogue corpus
tagged with dependency structures, and pro-
vides the most plausible dependency structure
for each utterance on the basis of the probabili-
ties. An experiment on dependency parsing for
driver?s utterances in CIAIR in-car spoken dia-
logue corpus has been made. The experimental
result has shown our method to be effective for
robust parsing of spoken language.
1 Introduction
With the recent advances of the continuous
speech recognition technology, a considerable
number of studies have been made on spoken
dialogue systems. For the purpose of smooth
interaction with the user, it is necessary for the
system to understand the spontaneous speech.
Since spoken language includes a lot of gram-
matically ill-formed linguistic phenomena such
as fillers, hesitations and self-repairs, grammar-
oriented approaches are not necessarily suited
to spoken language processing. A technique for
robust parsing is thus strongly required.
This paper describes the characteristic fea-
tures of Japanese spoken language on the ba-
sis of investigating a large-scale spoken dialogue
corpus from the viewpoint of dependency, and
moreover, proposes a method of dependency
parsing by taking account of such the features.
The conventional methods of dependency pars-
ing have assumed the following three syntactic
constraints (Kurohashi and Nagao, 1994):
1. No dependency is directed from right to
left.
2. Dependencies don?t cross each other.
3. Each bunsetsu 1 , except the last one, de-
pends on only one bunsetsu.
As far as we have investigated the corpus, how-
ever, many spoken utterance do not satisfy
these constraints because of inversion phenom-
ena, bunsetsus which don?t have the head bun-
setsu, and so on. Therefore, our parsing method
relaxes the first and third ones among the above
three constraints, that is, permits the depen-
dency direction from right to left and the bun-
setsu which doesn?t depend on any bunsetsu.
The parsing results are expressed by partial de-
pendency structures.
The method acquires in advance the proba-
bilities of dependencies from a spoken dialogue
corpus tagged with dependency structures, and
provides the most plausible dependency struc-
ture for each utterance on the basis of the prob-
abilities. Several techniques for dependency
parsing based on stochastic approaches have
been proposed so far. Fujio and Matsumoto
have used the probability based on the fre-
quency of cooccurrence between two bunsetsus
for dependency parsing (Fujio and Matsumoto,
1998). Uchimoto et al have proposed a tech-
nique for learning the dependency probability
model based on a maximum entropy method
(Uchimoto et al, 1999). However, since these
1A bunsetsu is one of the linguistic units in Japanese,
and roughly corresponds to a basic phrase in English.
A bunsetsu consists of one independent word and more
than zero ancillary words. A dependency is a modifica-
tion relation between two bunsetsus.
techniques are for written language, whether
they are available for spoken language or not
is not clear. As the technique for stochas-
tic parsing of spoken language, Den has sug-
gested a new idea for detecting and parsing
self-repaired expressions, however, the phenom-
ena with which the framework can cope are re-
stricted (Den, 1995).
On the other hand, our method provides the
most plausible dependency structures for nat-
ural speeches by utilizing stochastic informa-
tion. In order to evaluate the effectiveness of our
method, an experiment on dependency parsing
has been made. In the experiment, all driver?s
utterances in 81 spoken dialogues of CIAIR in-
car speech dialogue corpus (Kawaguchi et al,
2001) have been used. The experimental result
has shown our method to be available for robust
parsing of spontaneously spoken language.
2 Linguistic Analysis of Spontaneous
Speech
We have investigated spontaneously spoken ut-
terances in an in-car speech dialogue corpus
which is constructed at the Center for Inte-
grated Acoustic Information Research (CIAIR),
Nagoya University (Kawaguchi et al, 2001) The
corpus contains speeches of dialogue between
drivers and navigators (humans, a Wizard of
OZ system, or a spoken dialogue system) and
their transcripts.
2.1 CIAIR In-car Speech Dialogue
Corpus
Data collection project of in-car speech dia-
logues at CIAIR has started in 1999 (Kawaguchi
et al, 2002). The project has developed a pri-
vate car, and been collecting a total of about
140 hours of multimodal data such as speeches,
images, locations and so on. These data would
be available for investigating in-car speech dia-
logues.
The speech files are transcribed into ASCII
text files by hand. The example of a tran-
script is shown in Figure 1. As an advance
analysis, discourse tags are assigned to fillers,
hesitations, and so on. Furthermore, each
speech is segmented into utterance units by
a pause, and the exact start time and end
time are provided for them. The environ-
mental information about sex (male/female),
speaker?s role (driver/navigator), dialogue task
Figure 1: Sample transcription of dialogue
speech
(navigation/information retrieval/...), noise
(noisy/clean) is provided for each utterance
unit.
In order to study the features of in-car dia-
logue speeches, we have investigated all driver?s
utterance units of 195 dialogues. The num-
ber per utterance unit of fillers, hesitations and
slips, are 0.34, 0.07, 0,04, respectively. The fact
that the frequencies are not less than those of
human-human conversations suggests the in-car
speech of the corpus to be spontaneous.
2.2 Dependency Structure of Spoken
Language
In order to characterize spontaneous dialogue
speeches from the viewpoint of dependency,
we have constructed a spoken language cor-
pus with dependency structures. Dependency
analyses have been provided by hand for all
driver?s utterance units in 81 spoken dialogues
of the in-car speech corpus. The specifications
of part-of-speeches and dependency grammars
are in accordance with those of Kyoto Corpus
(Kurohashi and Nagao, 1997), which is one of
Japanese text corpora. We have provided the
following criteria for the linguistic phenomena
peculiar to spoken language:
? There is no bunsetsu on which fillers and
hesitations depend. They forms depen-
dency structures independently.
? A bunsetsu whose head bunsetsu is omitted
doesn?t depend on any bunsetsu.
? The specification of part-of-speeches has
been provided for the phrases peculiar to
spoken language by adding lexical entries
to the dictionary.
? We have defined one conversational turn as
a unit of dependency parsing. The depen-
Table 1: Corpus data for dependency analysis
Dialogues 81
Utterance units 7,781
Conversational turns 6,078
Bunsetsus 24,993
Dependencies 11,789
Dependencies per unit 1.52
Dependencies per turn 1.94
dencies might be over two utterance units,
but be not hardly over two conversational
turns.
The outline of the corpus with dependency anal-
yses is shown in Table 1. There exist 11,789
dependencies for 24,993 bunsetsus 2. The av-
erage number of dependencies per turn is 1.94,
and is exceedingly less than that of written lan-
guage such as newspaper articles (about 10 de-
pendencies). This does not necessarily mean
that dependency parsing of spoken language is
easy than that of written language. It is also
required to specify the bunsetsu with no head
bunsetsu because every bunsetsu does not de-
pend on another bunsetsu. In fact, the bunset-
sus which don?t have the head bunsetsu occupy
52.8% of the whole.
Next, we investigated inversion phenomena
and dependencies over two utterance units. 320
inversions, 3.8% of all utterance turns and
about 0.04 times per turn, are in this data. This
fact means that the inversion phenomena can
not be ignored in spoken language processing.
About 86.5% of inversions appear at the last
bunsetsu. On the other hand, 73 dependen-
cies, providing 5.4% of 1,362 turns consisting
of more than two units, are over two utterance
units. Therefore, we can conclude that utter-
ance units are not always sufficient as parsing
units of spoken language.
3 Stochastic Dependency Parsing
Our method provides the most plausible depen-
dency analysis for each spoken language utter-
ance unit by relaxing syntactic constraints and
utilizing stochastic information acquired from a
large-scale spoken dialogue corpus. In this pa-
per, we define one turn as a parsing unit accord-
2The frequency of filler bunsetsus is 3,049.
0
1000
2000
3000
4000
5000
6000
7000
8000
-4 -2 0 2 4 6 8
Distance between bunsetsus
Number of dependencies
-3 -1 1 3 5 7
Figure 2: Distance between dependencies and
its frequencies
ing to the result of our investigation described
in Section 2.2
3.1 Dependency Structural Constraints
As Section 1 has already pointed out, most
conventional techniques for Japanese depen-
dency parsing have assumed three syntactic
constraints. Since the phenomena which are not
hardly in written language appear frequently in
spoken language, the actual dependency struc-
ture does not satisfy such the constraints. Our
method relaxes the constraints for the purpose
of robust dependency parsing. That is, our
method considers that the bunsetsus, which
don?t have the head bunsetsu, such as fillers
and hesitations, depend on themselves (relax-
ing the constraint that each bunsetsu depends
on another only one bunsetsu). Moreover, we
permit that a bunsetsu depends on its left-side
bunsetsu to cope with the inversion phenomena
(relaxing the constraint that dependencies are
directed from left to right) 3.
3.2 Utilizing Stochastic Information
Our method calculates the plausibility of the
dependency structure by utilizing the stochastic
information. The following attributes are used
for that:
? Basic forms of independent words of a de-
pendent bunsetsu b
i
and a head bunsetsu
3Since the phenomena that dependencies cross each
other is very few, the constraint is not relaxed.
Table 2: Examples of the types of dependencies
Dependent bunsetsu Type of dependency
denwa-ga (telephone) case particle ?ga?
mise-de (at a store) case particle ?de?
hayaku (early) continuous form
ookii (big) adnominal form
kaeru (can buy) adnominal form
chotto (briefly) adverb
b
j
: h
i
, h
j
? Part-of-speeches of independent words of a
dependent bunsetsu b
i
and a head bunsetsu
b
j
: t
i
, t
j
? Type of the dependency of a bunsetsu b
i
:
r
i
? Distance between bunsetsus b
i
and b
j
: d
ij
? Number of pauses between bunsetsus b
i
and b
j
: p
ij
? Location of a dependent bunsetsu b
i
: l
i
Here, if a dependent bunsetsu b
i
has an ancillary
word, the type of the dependencies of a bunsetsu
b
i
, r
i
, is the lexicon, part-of-speech and conju-
gated form of the word, and if not so, r
i
is the
part-of-speech and the conjugated form of the
last morpheme. Table 2 shows several exam-
ples of the types of dependencies. The location
of the dependent bunsetsu means whether it is
the last one of the turn or not. As Section 2 in-
dicates, the method uses the location attribute
for calculating the probability of the inversion,
because most inverse phenomena tend to appear
at the last of the turn.
The probability of the dependency between
bunsetsus are calculated using these attribute
values as follows:
P (i rel? j|B)
=
C(i ? j, h
i
, h
j
, t
i
, t
j
, r
i
)
C(h
i
, h
j
, t
i
, t
j
, r
i
)
(1)
?
C(i ? j, r
i
, d
ij
, p
ij
, l
i
)
C(r
i
, d
ij
, p
ij
, l
i
)
Here, C is a cooccurrence frequency function
and B is a sequence of bunsetsus (b
1
b
2
? ? ?b
n
).
In the formula (1), the first term of the right
hand side expresses the probability of cooccur-
rence between the independent words, and the
second term does that of the distance between
bunsetsus. The problem of data sparseness is re-
duced by considering these phenomena to be in-
dependent each other and separating the prob-
abilities into two terms. The probability that a
bunsetsu which doesn?t have the head bunsetsu
can also be calculated in formula (1) by con-
sidering such the bunsetsu to depend on itself
(i.e., i = j). The probability that a dependency
structure for a sequence of bunsetsus B is S can
be calculated from the dependency probabilities
between bunsetsus as follows.
P (S|B) =
n
?
i=1
P (i rel? j|B) (2)
For a sequence of bunsetsus, B, the method
identifies the dependency structure with
?argmax
S
P (S|B)? satisfying the following
conditions:
? Dependencies do not cross each other.
? Each bunsetsu doesn?t no more than one
head bunsetsu.
That is, our method considers the dependency
structure whose probability is maximum to be
the most plausible one.
3.3 Parsing Example
The parsing example of a user?s utterance
sentence including a filler ?eto?, a hesita-
tion ?so?, a inversion between ?nai-ka-na? and
?chikaku-ni?, and a pause, ?Eto konbini nai-ka-
na ?pause? so sono chikaku-ni (Is there a conve-
nience store near there?)? is as follows:
The sequence of bunsetsus of the sentence is
?[eto (well)],[konbini (convenience store)],[nai-
ka-na (Is there?)],?pause?, [so], [sono (there)],
[chikaku-ni (near)]?. The types of dependent
of bunsetsus and the dependency probabilities
between bunsetsus are shown in Table 2 and
3, respectively. Table 3 expresses that, for in-
stance, the probability that ?konbini? depends
on ?nai-ka-na? is 0.40. Moreover, the probabil-
ity of that ?eto? depends on ?eto? means that
the probability of that ?eto? does not depend
on any bunsetsu. Calculating the probability
of every possible structure according to Table
3, that of the dependency structure shown in
Figure 3 becomes the maximum.
Table 3: Dependency probabilities between bunsetsus
eto konbini nai-ka-na so soko-no chikaku-ni
eto (well) 1.00 0.00 0.00 0.00 0.00 0.00
konbini (convenience store) 0.00 0.01 0.40 0.00 0.00 0.00
nai-ka-na (Is there?) 0.00 0.00 0.88 0.00 0.00 0.00
so (hesitation) 0.00 0.00 0.00 1.00 0.00 0.00
soko-no (there) 0.00 0.02 0.00 0.00 0.00 0.75
chikaku-ni (near) 0.00 0.00 0.80 0.00 0.00 0.02
      eto        konbini          nai-kana                   so    soko-no  chikaku-ni<pose>
Figure 3: Dependency structure of ?eto konbini
nai-kana ?pose? so soko-no chikaku-ni?
Table 4: Experimental result of dependency
parsing
Item (a) (b) (a)+(b)
Precision 82.0% 88.5% 85.5%
Recall 64.3% 83.3% 73.8%
(a): The result for 241 bunsetsus with a head
(b): The result for 240 bunsetsus with no head
(a)+(b): The result for 481 bunsetsus
4 Parsing Experiment
In order to evaluate the effectiveness of our
method, an experiment on dependency pars-
ing has been made using a corpus of CIAIR
(Kawaguchi et al, 2001).
4.1 Outline of Experiment
We used the same data as that for our investiga-
tions in Section 2.2. That is, among all driver?s
utterance units of 81 dialogues, 100 turns were
used for the test data, and 5978 turns for the
learning data. The test data, the average bun-
setsus per turn is 4.81, consists of 481 depen-
dencies.
4.2 Experimental Result
The results of the parsing experiment are shown
partially in Figure 4. Table 4 shows the evalu-
ation. For the parsing accuracy, both precision
and recall are measured. 355 of 415 dependen-
cies extracted by our method are correct depen-
dencies, providing 85.5% for precision rate and
73.8% for recall rate. We have confirmed that
the parsing accuracy of our method for spoken
language is as high as that of another meth-
ods for written language (Fujio and Matsumoto,
1998; Uchimoto et al, 1999).
Our method correctly specified 200 of 240
bunsetsus which don?t have the head bunsetsu.
Most of them are fillers, hesitations and so on.
It became clear that it is effective to utilize the
dependency probabilities for identifying them.
5 Concluding Remarks
This paper has proposed a method for depen-
dency parsing of Japanese spoken language.
The method can execute the robust analysis by
relaxing syntactic constraints of Japanese and
utilizing stochastic information. An experiment
on CIAIR in-car spoken dialogue corpus has
shown our method to be effective for sponta-
neous speech understanding.
This experiment has been made on the as-
sumption that the speech recognition system
has a perfect performance. Since the tran-
script generated by a continuous speech recog-
nition system, however, might include a lot of
recognition errors, exceedingly robust parsing
technologies are strongly required. In order to
demonstrate our method to be practical for au-
tomatic speech transcription, an experiment us-
ing a continuous speech recognition system will
be done.
Acknowledgement: The authors would like
to thank all members of SLP Group in our lab-
oratory for their contribution to the construc-
tion of the Japanese spoken language corpus
with the dependency analysis. This work is par-
tially supported by the Grand-in-Aid for COE
Example of correct parsing for inversion
e-to nedan-wa ryoho oshiete-morae-masu-ka daitai
(Well, could you tell me both prices?) 
Example of incorrect parsing (1)
Hm.. chushajo-no aru aru-ka-naa
(Is there a caf? with a parking lot nearby ?) 
chikaku-ni kissaten-te
Example of incorrect parsing (2)
Hm.. ramen-ga
(Is there a caf? with a parking lot nearby ?) 
ikura-gurai-no arun-ka
correct result
incorrect result
right answer
Figure 4: The results of parsing experiment (a part)
Research of the Ministry of Education, Science,
Sports and Culture, Japan and Aritificial Intel-
ligence Research Promotion Foundation.
References
Den, Y.: A Unified Approach to Parsing Spoken
Natural Language, Proceedings of 3rd Natu-
ral Language Processing Pacific Rime Sympo-
sium (NLPRS?95), pp. 574?579 (1995).
Fujio, M. and Matsumoto, Y.: Japanese Depen-
dency Structure Analysis based on Lexical-
ized Statistics, Proceedings of 3rd Conference
on Empirical Method for Natural Language
Processing (EMNLP?98), pp. 87?96 (1998).
Kawaguchi, N., Matsubara, S., Takeda, K.,
and Itakura, F.: Multi-Dimensional Data Ac-
quisition for Integrated Acoustic Information
Research, Proceedings of 3rd International
Conference on Language Resources and Eval-
uation (LREC2002), pp. 2043?2046 (2002).
Kawaguchi, N., Matsubara, S., Takeda, K. and
Itakura, F.: Multimedia Data Collection of
In-car Speech Communication, Proceedings of
7th European Conference on Speech Commu-
nication and Technology (Eurospeech2001),
pp. 2027?2030 (2001).
Kurohashi, S. and Nagao, M.: Kyoto Univer-
sity Text Courpus Project, Proceedings of 3rd
Conference of Association for Natural Lan-
guage Processing, pages:115?118 (1997). (In
Japanese)
Kurohashi, S. and Nagao, M.: ?KN Parser:
Japanese Dependency/Case Structure Ana-
lyzer? Proceedings of Workshop on Sharable
Natural Language Resources, pages:48?95
(1994).
Matsumoto, Y., Kitauchi, A., Yamashita, T.
and Hirano, Y.: Japanese Morphological
Analysis System Chasen version 2.0 Man-
ual, NAIST Techinical Report, NAIST-IS-
TR99009 (1999).
Uchimoto, K., Sekine, S. and Isahara, K.:
Japanese Dependency Structure Analysis
based on Maximum Entropy Models, Pro-
ceedings of 9th European Chapter of the
Association for Computational Linguistics
(EACL?99), pp. 196?203 (1999).
Coling 2008: Companion volume ? Posters and Demonstrations, pages 119?122
Manchester, August 2008
Construction of an Infrastructure for Providing Users
with Suitable Language Resources
Hitomi Tohyama? Shunsuke Kozawa? Kiyotaka Uchimoto?
Shigeki Matsubara? and Hitoshi Isahara?
?Nagoya University, Furo-cho, Chikusa-ku, Nagoya, 464-8601, Japan
{hitomi,kozawa,matubara}@el.itc.nagoya-u.ac.jp
?National Institute of Information and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, 619-0289, Japan
{uchimioto,isahara}@nict.go.jp
Abstract
Our research organization has been con-
structing a large scale database named
SHACHI by collecting detailed meta in-
formation on language resources (LRs) in
Asia and Western countries. The metadata
database contains more than 2,000 com-
piled LRs such as corpora, dictionaries,
thesauruses and lexicons, forming a large
scale metadata of LRs archive. Its meta-
data, an extended version of OLAC meta-
data set conforming to Dublin Core, have
been collected semi-automatically. This
paper explains the design and the structure
of the metadata database, as well as the re-
alization of the catalogue search tool.
1 Introduction
The construction of LRs such as corpora, dictio-
naries, thesauruses, etc., has boomed for years
throughout the world in its aim of encouraging
research and development in the main media of
spoken and written languages, and its importance
has also been widely recognized. Of the organiza-
tions willing to store and distribute LRs, there ex-
ist some consortia fulfilling their function such as
LDC1, ELRA2, CLARIN3, and OLAC4, in West-
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1LDC:Linguistic Data Consortium,
http://www.ldc.upenn.edu/
2ELRA: European LRs Association
3CLARIN: Common Language Resources and Technolo-
gies Infrastructure, http://www.ilsp.gr/clarin eng.html
4OLAC: Open Language Archives Community,
http://www.language-archives.org/
ern countries, and GSK5 which does so mainly in
Japan. However, those released LRs are scarcely
connected with each other because of the dif-
ference between written and spoken language as
well as the difference between languages such
as Japanese, English, and Chinese (OLAC User
Guide, 2008).
This situation makes it difficult for researchers
and users to find LRs which are useful for their re-
searches. In the meantime, by connecting system-
atically existing various LRs with Wrapper Pro-
gram, the attempt to realize multilingual transla-
tion services has already begun (Ishida et al 2008,
Hayashi et al 2008). Moreover, since language in-
formation tags given to those LRs and their data
formats are multifarious, each LR is operated in-
dividually. As LR development generally entails
enormous cost, it is highly desirable that the re-
search efficiency be enhanced by systematically
combining those existing LRs altogether and ex-
tending them, which will encourage an efficient
development of unprecedented LRs.
Our research organization has been constructing
a large scale metadata database named SHACHI6
by collecting detailed meta information on LRs
in Western and Asian countries. This research
project aims to extensively collect metadata such
as tag sets, formats, and usage information about
researches on those LRs. and recorded contents of
LRs existing at home and abroad and store them
systematically. Meanwhile, we have already de-
veloped a search system of LRs by the use of meta
information and are attempting the experiment of
widely providing meta information on our stored
5GSK: Gengo Shigen Kyokai; Language Resource Asso-
ciation, http://www.gsk.or.jp/
6SHACHI: Metadata Database of Language Resources-
SHACHI, Shachi means ?orca? in English.
119
Figure 1: A sample page of SHACHI catalogue (ex. Euro WordNet)
LRs to those from researchers to common users.
This metadata database has been now open to the
public in the Web and allows every Internet user
to access it for the search and read information of
LRs at will.
2 Purpose of Metadata Database
Construction
The purpose of the construction of the database is
the following fivefold.
1. To store language resource metadata:
SHACHI semi-manually collects detailed
metadata of language resources and con-
structs their detailed catalogues. Figure 1
shows a sample page of a LR catalogue stored
in SHACHI (ex. Euro WordNet). The cata-
logue provides more detailed meta informa-
tion than other LR consortia do.
2. To systematize language resource meta-
data: Language resource ontology is tenta-
tively constructed by classifying types of lan-
guage resources (in this paper, it is called ?on-
tology?). Figure 2 shows an example of its
ontology. At the moment, it is under investi-
gation what is the most useful and functional
ontology for users by developing some on-
tologies such as human-made ontology, semi-
automatically produced ontology, and auto-
matically produced ontology.
3. To make each language resource related
to each other: The detailed metadata en-
abled us to describe characteristics of each
language resource and to expectably specify
relationships among language resources. Fig-
ure 3 shows a part of the SHACHI search
screen. It shows language resources found as
a search result, the references to which these
language resources conform as well as other
language resources whose formats are com-
mon to theirs.
4. To statistically investigate language re-
sources: By statistically analyzing the meta-
data, users are able to grasp what kinds of lan-
guage resources exist in different part of the
world and to understand current tendencies of
language resources which have been available
to the public.
5. To promote the distribution of language re-
sources: Since this metadata database en-
ables users to easily gain access to language
resources in accordance with their needs,
owing to fully equipped search functions,
SHACHI will be able to support an effective
use and an efficient development of language
resources.
Some 2,000 resources of metadata have already
been collected in the database so far and they will
be enlarged by a further 3,000. To that end, it is
120
Figure 2: Automatically produced ontology
indispensable for us to work in cooperation with
language resource consortia at home and abroad
and to take the initiative in contributing to Asian
language resources.
3 SHACHI Metadata Set
3.1 Policy for Collecting Metadata
The LRs which our metadata database stores
should satisfy the following conditions:
? Those resources should be stored in a digital
format.
? Those resources should be one of the follow-
ing: corpus, dictionary, thesaurus, or lexicon.
(Numeric data are not considered to be the
subject of collection for SHACHI.)
? Those resources should be collected from En-
glish websites and its data must be open to the
public.
? Those resources should be created by re-
search institutions, researchers, or business
organizations. (Developed tools such as facet
search.)
LRs metadata database SHACHI covers meta
information provided by LR consortia such as
ELRA, LDC, and OLAC whose more detailed
metadata are fed into the database by semi-
automatic means of importing.
3.2 Extensions of Metadata Element
Since users sometimes search for LRs without a
clear objective, it is necessary for language re-
source providers to construct language resource
ontology. This database conforms to the OLAC
metadata set which is based on 15 kinds of fun-
damental elements of Dublin Core7 and consti-
tutes an extended vision of OLAC with 19 newly
added metadata elements which were judged to
be indispensable for describing characteristics of
LRs. SHACHI provides usage information about
how and in which situation language resource re-
searchers utilized each language resource, which
is also important for users. The usage informa-
tion about LRs is automatically retrieved from aca-
demic article databases (Kozawa et al 2008). (See
?Utilization? in Figure1).
3.3 Systematic Storage of LRs
Clear description of the relations among LRs can
be applied to the efficient development of LRs
and search tools for common users of database.
Figure 2 shows ontology generated through auto-
matic means, based on language resource metadata
stored in SHACHI. We first surveyed the frequency
of possible values of metadata element choicesand
generated the ontology by hierarchicalizing meta
elements of our meta categories. While ontology
can be constructed in various ways from different
standpoints, our ontology is particularly designed
for users to enable to find them efficiently by fol-
lowing the hierarchical classes of our ontology.
4 Search Tools for Providing
Users-Oriented Information
Figure 3 shows a screen image of a search re-
sult through SHACHI. This section discusses three
search functions provided in SHACHI.
4.1 Three Types of Search Functions
For the purpose of facilitating users of this meta-
data database to find their intended language re-
source catalogues, SHACHI provides three search
functions:
1. Keyword search function: This tool is suit-
able for users who have clear images to search
7Dublin Core Metadata Initiative, http://dublincore.org/
121
Figure 3: Catalogue search tool
for specified LRs and a technical knowledge
of language processing. It allows them to in-
put keywords as they want and to search all
words stored in SHACHI metadata archive.
2. Facet search function: This tool is suitable
for users who have a vague idea of what kind
of LR they want. It is equipped with a choice
of 15 kinds of metadata elements selected
from the SHACHI metadata set. The users
narrow down the target LRs one by one in
order to find the intended one. For example,
with one click on ?age?, three choices such as
?Childrenfs utterance??, ?Adultsf utterance??
and ?Both are OK?? will be shown.
3. Ontology search function: This tool was de-
veloped by adopting the idea acquired by sys-
tematizing LRs registered in SHACHI. When
using the ontology search function, users find
the intended LRs by following the vertical re-
lationship of the ontology. It was ascertained
that ontology search function tool had the
merit of enabling users to discover LRs that
have not been ever found by keyword search
and facet search functions.
5 Conclusion
In this paper, we reported on the design of
SHACHI, a metadata database of LRs now be-
ing developed, the expansion and construction of
metadata for it, and an actualization of a search
function. At present, it contains approximately
2,000 pieces of meta information on LRs such
as corpora, dictionaries and thesauruses. One of
SHACHIfs characteristic features is that with a
collection of tag sets, format samples, and us-
age information on LRs which is automatically re-
trieved from scholarly papers given to LRs. From
now on the SHACHI project is intended to promote
cooperation among other LRs consortia abroad as
well as in Japan and to take the initiative in con-
tributing to the development of LRs in Asia.
References
Ishida,T., Nadamoto, A., Murakami,Y., Inaba, R. et al
2008. A Non-Profit Operation Model for the Lan-
guage Grid, In proceedings of the 1st International
Conference on Global Interoperability for language
Resources, pp.114-121.
Kozawa, S., Tohyama, H., Uchimoto, K., Matsubara,
S., and Isahara. H. 2008. Automatic Acquisition of
Usage Infor-mation for Language Resources, In pro-
ceedings of the 6th edition of the Language Re-
sources and Evaluation Conference.
OLAC (Open Language Archives Communi-ty), 2008.
Searching of OLAC Metadata: User Guide, http://
www.language-archives.org/tools/search/searchDoc.html
Yoshihito Hayashi, Thierry Declerck, Paul Buitelaar,
Monica Monachini. 2008. Ontologies for a Global
Language Infrastructure, In proceedings of the 1st
International Conference on Global Interoperability
for language Resources, pp.105-112.
122
Construction of Structurally Annotated Spoken Dialogue Corpus
Shingo Kato
Graduate School of Information Science,
Dept. of Information Engineering,
Nagoya University
Furo-cho, Chikusa-ku, Nagoya
gotyan@el.itc.nagoya-u.ac.jp
Shigeki Matsubara
Yukiko Yamaguchi
Nobuo Kawaguchi
Information Technology Center,
Nagoya University
Furo-cho, Chikusa-ku, Nagoya
Abstract
This paper describes the structural an-
notation of a spoken dialogue corpus.
By statistically dealing with the corpus,
the automatic acquisition of dialogue-
structural rules is achieved. The di-
alogue structure is expressed as a bi-
nary tree and 789 dialogues consist-
ing of 8150 utterances in the CIAIR
speech corpus are annotated. To eval-
uate the scalability of the corpus for
creating dialogue-structural rules, a di-
alogue parsing experiment was con-
ducted.
1 Introduction
With the improvement of speech processing tech-
nologies, spoken dialogue systems that appropri-
ately respond to a user?s spontaneous utterances
and cooperatively execute a dialogue are desired.
It is important for cooperative spoken dialogue
systems to understand the intentions of a user?s
utterances, the purpose of the dialogue, and its
achievement state (Litman, 1990). To solve this
issue, several approaches have been so far pro-
posed. One of them is an approach in which the
system expresses the knowledge of the dialogue
with a frame and executes the dialogue accord-
ing to that frame (Goddeau, 1996; Niimi, 2001;
Oku, 2004). However, it is difficult to make a
frame that totally defines the content of the dia-
logue. Additionally, there is a tendency for the
dialogue style to be greatly affected by the frame.
Figure 1: The data collection vehicle(DCV)
In this paper, we describe the construction of
a structurally annotated spoken dialogue corpus.
By statistically dealing with the corpus, we can
achieve the automatic acquisition of dialogue-
structural rules. We suppose that the system can
figure out the state of the dialogue through the in-
cremental building of the dialogue structure.
We use the CIAIR in-car spoken dialogue cor-
pus (Kawaguchi, 2004; Kawaguchi, 2005), and
describe the dialogue structure as a binary tree.
The tree expresses the purpose of partial dia-
logues and the relations between utterances or
partial dialogues. The speaker?s intention tags
were provided in the transcription of the corpus.
We annotated 789 dialogues consisting of 8150
utterances. Due to the advantages of the dialogue-
40
0022 - 01:37:398-01:41:513 F:D:I:C:
(F ) [FILLER:well] &(F )
	
	 [delicious] &
 [Udon] &
 [restaurant] &
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 169?176,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Dependency Parsing of Japanese Spoken Monologue
Based on Clause Boundaries
Tomohiro Ohno?a) Shigeki Matsubara? Hideki Kashioka?
Takehiko Maruyama] and Yasuyoshi Inagaki\
?Graduate School of Information Science, Nagoya University, Japan
?Information Technology Center, Nagoya University, Japan
?ATR Spoken Language Communication Research Laboratories, Japan
]The National Institute for Japanese Language, Japan
\Faculty of Information Science and Technology, Aichi Prefectural University, Japan
a)ohno@el.itc.nagoya-u.ac.jp
Abstract
Spoken monologues feature greater sen-
tence length and structural complexity
than do spoken dialogues. To achieve high
parsing performance for spoken mono-
logues, it could prove effective to sim-
plify the structure by dividing a sentence
into suitable language units. This paper
proposes a method for dependency pars-
ing of Japanese monologues based on sen-
tence segmentation. In this method, the
dependency parsing is executed in two
stages: at the clause level and the sen-
tence level. First, the dependencies within
a clause are identified by dividing a sen-
tence into clauses and executing stochastic
dependency parsing for each clause. Next,
the dependencies over clause boundaries
are identified stochastically, and the de-
pendency structure of the entire sentence
is thus completed. An experiment using
a spoken monologue corpus shows this
method to be effective for efficient depen-
dency parsing of Japanese monologue sen-
tences.
1 Introduction
Recently, monologue data such as a lecture and
commentary by a professional have been consid-
ered as human valuable intellectual property and
have gathered attention. In applications, such as
automatic summarization, machine translation and
so on, for using these monologue data as intel-
lectual property effectively and efficiently, it is
necessary not only just to accumulate but also to
structure the monologue data. However, few at-
tempts have been made to parse spoken mono-
logues. Spontaneously spoken monologues in-
clude a lot of grammatically ill-formed linguistic
phenomena such as fillers, hesitations and self-
repairs. In order to robustly deal with their extra-
grammaticality, some techniques for parsing of di-
alogue sentences have been proposed (Core and
Schubert, 1999; Delmonte, 2003; Ohno et al,
2005b). On the other hand, monologues also have
the characteristic feature that a sentence is gen-
erally longer and structurally more complicated
than a sentence in dialogues which have been dealt
with by the previous researches. Therefore, for
a monologue sentence the parsing time would in-
crease and the parsing accuracy would decrease. It
is thought that more effective, high-performance
spoken monologue parsing could be achieved by
dividing a sentence into suitable language units for
simplicity.
This paper proposes a method for dependency
parsing of monologue sentences based on sen-
tence segmentation. The method executes depen-
dency parsing in two stages: at the clause level
and at the sentence level. First, a dependency rela-
tion from one bunsetsu1 to another within a clause
is identified by dividing a sentence into clauses
based on clause boundary detection and then ex-
ecuting stochastic dependency parsing for each
clause. Next, the dependency structure of the en-
tire sentence is completed by identifying the de-
pendencies over clause boundaries stochastically.
An experiment on monologue dependency pars-
ing showed that the parsing time can be drasti-
1A bunsetsu is the linguistic unit in Japanese that roughly
corresponds to a basic phrase in English. A bunsetsu con-
sists of one independent word and more than zero ancillary
words. A dependency is a modification relation in which a
dependent bunsetsu depends on a head bunsetsu. That is, the
dependent bunsetsu and the head bunsetsu work as modifier
and modifyee, respectively.
169
??
???
??
???
??? ????
???
??
????
?????
?
???
????
???
?
????
????
??
?Dependency relation whose dependent bunsetsu is not the last bunsetsu of a clause
?Dependency relation whose dependent bunsetsu is the last bunsetsu of a clause
?Bunsetsu
?Clause boundary
?Clause
The public opinion poll that the Prime Minister?s Office announced the other day indicates that 
the ratio of people advocating capital punishment is nearly 80%
the other
day
that the 
Prime
Minister?s
Office
announced The 
public
opinion
poll
indicates
that
capital
punishment
advocating the ratio 
of people
nearly
80%
is
Figure 1: Relation between clause boundary and
dependency structure
cally shortened and the parsing accuracy can be
increased.
This paper is organized as follows: The next
section describes a parsing unit of Japanese mono-
logue. Section 3 presents dependency parsing
based on clause boundaries. The parsing experi-
ment and the discussion are reported in Sections
4 and 5, respectively. The related works are de-
scribed in Section 6.
2 Parsing Unit of Japanese Monologues
Our method achieves an efficient parsing by adopt-
ing a shorter unit than a sentence as a parsing unit.
Since the search range of a dependency relation
can be narrowed by dividing a long monologue
sentence into small units, we can expect the pars-
ing time to be shortened.
2.1 Clauses and Dependencies
In Japanese, a clause basically contains one verb
phrase. Therefore, a complex sentence or a com-
pound sentence contains one or more clauses.
Moreover, since a clause constitutes a syntacti-
cally sufficient and semantically meaningful lan-
guage unit, it can be used as an alternative parsing
unit to a sentence.
Our proposed method assumes that a sentence
is a sequence of one or more clauses, and every
bunsetsu in a clause, except the final bunsetsu,
depends on another bunsetsu in the same clause.
As an example, the dependency structure of the
Japanese sentence:
????????????????????
?????????????????????
?????????????The public opinion
poll that the Prime Minister?s Office announced
the other day indicates that the ratio of people
advocating capital punishment is nearly 80%)
is presented in Fig. 1. This sentence consists of
four clauses:
? ?????????????? (that the
Prime Minister?s Office announced the other
day)
? ?????????? (The public opinion
poll indicates that)
? ?????????? (advocating capital
punishment)
? ???????????????????
(the ratio of people is nearly 80%)
Each clause forms a dependency structure (solid
arrows in Fig. 1), and a dependency relation from
the final bunsetsu links the clause with another
clause (dotted arrows in Fig. 1).
2.2 Clause Boundary Unit
In adopting a clause as an alternative parsing unit,
it is necessary to divide a monologue sentence
into clauses as the preprocessing for the follow-
ing dependency parsing. However, since some
kinds of clauses are embedded in main clauses,
it is fundamentally difficult to divide a mono-
logue into clauses in one dimension (Kashioka and
Maruyama, 2004).
Therefore, by using a clause boundary anno-
tation program (Maruyama et al, 2004), we ap-
proximately achieve the clause segmentation of
a monologue sentence. This program can iden-
tify units corresponding to clauses by detecting
the end boundaries of clauses. Furthermore, the
program can specify the positions and types of
clause boundaries simply from a local morpho-
logical analysis. That is, for a sentence mor-
phologically analyzed by ChaSen (Matsumoto et
al., 1999), the positions of clause boundaries are
identified and clause boundary labels are inserted
there. There exist 147 labels such as ?compound
clause? and ?adnominal clause.? 2
In our research, we adopt the unit sandwiched
between two clause boundaries detected by clause
boundary analysis, were called the clause bound-
ary unit, as an alternative parsing unit. Here, we
regard the label name provided for the end bound-
ary of a clause boundary unit as that unit?s type.
2The labels include a few other constituents that do not
strictly represent clause boundaries but can be regarded as be-
ing syntactically independent elements, such as ?topicalized
element,? ?conjunctives,? ?interjections,? and so on.
170
Table 1: 200 sentences in ?Asu-Wo-Yomu?
sentences 200
clause boundary units 951
bunsetsus 2,430
morphemes 6,017
dependencies over clause boundaries 94
2.3 Relation between Clause Boundary Units
and Dependency Structures
To clarify the relation between clause boundary
units and dependency structures, we investigated
the monologue corpus ?Asu-Wo-Yomu 3.? In the
investigation, we used 200 sentences for which
morphological analysis, bunsetsu segmentation,
clause boundary analysis, and dependency pars-
ing were automatically performed and then modi-
fied by hand. Here, the specification of the parts-
of-speech is in accordance with that of the IPA
parts-of-speech used in the ChaSen morphologi-
cal analyzer (Matsumoto et al, 1999), the rules
of the bunsetsu segmentation with those of CSJ
(Maekawa et al, 2000), the rules of the clause
boundary analysis with those of Maruyama et
al. (Maruyama et al, 2004), and the dependency
grammar with that of the Kyoto Corpus (Kuro-
hashi and Nagao, 1997).
Table 1 shows the results of analyzing the 200
sentences. Among the 1,479 bunsetsus in the dif-
ference set between all bunsetsus (2,430) and the
final bunsetsus (951) of clause boundary units,
only 94 bunsetsus depend on a bunsetsu located
outside the clause boundary unit. This result
means that 93.6% (1,385/1,479) of all dependency
relations are within a clause boundary unit. There-
fore, the results confirmed that the assumption
made by our research is valid to some extent.
3 Dependency Parsing Based on Clause
Boundaries
In accordance with the assumption described in
Section 2, in our method, the transcribed sentence
on which morphological analysis, clause bound-
ary detection, and bunsetsu segmentation are per-
formed is considered the input 4. The dependency
3Asu-Wo-Yomu is a collection of transcriptions of a TV
commentary program of the Japan Broadcasting Corporation
(NHK). The commentator speaks on some current social is-
sue for 10 minutes.
4It is difficult to preliminarily divide a monologue into
sentences because there are no clear sentence breaks in mono-
logues. However, since some methods for detecting sentence
boundaries have already been proposed (Huang and Zweig,
2002; Shitaoka et al, 2004), we assume that they can be de-
tected automatically before dependency parsing.
parsing is executed based on the following proce-
dures:
1. Clause-level parsing: The internal depen-
dency relations of clause boundary units are
identified for every clause boundary unit in
one sentence.
2. Sentence-level parsing: The dependency
relations in which the dependent unit is the fi-
nal bunsetsu of the clause boundary units are
identified.
In this paper, we describe a sequence of clause
boundary units in a sentence as C1 ? ? ?Cm, a se-
quence of bunsetsus in a clause boundary unit Ci
as bi1 ? ? ? bini , a dependency relation in which the
dependent bunsetsu is a bunsetsu bik as dep(bik),
and a dependency structure of a sentence as
{dep(b11), ? ? ? , dep(bmnm?1)}.
First, our method parses the dependency struc-
ture {dep(bi1), ? ? ? , dep(bini?1)} within the clause
boundary unit whenever a clause boundary unit
Ci is inputted. Then, it parses the dependency
structure {dep(b1n1), ? ? ? , dep(bm?1nm?1)}, which is a
set of dependency relations whose dependent bun-
setsu is the final bunsetsu of each clause boundary
unit in the input sentence. In addition, in both of
the above procedures, our method assumes the fol-
lowing three syntactic constraints:
1. No dependency is directed from right to left.
2. Dependencies don?t cross each other.
3. Each bunsetsu, except the final one in a sen-
tence, depends on only one bunsetsu.
These constraints are usually used for Japanese de-
pendency parsing.
3.1 Clause-level Dependency Parsing
Dependency parsing within a clause boundary
unit, when the sequence of bunsetsus in an input
clause boundary unit Ci is described as Bi (=
bi1 ? ? ? bini), identifies the dependency structure
Si (= {dep(bi1), ? ? ? , dep(bini?1)}), which max-
imizes the conditional probability P (Si|Bi). At
this level, the head bunsetsu of the final bunsetsu
bini of a clause boundary unit is not identified.
Assuming that each dependency is independent
of the others, P (Si|Bi) can be calculated as fol-
lows:
P (Si|Bi) =
ni?1?
k=1
P (bik rel? bil|Bi), (1)
171
where P (bik
rel? bil|Bi) is the probability that a bun-
setsu bik depends on a bunsetsu bil when the se-
quence of bunsetsus Bi is provided. Unlike the
conventional stochastic sentence-by-sentence de-
pendency parsing method, in our method, Bi is
the sequence of bunsetsus that constitutes not a
sentence but a clause. The structure Si, which
maximizes the conditional probability P (Si|Bi),
is regarded as the dependency structure of Bi and
calculated by dynamic programming (DP).
Next, we explain the calculation of P (bik
rel?
bil|Bi). First, the basic form of independent words
in a dependent bunsetsu is represented by hik, its
parts-of-speech tik, and type of dependency rik,
while the basic form of the independent word in
a head bunsetsu is represented by hil , and its parts-
of-speech til . Furthermore, the distance between
bunsetsus is described as diikl. Here, if a dependent
bunsetsu has one or more ancillary words, the type
of dependency is the lexicon, part-of-speech and
conjugated form of the rightmost ancillary word,
and if not so, it is the part-of-speech and conju-
gated form of the rightmost morpheme. The type
of dependency rik is the same attribute used in
our stochastic method proposed for robust depen-
dency parsing of spoken language dialogue (Ohno
et al, 2005b). Then diikl takes 1 or more than 1,
that is, a binary value. Incidentally, the above
attributes are the same as those used by the con-
ventional stochastic dependency parsing methods
(Collins, 1996; Ratnaparkhi, 1997; Fujio and Mat-
sumoto, 1998; Uchimoto et al, 1999; Charniak,
2000; Kudo and Matsumoto, 2002).
Additionally, we prepared the attribute eil to in-
dicate whether bil is the final bunsetsu of a clause
boundary unit. Since we can consider a clause
boundary unit as a unit corresponding to a sim-
ple sentence, we can treat the final bunsetsu of a
clause boundary unit as a sentence-end bunsetsu.
The attribute that indicates whether a head bun-
setsu is a sentence-end bunsetsu has often been
used in conventional sentence-by-sentence parsing
methods (e.g. Uchimoto et al, 1999).
By using the above attributes, the conditional
probability P (bik
rel? bil|Bi) is calculated as fol-
lows:
P (bik rel? bil|Bi) (2)
?= P (bik rel? bil|hik, hil, tik, til, rik, diikl, eil)
= F (b
i
k
rel? bil, hik, hil, tik, til, rik, diikl, eil)
F (hik, hil, tik, til, rik, diikl, eil)
.
Note that F is a co-occurrence frequency function.
In order to resolve the sparse data problems
caused by estimating P (bik
rel? bil|Bi) with formula
(2), we adopted the smoothing method described
by Fujio and Matsumoto (Fujio and Matsumoto,
1998): if F (hik, hil, tik, til, rik, diikl, eil) in formula (2)
is 0, we estimate P (bik
rel? bil|Bi) by using formula
(3).
P (bik rel? bil|Bi) (3)
?= P (bik rel? bil|tik, til, rik, diikl, eil)
= F (b
i
k
rel? bil, tik, til, rik, diikl, eil)
F (tik, til, rik, diikl, eil)
3.2 Sentence-level Dependency Parsing
Here, the head bunsetsu of the final bunsetsu
of a clause boundary unit is identified. Let
B (=B1 ? ? ?Bn) be the sequence of bunset-
sus of one sentence and Sfin be a set of de-
pendency relations whose dependent bunsetsu is
the final bunsetsu of a clause boundary unit,
{dep(b1n1), ? ? ? , dep(bm?1nm?1)}; then Sfin, which
makes P (Sfin|B) the maximum, is calculated by
DP. The P (Sfin|B) can be calculated as follows:
P (Sfin|B) =
m?1?
i=1
P (bini
rel? bjl |B), (4)
where P (bini
rel? bjl |B) is the probability that a
bunsetsu bini depends on a bunsetsu bjl when the
sequence of the sentence?s bunsetsus, B, is pro-
vided. Our method parses by giving consideration
to the dependency structures in each clause bound-
ary unit, which were previously parsed. That is,
the method does not consider all bunsetsus lo-
cated on the right-hand side as candidates for a
head bunsetsu but calculates only dependency re-
lations within each clause boundary unit that do
not cross any other relation in previously parsed
dependency structures. In the case of Fig. 1,
the method calculates by assuming that only three
bunsetsus ??? (the ratio of people),? or ???
????? (is)? can be the head bunsetsu of the
bunsetsu ???????? (advocating).?
In addition, P (bini
rel? bjl |B) is calculated as in
Eq. (5). Equation (5) uses all of the attributes used
in Eq. (2), in addition to the attribute sjl , which
indicates whether the head bunsetsu of bjl is the
final bunsetsu of a sentence. Here, we take into
172
Table 2: Size of experimental data set (Asu-Wo-
Yomu)
test data learning data
programs 8 95
sentences 500 5,532
clause boundary units 2,237 26,318
bunsetsus 5,298 65,821
morphemes 13,342 165,129
Note that the commentator of each program is different.
Table 3: Experimental results on parsing time
our method conv. method
average time (msec) 10.9 51.9
programming language: LISP
computer used: Pentium4 2.4 GHz, Linux
account the analysis result that about 70% of the
final bunsetsus of clause boundary units depend on
the final bunsetsu of other clause boundary units 5
and also use the attribute ejl at this phase.
P (bini
rel? bjl |B) (5)
?= P (bini
rel?bjl |hini , hjl , tini , tjl , rini , dijnil, e
j
l , sjl )
= F (b
ini
rel?bjl , hini , hjl , tini , tjl , rini , dijnil, e
j
l , sjl )
F (hini , hjl , tini , tjl , rini , dijnil, e
j
l , sjl )
4 Parsing Experiment
To evaluate the effectiveness of our method for
Japanese spoken monologue, we conducted an ex-
periment on dependency parsing.
4.1 Outline of Experiment
We used the spoken monologue corpus? Asu-
Wo-Yomu,?annotated with information on mor-
phological analysis, clause boundary detection,
bunsetsu segmentation, and dependency analy-
sis6. Table 2 shows the data used for the ex-
periment. We used 500 sentences as the test
data. Although our method assumes that a depen-
dency relation does not cross clause boundaries,
there were 152 dependency relations that contra-
dicted this assumption. This means that the depen-
dency accuracy of our method is not over 96.8%
(4,646/4,798). On the other hand, we used 5,532
sentences as the learning data.
To carry out comparative evaluation of our
method?s effectiveness, we executed parsing for
5We analyzed the 200 sentences described in Section 2.3
and confirmed 70.6% (522/751) of the final bunsetsus of
clause boundary units depended on the final bunsetsu of other
clause boundary units.
6Here, the specifications of these annotations are in accor-
dance with those described in Section 2.3.
0
50
100
150
200
250
300
350
400
0 5 10 15 20 25 30
Pa
rs
in
g 
tim
e 
[m
se
c]
Length of sentence [number of bunsetsu]
our method
conv. method
Figure 2: Relation between sentence length and
parsing time
the above-mentioned data by the following two
methods and obtained, respectively, the parsing
time and parsing accuracy.
? Our method: First, our method provides
clause boundaries for a sequence of bunset-
sus of an input sentence and identifies all
clause boundary units in a sentence by per-
forming clause boundary analysis (CBAP)
(Maruyama et al, 2004). After that, our
method executes the dependency parsing de-
scribed in Section 3.
? Conventional method: This method parses
a sentence at one time without dividing it into
clause boundary units. Here, the probability
that a bunsetsu depends on another bunsetsu,
when the sequence of bunsetsus of a sentence
is provided, is calculated as in Eq. (5), where
the attribute e was eliminated. This conven-
tional method has been implemented by us
based on the previous research (Fujio and
Matsumoto, 1998).
4.2 Experimental Results
The parsing times of both methods are shown in
Table 3. The parsing speed of our method im-
proves by about 5 times on average in comparison
with the conventional method. Here, the parsing
time of our method includes the time taken not
only for the dependency parsing but also for the
clause boundary analysis. The average time taken
for clause boundary analysis was about 1.2 mil-
lisecond per sentence. Therefore, the time cost of
performing clause boundary analysis as a prepro-
cessing of dependency parsing can be considered
small enough to disregard. Figure 2 shows the re-
lation between sentence length and parsing time
173
Table 4: Experimental results on parsing accuracy
our method conv. method
bunsetsu within a clause boundary unit (except final bunsetsu) 88.2% (2,701/3,061) 84.7% (2,592/3,061)
final bunsetsu of a clause boundary unit 65.6% (1,140/1,737) 63.3% (1,100/1,737)
total 80.1% (3,841/4,798) 76.9% (3,692/4,798)
Table 5: Experimental results on clause boundary
analysis (CBAP)
recall 95.7% (2,140/2,237)
precision 96.9% (2,140/2,209)
for both methods, and it is clear from this figure
that the parsing time of the conventional method
begins to rapidly increase when the length of a
sentence becomes 12 or more bunsetsus. In con-
trast, our method changes little in relation to pars-
ing time. Here, since the sentences used in the
experiment are composed of 11.8 bunsetsus on av-
erage, this result shows that our method is suitable
for improving the parsing time of a monologue
sentence whose length is longer than the average.
Table 4 shows the parsing accuracy of both
methods. The first line of Table 4 shows the
parsing accuracy for all bunsetsus within clause
boundary units except the final bunsetsus of the
clause boundary units. The second line shows
the parsing accuracy for the final bunsetsus of
all clause boundary units except the sentence-end
bunsetsus. We confirmed that our method could
analyze with a higher accuracy than the conven-
tional method. Here, Table 5 shows the accu-
racy of the clause boundary analysis executed by
CBAP. Since the precision and recall is high, we
can assume that the clause boundary analysis ex-
erts almost no harmful influence on the following
dependency parsing.
As mentioned above, it is clear that our method
is more effective than the conventional method in
shortening parsing time and increasing parsing ac-
curacy.
5 Discussions
Our method assumes that dependency relations
within a clause boundary unit do not cross clause
boundaries. Due to this assumption, the method
cannot correctly parse the dependency relations
over clause boundaries. However, the experi-
mental results indicated that the accuracy of our
method was higher than that of the conventional
method.
In this section, we first discuss the effect of our
method on parsing accuracy, separately for bun-
Table 6: Comparison of parsing accuracy between
conventional method and our method (for bunsetsu
within a clause boundary unit except final bun-
setsu)``````````conv. method
our method
correct incorrect total
correct 2,499 93 2,592
incorrect 202 267 469
total 2,701 360 3,061
setsus within clause boundary units (except the fi-
nal bunsetsus) and the final bunsetsus of clause
boundary units. Next, we discuss the problem of
our method?s inability to parse dependency rela-
tions over clause boundaries.
5.1 Parsing Accuracy for Bunsetsu within a
Clause Boundary Unit (except final
bunsetsu)
Table 6 compares parsing accuracies for bunsetsus
within clause boundary units (except the final bun-
setsus) between the conventional method and our
method. There are 3,061 bunsetsus within clause
boundary units except the final bunsetsu, among
which 2,499 were correctly parsed by both meth-
ods. There were 202 dependency relations cor-
rectly parsed by our method but incorrectly parsed
by the conventional method. This means that our
method can narrow down the candidates for a head
bunsetsu.
In contrast, 93 dependency relations were cor-
rectly parsed solely by the conventional method.
Among these, 46 were dependency relations over
clause boundaries, which cannot in principle be
parsed by our method. This means that our method
can correctly parse almost all of the dependency
relations that the conventional method can cor-
rectly parse except for dependency relations over
clause boundaries.
5.2 Parsing Accuracy for Final Bunsetsu of a
Clause Boundary Unit
We can see from Table 4 that the parsing accuracy
for the final bunsetsus of clause boundary units by
both methods is much worse than that for bunset-
sus within the clause boundary units (except the
final bunsetsus). This means that it is difficult
174
Table 7: Comparison of parsing accuracy between
conventional method and our method (for final
bunsetsu of a clause boundary unit)``````````conv. method
our method
correct incorrect total
correct 1037 63 1,100
incorrect 103 534 637
total 1,140 597 1,737
Table 8: Parsing accuracy for dependency rela-
tions over clause boundaries
our method conv. method
recall 1.3% (2/152) 30.3% (46/152)
precision 11.8% (2/ 17) 25.3% (46/182)
to identify dependency relations whose dependent
bunsetsu is the final one of a clause boundary unit.
Table 7 compares how the two methods parse
the dependency relations when the dependent bun-
setsu is the final bunsetsu of a clause bound-
ary unit. There are 1,737 dependency relations
whose dependent bunsetsu is the final bunsetsu of
a clause boundary unit, among which 1,037 were
correctly parsed by both methods. The number
of dependency relations correctly parsed only by
our method was 103. This number is higher than
that of dependency relations correctly parsed by
only the conventional method. This result might
be attributed to our method?s effect; that is, our
method narrows down the candidates internally for
a head bunsetsu based on the first-parsed depen-
dency structure for clause boundary units.
5.3 Dependency Relations over Clause
Boundaries
Table 8 shows the accuracy of both methods for
parsing dependency relations over clause bound-
aries. Since our method parses based on the as-
sumption that those dependency relations do not
exist, it cannot correctly parse anything. Al-
though, from the experimental results, our method
could identify two dependency relations over
clause boundaries, these were identified only be-
cause dependency parsing for some sentences was
performed based on wrong clause boundaries that
were provided by clause boundary analysis. On
the other hand, the conventional method correctly
parsed 46 dependency relations among 152 that
crossed a clause boundary in the test data. Since
the conventional method could correctly parse
only 30.3% of those dependency relations, we can
see that it is in principle difficult to identify the
dependency relations.
6 Related Works
Since monologue sentences tend to be long and
have complex structures, it is important to con-
sider the features. Although there have been
very few studies on parsing monologue sentences,
some studies on parsing written language have
dealt with long-sentence parsing. To resolve the
syntactic ambiguity of a long sentence, some of
them have focused attention on the ?clause.?
First, there are the studies that focused atten-
tion on compound clauses (Agarwal and Boggess,
1992; Kurohashi and Nagao, 1994). These tried
to improve the parsing accuracy of long sentences
by identifying the boundaries of coordinate struc-
tures. Next, other research efforts utilized the three
categories into which various types of subordinate
clauses are hierarchically classified based on the
?scope-embedding preference? of Japanese subor-
dinate clauses (Shirai et al, 1995; Utsuro et al,
2000). Furthermore, Kim et al (Kim and Lee,
2004) divided a sentence into ?S(ubject)-clauses,?
which were defined as a group of words containing
several predicates and their common subject. The
above studies have attempted to reduce the pars-
ing ambiguity between specific types of clauses in
order to improve the parsing accuracy of an entire
sentence.
On the other hand, our method utilizes all types
of clauses without limiting them to specific types
of clauses. To improve the accuracy of long-
sentence parsing, we thought that it would be more
effective to cyclopaedically divide a sentence into
all types of clauses and then parse the local de-
pendency structure of each clause. Moreover,
since our method can perform dependency pars-
ing clause-by-clause, we can reasonably expect
our method to be applicable to incremental pars-
ing (Ohno et al, 2005a).
7 Conclusions
In this paper, we proposed a technique for de-
pendency parsing of monologue sentences based
on clause-boundary detection. The method can
achieve more effective, high-performance spoken
monologue parsing by dividing a sentence into
clauses, which are considered as suitable language
units for simplicity. To evaluate the effectiveness
of our method for Japanese spoken monologue, we
conducted an experiment on dependency parsing
of the spoken monologue sentences recorded in
the ?Asu-Wo-Yomu.? From the experimental re-
175
sults, we confirmed that our method shortened the
parsing time and increased the parsing accuracy
compared with the conventional method, which
parses a sentence without dividing it into clauses.
Future research will include making a thorough
investigation into the relation between dependency
type and the type of clause boundary unit. After
that, we plan to investigate techniques for identi-
fying the dependency relations over clause bound-
aries. Furthermore, as the experiment described in
this paper has shown the effectiveness of our tech-
nique for dependency parsing of long sentences
in spoken monologues, so our technique can be
expected to be effective in written language also.
Therefore, we want to examine the effectiveness
by conducting the parsing experiment of long sen-
tences in written language such as newspaper arti-
cles.
8 Acknowledgements
This research was supported in part by a contract
with the Strategic Information and Communica-
tions R&D Promotion Programme, Ministry of In-
ternal Affairs and Communications and the Grand-
in-Aid for Young Scientists of JSPS. The first au-
thor is partially supported by JSPS Research Fel-
lowships for Young Scientists.
References
R. Agarwal and L. Boggess. 1992. A simple but use-
ful approach to conjunct indentification. In Proc. of
30th ACL, pages 15?21.
E. Charniak. 2000. A maximum-entropy-inspired
parser. In Proc. of 1st NAACL, pages 132?139.
M. Collins. 1996. A new statistical parser based on
bigram lexical dependencies. In Proc. of 34th ACL,
pages 184?191.
Mark G. Core and Lenhart K. Schubert. 1999. A syn-
tactic framework for speech repairs and other dis-
ruptions. In Proc. of 37th ACL, pages 413?420.
R. Delmonte. 2003. Parsing spontaneous speech. In
Proc. of 8th EUROSPEECH, pages 1999?2004.
M. Fujio and Y. Matsumoto. 1998. Japanese depen-
dency structure analysis based on lexicalized statis-
tics. In Proc. of 3rd EMNLP, pages 87?96.
J. Huang and G. Zweig. 2002. Maximum entropy
model for punctuation annotation from speech. In
Proc. of 7th ICSLP, pages 917?920.
H. Kashioka and T. Maruyama. 2004. Segmentation
of semantic unit in Japanese monologue. In Proc. of
ICSLT-O-COCOSDA 2004, pages 87?92.
M. Kim and J. Lee. 2004. Syntactic analysis of long
sentences based on s-clauses. In Proc. of 1st IJC-
NLP, pages 420?427.
T. Kudo and Y. Matsumoto. 2002. Japanese depen-
dency analyisis using cascaded chunking. In Proc.
of 6th CoNLL, pages 63?69.
S. Kurohashi and M. Nagao. 1994. A syntactic analy-
sis method of long Japanese sentences based on the
detection of conjunctive structures. Computational
Linguistics, 20(4):507?534.
S. Kurohashi and M. Nagao. 1997. Building a
Japanese parsed corpus while improving the parsing
system. In Proc. of 4th NLPRS, pages 451?456.
K. Maekawa, H. Koiso, S. Furui, and H. Isahara. 2000.
Spontaneous speech corpus of Japanese. In Proc. of
2nd LREC, pages 947?952.
T. Maruyama, H. Kashioka, T. Kumano, and
H. Tanaka. 2004. Development and evaluation
of Japanese clause boundaries annotation program.
Journal of Natural Language Processing, 11(3):39?
68. (In Japanese).
Y. Matsumoto, A. Kitauchi, T. Yamashita, and Y. Hi-
rano, 1999. Japanese Morphological Analysis Sys-
tem ChaSen version 2.0 Manual. NAIST Technical
Report, NAIST-IS-TR99009.
T. Ohno, S. Matsubara, H. Kashioka, N. Kato, and
Y. Inagaki. 2005a. Incremental dependency pars-
ing of Japanese spoken monologue based on clause
boundaries. In Proc. of 9th EUROSPEECH, pages
3449?3452.
T. Ohno, S. Matsubara, N. Kawaguchi, and Y. Inagaki.
2005b. Robust dependency parsing of spontaneous
Japanese spoken language. IEICE Transactions on
Information and Systems, E88-D(3):545?552.
A. Ratnaparkhi. 1997. A liner observed time statistical
parser based on maximum entropy models. In Proc.
of 2nd EMNLP, pages 1?10.
S. Shirai, S. Ikehara, A. Yokoo, and J. Kimura. 1995.
A new dependency analysis method based on se-
mantically embedded sentence structures and its per-
formance on Japanese subordinate clause. Jour-
nal of Information Processing Society of Japan,
36(10):2353?2361. (In Japanese).
K. Shitaoka, K. Uchimoto, T. Kawahara, and H. Isa-
hara. 2004. Dependency structure analysis and sen-
tence boundary detection in spontaneous Japanese.
In Proc. of 20th COLING, pages 1107?1113.
K. Uchimoto, S. Sekine, and K. Isahara. 1999.
Japanese dependency structure analysis based on
maximum entropy models. In Proc. of 9th EACL,
pages 196?203.
T. Utsuro, S. Nishiokayama, M. Fujio, and Y. Mat-
sumoto. 2000. Analyzing dependencies of Japanese
subordinate clauses based on statistics of scope em-
bedding preference. In Proc. of 6th ANLP, pages
110?117.
176
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 683?690,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Simultaneous English-Japanese Spoken Language Translation
Based on Incremental Dependency Parsing and Transfer
Koichiro Ryu
Graduate School of
Information Science,
Nagoya University
Furo-cho, Chikusa-ku,
Nagoya, 464-8601, Japan
ryu@el.itc.nagoya-u.ac.jp
Shigeki Matsubara
Information Technology Center,
Nagoya University
Furo-cho, Chikusa-ku,
Nagoya, 464-8601, Japan
Yasuyoshi Inagaki
Faculty of
Information Science
and Technology,
Aichi Prefectural University
Nagakute-cho, Aichi-gun,
Aichi-ken, 480-1198, Japan
Abstract
This paper proposes a method for incre-
mentally translating English spoken lan-
guage into Japanese. To realize simulta-
neous translation between languages with
different word order, such as English and
Japanese, our method utilizes the feature
that the word order of a target language
is flexible. To resolve the problem of
generating a grammatically incorrect sen-
tence, our method uses dependency struc-
tures and Japanese dependency constraints
to determine the word order of a transla-
tion. Moreover, by considering the fact
that the inversion of predicate expressions
occurs more frequently in Japanese spo-
ken language, our method takes advan-
tage of a predicate inversion to resolve the
problem that Japanese has the predicate at
the end of a sentence. Furthermore, our
method includes the function of canceling
an inversion by restating a predicate when
the translation is incomprehensible due to
the inversion. We implement a prototype
translation system and conduct an exper-
iment with all 578 sentences in the ATIS
corpus. The results indicate improvements
in comparison to two other methods.
1 Introduction
Recently, speech-to-speech translation has be-
come one of the important research topics in
machine translation. Projects concerning speech
translation such as TC-STAR (Hoge, 2002) and
DARPA Babylon have been executed, and con-
ferences on spoken language translation such as
IWSLT have been held. Though some speech
translation systems have been developed so far
(Frederking et al, 2002; Isotani et al, 2003; Liu
et al, 2003; Takezawa et al, 1998), these systems,
because of their sentence-by-sentence translation,
cannot start to translate a sentence until it has been
fully uttered. The following problems may arise in
cross-language communication:
? The conversation time become long since it
takes much time to translate
? The listener has to wait for the translation
since such systems increase the difference be-
tween the beginning time of the speaker?s ut-
terance and the beginning time of its transla-
tion
These problems are likely to cause some awk-
wardness in conversations. One effective method
of improving these problems is that a translation
system begins to translate the words without wait-
ing for the end of the speaker?s utterance, much as
a simultaneous interpreter does. This has been ver-
ified as possible by a study on comparing simul-
taneous interpretation with consecutive interpreta-
tion from the viewpoint of efficiency and smooth-
ness of cross-language conversations (Ohara et al,
2003).
There has also been some research on simulta-
neous machine interpretation with the aim of de-
veloping environments that support multilingual
communication (Mima et al, 1998; Casacuberta
et al, 2002; Matsubara and Inagaki, 1997).
To realize simultaneous translation between
languages with different word order, such as En-
glish and Japanese, our method utilizes the feature
that the word order of a target language is flexi-
ble. To resolve the problem that translation sys-
tems generates grammatically dubious sentence,
683
our method utilizes dependency structures and
Japanese dependency constraints to determine the
word order of a translation. Moreover, by consid-
ering the fact that the inversion of predicate ex-
pressions occurs more frequently in Japanese spo-
ken language, our method employs predicate in-
version to resolve the problem that Japanese has
the predicate at the end of the sentence. Further-
more, our method features the function of cancel-
ing an inversion by restating a predicate when the
translation is incomprehensible due to the inver-
sion. In the research described in this paper, we
implement a prototype translation system, and to
evaluate it, we conduct an experiment with all 578
sentences in the ATIS corpus.
This paper is organized as follows: Section
2 discusses an important problem in English-
Japanese simultaneous translation and explains the
idea of utilizing flexible word order. Section 3 in-
troduces our method for the generation in English-
Japanese simultaneous translation, and Section 4
describes the configuration of our system. Section
5 reports the experimental results, and the paper
concludes in Section 6.
2 Japanese in Simultaneous
English-Japanese Translation
In this section, we describe the problem of the
difference of word order between English and
Japanese in incremental English-Japanese transla-
tion. In addition, we outline an approach of si-
multaneous machine translation utilizing linguis-
tic phenomena, flexible word order, and inversion,
characterizing Japanese speech.
2.1 Difference of Word Order between
English and Japanese
Let us consider the following English:
(E1) I want to fly from San Francisco to Denver
next Monday.
The standard Japanese for (E1) is
(J1) raishu-no (?next?) getsuyobi-ni (?Monday?)
San Francisco-kara (?from?) Denver-he (?to?)
tobi-tai-to omoi-masu (?want to fly?).
Figure 1 shows the output timing when the trans-
lation is generated as incrementally as possible
in consideration of the word alignments between
(E1) and (J1). In Fig. 1, the flow of time is shown
from top to bottom. In this study, we assume
that the system translates input words chunk-by-
chunk. We define a simple noun phrase (e.g. San
OutputInput
raishu-no ( next) getsuyobi-ni ( Monday)
San Francisco-kara ( from)
Denver-he ( to)
tobi-tai-to omoi-masu ( want to fly)
next Monday 
I
want to fly
from
San Francisco
to
Denver
Figure 1: The output timing of the translation (J1)
OutputInput
raishu-no ( next) getsuyobi-ni ( Monday)next Monday 
I
want to fly
from
Denver-he ( to) tobi-tai-to omoi-masu ( want to fly)Denver
San Francisco-kara ( from)San Francisco
to
Figure 2: The output timing of the translation (J2)
Francisco, Denver and next Monday), a predicate
(e.g. want to fly) and each other word (e.g. I, from,
to) as a chunk. There is ?raishu-no getsuyobi-ni?
(?next Monday?) at the beginning of the transla-
tion (J1), and there is ?next Monday? correspond-
ing to ?raishu-no getsuyobi-ni? at the end of the
sentence (E1). Thus, the system cannot output
?raishu-no getsuyobi-ni? and its following trans-
lation until the whole sentence is uttered. This is
a fatal flaw in incremental English-Japanese trans-
lation because there exists an essential difference
between English and Japanese in the word order. It
is fundamentally impossible to cancel these prob-
lems as long as we assume (J1) to be the transla-
tion of (E1).
2.2 Utilizing Flexible Word Order in
Japanese
Japanese is a language with a relatively flexible
word order. Thus, it is possible that a Japanese
translation can be accepted even if it keeps the
word order of an English sentence. Let us con-
sider the following Japanese:
(J2) San Francisco-kara (?from?) Denver-he (?to?)
tobi-tai-to omoi-masu (?want to fly?) raishu-no
(?next?) getsuyobi-ni (?Monday?).
(J2) can be accepted as the translation of the sen-
tence (E1) and still keep the word order as close as
possible to the sentence (E1). Figure 2 shows the
output timing when the translation is generated as
incrementally as possible in consideration of the
word alignments between (E1) and (J2). The fig-
ure demonstrates that a translation system might
684
be able to output ?San Francisco -kara (?from?)?
when ?San Francisco? is input and ?Denver-he
(?to?) tobi-tai-to omoi-masu (?want to fly?)? when
?Denver? is input. If a translation system out-
puts the sentence (J2) as the translation of the
sentence (E1), the system can translate it incre-
mentally. The translation (J2) is not necessarily
an ideal translation because its word order differs
from that of the standard translation and it has an
inverted sentence structure. However the transla-
tion (J2) can be easily understood due to the high
flexibility of word order in Japanese. Moreover, in
spoken language machine translation, the high de-
gree of incrementality is preferred to that of qual-
ity. Therefore, our study positively utilizes flexi-
ble word order and inversion to realize incremen-
tal English-Japanese translation while keeping the
translation quality acceptable.
3 Japanese Generation based on
Dependency Structure
When an English-Japanese translation system in-
crementally translates an input sentence by utiliz-
ing flexible word order and inversion, it is pos-
sible that the system will generate a grammati-
cally incorrect Japanese sentence. Therefore, it
is necessary for the system to generate the trans-
lation while maintaining the translation quality at
an acceptable level as a correct Japanese sentence.
In this section, we describe how to generate an
English-Japanese translation that retains the word
order of the input sentence as much as possible
while keeping the quality acceptable.
3.1 Dependency Grammar in English and
Japanese
Dependency grammar illustrates the syntactic
structure of a sentence by linking individual
words. In each link, modifiers (dependents) are
connected to the word that they modify (head). In
Japanese, the dependency structure is usually de-
fined in terms of the relation between phrasal units
called bunsetsu1. The Japanese dependency rela-
tions are satisfied with the following constraints
(Kurohashi and Nagao, 1997):
? No dependency is directed from right to left.
? Dependencies do not cross each other.
1A bunsetsu is one of the linguistic units in Japanese, and
roughly corresponds to a basic phrase in English. A bunsetsu
consists of one independent word and more than zero ancil-
lary words. A dependency is a modification relation between
two bunsetsus.
Dependent
bunsetsu
Head 
bunsetsu
Dependency relation
Raishu-no getsuyobi-ni San Francisco-kara Denver-he  tobi-tai-to omoi-masu .
( next)       ( Monday)                      ( from)           ( to)       ( want to fly) 
Figure 3: The dependency structures of translation (J1)
San Francisco-kara Denver-he   tobi-tai-to omoi-masu raishu-no   getsuyobi-ni .
( from)           ( to)       ( want to fly)         ( next)     ( Monday)
Dependent
bunsetsu
Head 
bunsetsu
Inversion
Figure 4: The dependency structures of translation (J2)
? Each bunsetsu, except the last one, depends
on only one bunsetsu.
The translation (J1) is satisfied with these con-
straints as shown in Fig. 3. A sentence satis-
fying these constraints is deemed grammatically
correct sentence in Japanese. To meet this require-
ment, our method parses the dependency relations
between input chunks and generates a translation
satisfying Japanese dependency constraints.
3.2 Inversion
In this paper, we call the dependency relations
heading from right to left ?inversions?. Inversions
occur more frequently in spontaneous speech than
in written text in Japanese. That is to say, there
are some sentences in Japanese spoken language
that do not satisfy the constraint mentioned above.
Translation (J2) does not satisfy this constraint, as
shown in Fig. 4. We investigated the inversions
using the CIAIR corpus (Ohno et al, 2003) and
found the following features:
Feature 1 92.2% of the inversions are that the
head bunsetsu of the dependency relation is
a predicate. (predicate inversion)
Feature 2 The more the number of dependency
relations that depend on a predicate increases,
the more the frequency of predicate inver-
sions increases.
Feature 3 There are not three or more inversions
in a sentence.
From Feature 1, our method utilizes a predicate
inversion to retain the word order of an input sen-
tence. It also generates a predicate when the num-
ber of dependency relations that depend on a pred-
icate exceeds the constant R (from Feature 2). If
there are three or more inversions in the transla-
tion, the system cancels an inversion by restating
a predicate (from Feature 3).
685
Input
Output
POS tagging
Chunking
Syntactic parsing 
Transfer into dependency structure
Syntactic transfer
Lexicon transfer
Particle translation
POS dictionary
Chunk dictionary
Syntactic rule
Lexicon transfer
rule
Particle 
translation rule
Parsing
Transfer
Generation
Predicate translation
Determine word-order of translation
Predicate
translation rule
Figure 5: Configuration of our system
4 System Configuration
Figure 5 shows the configuration of our system.
The system translates an English speech transcript
into Japanese incrementally. It is composed of
three modules: incremental parsing, transfer and
generation. In the parsing module the parser deter-
mines the English dependency structure for input
words incrementally. In the transfer module, struc-
ture and lexicon transfer rules transform the En-
glish dependency structure into the Japanese case
structure. As for the generation module, the sys-
tem judges whether the translation of each chunk
can be output, and if so, outputs the translation
of the chunk. Figure 6 shows the processing flow
when the fragment ?I want to fly from San Fran-
cisco to Denver? of?2.1?is input. In the follow-
ing subsections we explain each module, referring
to Fig. 6.
4.1 Incremental Dependency Parsing
First, the system performs POS tagging for input
words and chunking (c.f. ?Chunk? in Fig. 6).
Next, we explain how to parse the English
phrase structure (c.f. ?English phrase structure? in
Fig. 6). When we parse the phrase structure for in-
put words incrementally, there arises the problem
of ambiguity; our method needs to determine only
one parsing result at a time. To resolve this prob-
lem our system selects the phrase structure of the
maximum likelihood at that time by using PCFG
(Probabilistic Context-Free Grammar) rules. To
resolve the problem of the processing time our sys-
tem sets a cut-off value.
NP_subj (I)
NP(?)
PP(from)
VP (want_to_fly)
S (want_to_fly)
*
?VP?(want_to_fly)
PP(to)
IN(from) IN(to)
NP(San Francisco)
NP(Denver)
*
*
Transfer into dependency structure
Syntactic parsing
POS Tagging & Chunking
English
dependency
structure
English
phrase
structure
Chunk
?NP_subj? ?VP? ?IN? ?NP? ?TO? ?NP?
I           want       to      fly     from   San Francisco    to      Denver  
I       want       to      fly     from   San Francisco   to    Denver  
I     want_to_fly from     ?San Francisco? to     Denver           ?
<predicate>
Syntacitc transfer &
Lexicon transfer
Lexicon transfer rule
San Francisco San Francisco
Denver                  Denver
I                                 nil 
want to fly           tobu (fly)  <hope>
Particle translation &
Predicate translation
Particle translation rule
Japanese
case
structure
Japanese
dependency
structure
<subject>
<from>
<subj>
<to>
nil   tobu(fly) <hope>     San Francisco       Denver         ?
Predicate translation rule
tobu(fly) <hope>   
tobi-tai-to-omoi-masu
nil  tobi-tai-to omoi-masu San Francisco-kara Denver-he           ?
(want-to-fly)                                  (from)               (to)
nil   San Francisco-kara Denver-he       tobi-tai-to omoi-masu ?
(from)            (to)         (want-to-fly)
Deside word-order of translation
<null>
San Francisco-kara Denver-he tobi-tai-to omoi-masu
(from)          (to)           (want-to-fly)
Output
<from>
<to>
tobu(fly)
kara (from)
he (to)
Syntactic transfer rule
<subj>
nil
nil
Japanese
translation
Input words
translation
*
Parsing
module
Transfer
module
Generation
module
tobu(fly)
Figure 6: The translation flow for the fragment ?I
want to fly from San Francisco to Denver?
Furthermore, the system transforms the English
phrase structure into an English dependency struc-
ture (c.f. ?English dependency structure? in Fig.
6). The dependency structure for the sentence can
be computed from the phrase structure for the in-
put words by defining the category for each rule in
CFG, called a ?head child? (Collins, 1999). The
head is indicated using an asterisk * in the phrase
structure of Fig. 6. In the ?English phrase struc-
ture,? the chunk in parentheses at each node is
the head chunk of the node that is determined by
the head information of the syntax rules. If the
head chunk (e.g. ?from?) of a child node (e.g.
PP(from)) differs from that of its parent node (e.g.
VP(want-to-fly)), the head chunk (e.g. ?from?) of
the child node depends on the head chunk (e.g.
?want-to-fly?) of the parent node. Some syntax
rules are also annotated with subject and object
information. Our system uses such information to
add Japanese function words to the translation of
the subject chunk or the object chunk in the gener-
ation module. To use a predicate inversion in the
686
generation module the system has to recognize the
predicate of an input sentence. This system recog-
nizes the chunk (e.g. ?want to fly?) on which the
subject chunk (e.g. ?I?) depends as a predicate.
4.2 Incremental Transfer
In the transfer module, structure and lexicon trans-
fer rules transform the English dependency struc-
ture into the Japanese case structure (?Japanese
case structure? in Fig. 6). In the structure transfer,
the system adds a type of relation to each depen-
dency relation according to the following rules.
? If the dependent chunk of a dependency rela-
tion is a subject or object (e.g. ?I?), then the
type of such dependency relation is ?subj? or
?obj?.
? If a chunk A (e.g. ?San Francisco?) indirectly
depends on another chunk B (e.g. ?want-
to-fly?) through a preposition (e.g. ?from?),
then the system creates a new dependency re-
lation where A depends on B directly, and the
type of the relation is the preposition.
? The type of the other relations is ?null?.
In the lexicon transfer, the system transforms each
English chunk into its Japanese translation.
4.3 Incremental Generation
In the generation module, the system transforms
the Japanese case structure into the Japanese de-
pendency structure by translating a particle and
a predicate. In attaching a particle (e.g. ?kara?
(from)) to the translation of a chunk (e.g. ?San
Francisco?), the system determines the attached
particle (e.g. ?kara? (from)) by particle transla-
tion rules. In translating a predicate (e.g. ?want
to fly?), the system translates a predicate by pred-
icate translation rules, and outputs the translation
of each chunk using the method described in Sec-
tion 3.
4.4 Example of Translation Process
Figure 7 shows the processing flow for the En-
glish sentence, ?I want to fly from San Francisco
to Denver next Monday.? In Fig. 7 the underlined
words indicate that they can be output at that time.
5 Experiment
5.1 Outline of Experiment
To evaluate our method, we conducted a transla-
tion experiment was made as follows. We imple-
mented the system in Java language on a 1.0-GHz
PentiumM PC with 512 MB of RAM. The OS was
Windows XP. The experiment used all 578 sen-
tences in the ATIS corpus with a parse tree, in the
Penn Treebank (Marcus et al 1993). In addition,
we used 533 syntax rules, which were extracted
from the corpus? parse tree. The position of the
head child in the grammatical rule was defined ac-
cording to Collins? method (Collins, 1999).
5.2 Evaluation Metric
Since an incremental translation system for spo-
ken dialogues is required to realize a quick and
informative response to support smooth communi-
cation, we evaluated the translation results of our
system in terms of both simultaneity and quality.
To evaluate the translation quality of our sys-
tem, each translation result of our system was as-
signed one of four ranks for translation quality by
a human translator:
A (Perfect): no problems in either information or
grammar
B (Fair): easy to understand but some important
information is missing or it is grammatically
flawed
C (Acceptable): broken but understandable with
effort
D (Nonsense): important information has been
translated incorrectly
To evaluate the simultaneity of our system, we
calculated the average delay time for translating
chunks using the following expression:
Average delay time =
?
k
d
k
n , (1)
where d
k
is the virtual elapsed time from inputting
the kth chunk until outputting its translated chunk.
(When a repetition is used, d
k
is the elapsed time
from inputting the kth chunk until restate its trans-
lated chunk.) The virtual elapsed time increases
by one unit of time whenever a chunk is input, n
is the total number of chunks in all of the test sen-
tences.
The average delay time is effective for evalu-
ating the simultaneity of translation. However, it
is difficult to evaluate whether our system actu-
ally improves the efficiency of a conversation. To
do so, we measured ?the speaker? and the inter-
preter?s utterance time.? ?The speaker? and the in-
terpreter ?utterance time? runs from the start time
of a speaker?s utterance to the end time of its trans-
lation. We cannot actually measure actual ?the
687
Table 1: Comparing our method (Y) with two other methods (X, Z)
Quality Average Speaker and interpreter
Method A A+B A+B+C delay time utterance time (sec)
X 7 (1.2%) 48 (8.3%) 92 (15.9%) 0 4.7
Y 40 (6.9%) 358 (61.9%) 413 (71.5%) 2.79 6.0
Z


















3.79 6.4
 







        
 	



 










	




































	











	





















 	

			
	

	
					
				

	
Figure 8: The relation between the speaker?s ut-
terance time and the time from the end time of the
speaker?s utterance to the end time of the transla-
tion
speaker? and the interpreter? utterance time? be-
cause our system does not include speech recog-
nition and synthesis. Thus, the processing time
of speech recognition and transfer text-to-speech
synthesis is zero, and the speaker?s utterance time
and the interpreter?s utterance time is calculated
virtually by assuming that the speaker?s and inter-
preter?s utterance speed is 125 ms per mora.
5.3 Experiment Results
To evaluate the translation quality and simultane-
ity of our system, we compared the translation re-
sults of our method (Y) with two other methods.
One method (X) translates the input chunks with
no delay time. The other method (Z) translates the
input chunks by waiting for the whole sentence to
be input, in as consecutive translation. We could
not evaluate the translation quality of the method
Z because we have not implemented the method Z.
And we virtually compute the delay time and the
utterance time. Table 1 shows the estimation re-
sults of methods X, Y and Z. Note, however, that
we virtually calculated the average delay time and
the speaker?s and interpreter?s utterance times in
method Z without translating the input sentence.
Table 1 indicates that our method Y achieved
a 55.6% improvement over method X in terms
of translation quality and a 1.0 improvement over
method Z for the average delay time.
Figure 8 shows the relation between the
speaker?s utterance time and the time from the end
time of the speaker?s utterance to the end time of
the translation. According to Fig. 8, the longer a
speaker speaks, the more the system reduces the
time from the end time of the speaker?s utterance
to the end time of the translation.
In Section 3, we explained the constant R. Ta-
ble 2 shows increases in R from 0 to 4, with the
results of the estimation of quality, the average de-
lay time, the number of inverted sentences and the
number of sentences with restatement. When we
set the constant to R = 2, the average delay time
improved by a 0.08 over that of method Y, and
the translation quality did not decrease remark-
ably. Note, however, that method Y did not utilize
any predicate inversions.
To ascertain the problem with our method,
we investigated 165 sentences whose translations
were assigned the level D when the system trans-
lated them by utilizing dependency constraints.
According to the investigation, the system gener-
ated grammatically incorrect sentences in the fol-
lowing cases:
? There is an interrogative word (e.g. ?what??
?which?) in the English sentence (64 sen-
tences).
? There are two or more predicates in the En-
glish sentence (25 sentences).
? There is a coordinate conjunction (e.g.
?and???or?) in the English sentence (21 sen-
tences).
Other cases of decreases in the translation quality
occurred when a English sentence was ill-formed
or when the system fails to parse.
6 Conclusion
In this paper, we have proposed a method for in-
crementally translating English spoken language
into Japanese. To realize simultaneous translation
688
Table 2: The results of each R (0 ? R ? 4)
Quality Average Sentences Sentences
R A A+B A+B+C delay time with inversion with restatement
0 8 (1.4%) 152 (26.3%) 363 (62.8%) 2.51 324 27
1 14 (2.4%) 174 (30.1%) 364 (63.0%) 2.53 289 29
2 36 (6.2%) 306 (52.9%) 396 (68.5%) 2.71 73 5
3 39 (6.7%) 344 (59.5%) 412 (71.3%) 2.79 28 2
4 40 (7.0%) 358 (61.9%) 412 (71.3%) 2.79 3 2
our method utilizes the feature that word order is
flexible in Japanese, and determines the word or-
der of a translation based on dependency struc-
tures and Japanese dependency constraints. More-
over, our method employs predicate inversion and
repetition to resolve the problem that Japanese has
a predicate at the end of a sentence. We imple-
mented a prototype system and conducted an ex-
periment with 578 sentences in the ATIS corpus.
We evaluated the translation results of our sys-
tem in terms of quality and simultaneity, confirm-
ing that our method achieved a 55.6% improve-
ment over the method of translating by retaining
the word order of an original with respect to trans-
lation quality, and a 1.0 improvement over the
method of consecutive translation regarding aver-
age delay time.
Acknoledgments
The authors would like to thank Prof. Dr. Toshiki
Sakabe. They also thank Yoshiyuki Watanabe,
Atsushi Mizuno and translator Sachiko Waki for
their contribution to our study.
References
F. Casacuberta, E. Vidal and J. M. Vilar. 2002. Ar-
chitectures for speech-to-speech translation using
finite-state models, Proceedings of Workshop on
Speech-to-Speech Translation: Algorithms and Sys-
tem, pages 39-44.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing, Ph.D. Thesis, University
of Pennsylvania,
R. Frederking, A. Blackk, R. Brow, J. Moody, and
E. Stein-brecher, 2002. Field Testing the Tongues
Speech-to-Speech Machin Translation System, Pro-
ceedings of the 3rd International Conference on
Language Resources and Evaluation(LREC-2002)
pages 160-164.
H. Hoge. 2002. Project Proposal TC-STAR: Make
Speech to Speech Translation Real, Proceedings of
the 3rd International Conference on Language Re-
sources and Evaluation(LREC-2002), pages 136-
141.
R. Isotani, K. Yamada, S. Ando, K. Hanazawa, S.
Ishikawa and K. Iso. 2003. Speech-to-Speech Trans-
lation Software PDAs for Travel Conversation, NEC
Research and Development, 44, No.2 pages 197-
202.
S. Kurohashi and M. Nagao. 1997. Building a Japanese
Parsed Corpus while Improving the Parsing System,
Proceedings of 4th Natural Language Processing
Pacific Rim Symposium, pages 451-456.
F. Liu, Y. Gao, L. Gu and M. Picheny. 2003. Noise Ro-
bustness in Speech to Speech Translation, IBM Tech
Report RC22874.
M. P. Marcus, B. Santorini and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: the Penn Treebank, Computational Linguis-
tics, 19(2):310-330.
S. Matsubara and Y. Inagaki. 1997. Incremental Trans-
fer in English-Japanese Machine Translation, IE-
ICE Transactions on Information and Systems,
(11):1122-1129.
H. Mima, H. Iida and O. Furuse. 1998. Simultaneous
Interpretation Utilizing Example-based Incremental
Transfer, Proceedings of 17th International Confer-
ence on Computational Linguistics and 36th Annual
Meeting of Association for Computational Linguis-
tics, pages 855-861.
M. Ohara, S. Matsubara, K. Ryu, N. Kawaguchi and Y.
Inagaki. 2003. Temporal Features of Cross-Lingual
Communication Mediated by Simultaneous Inter-
preting: An Analysis of Parallel Translation Cor-
pus in Comparison to Consecutive Interpreting, The
Journal of the Japan Association for Interpretation
Studies pages 35-53.
T. Ohno, S. Matsubrara, N. Kawaguchi and Y. In-
agaki. 2003. Spiral Construction of Syntactically
Annotated Spoken Language Corpus, Proceedings
of 2003 IEEE International Conference on Natural
Language Processing and Knowledge Engineering,
pages 477-483.
T. Takezawa, T. Morimoto, Y. Sagisaka, N. Campbell,
H. Iida, F. Sugaya, A. Yokoo and S. Yamamoto.
1998. A Japanese-to-English Speech Translation
System:ATR-MATRIX, Proceedings of 5th Interna-
tional Conference on Spoken Language Processing,
pages 957-960.
689
English dependency structure
Input
.
.
raishu-no
( next)
getsuyobi-ni
( Monday)
next 
Monday
Denver-he ( to)
tobi-tai-to omoi-
masu
( want to fly)
Denver
to
San Francisco
-kara ( from)
San 
Francisco
from
want to fly
nil
I
Output
Japanese dependency structure
Parse tree
NP_subj (I)
NP(next Monday)PP(from)
VP (want_to_fly)
S (want_to_fly)
*
?VP?(want_to_fly) PP(to)
IN(from) IN(to)NP(San Francisco) NP(Denver)
*
* *
$($)
S0($)
*
NP_subj (I)
NP(next Monday)PP(from)
VP (want_to_fly)
S (want_to_fly)
*
?VP?(want_to_fly) PP(to)
IN(from) IN(to)NP(San Francisco) NP(Denver)
*
* *
NP_subj (I)
NP(?)PP(from)
VP (want_to_fly)
S (want_to_fly)
*
?VP?(want_to_fly) PP(to)
IN(from) IN(to)NP(San Francisco) NP(Denver)
*
* *
NP_subj (I)
NP(?)PP(from)
VP (want_to_fly)
S (want_to_fly)
*
?VP?(want_to_fly) PP(?)
IN(from) NP(San Francisco)
*
*
NP_subj (I)
NP(?)PP(from)
VP (want_to_fly)
S (want_to_fly)
*
?VP?(want_to_fly) PP(?)
IN(from) NP(San Francisco)
*
*
IN(to) NP(?)*
NP_subj (I)
NP(?)PP(from)
VP (want_to_fly)
S (want_to_fly)
*
?VP?(want_to_fly) PP(?)
IN(from) NP(?)
*
*
NP_subj (I)
NP(?)PP(?)
VP (want_to_fly)
S (want_to_fly)
*
?VP?(want_to_fly) PP(?)
*
NP_subj (I) VP (?)
S (?)
*
I   want_to_fly from   San Francisco   to  Denver  next Monday  $
I   want_to_fly from   San Francisco   to  Denver  next Monday $(?)
I   want_to_fly from   San Francisco   to  Denver      NP(?) 
I   want_to_fly from   San Francisco   to      NP(?)     NP(?) 
I   want_to_fly from   San Francisco       PP(?)       NP(?)
I   want_to_fly from         NP(?)       PP(?)        NP(?)
$(?)
S0(?)
*
I   want_to_fly PP(?)          PP(?)        NP(?)
I      VP(?) 
nil  San Francisco-kara Denver-he  tobi-tai-to omoi-masu
raishu-no getsuyobi-ni $(?)
nil  San Francisco-kara
Denver-he tobi-tai-to omoi-masu NP(?)
nil San Francisco-kara
NP(?)-he NP(?)      tobi-tai-to omoi-masu
nil 
San Francisco-kara PP(?) NP(?)      tobi-tai-to omoi-masu
nil 
NP(?)-kara PP(?) NP(?)      tobi-tai-to omoi-masu
nil 
PP(?) PP(?) NP(?)     tobi-tai-to omoi-masu
nil VP(?)
nil  San Francisco-kara Denver-he  tobi-tai-to omoi-masu
raishu-no getsuyobi-ni $($)
Figure 7: The translation flow for ?I want to fly from San Francisco to Denver next Monday.?
690
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 531?539,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Linefeed Insertion into Japanese Spoken Monologue for Captioning
Tomohiro Ohno
Graduate School of
International Development,
Nagoya University, Japan
ohno@nagoya-u.jp
Masaki Murata
Graduate School of
Information Science,
Nagoya University, Japan
murata@el.itc.nagoya-u.ac.jp
Shigeki Matsubara
Information Technology Center,
Nagoya University, Japan
matubara@nagoya-u.jp
Abstract
To support the real-time understanding of
spoken monologue such as lectures and
commentaries, the development of a cap-
tioning system is required. In monologues,
since a sentence tends to be long, each
sentence is often displayed in multi lines
on one screen, it is necessary to insert
linefeeds into a text so that the text be-
comes easy to read. This paper proposes
a technique for inserting linefeeds into a
Japanese spoken monologue text as an el-
emental technique to generate the read-
able captions. Our method appropriately
inserts linefeeds into a sentence by ma-
chine learning, based on the information
such as dependencies, clause boundaries,
pauses and line length. An experiment us-
ing Japanese speech data has shown the ef-
fectiveness of our technique.
1 Introduction
Real-time captioning is a technique for support-
ing the speech understanding of deaf persons, el-
derly persons, or foreigners by displaying tran-
scribed texts of monologue speech such as lec-
tures. In recent years, there exist a lot of re-
searches about automatic captioning, and the tech-
niques of automatic speech recognition (ASR)
aimed for captioning have been developed (Bou-
lianne et al, 2006; Holter et al, 2000; Imai et
al., 2006; Munteanu et al, 2007; Saraclar et al,
2002; Xue et al, 2006). However, in order to gen-
erate captions which is easy to read, it is important
not only to recognize speech with high recognition
rate but also to properly display the transcribed
text on a screen (Hoogenboom et al, 2008). Es-
pecially, in spoken monologue, since a sentence
tends to be long, each sentence is often displayed
as a multi-line text on a screen. Therefore, proper
linefeed insertion for the displayed text is desired
so that the text becomes easy to read.
Until now, there existed few researches about
how to display text on a screen in automatic cap-
tioning. As the research about linefeed insertion,
Monma et al proposed a method based on pat-
terns of a sequence of morphemes (Monma et
al., 2003). However, the target of the research is
closed-captions of Japanese TV shows, in which
less than or equal to 2 lines text is displayed on
a screen and the text all switches to other text at
a time. In the work, the highest priority concept
on captioning is that one screen should be filled
with as much text as possible. Therefore, a se-
mantic boundary in a sentence is hardly taken into
account in linefeed insertion, and the readability
of the caption is hardly improved.
This paper proposes a technique for inserting
linefeeds into transcribed texts of Japanese mono-
logue speech as an elemental technique to gener-
ate readable captions. We assume that a screen for
displaying only multi-line caption is placed to pro-
vide the caption information to the audience on the
site of a lecture. In our method, the linefeeds are
inserted into only the boundaries between bunset-
sus1, and the linefeeds are appropriately inserted
into a sentence by machine learning, based on the
information such as morphemes, dependencies2,
clause boundaries, pauses and line length.
We conducted an experiment on inserting line-
feeds by using Japanese spoken monologue data.
As the results of inserting linefeeds for 1,714 sen-
tences, the recall and precision of our method were
82.66% and 80.24%, respectively. Our method
improved the performance dramatically compared
1Bunsetsu is a linguistic unit in Japanese that roughly cor-
responds to a basic phrase in English. A bunsetsu consists of
one independent word and zero or more ancillary words.
2A dependency in Japanese is a modification relation in
which a modifier bunsetsu depends on a modified bunsetsu.
That is, the modifier bunsetsu and the modified bunsetsu work
as modifier and modifyee, respectively.
531
Figure 1: Caption display of spoken monologue
with four baseline methods, which we established
for comparative evaluation. The effectiveness of
our method has been confirmed.
This paper is organized as follows: The next
section describes our assumed caption and the pre-
liminary analysis. Section 3 presents our linefeed
insertion technique. An experiment and discussion
are reported in Sections 4 and 5, respectively. Fi-
nally, Section 6 concludes the paper.
2 Linefeed Insertion for Spoken
Monologue
In our research, in an environment in which cap-
tions are displayed on the site of a lecture, we as-
sume that a screen for displaying only captions is
used. In the screen, multi lines are always dis-
played, being scrolled line by line. Figure 1 shows
our assumed environment in which captions are
displayed.
As shown in Figure 2, if the transcribed text of
speech is displayed in accordance with only the
width of a screen without considering the proper
points of linefeeds, the caption is not easy to read.
Especially, since the audience is forced to read
the caption in synchronization with the speaker?s
utterance speed, it is important that linefeeds are
properly inserted into the displayed text in consid-
eration of the readability as shown in Figure 3.
To investigate whether the line insertion facili-
tates the readability of the displayed texts, we con-
ducted an experiment using the transcribed text of
lecture speeches in the Simultaneous Interpreta-
tion Database (SIDB) (Matsubara et al, 2002). We
randomly selected 50 sentences from the data, and
then created the following two texts for each sen-
tence based on two different concepts about line-
feed insertion.
?1?Text into which linefeeds were forcibly in-
serted once every 20 characters
????????????????????
????????????????????
????????????????????
????????????????????
???????????????????
For example, environmental problem, 
population problem, AIDS problem and 
so on, a lot of global-scale problems 
have occurred, and unfortunately, 
these problems seem to continue 
during 21st century or to become 
worse if we look through blue glasses.
Figure 2: Caption of monologue speech
????????
?????????
??????????
???????????????????
????????????
??????????
??????????????
?????????????????
(For example, environmental problem)
(population problem)
(AIDS problem and so on)
a lot of global-scale problems 
have occurred
(and unfortunately, these problems)
(to continue during also 21st century)
(or if we look through blue glasses)
(seems to become worse)
Figure 3: Caption into which linefeeds are prop-
erly inserted
49
50
49
37
40
36
43
48
34
49
1 2 3 4 5 6 7 8 9 10
?1? Forcible insertion of linefeeds
?2? Proper insertion of linefeeds
subject ID
# of sentences
50
45
40
35
30
25
20
15
10
5
0
Figure 4: Result of investigation of effect of line-
feed insertion into transcription
?2?Text into which linefeeds were properly
inserted in consideration of readability by
hand3
Figure 2 and 3 show examples of the text (1) and
(2), respectively. 10 examinees decided which of
the two texts was more readable. Figure 4 shows
the result of the investigation. The ratio that each
examinee selected text (2) was 87.0% on average.
There was no sentence in the text group (1) which
was selected by more than 5 examinees. These
indicates that a text becomes more readable by
proper insertion of linefeeds.
Here, since a bunsetsu is the smallest seman-
tically meaningful language unit in Japanese, our
method adopts the bunsetsu boundaries as the can-
didates of points into which a linefeed is inserted.
In this paper, hereafter, we call a bunsetsu bound-
ary into which a linefeed is inserted a linefeed
point.
33 persons inserted linefeeds into the 50 sentences by dis-
cussing where to insert the linefeeds.
532
Table 1: Size of analysis data
sentence 221
bunsetsu 2,891
character 13,899
linefeed 883
character per line 13.2
3 Preliminary Analysis about Linefeed
Points
In our research, the points into which linefeeds
should be inserted is detected by using machine
learning. To find the effective features, we investi-
gated the spoken language corpus. In our investi-
gation, we used Japanese monologue speech data
in the SIDB (Matsubara et al, 2002). The data
is annotated by hand with information on mor-
phological analysis, bunsetsu segmentation, de-
pendency analysis, clause boundary detection, and
linefeeds insertion. Table 1 shows the size of the
analysis data. Among 2,670 (= 2, 891?221) bun-
setsu boundaries, which are candidates of linefeed
points, there existed 833 bunsetsu boundaries into
which linefeeds were inserted, that is, the ratio of
linefeed insertion was 31.2%.
The linefeeds were inserted by hand so that the
maximum number of characters per line is 20. We
set the number in consideration of the relation be-
tween readability and font size on the display. In
the analysis, we focused on the clause boundary,
dependency relation, line length, pause and mor-
pheme of line head, and investigated the relations
between them and linefeed points.
3.1 Clause Boundary and Linefeed Point
Since a clause is one of semantically meaningful
language units, the clause boundary is considered
to be a strong candidate of a linefeed point. In the
analysis data, there existed 969 clause boundaries
except sentence breaks. Among them, 490 were
the points into which linefeeds were inserted, that
is, the ratio of linefeed insertion was 51.1%. This
ratio is higher than that of bunsetsu boundaries.
This indicates that linefeeds tend to be inserted
into clause boundaries.
We investigated the ratio of linefeed insertion
about 42 types4 of clause boundaries, which were
seen in the analysis data. Table 2 shows the top 10
4In our research, we used the types of clause boundaries
defined by the Clause Boundary Annotation Program (Kash-
ioka and Maruyama, 2004).
Table 2: Ratio of linefeed insertion for clause
boundary type
type of ratio of linefeed
clause boundary insertion (%)
topicalized element-wa 50.8
discourse marker 12.0
quotational clause 22.1
adnominal clause 23.3
compound clause-te 90.2
supplement clause 68.0
compound clause-ga 100.0
compound clause-keredomo 100.0
condition clause-to 93.5
adnominal clause-toiu 27.3
clause boundary types about the occurrence fre-
quency, and each ratio of linefeed insertion. In
the case of ?compound clause-ga? and ?compound
clause-keredomo,? the ratio of linefeed insertion
was 100%. On the other hand, in the case of ?quo-
tational clause,? ?adnominal clause? and ?adnomi-
nal clause-toiu,? the ratio of linefeed insertion was
less than 30%. This means that the likelihood of
linefeed insertion is different according to the type
of the clause boundary.
3.2 Dependency Structure and Linefeed
Point
When a bunsetsu depends on the next bunsetsu, it
is thought that a linefeed is hard to be inserted into
the bunsetsu boundary between them because the
sequence of such bunsetsus constitutes a semanti-
cally meaningful unit. In the analysis data, there
existed 1,459 bunsetsus which depend on the next
bunsetsu. Among the bunsetsu boundaries right
after them, 192 were linefeed points, that is, the
ratio of linefeed insertions was 13.2%. This ra-
tio is less than half of that for all the bunsetsu
boundaries. On the other hand, when the bunsetsu
boundary right after the bunsetsu which does not
depend on the next bunsetsu, the ratio of linefeed
insertion was 52.7%.
Next, we focused on the type of the dependency
relation, by which the likelihood of linefeed inser-
tion is different. For example, when the bunsetsu
boundary right after a bunsetsu on which the final
bunsetsu of an adnominal clause depends, the ra-
tio of linefeed insertion was 43.1%. This ratio is
higher than that for all the bunsetsu boundaries.
In addition, we investigated the relation be-
533
???????????????????
???????????????????
:  dependency relation? bunsetsu
[Dependency structure]
[Result of linefeed insertion in the analysis data]
A writer of the magazine in which 
only old domestic cars are covered
asks to get a story about my car
?? ???
????
???? ??? ??? ?? ?? ??
????
???
?????
only 
domestic cars
in which 
are covered
old
of the 
magazine
a 
writer
my car
to get a
story about
ask
Figure 5: Relation between dependency structure
and linefeed points
tween a dependency structure and linefeed points,
that is, whether the dependency structure is closed
within a line or not. Here, a line whose depen-
dency structure is closed means that all bunsetsus,
except the final bunsetsu, in the line depend on one
of bunsetsus in the line. Since, in many of seman-
tically meaningful units, the dependency structure
is closed, the dependency structure of a line is con-
sidered to tend to be closed. In the analysis data,
among 883 lines, 599 lines? dependency structures
were closed.
Figure 5 shows the relation between depen-
dency structure and linefeed points. In this exam-
ple, linefeeds are not inserted right after bunset-
sus which depend on the next bunsetsu (e.g. ??
? (my)? and ??? (car)?). Instead, a linefeed is
inserted right after a bunsetsu which does not de-
pend on the next bunsetsu (???? (a writer)?).
In addition, the dependency structure in each line
is closed.
3.3 Line Length and Linefeed Point
An extremely-short line is considered to be hardly
generated because the readability goes down if the
length of each line is very different. In the analysis
data, a line whose length is less than or equal to 6
characters occupied only 7.59% of the total. This
indicates that linefeeds tend to be inserted into the
place where a line can maintain a certain length.
3.4 Pause and Linefeed Point
It is thought that a pause corresponds to a syn-
tactic boundary. Therefore, there are possibility
that a linefeed becomes more easily inserted into
a bunsetsu boundary at which a pause exists. In
our research, a pause is defined as a silent interval
equal to or longer than 200ms. In the analysis data,
among 748 bunsetsu boundaries at which a pause
exists, linefeeds were inserted into 471 bunsetsu
boundaries, that is, the ratio of linefeed insertion
was 62.97%. This ratio is higher than that for all
the bunsetsu boundaries, thus, we confirmed that
linefeeds tend to be inserted into bunsetsu bound-
aries at which a pause exists.
3.5 Morpheme Located in the Start of a Line
There exist some morphemes which are unlikely
to become a line head. We investigated the ratio
that each leftmost morpheme of all the bunsetsus
appears at a line head. Here, we focused on the
basic form and part-of-speech of a morpheme. The
morphemes which appeared 20 times and of which
the ratio of appearance at a line head was less than
10% were as follows:
? Basic form:
??? (think) [2/70]?, ??? (problem)
[0/42]?, ??? (do) [3/33]?, ??? (become)
[2/32]????? (necessary) [1/21]?
? Part-of-speech:
noun-non independent-general [0/40]?
noun-nai adjective stem [0/40]?
noun-non independent-adverbial [(0/27]
If the leftmost morpheme of a bunsetsu is one of
these, it is thought that a linefeed is hardly inserted
right after the bunsetsu.
4 Linefeed Insertion Technique
In our method, a sentence, on which morphologi-
cal analysis, bunsetsu segmentation, clause bound-
ary analysis and dependency analysis are per-
formed, is considered the input. Our method de-
cides whether or not to insert a linefeed into each
bunsetsu boundary in an input sentence. Under
the condition that the number of characters in each
line has to be less than or equal to the maximum
number of characters per line, our method identi-
fies the most appropriate combination among all
combinations of the points into which linefeeds
can be inserted, by using the probabilistic model.
In this paper, we describe an input sentence
which consists of n bunsetsus as B = b1 ? ? ? bn,
and the result of linefeeds insertion as R =
r1 ? ? ? rn. Here, ri is 1 if a linefeed is inserted right
after bunsetsu bi, and is 0 otherwise. We describe
a sequence of bunsetsus in the j-th line among the
m lines created by dividing an input sentence as
Lj = bj1 ? ? ? bjnj (1 ? j ? m), and then, r
j
k = 0 if
k ?= nj , and rjk = 1 otherwise.
534
4.1 Probabilistic Model for Linefeed
Insertion
When an input sentenceB is provided, our method
identifies the result of linefeeds insertionR, which
maximizes the conditional probability P (R|B).
Assuming that whether or not a linefeed is inserted
right after a bunsetsu is independent of other line-
feed points except the linefeed point of the start of
the line which contains the bunsetsu, P (R|B) can
be calculated as follows:
P (R|B) (1)
= P (r11 = 0, ? ? ? , r1n1 = 1, ? ? ? , r
m
1 = 0, ? ? ? , rmnm = 1|B)
?= P (r11 = 0|B) ? P (r12 = 0|r11 = 0, B) ? ? ? ?
?P (r1n1 = 1|r
1
n1?1 = 0, ? ? ? , r
1
1 = 0, B) ? ? ? ?
?P (rm1 = 0|rm?1nm?1 = 1, B) ? ? ? ?
?P (rmm = 1|rmnm?1 = 0, ? ? ? , r
m
1 = 0, rm?1nm?1 = 1, B)
where P (rjk = 1|r
j
k?1 = 0, ? ? ? , r
j
1 = 0, rj?1nj?1 =
1, B) is the probability that a linefeed is inserted
right after a bunsetsu bjk when the sequence of
bunsetsus B is provided and the linefeed point of
the start of the j-th line is identified. Similarly,
P (rjk = 0|r
j
k?1 = 0, ? ? ? , r
j
1 = 0, rj?1nj?1 = 1, B)
is the probability that a linefeed is not inserted
right after a bunsetsu bjk. These probabilities are
estimated by the maximum entropy method. The
result R which maximizes the conditional proba-
bility P (R|B) is regarded as the most appropriate
result of linefeed insertion, and calculated by dy-
namic programming.
4.2 Features on Maximum Entropy Method
To estimate P (rjk = 1|r
j
k?1 = 0, ? ? ? , r
j
1 =
0, rj?1nj?1 = 1, B) and P (r
j
k = 0|r
j
k?1 =
0, ? ? ? , rj1 = 0, rj?1nj?1 = 1, B) by the maximum
entropy method, we used the following features
based on the analysis described in Section 2.2.
Morphological information
? the rightmost independent morpheme (a part-
of-speech, an inflected form) and rightmost
morpheme (a part-of-speech) of a bunsetsu bjk
Clause boundary information
? whether or not a clause boundary exists right
after bjk
? a type of the clause boundary right after bjk (if
there exists a clause boundary)
Dependency information
? whether or not bjk depends on the next bun-
setsu
? whether or not bjk depends on the final bun-
setsu of a clause
? whether or not bjk depends on a bunsetsu to
which the number of characters from the start
of the line is less than or equal to the maxi-
mum number of characters
? whether or not bjk is depended on by the final
bunsetsu of an adnominal clause
? whether or not bjk is depended on by the bun-
setsu located right before it
? whether or not the dependency structure of
a sequence of bunsetsus between bjk and b
j
1,
which is the first bunsetsu of the line, is
closed
? whether or not there exists a bunsetsu which
depends on the modified bunsetsu of bjk,
among bunsetsus which are located after bjk
and to which the number of characters from
the start of the line is less than or equal to the
maximum number of characters
Line length
? any of the following is the class into which
the number of characters from the start of the
line to bjk is classified
? less than or equal to 2
? more than 2 and less than or equal to 6
? more than 6
Pause
? whether or not a pause exists right after bjk
Leftmost morpheme of a bunsetsu
? whether or not the basic form or part-of-
speech of the leftmost morpheme of the next
bunsetsu of bjk is one of the morphemes enu-
merated in Section 3.5.
5 Experiment
To evaluate the effectiveness of our method, we
conducted an experiment on inserting linefeeds by
using discourse speech data.
5.1 Outline of Experiment
As the experimental data, we used the transcribed
data of Japanese discourse speech in the SIDB
(Matsubara et al, 2002). All the data are anno-
tated with information on morphological analysis,
clause boundary detection and dependency anal-
ysis by hand. We performed a cross-validation
experiment by using 16 discourses. That is, we
535
repeated the experiment, in which we used one
discourse from among 16 discourses as the test
data and the others as the learning data, 16 times.
However, since we used 2 discourse among 16
discourses as the preliminary analysis data, we
evaluated the experimental result for the other 14
discourses (1,714 sentences, 20,707 bunsetsus).
Here, we used the maximum entropy method tool
(Zhang, 2008) with the default options except ?-i
2000.?
In the evaluation, we obtained recall, precision
and the ratio of sentences into which all linefeed
points were correctly inserted (hereinafter called
sentence accuracy). The recall and precision are
respectively defined as follows.
recall = # of correctly inserted LFs# of LFs in the correct data
precision = # of correctly inserted LFs# of automatically inserted LFs
For comparison, we established the following
four baseline methods.
1. Linefeeds are inserted into the rightmost bun-
setsu boundaries among the bunsetsu bound-
aries into which linefeeds can be inserted so
that the length of the line does not exceed
the maximum number of characters (Line-
feed insertion based on bunsetsu bound-
aries).
2. Linefeeds are inserted into the all clause
boundaries (Linefeed insertion based on
clause boundaries).
3. Linefeeds are inserted between adjacent bun-
setsus which do not depend on each other
(Linefeed insertion based on dependency
relations).
4. Linefeeds are inserted into the all bunsetsu
boundaries in which a pause exists (Linefeed
insertion based on pauses).
In the baseline 2, 3 and 4, if each condition is not
fulfilled within the maximum number of charac-
ters, a linefeed is inserted into the rightmost bun-
setsu boundary as well as the baseline 1.
In the experiment, we defined the maximum
number of characters per line as 20. The cor-
rect data of linefeed insertion were created by ex-
perts who were familiar with displaying captions.
There existed 5,497 inserted linefeeds in the 14
discourses, which were used in the evaluation.
Table 3: Experimental results
recall (%) precision (%) F-measure
our method 82.66 80.24 81.43
(4,544/5,497) (4,544/5,663)
baseline 1 27.47 34.51 30.59
(1,510/5,497) (1,510/4,376)
baseline 2 69.34 48.65 57.19
(3,812/5,497) (3,812/7,834)
baseline 3 89.48 53.73 67.14
(4,919/5,497) (4,919/9,155)
baseline 4 69.84 55.60 61.91
(3,893/5,497) (3,893/6,905)
5.2 Experimental Result
Table 3 shows the experimental results of the base-
lines and our method. The baseline 1 is very sim-
ple method which inserts linefeeds into the bun-
setsu boundaries so that the length of the line does
not exceed the maximum number of characters per
line. Therefore, the recall and precision were the
lowest.
In the result of baseline 2, the precision was
low. As described in the Section 3.1, the degree
in which linefeeds are inserted varies in differ-
ent types of clause boundaries. In the baseline
2, because linefeeds are also inserted into clause
boundaries which have the tendency that linefeeds
are hardly inserted, the unnecessary linefeeds are
considered to have been inserted.
The recall of baseline 3 was very high. This
is because, in the correct data, linefeeds were
hardly inserted between two neighboring bunset-
sus which are in a dependency relation. However,
the precision was low, because, in the baseline
3, linefeeds are invariably inserted between two
neighboring bunsetsus which are not in a depen-
dency relation.
In the baseline 4, both the recall and precision
were not good. The possible reason is that the bun-
setsu boundaries at which a pause exists do not
necessarily correspond to the linefeed points.
On the other hand, the F-measure and the sen-
tence accuracy of our method were 81.43 and
53.15%, respectively. Both of them were highest
among those of the four baseline, which showed
an effectiveness of our method.
5.3 Causes of Incorrect Linefeed Insertion
In this section, we discuss the causes of the in-
correct linefeed insertion occurred in our method.
Among 1,119 incorrectly inserted linefeeds, the
most frequent cause was that linefeeds were in-
536
??????????????????
????????
That is the period which I call
the first period without apology
Figure 6: Example of incorrect linefeed insertion
in ?adnominal clause.?
??????????????
?????
??????????????
??????????????????
(about how detail I can speak)
(I have a concern)
(from serious story to easy story )
(I want to speak)
Figure 7: Example of extra linefeed insertion
serted into clause boundaries of a ?adnominal
clause? type. The cause occupies 10.19% of the
total number of the incorrectly inserted linefeeds.
In the clause boundaries of the ?adnominal clause?
type, linefeeds should rarely be inserted funda-
mentally. However, in the result of our method,
a lot of linefeeds were inserted into the ?adnomi-
nal clause.? Figure 6 shows an example of those
results. In this example, a linefeed is inserted into
the ?adnominal clause? boundary which is located
right after the bunsetsu ????? (call).? The se-
mantic chunk ????????????? (is the
period which I call)? is divided.
As another cause, there existed 291 linefeeds
which divide otherwise one line according to the
correct data into two lines. Figure 7 shows an ex-
ample of the extra linefeed insertion. Although, in
the example, a linefeed is inserted between ???
???????????? (about how detail I
can speak)? and ?????? (I have a concern),?
the two lines are displayed in one line in the cor-
rect data. It is thought that, in our method, line-
feeds tend to be inserted even if a line has space to
spare.
6 Discussion
In this section, we discuss the experimental results
described in Section 5 to verify the effectiveness
of our method in more detail.
6.1 Subjective Evaluation of Linefeed
Insertion Result
The purpose of our research is to improve the read-
ability of the spoken monologue text by our line-
feed insertion. Therefore, we conducted a subjec-
tive evaluation of the texts which were generated
by the above-mentioned experiment.
In the subjective evaluation, examinees looked
at the two texts placed side-by-side between which
the only difference is linefeed points, and then se-
35
34
40
45
39
48
45
47 47
44
1 2 3 4 5 6 7 8 9 10
Baseline 3
Our method
subject ID
# of sentences
50
45
40
35
30
25
20
15
10
5
0
Figure 8: Result of subjective evaluation
lected the one which was felt more readable. Here,
we compared our method with the baseline 3, of
which F-measure was highest among four base-
lines described in Section 5.1. Ten examinees
evaluated 50 pairs of the results generated from
the same 50 randomly selected sentences.
Figure 8 shows the result of subjective evalua-
tion. This graph shows the number of each method
selected by each examinee. The ratio that our
method was selected was 94% in the highest case,
and 68% even in the lowest case. We confirmed
the effectiveness of our method for improving the
readability of the spoken monologue text.
On the other hand, there existed three sentences
for which more than 5 examinees judged that the
results of baseline 3 were more readable than those
of our method. From the analysis of the three sen-
tences, we found the following phenomena caused
text to be less readable
? Japanese syllabary characters (Hiragana) are
successionally displayed across a bunsetsu
boundary.
? The length of anteroposterior lines is ex-
tremely different each other.
Each example of the two causes is shown in
Figure 9 and 10, respectively. In Figure 9, a
bunsetsu boundary existed between Japanese syl-
labary characters ?????? (I)? and ?????
(if truth be told)? and these characters are succes-
sionally displayed in the same line. In these cases,
it becomes more difficult to identify the bunsetsu
boundary, therefore, the text is thought to become
difficult to read. In Figure 10, since the length of
the second line is extremely shorter than the first
line or third line, the text is thought to become dif-
ficult to read.
537
?????????????
???????????????????
????????
(Actually, I, if truth be told, I)
when I was a college student,  
(I) used to dodge  my train fare and
(be caught )
Actually, I, if truth be told, I used to dodge my train fare and be caught
when I was a college student.
Figure 9: Example of succession of hiragana
??????????????????
???
???????????????????
??????????????
I, the energy resources of which 
the remaining amount became little
in which humans who are in the past 
and future fight
(wrote a science-fiction novel)
(over)
I wrote a science-fiction novel, in which humans who are in the past and future 
fight over the energy resources of which the remaining amount became little.
Figure 10: Lines that have extremely different
length
Table 4: Other annotator?s results
recall (%) precision (%) F-measure
by human 89.82 (459/511) 89.82 (459/511) 89.82
our method 82.19 (420/511) 81.71 (420/514) 81.95
6.2 Comparison with Linefeeds Inserted by
Human
The concept of linefeed insertion for making the
caption be easy to read varies by the individual.
When multiple people insert linefeeds for the same
text, there is possibility that linefeeds are inserted
into different points.
Therefore, for one lecture data (128 sentences,
511 bunsetsus) in the experimental data, we con-
ducted an experiment on linefeed insertion by an
annotator who was not involved in the construc-
tion of the correct data. Table 4 shows the re-
call and the precision. The second line shows
the result of our method for the same lecture
data. In F-measure, our method achieved 91.24%
(81.95/89.82) of the result by the human annotator.
6.3 Performance of Linefeed Insertion Based
on Automatic Natural Language Analysis
In the experiment described in Section 5, we used
the linguistic information provided by human as
the features on the maximum entropy method.
However, compared with baseline 1, our method
uses a lot of linguistic information which should
be provided not by human but by natural language
analyzers under the real situation. Therefore, to
fairly evaluate our method and four baselines, we
conducted an experiment on linefeed insertion by
using the automatically provided information on
clause boundaries and dependency structures5.
5We used CBAP (Kashioka and Maruyama, 2004) as
a clause boundary analyzer and CaboCha (Kudo and Mat-
sumoto, 2002) with default learning data as a dependency
parser.
Table 5: Experimental results when information of
features are automatically provided
recall (%) precision (%) F-measure
our method 77.37 75.04 76.18
(4,253/5,497) (4,253/5,668)
baseline 1 27.47 34.51 30.59
(1,510/5,497) (1,510/4,376)
baseline 2 69.51 48.63 57.23
(3,821/5,497) (3,821/7,857)
baseline 3 84.01 52.03 64.26
(4,618/5,497) (4,618/8,876)
baseline 4 69.84 55.60 61.91
(3,893/5,497) (3.893/6,905)
Table 5 shows the result. Compared with Table
3, it shows the decreasing rate of the performance
of our method was more than those of four base-
lines which use simply only basic linguistic infor-
mation. However, the F-measure of our method
was more than 10% higher than those of four base-
lines.
7 Conclusion
This paper proposed a method for inserting line-
feeds into discourse speech data. Our method can
insert linefeeds so that captions become easy to
read, by using machine learning techniques on fea-
tures such as morphemes, dependencies, clause
boundaries, pauses and line length. An experi-
ment by using transcribed data of Japanese dis-
course speech showed the recall and precision was
82.66% and 80.24%, respectively, and we con-
firmed the effectiveness of our method.
In applying the linefeed insertion technique to
practical real-time captioning, we have to consider
not only the readability but also the simultaneity.
Since the input of our method is a sentence which
tends to be long in spoken monologue, in the fu-
ture, we will develop more simultaneous a tech-
nique in which the input is shorter than a sentence.
In addition, we assumed the speech recognition
system with perfect performance. To demonstrate
practicality of our method for automatic speech
transcription, an experiment using a continuous
speech recognition system will be performed in
the future.
Acknowledgments
This research was partially supported by the
Grant-in-Aid for Scientific Research (B) (No.
20300058) and Young Scientists (B) (No.
21700157) of JSPS, and by The Asahi Glass
Foundation.
538
References
G. Boulianne, J.-F. Beaumont, M. Boisvert,
J. Brousseau, P. Cardinal, C. Chapdelaine,
M. Comeau, P. Ouellet, and F. Osterrath. 2006.
Computer-assisted closed-captioning of live TV
broadcasts in French. In Proceedings of 9th Interna-
tional Conference on Spoken Language Processing,
pages 273?276.
T. Holter, E. Harborg, M. H. Johnsen, and T. Svendsen.
2000. ASR-based subtitling of live TV-programs for
the hearing impaired. In Proceedings of 6th Interna-
tional Conference on Spoken Language Processing,
volume 3, pages 570?573.
R. B. Hoogenboom, K. Uehara, T. Kanazawa,
S. Nakano, H. Kuroki, S. Ino, and T. Ifukube. 2008.
An application of real-time captioning system using
automatic speech recognition technology to college
efl education for deaf and hard-of-hearing students.
Gunma University Annual Research Reports, Cul-
tural Science Series, 57.
T. Imai, S. Sato, A. Kobayashi, K. Onoe, and
S. Homma. 2006. Online speech detection
and dual-gender speech recognition for captioning
broadcast news. In Proceedings of 9th International
Conference on Spoken Language Processing, pages
1602?1605.
H. Kashioka and T. Maruyama. 2004. Segmentation of
semantic units in Japanese monologues. In Proceed-
ings of ICSLT2004 and Oriental-COCOSDA2004,
pages 87?92.
T. Kudo and Y. Matsumoto. 2002. Japanese depen-
dency analysis using cascaded chunking. In Pro-
ceedings of 6th Conference on Computational Natu-
ral Language Learning, pages 63?69.
S. Matsubara, A. Takagi, N. Kawaguchi, and Y. Ina-
gaki. 2002. Bilingual spoken monologue corpus
for simultaneous machine interpretation research.
In Proceedings of 3rd International Conference on
Language Resources and Evaluation, pages 153?
159.
T. Monma, E. Sawamura, T. Fukushima, I. Maruyama,
T. Ehara, and K. Shirai. 2003. Automatic closed-
caption production system on TV programs for
hearing-impaired people. Systems and Computers
in Japan, 34(13):71?82.
C. Munteanu, G. Penn, and R. Baecker. 2007. Web-
based language modelling for automatic lecture tran-
scription. In Proceedings of 8th Annual Conference
of the International Speech Communication Associ-
ation, pages 2353?2356.
M. Saraclar, M. Riley, E. Bocchieri, and V. Gof-
fin. 2002. Towards automatic closed captioning:
Low latency real time broadcast news transcription.
In Proceedings of 7th International Conference on
Spoken Language Processing, pages 1741?1744.
J. Xue, R. Hu, and Y. Zhao. 2006. New improvements
in decoding speed and latency for automatic caption-
ing. In Proceedings of 9th International Conference
on Spoken Language Processing, pages 1630?1633.
L. Zhang. 2008. Maximum entropy mod-
eling toolkit for Python and C++. http:
//homepages.inf.ed.ac.uk/s0450736/
maxent toolkit.html. [Online; accessed
1-March-2008].
539
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 41?44,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
Incremental Parsing with Monotonic Adjoining Operation
Yoshihide Kato and Shigeki Matsubara
Information Technology Center, Nagoya University
Furo-cho, Chikusa-ku, Nagoya, 464-8601 Japan
{yosihide,matubara}@el.itc.nagoya-u.ac.jp
Abstract
This paper describes an incremental parser
based on an adjoining operation. By using
the operation, we can avoid the problem
of infinite local ambiguity in incremental
parsing. This paper further proposes a re-
stricted version of the adjoining operation,
which preserves lexical dependencies of
partial parse trees. Our experimental re-
sults showed that the restriction enhances
the accuracy of the incremental parsing.
1 Introduction
Incremental parser reads a sentence from left to
right, and produces partial parse trees which span
all words in each initial fragment of the sentence.
Incremental parsing is useful to realize real-time
spoken language processing systems, such as a si-
multaneous machine interpretation system, an au-
tomatic captioning system, or a spoken dialogue
system (Allen et al, 2001).
Several incremental parsing methods have been
proposed so far (Collins and Roark, 2004; Roark,
2001; Roark, 2004). In these methods, the parsers
can produce the candidates of partial parse trees
on a word-by-word basis. However, they suffer
from the problem of infinite local ambiguity, i.e.,
they may produce an infinite number of candidates
of partial parse trees. This problem is caused by
the fact that partial parse trees can have arbitrar-
ily nested left-recursive structures and there is no
information to predict the depth of nesting.
To solve the problem, this paper proposes an in-
cremental parsing method based on an adjoining
operation. By using the operation, we can avoid
the problem of infinite local ambiguity. This ap-
proach has been adopted by Lombardo and Sturt
(1997) and Kato et al (2004). However, this
raises another problem that their adjoining opera-
tions cannot preserve lexical dependencies of par-
tial parse trees. This paper proposes a restricted
version of the adjoining operation which preserves
lexical dependencies. Our experimental results
showed that the restriction enhances the accuracy
of the incremental parsing.
2 Incremental Parsing
This section gives a description of Collins and
Roark?s incremental parser (Collins and Roark,
2004) and discusses its problem.
Collins and Roark?s parser uses a grammar de-
fined by a 6-tuple G = (V, T, S,#, C,B). V is
a set of nonterminal symbols. T is a set of ter-
minal symbols. S is called a start symbol and
S ? V . # is a special symbol to mark the end
of a constituent. The rightmost child of every par-
ent is labeled with this symbol. This is necessary
to build a proper probabilistic parsing model. C
is a set of allowable chains. An allowable chain
is a sequence of nonterminal symbols followed by
a terminal symbol. Each chain corresponds to a
label sequence on a path from a node to its left-
most descendant leaf. B is a set of allowable
triples. An allowable triple is a tuple ?X,Y, Z?
where X,Y, Z ? V . The triple specifies which
nonterminal symbol Z is allowed to follow a non-
terminal symbol Y under a parent X .
For each initial fragment of a sentence, Collins
and Roark?s incremental parser produces partial
parse trees which span all words in the fragment.
Let us consider the parsing process as shown
in Figure 1. For the first word ?we?, the parser
produces the partial parse tree (a), if the allowable
chain ?S ? NP ? PRP ? we? exists in C. For
other chains which start with S and end with ?we?,
the parser produces partial parse trees by using the
chains. For the next word, the parser attaches the
chain ?VP?VBP? describe? to the partial parse
tree (a)
1
. The attachment is possible when the al-
lowable triple ?S, NP, VP? exists in B.
1
More precisely, the chain is attached after attaching end-
of-constituent# under the NP node.
41
WePRP
NP S(a)
WePRP
NP S(b)
describeVBP
VP
WePRP
NP S(c)
describeVBP
VP NPDTa
WePRP
NP S(d)
describeVBP
VP NP
DTa
WePRP
NP S(e)
describeVBP
VP NP
DTa
NP
NPNP
Figure 1: A process in incremental parsing
2.1 Infinite Local Ambiguity
Incremental parsing suffers from the problem of
infinite local ambiguity. The ambiguity is caused
by left-recursion. An infinite number of partial
parse trees are produced, because we cannot pre-
dict the depth of left-recursive nesting.
Let us consider the fragment ?We describe a.?
For this fragment, there exist several candidates of
partial parse trees. Figure 1 shows candidates of
partial parse trees. The partial parse tree (c) rep-
resents that the noun phrase which starts with ?a?
has no adjunct. The tree (d) represents that the
noun phrase has an adjunct or is a conjunct of a
coordinated noun phrase. The tree (e) represents
that the noun phrase has an adjunct and the noun
phrase with an adjunct is a conjunct of a coordi-
nated noun phrase. The partial parse trees (d) and
(e) are the instances of partial parse trees which
have left-recursive structures. The major problem
is that there is no information to determine the
depth of left-recursive nesting at this point.
3 Incremental Parsing Method Based on
Adjoining Operation
In order to avoid the problem of infinite local am-
biguity, the previous works have adopted the fol-
lowing approaches: (1) a beam search strategy
(Collins and Roark, 2004; Roark, 2001; Roark,
2004), (2) limiting the allowable chains to those
actually observed in the treebank (Collins and
Roark, 2004), and (3) transforming the parse trees
with a selective left-corner transformation (John-
son and Roark, 2000) before inducing the al-
lowable chains and allowable triples (Collins and
Roark, 2004). The first and second approaches can
prevent the parser from infinitely producing partial
parse trees, but the parser has to produce partial
parse trees as shown in Figure 1. The local ambi-
guity still remains. In the third approach, no left
recursive structure exists in the transformed gram-
mar, but the parse trees defined by the grammar are
different from those defined by the original gram-
mar. It is not clear if partial parse trees defined by
the transformed grammar represent syntactic rela-
tions correctly.
As an approach to solve these problems, we
introduce an adjoining operation to incremental
parsing. Lombardo and Sturt (1997) and Kato
et al (2004) have already adopted this approach.
However, their methods have another problem that
their adjoining operations cannot preserve lexical
dependencies of partial parse trees. To solve this
problem, this section proposes a restricted version
of the adjoining operation.
3.1 Adjoining Operation
An adjoining operation is used in Tree-Adjoining
Grammar (Joshi, 1985). The operation inserts a
tree into another tree. The inserted tree is called an
auxiliary tree. Each auxiliary tree has a leaf called
a foot which has the same nonterminal symbol as
its root. An adjoining operation is defined as fol-
lows:
adjoining An adjoining operation splits a parse
tree ? at a nonterminal node ? and inserts an
auxiliary tree ? having the same nonterminal
symbol as ?, i.e., combines the upper tree of
? with the root of ? and the lower tree of ?
with the foot of ?.
We write a
?,?
(?) for the partial parse tree obtained
by adjoining ? to ? at ?.
We use simplest auxiliary trees, which consist
of a root and a foot.
As we have seen in Figure 1, Collins and
Roark?s parser produces partial parse trees such as
(c), (d) and (e). On the other hand, by using the
adjoining operation, our parser produces only the
partial parse tree (c). When a left-recursive struc-
ture is required to parse the sentence, our parser
adjoins it. In the example above, the parser adjoins
the auxiliary tree ?NP ? NP? to the partial parse
tree (c) when the word ?for? is read. This enables
42
WePRP*
NP S
describeVBP*
VP* NPa  method WePRP*
NP S
describeVBP*
VP* NP
a  method
adjoining
NP* WePRP*
NP S
describeVBP*
VP* NP
a  methodNP* PPforIN*
Figure 2: Adjoining operation
WePRP*
NP S
describeVBP*
VP* NPJohn  's WePRP*
NP S
describeVBP*
VP* NPadjoining NPJohn  's
We  describe  John  's We  describe  John  's
(a) (b)
WePRP*
NP S
describeVBP*
VP* NPNPJohn  's
We  describe  John  's  method
(c)
NN*method
Figure 3: Non-monotonic adjoining operation
the parser to attach the allowable chain ?PP ? IN
? for?. The parsing process is shown in Figure 2.
3.2 Adjoining Operation and Monotonicity
By using the adjoining operation, we avoid the
problem of infinite local ambiguity. However, the
adjoining operation cannot preserve lexical depen-
dencies of partial parse trees. Lexical dependency
is a kind of relation between words, which repre-
sents head-modifier relation. We can map parse
trees to sets of lexical dependencies by identifying
the head-child of each constituent in the parse tree
(Collins, 1999).
Let us consider the parsing process as shown
in Figure 3. The partial parse tree (a) is a can-
didate for the initial fragment ?We describe John
?s?. We mark each head-child with a special sym-
bol ?. We obtain three lexical dependencies ?We
? describe?, ?John ? ?s? and ??s ? describe?
from (a). When the parser reads the next word
?method?, it produces the partial parse tree (b) by
adjoining the auxiliary tree ?NP ? NP?. The par-
tial parse tree (b) does not have ??s ? describe?.
The dependency ??s ? describe? is removed when
the parser adjoins the auxiliary tree ?NP ? NP? to
(a). This example demonstrates that the adjoining
operation cannot preserve lexical dependencies of
partial parse trees.
Now, we define the monotonicity of the adjoin-
ing operation. We say that adjoining an auxiliary
tree ? to a partial parse tree ? at a node ? is mono-
tonic when dep(?) ? dep(a
?,?
(?)) where dep is
the mapping from a parse tree to a set of dependen-
cies. An auxiliary tree ? is monotonic if adjoining
? to any partial parse tree is monotonic.
We want to exclude any non-monotonic auxil-
iary tree from the grammar. For this purpose, we
restrict the form of auxiliary trees. In our frame-
work, all auxiliary trees satisfy the following con-
straint:
? The foot of each auxiliary tree must be the
head-child of its parent.
The auxiliary tree ?NP ? NP
?
? satisfies the con-
straint, while ?NP ? NP? does not.
3.3 Our Incremental Parser
Our incremental parser is based on a probabilistic
parsing model which assigns a probability to each
operation. The probability of a partial parse tree is
defined by the product of the probabilities of the
operations used in its construction. The probabil-
ity of attaching an allowable chain c to a partial
parse tree ? is approximated as follows:
P (c | ?) = P
root
(R | P,L,H, t
H
, w
H
,D)
?P
template
(c
?
| R,P,L,H)
?P
word
(w | c
?
, t
h
, w
h
)
where R is the root label of c, c
?
is the sequence
which is obtained by omitting the last element
from c and w is the last element of c. The proba-
bility is conditioned on a limited context of ?. P
is a set of the ancestor labels of R. L is a set of the
left-sibling labels of R. H is the head label in L.
w
H
and t
H
are the head word and head tag of H ,
respectively. D is a set of distance features. w
h
and t
h
are the word and POS tag modified by w,
respectively. The adjoining probability is approxi-
mated as follows:
P (? | ?) = P
adjoining
(? | P,L,H,D)
where ? is an auxiliary tree or a special symbol
nil, the nil means that no auxiliary tree is ad-
joined. The limited contexts used in this model
are similar to the previous methods (Collins and
Roark, 2004; Roark, 2001; Roark, 2004).
To achieve efficient parsing, we use a beam
search strategy like the previous methods (Collins
and Roark, 2004; Roark, 2001; Roark, 2004). For
each word position i, our parser has a priority
queue H
i
. Each queue H
i
stores the only N -best
43
Table 1: Parsing results
LR(%) LP(%) F(%)
Roark (2004) 86.4 86.8 86.6
Collins and Roark (2004) 86.5 86.8 86.7
No adjoining 86.3 86.8 86.6
Non-monotonic adjoining 86.1 87.1 86.6
Monotonic adjoining 87.2 87.7 87.4
partial parse trees. In addition, the parser discards
the partial parse tree ? whose probability P (?) is
less than the P
?
? where P
?
is the highest proba-
bility on the queue H
i
and ? is a beam factor.
4 Experimental Evaluation
To evaluate the performance of our incremental
parser, we conducted a parsing experiment. We
implemented the following three types of incre-
mental parsers to assess the influence of the ad-
joining operation and its monotonicity: (1) with-
out adjoining operation, (2) with non-monotonic
adjoining operation, and (3) with monotonic ad-
joining operation. The grammars were extracted
from the parse trees in sections 02-21 of the Wall
Street Journal in Penn Treebank. We identified the
head-child in each constituent by using the head
rule of Collins (Collins, 1999). The probabilistic
models were built by using the maximum entropy
method. We set the beam-width N to 300 and the
beam factor ? to 10
?11
.
We evaluated the parsing accuracy by using sec-
tion 23. We measured labeled recall and labeled
precision. Table 1 shows the results
2
. Our in-
cremental parser is competitive with the previous
ones. The incremental parser with the monotonic
adjoining operation outperforms the others. The
result means that our proposed constraint of auxil-
iary trees improves parsing accuracy.
5 Conclusion
This paper has proposed an incremental parser
based on an adjoining operation to solve the prob-
lem of infinite local ambiguity. The adjoining
operation causes another problem that the parser
cannot preserve lexical dependencies of partial
parse trees. To tackle this problem, we defined
2
The best results of Collins and Roark (2004)
(LR=88.4%, LP=89.1% and F=88.8%) are achieved when
the parser utilizes the information about the final punctuation
and the look-ahead. However, the parsing process is not
on a word-by-word basis. The results shown in Table 1 are
achieved when the parser does not utilize such informations.
the monotonicity of adjoining operation and re-
stricted the form of auxiliary trees to satisfy the
constraint of the monotonicity. Our experimental
result showed that the restriction improved the ac-
curacy of our incremental parser.
In future work, we will investigate the incre-
mental parser for head-final language such as
Japanese. Head-final language includes many in-
direct left-recursive structures. In this paper, we
dealt with direct left-recursive structures only. To
process indirect left-recursive structures, we need
to extend our method.
References
James Allen, George Ferguson, and Amanda Stent.
2001. An architecture for more realistic conver-
sational systems. In Proceedings of International
Conference of Intelligent User Interfaces, pages 1?
8.
Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Pro-
ceedings of the 42nd Meeting of the Association for
Computational Linguistics (ACL?04), Main Volume,
pages 111?118, Barcelona, Spain, July.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Mark Johnson and Brian Roark. 2000. Compact
non-left-recursive grammars using the selective left-
corner transform and factoring. In Proceedings of
the 18th International Conference on Computational
Linguistics, pages 355?361, July.
Aravind K. Joshi. 1985. Tree adjoining grammars:
How much context sensitivity is required to provide
a reasonable structural description? In David R.
Dowty, Lauri Karttunen, and Arnold M. Zwicky, ed-
itors, Natural Language Parsing, pages 206?250.
Cambridge University Press.
Yoshihide Kato, Shigeki Matsubara, and Yasuyoshi In-
agaki. 2004. Stochastically evaluating the valid-
ity of partial parse trees in incremental parsing. In
Proceedings of the ACLWorkshop Incremental Pars-
ing: Bringing Engineering and Cognition Together,
pages 9?15, July.
Vincenzo Lombardo and Patrick Sturt. 1997. Incre-
mental processing and infinite local ambiguity. In
Proceedings of the 19th Annual Conference of the
Cognitive Science Society, pages 448?453.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276, June.
Brian Roark. 2004. Robust garden path parsing. Nat-
ural language engineering, 10(1):1?24.
44
Example-based Spoken Dialogue System using WOZ System Log
Hiroya MURAO *,**, Nobuo KAWAGUCHI **,? Shigeki MATSUBARA **,?
Yukiko YAMAGUCHI? Yasuyoshi INAGAKI?
* Digital Systems Development Center,SANYO Electric Co., Ltd.,
Hirakata-shi, Osaka, 573-8534 Japan
** Center for Integrated Acoustic Information Research,Nagoya University,
? Information Technology Center, Nagoya University,
Furo-cho, Chikusa-ku, Nagoya-shi, 464-8603 Japan
? The Faculty of Information Science and Technology, Aichi Prefectural University,
Nagakute-cho, Aichi-gun, Aichi, 480-1198, Japan
murao@hr.hm.rd.sanyo.co.jp
Abstract
This paper proposes a new framework for
a spoken dialogue system based on dia-
logue examples between human subjects
and the Wizard of OZ (WOZ) system. Us-
ing this framework and a model of infor-
mation retrieval dialogue, a spoken dia-
logue system for retrieving shop informa-
tion while driving in a car has been de-
signed. The system refers to the dialogue
examples to find an example that is suit-
able for generating a query or a reply. The
authors have also constructed a large-scale
dialogue database using a WOZ system,
which enables efficient collection of dia-
logue examples.
1 Introduction
Against the background of ever-increasing comput-
ing power, techniques for constructing spoken di-
alogue systems using large-scale speech and text
corpora have become the target of much research
(Levin et al, 1998; Young, 2002). In prior re-
search, the authors have proposed a spoken-dialogue
control technique using dialogue examples with the
aim of performing flexible dialogue control dur-
ing information-retrieval dialogue and of achieving
speech understanding robust against speech recog-
nition errors (Murao et al, 2001). This technique
uses input speech data and supplementary informa-
tion corresponding to input speech such as retrieval
formulas (queries) to form ?examples? that decide
system action. A system using this technique can-
not run effectively, however, without a large volume
of example data. Traditionally, though, collecting
human-to-human dialogue data and manually pro-
viding such supplementary information for each in-
stance of input speech has required considerable la-
bor.
In this paper, we address this problem and pro-
pose a new technique for constructing an example-
based dialogue system using, as example data, the
dialogue performed between a human subject and a
pseudo-spoken-dialogue system based on the Wiz-
ard of OZ (WOZ) scheme. We also describe a
specific spoken dialogue system for information re-
trieval that we constructed using this technique.
2 Dialogue Processing Based on Examples
We first provide an overview of example-based dia-
logue processing that we previously proposed (Mu-
rao et al, 2001).
2.1 Model of information retrieval dialogue
Given a scenario in which a human operator
searches an information database and returns infor-
mation to a user, dialog between the operator and
user can be modeled as shown in Fig. 1. The ele-
ments of this model are described below.
1. Request The user tells the operator the con-
tents of an inquiry and demands reference.
2. Retrieval The operator receiving the user?s re-
quest generates a query after referencing do-
main knowledge and current dialogue context
Domain Knowledge
and
Dialogue Context
(1)Request
(4)Reply
(2)Retrieval
(3)Search
     Results
Search
Tool
Information
Database
Queries
OperatorUser
request to query
result
 to re
ply
Search
Results
Figure 1: Information flow of information retrieval
dialogue
and then processes the query indirectly by ma-
nipulating a search tool such as an ordinary
computer.
3. Search results The search tool generates
search results.
4. Reply The operator returns a reply to the user
based on search results and dialogue context.
Setting up information flow in this way allows
us to view operator behavior in the following way.
Specifically, the operator in Fig. 1 makes two deci-
sions in the process of advancing dialog.
Decision 1: Generate a query after listening to user
speech
Decision 2: Generate a reply after receiving search
results
Here, an experienced operator would use more
than just the superficial information obtained from
user speech. To generate a query or reply that
best suits the user?s need at that time, the opera-
tor would also make use of domain knowledge, di-
alogue context, and the search results themselves.
In other words, this kind of dialogue processing can
be viewed as a mapping operation from input infor-
mation such as user speech and domain knowledge
to output information such as a query. With this in
mind, we considered whether a ?decision? to guide
such dialogue could be automatically performed by
referring to actual examples of behavior manifested
by an experienced human operator. In short, we de-
cided to store a large volume of dialogue examples,
i.e., mapping information, and to determine output
information for certain input information on the ba-
sis of mapping information stored in similar dia-
logue examples.
2.2 Generation of queries and replies based on
examples
2.2.1 Structure of example data
The two ?decisions? performed during the time of
information retrieval dialogue between the user and
operator can be expressed as a mapping between the
following input and output information.
? Input/output information in the decision for
generating a query:
Input User speech and dialogue context
Output Query
? Input/output information in the decision for
generating a reply:
Input User speech, dialogue context, and
search results
Output Reply
It is therefore sufficient to save those items that
cover such input and output information. Specifi-
cally, a large number of example data can be col-
lected using the following information as elements
to construct an example database.
1. Text of user speech
2. Query
3. Reply text
4. Search results
5. Dialogue context (past speech, grounding in-
formation, conversational objects , etc.)
The following describes the procedure for gener-
ating a query or reply with respect to input speech
by referencing an example database.
2.2.2 Query generation process
From among the examples in the example
database, the system extracts the one most similar
to the input speech and the dialogue context at that
time. It then adjusts the query in that example to fit
the input speech and generates a new query.
2.2.3 Reply generation process
The system performs a search based on the gen-
erated query and receives search results. It then ex-
tracts the most similar example from the example
database with respect to input speech, the dialogue
context at that time, and the search results. Finally,
the system adjusts the reply in that example to fit the
current conditions and generates a new reply.
2.3 Problem points
Operating a dialogue system based on dialogue
examples requires the construction of an example
database as described above. Constructing a large-
scale example database, moreover, requires a large
volume of dialogue text in which supplementary in-
formation such as queries and search results has
been provided with respect to input speech.
Up to now, we have been constructing an exam-
ple database by first collecting human-to-human di-
alogue and converting speech to text and then as-
signing queries, search results, and the like to each
instance of input speech. This, however, is a labori-
ous process. In addition, example data constructed
on the basis of human-to-human dialogue data may
have features different from those of human-to-
dialogue-system dialogue data. In other words, we
cannot call the above approach an optimal method
for constructing example data.
3 Construction of an Example Database
using the WOZ System
We propose the Wizard of OZ (WOZ) system as
one means of efficiently collecting dialogue data
that includes supplementary information attached to
speech. Carrying on a dialogue using WOZ makes
it possible to collect the information needed for
constructing an example database while collecting
speech data.
3.1 WOZ system
When carrying on a dialogue using the WOZ sys-
tem, the user feels that he or she is talking to a com-
pletely mechanical system despite the fact that a hu-
man being is actually being used for some of the
elements making up the dialogue system. Collect-
ing dialogue data by WOZ should therefore result in
Touch Panel
and
Display
Information
Database
Query 
Generation
Part
Speech 
Output
Reply 
Generation
Part
  Query
Reply Text
Log 
Information
Tree Structured
Keywords
  Search 
Results
The Operator
Speech Input
User
The WoZ Software
Reply-Statement
Bigram
Search 
Execution
Part
Speech 
Symthesis
Figure 2: Configuration of Wizard of OZ system
data that is closer to dialogue that would occur be-
tween a human and a machine.
Collecting spoken dialogue data using the WOZ
system has actually been performed a number of
times in the past (MADCOW, 1992; Bertenstam et
al., 1995; Life et al, 1996; Eskenazi et al, 1999;
San-Segundo et al, 2001; Lemmela and Boda, 2002;
Yoma et al, 2002). The objective of those stud-
ies, however, was to collect, analyze, and evaluate
dialogue data between people and artificial objects,
and in many cases, only one of the artificial-object?s
functions was taken over by a human, for example,
the speech recognition function.
Our study, however, goes further than the above.
In particular, we create special software (called
WOZ software) that allows a human being to per-
form the functions of interpreting user speech, gen-
erating queries and executing searches, and generat-
ing replies. We then propose a framework that en-
ables the operator (wizard) to carry on a dialogue
with the user while operating this WOZ software so
that obtained data can be used later to perform di-
rect control of a dialogue system. Specifically, we
configure a pseudo-spoken-dialogue system (WOZ)
consisting of WOZ software and an operator, hold
information retrieval dialogue between this system
and human subjects, and save the queries ,search re-
sults and reply statements generated at this time as
log information. We then use this log information
and text-converted speech to construct an example
database that can be used for dialogue control.
3.2 System configuration
Figure 2 shows the entire configuration of the WOZ
system that we constructed. In this configuration,
keywords 
search 
results
type of 
keywords
control buttons
Figure 3: An example of display of Wizard of OZ system (1): Query generation part
text input
buttons
search 
results
type of 
keywords
control buttons 
&
standard phrases
Figure 4: An example of display of Wizard of OZ system (2): Reply generation part
the WOZ software, which was created using the
C++ language, runs on a personal computer under
Windows2000. It consists of a screen for generat-
ing queries (query part) and a screen for generating
replies (reply part). Figures 3 and 4 show sample
screens of these parts. This GUI adopts a touch-
panel system to facilitate operations ? an operator
only has to touch a button on one of these screens
to generate a query, search an information database,
generate a reply, or output synthesized speech.
WOZ software must feature high operability to
achieve natural dialogue between the WOZ system
and a human user. When designing WOZ software
on the basis of a human-to-human dialogue corpus
that we previously collected, we used the following
techniques to enable the system to operate in real
time while carrying on a dialogue with the user.
First, the query part arranges keywords in a tree
structure by search type so that appropriate key-
words can be selected at a touch to generate a query
and retrieve information quickly 1 . Search results
are displayed at the bottom of the screen in list form.
Second, the reply part displays text-input buttons
for generating replies and a list of search results.
The text-input buttons correspond to words, phrases,
and short standard sentences, and pushing them in
1Queries that deal with context in regard to input speech are
currently not defined for the sake of simplicity in software op-
eration.
?Hungry, but not enough time.
?You want to eat Chineese noodle.
?Search Japanese food restaurant.
Figure 5: Examples of prompting panels
an appropriate order generates a reply in text form.
The arrangement of these text-input buttons on the
screen is based on connection frequency between
text elements (reply-statement bigram) as previously
determined from the human-to-human dialogue cor-
pus mentioned above. In other words, each text-
input button represents a text entry having the high-
est frequency of following the immediately previous
text entry to the left, which makes for quick genera-
tion of a reply. Furthermore, to enable quick input,
the section of the screen displaying the search results
has been designed so that the name portion of each
result can be touched directly and automatically in-
cluded in the reply. The generated reply in text form
is finally output in voice form via the speech synthe-
sis section of the system.
Switching back and forth between the query and
reply parts can be performed as needed using a
switch button. The reply part also includes but-
tons for instantly generating words and short phrases
of confirmation and encouragement (e.g., ?yes,? ?I
see?) while the user is speaking to create as natural
a dialogue as possible.
3.3 Collecting dialogue data by the WOZ
system
We targeted shop-information retrieval while driv-
ing a car as an information-retrieval application
based on spoken dialogue, and collected dialogue
data between the WOZ system and human subjects
(Kawaguchi et al, 2002). This data was collected
within an automobile driven by subjects each of
whom acted as a user searching for information. A
personal computer running the WOZ software was
placed in the automobile with the ?wizard? sitting
in the back seat. All spoken dialogue was recorded
using another personal computer.
Data collection was performed according to the
following procedure for a duration of about five min-
Table 1: Collected WOZ data
Number of Speech length Speech Units
sessions (min.)User WOZ User WOZ
487 499 791 13,828 12,487
utes per subject.
? A prompting panel such as shown in Fig. 5 is
presented to the subject.
? The subject converses freely with WOZ based
on the prompting panel shown.
The wizard operates the WOZ system while lis-
tening to the subject, that is, the wizard performs an
appropriate search and returns a reply using speech
synthesis 2 .
Table 1 shows the scale of collected data.
3.4 Constructing an example database using
WOZ log information
WOZ software was designed to output detailed log
information. This information consists mainly of
the following items. All log information is recorded
with time stamps.
? Speaker ID (input by the wizard when initiating
a dialogue)
? Query generated for the input speech in ques-
tion
? Search results returned for the generated query
(number of hits and shop IDs)
? Text of reply generated by the operator (wiz-
ard)
A saved WOZ log can be used to efficiently con-
struct an example database by the following proce-
dure. To begin with, a written record of user speech
is made based on the voice recording of spoken di-
alog with time information added. Next, based on
2The wizard generates queries, performs searches, and gen-
erates replies to the extent possible for speech to which defined
queries can be applied. If a query cannot be generated, the wiz-
ard will not keep trying and will generate only an appropriate
response.
(Well, search convenience stores near here.)
(I found CIRCLE-K Makinohara store and SUNKUS Kamenoi store near here.)
Search results
Dialogue history
The most similar 
example 
for query generation
The most similar 
example 
for reply generation
Input text
(Result of speech
 recognition)
Reply
Figure 6: A view of example-based dialogue system
Table 2: Configuration of constructed example
database
Number of Number of
sessions examples
243 1,206
the time information in the log output by WOZ soft-
ware, a correspondence is established between user
speech and queries and between search results and
replies.
We constructed an example database using a por-
tion of dialogue data collected in the above manner.
Table 2 summarizes the data used for this purpose.
Query and search-result correspondences were es-
tablished for about 20% of all user speech excluding
speech outside of the task in question and speech
outside of query specifications.
4 Spoken Dialogue System using Dialogue
Examples
We here describe a dialogue system that runs using
the example database that we constructed (see (Mu-
rao et al, 2001) for details). The task is to search for
shop information while inside an automobile. This
system was implemented using the C++ language
under Windows2000. Figure 6 shows a screen shot
of this example-based dialogue system.
4.1 System configuration
The following describes the components of this sys-
tem with reference to Fig. 7.
Dialogue example database (DEDB): Consists of
data constructed from dialogue text and log in-
formation output from WOZ software. Dia-
logue text is subjected to morphological anal-
ysis 3, and words essential to advancing the di-
alogue (e.g., shop name, facility name, food
name) are assigned word class tags based on
classes given to these words beforehand ac-
cording to meaning.
Word Class Database (WCDB): Consists of
words essential to the task in question and
classes given to them according to meaning.
Word classes are determined empirically based
on dialogue within the dialogue corpus.
Shop Information Database (SIDB): Consists of
a collection of information on about 800 restau-
rants and shops in Nagoya, the same as that
used in the WOZ system.
Speech Recognition: Uses ?Japanese Dictation
Toolkit(Kawahara et al, 2000)?. The lan-
guage model was created from the previously
collected human-to-human dialogue corpus.
3Using ChaSen morphological-analysis software for the
Japanese language (Asahara and Matsumoto, 2000).
Speech
Input
    Speech
Recognition   Query Generation
Search
Speech
Output
Dialogue Example
Database
(DEDB)
Word class
Database
(WCDB)
Shop Information
Database
(SIDB)
 Speech
Synthesis    Reply Generation
Figure 7: Configuration of example-based dialogue
system
Query Generation: Extracts from the DEDB the
example closest to current input speech and
conditions, modifies the query in that example
according to current conditions, and outputs the
result.
Search execution: Accesses the SIDB using the
generated query and obtains search results.
Reply Generation: Extracts from the DEDB the
example closest to input speech and search re-
sults, modifies the reply in that example ac-
cording to current conditions, and outputs the
result.
Speech Synthesis: Outputs replies in voice form
using a Japanese TTS (Text To Speech) soft-
ware ?EleganTalk Ver. 2.1? by Sanyo Electric
Co., Ltd. .
4.2 Operation
The following describes system operation (see Fig.
8 for a specific operation example).
Step 1: Extracting similar example for query
For a speech recognition result, the system
extracts the most similar example from the
DEDB. The robustness of the similarity cal-
culation between the input utterance and the
utterance in the DEDB should be considered
against the speech recognition error. Therefore,
a keyword matching method using the word
class information is adopted. For a speech
recognition result combined with a morpholog-
ical analysis result, independent words and the
Input:   Etto, spaghetti no omise ni ikitai na.
            (I'd like to go to a spaghetti restaurant.)
Keywords: [10: spaghetti],[omise (shop)],[iku (go)]
 1st:  U: <10:Curry> no [omise] ni [iki]tain desu kedo            
(I'd like to go to a curry restaurant. )
         Q: search KEY=<10:curry>
 2nd: U: <10: Ramen(noodles)> wo <tabe> ni [iki] taina       
(I'd like to eat noodles.)
         Q: search KEY=<10:ramen>
 3rd: U: [10: Spaghetti] de <yu-mei> na <tokoro> ga iidesu 
            ( I prefer a popular resutaurant for spaghetti.)
         Q: search KEY=<10:spaghetti>
Step1: Extracting similar example for query
Step2: Query Modification
Query in the similar case:      search KEY=<10:curry>
   Matched keywords pair:       ( <10:curry> , <10:spaghetti> )
                 Output Query:      search KEY=<10:spaghetti>
Step3: Search
  Iutput Query:      search KEY=<10:spaghetti>
 Search Result:     RESULT=NONE
Input:   Etto, spaghetti no omise ni ikitai na.
(I'd like to go to a spaghetti restaurant.)
Keywords: [10: spaghetti],[omise (shop)],[iku (go)]
1st:   U:<10: Ramen(noodles)> wo <tabe> ni [iki] taina 
(I'd like to eat noodles.)
        Q: search KEY=<10:ramen>
        S:<10:Ramen(noodles)> no [omise] wa chikaku ni arimasen 
              ( There are no noodle restaurants near here.)
        A: RESULT=NONE
2nd:  U:<10:Curry> no [omise] ni [iki]tain desu kedo  
(I'd like to go to a curry restaurant. )
        Q: search KEY=<10:curry>
        S:Hai, Curry no omise wa 5-ken arimasu          
             (Well, I found 5 curry restaurants.)
        A: RESULT=5, ID1=120,..,ID5=565
Step4: Extracting similar example for reply
 Search Result:     RESULT=NONE
Similar cases
{Similar cases
Step5: Reply Modification
     Reply in the similar case: 
                   <10:Ramen(noodles)> no [omise] wa chikaku ni arimasen 
                       ( There are no noodle restaurants near here.)
       Matched keywords pair:  
                  ( <10:Ramen(noodles)> , <10:spaghetti> )
                      Output Reply:  
                  <10:spaghetti> no [omise] wa chikaku ni arimasen
                       ( There are no spaghetti restaurants near here.)
Figure 8: Example of query and reply generation
important words to which the word class tags
are assigned according to the information in
the WCDB are regarded as the keywords, and
their similarity is calculated as follows. For
each transcription of a user?s utterances in the
DEDB, the number of matched words and the
number of important words which belong to
the same word class are accumulated with the
correspondent weight and the result is treated
as the similarity. The utterance which marks
the highest similarity is regarded as the most
similar one.
Step2: Query Modification The query for the ex-
tracted example is modified with reference to
the input utterance. The modification is per-
formed by replacing the keywords in the refer-
ence query using word class information.
Step 3: Search The SIDB is searched by using the
modified query and a search result is obtained.
Step 4: Extracting similar example for reply
The system extracts the most similar example
from the DEDB, by taking account of not only
the similarity between the input utterance and
the utterance in examples but also that between
the number of items in the search result and
that in the examples. Here, a total similarity
score is computed by performing a weighted
summation of two values: the utterance sim-
ilarity score and the search-results similarity
score obtained from the difference between
the number of search results in an example
and that obtained in Step 3. The search-results
similarity score is computed as follows.
When the number of search results by mod-
ified query is 0: Give the highest score to
examples in the example database with 0 num-
ber of search results and the lowest score to all
other examples.
When the number of search results by mod-
ified query is 1 or more: Give the high-
est score to examples in the example database
with the same number of search results and an
increasingly lower score as difference in the
number of search results becomes larger (use
heuristics).
For example, if not even one search result could
be obtained by the modified query, examples in
the example database with not even one search
result constitute a match.
Step 5: Reply Modification The reply statement
for the extracted example is modified with ref-
erence to the input utterance. The modification
is performed by replacing the words in the ref-
erence reply statement by using word class in-
formation. Then a speech synthesis module is
used to produce a reply speech.
4.3 Adding, modification, and deletion of
example data
This system allows example data to be added, mod-
ified, and deleted. When a failed operation occurs
while carrying on a dialogue, for example, buttons
located at the bottom of the screen can be used to
modify existing example data, add new examples,
and delete unnecessary examples.
5 Conclusion
This paper has proposed an efficient technique for
collecting example data using the Wizard of OZ
(WOZ) system for the purpose of guiding spoken di-
alogue using dialogue examples. This technique has
the following effects.
? Knowledge buried in the WOZ system log
(conversions from input speech to query and
reply, etc.) can be used as dialogue system
knowledge.
? Because dialogue is collected using the WOZ
system, the examples so collected are close to
dialogue that would occur in an environment
with an actual dialogue system. In other words,
dialogue examples can be collected under con-
ditions close to human-to-machine dialogue.
? The labor involved in recording speech neces-
sary for construction of an example database
can be reduced.
In future research, we plan to evaluate dialogue-
processing performance and context processing us-
ing example databases constructed with the WOZ
system.
References
Masayuki Asahara and Yuji Matsumoto. 2000. Extended
models and tools for high-performance part-of-speech
tagger. In Proceedings of COLING 2000, July.
J. Bertenstam, M. Blomberg, R. Carlson, K. Elenius,
B. Granstrom, J. Gustafson, S. Hunnicutt, J. Hogberg,
R. Lindell, L. Neovius, A. de Serpa-Leitao, L. Nord,
and N. Strom. 1995. The waxholm application data-
base. In Proceedings of Eurospeech-95, volume 1,
pages 833?836.
Maxine Eskenazi, Alexander Rudnicky, Karin Gregory,
Paul Constantinides Robert Brennan, Christina Ben-
nett, and Jwan Allen. 1999. Data collection and pro-
cessing in the carnegie mellon communicator. In Pro-
ceedings of Eurospeech-99, volume 6, pages 2695?
2698.
Nobuo Kawaguchi, Shigeki Matsubara, Kazuya Takeda,
and Fumitada Itakura. 2002. Multi-dimensional
data acquisition for integrated acoustic information
research. In Proc. of 3rd International Language
Resources and Evaluation Conference (LREC-2002),
pages 2043?2046.
T. Kawahara, A. Lee, T. Kobayashi, K. Takeda, N. Mine-
matsu, S. Sagayama, K. Itou, A. Ito, M. Yamamoto,
A. Yamada, T. Utsuro, and K. Shikano. 2000. Free
software toolkit for japanese large vocabulary contin-
uous speech recognition. In Proceedings of ICSLP-
2000, volume 4, pages 476?479.
Saija-Maaria Lemmela and Peter Pal Boda. 2002. Effi-
cient combination of type-in and wizard-of-oz tests in
speech interface development process. In Proceedings
of ICSLP-2002, pages 1477?1480.
Esther Levin, Roberto Pieraccini, and Wieland Eckert.
1998. Using markov decision processes for learning
dialogue strategies. In Proceedings of ICASSP98, vol-
ume 1, pages 201?204.
A. Life, I. Salter, J.N. Temem, F. Bernard, S. Rosset,
S. Bennacef, and L. Lamel. 1996. Data collection
for the mask kiosk: Woz vs prototype system. In Pro-
ceedings of ICSLP-96, pages 1672?1675.
MADCOW. 1992. Multi-site data collection for a spo-
ken language corpus. In DARPA Speech and Natural
Language Workshop ?92.
Hiroya Murao, Nobuo Kawaguchi, Shigeki Matsubara,
and Yasuyoshi Inagaki. 2001. Example-based query
generation for spontaneous speech. In Proceedings of
2001 IEEE Workshop on Automatic Speech Recogni-
tion and Understanding (ASRU2001).
R. San-Segundo, J.M. Montero, J.M. Gutierrez, A. Gal-
lardo, J.D. Romeral, and J.M. Pardo. 2001. A
telephone-based railway information system for span-
ish: Development of a methodology for spoken dia-
logue design. In Proceedings of SIGdial-2001, pages
140?148.
Nestor Becerra Yoma, Angela Cortes, Mauricio Hormaz-
abal, and Enrique Lopez. 2002. Wizard of oz evalua-
tion of a dialogue with communicator system in chile.
In Proceedings of ICSLP-2002, pages 2701?2704.
Steve Young. 2002. Talking to machines (statistically
speaking). In Proceedings of ICSLP-2002, pages 9?
16.
Stochastically Evaluating the Validity of Partial Parse Trees in
Incremental Parsing
Yoshihide Kato1, Shigeki Matsubara2 and Yasuyoshi Inagaki3
Graduate School of International Development, Nagoya University 1
Information Technology Center, Nagoya University 2
Furo-cho, Chikusa-ku, Nagoya, 464-8601 Japan
Faculty of Information Science and Technology, Aichi Prefectural University 3
1522-3 Ibaragabasama, Kumabari, Nagakute-cho, Aichi-gun, 480-1198 Japan
yosihide@gsid.nagoya-u.ac.jp
Abstract
This paper proposes a method for evaluating the
validity of partial parse trees constructed in incre-
mental parsing. Our method is based on stochastic
incremental parsing, and it incrementally evaluates
the validity for each partial parse tree on a word-
by-word basis. In our method, incremental parser
returns partial parse trees at the point where the va-
lidity for the partial parse tree becomes greater than
a threshold. Our technique is effective for improv-
ing the accuracy of incremental parsing.
1 Introduction
Real-time spoken language processing systems,
such as simultaneous machine interpretation sys-
tems, are required to quickly respond to users? utter-
ances. To fulfill the requirement, the system needs
to understand spoken language at least incremen-
tally (Allen et al, 2001; Inagaki and Matsubara,
1995; Milward and Cooper, 1994), that is, to ana-
lyze each input sentence from left to right and ac-
quire the content.
Several incremental parsing methods have been
proposed to date (Costa et al, 2001; Haddock,
1987; Matsubara et al, 1997; Milward, 1995;
Roark, 2001). These methods construct candidate
partial parse trees for initial fragments of the input
sentence on a word-by-word basis. However, these
methods contain local ambiguity problems that par-
tial parse trees representing valid syntactic relations
can not be determined without using information
from the rest of the input sentence.
On the other hand, Marcus proposed a method
of deterministically constructing valid partial parse
trees by looking ahead several words (Marcus,
1980), while Kato et al proposed an incremental
parsing which delays the decision of valid partial
parse trees (Kato et al, 2000). However, it is hard to
say that these methods realize broad-coverage incre-
mental parsing. The method in the literature (Mar-
cus, 1980) uses lookahead rules, which are con-
structed by hand, but it is not clear whether broad
coverage lookahead rules can be obtained. The
incremental parsing in the literature (Kato et al,
2000), which is based on context free grammar, is
infeasible to deal with large scale grammar, because
the parser exhaustively searches all candidate partial
parse trees in top-down fashion.
This paper proposes a probabilistic incremental
parser which evaluates the validity of partial parse
trees. Our method extracts a grammar from a tree-
bank, and the incremental parsing uses a beam-
search strategy so that it realizes broad-coverage
parsing. To resolve local ambiguity, the parser in-
crementally evaluates the validity of partial parse
trees on a word-by-word basis, and delays the deci-
sion of which partial parse trees should be returned,
until the validity for the partial parse tree becomes
greater than a threshold. Our technique is effective
for improving the accuracy of incremental parsing.
This paper is organized as follows: The next
section proposes a probabilistic incremental parser.
Section 3 discusses the validity of partial parse tree
constructed in incremental parsing. Section 4 pro-
poses a method of incrementally evaluating the va-
lidity of partial parse tree. In section 5, we report an
experimental evaluation of our method.
2 TAG-based Incremental Parsing
Our incremental parsing is based on tree adjoining
grammar (TAG) (Joshi, 1985). This section pro-
poses a TAG-based incremental parsing method.
2.1 TAG for Incremental Parsing
Firstly, we propose incremental-parsing-oriented
TAG (ITAG). An ITAG comprises two sets of ele-
mentary trees just like TAG: initial trees and auxil-
iary trees. The difference between ITAG and TAG
is the form of elementary trees. Every ITAG ini-
tial tree is leftmost-expanded. A tree is leftmost-
expanded if it is of the following forms:
1. [t]X , where t is a terminal symbol and X is a
nonterminal symbol.
SNP VPPRPI
VPVB NPfound
NPDT NN
a
NNdime NPDT NNthe
NN
wood
Initial trees:1 2
5 7 8
10
VPVB NPfound
3 ADJP
PPIN NPin
NPNP* PPIN NPin
VPVP*
Auxiliary trees:1 2
NPDT NN
a
6 JJ NPDT NNthe
9 JJ
VPVBfound
4
Figure 1: Examples of ITAG elementary trees
2. [?X1 ? ? ?Xk]X , where ? is a leftmost expanded
tree, X1, . . . , Xk, X are nonterminal symbols.
On the other hand, every ITAG auxiliary tree is of
the following form:
[X??X1 ? ? ?Xk]X
where ? is a leftmost expanded tree and X ,
X1, . . . , Xk are nonterminal symbols. X? is called
a foot node. Figure 1 shows examples of ITAG ele-
mentary trees.
These elemental trees can be combined by using
two operations: substitution and adjunction.
substitution The substitution operation replaces a
leftmost nonterminal leaf of a partial parse tree
? with an initial tree ? having the same nonter-
minal symbol at its root. We write s? for the
operation of substituting ? and s?(?) for the
result of applying s? to ?.
adjunction The adjunction operation splits a par-
tial parse tree ? at a nonterminal node having
no nonterminal leaf, and inserts an auxiliary
tree ? having the same nonterminal symbol at
its root. We write a? for the operation of ad-
joining ? and a?(?) for the result of applying
a? to ?.
The substitution operation is similar to rule expan-
sion of top-down incremental parsing such as (Mat-
subara et al, 1997; Roark, 2001). Furthermore,
by introducing the adjunction operation to incre-
mental parsing, we can expect that local ambiguity
of left-recursive structures is decreased (Lombardo
and Sturt, 1997).
Our proposed incremental parsing is based on
ITAG. When i-th word wi is scanned, the parser
combines elementary trees for wi with partial parse
trees for w1 ? ? ?wi?1 to construct the partial parse
trees for w1 ? ? ?wi?1wi.
As an example, let us consider incremental pars-
ing of the following sentence by using ITAG shown
in Figure 1:
I found a dime in the wood. (1)
Table 1 shows the process of tree construction
for the sentence (1). When the word ?found? is
scanned, partial parse trees #3, #4 and #5 are con-
structed by applying substitution operations to par-
tial parse tree #2 for the initial fragment ?I?. When
the word ?in? is scanned, partial parse trees #12 and
#13 are constructed by applying adjunction opera-
tions to partial parse tree #10 for the initial frag-
ment ?I found a dime?. This example shows that
the ITAG based incremental parsing is capable of
constructing partial parse trees of initial fragments
for every word input.
2.2 ITAG Extraction from Treebank
Here, we propose a method for extracting an ITAG
from a treebank to realize broad-coverage incre-
mental parsing. Our method decomposes parse trees
in treebank to obtain ITAG elementary trees. The
decomposition is as follows:
? for each node ?1 having no left-sibling, if the
parent ?p has the same nonterminal symbol as
?1, split the parse tree at ?1 and ?p, and com-
bine the upper tree and the lower tree. ?1 of
intermediate tree is a foot node.
? for each node ?2 having only one left-sibling,
if the parent ?p does not have the same nonter-
minal symbol as the left-sibling ?1 of ?2, split
the parse tree at ?2.
? for the other node ? in the parse tree, split the
parse tree at ?.
For example, The initial trees ?1, ?2, ?5, ?7 ?8 and
?10 and the auxiliary tree ?2 are extracted from the
parse tree #18 in Table 1.
Our proposed tree extraction is similar to the TAG
extractions proposed in the literatures (Chen and
Vijay-Shanker, 2000; Chiang, 2003; Xia, 1999).
The main difference between these methods is the
position of nodes at which parse trees are split.
While the methods in the literatures (Chen and
Vijay-Shanker, 2000; Chiang, 2003; Xia, 1999) uti-
lize a head percolation rule to split the parse trees at
complement nodes, our method splits the parse trees
Table 1: Incremental parsing process of ?I found a dime in the wood.?
word # partial parse tree
1 s
I 2 [[[I]prp]npvp]s
found 3 [[[I]prp]np[[found]vbnp]vp]s
4 [[[I]prp]np[[found]vbnp adjp]vp]s
5 [[[I]prp]np[[found]vb]vp]s
a 6 [[[I]prp]np[[found]vb[[a]dtnn]np]vp]s
7 [[[I]prp]np[[found]vb[[a]dtjj nn]np]vp]s
8 [[[I]prp]np[[found]vb[[a]dtnn]npadjp]vp]s
9 [[[I]prp]np[[found]vb[[a]dtjj nn]npadjp]vp]s
dime 10 [[[I]prp]np[[found]vb[[a]dt[dime]nn]np]vp]s
11 [[[I]prp]np[[found]vb[[a]dt[dime]nn]npadjp]vp]s
in 12 [[[I]prp]np[[[found]vb[[a]dt[dime]nn]np]vp[[in]innp]pp]vp]s
13 [[[I]prp]np[[found]vb[[[a]dt[dime]nn]np[[in]innp]pp]np]vp]s
the 14 [[[I]prp]np[[[found]vb[[a]dt[dime]nn]np]vp[[in]in[[the]dtnn]np]pp]vp]s
15 [[[I]prp]np[[[found]vb[[a]dt[dime]nn]np]vp[[in]in[[the]dtjj nn]np]pp]vp]s
16 [[[I]prp]np[[found]vb[[[a]dt[dime]nn]np[[in]in[[the]dtnn]np]pp]np]vp]s
17 [[[I]prp]np[[found]vb[[[a]dt[dime]nn]np[[in]in[[the]dtjj nn]np]pp]np]vp]s
wood 18 [[[I]prp]np[[[found]vb[[a]dt[dime]nn]np]vp[[in]in[[the]dt[wood]nn]np]pp]vp]s
19 [[[I]prp]np[[found]vb[[[a]dt[dime]nn]np[[in]in[[the]dt[wood]nn]np]pp]np]vp]s
at left recursive nodes and nodes having left-sibling.
The elementary trees extracted by our method are of
the forms described in section 2.1, and can be com-
bined from left to right on a word-by-word basis.
The property is suitable for incremental parsing. On
the other hand, the elementary trees obtained by the
method based on head information does not neces-
sarily have this property 1.
2.3 Probabilistic ITAG
This section describes probabilistic ITAG (PITAG)
which is utilized by evaluating partial parse trees in
incremental parsing. PITAG assigns a probability
to the event that an elementary tree is combined by
substitution or adjunction with another tree.
We induce the probability by maximum likeli-
hood estimation. Let ? be an initial tree and X be
the root symbol of ?. The probability that ? is sub-
stituted is calculated as follows:
P (s?) = C(s?)?
???I(X) C(s??)
(2)
where C(s?) is the count of the number of times of
applying substitution s? in the treebank, and I(X)
is the set of initial trees whose root is labeled with
X .
1For example, the tree extraction based on head informa-
tion splits the parse tree #18 at the node labeled with dt to ob-
tain the elementary tree [a]dt for ?a?. However, the tree [a]dt
cannot be combined with the partial parse tree for ?I found?,
since substitution node labeled with dt exists in the initial tree
[dt[dime]nn]np for ?dime? and not the partial parse trees for ?I
found?.
Let ? be a auxiliary tree and X be the root symbol
of ?. The probability that ? is adjoined is calculated
as follows:
P (a?) = C(a?)C(X) (3)
where C(X) is the count of the number of occur-
rences of symbol X . The probability that adjunction
is not applied is calculated as follows:
P (nilX) = 1?
?
??A(X)
P (a?) (4)
where nilX means that the adjunction is not applied
to a node labeled with X , and A(X) is the set of all
auxiliary trees whose root is labeled X .
In this PITAG formalism, the probability that el-
ementary trees are combined at each node depends
only on the nonterminal symbol of that node 2.
The probability of a parse tree is calculated by the
product of the probability of the operations which
are used in construction of the parse tree. For ex-
ample, the probability of each operation is given as
shown in Table 2. The probability of the partial
parse tree #12, which is constructed by using s?1 ,
s?2 , s?5 , s?7 , nilNP and a?2 , is 1 ? 0.7 ? 0.3 ?
0.5? 0.7? 0.7 = 0.05145.
We write P (?) for the probability of a partial
parse tree ?.
2The PITAG formalism corresponds to SLG(1) in the liter-
ature (Carroll and Weir, 2003).
Table 2: Probability of operations
operation probability
s?1 1.0
s?2 0.7
s?7 , s?10 0.5
s?5 , s?8 0.3
s?4 , s?6 , s?9 0.2
s?3 0.1
a?1 0.3
a?2 0.7
nilNP 0.7
nilV P 0.3
2.4 Parsing Strategies
In order to improve the efficiency of the parsing, we
adapt two parsing strategies as follows:
? If two partial parse trees have the same se-
quence of nodes to which ITAG operations are
applicable, then the lower probability tree can
be safely discarded.
? The parser only keeps n-best partial parse trees.
3 Validity of Partial Parse Trees
This section gives some definitions about the valid-
ity of a partial parse tree. Before describing the va-
lidity of a partial parse tree, we define the subsump-
tion relation between partial parse trees.
Definition 1 (subsumption relation) Let ? and ?
be partial parse trees. Then we write ? ? ? , if
s?(?) = ? , for some initial tree ? or a?(?) = ? ,
for some auxiliary tree ?. Let ?? be the reflexive
transitive closure of ?. We say that ? subsumes ? ,
if ? ?? ? . 2
That ? subsumes ? means that ? is the result of ap-
plying a substitution or an adjunction to ?. Figure 2
shows the subsumption relation between the partial
parse trees constructed for the sentence (1).
If a partial parse tree for an initial fragment repre-
sents a syntactic relation correctly, the partial parse
tree subsumes the correct parse tree for the input
sentence. We say that such a partial parse tree is
valid. The validity of a partial parse tree is defined
as follows:
Definition 2 (valid partial parse tree) Let ? be a
partial parse tree and w1 ? ? ?wn be an input sen-
tence. We say that ? is valid for w1 ? ? ?wn if ? sub-
sumes the correct parse tree for w1 ? ? ?wn. 2
#1 I #2 #3 #6
#7
found a dime #10 #12 #14 #18
#19#16#13
in the wood
subsumption relation
#4 #8
#9
#11
#15
#17
#5
Figure 2: Subsumption relation between partial
parse trees
#1 I #2 #3 #6
#7
found a dime #10 #12 #14 #18
#19#16#13
in the wood
subsumption relation
#4 #8
#9
#11
#15
#17
valid partial parse tree#5
Figure 3: Valid partial parse trees
For example, assume that the #18 is correct parse
tree for the sentence (1). Then partial parse tree #3
is valid for the sentence (1), because #3 ?? #18. On
the other hand, partial parse tree #4 and #5 are not
valid for (1). Figure 3 shows the valid partial parse
trees for the sentence (1).
4 Evaluating the Validity of Partial Parse
Tree
The validity of a partial parse tree for an initial frag-
ment depends on the rest of the sentence. For ex-
ample, the validity of the partial parse trees #3, #4
and #5 depends on the remaining input that follows
the word ?found.? This means that the validity dy-
namically varies for every word input. We define a
conditional validity of partial parse tree:
V (? | w1 ? ? ?wj) =
?
??Sub(?,w1???wj) P (?)?
??T (w1???wj) P (?)
(5)
where ? is a partial parse tree for an initial frag-
ment w1 ? ? ?wi(i ? j), T (w1 ? ? ?wj) is the set of
constructed partial parse trees for the initial frag-
ment w1 ? ? ?wj and Sub(?,w1 ? ? ?wj) is the subset
of T (w1 ? ? ?wj) whose elements are subsumed by ?.
The equation (5) represents the validity of ? on the
condition w1 ? ? ?wj . ? is valid for input sentence
if and only if some partial parse tree for w1 ? ? ?wj
subsumed by ? is valid. The equation 5 is the ratio
of such partial parse trees to the constructed partial
parse trees.
4.1 Output Partial Parse Trees
Kato et al proposed a method of delaying the deci-
sion of which partial parse trees should be returned
as the output, until the validity of partial parse trees
are guaranteed (Kato et al, 2000). The idea of
delaying the decision of the output is interesting.
However, delaying the decision until the validity are
guaranteed may cause the loss of incrementality of
the parsing.
To solve the problem, in our method, the in-
cremental parser returns high validity partial parse
trees rather than validity guaranteed partial parse
trees.
When the j-th word wj is scanned, our incremen-
tal parser returns the following partial parse:
argmax{?:V (?,w1???wj)??}l(?) (6)
where ? is a threshold between [0, 1] and l(?) is
the length of the initial fragment which is yielded
by ?. The output partial parse tree is the one for
the longest initial fragment in the partial parse trees
whose validity are greater than a threshold ?.
4.2 An Example
Let us consider a parsing example for the sentence
(1). We assume that the threshold ? = 0.8.
Let us consider when the partial parse tree
#3, which is valid for (1), is returned as output.
When the word ?found? is scanned, partial parse
trees #3, #4 and #5 are constructed. That is,
T (I found) = {#3,#4,#5}. As shown in Figure
2, Sub(#3, I found) = {#3}. Furthermore,
P (#3) = 0.7, P (#4) = 0.1 and P (#5) = 0.2.
Therefore, V alidity(#3, I found) =
0.7/(0.7 + 0.1 + 0.2) = 0.7. Because
V alidity(#3, I found) < ?, partial parse tree
#3 is not returned as the output at this point. The
parser only keeps #3 as a candidate partial parse
tree.
When the next word ?a? is scanned, partial parse
trees #6, #7, #8 and #9 are constructed, where
P (#6) = 0.21, P (#7) = 0.14, P (#8) = 0.03 and
P (#9) = 0.02. Sub(#3, I found a) = {#6,#7}.
Therefore, V alidity(#3, I found a) = (0.21 +
0.14)/(0.21+0.14+0.03+0.02) = 0.875. Because
V alidity(#3, I found a) ? ?, partial parse tree #3
is returned as the output.
Table 3 shows the output partial parse tree for ev-
ery word input.
Our incremental parser delays the decision of the
output as shown in this example.
Table 3: Output partial parse trees
input word output partial parse tree
I #2
found
a #3
dime #10
in #12
the
wood #18
5 Experimental Results
To evaluate the performance of our proposed
method, we performed a parsing experiment. The
parser was implemented in GNU Common Lisp on a
Linux PC. In the experiment, the inputs of the incre-
mental parser are POS sequences rather than word
sequences. We used 47247 initial trees and 2931
auxiliary trees for the experiment. The elementary
trees were extracted from the parse trees in sec-
tions 02-21 of the Wall Street Journal in Penn Tree-
bank (Marcus et al, 1993), which is transformed
by using parent-child annotation and left factoring
(Roark and Johnson, 1999). We set the beam-width
at 500.
The labeled precision and recall of the parsing
are 80.8% and 78.5%, respectively for the section
23 in Penn Treebank. We used the set of sentences
for which the outputs of the incremental parser are
identical to the correct parse trees in the Penn Tree-
bank. The number of these sentences is 451. The
average length of these sentences is 13.5 words.
We measured the delays and the precisions for va-
lidity thresholds 0.5, 0.6, 0.7, 0.8, 0.9 and 1.0.
We define the degree of delay as follows: Let
s = w1 ? ? ?wn be an input sentence and oj(s) be
the partial parse tree that is the output when the j-th
word wj is scanned. We define the degree of delay
when j-th word is scanned as follows:
D(j, s) = j ? l(oj(s)) (7)
We define maximum delay Dmax(s) and average
delay Dave(s) as follows:
Dmax(s) = max1?j?nD(j, s) (8)
Dave(s) = 1n
n?
j=1
D(j, s) (9)
The precision is defined as the percentage of valid
partial parse trees in the output.
Moreover, we measured the precision of the pars-
ing whose delay is always 0 and which returns the
Table 4: Precisions and delays
precision(%) Dmax Dave
? = 1.0 100.0 11.9 6.4
? = 0.9 97.3 7.5 2.9
? = 0.8 95.4 6.4 2.2
? = 0.7 92.5 5.5 1.8
? = 0.6 88.4 4.5 1.3
? = 0.5 83.0 3.4 0.9
baseline 73.6 0.0 0.0
0
2
4
6
8
10
12
14
70 75 80 85 90 95 100
delay(number of words)
precision(%)
Dmax
3
33333
3
Dave
?
?????
?
baseline
2
2
Figure 4: Relation between precision and delay
partial parse tree having highest probability. We call
it the parsing baseline.
Table 4 shows the precisions and delays. Figure
4 illustrates the relation between the precisions and
delays.
The experimental result demonstrates that there
is a precision/delay trade-off. Our proposed method
increases the precision in comparison with the base-
line, while returning the output is delayed. When
? = 1, it is guaranteed that the output partial parse
trees are valid, that is, our method is similar to the
method in the literature (Kato et al, 2000). In com-
parison with this case, our method when ? < 1 dra-
matically decreases the delay.
Although the result does not necessarily demon-
strates that our method is the best one, it achieves
both high-accuracy and short-delay to a certain ex-
tent.
6 Concluding Remarks
In this paper, we have proposed a method of evalu-
ating the validity that a partial parse tree constructed
in incremental parsing becomes valid. The method
is based on probabilistic incremental parsing. When
a word is scanned, the method incrementally calcu-
lates the validity for each partial parse tree and re-
turns the partial parse tree whose validity is greater
than a threshold. Our method delays the decision of
which partial parse tree should be returned.
To evaluate the performance of our method, we
conducted a parsing experiment using the Penn
Treebank. The experimental result shows that our
method improves the accuracy of incremental pars-
ing.
The experiment demonstrated a precision/delay
trade-off. To evaluate overall performance of in-
cremental parsing, we would like to investigate a
single measure into which delay and precision are
combined.
Acknowledgement
This work is partially supported by the Grant-in-Aid
for Scientific Research of the Ministry of Education,
Science, Sports and Culture, Japan (No. 15300044),
and The Tatematsu Foundation.
References
J. Allen, G. Ferguson, and A. Stent. 2001. An Ar-
chitecture for More Realistic Conversational Sys-
tems. In Proceedings of International Confer-
ence of Intelligent User Interfaces, pages 1?8.
J. Carroll and D. Weir. 2003. Encoding Frequency
Information in Stochastic Parsing Models. In
R. Bod, R. Scha, and K. Sima?an, editors, Data-
Oriented Parsing, pages 43?60. CSLI Publica-
tions, Stanford.
J. Chen and K. Vijay-Shanker. 2000. Automated
Extraction of TAGs from the Penn Treebank. In
Proceedings of the 6th International Workshop on
Parsing Technologies, pages 65?76.
D. Chiang. 2003. Statistical Parsing with an Auto-
matically Extracted Tree Adjoining Grammar. In
R. Bod, R. Scha, and K. Sima?an, editors, Data-
Oriented Parsing, pages 299?316. CSLI Publica-
tions, Stanford.
F. Costa, V. Lombardo, P. Frasconi, and Soda G.
2001. Wide Coverage Incremental Parsing by
Learning Attachment Preferences. In Proceed-
ings of the 7th Congress of the Italian Association
for Artificial Intelligence, pages 297?307.
N. J. Haddock. 1987. Incremental Interpretation
and Combinatory Categorial Grammar. In Pro-
ceedings of the 10th International Joint Confer-
ence on Artificial Intelligence, pages 661?663.
Y. Inagaki and S. Matsubara. 1995. Models for In-
cremental Interpretation of Natural Language. In
Proceedings of the 2nd Symposium on Natural
Language Processing, pages 51?60.
A. K. Joshi. 1985. Tree Adjoining Grammar: How
Much Context-Sensitivity is required to provide
reasonable structural descriptions? In D. R.
Dowty, L. Karttunen, and A. Zwicky, editors,
Natural Language Parsing, pages 206?250. Cam-
bridge University Press, Cambridge.
Y. Kato, S. Matsubara, K. Toyama, and Y. Ina-
gaki. 2000. Spoken Language Parsing based on
Incremental Disambiguation. In Proceedings of
the 6th International Conference on Spoken Lan-
guage Processing, volume 2, pages 999?1002.
V. Lombardo and P. Sturt. 1997. Incremental Pro-
cessing and Infinite Local Ambiguity. In Pro-
ceedings of the 19th Annual Conference of the
Cognitive Science Siciety, pages 448?453.
M. P. Marcus, B. Santorini, and M. A.
Marcinkiewicz. 1993. Building a Large Anno-
tated Corpus of English: the Penn Treebank.
Computational Linguistics, 19(2):310?330.
M Marcus. 1980. A Theory of Syntactic Recog-
nition for Natural Language. MIT Press, Cam-
brige, MA.
S. Matsubara, S. Asai, K. Toyama, and Y. Inagaki.
1997. Chart-based Parsing and Transfer in In-
cremental Spoken Language Translation. In Pro-
ceedings of the 4th Natural Language Processing
Pacific Rim Symposium, pages 521?524.
D. Milward and R. Cooper. 1994. Incremental In-
terpretation: Applications, Theory, and Relation-
ship to Dynamic Semantics. In Proceedings of
the 15th International Conference on Computa-
tional Linguistics, pages 748?754.
D. Milward. 1995. Incremental Interpretation of
Categorial Grammar. In Proceedings of the 7th
Conference of European Chapter of the Associ-
ation for Computational Linguistics, pages 119?
126.
B. Roark and M. Johnson. 1999. Efficient Prob-
abilistic Top-down and Left-corner Parsing. In
Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics, pages
421?428.
B. Roark. 2001. Probabilistic Top-Down Parsing
and Language Modeling. Computational Lin-
guistics, 27(2):249?276.
F. Xia. 1999. Extracting Tree Adjoining Gram-
mars from Bracketed corpora. In Proceedings of
the 5th Natural Language Processing Pacific Rim
Symposium, pages 398?403.
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1186?1196, Dublin, Ireland, August 23-29 2014.
Japanese Word Reordering Integrated with Dependency Parsing
Kazushi Yoshida
1,a)
Tomohiro Ohno
2,b)
Yoshihide Kato
3,c)
Shigeki Matsubara
1,d)
1
Graduate School of Information Science, Nagoya University, Japan
2
Information Technology Center, Nagoya University, Japan
3
Information & Communications, Nagoya University, Japan
a)
yoshida@db.ss.is.nagoya-u.ac.jp
b)
ohno@nagoya-u.jp
c)
yoshihide@icts.nagoya-u.ac.jp
d)
matubara@nagoya-u.jp
Abstract
Although Japanese has relatively free word order, Japanese word order is not completely arbitrary
and has some sort of preference. Since such preference is incompletely understood, even native
Japanese writers often write Japanese sentences which are grammatically well-formed but not
easy to read. This paper proposes a method for reordering words in a Japanese sentence so
that the sentence becomes more readable. Our method can identify more suitable word order
than conventional word reordering methods by concurrently performing dependency parsing and
word reordering instead of sequentially performing the two processing steps. As the result of an
experiment on word reordering using newspaper articles, we confirmed the effectiveness of our
method.
1 Introduction
Japanese has relatively free word order, and thus Japanese sentences which make sense can be written
without having a strong awareness of word order. However, Japanese word order is not completely
arbitrary and has some sort of preference. Since such preference is incompletely understood, even native
Japanese writers often write Japanese sentences which are grammatically well-formed but not easy to
read. The word reordering of such sentences enables the readability to be improved.
There have been proposed some methods for reordering words in a Japanese sentence so that the
sentence becomes easier to read (Uchimoto et al., 2000; Yokobayashi et al., 2004). In addition, there
exist a lot of researches for estimating appropriate word order in various languages (Filippova and Strube,
2007; Harbusch et al., 2006; Kruijff et al., 2001; Ringger et al., 2004; Shaw and Hatzivassiloglou, 1999).
Although most of these previous researches used syntactic information, the sentences they used there
were what had been previously parsed. It is a problem that word reordering suffers the influence of
parsing errors. Furthermore, as the related works, there are various researches on word reordering for
improving the performance of statistical machine translation (Goto et al., 2012; Elming, 2008; Ge, 2010;
Christoph and Hermann, 2003; Nizar, 2007). These researches consider information as to both a source
language and a target language to handle word order differences between them. Therefore, their problem
setting is different from that for improving the readability of a single language.
This paper proposes a method for reordering words in a Japanese sentence so that the sentence becomes
easier to read for revision support. Our proposed method concurrently performs dependency parsing
and word reordering for an input sentence of which the dependency structure is still unknown. Our
method can identify more suitable word order than conventional word reordering methods because it
can concurrently consider the preference of both word order and dependency. An experiment using
newspaper articles showed the effectiveness of our method.
2 Word Order and Dependency in Japanese Sentences
There have been a lot of researches on Japanese word order in linguistics (for example, Nihongo Kijutsu
Bunpo Kenkyukai, 2009; Saeki, 1998), which have marshalled fundamental contributing factors which
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1186
naganen(for years)
sekaiju-no(all over the world)
hitobito-ga(people)
torikun-de-ki-ta(have tackled)
kare-ga(He)
mondai-wo(theproblem)
tsuini(finally)
S1 (inappropriate word order)
kaiketsu-shi-ta(resolved)
?He finally resolved the problem that people all over the world have tackled for years.?
S2 (appropriate word order)
naganen(for years)
sekaiju-no(all over the world)
hitobito-ga(people)
torikun-de-ki-ta(have tackled)
kare-ga(He)
mondai-wo(theproblem)
tsuini(finally) kaiketsu-shi-ta(resolved)
Figure 1: Example of inappropriate/appropriate word order
decide the appropriate word order in detail. In a Japanese sentence, a predicate of the main clause is
fundamentally placed in last position, and thus, case elements, adverbial elements, or subordinate clauses
are located before it. In addition, case elements are basically placed in the order of a nominative, a dative
and an accusative. However, the basic order of case elements is often changed by being influenced from
grammatical and discourse factors. For example, it is pointed out that a long case element has strong
preference to be located at the beginning of a sentence even if the element is not nominative, as shown
in Figure 1.
In Figure 1, a box and an arrow express a bunsetsu
1
and a dependency relation respectively. Both the
sentences S1 and S2 have the same meaning which is translated as ?He finally resolved the problem that
people all over the world have tackled for years? in English. The difference between S1 and S2 is just in
their word orders in Japanese.
The word order of S1 is more difficult to read than that of S2 because the distance between the bun-
setsu ?kare-ga (He)? and its modified bunsetsu ?kaiketsu-shi-ta (resolved)? is large and thus the loads on
working memory become large. This example suggests that if the dependency structure of S1 is iden-
tified, that information is useful to reorder the word order of S1 to that of S2 so that it becomes easier
to read. In fact, most of the conventional word reordering methods have reordered words using the pre-
viously parsed dependency structure. However, the word order of S1 is thought to be more difficult to
parse than that of S2 because dependency parsers are usually trained on syntactically annotated corpora
in which sentences have the appropriate word order such as that in S2. This is why it is highly possible
that dependency parsing can achieve a higher accuracy by changing the word order of S1 to that of S2 in
advance.
The above observations indicate that word reordering and dependency parsing depend on each other.
Therefore, we consider it is more desirable to concurrently perform the two processings than to sequen-
tially perform them.
3 Word Reordering Method
In our method, a sentence, on which morphological analysis and bunsetsu segmentation have been per-
formed, is considered as the input
2
. We assume that the input sentence might have unsuitable word order,
1
Bunsetsu is a linguistic unit in Japanese that roughly corresponds to a basic phrase in English. A bunsetsu consists of one
independent word and zero or more ancillary words. A dependency relation in Japanese is a modification relation in which a
modifier bunsetsu depends on a modified bunsetsu. That is, the modifier bunsetsu and the modified bunsetsu work as modifier
and modifyee, respectively.
2
In order to focus attention on the comparison between our method and the conventional method, we assumed the input on
which the lower layer processings than dependency parsing have been performed. Even if morphological analysis and bunsetsu
segmentation are automatically performed on input sentences which have unsuitable word order, we can expect the accuracies
1187
which is not easy to read but grammatically well-formed. Our method identifies the suitable word order
which is easy to read by concurrently performing dependency parsing.
The simultaneous performing of dependency parsing and word reordering is realized by searching for
the maximum-likelihood pattern of word order and dependency structure for the input sentence. Note our
method reorders bunsetsus in a sentence without paraphrasing and does not reorder morphemes within a
bunsetsu.
3.1 Probabilistic Model for Word Reordering
When a sequence of bunsetsus in an input sentence B = b
1
? ? ?b
n
is provided, our method identifies the
structure S which maximizes P (S|B). The structure S is defined as a tuple S = ?O,D? where O =
{o
1,2
, o
1,3
, ? ? ? , o
1,n
, ? ? ? , o
i,j
, ? ? ? , o
n?2,n?1
, o
n?2,n
, o
n?1,n
} is the word order pattern after reordering
and D = {d
1
, ? ? ? , d
n?1
} is dependency structure. Here, o
i,j
(1 ? i < j ? n) expresses the order
between b
i
and b
j
after reordering. o
i,j
is 1 if b
i
is located before b
j
, and is 0 otherwise. In addition, d
i
expresses the dependency relation whose modifier bunsetsu is b
i
.
P (S|B) for a S = ?O,D? is calculated as follows.
P (S|B) = P (O,D|B)
=
?
P (O|B) ? P (D|O,B) ? P (D|B) ? P (O|D,B) (1)
Formula (1) is obtained for the product of the following two formulas. According to the probability
theory, the calculated result of Formula (1) is equal to those of Formulas (2) and (3). However, in practice,
since each factor in the formulas is estimated based on the corpus used for training, the calculated results
of these formulas are different from each other. We use Formula (1) to estimate P (S|B) by using both
values of P (D|O,B) and P (O|D,B). In fact, we pre-experimentally confirmed that the calculated
result of Formula (1) was better than those of the others.
P (O,D|B) = P (O|B) ? P (D|O,B) (2)
P (O,D|B) = P (D|B) ? P (O|D,B) (3)
Assuming that order o
i,j
between two bunsetsus is independent of that between other two bunsetsus
and that each dependency relation d
i
is independent of the others, each factor in Formula (1) can be
approximated as follows:
P (O|B)
?
=
n?1
?
i=1
n
?
j=i+1
P (o
i,j
|B) (4)
P (D|O,B)
?
=
n?1
?
i=1
P (d
i
|O,B) (5)
P (D|B)
?
=
n?1
?
i=1
P (d
i
|B) (6)
P (O|D,B)
?
=
n?1
?
i=1
n
?
j=i+1
P (o
i,j
|D,B) (7)
where P (o
i,j
|B) is the probability that the order between b
i
and b
j
is o
i,j
whenB is provided, P (d
i
|O,B)
is the probability that the dependency relation whose modifier bunsetsu is b
i
is d
i
when the sentence
generated by reordering B according to O is provided, P (d
i
|B) is the probability that the dependency
relation whose modifier bunsetsu is b
i
is d
i
when B is provided, and P (o
i,j
|D,B) is the probability
that the order between b
i
and b
j
is o
i,j
when B where the dependency relation is D is provided. These
probabilities are estimated by the maximum entropy method.
remain comparatively high. This is because their processings use mainly local information.
1188
To estimate P (d
i
|O,B), we used the features used in Uchimoto et al. (1999) except when eliminating
features about Japanese commas (called toten, which is a kind of punctuation) and quotation marks.
To estimate P (d
i
|B), we used the features which can be obtained without information about the order
of input bunsetsus among the features used in estimating P (d
i
|O,B). To estimate P (o
i,j
|D,B), if b
i
and b
j
modifies the same bunsetsu, we used the features used in Uchimoto et al. (2000), except when
eliminating features about parallel relations and semantic features. Otherwise, we used the features left
after eliminating features about modified bunsetsus from those used in the above-mentioned case. To
estimate P (o
i,j
|B), we used the features which can be obtained without dependency information among
the features used to estimate P (O
i,j
|D,B).
3.2 Search Algorithm
Since there are a huge number of the structures S = ?O,D? which are theoretically possible for an input
sentence B, an efficient algorithm is desired. However, since O and D are dependent on each other,
it is difficult to find the optimal structure efficiently. In our research, we extend CYK algorithm used
in conventional dependency parsing to efficiently find the suboptimal S = ?O,D? which maximizes
P (S|B) efficiently.
Our research assumes that an input sentence, which is grammatically well-formed, is reordered without
changing the meaning so that the sentence becomes much easier to read. From this assumption, we can
use following conditions for efficient search:
1. The dependency structure of an input sentence should satisfy the following Japanese syntactic con-
straints under the input word order:
? No dependency is directed from right to left.
? Dependencies don?t cross each other.
? Each bunsetsu, except the last one, depends on only one bunsetsu.
2. Even after the words are reordered, the dependency structure should satisfy the above-mentioned
Japanese syntactic constraints under the changed word order.
3. The dependency structures of a sentence before and after reordering should be identical.
Using the condition 1 and the condition 3, we can narrow down the search space of D to dependency
structures that satisfy Japanese syntactic constraints under the input word order. Furthermore, the search
space of O can be narrowed down to the word order patterns derived from the above narrowed depen-
dency structures based on the conditions 2 and 3. That is, after dependency structures possible for an
input sentence are narrowed down, we just have to find the word order patterns after reordering so that
each of the dependency structures is maintained and satisfies the Japanese syntactic constraints even
under the changed word order.
On the other hand, it is well known that CYK algorithm can efficiently find the optimal dependency
structure which satisfies Japanese syntactic constraints. Therefore, in our research, we have extended the
CYK algorithm for the conventional dependency parsing so that it can find the suboptimal D and O from
among the dependency structures and word order patterns which satisfy the conditions 1, 2 and 3.
3.2.1 Word Reordering Algorithm
Algorithm 1 shows our word reordering algorithm. In our algorithm, the n?n triangular matrixM
i,j
(1 ?
i ? j ? n) such as the left-side figure in Figure 2 is prepared for an input sentence consisting of n
numbers of bunsetsus. M
i,j
, the element of the triangular matrix M in the i-th row and j-th column, is
filled by argmax
S
i,j
P (S
i,j
|B
i,j
), which is the maximum-likelihood structure for an input subsequence
B
i,j
= b
i
? ? ? b
j
. In this section, for convenience of explanation, we represent S
i,j
as a sequence of
dependency relations d
x
(i ? x ? j). For example, S
i,j
= d
i
d
i+1
? ? ? d
0
j
means that the first bunsetsu
is b
i
, the second is b
i+1
, ? ? ? , the last is b
j
, and the dependency structure is {d
i
, d
i+1
, ? ? ? , d
j?1
}. Here,
if we need to clearly specify the modified bunsetsu, we represent the dependency relation that bunsetsu
1189
Algorithm 1 word reordering algorithm
1: input B
1,n
= b
1
? ? ? b
n
// input sentence
2: set M
i,j
(1 ? i ? j ? n) // triangular matrix
3: set C
i,j
(1 ? i ? j ? n) // set of structure candidates
4: for i = 1 to n do
5: M
i,i
= d
0
i
6: end for
7: for d = 1 to n? 1 do
8: for i = 1 to n? d do
9: j = i+ d
10: for k = i to j ? 1 do
11: C
i,j
= C
i,j
? ConcatReorder(M
i,k
,M
k+1,j
)
12: end for
13: M
i,j
= argmax
S
i,j
?C
i,j
P (S
i,j
|B
i,j
)
14: end for
15: end for
16: return M
1,n
Candidates generated by 
?By the concatenating process
?By the reordering process
M1,1= M1,2= M1,3= M1,4
M2,2= M2,3= M2,4
M3,3= M3,4=
M4,4=
2 2 3
3 4
4
Candidates generated by 
?By the concatenating process
2 3 4
23 4
? By the reordering process
2 3 4
is filled by the structure which maximizes among the following candidates.
Candidate 1:
Candidate 3:
Candidate 2:
? means that is located  before and depends on . For example, i j
?is filled by the maximum-likelihood 
structure for a subsequence from to .
3
1 1 2 2 31
No candidate is generated because has no child in .2 31 means .
by moving after , which is the first child of in 
Figure 2: Execution example of our search algorithm
b
x
modifies b
y
as d
y
x
. In addition, d
0
j
means that the last bunsetsu of the subsequence don?t modify any
bunsetsu.
First, the statements of the lines 4 to 6 fill each of diagonal elements M
i,i
(1 ? i ? n) with d
0
i
. Next,
the statements of the lines 7 to 15 fill M
i,j
in turn toward the upper right M
1,n
along the diagonal line,
starting from the diagonal elements M
i,i
. The maximum-likelihood structure which should fill an M
i,j
is found as follows:
The statements of the lines 10 to 12 repeat the process of generating candidates of the maximum-
likelihood structure from M
i,k
and M
k+1,j
by the function ConcatReorder, and adding them to the set
of structure candidates C
i,j
. The function ConcatReorder takes two arguments ofM
i,k
andM
k+1,j
and
returns the set of candidates of the maximum-likelihood structure which should fill M
i,j
. The function
ConcatReorder is composed of two processes: concatenating process and reordering process. First,
the concatenating process generates a candidate by simply concatenating M
i,k
and M
k+1,j
in turn about
the word order and connecting M
i,k
and M
k+1,j
by the dependency relation between the last bunsetsus
of them about the dependency structure, without changing the internal structure of each of them. For
example, whenM
i,k
= d
i
d
i+1
? ? ? d
k?1
d
0
k
andM
k+1,j
= d
k+1
d
k+2
? ? ? d
j?1
d
0
j
are given as the argument,
the concatenating process generates ?d
i
d
i+1
? ? ? d
k?1
d
j
k
d
k+1
d
k+2
? ? ? d
j?1
d
0
j
.?
1190
Second, the reordering process generates candidates by reordering words in the candidate generated
by the concatenating process. The reordering is executed on the following conditions. The first condition
is that the dependency structure is maintained and satisfies the Japanese syntactic constraints even under
the changed word order. The second condition is that the order of any two words within each of M
i,k
and M
k+1,j
is maintained. Concretely, the first reordered candidate is generated by moving M
i,k
after
the first (leftmost) child
3
of the last bunsetsu of M
k+1,j
among the children in M
k+1,j
. Then, the sec-
ond reordered candidate is generated by moving M
i,k
after the second child. The reordering process is
continued until the last reordered candidate is generated by moving M
i,k
after the last child. That is, the
number of candidates generated by the reordering process is equal to the number of children of the last
bunsetsu in M
k+1,j
. For example, when M
i,k
= d
i
d
i+1
? ? ? d
k?1
d
0
k
and M
k+1,j
= d
j
k+1
d
j
k+2
? ? ? d
j
j?1
d
0
j
,
which means all bunsetsus except the last one depend on the last one, are given, the reordering
process generates the following j ? k? 1 candidates: ?d
j
k+1
d
i
d
i+1
? ? ? d
k?1
d
j
k
d
j
k+2
d
j
k+3
? ? ? d
j
j?1
d
0
j
,?
?d
j
k+1
d
j
k+2
d
i
d
i+1
? ? ? d
k?1
d
j
k
d
j
k+3
d
j
k+4
? ? ? d
j
j?1
d
0
j
,? . . ., and ?d
j
k+1
d
j
k+2
? ? ? d
j
j?1
d
i
d
i+1
? ? ? d
k?1
d
j
k
d
0
j
.?
Therefore, in this case, the function ConcatReorder finally returns the set of candidates of which size
is j?k, which includes the candidates generated by the reordering process and a candidate generated by
the concatenating process. Next, in the line 13, our algorithm fills in argmax
S
i,j
?C
i,j
P (S
i,j
|B
i,j
) which
is the maximum-likelihood structure for a subsequence B
i,j
on M
i,j
.
Finally, our algorithm outputs M
1,n
as the maximum-likelihood structure of word order and depen-
dency structure for the input sentence.
Note that if the function ConcatReorder is changed to the function Concat in the line 11, our algorithm
becomes the same as CYK algorithm used in the conventional dependency parsing. The functionConcat
takes two arguments of M
i,k
and M
k+1,j
and generates a candidate of the maximum-likelihood structure
which should fill M
i,j
by the same way as the concatenating process in the function ConcatReorder.
Then, the function Concat returns the set which has the generated candidate as a element, of which size
is 1.
3.2.2 Execution Example of Word Reordering Algorithm
Figure 2 represents an example of execution of our word reordering algorithm in n = 4. The left side
of Figure 2 represents the triangle diagram which has 4 ? 4 dimensions. The elements of the triangle
diagram M
1,1
,M
2,2
,M
3,3
,M
4,4
,M
1,2
,M
2,3
,M
3,4
, and M
1,3
have already been filled in turn, and M
2,4
is being filled. The right side of Figure 2 shows the process of calculating the maximum-likelihood
structure which should fill M
2,4
. First, in the loop from the line 10 to the line 12 in Algorithm 1,
two structure candidates are generated by ConcatReorder(M
2,2
,M
3,4
). The candidate 1 is generated
by the concatenating process, that is, by simply concatenating M
2,2
and M
3,4
and connecting the last
bunsetsu of M
2,2
and that of M
3,4
. The candidate 2 is generated by the reordering process, that is, by
moving M
2,2
after b
3
, which is the first child of b
4
in M
3,4
. Second, the candidate 3 is generated by
the concatenating process in ConcatReorder(M
2,3
,M
4,4
). On the other hand, the reordering process in
ConcatReorder(M
2,3
,M
4,4
) generates no candidates because b
4
has no child inM
4,4
. Among the three
structures generated in the above way, the structure which maximizes P (S
2,4
|B) = P (O
2,4
, D
2,4
|B
2,4
)
fills M
2,4
.
4 Experiment
To evaluate the effectiveness of our method, we conducted an experiment on word reordering by using
Japanese newspaper articles.
4.1 Outline of Experiment
In the experiment, as the test data, we used sentences generated by only changing the word order of
newspaper article sentences in Kyoto Text Corpus (Kurohashi and Nagao, 1998), maintaining the depen-
dency structure. That is, we artificially generated sentences which made sense but were not easy to read,
3
When b
i
depends on b
j
, we call b
i
as a child of b
j
. Furthermore, if b
j
has more than or equal to one child, the children are
numbered from left to right based on their positions.
1191
kokkai-wo(the Diet)
toot-ta
(passed)
ato-demo(Even after)
seron-chosa-de-wa(according to opinion polls)
shohi-ze-zoze-ga(the consumption tax hike bill)
zoze-hantai-ga(opposing views to the bill)
Original sentence in newspaper articles (correct word order)
taise-da(are in majority)
?Even after the Diet passed the consumption tax hike bill,according to opinion polls opposing views to the bill are in majority.?
Test data (input sentence)
test data generation
kokkai-wo(the Diet)
toot-ta
(passed)
ato-demo(Even after)
seron-chosa-de-wa(according to opinion polls)
shohi-ze-zoze-ga(the consumption tax hike bill)
zoze-hantai-ga(opposing views to the bill)
taise-da(are in majority)
Figure 3: Example of test data generation
in order to focus solely on problems caused by unsuitable word order. Figure 3 shows an example of the
test data generation. The generation procedure is as follows:
1. Find a bunsetsu modified by multiple bunsetsus from the sentence end.
2. Change randomly the order of the sub-trees which modify such bunsetsu.
3. Iterate 1 and 2 until reaching the beginning of the sentence.
In Figure 3, the bunsetsus ?taise-da (are in the majority)? and ?toot-ta (passed)? are found as bunsetsus
modified by multiple bunsetsus. For example, when ?toot-ta (passed)? is found, the order of ?shohi-
ze-zoze-ga (the consumption tax hike bill)? and ?kokkai-wo (the Diet)? is randomly changed. In this
experiment, all Japanese commas (toten) in a sentence, and sentences which have quotation marks were
removed.
In this way, we artificially generated 865 sentences (7,620 bunsetsus) from newspaper articles of Jan.
9 in Kyoto Text Corpus and used them as the test data. As the training data, we used 7,976 sentences in 7
days? newspaper articles (Jan. 1, 3-8). Here, we used the maximum entropy method tool (Zhang, 2008)
with the default options except ?-i 1000.?
In the evaluation of word reordering, we obtained the following two measurements, which are defined
by Uchimoto et al. (2000):
? complete agreement: the percentage of the sentences in which all words? order completely agrees
with that of the original sentence.
? pair agreement: the percentage of the pairs of bunsetsus whose word order agrees with that in the
original sentence. (For example, in Figure 3, if the word order of the input sentence is not changed
after reordering, the pair agreement is 52.4% (= 11/
7
C
2
) because the 11 pairs out of the
7
C
2
pairs
are the same as those in the original sentence.)
In the evaluation of dependency parsing, we obtained the dependency accuracy (the percentage of
correctly analyzed dependencies out of all dependencies) and sentence accuracy (the percentage of
the sentences in which all the dependencies are analyzed correctly), which are defined by Sekine et al.
(2000).
For comparison, we established two baselines. Both of the baselines execute the dependency pars-
ing primarily, and then, perform the word reordering by using the conventional word reordering method
1192
Table 1: Experimental results (word reordering)
pair agreement complete agreement
our method 77.3% (30,190/38,838) 25.7% (222/865)
baseline 1 75.4% (29,279/38,838)
*
23.8% (206/865)
baseline 2 74.8% (29,067/38,838)
*
23.5% (203/865)
no reordering 61.5% (23,886/38,838)
*
8.0% (69/865)
*
Note that the agreements followed by * differ signifi-
cantly from those of our method (p < 0.05).
Table 2: Experimental results (dependency parsing)
dependency accuracy sentence accuracy
our method 78.4% (5,293/6,755) 35.3% (305/865)
baseline 1 79.2% (5,350/6,755) 31.6% (273/865)
*
baseline 2 81.2% (5,487/6,755)
*
32.1% (278/865)
*
Note that the accuracies followed by * differ sig-
nificantly from those of our method (p < 0.05).
(Uchimoto et al., 1999). The difference between the two is the method of dependency parsing. The
baselines 1 and 2 use the dependency parsing method proposed by Uchimoto et al. (2000) and the de-
pendency parsing tool CaboCha (Kudo and Matsumoto, 2002), respectively. The features used for the
word reordering in both the baselines are the same as those used to estimate P (o
i,j
|D,B) in our method.
Additionally, the features used for the dependency parsing in the baseline 1 are the same as those used to
estimate P (d
i
|O,B) in our method.
4.2 Experimental Results
Table 1 shows the experimental results on word reordering of our method and the baselines. Here, the
last row shows the agreements measured by comparing the input word order with the correct word order.
The agreements mean the values which can be achieved with no reordering
4
. The pair and complete
agreements of our method were highest among all. The pair agreement of our method is significantly
different from those of both the baselines (p < 0.05) although there is no significant difference between
the complete agreements of them.
Next, Table 2 shows the experimental results on dependency parsing. The sentence accuracy of our
method is significantly higher than those of both the baselines (p < 0.05). On the other hand, the
dependency accuracy of our method is significantly lower than that of the baseline 2 although there is no
significant difference between the dependency accuracies of our method and the baseline 1 (p > 0.05).
Here, if the input sentences had the correct word order, the dependency accuracies of the baselines 1 and
2 were 86.4% (5,835/6,755) and 88.1% (5,950/6,755), respectively. We can see that the unsuitable word
order caused a large decrease of the accuracies of the conventional dependency parsing methods. This is
why the word order agreements of the baselines were decreased.
Figure 4 shows an example of sentences of which all bunsetsus were correctly reordered and the de-
pendency structure was correctly parsed only by our method. We can see that our method can achieve
the complicated word reordering. On the other hand, Figure 5 shows an example of sentences incorrectly
reordered and parsed by our method. In this example, our method could not identify the correct modified
bunsetsu and the appropriate position of the bunsetsu ?arikata-wo (whole concept).? This is because the
dependency probability between the bunsetsu ?arikata-wo (whole concept)? and the bunsetsu ?fukume
4
Some input sentences were in complete agreement with the original ordering. There were some cases that the randomly
reordered sentences accidentally have the same word order as the original ones. In addition, there were some sentences in
which all bunsetsus except the last one depend on the next bunsetsu. The word order of such sentences is not changed by the
test data generation procedure because the procedure is executed on condition of maintaining the dependency structure.
1193
Input sentence(inappropriate word order) 
?Although I myself do not have an experience with a war, I think any generation should not glorify war.?
Output sentence(correct word order and dependency structure) itsu-no(any) sedai-mo(generation) bika-su-beki-de-nai-to(should not glorify)
senso-wo(with a war)
senso-wo(war)
taiken-shi-ta(have an  experience)
koto-wa(?)
watashi-jishin(I myself)
omou
(think)
itsu-no(any)
sedai-mo
(generation)
bika-su-beki-de-nai-to(should not glorify)
senso-wo(with a war)
senso-wo(war)
taiken-shi-ta(have an  experience)
koto-wa(?)
watashi-jishin(I myself)
nai-ga
(although do not)
omou
(think)
nai-ga
(although do not)
:  shows an alignment of a bunsetsu before and after reordering.:  shows a correct dependency relation.?  :  means there is no English word corresponding to the Japanese word.
Figure 4: Example of sentences correctly reordered and parsed by our method
?Whole concept of the examination of rice should be fundamentally revised including the transfer of control to a private sector or prefectural and city governments.?
Input sentence(inappropriate word order) 
Output sentence(incorrect word order and dependency structure)
kensa-no(of the exami-nation)
arikata-wo(whole concept)
todofuken-ya(or prefectural and city governments)
minkan-e-no(to a private sector)
kome-no(of rice)
ikan-mo(the transferof control)
fukume
(including)
konpon-teki-ni(fundamen-tally)
minaosu-beki-daro(should be revised)
Original sentence(correct word order and dependency structure)
kensa-no(of the exami-nation)
arikata-wo(whole concept)
todofuken-ya(or prefectural and city governments)
minkan-e-no(to a private sector)
kome-no(of rice)
ikan-mo(the transferof control)
fukume
(including)
konpon-teki-ni(fundamen-tally)
minaosu-beki-daro(should be revised)
kensa-no(of the exami-nation)
arikata-wo(whole concept)
todofuken-ya(or prefectural and city governments)
minkan-e-no(to a private sector)
kome-no(of rice)
ikan-mo(the transferof control)
fukume
(including)
konpon-teki-ni(fundamen-tally)
minaosu-beki-daro(should be revised)
: shows an alignment of a bunsetsu before and after reordering.:  shows a correct dependency relation.:  shows an incorrect dependency relation.
Figure 5: Example of sentences incorrectly reordered and parsed by our method
(including)? is higher than the one between the bunsetsu ?arikata-wo (whole concept)? and the bunsetsu
?minaosu-beki-daro (should be revised)?, and the probability that the bunsetsu ?arikata-wo (whole con-
cept)? is located at the left side of ?fukume (including)? is higher than that of the right side. Since the
word order of the output sentence has a strong probability of causing a wrong interpretation like ?The
transfer of control to a private sector or prefectural and city governments should be fundamentally re-
vised including whole concept of the examination of rice.?, this reordering has a harmful influence on
the comprehension. We need to study techniques for avoiding the word order which causes the change
of meanings in an input sentence.
From the above, we confirmed the effectiveness of our method on word reordering and dependency
parsing of a sentence of which the word order is not easy to read.
5 Conclusion
This paper proposed the method for reordering bunsetsus in a Japanese sentence. Our method can identify
suitable word order by concurrently performing word reordering and dependency parsing. Based on the
1194
idea of limiting the search space using the Japanese syntactic constraints, we made the search algorithm
by extending the CYK algorithm used in the conventional dependency parsing, and found the optimal
structure efficiently. The result of the experiment using newspaper articles showed the effectiveness of
our method.
In our future works, we would like to collect sentences written by Japanese subjects who do not have
much writing skills, to conduct an experiment using those sentences. In addition, we would like to
conduct a subjective evaluation to investigate whether the output sentences are indeed more readable
than the input ones.
Acknowledgments
This research was partially supported by the Grant-in-Aid for Young Scientists (B) (No.25730134) and
Challenging Exploratory Research (No.24650066) of JSPS.
References
Tillmann Christoph and Ney Hermann. 2003. Word reordering and a dynamic programming beam search
algorithm for statistical machine translation. Computational Linguistics, 29(1):97?133.
Jakob Elming. 2008. Syntactic reordering integrated with phrase-based SMT. In Proceedings of the
22nd International Conference on Computational Linguistics (COLING2008), pages 209?216.
Katja Filippova and Michael Strube. 2007. Generating constituent order in German clauses. In Proceed-
ings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL2007), pages
320?327.
Niyu Ge. 2010. A direct syntax-driven reordering model for phrase-based machine translation. In
Proceedings of Human Language Technologies: The 11th Annual Conference of the North American
Chapter of the Association for Computational Linguistics (NAACL-HLT2010), pages 849?857.
Geert-Jan M. Kruijff, Ivana Kruijff-Korbayov?a, John Bateman, and Elke Teich. 2001. Linear order as
higher-level decision: Information structure in strategic and tactical generation. In Proceedings of the
8th European Workshop on Natural Language Generation (ENLG2001), pages 74?83.
Isao Goto, Masao Utiyama, and Eiichiro Sumita. 2012. Post-ordering by parsing for Japanese-English
statistical machine translation. In Proceedings of the 50th Annual Meeting of the Association for
Computational Linguistics (ACL2012), pages 311?316.
Karin Harbusch, Gerard Kempen, Camiel van Breugel, and Ulrich Koch. 2006. A generation-oriented
workbench for performance grammar: Capturing linear order variability in German and Dutch. In
Proceedings of the 4th International Natural Language Generation Conference (INLG2006), pages
9?11.
Nihongo Kijutsu Bunpo Kenkyukai, editor. 2009. Gendai nihongo bunpo 7 (Contemporary Japanese
Grammar 7), pages 165?182. Kuroshio Shuppan. (In Japanese).
Taku Kudo and Yuji Matsumoto. 2002. Japanese dependency analysis using cascaded chunking. In Pro-
ceedings of the 6th Conference on Computational Natural Language Learning (CoNLL2002), pages
63?69.
Sadao Kurohashi and Makoto Nagao. 1998. Building a Japanese parsed corpus while improving the
parsing system. In Proceedings of the 1st International Conference on Language Resources and
Evaluation (LREC ?98), pages 719?724.
Habash Nizar. 2007. Syntactic preprocessing for statistical machine translation. In Proceedings of the
11th Machine Translation Summit (MT SUMMIT XI), pages 215?222.
Eric Ringger, Michael Gamon, Robert C. Moore, David Rojas, Martine Smets, and Simon Corston-
Oliver. 2004. Linguistically informed statistical models of constituent structure for ordering in sen-
tence realization. In Proceedings of the 20th International Conference on Computational Linguis-
tics (COLING2004), pages 673?679.
1195
Tetsuo Saeki. 1998. Yosetsu nihonbun no gojun (Survey: Word Order in Japanese Sentences). Kuroshio
Shuppan. (In Japanese).
Satoshi Sekine, Kiyotaka Uchimoto, and Hitoshi Isahara. 2000. Backward beam search algorithm for de-
pendency analysis of Japanese. In Proceedings of the 18th International Conference on Computational
Linguistics (COLING2000), volume 2, pages 754?760.
James Shaw and Vasileios Hatzivassiloglou. 1999. Ordering among premodifiers. In Proceedings of the
37th Annual Meeting of the Association for Computational Linguistics (ACL ?99), pages 135?143.
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isahara. 1999. Japanese dependency structure analysis
based on maximum entropy models. In Proceedings of the 9th Conference of the European Chapter
of the Association for Computational Linguistics (EACL ?99), pages 196?203.
Kiyotaka Uchimoto, Masaki Murata, Qing Ma, Satoshi Sekine, and Hitoshi Isahara. 2000. Word order
acquisition from corpora. In Proceedings of the 18th International Conference on Computational
Linguistics (COLING2000), volume 2, pages 871?877.
Hiroshi Yokobayashi, Akira Suganuma, and Rin-ichiro Taniguchi. 2004. Generating candidates for
rewriting based on an indicator of complex dependency and it?s application to a writing tool. Journal
of Information Processing Society of Japan, 45(5):1451?1459. (In Japanese).
Le Zhang. 2008. Maximum entropy modeling toolkit for Python and C++. http://homepages.
inf.ed.ac.uk/s0450736/maxent_toolkit.html. [Online; accessed 1-March-2008].
1196
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 892?901,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Automatic Comma Insertion for Japanese Text Generation
Masaki Murata
Graduate School of
Information Science,
Nagoya University, Japan
murata@el.itc.nagoya-u.ac.jp
Tomohiro Ohno
Graduate School of
International Development,
Nagoya University, Japan
ohno@nagoya-u.jp
Shigeki Matsubara
Graduate School of
Information Science,
Nagoya University, Japan
matubara@nagoya-u.jp
Abstract
This paper proposes a method for automat-
ically inserting commas into Japanese texts.
In Japanese sentences, commas play an im-
portant role in explicitly separating the con-
stituents, such as words and phrases, of a sen-
tence. The method can be used as an ele-
mental technology for natural language gen-
eration such as speech recognition and ma-
chine translation, or in writing-support tools
for non-native speakers. We categorized the
usages of commas and investigated the ap-
pearance tendency of each category. In this
method, the positions where commas should
be inserted are decided based on a machine
learning approach. We conducted a comma
insertion experiment using a text corpus and
confirmed the effectiveness of our method.
1 Introduction
In Japanese sentences, commas are inserted to mark
word boundaries that might be otherwise unclear be-
cause Japanese is a non-segmented language. They
are also inserted at sharp semantic boundaries to im-
prove the readability of a sentence. While there is a
tendency about the positions where commas should
be inserted in a Japanese sentence, there is no clear
standard for these positions. Therefore, it is hard
for non-natives of Japanese such as foreign students
to insert commas properly, and the method for au-
tomatic comma insertion is required to support sen-
tence generation by such people. In addition, this
method is expected to be useful for improving read-
ability of texts generated by automatic speech recog-
nition or machine translation.
This paper proposes a method for automatically
inserting commas into Japanese texts. There are
several usages of commas, and the positions to in-
sert commas depend on these usages. Therefore,
we grouped the usages of commas into nine cate-
gories, and investigated the appearance tendency for
each category to find the effective features of ma-
chine learning by using Japanese newspaper arti-
cles. Based on the analysis of comma positions, our
method decides whether or not to insert a comma
at each bunsetsu1 boundary in an input sentence by
machine learning.
We conducted an experiment on comma insertion
using the Kyoto Text Corpus (Kurohashi and Nagao,
1998), and obtained higher recall and precision than
those of the baseline, leading us to confirm the ef-
fectiveness of our method.
This paper is organized as follows: The next sec-
tion presents related works. Section 3 gives prelim-
inary analyses. Section 4 explains how our comma
insertion method works. An experiment and discus-
sions are presented in Sections 5 and 6, respectively.
2 Related Works
There have been many investigations on comma in-
sertion into output texts of speech recognition sys-
tems to improve the readability (Christensen et al,
2001; Kim and Woodland, 2001; Liu et al, 2006;
Shimizu et al, 2008). Their methods insert commas
using pause information of speakers, based on the
idea that a point at which a speaker takes a breath
partly corresponds to a point where a comma is in-
serted. However, since pause information cannot be
obtained from texts, we cannot use this approach be-
cause our targets are written texts.
In addition, there have been some investigations
1Bunsetsu is a linguistic unit in Japanese that roughly corre-
sponds to a basic phrase in English. A bunsetsu consists of one
independent word and zero or more ancillary words.
892
on comma insertion into non-Japanese written texts
(White and Rajkumar, 2008; Guo et al, 2010). In
Japanese, there are several usages of commas, and
some usages are specific to Japanese due to its lin-
guistic nature. Therefore, just adopting the above
mentioned methods, which have been developed
to process non-Japanese texts, is not sufficient to
enable high-quality comma insertion into Japanese
sentences. Development of a method based on the
detailed analysis of Japanese commas is required.
Furthermore, there have been some investiga-
tions on comma insertion into Japanese written texts
(Hayashi, 1992; Suzuki et al, 1995). These investi-
gations have adopted rule-based methods. However,
the number of their rules is not necessarily sufficient,
and no quantitative evaluation has been performed.
3 Analyses on Comma Usages
There have been several discussions on commas,
including the draft of ?Kutou-hou (punctuation)?
made by Archives Division, Minister?s Secretariat,
Japanese Ministry of Education, Science and Cul-
ture in 1906. There are several usages of commas,
and depending on the usage, the types of positions
where commas are inserted are different. First, we
examined some previous publications on commas
(Honda, 1982; Inukai, 2002; Shogakukan?s editior-
ial department, 2007). Based on the results of the ex-
amination, we classified the usages of commas into
nine categories shown in Table 1. Here, commas
in Japanese sentences and commas in English sen-
tences have some common roles. In Japanese sen-
tences, some commas have the same roles as com-
mas in English sentences, but some commas have
roles specific to Japanese due to its linguistic nature
such as ?Japanese is a non-segmented language? or
?Japanese has kanji characters and katakana charac-
ters.?
In our study, positions where a comma should
be inserted are detected by using machine learning.
We investigated the Kyoto Text Corpus version 4.0
(Kurohashi and Nagao, 1998) to find the effective
features. The Kyoto Text Corpus is a collection of
Japanese articles of Mainichi newspaper. We used
the articles on January 1st and from January 3rd to
11th in 1995 as the analysis data. Table 2 shows
the size of the data. The data had been manually
Table 1: Categorization of usages of commas
# usage of comma
1 commas between clauses
2 commas indicating clear dependency relations
3 commas for avoiding reading mistakes and
reading difficulty
4 commas indicating the subject
5 commas inserted after a conjunction or
adverb at the beginning of a sentence
6 commas inserted between parallel words or
phrases
7 commas inserted after an adverbial phrase to
indicate time
8 commas emphasizing the adjacent word
9 other
Table 2: Size of the analysis data
sentences 11,821
bunsetsus 117,501
characters 503,970
commas 16,595
characters per sentence 42.63
annotated with information on morphological anal-
ysis, bunsetsu segmentation and dependency2 anal-
ysis. Clause boundaries were detected by the clause
boundary detection program CBAP (Kashioka and
Maruyama, 2004).
Out of all the inserted commas, only 1.43%
were inserted at positions which were not bunsetsu
boundaries. Therefore, we analyzed only commas
inserted at bunsetsu boundaries. Of 105,680 bun-
setsu boundaries, commas were inserted at 16,357
bunsetsu boundaries, that is, the rate of comma
insertion was 15.48%. In the following sections,
we focus on morphemes, clause boundaries, depen-
dency relation and the number of characters between
commas, and investigate their relations with com-
mas.
3.1 Commas between Clauses
If a sentence consists of several clauses, inserting
a comma between clauses makes clear the sentence
2A dependency in a Japanese sentence is a modification re-
lation in which a modifier bunsetsu depends on a modified bun-
setsu. That is, the modifier bunsetsu and the modified bunsetsu
work as a modifier and a modifyee, respectively.
893
Table 3: Rates of comma insertion according to the clause
boundary type
type of clause boundary ratio of comma
insertion (%)
topicalized element-wa 16.94 (1,446/8,536)
adnominal clause 0.72 (43/5,960)
continuous clause 84.57 (2,685/3,175)
compound clause-te 23.31 (394/1,690)
quotational clause 4.40 (74/1,680)
supplement clause 17.53 (245/1,398)
discourse marker 60.13 (650/1,081)
compound clause-ga 93.85 (946/1,008)
compound clause-de 84.52 (606/717)
condition clause-to 81.66 (423/518)
structure. Therefore, a clause boundary is consid-
ered to be a strong candidate of a position where a
comma is inserted. For example, in the following
sentence3:
? ??????????????????????
?????????????????????
(Toward lifting the sanctions imposed on Iraq by
United Nations, the aim seems to be to request fur-
ther cooperation from France, which has close ties
to Iraq.)
a comma is inserted at the clause boundary right af-
ter the continuous clause ???????????
??? (Toward lifting the sanctions imposed
on Iraq by United Nations).? Like this example, the
same usage of commas is seen in English as well.
In the analysis data, there existed 29,278 clause
boundaries excluding sentence breaks. Among
them, commas were inserted at 8,805 positions
(30.01%). The rate is higher than that of bunsetsu
boundaries. This indicates that commas tend to be
inserted at clause boundaries.
We investigated the rate of comma insertion about
114 types4 of clause boundaries. Table 3 shows the
top 10 clause boundary types according to the oc-
currence frequency, and the rates of comma inser-
3We underlined commas which we mentioned in the exam-
ple and the corresponding positions in the translation of the ex-
ample.
4In our research, we used the types of clause boundaries de-
fined by the Clause Boundary Annotation Program (Kashioka
and Maruyama, 2004).
!"#$%&'()*+,-./.012345678#9:;1<=>?@ABC6*DE>1FG+HIJKLMBC6N!"#$%&'()*+',+$-.,/0.&$($,.#1.2$/3$1+.$42/*1+$/3$1+.$*/256)$1+.$&12/#4$72.&.#,.$/3$8+'#($#/1$/#59$-2'#4&$+/7.$-:1$(5&/$,(:&.&$6'33',:51$72/-5.0&;<!!"#$%&$" !'%(%#'%#)*+,%-.&/0#
!"#
!"#$%&#'!()*
$%&'()*
+#,&-$&(#!"#$%&#.(!'$%
+,-
/&,!0&1
./.0
2-#312+
12345
1$(!-.
67#
!"#4%2-+
89:
5(&1&-,&
;<=
%!5&
>?@AB5*CD=
-!$#!-)6#/(2-.1
EF+
*2""2,7)$
GHI
5(!/)&01
JKLAB5
,+71&1
Figure 1: Commas making clear dependency relations
tion. In cases of ?continuous clause? and ?com-
pound clause-de,? the rates were higher than 84%.
On the other hand, in cases of ?adnominal clause?
and ?quotational clause,? the rates were lower than
5%. This means that the likelihoods of comma inser-
tion are different according to the clause boundary
type.
3.2 Commas and Dependency Structure
Commas have a role to make dependency relations
clearer. Commas tend to be inserted right after a
bunsetsu that depends on a distant bunsetsu. In Fig-
ure 1, although the bunsetsu ?? ? (in Asia)?
depends on the bunsetsu ?? ? ? (causes),?
if the comma right after the bunsetsu ????? (in
Asia)? is not inserted, the readers might mistakenly
understand that the bunsetsu ????? (in Asia)?
depends on the next bunsetsu ?????? (strong).?
To avoid the mistake, the comma is inserted.
In the analysis data, there existed 66,984 bunset-
sus which depend on the next bunsetsu. Among the
bunsetsu boundaries right after them, 2,302 (3.44%)
were the positions where a comma was inserted. On
the other hand, in the case of a bunsetsu bound-
ary right after a bunsetsu which does not depend on
the next bunsetsu, the rate of comma insertion was
36.32% (14,055/38,696).
In addition, when the modifyee of a bunsetsu is
located outside the clause containing the bunsetsu,
i.e. to the right of the clause end, commas are con-
sidered to be more frequently inserted right after the
bunsetsu because such bunsetsu causes more com-
plex dependency structure. The rate of comma in-
sertion right after such bunsetsu is 54.24%.
894
3.3 Commas for Avoiding Reading Mistakes
and Reading Difficulty
Although, unlike English, Japanese is a non-
segmented language, word boundaries are easy to
detect because Japanese has three types of charac-
ters; hiragana characters, katakana characters, and
kanji characters. However, if the same types of char-
acters appear sequentially, readers may make a read-
ing mistake or feel difficulty in reading them. To
avoid such mistakes and difficulty, there is a usage
of commas specific to Japanese.
In the following example, a comma is inserted
between two sequentially appearing words ??
(burned)? and ?? (ashes)? both of which consist of
only kanji characters.
? ?????????????????????
??????????????????????
??????????(He seemed to acknowledge
that he had carried the corpse of Mr. Kawasaki to an
acquaintance in Hanasaki, Katashina-mura, Tone-
gun, Gunma Prefecture, burned it and abandoned
its ashes in the mountain forest in Katashina-mura.)
The comma was inserted because if there was no
comma, the word boundary would become unclear
and reading difficulty would be caused. Among
2,409 bunsetsu boundaries over which kanji charac-
ters appeared sequentially, commas were inserted at
2,188 (90.83%) bunsetsu boundaries. In the case of
katakana characters, the rate was 97.69% (211/216).
Commas tend to be inserted at most bunsetsu bound-
aries if kanji characters or katakana characters se-
quentially appear over a boundary.
3.4 Commas Indicating the Subject
Commas are considered to be inserted right after a
bunsetsu that represents the subject of a sentence.
For example, in Figure 2, a comma is inserted right
after the bunsetsu ??? (war)? to indicate that the
bunsetsu is the subject of the sentence. Here, we pay
attention to the clause boundary of the type ?topi-
calized element-wa.? The rate that commas were in-
serted at the clause boundaries ?topicalized element-
wa? was 16.94% (1,446/8,536). This rate is almost
the same as that of bunsetsu boundaries. On the
other hand, the commas inserted at the clause bound-
aries ?topicalized element-wa? accounted for 8.84%
(1,446/16,357) of all the inserted commas.
!"#$%&'()*+,-./"01234567-809:;<34=>?@ABC!"#$%&'#()*#+&#,)-.%/+) 0)1#2345#)()6#732%&'#8*%-#)#/393$%4-3&5:#)&/#0)1#+1%$)53/#503#*+93*#8*%-#4%$$;5+%&#35<=>!"#$%&'!"#$%&'()!% ()*("+ *+,,%- -."/./0("+ 01231-(&#%)/2/*(3&/"4 4565%6#0/34#%,%7 7/45/#-!2/- 89:;231-(&#3(**84!("#/49: <=>?@A5%6#!6(*%4/)!!"#$%&$" !'%(%#'%#)*+,%-.&/0#
Figure 2: Comma insertion at the clause boundary ?topi-
calized element-wa?
In the case of the clause boundary ?topicalized
element-wa? right after a bunsetsu which does not
depend on the next bunsetsu (e.g., the bunsetsu ??
? (war)? in Figure 2), the rate of comma inser-
tion was 20.71% (1,426/6,886). The rate is higher
than that of all the clause boundaries ?topicalized
element-wa.? This shows that commas tend to be
especially inserted at the ?topicalized element-wa?
right after bunsetsus which do not depend on the
next bunsetsu.
3.5 Commas after Conjunction or Adverb
Commas tend to be inserted right after a conjunc-
tion or an adverb located at the beginning of a sen-
tence. These commas correspond to English com-
mas which are inserted right after a word such as
?however? and ?furthermore? located at the begin-
ning of a sentence.
? ??????????????????????
(However, I do not feel like agreeing on it.)
In the analysis data, there existed 695 bunset-
sus whose rightmost morpheme is a conjunction
and which are located at the beginning of a sen-
tence. Among them, commas were inserted right
after 498 (71.65%) bunsetsus. In the case of bun-
setsus whose rightmost morpheme is an adverb, the
rate was 30.97% (140/452).
3.6 Commas Inserted between Parallel Words
or Phrases
Commas have a function which makes clear sepa-
ration between parallel words or phrases. The fol-
lowing example shows commas separating parallel
nouns.
895
? ??????????????????????
????????????????????(The
United Nations should play a lot of roles in a broad
range of fields, such as the global environment,
population, and food.)
In this example, commas are inserted to separate
parallel nouns ? ? (environment),? ?? (popula-
tion)? and ? ? (food)?. In English, there are com-
mas which perform the same role. In fact, commas
were inserted between ?environment? and ?popula-
tion? and between ?population? and ?food? in the
translation of the above example. When bunsetsus
whose rightmost morpheme is a noun appear se-
quentially, the rate of comma insertion between such
bunsetsus is 59.39% (3,330/5,607).
Also, commas are inserted to separate parallel
phrases. In the following example,
? ??????????????????????
??????????????????????
????????(The menu is decided by avoid-
ing the menu the Prime Minister ate on the previous
night, and by considering the balance between the
Japanese food and the European food.)
a comma is inserted right after the bunsetsu ???
? (avoiding)? to make clear separation between the
parallel phrases ?????????? (by avoid-
ing the menu)? and ??????????????
?? (by considering the balance between the
Japanese food and the European food).? The rate
of comma insertion between two parallel phrases is
79.89% (751/940). This is much higher than that of
bunsetsu boundaries, indicating that commas tend to
be inserted when phrases are paralleled.
3.7 Number of Characters between Commas
If there are too many commas at a short distance,
the sentence becomes hard to read. Therefore, the
number of characters between commas is expected
to be not too small. Also, because a long sequence
of characters without a comma is generated if the
distance between commas is very long, the occur-
rence frequency of such sequences of characters is
considered to be low.
We investigated the number of characters between
commas and its occurrence frequency. Figure 3
!
"!!
#!!
$!!
%!!
&!!
'!!
(!!
)!!
" $ & ( * "" "$ "& "( "* #" #$ #& #( #* $" $$ $& $( $* %" %$
!"
"#
$%
&"
%'
($
%)
#%
&"
*
!"#$%&'()'*+,&,*-%&.
Figure 3: Number of characters between commas and its
occurrence frequency
shows the results of investigation. When the num-
ber of characters between commas is either large or
small, the occurrence frequency is low.
4 Comma Insertion Method
In our method, a sentence, on which morphologi-
cal analysis, bunsetsu segmentation, clause bound-
ary analysis and dependency analysis have been per-
formed, is considered the input. Our method de-
cides whether or not to insert a comma at each bun-
setsu boundary in an input sentence. Based on the
analysis results in Section 3, our method adopts the
bunsetsu boundaries as candidate positions where a
comma is inserted. Our method identifies the most
appropriate combination among all combinations of
positions where a comma can be inserted, by using
the probabilistic model. In this paper, input sen-
tences which consist of n bunsetsus are represented
by B = b1 ? ? ? bn, and the results of comma inser-
tion by R = r1 ? ? ? rn. Here, ri is 1 if a comma
is inserted right after bunsetsu bi, and 0 otherwise.
We indicate the j-th sequence of bunsetsus created
by dividing an input sentence into m sequences as
Lj = b
j
1 ? ? ? b
j
nj (1 ? j ? m), and then, r
j
k = 0 if
1 ? k < nj , and rjk = 1 if k = nj .
4.1 Probabilistic Model for Comma Insertion
When an input sentence B is provided, our method
identifies the comma insertion R that maximizes
the conditional probability P (R|B). Assuming that
whether or not to insert a comma right after a bun-
setsu is independent of other commas except the
896
Table 4: Features used for the maximum entropy method
morphological the rightmost independent morpheme, i.e. head word, (part-of-speech and inflected form) and
information rightmost morpheme (part-of-speech) of a bunsetsu bjk
the rightmost morpheme (a surface form) of bjk if the rightmost morpheme is a particle
the first morpheme (part-of-speech) of bjk+1
commas inserted whether or not a clause boundary exists right after bjk
between clauses type of a clause boundary right after bjk if there exists a clause boundary
commas indicating whether or not bjk depends on the next bunsetsu
clear dependency whether or not bjk depends on a bunsetsu located after the final bunsetsu of the clause including
relations the next bunsetsu of bjk
whether or not bjk is depended on by the bunsetsu located right before it
whether or not the dependency structure of a sequence of bunsetsus between bjk and b
j
1 is closed
commas avoiding whether or not both the rightmost morpheme of bjk and first morpheme of b
j
k+1 are kanji
reading mistakes and characters
reading difficulty whether or not both the rightmost morpheme of bjk and first morpheme of b
j
k+1 are katakana
characters
commas indicating whether or not there exists a clause boundary ?topicalized element-wa? right after bjk and b
j
k
the subject depends on the next bunsetsu
whether or not there exists a clause boundary ?topicalized element-wa? right after bjk and the
string of characters right before bjk is ? ? (dewa)?
the number of characters in a phrase indicating the subject5 if there exists a clause boundary
?topicalized element-wa? right after bjk
whether or not a clause boundary ?topicalized element-wa? exists right after bjk and a bunsetsu
whose rightmost morpheme is a verb depends on the modified bunsetsu of bjk
commas inserted whether or not bjk appears at the beginning of a sentence and its rightmost morpheme is a
after a conjunction conjunction
or adverb at the be-
ginning of a sentence
whether or not bjk appears at the beginning of a sentence and its rightmost morpheme is an
adverb
commas inserted whether or not both the rightmost morphemes of bjk and b
j
k+1 are nouns
between parallel whether or not a predicate at the sentence end is depended on by bjk whose rightmost
words or phrases independent morpheme is a verb and by any of the bunsetsus which are located after bjk and of
which the rightmost independent morpheme is a verb
number of characters one of the following 4 categories if the number of characters from bj1 to bjk is found there
from bj1 to bjk ([num = 1], [2 ? num ? 3], [4 ? num ? 21], [22 ? num])
one appearing immediately before that bunsetsu,
P (R|B) can be calculated as follows:
P (R|B) (1)
=P (r11 = 0, ? ? ? , r
1
n1?1 = 0, r
1
n1 = 1, ? ? ? ,
rm1 = 0, ? ? ? , r
m
nm?1 = 0, r
m
nm = 1|B)
?=P (r11 = 0|B)? ? ? ?
?P (r1n1?1 = 0|r
1
n1?2 = 0, ? ? ? , r
1
1 = 0, B)
?P (r1n1 = 1|r
1
n1?1 = 0, ? ? ? , r
1
1 = 0, B)? ? ? ?
?P (rm1 = 0|r
m?1
nm?1 = 1, B)? ? ? ?
?P (rmnm?1 = 0|r
m
nm?2= 0,? ? ?, r
m
1 = 0, r
m?1
nm?1= 1, B)
?P (rmnm = 1|r
m
nm?1 = 0, ? ? ? , r
m
1 = 0, r
m?1
nm?1 = 1, B)
where P (rjk = 1|r
j
k?1 = 0, ? ? ? , r
j
1 = 0, r
j?1
nj?1 =
1, B) is the probability that a comma is inserted right
after a bunsetsu bjk when the sequence of bunset-
sus B is provided and the position of j-th comma is
identified. Similarly, P (rjk = 0|r
j
k?1 = 0, ? ? ? , r
j
1 =
0, rj?1nj?1 = 1, B) is the probability that a comma
is not inserted right after a bunsetsu bjk. These
probabilities are estimated by the maximum entropy
method. The result R which maximizes the condi-
tional probability P (R|B) is regarded as the most
appropriate result of comma insertion, and calcu-
lated by dynamic programming.
897
4.2 Features on Maximum Entropy Method
To estimate P (rjk = 1|r
j
k?1 = 0, ? ? ? , r
j
1 =
0, rj?1nj?1 = 1, B) and P (r
j
k = 0|r
j
k?1 = 0, ? ? ? , r
j
1 =
0, rj?1nj?1 = 1, B) by the maximum entropy method,
we used the features in Table 4 based on the analysis
described in Section 3.
5 Experiment
To evaluate the effectiveness of our method, we con-
ducted an experiment using a Japanese text corpus.
5.1 Outline of Experiment
As the experimental data, we used the newspaper ar-
ticles in the Kyoto Text Corpus version 4.0 (Kuro-
hashi and Nagao, 1998). We used the articles from
January 14th to 17th as the test data. The training
data is same as the analysis data. Table 5 shows the
size of the test data. Here, we used the maximum
entropy method tool (Le, 2008) with the default op-
tions except ?-i 2000.?
In the evaluation, we obtained the recall, the pre-
cision and their harmonic mean, i.e., F-measure.
The recall and precision are respectively defined as
follows.
recall=
# of correctly inserted commas
# of commas in the correct data
precision=
# of correctly inserted commas
# of inserted commas
In our research, to realize automatic comma in-
sertion with high quality, we analyzed each usage of
commas and decided the features for the ME method
based on the analysis. To confirm the effectiveness
of our features, we established the baseline method
as a comparative method whereby commas are in-
serted by the ME method in which only simple mor-
phological information is used. The baseline method
uses the morphological information in Table 4 and
the information of the rightmost morpheme (a sur-
face form) of a bunsetsu as features.
5.2 Experimental Results
Table 6 shows the experimental results of the base-
line and our method. The recall and precision
were 69.13% and 84.13% respectively, and we con-
firmed that our method had higher performance than
Table 5: Size of test data
sentences 4,659
bunsetsus 46,511
characters 198,899
commas 6,549
characters per sentence 42.69
Table 6: Experimental results
recall precision F-measure
our 69.13% 84.13% 75.90
method (4,527/6,549) (4,527/5,381)
baseline 51.38% 70.90% 59.58
(3,365/6,549) (3,365/4,746)
the baseline method. The percentage of sentences
wherein all commas were correctly inserted was
55.81%.
Figure 4 shows the comparison between the re-
sults of our method and the baseline method. The
baseline method was not able to insert commas right
after the bunsetsu ???????? (are floated)? or
?? ? ?? (not decided)? but inserted com-
mas at unnatural positions such as between ? ??
(calling himself)? and ?????? (the vice com-
mander).? On the other hand, our method was able
to insert commas properly at such bunsetsu bound-
aries.
6 Discussion
6.1 Error Analysis
Among positions where commas existed in the test
data, there existed 2,022 positions where our method
did not insert commas. Among them, 862 were
clause boundaries, and the clause boundary ?topical-
ized element-wa? accounted for 53.36% (460/862)
of them. There were a lot of clause boundaries of
the type ?topicalized element-wa,? and the number
of commas inserted at such boundaries was large.
But, the rate of comma insertion itself was not very
high. We can say that the four features about ?topi-
calized element-wa? did not always work well. Ta-
5Phrases indicating the subject is a sequence of bunsetsus
consisting of bjk and all bunsetsus that are connected to b
j
k when
we trace their dependency relationship in modifier-to-modifyee
direction.
898
!"#$%&'()*+,-./0-123456789:;<0=>?@ABCD=EFGHIJKLMGNBOPQRSTUO=BV$WUXYGZ[\O]^MWT_`=abcdef
!"#$%#&'()*)+,+-&.)/&0/1%.2&3)*/4&3+$*5/.&(/3,/.+,2-&6/.(%50#
'7+8%5*-&.)/&9+2#,&#4&':%9# 3*.2&*5&;)*9+5/- <%5*# =+.#2+9+-&
.)/&4#,9/,&>+$#,&9*5*(./,&+50&.)/&7,*./,&6+*3)* ;+8+*2+ +,/&
4>#+./0&+(&.)/&3+50*0+./-)#7/?/,-&(*53/&.)/&4,+9/7#,8&#4&
.)/&1,/0*3+./0&1#>*.*3+>&1+,.2&*(&5#.&0/3*0/0- .)/&3##,0*5+.*#5&
9+8/(&>*..>/&)/+07+2@&A
!"#$%&'(!)*
!"#$%&'()*+,-./0-123456789:;<0>?@ABCD=EFGHIJKLMGNBOPQRSTUOBV$WUXYGZ[\O]^MWT_`abcdef
!"#$%#&'()*)+,+-&.)/&0/1%.2&3)*/4&3+$*5/.&(/3,/.+,2-&6/.(%50#
'7+8%5*-&.)/&9+2#,&#4&':%9# 3*.2&*5&;)*9+5/ <%5*# =+.#2+9+-&
.)/&4#,9/,&>+$#,&9*5*(./,&+50&.)/&7,*./,&6+*3)* ;+8+*2+ +,/&
4>#+./0&+(&.)/&3+50*0+./ )#7/?/,-&(*53/&.)/&4,+9/7#,8&#4&
.)/&1,/0*3+./0&1#>*.*3+>&1+,.2&*(&5#.&0/3*0/0 .)/&3##,0*5+.*#5&
9+8/(&>*..>/&)/+07+2@&A
+,-&./0&*
ghij$NkU/lm-Ono:&pUO=qrGlm-cstf
!B)*>/&.)/&?*3/&3#99+50/, 3+>>*5C&)*9(/>4&D+,3#(&+11/+,(&
*5&1%$>*3-&+5&+3.%+>&3#99+50/,&*(&%53/,.+*5@A
!"#$%&'(!)*
ghij$NkU=/lm-Ono:&pUO=qrGlm-cstf
!B)*>/&.)/&?*3/&3#99+50/,- 3+>>*5C&)*9(/>4&D+,3#(&+11/+,(&*5&
1%$>*3-&+5&+3.%+>&3#99+50/,&*(&%53/,.+*5@A
+,-&./0&*
Figure 4: Comparison of the results of our method and
baseline method
ble 7 shows the results of comma insertion at the
clause boundaries ?topicalized element-wa.? While
there existed 601 commas at such boundaries in the
test data, only 141 commas were inserted correctly.
We need to consider more effective features about
?topicalized element-wa.?
As for other cases, there existed 130 bunsetsu
boundaries between parallel words where commas
were not inserted. One example of such case is
shown below.
? correct data:
? ???? ?????????
????????????????????
? (Put pork backfat, garlic, ginger and shredded
green onion in a bowl, and add red bell peppers for
color.)
Table 7: Result of comma insertion at the clause bound-
aries ?topicalized element-wa.?
recall precision F-measure
23.46% 59.49% 33.65
(141/601) (141/237)
? our method:
??????????????????????
??????????????????????
(Put pork backfat garlic, ginger and shredded green
onion in a bowl, and add red bell peppers for color.)
In the correct data, a comma was inserted between
the bunsetsu ?? (backfat)? and ????? (gar-
lic).?
If a comma should be inserted right after the bun-
setsu ?? ? (backfat),? the number of characters be-
tween commas would become too small to be judged
as appropriate by the proposed method. So, the fea-
ture about the number of characters between com-
mas may have had harmful effects there. On the
other hand, a comma was inserted properly between
the bunsetsu ????? (garlic)? and ? ???
(ginger).? This is because katakana characters ap-
peared sequentially in addition to appearing as par-
allel nouns.
6.2 Unnatural Comma Insertion
When commas are inserted at obviously unnatural
positions, they have a major impact on the under-
standing of a sentence by readers. Here, we inves-
tigated how many commas had been inserted at ob-
viously unnatural positions by our method. For the
article on January 14th (217 sentences, 2,349 bun-
setsus) in the test data, we examined 47 commas in-
serted incorrectly. Three persons decided whether
or not the inserted commas were obviously unnat-
ural through consultations. Concretely, when all of
the three persons felt that an inserted comma would
make readers understand wrongly the meaning of
the sentence, the comma was judged to be obviously
unnatural.
Among 47 commas, 4 commas were judged obvi-
ously unnatural. This result shows that our method
is capable of inserting commas at natural positions
on some level.
899
Table 8: Comparison with human judgement
recall precision F-measure
by human 78.30% 80.58% 79.42
(249/318) (249/309)
our method 71.07% 82.78% 76.48
(226/318) (226/273)
6.3 Comparison with Human Judgement
In our experiment, we evaluated the results of
comma insertion of our method by comparing them
with the correct data. However, the sufficient level
to be reached by automatic comma insertion is un-
certain. Here, we evaluated our method by com-
paring them with the results of comma insertion by
another person. By using the same data as used in
the subsection 6.2, we conducted an experiment on
comma insertion by an annotator who was familiar
with writing Japanese documents. Table 8 shows the
recall, the precision and the F-measure. The second
row shows the results of our method for the same
data. As the F-measure of the annotator was 79.42,
it turned out that comma insertion task was diffi-
cult even for humans. For F-measure, our method
achieved 96.30% (76.48/79.42) of the annotator?s re-
sult. Also, the precision of our method was 82.78%.
Although the comma insertion task is difficult, our
method was able to properly insert commas.
7 Conclusion
This paper proposed a method for inserting commas
into Japanese texts. Our method appropriately in-
serts commas based on the machine learning method
using such features as morphemes, dependencies
and clause boundaries. An experiment by using the
Kyoto Text Corpus (Kurohashi and Nagao, 1998)
showed an F-measure of 75.90, and we confirmed
the effectiveness of our method.
The analysis of the experimental results showed
that our method cannot insert commas of the par-
ticular usage. As a future work, it is necessary to
find more useful features for commas of this usage
and improve the recall of our method. Also, we will
examine ?commas emphasizing the adjacent word?
which were not included in our targets.
Acknowledgments
This research was partially supported by the Grant-
in-Aids for Scientific Research (B) (No. 22300051)
and Young Scientists (B) (No. 21700157), and by
the Continuation Grants for Young Researchers of
The Asahi Glass Foundation.
References
Heidi Christensen, Yoshihiko Gotoh, and Steve Renals.
2001. Punctuation annotation using statistical prosody
models. In Proceedings of ISCA Workshop on Prosody
in Speech Recognition and Understanding, pages 35?
40.
Yuqing Guo, Haifeng Wang, and Josef van Genabith.
2010. A linguistically inspired statistical model for
Chinese punctuation generation. ACM Transactions
on Asian Language Information Processing, 9(2):6:1?
6:27.
Yoshihiko Hayashi. 1992. A three-level revision model
for improving Japanese bad-styled expressions. In
Proceeding of 14th International Conference on Com-
putational Linguistics, pages 665?671.
Katsuichi Honda. 1982. Nihongo no sakubun gijutsu
(Japanese writing skill). Asahi Shimbun Publications
Inc. (In Japanese).
Takashi Inukai. 2002. Moji hyouki tankyuhou (Method of
questioning characters and notations). Asakura Pul-
ishing Co., Ltd. (In Japanese).
Hideki Kashioka and Takehiko Maruyama. 2004. Seg-
mentation of semantic unit in Japanese monologue.
In Proceedings of International Conference on Speech
Language Technology and Oriental COCOSDA, pages
87?92.
Ji-hwan Kim and P. C. Woodland. 2001. The use of
prosody in a combined system for punctuation gener-
ation and speech recognition. In Proceedings of 7th
European Conference on Speech Communication and
Technology, pages 2757?2760.
Sadao Kurohashi and Makoto Nagao. 1998. Building
a Japanese parsed corpus while improving the parsing
system. In Proceedings of 1st International Confer-
ence on Language Resources and Evaluation, pages
719?724.
Zhang Le. 2008. Maximum entropy modeling toolkit for
python and c++. http://homepages.inf.ed.
ac.uk/s0450736/maxent toolkit.html.
[Online; accessed 1-March-2008].
Yang Liu, Elizabeth Shriberg, Andreas Stolcke, Dustin
Hillard, Mari Ostendorf, and Mary Harper. 2006. En-
riching speech recognition with automatic detection of
900
sentence boundaries and disfluencies. IEEE Trans-
actions on Audio, Speech, and Language Processing,
14(5):1526?1540.
Tohru Shimizu, Satoshi Nakamura, and Tatsuya Kawa-
hara. 2008. Effect of punctuation marks for speech
translation unit boundary detection. IEICE technical
report. Speech, 108(338):127?131. (In Japanese).
Shogakukan?s editiorial department. 2007. kutoten,
kigou, hugou katuyoujiten (dictionary of punctuations
and symbols ). Shogakukan. (In Japanese).
Eiji Suzuki, Shizuo Shimada, Kunio Kondo, and Hisashi
Sato. 1995. Automatic recognition of optimal punc-
tuation in Japanese documents. In Proceedings of
50th National Convention of IPSJ, 50(3):185?186. (In
Japanese).
Michael White and Rajakrishnan Rajkumar. 2008.
A more precise analysis of punctuation for broad-
coverage surface realization with CCG. In Proceed-
ings of Workshop on Grammar Engineering Across
Frameworks, pages 17?24.
901
Proceedings of the ACL 2010 Conference Short Papers, pages 74?79,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Correcting Errors in a Treebank Based on
Synchronous Tree Substitution Grammar
Yoshihide Kato1 and Shigeki Matsubara2
1Information Technology Center, Nagoya University
2Graduate School of Information Science, Nagoya University
Furo-cho, Chikusa-ku, Nagoya, 464-8601 Japan
yosihide@el.itc.nagoya-u.ac.jp
Abstract
This paper proposes a method of correct-
ing annotation errors in a treebank. By us-
ing a synchronous grammar, the method
transforms parse trees containing annota-
tion errors into the ones whose errors are
corrected. The synchronous grammar is
automatically induced from the treebank.
We report an experimental result of apply-
ing our method to the Penn Treebank. The
result demonstrates that our method cor-
rects syntactic annotation errors with high
precision.
1 Introduction
Annotated corpora play an important role in the
fields such as theoretical linguistic researches or
the development of NLP systems. However, they
often contain annotation errors which are caused
by a manual or semi-manual mark-up process.
These errors are problematic for corpus-based re-
searches.
To solve this problem, several error detection
and correction methods have been proposed so far
(Eskin, 2000; Nakagawa and Matsumoto, 2002;
Dickinson and Meurers, 2003a; Dickinson and
Meurers, 2003b; Ule and Simov, 2004; Murata
et al, 2005; Dickinson and Meurers, 2005; Boyd
et al, 2008). These methods detect corpus posi-
tions which are marked up incorrectly, and find
the correct labels (e.g. pos-tags) for those posi-
tions. However, the methods cannot correct errors
in structural annotation. This means that they are
insufficient to correct annotation errors in a tree-
bank.
This paper proposes a method of correcting er-
rors in structural annotation. Our method is based
on a synchronous grammar formalism, called syn-
chronous tree substitution grammar (STSG) (Eis-
ner, 2003), which defines a tree-to-tree transfor-
mation. By using an STSG, our method trans-
forms parse trees containing errors into the ones
whose errors are corrected. The grammar is au-
tomatically induced from the treebank. To select
STSG rules which are useful for error correction,
we define a score function based on the occurrence
frequencies of the rules. An experimental result
shows that the selected rules archive high preci-
sion.
This paper is organized as follows: Section 2
gives an overview of previous work. Section 3 ex-
plains our method of correcting errors in a tree-
bank. Section 4 reports an experimental result us-
ing the Penn Treebank.
2 Previous Work
This section summarizes previous methods for
correcting errors in corpus annotation and dis-
cusses their problem.
Some research addresses the detection of er-
rors in pos-annotation (Nakagawa andMatsumoto,
2002; Dickinson and Meurers, 2003a), syntactic
annotation (Dickinson and Meurers, 2003b; Ule
and Simov, 2004; Dickinson and Meurers, 2005),
and dependency annotation (Boyd et al, 2008).
These methods only detect corpus positions where
errors occur. It is unclear how we can correct the
errors.
Several methods can correct annotation errors
(Eskin, 2000; Murata et al, 2005). These meth-
ods are to correct tag-annotation errors, that is,
they simply suggest a candidate tag for each po-
sition where an error is detected. The methods
cannot correct syntactic annotation errors, because
syntactic annotation is structural. There is no ap-
proach to correct structural annotation errors.
To clarify the problem, let us consider an exam-
ple. Figure 1 depicts two parse trees annotated ac-
cording to the Penn Treebank annotation 1. The
10 and *T* are null elements.
74
SNP VP .DTThat PRNS, NP VPPRPthey VBPsay SBAR-NONE- S-NONE-0 *T*
, ,,
MDwill VPVBbe ADJPJJgood PPINfor NPNNSbonds
.
SNP VP .DTThat PRNS, NP VPPRPthey VBPsay SBAR-NONE- S-NONE-0 *T*
, ,, MDwill VPVBbe ADJPJJgood PPINfor NPNNSbonds
.
(a) incorrect parse tree
(b) correct parse tree
Figure 1: An example of a treebank error
parse tree (a) contains errors and the parse tree
(b) is the corrected version. In the parse tree (a),
the positions of the two subtrees (, ,) are erro-
neous. To correct the errors, we need to move the
subtrees to the positions which are directly dom-
inated by the node PRN. This example demon-
strates that we need a framework of transforming
tree structures to correct structural annotation er-
rors.
3 Correcting Errors by Using
Synchronous Grammar
To solve the problem described in Section 2, this
section proposes a method of correcting structural
annotation errors by using a synchronous tree sub-
stitution grammar (STSG) (Eisner, 2003). An
STSG defines a tree-to-tree transformation. Our
method induces an STSG which transforms parse
trees containing errors into the ones whose errors
are corrected.
3.1 Synchronous Tree Substitution Grammar
First of all, we describe the STSG formalism. An
STSG defines a set of tree pairs. An STSG can be
treated as a tree transducer which takes a tree as
input and produces a tree as output. Each grammar
rule consists of the following elements:
? a pair of trees called elementary trees
PRNS,1 NP2 VP3 ,4
PRNS,1 NP2 VP3 ,4
source target
Figure 2: An example of an STSG rule
? a one-to-one alignment between nodes in the
elementary trees
For a tree pair ?t, t??, the tree t and t? are
called source and target, respectively. The non-
terminal leaves of elementary trees are called fron-
tier nodes. There exists a one-to-one alignment
between the frontier nodes in t and t?. The rule
means that the structure which matches the source
elementary tree is transformed into the structure
which is represented by the target elementary tree.
Figure 2 shows an example of an STSG rule. The
subscripts indicate the alignment. This rule can
correct the errors in the parse tree (a) depicted in
Figure 1.
An STSG derives tree pairs. Any derivation
process starts with the pair of nodes labeled with
special symbols called start symbols. A derivation
proceeds in the following steps:
1. Choose a pair of frontier nodes ??, ??? for
which there exists an alignment.
2. Choose a rule ?t, t?? s.t. label(?) = root(t)
and label(??) = root(t?) where label(?) is
the label of ? and root(t) is the root label of
t.
3. Substitute t and t? into ? and ??, respectively.
Figure 3 shows a derivation process in an STSG.
In the rest of the paper, we focus on the rules
in which the source elementary tree is not identi-
cal to its target, since such identical rules cannot
contribute to error correction.
3.2 Inducing an STSG for Error Correction
This section describes a method of inducing an
STSG for error correction. The basic idea of
our method is similar to the method presented by
Dickinson andMeurers (2003b). Their method de-
tects errors by seeking word sequences satisfying
the following conditions:
? The word sequence occurs more than once in
the corpus.
75
S SSNP VPPRN . SNP VPPRN .DT DTThat ThatSNP VPPRN . SNP VPPRN .DT DTThat ThatS, NP VP , S, NP VP ,SNP VPPRN . SNP VPPRN .DT DTThat ThatS, NP VP , S, NP VP ,, ,PRPthey PRPthey
(a)(b)
(c)
(d)
Figure 3: A derivation process of tree pairs in an
STSG
? Different syntactic labels are assigned to the
occurrences of the word sequence.
Unlike their method, our method seeks word se-
quences whose occurrences have different partial
parse trees. We call a collection of these word
sequences with partial parse trees pseudo paral-
lel corpus. Moreover, our method extracts STSG
rules which transform the one partial tree into the
other.
3.2.1 Constructing a Pseudo Parallel Corpus
Our method firstly constructs a pseudo parallel
corpus which represents a correspondence be-
tween parse trees containing errors and the ones
whose errors are corrected. The procedure is as
follows: Let T be the set of the parse trees oc-
curring in the corpus. We write Sub(?) for the
set which consists of the partial parse trees in-
cluded in the parse tree ?. A pseudo parallel cor-
pus Para(T ) is constructed as follows:
Para(T ) = {??, ? ?? | ?, ? ? ?
?
??T
Sub(?)
? ? ?= ? ?
? yield(?) = yield(? ?)
? root(?) = root(? ?)}
PRNS,1 NP2 VP4PRP3they VBP5say SBAR6-NONE-7 S8-NONE-90 *T*
, ,10,
PRNS,1 NP2 VP4PRP3they VBP5say SBAR6-NONE-7 S8-NONE-90 *T*
, ,10,
Figure 4: An example of a partial parse tree pair
in a pseudo parallel corpusSNP VP .DTThat PRNS, NP VPPRPthey VBPsay SBAR-NONE- S-NONE-0 *T*
, ,, VBDwill ADJP PPINof NPPRP$his NNSabilities
.JJproud
Figure 5: Another example of a parse tree contain-
ing a word sequence ?, they say ,?
where yield(?) is the word sequence dominated
by ? .
Let us consider an example. If the parse trees
depicted in Figure 1 exist in the treebank T , the
pair of partial parse trees depicted in Figure 4 is
an element of Para(T ). We also obtain this pair
in the case where there exists not the parse tree
(b) depicted in Figure 1 but the parse tree depicted
in Figure 5, which contains the word sequence ?,
they say ,?.
3.2.2 Inducing a Grammar from a Pseudo
Parallel Corpus
Our method induces an STSG from the pseudo
parallel corpus according to the method proposed
by Cohn and Lapata (2009). Cohn and Lapata?s
method can induce an STSG which represents a
correspondence in a parallel corpus. Their method
firstly determine an alignment of nodes between
pairs of trees in the parallel corpus and extracts
STSG rules according to the alignments.
For partial parse trees ? and ? ?, we define a node
alignment C(?, ? ?) as follows:
C(?, ? ?) = {??, ??? | ? ? Node(?)
? ?? ? Node(? ?)
? ? is not the root of ?
76
? ?? is not the root of ? ?
? label(?) = label(??)
? yield(?) = yield(??)}
where Node(?) is the set of the nodes in ? , and
yield(?) is the word sequence dominated by ?.
Figure 4 shows an example of a node alignment.
The subscripts indicate the alignment.
An STSG rule is extracted by deleting nodes in
a partial parse tree pair ??, ? ?? ? Para(T ). The
procedure is as follows:
? For each ??, ??? ? C(?, ? ?), delete the de-
scendants of ? and ??.
For example, the rule shown in Figure 2 is ex-
tracted from the pair shown in Figure 4.
3.3 Rule Selection
Some rules extracted by the procedure in Section
3.2 are not useful for error correction, since the
pseudo parallel corpus contains tree pairs whose
source tree is correct or whose target tree is incor-
rect. The rules which are extracted from such pairs
can be harmful. To select rules which are use-
ful for error correction, we define a score function
which is based on the occurrence frequencies of
elementary trees in the treebank. The score func-
tion is defined as follows:
Score(?t, t??) = f(t
?)
f(t) + f(t?)
where f(?) is the occurrence frequency in the tree-
bank. The score function ranges from 0 to 1. We
assume that the occurrence frequency of an ele-
mentary tree matching incorrect parse trees is very
low. According to this assumption, the score func-
tion Score(?t, t??) is high when the source ele-
mentary tree t matches incorrect parse trees and
the target elementary tree t? matches correct parse
trees. Therefore, STSG rules with high scores are
regarded to be useful for error correction.
4 An Experiment
To evaluate the effectiveness of our method, we
conducted an experiment using the Penn Treebank
(Marcus et al, 1993).
We used 49208 sentences in Wall Street Journal
sections. We induced STSG rules by applying our
method to the corpus. We obtained 8776 rules. We
PRN, SNP ,
PRN SNPNP VP
SNP VP
NPNP NPIN NP
NPNP PPIN NP
(1) (2)
(4)
source target
VP , S ,NP VP(3) PPIN NNSDT PPIN NPDT NNS
Figure 6: Examples of error correction rules in-
duced from the Penn Treebank
measured the precision of the rules. The precision
is defined as follows:
precision = # of the positions where an error is corrected
# of the positions to which some rule is applied
We manually checked whether each rule appli-
cation corrected an error, because the corrected
treebank does not exist2. Furthermore, we only
evaluated the first 100 rules which are ordered by
the score function described in Section 3.3, since
it is time-consuming and expensive to evaluate all
of the rules. These 100 rules were applied at 331
positions. The precision of the rules is 71.9%. For
each rule, we measured the precision of it. 70 rules
achieved 100% precision. These results demon-
strate that our method can correct syntactic anno-
tation errors with high precision. Moreover, 30
rules of the 70 rules transformed bracketed struc-
tures. This fact shows that the treebank contains
structural errors which cannot be dealt with by the
previous methods.
Figure 6 depicts examples of error correction
rules which achieved 100% precision. Rule (1),
(2) and (3) are rules which transform bracketed
structures. Rule (4) simply replaces a node la-
bel. Rule (1) corrects an erroneous position of a
comma (see Figure 7 (a)). Rule (2) deletes a use-
less node NP in a subject position (see Figure 7
(b)). Rule (3) inserts a node NP (see Figure 7 (c)).
Rule (4) replaces a node label NP with the cor-
rect label PP (see Figure 7 (d)). These examples
demonstrate that our method can correct syntactic
annotation errors.
Figure 8 depicts an example where our method
detected an annotation error but could not correct
it. To correct the error, we need to attach the node
2This also means that we cannot measure the recall of the
rules.
77
PRN, SNP ,VPI think
PRN, SNP ,VPI think NP
NP S VPis  one  good  oneall  you  need
NP S VPis  one  good  oneall  you  need
IN PP NNSof DTthe respondents IN
PP
NNSof DTthe respondents
NP
NP NP NP
the  U.S.only  two  or  three  other  major  banks
IN NPin
NP NP PP
the  U.S.only  two  or  three  other  major  banks
IN NPin
(a) (b)
(c)
(d)
Figure 7: Examples of correcting syntactic annotation errors
SPP SBAR,INAt NPNPCD10:33
, SPP SBAR,INAt NPCD10:33 ,when ... when ...
Figure 8: An example where our method detected
an annotation error but could not correct it
SBAR under the node NP. We found that 22 of the
rule applications were of this type.
Figure 9 depicts a false positive example
where our method mistakenly transformed a cor-
rect syntactic structure. The score of the rule
is very high, since the source elementary tree
(TOP (NP NP VP .)) is less frequent. This
example shows that our method has a risk of
changing correct annotations of less frequent syn-
tactic structures.
5 Conclusion
This paper proposes a method of correcting er-
rors in a treebank by using a synchronous tree
substitution grammar. Our method constructs a
pseudo parallel corpus from the treebank and ex-
tracts STSG rules from the parallel corpus. The
experimental result demonstrates that we can ob-
tain error correction rules with high precision.
TOP
NP .VP
based on quotations atfive major banksThe average of interbank offered rates
NP
TOP
NP .VP
based on quotations atfive major banksThe average of interbank offered rates
S
Figure 9: A false positive example where a correct
syntactic structure was mistakenly transformed
In future work, we will explore a method of in-
creasing the recall of error correction by construct-
ing a wide-coverage STSG.
Acknowledgements
This research is partially supported by the Grant-
in-Aid for Scientific Research (B) (No. 22300051)
of JSPS and by the Kayamori Foundation of Infor-
mational Science Advancement.
78
References
Adriane Boyd, Markus Dickinson, and Detmar Meur-
ers. 2008. On detecting errors in dependency tree-
banks. Research on Language and Computation,
6(2):113?137.
Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial
Intelligence Research, 34(1):637?674.
Markus Dickinson and Detmar Meurers. 2003a. De-
tecting errors in part-of-speech annotation. In Pro-
ceedings of the 10th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 107?114.
Markus Dickinson and Detmar Meurers. 2003b. De-
tecting inconsistencies in treebanks. In Proceedings
of the SecondWorkshop on Treebanks and Linguistic
Theories.
Markus Dickinson and W. Detmar Meurers. 2005.
Prune diseased branches to get healthy trees! how
to find erroneous local trees in a treebank and why
it matters. In Proceedings of the 4th Workshop on
Treebanks and Linguistic Theories.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics, Companion Volume, pages
205?208.
Eleazar Eskin. 2000. Detecting errors within a corpus
using anomaly detection. In Proceedings of the 1st
North American chapter of the Association for Com-
putational Linguistics Conference, pages 148?153.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2):310?330.
Masaki Murata, Masao Utiyama, Kiyotaka Uchimoto,
Hitoshi Isahara, and Qing Ma. 2005. Correction of
errors in a verb modality corpus for machine transla-
tion with a machine-learning method. ACM Trans-
actions on Asian Language Information Processing,
4(1):18?37.
Tetsuji Nakagawa and Yuji Matsumoto. 2002. Detect-
ing errors in corpora using support vector machines.
In Proceedings of the 19th Internatinal Conference
on Computatinal Linguistics, pages 709?715.
Tylman Ule and Kiril Simov. 2004. Unexpected pro-
ductions may well be errors. In Proceedings of 4th
International Conference on Language Resources
and Evaluation, pages 1795?1798.
79
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 205?208,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Coherent Back-Channel Feedback Tagging of
In-Car Spoken Dialogue Corpus
Yuki Kamiya
Graduate School of
Information Science,
Nagoya University, Japan
kamiya@el.itc.nagoya-u.ac.jp
Tomohiro Ohno
Graduate School of
International Development,
Nagoya University, Japan
ohno@nagoya-u.jp
Shigeki Matsubara
Graduate School of
Information Science,
Nagoya University, Japan
matubara@nagoya-u.jp
Abstract
This paper describes the design of a back-
channel feedback corpus and its evalua-
tion, aiming at realizing in-car spoken di-
alogue systems with high responsiveness.
We constructed our corpus by annotating
the existing in-car spoken dialogue data
with back-channel feedback timing infor-
mation in an off-line environment. Our
corpus can be practically used in devel-
oping dialogue systems which can pro-
vide verbal back-channel feedbacks. As
the results of our evaluation, we confirmed
that our proposed design enabled the con-
struction of back-channel feedback cor-
pora with high coherency and naturalness.
1 Introduction
In-car spoken dialogue processing is one of the
most prevailing applications of speech technol-
ogy. Until now, to realize the system which can
surely achieve such tasks navigation and informa-
tion retrieval, the development of speech recogni-
tion, speech understanding, dialogue control and
so on has been promoted. Now, it becomes impor-
tant to increase responsiveness of the system not
only for the efficient achievement of the task but
for increasing drivers? comfortableness in a dia-
logue.
One way to increase responsiveness of a sys-
tem is to timely disclose system?s state of under-
standing, by making the system show some kind
of reaction during user?s utterances. In human
dialogues, such disclosure is performed by ac-
tions such as nods, facial expressions, gestures and
back-channel feedbacks. However, since drivers
do not look towards a spoken dialogue system
while driving, the system has to inevitably use
voice responses, that is, back-channel feedbacks.
Furthermore, in the response strategy for realiz-
ing in-car dialogues in which drivers feel com-
fortable, it is necessary for the system to provide
back-channel feedbacks during driver?s utterances
aggressively as well as timely.
This paper describes the design of a back-
channel feedback corpus having coherency (tag-
ging is performed by different annotators equally)
and naturalness, and its evaluation, aiming at re-
alizing in-car spoken dialogue systems with high
responsiveness. Although there have been sev-
eral researches on back-channel feedback timings
(Cathcart et al, 2003; Maynard, 1989; Takeuchi
et al, 2004; Ward and Tsukahara, 2000), in many
of them, back-channel feedback timings in human
dialogues were observed and analyzed by using
a general spoken dialogue corpus. On the other
hand, we constructed our corpus by annotating the
existing in-car spoken dialogue data with back-
channel feedback timing information in an off-line
environment. Our corpus can be practically used
in developing dialogue systems which can provide
back-channel feedbacks.
In our research, the driver utterances (11,181
turns) in the CIAIR in-car spoken dialogue corpus
(Kawaguchi et al, 2005) were used as the existing
data. We created the Web interface for the anno-
tation of back-channel feedbacks and constructed
the corpus including 5,416 back-channel feed-
backs. Experiments have shown that our proposed
corpus design enabled the construction of back-
channel feedback corpora with high coherency and
naturalness.
2 Corpus Design
A back-channel feedback is a sign to inform a
speaker that the listener received the speaker?s ut-
terances. Thus, in an in-car dialogue between a
driver and a system, it is preferable that the sys-
tem provides as many back-channel feedbacks as
possible. However, if back-channel feedbacks are
unnecessarily provided, they can not play the pri-
mary role because the driver wonders if the system
really comprehends the speech.
205
For this reason, the timings at which the sys-
tem provides back-channel feedbacks become im-
portant. Several researches investigated back-
channel feedback timings in human-human dia-
logues (Cathcart et al, 2003; Maynard, 1989;
Takeuchi et al, 2004; Ward and Tsukahara, 2000).
They reported back-channel feedbacks had the fol-
lowing tendencies: ?within or after a pause,? ?after
a conjunction or sentence-final particle,? and ?af-
ter a clause wherein the final pitch descends.?
However, it is difficult to systematize the ap-
propriate timings of back-channel feedbacks since
their detection is intertwined in a complex way
with various acoustic and linguistic factors. Al-
though machine learning using large-scale data
would be a solution to the problem, existing spo-
ken dialogue corpora are not suitable for direct
use as data, because the timings of the back-
channel feedbacks lack coherency due to the in-
fluence of factors such as the psychological state
of a speaker, the environment and so on.
In our research, to create more pragmatic data
in which the above-mentioned problem is solved,
we constructed the back-channel feedback corpus
with coherency. To this end, we established the
following policies for annotation:
? Comprehensive tagging: Back-channel
feedback tags are provided for all timings
which are not unnatural. In human-human
dialogues, there are some cases that even if a
timing is suited for providing a back-channel
feedback, no back-channel feedback is not
provided (Ward and Tsukahara, 2000). On
the other hand, in our corpus, comprehensive
tagging enables coherent tagging.
? Off-line tagging: Annotators tag all tim-
ings at which back-channel feedbacks can be
provided after listening to the target speech
one or more times. Compared with providing
back-channel feedbacks in on-line environ-
ment, the off-line annotation decreases the
chances of tagging wrong positions or failing
in tagging back-channel feedbacks, realizing
coherent tagging.
? Discretization of tagging points: Tagging
is performed for each segment into which
driver?s utterances are divided. In a nor-
mal dialogue, the listener can provide back-
channel feedbacks whenever he/she wants to,
but the inconsistency in the timings to give
such feedbacks becomes larger in exchange
0035 - 03:10:170-03:13:119 F:D:I:D:(F ?) (well?)                   &   to?? (clothes)                  &   fuku-o???????? (I wan to buy, so) &   kai-tai-n-da-kedo??? (somewhere)           &   dok-ka???<H>                  (near here)              &   chikaku-ni<H>0036 - 03:15:132-03:16:623 F:D:I:DI:?? (an inexpensive)     &   yasui?? (shop)                     &   o-mise?????<SB>        (is there)                 &   aru-ka-na<SB>0037 - 03:17:302-03:20:887 F:O:I:AO:?? (here)                      &   kono????? (near)                      &   chikaku-desu-to?????? (ANNEX)               &   anekkusu-to??????? (Nagoya PARCO)  &    nagoya-paruko-ga??????<SB>   (there are)                &   gozai-masu-ga<SB>
Well?, I want to buy clothes, so, is there  an inexpensive shop somewhere near here?
Near here, there are ANNEX and Nagoya PARCO.
driver?s utterance
driver?s turn
operator?s turn
driver?s utterance
operator?s utterance
Figure 1: Sample of transcribed text
for smaller restrictions. The discretization of
tagging points enables not only coherent tag-
ging but also the reduction of tagging cost.
? Elaboration using synthesized sound: An
annotator checks the validity of the anno-
tation by listening to the sounds. In other
words, an annotator elaborates the annotation
by revising it many times by listening to the
automatically created dialogue sound which
includes not only driver?s voices but also
sounds of back-channel feedbacks generated
according to the provided timings. The back-
channel feedbacks had been synthesized by
using a speech synthesizer because our cor-
pus aims to be used for implementing the
systemwhich can provide back-channel feed-
backs.
3 Corpus Construction
We constructed the back-channel feedback corpus
by annotating an in-car speech dialogue corpus.
3.1 CIAIR in-car spoken dialogue corpus
We used the CIAIR in-car spoken dialogue corpus
(Kawaguchi et al, 2005) as the target of annota-
tion. The corpus consists of the speech and tran-
scription data of dialogues between a driver and
an operator about shopping guides, driving direc-
tions, and so on. Figure 1 shows an example of
the transcription. We used only the utterances of
drivers in the corpus. We divided the utterances
into morphemes by using the morphological ana-
lyzer Chasen1. In addition, each morpheme was
provided start and end times estimated by using
the continuous speech recognition system Julius2.
3.2 Tagging of spoken dialogue corpus
We constructed the corpus by providing the back-
channel feedback tags at the proper timings for
the driver?s utterances, according to the design de-
scribed in Section 2.
1http://chasen-legacy.sourceforge.jp
2http://julius.sourceforge.jp
206
sp [short pause](F?) (Well?)? (clothes)? (no translation)sp [short pause]?? (buy)?? (want to)? (no translation)? (no translation)?? (so)?? (somewhere)? (no translation)?? (near hear)? (no translation)sp [short pause]pause [pause]?? (inexpensive)? (no translation)? (shop)?? (is there)? (no translation)?? (no translation)
0.0000.0300.0900.3400.5200.6100.8501.0801.1501.2401.4201.6701.8502.1902.8803.0804.9925.3625.4225.6525.8325.982
0.0300.0900.3400.5200.6100.8501.0801.1501.2401.4201.6701.8502.1902.8803.0804.9925.3625.4225.6525.8325.9826.272
content start time end time
Figure 2: Sample of division of a dialogue turn
into basic segments
For ?comprehensive tagging,? an annotator lis-
tens to each dialogue turn3 from the start and tags
a position where a back-channel feedback can be
provided when the timing is found. Here, the tim-
ing of the last back-channel feedback is also used
for judging whether or not the timing is unnatural.
For ?off-line tagging,? an annotator tags the
transcribed text of each dialogue turn of drivers.
To perform ?discretization of tagging points,? a
dialogue turn is assumed to be a sequence of mor-
phemes or pauses (hereafter, we call them basic
segments), which are continuously arranged on
the time axis, and it is judged whether or not a
back-channel feedback should be provided at each
basic segment. Here, in consideration of the un-
equal pause durations, if the length of a pause is
over 200ms, the pause is divided into the initial
200ms pause and the subsequent pause, each of
which is considered as a basic segment. Figure 2
shows an example of a dialogue turn divided into
basic segments.
Furthermore, for ?elaboration using synthesized
sound,? we prepared the annotation environment
where the dialogue sound including not only
driver?s voice but also back-channel feedbacks
generated according to the provided timings is au-
tomatically created in real time for annotators to
listen to. There are several types of back-channel
feedbacks and in normal conversations, we choose
and use appropriate back-channel feedbacks from
among them according to the scene. In our study,
3A dialogue turn is defined as the interval between the
time at which the driver starts to utter just after the opera-
tor finishes uttering and the time at which the driver finishes
uttering just before the operator starts to utter.
play button
turn ID
driver ID
update button
list of turn IDs
Figure 3: Web interface for tagging
Table 1: Size of back-channel feedback corpus
drivers 346
dialogue turns 11,181
clauses 16,896
bunsetsus4 12,689
morpheme segments 94,030
pause segments 19,142
back-channel feedbacks 5,416
we used the most general form ??? hai (yes)?
for the synthesized speech since our focus was
on the timing of back-channel feedbacks. The
back-channel feedbacks had been created by us-
ing Hitachi?s speech synthesizer ?HitVoice,? and
one feedback was placed 50 milli-seconds after the
start time of a tagged basic segment.
We developed a Web interface for tagging back-
channel feedbacks. Figure 3 shows the Web inter-
face. The interface displays a sequence of basic
segments in a dialogue turn in table format. Anno-
tators perform tagging by checking basic segments
where a back-channel feedback can be provided.
3.3 Size of back-channel feedback corpus
Table 1 shows the size of our corpus constructed
by two trained annotators. The corpus includes
5,416 back-channel feedbacks. This means that a
back-channel feedback is generated at intervals of
about 21 basic segments.
4 Corpus Evaluation
We conducted experiments for evaluating the tag-
ging in the constructed corpus.
4Bunsetsu is a linguistic unit in Japanese that roughly cor-
responds to a basic phrase in English. A bunsetsu consists of
one independent word and zero or more ancillary words.
207
Table 2: Kappa values of the existing corpus
a,c a,d a,b c,d b,c b,d
? 0.536 0.438 0.322 0.311 0.310 0.167
4.1 Coherency of corpus tagging
We conducted an evaluation experiment to con-
firm that the tagging is coherently performed in
the corpus. In the experiment, two different an-
notators performed tagging on the same data, and
then we measured the degree of the agreement be-
tween them. As the indicator, we used Cohen?s
kappa value (Cohen, 1960), calculated as follows:
? = P (O)? P (E)
1? P (E)
where P (O) is the observed agreement between
annotators, and P (E) is the hypothetical proba-
bility of chance agreement. A subject who has
a certain level of knowledge annotated 673 dia-
logue turns. The kappa value was 0.731 (P (O) =
0.975, P (E) = 0.907), and thus we can see the
substantial agreement between annotators.
As the target for comparison, we used the kappa
value in the existing back-channel feedback cor-
pus (Kamiya et al, 2010). The corpus had been
constructed by the way that the recorded driver?s
voice was replayed and 4 subjects independently
produced back-channel feedbacks for the same
sound. This means that the policies for tagging
the existing corpus differ from those of our corpus,
and are ?on-line tagging,? ?tagging on the time
axis? and ?tagging without elaborating.? In the
exisiting corpus, 297 dialogue turns were used as
driver?s sound. Table 2 shows the kappa value be-
tween two among the 4 subjects. The kappa value
of our corpus was higher than that between any
subjects of the existing corpus, substantiating the
high coherency of our corpus.
4.2 Validity of corpus tagging
In our corpus, we discretized the tagging points
to enhance the coherency of tagging. However,
such constraint restricts the points available for
tagging and may make annotators provide tags at
the unnatural timings. Therefore, we conducted
a subjective experiment to evaluate the natural-
ness of the back-channel feedback timings. In
the experiment, one subject listened to the replay
of our back-channel feedback corpus and subjec-
tively judged the naturalness of each timing. The
back-channel feedback sound was generated in the
same way described in Section 3.2.
In the experiment, we used 345 dialogue turns
including 131 back-channel feedbacks. 98.47%
of all the back-channel feedbacks were judged to
be natural. Only 2 back-channel feedbacks were
judged to be unnatural because the intervals be-
tween them and the back-channel feedbacks pro-
vided immediately before them were felt too short.
This showed the validity of our discretization of
tagging points.
5 Conclusion
This paper described the design, construction and
evaluation of the back-channel feedback corpus
which had the coherency of tagged back-channel
feedback timings. We constructed the spoken di-
alogue corpus including 5,416 back-channel feed-
backs in 11,181 dialogue turns. The results of our
evaluation confirmed high coherency and enough
naturalness of our corpus.
In the future, we will use our corpus to see
to what extent the timings of back-channel feed-
backs that have been annotated correlate with the
cues provided by earlier researchers. Then we will
develop a system which can detect back-channel
feedback timings comprehensively.
Acknowledgments: This research was supported
in part by the Grant-in-Aid for Challenging Ex-
ploratory Research (No.21650028) of JSPS.
References
N. Cathcart, J. Carletta, and E. Klein. 2003. A shal-
low model of backchannel continuers in spoken dia-
logue. In Proc. of 10th EACL, pages 51?58.
J. Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Mea-
surement, 20:37?46.
Y. Kamiya, T. Ohno, S. Matsubara, and H. Kashioka.
2010. Construction of back-channel utterance cor-
pus for responsive spoken dialogue system develop-
ment. In Proc. of 7th LREC.
N. Kawaguchi, S. Matsubara, K. Takeda, and
F. Itakura. 2005. CIAIR in-car speech corpus ?
influence of driving status?. IEICE Trans. on Info.
and Sys., E88-D(3):578?582.
S. K. Maynard. 1989. Japanese conversation :
self-contextualization through structure and interac-
tional management. Ablex.
M. Takeuchi, N. Kitaoka, and S. Nakagawa. 2004.
Timing detection for realtime dialog systems using
prosodic and linguistic information. In Proc. of
Speech Prosody 2004, pages 529?532.
N. Ward and W. Tsukahara. 2000. Prosodic features
which cue back-channel responses in English and
Japanese. Journal of Pragmatics, 32:1177?1207.
208
