Vaakkriti: Sanskrit Tokenizer
Aasish Pappu and Ratna Sanyal
Indian Institute of Information Technology, Allahabad (U.P.), India
{ akpappu b03, rsanyal}@iiita.ac.in
Abstract
Machine Translation has evolved tremen-
dously in the recent time and stood as center
of research interest for many computer
scientists. Developing a Machine Transla-
tion system for ancient languages is much
more fascinating and challenging task. A
detailed study of Sanskrit language reveals
that its well-structured and finely orga-
nized grammar has affinity for automated
translation systems. This paper provides
necessary analysis of Sanskrit Grammar in
the perspective of Machine Translation and
also provides one of the possible solution for
Samaas Vigraha(Compound Dissolution).
Keywords: Machine Translation, Sanskrit,
Natural Language Parser, Samaas Vigraha,
Tokenization
1 Introduction
Sanskrit language and its grammar had exterted an
emphatic impact on Computer Science and related
research areas. It has resulted to put in extensive ef-
forts in the field ofMachine Translation(hereafter re-
ferred as MT). MT of Sanskrit is never an easy task,
because of structural vastness of its Grammar. Be-
sides, its strutural vastness Sanskrit Grammar is well
organized and least ambigious compared to other
natural languages, illustrated by the fact of increas-
ing fascination for this ancient Aryan language. Its
grammar possesses well organized rules and meta
rules to infer those rules, thus proving to be a pow-
erful analogy to context free grammar of a computer
language.
Subsequently, it supports the idea of developing a
parser for Sanskrit language, that would be helpful
in developing a full-fledged MT system. As a part of
development of parser, there are other important as-
pects to be taken care off. A morphological analyser
and a tokenizer are two of the important components
that play a vital role in the parser. A morpholog-
ical analyser is used for identification of the base
words from their morphonemes, further to under-
stand the semantics of the original text. A tokenizer
also plays its significant part in a parser, by identi-
fying the group or collection of words, existing as a
single and complex word in a sentence. Later on, it
breaks up the complex word into its constituents in
their appropriate forms. In Sanskrit, mainly we have
two categories of complex words. They are
? Sandhi
? Samaas
1.1 Sandhi and Samaas
Sandhi: When two words combine to produce a new
word whose point of combination is result of anni-
hilation of case-end of former word and case-begin
of latter. In short, the resulted new character that
has been created at the point of combination is ex-
actly equivalent to the sound produced when those
two words are uttered without a pause. The inverse
procedure to Sandhi-formation is known as Sandhi
Wicched.
On the other hand, when two or more words are
combined, based on their semantics then the result-
ing word is known as Samaas or Compound. Unlike
577
Sandhi, the point of combination in Samaas may or
may not be a deformed in the resulting word. The in-
verse procedure of break-up of a Samaas is known as
Samaas Vigraha. Considering the complexity of this
problem, we restricted our focus to Samaas Vigraha
or Compound Dissolution(hereafter Compound Dis-
solution is referred as CD for convenience).
1.2 Organization of the Paper
Initially, we would discuss about the problem of fo-
cus and the main objective of this paper in detail.
Further, a little overview about the Sanskrit grammar
and Knowledge Representation, that are required to
understand the underlying concepts of the system.
Then, we would brief about the existing systems in
this areas and the related areas of interest. Later on,
we would give a detailed description of the architec-
ture of Vaakkriti. We would give a detailed analysis
of the results of our system and finally, throw some
light over our contribution to this research area.We
shall conclude with some of drawbacks of our sys-
tem and the challenges we have faced.
2 The Problem
Semantics being the prime focus, we need to learn
the factors that effect the formation of a compound
from the set of atomic words. The basic problem
is identification of factors, by thorough analysis of
language structure or with the help of a linguist. Es-
pecially various examples of Samaas must be exten-
sively observed. After identification of factors, we
need to find out the appropriate form of Knowledge
Representation for the rule-base. Here, knowledge
being the rules, based on which a particular com-
pound is formed. The importance of CD can be
clearly understood, during the process of tokeniza-
tion. A well-defined set of rules in Sanskrit can
be found in ?Ashtadyayi?, authored by 3rd century
grammarian and linguist Panini. Ashtadyayi con-
tains rules of Grammar in a concise form, distributed
over eight chapters. Our rule-base system would be
based on the work of Kale et. al, that has detailed
description of Paninian Grammar.
3 Sanskrit Grammar
As we have already mentioned that, it is necessary
to know some of the basic concepts of the Sanskrit
grammar. First, we would give some important def-
initions of terms that are frequently used in this pa-
per.
3.1 Important Definitions
3.1.1 Vibhakti(Declension)
Sanskrit is a highly inflected language with three
grammatical genders (masculine, feminine, neuter)
and three numbers (singular, plural, dual). It has
eight cases: nominative, vocative, accusative, instru-
mental, dative, ablative, genitive, and locative.
3.1.2 Dhatupata(Verbal Conjugation)
The verbs tenses (a very inexact application of the
word, since more distinctions than simply tense are
expressed) are organized into four ?systems? (as well
as gerunds and infinitives, and such creatures as in-
tensives or frequentatives, desideratives, causatives,
and benedictives derived from more basic forms)
based on the different stem forms (derived from ver-
bal roots) used in conjugation. There are four tense
systems:
? Present (Present, Imperfect, Imperative, Opta-
tive)
? Perfect
? Aorist
? Future (Future, Conditional)
3.2 Factors that effect
The list of factors that are involved in a rule are
? Part of Speech(hereafter referred as POS)
? List of Words(a token must be among a set of
words to satisfy a rule)
? Case-End
? Case-Begin
? Declension
? Sense(a token with a particular sense is only
qualified)
? Meaning
? Affix
578
? Affix Type(Taddita and Kriti)
? Number(sng, two, mny)(hereafter we refer
number as num)
? Gender(mas, fem, neu)
The list of actions that act as functions in the con-
sequent of a rule are:-
? setDecl(set the declension case for a specified
token)
? addBefore(add a string before a specified to-
ken)
? addAfter(add a string after a specified token)
? setNumber(set the number of a to-
ken(sng,two,mny))
? replace(replace a token with a string related to
it)
3.3 Compounds
Nominal compounds occur with various structures,
however morphologically speaking they are essen-
tially the same. Each noun (or adjective) is in its
(weak) stem form, with only the final element re-
ceiving case inflection. Some examples of nominal
compounds include:
Itaretara
Example: rAml#mZBrtf/`?A,(RAmaLakshmaNaBaratAH)
to rAm, c l#mZ, c Brt, c f/`?,(RAma ca, LakshmaNa
ca, Barata ca)
Rule: ?token POS(token,noun) ? setDecl(token,nom)?
addAfter(token,c)
Samahaara
Example: pAZFpAdO(pANIpAdau)to
pAZF c pAddm^ c(pANI ca pADam)
Rule: ?token,?sense POS(token,noun) ?
SenseOf(token, sense) ? setDecl(token,nom)?
addAfter(token,c)
Dvitiya(Accusative) Tatpurusha
Example: d`,?AtFt,(dukhatItaH)to
d`,?m^ atFt,(dukham atItaH)
Rule: POS(token1,noun) ? WordList(token2,E?t ,
atFt , pEtt , gt , a(y-t , ?A? , aAp? , gmF , b`B`"`)
?setDecl(token1,acc)
Trutiya(Instrumental) Tatpurusha
Example: d`,?AtFt,to
d`,?m^ atFt,
Rule: POS(token1,noun) ? (POS(token2,verb) ?
WordList(token2,p?v? ,sd? `f ,Un))? setDecl(token1,ins)
Chaturthi(Dative) Tatpurusha
Example: y?pdAz(yupadaru)to y?py dAz(yupaya daru)
Rule: POS(token1,noun) ? (Sense(token2,?material?)
? WordList(token2,aT? ,bEl , Eht , s`? ,rE"t))?
setDecl(token1,dat)
Panchami(Ablative) Tatpurusha
Example: cOrBym^(cOrabayam)to cOrAd^ Bym^(cOraad
bayam)
Rule: POS(token1,noun) ? (WordList(token2,
By ,BFt ,BFEt ,BF, ,ap?t ,apoY , m`? ,pEtt ,apv-t))?
setDecl(token1,abl)
Shashti(Genitive) Tatpurusha
Example: rAjp`zq,(rAjapurushaH)to rAj?
p`zq,(rAjangya PurushaH)
Rule: POS(token1,noun) ? (POS(token2,noun)??
POS(token2,verb)?? NumeralType(token2,ordinal)??
SenseOf(token2,?quality?))? setDecl(token1,gen)
Saptami(Locative) Tatpurusha
Example: ngrkAk,(nagarAkAkaH)to ngr? kAk,
iv(nagare kAkaH iva)
Rule: POS(token1,noun)
? (MeaningOf(token2,?crow?)?
SenseOf(token2,?contempt?))? setDecl(token1,loc)?
addAfter(token2, iv)
4 Knowledge Representation
We have already learnt that the process of CD is sup-
ported by a rule-base system. A production system
is a good illustration to understand a rule-base sys-
tem. To represent a complex rule, it would be bet-
ter to use First Order Predicate Logic(FOPL). Un-
der FOPL a rule can be written as of the form P (a)?
Q(a)?Q(b)?R(c) ? Action1(a)?Action2(b)?Action1(c)
where P,Q and R are predicates
a, b and c are constant symbols
Action is a function symbol
The rule-base system of Vaakkriti is de-
veloped considering the factors as pred-
icates and the tokens as constant sym-
bols. A sample rule would look like this
579
POS(tok1, noun) ? (POS(tok2, verb) decl(tok2, acc)) ?
setDecl(token1, acc).
5 Related Work
In the recent times many efforts have been made to
develop various utilities for Sanskrit. The tools de-
veloped includes Sanskrit to other Indian Language
transliteration tools, simple and primitive transla-
tion tools, many grammar analysing tools and many
more learning tools. Some of the important works
includes Anusaraka, a primitive machine translation
tool developed by Akshar et. al. Anusaraka tries
to take advantage of the relative strengths of the
computer and the human reader, where the com-
puter takes the language load and leaves the world
knowledge load on the reader. Besides, these tools,
there are some beautiful theory-based research work
was also done. The concept of Indian Network Lan-
guage(INL) is one of such concepts that was pro-
posed by Anupam et. al. It gives a hypothesis to
consider Sanskrit as INL because of its important
properties like free word order and inherent seman-
tic net structure. There are few other interesting re-
search concepts that have been analysed in the con-
text of Sanskrit language. Rick Braggs et. al have
shown in his article how Knowledge Representation
in the language of Sanskrit is one of those wonderful
concept to show that Semantic Nets. Semantic Nets
are concept respresenting structures, that show how
a concept is related to other concepts semantically, a
semantic net would like in the figure below. Another
beautiful research work was comparison of Paninian
Grammar and Computer language Grammar. Bhate
et al has analysed to show that how well organized
and structured is Sanskrit Grammar and its forgot-
ten valuable contributions to the field of Computer
Science.
6 Architecture
An Itrans standard formatted devanagiri text is given
as input to the system and the output of the system
is the set of tokens produced after CD. The list of
components in the system are listed below:
? Input Processor
? Symbol Table
? Knowledge Base
? Inference Engine
? Database
? Rule-Base Editor
The architecture of Vaakkriti can be seen in the fig-
ure
Figure 1: Architecture of Vaakriti
The algorithm of Vakkriti is given below:- A de-
Algorithm 1 Algorithm of Vaakkriti
1: input? Itrans-Devanagiri Text
2: input? ? breakUp(input)
3: tokenList? tentativeTokenize(input?)
4: tokenInfoList? tokenList
5: for tokeni in tokenInfoList do
6: token(i)? extractInfo(tokeni
7: update token(i) in tokenInfoList
8: end for
9: for each rule(r) in Knowledge-Base(KB) do
10: result? infer(r,tokenInfoList)
11: if result is true then
return r
12: end if
13: end for
tailed description of each component is as follows.
580
6.1 Input Processor
The unstemmed compound taken as input to the sys-
tem is a string in itrans format. First, Input Processor
breaks the itrans string into chunks of characters on
the basis of Devanagiri Character set. The heuristic
for break up procedure is given below:-
The reason behind the breakup procedure is to
ease the process of breaking the string into words
in their tentative forms. If a string is considered as
it is without breakup into devanagiri characters, then
there is a high chance of ambiguity while lookup in
the dictionary. For example:-
Without breakup of input string
aja
ajagaraH-- Found this word
With breakup of string into character sequences
a,ja
a,ja,ga,raH
Later on the chunks of characters are processed as
in the procedure below:-
The words lying in input string are tentatively
guessed by maintaining a stack of character se-
quences, thus checking with the dictionary for the
right word. But, in most of the cases, the word in
the input string do not have an exact match in the
dictionary. This is because of the matra appended to
Case-End of a word. Therefore, we have generated
tokens for each matra and tried to find it in the dic-
tionary. If the word is found, then the word along
with its meaning is stored in the Symbol Table.
6.2 Symbol Table
Now, we shall discuss more about how a Symbol
Table fetches those subtle information of a token.
Symbol table extracts token information in the fol-
lowing manner:-
6.2.1 Part of Speech
Part of Speech is identified with the help of stan-
dard Monier Williams Dictionary, List of Adverbs,
List of Prepositions, List of Numerals.
6.2.2 Sense and Meaning
First, meaning of the token is known from the dic-
tionary and the sense of the token is fetched through
a special kind of procedure. The technique has fol-
lowing steps:-
1. Identify the nouns in the meaning phrase.
2. Find sense for each noun with the help of En-
glish Wordnet.
3. Find a list of ?common? senses for all the
nouns.
4. That list of senses is assumed to the sense of a
token.
6.2.3 Gender and Number
These are fetched from the XML database.
6.3 Knowledge Base
The Knowledge Base(KB) contains facts and rules
that supports the system, for identifying a given in-
put. The KB has been classified well, according to
the Rule Sets. A Rule Set is a set of rules that are
meant for a particular type of compound. Infact, a
new rule set can be created whenever there is a new
part of speech to be dealt with. It has been assumed
that, a rule has clauses(both unit and definite) on an-
tescendent side, whose number is equal to tentative
number of tokens in the input parsed string. On the
other hand, the consequent or conclusion contains
the list of actions that has to be operated over the to-
kens(in the input string) by the system. More about
the rule structure in the next section.
The KB is well integrated with the Rule Base Ed-
itor(RBE) and the Inference Engine. Currently, it
contains limited number of rules this makes the KB
non-monotonic, yet it can be made monotonic, by
addition of new rules.
6.4 Database
There is a large database that supports the whole sys-
tem of Vaakriti. The database is contained in the
form of XML files. There are following tables in the
database:-
? Nouns, Adjectives, Numerals Declensions.
? Adverbs, Conjunctions and Prepositions.
? Dictionary Database.
? Preverbs database.
? Other Morphonemes.
581
6.5 Inference Engine
Whenever premises of a particular are satisified by
the input parse string, then it is said that a rule is
fired. A fired rule applies its consequent part over
the parsed string to result in actual goal. This proce-
dure is known as Rule Inference.
6.6 Rule Base Editor
The sole motive of Rule-Base Editor is to free the
Knowledge Engineer free from rule entry. A Lin-
guist with little training to operate the GUI can be
provided, would suffice this task.
7 Results
The system has been tested with many examples that
have been taken from the book written by Kale et al
The set of examples have been chosen from differ-
ent set of Compounds. In most of the cases system
has given correct results with a precision of 90%,
but in some of the cases that involve sense, it be-
came quite difficult to produce the result. Lack of
linguistic tools like Wordnet for Sanskrit language
imposes limitations on word sense disambiguation.
We have developed a sense list for a limited set of
words by observing some of the important sanskrit
texts, based on the knowledge we have acquired.
8 Our Contribution
We have proposed a utility called Rule-Base Editor,
besides our actual work on CD. The motive behind
Rule-Base Editor is to induce the property of flexi-
bility into the system. It always avails a linguist to
enter new rules with the help of Rule-Base Editor
without any support from knowledge engineer.
We have already learnt that Samaas Vigraha(CD)
is the most important aspect of the tokenization
phase in a parser. Implicitly, the acquisition of fac-
tors and rules also gather equal importance. Signify-
ing this fact, we have done rigorous survey over the
grammar to identify these factors. Hence, we assert
that our system will be a significant contribution in
this area of research.
9 Future Scope and Conclusion
We assert that Vaakkriti would be a preliminary con-
tribution to the realm of NLP. Adding to the major
works that have been done already, Vaakkriti is an
attempt to enhance the existing works. We would
extend the current system and develop a full-fledged
parser that will suffice most of the requirements of
MTsystem.
Although, it looks the way that the problem has
been solved, but the actual problems arouses when
a Sanskrit poem is given as input to a MT system.
Usually, a sanskrit poem conveys more than one
meaning and sometimes figure of speech is used,
that adds fuel to the fire. This becomes a herculean
task for a MT system and it will remain as a myth
forever.
Acknowledgements
The authors would like to specially thank Gerard
Huet for providing linguistic database of declensions
and verbal roots, that was quite helpful in making
our system fine and complete. The authors grate-
fully acknowledge financial support from the Uni-
versal Digital Library project, funded by the Min-
istry of Communication and Information Technol-
ogy (MCIT) India and also Indian Institute of Infor-
mation Technology, Allahabad.
References
Higher Sanskrit Grammar, M. R. Kale, Motilal Banarasi-
Dass Publishers.
?Paninis Grammar and Computer Science?, Saroja Bhate
and Subhash Kak, Annals of the Bhandarkar Oriental
Research Institute, vol. 72, 1993, pp. 79-94.
?Knowledge Representation in Sanskrit and Artificial In-
telligence?, Rick Briggs
?Artificial Intelligence?, Elain Rich and Kevin Knight,
2nd Edition, Tata McGrawHill, 1991.
?Artificial Intelligence, AModern Approach? Stuart Rus-
sell and Peter Norvig, 2nd Edition, Pearson Education,
2003.
?Sanskrit as Indian Networking Language: A Sanskrit
Parser?, Anupam, 2004.
?Natural Language Processing, A Paninian Perspective?,
Akshar Bharti, Vineet Chaitanya and Rajeev Sangal,
Pearson Education.
?Natural Language Processing using PROLOG? by M.
Gerald, M Chris, Addison and Wisley, 1989.
?Cognitive Science Learning Resource?,
http://www.comp.leeds.ac.uk/ugadmit/cogsci/knowled
582
Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 97?104,
Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language Processing
Named Entity Recognition for Indian Languages 
Animesh Nayan, B. Ravi Kiran Rao, Pawandeep Singh, 
Sudip Sanyal and Ratna Sanyal 
Indian Institute of Information Technology 
Allahabad, India 
e-mail@domain 
 
 
Abstract 
Abstract Stub This paper talks about a new 
approach to     recognize named entities for 
Indian languages. Phonetic matching tech-
nique is used to match the strings of differ-
ent languages on the basis of their similar 
sounding property. We have tested our sys-
tem with a comparable corpus of English 
and Hindi language data. This approach is 
language independent and requires only a 
set of rules appropriate for a language. 
1 Introduction 
Named Entity Recognition (NER) is a subtask of 
machine translation and information retrieval. 
Named entities are words which belong to certain 
categories like persons, places, organizations, nu-
merical quantities, expressions of times etc. A 
large number of techniques have been developed to 
recognize named entities for different languages. 
Some of them are Rule based and others are Statis-
tical techniques. The rule based approach uses the 
morphological and contextual evidence (Kim and 
Woodland, 2000) of a natural language and conse-
quently determines the named entities. This even-
tually leads to formation of some language specific 
rules for identifying named entities. The statistical 
techniques use large annotated data to train a 
model (Malouf, 2002) (like Hidden Markov 
Model) and subsequently examine it with the test 
data. Both the methods mentioned above require 
the efforts of a language expert. An appropriately 
large set of annotated data is yet to be made avail-
able for the Indian Languages. Consequently, the 
application of the statistical technique for Indian 
Languages is not very feasible.  
This paper deals with a new technique to recog-
nize named entities of different languages. Our 
approach does not use the previously mentioned 
techniques. Instead, we use an approach that not 
only reduces the burden of collecting and annotat-
ing data, but is language independent as well. We 
use this method to build a multilingual named en-
tity list that can be used by the named entity recog-
nizer. Our method recognizes and finds the actual 
representation of the named entities in the target 
language from an untagged corpus. Our idea was 
to match the two representations of the same 
named entity in two different languages using a 
phonetic matching algorithm. This comes from the 
property of named entities that they sound similar 
when written in native script or any other script. 
However this cross-lingual matching is not a trivial 
task. First of all, the two strings to be matched 
have to be represented in a common script. So we 
face two choices here. Either we should convert 
the two strings into some common intermediate 
representation (ex. Phonemic representation) or 
transliterate the name written in Indian language to 
English and then look for phonetic equivalence. 
Our engine has been tested for Hindi. After making 
transliteration rules for Hindi, we used a variation 
of the Editex algorithm to match the transliterated 
string with entries in English named entity data-
base to find a match. Here it is worthwhile to men-
tion that certain class of name entities which are 
not similar sounding (mostly phrases) cannot be 
extracted through this cross-lingual matching. E.g. 
?United Nations?, ?Government of India? etc. Ab-
breviations which are spelled character by charac-
97
ter in both the languages can however be extracted. 
E.g. BBC ( ), LTTE ( ) etc.  
In the next section we have given the system ar-
chitecture. The logical flow and overall description 
of the system are discussed here. Our own set of 
transliteration rules in Hindi are given in the third 
section. In the fourth section we define our base-
line task. Our system has been tested with a paral-
lel corpus which consisted of both English and 
Hindi language data. The results obtained using 
our system is described in the fifth section together 
with an analysis. Conclusions are presented in the 
last section together with directions for future im-
provements. 
2 System Architecture: Logical Flow and 
overall description of the System 
The system architecture is shown in Figure 1. It 
consists of the following modules: 
        
 
Figure 1: System Architecture 
 
2.1 Crawler 
The crawler is a web-bot or spider which browses 
the web in an automated manner. It starts with a 
list of Uniform Resource Locators (URL) that it is 
to visit, called the seeds. As the crawler visits these 
URL?s it collects all the hyperlinks and adds them 
to a queue. URL?s from the queue are crawled fur-
ther. Since the crawler collects the data from web, 
the data collection is fully automated. The crawler 
gathers data for both English and other Indian lan-
guages. The data collected for English is used to 
populate the English named entity database which 
is significantly accurate. We have used the freely 
available Stanford Named Entity Recognizer 
(Finkel, Grenager, and Manning, 2005) in our en-
gine. The data collected for Indian languages will 
be used to build a database of named entities for 
the given language. 
2.2 Parser 
The crawler saves the content in an html form  
onto the system. The parser parses these html files. 
Additionally the parser can also parse the PDF as 
well as RTF files. The output of the parser is 
passed to the corresponding modules for the two 
different languages. 
2.3 Phonetic Matcher 
Phonetic matching is the task of matching two rep-
resentations of the same name. A name may have 
more than one representation in its native script 
itself. If the name is represented in a script other 
than its native script, there may be large number of 
potential variants for its representation. Phonetic 
matching is a fuzzy string matching technique in 
which we match strings on the basis of their simi-
lar sounding property and not identity. Most com-
mon phonetic matching techniques are Soundex 
and Editex. These techniques are used to match 
two representations of the same name in English. 
We survey the techniques in the following subsec-
tions.  
 
2.3.1 Soundex 
 
Soundex algorithm was designed by Odell and 
Russell in 1918 to find spelling variation of names. 
It represents classes of sounds which can be 
lumped together. The classes for the algorithm are 
shown in Appendix A. These classes are placed for 
phonetic matching according to the following algo-
rithm: 
1. Replace all but the first letter of the string 
by its phonetic code. 
2. Eliminate any adjacent representation of 
codes. 
3. Eliminate all occurrences of code 0 i.e. 
eliminate all vowels. 
4. Return the first four characters of the re-
sulting string. 
5. Examples: Dickson = d25, Dikson = d25.  
Two names match if they have the same soun-
dex representation. This method does not account 
98
for vowels and hence is not accurate for cross-
lingual matching. 
 
2.3.2 Editex 
 
The Editex algorithm was designed by Zobel and 
Dart (Zobel and Dart,1996).  It is an enhancement 
of the Levenshtein (Levenshtein, 1966) edit dis-
tance algorithm. The Levenshtein algorithm meas-
ures the edit distance between two strings where 
edit distance is defined as the minimum number of 
basic operations required to match one string to the 
other where the basic operations are insertion, de-
letion and substitution. Insertion and deletion costs 
are 1 and substitution cost is given by a function 
subst_cost (Xi, Yj) which returns 0 if the two char-
acters Xi and Yj are same and 1, if they are differ-
ent. The score dist [m, n] is returned as the edit 
distance between two strings. A score of zero im-
plies a perfect match. 
The algorithm has O (mn) time and space com-
plexity where m and n are the lengths of the two 
strings respectively. The pseudo code for the 
Levenshtein edit distance algorithm is described in 
Appendix B. Editex groups similar sounding pho-
nemes into equivalence classes. The substitution 
cost is determined by a function S (Xi, Yj) that 
returns 0 if the two characters Xi and Yj are same, 
1 if they lie in the same equivalence class and 2 
otherwise. The insertion and substitution costs are 
determined by a function D (Xi-1, Xi) which is 
almost same as S (Xi, Yj) except for the difference 
that it compares letters of the same string and it 
returns 1 if Xi-1 is ?h? or ?w? and Xi-1 is not equal 
to Xi. The editex equivalence classes and the ed-
itex pseudo-code are given in Appendix C. 
Editex performs fairly better than Soundex and 
Leveinshtein edit distance algorithms. However 
further enhancements in Editex are also possible. 
?Tapering? is one enhancement in which we weigh 
mismatches at the beginning of the string with 
higher score than mismatches towards the end 
(Zobel and Dart, 1996). Other enhancements are 
those in which input strings are mapped to their 
phonemic representation, called phonometric 
methods (Zobel and Dart, 1996).  
3 Transliteration rules 
To perform phonetic matching of two different 
representations of a named entity, we need both of 
them in a common script. We choose to transliter-
ate the named entity in Indian language to English. 
The transliteration rules for a language must be 
written for the same. We have written our own set 
of transliteration rules for Hindi. These can be de-
scribed briefly as under 
The entity to be transliterated is scanned character by 
character from left to right. Each character of Hindi is 
mapped to an equivalent character/set of character in 
English according to a mapping function. The charac-
ter set generated by the function is appended into a 
string as per the rules. E.g.  ?? = ?? + ?   is a single 
character representation in Unicode (???) and maps to 
?Ka?. 
1. Start with an empty string. When a conso-
nant or singleton vowel (not as ?matra?) is 
encountered append the set of characters 
returned by mapping function. 
2. When a consonant is followed by a vowel 
the preceding ?a? should be removed and 
the character set for the vowel should be 
appended. E.g. ?? consists of two charac-
ters ? + . Once we encounter ? we 
append ?ka? and when is encountered 
next we remove the ?a? and append the 
mapping for i.e. ?e?. This rule applies in 
general to all the vowels. 
3. If the transliterated string has ?a? as its last 
character while it doesn?t have the vowel 
 as last character of Hindi string, re-
move this occurrence of ?a?. The last 
vowel in Hindi is very important as two al-
together different words may have the only 
difference in the last vowel. E.g.   ????? 
and ?????? are proper nouns having dif-
ferent genders. Their English representa-
tions are ?Kamal? and ?Kamla? respec-
tively.  
 
The transliteration always performs a one to one 
mapping of a character in Hindi to a set of charac-
ters in English. However the English representa-
tion may have different character sets for the same 
Hindi character in different names. E.g.  ????? is 
?Kamal? while ??????? is ?Cricket?.  ??? is often 
represented by ?K? for Hindi names, by ?C? for 
99
English names and by ?Q? for Urdu names. The 
Editex algorithm groups these letters in the same 
equivalence class.  
4 Baseline Task 
At the core of our method lies the phonetic match-
ing algorithm. We have modified the Editex algo-
rithm as mentioned in Appendix C. Editex can be 
modified to take into account that there can be 
more than three (0, 1, 2) levels of acceptability for 
substitutions due to the inherent properties of par-
ticular languages. For example, say ?ckq? is one 
equivalence class in Editex. ?c? and ?k? have a sub-
stitution cost of 1. We may reduce this substitution 
cost to 0.5 for a language in which it is highly 
probable that the same character maps to ?c? and 
?k? in the English representation of its names.  
Thus the equivalence classes and the substitution 
costs in Editex can be modified for cross-lingual 
phonetic matching. There can also be further lan-
guage specific enhancements. The following algo-
rithm along with some language specific enhance-
ments was implemented for Hindi. 
4.1 Abbreviation Check 
Abbreviations form an important class of named 
entities. So, we first check whether the Hindi string 
is an abbreviation in which the English characters 
are spelled individually. For each English alphabet 
we have some unique Hindi representation. The 
function performs accurately most of the time and 
extracts such named entities. If we are able to find 
out that the string is an abbreviation, the corre-
sponding English representation can be returned by 
the function itself, hence there is no need of further 
matching. If the string is not an abbreviation, we 
proceed to the actual matching algorithm. 
4.2 4.2. First letter matching 
The first letters of the two strings must either be 
the same or should belong to the same equivalence 
class. The equivalence classes for first character 
matching are: 
 
      "ckq", "wbv", "iy?,"jz", "aeiou"  
 
The English named entity database must be in-
dexed according to the first letter of the named en-
tity so that we only search for matches in those 
indexes which fall into the same equivalence class. 
This is very important for the computational effi-
ciency of the engine as it reduces the search space. 
4.3 Preprocessing 
Often the phonetic inconsistencies in English lead 
to low matching score for two representation of the 
same name. To take this into account, before 
matching the two strings the named entity retrieved 
from English Named entity database is preproc-
essed to form a new string. We have used the fa-
mous ?Mark Twain?s plan for the improvement of 
English spelling? (http://grammar.ccc.commnet.edu/ 
grammar/twain.htm) added with some more rules. 
This way we tackle the problem of more than one 
possible character sets for some vowels since only 
one of them can be chosen during transliteration. 
We also tackle some other problems like silent-
alphabets and repeated alphabets so that the prob-
ability of generating high matching score in-
creases. The following set of rules for preprocess-
ing was used. 
1. Change all occurrences of ?oo? to ?u?. 
(both character sets are for the vowel  ) 
2. Change all occurrences of ?ee? to ?i?   
(both character sets are for the vowel   ) 
3. Change all occurrences of ?f? to ph? 
4. Change all occurrences of ?au? to ?o? 
5. If a word starts with "x", replace the "x" 
with a "z".  Change all the remaining "x"s 
to "ks"s. 
6. If a "c" is directly followed by an "e" or 
"i", change the "c" to an "s" 
7. If a "c" is directly followed by a "k", re-
move the "c". Keep applying this rule as 
necessary    (Example: "cck" becomes 
"k".) 
8. If a word starts with "sch", change the 
"sch" to a "sk". 
9. If a "ch" is directly followed by an "r", 
change the "ch" to a "k". 
10. After applying the above rules, change all 
"c"s that are not directly followed by an 
"h", to a "k". (This includes all "c"s that 
are last letter of a word)  
11. If a word starts with "kn" change "kn" 
to?n?  
12. Change all double consonants of the same 
letter to a single consonant. A consonant is 
any letter that is not one of "a, e, i, o, u." 
(Example: "apple" becomes "aple"). Keep 
100
applying this rule as necessary (Example: 
"zzz" becomes "z".) 
4.4 Editex Score 
Now the transliterated string and the preprocessed 
string are compared to generate an editex score. 
The equivalence classes we used were similar to as 
proposed in the original editex algorithm except 
for some language specific changes for Hindi.  
Length of the two strings has to be considered 
while deciding the threshold score for a match oth-
erwise there can be greater number of mismatches 
for small strings. So we normalize  editex score as  
d = [1- {editex(X, Y) / (length(X) + length(Y)}] 
The decided threshold for match was 0.86.  A 
score above threshold guarantees equivalence of 
the two representations. The results are shown in 
Table-1. 
 
 
Hindi 
NE 
English 
NE 
Transliteration 
Output 
Editex  
Score 
?????? Hindi  Hindi 1.0 
??????? Philistini Phalastini 0.9 
????????? Bangladesh      Bangladesh 1.0 
??????? Jharkhand Jharakhand     0.894 
?????? Pashchim Pashchim 1.0 
????? Bengal Bangal 0.916 
???? Bharat Bharat 1.0 
????? Cricket Kriket 0.923 
??? Greg Greg 1.0 
???? Chappel Chaipal 0.857 
????? Mahendra Mahendr 0.933 
?????  Rahul Rahul 1.0 
???? Dravid Dravid 1.0 
???????? Chattisgarh Chattisagadh 0.866 
 
 
Table-1: Hindi named entities with transliteration 
output and normalized Editex scores 
 
 
5 Results and Analysis 
We have tested our system with a parallel corpus 
which consisted of both English and Hindi lan-
guage data. Further we used the web crawler to 
populate our NE list of both the languages thus 
embedding the concept of comparable corpus. The 
results for English obtained using parallel corpus 
are:  
Precision: 81.40% and Recall: 81.39%  
 
This corpus carried named entities from the do-
main of travel, tourism and culture. Further for 
classifying the results for Hindi we used the defini-
tion of named entities as given by Chinchor (Chin-
chor, 1997) as for entity names organizations (OE), 
person names (PE) and location names (LE). The 
results for numeric expressions (monetary values 
and percentages) and temporal expressions (dates 
and times) were not considered for results because 
it is a trivial task to build grammar rules for such 
entities which appear quite regularly.  
We have focused on OE, PE and LE named enti-
ties for Hindi so that we can analyze the perform-
ance on new and hitherto undiscovered entities 
which come into existence with the passage of 
time. This premise provides the real basis for chal-
lenging the performance of any NER technique for 
Indian Languages. 
The testing on the corpus of around 1000 sen-
tences revealed the following results for Hindi: 
? Precision for all named entities 
(PE+OE+LE): 80.2% 
? Recall for PE (person entity names): 
47.4% 
? Recall for OE (organization entity names): 
42.9% 
? Recall for LE (location entity names): 
74.6%  
It is important to observe here that the engine 
shows good recall for location entity names (LE) 
which were more abundant in the corpus. Besides 
this, the corpus had a heterogeneous mix of named 
entities with tourism-related information not only 
from India but also from the continents of South 
America and Antarctica. A good recall percentage 
for Hindi location entity names is encouraging as 
the named entities related to South America and 
Antarctica did not have phonetic similarity with 
101
the native entities available from tourism informa-
tion from India. This gives good credence to the 
phonetic matching approach used above. Causes 
for the comparatively lower recall percentage 
among person entity names and organization entity 
names are under further investigation. 
6 Conclusions 
We have used the phonetic matching technique to 
match the strings of different languages on the ba-
sis of their similar sounding property. As the Pho-
netic Matcher module is tested for more data, more 
generic rules can be made to improve its accuracy. 
The Engine should be improved so that it may rec-
ognize phrasal named entities and abbreviations. 
The engine will work for any language if the pho-
netic matching rules are written for that language. 
We can also develop a crawler which will be fo-
cused upon a certain domain of interest. Focused 
crawlers are very important for generating re-
sources for natural language processing. A focused 
crawler application is an intelligent agent that 
crawls the web for content related to a specific 
domain. This kind of crawler could be used in the 
future for purposes of data collection for a particu-
lar domain. 
7 Acknowledgements 
The authors gratefully acknowledge financial assis-
tance from TDIL, MCIT (Govt. of India). 
References 
Chinchor, N. 1997. MUC-7 Named entity task defini-
tion. In Proceedings of the 7th Message Understand-
ing Conference (MUC-7) 
Finkel, Jenny Rose, Grenager, Trond and Manning, 
Christopher. 2005. Incorporating Non-local Informa-
tion into Information Extraction Systems by Gibbs 
Sampling. Proceedings of the 43rd Annual Meeting 
of the Association for Computational Linguistics 
(ACL 2005), pp. 363-370.  
Kim, J. and Woodland, P.C. 2000a. Rule Based Named 
Entity Recognition. Technical Report CUED/ FIN-
FENG/TR.385, Cambridge University Engineering 
Department, 2000. 
Malouf, Robert. 2002 Markov models for language-
independent named entity recognition. In Proceed-
ings of CoNLL-2002 Taipei, Taiwan, pages 591-599.  
Levenshtein, V.I. 1966. Binary codes capable of cor-
recting deletions, insertions, and reversals. Soviet 
Physics Doklady 10: 707?710. 
Zobel, Justin and Dart, Philip. 1996. Phonetic string 
matching: Lessons from information retrieval. In 
Proceedings of the Eighteenth ACM SIGIR Interna-
tional Conference on Research and Development in 
Information Retrieval, Zurich, Switzerland, August 
1996, pp. 166-173.  
Appendix A: Soundex classes 
 
Code Letters Code Letters 
0 aeiouyhw 4 l 
1 bpfv 5 mn 
2 cgjkqsxz 6 R 
3 dt   
 
Appendix B: Pseudo code for Leveinshtein edit dis-
tance: 
 
Input: Two strings, X and Y 
Output: The minimum edit dis-
tance between X and Y 
 
m ? length(X) 
n ? length(Y) 
 
for i =0 to m do 
dist[i, 0] ? i 
 
for j = 0 to n do 
dist[0, j] ? j 
 
for i = 1 to m do 
for j = 1 to n do 
 
dist[i, j] =  
min{ 
dist[i-1, j]+inser_cost, 
   dist[i-1, j-1] 
   + subst_cost[Xi, Yj], 
   dist[i, j-1] + delet_cost      
} 
end 
 
Appendix C: Editex Equivalence Classes: 
 
 
aeiouy       bp       ckq       dt       lr        mn 
gj              fpy       sxz      csz 
 
102
Pseudo code for Editex Algorithm 
 
Input: Two strings, X and Y 
Output: The editex distance 
between X and Y 
 m = length(X) 
 n = length(Y) 
 
editex_dist[0, 0] = 0 
 
for i = 1 to m do 
  
editex_dist[i, 0] 
  = editex_dist[i-1, 0] 
  + D(Xi-1, Xi) 
 
for j = 0 to n do 
 
editex_dist[0, j] 
  = editex_dist[0, j-1]   
  + D(Yj-1, Yj) 
 
for i = 1 to m do 
for j = 1 to n do 
 
editex_dist[i, j] =  
  min { editex_dist[i-1, j] 
  + D(Xi-1, Xi), 
  editex_dist[i-1, j-1] 
  + S(X, Yj), 
  editex_dist[i, j-1] 
  + D(Yj-1, Yj) 
 
end 
103
 104
