Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 143?151,
Beijing, August 2010
End-to-End Coreference Resolution via Hypergraph Partitioning
Jie Cai and Michael Strube
Natural Language Processing Group
Heidelberg Institute for Theoretical Studies gGmbH
(jie.cai|michael.strube)@h-its.org
Abstract
We describe a novel approach to coref-
erence resolution which implements a
global decision via hypergraph partition-
ing. In constrast to almost all previ-
ous approaches, we do not rely on sep-
arate classification and clustering steps,
but perform coreference resolution glob-
ally in one step. Our hypergraph-based
global model implemented within an end-
to-end coreference resolution system out-
performs two strong baselines (Soon et al,
2001; Bengtson & Roth, 2008) using sys-
tem mentions only.
1 Introduction
Coreference resolution is the task of grouping
mentions of entities into sets so that all mentions
in one set refer to the same entity. Most recent
approaches to coreference resolution divide this
task into two steps: (1) a classification step which
determines whether a pair of mentions is corefer-
ent or which outputs a confidence value, and (2)
a clustering step which groups mentions into enti-
ties based on the output of step 1.
The classification steps of most approaches
vary in the choice of the classifier (e.g. decision
tree classifiers (Soon et al, 2001), maximum en-
tropy classification (Luo et al, 2004), SVM clas-
sifiers (Rahman & Ng, 2009)) and the number of
features used (Soon et al (2001) employ a set of
twelve simple but effective features while e.g., Ng
& Cardie (2002) and Bengtson & Roth (2008) de-
vise much richer feature sets).
The clustering step exhibits much more varia-
tion: Local variants utilize a closest-first decision
(Soon et al, 2001), where a mention is resolved to
its closest possible antecedent, or a best-first deci-
sion (Ng & Cardie, 2002), where a mention is re-
solved to its most confident antecedent (based on
the confidence value returned by step 1). Global
variants attempt to consider all possible cluster-
ing possibilites by creating and searching a Bell
tree (Luo et al, 2004), by learning the optimal
search strategy itself (Daume? III & Marcu, 2005),
by building a graph representation and applying
graph clustering techniques (Nicolae & Nicolae,
2006), or by employing integer linear program-
ming (Klenner, 2007; Denis & Baldridge, 2009).
Since these methods base their global clustering
step on a local pairwise model, some global infor-
mation which could have guided step 2 is already
lost. The twin-candidate model (Yang et al, 2008)
replaces the pairwise model by learning prefer-
ences between two antecedent candidates in step
1 and applies tournament schemes instead of the
clustering in step 2.
There is little work which deviates from this
two-step scheme. Culotta et al (2007) introduce a
first-order probabilistic model which implements
features over sets of mentions and thus operates
directly on entities.
In this paper we describe a novel approach to
coreference resolution which avoids the division
into two steps and instead performs a global deci-
sion in one step. We represent a document as a hy-
pergraph, where the vertices denote mentions and
the edges denote relational features between men-
tions. Coreference resolution is performed glob-
ally in one step by partitioning the hypergraph into
subhypergraphs so that all mentions in one subhy-
pergraph refer to the same entity. Our model out-
143
performs two strong baselines, Soon et al (2001)
and Bengtson & Roth (2008).
Soon et al (2001) developed an end-to-end
coreference resolution system for the MUC data,
i.e., a system which processes raw documents
as input and produces annotated ones as output.
However, with the advent of the ACE data, many
systems either evaluated only true mentions, i.e.
mentions which are included in the annotation,
the so-called key, or even received true informa-
tion for mention boundaries, heads of mentions
and mention type (Culotta et al, 2007, inter alia).
While these papers report impressive results it has
been concluded that this experimental setup sim-
plifies the task and leads to an unrealistic surro-
gate for the coreference resolution problem (Stoy-
anov et al, 2009, p.657, p660). We argue that
the field should move towards a realistic setting
using system mentions, i.e. automatically deter-
mined mention boundaries and types. In this pa-
per we report results using our end-to-end coref-
erence resolution system, COPA, without relying
on unrealistic assumptions.
2 Related Work
Soon et al (2001) transform the coreference res-
olution problem straightforwardly into a pairwise
classification task making it accessible to standard
machine learning classifiers. They use a set of
twelve powerful features. Their system is based
solely on information of the mention pair anaphor
and antecedent. It does not take any information
of other mentions into account. However, it turned
out that it is difficult to improve upon their re-
sults just by applying a more sophisticated learn-
ing method and without improving the features.
We use a reimplementation of their system as first
baseline. Bengtson & Roth (2008) push this ap-
proach to the limit by devising a much more in-
formative feature set. They report the best results
to date on the ACE 2004 data using true mentions.
We use their system combined with our prepro-
cessing components as second baseline.
Luo et al (2004) perform the clustering step
within a Bell tree representation. Hence their
system theoretically has access to all possible
outcomes making it a potentially global system.
However, the classification step is still based on
a pairwise model. Also since the search space in
the Bell tree is too large they have to apply search
heuristics. Hence, their approach loses much of
the power of a truly global approach.
Culotta et al (2007) introduce a first-order
probabilistic model which implements features
over sets of mentions. They use four features for
their first-order model. The first is an enumeration
over pairs of noun phrases. The second is the out-
put of a pairwise model. The third is the cluster
size. The fourth counts mention type, number and
gender in each cluster. Still, their model is based
mostly on information about pairs of mentions.
They assume true mentions as input. It is not clear
whether the improvement in results translates to
system mentions.
Nicolae & Nicolae (2006) describe a graph-
based approach which superficially resembles our
approach. However, they still implement a two
step coreference resolution approach and apply
the global graph-based model only to step 2. They
report considerable improvements over state-of-
the-art systems including Luo et al (2004). How-
ever, since they not only change the clustering
strategy but also the features for step 1, it is not
clear whether the improvements are due to the
graph-based clustering technique. We, instead,
describe a graph-based approach which performs
classification and clustering in one step. We com-
pare our approach with two competitive systems
using the same feature sets.
3 COPA: Coreference Partitioner
The COPA system consists of learning modules
which learn hyperedge weights from the training
data, and resolution modules which create a hy-
pergraph representation for the testing data and
perform partitioning to produce subhypergraphs,
each of which represents an entity. An example
analysis of a short document involving the two en-
tities, BARACK OBAMA and NICOLAS SARKOZY
illustrates how COPA works.
[US President Barack Obama] came to Toronto today.
[Obama] discussed the financial crisis with [President
Sarkozy].
[He] talked to him [him] about the recent downturn of the
European markets.
[Barack Obama] will leave Toronto tomorrow.
144
A hypergraph (Figure (1a)) is built for this
document based on three features. Two hyper-
edges denote the feature partial string match,
{US President Barack Obama, Barack Obama,
Obama} and {US President Barack Obama, Pres-
ident Sarkozy}. One hyperedge denotes the fea-
ture pronoun match, {he, him}. Two hyperedges
denote the feature all speak, {Obama, he} and
{President Sarkozy, him}.
On this initial representation, a spectral clus-
tering technique is applied to find two partitions
which have the strongest within-cluster connec-
tions and the weakest between-clusters relations.
The cut found is called Normalized Cut, which
avoids trivial partitions frequently output by the
min-cut algorithm. The two output subhyper-
graphs (Figure (1b)) correspond to two resolved
entities shown on both sides of the bold dashed
line. In real cases, recursive cutting is applied
to all the subhypergraphs resulting from previous
steps, until a stopping criterion is reached.
Figure 1: Hypergraph-based representation
3.1 HyperEdgeLearner
COPA needs training data only for computing the
hyperedge weights. Hyperedges represent fea-
tures. Each hyperedge corresponds to a feature
instance modeling a simple relation between two
or more mentions. This leads to initially overlap-
ping sets of mentions. Hyperedges are assigned
weights which are calculated based on the train-
ing data as the percentage of the initial edges (as
illustrated in Figure (1a)) being in fact coreferent.
The weights for some of Soon et al (2001)?s fea-
tures learned from the ACE 2004 training data are
given in Table 1.
Edge Name Weight
Alias 0.777
StrMatch Pron 0.702
Appositive 0.568
StrMatch Npron 0.657
ContinuousDistAgree 0.403
Table 1: Hyperedge weights for ACE 2004 data
3.2 Coreference Resolution Modules
Unlike pairwise models, COPA processes a docu-
ment globally in one step, taking care of the pref-
erence information among all the mentions at the
same time and clustering them into sets directly.
A raw document is represented as a single hyper-
graph with multiple edges. The hypergraph re-
solver partitions the simple hypergraph into sev-
eral subhypergraphs, each corresponding to one
set of coreferent mentions (see e.g. Figure (1b)
which contains two subhypergraphs).
3.2.1 HGModelBuilder
A single document is represented in a hyper-
graph with basic relational features. Each hyper-
edge in a graph corresponds to an instance of one
of those features with the weight assigned by the
HyperEdgeLearner. Instead of connecting nodes
with the target relation as usually done in graph
models, COPA builds the graph directly out of a
set of low dimensional features without any as-
sumptions for a distance metric.
3.2.2 HGResolver
In order to partition the hypergraph we adopt
a spectral clustering algorithm. Spectral cluster-
ing techniques use information obtained from the
eigenvalues and eigenvectors of the graph Lapla-
cian to cluster the vertices. They are simple to im-
plement and reasonably fast and have been shown
to frequently outperform traditional clustering al-
gorithms such as k-means. These techniques have
145
Algorithm 1 R2 partitioner
Note: { L = I ?Dv?
1
2HWDe?1HTDv?
1
2 }
Note: { Ncut(S) := vol?S( 1volS + 1volSc )}
input: target hypergraph HG, predefined ??
Given a HG, construct its Dv , H , W and De
Compute L for HG
Solve the L for the second smallest eigenvector V2
for each splitting point in V2 do
calculate Ncuti
end for
Choose the splitting point with min
i
(Ncuti)
Generate two subHGs
if min
i
(Ncuti) < ?? then
for each subHG do
Bi-partition the subHG with the R2 partitioner
end for
else
Output the current subHG
end if
output: partitioned HG
many applications, e.g. image segmentation (Shi
& Malik, 2000).
We adopt two variants of spectral clustering,
recursive 2-way partitioning (R2 partitioner) and
flat-K partitioning. Since flat-K partitioning did
not perform as well we focus here on recursive 2-
way partitioning. In contrast to flat-K partitioning,
this method does not need any information about
the number of target sets. Instead a stopping cri-
terion ?? has to be provided. ?? is adjusted on
development data (see Algorithm 1).
In order to apply spectral clustering to hyper-
graphs we follow Agarwal et al (2005). All ex-
perimental results are obtained using symmetric
Laplacians (Lsym) (von Luxburg, 2007).
Given a hypergraph HG, a set of matrices is
generated. Dv and De denote the diagonal matri-
ces containing the vertex and hyperedge degrees
respectively. |V | ? |E| matrix H represents the
HG with the entries h(v, e) = 1 if v ? e and 0
otherwise. HT is the transpose of H . W is the
diagonal matrix with the edge weights. S is one
of the subhypergraphs generated from a cut in the
HG, where Ncut(S) is the cut?s value.
Using Normalized Cut does not generate sin-
gleton clusters, hence a heuristic singleton detec-
tion strategy is used in COPA. We apply a thresh-
old ? to each node in the graph. If a node?s degree
is below the threshold, the node will be removed.
3.3 Complexity of HGResolver
Since edge weights are assigned using simple de-
scriptive statistics, the time HGResolver needs for
building the graph Laplacian matrix is insubstan-
tial. For eigensolving, we use an open source li-
brary provided by the Colt project1which imple-
ments a Householder-QL algorithm to solve the
eigenvalue decomposition. When applied to the
symmetric graph Laplacian, the complexity of the
eigensolving is given by O(n3), where n is the
number of mentions in a hypergraph. Since there
are only a few hundred mentions per document in
our data, this complexity is not an issue (spectral
clustering gets problematic when applied to mil-
lions of data points).
4 Features
The HGModelBuilder allows hyperedges with a
degree higher than two to grow throughout the
building process. This type of edge is mergeable.
Edges with a degree of two describe pairwise rela-
tions. Thus these edges are non-mergeable. This
way any kind of relational features can be incor-
porated into the hypergraph model.
Features are represented as types of hyperedges
(in Figure (1b) the two hyperedges marked by ??
??? are of the same type). Any realized edge is an
instance of the corresponding edge type. All in-
stances derived from the same type have the same
weight, but they may get reweighted by the dis-
tance feature (Section 4.4).
In the following Subsections we describe the
features used in our experiments. We use the en-
tire set for obtaining the final results. We restrict
ourselves to Soon et al (2001)?s features when we
compare our system with theirs in order to assess
the impact of our model regardless of features (we
use features 1., 2., 3., 6., 7., 11., 13.).
4.1 Hyperedges With a Degree > 2
High degree edges are the particular property of
the hypergraph which allows to include all types
of relational features into our model. The edges
are built through pairwise relations and, if consis-
tent, get incrementally merged into larger edges.
1http://acs.lbl.gov/
?
hoschek/colt/
146
High degree edges are not sensitive to positional
information from the documents.
(1) StrMatch Npron & (2) StrMatch Pron:
After discarding stop words, if the strings of men-
tions completely match and are not pronouns, they
are put into edges of the StrMatch Npron type.
When the matched mentions are pronouns, they
are put into the StrMatch Pron type edges.
(3) Alias: After discarding stop words, if men-
tions are aliases of each other (i.e. proper names
with partial match, full names and acronyms of
organizations, etc.), they are put into edges of the
Alias type.
(4) Synonym: If, according to WordNet, men-
tions are synonymous, they are put into an edge of
the Synonym type.
(5) AllSpeak: Mentions which appear within a
window of two words of a verb meaning to say
form an edge of the AllSpeak type.
(6) Agreement: If mentions agree in Gender,
Number and Semantic Class they are put in edges
of the Agreement type. Because Gender, Num-
ber and Semantic Class are strong negative coref-
erence indicators ? in contrast to e.g. StrMatch ?
and hence weak positive features, they are com-
bined into the one feature Agreement.
4.2 Hyperedges With a Degree = 2
Features which have been used by pairwise mod-
els are easily integrated into the hypergraph model
by generating edges with only two vertices. Infor-
mation sensitive to relative distance is represented
by pairwise edges.
(7) Apposition & (8) RelativePronoun: If two
mentions are in a appositive structure, they are put
in an edge of type Apposition. If the latter mention
is a relative pronoun, the mentions are put in an
edge of type RelativePronoun.
(9) HeadModMatch: If the syntactic heads of
two mentions match, and if their modifiers do not
contradict each other, the mentions are put in an
edge of type HeadModMatch.
(10) SubString: If a mention is the substring
of another one, they are put into an edge of type
SubString.
4.3 MentionType and EntityType
In our model (11) mention type can only reason-
ably be used when it is conjoined with other fea-
tures, since mention type itself describes an at-
tribute of single mentions. In COPA, it is con-
joined with other features to form hyperedges, e.g.
the StrMatch Pron edge. We use the same strat-
egy to represent (12) entity type.
4.4 Distance Weights
Our hypergraph model does not have any obvi-
ous means to encode distance information. How-
ever, the distance between two mentions plays
an important role in coreference resolution, es-
pecially for resolving pronouns. We do not en-
code distance as feature, because this would intro-
duce many two-degree-hyperedges which would
be computationally very expensive without much
gain in performance. Instead, we use distance to
reweight two-degree-hyperedges, which are sen-
sitive to positional information.
We experimented with two types of distance
weights: One is (13) sentence distance as used in
Soon et al (2001)?s feature set, while the other is
(14) compatible mentions distance as introduced
by Bengtson & Roth (2008).
5 Experiments
We compare COPA?s performance with two im-
plementations of pairwise models. The first base-
line is the BART (Versley et al, 2008) reimple-
mentation of Soon et al (2001), with few but ef-
fective features. Our second baseline is Bengtson
& Roth (2008), which exploits a much larger fea-
ture set while keeping the machine learning ap-
proach simple. Bengtson & Roth (2008) show
that their system outperforms much more sophis-
ticated machine learning approaches such as Cu-
lotta et al (2007), who reported the best results
on true mentions before Bengtson & Roth (2008).
Hence, Bengtson & Roth (2008) seems to be a rea-
sonable competitor for evaluating COPA.
In order to report realistic results, we neither
assume true mentions as input nor do we evalu-
ate only on true mentions. Instead, we use an in-
house mention tagger for automatically extracting
mentions.
147
5.1 Data
We use the MUC6 data (Chinchor & Sund-
heim, 2003) with standard training/testing divi-
sions (30/30) as well as the MUC7 data (Chin-
chor, 2001) (30/20). Since we do not have ac-
cess to the official ACE testing data (only avail-
able to ACE participants), we follow Bengtson &
Roth (2008) for dividing the ACE 2004 English
training data (Mitchell et al, 2004) into training,
development and testing partitions (268/76/107).
We randomly split the 252 ACE 2003 training
documents (Mitchell et al, 2003) using the same
proportions into training, development and testing
(151/38/63). The systems were tuned on develop-
ment and run only once on testing data.
5.2 Mention Tagger
We implement a classification-based mention tag-
ger, which tags each NP chunk as ACE mention or
not, with neccessary post-processing for embed-
ded mentions. For the ACE 2004 testing data, we
cover 75.8% of the heads with 73.5% accuracy.
5.3 Evaluation Metrics
We evaluate COPA with three coreference resolu-
tion evaluation metrics: the B3-algorithm (Bagga
& Baldwin, 1998), the CEAF-algorithm (Luo,
2005), and, for the sake of completeness, the
MUC-score (Vilain et al, 1995).
Since the MUC-score does not evaluate single-
ton entities, it only partially evaluates the perfor-
mance for ACE data, which includes singleton
entities in the keys. The B3-algorithm (Bagga
& Baldwin, 1998) addresses this problem of the
MUC-score by conducting calculations based on
mentions instead of coreference relations. How-
ever, another problematic issue emerges when
system mentions have to be dealt with: B3 as-
sumes the mentions in the key and in the response
to be identical, which is unlikely when a men-
tion tagger is used to create system mentions.
The CEAF-algorithm aligns entities in key and
response by means of a similarity metric, which
is motivated by B3?s shortcoming of using one
entity multiple times (Luo, 2005). However, al-
though CEAF theoretically does not require to
have the same number of mentions in key and
response, the algorithm still cannot be directly
applied to end-to-end coreference resolution sys-
tems, because the similarity metric is influenced
by the number of mentions in key and response.
Hence, both the B3- and CEAF-algorithms
have to be extended to deal with system mentions
which are not in the key and true mentions not
extracted by the system, so called twinless men-
tions (Stoyanov et al, 2009). Two variants of
the B3-algorithm are proposed by Stoyanov et al
(2009), B3all and B30 . B3all tries to assign intu-
itive precision and recall to the twinless system
mentions and twinless key mentions, while keep-
ing the size of the system mention set and the key
mention set unchanged (which are different from
each other). For twinless mentions, B3all discards
twinless key mentions for precision and twinless
system mentions for recall. Discarding parts of
the key mentions, however, makes the fair com-
parison of precision values difficult. B30 produces
counter-intuitive precision by discarding all twin-
less system mentions. Although it penalizes the
recall of all twinless key mentions, so that the F-
scores are balanced, it is still too lenient (for fur-
ther analyses see Cai & Strube (2010)).
We devise two variants of the B3- and CEAF-
algorithms, namely B3sys and CEAFsys. For com-
puting precision, the algorithms put all twinless
true mentions into the response even if they were
not extracted. All twinless system mentions which
were deemed not coreferent are discarded. Only
twinless system mentions which were mistakenly
resolved are put into the key. Hence, the system
is penalized for resolving mentions not found in
the key. For recall the algorithms only consider
mentions from the original key by discarding all
the twinless system mentions and putting twin-
less true mentions into the response as singletons
(algorithm details, simulations and comparison of
different systems and metrics are provided in Cai
& Strube (2010)). For CEAFsys, ?3 (Luo, 2005)
is used. B3sys and CEAFsys report results for end-
to-end coreference resolution systems adequately.
5.4 Baselines
We compare COPA?s performance with two base-
lines: SOON ? the BART (Versley et al, 2008)
reimplementation of Soon et al (2001) ? and
148
SOON COPA with R2 partitioner
R P F R P F ?? ?
MUC MUC6 59.4 67.9 63.4 62.8 66.4 64.5 0.08 0.03
MUC7 52.3 67.1 58.8 55.2 66.1 60.1 0.05 0.01
ACE 2003 56.7 75.8 64.9 60.8 75.1 67.2 0.07 0.03
ACE 2004 50.4 67.4 57.7 54.1 67.3 60.0 0.05 0.04
B3sys MUC6 53.1 78.9 63.5 56.4 76.3 64.1 0.08 0.03
MUC7 49.8 80.0 61.4 53.3 76.1 62.7 0.05 0.01
ACE 2003 66.9 87.7 75.9 71.5 83.3 77.0 0.07 0.03
ACE 2004 64.7 85.7 73.8 67.3 83.4 74.5 0.07 0.03
CEAFsys MUC6 56.9 53.0 54.9 62.2 57.5 59.8 0.08 0.03
MUC7 57.3 54.3 55.7 58.3 54.2 56.2 0.06 0.01
ACE 2003 71.0 68.7 69.8 71.1 68.3 69.7 0.07 0.03
ACE 2004 67.9 65.2 66.5 68.5 65.5 67.0 0.07 0.03
Table 3: SOON vs. COPA R2 (SOON features, system mentions, bold indicates significant improvement
in F-score over SOON according to a paired-t test with p < 0.05)
SOON B&R
R P F R P F
B3sys 64.7 85.7 73.8 66.3 85.8 74.8
Table 2: Baselines on ACE 2004
B&R ? Bengtson & Roth (2008)2. All systems
share BART?s preprocessing components and our
in-house ACE mention tagger.
In Table 2 we report the performance of SOON
and B&R on the ACE 2004 testing data using
the BART preprocessing components and our in-
house ACE mention tagger. For evaluation we use
B3sys only, since Bengtson & Roth (2008)?s sys-
tem does not allow to easily integrate CEAF.
B&R considerably outperforms SOON (we can-
not compute statistical significance, because we
do not have access to results for single documents
in B&R). The difference, however, is not as big
as we expected. Bengtson & Roth (2008) re-
ported very good results when using true men-
tions. For evaluating on system mentions, how-
ever, they were using a too lenient variant of B3
(Stoyanov et al, 2009) which discards all twinless
mentions. When replacing this with B3sys the dif-
ference between SOON and B&R shrinks.
5.5 Results
In both comparisons, COPA uses the same fea-
tures as the corresponding baseline system.
2http://l2r.cs.uiuc.edu/
?
cogcomp/
asoftware.php?skey=FLBJCOREF
5.5.1 COPA vs. SOON
In Table 3 we compare the SOON-baseline with
COPA using the R2 partitioner (parameters ?? and
? optimized on development data). Even though
COPA and SOON use the same features, COPA
consistently outperforms SOON on all data sets
using all evaluation metrics. With the exception of
the MUC7, the ACE 2003 and the ACE 2004 data
evaluated with CEAFsys, all of COPA?s improve-
ments are statistically significant. When evaluated
using MUC and B3sys, COPA with the R2 parti-
tioner boosts recall in all datasets while losing in
precision. This shows that global hypergraph par-
titioning models the coreference resolution task
more adequately than Soon et al (2001)?s local
model ? even when using the very same features.
5.5.2 COPA vs. B&R
In Table 4 we compare the B&R system (using our
preprocessing components and mention tagger),
and COPA with the R2 partitioner using B&R fea-
tures. COPA does not use the learned features
from B&R, as this would have implied to embed a
pairwise coreference resolution system in COPA.
We report results for ACE 2003 and ACE 2004.
The parameters are optimized on the ACE 2004
data. COPA with the R2 partitioner outperforms
B&R on both datasets (we cannot compute statisti-
cal significance, because we do not have access to
results for single documents in B&R). Bengtson &
Roth (2008) developed their system on ACE 2004
data and never exposed it to ACE 2003 data. We
suspect that the relatively poor result of B&R on
ACE 2003 data is caused by overfitting to ACE
149
B&R COPA with R2 partitioner
R P F R P F
B3sys ACE 2003 56.4 97.3 71.4 70.3 86.5 77.5
ACE 2004 66.3 85.8 74.8 68.4 84.4 75.6
Table 4: B&R vs. COPA R2 (B&R features, system mentions)
2004. Again, COPA gains in recall and loses
in precision. This shows that COPA is a highly
competetive system as it outperforms Bengtson &
Roth (2008)?s system which has been claimed to
have the best performance on the ACE 2004 data.
5.5.3 Running Time
On a machine with 2 AMD Opteron CPUs and 8
GB RAM, COPA finishes preprocessing, training
and partitioning the ACE 2004 dataset in 15 min-
utes, which is slightly faster than our duplicated
SOON baseline.
6 Discussion and Outlook
Most previous attempts to solve the coreference
resolution task globally have been hampered by
employing a local pairwise model in the classifi-
cation step (step 1) while only the clustering step
realizes a global approach, e.g. Luo et al (2004),
Nicolae & Nicolae (2006), Klenner (2007), De-
nis & Baldridge (2009), lesser so Culotta et al
(2007). It has been also observed that improve-
ments in performance on true mentions do not
necessarily translate into performance improve-
ments on system mentions (Ng, 2008).
In this paper we describe a coreference reso-
lution system, COPA, which implements a global
decision in one step via hypergraph partitioning.
COPA looks at the whole graph at once which en-
ables it to outperform two strong baselines (Soon
et al, 2001; Bengtson & Roth, 2008). COPA?s
hypergraph-based strategy can be taken as a gen-
eral preference model, where the preference for
one mention depends on information on all other
mentions.
We follow Stoyanov et al (2009) and argue
that evaluating the performance of coreference
resolution systems on true mentions is unrealis-
tic. Hence we integrate an ACE mention tag-
ger into our system, tune the system towards the
real task, and evaluate only using system men-
tions. While Ng (2008) could not show that su-
perior models achieved superior results on sys-
tem mentions, COPA was able to outperform
Bengtson & Roth (2008)?s system which has been
claimed to achieve the best performance on the
ACE 2004 data (using true mentions, Bengtson &
Roth (2008) did not report any comparison with
other systems using system mentions).
An error analysis revealed that there were some
cluster-level inconsistencies in the COPA output.
Enforcing this consistency would require a global
strategy to propagate constraints, so that con-
straints can be included in the hypergraph parti-
tioning properly. We are currently exploring con-
strained clustering, a field which has been very
active recently (Basu et al, 2009). Using con-
strained clustering methods may allow us to in-
tegrate negative information as constraints instead
of combining several weak positive features to one
which is still weak (e.g. our Agreement feature).
For an application of constrained clustering to the
related task of database record linkage, see Bhat-
tacharya & Getoor (2009).
Graph models cannot deal well with positional
information, such as distance between mentions
or the sequential ordering of mentions in a doc-
ument. We implemented distance as weights on
hyperedges which resulted in decent performance.
However, this is limited to pairwise relations and
thus does not exploit the power of the high de-
gree relations available in COPA. We expect fur-
ther improvements, once we manage to include
positional information directly.
Acknowledgements. This work has been
funded by the Klaus Tschira Foundation, Hei-
delberg, Germany. The first author has been
supported by a HITS PhD. scholarship. We would
like to thank Byoung-Tak Zhang for bringing
hypergraphs to our attention and `Eva Mu?jdricza-
Maydt for implementing the mention tagger.
Finally we would like to thank our colleagues in
the HITS NLP group for providing us with useful
comments.
150
References
Agarwal, Sameer, Jonwoo Lim, Lihi Zelnik-Manor, Pietro
Perona, David Kriegman & Serge Belongie (2005). Be-
yond pairwise clustering. In Proceedings of the IEEE
Computer Society Conference on Computer Vision and
Pattern Recognition (CVPR?05), Vol. 2, pp. 838?845.
Bagga, Amit & Breck Baldwin (1998). Algorithms for scor-
ing coreference chains. In Proceedings of the 1st Inter-
national Conference on Language Resources and Evalu-
ation, Granada, Spain, 28?30 May 1998, pp. 563?566.
Basu, Sugato, Ian Davidson & Kiri L. Wagstaff (Eds.)
(2009). Constrained Clustering: Advances in Algorithms,
Theory, and Applications. Boca Raton, Flo.: CRC Press.
Bengtson, Eric & Dan Roth (2008). Understanding the value
of features for coreference resolution. In Proceedings of
the 2008 Conference on Empirical Methods in Natural
Language Processing, Waikiki, Honolulu, Hawaii, 25-27
October 2008, pp. 294?303.
Bhattacharya, Indrajit & Lise Getoor (2009). Collective re-
lational clustering. In S. Basu, I. Davidson & K. Wagstaff
(Eds.), Constrained Clustering: Advances in Algorithms,
Theory, and Applications, pp. 221?244. Boca Raton, Flo.:
CRC Press.
Cai, Jie & Michael Strube (2010). Evaluation metrics for
end-to-end coreference resolution systems. In Proceed-
ings of the SIGdial 2010 Conference: The 11th Annual
Meeting of the Special Interest Group on Discourse and
Dialogue, Tokyo, Japan, 24?25 September 2010. To ap-
pear.
Chinchor, Nancy (2001). Message Understanding Confer-
ence (MUC) 7. LDC2001T02, Philadelphia, Penn: Lin-
guistic Data Consortium.
Chinchor, Nancy & Beth Sundheim (2003). Message Under-
standing Conference (MUC) 6. LDC2003T13, Philadel-
phia, Penn: Linguistic Data Consortium.
Culotta, Aron, Michael Wick & Andrew McCallum (2007).
First-order probabilistic models for coreference resolu-
tion. In Proceedings of Human Language Technologies
2007: The Conference of the North American Chapter of
the Association for Computational Linguistics, Rochester,
N.Y., 22?27 April 2007, pp. 81?88.
Daume? III, Hal & Daniel Marcu (2005). A large-scale ex-
ploration of effective global features for a joint entity de-
tection and tracking model. In Proceedings of the Human
Language Technology Conference and the 2005 Confer-
ence on Empirical Methods in Natural Language Process-
ing, Vancouver, B.C., Canada, 6?8 October 2005, pp. 97?
104.
Denis, Pascal & Jason Baldridge (2009). Global joint models
for coreference resolution and named entity classification.
Procesamiento del Lenguaje Natural, 42:87?96.
Klenner, Manfred (2007). Enforcing consistency on coref-
erence sets. In Proceedings of the International Confer-
ence on Recent Advances in Natural Language Process-
ing, Borovets, Bulgaria, 27?29 September 2007, pp. 323?
328.
Luo, Xiaoqiang (2005). On coreference resolution perfor-
mance metrics. In Proceedings of the Human Language
Technology Conference and the 2005 Conference on Em-
pirical Methods in Natural Language Processing, Van-
couver, B.C., Canada, 6?8 October 2005, pp. 25?32.
Luo, Xiaoqiang, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla & Salim Roukos (2004). A mention-
synchronous coreference resolution algorithm based on
the Bell Tree. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics,
Barcelona, Spain, 21?26 July 2004, pp. 136?143.
Mitchell, Alexis, Stephanie Strassel, Shudong Huang &
Ramez Zakhary (2004). ACE 2004 Multilingual Training
Corpus. LDC2005T09, Philadelphia, Penn.: Linguistic
Data Consortium.
Mitchell, Alexis, Stephanie Strassel, Mark Przybocki,
JK Davis, George Doddington, Ralph Grishman, Adam
Meyers, Ada Brunstain, Lisa Ferro & Beth Sundheim
(2003). TIDES Extraction (ACE) 2003 Multilingual
Training Data. LDC2004T09, Philadelphia, Penn.: Lin-
guistic Data Consortium.
Ng, Vincent (2008). Unsupervised models for corefer-
ence resolution. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Processing,
Waikiki, Honolulu, Hawaii, 25-27 October 2008, pp. 640?
649.
Ng, Vincent & Claire Cardie (2002). Improving machine
learning approaches to coreference resolution. In Pro-
ceedings of the 40th Annual Meeting of the Association
for Computational Linguistics, Philadelphia, Penn., 7?12
July 2002, pp. 104?111.
Nicolae, Cristina & Gabriel Nicolae (2006). BestCut: A
graph algorithm for coreference resolution. In Proceed-
ings of the 2006 Conference on Empirical Methods in Nat-
ural Language Processing, Sydney, Australia, 22?23 July
2006, pp. 275?283.
Rahman, Altaf & Vincent Ng (2009). Supervised models
for coreference resolution. In Proceedings of the 2009
Conference on Empirical Methods in Natural Language
Processing, Singapore, 6-7 August 2009, pp. 968?977.
Shi, Jianbo & Jitendra Malik (2000). Normalized cuts and
image segmentation. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, 22(8):888?905.
Soon, Wee Meng, Hwee Tou Ng & Daniel Chung Yong
Lim (2001). A machine learning approach to coreference
resolution of noun phrases. Computational Linguistics,
27(4):521?544.
Stoyanov, Veselin, Nathan Gilbert, Claire Cardie & Ellen
Riloff (2009). Conundrums in noun phrase coreference
resolution: Making sense of the state-of-the-art. In Pro-
ceedings of the Joint Conference of the 47th Annual Meet-
ing of the Association for Computational Linguistics and
the 4th International Joint Conference on Natural Lan-
guage Processing, Singapore, 2?7 August 2009, pp. 656?
664.
Versley, Yannick, Simone Paolo Ponzetto, Massimo Poesio,
Vladimir Eidelman, Alan Jern, Jason Smith, Xiaofeng
Yang & Alessandro Moschitti (2008). BART: A mod-
ular toolkit for coreference resolution. In Companion
Volume to the Proceedings of the 46th Annual Meeting
of the Association for Computational Linguistics, Colum-
bus, Ohio, 15?20 June 2008, pp. 9?12.
Vilain, Marc, John Burger, John Aberdeen, Dennis Connolly
& Lynette Hirschman (1995). A model-theoretic corefer-
ence scoring scheme. In Proceedings of the 6th Message
Understanding Conference (MUC-6), pp. 45?52. San Ma-
teo, Cal.: Morgan Kaufmann.
von Luxburg, Ulrike (2007). A tutorial on spectral clustering.
Statistics and Computing, 17(4):395?416.
Yang, Xiaofeng, Jian Su & Chew Lim Tan (2008). A twin-
candidate model for learning-based anaphora resolution.
Computational Linguistics, 34(3):327?356.
151
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 28?36,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Evaluation Metrics For End-to-End Coreference Resolution Systems
Jie Cai and Michael Strube
Heidelberg Institute for Theoretical Studies gGmbH
Schlo?-Wolfsbrunnenweg 35
69118 Heidelberg, Germany
(jie.cai|michael.strube)@h-its.org
Abstract
Commonly used coreference resolution
evaluation metrics can only be applied to
key mentions, i.e. already annotated men-
tions. We here propose two variants of the
B3 and CEAF coreference resolution eval-
uation algorithms which can be applied
to coreference resolution systems dealing
with system mentions, i.e. automatically
determined mentions. Our experiments
show that our variants lead to intuitive and
reliable results.
1 Introduction
The coreference resolution problem can be di-
vided into two steps: (1) determining mentions,
i.e., whether an expression is referential and
can take part in a coreferential relationship, and
(2) deciding whether mentions are coreferent or
not. Most recent research on coreference res-
olution simplifies the resolution task by provid-
ing the system with key mentions, i.e. already an-
notated mentions (Luo et al (2004), Denis &
Baldridge (2007), Culotta et al (2007), Haghighi
& Klein (2007), inter alia; see also the task de-
scription of the recent SemEval task on coref-
erence resolution at http://stel.ub.edu/
semeval2010-coref), or ignores an impor-
tant part of the problem by evaluating on key men-
tions only (Ponzetto & Strube, 2006; Bengtson &
Roth, 2008, inter alia). We follow here Stoyanov
et al (2009, p.657) in arguing that such evalua-
tions are ?an unrealistic surrogate for the original
problem? and ask researchers to evaluate end-to-
end coreference resolution systems.
However, the evaluation of end-to-end coref-
erence resolution systems has been inconsistent
making it impossible to compare the results. Nico-
lae & Nicolae (2006) evaluate using the MUC
score (Vilain et al, 1995) and the CEAF algorithm
(Luo, 2005) without modifications. Yang et al
(2008) use only the MUC score. Bengtson & Roth
(2008) and Stoyanov et al (2009) derive variants
from the B3 algorithm (Bagga & Baldwin, 1998).
Rahman & Ng (2009) propose their own variants
of B3 and CEAF. Unfortunately, some of the met-
rics? descriptions are so concise that they leave too
much room for interpretation. Also, some of the
metrics proposed are too lenient or are more sen-
sitive to mention detection than to coreference res-
olution. Hence, though standard corpora are used,
the results are not comparable.
This paper attempts to fill that desideratum by
analysing several variants of the B3 and CEAF al-
gorithms. We propose two new variants, namely
B3sys and CEAFsys, and provide algorithmic de-
tails in Section 2. We describe two experiments in
Section 3 showing that B3sys and CEAFsys lead to
intuitive and reliable results. Implementations of
B3sys and CEAFsys are available open source along
with extended examples1.
2 Coreference Evaluation Metrics
We discuss the problems which arise when apply-
ing the most prevalent coreference resolution eval-
uation metrics to end-to-end systems and propose
our variants which overcome those problems. We
provide detailed analyses of illustrative examples.
2.1 MUC
The MUC score (Vilain et al, 1995) counts
the minimum number of links between mentions
to be inserted or deleted when mapping a sys-
tem response to a gold standard key set. Al-
though pairwise links capture the information
in a set, they cannot represent singleton en-
tities, i.e. entities, which are mentioned only
once. Therefore, the MUC score is not suitable
for the ACE data (http://www.itl.nist.
1http://www.h-its.org/nlp/download
28
gov/iad/mig/tests/ace/), which includes
singleton entities in the keys. Moreover, the MUC
score does not give credit for separating singleton
entities from other chains. This becomes problem-
atic in a realistic system setup, when mentions are
extracted automatically.
2.2 B3
The B3 algorithm (Bagga & Baldwin, 1998) over-
comes the shortcomings of the MUC score. In-
stead of looking at the links, B3 computes preci-
sion and recall for all mentions in the document,
which are then combined to produce the final pre-
cision and recall numbers for the entire output.
For each mention, the B3 algorithm computes a
precision and recall score using equations 1 and 2:
Precision(mi) =
|Rmi ? Kmi |
|Rmi |
(1)
Recall(mi) =
|Rmi ? Kmi |
|Kmi |
(2)
where Rmi is the response chain (i.e. the system
output) which includes the mention mi, and Kmi
is the key chain (manually annotated gold stan-
dard) with mi. The overall precision and recall are
computed by averaging them over all mentions.
Since B3?s calculations are based on mentions,
singletons are taken into account. However, a
problematic issue arises when system mentions
have to be dealt with: B3 assumes the mentions in
the key and in the response to be identical. Hence,
B3 has to be extended to deal with system men-
tions which are not in the key and key mentions
not extracted by the system, so called twinless
mentions (Stoyanov et al, 2009).
2.2.1 Existing B3 variants
A few variants of the B3 algorithm for dealing with
system mentions have been introduced recently.
Stoyanov et al (2009) suggest two variants of the
B3 algorithm to deal with system mentions, B30 and
B3all
2
. For example, a key and a response are pro-
vided as below:
Key : {a b c}
Response: {a b d}
B30 discards all twinless system mentions (i.e.
mention d) and penalizes recall by setting
recallmi = 0 for all twinless key mentions (i.e.
mention c). The B30 precision, recall and F-score
2Our discussion of B30 and B3all is based on the analysis
of the source code available at http://www.cs.utah.
edu/nlp/reconcile/.
Set 1
System 1 key {a b c}
response {a b d}
P R F
B30 1.0 0.444 0.615
B3all 0.556 0.556 0.556
B3r&n 0.556 0.556 0.556
B3sys 0.667 0.556 0.606
CEAFsys 0.5 0.667 0.572
System 2 key {a b c}
response {a b d e}
P R F
B30 1.0 0.444 0.615
B3all 0.375 0.556 0.448
B3r&n 0.375 0.556 0.448
B3sys 0.5 0.556 0.527
CEAFsys 0.4 0.667 0.500
Table 1: Problems of B30
(i.e. F = 2 ? Precision?RecallPrecision+Recall ) for the example are
calculated as:
PrB30 =
1
2 ( 22 + 22 ) = 1.0
RecB30 =
1
3 ( 23 + 23 + 0)
.= 0.444
FB30 = 2 ?
1.0?0.444
1.0+0.444
.= 0.615
B3all retains twinless system mentions. It assigns
1/|Rmi | to a twinless system mention as its preci-
sion and similarly 1/|Kmi | to a twinless key men-
tion as its recall. For the same example above, the
B3all precision, recall and F-score are given by:
PrB3
all
= 13 ( 23 + 23 + 13 )
.= 0.556
RecB3
all
= 13 ( 23 + 23 + 13 )
.= 0.556
FB3
all
= 2 ? 0.556?0.5560.556+0.444
.= 0.556
Tables 1, 2 and 3 illustrate the problems with B30
and B3all. The rows labeled System give the origi-
nal keys and system responses while the rows la-
beled B30, B3all and B3sys show the performance gen-
erated by Stoyanov et al?s variants and the one
we introduce in this paper, B3sys (the row labeled
CEAFsys is discussed in Subsection 2.3).
In Table 1, there are two system outputs (i.e.
System 1 and System 2). Mentions d and e are
the twinless system mentions erroneously resolved
and c a twinless key mention. System 1 is sup-
posed to be slightly better with respect to preci-
sion, because System 2 produces one more spu-
rious resolution (i.e. for mention e ). However,
B30 computes exactly the same numbers for both
systems. Hence, there is no penalty for erroneous
coreference relations in B30, if the mentions do not
appear in the key, e.g. putting mentions d or e in
Set 1 does not count as precision errors. ? B30
is too lenient by only evaluating the correctly ex-
tacted mentions.
29
Set 1 Singletons
System 1 key {a b c}
response {a b d}
P R F
B3all 0.556 0.556 0.556
B3r&n 0.556 0.556 0.556
B3sys 0.667 0.556 0.606
CEAFsys 0.5 0.667 0.572
System 2 key {a b c}
response {a b d} {c}
P R F
B3all 0.667 0.556 0.606
B3r&n 0.667 0.556 0.606
B3sys 0.667 0.556 0.606
CEAFsys 0.5 0.667 0.572
Table 2: Problems of B3all (1)
Set 1 Singletons
System 1 key {a b}
response {a b d}
P R F
B3all 0.556 1.0 0.715
B3r&n 0.556 1.0 0.715
B3sys 0.556 1.0 0.715
CEAFsys 0.667 1.0 0.800
System 2 key {a b}
response {a b d} {i} {j} {k}
P R F
B3all 0.778 1.0 0.875
B3r&n 0.556 1.0 0.715
B3sys 0.556 1.0 0.715
CEAFsys 0.667 1.0 0.800
Table 3: Problems of B3all (2)
B3all deals well with the problem illustrated in
Table 1, the figures reported correspond to in-
tuition. However, B3all can output different re-
sults for identical coreference resolutions when
exposed to different mention taggers as shown in
Tables 2 and 3. B3all manages to penalize erro-
neous resolutions for twinless system mentions,
however, it ignores twinless key mentions when
measuring precision. In Table 2, System 1 and Sys-
tem 2 generate the same outputs, except that the
mention tagger in System 2 also extracts mention
c. Intuitively, the same numbers are expected for
both systems. However, B3all gives a higher preci-
sion to System 2, which results in a higher F-score.
B3all retains all twinless system mentions, as can
be seen in Table 3. System 2?s mention tagger tags
more mentions (i.e. the mentions i, j and k), while
both System 1 and System 2 have identical coref-
erence resolution performance. Still, B3all outputs
quite different results for precision and thus for F-
score. This is due to the credit B3all takes from un-
resolved singleton twinless system mentions (i.e.
mention i, j, k in System 2). Since the metric is ex-
pected to evaluate the end-to-end coreference sys-
tem performance rather than the mention tagging
quality, it is not satisfying to observe that B3all?s
numbers actually fluctuate when the system is ex-
posed to different mention taggers.
Rahman & Ng (2009) apply another variant, de-
noted here as B3r&n. They remove only those twin-
less system mentions that are singletons before ap-
plying the B3 algorithm. So, a system would not
be rewarded by the the spurious mentions which
are correctly identified as singletons during reso-
lution (as has been the case with B3all?s higher pre-
cision for System 2 as can be seen in Table 3).
We assume that Rahman & Ng apply a strategy
similar to B3all after the removing step (this is not
clear in Rahman & Ng (2009)). While it avoids the
problem with singleton twinless system mentions,
B3r&n still suffers from the problem dealing with
twinless key mentions, as illustrated in Table 2.
2.2.2 B3sys
We here propose a coreference resolution evalua-
tion metric, B3sys, which deals with system men-
tions more adequately (see the rows labeled B3sys
in Tables 1, 2, 3, 4 and 5). We put all twinless key
mentions into the response as singletons which en-
ables B3sys to penalize non-resolved coreferent key
mentions without penalizing non-resolved single-
ton key mentions, and also avoids the problem B3all
and B3r&n have as shown in Table 2. All twinless
system mentions which were deemed not coref-
erent (hence being singletons) are discarded. To
calculate B3sys precision, all twinless system men-
tions which were mistakenly resolved are put into
the key since they are spurious resolutions (equiv-
alent to the assignment operations in B3all), which
should be penalized by precision. Unlike B3all,
B3sys does not benefit from unresolved twinless
system mentions (i.e. the twinless singleton sys-
tem mentions). For recall, the algorithm only goes
through the original key sets, similar to B3all and
B3r&n. Details are given in Algorithm 1.
For example, a coreference resolution system
has the following key and response:
Key : {a b c}
Response: {a b d} {i j}
To calculate the precision of B3sys, the key and re-
sponse are altered to:
Keyp : {a b c} {d} {i} {j}
Responsep: {a b d} {i j} {c}
30
Algorithm 1 B3sys
Input: key sets key, response sets response
Output: precision P , recall R and F-score F
1: Discard all the singleton twinless system mentions in
response;
2: Put all the twinless annotated mentions into response;
3: if calculating precision then
4: Merge all the remaining twinless system mentions
with key to form keyp;
5: Use response to form responsep
6: Through keyp and responsep;
7: Calculate B3 precision P .
8: end if
9: if calculating recall then
10: Discard all the remaining twinless system mentions in
response to from responser;
11: Use key to form keyr
12: Through keyr and responser;
13: Calculate B3 recall R
14: end if
15: Calculate F-score F
So, the precision of B3sys is given by:
PrB3sys =
1
6 ( 23 + 23 + 13 + 12 + 12 + 1)
.= 0.611
The modified key and response for recall are:
Keyr : {a b c}
Responser: {a b} {c}
The resulting recall of B3sys is:
RecB3sys =
1
3 ( 23 + 23 + 13 )
.= 0.556
Thus the F-score number is calculated as:
FB3sys = 2 ?
0.611?0.556
0.611+0.556
.= 0.582
B3sys indicates more adequately the performance of
end-to-end coreference resolution systems. It is
not easily tricked by different mention taggers3.
2.3 CEAF
Luo (2005) criticizes the B3 algorithm for using
entities more than one time, because B3 computes
precision and recall of mentions by comparing en-
tities containing that mention. Hence Luo pro-
poses the CEAF algorithm which aligns entities in
key and response. CEAF applies a similarity met-
ric (which could be either mention based or entity
based) for each pair of entities (i.e. a set of men-
tions) to measure the goodness of each possible
alignment. The best mapping is used for calculat-
ing CEAF precision, recall and F-measure.
Luo proposes two entity based similarity met-
rics (Equation 3 and 4) for an entity pair (Ki, Rj)
originating from key, Ki, and response, Rj .
?3(Ki, Rj) = |Ki ? Rj | (3)
?4(Ki, Rj) =
2|Ki ? Rj |
|Ki| + |Rj |
(4)
3Further example analyses can be found in Appendix A.
The CEAF precision and recall are derived from
the alignment which has the best total similarity
(denoted as ?(g?)), shown in Equations 5 and 6.
Precision = ?(g
?)
?
i ?(Ri, Ri)
(5)
Recall = ?(g
?)
?
i ?(Ki,Ki)
(6)
If not specified otherwise, we apply Luo?s ?3(?, ?)
in the example illustrations. We denote the origi-
nal CEAF algorithm as CEAForig.
Detailed calculations are illustrated below:
Key : {a b c}
Response: {a b d}
The CEAForig ?3(?, ?) are given by:
?3(K1, R1) = 2 (K1 : {abc};R1 : {abd})
?3(K1,K1) = 3
?3(R1, R1) = 3
So the CEAForig evaluation numbers are:
PrCEAForig = 23 = 0.667
RecCEAForig = 23 = 0.667
FCEAForig = 2 ? 0.667?0.6670.667+0.667 = 0.667
2.3.1 Problems of CEAForig
CEAForig was intended to deal with key mentions.
Its adaptation to system mentions has not been ad-
dressed explicitly. Although CEAForig theoreti-
cally does not require to have the same number of
mentions in key and response, it still cannot be di-
rectly applied to end-to-end systems, because the
entity alignments are based on mention mappings.
As can be seen from Table 4, CEAForig fails
to produce intuitive results for system mentions.
System 2 outputs one more spurious entity (con-
taining mention i and j) than System 1 does, how-
ever, achieves a same CEAForig precision. Since
twinless system mentions do not have mappings in
key, they contribute nothing to the mapping simi-
larity. So, resolution mistakes for system mentions
are not calculated, and moreover, the precision is
easily skewed by the number of output entities.
CEAForig reports very low precision for system
mentions (see also Stoyanov et al (2009)).
2.3.2 Existing CEAF variants
Rahman & Ng (2009) briefly introduce their
CEAF variant, which is denoted as CEAFr&n
here. They use ?3(?, ?), which results in equal
CEAFr&n precision and recall figures when using
true mentions. Since Rahman & Ng?s experiments
using system mentions produce unequal precision
and recall figures, we assume that, after removing
31
Set 1 Set 2 Singletons
System 1 key {a b c}
response {a b} {c} {i} {j}
P R F
CEAForig 0.4 0.667 0.500
B3sys 1.0 0.556 0.715
CEAFsys 0.667 0.667 0.667
System 2 key {a b c}
response {a b} {i j} {c}
P R F
CEAForig 0.4 0.667 0.500
B3sys 0.8 0.556 0.656
CEAFsys 0.6 0.667 0.632
Table 4: Problems of CEAForig
Set 1 Set 2 Set 3 Singletons
System 1 key {a b c}
response {a b} {i j} {k l} {c}
P R F
CEAFr&n 0.286 0.667 0.400
B3sys 0.714 0.556 0.625
CEAFsys 0.571 0.667 0.615
System 2 key {a b c}
response {a b} {i j k l} {c}
P R F
CEAFr&n 0.286 0.667 0.400
B3sys 0.571 0.556 0.563
CEAFsys 0.429 0.667 0.522
Table 5: Problems of CEAFr&n
twinless singleton system mentions, they do not
put any twinless mentions into the other set. In the
example in Table 5, CEAFr&n does not penalize
adequately the incorrectly resolved entities con-
sisting of twinless sytem mentions. So CEAFr&n
does not tell the difference between System 1 and
System 2. It can be concluded from the examples
that the same number of mentions in key and re-
sponse is needed for computing the CEAF score.
2.3.3 CEAFsys
We propose to adjust CEAF in the same way as
we did for B3sys, resulting in CEAFsys. We put
all twinless key mentions into the response as sin-
gletons. All singleton twinless system mentions
are discarded. For calculating CEAFsys precision,
all twinless system mentions which were mistak-
enly resolved are put into the key. For computing
CEAFsys recall, only the original key sets are con-
sidered. That way CEAFsys deals adequately with
system mentions (see Algorithm 2 for details).
Algorithm 2 CEAFsys
Input: key sets key, response sets response
Output: precision P , recall R and F-score F
1: Discard all the singleton twinless system mentions in
response;
2: Put all the twinless annotated mentions into response;
3: if calculating precision then
4: Merge all the remaining twinless system mentions
with key to form keyp;
5: Use response to form responsep
6: Form Map g? between keyp and responsep
7: Calculate CEAF precision P using ?3(?, ?)
8: end if
9: if calculating recall then
10: Discard all the remaining twinless system mentions in
response to form responser;
11: Use key to form keyr
12: Form Map g? between keyr and responser
13: Calculate CEAF recall R using ?3(?, ?)
14: end if
15: Calculate F-score F
Taking System 2 in Table 4 as an example, key and
response are altered for precision:
Keyp : {a b c} {i} {j}
Responsep: {a b d} {i j} {c}
So the ?3(?, ?) are as below, only listing the best
mappings:
?3(K1, R1) = 2 (K1 : {abc};R1 : {abd})
?3(K2, R2) = 1 (K2 : {i};R2 : {ij})
?3(?, R3) = 0 (R3 : {c})
?3(R1, R1) = 3
?3(R2, R2) = 2
?3(R3, R3) = 1
The precision is thus give by:
PrCEAFsys = 2+1+03+2+1 = 0.6
The key and response for recall are:
Keyr : {a b c}
Responser: {a b} {c}
The resulting ?3(?, ?) are:
?3(K1, R1) = 2(K1 : {abc};R1 : {ab})
?3(?, R2) = 0(R2 : {c})
?3(K1,K1) = 3
?3(R1, R1) = 2
?3(R2, R2) = 1
The recall and F-score are thus calculated as:
RecCEAFsys = 23 = 0.667
FCEAFsys = 2 ? 0.6?0.6670.6+0.667 = 0.632
However, one additional complication arises
with regard to the similarity metrics used by
CEAF. It turns out that only ?3(?, ?) is suitable
for dealing with system mentions while ?4(?, ?)
produces uninituitive results (see Table 6).
?4(?, ?) computes a normalized similarity for
each entity pair using the summed number of men-
tions in the key and the response. CEAF precision
then distributes that similarity evenly over the re-
sponse set. Spurious system entities, such as the
one with mention i and j in Table 6, are not pe-
nalized. ?3(?, ?) calculates unnormalized similar-
ities. It compares the two systems in Table 6 ade-
quately. Hence we use only ?3(?, ?) in CEAFsys.
32
Set 1 Singletons
System 1 key {a b c}
response {a b} {c} {i} {j}
P R F
?4(?, ?) 0.4 0.8 0.533
?3(?, ?) 0.667 0.667 0.667
System 2 key {a b c}
response {a b} {i j} {c}
P R F
?4(?, ?) 0.489 0.8 0.607
?3(?, ?) 0.6 0.667 0.632
Table 6: Problems of ?4(?, ?)
When normalizing the similarities by the num-
ber of entities or mentions in the key (for recall)
and the response (for precision), the CEAF al-
gorithm considers all entities or mentions to be
equally important. Hence CEAF tends to compute
quite low precision for system mentions which
does not represent the system performance ade-
quately. Here, we do not address this issue.
2.4 BLANC
Recently, a new coreference resolution evalua-
tion algorithm, BLANC, has been introduced (Re-
casens & Hovy, 2010). This measure implements
the Rand index (Rand, 1971) which has been orig-
inally developed to evaluate clustering methods.
The BLANC algorithm deals correctly with sin-
gleton entities and rewards correct entities accord-
ing to the number of mentions. However, a ba-
sic assumption behind BLANC is, that the sum of
all coreferential and non-coreferential links is con-
stant for a given set of mentions. This implies that
BLANC assumes identical mentions in key and re-
sponse. It is not clear how to adapt BLANC to sys-
tem mentions. We do not address this issue here.
3 Experiments
While Section 2 used toy examples to motivate our
metrics B3sys and CEAFsys, we here report results
on two larger experiments using ACE2004 data.
3.1 Data and Mention Taggers
We use the ACE2004 (Mitchell et al, 2004) En-
glish training data which we split into three sets
following Bengtson & Roth (2008): Train (268
docs), Dev (76), and Test (107). We use two in-
house mention taggers. The first (SM1) imple-
ments a heuristic aiming at high recall. The second
(SM2) uses the J48 decision tree classifier (Wit-
ten & Frank, 2005). The number of detected men-
tions, head coverage, and accuracy on testing data
SM1 SM2
training mentions 31,370 16,081
twin mentions 13,072 14,179
development mentions 8,045 ?
twin mentions 3,371 ?
test mentions 8,387 4,956
twin mentions 4,242 4,212
head coverage 79.3% 73.3%
accuracy 57.3% 81.2%
Table 7: Mention Taggers on ACE2004 Data
are shown in Table 7.
3.2 Artificial Setting
For the artificial setting we report results on the
development data using the SM1 tagger. To illus-
trate the stability of the evaluation metrics with
respect to different mention taggers, we reduce
the number of twinless system mentions in inter-
vals of 10%, while correct (non-twinless) ones are
kept untouched. The coreference resolution sys-
tem used is the BART (Versley et al, 2008) reim-
plementation of Soon et al (2001). The results are
plotted in Figures 1 and 2.
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0 0.2 0.4 0.6 0.8 1
F-
sc
or
e 
fo
r A
CE
04
 D
ev
el
op
m
en
t D
at
a
Proportion of twinless system mentions used in the experiment
MUC
BCubedsys
BCubed0
BCubedall
BCubedng
Figure 1: Artificial Setting B3 Variants
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0 0.2 0.4 0.6 0.8 1
F-
sc
or
e 
fo
r A
CE
04
 D
ev
el
op
m
en
t D
at
a
Proportion of twinless system mentions used in the experiment
MUC
CEAFsys
CEAForig
CEAFng
Figure 2: Artificial Setting CEAF Variants
33
MUC
R Pr F
Soon (SM1) 51.7 53.1 52.4
Soon (SM2) 49.1 69.9 57.7
Table 8: Realistic Setting MUC
Omitting twinless system mentions from the
training data while keeping the number of cor-
rect mentions constant should improve the corefer-
ence resolution performance, because a more pre-
cise coreference resolution model is obtained. As
can be seen from Figures 1 and 2, the MUC-score,
B3sys and CEAFsys follow this intuition.
B30 is almost constant. It does not take twinless
mentions into account. B3all?s curve, also, has a
lower slope in comparison to B3sys and MUC (i.e.
B3all computes similar numbers for worse models).
This shows that the B3all score can be tricked by
using a high recall mention tagger, e.g. in cases
with the worse models (i.e. ones on the left side
of the figures) which have much more twinless
system mentions. The original CEAF algorithm,
CEAForig, is too sensitive to the input system
mentions making it less reliable. CEAFsys is par-
allel to B3sys. Thus both of our metrics exhibit the
same intuition.
3.3 Realistic Setting
3.3.1 Experiment 1
For the realistic setting we compare SM1 and SM2
as preprocessing components for the BART (Ver-
sley et al, 2008) reimplementation of Soon et al
(2001). The coreference resolution system with
the SM2 tagger performs better, because a better
coreference model is achieved from system men-
tions with higher accuracy.
The MUC, B3sys and CEAFsys metrics have the
same tendency when applied to systems with dif-
ferent mention taggers (Table 8, 9 and 10 and the
bold numbers are higher with a p-value of 0.05,
by a paired-t test). Since the MUC scorer does
not evaluate singleton entities, it produces too low
numbers which are not informative any more.
As shown in Table 9, B3all reports counter-
intuitive results when a system is fed with system
mentions generated by different mention taggers.
B3all cannot be used to evaluate two different end-
to-end coreference resolution systems, because the
mention tagger is likely to have bigger impact than
the coreference resolution system. B30 fails to gen-
erate the right comparison too, because it is too
B3sys B30
R Pr F R Pr F
Soon (SM2) 64.1 87.3 73.9 54.7 91.3 68.4
Bengtson 66.1 81.9 73.1 69.5 74.7 72.0
Table 11: Realistic Setting
lenient by ignoring all twinless mentions.
The CEAForig numbers in Table 10 illustrate the
big influence the system mentions have on preci-
sion (e.g. the very low precision number for Soon
(SM1)). The big improvement for Soon (SM2) is
largely due to the system mentions it uses, rather
than to different coreference models.
Both B3r&n and CEAFr&n show no serious prob-
lems in the experimental results. However, as dis-
cussed before, they fail to penalize the spurious
entities with twinless system mentions adequately.
3.3.2 Experiment 2
We compare results of Bengtson & Roth?s (2008)
system with our Soon (SM2) system. Bengtson &
Roth?s embedded mention tagger aims at high pre-
cision, generating half of the mentions SM1 gen-
erates (explicit statistics are not available to us).
Bengtson & Roth report a B3 F-score for sys-
tem mentions, which is very close to the one for
true mentions. Their B3-variant does not impute
errors of twinless mentions and is assumed to be
quite similar to the B30 strategy.
We integrate both the B30 and B3sys variants into
their system and show results in Table 11 (we can-
not report significance, because we do not have ac-
cess to results for single documents in Bengtson &
Roth?s system). It can be seen that, when different
variants of evaluation metrics are applied, the per-
formance of the systems vary wildly.
4 Conclusions
In this paper, we address problems of commonly
used evaluation metrics for coreference resolution
and suggest two variants for B3 and CEAF, called
B3sys and CEAFsys. In contrast to the variants
proposed by Stoyanov et al (2009), B3sys and
CEAFsys are able to deal with end-to-end systems
which do not use any gold information. The num-
bers produced by B3sys and CEAFsys are able to
indicate the resolution performance of a system
more adequately, without being tricked easily by
twisting preprocessing components. We believe
that the explicit description of evaluation metrics,
as given in this paper, is a precondition for the re-
34
B3sys B30 B3all B3r&n
R Pr F R Pr F R Pr F R Pr F
Soon (SM1) 65.7 76.8 70.8 57.0 91.1 70.1 65.1 85.8 74.0 65.1 78.7 71.2
Soon (SM2) 64.1 87.3 73.9 54.7 91.3 68.4 64.3 87.1 73.9 64.3 84.9 73.2
Table 9: Realistic Setting B3 Variants
CEAFsys CEAForig CEAFr&n
R Pr F R Pr F R Pr F
Soon (SM1) 66.4 61.2 63.7 62.0 39.9 48.5 62.1 59.8 60.9
Soon (SM2) 67.4 65.2 66.3 60.0 56.6 58.2 60.0 66.2 62.9
Table 10: Realistic Setting CEAF Variants
liabe comparison of end-to-end coreference reso-
lution systems.
Acknowledgements. This work has been
funded by the Klaus Tschira Foundation, Hei-
delberg, Germany. The first author has been
supported by a HITS PhD. scholarship. We
would like to thank ?Eva Mu?jdricza-Maydt for
implementing the mention taggers.
References
Bagga, Amit & Breck Baldwin (1998). Algorithms for scor-
ing coreference chains. In Proceedings of the 1st Inter-
national Conference on Language Resources and Evalua-
tion, Granada, Spain, 28?30 May 1998, pp. 563?566.
Bengtson, Eric & Dan Roth (2008). Understanding the value
of features for coreference resolution. In Proceedings of
the 2008 Conference on Empirical Methods in Natural
Language Processing, Waikiki, Honolulu, Hawaii, 25-27
October 2008, pp. 294?303.
Culotta, Aron, Michael Wick & Andrew McCallum (2007).
First-order probabilistic models for coreference resolu-
tion. In Proceedings of Human Language Technologies
2007: The Conference of the North American Chapter of
the Association for Computational Linguistics, Rochester,
N.Y., 22?27 April 2007, pp. 81?88.
Denis, Pascal & Jason Baldridge (2007). Joint determination
of anaphoricity and coreference resolution using integer
programming. In Proceedings of Human Language Tech-
nologies 2007: The Conference of the North American
Chapter of the Association for Computational Linguistics,
Rochester, N.Y., 22?27 April 2007, pp. 236?243.
Haghighi, Aria & Dan Klein (2007). Unsupervised coref-
erence resolution in a nonparametric Bayesian model. In
Proceedings of the 45th Annual Meeting of the Association
for Computational Linguistics, Prague, Czech Republic,
23?30 June 2007, pp. 848?855.
Luo, Xiaoqiang (2005). On coreference resolution perfor-
mance metrics. In Proceedings of the Human Language
Technology Conference and the 2005 Conference on Em-
pirical Methods in Natural Language Processing, Vancou-
ver, B.C., Canada, 6?8 October 2005, pp. 25?32.
Luo, Xiaoqiang, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla & Salim Roukos (2004). A mention-
synchronous coreference resolution algorithm based on
the Bell Tree. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics,
Barcelona, Spain, 21?26 July 2004, pp. 136?143.
Mitchell, Alexis, Stephanie Strassel, Shudong Huang &
Ramez Zakhary (2004). ACE 2004 Multilingual Training
Corpus. LDC2005T09, Philadelphia, Penn.: Linguistic
Data Consortium.
Nicolae, Cristina & Gabriel Nicolae (2006). BestCut: A
graph algorithm for coreference resolution. In Proceed-
ings of the 2006 Conference on Empirical Methods in Nat-
ural Language Processing, Sydney, Australia, 22?23 July
2006, pp. 275?283.
Ponzetto, Simone Paolo & Michael Strube (2006). Exploiting
semantic role labeling, WordNet and Wikipedia for coref-
erence resolution. In Proceedings of the Human Language
Technology Conference of the North American Chapter of
the Association for Computational Linguistics, New York,
N.Y., 4?9 June 2006, pp. 192?199.
Rahman, Altaf & Vincent Ng (2009). Supervised models for
coreference resolution. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language Pro-
cessing, Singapore, 6-7 August 2009, pp. 968?977.
Rand, William R. (1971). Objective criteria for the evaluation
of clustering methods. Journal of the American Statistical
Association, 66(336):846?850.
Recasens, Marta & Eduard Hovy (2010). BLANC: Imple-
menting the Rand index for coreference evaluation. Sub-
mitted.
Soon, Wee Meng, Hwee Tou Ng & Daniel Chung Yong
Lim (2001). A machine learning approach to coreference
resolution of noun phrases. Computational Linguistics,
27(4):521?544.
Stoyanov, Veselin, Nathan Gilbert, Claire Cardie & Ellen
Riloff (2009). Conundrums in noun phrase coreference
resolution: Making sense of the state-of-the-art. In Pro-
ceedings of the Joint Conference of the 47th Annual Meet-
ing of the Association for Computational Linguistics and
the 4th International Joint Conference on Natural Lan-
guage Processing, Singapore, 2?7 August 2009, pp. 656?
664.
Versley, Yannick, Simone Paolo Ponzetto, Massimo Poesio,
Vladimir Eidelman, Alan Jern, Jason Smith, Xiaofeng
Yang & Alessandro Moschitti (2008). BART: A modular
toolkit for coreference resolution. In Companion Volume
to the Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics, Columbus, Ohio,
15?20 June 2008, pp. 9?12.
Vilain, Marc, John Burger, John Aberdeen, Dennis Connolly
& Lynette Hirschman (1995). A model-theoretic corefer-
ence scoring scheme. In Proceedings of the 6th Message
Understanding Conference (MUC-6), pp. 45?52. San Ma-
teo, Cal.: Morgan Kaufmann.
Witten, Ian H. & Eibe Frank (2005). Data Mining: Practical
Machine Learning Tools and Techniques (2nd ed.). San
Francisco, Cal.: Morgan Kaufmann.
Yang, Xiaofeng, Jian Su & Chew Lim Tan (2008). A twin-
candidate model for learning-based anaphora resolution.
Computational Linguistics, 34(3):327?356.
35
A B3sys Example Output
Here, we provide additional examples for analyzing the behavior of B3sys where we systematically vary
system outputs. Since we proposed B3sys for dealing with end-to-end systems, we consider only examples
also containing twinless mentions. The systems in Table 12 and 14 generate different twinless key
mentions while keeping the twinless system mentions untouched. In Table 13 and 15, the number of
twinless system mentions changes through different responses and the number of twinless key mentions
is fixed.
In Table 12, B3sys recall goes up when more key mentions are resolved into the correct set. And the
precision stays the same, because there is no change in the number of the erroneous resolutoins (i.e. the
spurious cluster with mentions i and j). For the examples in Tables 13 and 15, B3sys gives worse precision
to the outputs with more spurious resolutions, and the same recall if the systems resolve key mentions in
the same way. Since the set of key mentions intersects with the set of twinless system mentions in Table
14, we do not have an intuitive explanation for the decrease in precision from response1 to response4.
However, both the F-score and the recall still show the right tendency.
Set 1 Set 2 B3sys
key {a b c d e} P R F
response1 {a b} {i j} 0.857 0.280 0.422
response2 {a b c} {i j} 0.857 0.440 0.581
response3 {a b c d} {i j} 0.857 0.68 0.784
response4 {a b c d e} {i j} 0.857 1.0 0.923
Table 12: Analysis of B3sys 1
Set 1 Set 2 B3sys
key {a b c d e} P R F
response1 {a b c} {i j} 0.857 0.440 0.581
response2 {a b c} {i j k} 0.75 0.440 0.555
response3 {a b c} {i j k l} 0.667 0.440 0.530
response4 {a b c} {i j k l m} 0.6 0.440 0.508
Table 13: Analysis of B3sys 2
Set 1 B3sys
key {a b c d e} P R F
response1 {a b i j} 0.643 0.280 0.390
response2 {a b c i j} 0.6 0.440 0.508
response3 {a b c d i j} 0.571 0.68 0.621
response4 {a b c d e i j} 0.551 1.0 0.711
Table 14: Analysis of B3sys 3
Set 1 B3sys
key {a b c d e} P R F
response1 {a b c i j} 0.6 0.440 0.508
response2 {a b c i j k} 0.5 0.440 0.468
response3 {a b c i j k l} 0.429 0.440 0.434
response4 {a b c i j k l m} 0.375 0.440 0.405
Table 15: Analysis of B3sys 4
36
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 56?60,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Unrestricted Coreference Resolution via Global Hypergraph Partitioning
Jie Cai and ?Eva Mu?jdricza-Maydt and Michael Strube
Natural Language Processing Group
Heidelberg Institute for Theoretical Studies gGmbH
Heidelberg, Germany
(jie.cai|eva.mujdriczamaydt|michael.strube)@h-its.org
Abstract
We present our end-to-end coreference res-
olution system, COPA, which implements a
global decision via hypergraph partitioning.
In constrast to almost all previous approaches,
we do not rely on separate classification and
clustering steps, but perform coreference res-
olution globally in one step. COPA represents
each document as a hypergraph and partitions
it with a spectral clustering algorithm. Various
types of relational features can be easily incor-
porated in this framwork. COPA has partici-
pated in the open setting of the CoNLL shared
task on modeling unrestricted coreference.
1 Introduction
Coreference resolution is the task of grouping men-
tions of entities into sets so that all mentions in
one set refer to the same entity. Most recent ap-
proaches to coreference resolution divide this task
into two steps: (1) a classification step which de-
termines whether a pair of mentions is coreferent or
which outputs a confidence value, and (2) a cluster-
ing step which groups mentions into entities based
on the output of step 1.
In this paper we present an end-to-end corefer-
ence resolution system, COPA, which avoids the di-
vision into two steps and instead performs a global
decision in one step. The system presents a doc-
ument as a hypergraph, where the vertices denote
mentions and the edges denote relational features
between mentions. Coreference resolution is then
performed globally in one step by partitioning the
hypergraph into subhypergraphs so that all mentions
in one subhypergraph refer to the same entity (Cai
and Strube, 2010). COPA assigns edge weights by
applying simple descriptive statistics on the tranin-
ing data. Since COPA does not need to learn an
explicit model, we used only 30% of the CoNLL
shared task training data. We did this not for effi-
ciency reasons, only for convenience.
While COPA has been developed originally to
perform coreference resolution on MUC and ACE
data (Cai and Strube, 2010), the move to the
OntoNotes data (Weischedel et al, 2011) required
mainly to update the mention detector and the fea-
ture set. Since several off-the-shelf preprocessing
components are used, COPA participated in the open
setting of the CoNLL shared task on modeling unre-
stricted coreference (Pradhan et al, 2011). We did
not make extensive use of information beyond infor-
mation from the closed class setting.
2 Preprocessing
COPA is implemented on top of the BART-toolkit
(Versley et al, 2008). Documents are transformed
into the MMAX2-format (Mu?ller and Strube, 2006)
which allows for easy visualization and (linguis-
tic) debugging. Each document is stored in several
XML-files representing different layers of annota-
tions. These annotations are created by a pipeline
of preprocessing components. We use the Stan-
ford MaxentTagger (Toutanova et al, 2003) for part-
of-speech tagging, and the Stanford Named En-
tity Recognizer (Finkel et al, 2005) for annotat-
ing named entities. In order to derive syntactic
information, we use the Charniak/Johnson rerank-
ing parser (Charniak and Johnson, 2005) com-
56
bined with a constituent-to-dependency conversion
Tool (http://nlp.cs.lth.se/software/
treebank_converter). The preprocessing
models are not trained on CoNLL data, so we only
participated in the open task.
We have implemented an in-house mention detec-
tor, which makes use of the parsing output, the part-
of-speech tags, as well as the chunks from the Yam-
cha Chunker (Kudoh and Matsumoto, 2000). For
the OntoNotes data, the mention detector annotates
the biggest noun phrase spans.
3 COPA: Coreference Partitioner
The COPA system consists of modules which derive
hyperedges from features and assign edge weights
indicating a positive correlation with the coreference
relation, and resolution modules which create a hy-
pergraph representation for the testing data and per-
form partitioning to produce subhypergraphs, each
of which represents an entity.
3.1 HyperEdgeCreator
COPA needs training data only for computing the
hyperedge weights. Hyperedges represent features.
Each hyperedge corresponds to a feature instance
modeling a simple relation between two or more
mentions. This leads to initially overlapping sets of
mentions. Hyperedges are assigned weights which
are calculated on the training data as the percentage
of the initial edges being in fact coreferent. Due to
the simple strategy of assigning edge weights, only
a reasonable size of training data is needed.
3.2 Coreference Resolution Modules
Unlike pairwise models, COPA processes a docu-
ment globally in one step, taking care of the pref-
erence information among all the mentions simul-
taneously and clustering them into sets directly. A
document is represented as a single hypergraph with
multiple edges. The hypergraph resolver partitions
the hypergraph into several sub-hypergraphs, each
corresponding to one set of coreferent mentions.
3.2.1 HGModelBuilder
A single document is represented in a hypergraph
with basic relational features. Each hyperedge in a
graph corresponds to an instance of one of those fea-
tures with the weight assigned by the HyperEdge-
Learner. Instead of connecting nodes with the tar-
get relation as usually done in graph models, COPA
builds the graph directly out of low dimensional fea-
tures without assuming a distance metric.
3.2.2 HGResolver
In order to partition the hypergraph we adopt a
spectral clustering algorithm (Agarwal et al, 2005).
All experimental results are obtained using symmet-
ric Laplacians (Lsym) (von Luxburg, 2007).
We apply the recursive variant of spectral clus-
tering, recursive 2-way partitioning (R2 partitioner)
(Cai and Strube, 2010). This method does not need
any information about the number of target sets (the
number k of clusters). Instead a stopping criterion
?? has to be provided which is adjusted on develop-
ment data.
3.3 Complexity of HGResolver
Since edge weights are assigned using simple de-
scriptive statistics, the time HGResolver needs for
building the graph Laplacian matrix is not substan-
tial. For eigensolving, we use an open source library
provided by the Colt project1which implements a
Householder-QL algorithm to solve the eigenvalue
decomposition. When applied to the symmetric
graph Laplacian, the complexity of the eigensolv-
ing is given by O(n3), where n is the number of
mentions in a hypergraph. Since there are only a
few hundred mentions per document in our data, this
complexity is not an issue. Spectral clustering gets
problematic when applied to millions of data points.
4 Features
In our system, features are represented as types of
hyperedges. Any realized edge is an instance of the
corresponding edge type. All instances derived from
the same type have the same weight, but they may
get reweighed by the distance feature (see Cai and
Strube (2010)). We use three types of features:
negative: prevent edges between mentions;
positive: generate strong edges between mentions;
weak: add edges to an existing graph without intro-
ducing new vertices;
1http://acs.lbl.gov/
?
hoschek/colt/
57
In the following subsections we describe the fea-
tures used in our experiments. Some of the fea-
tures described in Cai and Strube (2010) had to be
changed to cope with the OntoNotes data. We also
introduced a few more features (in particular in or-
der to deal with the dialogue section in the data).
4.1 Negative Features
Negative features describe pairwise relations which
are most likely not coreferent. While we imple-
mented this information as weak positive features in
Cai and Strube (2010), here we apply these features
before graph construction as global variables.
When two mentions are connected by a negative
relation, no edges will be built between them in the
graph. For instance, no edges are allowed between
the mention Hillary Clinton and the mention he due
to incompatible gender.
(1) N Gender, (2) N Number: Two mentions do
not agree in gender or number.
(3) N SemanticClass: Two mentions do not
agree in semantic class (only the Object, Date and
Person top categories derived from WordNet (Fell-
baum, 1998) are used).
(4) N Mod: Two mentions have the same syntac-
tic heads, and the anaphor has a pre-modifier which
does not occur in the antecedent and does not con-
tradict the antecedent.
(5) N DSPrn: Two first person pronouns in direct
speeches assigned to different speakers.
(6) N ContraSubjObj: Two mentions are in the
subject and object positions of the same verb, and
the anaphor is a non-possesive pronoun.
4.2 Positive Features
The majority of well studied coreference features
(e.g. Stoyanov et al (2009)) are actually positive
coreference indicators. In our system, the mentions
which participate in positive relations are included
in the graph representation.
(7) StrMatch Npron & (8) StrMatch Pron: Af-
ter discarding stop words, if the strings of mentions
completely match and are not pronouns, they are put
into edges of the StrMatch Npron type. When the
matched mentions are pronouns, they are put into
the StrMatch Pron type edges.
(9) Alias: After discarding stop words, if men-
tions are aliases of each other (i.e. proper names with
partial match, full names and acronyms, etc.).
(10) HeadMatch: If the syntactic heads of men-
tions match.
(11) Nprn Prn: If the antecedent is not a pro-
noun and the anaphor is a pronoun. This feature is
restricted to a sentence distance of 2. Though it is
not highly weighted, it is crucial for integrating pro-
nouns into the graph.
(12) Speaker12Prn: If the speaker of the second
person pronoun is talking to the speaker of the first
person pronoun. The mentions contain only first or
second person pronouns.
(13) DSPrn: If one of the mentions is the subject
of a speak verb, and other mentions are first person
pronouns within the corresponding direct speech.
(14) ReflexivePrn: If the anaphor is a reflexive
pronoun, and the antecedent is subject of the sen-
tence.
(15) PossPrn: If the anaphor is a possesive pro-
noun, and the antecedent is the subject of the sen-
tence or the subclause.
(16) GPEIsA: If the antecedent is a Named Entity
of GPE entity type (i.e. one of the ACE entity type
(NIST, 2004)), and the anaphor is a definite expres-
sion of the same type.
(17) OrgIsA: If the antecedent is a Named En-
tity of Organization entity type, and the anaphor is a
definite expression of the same type.
4.3 Weak Features
Weak features are weak coreference indicators. Us-
ing them as positive features would introduce too
much noise to the graph (i.e. a graph with too many
singletons). We apply weak features only to men-
tions already integrated in the graph, so that weak
information provides it with a richer structure.
(18) W Speak: If mentions occur with a word
meaning to say in a window size of two words.
(19) W Subject: If mentions are subjects.
(20) W Synonym: If mentions are synonymous
as indicated by WordNet.
5 Results
We submitted COPA?s results to the open setting
in the CoNLL shared task on modeling unrestricted
coreference. We used only 30% of the training data
58
(randomly selected) and the 20 features described in
Section 4.
The stopping criterion ?? (see Section 3) is tuned
on development data to optimize the final corefer-
ence scores. A value of 0.06 is chosen for testing.
COPA?s results on development set (which con-
sists of 202 files) and on testing set are displayed in
Table 1 and Table 2 respectively. The Overall num-
bers in both tables are the average scores of MUC,
BCUBED and CEAF (E).
Metric R P F1
MUC 52.69 57.94 55.19
BCUBED 64.26 73.39 68.52
CEAF (M) 54.44 54.44 54.44
CEAF (E) 45.73 40.92 43.19
BLANC 69.78 75.26 72.13
Overall 55.63
Table 1: COPA?s results on CoNLL development set
Metric R P F1
MUC 56.73 58.90 57.80
BCUBED 64.60 71.03 67.66
CEAF (M) 53.37 53.37 53.37
CEAF (E) 42.71 40.68 41.67
BLANC 69.77 73.96 71.62
Overall 55.71
Table 2: COPA?s results on CoNLL testing set
6 Mention Detection Errors
As described in Section 2, our mention detection is
based on automatically extracted information, such
as syntactic parses and basic noun phrase chunks.
Since there is no minimum span information pro-
vided in the OntoNotes data (in constrast to the pre-
vious standard corpus, ACE), exact mention bound-
ary detection is required. A lot of the spurious
mentions in our system are generated due to mis-
matches of ending or starting punctuations, and the
OntoNotes annotation is also not consistent in this
regard. Our current mention detector does not ex-
tract verb phrases. Therefore it misses all the Event
mentions in the OntoNotes corpus.
We are planning to include idiomatic expression
identification into our mention detector, which will
help to avoid detecting a lot of spurious mentions,
such as God in the phrase for God?s sake.
7 COPA Errors
Besides the fact that the current COPA is not resolv-
ing any event coreferences, our in-house mention de-
tector performs weakly in extracting date mentions
too. As a result, the system outputs several spuri-
ous coreference sets, for instance a set containing
the September from the mention 15th September.
A large amount of the recall loss in our system is
due to the lack of the world knowledge. For exam-
ple, COPA does not resolve the mention the Europe
station correctly into the entity Radio Free Europe,
for it has no knowledge that the entity is a station.
Some more difficult coreference phenomena in
OntoNotes data might require a reasoning mecha-
nism. To be able to connect the mention the vic-
tim with the mention the groom?s brother, the event
of the brother being killed needs to be intepreted by
the system.
We also observed from the experiments that the
resolution of the it mentions are quite inaccurate.
Although our mention detector takes care of dis-
carding pleonastic it?s, there are still a lot of them
left which introduce wrong coreference sets. Since
the it?s do not contain enough information by them-
selves, more features exploring their local syntax are
necessary.
8 Conclusions
In this paper we described a coreference resolution
system, COPA, which implements a global decision
in one step via hypergraph partitioning. COPA?s
hypergraph-based strategy is a general preference
model, where the preference for one mention de-
pends on information on all other mentions.
The system implements three types of relational
features ? negative, positive and weak features, and
assigns the edge weights according to the statitics
from the training data. Since the weights are robust
with respect to the amount of training data we used
only 30% of the training data.
Acknowledgements. This work has been funded
by the Klaus Tschira Foundation, Heidelberg, Ger-
many. The first author has been supported by a HITS
PhD. scholarship.
59
References
Sameer Agarwal, Jonwoo Lim, Lihi Zelnik-Manor, Pietro
Perona, David Kriegman, and Serge Belongie. 2005.
Beyond pairwise clustering. In Proceedings of the
IEEE Computer Society Conference on Computer Vi-
sion and Pattern Recognition (CVPR?05), volume 2,
pages 838?845.
Jie Cai and Michael Strube. 2010. End-to-end coref-
erence resolution via hypergraph partitioning. In
Proceedings of the 23rd International Conference on
Computational Linguistics, Beijing, China, 23?27 Au-
gust 2010, pages 143?151.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEent discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics,
Ann Arbor, Mich., 25?30 June 2005, pages 173?180.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
Mass.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting
of the Association for Computational Linguistics, Ann
Arbor, Mich., 25?30 June 2005, pages 363?370.
Taku Kudoh and Yuji Matsumoto. 2000. Use of Support
Vector Machines for chunk identification. In Proceed-
ings of the 4th Conference on Computational Natural
Language Learning, Lisbon, Portugal, 13?14 Septem-
ber 2000, pages 142?144.
Christoph Mu?ller and Michael Strube. 2006. Multi-level
annotation of linguistic data with MMAX2. In Sabine
Braun, Kurt Kohn, and Joybrato Mukherjee, editors,
Corpus Technology and Language Pedagogy: New Re-
sources, New Tools, New Methods, pages 197?214. Pe-
ter Lang: Frankfurt a.M., Germany.
NIST. 2004. The ACE evaluation plan:
Evaluation of the recognition of ACE en-
tities, ACE relations and ACE events.
http://www.itl.nist.gov/iad/mig//tests/ace/2004/doc/
ace04-evalplan-v7.pdf.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. CoNLL-2011 Shared Task: Modeling unre-
stricted coreference in OntoNotes. In Proceedings of
the Shared Task of 15th Conference on Computational
Natural Language Learning, Portland, Oreg., 23?24
June 2011.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase coref-
erence resolution: Making sense of the state-of-the-
art. In Proceedings of the Joint Conference of the 47th
Annual Meeting of the Association for Computational
Linguistics and the 4th International Joint Conference
on Natural Language Processing, Singapore, 2?7 Au-
gust 2009, pages 656?664.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of the Human Language Technology Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics, Edmonton, Al-
berta, Canada, 27 May ?1 June 2003, pages 252?259.
Yannick Versley, Simone Paolo Ponzetto, Massimo Poe-
sio, Vladimir Eidelman, Alan Jern, Jason Smith,
Xiaofeng Yang, and Alessandro Moschitti. 2008.
BART: A modular toolkit for coreference resolution.
In Companion Volume to the Proceedings of the 46th
Annual Meeting of the Association for Computational
Linguistics, Columbus, Ohio, 15?20 June 2008, pages
9?12.
Ulrike von Luxburg. 2007. A tutorial on spectral cluster-
ing. Statistics and Computing, 17(4):395?416.
Ralph Weischedel, Martha Palmer, Mitchell Marcus, Ed-
uard Hovy, Sameer Pradhan, Lance Ramshaw, Ni-
anwen Xue, Ann Taylor, Jeff Kaufman, Michelle
Franchini, Mohammed El-Bachouti, Robert Belvin,
and Ann Houston. 2011. OntoNotes release 4.0.
LDC2011T03, Philadelphia, Penn.: Linguistic Data
Consortium.
60
Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 100?106,
Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational Linguistics
A Multigraph Model for Coreference Resolution
Sebastian Martschat, Jie Cai, Samuel Broscheit, E?va Mu?jdricza-Maydt, Michael Strube
Natural Language Processing Group
Heidelberg Institute for Theoretical Studies gGmbH
Heidelberg, Germany
(sebastian.martschat|jie.cai|michael.strube)@h-its.org
Abstract
This paper presents HITS? coreference reso-
lution system that participated in the CoNLL-
2012 shared task on multilingual unrestricted
coreference resolution. Our system employs a
simple multigraph representation of the rela-
tion between mentions in a document, where
the nodes correspond to mentions and the
edges correspond to relations between the
mentions. Entities are obtained via greedy
clustering. We participated in the closed tasks
for English and Chinese. Our system ranked
second in the English closed task.
1 Introduction
Coreference resolution is the task of determining
which mentions in a text refer to the same entity.
This paper describes HITS? system for the CoNLL-
2012 Shared Task on multilingual unrestricted coref-
erence resolution, where the goal is to build a system
for coreference resolution in an end-to-end multilin-
gual setting (Pradhan et al, 2012). We participated
in the closed tasks for English and Chinese and fo-
cused on English. Our system ranked second in the
English closed task.
Being conceptually similar to and building upon
Cai et al (2011b), our system is based on a directed
multigraph representation of a document. A multi-
graph is a graph where two nodes can be connected
by more than one edge. In our model, nodes rep-
resent mentions and edges are built from relations
between the mentions. The entities to be inferred
correspond to clusters in the multigraph.
Our model allows for directly representing any
kind of relations between pairs of mentions in a
graph structure. Inference over this graph can har-
ness structural properties and the rich set of encoded
relations. In order to serve as a basis for further
work, the components of our system were designed
to work as simple as possible. Therefore our system
relies mostly on local information between pairs of
mentions.
2 Architecture
Our system is implemented on top of the BART
toolkit (Versley et al, 2008). To compute the coref-
erence clusters in a document, we first extract a set
of mentions M = {m1, . . . ,mn} ordered according
to their position in the text (Section 2.1). We then
build a directed multigraph where the set of nodes
is M and edges are induced by relations between
mentions (Section 2.4). The relations we use in our
system are coreference indicators like string match-
ing or alias (Section 3). For every relation R, we
compute a weight wR using the training data (Sec-
tion 2.3). We then assign the weight wR to any edge
that is induced by the relation R. Depending on dis-
tance and connectivity properties of the graph the
weights may change (Section 2.4.1). Given the con-
structed graph with edge weights, we go through the
mentions according to their position in the text and
perform greedy clustering (Section 2.6). For Chi-
nese, we employ spectral clustering (Section 2.5) as
adopted in Cai et al (2011b) before the greedy clus-
tering step to reduce the number of candidate an-
tecedents for a mention. The components of our sys-
tem are described in the following subsections.
100
2.1 Mention Extraction
Noun phrases are extracted from the provided parse
and named entity annotation layers. For embedded
mentions with the same head, we only keep the men-
tion with the largest span.
2.1.1 English
For English we identify eight different mention
types: common noun, proper noun, personal pro-
noun, demonstrative pronoun, possessive pronoun,
coordinated noun phrase, quantifying noun phrase
(some of ..., 17 of ...) and quantified noun phrase
(the armed men in one of the armed men). The head
for a common noun or a quantified noun is com-
puted using the SemanticHeadFinder from the Stan-
ford Parser1. The head for a proper noun starts at
the first token tagged as a noun until a punctuation,
preposition or subclause is encountered. Coordina-
tions have the CC tagged token as head and quanti-
fying noun phrases have the quantifier as head.
In a postprocessing step we filter out adjectival
use of nations and named entities with semantic
class Money, Percent or Cardinal. We discard men-
tions whose head is embedded in another mention?s
head. Pleonastic pronouns are identified and dis-
carded via a modified version of the patterns used
by Lee et al (2011).
2.1.2 Chinese
For Chinese we detect four mention types: com-
mon noun, proper noun, pronoun and coordination.
The head detection for Chinese is provided by the
SunJurafskyChineseHeadFinder from the Standford
Parser, except for proper nouns whose head is set to
the mention?s rightmost token.
The remaining processing is similar to the men-
tion detection for English.
2.2 Preprocessing
We extract the information in the provided an-
notation layers and transform the predicted con-
stituent parse trees into dependency parse trees.
We work with two different dependency represen-
tations, one obtained via the converter implemented
1http://nlp.stanford.edu/software/
lex-parser.shtml
in Stanford?s NLP suite2, the other using LTH?s
constituent-to-dependency conversion tool3. For
pronouns, we determine number and gender using
tables containing a mapping of pronouns to their
gender and number.
2.2.1 English
For English, number and gender for common
nouns are computed via a comparison of head
lemma to head and using the number and gender
data of Bergsma and Lin (2006). Quantified noun
phrases are always plural. We compute semantic
classes via a WordNet (Fellbaum, 1998) lookup.
2.2.2 Chinese
For Chinese, we simply determine number and
gender by searching for the corresponding desig-
nators, since plural mentions mostly end with ?,
while ?? (sir) and ?? (lady) often suggest gen-
der information. To identify demonstrative and defi-
nite noun phrases, we check whether they start with
a definite/demonstrative indicator (e.g. ? (this) or
? (that)). We use lists of named entities extracted
from the training data to determine named entities
and their semantic class in development and testing
data.
2.3 Computing Weights for Relations
We compute weights for relations using simple de-
scriptive statistics on training documents. Since this
is a robust approach to learning weights for the type
of graph model we employ (Cai et al, 2011b; Cai
et al, 2011a), we use only a fraction of the available
training data. We took a random subset consisting of
around 20% for English and 15% for Chinese of the
training data. For every document in this set and ev-
ery relation R, we go through the set M of extracted
mentions and compute for every pair (mi,mj) with
i > j whether R holds for this pair. The weight wR
for R is then the number of coreferent pairs with R
divided by the number of all pairs with R.
2.4 Graph Construction
The set of relations we employ consists of two sub-
sets: negative relations R? which enforce that no
2http://nlp.stanford.edu/software/
stanford-dependencies.shtml
3http://nlp.cs.lth.se/software/treebank_
converter/
101
edge is built between two mentions, and positive re-
lations R+ that induce edges. Again, we go through
M in a left-to-right fashion. If for two mentions mi,
mj with i > j a negative relation R? holds, no edge
between mi and mj can be built. Otherwise we add
an edge from mi to mj for every positive relation
R+ such that R+(mi,mj) is true. The structure ob-
tained by this construction is a directed multigraph.
We handle copula relations similar to Lee et al
(2011): if mi is this and the pair (mi,mj) is in a
copula relation (like This is the World), we remove
mj and replace mj in all edges involving it by mi.
For Chinese, we handle ?role appositives? as intro-
duced by Haghighi and Klein (2009) analogously.
2.4.1 Assigning Weights to Edges
Initially, any edge (mi,mj) induced by the rela-
tion R has the weight wR computed as described
in Section 2.3. If R is a transitive relation, we di-
vide the weight by the number of mentions con-
nected by this relation. This corresponds to the way
edge weights are assigned during the spectral em-
bedding in Cai et al (2011b). If R is a relation sen-
sitive to distance like compatibility between a com-
mon/proper noun and a pronoun, the weight is al-
tered according to the distance between mi and mj .
2.4.2 An Example
We demonstrate the graph construction by a sim-
ple example. Consider a document consisting of the
following three sentences.
Barack Obama and Nicolas Sarkozy met
in Toronto yesterday. They discussed the
financial crisis. Sarkozy left today.
Let us assume that our system identifies Barack
Obama (m1), Nicolas Sarkozy (m2), Barack Obama
and Nicolas Sarkozy (m3), They (m4) and Sarkozy
(m5) as mentions. We consider these mentions and
the relations N Number, P Nprn Prn, P Alias and
P Subject described in Section 3. The graph con-
structed according to the algorithm described in this
section is displayed in Figure 1.
Observe the effect of the negative relation
N Number: while P Nprn Prn holds for the pair
Barack Obama (m1) and They (m4), the mentions
do not agree in number. Hence N Number holds for
this pair and no edge from m4 to m1 can be built.
m2 m5
m3 m4
P Alias
P Nprn Prn
P Subject
Figure 1: An example graph. Nodes represent mentions,
edges are induced by relations between the mentions.
2.5 Spectral Clustering
For Chinese we apply spectral clustering before the
final greedy clustering phase. In order to be able to
apply spectral clustering, we make the graph undi-
rected and merge parallel edges into one edge, sum-
ming up all weights. Due to the way edge weights
are computed, the resulting undirected simple graph
corresponds to the graph Cai et al (2011b) use as
input to the spectral clustering algorithm. Spectral
clustering is now performed as in Cai et al (2011b).
2.6 Greedy Clustering
To describe our clustering algorithm, we use some
additional terminology: if there exists an edge from
m to n we say that m is a parent of n and that n is a
child of m.
In the last step, we go through the mentions from
left to right. Let mi be the mention in focus. For
English, we consider all children of mi as possible
antecedents. For Chinese we restrict the possible an-
tecedents to all children that are in the same cluster
obtained by spectral clustering.
If mi is a pronoun, we determine mj such that
the sum over all weights of edges from mi to mj is
maximized. We then assign mi and mj to the same
entity. In English, if mi is a parent of a noun phrase
m that embeds mj , we instead assign mi and m to
the same entity.
For Chinese, all other noun phrases are assigned
to the same entity as all their children in the cluster
obtained by spectral clustering. For English, we are
more restrictive: definites and demonstratives are as-
signed to the same cluster as their closest (according
to the position of the mentions in the text) child.
Negative relations may also be applied as con-
straints in this phase. Before assigning mi to the
same entity as a set of mentions C, we check for
102
every m ? C and every negative relation R?
that we want to incorporate as a constraint whether
R?(mi,m) holds. If yes, we do not assign mi to the
same entity as the mentions in C.
2.7 Complexity
Our algorithms for weight computation, graph con-
struction and greedy clustering look at all pairs of
mentions in a document and perform simple calcu-
lations, which leads to a time complexity of O
(
n2
)
per document, where n is the number of mentions
in a document. When performing spectral cluster-
ing, this increases to O
(
n3
)
. Since we deal with
at most a few hundred mentions per document, the
cubic running time is not an issue.
3 Relations
In our system relations serve as templates for build-
ing or disallowing edges between mentions. We
distinguish between positive and negative relations:
negative relations disallow edges between mentions,
positive relations build edges between mentions.
Negative relations can also be used as constraints
during clustering, while positive relations may also
be applied as ?weak? relations: in this case, we only
add the induced edge when the two mentions under
consideration are already included in the graph after
considering all the non-weak relations.
Most of the relations presented here were already
used in our system for last year?s shared task (Cai et
al., 2011b). The set of relations was enriched mainly
to resolve pronouns in dialogue and to resolve pro-
nouns that do not carry much information by them-
selves like it and they.
3.1 Negative Relations
(1) N Gender, (2) N Number: Two mentions do
not agree in gender or number.
(3) N SemanticClass: Two mentions do not agree
in semantic class (only the Object, Date and Per-
son top categories derived from WordNet (Fell-
baum, 1998) are used).
(4) N ItDist: The anaphor is it or they and the sen-
tence distance to the antecedent is larger than
one.
(5) N BarePlural: Two mentions that are both bare
plurals.
(6) N Speaker12Prn: Two first person pronouns
or two second person pronouns with different
speakers, or one first person pronoun and one
second person pronoun with the same speaker.
(7) N DSprn: Two first person pronouns in direct
speech assigned to different speakers.
(8) N ContraSubjObj: Two mentions are in the
subject and object positions of the same verb,
and the anaphor is a non-possessive pronoun.
(9) N Mod: Two mentions have the same syntac-
tic heads, and the anaphor has a pre- or post-
modifier which does not occur in the antecedent
and does not contradict the antecedent.
(10) N Embedding: Two mentions where one em-
beds the other, which is not a reflexive or posses-
sive pronoun.
(11) N 2PrnNonSpeech: Two second person pro-
nouns without speaker information and not in di-
rect speech.
3.2 Positive Relations
(12) P StrMatch Npron, (13) P StrMatch Pron:
After discarding stop words, if the strings of
mentions completely match and are not pro-
nouns, the relation P StrMatch Npron holds.
When the matched mentions are pronouns,
P StrMatch Pron holds.
(14) P HeadMatch: If the syntactic heads of men-
tions match.
(15) P Nprn Prn: If the antecedent is not a pro-
noun and the anaphor is a pronoun. This relation
is restricted to a sentence distance of 1.
(16) P Alias: If mentions are aliases of each other
(i.e. proper names with partial match, full names
and acronyms, etc.).
(17) P Speaker12Prn: If the speaker of the second
person pronoun is talking to the speaker of the
first person pronoun. The mentions contain only
first or second person pronouns.
(18) P DSPrn: If one mention is subject of a speak
verb, and the other mention is a first person pro-
noun within the corresponding direct speech.
(19) P ReflPrn: If the anaphor is a reflexive pro-
noun, and the antecedent is the subject of the
sentence.
103
(20) P PossPrn: If the anaphor is a possessive pro-
noun, and the antecedent is the subject of the
sentence or the subclause.
(21) P GPEIsA: If the antecedent is a Named En-
tity of GPE entity type and the anaphor is a def-
inite expression of the same type.
(22) P PossPrnEmbedding: If the anaphor is a
possessive pronoun and is embedded in the an-
tecedent.
(23) P VerbAgree: If the anaphor is a pronoun and
has the same predicate as the antecedent.
(24) P Subject & (25) P Object: If both mentions
are subjects/objects (applies only if the anaphor
is it or they).
(26) P SemClassPrn: If the anaphor is a pronoun,
the antecedent is not a pronoun, and both have
semantic class Person.
For English, we used all relations except for (21) and
(26). Relations (1), (2) and (10) were incorporated
as constraints during greedy clustering. For Chinese,
we used relations (1) ? (6), (12) ? (15), (21) and (26).
(26) was incorporated as a weak relation.
4 Results
We submitted to the closed tasks for English and
Chinese. The results on the English development
set and testing set are displayed in Table 1 and Table
2 respectively. To indicate the progress we achieved
within one year, Table 3 shows the performance of
our system on the CoNLL ?11 development data set
compared to last year?s results (Cai et al, 2011b).
The Overall number is the average of MUC, B3
and CEAF (E), MD is the mention detection score.
Overall, we gained over 5% F1 some of which can
be attributed to improved mention detection.
Metric R P F1
MD 73.96 75.69 74.81
MUC 64.93 68.69 66.76
B3 68.42 75.77 71.91
CEAF (M) 61.23 61.23 61.23
CEAF (E) 49.61 45.60 47.52
BLANC 77.81 80.75 79.19
Overall 62.06
Table 1: Results on the English CoNLL ?12 development
set
Metric R P F1
MD 74.23 76.10 75.15
MUC 65.21 68.83 66.97
B3 66.50 74.69 70.36
CEAF (M) 59.61 59.61 59.61
CEAF (E) 48.64 44.72 46.60
BLANC 73.29 78.94 75.73
Overall 61.31
Table 2: Results on the English CoNLL ?12 testing set
Metric R P F1 2011 F1
MD 70.84 73.08 71.94 66.28
MUC 60.80 65.09 62.87 55.19
B3 68.37 75.89 71.94 68.52
CEAF (M) 60.42 60.42 60.42 54.44
CEAF (E) 50.40 46.11 48.16 43.19
BLANC 75.44 79.26 77.19 72.13
Overall 60.99 55.63
Table 3: Results on the English CoNLL ?11 development
set compared to Cai et al (2011b)
Table 4 and Table 5 display our results on Chinese
development data and testing data respectively.
Metric R P F1
MD 52.45 71.50 60.51
MUC 45.90 67.07 54.50
B3 58.94 84.26 69.36
CEAF (M) 53.60 53.60 53.60
CEAF (E) 50.73 34.24 40.89
BLANC 66.17 83.11 71.45
Overall 54.92
Table 4: Results on the Chinese CoNLL ?12 development
set
Metric R P F1
MD 48.49 74.02 58.60
MUC 42.71 67.80 52.41
B3 55.37 85.24 67.13
CEAF (M) 51.30 51.30 51.30
CEAF (E) 51.81 32.46 39.92
BLANC 63.96 82.81 69.18
Overall 53.15
Table 5: Results on the Chinese CoNLL ?12 testing set
Because none of our team members has knowl-
edge of the Arabic language we did not attempt to
104
run our system on the Arabic datasets and therefore
our official score for this language is considered to
be 0. The combined official score of our submission
is (0.0 + 53.15 + 61.31)/3 = 38.15. In the closed
task our system was the second best performing sys-
tem for English and the eighth best performing sys-
tem for Chinese.
5 Error analysis
We did not attempt to resolve event coreference and
did not incorporate world knowledge which is re-
sponsible for many recall errors our system makes.
Since we use a simple greedy strategy for clus-
tering that goes through the mentions left-to-right,
errors in clustering propagate, which gives rise to
cluster-level inconsistencies. We observed a drop in
performance when using more negative relations as
constraints. A more sophisticated clustering strat-
egy that allows a more refined use of constraints is
needed.
5.1 English
Our detection of copula and appositive relations is
quite inaccurate, which is why we limit the incor-
poration of copulas to cases where the antecedent is
this and left appositives out.
We aim for high precision regarding the usage of
the negative relation N Modifier. This leads to some
loss in recall. For example, our system does not as-
sign the just-completed Paralympics and the 12-day
Paralympics to the same entity. Such cases require a
more involved reasoning scheme to decide whether
the modifiers are actually contradicting each other.
Non-referring pronouns constitute another source
of errors. While we improved detection of pleonas-
tic it compared to last year?s system, a lot of them
are not filtered out. Our system also does not distin-
guish well between generic and non-generic uses of
you and we, which hurts precision.
5.2 Chinese
Since each Chinese character carries its own mean-
ing, there are multiple ways to express the same en-
tity by combining different characters into a word.
Both syntactic heads and modifiers can be replaced
by similar words or by abbreviated versions. From
??? (outside people) to ???? (outside eth-
nic group) the head is replaced, while from ??
? (Diana) to ?? ?? ? ?? (charming Di
Princess) the name is abbreviated.
Modifier replacement is more difficult to cope
with, our system does not recognize that ?? ?
??? (starting-over counting-votes job) and??
?? (verifying-votes job) are coreferent. It is also
not trivial to separate characters from words (e.g. by
separating ? and ?) to resolve such cases, since
it will introduce too much noise as a consequence.
In order to tackle this problem, a smart scheme to
propagate similarities from partial words to the en-
tire mentions and a knowledge base upon which re-
liable similarities can be retrieved are necessary.
In contrast to English there is no strict enforce-
ment of using definite noun phrases when referring
to an antecedent in Chinese. Both ???? (the
talk) and?? (talk) can corefer with the antecedent
??????????? (Clinton?s talk during
Hanoi election). This makes it very difficult to dis-
tinguish generic expressions from referential ones.
In the submitted version of our system, we simply
ignore the nominal anaphors which do not start with
definite articles or demonstratives.
6 Conclusions
In this paper we presented a graph-based model for
coreference resolution. It captures pairwise relations
between mentions via edges induced by relations.
Entities are obtained by graph clustering. Discrim-
inative information can be incorporated as negative
relations or as constraints during clustering.
We described our system?s architecture and the re-
lations it employs, highlighting differences and sim-
ilarities to our system from last year?s shared task.
Designed to work as a basis for further work, our
system works mainly by exploring the relationship
between pairs of mentions. Due to its modular archi-
tecture, our system can be extended by components
taking global information into account, for example
for weight learning or clustering.
We focused on the closed task for English in
which our system achieved competitive perfor-
mance, being ranked second out of 15 participants.
Acknowledgments. This work has been funded
by the Klaus Tschira Foundation, Heidelberg, Ger-
many. The first and the second authors have been
supported by a HITS PhD. scholarship.
105
References
Shane Bergsma and Dekang Lin. 2006. Bootstrapping
path-based pronoun resolution. In Proceedings of the
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics, Sydney, Australia, 17?
21 July 2006, pages 33?40.
Jie Cai, E?va Mu?jdricza-Maydt, Yufang Hou, and Michael
Strube. 2011a. Weakly supervised graph-based coref-
erence resolution for clinical data. In Proceedings of
the 5th i2b2 Shared Tasks and Workshop on Chal-
lenges in Natural Language Processing for Clinical
Data, Washington, D.C., 20-21 October 2011. To ap-
pear.
Jie Cai, E?va Mu?jdricza-Maydt, and Michael Strube.
2011b. Unrestricted coreference resolution via global
hypergraph partitioning. In Proceedings of the Shared
Task of the 15th Conference on Computational Natu-
ral Language Learning, Portland, Oreg., 23?24 June
2011, pages 56?60.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
Mass.
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, Singapore,
6?7 August 2009, pages 1152?1161.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford?s multi-pass sieve coreference resolution sys-
tem at the CoNLL-2011 shared task. In Proceedings
of the Shared Task of the 15th Conference on Compu-
tational Natural Language Learning, Portland, Oreg.,
23?24 June 2011, pages 28?34.
Sameer Pradhan, Alessandro Moschitti, and Nianwen
Xue. 2012. CoNLL-2012 Shared Task: Modeling
multilingual unrestricted coreference in OntoNotes. In
Proceedings of the Shared Task of the 16th Confer-
ence on Computational Natural Language Learning,
Jeju Island, Korea, 12?14 July 2012. This volume.
Yannick Versley, Simone Paolo Ponzetto, Massimo Poe-
sio, Vladimir Eidelman, Alan Jern, Jason Smith,
Xiaofeng Yang, and Alessandro Moschitti. 2008.
BART: A modular toolkit for coreference resolution.
In Companion Volume to the Proceedings of the 46th
Annual Meeting of the Association for Computational
Linguistics, Columbus, Ohio, 15?20 June 2008, pages
9?12.
106
