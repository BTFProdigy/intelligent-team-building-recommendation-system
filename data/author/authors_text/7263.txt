Robust  German Noun Chunk ing  
Wi th  a Probab i l i s t i c  Context -F ree  Grammar  
Helmut  Schmid  and Sab ine  Schu l te  im Walde* 
Institut fiir Maschinelle Sprachverarbeitung 
Univers i tS t  S tut tgar t  
Azenbergst ra f ie  12, 70174 Stut tgar t ,  Germany 
{schmid, s chulte}@ims, uni-stuttgart, de 
Abst rac t  
We present a noun chunker for German which is 
based on a head-lexicalised probabilistic context- 
fl'ee grammar. A manually developed grammar 
was semi-automatically extended with robustness 
rules in order to allow parsing of unrestricted text. 
Tile model parmncters were learned from unlabellcd 
training data by a probabilistic ontext-fl'ee parser. 
For extracting noun chunks, the parser generates 
all possible noun chunk analyses, scores them with 
a novel algorithm which maximizes tile best chunk 
sequence criterion, and chooscs the most probable 
chunk sequence. An evaluation of the chunker on 
2,140 hand-annotated noun chunks yielded 92% re- 
call and 93% precision. 
1 In t roduct ion  
A fro'lilt oh'unicef i narks  the noun chunks in a sen- 
tence as in the tbllowing example: 
(Wirtschaftsbosse) mit (zweitblhaftem Ruf) 
economy (:hef~ with doubtable reputation 
sind an (der in (Engt)Sssen) mlgewandten Fiihrung) 
are in the in bottlenecks apl)lied guidance 
(des Landes) beteiligt. 
of the country involved. 
'Leading economists with doubtable reI)u- 
tations are involved in guiding the country 
in times of bottlenecks.' 
A tool which identifies noun chunks is useflfl for 
term extraction (most technical terms are nouns or 
comI)lex noun groups), for lexicograt)hic lmrposes 
(see (\]~panainen a d JSrvinen, 1.998) on syntacti- 
cally organised concordancing), and as index terms 
for information retrieval. Chunkers may also mark 
other types of chunks like verb groups, adverbial 
t)hrases or adjectival I)hrases. 
Several methods have been develoI)ed tbr noun 
chunking. Church's noun phrase tagger (Church, 
1988), one of the first; noun ehunkers, was based on a 
Hidden Markov Model (HMM) similar to those used 
* Thanks  to Mats  Rooth  and  Uli I Ieid for many helpfl f l  com-  
ir lonts. 
for part-of-speech tagging. Another HMM-bascd ap- 
proach has been developed by Mats Rooth (Rooth, 
1992). It integrates two HMMs; one of them mod- 
els noun chunks internally, the other models the 
context of noun chunks. Abney's cascaded finite- 
state parser (Almey, 1996) also contains a process- 
ing step which recognises noun chunks and other 
types of chunks. Ramshaw and Marcus (Ramshaw 
and Marcus, 1995) successflflly applied Eric Brill's 
transformation-based l arning method to the chunk- 
ing problem. Voutilainen's NPtool (Voutilainen, 
1993) is based on his constraint-grammar system. 
Finally, Brants (Brmlts, 1999) described a Ger- 
man clumker which was implemented with cascaded 
Markov Models. 
In this 1)aper, a prol)abilistic ontext-free parser 
is aI)I)lied to the noui, chunking task. Tile Ger- 
man grammar used in the experiments was semi- 
autolnati(:ally extended with robustness rules in o f  
der to be able to process arbitrary int)ut. The gram- 
mar parameters were trained on unlabelled ata. A 
novel algorithm is used for noun chunk extraction. 
It maximises the t)robability of the chunk set. 
The tbllowing section introduces the grammar 
fi'alnework, followed by a description of the chunk- 
ing algorithm in section 3, and the experiments and 
their evaluation in section 4. 
2 The  Grammar  
The German grammar is a head-lexicalised I)roba- 
bilistic context-free grainmar. Section 2.1 defines 
probabilistic ontext-ti'ee grammars and their head- 
lexicalised refinement. Section 2.2 introduces our 
grmnmar architecture, focusing on noun chunks. 
The robustness rules for the ehunker a.re described 
in section 2.3. 
2.1 (Head-Lexiealised) Probab i l i s t ie  
Context-Free Grammars 
A probabilistic context-free g~tmmar (PCFG) is a 
context-free grmnmar which assigns a probability P
to each context-fl'ee grammar ule in the rule set 
R. The probability of a parse tree T is defined 
as \[I,.eRP(,.)'"l, whore I"1 is the number of times 
rule r was applied to build T. The parmneters of 
726 
PCFGs ca:: be learned f'rom unparsed corpora us- 
ing the Inside-Outside algorithm (Lari and Young, 
1990). 
Hcad-lcxicaliscd probabilistic contczt-frcc .qra?lt- 
mar;s (It-L PCFG) (Carroll and Rooth, 1998) ex- 
tend the PCFG al)proach by incorl)orating informa- 
tion about the lcxi('al head of constituents into the 
t)robal)ilistic model.: Each node in a parse of a H- 
L PCFG is labelled with a ca.tegory and the lexi- 
cal head of the category. A H-L PCFG rule looks 
like a PCFG rule in which one of the daughters 
has been marked as the head. The rule i)robabili- 
ties l~.uze(C --+ alC) m'e replaced by lexicalised rule 
probabilities l~,~t~(C -~ o~\]C, h) where h is the lex- 
ical head of the lnother constituent C. The prob- 
ability of a rule therefore depends not only on the 
category of the mother node, but also on its lexi- 
cal head. Assume that the grmnmar has two rules 
VP -+ V NP and VP ---> V. Then the transitive verb 
buy should have a higher probability for the for- 
mer rule whereas the latter rule shouhl be more 
likely for intrm:sitive verbs like sh'.cp. II-L PCFGs 
incorporate another type of parameters called lexi- 
cal choice probabilities. The lexical choice probabil- 
ity l~hoi,,~ (hd\]C,z, G,~, h,,,, ) rel)reseitt,s the prolml)ility 
that a node of category Cd with a mother node of 
category C,~ and lexical head h,,,, bears the texical 
head h,d. The probability of a parse tree is obtained 
by multiplying lexicalised rule protml)ilities and lex~ 
ical choice ln'obal)ilities for all nodes. Since it is 
possible to transform H-L PCFGs into PCFGs, the 
PCFG algorithms are ai)l)licable to I\]:-L PCF(4s. 
2.2 Noun Chunks  in the German Grammar  
Currently, the German grammar contains d,619 rules 
and covers 92% of our 15 million words of verb 
final and relative clauses ~. Tl:e structmal :lOtln 
chunk concel)t in tim grammar is defined accord- 
ing to Almey's chunk style (Abney, 1991) who de- 
scribes chunks as syntactic units which correspond 
in some way to prosodic 1)atterns, containing a con- 
tent word surrounded t)y some function word(s): all 
words from the beginning of the noun 1)hrase to the 
head noun are included. :~ The difl'erent kinds of noun 
chunks covered by our grmnmar are listed below and 
illustrated with exmnples: 
.. a combination of a non-obligatory deternfiner, 
optional adjectives or cardinals and the noun 
1Other types of lexicalised PCFGs have been (h!scrib('.d in 
(Charniak, 1997), (Collins, 1997), (G'oodman, 1997), (Chcll)a 
and .lelinek, 1998) mid (Eisner and Sat:a, 1999). 
2'l'he restricted (:orl)ora were exl;ra(:ted mltomatically from 
the llugc German Corpus (I1GC), a collection of German 
newsl)al)ers as well as sl)ecialiscd magazines ibr industry, law, 
computer science. 
3As you will sc'e below, there is one exception, noun chunks 
refinc.d by a proper nanm, which end with the. name instead 
of the head noun. 
itself: 
(1) cine gutc Mec 
a good idea 
(2) viclcn Menschcn 
for many 1)eot)le 
(3) dercn kiinstliche Stimme 
whose m'tificial voice 
(4) elf Ladungen 
eleven cargos 
(5) Wasscr 
water 
and prepositional phrases where the definite m'- 
ticle of the embedded noun chunk is morpholog- 
ically combined with a 1)rel)osition, so the pure 
noun chunk could not be set)arated: 
(6) zum Schluss 
at the end 
? personal pronouns: ich (I), mir (me) 
? reflexive pronouns: reich (myself), sich (him- 
self/herself/itself) 
? possessive pronou: l s :  
(7) Mcins ist sauber. 
Mine is clean. 
? demonstrative t ) ronouns :  
(8) Jcncr ffi.hrt viel sclmeller. 
That one goes much faster. 
? indefinite 1)ronom~s: 
(9) Einige sind durchgefifllen. 
Some failed. 
? relative 1)ronouns: 
(10) Ich mag Menschen, die ehrlich sind. 
I like peol)le who are honest. 
? nonfinalised adjectives: Wichtigcm (important 
things) 
? l)roper nmnes: Christoph, Kolumbus 
? a noun chunk refined by a prol)er name: 
(1.1.) der Erobere.r Christoph Kolumbus 
the conquerer Christoph Cohlmbus 
? cardinals indicating a ycm': 
(1.2) Ich begann 1996. 
I started 1996. 
The chunks may be recursive in case they appear as 
c, omplement of an adjectival phrase, as in (dcr (ira 
Rc.qc, 0 wartendc 5'oh, n) (the son who was waiting in 
the rain). 
Noun chunks have features for case, without fi:r- 
ther agreement features for nouns and verbs. The 
case is constrained by the time:ion of the noun 
chunk, as verbal or adjectival co:nplement with nom- 
inative, accusative, dative or genitive case, as mod- 
ifier with genit ive case, or as part of a prel:ositional 
727 
phrase (also in the special case representing a prepo- 
sitional phrase itself) with accusative or dative case. 
Both structure mid case of noun phrases may be 
ambiguous and have to be disambiguated: 
? ambiguity concenfing structure: 
diesen (this) is disregarding the context a 
demonstrative pronoun mnbiguous between rep- 
resenting a standalone noun chunk (cf. example 
(8)) or a determiner within a noun chunk (cf. 
example (2)) 
? mnbiguity concerning case: 
die Beitriige (the contributions) is disregarding 
tile context ambiguous between onfinative mid 
accusative case 
The disambiguation is learned during grammar 
training, since the lexicalised rule probabilities as 
well as the lexical choice probabilities tend to enforce 
the correct structure and case information. Con- 
sidering the above examples, the trained grmnmar 
should be able to parse diesen I(rie9 (this war) as 
one noun chunk instead of two (with diesen repre- 
senting a standalone noun clmnk) because of (i) the 
preferred use of denlonstrative pronouns as deter- 
miners (+ lexicalised rule probabilities), and (ii) the 
lexical coherence between the two words (~ lexical 
choice probabilities); in a sentence like er zahlte die 
Beitr@e (lie paid the contributions) the accusative 
case of the latter noun chunk should be identified 
because of the lexical coherence between the verb 
zaMen (pay) and the lexical head of the subcate- 
gorised noun phrase Beitrag (contribution) as re- 
lated direct object head (+ lexical choice probat)il- 
ides). 
2.3 Robustness Rules 
Tile Gernlan grammar covers over 90% of tile clauses 
of our verb final and relative clause corpora. This 
is sufiqcient for the extraction of lexical infornlation, 
e.g. the subcategorisation f verbs (see (Beil et al, 
1999)). For chunking, however, it is usually neces- 
sary to analyse all sentences. Therefore, the gram- 
mar was augmented with a set of robustness rules. 
Three types of robustness rules have been consid- 
ered, namely unigram rules, bigram rules aud tri- 
gram rules. 
Unigram rules are rules of the form X -+ YP X, 
where YP is a grammatical category and X is a new 
category. If such a rule is added for each grammar 
category 4, the coverage is 100% because the gram- 
mar is then able to generate any sequence of category 
labels, hi practice, some of the rules can be omitted 
while still retaining full coverage: e.g. the rule X -+ 
4Also needed are two rules which start and terminate the 
"X chain". We used the rules T0P --+ START X and X --+ END. 
START and END expand to SGML tags which mark the begin- 
ning and the end of a sentence, respectively. 
ADV X is not necessary if the grmnmar already con- 
tains tile rules ADVP --+ ADV and X --+ ADVP X. Uni- 
gram rules are insensitive to their context so that all 
permutations of the categories which are generated 
by the X chain have the stone probability. 
The second type of robustness rules, called trigram 
rules (Carroll and Rooth, 1998) is more context sen- 
sitive. Trigram rules have the form X:Y -+ Y Y:Z 
where X, Y, Z are categories and X:Y and Y:Z are 
new categories. \]Mgram rules choose the next cat- 
egory on the basis of the two preceding categories. 
Therefore the number of rules grows as the number 
of categories raises to the third power. For exam- 
ple, 125,000 trigrmn rules are needed to generate 50 
different categories in arbitrary order. 
Since unigram rules are context insensitive and 
trigram rules are too numerous, a tlfird type of ro- 
bustness rules, called bi9ram rules, was developed. 
A bigram rule actually consists of two rules, a rule 
of the form :Y --+ Y Y: which generates the COl> 
stituent Y deternlinistically, and a rule Y: -~ :Z 
which selects the next constituent Z based on the 
current one. Given n categories, we obtain n rules 
of the first form and n 2 rules of the second fornl. 
Even when categories which directly project o some 
other category were oufitted in the generation of the 
bigram rules for our Germm~ grmnmar, the num- 
ber of rules was still fairly large. Hence we gener- 
alised some of the grammatical categories by adding 
additional chain rules. For example, the preposi- 
tional phrase categories PP. Akk : an, PP. Akk : auf,  
PP.Akk:gegen etc. were generalised to PPX by 
adding the rules PPX --~ PP.Akk:an et(:. Instead of 
n + 1 t)igram rules for each of tlm 23 prepositional 
categories, we now obtained only n + 2 rules with 
the new category PPX. Altogedmr, 3,332 robustness 
rules were added. 
3 Chunk ing  
A head-lexicalised probabilistic ontext-fl'ee parser, 
called LoPar (Schnfid, 1999), was used for pa.rs- 
ing. The f'unctionality of LoPar encompasses lmrely 
synlbolic parsing as well as Vitcrbi parsing, inside- 
outside computation, POS tagging, chunking and 
training with PCFGs as well as H-L PCFGs. Be- 
cause of the large number of parameters in l)al'tic- 
ular of H-L PCFGs, the parser smoothes the prob- 
ability distributions in order to re,old zero proba- 
bilities. The absolute discounting method (Ney et 
al., 1994) was adapted to fractional counts for this 
purpose. LoPar also supports lemmatisation of the 
lexical heads of a H-L PCFG. Tile input to the 
parser consists of ambiguously tagged words. The 
tags are provided by a German morphological mlal- 
yser (Schiller and St6ckert, 1995). 
The best chunk set of a sentence is defined as tile 
set of chunks (with category, start mid end position) 
728 
for which the stun of the prolmbilities of all parses 
which c, ontain exactly that chunk set is maximal. 
The chunk set of the most likely parse (i.e. Viterbi 
parse) is not necessarily the best chunk set according 
to this definition, as the folh)wing PCFG shows. 
S -~A 0.6 B -+x 1.0 
S -~ B 0.4 C - -+x  1.0 
A -~C 0.5 l ) -+x  1.0 
A -+ D 0.5 
This grmmnar generates the three parse trees (S 
(A (C x ) ) ) ,  (S (A (D x ) ) ) ,and  (S (B x)) .  The 
parse tree probal)ilities are 0.3, 0.3 and 0.4, respec- 
tively. The last parse is therefore the Viterbi parse of 
x. Now assume that {A,B} is the set; of chunk cate- 
gories. The most likely chunk set is then { (A, 0 ,1)} 
because the sum of the l/robal/ilities of all parses 
which contain h is 0.6, whereas the sum over tit(; 
l/robal/ilities of all 1)arses containing B is only 0.4. 
computeChunks i a slightly simlllified l)seudo- 
code version of the actual chunking algorithm: 
computeChunks(G, Prul,:) 
hfitialize float array p\[Gv\] 
Initialize chunk set array.ch.'unks\[Gv\] 
for each vertex v ill GV in bottom-up order do 
if v is an or-node then 
Initialize float array prob\[ch:unk.@l(v)\]\] to 0 
fin" each daughter 'u C d(v) do 
-,-p\[',,,\] 
<- v,.ol,\[d 
p\[,,,\] +- 
else 
v\[q <- II,<,,(,,t 2,\[,*\] 
<- 
if v is labelled with a chunk category (7 then 
ch/,,,,a:.~\[v\] +- ~-l,,,,,,~a:.~\[,,\] U {(C, .,'t,,'t(,,), c,,.d(v))} 
return ch,'.,Tzks\[,'oot(G)\] 
computeChunks takes two arguments. The first, ar- 
gument is a parse fore.st G which is represented as an 
and-or-graph. Gv is the set of vertices. The second 
argument is the rule probability vector, d is a flmc- 
tion which returns the daughters of a vertex. The al- 
gorithm comtmtes the best elmnk set; ch,,,m, ks\[v\] and 
the corresponding I)robability ply\] for all vertices v 
in bottom-up order, chunks\[d(v)\] returns the set of 
chunk sets of the daughter nodes of vertex v. r'ule(v) 
reSurns tile rule which created v and is only defined 
tbr and-nodes, start(v) and end(v) return the start 
aml end position of the constituent represented by 
V. 
The chunking algorithm was extmrimentally eoltl- 
pared with chunk extraction fl'om Viterbi parses. In 
35 out of 41 ewfluation rims with different parame- 
ter settings '~, the f-score of tile chunking algorithm 
S'Fhe runs differed wrt. training strategy and number of 
iterations. See section 4 for details. 
was better than that of the Viterbi algorithm. The 
average f-score of the chunking algorithm was 84.7 % 
compared to 84.0 % for the Viterbi algorithm. 
4 Experiments 
We performed two main chunking experiments, hfi- 
tially, the parser trained the chunk grammar based 
on the restricted grmnmar described in section 2 ac- 
cording to tbur different training strategies. A pre- 
ferred training strategy was then applied to inves- 
tigate the potential of grammar efim;ment and ex- 
tended training data. 
4.1 Tra in ing  
Ill the frst  exlmriment, the chunker version of the 
grmmnar was trained oil a corpus comprising a 1 
million word subcortms of relative clauses, a 1 mil- 
lion word subeorpus of verb final clauses and 2 mil- 
lion words of consecutive text. All data had been 
extracted from the Huge German Corpus. The test 
data used for the later evahmtion was not included 
in the training corpus. 
For training strategy 1, the elmnker gralnmar was 
first; trained on the whole cortms in mflexiealised 
mode, i.e. like a PCFG. The tmrmneters were rees- 
timated once in the middle and once at the end of 
the eorlms. In the next stel) , the grammar was lexi- 
calised, i.e. the parser computed |;tie parse probabil- 
ities with the unlexicalised model, lint extracted De- 
quencies for the lexicalised model. These fl'equencies 
were summed over the. whole eorl)us. Three more 
iterations on the whole corpus tbllowed in which 
the parmneters of the lexicalised model were rees- 
timate(t. 
The parameters of the unlexicalised chunker gram- 
mar were initialised in the following way: a fl'e- 
queney of 7500 was assigned to all original granunar 
rules and 0 to the majority of robustness rules. The 
parmneters were then estimated on the basis of these 
Dequencies. Because of the smoothing, the t)roba- 
bilities of the robustness rules were small lint not 
zero.  
For training strategy 2, the chunker rules were 
initialised with frequencies fl'om a grammar without 
robustness rule extensions, which had been trained 
mflexiealised on a 4 million subeortms of verb final 
clauses and a 4 million word subcorpus of relative 
c lauses .  
Training strategy 3 again set the fi'equency of the 
original rules to 7500 and of tile robustness rules to 
0. The parser trained with three unlexicalised iter- 
ations over the whole training corpus, reestimating 
the parameters only at the end of the corpus, ill o f  
der to find out; whether the lexicalised probabilistic 
parser had been better than tile fully trained mflexi- 
calised parser on the task of chunk parsing. Training 
strategy 4 repeated this procedure, but with initial- 
729 
ising the chunker frequencies on basis of a trained 
gramnlar. 
For each training strategy, further iterations were 
added until the precision and recall values ceased to 
improve. 
For the second part of the experiments, the base 
grammar was extended with a few simple verb-first 
and verb-second clause rules. Strategy 4 was applied 
for training the ehunker 
(A) on the same training corpus as betbre, i.e. 2 
million words of relative and verb final clauses, 
and 2 million words of unrestricted corpus data 
from the HGC, 
(B) on a training corpus consisting of 10 million 
words of unrestricted corpus data from the 
HGC. 
4.2 Eva luat ion  
The evaluation of tile ctmnker was carried out 
on noun chunks from 378 unrestricted sentences 
from the German newspaper Frankfu~'ter Allgcmci~c 
Zeitun9 (FAZ). Two persons independently anno- 
tated all noun chunks in the corpus -a  total of 2,140 
noun chunks-, according to the noun chunk deft- 
nition in section 2.2, without considering rammar 
coverage, i.e. noun chunks not actually covered by 
the grammar (e.g. noun chunk ellipsis such as die 
klcinc~ \[ \]N) were annotated as such. As labels, we 
used the identifier NC plus case information: NC. Nom, 
IqC. Ace, NC. Dat, NC.Gen. In addition, we included 
identifiers for prepositional phrases where the prepo- 
sition is nlorphologically merged with the definite 
article, (el. example (6)), also including case infor- 
mation: PNC.Acc, PNC.Dat. 
For each training strategy described in section 4.1 
we evaluated the chunker before the training process 
and after each training iteration: the model in its 
current training state parsed the test sentences and 
extracted the most probable clnmk sequence as de- 
fined in section 3. We then compared the extracted 
noun elmnks with tile haud-ammtated data, accord- 
ing to 
* the range of the chunks, i.e. (lid the chunker 
find a chunk at all? 
. the range and the identifier of the chunks, i.e. 
did the ehunker find a chunk and identify the 
correct syntactic ategory and case? 
Figures 1 and 2 display the results of the eval- 
uation in tile first experiment, ? according to noun 
chunk range only and according to noun chunk 
range, syntactic category and case, respectively. 
Bold font highlights the best versions. 
Training strategy 2 with two iterations of lexi- 
calised training produced tile best f-scores tbr noun 
6The lexieatised ehunker versions obtained by strategy 2 
were also utilised for parsing the test sentences unlexiealised. 
chunk boundary recognition if unlexicalised parsing 
was done. The respective precision and recall val- 
ues were 93.06% and 92.19%. For recognising noun 
chunks with range, category and case, the best; chun- 
ker version was created by training strategy 4, after 
five iterations of unlexicalised training; precision and 
recall values were 79.28% and 76.75%, respectively. 
From the experimental results, we can conclude 
that: 
1. initialisation of the chunker grammar frequen- 
cies on the basis of a trained grammar improves 
the untrained version of the elumker, but the 
difference vanishes in the training process 
2. unlexicalised parsing is sufficient for noun chunk 
extraction; for extraction of chunks with case 
ilfformation, unlexicalised training turned out 
to be even more successflfl than a combination 
with lexicalised training 
Figures 3 and 4 display the results of the evalu- 
ation concerning the second experilnent, compared 
to the initial w, lues from the first experiinent. 
Extending the base grammar and the training cor- 
pus slightly increased precision and recall values for 
recognising noun chunks according to range only. 
The main inlprovement was ill noun chunk recogni- 
tion according to range, category and case: precision 
and recall values increased to 83.88% and 83.21%, 
respectively. 
4.3 Fai lure Ana lys i s  
A comparison of the parsed noun chunks with the 
mmotated ata showed that failure in detecting a 
noun chunk was mainly caused by proper names, 
for exalnple Neta~j(E~,~t, abbreviations like OSZE, or 
composita like So~tth Ch, ina Mor,ti,tg Post. The di- 
versity of proper names makes it difficult for tile 
chunker to learn them properly. On the one hand, 
the lexieal infornl~tion for proper names is unreliable 
because Inany proper na lnes  were not  reeognised as 
such. On the other hand, most prot)er names are 
too rare to learn reliable statistics tbr them. 
Minor mistakes were cruised by (a) articles which 
are morphologically identical to noun chunks con- 
sisting of a pronoun, for example den Rc,t,t~,e,'~ (tiLe 
pensionersd(,t,) was analysed as two noun clumks, dcTt 
(demonstrative pronoun) and Rent~t,e~'7t, (b) capital 
letter eonfnsion: since Gerinan nouns typically start 
with capital etters, sentence beginnings are wrongly 
interpreted as nouns, for example Wiirden as the 
conditional of the auxiliary wcrdc~ (to become) is 
interpreted as the dative case of Wib'dc (dignity), (e) 
noun chunk internal umctuation as in seine ' Pa~'t- 
,tcr' ' (his ' partners '). 
Failure in assigning the correct; syntactic cate- 
gory and case to a noun chunk was mainly caused 
by (a) assigning accusative case instead of nomina- 
tive case, and (b) assigning dative case or nomina- 
730 
la':~ined 
I(;xl 
lex2 
I(;x3 
lex4 
lex5 
lexO 
lexl. 
lo, x2 
lc, x3 
Strategy 1 Str~ttegy 2 -l)~trsed unlex Str~tegy 3 Strategy 4 
1)roe I'e{'. 
83.63% 
91.88% 
89.13% 
88.37% 
88.25% 
88.17% 
83.63% 
89.62% 
89.71% 
89.52% 
89.57% 
89.62% 
prec r(~c 
90.22% 99.18% 
92.84% 91.58% 
90.12% 90.41% 
88.97% 89.76% 
89.79% 90.46% 
89.d2% 90.13% 
\])I'(!C 1"(~(~ 
90.22% 00.18% 
92.84% 9J .58% 
93.01% 91.49% 
93.02% 91.67% 
93.06% 92.19% 
.93.05% 92.05% 
prec leC 
83.(13% 83.63% 
:H .aa% 8:).62% 
:)2.55% 90.04% 
92.78% 90.22% 
l) r('.c l'eC 
90.22% 90.18% 
92.84% 91.58% 
:)3.01% 9\] .49% 
92.95% 91.25% 
93.(19% 90.79% 
93 .29% 9(I.32% 
Figure 1: Comt)aring training strategies: noun chunk (',valuation according t;o range only 
St;l'~tl;(;gy 1 Strategy 2 parsed lmlex Stl';tl;(.~gy 23 Strategy 4 
l)re(: re(: 
untraille(1 
unlexl 
unlex2 
unlex:~ 
unlex4 
ulllox5 
lex0 
lexl 
lex2 
lex3 
63.52% 
74.50% 
7:{.68% 
72.02% 
72.76% 
711.97% 
63.52% 
78.11% 
73.15% 
72.97% 
73.85% 
73.15% 
\])re(" I'(\]C 
72.02% 71.98% 
75.87% 74.84% 
75.10% 75.35% 
74.69% 75.35% 
75.2(i% 75.82% 
75.03% 75.63% 
\] )F(!C r(~C 
72.02% 71.98% 
75.87% 74.84% 
78.27% 76.99% 
77.27% 7(1.15% 
77.48% 76.75% 
77.45% 7(i.61% 
l)l'(!(; 1"(IC 
63.52% 63.52% 
74.50% 73.t 1% 
76.88% 74.79% 
77.97% 75.82% 
pl'OC F(~C 
72.02% 71.98% 
75.87% 74.84% 
78.27% 76.99% 
78.70% 77.27% 
78.80% 76.85% 
79.28% 76.75% 
Figure 2: Comi)aring training strategies: noun chun\]{ evahtal;ion according l;o range and label 
tive case instead of a(:cusal;ive (:an(;. 2}he confl,sion 
between l~ominal;ive an(t accusative case is due (;() 
1;he facl; that both cases at(', (~xI)rc'ss(.'(t t)y i(lenl;ical 
m()ri)tlology in l;he f(;minine and neul;ra\] genders in 
G(n'man. The morl)hologi(" similarity l)ei:ween a(:- 
(:u,;ative and (lative is less substantial, but esl)ecially 
prol)er names and bar(; nouns are sl;ill sul)jecl; 1;o (:(m- 
fllsion. As |;lie evaluation resull;s show, the (lisLinc- 
tion between 1;he cases could be learned in general, 
bul; morl)hological similaril;y and in addil;ion l;he rel- 
atively Dee word order in German impose high de- 
mure(Is on the n(;(:essary i)rot)a|)ilil;y model. 
5 Summary 
We t)resenl;ed a German noun ('hunker for unre- 
stricted text. The chunker in based on a head- 
lexicalised probabilistic context-free grammar and 
|;r~,ined on unlal)elled data. ~\]'he base grammar was 
semi-automatically augmenl;ed with robust, heSS rules 
in order to cover unrestricted input. An algorit;hm 
for chunk exl, ract, ion was develoi)ed which maximises 
the probabilil;y of l;he chunk set;s rather than the 
probability of single t)arses like l;he Vil;erl)i algo- 
rithm. 
German noun chunks were del;ected wil;h 93% 1)re- 
cision and 92(~) re(:all. Asking the clmnker to addi- 
tionally identil~y the syntactic ategory and l;he case 
of the chunks resulted in recall of 83% and precision 
of 84~). A COml)arison of different training strate- 
gies showed that unlexicalised parsing inforlnation 
xv~s sufIi('ienl; for noun chunk extra(:l;ion wil;h and 
wil;ho111; (:~s('. informal;ion. The base gralltllt~r played 
an iml)orl;ant role in the chunker dev(dot)ment: (i) 
building the (:hunker on |,11(; basis of an ~dready 
train(~(t gr~mmmr iml)rov(~d the chtmker rules, and 
(ii) relining l;he base grammar wil;h even simple verb- 
tirs\[; and verl)-se(:ond rules improved accuracy, so it 
should \])e worthwhile to flirt;her extend lhe grammar 
rules. Increasing l;he ~mlounl; of training (tal;a also 
improved noun ('hunk r(;cognition, especially case 
disaml)iguat;ion. I~(;IA:er heuristi(:s for guessing the 
I)arts-of-st)eech of unknown words should flu'ther im- 
prove l;he noun chunk recognition, since lnalk~, errors 
were ('ause(1 l)y llnk\]~own words. 
References  
Steven Abney. 1991. Parsing by Chunks. In Robert 
Berwick, Steven Almey, and Carol Tenny, editors, 
Pri'~tciplc-Bascd ParsiTt.q. Kluwer Academic Pul)- 
lishers. 
St;even Almey. 1996. Partial Parsing via Finite- 
St;ate Cascades. In ProcccdiTt.qs of the \]~;SSLLI '96 
Rob't, st Pa,;s'i~zq Wo,'t~shop. 
Franz 13eil, Glenn Carroll, Det, lef Prescher, Stefim 
Riezler, and Mal;s ll,ooth. 1999. Inside-Outside 
Estimation of a Lexi('alize(t PCFG for German. 
In ProceediT~,.qs of th, e 37th Annual Mceti,z!l of the 
ACL, pages 269-276. 
731 
untrained 
unlexl 
reflex2 
unlex3 
reflex4 
unlex5 
unlex6 
unlex7 
Strategy 4 
Initial A B 
pree rec prec t}rec rec 
90.22% 90.18% 
92.84% 91.58% 
93.01% 91.49% 
92.95% 91.25% 
93.09% 90.7{}% 
93.29% 90.32% 
90.43% 
91.65% 
92.45% 
92.64% 
93.11% 
93.20% 
90.60% 
91.35% 
91.58% 
91.21% 
91.07% 
91.02% 
90.43% 
91.52% 
91.89% 
92.21% 
92.73% 
92.73% 
92.91% 
92.83% 
Figure 3: Granunar and training data extensions: noun chunk evaluation 
rec 
90.60% 
9\].35% 
91.67% 
91..86% 
91..86% 
91.86% 
91.96% 
92.10% 
according to range only 
untrained 
unlexl 
unlex2 
unlcx3 
unlcx4 
unlex5 
unlex6 
unlex7 
Strategy 4 
Initial A B 
prec rec prec rec 1)roe ree 
72.02% 71.98% 
75.87% 74.84% 
78.27% 76.99% 
78.70% 77".27% 
78.80% 76.85% 
79.28% 76.75% 
74.42% 
78.51% 
80.74% 
81.24% 
81.83% 
81.85% 
74.56% 
78.25% 
79.98% 
79.98% 
80.03% 
79.93% 
74.42% 
77.60% 
79.89% 
81.17% 
82.44% 
82.53% 
82.94% 
88.88% 
74.56% 
77.46% 
79.70% 
80.87% 
81.67% 
81.76% 
82.09% 
83.21% 
Figure 4: Grammar and trailfing data extensions: noun chunk evaluation according to range mid label 
Thorsten Brants. 1999. Cascaded inarkov models. 
Ill Proceedings of EA CL '99. 
Glenn Carroll and Mats Rooth. 1998. Valence In- 
duction with a Head-Lexicalized PCFG. In Pro- 
ccedings of Third Conference on Empirical Meth- 
ods in Natural Language Processing. 
Eugene Charniak. 1997. Statistical Parsing with a 
Context-Free Granunar and Word Statistics. In 
Proceedings of the l~th National Confcrence on 
Artificial Intelligence. 
Ciprian Chelba and Frederick Jelinek. 1998. Ex- 
ploiting Syntactic Structure for Language Mod- 
eling, hi Proceedings of the 35th Annual Meeting 
of the A CL. 
Kenneth W. Church. 1988. A Stochastic Parts Pro- 
granl and Noun Phrase Pm'ser for unrestricted 
Text. In Proceedings of the Second Conference on 
Applied Natural Language Processing, pages 136- 
143. 
Michael Collins. 1997. Three Generative, Lexi- 
calised Models for Statistical Parsing. In Proceed- 
ings of the 35th Annual Meeting of the A CL. 
Jason Eisner and Giorgio Satta. 1999. Efficient 
Parsing for Bilexical Context-Free Grmmnars and 
Head Automaton Grammars. In Procecdings of 
the 37th Annual Meeting of the ACL, pages 457- 
464. 
Joshua Goodman. 1996. Parsing Algorithms and 
Metrics. In Proceedings of the 34th Annual Meet- 
ing of the ACL, pages 177-183. 
Joshna Goodman. 1997. Probabilistic Feature 
Grammars. In Procecdings of the 5th Interna- 
tional Workshop on Parsing Technologies, pages 
89-100. 
K. Lari and S. Young. 1990. The Estimation 
of Stochastic Context-Free Grmmnars using the 
Inside-Outside Algorithm. Computation Specch 
and Language Proccssing, 4:35-56. 
Herlnann Ney, U. Essen, and R. Kneser. 1994. 
On Structuring Probabilistic Dependencies in 
Stochastic Language Modelling. Computer Speech 
and Language, 8:1 38. 
L. Ramshaw and M. Marcus. 1995. Text Chunking 
using Transtbrlnation-Based Learning. Ill Pro- 
ccedings of thc Third Workshop on Vcry LaTye 
Corpora, pages 82-94. 
Mats Rooth. 1992. Statistical NP Tagging. Unpub- 
lished manuscript. 
Anne Schiller and Chris StSckert, 1995. DMOR. 
Institut fiir Maschinelle Spraehverarbeitung, Uni- 
versit~t Stuttgart. 
Hehnut Schmid. 1999. Lopar: Design and hn- 
plementation. Technical report, Institut fiir 
Maschinelle Sprachverarbeitung, Universit'gt 
StuttgArt. 
Pasi Tapanainen and Time JSrvinen. 1998. De- 
pendeney Concordances. International Journal of 
Lexicography, 11(3):187-203. 
Atro Voutilainen. 1993. NPtool, a Detector of En- 
glish Noun Phrases. In Proceedings of the Work- 
shop on Vcry Large Corpora, pages 48-57. 
732 
  
		 
		Efcient Parsing of Highly Ambiguous Context-Free Grammars
with Bit Vectors
Helmut Schmid
Institute for Computational Linguistics
University of Stuttgart
Azenbergstr. 12
D-70174 Stuttgart
Germany
schmid@ims.uni-stuttgart.de
Abstract
An efficient bit-vector-based CKY-style parser
for context-free parsing is presented. The parser
computes a compact parse forest representation
of the complete set of possible analyses for
large treebank grammars and long input sen-
tences. The parser uses bit-vector operations
to parallelise the basic parsing operations. The
parser is particularly useful when all analyses
are needed rather than just the most probable
one.
1 Introduction
Large context-free grammars extracted from tree-
banks achieve high coverage and accuracy, but
they are difficult to parse with because of their
massive ambiguity. The application of standard
chart-parsing techniques often fails due to excessive
memory and runtime requirements.
Treebank grammars are mostly used as probabilis-
tic grammars and users are usually only interested
in the best analysis, the Viterbi parse. To speed up
Viterbi parsing, sophisticated search strategies have
been developed which find the most probable anal-
ysis without examining the whole set of possible
analyses (Charniak et al, 1998; Klein and Manning,
2003a). These methods reduce the number of gener-
ated edges, but increase the amount of time needed
for each edge. The parser described in this paper
follows a contrary approach: instead of reducing the
number of edges, it minimises the costs of building
edges in terms of memory and runtime.
The new parser, called BitPar, is based on a bit-
vector implementation (cf. (Graham et al, 1980))
of the well-known Cocke-Younger-Kasami (CKY)
algorithm (Kasami, 1965; Younger, 1967). It builds
a compact ?parse forest? representation of all anal-
yses in two steps. In the first step, a CKY-style
recogniser fills the chart with constituents. In the
second step, the parse forest is built top-down from
the chart. Viterbi parses are computed in four steps.
Again, the first step is a CKY recogniser which is
followed by a top-down filtering of the chart, the
bottom-up computation of the Viterbi probabilities,
and the top-down extraction of the best parse.
The rest of the paper is organised as follows: Sec-
tion 2 explains the transformation of the grammar
to Chomsky normal form. The following sections
describe the recogniser algorithm (Sec. 3), improve-
ments of the recogniser by means of bit-vector op-
erations (Sec. 4), and the generation of parse forests
(Sec. 5), and Viterbi parses (Sec. 6). Section 7 dis-
cusses the advantages of the new architecture, Sec-
tion 8 describes experimental results, and Section 9
summarises the paper.
2 Grammar Transformation
The CKY algorithm requires a grammar in Chom-
sky normal form where the right-hand side of each
rule either consists of two non-terminals or a sin-
gle terminal symbol. BitPar uses a modified ver-
sion of the CKY algorithm allowing also chain rules
(rules with a single non-terminal on the right-hand
side). BitPar expects that the input grammar is al-
ready epsilon-free and that terminal symbols only
occur in unary rules. Rules with more than 2 non-
terminals on the right-hand side are split into binary
rules by applying a transformation algorithm pro-
posed by Andreas Eisele1. It is a greedy algorithm
which tries to minimise the number of binarised
rules by combining frequently cooccurring symbols
first. The algorithm consists of the following two
steps which are iterated until all rules are either bi-
nary or unary.
1. Compute the frequencies of the pairs of neigh-
boring symbols on the right-hand sides of
rules. (The rule A   B C D, e.g., adds 1 to
the counts of  B,C  and  C,D  , respectively.)
2. Determine the most frequent pair  A,B  . Add
a new non-terminal X. Replace the symbol pair
1personal communication
A B in all grammar rules with X. Finally, add
the rule X   A B to the grammar.
3 Computation of the Chart
In the first step, the parser computes the CKY-style
recogniser chart with the algorithm shown in Fig-
ure 1. It uses the transformed grammar with gram-
mar rules P and non-terminal symbol set N. The
chart is conceptually a three-dimensional bit array
containing one bit for each possible constituent. A
bit is 1 if the respective constituent has been inserted
into the chart and 0 otherwise. The chart is indexed
by the start position, the end position and the label
of a constituent2 . Initially all bits are 0. This chart
representation is particularly efficient for highly am-
biguous grammars like treebank grammars where
the chart is densely filled.
1 recognise(P,N,w  ,...,w  )
2 allocate and initialise chart[1..n][1..n][N] to 0
3 for e  1 to n do
4 for each non-terminal A with A  	
 P do
5 chart[e][e]  chart[e][e] | chainvec[A]
6 for b  e  1 down to 1 do
7 for each non-terminal A 
 N do
8 if chart[b][e][A] = 0 and
derivable(P,N,b,e,A) then
9 chart[b][e]  chart[b][e] | chainvec[A]
10 derivable(P,N,b,e,A)
11 for each rule A   B C 
 P do
12 for m  b to e  1 do
13 if chart[b][m][B] = 1 and
chart[m  1][e][C] = 1 then
14 return true
15 return false
Figure 1: CKY-recogniser
Like other CKY-style parsers, the recogniser con-
sists of several nested loops. The first loop (line 3
in Fig. 1) iterates over the end positions e of con-
stituents, inserts the parts of speech of the next word
(lines 4 and 5) into the chart, and then builds in-
creasingly larger constituents ending at position e.
To this end, it iterates over the start positions b from
e-1 down to 1 (line 6) and over all non-terminals
A (line 7). Inside the innermost loop, the function
derivable is called to compute whether a con-
stituent of category A covering words  through

 is derivable from smaller constituents via some
2Start and end position of a constituent are the indices of
the first and the last word covered by the constituent.
binary rule. derivable loops over all rules
A   B C with the symbol A on the left-hand side
(line 11) and over all possible end positions m of
the first symbol on the right-hand side of the rule
(line 12). If the chart contains B from position b to
m and C from position m+1 to e (line 13), the func-
tion returns true (line 14), indicating that  through
  are reducible to the non-terminal A. Otherwise,
the function returns false (line 15).
In order to deal with chain rules, the parser precom-
putes for each category C the set of non-terminals D
which are derivable from C by a sequence of chain
rule reductions, i.e. for which D  C holds, and
stores them in the bit vector chainvec[C]. The set
includes C itself. Given the grammar rules NP  
DT N1, NP   N1, N1   JJ N1 and N1   N, the
bits for NP, N1 and N are set in chainvec[N]. When
a new constituent of category A starting at posi-
tion b and ending at position e has been recognised,
all the constituents reachable from A by means of
chain rules are simultaneously added to the chart
by or-ing the precomputed bit vector chainvec[A]
to chart[b][e] (see lines 5 and 9 in Fig. 1).
The first parsing step is a pure recogniser which
computes the set of constituents to which the in-
put words can be reduced, but not their analyses.
Therefore it is not necessary to look for further anal-
yses once the first analysis of a constituent has been
found. The function derivable therefore returns
as soon as the first analysis is finished (line 13 and
14), and derivable is not called if the respective
constituent was previously derived by chain rules
(line 8).
Because only one analysis has to be found and some
rules are more likely than others, the algorithm is
optimised by trying the different rules for each cat-
egory in order of decreasing frequency (line 11).
The frequency information is collected online dur-
ing parsing.
Derivation of constituents by means of chain rules
is much cheaper than derivation via binary rules.
Therefore the categories in line 7 are ordered such
that categories from which many other categories
are derivable through chain rules, come first.
The chart is actually implemented as a single large
bit-vector with access functions translating index
triples (start position, end position, and symbol
number) to vector positions. The bits in the chart
are ordered such that chart[b][e][n+1] follows after
chart[b][e][n], allowing the efficient insertion of a
set of bits with an or-operation on bit vectors.
4 Using Bit-Vector Operations
The function derivable is the most time-
consuming part of the recogniser, because it is the
only part whose overall runtime grows cubically
with sentence length. The inner loop of the function
iterates over the possible end positions of the first
child constituent and computes an and-operation
for each position. This loop can be replaced by
a single and-operation on two bit vectors, where
the first bit vector contains the bits stored in
chart[b][b][B], chart[b][b+1][B] ... chart[b][e-1][B]
and the second bit vector contains the bits stored in
chart[b+1][e][C], chart[b+2][e][C] ... chart[e][e][C].
The bit-vector operation is overall more efficient
than the solution shown in Figure 1 if the extrac-
tion of the two bit vectors from the chart is fast
enough. If the bits in the chart are ordered such that
chart[b][1][A] ... chart[b][N][A] are in sequence,
the first bit vector can be efficiently extracted
by block-wise copying. The same holds for the
second bit vector if the bits are ordered such that
chart[1][e][A] ... chart[n][e][A] are in sequence.
Therefore, the chart of the parser which uses
bit-vector operations, internally consists of two bit
vectors. New bits are inserted in both vectors.
1 recognise(P,N,w  ,...,w  )
2 allocate and initialise chart[1..n][1..n][N] to 0
3 allocate vec[N]
4 for e  1 to n do
5 initialise vec[N] to 0
6 for each non-terminal A with A    
 P do
7 vec  vec | chainvec[A]
8 chart[e][e]  chart[e][e] | vec
9 for b  e  1 down to 1 do
10 initialise vec[N] to 0
11 for each non-terminal A 
 N do
12 if vec[A] = 0 and derivable(P,N,b,e,A) then
13 vec  vec | chainvec[A]
14 chart[b][e]  chart[b][e] | vec
15 derivable(P,N,b,e,A)
16 for each rule A   B C 
 P do
17 vec1  chart[b][b...e-1][B]
18 vec2  chart[b+1...e][e][C]
19 return vec1 & vec2  0
Figure 2: optimised CKY-recogniser
Due to the new representation of the chart, the in-
sertion of bits into the chart by means of the opera-
tion chart[b][e]  chart[b][e] | vec cannot be done
with bit vector operations, anymore. Instead, each
1-bit of the bit vector has to be set separately in
both copies of the chart. Binary search is used to
extract the 1-bits from each machine word of a bit
vector. This is more efficient than checking all bits
sequentially if the number of 1-bits is small. Fig-
ure 3 shows how the 1-bits would be extracted from
a 4-bit word v and stored in the set s. The first line
checks whether any bit is set in v. If so, the second
line checks whether one of the first two bits is set.
If so, the third line checks whether the first bit is 1
and, if true, adds 0 to s. Then it checks whether the
second bit is 1 and so on.
1 if  then
2 if ff  	
			Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 777?784
Manchester, August 2008
Estimation of Conditional Probabilities With Decision Trees and an
Application to Fine-Grained POS Tagging
Helmut Schmid and Florian Laws
IMS, University of Stuttgart
{schmid,lawsfn}@ims.uni-stuttgart.de
Abstract
We present a HMM part-of-speech tag-
ging method which is particularly suited
for POS tagsets with a large number of
fine-grained tags. It is based on three ideas:
(1) splitting of the POS tags into attribute
vectors and decomposition of the contex-
tual POS probabilities of the HMM into a
product of attribute probabilities, (2) esti-
mation of the contextual probabilities with
decision trees, and (3) use of high-order
HMMs. In experiments on German and
Czech data, our tagger outperformed state-
of-the-art POS taggers.
1 Introduction
A Hidden-Markov-Model part-of-speech tagger
(Brants, 2000, e.g.) computes the most probable
POS tag sequence
?
t
N
1
=
?
t
1
, ...,
?
t
N
for a given word
sequence w
N
1
.
?
t
N
1
= argmax
t
N
1
p(t
N
1
, w
N
1
)
The joint probability of the two sequences is de-
fined as the product of context probabilities and
lexical probabilities over all POS tags:
p(t
N
1
, w
N
1
) =
N
?
i=1
p(t
i
|t
i?1
i?k
)
? ?? ?
context prob.
p(w
i
|t
i
)
? ?? ?
lexical prob.
(1)
HMM taggers are fast and were successfully ap-
plied to a wide range of languages and training cor-
pora.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
POS taggers are usually trained on corpora with
between 50 and 150 different POS tags. Tagsets
of this size contain little or no information about
number, gender, case and similar morphosyntac-
tic features. For languages with a rich morphol-
ogy such as German or Czech, more fine-grained
tagsets are often considered more appropriate. The
additional information may also help to disam-
biguate the (base) part of speech. Without gender
information, for instance, it is difficult for a tagger
to correctly disambiguate the German sentence Ist
das Realit?at? (Is that reality?). The word das is
ambiguous between an article and a demonstrative.
Because of the lack of gender agreement between
das (neuter) and the noun Realit?at (feminine), the
article reading must be wrong.
The German Tiger treebank (Brants et al, 2002)
is an example of a corpus with a more fine-grained
tagset (over 700 tags overall). Large tagsets aggra-
vate sparse data problems. As an example, take the
German sentence Das zu versteuernde Einkommen
sinkt (?The to be taxed income decreases?; The
taxable income decreases). This sentence should
be tagged as shown in table 1.
Das ART.Def.Nom.Sg.Neut
zu PART.Zu
versteuernde ADJA.Pos.Nom.Sg.Neut
Einkommen N.Reg.Nom.Sg.Neut
sinkt VFIN.Full.3.Sg.Pres.Ind
. SYM.Pun.Sent
Table 1: Correct POS tags for the German sentence
Das zu versteuernde Einkommen sinkt.
Unfortunately, the POS trigram consisting of
the tags of the first three words does not occur
in the Tiger corpus. (Neither does the pair con-
sisting of the first two tags.) The unsmoothed
777
context probability of the third POS tag is there-
fore 0. If the probability is smoothed with the
backoff distribution p(?|PART.Zu), the most
probable tag is ADJA.Pos.Acc.Sg.Fem rather than
ADJA.Pos.Nom.Sg.Neut. Thus, the agreement be-
tween the article and the adjective is not checked
anymore.
A closer inspection of the Tiger corpus reveals
that it actually contains all the information needed
to completely disambiguate each component of the
POS tag ADJA.Pos.Nom.Sg.Neut:
? All words appearing after an article (ART)
and the infinitive particle zu (PART.zu) are at-
tributive adjectives (ADJA) (10 of 10 cases).
? All adjectives appearing after an article and
a particle (PART) have the degree positive
(Pos) (39 of 39 cases).
? All adjectives appearing after a nominative
article and a particle have nominative case (11
of 11 cases).
? All adjectives appearing after a singular arti-
cle and a particle are singular (32 of 32 cases).
? All adjectives appearing after a neuter article
and a particle are neuter (4 of 4 cases).
By (1) decomposing the context probability of
ADJA.Pos.Nom.Sg.Neut into a product of attribute
probabilities
p(ADJA | 2:ART, 2:ART.Def, 2:ART.Nom,
2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu)
? p(Pos| 2:ART, 2:ART.Def, 2:ART.Nom,
2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu,
0:ADJA)
? p(Nom | 2:ART, 2:ART.Def, 2:ART.Nom,
2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu,
0:ADJA, 0:ADJA.Pos)
? p(Sg | 2:ART, 2:ART.Def, 2:ART.Nom,
2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu,
0:ADJA, 0:ADJA.Pos, 0:ADJA.Nom)
? p(Neut | 2:ART, 2:ART.Def, 2:ART.Nom,
2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu,
0:ADJA, 0:ADJA.Pos, 0:ADJA.Nom, 0:ADJA.Sg)
and (2) selecting the relevant context attributes
for the prediction of each attribute, we obtain the
following expression for the context probability:
p(ADJA | ART, PART.Zu)
? p(Pos | 2:ART, 1:PART, 0:ADJA)
? p(Nom | 2:ART.Nom, 1:PART.Zu, 0:ADJA)
? p(Sg | 2:ART.Sg, 1:PART.Zu, 0:ADJA)
? p(Neut | 2:ART.Neut, 1:PART.Zu, 0:ADJA)
The conditional probability of each attribute is
1. Hence the context probability of the whole tag is
also 1. Without having observed the given context,
it is possible to deduce that the observed POS tag
is the only possible tag in this context.
These considerations motivate an HMM tagging
approach which decomposes the POS tags into a
set of simple attributes, and uses decision trees to
estimate the probability of each attribute. Deci-
sion trees are ideal for this task because the iden-
tification of relevant attribute combinations is at
the heart of this method. The backoff smoothing
methods of traditional n-gram POS taggers require
an ordering of the reduced contexts which is not
available, here. Discriminatively trained taggers,
on the other hand, have difficulties to handle the
huge number of features which are active at the
same time if any possible combination of context
attributes defines a separate feature.
2 Decision Trees
Decision trees (Breiman et al, 1984; Quinlan,
1993) are normally used as classifiers, i.e. they as-
sign classes to objects which are represented as at-
tribute vectors. The non-terminal nodes are labeled
with attribute tests, the edges with the possible out-
comes of a test, and the terminal nodes are labeled
with classes. An object is classified by evaluating
the test of the top node on the object, following the
respective edge to a daughter node, evaluating the
test of the daughter node, and so on until a termi-
nal node is reached whose class is assigned to the
object.
Decision Trees are turned into probability esti-
mation trees by storing a probability for each pos-
sible class at the terminal nodes instead of a single
result class. Figure 1 shows a probability estima-
tion tree for the prediction of the probability of the
nominative attribute of nouns.
2.1 Induction of Decision Trees
Decision trees are incrementally built by first se-
lecting the test which splits the manually anno-
tated training sample into the most homogeneous
subsets with respect to the class. This test, which
maximizes the information gain
1
wrt. the class, is
1
The information gain measures how much the test de-
creases the uncertainty about the class. It is the difference
between the entropy of the empirical distribution of the class
variable in the training set and the weighted average entropy
778
2:N.Reg
p=0.571 p=0.938
p=0.999
0:N.Name
1:ART.Nom
0:N.Name 0:N.Name
p=0.948 p=0.998 ....
1:ADJA.Nom
yes
yes no
noyes no
yes no
no
yes
Figure 1: Probability estimation tree for the nomi-
native case of nouns. The test 1:ART.Nom checks
if the preceding word is a nominative article.
assigned to the top node. The tree is recursively
expanded by selecting the best test for each sub-
set and so on, until all objects of the current subset
belong to the same class. In a second step, the de-
cision tree may be pruned in order to avoid overfit-
ting to the training data.
Our tagger generates a predictor for each feature
(such as base POS, number, gender etc.) Instead of
using a single tree for the prediction of all possible
values of a feature (such as noun, article, etc. for
base POS), the tagger builds a separate decision
tree for each value. The motivation was that a tree
which predicts a single value (say verb) does not
fragment the data with tests which are only rele-
vant for the distinction of two other values (e.g. ar-
ticle and possessive pronoun).
2
Furthermore, we
observed that such two-class decision trees require
no optimization of the pruning threshold (see also
section 2.2.)
The tree induction algorithm only considers bi-
nary tests, which check whether some particular
attribute is present or not. The best test for each
node is selected with the standard information gain
criterion. The recursive tree building process ter-
minates if the information gain is 0. The decision
tree is pruned with the pruning criterion described
below.
Since the tagger creates a separate tree for each
attribute, the probabilities of a set of competing at-
tributes such as masculine, feminine, and neuter
will not exactly sum up to 1. To understand why,
assume that there are three trees for the gender at-
tributes. Two of them (say the trees for mascu-
line and feminine) consist of a single terminal node
in the two subsets. The weight of each subset is proportional
to its size.
2
We did not directly compare the two alternatives (two-
valued vs. multi-valued tests), because the implementational
effort required would have been too large.
which returns a probability of 0.3. The third tree
for neuter has one non-terminal and two terminal
nodes returning a probability of 0.3 and 0.5, re-
spectively. The sum of probabilities is therefore
either 0.9 or 1.1, but never exactly 1. This problem
is solved by renormalizing the probabilities.
The probability of an attribute (such as ?Nom?)
is always conditioned on the respective base POS
(such as ?N?) (unless the predicted attribute is the
base POS) in order to make sure that the probabil-
ity of an attribute is 0 if it never appeared with the
respective base POS. All context attributes other
than the base POS are always used in combination
with the base POS. A typical context attribute is
?1:ART.Nom? which states that the preceding tag
is an article with the attribute ?Nom?. ?1:ART? is
also a valid attribute specification, but ?1:Nom? is
not.
The tagger further restricts the set of possible
test attributes by requiring that some attribute of
the POS tag at position i-k (i=position of the pre-
dicted POS tag, k ? 1) must have been used be-
fore an attribute of the POS tag at position i-(k+1)
may be examined. This restriction improved the
tagging accuracy for large contexts.
2.2 Pruning Criterion
The tagger applies
3
the critical-value pruning strat-
egy proposed by (Mingers, 1989). A node is
pruned if the information gain of the best test mul-
tiplied by the size of the data subsample is below a
given threshold.
To illustrate the pruning, assume that D is the
data of the current node with 50 positive and 25
negative elements, and that D
1
(with 20 positive
and 20 negative elements) and D
2
(with 30 posi-
tive and 5 negative elements) are the two subsets
induced by the best test. The entropy of D is
?2/3 log
2
2/3 ? 1/3 log
2
1/3 = 0.92, the entropy
ofD
1
is?1/2 log
2
1/2?1/2 log
2
1/2 = 1, and the
entropy of D
2
is ?6/7 log
2
6/7 ? 1/7 log
2
1/7 =
0.59. The information gain is therefore 0.92 ?
(8/15 ? 1 ? 7/15 ? 0.59) = 0.11. The resulting
score is 75 ? 0.11 = 8.25. Given a threshold of 6,
the node is therefore not pruned.
We experimented with pre-pruning (where a
node is always pruned if the gain is below the
3
We also experimented with a pruning criterion based on
binomial tests, which returned smaller trees with a slightly
lower accuracy, although the difference in accuracy was never
larger than 0.1% for any context size. Thus, the simpler prun-
ing strategy presented here was chosen.
779
threshold) as well as post-pruning (where a node
is only pruned if its sub-nodes are terminal nodes
or pruned nodes). The performance of pre-pruning
was slightly better and it was less dependent on
the choice of the pruning threshold. A threshold
of 6 consistently produced optimal or near optimal
results for pre-pruning. Thus, pre-pruning with a
threshold of 6 was used in the experiments.
3 Splitting of the POS Tags
The tagger treats dots in POS tag labels as attribute
separators. The first attribute of a POS tag is the
main category. The number of additional attributes
is fixed for each main category. The additional
attributes are category-specific. The singular at-
tribute of a noun and an adjective POS tag are
therefore two different attributes.
4
Each position in the POS tags of a given cate-
gory corresponds to a feature. The attributes oc-
curring at a certain position constitute the value set
of the feature.
4 Our Tagger
Our tagger is a HMM tagger which decomposes
the context probabilities into a product of attribute
probabilities. The probability of an attribute given
the attributes of the preceding POS tags as well as
the preceding attributes of the predicted POS tag
is estimated with a decision tree as described be-
fore. The probabilities at the terminal nodes of the
decision trees are smoothed with the parent node
probabilities (which themselves were smoothed in
the same way). The smoothing is implemented by
adding the weighted class probabilities p
p
(c) of the
parent node to the frequencies f(c) before normal-
izing them to probabilities:
p(c) =
f(c) + ?p
p
(c)
? +
?
c
f(c)
The weight ? was fixed to 1 after a few experi-
ments on development data. This smoothing strat-
egy is closely related to Witten-Bell smoothing.
The probabilities are normalized by dividing them
by the total probability of all attribute values of the
respective feature (see section 2.1).
The best tag sequence is computed with the
Viterbi algorithm. The main differences of our tag-
ger to a standard trigram tagger are that the order of
the Markov model (the k in equation 1) is not fixed
4
This is the reason why the attribute tests in figure 1 used
complex attributes such as ART.Nom rather than Nom.
and that the context probability p(t
i
|t
i?1
i?k
) is inter-
nally computed as a product of attribute probabili-
ties. In order to increase the speed, the tagger also
applies a beam-search strategy which prunes all
search paths whose probability is below the prob-
ability of the best path times a threshold. With a
threshold of 10
?3
or lower, the influence of prun-
ing on the tagging accuracy was negligible.
4.1 Supplementary Lexicon
The tagger may use an external lexicon which sup-
plies entries for additional words which are not
found in the training corpus, and additional tags for
words which did occur in the training data. If an
external lexicon is provided, the lexical probabili-
ties are smoothed as follows: The tagger computes
the average tag probabilities of all words with the
same set of possible POS tags. The Witten-Bell
method is then applied to smooth the lexical prob-
abilities with the average probabilities.
If the word w was observed with N different
tags, and f(w, t) is the joint frequency of w and
POS tag t, and p(t|[w]) is the average probability
of t among words with the same set of possible
tags as w, then the smoothed probability of t given
w is defined as follows:
p(t|w) =
f(w, t) + Np(t|[w])
f(w) + N
The smoothed estimates of p(tag|word) are di-
vided by the prior probability p(tag) of the tag and
used instead of p(word|tag).
5
4.2 Unknown Words
The lexical probabilities of unknown words are
obtained as follows: The unknown words are di-
vided into four disjoint classes
6
with numeric ex-
pressions, words starting with an upper-case letter,
words starting with a lower-case letter, and a fourth
class for the other words. The tagger builds a suf-
fix trie for each class of unknown words using the
known word types from that class. The maximal
length of the suffixes is 7.
The suffix tries are pruned until (i) all suffixes
have a frequency of at least 5 and (ii) the informa-
tion gain multiplied by the suffix frequency and di-
5
p(word|tag) is equal to p(tag|word)p(word)/p(tag)
and p(word) is a constant if the tokenization is unambiguous.
Therefore dropping the factor p(word) has no influence on
the ranking of the different tag sequences.
6
In earlier experiments, we had used a much larger num-
ber of word classes. Decreasing their number to 4 turned out
to be better.
780
vided by the number of different POS tags is above
a threshold of 1. More precisely, if T
?
is the set of
POS tags that occurred with suffix ?, |T | is the
size of the set T , f
?
is the frequency of suffix ?,
and p
?
(t) is the probability of POS tag t among the
words with suffix ?, then the following condition
must hold:
f
a?
|T
a?
|
?
t?T
a?
p
a?
(t) log
p
a?
(t)
p
?
(t)
< 1
The POS probabilities are recursively smoothed
with the POS probabilities of shorter suffixes us-
ing Witten-Bell smoothing.
5 Evaluation
Our tagger was first evaluated on data from the
German Tiger treebank. The results were com-
pared to those obtained with the TnT tagger
(Brants, 2000) and the SVMTool (Gim?enez and
M`arquez, 2004), which is based on support vec-
tor machines.
7
The training of the SVMTool took
more than a day. Therefore it was not possible to
optimize the parameters systematically. We took
standard features from a 5 word window and M4-
LRL training without optimization of the regular-
ization parameter C.
In a second experiment, our tagger was also
evaluated on the Czech Academic corpus 1.0
(Hladk?a et al, 2007) and compared to the TnT tag-
ger.
5.1 Tiger Corpus
The German Tiger treebank (Brants et al, 2002)
contains over 888,000 tokens. It is annotated with
POS tags from the coarse-grained STTS tagset
and with additional features encoding informa-
tion about number, gender, case, person, degree,
tense, and mood. After deleting problematic sen-
tences (e.g. with an incomplete annotation) and au-
tomatically correcting some easily detectable er-
rors, 885,707 tokens were left. The first 80% were
used as training data, the first half of the rest as
development data, and the last 10% as test data.
Some of the 54 STTS labels were mapped to
new labels with dots, which reduced the number
of main categories to 23. Examples are the nom-
inal POS tags NN and NE which were mapped to
N.Reg and N.Name. Some lexically decidable dis-
tinctions missing in the Tiger corpus have been
7
It was planned to include also the Stanford tagger
(Toutanova et al, 2003) in this comparison, but it was not
possible to train it on the Tiger data.
automatically added. Examples are the distinc-
tion between definite and indefinite articles, and
the distinction between hyphens, slashes, left and
right parentheses, quotation marks, and other sym-
bols which the Tiger treebank annotates with ?$(?.
A supplementary lexicon was created by analyz-
ing a word list which included all words from the
training, development, and test data with a German
computational morphology. The analyses gener-
ated by the morphology were mapped to the Tiger
tagset. Note that only the words, but not the POS
tags from the test and development data were used,
here. Therefore, it is always possible to create a
supplementary lexicon for the corpus to be pro-
cessed.
In case of the TnT tagger, the entries of the sup-
plementary lexicon were added to the regular lex-
icon with a default frequency of 1 if the word/tag-
pair was unknown, and with a frequency propor-
tional to the prior probability of the tag if the word
was unknown. This strategy returned the best re-
sults on the development data. In case of the SVM-
Tool, we were not able to successfully integrate the
supplementary lexicon.
5.1.1 Refined Tagset
Prepositions are not annotated with case in the
Tiger treebank, although this information is impor-
tant for the disambiguation of the case of the next
noun phrase. In order to provide the tagger with
some information about the case of prepositions,
a second training corpus was created in which
prepositions which always select the same case,
such as durch (through), were annotated with this
case (APPR.Acc). Prepositions which select gen-
itive case, but also occur with dative case
8
, were
tagged with APPR.Gen. The more frequent ones
of the remaining prepositions, such as in (in), were
lexicalized (APPR.in). The refined tagset alo dis-
tinguished between the auxiliaries sein, haben, and
werden, and used lexicalized tags for the coor-
dinating conjunctions aber, doch, denn, wie, bis,
noch, and als whose distribution differs from the
distribution of prototypical coordinating conjunc-
tions such as und (and) or oder (or).
For evaluation purposes, the refined tags are
mapped back to the original tags. This mapping
is unambiguous.
8
In German, the genitive case of arguments is more and
more replaced by the dative.
781
tagger default refined ref.+lexicon
baseline 67.3 67.3 69.4
TnT 86.3 86.9 90.4
SVMTool 86.6 86.6 ?
2 tags 87.0 87.9 91.5
10 tags 87.6 88.5 92.2
Table 2: Tagging accuracies on development data
in percent. Results for 2 and for 10 preceding POS
tags as context are reported for our tagger.
5.1.2 Results
Table 2 summarizes the results obtained with
different taggers and tagsets on the development
data. The accuracy of a baseline tagger which
chooses the most probable tag
9
ignoring the con-
text is 67.3% without and 69.4% with the supple-
mentary lexicon.
The TnT tagger achieves 86.3% accuracy on the
default tagset. A tag is considered correct if all
attributes are correct. The tagset refinement in-
creases the accuracy by about 0.6%, and the ex-
ternal lexicon by another 3.5%.
The SVMTool is slightly better than the TnT
tagger on the default tagset, but shows little im-
provement from the tagset refinement. Apparently,
the lexical features used by the SVMTool encode
most of the information of the tagset refinement.
With a context of two preceding POS tags (sim-
ilar to the trigram tagger TnT), our tagger outper-
forms TnT by 0.7% on the default tagset, by 1%
on the refined tagset, and by 1.1% on the refined
tagset plus the additional lexicon. A larger context
of up to 10 preceding POS tags further increased
the accuracy by 0.6, 0.6, and 0.7%, respectively.
default refined ref.+lexicon
TnT STTS 97.28
TnT Tiger 97.17 97.26 97.51
10 tags 97.39 97.57 97.97
Table 3: STTS accuracies of the TnT tagger trained
on the STTS tagset, the TnT tagger trained on the
Tiger tagset, and our tagger trained on the Tiger
tagset.
These figures are considerably lower than
e.g. the 96.7% accuracy reported in Brants (2000)
for the Negra treebank which is annotated with
STTS tags without agreement features. This is to
9
Unknown words are tagged by choosing the most fre-
quent tag of words with the same capitalization.
be expected, however, because the STTS tagset is
much smaller. Table 3 shows the results of an eval-
uation based on the plain STTS tagset. The first
result was obtained with TnT trained on Tiger data
which was mapped to STTS before. The second
row contains the results for the TnT tagger when
it is trained on the Tiger data and the output is
mapped to STTS. The third row gives the corre-
sponding figures for our tagger.
91.491.5
91.691.7
91.891.9
9292.1
92.292.3
2 3 4 5 6 7 8 9 10
Figure 2: Tagging accuracy on development data
depending on context size
Figure 2 shows that the tagging accuracy tends
to increase with the context size. The best results
are obtained with a context size of 10. What type
of information is relevant across a distance of ten
words? A good example is the decision tree for the
attribute first person of finite verbs, which looks
for a first person pronoun at positions -1 through
-10 (relative to the position of the current word) in
this order. Since German is a verb-final language,
these tests clearly make sense.
Table 4 shows the performance on the test data.
Our tagger was used with a context size of 10. The
suffix length parameter of the TnT tagger was set
to 6 without lexicon and to 3 with lexicon. These
values were optimal on the development data. The
accuracy of our tagger is lower than on the devel-
opment data. This could be due to the higher rate
of unknown words (10.0% vs. 7.7%). Relative to
the TnT tagger, however, the accuracy is quite sim-
ilar for test and development data. The differences
between the two taggers are significant.
10
tagger default refined ref.+lexicon
TnT 83.45 84.11 89.14
our tagger 85.00 85.92 91.07
Table 4: Tagging accuracies on test data.
By far the most frequent tagging error was the
confusion of nominative and accusative case. If
10
726 sentences were better tagged by TnT (i.e. with few
errors), 1450 sentences were better tagged by our tagger. The
resulting score of a binomial test is below 0.001.
782
this error is not counted, the tagging accuracy
on the development data rises from 92.17% to
94.27%.
Our tagger is quite fast, although not as fast as
the TnT tagger. With a context size of 3 (10), it an-
notates 7000 (2000) tokens per second on a com-
puter with an Athlon X2 4600 CPU. The training
with a context size of 10 took about 4 minutes.
5.2 Czech Academic Corpus
We also evaluated our tagger on the Czech Aca-
demic corpus (Hladk?a et al, 2007) which contains
652.131 tokens and about 1200 different POS tags.
The data was divided into 80% training data, 10%
development data and 10% test data.
88.5
88.6
88.7
88.8
88.9
89
2 3 4 5 6 7 8 9 10
?context-data2?
Figure 3: Accuracy on development data depend-
ing on context size
The best accuracy of our tagger on the develop-
ment set was 88.9% obtained with a context of 4
preceding POS tags. The best accuracy of the TnT
tagger was 88.2% with a maximal suffix length of
5. The corresponding figures for the test data are
89.53% for our tagger and 88.88% for the TnT tag-
ger. The difference is significant.
6 Discussion
Our tagger combines two ideas, the decomposition
of the probability of complex POS tags into a prod-
uct of feature probabilities, and the estimation of
the conditional probabilities with decision trees. A
similar idea was previously presented in Kempe
(1994), but apparently never applied again. The
tagging accuracy reported by Kempe was below
that of a traditional trigram tagger. Unlike him,
we found that our tagging method out-performed
state-of-the-art POS taggers on fine-grained POS
tagging even if only a trigram context was used.
Schmid (1994) and M`arquez (1999) used deci-
sion trees for the estimation of contextual tag prob-
abilities, but without a decomposition of the tag
probability. Magerman (1994) applied probabilis-
tic decision trees to parsing, but not with a genera-
tive model.
Provost & Domingos (2003) noted that well-
known decision tree induction algorithms such as
C4.5 (Quinlan, 1993) or CART (Breiman et al,
1984) fail to produce accurate probability esti-
mates. They proposed to grow the decision trees to
their maximal size without pruning, and to smooth
the probability estimates with add-1 smoothing
(also known as the Laplace correction). Ferri
et al (2003) describe a more complex backoff
smoothing method. Contrary to them, we ap-
plied pruning and found that some pruning (thresh-
old=6) gives better results than no pruning (thresh-
old=0). Another difference is that we used N two-
class trees with normalization to predict the prob-
abilities of N classes. These two-class trees can be
pruned with a fixed pruning threshold. Hence there
is no need to put aside training data for parameter
tuning.
An open question is whether the SVMTool (or
other discriminatively trained taggers) could out-
perform the presented tagger if the same decompo-
sition of POS tags and the same context size was
used. We think that this might be the case if the
SVM features are restricted to the set of relevant
attribute combinations discovered by the decision
tree, but we doubt that it is possible to train the
SVMTool (or other discriminatively trained tag-
gers) without such a restriction given the difficul-
ties to train it with the standard context size.
Czech POS tagging has been extensively stud-
ied in the past (Haji?c and Vidov?a-Hladk?a, 1998;
Haji?c et al, 2001; Votrubec, 2006). Spoustov et
al. (2007) compared several POS taggers includ-
ing an n-gram tagger and a discriminatively trained
tagger (Mor?ce), and evaluated them on the Prague
Dependency Treebank (PDT 2.0). Mor?ce?s tag-
ging accuracy was 95.12%, 0.3% better than the
n-gram tagger. A hybrid system based on four
different tagging methods reached an accuracy of
95.68%. Because of the different corpora used and
the different amounts of lexical information avail-
able, a direct comparison to our results is difficult.
Furthermore, our tagger uses no corpus-specific
heuristics, whereas Mor?ce e.g. is optimized for
Czech POS tagging.
The German tagging results are, to the best of
our knowledge, the first published results for fine-
grained POS tagging with the Tiger tagset.
783
7 Summary
We presented a HMM POS tagger for fine-grained
tagsets which splits the POS tags into attribute
vectors and estimates the conditional probabili-
ties of the attributes with decision trees. In ex-
periments with German and Czech corpora, this
method achieved a higher tagging accuracy than
two state-of-the-art general-purpose POS taggers
(TnT and SVMTool).
References
Brants, Sabine, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories, Sozopol.
Brants, Thorsten. 2000. TnT - a statistical part-of-
speech tagger. In Proceedings of the Sixth Applied
Natural Language Processing Conference ANLP-
2000, Seattle, WA.
Breiman, L., J. H. Friedman, R. A. Olshen, and C. J.
Stone. 1984. Classification and Regression Trees.
Wadsworth and Brooks, Pacific Grove CA.
Ferri, C., P. Flach, and J. Hern?andez-Orallo. 2003. Im-
proving the AUC of probabilistic estimators trees. In
Proceedings of 14th European Conference on Ma-
chine Learning (ECML?03), pages 121?132.
Gim?enez, Jes?us and Llu??s M`arquez. 2004. SVMTool:
A general POS tagger generator based on support
vector machines. In Proceedings of the IV Interna-
tional Conference on Language Resources and Eval-
uation (LREC?04), pages 43?46, Lisbon, Portugal.
Haji?c, Jan and Barbora Vidov?a-Hladk?a. 1998. Tag-
ging inflective languages: Prediction of morpholog-
ical categories for a rich, structured tagset. In Pro-
ceedings of ACL-COLING?98, Montreal, Canada.
Haji?c, Jan, Pavel Krbec, Karel Oliva, Pavel Kv?eto?n,
and Vladim??r Petkevi?c. 2001. Serial combination of
rules and statistics: A case study in czech tagging. In
Proceedings of the 39th Annual Meeting of the ACL,
Toulouse, France.
Hladk?a, Barbora Vidov?a, Jan Hajic, Jir?? Hana, Jaroslava
Hlav?acov?a, Jir?? M??rovsk?y, and Jan Votrubec. 2007.
Czech Academic Corpus 1.0 Guide. Karolinum
Press, Prag, Czechia.
Kempe, Andr?e. 1994. Probabilistic tagging with fea-
ture structures. In Proceedings of the 15th Inter-
national Conference on Computational Linguistics
(COLING 1994), pages 161?165, Kyoto, Japan.
Magerman, David M. 1994. Natural Language Pro-
cessing as Statistical Pattern Recognition. Ph.D.
thesis, Stanford University.
M`arquez, Llu??s. 1999. POS Tagging : A Ma-
chine Learning Approach based on Decision Trees.
Ph.D. thesis, Dep. LSI, Universitat Politecnica de
Catalunya (UPC), Barcelona, Spain, July.
Mingers, John. 1989. An empirical comparison of
pruning methods for decision tree induction. Ma-
chine Learning, 4:227?243.
Provost, Foster and Pedro Domingos. 2003. Tree
induction for probability-based ranking. Machine
Learning, 52(3):199?215.
Quinlan, J. Ross. 1993. C4.5 : Programs for Machine
Learning. Morgan Kaufmann, San Mateo , CA.
Schmid, Helmut. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, pages 44?49, Manchester, UK.
Spoustov?a, Drahom??ra, Jan Haji?c, Jan Votrubec, Pavel
Krbec, and Pavel Kv?eto?n. 2007. The best of two
worlds: Cooperation of statistical and rule-based tag-
gers for czech. In Proceedings of the Workshop on
Balto-Slavonic Natural Language Processing, pages
67?74, Prague, Czech Republic, June.
Toutanova, Kristina, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network. In
Proceedings of HLT-NAACL 2003, pages 252?259,
Edmonton, Canada.
Votrubec, Jan. 2006. Morphological tagging based on
averaged perceptron. In Proceedings of the 15th An-
nual Conference of Doctoral Students (WDS), pages
191?195.
784
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 692?700,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
 
 
Tagging Urdu Text with Parts of Speech: A Tagger Comparison 
 
Hassan Sajjad 
Universit?t Stuttgart 
Stuttgart. Germany 
sajjad@ims.uni-stuttgart.de 
Helmut Schmid 
Universit?t Stuttgart 
Stuttgart, Germany 
schmid@ims.uni-stuttgart.de 
  
 
  
Abstract 
In this paper, four state-of-art probabilistic 
taggers i.e. TnT tagger, TreeTagger, RF tagger 
and SVM tool, are applied to the Urdu lan-
guage. For the purpose of the experiment, a 
syntactic tagset is proposed. A training corpus 
of 100,000 tokens is used to train the models. 
Using the lexicon extracted from the training 
corpus, SVM tool shows the best accuracy of 
94.15%. After providing a separate lexicon of 
70,568 types, SVM tool again shows the best 
accuracy of 95.66%. 
1 Urdu Language 
Urdu belongs to the Indo-Aryan language family. 
It is the national language of Pakistan and is one 
of the official languages of India. The majority 
of the speakers of Urdu spread over the area of 
South Asia, South Africa and the United King-
dom1. 
Urdu is a free order language with general 
word order SOV. It shares its phonological, mor-
phological and syntactic structures with Hindi. 
Some linguists considered them as two different 
dialects of one language (Bhatia and Koul, 
2000). However, Urdu is written in Perso-arabic 
script and inherits most of the vocabulary from 
Arabic and Persian. On the other hand, Hindi is 
written in Devanagari script and inherits vocabu-
lary from Sanskrit. 
Urdu is a morphologically rich language. 
Forms of the verb, as well as case, gender, and 
number are expressed by the morphology. Urdu 
represents case with a separate character after the 
head noun of the noun phrase. Due to their sepa-
rate occurrence and their place of occurrence, 
they are sometimes considered as postpositions. 
Considering them as case markers, Urdu has no-
                                                 
1 http://www.ethnologue.com/14/show_language.asp? 
code=URD 
minative, ergative, accusative, dative, instrumen-
tal, genitive and locative cases (Butt, 1995: pg 
10). The Urdu verb phrase contains a main verb, 
a light verb describing the aspect, and a tense 
verb describing the tense of the phrase (Hardie, 
2003; Hardie, 2003a). 
2 Urdu Tagset 
There are various questions that need to be ans-
wered during the design of a tagset. The granu-
larity of the tagset is the first problem in this re-
gard. A tagset may consist either of general parts 
of speech only or it may consist of additional 
morpho-syntactic categories such as number, 
gender and case. In order to facilitate the tagger 
training and to reduce the lexical and syntactic 
ambiguity, we decided to concentrate on the syn-
tactic categories of the language. Purely syntactic 
categories lead to a smaller number of tags which 
also improves the accuracy of manual tagging2 
(Marcus et al, 1993). 
Urdu is influenced from Arabic, and can 
be considered as having three main parts of 
speech, namely noun, verb and particle (Platts, 
1909; Javed, 1981; Haq, 1987). However, some 
grammarians proposed ten main parts of speech 
for Urdu (Schmidt, 1999). The work of Urdu 
grammar writers provides a full overview of all 
the features of the language. However, in the 
perspective of the tagset, their analysis is lacking 
the computational grounds. The semantic, mor-
phological and syntactic categories are mixed in 
their distribution of parts of speech. For example, 
Haq (1987) divides the common nouns into sit-
uational (smile, sadness, darkness), locative 
(park, office, morning, evening), instrumental 
(knife, sword) and collective nouns (army, data). 
In 2003, Hardie proposed the first com-
putational part of speech tagset for Urdu (Hardie, 
                                                 
2 A part of speech tagger for Indian languages, available at 
http://shiva.iiit.ac.in/SPSAL2007 /iiit_tagset_guidelines.pdf 
692
  
2003a). It is a morpho-syntactic tagset based on 
the EAGLES guidelines. The tagset contains 350 
different tags with information about number, 
gender, case, etc. (van Halteren, 2005). The 
EAGLES guidelines are based on three levels, 
major word classes, recommended attributes and 
optional attributes. Major word classes include 
thirteen tags: noun, verb, adjective, pro-
noun/determiner, article, adverb, adposition, con-
junction, numeral, interjection, unassigned, resi-
dual and punctuation. The recommended 
attributes include number, gender, case, finite-
ness, voice, etc.3 In this paper, we will focus on 
purely syntactic distributions thus will not go 
into the details of the recommended attributes of 
the EAGLES guidelines. Considering the 
EAGLES guidelines and the tagset of Hardie in 
comparison with the general parts of speech of 
Urdu, there are no articles in Urdu. Due to the 
phrase level and semantic differences, pronoun 
and demonstrative are separate parts of speech in 
Urdu. In the Hardie tagset, the possessive pro-
nouns like  /mera/ (my), 	
 /tumhara/ 
(your), 	 /humara/ (our) are assigned to the 
category of possessive adjective. Most of the Ur-
du grammarians consider them as pronouns 
(Platts, 1909; Javed, 1981; Haq, 1987). However, 
all these possessive pronouns require a noun in 
their noun phrase, thus show a similar behavior 
as demonstratives. The locative and temporal 
adverbs ( /yahan/ (here),  /wahan/ (there), 
 /ab/ (now), etc.) and, the locative and tempor-
al nouns ( /subah/ (morning),    /sham/ 
(evening),    /gher/ (home)) appear in a very 
similar syntactic context. In order to keep the 
structure of pronoun and noun consistent, loca-
tive and temporal adverbs are treated as pro-
nouns. The tense and aspect of a verb in Urdu is 
represented by a sequence of auxiliaries. Consid-
er the example4: 
 
       
      Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 515?522, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Disambiguation of Morphological Structure using a PCFG
Helmut Schmid
Institute for Natural Language Processing (IMS)
University of Stuttgart
Germany
schmid@ims.uni-stuttgart.de
Abstract
German has a productive morphology and
allows the creation of complex words
which are often highly ambiguous. This
paper reports on the development of a
head-lexicalized PCFG for the disam-
biguation of German morphological anal-
yses. The grammar is trained on unla-
beled data using the Inside-Outside algo-
rithm. The parser achieves a precision
of more than 68% on difficult test data,
which is 23% more than the baseline ob-
tained by randomly choosing one of the
simplest analyses. Remarkable is the fact
that precision drops to 52% without lexi-
calization.
1 Introduction
German words may be as complex as the fol-
lowing title of a bill: Rindfleischetikettierungs-
u?berwachungsaufgabenu?bertragungsgesetz (law for
the transfer of the task of controlling the labeling
of beef). The complexity is due to the productive
morphological processes of derivation (e.g. Etiket-
tierung (labeling) = Etikett (label) + ier (deriva-
tional suffix) + ung (nominalization suffix)) and
compounding (e.g. Rindfleisch (beef) = Rind (cattle)
+ Fleisch (meat)). For many words, there is more
than one possible analysis. The German SMOR
morphology (Schmid et al, 2004) e.g. generates 24
analyses for the word Abteilungen. If differences in
the case feature are ignored, there are still six analy-
ses, all of them plural:
? Abt (abbot) Ei (egg) Lunge (lung) n (plural in-
flectional ending)
? Abt (abbot) ei (abbot ? abbey) Lunge (lung) n
(plural inflectional ending)
? Abt (abbot) eil (hurry) ung (nominalization suf-
fix) en (plural inflectional ending)
? Abtei (abbey) Lunge (lung) n (plural inflec-
tional ending)
? Abteilung (department) en (plural inflectional
ending)
? ab (separable verb prefix) teil (divide) ung
(nominalization suffix) en (plural inflectional
ending)
Here ? and in many other cases, as well ? the least
complex analysis (defined as the number of deriva-
tion and compounding steps), namely the plural of
Abteilung (department), is the correct one. This
heuristic is not always successful, however. The
word Reisende e.g. is analyzed as the compound of
Reis (rice) + Ende (end), and alternately as the nom-
inalization of the present participle reisend (travel-
ing). The latter one is correct although it requires
two derivational steps (formation of the participle
plus nominalization), while the former requires only
one compounding step.
The least complex analysis is not necessarily
unique. One reason is, that German word forms are
often ambiguous wrt. number, gender and case. The
adjective kleine (small) e.g. receives 7 analyses by
SMOR which differ only in the agreement features.
515
Another reason is that word forms are ambiguous
wrt. the part of speech. The word gerecht e.g. is ei-
ther an adjective (fair) or the past participle of the
verb rechen (to rake). Similarly, the word gerade
is either an adjective (straight) or an adverb (just).
These ambiguities can be resolved based on the con-
text e.g. with a part-of-speech tagger.
Other types of ambiguities are not disambiguated
by the syntactic context because the morphosyntac-
tic features are invariant. Compounds with three el-
ements like Sonderpreisliste, for instance, are sys-
tematically ambiguous between a left-branching (list
of special prices) and a right-branching structure
(special price-list), but unambiguous regarding their
part of speech and agreement features. Some word
forms like Schmerzfreiheit (absence of pain) can ei-
ther be analyzed as derivations (schmerzfrei-heit ?
?painless-ness?) or as compounds (Schmerz-freiheit
? ?pain freedom?). Again, there is no difference in
the morphosyntactic features. A further source of
ambiguity are the stems: Consider the word Mit-
telzuweisung (allocation of resources). The com-
pounding stem mittel could either originate from the
adjective mittel (average) or from the noun Mittel
(means). All these ambiguities are not resolvable by
the syntactic context because their syntactic prop-
erties are identical. However, most of these words
have a preferred reading. Nah verkehrs zug (com-
muter train) e.g. is likely to have a left-branching
structure, whereas the correct analysis of Computer
bild schirm (computer monitor) is right-branching.
Considering these features of German morphol-
ogy, the following disambiguation strategy for mor-
phological ambiguities is proposed: Frequent words
should be manually disambiguated and the cor-
rect analysis/analyses should be explicitly stored in
the lexicon. Ambiguities involving different mor-
phosyntactic features should be resolved by a tagger
or parser. The elimination of the remaining ambigui-
ties, namely ambiguities of rare words which are not
reflected by the morphosyntactic features, requires a
different method. A general strategy is to generate
the set of possible analyses, to rank them accord-
ing to some criterion and to return the best analysis
(or analyses). One very simple ranking criterion is
the complexity of the analysis e.g. measured by the
number of derivational and compounding steps. We
will use this criterion as a baseline to which we com-
pare our method.
Given an FST-based morphological analyzer and
a training corpus consisting of manually disam-
biguated analyses, it is also possible to estimate tran-
sition probabilities for the finite state transducer and
to disambiguate by choosing the most probable path
through the transducer network for a given word. A
drawback of this approach is the limitation of the
type of analyses that finite state transducers can gen-
erate. A finite state transducer maps a regular lan-
guage, the set of word forms, to another regular lan-
guage, the set of analyses. Therefore it is not able to
produce structured analyses as shown in figure 1 (for
arbitrary depths). It also fails to represent non-local
dependencies, like the one between Vertrag (con-
tract) and Lo?sung (solution) in the second analysis
of figure 1.
ung
auf
Losung..
N
N N
V N V N
miet Vertrag saufV
los..
N N
N
V N
miet Vertrags
V Suff
Pref
Figure 1: Two morphological analyses of the Ger-
man word Mietvertragsauflo?sung (leasing contract
cancellation); the first one is correct.
Given the limitations of weighted finite-state
transducers, we propose to use a more power-
ful formalism, namely head-lexicalized probabilis-
tic context-free grammars (Carroll and Rooth, 1998;
Charniak, 1997) to rank the analyses. Context-free
grammars have, of course, no difficulties to generate
the analyses shown in figure 1. By assigning prob-
abilities to the grammar rules, we obtain a proba-
bilistic context-free grammar (PCFG) which allows
the parser to distinguish between frequent and rare
morphological constructions. Nouns e.g. are much
more likely to be compounds than verbs. In head-
lexicalized PCFGs (HL-PCFGs), the probability of
a rule also depends on the lexical head of the con-
stituent. HL-PCFGs are therefore able to learn that
nouns headed by Problem (problem) are more likely
to be compounds (e.g. Schulprobleme (school prob-
lems)) than nouns headed by Samstag (Saturday).
Moreover, HL-PCFGs represent lexical dependen-
516
cies like that between Vertrag and Lo?sung in fig-
ure 1.
b b b b
b b
b
b
Abt
Ab ung
lunge
en
nei
eil
Abtei
Abteilung
teil
Figure 2: Morpheme lattice
In this paper, we present a HL-PCFG-based dis-
ambiguator for German. Using the SMOR morpho-
logical analyzer, the input words are first split into
morpheme sequences and then analyzed with a HL-
PCFG parser. Due to ambiguities, the parser?s input
is actually a lattice rather than a sequence (see the
example in figure 2).
The rest of the paper is organized as follows:
In section 2, we briefly review head-lexicalized
PCFGs. Section 3 summarizes some important fea-
tures of SMOR. The development of the grammar
will be described in section 4. Section 5 explains the
training strategy, and section 6 reports on the annota-
tion of the test data. Section 7 presents the results of
an evaluation, section 8 comments on related work,
and section 9 summarizes the main points of the pa-
per. Finally, section 10 gives an outlook on future
work.
2 Head-Lexicalized PCFGs
A head-lexicalized parse tree is a parse tree in which
each constituent is labeled with its category and its
lexical head. The lexical head of a terminal symbol
is the symbol itself and the lexical head of a non-
terminal symbol is the lexical head of its (unique)
head child.
In a head-lexicalized PCFG (HL-PCFG) (Carroll
and Rooth, 1998; Charniak, 1997), one symbol on
the right-hand side of each rule is marked as the
head. A HL-PCFG assumes that (i) the probability
of a rule depends on the category and the lexical
head of the expanded constituent and (ii) that the
lexical head of a non-head node depends on its
own category, and the category and the lexical
head of the parent node. The probability of a
head-lexicalized parse tree is therefore:
pstart(cat(root)) pstart(head(root)|cat(root))??
n?N prule(rule(n)|cat(n), head(n))??
n?A phead(head(n)|cat(n), pcat(n), phead(n))
where
root is the root node of the parse tree
cat(n) is the syntactic category of node n
head(n) is the lexical head of node n
rule(n) is the grammar rule which expands node n
pcat(n) is the syntactic category of the parent of n
phead(n) is the lexical head of the parent of n
HL-PCFGs have a large number of parameters
which need to estimated from training data. In
order to avoid sparse data problems, the parameters
usually have to be smoothed. HL-PCFGs can either
be trained on labeled data (supervised training) or
on unlabeled data (unsupervised training) using
the Inside-Outside algorithm, an instance of the
EM algorithm. Training on labeled data usually
gives better results, but it requires a treebank
which is expensive to create. In our experiments,
we used unsupervised training with the LoPar
parser which is available at http://www.ims.uni-
stuttgart.de/projekte/gramotron/SOFTWARE/LoPar-
en.html.
3 SMOR
SMOR (Schmid et al, 2004) is a German FST-
based morphological analyzer which covers inflec-
tion, compounding, and prefix as well as suffix
derivation. It builds on earlier work reported in
(Schiller, 1996) and (Schmid et al, 2001).
SMOR uses features to represent derivation con-
straints. German derivational suffixes select their
base in terms of part of speech, the stem type
(derivation or compounding stem)1, the origin (na-
tive, classical, foreign), and the structure (simplex,
compound, prefix derivation, suffix derivation) of
the stem which they combine with. This informa-
tion is encoded with features. The German deriva-
tion suffix lich e.g. combines with a simplex deriva-
tion stem of a native noun to form an adjective. The
feature constraints of lich are therefore (1) part of
speech = NN (2) stem type = deriv (3) origin = na-
tive and (4) structure = simplex.
1Suffixes which combine with compounding stems histori-
cally evolved from compounding constructions.
517
4 The Grammar
The grammar used by the morphological disam-
biguator has a small set of rather general cate-
gories for prefixes (P), suffixes (S), uninflected base
stems (B), uninflected base suffixes (SB), inflec-
tional endings (F) and other morphemes (W). There
is only one rule for compounding and prefix and
suffix derivation, respectively, and two rules for the
stem and suffix inflection. Additional rules intro-
duce the start symbol TOP and generate special
word forms like hyphenated (Thomas-Mann-Stra?e)
or truncated words (Vor-). Overall, the base gram-
mar has 13 rules. Inflection is always attached low
in order to avoid spurious ambiguities. The part of
speech is encoded as a feature.
Like SMOR, the grammar encodes derivation
constraints with features. Number, gender and case
are not encoded. Ambiguities in the agreement fea-
tures are therefore not reflected in the parses which
the grammar generates. This allows us to abstract
away from this type of ambiguity which cannot be
resolved without contextual information. If some
application requires agreement information, it has to
be reinserted after disambiguation.
The feature grammar is compiled into a context-
free grammar with 1973 rules. In order to reduce
the grammar size, the features for origin and com-
plexity were not compiled out. Figure 3 shows a
compounding rule (building a noun base stem from
a noun compounding stem and a noun base stem),
a suffix derivation rule (building an adjective base
stem from a noun derivation stem and a derivation
suffix), a prefix derivation rule (prefixing a verbal
compounding stem) and two inflection rules (for the
inflection of a noun and a nominal derivation suffix,
respectively) from the resulting grammar. The quote
symbol marks the head of a rule.
W.NN.base ? W.NN.compound W.NN.base?
W.ADJ.base ? W.NN.deriv S.NN.deriv.ADJ.base
W.V.compound ? P.V W.V.compound?
W.NN.base ? B.NN.base? F.NN
S.ADJ.deriv.NN.base ? SB.ADJ.deriv.NN.base?
F.NN
Figure 3: Some rules from the context-free grammar
The parser retrieves the categories of the mor-
phemes from a lexicon which also contains infor-
mation about the standard form of a morpheme.
The representation of the morphemes returned by
the FST-based word splitter is close to the surface
form. Only capitalization is taken over from the
standard form. The adjective ursa?chlich (causal), for
instance, is split into Ursa?ch and lich. The lexicon
assigns to Ursa?ch the category W.NN.deriv and the
standard form Ursache (cause).
5 PCFG Training
PCFG training normally requires manually anno-
tated training data. Because a treebank of Ger-
man morphological analyses was not available,
we decided to try unsupervised training using the
Inside-Outside algorithm (Lari and Young, 1990).
We worked with unlexicalized as well as head-
lexicalized PCFGs (Carroll and Rooth, 1998; Char-
niak, 1997). The lexicalized models used the stan-
dard form of the morphemes (see the previous sec-
tion) as heads.
The word list from a German 300 million word
newspaper corpus was used as training data. From
the 3.2 million tokens in the word list, SMOR suc-
cessfully analyzed 2.3 million tokens which were
used in the experiment. Training was either type-
based (with each word form having the same weight)
or token-based (with weights proportional to the fre-
quency). We experimented with uniform and non-
uniform initial distributions. In the uniform model,
each rule had an initial frequency of 1 from which
the probabilities were estimated. In the non-uniform
model, the frequency of two classes of rules was in-
creased to 1000. The first class are the rules which
expand the start symbol TOP to an adjective or ad-
verb, leading to a preference of these word classes
over other word classes, in particular verbs. The
second class is formed by rules generating inflec-
tional endings, which induces a preference for sim-
pler analyses.
6 Test Data
The test data was extracted from a corpus of the Ger-
man newspaper Die Zeit which was not part of the
training corpus. We prepared two different test cor-
pora. The first test corpus (data1) consisted of 425
words extracted from a randomly selected part of the
518
corpus. We only extracted words with at least one
letter which were ambiguous (ignoring ambiguities
in number, gender and case) and either nouns, verbs
or adjectives and not from the beginning of a sen-
tence. Duplicates were retained. The words were
parsed and manually disambiguated. We looked at
the context of a word, where this was necessary for
disambiguation. Words without a correct analysis
were deleted.
In order to obtain more information on the types
of ambiguity and their frequency, 200 words were
manually classified wrt. the class of the ambiguity.
The following results were obtained:
? 39 words (25%) were ambiguous between an
adjective and a verb like gerecht - ?just? (ad-
jective) vs. past participle of rechen (to rake).
? 28 words (18%) were ambiguous between a
noun and a proper name like Mann - ?man? vs.
Thomas Mann
? 19 words were ambiguous between an adjective
and an adverb like gerade - ?straight? vs. ?just?
(adverb)
? 14 words (9%) showed a complex ambiguity
involving derivation and compounding like the
word u?berlieferung (tradition) which is either
a nominalization of the prefix verb u?berliefern
(to bequeath) or a compound of the stems u?ber
(over) and Lieferung (delivery).
? 13 words (8%) were compounds which were
ambiguous between a left-branching and a
right-branching structure like Welt rekord ho?he
(world record height)
? In 10 words (5%), there was an ambiguity be-
tween an adjective and a proper name or noun
stem - as in Ho?chstleistung (maximum perfor-
mance) where ho?chst can be derived from the
proper name Ho?chst (a German city) or the su-
perlative ho?chst (highest)
? 6 words (3%) showed a systematic ambigu-
ity between an adjective and a noun caused
by adding the suffix er to a city name, like
Moskauer - ?Moskau related? vs. ?person from
Moskau?
? Another 6 words were ambiguous between two
different noun stems like Halle which is either
singular form of Halle (hall) or the plural form
of Hall (reverberation)
Overall 50% of the ambiguities involved a part-of-
speech ambiguity.
The second set of test data (data2) was designed
to contain only infrequent words which were not
ambiguous wrt. part of speech. It was extracted
from the same newspaper corpus. Here, we ex-
cluded words which were (1) sentence-initial (in or-
der to avoid problems with capitalized words) (2)
not analyzed by SMOR (3) ambiguous wrt. part of
speech (4) from closed word classes or (5) simplex
words. Furthermore, we extracted only words with
more than one simplest2 analysis, in order to make
the test data more challenging. The extracted words
were sorted by frequency and a block of 1000 word
forms was randomly selected from the lower fre-
quency range. All of them had occurred 4 times. We
focussed on rare words because frequent words are
better disambiguated manually and stored in a table
(see the discussion in the introduction).
The 1000 selected word forms were parsed and
manually disambiguated. 193 problematic words
were deleted from the evaluation set because either
(1) no analysis was correct (e.g. Elsevier, which was
not analyzed as a proper name) or (2) there was a
true ambiguity (e.g. Rottweiler which is either a dog
breed or a person from the city of Rottweil or (3)
the lemma was not unique (Drehtu?r (revolving door)
could be lemmatized to Drehtu?r or Drehtu?re with no
difference in meaning.) or (4) several analyses were
equivalent. The disambiguation was often difficult.
Even among the words retained in the test set, there
were many that we were not fully sure about. An
example is the compound Natur eis bahn (?natural
ice rink?) which we decided to analyze as Natur-
Eisbahn (nature ice-rink) rather than Natureis-Bahn
(nature-ice rink).
7 Results
The parser was trained using the Inside-Outside al-
gorithm. By default, (a) the initialization of the rule
2The complexity of an analysis is measured by the number
of derivation and compounding steps.
519
probabilities was non-uniform as described in sec-
tion 5, (b) training was based on tokens (i.e. the
frequency of the training items was taken into ac-
count), and (c) all training iterations were lexical-
ized. Training was quite fast. One training iteration
on 2.3 million word forms took about 10 minutes on
a Pentium IV running at 3 GHz.
Figure 4 shows the exact match accuracy of the
Viterbi parses depending on the number of training
iterations, which ranges from 0 (the initial, untrained
model) to 15. For comparison, a baseline result is
shown which was obtained by selecting the set of
simplest analyses and choosing one of them at ran-
dom3. The baseline accuracy was 45.3%. The pars-
ing accuracy of the default model jumps from a start-
ing value of 41.8% for the untrained model (which is
below the baseline) to 58.5% after a single training
iteration. The peak performance is reached after 8
iterations with 65.4%. The average accuracy of the
models obtained after 6-25 iterations is 65.1%.
40
45
50
55
60
65
70
0 2 4 6 8 10 12 14 16
ac
cu
ra
cy
iterations
default
type-based
uniform
unlexicalisedbaseline
Figure 4: Exact match accuracy on data2
Results obtained with type-based training, where
each word receives the same weight ignoring its
frequency, were virtually identical to those of the
default model. If the parser training was started
with a uniform initial model, however, the accu-
racy dropped by about 6 percentage points. Figure 4
also shows that the performance of an unlexicalized
3In fact, we counted a word with n simplest analyses as 1/n
correct instead of actually selecting one analysis at random, in
order to avoid a dependency of the baseline result on the random
number generation.
PCFG is about 13% lower.
We also experimented with a combination of un-
lexicalized and lexicalized training. Lexicalized
models have a huge number of parameters. There-
fore, there is a large number of locally optimal pa-
rameter settings to which the unsupervised training
can be attracted. Purely lexicalized training is likely
to get stuck in a local optimum which is close to the
starting point. Unlexicalized models, on the other
hand, have fewer parameters, a smaller number of
local optima and a smoother search space. Unlex-
icalized training is therefore more likely to reach a
globally (near-)optimal point and provides a better
starting point for the lexicalized training.
Figure 5 shows that initial unlexicalized training
indeed improves the accuracy of the parser. With
one iteration of unlexicalized training (see ?unlex 1?
in figure 5), the accuracy increased by about 3%.
The maximum of 68.4% was reached after 4 iter-
ations of lexicalized training. The results obtained
with 2 iterations of unlexicalized training were very
similar. With 3 iterations, the performance dropped
almost to the level of the default model. It seems
that some of the general preferences learned during
unlexicalized training are so strong after three itera-
tions that they cannot be overruled anymore by the
lexeme-specific preferences learned in the lexical-
ized training.
40
45
50
55
60
65
70
0 2 4 6 8 10 12 14 16 18
ac
cu
ra
cy
iterations
default
unlex 1
unlex 2
unlex 3
Figure 5: Results on data2 with 0 (default), 1, 2,
or 3 iterations of unlexicalized training, followed by
lexicalized training
In order to assess the parsing results qualitatively,
520
100 parsing errors of version ?unlex 2? were ran-
domly selected and inspected. It turned out that
the parser always preferred right-branching struc-
tures over left-branching structures in complex com-
pounds with three or more elements, which resulted
in 57 errors caused by left-branching structures.
Grammars trained without the initial unlexicalized
training showed no systematic preference for right-
branching structures. In the test data, left-branching
structures were two times more frequent than right-
branching structures.
29 disambiguation errors resulted from selecting
the wrong stem although the structure of the analy-
sis was otherwise correct. In the word Rechtskon-
struktion (legal construction), for instance, the first
element Rechtswas derived from the adjective rechts
(right) rather than the noun Recht (law). Similarly,
the adjective quelloffen (open-source) was derived
from the verb quellen (to swell) rather than the noun
Quelle (source).
Six errors involved a combination of compound-
ing and suffix derivation (e.g. the word Flugbe-
gleiterin (stewardess)). The parser preferred the
analysis where the derivation is applied first (Flug-
Begleiterin (flight attendant[female])), whereas in
the gold standard analysis, the compound is formed
first (Flugbegleiter-in (steward-ess).
In order to better understand the benefits of unlex-
icalized training, we also examined the differences
between the best model obtained with one iteration
of unlexicalized (unlex1), and the best model ob-
tained without unlexicalized training (default).
30 cases involved left-branching vs. right-
branching compounds. The unlex1 model showed a
higher preference for right-branching structures than
the default model, but produced also left-branching
structures (unlike the model unlex2). In 15 of
the 30 cases, unlex1 correctly decided for a right-
branching structure; in 13 cases, unlex1 was wrong
with proposing a right-branching structure. In two
cases, unlex1 correctly predicted a left-branching
structure and the default model predicted a right-
branching structure.
32 differences were caused by lexical ambigui-
ties. In 24 cases, only one stem was ambiguous. 15
times unlex1 was right (e.g. Moskaureise - Moskow
trip[sg] vs. Moskow rice[pl]) and nine times the de-
fault model was right (e.g. Jodtabletten - iodine pill
vs. iodine tablet). In 8 cases, two morphemes were
involved in the ambiguity. In all these cases, un-
lex1 generated the correct analysis (e.g. Sportraum -
?sport room? vs. ?Spor[name] dream?).
Nine ambiguities involved the length of verb pre-
fixes. Six times, unlex1 correctly decided for a
longer prefix (e.g. gegenu?ber-stehen (to face) instead
of gegen-u?berstehen (to ?counter-survive?).
51
52
53
54
55
56
57
58
59
0 5 10 15 20 25
ac
cu
ra
cy
iterations
default
unlex 1
unlex 2
Figure 6: Accuracy on data1 after 0, 1, or 2 itera-
tions of unlexicalized training followed by lexical-
ized training
In another experiment, we tested the parser on the
first test data set (data1) where simplex words, part-
of-speech ambiguities, frequent words and repeated
occurrences were not removed. The baseline accu-
racy on this data was 43.75%. Figure 6 shows the
results obtained with different numbers of unlexical-
ized training iterations analogous to figure 5. Strictly
lexicalized training produced the best results, here.
The maximal accuracy was 58.59% which was ob-
tained after 7 iterations. In contrast to the exper-
iments on data2, the accuracy decreased by more
than 1.5% when the training was continued. As said
in the introduction, we think that part-of-speech am-
biguities are better resolved by a part-of-speech tag-
ger and that frequent words can be disambiguated
manually.
8 Related Work
New methods are often first developed for English
and later adapted to other languages. This might ex-
plain why morphological disambiguation has been
521
so rarely addressed in the past: English morphology
is seldom ambiguous except for noun compounds.
We are not aware of any work on the disam-
biguation of morphological analyses which is di-
rectly comparable to ours. Mark Lauer (1995) only
considered English noun compounds and applied a
different disambiguation strategy based on word as-
sociation scores.
Koehn and Knight (2003) proposed a splitting
method for German compounds and showed that
it improves statistical machine translation. Com-
pounds are split into smaller pieces (which have to
be words themselves) if the geometric mean of the
word frequencies of the pieces is higher than the fre-
quency of the compound. Information from a bilin-
gual corpus is used to improve the splitting accuracy.
Andreas Eisele (unpublished work) implemented
a statistical disambiguator for German based on
weighted finite-state transducers as described in the
introduction. However, his system fails to represent
and disambiguate the ambiguities observed in com-
pounds with three or more elements and similar con-
structions with structural ambiguities.
9 Summary
We presented a disambiguation method for German
morphological analyses which is based on a head-
lexicalized probabilistic context-free grammar. The
words are split into morpheme lattices by a finite
state morphology, and then parsed with the prob-
abilistic context-free grammar. The grammar was
trained on unlabeled data using the Inside-Outside
algorithm and evaluated on 807 manually disam-
biguated analyses of infrequent words. Different
training strategies have been compared. A com-
bination of one iteration of unlexicalized training
and four iterations of lexicalized training returned
the best results with over 68% exact match accu-
racy, compared to a baseline of 45% which was ob-
tained by randomly choosing one of the minimally
complex analyses. Without lexicalization, the ac-
curacy dropped by 15 percentage points, indicating
that lexicalization is essential for morphological dis-
ambiguation.
10 Future Work
There are several starting points for improvement.
Guidelines should be developed for the manual an-
notation of data in order to make it less dependent on
the annotator?s intuitions. More data should be an-
notated to create a treebank of morphological anal-
yses. Given such a treebank, the parser could be
trained on labeled data or on a combination of la-
beled and unlabeled data, which presumably would
further increase the parsing accuracy.
References
Glenn Carroll and Mats Rooth. 1998. Valence induc-
tion with a head-lexicalized PCFG. In Proceedings of
the Third Conference on Empirical Methods in Natural
Language Processing, Granada, Spain.
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word Statistics. In Proceed-
ings of the 14th National Conference on Artificial In-
telligence, Menlo Parc.
Philipp Koehn and Kevin Knight. 2003. Empirical meth-
ods for compound splitting. In Proceedings of the 10th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics, Budapest, Hun-
gary.
K. Lari and S. Young. 1990. The estimation of stochas-
tic context-free grammars using the inside-outside al-
gorithm. Computation Speech and Language Process-
ing, 4:35?56.
Mark Lauer. 1995. Corpus statistics meet the noun com-
pound: Some empirical results. In Proceedings of the
33rd Annual Meeting of the ACL, Massachusetts In-
stitute of Technology, pages 47?54, Cambridge, Mass.
electronically available at http://xxx.lanl.gov/abs/cmp-
lg/9504033.
Anne Schiller. 1996. Deutsche Flexions- und Kom-
positionsmorphologie mit PC-KIMMO. In Roland
Hausser, editor, Proceedings, 1. Morpholympics, Er-
langen, 7./8. Mrz 1994, Tu?bingen. Niemeyer.
Tanja Schmid, Anke Ldeling, Bettina Suberlich, Ulrich
Heid, and Bernd Mbius. 2001. DeKo: Ein System zur
Analyse komplexer Wrter. In GLDV - Jahrestagung
2001, pages 49?57.
Helmut Schmid, Arne Fitschen, and Ulrich Heid. 2004.
SMOR: A German computational morphology cover-
ing derivation, composition and inflection. In Pro-
ceedings of the 4th International Conference on Lan-
guage Resources and Evaluation, volume 4, pages
1263?1266, Lisbon, Portugal.
522
Parse Forest Computation of Expected Governors
Helmut Schmid
Institute for Computational Linguistics
University of Stuttgart
Azenbergstr. 12
70174 Stuttgart, Germany
schmid@ims.uni-stuttgart.de
Mats Rooth
Department of Linguistics
Cornell University
Morrill Hall
Ithaca, NY 14853, USA
mats@cs.cornell.edu
Abstract
In a headed tree, each terminal word
can be uniquely labeled with a gov-
erning word and grammatical relation.
This labeling is a summary of a syn-
tactic analysis which eliminates detail,
reflects aspects of semantics, and for
some grammatical relations (such as
subject of finite verb) is nearly un-
controversial. We define a notion
of expected governor markup, which
sums vectors indexed by governors and
scaled by probabilistic tree weights.
The quantity is computed in a parse for-
est representation of the set of tree anal-
yses for a given sentence, using vector
sums and scaling by inside probability
and flow.
1 Introduction
A labeled headed tree is one in which each non-
terminal vertex has a distinguished head child,
and in the usual way non-terminal nodes are la-
beled with non-terminal symbols (syntactic cat-
egories such as NP) and terminal vertices are
labeled with terminal symbols (words such as
The governor algorithm was designed and implemented
in the Reading Comprehension research group in the 2000
Workshop on Language Engineering at Johns Hopkins Uni-
versity. Thanks to Marc Light, Ellen Riloff, Pranav Anand,
Brianne Brown, Eric Breck, Gideon Mann, and Mike Thelen
for discussion and assistance. Oral presentations were made
at that workshop in August 2000, and at the University of
Sussex in January 2001. Thanks to Fred Jelinek, John Car-
roll, and other members of the audiences for their comments.
S  
NP  	
 
Peter
VP  
V  
reads
NP   	 
NP    
D 	 
every
N   	 
paper
PP:on    
P:on 
on
NP   

N   

markup
Figure 1: A tree with percolated lexical heads.
reads).1 We work with syntactic trees in which
terminals are in addition labeled with uninflected
word forms (lemmas) derived from the lexicon.
By percolating lemmas up the chains of heads,
each node in a headed tree may be labeled with
a lexical head. Figure 1 is an example, where lex-
ical heads are written as subscripts. We use the
notation
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 177?184,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Trace Prediction and Recovery
With Unlexicalized PCFGs and Slash Features
Helmut Schmid
IMS, University of Stuttgart
schmid@ims.uni-stuttgart.de
Abstract
This paper describes a parser which gen-
erates parse trees with empty elements in
which traces and fillers are co-indexed.
The parser is an unlexicalized PCFG
parser which is guaranteed to return the
most probable parse. The grammar is
extracted from a version of the PENN
treebank which was automatically anno-
tated with features in the style of Klein
and Manning (2003). The annotation in-
cludes GPSG-style slash features which
link traces and fillers, and other features
which improve the general parsing accu-
racy. In an evaluation on the PENN tree-
bank (Marcus et al, 1993), the parser
outperformed other unlexicalized PCFG
parsers in terms of labeled bracketing f-
score. Its results for the empty cate-
gory prediction task and the trace-filler co-
indexation task exceed all previously re-
ported results with 84.1% and 77.4% f-
score, respectively.
1 Introduction
Empty categories (also called null elements) are
used in the annotation of the PENN treebank (Mar-
cus et al, 1993) in order to represent syntactic
phenomena like constituent movement (e.g. wh-
extraction), discontinuous constituents, and miss-
ing elements (PRO elements, empty complemen-
tizers and relative pronouns). Moved constituents
are co-indexed with a trace which is located at
the position where the moved constituent is to be
interpreted. Figure 1 shows an example of con-
stituent movement in a relative clause.
Empty categories provide important informa-
tion for the semantic interpretation, in particular
NP
NP
NNS
things
SBAR
WHPP-1
IN
of
WHNP
WDT
which
S
NP-SBJ
PRP
they
VP
VBP
are
ADJP-PRD
JJ
unaware
PP
-NONE-
*T*-1
Figure 1: Co-indexation of traces and fillers
for determining the predicate-argument structure
of a sentence. However, most broad-coverage sta-
tistical parsers (Collins, 1997; Charniak, 2000,
and others) which are trained on the PENN tree-
bank generate parse trees without empty cate-
gories. In order to augment such parsers with
empty category prediction, three rather different
strategies have been proposed: (i) pre-processing
of the input sentence with a tagger which inserts
empty categories into the input string of the parser
(Dienes and Dubey, 2003b; Dienes and Dubey,
2003a). The parser treats the empty elements
like normal input tokens. (ii) post-processing
of the parse trees with a pattern matcher which
adds empty categories after parsing (Johnson,
2001; Campbell, 2004; Levy and Manning, 2004)
(iii) in-processing of the empty categories with a
slash percolation mechanism (Dienes and Dubey,
2003b; Dienes and Dubey, 2003a). The empty el-
ements are here generated by the grammar.
Good results have been obtained with all three
approaches, but (Dienes and Dubey, 2003b) re-
ported that in their experiments, the in-processing
of the empty categories only worked with lexi-
calized parsing. They explain that their unlex-
177
icalized PCFG parser produced poor results be-
cause the beam search strategy applied there elim-
inated many correct constituents with empty ele-
ments. The scores of these constituents were too
low compared with the scores of constituents with-
out empty elements. They speculated that ?doing
an exhaustive search might help? here.
In this paper, we confirm this hypothesis and
show that it is possible to accurately predict empty
categories with unlexicalized PCFG parsing and
slash features if the true Viterbi parse is com-
puted. In our experiments, we used the BitPar
parser (Schmid, 2004) and a PCFG which was ex-
tracted from a version of the PENN treebank that
was automatically annotated with features in the
style of (Klein and Manning, 2003).
2 Feature Annotation
A context-free grammar which generates empty
categories has to make sure that a filler exists for
each trace and vice versa. A well-known tech-
nique which enforces this constraint is the GPSG-
style percolation of a slash feature: All con-
stituents on the direct path from the trace to the
filler are annotated with a special feature which
represents the category of the filler as shown in fig-
ure 2. In order to restore the original treebank an-
NP
NP
NNS
things
SBAR
WHPP/WHPP
IN
of
WHNP
WDT
which
S/WHPP
NP-SBJ
PRP
they
VP/WHPP
VBP
are
ADJP-PRD/WHPP
JJ
unaware
PP/WHPP
-NONE-/WHPP
*T*/WHPP
Figure 2: Slash features: The filler node of cate-
gory WHNP is linked to the trace node via perco-
lation of a slash feature. The trace node is labeled
with *T*.
notation with co-reference indices from the repre-
sentation with slash features, the parse tree has to
be traversed starting at a trace node and following
the nodes annotated with the respective filler cate-
gory until the filler node is encountered. Normally,
the filler node is a sister node of an ancestor node
of the trace, i.e. the filler c-commands the trace
node, but in case of clausal fillers it is also possi-
ble that the filler dominates the trace. An example
is the sentence ?S-1 She had ? he informed her *-
1 ? kidney trouble? whose parse tree is shown in
figure 3.
Besides the slash features, we used other fea-
tures in order to improve the parsing accuracy of
the PCFG, inspired by the work of Klein and Man-
ning (2003). The most important ones of these
features1 will now be described in detail. Sec-
tion 4.3 shows the impact of these features on
labeled bracketing accuracy and empty category
prediction.
VP feature VPs were annotated with a feature
that distinguishes between finite, infinitive, to-
infinitive, gerund, past participle, and passive VPs.
S feature The S node feature distinguishes be-
tween imperatives, finite clauses, and several types
of small clauses.
Parent features Modifier categories like SBAR,
PP, ADVP, RB and NP-ADV were annotated with
a parent feature (cf. Johnson (1998)). The
parent features distinguish between verbal (VP),
adjectival (ADJP, WHADJP), adverbial (ADVP,
WHADVP), nominal (NP, WHNP, QP), preposi-
tional (PP) and other parents.
PENN tags The PENN treebank annotation uses
semantic tags to refine syntactic categories. Most
parsers ignore this information. We preserved
the tags ADV, CLR, DIR, EXT, IMP, LGS, LOC,
MNR, NOM, PRD, PRP, SBJ and TMP in combi-
nation with selected categories.
Auxiliary feature We added a feature to the
part-of-speech tags of verbs in order to distinguish
between be, do, have, and full verbs.
Agreement feature Finite VPs are marked with
3s (n3s) if they are headed by a verb with part-of-
speech VBZ (VBP).
Genitive feature NP nodes which dominate a
node of the category POS (possessive marker) are
marked with a genitive flag.
Base NPs NPs dominating a node of category
NN, NNS, NNP, NNPS, DT, CD, JJ, JJR, JJS, PRP,
RB, or EX are marked as base NPs.
1The complete annotation program is available
from the author?s home page at http://www.ims.uni-
stuttgart.de/ schmid
178
S-1
NP-SBJ
PRP
She
VP
VBD
had
PRN
:
?
S
NP-SBJ
PRP
he
VP
VBD
informed
NP
PRP
her
SBAR
-NONE-
0
S
-NONE-
*T*-1
:
?
NP
NN
kidney
NN
trouble
.
.
Figure 3: Example of a filler which dominates its trace
IN feature The part-of-speech tags of the 45
most frequent prepositions were lexicalized by
adding the preposition as a feature. The new part-
of-speech tag of the preposition ?by? is ?IN/by?.
Irregular adverbs The part-of-speech tags of
the adverbs ?as?, ?so?, ?about?, and ?not? were
also lexicalized.
Currency feature NP and QP nodes are marked
with a currency flag if they dominate a node of
category $, #, or SYM.
Percent feature Nodes of the category NP or
QP are marked with a percent flag if they dominate
the subtree (NN %). Any node which immediately
dominates the token %, is marked, as well.
Punctuation feature Nodes which dominate
sentential punctuation (.?!) are marked.
DT feature Nodes of category DT are split into
indefinite articles (a, an), definite articles (the),
and demonstratives (this, that, those, these).
WH feature The wh-tags (WDT, WP, WRB,
WDT) of the words which, what, who, how, and
that are also lexicalized.
Colon feature The part-of-speech tag ?:? was re-
placed with ?;?, ??? or ?...? if it dominated a cor-
responding token.
DomV feature Nodes of a non-verbal syntactic
category are marked with a feature if they domi-
nate a node of category VP, SINV, S, SQ, SBAR,
or SBARQ.
Gap feature S nodes dominating an empty NP
are marked with the feature gap.
Subcategorization feature The part-of-speech
tags of verbs are annotated with a feature which
encodes the sequence of arguments. The encod-
ing maps reflexive NPs to r, NP/NP-PRD/SBAR-
NOM to n, ADJP-PRD to j, ADVP-PRD to a,
PRT to t, PP/PP-DIR to p, SBAR/SBAR-CLR to
b, S/fin to sf, S/ppres/gap to sg, S/to/gap to st,
other S nodes to so, VP/ppres to vg, VP/ppast to
vn, VP/pas to vp, VP/inf to vi, and other VPs to
vo. A verb with an NP and a PP argument, for
instance, is annotated with the feature np.
Adjectives, adverbs, and nouns may also get a
subcat feature which encodes a single argument
using a less fine-grained encoding which maps PP
to p, NP to n, S to s, and SBAR to b. A node of
category NN or NNS e.g. is marked with a subcat
feature if it is followed by an argument category
unless the argument is a PP which is headed by
the preposition of.
RC feature In relative clauses with an empty
relative pronoun of category WHADVP, we mark
the SBAR node of the relative clause, the NP node
to which it is attached, and its head child of cate-
gory NN or NNS, if the head word is either way,
ways, reason, reasons, day, days, time, moment,
place, or position. This feature helps the parser
to correctly insert WHADVP rather than WHNP.
Figure 4 shows a sample tree.
TMP features Each node on the path between
an NP-TMP or PP-TMP node and its nominal head
is labeled with the feature tmp. This feature helps
the parser to identify temporal NPs and PPs.
MNR and EXT features Similarly, each node
on the path between an NP-EXT, NP-MNR or
ADVP-TMP node and its head is labeled with the
179
NP
NP/x
NN/x
time
SBAR/x
WHADVP-1
-NONE-
0
S
NP-SBJ
-NONE-
*
VP
TO
to
VP
VB
relax
ADVP-TMP
-NONE-
*T*-1
Figure 4: Annotation of relative clauses with
empty relative pronoun of category WHADVP
feature ext or mnr.
ADJP features Nodes of category ADJP which
are dominated by an NP node are labeled with the
feature ?post? if they are in final position and the
feature ?attr? otherwise.
JJ feature Nodes of category JJ which are dom-
inated by an ADJP-PRD node are labeled with the
feature ?prd?.
JJ-tmp feature JJ nodes which are dominated
by an NP-TMP node and which themselves dom-
inate one of the words ?last?, ?next?, ?late?, ?pre-
vious?, ?early?, or ?past? are labeled with tmp.
QP feature If some node dominates an NP node
followed by an NP-ADV node as in (NP (NP one
dollar) (NP-ADV a day)), the first child NP node
is labeled with the feature ?qp?. If the parent is an
NP node, it is also labeled with ?qp?.
NP-pp feature NP nodes which dominate a PP
node are labeled with the feature pp. If this PP
itself is headed by the preposition of, then it is an-
notated with the feature of.
MWL feature In adverbial phrases which nei-
ther dominate an adverb nor another adverbial
phrase, we lexicalize the part-of-speech tags of a
small set of words like ?least? (at least), ?kind?, or
?sort? which appear frequently in such adverbial
phrases.
Case feature Pronouns like he or him , but not
ambiguous pronouns like it are marked with nom
or acc, respectively.
Expletives If a subject NP dominates an NP
which consists of the pronoun it, and an S-trace in
sentences like It is important to..., the dominated
NP is marked with the feature expl.
LST feature The parent nodes of LST nodes2
are marked with the feature lst.
Complex conjunctions In SBAR constituents
starting with an IN and an NN child node (usu-
ally indicating one of the two complex conjunc-
tions ?in order to? or ?in case of?), we mark the
NN child with the feature sbar.
LGS feature The PENN treebank marks the
logical subject of passive clauses which are real-
ized by a by-PP with the semantic tag LGS. We
move this tag to the dominating PP.
OC feature Verbs are marked with an object
control feature if they have an NP argument which
dominates an NP filler and an S argument which
dominates an NP trace. An example is the sen-
tence She asked him to come.
Corrections The part-of-speech tags of the
PENN treebank are not always correct. Some of
the errors (like the tag NNS in VP-initial position)
can be identified and corrected automatically in
the training data. Correcting tags did not always
improve parsing accuracy, so it was done selec-
tively.
The gap and domV features described above
were also used by Klein and Manning (2003).
All features were automatically added to the
PENN treebank by means of an annotation pro-
gram. Figure 5 shows an example of an annotated
parse tree.
3 Parameter Smoothing
We extracted the grammar from sections 2?21 of
the annotated version of the PENN treebank. In
order to increase the coverage of the grammar,
we selectively applied markovization to the gram-
mar (cf. Klein and Manning (2003)) by replacing
long infrequent rules with a set of binary rules.
Markovization was only applied if none of the
non-terminals on the right hand side of the rule
had a slash feature in order to avoid negative ef-
fects on the slash feature percolation mechanism.
The probabilities of the grammar rules were
directly estimated with relative frequencies. No
smoothing was applied, here. The lexical prob-
abilities, on the other hand, were smoothed with
2LST annotates the list symbol in enumerations.
180
S/fin/.
NP-SBJ/3s/domV_<S>
NP/base/3s/expl
PRP/expl
It
S_<S>
-NONE-_<S>
*EXP*_#<S>
VP/3s+<S>
VBZ/pst
?s
PP/V
IN/up
up
PP/PP
TO
to
NP/base
PRP
you
S/to/gap+#<S>
NP-SBJ
-NONE-
*
VP/to
TO
to
VP/inf
VV/r
protect
NP/refl/base
PRP/refl
yourself
Figure 5: An Annotated Parse Tree
the following technique which was adopted from
Klein and Manning (2003). Each word is assigned
to one of 216 word classes. The word classes
are defined with regular expressions. Examples
are the class [A-Za-z0-9-]+-oldwhich con-
tains the word 20-year-old, the class [a-z][a-
z]+ifies which contains clarifies, and a class
which contains a list of capitalized adjectives like
Advanced. The word classes are ordered. If a
string is matched by the regular expressions of
more than one word class, then it is assigned to the
first of these word classes. For each word class,
we compute part-of-speech probabilities with rel-
ative frequencies. The part-of-speech frequen-
cies
 	
of a word

are smoothed by adding
the part-of-speech probability 

 	
of the word
class
 
according to equation 1 in order to ob-
tain the smoothed frequency 
 	
. The part-of-
speech probability of the word class is weighted
by a parameter  whose value was set to 4 after
testing on held-out data. The lexical probabilities
are finally estimated from the smoothed frequen-
cies according to equation 2.

 	 	Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 96?103,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Phonological Constraints and Morphological Preprocessing for
Grapheme-to-Phoneme Conversion
Vera Demberg
School of Informatics
University of Edinburgh
Edinburgh, EH8 9LW, GB
v.demberg@sms.ed.ac.uk
Helmut Schmid
IMS
University of Stuttgart
D-70174 Stuttgart
schmid@ims.uni-stuttgart.de
Gregor Mo?hler
Speech Technologies
IBM Deutschland Entwicklung
D-71072 Bo?blingen
moehler@de.ibm.com
Abstract
Grapheme-to-phoneme conversion (g2p) is a
core component of any text-to-speech sys-
tem. We show that adding simple syllab-
ification and stress assignment constraints,
namely ?one nucleus per syllable? and ?one
main stress per word?, to a joint n-gram
model for g2p conversion leads to a dramatic
improvement in conversion accuracy.
Secondly, we assessed morphological pre-
processing for g2p conversion. While mor-
phological information has been incorpo-
rated in some past systems, its contribution
has never been quantitatively assessed for
German. We compare the relevance of mor-
phological preprocessing with respect to the
morphological segmentation method, train-
ing set size, the g2p conversion algorithm,
and two languages, English and German.
1 Introduction
Grapheme-to-Phoneme conversion (g2p) is the task
of converting a word from its spelling (e.g. ?Stern-
aniso?l?, Engl: star-anise oil) to its pronunciation
(/"StERnPani:sP?:l/). Speech synthesis modules with
a g2p component are used in text-to-speech (TTS)
systems and can be be applied in spoken dialogue
systems or speech-to-speech translation systems.
1.1 Syllabification and Stress in g2p conversion
In order to correctly synthesize a word, it is not only
necessary to convert the letters into phonemes, but
also to syllabify the word and to assign word stress.
The problems of word phonemization, syllabifica-
tion and word stress assignment are inter-dependent.
Information about the position of a syllable bound-
ary helps grapheme-to-phoneme conversion. (Marc-
hand and Damper, 2005) report a word error rate
(WER) reduction of approx. 5 percentage points for
English when the letter string is augmented with syl-
labification information. The same holds vice-versa:
we found that WER was reduced by 50% when run-
ning our syllabifier on phonemes instead of letters
(see Table 4). Finally, word stress is usually defined
on syllables; in languages where word stress is as-
sumed1 to partly depend on syllable weight (such as
German or Dutch), it is important to know where ex-
actly the syllable boundaries are in order to correctly
calculate syllable weight. For German, (Mu?ller,
2001) show that information about stress assignment
and the position of a syllable within a word improve
g2p conversion.
1.2 Morphological Preprocessing
It has been argued that using morphological in-
formation is important for languages where mor-
phology has an important influence on pronuncia-
tion, syllabification and word stress such as Ger-
man, Dutch, Swedish or, to a smaller extent, also
English (Sproat, 1996; Mo?bius, 2001; Pounder and
Kommenda, 1986; Black et al, 1998; Taylor, 2005).
Unfortunately, these papers do not quantify the con-
tribution of morphological preprocessing in the task.
Important questions when considering the inte-
gration of a morphological component into a speech
1This issue is controversial among linguists; for an overview
see (Jessen, 1998).
96
synthesis system are 1) How large are the im-
provements to be gained from morphological pre-
processing? 2) Must the morphological system be
perfect or can performance improvements also be
reached with relatively simple morphological com-
ponents? and 3) How much does the benefit to
be expected from explicit morphological informa-
tion depend on the g2p algorithm? To determine
these factors, we compared morphological segmen-
tations based on manual morphological annotation
from CELEX to two rule-based systems and several
unsupervised data-based approaches. We also anal-
ysed the role of explicit morphological preprocess-
ing on data sets of different sizes and compared its
relevance with respect to a decision tree and a joint
n-gram model for g2p conversion.
The paper is structured as follows: We introduce
the g2p conversion model we used in section 2 and
explain how we implemented the phonological con-
straints in section 3. Section 4 is concerned with
the relation between morphology, word pronuncia-
tion, syllabification and word stress in German, and
presents different sources for morphological seg-
mentation. In section 5, we evaluate the contribution
of each of the components and compare our meth-
ods to state-of-the-art systems. Section 6 summa-
rizes our results.
2 Methods
We used a joint n-gram model for the grapheme-
to-phoneme conversion task. Models of this type
have previously been shown to yield very good g2p
conversion results (Bisani and Ney, 2002; Galescu
and Allen, 2001; Chen, 2003). Models that do not
use joint letter-phoneme states, and therefore are not
conditional on the preceding letters, but only on the
actual letter and the preceding phonemes, achieved
inferior results. Examples of such approaches using
Hidden Markov Models are (Rentzepopoulos and
Kokkinakis, 1991) (who applied the HMM to the
related task of phoneme-to-grapheme conversion),
(Taylor, 2005) and (Minker, 1996).
The g2p task is formulated as searching for the
most probable sequence of phonemes given the or-
thographic form of a word. One can think of it as a
tagging problem where each letter is tagged with a
(possibly empty) phoneme-sequence p. In our par-
ticular implementation, the model is defined as a
higher-order Hidden Markov Model, where the hid-
den states are a letter?phoneme-sequence pair ?l; p?,
and the observed symbols are the letters l. The out-
put probability of a hidden state is then equal to one,
since all hidden states that do not contain the ob-
served letter are pruned directly.
The model for grapheme-to-phoneme conver-
sion uses the Viterbi algorithm to efficiently com-
pute the most probable sequence p?n1 of phonemes
p?1, p?2, ..., p?n for a given letter sequence ln1 . The
probability of a letter?phon-seq pair depends on the
k preceding letter?phon-seq pairs. Dummy states ?#?
are appended at both ends of each word to indicate
the word boundary and to ensure that all conditional
probabilities are well-defined.
p?n1 = argmax
pn1
n+1?
i=1
P (?l; p?i | ?l; p?
i?1
i?k)
In an integrated model where g2p conversion, syl-
labification and word stress assignment are all per-
formed at the same time, a state additionally con-
tains a syllable boundary flag b and a stress flag a,
yielding ?l; p; b; a?i.
As an alternative architecture, we also designed a
modular system that comprises one component for
syllabification and one for word stress assignment.
The model for syllabification computes the most
probable sequence b?n1 of syllable boundary-tags b?1,
b?2, ..., b?n for a given letter sequence ln1 .
b?n1 = argmax
bn1
n+1?
i=1
P (?l; b?i | ?l; b?
i?1
i?k)
The stress assignment model works on syllables.
It computes the most probable sequence a?n1 of word
accent-tags a?1, a?2, ..., a?n for a given syllable se-
quence syln1 .
a?n1 = argmax
an1
n+1?
i=1
P (?syl; a?i | ?syl; a?
i?1
i?k)
2.1 Smoothing
Because of major data sparseness problems, smooth-
ing is an important issue, in particular for the stress
model which is based on syllable?stress-tag pairs.
Performance varied by up to 20% in function of the
smoothing algorithm chosen. Best results were ob-
tained when using a variant of Modified Kneser-Ney
Smoothing2 (Chen and Goodman, 1996).
2For a formal definition, see(Demberg, 2006).
97
2.2 Pruning
In the g2p-model, each letter can on average map
onto one of 12 alternative phoneme-sequences.
When working with 5-grams3, there are about 125 =
250,000 state sequences. To improve time and space
efficiency, we implemented a simple pruning strat-
egy that only considers the t best states at any mo-
ment in time. With a threshold of t = 15, about 120
words are processed per minute on a 1.5GHz ma-
chine. Conversion quality is only marginally worse
than when the whole search space is calculated.
Running time for English is faster, because the av-
erage number of candidate phonemes for each let-
ter is lower. We measured running time (including
training and the actual g2p conversion in 10-fold
cross validation) for a Perl implementation of our
algorithm on the English NetTalk corpus (20,008
words) on an Intel Pentium 4, 3.0 GHz machine.
Running time was less than 1h for each of the fol-
lowing three test conditions: c1) g2p conversion
only, c2) syllabification first, then g2p conversion,
c3) simultaneous g2p conversion and syllabification,
given perfect syllable boundary input, c4) simulta-
neous g2p conversion and syllabification when cor-
rect syllabification is not available beforehand. This
is much faster than the times for Pronunciation by
Analogy (PbA) (Marchand and Damper, 2005) on
the same corpus. Marchand and Damper reported a
processing time of several hours for c4), two days
for c2) and several days for c3).
2.3 Alignment
Our current implementation of the joint n-gram
model is not integrated with an automatic alignment
procedure. We therefore first aligned letters and
phonemes in a separate, semi-automatic step. Each
letter was aligned with zero to two phonemes and,
in the integrated model, zero or one syllable bound-
aries and stress markers.
3 Integration of Phonological Constraints
When analysing the results from the model that does
g2p conversion, syllabification and stress assign-
3There is a trade-off between long context windows which
capture the context accurately and data sparseness issues. The
optimal value k for the context window size depends on the
source language (existence of multiletter graphemes, complex-
ity of syllables etc.).
ment in a single step, we found that a large propor-
tion of the errors was due to the violation of basic
phonological constraints.
Some syllables had no syllable nucleus, while
others contained several vowels. The reason for the
errors is that German syllables can be very long and
therefore sparse, often causing the model to back-
off to smaller contexts. If the context is too small to
cover the syllable, the model cannot decide whether
the current syllable contains a nucleus.
In stress assignment, this problem is even worse:
the context window rarely covers the whole word.
The algorithm does not know whether it already as-
signed a word stress outside the context window.
This leads to a high error rate with 15-20% of in-
correctly stressed words. Thereof, 37% have more
than one main stress, about 27% are not assigned any
stress and 36% are stressed in the wrong position.
This means that we can hope to reduce the errors by
almost 2/3 by using phonological constraints.
Word stress assignment is a difficult problem in
German because the underlying processes involve
some deeper morphological knowledge which is not
available to the simple model. In complex words,
stress mainly depends on morphological structure
(i.e. on the compositionality of compounds and
on the stressing status of affixes). Word stress in
simplex words is assumed to depend on the sylla-
ble position within the word stem and on syllable
weight. The current language-independent approach
does not model these processes, but only captures
some of its statistics.
Simple constraints can help to overcome the prob-
lem of lacking context by explicitly requiring that
every syllable must have exactly one syllable nu-
cleus and that every word must have exactly one syl-
lable receiving primary stress.
3.1 Implementation
Our goal is to find the most probable syllabified
and stressed phonemization of a word that does not
violate the constraints. We tried two different ap-
proaches to enforce the constraints.
In the first variant (v1), we modified the proba-
bility model to enforce the constraints. Each state
now corresponds to a sequence of 4-tuples consist-
ing of a letter l, a phoneme sequence p, a syllable
boundary tag b, an accent tag a (as before) plus two
98
new flags A and N which indicate whether an ac-
cent/nucleus precedes or not. The A and N flags of
the new state are a function of its accent and syllable
boundary tag and the A and N flag of the preceding
state. They split each state into four new states. The
new transition probabilities are defined as:
P (?l; p; b; a?i | ?l; p; b; a?
i?1
i?k , A,N)
The probability is 0 if the transition violates a con-
straint, e.g., when the A flag is set and ai indicates
another accent.
A positive side effect of the syllable flag is that it
stores separate phonemization probabilities for con-
sonants in the syllable onset vs. consonants in the
coda. The flag in the onset is 0 since the nucleus has
not yet been encountered, whereas it is set to 1 in the
coda. In German, this can e.g. help in for syllable-
final devoicing of voiced stops and fricatives.
The increase in the number of states aggravates
sparse-data problems. Therefore, we implemented
another variant (v2) which uses the same set of states
(with A and N flags), but with the transition proba-
bilities of the original model, which did not enforce
the constraints. Instead, we modified the Viterbi al-
gorithm to eliminate the invalid transitions: For ex-
ample, a transition from a state with the A flag set
to a state where ai introduces a second stress, is al-
ways ignored. On small data sets, better results were
achieved with v2 (see Table 5).
4 Morphological Preprocessing
In German, information about morphological
boundaries is needed to correctly insert glottal stops
[P] in complex words, to determine irregular pro-
nunciation of affixes (v is pronounced [v] in ver-
tikal but [f] in ver+ticker+n, and the suffix syllable
heit is not stressed although superheavy and word
final) and to disambiguate letters (e.g. e is always
pronounced /@/ when occurring in inflectional suf-
fixes). Vowel length and quality has been argued
to also depend on morphological structure (Pounder
and Kommenda, 1986). Furthermore, morphologi-
cal boundaries overrun default syllabification rules,
such as the maximum onset principle.
Applying default syllabification to the word
?Sternaniso?l? would result in a syllabification into
Ster-na-ni-so?l (and subsequent phonemiza-
tion to something like /StE?"na:niz?:l/) instead of
Stern-a-nis-o?l (/"StE?nPani:sP?:l/). Syllabifi-
cation in turn affects phonemization since voiced
fricatives and stops are devoiced in syllable-final po-
sition. Morphological information also helps for
graphemic parsing of words such as ?Ro?schen?
(Engl: little rose) where the morphological bound-
ary between Ro?s and chen causes the string sch to
be transcribed to /s?/ instead of /S/. Similar ambigui-
ties can arise for all other sounds that are represented
by several letters in orthography (e.g. doubled con-
sonants, diphtongs, ie, ph, th), and is also valid for
English. Finally, morphological information is also
crucial to determine word stress in morphologically
complex words.
4.1 Methods for Morphological Segmentation
Good segmentation performance on arbitrary words
is hard to achieve. We compared several approaches
with different amounts of built-in knowledge. The
morphological information is encoded in the let-
ter string, where different digits represent different
kinds of morphological boundaries (prefixes, stems,
derivational and inflectional suffixes).
Manual Annotation from CELEX
To determine the upper bound of what can be
achieved when exploiting perfect morphological in-
formation, we extracted morphological boundaries
and boundary types from the CELEX database.
The manual annotation is not perfect as it con-
tains some errors and many cases where words are
not decomposed entirely. The words tagged [F] for
?lexicalized inflection?, e.g. gedra?ngt (past partici-
ple of dra?ngen, Engl: push) were decomposed semi-
automatically for the purpose of this evaluation. As
expected, annotating words with CELEX morpho-
logical segmentation yielded the best g2p conver-
sion results. Manual annotation is only available for
a small number of words. Therefore, only automati-
cally annotated morphological information can scale
up to real applications.
Rule-based Systems
The traditional approach is to use large morpheme
lexica and a set of rules that segment words into af-
fixes and stems. Drawbacks of using such a system
are the high development costs, limited coverage
99
and problems with ambiguity resolution between al-
ternative analyses of a word.
The two rule-based systems we evaluated, the
ETI4 morphological system and SMOR5 (Schmid et
al., 2004), are both high-quality systems with large
lexica that have been developed over several years.
Their performance results can help to estimate what
can realistically be expected from an automatic seg-
mentation system. Both of the rule-based systems
achieved an F-score of approx. 80% morphological
boundaries correct with respect to CELEX manual
annotation.
Unsupervised Morphological Systems
Most attractive among automatic systems are
methods that use unsupervised learning, because
these require neither an expert linguist to build large
rule-sets and lexica nor large manually annotated
word lists, but only large amounts of tokenized
text, which can be acquired e.g. from the internet.
Unsupervised methods are in principle6 language-
independent, and can therefore easily be applied to
other languages.
We compared four different state-of-the-art unsu-
pervised systems for morphological decomposition
(cf. (Demberg, 2006; Demberg, 2007)). The algo-
rithms were trained on a German newspaper cor-
pus (taz), containing about 240 million words. The
same algorithms have previously been shown to help
a speech recognition task (Kurimo et al, 2006).
5 Experimental Evaluations
5.1 Training Set and Test Set Design
The German corpus used in these experiments is
CELEX (German Linguistic User Guide, 1995).
CELEX contains a phonemic representation of each
4Eloquent Technology, Inc. (ETI) TTS system.
http://www.mindspring.com/?ssshp/ssshp_cd/
ss_eloq.htm
5The lexicon used by SMOR, IMSLEX, contains morpho-
logically complex entries, which leads to high precision and low
recall. The results reported here refer to a version of SMOR,
where the lexicon entries were decomposed using a rather na??ve
high-recall segmentation method. SMOR itself does not disam-
biguate morphological analyses of a word. Our version used
transition weights learnt from CELEX morphological annota-
tion. For more details refer to (Demberg, 2006).
6Most systems make some assumptions about the underly-
ing morphological system, for instance that morphology is a
concatenative process, that stems have a certain minimal length
or that prefixing and suffixing are the most relevant phenomena.
word, syllable boundaries and word stress infor-
mation. Furthermore, it contains manually verified
morphological boundaries.
Our training set contains approx. 240,000 words
and the test set consists of 12,326 words. The test
set is designed such that word stems in training and
test sets are disjoint, i.e. the inflections of a certain
stem are either all in the training set or all in the test
set. Stem overlap between training and test set only
occurs in compounds and derivations. If a simple
random splitting (90% for training set, 10% for test
set) is used on inflected corpora, results are much
better: Word error rates (WER) are about 60% lower
when the set of stems in training and test set are not
disjoint. The same effect can also be observed for
the syllabification task (see Table 4).
5.2 Results for the Joint n-gram Model
The joint n-gram model is language-independent.
An aligned corpus with words and their pronuncia-
tions is needed, but no further adaptation is required.
Table 1 shows the performance of our model in
comparison to alternative approaches on the German
and English versions of the CELEX corpus, the En-
glish NetTalk corpus, the English Teacher?s Word
Book (TWB) corpus, the English beep corpus and
the French Brulex corpus. The joint n-gram model
performs significantly better than the decision tree
(essentially based on (Lucassen and Mercer, 1984)),
and achieves scores comparable to the Pronuncia-
tion by Analogy (PbA) algorithm (Marchand and
Damper, 2005). For the Nettalk data, we also com-
pared the influence of syllable boundary annotation
from a) automatically learnt and b) manually anno-
tated syllabification information on phoneme accu-
racy. Automatic syllabification for our model in-
tegrated phonological constraints (as described in
section 3.1), and therefore led to an improvement
in phoneme accuracy, while the word error rate in-
creased for the PbA approach, which does not incor-
porate such constraints.
(Chen, 2003) also used a joint n-gram model.
The two approaches differ in that Chen uses small
chunks (?(l : |0..1|) : (p : |0..1|)? pairs only) and it-
eratively optimizes letter-phoneme alignment during
training. Chen smoothes higher-order Markov Mod-
els with Gaussian Priors and implements additional
language modelling such as consonant doubling.
100
corpus size jnt n-gr PbA Chen dec.tree
G - CELEX 230k 7.5% 15.0%
E - Nettalk 20k 35.4% 34.65% 34.6%
a) auto.syll 35.3% 35.2%
b) man.syll 29.4% 28.3%
E - TWB 18k 28.5% 28.2%
E - beep 200k 14.3% 13.3%
E - CELEX 100k 23.7% 31.7%
F - Brulex 27k 10.9%
Table 1: Word error rates for different g2p conver-
sion algorithms. Constraints were only used in the
E-Nettalk auto. syll condition.
5.3 Benefit of Integrating Constraints
The accuracy improvements achieved by integrat-
ing the constraints (see Table 2) are highly statis-
tically significant. The numbers for conditions ?G-
syllab.+stress+g2p? and ?E-syllab.+g2p? in Table 2
differ from the numbers for ?G-CELEX? and ?E-
Nettalk? in Table 1 because phoneme conversion
errors, syllabification errors and stress assignment
errors are all counted towards word error rates re-
ported in Table 2.
Word error rate in the combined g2p-syllable-
stress model was reduced from 21.5% to 13.7%. For
the separate tasks, we observed similar effects: The
word error rate for inserting syllable boundaries was
reduced from 3.48% to 3.1% on letters and from
1.84% to 1.53% on phonemes. Most significantly,
word error rate was decreased from 30.9% to 9.9%
for word stress assignment on graphemes.
We also found similarly important improvements
when applying the syllabification constraint to En-
glish grapheme-to-phoneme conversion and syllabi-
fication. This suggests that our findings are not spe-
cific to German but that this kind of general con-
straints can be beneficial for a range of languages.
no constr. constraint(s)
G - syllab.+stress+g2p 21.5% 13.7%
G - syllab. on letters 3.5% 3.1%
G - syllab. on phonemes 1.84% 1.53%
G - stress assignm. on letters 30.9% 9.9%
E - syllab.+g2p 40.5% 37.5%
E - syllab. on phonemes 12.7% 8.8%
Table 2: Improving performance on g2p conver-
sion, syllabification and stress assignment through
the introduction of constraints. The table shows
word error rates for German CELEX (G) and En-
glish NetTalk (E).
5.4 Modularity
Modularity is an advantage if the individual compo-
nents are more specialized to their task (e.g. by ap-
plying a particular level of description of the prob-
lem, or by incorporating some additional source of
knowledge).In a modular system, one component
can easily be substituted by another ? for example,
if a better way of doing stress assignment in German
was found. On the other hand, keeping everything in
one module for strongly inter-dependent tasks (such
as determining word stress and phonemization) al-
lows us to simultaneously optimize for the best com-
bination of phonemes and stress.
Best results were obtained from the joint n-gram
model that does syllabification, stress assignment
and g2p conversion all in a single step and inte-
grates phonological constraints for syllabification
and word stress (WER = 14.4% using method v1,
WER = 13.7% using method v2). If the modular ar-
chitecture is chosen, best results are obtained when
g2p conversion is done before syllabification and
stress assignment (15.2% WER), whereas doing syl-
labification and stress assignment first and then g2p
conversion leads to a WER of 16.6%. We can con-
clude from this finding that an integrated approach is
superior to a pipeline architecture for strongly inter-
dependent tasks such as these.
5.5 The Contribution of Morphological
Preprocessing
A statistically significant (according to a two-tailed
t-test) improvement in g2p conversion accuracy
(from 13.7% WER to 13.2% WER) was obtained
with the manually annotated morphological bound-
aries from CELEX. The segmentation from both of
the rule-based systems (ETI and SMOR) also re-
sulted in an accuracy increase with respect to the
baseline (13.6% WER), which is not annotated with
morphological boundaries.
Among the unsupervised systems, best results7 on
the g2p task with morphological annotation were ob-
tained with the RePortS system (Keshava and Pitler,
2006). But none of the segmentations led to an er-
ror reduction when compared to a baseline that used
no morphological information (see Table 3). Word
error rate even increased when the quality of the
7For all results refer to (Demberg, 2006).
101
Precis. Recall F-Meas. WER
RePortS (unsuperv.) 71.1% 50.7% 59.2% 15.1%
no morphology 13.7%
SMOR (rule-based) 87.1% 80.4% 83.6%
ETI (rule-based) 75.4% 84.1% 79.5% 13.6%
CELEX (manual) 100% 100% 100% 13.2%
Table 3: Systems evaluation on German CELEX
manual annotation and on the g2p task using a joint
n-gram model. WERs refer to implementation v2.
morphological segmentation was too low (the unsu-
pervised algorithms achieved 52%-62% F-measure
with respect to CELEX manual annotation).
Table 4 shows that high-quality morphological
information can also significantly improve perfor-
mance on a syllabification task for German. We used
the syllabifier described in (Schmid et al, 2005),
which works similar to the joint n-gram model used
for g2p conversion. Just as for g2p conversion, we
found a significant accuracy improvement when us-
ing the manually annotated data, a smaller improve-
ment for using data from the rule-based morpholog-
ical system, and no improvement when using seg-
mentations from an unsupervised algorithm. Syllab-
ification works best when performed on phonemes,
because syllables are phonological units and there-
fore can be determined most easily in terms of
phonological entities such as phonemes.
Whether morphological segmentation is worth the
effort depends on many factors such as training set
size, the g2p algorithm and the language considered.
disj. stems random
RePortS (unsupervised morph.) 4.95%
no morphology 3.10% 0.72%
ETI (rule-based morph.) 2.63%
CELEX (manual annot.) 1.91% 0.53%
on phonemes 1.53% 0.18%
Table 4: Word error rates (WER) for syllabification
with a joint n-gram model for two different training
and test set designs (see Section 5.1).
Morphology for Data Sparseness Reduction
Probably the most important aspect of morpho-
logical segmentation information is that it can help
to resolve data sparseness issues. Because of the ad-
ditional knowledge given to the system through the
morphological information, similarly-behaving let-
ter sequences can be grouped more effectively.
Therefore, we hypothesized that morphological
information is most beneficial in situations where
the training corpus is rather small. Our findings con-
firm this expectation, as the relative error reduction
through morphological annotation for a training cor-
pus of 9,600 words is 6.67%, while it is only 3.65%
for a 240,000-word training corpus.
In our implementation, the stress flags and sylla-
ble flags we use to enforce the phonological con-
straints increase data sparseness. We found v2 (the
implementation that uses the states without stress
and syllable flags and enforces the constraints by
eliminating invalid transitions, cf. section 3.1) to
outperform the integrated version, v1, and more sig-
nificantly in the case of more severe data sparseness.
The only condition when we found v1 to perform
better than v2 was with a large data set and addi-
tional data sparseness reduction through morpholog-
ical annotation, as in section 4 (see Table 5).
WER: designs v1 v2
data set size 240k 9.6k 240k 9.6k
no morph. 14.4% 32.3% 13.7% 25.5%
CELEX 12.5% 29% 13.2% 23.8%
Table 5: The interactions of constraints in training
and different levels of data sparseness.
g2p Conversion Algorithms
The benefit of using morphological preprocessing
is also affected by the algorithm that is used for g2p
conversion. Therefore, we also evaluated the relative
improvement of morphological annotation when us-
ing a decision tree for g2p conversion.
Decision trees were one of the first data-based ap-
proaches to g2p and are still widely used (Kienappel
and Kneser, 2001; Black et al, 1998). The tree?s
efficiency and ability for generalization largely de-
pends on pruning and the choice of possible ques-
tions. In our implementation, the decision tree can
ask about letters within a context window of five
back and five ahead, about five phonemes back and
groups of letters (e.g. consonants vs. vowels).
Both the decision tree and the joint n-gram model
convert graphemes to phonemes, insert syllable
boundaries and assign word stress in a single step
(marked as ?WER-ss? in Table 6. The imple-
mentation of the joint n-gram model incorporates
the phonological constraints described in section 3
(?WER-ss+?). Our main finding is that the joint
n-gram model profits less from morphological an-
notation. Without the constraints, the performance
102
difference is smaller: the joint n-gram model then
achieves a word error rate of 21.5% on the no-
morphology-condition.
In very recent work, (Demberg, 2007) developed
an unsupervised algorithm (f-meas: 68%; an exten-
sion of RePortS) whose segmentations improve g2p
when using a the decision tree (PER: 3.45%).
decision tree joint n-gram
PER WER-ss PER WER-ss+
RePortS 3.83% 28.3% 15.1%
no morph. 3.63% 26.59% 2.52% 13.7%
ETI 2.8% 21.13% 2.53% 13.6%
CELEX 2.64% 21.64% 2.36% 13.2%
Table 6: The effect of morphological preprocessing
on phoneme error rates (PER) and word error rates
(WER) in grapheme-to-phoneme conversion.
Morphology for other Languages
We also investigated the effect of morphological
information on g2p conversion and syllabification
in English, using manually annotated morphological
boundaries from CELEX and the automatic unsuper-
vised RePortS system which achieves an F-score of
about 77% for English. The cases where morpho-
logical information affects word pronunciation are
relatively few in comparison to German, therefore
the overall effect is rather weak and we did not even
find improvements with perfect boundaries.
6 Conclusions
Our results confirm that the integration of phonolog-
ical constraints ?one nucleus per syllable? and ?one
main stress per word? can significantly boost ac-
curacy for g2p conversion in German and English.
We implemented the constraints using a joint n-
gram model for g2p conversion, which is language-
independent and well-suited to the g2p task.
We systematically evaluated the benefit to be
gained from morphological preprocessing on g2p
conversion and syllabification. We found that mor-
phological segmentations from rule-based systems
led to some improvement. But the magnitude of
the accuracy improvement strongly depends on the
g2p algorithm and on training set size. State-of-
the-art unsupervised morphological systems do not
yet yield sufficiently good segmentations to help the
task, if a good conversion algorithm is used: Low
quality segmentation even led to higher error rates.
Acknowledgments
We would like to thank Hinrich Schu?tze, Frank Keller and the
ACL reviewers for valuable comments and discussion.
The first author was supported by Evangelisches Studienwerk
e.V. Villigst.
References
M. Bisani and H. Ney. 2002. Investigations on joint multigram
models for grapheme-to-phoneme conversion. In ICSLP.
A. Black, K. Lenzo, and V. Pagel. 1998. Issues in building gen-
eral letter to sound rules. In 3. ESCA on Speech Synthesis.
SF Chen and J Goodman. 1996. An empirical study of smooth-
ing techniques for language modeling. In Proc. of ACL.
S. F. Chen. 2003. Conditional and joint models for grapheme-
to-phoneme conversion. In Eurospeech.
V. Demberg. 2006. Letter-to-phoneme conversion for a Ger-
man TTS-System. Master?s thesis. IMS, Univ. of Stuttgart.
V. Demberg. 2007. A language-independent unsupervised
model for morphological segmentation. In Proc. of ACL-07.
L. Galescu and J. Allen. 2001. Bi-directional conversion be-
tween graphemes and phonemes using a joint n-gram model.
In Proc. of the 4th ISCA Workshop on Speech Synthesis.
CELEX German Linguistic User Guide, 1995. Center for Lex-
ical Information. Max-Planck-Institut for Psycholinguistics,
Nijmegen.
M. Jessen, 1998. Word Prosodic Systems in the Languages of
Europe. Mouton de Gruyter: Berlin.
S. Keshava and E. Pitler. 2006. A simpler, intuitive approach
to morpheme induction. In Proceedings of 2nd Pascal Chal-
lenges Workshop, pages 31?35, Venice, Italy.
A. K. Kienappel and R. Kneser. 2001. Designing very com-
pact decision trees for grapheme-to-phoneme transcription.
In Eurospeech, Scandinavia.
M. Kurimo, M. Creutz, M. Varjokallio, E. Arisoy, and M. Sar-
aclar. 2006. Unsupervsied segmentation of words into mor-
phemes ? Challenge 2005: An introduction and evaluation
report. In Proc. of 2nd Pascal Challenges Workshop, Italy.
J. Lucassen and R. Mercer. 1984. An information theoretic
approach to the automatic determination of phonemic base-
forms. In ICASSP 9.
Y. Marchand and R. I. Damper. 2005. Can syllabification im-
prove pronunciation by analogy of English? Natural Lan-
guage Engineering.
W. Minker. 1996. Grapheme-to-phoneme conversion - an ap-
proach based on hidden markov models.
B. Mo?bius. 2001. German and Multilingual Speech Synthesis.
phonetic AIMS, Arbeitspapiere des Instituts fu?r Maschinelle
Spachverarbeitung.
K. Mu?ller. 2001. Automatic detection of syllable boundaries
combining the advantages of treebank and bracketed corpora
training. In Proceedings of ACL, pages 402?409.
A. Pounder and M. Kommenda. 1986. Morphological analysis
for a German text-to-speech system. In COLING 1986.
P.A. Rentzepopoulos and G.K. Kokkinakis. 1991. Phoneme to
grapheme conversion using HMM. In Eurospeech.
H. Schmid, A. Fitschen, and U. Heid. 2004. SMOR: A German
computational morphology covering derivation, composition
and inflection. In Proc. of LREC.
H. Schmid, B. Mo?bius, and J. Weidenkaff. 2005. Tagging syl-
lable boundaries with hidden Markov models. IMS, unpub.
R. Sproat. 1996. Multilingual text analysis for text-to-speech
synthesis. In Proc. ICSLP ?96, Philadelphia, PA.
P. Taylor. 2005. Hidden Markov models for grapheme to
phoneme conversion. In INTERSPEECH.
103
Proceedings of ACL-08: HLT, pages 496?504,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Combining EM Training and the MDL Principle for an
Automatic Verb Classification incorporating Selectional Preferences
Sabine Schulte im Walde, Christian Hying, Christian Scheible, Helmut Schmid
Institute for Natural Language Processing
University of Stuttgart, Germany
{schulte,hyingcn,scheibcn,schmid}@ims.uni-stuttgart.de
Abstract
This paper presents an innovative, complex
approach to semantic verb classification that
relies on selectional preferences as verb prop-
erties. The probabilistic verb class model un-
derlying the semantic classes is trained by
a combination of the EM algorithm and the
MDL principle, providing soft clusters with
two dimensions (verb senses and subcategori-
sation frames with selectional preferences) as
a result. A language-model-based evaluation
shows that after 10 training iterations the verb
class model results are above the baseline re-
sults.
1 Introduction
In recent years, the computational linguistics com-
munity has developed an impressive number of se-
mantic verb classifications, i.e., classifications that
generalise over verbs according to their semantic
properties. Intuitive examples of such classifica-
tions are the MOTION WITH A VEHICLE class, in-
cluding verbs such as drive, fly, row, etc., or the
BREAK A SOLID SURFACE WITH AN INSTRUMENT
class, including verbs such as break, crush, frac-
ture, smash, etc. Semantic verb classifications are
of great interest to computational linguistics, specifi-
cally regarding the pervasive problem of data sparse-
ness in the processing of natural language. Up to
now, such classifications have been used in applica-
tions such as word sense disambiguation (Dorr and
Jones, 1996; Kohomban and Lee, 2005), machine
translation (Prescher et al, 2000; Koehn and Hoang,
2007), document classification (Klavans and Kan,
1998), and in statistical lexical acquisition in gen-
eral (Rooth et al, 1999; Merlo and Stevenson, 2001;
Korhonen, 2002; Schulte im Walde, 2006).
Given that the creation of semantic verb classi-
fications is not an end task in itself, but depends
on the application scenario of the classification, we
find various approaches to an automatic induction of
semantic verb classifications. For example, Siegel
and McKeown (2000) used several machine learn-
ing algorithms to perform an automatic aspectual
classification of English verbs into event and sta-
tive verbs. Merlo and Stevenson (2001) presented
an automatic classification of three types of English
intransitive verbs, based on argument structure and
heuristics to thematic relations. Pereira et al (1993)
and Rooth et al (1999) relied on the Expectation-
Maximisation algorithm to induce soft clusters of
verbs, based on the verbs? direct object nouns. Sim-
ilarly, Korhonen et al (2003) relied on the Informa-
tion Bottleneck (Tishby et al, 1999) and subcate-
gorisation frame types to induce soft verb clusters.
This paper presents an innovative, complex ap-
proach to semantic verb classes that relies on se-
lectional preferences as verb properties. The un-
derlying linguistic assumption for this verb class
model is that verbs which agree on their selec-
tional preferences belong to a common seman-
tic class. The model is implemented as a soft-
clustering approach, in order to capture the poly-
semy of the verbs. The training procedure uses the
Expectation-Maximisation (EM) algorithm (Baum,
1972) to iteratively improve the probabilistic param-
eters of the model, and applies the Minimum De-
scription Length (MDL) principle (Rissanen, 1978)
to induce WordNet-based selectional preferences for
arguments within subcategorisation frames. Our
model is potentially useful for lexical induction
(e.g., verb senses, subcategorisation and selectional
preferences, collocations, and verb alternations),
496
and for NLP applications in sparse data situations.
In this paper, we provide an evaluation based on a
language model.
The remainder of the paper is organised as fol-
lows. Section 2 introduces our probabilistic verb
class model, the EM training, and how we incor-
porate the MDL principle. Section 3 describes the
clustering experiments, including the experimental
setup, the evaluation, and the results. Section 4 re-
ports on related work, before we close with a sum-
mary and outlook in Section 5.
2 Verb Class Model
2.1 Probabilistic Model
This paper suggests a probabilistic model of verb
classes that groups verbs into clusters with simi-
lar subcategorisation frames and selectional prefer-
ences. Verbs may be assigned to several clusters
(soft clustering) which allows the model to describe
the subcategorisation properties of several verb read-
ings separately. The number of clusters is defined
in advance, but the assignment of the verbs to the
clusters is learnt during training. It is assumed that
all verb readings belonging to one cluster have simi-
lar subcategorisation and selectional properties. The
selectional preferences are expressed in terms of se-
mantic concepts from WordNet, rather than a set of
individual words. Finally, the model assumes that
the different arguments are mutually independent for
all subcategorisation frames of a cluster. From the
last assumption, it follows that any statistical depen-
dency between the arguments of a verb has to be ex-
plained by multiple readings.
The statistical model is characterised by the fol-
lowing equation which defines the probability of a
verb v with a subcategorisation frame f and argu-
ments a1, ..., anf :
p(v, f, a1, ..., anf ) =
?
c
p(c) p(v|c) p(f |c) ?
nf
?
i=1
?
r?R
p(r|c, f, i) p(ai|r)
The model describes a stochastic process which gen-
erates a verb-argument tuple like ?speak, subj-pp.to,
professor, audience? by
1. selecting some cluster c, e.g. c3 (which might
correspond to a set of communication verbs),
with probability p(c3),
2. selecting a verb v, here the verb speak, from
cluster c3 with probability p(speak|c3),
3. selecting a subcategorisation frame f , here
subj-pp.to, with probability p(subj-pp.to|c3);
note that the frame probability only depends on
the cluster, and not on the verb,
4. selecting a WordNet concept r for each argu-
ment slot, e.g. person for the first slot with
probability p(person|c3, subj-pp.to, 1) and so-
cial group for the second slot with probability
p(social group|c3, subj-pp.to, 2),
5. selecting a word ai to instantiate each con-
cept as argument i; in our example, we
might choose professor for person with
probability p(professor|person) and au-
dience for social group with probability
p(audience|social group).
The model contains two hidden variables, namely
the clusters c and the selectional preferences r. In or-
der to obtain the overall probability of a given verb-
argument tuple, we have to sum over all possible val-
ues of these hidden variables.
The assumption that the arguments are indepen-
dent of the verb given the cluster is essential for ob-
taining a clustering algorithm because it forces the
EM algorithm to make the verbs within a cluster as
similar as possible.1 The assumption that the differ-
ent arguments of a verb are mutually independent is
important to reduce the parameter set to a tractable
size
The fact that verbs select for concepts rather than
individual words also reduces the number of param-
eters and helps to avoid sparse data problems. The
application of the MDL principle guarantees that no
important information is lost.
The probabilities p(r|c, f, i) and p(a|r) men-
tioned above are not represented as atomic enti-
ties. Instead, we follow an approach by Abney
1The EM algorithm adjusts the model parameters in such a
way that the probability assigned to the training tuples is max-
imised. Given the model constraints, the data probability can
only be maximised by making the verbs within a cluster as sim-
ilar to each other as possible, regarding the required arguments.
497
and Light (1999) and turn WordNet into a Hidden
Markov model (HMM). We create a new pseudo-
concept for each WordNet noun and add it as a hy-
ponym to each synset containing this word. In ad-
dition, we assign a probability to each hypernymy?
hyponymy transition, such that the probabilities of
the hyponymy links of a synset sum up to 1. The
pseudo-concept nodes emit the respective word with
a probability of 1, whereas the regular concept nodes
are non-emitting nodes. The probability of a path
in this (a priori) WordNet HMM is the product of
the probabilities of the transitions within the path.
The probability p(a|r) is then defined as the sum
of the probabilities of all paths from the concept r
to the word a. Similarly, we create a partial Word-
Net HMM for each argument slot ?c, f, i? which en-
codes the selectional preferences. It contains only
the WordNet concepts that the slot selects for, ac-
cording to the MDL principle (cf. Section 2.3), and
the dominating concepts. The probability p(r|c, f, i)
is the total probability of all paths from the top-most
WordNet concept entity to the terminal node r.
2.2 EM Training
The model is trained on verb-argument tuples of
the form described above, i.e., consisting of a verb
and a subcategorisation frame, plus the nominal2
heads of the arguments. The tuples may be ex-
tracted from parsed data, or from a treebank. Be-
cause of the hidden variables, the model is trained
iteratively with the Expectation-Maximisation algo-
rithm (Baum, 1972). The parameters are randomly
initialised and then re-estimated with the Inside-
Outside algorithm (Lari and Young, 1990) which is
an instance of the EM algorithm for training Proba-
bilistic Context-Free Grammars (PCFGs).
The PCFG training algorithm is applicable here
because we can define a PCFG for each of our mod-
els which generates the same verb-argument tuples
with the same probability. The PCFG is defined as
follows:
(1) The start symbol is TOP.
(2) For each cluster c, we add a rule TOP ? Vc Ac
whose probability is p(c).
2Arguments with lexical heads other than nouns (e.g., sub-
categorised clauses) are not included in the selectional prefer-
ence induction.
(3) For each verb v in cluster c, we add a rule
Vc ? v with probability p(v|c).
(4) For each subcategorisation frame f of cluster c
with length n, we add a rule Ac ? f Rc,f,1,entity
... Rc,f,n,entity with probability p(f |c).
(5) For each transition from a node r to a node r?
in the selectional preference model for slot i of
the subcategorisation frame f of cluster c, we
add a rule Rc,f,i,r ? Rc,f,i,r? whose probability
is the transition probability from r to r? in the
respective WordNet-HMM.
(6) For each terminal node r in the selectional pref-
erence model, we add a rule Rc,f,i,r ? Rr whose
probability is 1. With this rule, we ?jump? from
the selectional restriction model to the corre-
sponding node in the a priori model.
(7) For each transition from a node r to a node r?
in the a priori model, we add a rule Rr ? Rr?
whose probability is the transition probability
from r to r? in the a priori WordNet-HMM.
(8) For each word node a in the a priori model, we
add a rule Ra ? a whose probability is 1.
Based on the above definitions, a partial ?parse? for
?speak subj-pp.to professor audience?, referring to
cluster 3 and one possible WordNet path, is shown in
Figure 1. The connections within R3 (R3,...,entity?
R3,...,person/group) and within R (Rperson/group?
Rprofessor/audience) refer to sequential applications
of rule types (5) and (7), respectively.
TOP
V3
speak
A3
subj-pp.to R3,subj?pp.to,1,entity
R3,subj?pp.to,1,person
Rperson
Rprofessor
professor
R3,subj?pp.to,2,entity
R3,subj?pp.to,2,group
Rgroup
Raudience
audience
Figure 1: Example parse tree.
The EM training algorithm maximises the likelihood
of the training data.
498
2.3 MDL Principle
A model with a large number of fine-grained con-
cepts as selectional preferences assigns a higher
likelihood to the data than a model with a small num-
ber of general concepts, because in general a larger
number of parameters is better in describing train-
ing data. Consequently, the EM algorithm a pri-
ori prefers fine-grained concepts but ? due to sparse
data problems ? tends to overfit the training data. In
order to find selectional preferences with an appro-
priate granularity, we apply the Minimum Descrip-
tion Length principle, an approach from Information
Theory. According to the MDL principle, the model
with minimal description length should be chosen.
The description length itself is the sum of the model
length and the data length, with the model length
defined as the number of bits needed to encode the
model and its parameters, and the data length de-
fined as the number of bits required to encode the
training data with the given model. According to
coding theory, an optimal encoding uses ?log2p
bits, on average, to encode data whose probability
is p. Usually, the model length increases and the
data length decreases as more parameters are added
to a model. The MDL principle finds a compromise
between the size of the model and the accuracy of
the data description.
Our selectional preference model relies on Li and
Abe (1998), applying the MDL principle to deter-
mine selectional preferences of verbs and their argu-
ments, by means of a concept hierarchy ordered by
hypernym/hyponym relations. Given a set of nouns
within a specific argument slot as a sample, the ap-
proach finds the cut3 in a concept hierarchy which
minimises the sum of encoding both the model and
the data. The model length (ML) is defined as
ML = k2 ? log2 |S|,
with k the number of concepts in the partial hierar-
chy between the top concept and the concepts in the
cut, and |S| the sample size, i.e., the total frequency
of the data set. The data length (DL) is defined as
DL = ?
?
n?S
log2 p(n).
3A cut is defined as a set of concepts in the concept hier-
archy that defines a partition of the ?leaf? concepts (the lowest
concepts in the hierarchy), viewing each concept in the cut as
representing the set of all leaf concepts it dominates.
The probability of a noun p(n) is determined by di-
viding the total probability of the concept class the
noun belongs to, p(concept), by the size of that
class, |concept|, i.e., the number of nouns that are
dominated by that concept:
p(n) = p(concept)|concept| .
The higher the concept within the hierarchy, the
more nouns receive an equal probability, and the
greater is the data length.
The probability of the concept class in turn is de-
termined by dividing the frequency of the concept
class f(concept) by the sample size:
p(concept) = f(concept)|S| ,
where f(concept) is calculated by upward propaga-
tion of the frequencies of the nominal lexemes from
the data sample through the hierarchy. For exam-
ple, if the nouns coffee, tea, milk appeared with fre-
quencies 25, 50, 3, respectively, within a specific ar-
gument slot, then their hypernym concept beverage
would be assigned a frequency of 78, and these 78
would be propagated further upwards to the next hy-
pernyms, etc. As a result, each concept class is as-
signed a fraction of the frequency of the whole data
set (and the top concept receives the total frequency
of the data set). For calculating p(concept) (and the
overall data length), though, only the concept classes
within the cut through the hierarchy are relevant.
Our model uses WordNet 3.0 as the concept hier-
archy, and comprises one (complete) a priori Word-
Net model for the lexical head probabilities p(a|r)
and one (partial) model for each selectional proba-
bility distribution p(r|c, f, i), cf. Section 2.1.
2.4 Combining EM and MDL
The training procedure that combines the EM train-
ing with the MDL principle can be summarised as
follows.
1. The probabilities of a verb class model with c
classes and a pre-defined set of verbs and frames
are initialised randomly. The selectional preference
models start out with the most general WordNet con-
cept only, i.e., the partial WordNet hierarchies un-
derlying the probabilities p(r|c, f, i) initially only
contain the concept r for entity.
499
2. The model is trained for a pre-defined num-
ber of iterations. In each iteration, not only the
model probabilities are re-estimated and maximised
(as done by EM), but also the cuts through the con-
cept hierarchies that represent the various selectional
preference models are re-assessed. In each iteration,
the following steps are performed.
(a) The partial WordNet hierarchies that represent
the selectional preference models are expanded to
include the hyponyms of the respective leaf con-
cepts of the partial hierarchies. I.e., in the first itera-
tion, all models are expanded towards the hyponyms
of entity, and in subsequent iterations each selec-
tional preference model is expanded to include the
hyponyms of the leaf nodes in the partial hierarchies
resulting from the previous iteration. This expansion
step allows the selection models to become more and
more detailed, as the training proceeds and the verb
clusters (and their selectional restrictions) become
increasingly specific.
(b) The training tuples are processed: For each tu-
ple, a PCFG parse forest as indicated by Figure 1
is done, and the Inside-Outside algorithm is applied
to estimate the frequencies of the ?parse tree rules?,
given the current model probabilities.
(c) The MDL principle is applied to each selectional
preference model: Starting from the respective leaf
concepts in the partial hierarchies, MDL is calcu-
lated to compare each set of hyponym concepts that
share a hypernym with the respective hypernym con-
cept. If the MDL is lower for the set of hyponyms
than the hypernym, the hyponyms are left in the par-
tial hierarchy. Otherwise the expansion of the hyper-
nym towards the hyponyms is undone and we con-
tinue recursively upwards the hierarchy, calculating
MDL to compare the former hypernym and its co-
hyponyms with the next upper hypernym, etc. The
recursion allows the training algorithm to remove
nodes which were added in earlier iterations and are
no longer relevant. It stops if the MDL is lower for
the hyponyms than for the hypernym.
This step results in selectional preference models
that minimally contain the top concept entity, and
maximally contain the partial WordNet hierarchy
between entity and the concept classes that have
been expanded within this iteration.
(d) The probabilities of the verb class model are
maximised based on the frequency estimates ob-
tained in step (b).
3 Experiments
The model is generally applicable to all languages
for which WordNet exists, and for which the Word-
Net functions provided by Princeton University are
available. For the purposes of this paper, we choose
English as a case study.
3.1 Experimental Setup
The input data for training the verb class mod-
els were derived from Viterbi parses of the whole
British National Corpus, using the lexicalised PCFG
for English by Carroll and Rooth (1998). We took
only active clauses into account, and disregarded
auxiliary and modal verbs as well as particle verbs,
leaving a total of 4,852,371 Viterbi parses. Those in-
put tuples were then divided into 90% training data
and 10% test data, providing 4,367,130 training tu-
ples (over 2,769,804 types), and 485,241 test tuples
(over 368,103 types).
As we wanted to train and assess our verb class
model under various conditions, we used different
fractions of the training data in different training
regimes. Because of time and memory constraints,
we only used training tuples that appeared at least
twice. (For the sake of comparison, we also trained
one model on all tuples.) Furthermore, we dis-
regarded tuples with personal pronoun arguments;
they are not represented in WordNet, and even if
they are added (e.g. to general concepts such as
person, entity) they have a rather destructive ef-
fect. We considered two subsets of the subcate-
gorisation frames with 10 and 20 elements, which
were chosen according to their overall frequency in
the training data; for example, the 10 most frequent
frame types were subj:obj, subj, subj:ap, subj:to,
subj:obj:obj2, subj:obj:pp-in, subj:adv, subj:pp-in,
subj:vbase, subj:that.4 When relying on theses
10/20 subcategorisation frames, plus including the
above restrictions, we were left with 39,773/158,134
and 42,826/166,303 training tuple types/tokens, re-
spectively. The overall number of training tuples
4A frame lists its arguments, separated by ?:?. Most argu-
ments within the frame types should be self-explanatory. ap is
an adjectival phrase.
500
was therefore much smaller than the generally avail-
able data. The corresponding numbers including tu-
ples with a frequency of one were 478,717/597,078
and 577,755/701,232.
The number of clusters in the experiments was ei-
ther 20 or 50, and we used up to 50 iterations over
the training tuples. The model probabilities were
output after each 5th iteration. The output comprises
all model probabilities introduced in Section 2.1.
The following sections describe the evaluation of the
experiments, and the results.
3.2 Evaluation
One of the goals in the development of the presented
verb class model was to obtain an accurate statistical
model of verb-argument tuples, i.e. a model which
precisely predicts the tuple probabilities. In order
to evaluate the performance of the model in this re-
spect, we conducted an evaluation experiment, in
which we computed the probability which the verb
class model assigns to our test tuples and compared
it to the corresponding probability assigned by a
baseline model. The model with the higher proba-
bility is judged the better model.
We expected that the verb class model would
perform better than the baseline model on tuples
where one or more of the arguments were not ob-
served with the respective verb, because either the
argument itself or a semantically similar argument
(according to the selectional preferences) was ob-
served with verbs belonging to the same cluster. We
also expected that the verb class model assigns a
lower probability than the baseline model to test tu-
ples which frequently occurred in the training data,
since the verb class model fails to describe precisely
the idiosyncratic properties of verbs which are not
shared by the other verbs of its cluster.
The Baseline Model The baseline model decom-
poses the probability of a verb-argument tuple into a
product of conditional probabilities:5
p(v, f, anf1 ) = p(v) p(f |v)
nf
?
i=1
p(ai|ai?11 , ?v, f?, fi)
5fi is the label of the ith slot. The verb and the subcategori-
sation frame are enclosed in angle brackets because they are
treated as a unit during smoothing.
The probability of our example tuple ?speak,
subj-pp.to, professor, audience? in the base-
line model is then p(speak) p(subj-pp.to|speak)
p(professor|?speak, subj-pp.to?, subj) p(audience|
professor, ?speak, subj-pp.to?, pp.to).
The model contains no hidden variables. Thus the
parameters can be directly estimated from the train-
ing data with relative frequencies. The parameter
estimates are smoothed with modified Kneser-Ney
smoothing (Chen and Goodman, 1998), such that
the probability of each tuple is positive.
Smoothing of the Verb Class Model Although
the verb class model has a built-in smoothing capac-
ity, it needs additional smoothing for two reasons:
Firstly, some of the nouns in the test data did not
occur in the training data. The verb class model
assigns a zero probability to such nouns. Hence
we smoothed the concept instantiation probabilities
p(noun|concept) with Witten-Bell smoothing (Chen
and Goodman, 1998). Secondly, we smoothed the
probabilities of the concepts in the selectional pref-
erence models where zero probabilities may occur.
The smoothing ensures that the verb class model
assigns a positive probability to each verb-argument
tuple with a known verb, a known subcategorisation
frame, and arguments which are in WordNet. Other
tuples were excluded from the evaluation because
the verb class model cannot deal with them.
3.3 Results
The evaluation results of our classification experi-
ments are presented in Table 1, for 20 and 50 clus-
ters, with 10 and 20 subcategorisation frame types.
The table cells provide the loge of the probabilities
per tuple token. The probabilities increase with the
number of iterations, flattening out after approx. 25
iterations, as illustrated by Figure 2. Both for 10
and 20 frames, the results are better for 50 than for
20 clusters, with small differences between 10 and
20 frames. The results vary between -11.850 and
-10.620 (for 5-50 iterations), in comparison to base-
line values of -11.546 and -11.770 for 10 and 20
frames, respectively. The results thus show that our
verb class model results are above the baseline re-
sults after 10 iterations; this means that our statis-
tical model then assigns higher probabilities to the
test tuples than the baseline model.
501
No. of Iteration
Clusters 5 10 15 20 25 30 35 40 45 50
10 frames
20 -11.770 -11.408 -10.978 -10.900 -10.853 -10.841 -10.831 -10.823 -10.817 -10.812
50 -11.850 -11.452 -11.061 -10.904 -10.730 -10.690 -10.668 -10.628 -10.625 -10.620
20 frames
20 -11.769 -11.430 -11.186 -10.971 -10.921 -10.899 -10.886 -10.875 -10.873 -10.869
50 -11.841 -11.472 -11.018 -10.850 -10.737 -10.728 -10.706 -10.680 -10.662 -10.648
Table 1: Clustering results ? BNC tuples.
Figure 2: Illustration of clustering results.
Including input tuples with a frequency of one in
the training data with 10 subcategorisation frames
(as mentioned in Section 3.1) decreases the loge per
tuple to between -13.151 and -12.498 (for 5-50 it-
erations), with similar training behaviour as in Fig-
ure 2, and in comparsion to a baseline of -17.988.
The differences in the result indicate that the mod-
els including the hapax legomena are worse than the
models that excluded the sparse events; at the same
time, the differences between baseline and cluster-
ing model are larger.
In order to get an intuition about the qualitative
results of the clusterings, we select two example
clusters that illustrate that the idea of the verb class
model has been realised within the clusters. Ac-
cording to our own intuition, the clusters are over-
all semantically impressive, beyond the examples.
Future work will assess by semantics-based eval-
uations of the clusters (such as pseudo-word dis-
ambiguation, or a comparison against existing verb
classifications), whether this intuition is justified,
whether it transfers to the majority of verbs within
the cluster analyses, and whether the clusters cap-
ture polysemic verbs appropriately.
The two examples are taken from the 10 frame/50
cluster verb class model, with probabilities of 0.05
and 0.04. The ten most probable verbs in the first
cluster are show, suggest, indicate, reveal, find, im-
ply, conclude, demonstrate, state, mean, with the
two most probable frame types subj and subj:that,
i.e., the intransitive frame, and a frame that subcat-
egorises a that clause. As selectional preferences
within the intransitive frame (and quite similarly
in the subj:that frame), the most probable concept
classes6 are study, report, survey, name, research,
result, evidence. The underlined nouns represent
specific concept classes, because they are leaf nodes
in the selectional preference hierarchy, thus refer-
ring to very specific selectional preferences, which
are potentially useful for collocation induction. The
ten most probable verbs in the second cluster are
arise, remain, exist, continue, need, occur, change,
improve, begin, become, with the intransitive frame
being most probable. The most probable concept
classes are problem, condition, question, natural
phenomenon, situation. The two examples illustrate
that the verbs within a cluster are semantically re-
lated, and that they share obvious subcategorisation
frames with intuitively plausible selectional prefer-
ences.
4 Related Work
Our model is an extension of and thus most closely
related to the latent semantic clustering (LSC) model
(Rooth et al, 1999) for verb-argument pairs ?v, a?
which defines their probability as follows:
p(v, a) =
?
c
p(c) p(v|c) p(a|c)
In comparison to our model, the LSC model only
considers a single argument (such as direct objects),
6For readability, we only list one noun per WordNet concept.
502
or a fixed number of arguments from one particu-
lar subcategorisation frame, whereas our model de-
fines a probability distribution over all subcategori-
sation frames. Furthermore, our model specifies se-
lectional preferences in terms of general WordNet
concepts rather than sets of individual words.
In a similar vein, our model is both similar and
distinct in comparison to the soft clustering ap-
proaches by Pereira et al (1993) and Korhonen et
al. (2003). Pereira et al (1993) suggested determin-
istic annealing to cluster verb-argument pairs into
classes of verbs and nouns. On the one hand, their
model is asymmetric, thus not giving the same in-
terpretation power to verbs and arguments; on the
other hand, the model provides a more fine-grained
clustering for nouns, in the form of an additional hi-
erarchical structure of the noun clusters. Korhonen
et al (2003) used verb-frame pairs (instead of verb-
argument pairs) to cluster verbs relying on the Infor-
mation Bottleneck (Tishby et al, 1999). They had
a focus on the interpretation of verbal polysemy as
represented by the soft clusters. The main difference
of our model in comparison to the above two models
is, again, that we incorporate selectional preferences
(rather than individual words, or subcategorisation
frames).
In addition to the above soft-clustering models,
various approaches towards semantic verb classifi-
cation have relied on hard-clustering models, thus
simplifying the notion of verbal polysemy. Two
large-scale approaches of this kind are Schulte im
Walde (2006), who used k-Means on verb subcat-
egorisation frames and verbal arguments to cluster
verbs semantically, and Joanis et al (2008), who ap-
plied Support Vector Machines to a variety of verb
features, including subcategorisation slots, tense,
voice, and an approximation to animacy. To the
best of our knowledge, Schulte im Walde (2006) is
the only hard-clustering approach that previously in-
corporated selectional preferences as verb features.
However, her model was not soft-clustering, and
she only used a simple approach to represent selec-
tional preferences by WordNet?s top-level concepts,
instead of making use of the whole hierarchy and
more sophisticated methods, as in the current paper.
Last but not least, there are other models of se-
lectional preferences than the MDL model we used
in our paper. Most such models also rely on the
WordNet hierarchy (Resnik, 1997; Abney and Light,
1999; Ciaramita and Johnson, 2000; Clark and Weir,
2002). Brockmann and Lapata (2003) compared
some of the models against human judgements on
the acceptability of sentences, and demonstrated that
the models were significantly correlated with human
ratings, and that no model performed best; rather,
the different methods are suited for different argu-
ment relations.
5 Summary and Outlook
This paper presented an innovative, complex ap-
proach to semantic verb classes that relies on se-
lectional preferences as verb properties. The prob-
abilistic verb class model underlying the semantic
classes was trained by a combination of the EM al-
gorithm and the MDL principle, providing soft clus-
ters with two dimensions (verb senses and subcate-
gorisation frames with selectional preferences) as a
result. A language model-based evaluation showed
that after 10 training iterations the verb class model
results are above the baseline results.
We plan to improve the verb class model with re-
spect to (i) a concept-wise (instead of a cut-wise)
implementation of the MDL principle, to operate on
concepts instead of combinations of concepts; and
(ii) variations of the concept hierarchy, using e.g. the
sense-clustered WordNets from the Stanford Word-
Net Project (Snow et al, 2007), or a WordNet ver-
sion improved by concepts from DOLCE (Gangemi
et al, 2003), to check on the influence of concep-
tual details on the clustering results. Furthermore,
we aim to use the verb class model in NLP tasks, (i)
as resource for lexical induction of verb senses, verb
alternations, and collocations, and (ii) as a lexical
resource for the statistical disambiguation of parse
trees.
References
Steven Abney and Marc Light. 1999. Hiding a Seman-
tic Class Hierarchy in a Markow Model. In Proceed-
ings of the ACL Workshop on Unsupervised Learning
in Natural Language Processing, pages 1?8, College
Park, MD.
Leonard E. Baum. 1972. An Inequality and Associated
Maximization Technique in Statistical Estimation for
Probabilistic Functions of Markov Processes. Inequal-
ities, III:1?8.
503
Carsten Brockmann and Mirella Lapata. 2003. Evaluat-
ing and Combining Approaches to Selectional Prefer-
ence Acquisition. In Proceedings of the 10th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, pages 27?34, Budapest,
Hungary.
Glenn Carroll and Mats Rooth. 1998. Valence Induction
with a Head-Lexicalized PCFG. In Proceedings of the
3rd Conference on Empirical Methods in Natural Lan-
guage Processing, Granada, Spain.
Stanley Chen and Joshua Goodman. 1998. An Empirical
Study of Smoothing Techniques for Language Model-
ing. Technical Report TR-10-98, Center for Research
in Computing Technology, Harvard University.
Massimiliano Ciaramita and Mark Johnson. 2000. Ex-
plaining away Ambiguity: Learning Verb Selectional
Preference with Bayesian Networks. In Proceedings
of the 18th International Conference on Computa-
tional Linguistics, pages 187?193, Saarbru?cken, Ger-
many.
Stephen Clark and David Weir. 2002. Class-Based Prob-
ability Estimation using a Semantic Hierarchy. Com-
putational Linguistics, 28(2):187?206.
Bonnie J. Dorr and Doug Jones. 1996. Role of Word
Sense Disambiguation in Lexical Acquisition: Predict-
ing Semantics from Syntactic Cues. In Proceedings of
the 16th International Conference on Computational
Linguistics, pages 322?327, Copenhagen, Denmark.
Aldo Gangemi, Nicola Guarino, Claudio Masolo, and
Alessandro Oltramari. 2003. Sweetening WordNet
with DOLCE. AI Magazine, 24(3):13?24.
Eric Joanis, Suzanne Stevenson, and David James. 2008?
A General Feature Space for Automatic Verb Classifi-
cation. Natural Language Engineering. To appear.
Judith L. Klavans and Min-Yen Kan. 1998. The Role
of Verbs in Document Analysis. In Proceedings of
the 17th International Conference on Computational
Linguistics and the 36th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 680?686,
Montreal, Canada.
Philipp Koehn and Hieu Hoang. 2007. Factored Trans-
lation Models. In Proceedings of the Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 868?876, Prague, Czech Republic.
Upali S. Kohomban and Wee Sun Lee. 2005. Learning
Semantic Classes for Word Sense Disambiguation. In
Proceedings of the 43rd Annual Meeting on Associa-
tion for Computational Linguistics, pages 34?41, Ann
Arbor, MI.
Anna Korhonen, Yuval Krymolowski, and Zvika Marx.
2003. Clustering Polysemic Subcategorization Frame
Distributions Semantically. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 64?71, Sapporo, Japan.
Anna Korhonen. 2002. Subcategorization Acquisition.
Ph.D. thesis, University of Cambridge, Computer Lab-
oratory. Technical Report UCAM-CL-TR-530.
Karim Lari and Steve J. Young. 1990. The Estimation of
Stochastic Context-Free Grammars using the Inside-
Outside Algorithm. Computer Speech and Language,
4:35?56.
Hang Li and Naoki Abe. 1998. Generalizing Case
Frames Using a Thesaurus and the MDL Principle.
Computational Linguistics, 24(2):217?244.
Paola Merlo and Suzanne Stevenson. 2001. Automatic
Verb Classification Based on Statistical Distributions
of Argument Structure. Computational Linguistics,
27(3):373?408.
Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993.
Distributional Clustering of English Words. In Pro-
ceedings of the 31st Annual Meeting of the Associ-
ation for Computational Linguistics, pages 183?190,
Columbus, OH.
Detlef Prescher, Stefan Riezler, and Mats Rooth. 2000.
Using a Probabilistic Class-Based Lexicon for Lexical
Ambiguity Resolution. In Proceedings of the 18th In-
ternational Conference on Computational Linguistics.
Philip Resnik. 1997. Selectional Preference and Sense
Disambiguation. In Proceedings of the ACL SIGLEX
Workshop on Tagging Text with Lexical Semantics:
Why, What, and How?, Washington, DC.
Jorma Rissanen. 1978. Modeling by Shortest Data De-
scription. Automatica, 14:465?471.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Car-
roll, and Franz Beil. 1999. Inducing a Semantically
Annotated Lexicon via EM-Based Clustering. In Pro-
ceedings of the 37th Annual Meeting of the Association
for Computational Linguistics, Maryland, MD.
Sabine Schulte im Walde. 2006. Experiments on the Au-
tomatic Induction of German Semantic Verb Classes.
Computational Linguistics, 32(2):159?194.
Eric V. Siegel and Kathleen R. McKeown. 2000.
Learning Methods to Combine Linguistic Indica-
tors: Improving Aspectual Classification and Reveal-
ing Linguistic Insights. Computational Linguistics,
26(4):595?628.
Rion Snow, Sushant Prakash, Daniel Jurafsky, and An-
drew Y. Ng. 2007. Learning to Merge Word Senses.
In Proceedings of the joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, Prague, Czech
Republic.
Naftali Tishby, Fernando Pereira, and William Bialek.
1999. The Information Bottleneck Method. In Pro-
ceedings of the 37th Annual Conference on Communi-
cation, Control, and Computing, Monticello, IL.
504
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 421?432, Dublin, Ireland, August 23-29 2014.
Investigating the Usefulness of Generalized Word Representations in SMT
Nadir Durrani
University of Edinburgh
dnadir@inf.ed.ac.uk
Helmut Schmid Alexander Fraser
Ludwig Maximilian University Munich
fraser,schmid@cis.uni-muenchen.de
Philipp Koehn
University of Edinburgh
pkoehn@inf.ed.ac.uk
Abstract
We investigate the use of generalized representations (POS, morphological analysis and word
clusters) in phrase-based models and the N-gram-based Operation Sequence Model (OSM). Our
integration enables these models to learn richer lexical and reordering patterns, consider wider
contextual information and generalize better in sparse data conditions. When interpolating gen-
eralized OSM models on the standard IWSLT and WMT tasks we observed improvements of up
to +1.35 on the English-to-German task and +0.63 for the German-to-English task. Using auto-
matically generated word classes in standard phrase-based models and the OSM models yields
an average improvement of +0.80 across 8 language pairs on the IWSLT shared task.
1 Introduction
The increasing availability of digital text has galvanized the use of empirical methods in many fields
including Machine Translation. Given bilingual text, it is now possible to automatically learn translation
rules that required years of effort previously. Bilingual data, however, is abundantly available for only a
handful of language pairs. The problem of reliably estimating statistical models for translation becomes
more of a challenge under sparse data conditions especially when translating into morphologically rich
or syntactically divergent languages. The former becomes challenging due to lexical sparsity and the
latter suffers from sparsity in learning underlying reordering patterns. The last decade of research in
Statistical Machine Translation has witnessed many attempts to integrate linguistic analysis into SMT
models, to address the challenges of (i) translating into morphologically rich language languages, (ii)
modeling syntactic divergence across languages for better generalization in sparse data conditions.
The integration of the Operation Sequence Model into phrase-based paradigm (Durrani et al., 2013a;
Durrani et al., 2013b) improved the reordering capability and addressed the problem of the phrasal inde-
pendence assumption in the phrase-based models. The OSM model integrates translation and reordering
into a single generative story. By jointly considering translation and reordering context across phrasal
boundaries, the OSM model considers much richer conditioning than phrasal translation and lexicalized
reordering models. However, due to data sparsity the model often falls back to very small context sizes.
We address this problem by learning operation sequences over generalized representations such as POS
and Morph tags. This enables us to learn richer translation and reordering patterns that can general-
ize better in sparse data conditions. The model benefits from wider contextual information as we show
empirically in our results.
We investigate two methods to combine generalized OSM models with the lexically driven OSM
model and experimented on German-English translation tasks. Our best system that uses a linear combi-
nation of different OSM models gives significant improvements over a competitive baseline system. An
improvement of up to +1.35 was observed on the English-to-German and up to +0.63 BLEU points on
the German-to-English task over a factored augmented baseline system (Koehn and Hoang, 2007).
POS taggers and morphological analyzers, however, are not available for many resource poor lan-
guages. In the second half of the paper we investigate whether annotating the data with automatic word
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
421
clusters helps improve the performance. Word clustering is similar to POS-tagging/Morphological anno-
tation except that it also captures interesting syntactic and lexical semantics, for example countries and
languages are grouped in separate clusters, animate objects are differentiated from inanimate objects,
colors are grouped in a separate cluster etc. Word clusters, however, deterministically map each word
type to a unique
1
cluster, unlike POS/Morph tagging, and therefore might be less useful for disambigua-
tion. We use the mkcls utility in GIZA (Och and Ney, 2003) to cluster source and target vocabularies
into classes and will therefore refer to automatic classes as Och clusters/classes in this paper.
We first use Och classes as an additional factor in phrase-based translation model, along with a target
LM model over cluster-ids to improve the baseline system. We then additionally use the OSM model
over cluster-ids. Our experiments include translation from English to Dutch, French, Italian, Polish,
Portuguese, Russian, Spanish, Slovenian and Turkish on IWSLT shared task data. Our results show an
average improvement of +0.80, ranging from +0.41 to +2.02. Compared to the improved baseline system
obtained by using Och classes as a factor in phrase-based translation models, adding an OSM model over
cluster-ids improved performance in four (French, Spanish, Dutch and Slovenian) out of eight cases. In
other cases performance stayed constant or dropped slightly. We also used POS annotations for three
tasks, namely translating from English into French, Spanish and Dutch to compare the performance of
the two different kinds of generalizations. Surprisingly, using Och classes always performed better than
using POS annotations. The rest of the paper is organized as follows. Section 2 gives an account on
related work. Section 3 discusses the factor-based OSM model. Section 4 presents the experimental
setup and the results. Section 5 concludes the paper.
2 Related Work
Previous work on integrating linguistic knowledge into SMT models can be broken into two groups. The
first group focuses on using linguistic knowledge to improve reordering between syntactically different
languages. A second group focuses on translating into morphologically rich languages.
Initial efforts to use linguistic annotation focused on rearranging source sentences to be in the target
order. Xia and McCord (2004) proposed a method to automatically learn rewrite rules to preorder source
sentences. Collins et al. (2005) and Popovi?c and Ney (2006) proposed methods for reordering the source
using a small set of handcrafted rules. Crego and Mari?no (2007) use syntactic trees to derive rewrite
rules. Hoang and Koehn (2009) used POS tags to create templates for surface word translation to create
longer phrase translation. A whole new paradigm of using syntactic annotation to address long range
reorderings has emerged following Galley et al. (2006), Zollmann and Venugopal (2006), Chiang (2007)
etc. Crego and Yvon (2010) and Niehues et al. (2011) used a Tuple Sequence Model (TSM) over POS
tags in an N-gram-based search to improve mid-range reorderings. Our work is similar to them except
that OSM model is substantially different from the TSM model as it integrates both the translation and
reordering mechanisms into a combined model. Therefore both translation and reordering decisions can
benefit from richer generalized representations.
A second group of work addresses the problem of translating into morphologically richer languages.
The idea of translating to stems and then inflecting the stems in a separate step has been studied by
Toutanova et al. (2008), de Gispert and Mari?no (2008), Fraser et al. (2012), Chahuneau et al. (2013) and
others. Koehn and Hoang (2007) proposed to integrate different levels of linguistic information as factors
into the phrase-based translation model. Yeniterzi and Oflazer (2010) used source syntactic structures as
additional complex tag factors for English-to-Turkish phrase-based machine translation. Green and DeN-
ero (2012) proposed a target-side, class-based agreement model to handle morpho-syntactic agreement
errors when translating from English-to-Arabic. El Kholy and Habash (2012) tested three models to find
out which features are best handled by modeling them as a part of translation, and which ones are better
predicted through generation, also in the English-to-Arabic task. Several researchers attempted to use
word lattices to handle generalized representation (Dyer et al., 2008; Hardmeier et al., 2010; Wuebker
and Ney, 2012). Automatically clustering the training data into word classes in order to obtain smoother
1
We are referring to hard clustering here. Soft clustering is intractable as it requires a marginalization over all possible
classes when calculating the n-gram probabilities.
422
Figure 1: Operation Sequence Model ? Training Sentence with Generation and Test Sentences
distributions and better generalizations has been a widely known and applied technique in natural lan-
guage processing. Training based on word classes has been previously explored by various researchers.
Cherry (2013) addressed data sparsity in lexicalized reordering models by using sparse features based
on word classes. Other parallel attempts on using word-class models include Wuebker et al. (2013),
Chahuneau et al. (2013) and Bisazza and Monz (2014).
More recent research has started to set apart from the conventional maximum likelihood estimates
toward neural network-based models that use continuous space representation (Schwenk, 2012; Le et al.,
2012; Hu et al., 2014; Gao et al., 2014). Although these methods have achieved impressive improve-
ments, traditional models continue to dominate the field due to their simplicity and low computational
complexity. How much of the improvement will be retained when scaling these models to all available
data instead of a limited amount will be interesting.
3 Operation Sequence Model
The Operation Sequence Model (Durrani et al., 2011) is an instance of the N-gram based SMT framework
(Casacuberta and Vidal, 2004; Mari?no et al., 2006). It represents the translation process through a
sequence of operations. An operation can be to simultaneously generate source or target words or to
perform reordering. Reordering is carried out through jump and gap operations. The model is different
from its ancestors in that it strongly integrates translation and reordering into a single generative story in
which translation decisions can influence and get impacted by the reordering decisions and vice versa.
Given a bilingual sentence pair < F,E > and its alignment A, a sequence of operations o
1
, o
2
. . . , o
J
is generated deterministically through a conversion algorithm. The model is learned by learning Markov
chains over these sequences and is formally defined as:
p
osm
(F,E,A) =
J
?
j=1
p(o
j
|o
j?n+1
, ..., o
j?1
)
Figure 1 shows an example of an aligned bilingual sentence pair and the corresponding operation se-
quence used to generate it. There is a 1-1 correspondence between a sentence pair and its operation
sequence. We thus get a unique sequence for every bilingual sentence pair given the alignment.
3.1 Motivation
Due to data sparsity it is impossible to observe all possible reordering patterns with all possible lexical
choices in translation operations. The lexically driven OSM model therefore often backs off to very
small context sizes. Coming back to the training example in Figure 1. The useful reordering pattern
423
learned through this example is:
Ich kann umstellen? I can rearrange
which is memorized through the operation sequence:
Generate(Ich, I) ? Generate(kann, can) ? Insert Gap ? Generate(umstellen, rearrange)
It can generalize to the test sentence shown in Figure 1(a). However, it fails to generalize to the sentences
in Figure 1(b) and (c) although the underlying reordering pattern is the same. The second part of the
German verb complex usually appears at the end of a clause or a sentence and needs to be moved in order
to produce the correct English word order. However, due to data sparsity such a combination of lexical
decisions and reordering decisions may not be observed during training. The model would therefore fail
to generalize in such circumstances. This problem can be addressed by learning a generalized form of
the same reordering rule. By annotating the corpus with word classes such as POS tags, we obtain the
reordering pattern:
PPER VMFIN VVINF? PP MD VB
memorized through the operation sequence:
Generate (PPER,PP) ? Generate (VMFIN,MD) ? Insert Gap ? Generate (VVINF,VB)
This rule generalizes to all the test sentences in Figure 1. Since the OSM model strongly couples
translation and reordering, the probability of each translation or reordering operation depends on the
n previous translation/reordering decisions. The generalization of the model by replacing words with
POS tags allows the model to consider a wider syntactic context, thus improving lexical decisions and
the reordering capability of the model. Using different kinds of word classes, we can also control the
type of abstraction. Using lemmas for example, we can map different forms of the verb ?k?onnen ? can?
(kann, kannst, konnte) to a single class. Och clusters can provide different levels of granularity.
3.2 Models
Given that we can learn OSM models over different word representations, the question then is how
to combine the lexically driven OSM model with an OSM model based on a generalized word repre-
sentation. The simplest approach is to treat each OSM model as a separate feature in the log-linear
framework, thus summing up the weighted log probabilities. The effect of this is similar to an And
operation. A translation is considered good if both, the word-based OSM and the POS-based OSM
models indicate that it is a good translation. However, an Or operation might be more desirable in
some scenarios. The operation Generate (trotz, in spite of) should be ranked high although the POS-
based operation Generate(APPR, IN IN IN) is improbable. Similarly, the generalized operation sequence:
Insert Gap ? Generate (ADJ, JJ) ? Jump Back ? Generate (NOM, NN)
that captures the swapping of noun and adjective in French-English, should be ranked higher
even though noir (black) never appeared after cheval (horse) during training and the sequence:
Insert Gap ? Generate (noir, black) ? Jump Back ? Generate (cheval , horse)
is never observed. Instead of using both the models, a single model that could switch between
different generalized OSMs during translation and choose the one which gives the best prediction
in each situation, can be used. In order to achieve this effect, we formulated a second model that
interpolates the lexically driven OSM model with its generalized variants. However, we can only
424
interpolate two models that predict the same representation. The lexically driven OSM predicts the
surface forms whereas the POS-based OSM predicts POS translations. To make the two comparable,
we multiply the POS-based OSM probability with the probability of the lexical operation given the POS
operation. More specifically the probability of the generalized model gm can be defined as:
p
gm
(o
j
|o
j?1
j?n+1
) = p
osm
pos
(o
?
j
|o
?
j?1
j?n+1
) p(o
j
|o
?
j
) (1)
where p
osm
pos
is the operation sequence model learned over POS tags and p(o
j
|o
?
j
) is the probability of
the lexical operation given the POS-based operation. It is 1 for all reordering operations. We assume here
that for each lexical operation o
j
a corresponding POS-based operation o
?
j
is uniquely determined. With
p
osm
sur
= p
osm
sur
(o
j
|o
j?1
j?n+1
) (lexically driven OSM model) and p
gm
= p
gm
(o
j
|o
j?1
j?n+1
) (generalized
OSM model as described above), the overall probability of the new model p
osm
is defined as:
p
osm
= ?p
osm
sur
+ (1? ?)p
gm
(2)
Such an interpolation is expensive in the discriminative training. It would require a sub-tuning routine
inside of tuning, a main loop to train all the features including the OSM model and an inner loop to
distribute the weight assigned to OSM model among lexically driven and POS-based OSM models. We
therefore just take the larger one of the two model values and add a POS-based translation penalty ?. The
value of this penalty is the number of times that the POS-based operation was chosen when translating
a sentence. This penalty acts similarly as the prior ? above. Using this formulation, the model could
therefore be redefined as:
p
osm
=
{
p
osm
sur
if p
osm
sur
? e
?
p
gm
e
?
p
gm
otherwise
(3)
where ? is the weight for the POS driven translation penalty ?. This allows the optimizer to control
whether it prefers the lexically driven or the POS-driven OSM model. By setting a very low weight ?
the optimizer can force the translator to always choose lexically driven OSM. This formulation can be
extended to multiple generalized OSM models based on e.g. POS tags, morphological tags, or word
clusters. Equation 2 can be rewritten as follows:
p
osm
= ?
1
p
osm
sur
+
n
?
i=2
?
i
p
gm
i
(4)
with
?
n
i=1
?
i
= 1 and p
gm
i
defined analogous to Equation 1.
Setting p
gm
1
= p
osm
sur
and ?
1
= 0, we can again simplify Equation 4 by taking the maximum to:
p
osm
=
n
max
i=1
e
?
i
p
gm
i
(5)
We use a translation penalty ?
i
for each generalized model and tune its weight ?
i
along with the weights
of other features. We will refer to this model as Model
or
in this paper and the commonly used log-
linear interpolation of the features as Model
and
. The intuition behind Model
or
is that we back-off
to generalized representations only when the lexically driven model doesn?t provide enough contextual
evidence. The downside of this approach, however, is that unlike Model
and
, it cannot distribute weights
over multiple features and solely relies on a single model.
4 Evaluation
Data: We ran experiments with data made available for the translation task of the IWSLT-13 (Cettolo et
al., 2013): International Workshop on Spoken Language Translation
2
and WMT-13 (Bojar et al., 2013):
Eighth Workshop on Statistical Machine Translation.
3
The sizes of bitext used for the estimation of
translation and monolingual language models are reported in Table 1.
We used LoPar (Schmid, 2000) to obtain morphological analysis and POS annotation of German and
MXPOST (Ratnaparkhi, 1998), a maximum entropy model for English POS tags. For other language
pairs we used TreeTagger (Schmid, 1994).
2
http://www.iwslt2013.org/
3
http://www.statmt.org/wmt13/
425
Pair Parallel Monolingual Pair Parallel Monolingual Pair Parallel Monolingual
de?en ?4.6 M ?287.3 M en?de ?4.6 M ?59.5 M en?fr ?5.5 M ?69 M
en?es ?4.1 M ?59.6 M en?nl ?2.1 M ?21.7 M en?ru ?1.15 M ?21 M
en?pt ?1.0 M ?2.3 M en?pl ?0.77 M ?0.8 M en?sl ?0.63 M ?0.65 M
en?tr ?0.13 M ?0.14 M
Table 1: Number of Sentences (in Millions) used for Training
Model iwslt
10
wmt
13
iwslt
10
wmt
13
English-to-German German-to-English
Baseline 23.56 20.38 31.46 27.27
M
and
(pos,pos)
23.93?+0.37 20.61 ?+0.23 31.91?+0.45 27.55 ?+0.28
M
and
(pos,morph)
24.62?+1.06 20.88?+0.50 32.09?+0.63 27.62?+0.35
M
and
(all)
24.91?+1.35 20.93?+0.55 32.00?+0.54 27.71?+0.44
M
or
(pos,pos)
23.61 ?+0.05 20.24 ?-0.14 31.55 ?+0.09 27.32 ?+0.05
M
or
(pos,morph)
23.83 ?+0.27 20.44 ?+0.08 31.58 ?+0.12 27.20 ?-0.07
M
or
(all)
23.88 ?+0.32 20.55 ?+0.17 31.40 ?-0.06 27.15 ?-0.12
Table 2: Evaluating Generalized OSM Models for German-English pairs ? Bold: Statistically Significant
(Koehn, 2004) w.r.t Baseline
Baseline System: We trained a Moses system (Koehn et al., 2007), replicating the settings described
in (Birch et al., 2013) developed for the 2013 Workshop on Spoken Language Translation. The features
included: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ align-
ments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011)
used at runtime, a lexically-driven 5-gram operation sequence model (Durrani et al., 2013b) with 4
additional supportive features: 2 gap-based penalties, 1 distance-based feature and 1 deletion penalty,
lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6,
100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), Cube Prun-
ing (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the
no-reordering-over-punctuation heuristic. We used the compact phrase table representation by Junczys-
Dowmunt (2012). For our German-to-English experiments, we used compound splitting (Koehn and
Knight, 2003). German-to-English and English-to-German baseline systems also used POS and mor-
phological target sequence models built on the in-domain subset of the parallel corpus using Kneser-Ney
smoothed 7-gram models and as additional factors in phrase translation models (Koehn and Hoang,
2007). We used an unsupervised transliteration model (Durrani et al., 2014) to transliterate OOV words
when translating into Russian.
Tuning and Test: The systems were tuned on the dev2010 dataset and evaluated on the test2010-2013
datasets made available for the IWSLT-13 workshop. We performed a secondary set of experiments
for German-English pairs using tuning and test sets made available for the WMT-13 workshop. We
concatenated the news-test sets 2008 and 2009 to obtain a large dev-set of 4576 sentences. Evaluation
was performed on the news-test set 2013 which contains 3000 sentences. Tuning was performed using
the k-best batch MIRA algorithm (Cherry and Foster, 2012) with at most 25 iterations. We use BLEU
(Papineni et al., 2002) as a metric to evaluate our results.
Results I ? Using Linguistic Annotation: We trained 5-gram OSM models over different representa-
tions and added these to the baseline system. First we evaluated Model
and
(M
and
) which uses a MIRA
tuned linear combination of different OSM models versus Model
or
(M
or
) which computes only one
OSM model but allows the generator to switch between different OSM models built on various gener-
alized forms. Table 2 shows results from running experiments on German-English pairs. We found that
the simpler model Model
and
outperforms Model
or
in all the experiments. Model
or
does not give
significant improvements over the baseline system and shows an occasional drop. This result is contrary
to the expectation formulated in Section 3.2. We speculate that the optimizer faces problems to train this
kind of model, because it cannot take into account that the selected OSM model can change when the
weight parameter is modified. It assumes that the feature stays constant. In our formulation the same
426
derivation can occur with different feature scores in different decoding runs and the optimizer is unable
to handle this. Our speculation is based on the observation of ?
?
, the weight of feature ? which allows
the translator to switch between different OSM models. The value of ?
?
was not stable across different
iterations and different experiments.
Model
and
consistently improves the baseline. Adding an OSM model over [pos, morph] (source:pos,
target:morph) combination gave the best results, giving a statistically significant gain of +1.06 on the
iwslt
10
test-set and +0.50 on the wmt
13
test-set. Using an OSM model over a [pos,pos] combination
also showed improvements, however, not as much as using morphological tags. Morphological tags pro-
vide richer information for disambiguation when translating into German. Note that the baseline system
also used a target sequence model over morphological tags. Nevertheless using an OSM [pos,morph]
model still gives significant improvements which shows that learning a joint model over source and tar-
get units is more fruitful than only considering target-side information. Using both the models together
gave best results for English-to-German giving a further improvement of +0.29 on the iwslt
10
task but
no real gain on the wmt
13
task. Using morphological tags also produced the best results for the German-
to-English pair, giving a statistically significant gain of +0.63 on iwslt
10
and +0.35 on wmt
13
. Using
both the models together did not give any further significant improvements. The results changed by
+0.10 and -0.09 on the wmt
13
and iwslt
10
test-sets respectively.
Results-II ? Using Och Classes: In our secondary experiments we tested the effect of using Och
clusters. The overall goal was to study whether using unsupervised word classes can serve the same
purpose as POS tags and to compare the two methods of annotating the data. We obtained Och clusters
using the mkcls utility (Och, 1999) in GIZA++ (Och and Ney, 2003). This is generally run during
the alignment process where data is divided into 50 classes to estimate IBM Model-4. Chahuneau et
al. (2013) found mapping data to 600 Och clusters useful, so we used this as well. We additionally
experimented with using 200 and 1000 classes. We integrated Och clusters as additional factors
4
when
training the phrase-translation models and used a monolingual n-gram model over cluster-ids built on the
target-side of the in-domain corpus. Then we added a 5-gram OSM model over cluster-ids. We replace
surface forms with their cluster-ids in source and target corpus and convert it to operation sequences,
that jointly generate source and target cluster-ids. We only used Model
and
for these experiments when
adding an OSM model over cluster-ids.
B
0
50 200 600 1000 POS 50 200 600 1000 POS
Target Sequence Model over Word Clusters Operation Sequence Model over Word Clusters
en? fr 33.17 33.30 33.40 33.05 33.05 33.14 33.76 33.74 33.58 33.75 33.03
en? es 34.14 34.33 34.58 34.46 33.96 33.91 34.73 34.62 34.60 34.55 34.35
en? nl 26.51 26.67 26.15 26.31 26.47 26.55 26.91 26.52 26.61 26.49 26.62
en? ru 13.12 13.34 13.51 13.53 13.97 ? 13.61 13.66 13.80 13.63 ?
en? sl 17.98 18.67 18.55 17.67 17.97 ? 18.64 18.91 18.17 17.98 ?
en? pt 30.80 31.62 32.21 32.40 32.44 ? 31.77 32.44 32.34 31.90 ?
en? pl 9.74 9.90 10.11 10.05 10.43 ? 10.06 10.19 10.24 10.14 ?
en? tr 7.18 7.43 7.45 7.50 7.50 ? 7.26 7.28 7.51 7.54 ?
Table 3: Evaluating Phrase-based and N-gram-based Translation Models over Och Clusters
Table 3 shows results from using models based on cluster-ids. The left side of the table evaluate the
use of adding a target sequence model over cluster-ids using a factored-based translation model. Results
improved consistently in all resource poor languages (pt, pl, tr) giving significant improvements in most
of the cases. Mixed results were obtained for the pairs with a reasonable amount of parallel data (fr,
es, nl), showing an occasional drop in performance. However, improvements can be found for all the
language pairs.
4
Note that adding cluster-ids in factored models alone has no impact in this scenario, as we are using hard clustering (each
word deterministically maps onto a unique cluster-id). In a joint source-target factored model which is what we are using, it
will result in an identical distribution as the baseline system.
427
In the right half of the table we tested whether additionally using an OSM model built over cluster-ids,
on top of a phrase-based system that uses cluster-ids as factor and target language model, improves the
performance any further. Consistent improvements were seen in Spanish and French. Better systems
were produced in the case of French, Spanish, Dutch and Slovenian. No improvements were observed
for Turkish and Portuguese whereas the performance got worse in Polish and Russian.
Using 50 classes consistently improved the baseline. Different numbers of clusters provide different
levels of abstraction and granularity. We also tried using OSM models over different numbers of clus-
ters simultaneously for English-to-Spanish, English-to-French and English-to-Dutch pairs in an effort to
explore whether using different numbers of clusters to classify data provides different information. A
slight gain was observed for EN-ES as the best system improved from 34.73 to 34.95. No further gains
were observed for the other two pairs.
We also used POS annotation as a factor instead of Och clusters in French, Spanish and Dutch. See
the POS columns of Table 3. Using POS as an additional factor, did not improve over the baseline
performance. A significant drop was seen in the case of English-to-Spanish. Using a POS-based OSM
on top of the POS-based phrase-model did not help either except for Spanish where results got improved
by +0.44 over its phrase-based variant that used a POS factor. However, using Och clusters produced
better results in all three cases. We speculate that the reason for this result is that Och clusters are
more evenly distributed as compared to POS tags where the distribution is biased toward noun class
and secondly Och clusters are optimized for language modeling. Also each word is deterministically
mapped to a single class but can have multiple POS tags. The latter thus causes a sparser translation
model. Finally Table 4 shows the comparison of results on iwslt
11?13
by running baseline B
0
and best
systems B
x
in Tables 3.
iwslt
11
iwslt
12
iwslt
13
Avg
B
0
B
x
B
0
B
x
B
0
B
x
B
0
B
x
?
en? fr 39.84 40.63 40.50 41.24 ? ? 40.24 40.94 +0.70
en? es 32.89 33.24 26.45 26.81 34.01 34.73 31.12 31.60 +0.48
en? nl 30.01 30.31 26.40 26.72 24.96 25.57 27.12 27.53 +0.41
en? ru 14.93 15.91 13.01 13.53 15.65 16.4 14.53 15.28 +0.75
en? sl ? ? 11.34 12.40 12.85 13.73 12.09 13.10 +1.01
en? pt 31.61 33.62 33.24 34.91 30.83 33.24 31.89 33.92 +2.02
en? pl 12.73 13.13 9.52 10.50 11.30 11.54 11.18 11.72 +0.53
en? tr 7.01 7.42 6.99 7.43 6.21 6.84 6.74 7.23 +0.49
Avg 24.15 24.89 20.93 21.69 19.40 20.29 21.49 22.29 +0.80
Table 4: Evaluating on Test Sets iwslt
11?13
? B
0
= Baseline System, B
x
= Best Systems in Tables 2
Analysis: In a post-evaluation analysis we confirmed whether using generalized OSM models actually
consider a wider contextual window than its lexically driven variant. The graph shown in Figure 2 shows
average context size considered (on top of each set of bars) and percentages of 1-5 gram matches by
different OSM models. The results show that the probability of an operation is conditioned on less than a
trigram in the OSM model over surface forms. In comparison OSM models over POS, morph or cluster-
ids consider a window of roughly 4 previous operations thus considering more contextual information.
The percentage of 5-gram matches increases from 15.5% to 59.2% using POS-based OSM model and
up to 45.6% in morph-based OSM model, the number of unigram matches are decreased from 8.30% to
less than 1% in both the models. Similar observation is made for the OSM models over clusters where
5-gram matches improve from 12% to 30% on average, showing the ability of the generalized models to
use richer conditioning thus improving the translation quality.
We also analyzed what kind of words are clustered together using Och classes and found that clusters
capture both syntax and lexical semantics. Figure 2 (b) shows several useful clusters to exhibit this. We
also saw negative examples where words from different classes are clustered together. ?Boy?, ?Girl? and
?Man? for example were clustered into a single class but ?Woman? in another. Similarly ?Grey? and
?Orange? were grouped together with animated objects.
428
Figure 2: (a) Average Size of N-grams Used in Different OSM Models and Percentages of 1-5 Gram
Matches in Three Language Pairs (b) Different Word Clusters using 50 Classes
5 Conclusion
In this paper we investigated the usefulness of integrating word classes in phrase-based models and
Operation Sequence N-gram models. We explored two models of interpolating generalized OSM models
and tested variations on the standard IWSLT and WMT tasks. Our results showed that the simpler more
commonly used method of integrating the models in the log-linear framework worked best. We showed
that by learning OSM models over generalized POS and morphological representations, we were able
to build richer models that outperformed state-of-the-art baseline systems. Statistically significant gains
of up to +1.35 and +0.63 were observed in English-to-German and German-to-English tasks. We also
made use of Och classes as additional factors in phrase translation and language models. These were
tested translating from English to 8 different languages which includes a mixture of morphologically
rich (French, Spanish and Russian, Dutch, and Turkish) and sparse data (Portuguese, Polish, Slovenian
and Turkish) languages. Our results show that using clusters was helpful in all of the cases. Using
the OSM model over word-clusters additionally improved the performance further. Our results show an
average improvement of +0.80, ranging from +0.41 to +2.02. Our EN-FR systems were ranked third (on
tst2013) and second (on tst2011-tst2012) in IWSLT-13 translation task following EU-Bridge (Freitag et
al., 2013) which used our output for system combination. The code to train class-based models has been
made available to the research community via the Moses toolkit. See Advanced Features
5
in the Moses
Decoder for details.
Acknowledgements
We would like to thank the anonymous reviewers for their helpful feedback and suggestions. The re-
search leading to these results has received funding from the European Union Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agreements n
?
287658 (EU-Bridge) and n
?
287688 (MateCat).
Alexander Fraser was funded by Deutsche Forschungsgemeinschaft grant Models of Morphosyntax for
Statistical Machine Translation. Helmut Schmid was supported by Deutsche Forschungsgemeinschaft
grant SFB 732. This publication only reflects the authors? views.
References
Alexandra Birch, Nadir Durrani, and Philipp Koehn. 2013. Edinburgh SLT and MT System Description for the
IWSLT 2013 Evaluation. In Proceedings of the 10th International Workshop on Spoken Language Translation,
5
http://www.statmt.org/moses/?n=Moses.AdvancedFeatures
429
pages 40?48, Heidelberg, Germany, December.
Arianna Bisazza and Christof Monz. 2014. Class-Based Language Modeling for Translating into Morphologically
Rich Languages. In Proceedings of the 25th Annual Conference on Computational Linguistics (COLING),
Dublin, Ireland, August.
Ondrej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof
Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine
Translation. In Eighth Workshop on Statistical Machine Translation, WMT-2013, pages 1?44, Sofia, Bulgaria.
Francisco Casacuberta and Enrique Vidal. 2004. Machine Translation with Inferred Stochastic Finite-State Trans-
ducers. Computational Linguistics, 30:205?225.
Mauro Cettolo, Jan Niehues, Sebastian St?uker, Luisa Bentivogli, and Marcello Federico. 2013. Report on the 10th
IWSLT Evaluation Campaign. Proceedings of the International Workshop on Spoken Language Translation,
Heidelberg, Germany.
Victor Chahuneau, Eva Schlinger, Noah A. Smith, and Chris Dyer. 2013. Translating into Morphologically Rich
Languages with Synthetic Phrases. In Proceedings of the 2013 Conference on Empirical Methods in Natural
Language Processing.
Colin Cherry and George Foster. 2012. Batch Tuning Strategies for Statistical Machine Translation. In Proceed-
ings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, pages 427?436, Montr?eal, Canada, June. Association for Computational Lin-
guistics.
Colin Cherry. 2013. Improved Reordering for Phrase-Based Translation using Sparse Features. In Proceedings of
the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, pages 22?31, Atlanta, Georgia, June. Association for Computational Linguistics.
David Chiang. 2007. Hierarchical Phrase-Based Translation. Computational Linguistics, 33(2):201?228.
Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005. Clause Restructuring for Statistical Machine Trans-
lation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL?05),
pages 531?540, Ann Arbor, MI.
Josep M. Crego and Jos?e B. Mari?no. 2007. Syntax-Enhanced N-gram-Based SMT. In Proceedings of the 11th
Machine Translation Summit, MT Summit XI, pages 111?118.
Josep M. Crego and Franc?ois Yvon. 2010. Improving Reordering with Linguistically Informed Bilingual N-
Grams. In Coling 2010: Posters, pages 197?205, Beijing, China, August. Coling 2010 Organizing Committee.
Adri`a de Gispert and Jos?e B. Mari?no. 2008. On the Impact of Morphology in English to Spanish statistical MT.
Speech Communication, 50(11-12):1034?1046.
Nadir Durrani, Helmut Schmid, and Alexander Fraser. 2011. A Joint Sequence Translation Model with Integrated
Reordering. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:
Human Language Technologies, pages 1045?1054, Portland, Oregon, USA, June.
Nadir Durrani, Alexander Fraser, and Helmut Schmid. 2013a. Model With Minimal Translation Units, But Decode
With Phrases. In The 2013 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, Atlanta, Georgia, USA, June. Association for Computational Lin-
guistics.
Nadir Durrani, Alexander Fraser, Helmut Schmid, Hieu Hoang, and Philipp Koehn. 2013b. Can Markov Models
Over Minimal Translation Units Help Phrase-Based SMT? In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics, Sofia, Bulgaria, August. Association for Computational Linguistics.
Nadir Durrani, Hassan Sajjad, Hieu Hoang, and Philipp Koehn. 2014. Integrating an Unsupervised Transliteration
Model into Statistical Machine Translation. In Proceedings of the 15th Conference of the European Chapter of
the ACL (EACL 2014), Gothenburg, Sweden, April. Association for Computational Linguistics.
Christopher Dyer, Smaranda Muresan, and Philip Resnik. 2008. Generalizing Word Lattice Translation. In
Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1012?
1020, Columbus, OH, USA. The Association for Computer Linguistics.
Ahmed El Kholy and Nizar Habash. 2012. Translate, Predict or Generate: Modeling Rich Morphology in Statis-
tical Machine Translation. volume 12.
430
Alexander Fraser, Marion Weller, Aoife Cahill, and Fabienne Cap. 2012. Modeling Inflection and Word-
Formation in SMT. In Proceedings of the 13th Conference of the European Chapter of the Association for
Computational Linguistics, pages 664?674, Avignon, France, April. Association for Computational Linguistics.
Markus Freitag, Stephan Peitz, Joern Wuebker, Hermann Ney, Nadir Durrani, Matthias Huck, Philipp Koehn,
Thanh-Le Ha, Jan Niehues, Mohammed Mediani, Teresa Herrmann, Alex Waibel, Nicola Bertoldi, Mauro Cet-
tolo, and Marcello Federico. 2013. EU-BRIDGE MT: Text Translation of Talks in the EU-BRIDGE Project. In
International Workshop on Spoken Language Translation, Heidelberg, Germany, December.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer.
2006. Scalable Inference and Training of Context-Rich Syntactic Translation Models. In Proceedings of
COLING-ACL, pages 961?968, Sydney, Australia. Association for Computational Linguistics.
Jianfeng Gao, Xiaodong He, Scott Wen-tau Yih, and Li Deng. 2014. Learning Continuous Phrase Representations
for Translation Modeling. In Proceedings of the Association for Computational Linguistics, Baltimore, MD,
USA, June.
Spence Green and John DeNero. 2012. A Class-Based Agreement Model for Generating Accurately Inflected
Translations. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 146?155, Jeju Island, Korea, July. Association for Computational Linguistics.
Christian Hardmeier, Arianna Bisazza, and Marcello Federico. 2010. FBK at WMT 2010: Word Lattices for Mor-
phological Reduction and Chunk-Based Reordering. In Proceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 88?92, Uppsala, Sweden, July. Association for Computational
Linguistics.
Eva Hasler, Barry Haddow, and Philipp Koehn. 2012. Sparse Lexicalised features and Topic Adaptation for
SMT. In Proceedings of the seventh International Workshop on Spoken Language Translation (IWSLT), pages
268?275.
Kenneth Heafield. 2011. KenLM: Faster and Smaller Language Model Queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages 187?197, Edinburgh, Scotland, United Kingdom, July.
Hieu Hoang and Philipp Koehn. 2009. Improving Mid-Range Re-Ordering Using Templates of Factors. In
Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pages 372?379, Athens,
Greece, March. Association for Computational Linguistics.
Yuening Hu, Michael Auli, Qin Gao, and Jianfeng Gao. 2014. Minimum Translation Modeling with Recurrent
Neural Networks. In Proceedings of the 14th Conference of the European Chapter of the Association for Com-
putational Linguistics, pages 20?29, Gothenburg, Sweden, April. Association for Computational Linguistics.
Liang Huang and David Chiang. 2007. Forest Rescoring: Faster Decoding with Integrated Language Models.
In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 144?151,
Prague, Czech Republic, June. Association for Computational Linguistics.
Marcin Junczys-Dowmunt. 2012. Phrasal Rank-Encoding: Exploiting Phrase Redundancy and Translational
Relations for Phrase Table Compression. The Prague Bulletin of Mathematical Linguistics, 98:63?74.
Philipp Koehn and Hieu Hoang. 2007. Factored Translation Models. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Processing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 868?876, Prague, Czech Republic, June. Association for Computational Linguistics.
Philipp Koehn and Kevin Knight. 2003. Empirical methods for compound splitting. In Proceedings of the 10th
Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 187?193,
Morristown, NJ.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan
Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In ACL 2007 Demonstrations,
Prague, Czech Republic.
Philipp Koehn. 2004. Statistical Significance Tests for Machine Translation Evaluation. In Dekang Lin and Dekai
Wu, editors, Proceedings of EMNLP 2004, pages 388?395, Barcelona, Spain, July.
Shankar Kumar and William J. Byrne. 2004. Minimum Bayes-Risk Decoding for Statistical Machine Translation.
In HLT-NAACL, pages 169?176.
431
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon. 2012. Continuous Space Translation Models with Neural
Networks. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Com-
putational Linguistics: Human Language Technologies, pages 39?48, Montr?eal, Canada, June. Association for
Computational Linguistics.
Jos?e B. Mari?no, Rafael E. Banchs, Josep M. Crego, Adri`a de Gispert, Patrik Lambert, Jos?e A. R. Fonollosa, and
Marta R. Costa-juss`a. 2006. N-gram-Based Machine Translation. Computational Linguistics, 32(4):527?549.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and Alex Waibel. 2011. Wider Context by Using Bilingual Lan-
guage Models in Machine Translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation,
pages 198?206, Edinburgh, Scotland, July. Association for Computational Linguistics.
Franz J. Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Franz J. Och. 1999. An Efficient Method for Determining Bilingual Word Classes. In Processings of EACL, pages
71?76, Bergen, Norway.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: A Method for Automatic Eval-
uation of Machine Translation. In Proceedings of the 40th Annual Meeting on Association for Computational
Linguistics, ACL ?02, pages 311?318, Morristown, NJ, USA.
Maja Popovi?c and Hermann Ney. 2006. POS-based Word Reorderings for Statistical Machine Translation. In
International Conference on Language Resources and Evaluation, pages 1278?1283, Genoa, Italy.
Adwait Ratnaparkhi. 1998. Maximum Entropy Models for Natural Language Ambiguity Resolution. Ph.D. thesis,
University of Pennsylvania, Philadelphia, PA.
Helmut Schmid. 1994. Probabilistic Part-of-Speech Tagging Using Decision Trees. In International Conference
on New Methods in Language Processing, pages 44?49, Manchester, UK.
Helmut Schmid. 2000. Lopar: Design and implementation. Bericht des sonderforschungsbereiches ?sprachtheo-
retische grundlagen fr die computerlinguistik?, Institute for Computational Linguistics, University of Stuttgart.
Holger Schwenk. 2012. Continuous Space Translation Models for Phrase-Based Statistical Machine Translation.
In Proceedings of COLING 2012: Posters, pages 1071?1080, Mumbai, India, December. The COLING 2012
Organizing Committee.
Kristina Toutanova, Hisami Suzuki, and Achim Ruopp. 2008. Applying Morphology Generation Models to
Machine Translation. In Proceedings of ACL-08: HLT, pages 514?522, Columbus, Ohio, June. Association for
Computational Linguistics.
Joern Wuebker and Hermann Ney. 2012. Phrase Model Training for Statistical Machine Translation with Word
Lattices of Preprocessing Alternatives. In NAACL 2012 Seventh Workshop on Statistical Machine Translation,
pages 450?459, Montreal, Canada, June. Association for Computational Linguistics.
Joern Wuebker, Stephan Peitz, Felix Rietig, and Hermann Ney. 2013. Improving Statistical Machine Translation
with Word Class Models. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language
Processing, pages 1377?1381, Seattle, Washington, USA, October. Association for Computational Linguistics.
Fei Xia and Michael McCord. 2004. Improving a Statistical MT System with Automatically Learned Rewrite
Patterns. In Proceedings of Coling 2004, pages 508?514, Geneva, Switzerland, Aug 23?Aug 27. COLING.
Reyyan Yeniterzi and Kemal Oflazer. 2010. Syntax-to-Morphology Mapping in Factored Phrase-Based Statistical
Machine Translation from English to Turkish. In Proceedings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 454?464, Uppsala, Sweden, July. Association for Computational Linguistics.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax Augmented Machine Translation via Chart Parsing.
In Proceedings on the Workshop on Statistical Machine Translation, pages 138?141, New York City, June.
Association for Computational Linguistics.
432
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1038?1047, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Forest Reranking through Subtree Ranking
Richa?rd Farkas, Helmut Schmid
Institute for Natural Language Processing
University of Stuttgart
{farkas,schmid}@ims.uni-stuttgart.de
Abstract
We propose the subtree ranking approach to
parse forest reranking which is a general-
ization of current perceptron-based reranking
methods. For the training of the reranker,
we extract competing local subtrees, hence
the training instances (candidate subtree sets)
are very similar to those used during beam-
search parsing. This leads to better param-
eter optimization. Another chief advantage
of the framework is that arbitrary learning to
rank methods can be applied. We evaluated
our reranking approach on German and En-
glish phrase structure parsing tasks and com-
pared it to various state-of-the-art reranking
approaches such as the perceptron-based for-
est reranker. The subtree ranking approach
with a Maximum Entropy model significantly
outperformed the other approaches.
1 Introduction
Reranking has become a popular technique for
solving various structured prediction tasks, such
as phrase-structure (Collins, 2000) and depen-
dency parsing (Hall, 2007), semantic role labeling
(Toutanova et al 2008) and machine translation
(Shen et al 2004). The idea is to (re)rank candi-
dates extracted by a base system exploiting a rich
feature set and operating at a global (usually sen-
tence) level. Reranking achieved significant gains
over the base system in many tasks because it has
access to information/features which are not com-
putable in the base system. Reranking also outper-
forms discriminative approaches which try to han-
dle the entire candidate universe (cf. Turian et al
(2006)) because the base system effectively and ef-
ficiently filters out many bad candidates and makes
the problem tractable.
The standard approach for reranking is the n-best
list ranking procedure, where the base system ex-
tracts its top n global-level candidates with associ-
ated goodness scores that define an initial ranking.
Then the task is to rerank these candidates by us-
ing a rich feature set. The bottleneck of this ap-
proach is the small number of candidates consid-
ered. Compared to n-best lists, packed parse forests
encode more candidates in a compact way. For-
est reranking methods have been proposed, which
can exploit the richer set of candidates and they
have been successfully applied for phrase-structure
(Huang, 2008), dependency (Hayashi et al 2011)
parsing and machine translation (Li and Khudanpur,
2009) as well.
Huang (2008) introduced the perceptron-based
forest reranking approach. The core of the algo-
rithm is a beam-search based decoder operating on
the packed forest in a bottom-up manner. It follows
the assumption that the feature values of the whole
structure are the sum of the feature values of the lo-
cal elements and they are designed to the usage of
the perceptron update. Under these assumptions a
1-best Viterbi or beam-search decoder can be effi-
ciently employed at parsing and training time. Dur-
ing training, it decodes the 1-best complete parse
then it makes the perceptron update against the or-
acle parse, i.e. the perceptron is trained at the global
(sentence) level.
We propose here a subtree ranker approach
which can be regarded as a generalization of this for-
1038
est reranking procedure. In contrast to updating on
a single (sub)tree per sentence using only the 1-best
parse (perceptron-based forest reranking), the sub-
tree ranker exploits subtrees of all sizes from a sen-
tence and trains a (re)ranker utilising several deriva-
tions of the constituent in question. During parsing
we conduct a beam-search extraction by asking the
ranker to select the k best subtrees among the pos-
sible candidates of each forest node. The chief mo-
tivation for this approach is that in this way, train-
ing and prediction are carried out on similar local
candidate lists which we expect to be favorable to
the learning mechanism. We empirically prove that
the trained discriminative rankers benefit from hav-
ing access to a larger amount of subtree candidates.
Moreover, in this framework any kind of learning
to rank methods can be chosen as ranker, including
pair-wise and list-wise classifiers (Li, 2011).
The contributions of this paper are the following:
? We extend the perceptron-based forest
rerankers to the subtree ranker forest reranking
framework which allows to replace the per-
ceptron update by any kind of learning to rank
procedure.
? We report experimental results on German
and English phrase-structure parsing compar-
ing subtree rerankers to various other rerankers
showing a significant improvement over the
perceptron-based forest reranker approach.
2 Related Work
Our method is closely related to the work of Huang
(2008), who introduced forest-based reranking for
phrase structure parsing. The proposed frame-
work can be regarded as an extension of this ap-
proach. It has several advantages compared with
the perceptron-based forest reranker. In this paper
we focus on the most important one ? and briefly
discuss two others in Section 5 ? which is enabling
the use of any kind of learning to rank approaches.
While the perceptron is fast to train, other machine
learning approaches usually outperform it. Most of
the existing learning to rank approaches are built on
linear models and evaluate the candidates indepen-
dently of each other (such as MaxEnt (Charniak and
Johnson, 2005), SVMRank (Joachims, 2002), Soft-
Rank (Guiver and Snelson, 2008)). Thus the choice
of the learning method does not influence parsing
time. We believe that the real bottleneck of parsing
applications is parsing time and not training time.
On the other hand, they can learn a better model
(at the cost of higher training time) than the Per-
ceptron. In theory, we can imagine learning to rank
approaches which can not be reduced to the indi-
vidual scoring of candidates at prediction time, for
instance a decision tree-based pairwise ranker. Al-
though such methods would also fit into the general
subtree framework, they are not employed in prac-
tice (Li, 2011).
The subtree ranking approach is a generalization
of the perceptron-based approach. If the ranking
algorithm is the Averaged Perceptron, the parsing
algorithm reduces to perceptron-based forest pars-
ing. If the ?selection strategy? utilizes the base sys-
tem ranking and training starts with a filtering step
which keeps only candidate sets from the root node
of the forest we get the offline version of the training
procedure of the perceptron-based forest reranker of
Huang (2008).
As our approach is based on local ranking (local
update in the online learning literature), it is highly
related to early update which looks for the first lo-
cal decision point where the oracle parse falls out
from the beam. Early update was introduced by
Collins and Roark (2004) for incremental parsing
and adopted to forest reranking by Wang and Zong
(2011).
Besides phrase structure parsing, the forest
reranking approach was successfully applied for de-
pendency parsing as well. Hayashi et al(2011) in-
troduced a procedure where the interpolation of a
generative and a forest-based discriminative parser
is exploited.
From the algorithmic point of view, our approach
is probably most closely related to Searn (Daume?
et al 2009) and Magerman (1995) as we also em-
ploy a particular machine learned model for a se-
quence of local decisions. The topological order of
the parse forest nodes can form the ?sequence of
choices? of Searn. The biggest differences between
our approach and Searn are that we propose an ap-
proach employing beam search and the ?policy? is a
ranker in our framework instead of a multiclass clas-
sifier as there are no ?actions? here, instead we have
to choose from candidate sets in the forest reranking
1039
framework. In a wider sense, our approach can be
regarded ? like Searn ? as an Inverse Reinforcement
Learning approach where ?one is given an environ-
ment and a set of trajectories and the problem is to
find a reward function such that an agent acting opti-
mally with respect to the reward function would fol-
low trajectories that match those in the training set?
(Neu and Szepesva?ri, 2009). Neu and Szepesva?ri
(2009) introduced the top-down parsing Markov De-
cision Processes and experiment with several inverse
reinforcement learning methods. The forest rerank-
ing approaches are bottom-up parsers which would
require a new (non-straightforward) definition of a
corresponding Markov Decision Process.
3 Subtree Ranking-based Forest
Reranking
A packed parse forest is a compact representation
of possible parses for a given sentence. A forest has
the structure of a hypergraph, whose nodes V are the
elementary units of the underlying structured predic-
tion problem and the hyperedges E are the possible
deductive steps from the nodes. In this paper we
experimented with phrase-structure parse reranking.
In this framework nodes correspond to constituents
spanning a certain scope of the input sentence and a
hyperedge e links a parent node head(e) to its chil-
dren tails(e) (i.e. a hyperedge is a CFG rule in con-
text).
The forest is extracted from the chart of a base
PCFG parser, usually employing a heavy pruning
strategy. Then the goal of a forest reranker is to find
the best parse of the input sentence exploiting a fea-
ture representation of (sub)trees.
We sketch the parsing procedure of the subtree
ranker in Algorithm 1. It is a bottom-up beam-
search parser operating on the hypergraph. At each
node v we store the k best subtrees S(v) headed by
the node. The S(v) lists contain the k top-ranked
subtrees by the ranker R among the candidates in the
beam. The set of candidate subtrees at a node is the
union of the candidates at the different hyperedges.
The set of candidate subtrees at a certain hyperedge,
in turn, is formed by the Cartesian product ?S(vi)
of the k-best subtrees stored at the child nodes vi.
The final output of forest ranking is the 1-best sub-
tree headed by the goal node S1(vgoal).
Algorithm 1 Subtree Ranking
Require: ?V,E? forest, R ranker
for all v ? V in bottom-up topological order do
C ? ?
for all e ? E, head(e) = v do
C ? C ? (?S(vi)) , vi ? tails(e)
end for
S(v)? Rk(C)
end for
return S1(vgoal)
For training the ranker we propose to extract lo-
cal candidate lists from the forests which share the
characteristics of the candidates at parsing time. Al-
gorithm 2 depicts the training procedure of the sub-
tree ranker.
As forests sometimes do not contain the gold stan-
dard tree, we extract an oracle tree instead, which
is the closest derivable tree in the forest to the gold
standard tree (Collins, 2000). Then we optimize the
parser for ranking the oracle tree at the top. This pro-
cedure is beneficial to training since the objective is
a reachable state. In Algorithm 2, we extract the ora-
cle tree from the parses encoded in the forest ?V,E?i
for the ith training sentence, which is the tree with
the highest F-score when compared to the gold stan-
dard tree yi. For each of the training sentences we
calculate the oracle subtrees for each node {Ov} of
the corresponding parse forest. We follow the dy-
namic programming approach of Huang (2008) for
the extraction of the forest oracle. The goal of this
algorithm is to extract the full oracle tree, but as a
side product it calculates the best possible subtree
for all nodes including the nodes outside of the full
oracle tree as well.
After computing the oracle subtrees, we crawl
the forests bottom-up and extract a training instance
?C,Ov? at each node v which consists of the candi-
date set C and the oracle Ov at that node. The cre-
ation of candidate lists is exactly the same as it was
at parsing time. Then we create training instances
from each of the candidate lists and form the set of
subtrees S(v) which is stored for candidate extrac-
tion at the higher levels of the forest (later steps in
the training instance extraction).
A crucial design question is how to form the S(v)
sets during training, which is the task of the selection
1040
Algorithm 2 Subtree Ranker Training
Require: {?V,E?i, yi}N1 , SS selection strategy
T ? ?
for all i? 1...N do
O ? oracle extractor(?V,E?i, yi)
for all v ? Vi in bottom-up topological order
do
C ? ?
for all e ? E, head(e) = v do
C ? C ? (?S(vj)) , vj ? tails(e)
end for
T ? T ? ?C,Ov?
S(v)? SS(C,Ov)
end for
end for
R? train reranker(T )
return R
strategy SS. One possible solution is to keep the k
best oracle subtrees, i.e. the k subtrees closest to the
gold standard parse, which is analogous to using the
gold standard labels in Maximum Entropy Markov
Models for sequence labeling problems (we refer
this selection strategy as ?oracle subtree? later on).
The problem with this solution is that if the rankers
have been trained on the oracle subtrees potentially
leads to a suboptimal performance as the outputs of
the ranker at prediction time are noisy. Note that
this approach is not a classical beam-based decod-
ing anymore as the ?beam? is maintained according
to the oracle parses and there is no model which in-
fluences that. An alternative solution ? beam-based
decoding ? is to use a ranker model to extract the
S(v) set in training time as well. In the general
reranking approach, we assume that the ranking of
the base parser is reliable. So we store the k best
subtrees according to the base system in S(v) (the
?base system ranking? selection strategy). Note that
the general framework keeps this question open and
lets the implementations define a selection strategy
SS.
After extracting the training instances T we can
train an arbitrary ranker R offline. Note that the
extraction of candidate lists is exactly the same in
Algorithm 1 and 2 while the creation of Sv can be
different.
4 Experiments
We carried out experiments on English and German
phrase-structure reranking. As evaluation metric, we
used the standard evalb implementation of PAR-
SEVAL on every sentence without length limitation
and we start from raw sentences without gold stan-
dard POS tagging. As the grammatical functions of
constituents are important from a downstream ap-
plication point of view ? especially in German ? we
also report PARSEVAL scores on the conflation of
constituent labels and grammatical functions. These
scores are shown in brackets in Table 2.
4.1 Datasets
We used the Wall Street Journal subcorpus of the
Ontonotes v4.0 corpus (Weischedel et al 2011)1 for
English. As usual sections 2-21, 23 and 24 served as
training set (30,060 sentences), test set (1,640 sen-
tences), and development set (1,336 sentences), re-
spectively. Using the Ontonotes version enables us
to assess parser robustness. To this end, we eval-
uated our models also on the weblog subcorpus of
the Ontonotes v4.0 corpus which consists of 15,103
sentences.
For German we used the Tiger treebank (Brants
et al 2002). We take the first 40,474 sentences of
the Tiger treebank as training data, the next 5,000
sentences as development data, and the last 5,000
sentences as test data.
4.2 Implementation of the Generic Framework
We investigate the Averaged Perceptron and a Maxi-
mum Entropy ranker as the reranker R in the subtree
ranking framework. The Maximum Entropy ranker
model is optimized with a loss function which is
the negative log conditional likelihood of the ora-
cle trees relative to the candidate sets. In the case of
multiple oracles we optimize for the sum of the ora-
cle trees? posterior probabilities (Charniak and John-
son, 2005).
In our setup the parsing algorithm is identical
to the perceptron-based forest reranker of Huang
(2008) because both the Averaged Perceptron and
the Maximum Entropy rankers score the local sub-
tree candidates independently of each other using
1Note that it contains less sentences and a slightly modified
annotation schema than the Penn Treebank.
1041
a linear model. There is no need to compute the
global normalization constant of the Maximum En-
tropy model because we only need the ranking and
not the probabilities. Hence the difference is in how
to train the ranker model.
We experimented with both the ?oracle subtree?
and the ?base system ranking? selection strategies
(see Section 3).
4.3 Five Methods for Forest-based Reranking
We conducted comparative experiments employing
the proposed subtree ranking approach and state-of-
the-art methods for forest reranking. Note that they
are equivalent in parsing time as each of them uses
beam-search with a linear classifier, on the other
hand they are radically different in their training.
? The original perceptron-based forest reranker
of Huang (2008) (?perceptron with global train-
ing?).
? The same method employing the early-update
updating mechanism instead of the global up-
date. Wang and Zong (2011) reported a signif-
icant gain using this update over the standard
global update (?perceptron with early update?).
? Similar to learning a perceptron at the global
level and then applying it at local decisions,
we can train a Maximum Entropy ranker at the
global level utilizing the n-best full parse can-
didates of the base parser, then use this model
for local decision making. So we train the
standard n-best rerankers (Charniak and John-
son, 2005) and then apply them in the beam-
search-based Viterbi parser (?n-best list train-
ing?). Applying the feature weights adjusted in
this approach in the forest-based decoding out-
performs the standard n-best list decoding by
an F-score of 0.3 on the German dataset.
? The subtree ranker method using the Averaged
Perceptron reranker. This is different from the
?perceptron with global training? as we conduct
updates at every local decision point and we do
offline training (?subtree ranking by AvgPer?).
? The subtree ranker method using Maximum
Entropy training (?subtree ranking by Max-
Ent?).
We (re)implemented these methods and used the
same forests and the same feature sets for the com-
parative experiments.
4.4 Implementation Details
We used the first-stage PCFG parser of Charniak
and Johnson (2005) for English and BitPar (Schmid,
2004) for German. BitPar employs a grammar engi-
neered for German (for details please refer to Farkas
et al(2011)). These two parsers are state-of-the-art
PCFG parsers for English and German, respectively.
For German the base parser and the reranker oper-
ate on the conflation of constituent labels and gram-
matical functions. For English, we used the forest
extraction and pruning code of Huang (2008). The
pruning removes hyperedges where the difference
between the cost of the best derivation using this hy-
peredge and the cost of the globally best derivation
is above some threshold. For German, we used the
pruned parse forest of Bitpar (Schmid, 2004). Af-
ter computing the posterior probability of each hy-
peredge given the input sentence, Bitpar prunes the
parse forest by deleting hyperedges whose posterior
probability is below some threshold. (We used the
threshold 0.01).
We employed an Averaged Perceptron (for ?per-
ceptron with global training?, ?perceptron with early
update? and ?subtree ranking by AvgPer?) and a
Maximum Entropy reranker (for ?subtree ranking
by MaxEnt? and ?n-best list training?). For the per-
ceptron reranker, we used the Joshua implementa-
tion2. The optimal number of iterations was deter-
mined on the development set. For the Maximum
Entropy reranker we used the RankMaxEnt imple-
mentation of the Mallet package (McCallum, 2002)
modified to use the objective function of Charniak
and Johnson (2005) and we optimized the L2 regu-
larizer coefficient on the development set.
The beam-size were set to 15 (the value suggested
by Huang (2008)) during parsing and the training
of the ?perceptron with global training? and ?percep-
tron with early update? models. We used k = 3 for
training the ?subtree ranking by AvgPer? and ?sub-
tree ranking by MaxEnt? rankers (see Section 5 for
a discussion on this).
In the English experiments, we followed (Huang,
2http://joshua.sourceforge.net/Joshua/
1042
Tiger test WSJ dev WSJ test WB
base system (1-best) 76.84 (65.91) 89.29 88.63 81.86
oracle tree 90.66 (80.38) 97.31 97.30 94.18
Table 1: The lower and upper bounds for rerankers on the four evaluation datasets. The numbers in brackets refers to
evaluation with grammatical function labels on the German dataset.
Tiger test WSJ dev WSJ test WB
perceptron with global training 78.39 (67.79) 90.58 89.60 82.87
perceptron with early update 78.83 (68.05) 90.81? 90.01 83.03?
n-best list training 78.75 (68.04) 90.89 90.11 83.55
subtree ranking by AvgPer 78.54? (67.97?) 90.65? 89.97 83.04?
subtree ranking by MaxEnt 79.36 (68.72) 91.14 90.32 83.83
Table 2: The results achieved by various forest rerankers. The difference between the scores marked by ? and the
?perceptron with global training? were not statistically significant with p < 0.005 according to the the McNemar test.
All other results are statistically different from this baseline.
2008) and selectively re-implemented feature tem-
plates from (Collins, 2000) and Charniak and John-
son (2005). For German we re-implemented the
feature templates of Versley and Rehbein (2009)
which is the state-of-the-art feature set for German.
It consists of features constructed from the lexical-
ized parse tree and its typed dependencies along
with features based on external statistical informa-
tion (such as the clustering of unknown words ac-
cording to their context of occurrence and PP attach-
ment statistics gathered from the automatically POS
tagged DE-WaC corpus, a 1.7G words sample of the
German-language WWW). We filtered out rare fea-
tures which occurred in less than 10 forests (we used
the same non-tuned threshold for the English and
German training sets as well).
We also re-implemented the oracle extraction pro-
cedure of Huang (2008) and extended its convolu-
tion and translation operators for using the base sys-
tem score as tie breaker.
4.5 Results
Table 1 shows the results of the 1-best parse of the
base system and the oracle scores ? i.e. the lower
and upper bounds for the rerankers ? for the four
evaluation datasets used in our experiments. The
German and the weblog datasets are more difficult
for the parsers.
The following table summarizes the characteris-
tics of the subtree ranker?s training sample of the
German and English datasets by employing the ?or-
acle subtree? selection strategy:
Tiger train WSJ train
#candidate lists 266,808 1,431,058
avg. size of cand. lists 3.2 5.7
#features before filtering 2,683,552 22,164,931
#features after filtering 94,164 858,610
Table 3: The sizes of the subtree ranker training datasets
at k = 3.
Using this selection strategy the training dataset
is smaller than the training dataset of the n-best list
rankers ? where offline trainers are employed as well
? as the total number of candidates is similar (and
even less in the Tiger corpus) while there are fewer
firing features at the subtrees than at full trees.
Table 2 summarizes the results achieved by vari-
ous forest rerankers. Both subtree rankers used the
oracle subtrees as the selection strategy of Algo-
rithm 2. The ?subtree ranking by MaxEnt? method
significantly outperformed the perceptron-based for-
est reranking algorithms at each of the datasets and
seems to be more robust as its advantage on the out-
domain data ?WB? is higher compared with the in-
domain ?WSJ? datasets. The early update improves
the perceptron based forest rerankers which is in line
with the results reported by Wang and Zong (2011).
The ?n-best list training? method works surprisingly
well. It outperforms both perceptron-based forest
1043
rerankers on the English datasets (while achieving a
smaller F-score than the perceptron with early up-
date on the Tiger corpus) which demonstrates the
potential of utilizing larger candidate lists for dis-
criminative training of rerankers. The comparison of
the ?subtree ranking by AvgPer? row and the ?subtree
ranking by MaxEnt? row shows a clear advantage of
the Maximum Entropy training mechanism over the
Averaged Perceptron.
Besides the ?oracle subtree? selection strategy we
also experimented with the ?base system ranking?
selection strategy with subtree Maximum Entropy
ranker. Table 4 compares the accuracies of the two
strategies. The difference between the two strate-
gies varies among datasets. In the German dataset,
they are competitive and the prediction of grammati-
cal functions benefits from the ?base system ranking?
strategy, while it performs considerably worse at the
English datasets.
Tiger test WSJ test WB
oracle SS 79.36 (68.72) 90.32 83.83
base sys SS 79.34 (68.84) 89.97 83.34
Table 4: The results of the two selection strategies. Using
the oracle trees proved to be better on each of the datasets.
Extracting candidate lists from each of the local
decision points might seem to be redundant. To gain
some insight into this question, we investigated the
effect of training instance filtering strategies on the
Tiger treebank. We removed the training instances
from the training sample T where the F-score of
the oracle (sub)tree against the gold standard tree is
less than a certain threshold (this data selection pro-
cedure was inspired by Li and Khudanpur (2008)).
The idea behind this data selection is to eliminate
bad training examples which might push the learner
into the wrong direction. Figure 1 depicts the results
on the Tiger treebank as a function of this data se-
lection threshold.
With this data selection strategy we could further
gain 0.22 F-score percentage points achieving 79.58
(68.87) and we can conclude that omitting candidate
sets far from the gold-standard tree helps training.
Figure 1 also shows that too strict filtering hurts the
performance. The result with threshold=90 is worse
than the result without filtering. We should note
that similar data selection methods can be applied
0 20 40 60 8079.2
5
79.3
5
79.4
5
79.5
5
data filtering threshold (F?score)
PAR
SEVA
L F?s
core
Figure 1: The effect of data selection on the Tiger test set.
to each of the baseline systems and the comparison
to them would be fair with conducting that. Thus
we consider our results without data selection to be
final.
5 Discussion
We experimentally showed in the previous section
that the subtree forest reranking approach with Max-
imum Entropy models significantly outperforms the
perceptron-based forest reranking approach. This
improvement must be the result of differences in the
training algorithms because there is no difference
between the two approaches at parse time, as we dis-
cussed in Section 4.2.
There are two sources of these improvements.
(i) We use local subtrees as training instances in-
stead of using the global parses exclusively. The
most important difference between the training of
the perceptron-based forest reranker and the subtree
forest reranker is that we train on subtrees (extract
candidate sets) outside of the Viterbi parses as well,
i.e. our intuition is that the training of the discrimi-
native model can benefit from seeing good and bad
subtrees far from the best parses as well. (ii) The
subtree ranker framework enables us to employ the
Maximum Entropy ranker on multiple candidates,
which usually outperforms the Averaged Perceptron.
The results of Table 2 can be considered as two
paths from the ?perceptron with global training?
to the ?subtree ranking by MaxEnt? applying these
1044
sources of improvements. If we use (i) and stay with
the Averaged Perceptron as learning algorithm we
get ?subtree ranking by AvgPer?. If we additionally
replace the Averaged Perceptron by Maximum En-
tropy ? i.e. follow (ii) ? we arrive at ?subtree ranking
by MaxEnt?. On the other hand, the ?n-best training?
uses global trees and Maximum Entropy for train-
ing, so the reason of the difference between ?per-
ceptron with global training? and ?n-best training? is
(ii). Then we arrive at ?subtree ranking by MaxEnt?
by (i). This line of thoughts and the figures of Ta-
ble 2 indicate that the added value of (i) and (ii) are
similar in magnitude.
5.1 Error Analysis
For understanding the added value of the proposed
subtree ranking method, we manually investigated
sentences from the German development set and
compared the parses of the ?perceptron with global
training? with the ?subtree ranking by MaxEnt?. We
could not found any linguistic phenomena which
was handled clearly better by the subtree ranker3,
but it made considerably more fixes than errors in
the following cases:
? the attachment of adverbs,
? the unary branching verbal phrases and
? extremely short sentences which does not con-
tain any verb (fragments).
5.2 Novel Opportunities with the Subtree
Ranking Framework
A generalization issue of the subtree ranking ap-
proach is that it allows to use any kind of feature
representation and arbitrary aggregation of local
features. The basic assumption of training on the
global (sentence) level in the perceptron reranking
framework is that the feature vector of a subtree is
the sum of the feature vectors of the children and
the features extracted from the root of the subtree
in question. This decomposability assumption pro-
vides a fine framework in the case of binary features
which fire if a certain linguistic phenomenon occurs.
On the other hand, this is not straightforward in the
3We believe that this might be the case only if we would
introduce new information (e.g. features) for the system.
presence of real valued features. For example, Ver-
sley and Rehbein (2009) introduce real-valued fea-
tures for supporting German PP-attachment recogni-
tion ? the mutual information of noun and preposi-
tion co-occurrence estimated from a huge unlabeled
corpus ? and this single feature template (about 80
features) could achieve a gain of 1 point in phrase
structure parsing accuracy while the same improve-
ment can be achieved by several feature templates
and millions of binary features. The aggregation of
such feature values can be different from summing,
for instance the semantics of the feature can demand
averaging, minimum, maximum or introducing new
features etc. Another opportunity for extending cur-
rent approaches is to employ utility functions on top
of the sum of the binary feature values. Each of these
extensions fits into the proposed framework.
The subtree ranking framework also enables the
usage of different models at different kinds of
nodes. For example, different models can be trained
for ranking subtress headed by noun phrases and for
verb phrases. This is not feasible in the perceptron-
based forest ranker which sums up features and up-
dates feature weights at the sentence level while the
ranker R in Algorithm 2 can refer to several models
because we handle local decisions separately. This
approach would not hurt parsing speed as one par-
ticular model is asked at each node, but it multiplies
memory requirements. This is an approach which
the subtree ranking framework allows, but which
would not fit to the global level updates of the per-
ceptron forest rerankers.
As a first step in this direction of research we ex-
perimented with training three different Maximum
Entropy models using the same feature representa-
tion, the first only on candidate lists extracted from
noun phrase nodes, the second on verb phrase nodes
and the third on all nodes (i.e. the third model is
equivalent to the ?subtree MaxEnt? model). Then at
prediction time, we ask that model (out of the three)
which is responsible for ranking the candidates of
the current type of node. This approach performed
worse than the single model approach achieving an
F-scores of 79.24 (68.46) on the Tiger test dataset.
This negative results ? compared with 79.36 (68.72)
achieved by a single model ? is probably due to data
sparsity problems. The amount of training samples
for noun phrases is 6% of the full training sample
1045
and it seems that a better model can be learned from
a much bigger but more heterogeneous dataset.
5.3 On the Efficiency of Subtree Ranking
In subtree ranking, we extract a larger number
of training instances (candidate lists) than the
perceptron-based approach which extracts exactly
one instance from a sentence. Moreover, the can-
didate lists are longer than the perceptron-based ap-
proach (where 2 ?candidates? are compared against
each other). Training on this larger set (refer Table 3
for concrete figures) consumes more space and time.
In our implementation, we keep the whole train-
ing dataset in the memory. With this implementation
the whole training process (feature extraction, can-
didate extraction and training the Maximum Entropy
ranker) takes 3 hours and uses 10GB of memory at
k = 1 and it takes 20 hours and uses 60GB of mem-
ory at k = 3 ((Huang, 2008) reported 5.3 and 27.3
hours at beam-sizes of 1 and 15 respectively but it
used only 1.2GB of memory). The in-depth investi-
gation of the effect of k is among our future plans.
6 Conclusions
We presented a subtree ranking approach to parse
forest reranking, which is a generalization of current
reranking methods. The main advantages of our ap-
proach are: (i) The candidate lists used during train-
ing are very similar to those used during parsing,
which leads to better parameter optimization. (ii)
Arbitrary ranking methods can be applied in our ap-
proach. (iii) The reranking models need not to be
decomposable.
We evaluated our parse reranking approach on
German and English phrase structure parsing tasks
and compared it to various state-of-the-art rerank-
ing approaches such as the perceptron-based for-
est reranker (Huang, 2008). The subtree reranking
approach with a Maximum Entropy model signifi-
cantly outperformed the other approaches.
We conjecture two reasons for this result: (i) By
training on all subtrees instead of Viterbi parses or
n-best parses only, we use the available training
data more effectively. (ii) The subtree ranker frame-
work allows us to use a standard Maximum Entropy
learner in parse-forest training instead of the Percep-
tron, which is usually superior.
Acknowledgements
We thank Liang Huang to provide us the modi-
fied version of the Charniak parser, which output a
packed forest for each sentence along with his forest
pruning code.
This work was founded by the Deutsche
Forschungsgemeinschaft grant SFB 732, project D4.
1046
References
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER tree-
bank. In Proceedings of the Workshop on Treebanks
and Linguistic Theories, pages 24?41.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL ?05,
pages 173?180.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceedings
of the 42nd Meeting of the Association for Computa-
tional Linguistics (ACL?04), Main Volume, pages 111?
118.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Proceedings of the Seven-
teenth International Conference onMachine Learning,
ICML ?00, pages 175?182.
Hal Daume?, III, John Langford, and Daniel Marcu. 2009.
Search-based structured prediction. Machine Learn-
ing, 75(3):297?325, June.
Richa`rd Farkas, Bernd Bohnet, and Helmut Schmid.
2011. Features for phrase-structure reranking from
dependency parses. In Proceedings of the 12th Inter-
national Conference on Parsing Technologies, pages
209?214.
John Guiver and Edward Snelson. 2008. Learning to
rank with softrank and gaussian processes. In Pro-
ceedings of the 31st annual international ACM SIGIR
conference on Research and development in informa-
tion retrieval, SIGIR ?08, pages 259?266.
Keith Hall. 2007. K-best spanning tree parsing. In Pro-
ceedings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 392?399, June.
Katsuhiko Hayashi, Taro Watanabe, Masayuki Asahara,
and Yuji Matsumoto. 2011. Third-order varia-
tional reranking on packed-shared dependency forests.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1479?1488.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL-08: HLT, pages 586?594.
T. Joachims. 2002. Optimizing search engines using
clickthrough data. In ACM SIGKDD Conference on
Knowledge Discovery and Data Mining (KDD), pages
133?142.
Zhifei Li and Sanjeev Khudanpur. 2008. Large-scale
discriminative n-gram language models for statistical
machine translation. In Proceedings of the 8th AMTA
conference, pages 133?142.
Z. Li and S. Khudanpur, 2009. GALE book chapter on
?MT From Text?, chapter Forest reranking for machine
translation with the perceptron algorithm.
Hang Li. 2011. Learning to Rank for Information Re-
trieval and Natural Language Processing. Synthesis
Lectures on Human Language Technologies. Morgan
& Claypool Publishers.
David M. Magerman. 1995. Statistical decision-tree
models for parsing. In Proceedings of the 33rd Annual
Meeting of the Association for Computational Linguis-
tics, pages 276?283, June.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Gergely Neu and Csaba Szepesva?ri. 2009. Training
parsers by inverse reinforcement learning. Machine
Learning, 77(2?3):303?337.
Helmut Schmid. 2004. Efficient parsing of highly am-
biguous context-free grammars with bit vectors. In
Proceedings of Coling 2004, pages 162?168.
Libin Shen, Anoop Sarkar, and Franz Josef Och. 2004.
Discriminative reranking for machine translation. In
Daniel Marcu Susan Dumais and Salim Roukos, ed-
itors, HLT-NAACL 2004: Main Proceedings, pages
177?184.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A global joint model for semantic
role labeling. Computational Linguistics, 34(2):161?
191.
Joseph P. Turian, Benjamin Wellington, and I. Dan
Melamed. 2006. Scalable discriminative learning for
natural language parsing and translation. In NIPS,
pages 1409?1416.
Yannick Versley and Ines Rehbein. 2009. Scalable dis-
criminative parsing for german. In Proceedings of the
11th International Conference on Parsing Technolo-
gies (IWPT?09), pages 134?137.
Zhiguo Wang and Chengqing Zong. 2011. Parse rerank-
ing based on higher-order lexical dependencies. In
Proceedings of 5th International Joint Conference on
Natural Language Processing, pages 1251?1259.
Ralph Weischedel, Eduard Hovy, Martha Palmer, Mitch
Marcus, Robert Belvin, Sameer Pradhan, Lance
Ramshaw, and Nianwen Xue, 2011. Handbook of Nat-
ural Language Processing and Machine Translation.,
chapter OntoNotes: A Large Training Corpus for En-
hanced Processing.
1047
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 322?332,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Efficient Higher-Order CRFs for Morphological Tagging
Thomas Mu?ller??, Helmut Schmid??, and Hinrich Schu?tze?
?Center for Information and Language Processing, University of Munich, Germany
?Institute for Natural Language Processing , University of Stuttgart, Germany
muellets@cis.lmu.de
Abstract
Training higher-order conditional random
fields is prohibitive for huge tag sets. We
present an approximated conditional random
field using coarse-to-fine decoding and early
updating. We show that our implementation
yields fast and accurate morphological taggers
across six languages with different morpho-
logical properties and that across languages
higher-order models give significant improve-
ments over 1st-order models.
1 Introduction
Conditional Random Fields (CRFs) (Lafferty et al,
2001) are arguably one of the best performing se-
quence prediction models for many Natural Lan-
guage Processing (NLP) tasks. During CRF train-
ing forward-backward computations, a form of dy-
namic programming, dominate the asymptotic run-
time. The training and also decoding times thus
depend polynomially on the size of the tagset and
exponentially on the order of the CRF. This prob-
ably explains why CRFs, despite their outstanding
accuracy, normally only are applied to tasks with
small tagsets such as Named Entity Recognition and
Chunking; if they are applied to tasks with bigger
tagsets ? e.g., to part-of-speech (POS) tagging for
English ? then they generally are used as 1st-order
models.
In this paper, we demonstrate that fast and accu-
rate CRF training and tagging is possible for large
tagsets of even thousands of tags by approximat-
ing the CRF objective function using coarse-to-fine
decoding (Charniak and Johnson, 2005; Rush and
Petrov, 2012). Our pruned CRF (PCRF) model has
much smaller runtime than higher-order CRF mod-
els and may thus lead to an even broader application
of CRFs across NLP tagging tasks.
We use POS tagging and combined POS and
morphological (POS+MORPH) tagging to demon-
strate the properties and benefits of our approach.
POS+MORPH disambiguation is an important pre-
processing step for syntactic parsing. It is
usually tackled by applying sequence prediction.
POS+MORPH tagging is also a good example of a
task where CRFs are rarely applied as the tagsets are
often so big that even 1st-order dynamic program-
ming is too expensive. A workaround is to restrict
the possible tag candidates per position by using ei-
ther morphological analyzers (MAs), dictionaries or
heuristics (Hajic?, 2000). In this paper, however we
show that when using pruning (i.e., PCRFs), CRFs
can be trained in reasonable time, which makes hard
constraints unnecessary.
In this paper, we run successful experiments on
six languages with different morphological prop-
erties; we interpret this as evidence that our ap-
proach is a general solution to the problem of
POS+MORPH tagging. The tagsets in our experi-
ments range from small sizes of 12 to large sizes of
up to 1811. We will see that even for the smallest
tagset, PCRFs need only 40% of the training time of
standard CRFs. For the bigger tagset sizes we can
reduce training times from several days to several
hours. We will also show that training higher-order
PCRF models takes only several minutes longer than
training 1st-order models and ? depending on the
language ? may lead to substantial accuracy im-
322
Language Sentences Tokens POS MORPH POS+MORPH OOV
Tags Tags Tags Rate
ar (Arabic) 15,760 614,050 38 516 516 4.58%
cs (Czech) 38,727 652,544 12 1,811 1,811 8.58%
en (English) 38,219 912,344 45 45 3.34%
es (Spanish) 14,329 427,442 12 264 303 6.47%
de (German) 40,472 719,530 54 255 681 7.64%
hu (Hungarian) 61,034 1,116,722 57 1,028 1,071 10.71%
Table 1: Training set statistics. Out-Of-Vocabulary (OOV) rate is regarding the development sets.
provements. For example in German POS+MORPH
tagging, a 1st-order model (trained in 32 minutes)
achieves an accuracy of 88.96 while a 3rd-order
model (trained in 35 minutes) achieves an accuracy
of 90.60.
The remainder of the paper is structured as fol-
lows: Section 2 describes our CRF implementa-
tion1 and the feature set used. Section 3 sum-
marizes related work on tagging with CRFs, effi-
cient CRF tagging and coarse-to-fine decoding. Sec-
tion 4 describes experiments on POS tagging and
POS+MORPH tagging and Section 5 summarizes
the main contributions of the paper.
2 Methodology
2.1 Standard CRF Training
In a standard CRF we model our sentences using a
globally normalized log-linear model. The proba-
bility of a tag sequence ~y given a sentence ~x is then
given as:
p(~y|~x) =
exp
?
t,i ?i ? ?i(~y, ~x, t)
Z(~?, ~x)
Z(~?, ~x) =
?
~y
exp
?
t,i
?i ? ?i(~y, ~x, t)
Where t and i are token and feature indexes, ?i is
a feature function, ?i is a feature weight and Z is a
normalization constant. During training the feature
weights ? are set to maximize the conditional log-
likelihood of the training data D:
1Our java implementation MarMoT is available at
https://code.google.com/p/cistern/
llD(~?) =
?
(~x,~y)?D
log p(~y|~x,~?)
In order to use numerical optimization we have to
calculate the gradient of the log-likelihood, which is
a vector of partial derivatives ?llD(~?)/??i. For a
training sentence ~x, ~y and a token index t the deriva-
tive wrt feature i is given by:
?i(~y, ~x, t)?
?
~y?
?i(~y
?, ~x, t) p(~y?|~x,~?)
This is the difference between the empirical fea-
ture count in the training data and the estimated
count in the current model ~?. For a 1st-order model,
we can replace the expensive sum over all possible
tag sequences ~y? by a sum over all pairs of tags:
?i(yt, yt+1, ~x, t)?
?
y,y?
?i(y, y
?, ~x, t) p(y, y?|~x,~?)
The probability of a tag pair p(y, y?|~x,~?) can then
be calculated efficiently using the forward-backward
algorithm. If we further reduce the complexity of the
model to a 0-order model, we obtain simple maxi-
mum entropy model updates:
?i(yt, ~x, t)?
?
y
?i(y, ~x, t) p(y|~x,~?)
2.2 Pruned CRF Training
As we discussed in the introduction, we want to de-
code sentences by applying a variant of coarse-to-
fine tagging. Naively, to later tag with nth-order
323
accuracy we would train a series of n CRFs of in-
creasing order. We would then use the CRF of order
n ? 1 to restrict the input of the CRF of order n.
In this paper we approximate this approach, but do
so while training only one integrated model. This
way we can save both memory (by sharing feature
weights between different models) and training time
(by saving lower-order updates).
The main idea of our approach is to create increas-
ingly complex lattices and to filter candidate states
at every step to prevent a polynomial increase in lat-
tice size. The first step is to create a 0-order lat-
tice, which as discussed above, is identical to a se-
ries of independent local maximum entropy models
p(y|~x, t). The models base their prediction on the
current word xt and the immediate lexical context.
We then calculate the posterior probabilities and re-
move states y with p(y|~x, t) < ?0 from the lattice,
where ?0 is a parameter. The resulting reduced lat-
tice is similar to what we would obtain using candi-
date selection based on an MA.
We can now create a first order lattice by adding
transitions to the pruned lattice and pruning with
threshold ?1. The only difference to 0-order prun-
ing is that we now have to run forward-backward
to calculate the probabilities p(y|~x, t). Note that in
theory we could also apply the pruning to transition
probabilities of the form p(y, y?|~x, t); however, this
does not seem to yield more accurate models and is
less efficient than state pruning.
For higher-order lattices we merge pairs of states
into new states, add transitions and prune with
threshold ?i.
We train the model using l1-regularized Stochas-
tic Gradient Descent (SGD) (Tsuruoka et al, 2009).
We would like to create a cascade of increasingly
complex lattices and update the weight vector with
the gradient of the last lattice. The updates, how-
ever, are undefined if the gold sequence is pruned
from the lattice. A solution would be to simply rein-
sert the gold sequence, but this yields poor results
as the model never learns to keep the gold sequence
in the lower-order lattices. As an alternative we per-
form the gradient update with the highest lattice still
containing the gold sequence. This approach is sim-
ilar to ?early updating? (Collins and Roark, 2004)
in perceptron learning, where during beam search
an update with the highest scoring partial hypothe-
1: function GETSUMLATTICE(sentence, ~? )
2: gold-tags? getTags(sentence)
3: candidates? getAllCandidates(sentence)
4: lattice? ZeroOrderLattice(candidates)
5: for i = 1? n do
6: candidates? lattice. prune(?i?1)
7: if gold-tags 6? candidates then
8: return lattice
9: end if
10: if i > 1 then
11: candidates? mergeStates(candidates)
12: end if
13: candidates? addTransitions(candidates)
14: lattice? SequenceLattice(candidates, i)
15: end for
16: return lattice
17: end function
Figure 1: Lattice generation during training
sis is performed whenever the gold candidate falls
out of the beam. Intuitively, we are trying to opti-
mize an nth-order CRF objective function, but ap-
ply small lower-order corrections to the weight vec-
tor when necessary to keep the gold candidate in the
lattice. Figure 1 illustrates the lattice generation pro-
cess. The lattice generation during decoding is iden-
tical, except that we always return a lattice of the
highest order n.
The savings in training time of this integrated ap-
proach are large; e.g., training a maximum entropy
model over a tagset of roughly 1800 tags and more
than half a million instances is slow as we have to
apply 1800 weight vector updates for every token
in the training set and every SGD iteration. In the
integrated model we only have to apply 1800 up-
dates when we lose the gold sequence during fil-
tering. Thus, in our implementation training a 0-
order model for Czech takes roughly twice as long
as training a 1st-order model.
2.3 Threshold Estimation
Our approach would not work if we were to set the
parameters ?i to fixed predetermined values; e.g.,
the ?i depend on the size of the tagset and should
be adapted during training as we start the training
with a uniform model that becomes more specific.
We therefore set the ?i by specifying ?i, the average
number of tags per position that should remain in
the lattice after pruning. This also guarantees sta-
ble lattice sizes and thus stable training times. We
324
achieve stable average number of tags per position
by setting the ?i dynamically during training: we
measure the real average number of candidates per
position ??i and apply corrections after processing a
certain fraction of the sentences of the training set.
The updates are of the form:
?i =
{
+0.1 ? ?i if ??i < ?i
?0.1 ? ?i if ??i > ?i
Figure 2 shows an example training run for Ger-
man with ?0 = 4. Here the 0-order lattice reduces
the number of tags per position from 681 to 4 losing
roughly 15% of the gold sequences of the develop-
ment set, which means that for 85% of the sentences
the correct candidate is still in the lattice. This cor-
responds to more than 99% of the tokens. We can
also see that after two iterations only a very small
number of 0-order updates have to be performed.
2.4 Tag Decomposition
As we discussed before for the very large
POS+MORPH tagsets, most of the decoding time is
spent on the 0-order level. To decrease the number
of tag candidates in the 0-order model, we decode in
two steps by separating the fully specified tag into a
coarse-grained part-of-speech (POS) tag and a fine-
grained MORPH tag containing the morphological
features. We then first build a lattice over POS can-
didates and apply our pruning strategy. In a second
step we expand the remaining POS tags into all the
combinations with MORPH tags that were seen in
the training set. We thus build a sequence of lattices
of both increasing order and increasing tag complex-
ity.
2.5 Feature Set
We use the features of Ratnaparkhi (1996) and Man-
ning (2011): the current, preceding and succeed-
ing words as unigrams and bigrams and for rare
words prefixes and suffixes up to length 10, and
the occurrence of capital characters, digits and spe-
cial characters. We define a rare word as a word
with training set frequency ? 10. We concate-
nate every feature with the POS and MORPH tag
and every morphological feature. E.g., for the word
?der?, the POS tag art (article) and the MORPH
tag gen|sg|fem (genitive, singular, feminine) we
 0
 0.05
 0.1
 0.15
 0.2
 0  1  2  3  4  5  6  7  8  9  10
Unre
acha
ble g
old c
and
idat
es
Epochs
traindev
Figure 2: Example training run of a pruned 1st-order
model on German showing the fraction of pruned gold se-
quences (= sentences) during training for training (train)
and development sets (dev).
get the following features for the current word tem-
plate: der+art, der+gen|sg|fem, der+gen,
der+sg and der+fem.
We also use an additional binary feature, which
indicates whether the current word has been seen
with the current tag or ? if the word is rare ? whether
the tag is in a set of open tag classes. The open tag
classes are estimated by 10-fold cross validation on
the training set: We first use the folds to estimate
how often a tag is seen with an unknown word. We
then consider tags with a relative frequency ? 10?4
as open tag classes. While this is a heuristic, it is
safer to use a ?soft? heuristic as a feature in the lat-
tice than a hard constraint.
For some experiments we also use the output of a
morphological analyzer (MA). In that case we sim-
ply use every analysis of the MA as a simple nom-
inal feature. This approach is attractive because it
does not require the output of the MA and the an-
notation of the treebank to be identical; in fact, it
can even be used if treebank annotation and MA use
completely different features.
Because the weight vector dimensionality is high
for large tagsets and productive languages, we use a
hash kernel (Shi et al, 2009) to keep the dimension-
ality constant.
3 Related Work
Smith et al (2005) use CRFs for POS+MORPH tag-
ging, but use a morphological analyzer for candidate
selection. They report training times of several days
325
and that they had to use simplified models for Czech.
Several methods have been proposed to reduce
CRF training times. Stochastic gradient descent can
be applied to reduce the training time by a factor of 5
(Tsuruoka et al, 2009) and without drastic losses in
accuracy. Lavergne et al (2010) make use of feature
sparsity to significantly speed up training for mod-
erate tagset sizes (< 100) and huge feature spaces.
It is unclear if their approach would also work for
huge tag sets (> 1000).
Coarse-to-fine decoding has been successfully ap-
plied to CYK parsing where full dynamic program-
ming is often intractable when big grammars are
used (Charniak and Johnson, 2005). Weiss and
Taskar (2010) develop cascades of models of in-
creasing complexity in a framework based on per-
ceptron learning and an explicit trade-off between
accuracy and efficiency.
Kaji et al (2010) propose a modified Viterbi algo-
rithm that is still optimal but depending on task and
especially for big tag sets might be several orders of
magnitude faster. While their algorithm can be used
to produce fast decoders, there is no such modifica-
tion for the forward-backward algorithm used during
CRF training.
4 Experiments
We run POS+MORPH tagging experiments on Ara-
bic (ar), Czech (cs), Spanish (es), German (de) and
Hungarian (hu). The following table shows the type-
token (T/T) ratio, the average number of tags of ev-
ery word form that occurs more than once in the
training set (A) and the number of tags of the most
ambiguous word form (A?):
T/T A A?
ar 0.06 2.06 17
cs 0.13 1.64 23
es 0.09 1.14 9
de 0.11 2.15 44
hu 0.11 1.11 10
Arabic is a Semitic language with nonconcate-
native morphology. An additional difficulty is that
vowels are often not written in Arabic script. This
introduces a high number of ambiguities; on the
other hand it reduces the type-token ratio, which
generally makes learning easier. In this paper, we
work with the transliteration of Arabic provided in
the Penn Arabic Treebank. Czech is a highly inflect-
ing Slavic language with a large number of morpho-
logical features. Spanish is a Romance language.
Based on the statistics above we can see that it has
few POS+MORPH ambiguities. It is also the lan-
guage with the smallest tagset and the only language
in our setup that ? with a few exceptions ? does not
mark case. German is a Germanic language and ?
based on the statistics above ? the language with
the most ambiguous morphology. The reason is that
it only has a small number of inflectional suffixes.
The total number of nominal inflectional suffixes for
example is five. A good example for a highly am-
biguous suffix is ?en?, which is a marker for infini-
tive verb forms, for the 1st and 3rd person plural and
for the polite 2nd person singular. Additionally, it
marks plural nouns of all cases and singular nouns
in genitive, dative and accusative case.
Hungarian is a Finno-Ugric language with an ag-
glutinative morphology; this results in a high type-
token ratio, but also the lowest level of word form
ambiguity among the selected languages.
POS tagging experiments are run on all the lan-
guages above and also on English.
4.1 Resources
For Arabic we use the Penn Arabic Tree-
bank (Maamouri et al, 2004), parts 1?3 in
their latest versions (LDC2010T08, LDC2010T13,
LDC2011T09). As training set we use parts 1 and 2
and part 3 up to section ANN20020815.0083. All
consecutive sections up to ANN20021015.0096
are used as development set and the remainder as
test set. We use the unvocalized and pretokenized
transliterations as input. For Czech and Spanish,
we use the CoNLL 2009 data sets (Hajic? et al,
2009); for German, the TIGER treebank (Brants et
al., 2002) with the split from Fraser et al (2013);
for Hungarian, the Szeged treebank (Csendes et al,
2005) with the split from Farkas et al (2012). For
English we use the Penn Treebank (Marcus et al,
1993) with the split from Toutanova et al (2003).
We also compute the possible POS+MORPH tags
for every word using MAs. For Arabic we use the
AraMorph reimplementation of Buckwalter (2002),
for Czech the ?free? morphology (Hajic?, 2001), for
Spanish Freeling (Padro? and Stanilovsky, 2012), for
German DMOR (Schiller, 1995) and for Hungarian
326
Magyarlanc 2.0 (Zsibrita et al, 2013).
4.2 Setup
To compare the training and decoding times we run
all experiments on the same test machine, which fea-
tures two Hexa-Core Intel Xeon X5680 CPUs with
3,33 GHz and 6 cores each and 144 GB of mem-
ory. The baseline tagger and our PCRF implemen-
tation are run single threaded.2 The taggers are im-
plemented in different programming languages and
with different degrees of optimization; still, the run
times are indicative of comparative performance to
be expected in practice.
Our Java implementation is always run with 10
SGD iterations and a regularization parameter of
0.1, which for German was the optimal value out of
{0, 0.01, 0.1, 1.0}. We follow Tsuruoka et al (2009)
in our implementation of SGD and shuffle the train-
ing set between epochs. All numbers shown are av-
erages over 5 independent runs. Where not noted
otherwise, we use ?0 = 4, ?1 = 2 and ?2 = 1.5.
We found that higher values do not consistently in-
crease performance on the development set, but re-
sult in much higher training times.
4.3 POS Experiments
In a first experiment we evaluate the speed and ac-
curacy of CRFs and PCRFs on the POS tagsets.
As shown in Table 1 the tagset sizes range from
12 for Czech and Spanish to 54 and 57 for Ger-
man and Hungarian, with Arabic (38) and English
(45) in between. The results of our experiments are
given in Table 2. For the 1st-order models, we ob-
serve speed-ups in training time from 2.3 to 31 at no
loss in accuracy. For all languages, training pruned
higher-order models is faster than training unpruned
1st-order models and yields more accurate models.
Accuracy improvements range from 0.08 for Hun-
garian to 0.25 for German. We can conclude that
for small and medium tagset sizes PCRFs give sub-
stantial improvements in both training and decod-
ing speed3 and thus allow for higher-order tagging,
2Our tagger might actually use more than one core because
the Java garbage collection is run in parallel.
3Decoding speeds are provided in an appendix submitted
separately.
which for all languages leads to significant4 accu-
racy improvements.
4.4 POS+MORPH Oracle Experiments
Ideally, for the full POS+MORPH tagset we would
also compare our results to an unpruned CRF, but
our implementation turned out to be too slow to do
the required number of experiments. For German,
the model processed ? 0.1 sentences per second
during training; so running 10 SGD iterations on
the 40,472 sentences would take more than a month.
We therefore compare our model against models that
perform oracle pruning, which means we perform
standard pruning, but always keep the gold candi-
date in the lattice. The oracle pruning is applied dur-
ing training and testing on the development set. The
oracle model performance is thus an upper bound for
the performance of an unpruned CRF.
The most interesting pruning step happens at the
0-order level when we reduce from hundreds of can-
didates to just a couple. Table 3 shows the results for
1st-order CRFs.
We can roughly group the five languages into
three groups: for Spanish and Hungarian the dam-
age is negligible, for Arabic we see a small decrease
of 0.07 and only for Czech and German we observe
considerable differences of 0.14 and 0.37. Surpris-
ingly, doubling the number of candidates per posi-
tion does not lead to significant improvements.
We can conclude that except for Czech and Ger-
man losses due to pruning are insignificant.
4.5 POS+MORPH Higher-Order Experiments
One argument for PCRFs is that while they might
be less accurate than standard CRFs they allow to
train higher-order models, which in turn might be
more accurate than their standard lower-order coun-
terparts. In this section, we investigate how big the
improvements of higher-order models are. The re-
sults are given in the following table:
n ar cs es de hu
1 90.90 92.45 97.95 88.96 96.47
2 91.86* 93.06* 98.01 90.27* 96.57*
3 91.88* 92.97* 97.87 90.60* 96.50
4Throughout the paper we establish significance by running
approximate randomization tests on sentences (Yeh, 2000).
327
ar cs es de hu en
n TT ACC TT ACC TT ACC TT ACC TT ACC TT ACC
CRF 1 106 96.21 10 98.95 7 98.51 234 97.69 374 97.63 154 97.05
PCRF 1 5 96.21 4 98.96 3 98.52 7 97.70 12 97.64 5 97.07
PCRF 2 6 96.43* 5 99.01* 3 98.65* 9 97.91* 13 97.71* 6 97.21*
PCRF 3 6 96.43* 6 99.03* 4 98.66* 9 97.94* 14 97.69 6 97.19*
Table 2: POS tagging experiments with pruned and unpruned CRFs with different orders n. For every language the
training time in minutes (TT) and the POS accuracy (ACC) are given. * indicates models significantly better than CRF
(first line).
ar cs es de hu
1 Oracle ?0 = 4 90.97 92.59 97.91 89.33 96.48
2 Model ?0 = 4 90.90 92.45* 97.95 88.96* 96.47
3 Model ?0 = 8 90.89 92.48* 97.94 88.94* 96.47
Table 3: Accuracies for models with and without oracle pruning. * indicates models significantly worse than the oracle
model.
We see that 2nd-order models give improvements for
all languages. For Spanish and Hungarian we see
minor improvements ? 0.1.
For Czech we see a moderate improvement of
0.61 and for Arabic and German we observe sub-
stantial improvements of 0.96 and 1.31. An analysis
on the development set revealed that for all three lan-
guages, case is the morphological feature that bene-
fits most from higher-order models. A possible ex-
planation is that case has a high correlation with syn-
tactic relations and is thus affected by long-distance
dependencies.
German is the only language where fourgram
models give an additional improvement over trigram
models. The reason seem to be sentences with long-
range dependencies, e.g., ?Die Rebellen haben kein
Lo?segeld verlangt? (The rebels have not demanded
any ransom); ?verlangt? (demanded) is a past partic-
ple that is separated from the auxilary verb ?haben?
(have). The 2nd-order model does not consider
enough context and misclassifies ?verlangt? as a fi-
nite verb form, while the 3rd-order model tags it cor-
rectly.
We can also conclude that the improvements for
higher-order models are always higher than the loss
we estimated in the oracle experiments. More pre-
cisely we see that if a language has a low number of
word form ambiguities (e.g., Hungarian) we observe
a small loss during 0-order pruning but we also have
to expect less of an improvement when increasing
the order of the model. For languages with a high
number of word form ambiguities (e.g., German) we
must anticipate some loss during 0-order pruning,
but we also see substantial benefits for higher-order
models.
Surprisingly, we found that higher-order PCRF
models can also avoid the pruning errors of lower-
order models. Here is an example from the German
data. The word ?Januar? (January) is ambiguous: in
the training set, it occurs 108 times as dative, 9 times
as accusative and only 5 times as nominative. The
development set contains 48 nominative instances of
?Januar? in datelines at the end of news articles, e.g.,
?TEL AVIV, 3. Januar?. For these 48 occurrences,
(i) the oracle model in Table 3 selects the correct
case nominative, (ii) the 1st-order PCRF model se-
lects the incorrect case accusative, and (iii) the 2nd-
and 3rd-order models select ? unlike the 1st-order
model ? the correct case nominative. Our interpreta-
tion is that the correct nominative reading is pruned
from the 0-order lattice. However, the higher-order
models can put less weight on 0-order features as
they have access to more context to disambiguate the
sequence. The lower weights of order-0 result in a
more uniform posterior distribution and the nomina-
tive reading is not pruned from the lattice.
4.6 Experiments with Morph. Analyzers
In this section we compare the improvements of
higher-order models when used with MAs. The re-
328
ar cs es de hu en
TT ACC TT ACC TT ACC TT ACC TT ACC TT ACC
SVMTool 178 96.39 935 98.94 64 98.42 899 97.29 2653 97.42 253 97.09
Morfette 9 95.91 6 99.00 3 98.43 16 97.28 30 97.53 17 96.85
CRFSuite 4 96.20 2 99.02 2 98.40 8 97.57 15 97.48 8 96.80
Stanford 29 95.98 8 99.08 7 98.53 51 97.70 40 97.53 65 97.24
PCRF 1 5 96.21* 4 98.96* 3 98.52 7 97.70 12 97.64* 5 97.07*
PCRF 2 6 96.43 5 99.01* 3 98.65* 9 97.91* 13 97.71* 6 97.21
PCRF 3 6 96.43 6 99.03 4 98.66* 9 97.94* 14 97.69* 6 97.19
Table 4: Development results for POS tagging. Given are training times in minutes (TT) and accuracies (ACC).
Best baseline results are underlined and the overall best results bold. * indicates a significant difference (positive or
negative) between the best baseline and a PCRF model.
ar cs es de hu en
SVMTool 96.19 98.82 98.44 96.44 97.32 97.12
Morfette 95.55 98.91 98.41 96.68 97.28 96.89
CRFSuite 95.97 98.91 98.40 96.82 97.32 96.94
Stanford 95.75 98.99 98.50 97.09 97.32 97.28
PCRF 1 96.03* 98.83* 98.46 97.11 97.44* 97.09*
PCRF 2 96.11 98.88* 98.66* 97.36* 97.50* 97.23
PCRF 3 96.14 98.87* 98.66* 97.44* 97.49* 97.19*
Table 5: Test results for POS tagging. Best baseline results are underlined and the overall best results bold. * indicates
a significant difference between the best baseline and a PCRF model.
ar cs es de hu
TT ACC TT ACC TT ACC TT ACC TT ACC
SVMTool 454 89.91 2454 89.91 64 97.63 1649 85.98 3697 95.61
RFTagger 4 89.09 3 90.38 1 97.44 5 87.10 10 95.06
Morfette 132 89.97 539 90.37 63 97.71 286 85.90 540 95.99
CRFSuite 309 89.33 9274 91.10 69 97.53 1295 87.78 5467 95.95
PCRF 1 22 90.90* 301 92.45* 25 97.95* 32 88.96* 230 96.47*
PCRF 2 26 91.86* 318 93.06* 32 98.01* 37 90.27* 242 96.57*
PCRF 3 26 91.88* 318 92.97* 35 97.87* 37 90.60* 241 96.50*
Table 6: Development results for POS+MORPH tagging. Given are training times in minutes (TT) and accuracies
(ACC). Best baseline results are underlined and the overall best results bold. * indicates a significant difference
between the best baseline and a PCRF model.
ar cs es de hu
SVMTool 89.58 89.62 97.56 83.42 95.57
RFTagger 88.76 90.43 97.35 84.28 94.99
Morfette 89.62 90.01 97.58 83.48 95.79
CRFSuite 89.05 90.97 97.60 85.68 95.82
PCRF 1 90.32* 92.31* 97.82* 86.92* 96.22*
PCRF 2 91.29* 92.94* 97.93* 88.48* 96.34*
PCRF 3 91.22* 92.99* 97.82* 88.58* 96.29*
Table 7: Test results for POS+MORPH tagging. Best baseline results are underlined and the overall best results bold.
* indicates a significant difference between the best baseline and a PCRF model.
329
sults are given in the following table:
n ar cs es de hu
1 90.90? 92.45? 97.95? 88.96? 96.47?
2 91.86+ 93.06 98.01? 90.27+ 96.57?
3 91.88+ 92.97? 97.87? 90.60+ 96.50?
MA 1 91.22 93.21 98.27 89.82 97.28
MA 2 92.16+ 93.87+ 98.37+ 91.31+ 97.51+
MA 3 92.14+ 93.88+ 98.28 91.65+ 97.48+
Plus and minus indicate models that are signif-
icantly better or worse than MA1. We can see
that the improvements due to higher-order models
are orthogonal to the improvements due to MAs
for all languages. This was to be expected as
MAs provide additional lexical knowledge while
higher-order models provide additional information
about the context. For Arabic and German the
improvements of higher-order models are bigger
than the improvements due to MAs.
4.7 Comparison with Baselines
We use the following baselines: SVMTool
(Gime?nez and Ma`rquez, 2004), an SVM-based dis-
criminative tagger; RFTagger (Schmid and Laws,
2008), an n-gram Hidden Markov Model (HMM)
tagger developed for POS+MORPH tagging; Mor-
fette (Chrupa?a et al, 2008), an averaged percep-
tron with beam search decoder; CRFSuite (Okazaki,
2007), a fast CRF implementation; and the Stanford
Tagger (Toutanova et al, 2003), a bidirectional Max-
imum Entropy Markov Model. For POS+MORPH
tagging, all baselines are trained on the concatena-
tion of POS tag and MORPH tag. We run SVM-
Tool with the standard feature set and the optimal
c-values ? {0.1, 1, 10}. Morfette is run with the de-
fault options. For CRFSuite we use l2-regularized
SGD training. We use the optimal regularization pa-
rameter ? {0.01, 0.1, 1.0} and stop after 30 itera-
tions where we reach a relative improvement in reg-
ularized likelihood of at most 0.01 for all languages.
The feature set is identical to our model except for
some restrictions: we only use concatenations with
the full tag and we do not use the binary feature that
indicates whether a word-tag combination has been
observed. We also had to restrict the combinations
of tag and features to those observed in the training
set5. Otherwise the memory requirements would ex-
ceed the memory of our test machine (144 GB) for
Czech and Hungarian. The Stanford Tagger is used
5We set the CRFSuite option possible states = 0
as a bidirectional 2nd-order model and trained us-
ing OWL-BFGS. For Arabic, German and English
we use the language specific feature sets and for the
other languages the English feature set.
Development set results for POS tagging are
shown in Table 4. We can observe that Morfette,
CRFSuite and the PCRF models for different orders
have training times in the same order of magnitude.
For Arabic, Czech and English, the PCRF accuracy
is comparable to the best baseline models. For the
other languages we see improvements of 0.13 for
Spanish, 0.18 for Hungarian and 0.24 for German.
Evaluation on the test set confirms these results, see
Table 5.6
The POS+MORPH tagging development set re-
sults are presented in Table 6. Morfette is the fastest
discriminative baseline tagger. In comparison with
Morfette the speed up for 3rd-order PCRFs lies be-
tween 1.7 for Czech and 5 for Arabic. Morfette
gives the best baseline results for Arabic, Spanish
and Hungarian and CRFSuite for Czech and Ger-
man. The accuracy improvements of the best PCRF
models over the best baseline models range from
0.27 for Spanish over 0.58 for Hungarian, 1.91 for
Arabic, 1.96 for Czech to 2.82 for German. The test
set experiments in Table 7 confirm these results.
5 Conclusion
We presented the pruned CRF (PCRF) model for
very large tagsets. The model is based on coarse-to-
fine decoding and stochastic gradient descent train-
ing with early updating. We showed that for mod-
erate tagset sizes of ? 50, the model gives signif-
icant speed-ups over a standard CRF with negligi-
ble losses in accuracy. Furthermore, we showed that
training and tagging for approximated trigram and
fourgram models is still faster than standard 1st-
order tagging, but yields significant improvements
in accuracy.
In oracle experiments with POS+MORPH tagsets
we demonstrated that the losses due to our approx-
imation depend on the word level ambiguity of the
respective language and are moderate (? 0.14) ex-
cept for German where we observed a loss of 0.37.
6Gime?nez and Ma`rquez (2004) report an accuracy of 97.16
instead of 97.12 for SVMTool for English and Manning (2011)
an accuracy of 97.29 instead of 97.28 for the Stanford tagger.
330
We also showed that higher order tagging ? which
is prohibitive for standard CRF implementations ?
yields significant improvements over unpruned 1st-
order models. Analogous to the oracle experiments
we observed big improvements for languages with a
high level of POS+MORPH ambiguity such as Ger-
man and smaller improvements for languages with
less ambiguity such as Hungarian and Spanish.
Acknowledgments
The first author is a recipient of the Google Europe
Fellowship in Natural Language Processing, and this
research is supported in part by this Google Fellow-
ship. This research was also funded by DFG (grant
SFB 732).
References
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER tree-
bank. In Proceedings of the workshop on treebanks
and linguistic theories.
Tim Buckwalter. 2002. Buckwalter Arabic Morpholog-
ical Analyzer Version 1.0. Linguistic Data Consor-
tium, University of Pennsylvania, 2002. LDC Catalog
No.: LDC2002L49.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of ACL.
Grzegorz Chrupa?a, Georgiana Dinu, and Josef van Gen-
abith. 2008. Learning morphology with Morfette. In
Proceedings of LREC.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceedings
of ACL.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP.
Do?ra Csendes, Ja?nos Csirik, Tibor Gyimo?thy, and Andra?s
Kocsor. 2005. The Szeged treebank. In Proceedings
of Text, Speech and Dialogue.
Richa?rd Farkas, Veronika Vincze, and Helmut Schmid.
2012. Dependency parsing of Hungarian: Baseline re-
sults and challenges. In Proceedings of EACL.
Alexander Fraser, Helmut Schmid, Richa?rd Farkas, Ren-
jing Wang, and Hinrich Schu?tze. 2013. Knowl-
edge Sources for Constituent Parsing of German, a
Morphologically Rich and Less-Configurational Lan-
guage. Computational Linguistics.
Jesu?s Gime?nez and Lluis Ma`rquez. 2004. Svmtool: A
general POS tagger generator based on Support Vector
Machines. In Proceedings of LREC.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, et al 2009. The CoNLL-2009
shared task: Syntactic and semantic dependencies in
multiple languages. In Proceedings of CoNLL.
Jan Hajic?. 2000. Morphological tagging: Data vs. dictio-
naries. In Proceedings of NAACL.
Jan Hajic?. 2001. Czech ?Free? Morphology. URL
http://ufal.mff.cuni.cz/pdt/Morphology and Tagging.
Nobuhiro Kaji, Yasuhiro Fujiwara, Naoki Yoshinaga, and
Masaru Kitsuregawa. 2010. Efficient staggered de-
coding for sequence labeling. In Proceedings of ACL.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of ICML.
Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.
2010. Practical very large scale CRFs. In Proceed-
ings of ACL.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The Penn Arabic treebank:
Building a large-scale annotated Arabic corpus. In
Proceedings of NEMLAR.
Christopher D Manning. 2011. Part-of-speech tagging
from 97% to 100%: Is it time for some linguistics?
In Computational Linguistics and Intelligent Text Pro-
cessing. Springer.
Mitchell P. Marcus, Mary A. Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
linguistics.
Naoaki Okazaki. 2007. Crfsuite: A fast implemen-
tation of conditional random fields (CRFs). URL
http://www.chokkan.org/software/crfsuite.
Llu??s Padro? and Evgeny Stanilovsky. 2012. Freeling
3.0: Towards Wider Multilinguality. In Proceedings
of LREC.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In EMNLP.
Alexander M. Rush and Slav Petrov. 2012. Vine pruning
for efficient multi-pass dependency parsing. In Pro-
ceedings of NAACL.
Anne Schiller. 1995. DMOR Benutzerhandbuch. Uni-
versita?t Stuttgart, Institut fu?r maschinelle Sprachver-
arbeitung.
Helmut Schmid and Florian Laws. 2008. Estimation of
conditional probabilities with decision trees and an ap-
plication to fine-grained POS tagging. In Proceedings
of COLING.
331
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of NEMLP.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided Learning for Bidirectional Sequence Classifi-
cation. In Proceedings of ACL.
Qinfeng Shi, James Petterson, Gideon Dror, John Lang-
ford, Alex Smola, and S.V.N. Vishwanathan. 2009.
Hash Kernels for Structured Data. J. Mach. Learn.
Res.
Noah A. Smith, David A. Smith, and Roy W. Tromble.
2005. Context-based morphological disambiguation
with random fields. In Proceedings of EMNLP.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of NAACL.
Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ana-
niadou. 2009. Stochastic gradient descent training
for L1-regularized log-linear models with cumulative
penalty. In Proceedings of ACL.
David Weiss and Ben Taskar. 2010. Structured predic-
tion cascades. In In Proceedings of AISTATS.
Alexander Yeh. 2000. More accurate tests for the statis-
tical significance of result differences. In Proceedings
of COLING.
Ja?nos Zsibrita, Veronika Vincze, and Richa?rd Farkas.
2013. Magyarlanc 2.0: Szintaktikai elemze?s e?s fel-
gyors??tott szo?faji egye?rtelmu?s??te?s. In IX. Magyar
Sza?m??to?ge?pes Nyelve?szeti Konferencia.
332
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 963?967,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Dependency parsing with latent refinements of part-of-speech tags
Thomas M?uller
?
, Richard Farkas
?
, Alex Judea
?
, Helmut Schmid
?
, and Hinrich Sch?utze
?
?
Center for Information and Language Processing, University of Munich, Germany
?
Department of Informatics, University of Szeged, Hungary
?
Heidelberg Institute for Theoretical Studies, Heidelberg, Germany
muellets@cis.lmu.de
Abstract
In this paper we propose a method to
increase dependency parser performance
without using additional labeled or unla-
beled data by refining the layer of pre-
dicted part-of-speech (POS) tags. We per-
form experiments on English and Ger-
man and show significant improvements
for both languages. The refinement is
based on generative split-merge training
for Hidden Markov models (HMMs).
1 Introduction
Probabilistic Context-free Grammars with latent
annotations (PCFG-LA) have been shown (Petrov
et al., 2006) to yield phrase structure parsers
with state-of-the-art accuracy. While Hidden
Markov Models with latent annotations (HMM-
LA) (Huang et al., 2009), stay somewhat behind
the performance of state-of-the-art discriminative
taggers (Eidelman et al., 2010). In this paper we
address the question of whether the resulting la-
tent POS tags are linguistically meaningful and
useful for upstream tasks such as syntactic pars-
ing. We find that this is indeed the case, lead-
ing to a procedure that significantly increases the
performance of dependency parsers. The proce-
dure is attractive because the refinement of pre-
dicted part-of-speech sequences using a coarse-to-
fine strategy (Petrov and Klein, 2007) is fast and
efficient. More precisely, we show that incorpo-
rating the induced POS into a state-of-the-art de-
pendency parser (Bohnet, 2010) gives increases in
Labeled Attachment Score (LAS): from 90.34 to
90.57 for English and from 87.92 to 88.24 (resp.
88.35 to 88.51) for German without using (resp.
with using) morphological features.
2 Related Work
Petrov et al. (2006) introduce generative split-
merge training for PCFGs and provide a fully au-
tomatic method for training state-of-the-art phrase
structure parsers. They argue that the resulting la-
tent annotations are linguistically meaningful. Sun
et al. (2008) induce latent sub-states into CRFs and
show that noun phrase (NP) recognition can be im-
proved, especially if no part-of-speech features are
available. Huang et al. (2009) apply split-merge
training to create HMMs with latent annotations
(HMM-LA) for Chinese POS tagging. They re-
port that the method outperforms standard gener-
ative bigram and trigram tagging, but do not com-
pare to discriminative methods. Eidelman et al.
(2010) show that a bidirectional variant of latent
HMMs with incorporation of prosodic information
can yield state-of-the-art results in POS tagging of
conversational speech.
3 Split-Merge Training for HMMs
Split-merge training for HMMs (Huang et al.,
2009) iteratively splits every tag into two subtags.
Word emission and tag transition probabilities of
subtags are then initialized close to the values of
the parent tags but with some randomness to break
symmetry. Using expectation?maximization (EM)
training the parameters can then be set to a local
maximum of the training data likelihood. After
this split phase, the merge phase reverts splits that
only lead to small improvements in the likelihood
function in order to increase the robustness of the
model. This approach requires an approximation
of the gain in likelihood of every split analogous
to Petrov et al. (2006) as an exact computation is
not feasible.
We have observed that this procedure is not
963
Universal Tag Feature Tag
0
Tag
1
English Adjectives p(w|t) more (0.05) many (0.03) last (0.03) new (0.03) other (0.03) first (0.02)
(ADJ) p(u|t) VERB (0.32) ADV (0.27) NOUN (0.14) DET (0.39) ADP (0.17) ADJ (0.10)
Particles p(w|t) ?s (0.93) ? (0.07) to (0.89) up (0.04) out (0.02) off (0.01)
(PRT) p(b|t) POS (1.00) TO (0.89) RP (0.10)
Prepositions p(w|t) that (0.11) in (0.10) by (0.09) of (0.43) in (0.19) for (0.11)
(ADP) p(u|t) VERB (0.46) NOUN (0.15) . (0.13) NOUN (0.84) NUM (0.06) ADJ (0.03)
Pronouns p(w|t) its (0.30) their (0.15) his (0.14) it (0.21) he (0.16) they (0.12)
(PRON) p(b|t) PRP$ (0.68) PRP (0.26) WP (0.05) PRP (0.87) WP (0.11) PRP$ (0.02)
Verbs p(w|t) be (0.06) been (0.02) have (0.02) is (0.10) said (0.08) was (0.05)
(VERB) p(u|t) VERB (0.38) PRT (0.22) ADV (0.11) NOUN (0.52) PRON (0.20) . (0.12)
German Conjunctions p(w|t) da? (0.26) wenn (0.08) um (0.06) und (0.76) oder (0.07) als (0.06)
(CONJ) p(b|t) KOUS (0.58) KON (0.30) KOUI (0.06) KON (0.88) KOKOM (0.10) APPR (0.02)
Particles p(w|t) an (0.13) aus (0.10) ab (0.09) nicht (0.49) zu (0.46) Nicht (0.01)
(PRT) p(b|t) PTKVZ (0.92) ADV (0.04) ADJD (0.01) PTKNEG (0.52) PTKZU (0.44) PTKA (0.02)
Pronouns p(w|t) sich (0.13) die (0.08) es (0.07) ihre (0.06) seine (0.05) seiner (0.05)
(PRON) p(b|t) PPER (0.33) PRF (0.14) PRELS (0.14) PPOSAT (0.40) PIAT (0.34) PDAT (0.16)
Verbs p(w|t) werden (0.04) worden (0.02) ist (0.02) ist (0.07) hat (0.04) sind (0.03)
(VERB) p(u|t) NOUN (0.46) VERB (0.22) PRT (0.10) NOUN (0.49) . (0.19) PRON (0.16)
Table 1: Induced sub-tags and their statistics, word forms (p(w|t)), treebank tag (p(b|t)) and preceding
Universal tag probability (p(u|t)). Bold: linguistically interesting differences.
only a way to increase HMM tagger performance
but also yields annotations that are to a consid-
erable extent linguistically interpretable. As an
example we discuss some splits that occurred af-
ter a particular split-merge step for English and
German. For the sake of comparability we ap-
plied the split to the Universal Tagset (Petrov et
al., 2011). Table 1 shows the statistics used for
this analysis. The Universal POS tag set puts the
three Penn-Treebank tags RP (particle), POS (pos-
sessive marker) and TO into one particle tag (see
?PRT? in English part of the table). The training
essentially reverses this by splitting particles first
into possessive and non-possessive markers and in
a subsequent split the non-possessives into TO and
particles. For German we have a similar split into
verb particles, negation particles like nicht ?not?
and the infinitive marker zu ?to? (?PRT?) in the
German part of the table). English prepositions
get split by proximity to verbs or nouns (?ADP?).
Subordinate conjunctions like that, which in the
Penn-Treebank annotation are part of the prepo-
sition tag IN, get assigned to the sub-class next
to verbs. For German we also see a separation
of ?CONJ? into predominantly subordinate con-
junctions (Tag 0) and predominantly coordinating
conjunctions (Tag 1). For both languages adjec-
tives get split by predicative and attributive use.
For English the predicative sub-class also seems
to hold rather atypical adjectives like ?such? and
?last.? For English, verbs (?VERB?) get split into
a predominantly infinite tag (Tag 0) and a predom-
inantly finite tag (Tag 1) while for German we get
a separation by verb position. In German we get a
separation of pronouns (?PRON?) into possessive
and non-possessive; in English, pronouns get split
by predominant usage in subject position (Tag 0)
and as possessives (Tag 1).
Our implementation of HMM-LA has been re-
leased under an open-source licence.
1
In the next section we evaluate the utility of
these annotations for dependency parsing.
4 Dependency Parsing
In this section we investigate the utility of in-
duced POS as features for dependency parsing.
We run our experiments on the CoNLL-2009 data
sets (Haji?c et al., 2009) for English and German.
As a baseline system we use the latest version
of the mate-tools parser (Bohnet, 2010).
3
It was
the highest scoring syntactic parser for German
and English in the CoNLL 2009 shared task eval-
uation. The parser gets automatically annotated
lemmas, POS and morphological features as input
which are part of the CoNLL-2009 data sets.
In this experiment we want to examine the ben-
efits of tag refinements isolated from the improve-
ments caused by using two taggers in parallel,
thus we train the HMM-LA on the automatically
tagged POS sequences of the training set and use
it to add an additional layer of refined POS to the
input data of the parser. We do this by calculating
the forward-backward charts that are also used in
the E-steps during training ? in these charts base
1
https://code.google.com/p/cistern/
1
Unlabeled Attachment Score
3
We use v3.3 of Bohnet?s graph-based parser.
964
#Tags ?
LAS
max
LAS
?
LAS
?
UAS
max
UAS
?
UAS
English Baseline 88.43 91.46
58 88.52 (88.59) 0.06 91.52 (91.61) 0.08
73 88.55 (88.61) 0.05 91.54 (91.59) 0.04
92 88.60 (88.71) 0.08 91.60 (91.72) 0.08
115 88.62 (88.73) 0.07 91.58 (91.71) 0.08
144 88.60 (88.70) 0.07 91.60 (91.71) 0.07
German (no feat.) Baseline 87.06 89.54
85 87.09 (87.18) 0.06 89.61 (89.67) 0.04
107 87.23 (87.36) 0.09 89.74 (89.83) 0.08
134 87.22 (87.31) 0.09 89.75 (89.86) 0.09
German (feat.) Baseline 87.35 89.75
85 87.33 (87.47) 0.11 89.76 (89.88) 0.09
107 87.43 (87.73) 0.16 89.81 (90.14) 0.17
134 87.38 (87.53) 0.08 89.75 (89.89) 0.08
Table 2: LAS and UAS
1
mean (?), best value (max) and std. deviation (?) for the development set for
English and German dependency parsing with (feat.) and without morphological features (no feat.).
tags of the refined tags are constrained to be iden-
tical to the automatically predicted tags.
We use 100 EM iterations after each split and
merge phase. The percentage of splits reverted in
each merge phase is set to .75.
We integrate the tags by adding one additional
feature for every edge: the conjunction of latent
tags of the two words connected by the edge.
Table 2 shows results of our experiments. All
numbers are averages of five independent runs.
For English the smaller models with 58 and 73
tags achieve improvements of ?.1. The improve-
ments for the larger tag sets are ?.2. The best
individual model improves LAS by .3. For the
German experiments without morphological fea-
tures we get only marginal average improvements
for the smallest tag set and improvements of ?.15
for the bigger tag sets. The average ULA scores
for 107 and 134 tags are at the same level as the
ULA scores of the baseline with morph. features.
The best model improves LAS by .3. For German
with morphological features the absolute differ-
ences are smaller: The smallest tag set does not
improve the parser on average. For the tag set
of 107 tags the average improvement is .08. The
best model improves LAS by .38. In all experi-
ments we see the highest improvements for tag set
sizes of roughly the same size (115 for English,
107 for German). While average improvements
are low (esp. for German with morphological fea-
tures), peak improvements are substantial.
Running the best English system on the test set
gives an improvement in LAS from 90.34 to 90.57;
this improvement is significant
4
(p < .02). For
German we get an improvement from 87.92 to
4
Approx. randomization test (Yeh, 2000) on LAS scores
88.24 without and from 88.35 to 88.51 with mor-
phological features. The difference between the
values without morphological features is signifi-
cant (p < .05), but the difference between mod-
els with morphological features is not (p = .26).
However, the difference between the baseline sys-
tem with morphological features and the best sys-
tem without morphological features is also not sig-
nificant (p = .49).
We can conclude that HMM-LA tags can sig-
nificantly improve parsing results. For German we
see that HMM-LA tags can substitute morpholog-
ical features up to an insignificant difference. We
also see that morphological features and HMM-
LA seem to be correlated as combining the two
gives only insignificant improvements.
5 Contribution Analysis
In this section we try to find statistical evidence
for why a parser using a fine-grained tag set might
outperform a parser based on treebank tags only.
The results indicate that an induced latent tag
set as a whole increases parsing performance.
However, not every split made by the HMM-LA
seems to be useful for the parser. The scatter plots
in Figure 1 show that there is no strict correlation
between tagging accuracy of a model and the re-
sulting LAS. This is expected as the latent induc-
tion optimizes a tagging objective function, which
does not directly translate into better parsing per-
formance. An example is lexicalization. Most
latent models for English create a subtag for the
preposition ?of?. This is useful for a HMM as ?of?
is frequent and has a very specific context. A lexi-
calized syntactic parser, however, does not benefit
from such a tag.
965
l l l ll
88.40 88.45 88.50 88.55 88.60 88.65 88.70 88.75
97.5
97.6
97.7
97.8
97.9
98.0
LAS
Taggin
g Accu
racy
87.00 87.05 87.10 87.15 87.20 87.25 87.30 87.35
97.10
97.12
97.14
97.16
97.18
97.20
LAS
Taggin
g Accu
racy
87.2 87.3 87.4 87.5 87.6 87.7
97.10
97.12
97.14
97.16
97.18
97.20
LAS
Taggin
g Accu
racy
Figure 1: Scatter plots of LAS vs tagging accuracy for English (left) and German without (middle) and
with (right) morphological features. English tag set sizes are 58 (squares), 73 (diamonds), 92 (trian-
gles), 115 (triangles pointing downwards) and 144 (circles). German tag set sizes are 85 (squares), 107
(diamonds) and 134 (triangles). The dashed lines indicate the baselines.
We base the remainder of our analysis on the
results of the baseline parser on the English devel-
opment set and the results of the best performing
latent model. The best performing model has a
LAS score of 88.73 vs 88.43 for the baseline, a dif-
ference of .3. If we just look at the LAS of words
with incorrectly predicted POS we see a difference
of 1.49. A look at the data shows that the latent
model helps the parser to identify words that might
have been annotated incorrectly. As an example
consider plural nouns (NNS) and two of their la-
tent subtags NNS
1
and NNS
2
and how often they
get classified correctly and misclassified as proper
nouns (NNPS):
NNS NNPS
NNS 2019 104
NNS
1
90 72
NNS
2
1100 13
. . . . . . . . .
We see that NNS
1
is roughly equally likely to
be a NNPS or NNS while NNS
2
gives much more
confidence of the actual POS being NNS. So one
benefit of HMM-LA POS tag sets are tags of dif-
ferent levels of confidence.
Another positive effect is that latent POS tags
have a higher correlation with certain dependency
relations. Consider proper nouns (NNP):
NAME NMOD SBJ
NNP 962 662 468
NNP
1
10 27 206
NNP
2
24 50 137
. . . . . . . . . . . .
We see that NNP
1
and NNP
2
are more likely
to appear in subject relations. NNP
1
contains sur-
names; the most frequent word forms are Keating,
Papandreou and Kaye. In contrast, NNP
2
con-
tains company names such as Sony, NBC and Key-
stone. This explains why the difference in LAS is
twice as high for NNPs as on average.
For German we see similar effects and the an-
ticipated correlation with morphology. The 5 de-
terminer subtags, for example, strongly correlate
with grammatical case:
Nom Gen Dat Acc
ART 1185 636 756 961
ART
1
367 7 38
ART
2
11 28 682 21
ART
3
6 602 7 3
ART
4
39 43 429
ART
5
762 6 17 470
6 Conclusion and Future Work
We have shown that HMMs with latent anno-
tations (HMMLA) can generate latent part-of-
speech tagsets are linguistically interpretable and
can be used to improve dependency parsing. Our
best systems improve an English parser from a
LAS of 90.34 to 90.57 and a German parser from
87.92 to 88.24 when not using morphological fea-
tures and from 88.35 to 88.51 when using mor-
phological features . Our analysis of the parsing
results shows that the major reasons for the im-
provements are: the separation of POS tags into
more and less trustworthy subtags, the creation of
POS subtags with higher correlation to certain de-
pendency labels and for German a correlation of
tags and morphological features such as case.
7 Future Work
The procedure works well in general. However,
not every split is useful for the parser; e.g., as
966
discussed above lexicalization increases HMM ac-
curacy, but does not help an already lexicalized
parser. We would like to use additional informa-
tion (e.g., from the dependency trees) to identify
useless splits. The different granularities of the hi-
erarchy induced by split-merge training are poten-
tially useful. However, the levels of the hierarchy
are incomparable: a child tag is in general not a
subtag of a parent tag. We think that coupling par-
ents and children in the tag hierarchy might be one
way to force a consistent hierarchy.
Acknowledgments
We would like to thank the anonymous reviewers
for their comments. The first author is a recipient
of the Google Europe Fellowship in Natural Lan-
guage Processing, and this research is supported in
part by this Google Fellowship and by DFG (grant
SFB 732). Most of this work was conducted while
the authors worked at the Institute for Natural Lan-
guage Processing of the University of Stuttgart.
References
Bernd Bohnet. 2010. Very high accuracy and fast de-
pendency parsing is not a contradiction. In Proceed-
ings of COLING.
Vladimir Eidelman, Zhongqiang Huang, and Mary
Harper. 2010. Lessons learned in part-of-speech
tagging of conversational speech. In Proceedings of
EMNLP.
Jan Haji?c, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart??, Llu??s
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad?o, Jan
?
St?ep?anek, et al. 2009. The conll-2009
shared task: Syntactic and semantic dependencies in
multiple languages. In Proceedings of CoNLL.
Zhongqiang Huang, Vladimir Eidelman, and Mary
Harper. 2009. Improving a simple bigram hmm
part-of-speech tagger by latent annotation and self-
training. In Proceedings of NAACL.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Proceedings of
NAACL.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of ACL.
Slav Petrov, Dipanjan Das, and Ryan McDon-
ald. 2011. A universal part-of-speech tagset.
ArXiv:1104.2086v1.
Xu Sun, Louis-Philippe Morency, Daisuke Okanohara,
and Jun?ichi Tsujii. 2008. Modeling latent-dynamic
in shallow parsing: a latent conditional model with
improved inference. In Proceedings of COLING.
Alexander Yeh. 2000. More accurate tests for the
statistical significance of result differences. In Pro-
ceedings of COLING.
967
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 55?65,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Dependency Parsing of Hungarian: Baseline Results and Challenges
Richa?rd Farkas1, Veronika Vincze2, Helmut Schmid1
1Institute for Natural Language Processing, University of Stuttgart
{farkas,schmid}@ims.uni-stuttgart.de
2Research Group on Artificial Intelligence, Hungarian Academy of Sciences
vinczev@inf.u-szeged.hu
Abstract
Hungarian is a stereotype of morpholog-
ically rich and non-configurational lan-
guages. Here, we introduce results on de-
pendency parsing of Hungarian that em-
ploy a 80K, multi-domain, fully manu-
ally annotated corpus, the Szeged Depen-
dency Treebank. We show that the results
achieved by state-of-the-art data-driven
parsers on Hungarian and English (which is
at the other end of the configurational-non-
configurational spectrum) are quite simi-
lar to each other in terms of attachment
scores. We reveal the reasons for this and
present a systematic and comparative lin-
guistically motivated error analysis on both
languages. This analysis highlights that ad-
dressing the language-specific phenomena
is required for a further remarkable error re-
duction.
1 Introduction
From the viewpoint of syntactic parsing, the lan-
guages of the world are usually categorized ac-
cording to their level of configurationality. At one
end, there is English, a strongly configurational
language while Hungarian is at the other end of
the spectrum. It has very few fixed structures
at the sentence level. Leaving aside the issue of
the internal structure of NPs, most sentence-level
syntactic information in Hungarian is conveyed
by morphology, not by configuration (E?. Kiss,
2002).
A large part of the methodology for syntactic
parsing has been developed for English. How-
ever, parsing non-configurational and less config-
urational languages requires different techniques.
In this study, we present results on Hungarian de-
pendency parsing and we investigate this general
issue in the case of English and Hungarian.
We employed three state-of-the-art data-driven
parsers (Nivre et al 2004; McDonald et al 2005;
Bohnet, 2010), which achieved (un)labeled at-
tachment scores on Hungarian not so different
from the corresponding English scores (and even
higher on certain domains/subcorpora). Our in-
vestigations show that the feature representation
used by the data-driven parsers is so rich that they
can ? without any modification ? effectively learn
a reasonable model for non-configurational lan-
guages as well.
We also conducted a systematic and compar-
ative error analysis of the system?s outputs for
Hungarian and English. This analysis highlights
the challenges of parsing Hungarian and sug-
gests that the further improvement of parsers re-
quires special handling of language-specific phe-
nomena. We believe that some of our findings
can be relevant for intermediate languages on the
configurational-non-configurational spectrum.
2 Chief Characteristics of the
Hungarian Morphosyntax
Hungarian is an agglutinative language, which
means that a word can have hundreds of word
forms due to inflectional or derivational affixa-
tion. A lot of grammatical information is encoded
in morphology and Hungarian is a stereotype of
morphologically rich languages. The Hungarian
word order is free in the sense that the positions
of the subject, the object and the verb are not fixed
within the sentence, but word order is related to
information structure, e.g. new (or emphatic) in-
formation (the focus) always precedes the verb
55
and old information (the topic) precedes the focus
position. Thus, the position relative to the verb
has no predictive force as regards the syntactic
function of the given argument: while in English,
the noun phrase before the verb is most typically
the subject, in Hungarian, it is the focus of the
sentence, which itself can be the subject, object
or any other argument (E?. Kiss, 2002).
The grammatical function of words is deter-
mined by case suffixes as in gyerek ?child? ? gye-
reknek (child-DAT) ?for (a/the) child?. Hungarian
nouns can have about 20 cases1 which mark the
relationship between the head and its arguments
and adjuncts. Although there are postpositions
in Hungarian, case suffixes can also express re-
lations that are expressed by prepositions in En-
glish.
Verbs are inflected for person and number and
the definiteness of the object. Since conjugational
information is sufficient to deduce the pronominal
subject or object, they are typically omitted from
the sentence: Va?rlak (wait-1SG2OBJ) ?I am wait-
ing for you?. This pro-drop feature of Hungar-
ian leads to the fact that there are several clauses
without an overt subject or object.
Another peculiarity of Hungarian is that the
third person singular present tense indicative form
of the copula is phonologically empty, i.e. there
are apparently verbless sentences in Hungarian:
A ha?z nagy (the house big) ?The house is big?.
However, in other tenses or moods, the copula
is present as in A ha?z nagy lesz (the house big
will.be) ?The house will be big?.
There are two possessive constructions in
Hungarian. First, the possessive relation is only
marked on the possessed noun (in contrast, it is
marked only on the possessor in English): a fiu?
kutya?ja (the boy dog-POSS) ?the boy?s dog?. Sec-
ond, both the possessor and the possessed bear a
possessive marker: a fiu?nak a kutya?ja (the boy-
DAT the dog-POSS) ?the boy?s dog?. In the latter
case, the possessor and the possessed may not be
adjacent within the sentence as in A fiu?nak la?tta a
kutya?ja?t (the boy-DAT see-PAST3SGOBJ the dog-
POSS-ACC) ?He saw the boy?s dog?, which results
in a non-projective syntactic tree. Note that in
the first case, the form of the possessor coincides
1Hungarian grammars and morphological coding sys-
tems do not agree on the exact number of cases, some rare
suffixes are treated as derivational suffixes in one grammar
and as case suffixes in others; see e.g. Farkas et al(2010).
with that of a nominative noun while in the second
case, it coincides with a dative noun.
According to these facts, a Hungarian parser
must rely much more on morphological analysis
than e.g. an English one since in Hungarian it
is morphemes that mostly encode morphosyntac-
tic information. One of the consequences of this
is that Hungarian sentences are shorter in terms
of word numbers than English ones. Based on
the word counts of the Hungarian?English paral-
lel corpus Hunglish (Varga et al 2005), an En-
glish sentence contains 20.5% more words than its
Hungarian equivalent. These extra words in En-
glish are most frequently prepositions, pronomi-
nal subjects or objects, whose parent and depen-
dency label are relatively easy to identify (com-
pared to other word classes). This train of thought
indicates that the cross-lingual comparison of fi-
nal parser scores should be conducted very care-
fully.
3 Related work
We decided to focus on dependency parsing in
this study as it is a superior framework for non-
configurational languages. It has gained inter-
est in natural language processing recently be-
cause the representation itself does not require
the words inside of constituents to be consecu-
tive and it naturally represent discontinuous con-
structions, which are frequent in languages where
grammatical relations are often signaled by mor-
phology instead of word order (McDonald and
Nivre, 2011). The two main efficient approaches
for dependency parsing are the graph-based and
the transition-based parsers. The graph-based
models look for the highest scoring directed span-
ning tree in the complete graph whose nodes are
the words of the sentence in question. They solve
the machine learning problem of finding the opti-
mal scoring function of subgraphs (Eisner, 1996;
McDonald et al 2005). The transition-based ap-
proaches parse a sentence in a single left-to-right
pass over the words. The next transition in these
systems is predicted by a classifier that is based
on history-related features (Kudo and Matsumoto,
2002; Nivre et al 2004).
Although the available treebanks for Hungar-
ian are relatively big (82K sentences) and fully
manually annotated, the studies on parsing Hun-
garian are rather limited. The Szeged (Con-
stituency) Treebank (Csendes et al 2005) con-
56
sists of six domains ? namely, short business
news, newspaper, law, literature, compositions
and informatics ? and it is manually annotated
for the possible alternatives of words? morpho-
logical analyses, the disambiguated analysis and
constituency trees. We are aware of only two
articles on phrase-structure parsers which were
trained and evaluated on this corpus (Barta et al
2005; Iva?n et al 2007) and there are a few studies
on hand-crafted parsers reporting results on small
own corpora (Babarczy et al 2005; Pro?sze?ky et
al., 2004).
The Szeged Dependency Treebank (Vincze et
al., 2010) was constructed by first automatically
converting the phrase-structure trees into depen-
dency trees, then each of them was manually
investigated and corrected. We note that the
dependency treebank contains more information
than the constituency one as linguistic phenom-
ena (like discontinuous structures) were not anno-
tated in the former corpus, but were added to the
dependency treebank. To the best of our knowl-
edge no parser results have been published on this
corpus. Both corpora are available at www.inf.
u-szeged.hu/rgai/SzegedTreebank.
The multilingual track of the CoNLL-2007
Shared Task (Nivre et al 2007) addressed also
the task of dependency parsing of Hungarian. The
Hungarian corpus used for the shared task con-
sists of automatically converted dependency trees
from the Szeged Constituency Treebank. Several
issues of the automatic conversion tool were re-
considered before the manual annotation of the
Szeged Dependency Treebank was launched and
the annotation guidelines contained instructions
related to linguistic phenomena which could not
be converted from the constituency representa-
tion ? for a detailed discussion, see Vincze et al
(2010). Hence the annotation schemata of the
CoNLL-2007 Hungarian corpus and the Szeged
Dependency Treebank are rather different and the
final scores reported for the former are not di-
rectly comparable with our reported scores here
(see Section 5).
4 The Szeged Dependency Treebank
We utilize the Szeged Dependency Treebank
(Vincze et al 2010) as the basis of our experi-
ments for Hungarian dependency parsing. It con-
tains 82,000 sentences, 1.2 million words and
250,000 punctuation marks from six domains.
The annotation employs 16 coarse grained POS
tags, 95 morphological feature values and 29 de-
pendency labels. 19.6% of the sentences in the
corpus contain non-projective edges and 1.8% of
the edges are non-projective2, which is almost 5
times more frequent than in English and is the
same as the Czech non-projectivity level (Buch-
holz and Marsi, 2006). Here we discuss two an-
notation principles along with our modifications
in the dataset for this study which strongly influ-
ence the parsers? accuracies.
Named Entities (NEs) were treated as one to-
ken in the Szeged Dependency Treebank. Assum-
ing a perfect phrase recogniser on the whitespace
tokenised input for them is quite unrealistic. Thus
we decided to split them into tokens for this study.
The new tokens automatically got a proper noun
with default morphological features morphologi-
cal analysis except for the last token ? the head of
the phrase ?, which inherited the morphological
analysis of the original multiword unit (which can
contain various grammatical information). This
resulted in an N N N N POS sequence for Kova?cs
e?s ta?rsa kft. ?Smith and Co. Ltd.? which would
be annotated as N C N N in the Penn Treebank.
Moreover, we did not annotate any internal struc-
ture of Named Entities. We consider the last word
of multiword named entities as the head because
of morphological reasons (the last word of multi-
word units gets inflected in Hungarian) and all the
previous elements are attached to the succeeding
word, i.e. the penultimate word is attached to the
last word, the antepenultimate word to the penulti-
mate one etc. The reasons for these considerations
are that we believe that there are no downstream
applications which can exploit the information of
the internal structures of Named Entities and we
imagine a pipeline where a Named Entity Recog-
niser precedes the parsing step.
Empty copula: In the verbless clauses (pred-
icative nouns or adjectives) the Szeged Depen-
dency Treebank introduces virtual nodes (16,000
items in the corpus). This solution means that
a similar tree structure is ascribed to the same
sentence in the present third person singular and
all the other tenses / persons. A further argu-
ment for the use of a virtual node is that the vir-
tual node is always present at the syntactic level
2Using the transitive closure definition of Nivre and Nils-
son (2005).
57
corpus Malt MST Mate
ULA LAS ULA LAS ULA LAS
Hungarian
dev 88.3 (89.9) 85.7 (87.9) 86.9 (88.5) 80.9 (82.9) 89.7 (91.1) 86.8 (89.0)
test 88.7 (90.2) 86.1 (88.2) 87.5 (89.0) 81.6 (83.5) 90.1 (91.5) 87.2 (89.4)
English
dev 87.8 (89.1) 84.5 (86.1) 89.4 (91.2) 86.1 (87.7) 91.6 (92.7) 88.5 (90.0)
test 88.8 (89.9) 86.2 (87.6) 90.7 (91.8) 87.7 (89.2) 92.6 (93.4) 90.3 (91.5)
Table 1: Results achieved by the three parsers on the (full) Hungarian (Szeged Dependency Treebank) and
English (CoNLL-2009) datasets. The scores in brackets are achieved with gold-standard POS tagging.
since it is overt in all the other forms, tenses and
moods of the verb. Still, the state-of-the-art de-
pendency parsers cannot handle virtual nodes. For
this study, we followed the solution of the Prague
Dependency Treebank (Hajic? et al 2000) and vir-
tual nodes were removed from the gold standard
annotation and all of their dependents were at-
tached to the head of the original virtual node and
they were given a dedicated edge label (Exd).
Dataset splits: We formed training, develop-
ment and test sets from the corpus where each
set consists of texts from each of the domains.
We paid attention to the issue that a document
should not be separated into different datasets be-
cause it could result in a situation where a part of
the test document was seen in the training dataset
(which is unrealistic because of unknown words,
style and frequently used grammatical structures).
As the fiction subcorpus consists of three books
and the law subcorpus consists of two rules, we
took half of one of the documents for the test
and development sets and used the other part(s)
for training there. This principle was followed at
our cross-fold-validation experiments as well ex-
cept for the law subcorpus. We applied 3 folds for
cross-validation for the fiction subcorpus, other-
wise we used 10 folds (splitting at documentary
boundaries would yield a training fold consisting
of just 3000 sentences).3
5 Experiments
We carried out experiments using three state-of-
the-art parsers on the Szeged Dependency Tree-
bank (Vincze et al 2010) and on the English
datasets of the CoNLL-2009 Shared Task (Hajic?
et al 2009).
3Both the training/development/test and the cross-
validation splits are available at www.inf.u-szeged.
hu/rgai/SzegedTreebank.
Tools: We employed a finite state automata-
based morphological analyser constructed from
the morphdb.hu lexical resource (Tro?n et al
2006) and we used the MSD-style morphological
code system of the Szeged TreeBank (Alexin et
al., 2003). The output of the morphological anal-
yser is a set of possible lemma?morphological
analysis pairs. This set of possible morphologi-
cal analyses for a word form is then used as pos-
sible alternatives ? instead of open and closed tag
sets ? in a standard sequential POS tagger. Here,
we applied the Conditional Random Fields-based
Stanford POS tagger (Toutanova et al 2003) and
carried out 5-fold-cross POS training/tagging in-
side the subcorpora.4 For the English experiments
we used the predicted POS tags provided for the
CoNLL-2009 shared task (Hajic? et al 2009).
As the dependency parser we employed three
state-of-the-art data-driven parsers, a transition-
based parser (Malt) and two graph-based parsers
(MST and Mate parsers). The Malt parser (Nivre
et al 2004) is a transition-based system, which
uses an arc-eager system along with support vec-
tor machines to learn the scoring function for tran-
sitions and which uses greedy, deterministic one-
best search at parsing time. As one of the graph-
based parsers, we employed the MST parser (Mc-
Donald et al 2005) with a second-order feature
decoder. It uses an approximate exhaustive search
for unlabeled parsing, then a separate arc label
classifier is applied to label each arc. The Mate
parser (Bohnet, 2010) is an efficient second or-
der dependency parser that models the interaction
between siblings as well as grandchildren (Car-
reras, 2007). Its decoder works on labeled edges,
i.e. it uses a single-step approach for obtaining
labeled dependency trees. Mate uses a rich and
4The JAVA implementation of the morphological anal-
yser and the slightly modified POS tagger along with trained
models are available at http://www.inf.u-szeged.
hu/rgai/magyarlanc.
58
corpus #sent. length CPOS DPOS ULA all ULA LAS all LAS
newspaper 9189 21.6 97.2 96.5 88.0 (90.0) +0.8 84.7 (87.5) +1.0
short business 8616 23.6 98.0 97.7 93.8 (94.8) +0.3 91.9 (93.4) +0.4
fiction 9279 12.6 96.9 95.8 87.7 (89.4) -0.5 83.7 (86.2) -0.3
law 8347 27.3 98.3 98.1 90.6 (90.7) +0.2 88.9 (89.0) +0.2
computer 8653 21.9 96.4 95.8 91.3 (92.8) -1.2 88.9 (91.2) -1.6
composition 22248 13.7 96.7 95.6 92.7 (93.9) +0.3 88.9 (91.0) +0.3
Table 2: Domain results achieved by the Mate parser in cross-validation settings. The scores in brackets are
achieved with gold-standard POS tagging. The ?all? columns contain the added value of extending the training
sets with each of the five out-domain subcorpora.
well-engineered feature set and it is enhanced by
a Hash Kernel, which leads to higher accuracy.
Evaluation metrics: We apply the Labeled At-
tachment Score (LAS) and Unlabeled Attachment
Score (ULA), taking into account punctuation as
well for evaluating dependency parsers and the
accuracy on the main POS tags (CPOS) and a
fine-grained morphological accuracy (DPOS) for
evaluating the POS tagger. In the latter, the analy-
sis is regarded as correct if the main POS tag and
each of the morphological features of the token in
question are correct.
Results: Table 1 shows the results got by the
parsers on the whole Hungarian corpora and on
the English datasets. The most important point
is that scores are not different from the English
scores (although they are not directly compara-
ble). To understand the reasons for this, we man-
ually investigated the set of firing features with
the highest weights in the Mate parser. Although
the assessment of individual feature contributions
to a particular decoder decision is not straightfor-
ward, we observed that features encoding config-
urational information (i.e. the direction or length
of an edge, the words or POS tag sequences/sets
between the governor and the dependent) were
frequently among the highest weighted features
in English but were extremely rare in Hungarian.
For instance, one of the top weighted features for
a subject dependency in English was the ?there is
no word between the head and the dependent? fea-
ture while this never occurred among the top fea-
tures in Hungarian.
As a control experiment, we trained the Mate
parser only having access to the gold-standard
POS tag sequences of the sentences, i.e. we
switched off the lexicalization and detailed mor-
phological information. The goal of this experi-
ment was to gain an insight into the performance
of the parsers which can only access configura-
tional information. These parsers achieved worse
results than the full parsers by 6.8 ULA, 20.3 LAS
and 2.9 ULA, 6.4 LAS on the development sets
of Hungarian and English, respectively. As ex-
pected, Hungarian suffers much more when the
parser has to learn from configurational informa-
tion only, especially when grammatical functions
have to be predicted (LAS). Despite this, the re-
sults of Table 1 show that the parsers can practi-
cally eliminate this gap by learning from morpho-
logical features (and lexicalization). This means
that the data-driven parsers employing a very rich
feature set can learn a model which effectively
captures the dependency structures using feature
weights which are radically different from the
ones used for English.
Another cause of the relatively high scores is
that the CPOS accuracy scores on Hungarian
and English are almost equal: 97.2 and 97.3, re-
spectively. This also explains the small differ-
ence between the results got by gold-standard and
predicted POS tags. Moreover, the parser can
also exploit the morphological features as input
in Hungarian.
The Mate parser outperformed the other two
parsers on each of the four datasets. Comparing
the two graph-based parsers Mate and MST, the
gap between them was twice as big in LAS than in
ULA in Hungarian, which demonstrates that the
one-step approach looking for the maximum
labeled spanning tree is more suitable for Hun-
garian than the two-step arc labeling approach of
MST. This probably holds for other morpholog-
ically rich languages too as the decoder can ex-
ploit information from the labels of decoded arcs.
Based on these results, we decided to use only
Mate for our further experiments.
59
Table 2 provides an insight into the effect of
domain differences on POS tagging and pars-
ing scores. There is a noticeable difference be-
tween the ?newspaper? and the ?short business
news? corpora. Although these domains seem to
be close to each other at the first glance (both are
news), they have different characteristics. On the
one hand, short business news is a very narrow
domain consisting of 2-3 sentence long financial
short reports. It frequently uses the same gram-
matical structures (like ?Stock indexes rose X per-
cent at the Y Stock on Wednesday?) and the lexi-
con is also limited. On the other hand, the news-
paper subcorpus consists of full journal articles
covering various domains and it has a fancy jour-
nalist style.
The effect of extending the training dataset with
out-of-domain parses is not convincing. In spite
of the ten times bigger training datasets, there
are two subcorpora where they just harmed the
parser, and the improvement on other subcorpora
is less than 1 percent. This demonstrates well the
domain-dependence of parsing.
The parser and the POS tagger react to do-
main difficulties in a similar way, according to
the first four rows of Table 2. This observation
holds for the scores of the parsers working with
gold-standard POS tags, which suggests that do-
main difficulties harm POS tagging and parsing as
well. Regarding the two last subcorpora, the com-
positions consist of very short and usually simple
sentences and the training corpora are twice as big
compared with other subcorpora. Both factors are
probably the reasons for the good parsing perfor-
mance. In the computer corpus, there are many
English terms which are manually tagged with an
?unknown? tag. They could not be accurately pre-
dicted by the POS tagger but the parser could pre-
dict their syntactic role.
Table 2 also tells us that the difference between
CPOS and DPOS is usually less than 1 percent.
This experimentally supports that the ambigu-
ity among alternative morphological analyses
is mostly present at the POS-level and the mor-
phological features are efficiently identified by
our morphological analyser. The most frequent
morphological features which cannot be disam-
biguated at the word level are related to suffixes
with multiple functions or the word itself cannot
be unambiguously segmented into morphemes.
Although the number of such ambiguous cases is
low, they form important features for the parser,
thus we will focus on the more accurate handling
of these cases in future work.
Comparison to CoNLL-2007 results: The
best performing participant of the CoNLL-2007
Shared Task (Nivre et al 2007) achieved an ULA
of 83.6 and LAS of 80.3 (Hall et al 2007) on
the Hungarian corpus. The difference between the
top performing English and Hungarian systems
were 8.14 ULA and 9.3 LAS. The results reported
in 2007 were significantly lower and the gap be-
tween English and Hungarian is higher than our
current values. To locate the sources of difference
we carried out other experiments with Mate on
the CoNLL-2007 dataset using the gold-standard
POS tags (the shared task used gold-standard POS
tags for evaluation).
First we trained and evaluated Mate on the
original CoNLL-2007 datasets, where it achieved
ULA 84.3 and LAS 80.0. Then we used the sen-
tences of the CoNLL-2007 datasets but with the
new, manual annotation. Here, Mate achieved
ULA 88.6 and LAS 85.5, which means that the
modified annotation schema and the less erro-
neous/noisy annotation caused an improvement of
ULA 4.3 and LAS 5.5. The annotation schema
changed a lot: coordination had to be corrected
manually since it is treated differently after con-
version, moreover, the internal structure of ad-
jectival/participial phrases was not marked in the
original constituency treebank, so it was also
added manually (Vincze et al 2010). The im-
provement in the labeled attachment score is prob-
ably due to the reduction of the label set (from 49
to 29 labels), which step was justified by the fact
that some morphosyntactic information was dou-
bly coded in the case of nouns (e.g. ha?zzal (house-
INS) ?with the/a house?) in the original CoNLL-
2007 dataset ? first, by their morphological case
(Cas=ins) and second, by their dependency label
(INS).
Lastly, as the CoNLL-2007 sentences came
from the newspaper subcorpus, we can compare
these scores with the ULA 90.0 and LAS 87.5
of Table 2. The ULA 1.5 and LAS 2.0 differ-
ences are the result of the bigger training corpus
(9189 sentences on average compared to 6390 in
the CoNLL-2007 dataset).
60
Hungarian English
label attachment label attachment
virtual nodes 31.5% 39.5% multiword NEs 15.2% 17.6%
conjunctions and negation ? 11.2% PP-attachment ? 15.9%
noun attachment ? 9.6% non-canonical word order 6.4% 6.5%
more than 1 premodifier ? 5.1% misplaced clause ? 9.7%
coordination 13.5% 16.5% coordination 8.5% 12.5%
mislabeled adverb 16.3% ? mislabeled adverb 40.1% ?
annotation errors 10.7% 6.8% annotation errors 9.7% 8.5%
other 28.0% 11.3% other 20.1% 29.3%
TOTAL 100% 100% TOTAL 100% 100%
Table 3: The most frequent corpus-specific and general attachment and labeling error categories (based on a
manual investigation of 200?200 erroneous sentences).
6 A Systematic Error Analysis
In order to discover specialties and challenges of
Hungarian dependency parsing, we conducted an
error analysis of parsed texts from the newspaper
domain both in English and Hungarian. 200 ran-
domly selected erroneous sentences from the out-
put of Mate were investigated in both languages
and we categorized the errors on the basis of the
linguistic phenomenon responsible for the errors
? for instance, when an error occurred because of
the incorrect identification of a multiword Named
Entity containing a conjunction, we treated it as
a Named Entity error instead of a conjunction er-
ror ?, i.e. our goal was to reveal the real linguistic
sources of errors rather than deducing from auto-
matically countable attachment/labeling statistics.
We used the parses based on gold-standard
POS tagging for this analysis as our goal was to
identify the challenges of parsing independently
of the challenges of POS tagging. The error cate-
gories are summarized in Table 3 along with their
relative contribution to attachment and labeling
errors. This table contains the categories with
over 5% relative frequency.5
The 200 sentences contained 429/319 and
353/330 attachment/labeling errors in Hungarian
and English, respectively. In Hungarian, attach-
ment errors outnumber label errors to a great ex-
tent whereas in English, their distribution is basi-
cally the same. This might be attributed to the
higher level of non-projectivity (see Section 4)
and to the more fine-grained label set of the En-
glish dataset (36 against 29 labels in English and
5The full tables are available at www.inf.u-szeged.
hu/rgai/SzegedTreebank.
Hungarian, respectively).
Virtual nodes: In Hungarian, the most common
source of parsing errors was virtual nodes. As
there are quite a lot of verbless clauses in Hungar-
ian (see Section 2 on sentences without copula), it
might be difficult to figure out the proper depen-
dency relations within the sentence, since the verb
plays the central role in the sentence, cf. Tesnie`re
(1959). Our parser was not efficient in identify-
ing the structure of such sentences, probably due
to the lack of information for data-driven parsers
(each edge is labeled as Exd while they have sim-
ilar features to ordinary edges). We also note that
the output of the current system with Exd labels
does not contain too much information for down-
stream applications of parsing. The appropriate
handling of virtual nodes is an important direction
for future work.
Noun attachment: In Hungarian, the nomi-
nal arguments of infinitives and participles were
frequently erroneously attached to the main
verb. Take the following sentence: A Horn-
kabinet ideje?n jo?l beva?lt mo?dszerhez pro?ba?lnak
meg visszate?rni (the Horn-government time-
3SGPOSS-SUP well tried method-ALL try-3PL
PREVERB return-INF) ?They are trying to return
to the well-tried method of the Horn government?.
In this sentence, a Horn-kabinet ideje?n ?during
the Horn government? is a modifier of the past
participle beva?lt ?well-tried?, however, it is at-
tached to the main verb pro?ba?lnak ?they are try-
ing? by the parser. Moreover, mo?dszerhez ?to the
method? is an argument of the infinitive visszate?r-
ni ?to return?, but the parser links it to the main
61
verb. In free word order languages, the order of
the arguments of the infinitive and the main verb
may get mixed, which is called scrambling (Ross,
1986). This is not a common source of error in
English as arguments cannot scramble.
Article attachment: In Hungarian, if there is
an article before a prenominal modifier, it can be-
long to the head noun and to the modifier as well.
In a szoba ajtaja (the room door-3SGPOSS) ?the
door of the room? the article belongs to the modi-
fier but when the prenominal modifier cannot have
an article (e.g. a februa?rban indulo? projekt (the
February-INE starting project) ?the project start-
ing in February?), it is attached to the head noun
(i.e. to projekt ?project?). It was not always clear
for the parser which parent to select for the arti-
cle. In contrast, these cases are not problematic
in English since the modifier typically follows the
head and thus each article precedes its head noun.
Conjunctions or negation words ? most typ-
ically the words is ?too?, csak ?only/just? and
nem/sem ?not? ? were much more frequently at-
tached to the wrong node in Hungarian than in
English. In Hungarian, they are ambiguous be-
tween being adverbs and conjunctions and it is
mostly their conjunctive uses which are problem-
atic from the viewpoint of parsing. On the other
hand, these words have an important role in mark-
ing the information structure of the sentence: they
are usually attached to the element in focus posi-
tion, and if there is no focus, they are attached
to the verb. However, sentences with or with-
out focus can have similar word order but their
stress pattern is different. Dependency parsers
obviously cannot recognize stress patterns, hence
conjunctions and negation words are sometimes
erroneously attached to the verb in Hungarian.
English sentences with non-canonical word
order (e.g. questions) were often incorrectly
parsed, e.g. the noun following the main verb is
the object in sentences like Replied a salesman:
?Exactly.?, where it is the subject that follows the
verb for stylistic reasons. However, in Hungarian,
morphological information is of help in such sen-
tences, as it is not the position relative to the verb
but the case suffix that determines the grammati-
cal role of the noun.
In English, high or low PP-attachment was
responsible for many parsing ambiguities: most
typically, the prepositional complement which
follows the head was attached to the verb instead
of the noun or vice versa. In contrast, Hungarian
is a head-after-dependent language, which means
that dependents most often occur before the head.
Furthermore, there are no prepositions in Hungar-
ian, and grammatical relations encoded by prepo-
sitions in English are conveyed by suffixes or
postpositions. Thus, if there is a modifier before
the nominal head, it requires the presence of a
participle as in: Felvette a kirakatban levo? ruha?t
(take.on-PAST3SGOBJ the shop.window-INE be-
ing dress-ACC) ?She put on the dress in the shop
window?. The English sentence is ambiguous (ei-
ther the event happens in the shop window or the
dress was originally in the shop window) while
the Hungarian has only the latter meaning.6
General dependency parsing difficulties:
There were certain structures that led to typical
label and/or attachment errors in both languages.
The most frequent one among them is coordi-
nation. However, it should be mentioned that
syntactic ambiguities are often problematic even
for humans to disambiguate without contextual
or background semantic knowledge.
In the case of label errors, the relation between
the given node and its parent was labeled incor-
rectly. In both English and Hungarian, one of the
most common errors of this type was mislabeled
adverbs and adverbial phrases, e.g. locative ad-
verbs were labeled as ADV/MODE. However, the
frequency rate of this error type is much higher
in English than in Hungarian, which may be re-
lated to the fact that in the English corpus, there
is a much more balanced distribution of adverbial
labels than in the Hungarian one (where the cat-
egories MODE and TLOCY are responsible for
90% of the occurrences). Assigning the most fre-
quent label of the training dataset to each adverb
yields an accuracy of 82% in English and 93% in
Hungarian, which suggests that there is a higher
level of ambiguity for English adverbial phrases.
For instance, the preposition by may introduce an
adverbial modifier of manner (MNR) in by cre-
ating a bill and the agent in a passive sentence
(LGS). Thus, labeling adverbs seems to be a more
6However, there exists a head-before-dependent version
of the sentence (Felvette a ruha?t a kirakatban), whose pre-
ferred reading is ?She was in the shop window while dressing
up?, that is, the modifier belongs to the verb.
62
difficult task in English.7
Clauses were also often mislabeled in both lan-
guages, most typically when there was no overt
conjunction between clauses. Another source of
error was when more than one modifier occurred
before a noun (5.1% and 4.2% of attachment er-
rors in Hungarian and in English): in these cases,
the first modifier could belong to the noun (a
brown Japanese car) or to the second modifier (a
brown haired girl).
Multiword Named Entities: As we mentioned
in Section 4, members of multiword Named Enti-
ties had a proper noun POS-tag and an NE label
in our dataset. Hence when parsing is based on
gold standard POS-tags, their recognition is al-
most perfect while it is a frequent source or er-
rors in the CoNLL-2009 corpus. We investigated
the parse of our 200 sentences with predicted POS
tags at NEs and found that this introduces several
errors (about 5% of both attachment and labeling
errors) in Hungarian. On the other hand, the re-
sults are only slightly worse in English, i.e. iden-
tifying the inner structure of NEs does not depend
on whether the parser builds on gold standard or
predicted POS-tags since function words like con-
junctions or prepositions ? which mark grammat-
ical relations ? are tagged in the same way in both
cases. The relative frequency of this error type is
much higher in English even when the Hungar-
ian parser does not have access to the gold proper
noun POS tags. The reason for this is simple: in
the Penn Treebank the correct internal structure of
the NEs has to be identified beyond the ?phrase
boundaries? while in Hungarian their members
just form a chain.
Annotation errors: We note that our analysis
took into account only sentences which contained
at least one parsing error and we crawled only
the dependencies where the gold standard anno-
tation and the output of the parser did not match.
Hence, the frequency of annotation errors is prob-
ably higher than we found (about 1% of the en-
tire set of dependencies) during our investigation
as there could be annotation errors in the ?error-
free? sentences and also in the investigated sen-
tences where the parser agrees with that error.
7We would nevertheless like to point out that adverbial
labels have a highly semantic nature, i.e. it could be argued
that it is not the syntactic parser that should identify them but
a semantic processor.
7 Conclusions
We showed that state-of-the-art dependency
parsers achieve similar results ? in terms of at-
tachment scores ? on Hungarian and English. Al-
though the results with this comparison should be
taken with a pinch of salt ? as sentence lengths
(and information encoded in single words) differ,
domain differences and annotation schema diver-
gences are uncatchable ? we conclude that parsing
Hungarian is just as hard a task as parsing English.
We argued that this is due to the relatively good
POS tagging accuracy (which is a consequence
of the low ambiguity of alternative morphological
analyses of a sentence and the good coverage of
the morphological analyser) and the fact that data-
driven dependency parsers employ a rich feature
representation which enables them to learn differ-
ent kinds of feature weight profiles.
We also discussed the domain differences
among the subcorpora of the Szeged Dependency
Treebank and their effect on parsing results. Our
results support that there can be higher differences
in parsing scores among domains in one language
than among corpora from a similar domain but
different languages (which again marks pitfalls of
inter-language comparison of parsing scores).
Our systematic error analysis showed that han-
dling the virtual nodes (mostly empty copula) is
a frequent source of errors. We identified several
phenomena which are not typically listed as Hun-
garian syntax-specific features but are challeng-
ing for the current data-driven parsers, however,
they are not problematic in English (like the at-
tachment of conjunctions and negation words and
the attachment problem of nouns and articles).
We concluded ? based on our quantitative analy-
sis ? that a further notable error reduction is only
achievable if distinctive attention is paid to these
language-specific phenomena.
We intend to investigate the problem of vir-
tual nodes in dependency parsing in more depth
and to implement new feature templates for the
Hungarian-specific challenges as future work.
Acknowledgments
This work was supported in part by the Deutsche
Forschungsgemeinschaft grant SFB 732 and the
NIH grant (project codename MASZEKER) of
the Hungarian government.
63
References
Zolta?n Alexin, Ja?nos Csirik, Tibor Gyimo?thy, Ka?roly
Bibok, Csaba Hatvani, Ga?bor Pro?sze?ky, and La?szlo?
Tihanyi. 2003. Annotated Hungarian National Cor-
pus. In Proceedings of the EACL, pages 53?56.
Anna Babarczy, Ba?lint Ga?bor, Ga?bor Hamp, and
Andra?s Rung. 2005. Hunpars: a rule-based sen-
tence parser for Hungarian. In Proceedings of the
6th International Symposium on Computational In-
telligence.
Csongor Barta, Do?ra Csendes, Ja?nos Csirik, Andra?s
Ho?cza, Andra?s Kocsor, and Korne?l Kova?cs. 2005.
Learning syntactic tree patterns from a balanced
Hungarian natural language database, the Szeged
Treebank. In Proceedings of 2005 IEEE Interna-
tional Conference on Natural Language Processing
and Knowledge Engineering, pages 225 ? 231.
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (Coling 2010), pages 89?97.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
Shared Task on Multilingual Dependency Parsing.
In Proceedings of the Tenth Conference on Com-
putational Natural Language Learning (CoNLL-X),
pages 149?164.
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proceed-
ings of the CoNLL Shared Task Session of EMNLP-
CoNLL 2007, pages 957?961.
Do?ra Csendes, Ja?nos Csirik, Tibor Gyimo?thy, and
Andra?s Kocsor. 2005. The Szeged Treebank. In
TSD, pages 123?131.
Katalin E?. Kiss. 2002. The Syntax of Hungarian.
Cambridge University Press, Cambridge.
Jason M. Eisner. 1996. Three new probabilistic mod-
els for dependency parsing: an exploration. In Pro-
ceedings of the 16th conference on Computational
linguistics - Volume 1, COLING ?96, pages 340?
345.
Richa?rd Farkas, Da?niel Szeredi, Da?niel Varga, and
Veronika Vincze. 2010. MSD-KR harmoniza?cio? a
Szeged Treebank 2.5-ben [Harmonizing MSD and
KR codes in the Szeged Treebank 2.5]. In VII. Ma-
gyar Sza?m??to?ge?pes Nyelve?szeti Konferencia, pages
349?353.
Jan Hajic?, Alena Bo?hmova?, Eva Hajic?ova?, and Barbora
Vidova?-Hladka?. 2000. The Prague Dependency
Treebank: A Three-Level Annotation Scenario. In
Anne Abeille?, editor, Treebanks: Building and
Using Parsed Corpora, pages 103?127. Amster-
dam:Kluwer.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 Shared Task: Syntactic and Semantic Depen-
dencies in Multiple Languages. In Proceedings of
the Thirteenth Conference on Computational Nat-
ural Language Learning (CoNLL 2009): Shared
Task, pages 1?18.
Johan Hall, Jens Nilsson, Joakim Nivre, Gu?lsen
Eryigit, Bea?ta Megyesi, Mattias Nilsson, and
Markus Saers. 2007. Single Malt or Blended?
A Study in Multilingual Parser Optimization. In
Proceedings of the CoNLL Shared Task Session of
EMNLP-CoNLL 2007, pages 933?939.
Szila?rd Iva?n, Ro?bert Orma?ndi, and Andra?s Kocsor.
2007. Magyar mondatok SVM alapu? szintaxis
elemze?se [SVM-based syntactic parsing of Hun-
garian sentences]. In V. Magyar Sza?m??to?ge?pes
Nyelve?szeti Konferencia, pages 281?283.
Taku Kudo and Yuji Matsumoto. 2002. Japanese
dependency analysis using cascaded chunking. In
Proceedings of the 6th Conference on Natural Lan-
guage Learning - Volume 20, COLING-02, pages
1?7.
Ryan McDonald and Joakim Nivre. 2011. Analyzing
and integrating dependency parsers. Computational
Linguistics, 37:197?230.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-Projective Dependency Pars-
ing using Spanning Tree Algorithms. In Proceed-
ings of Human Language Technology Conference
and Conference on Empirical Methods in Natural
Language Processing, pages 523?530.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-
Projective Dependency Parsing. In Proceedings
of the 43rd Annual Meeting of the Association
for Computational Linguistics (ACL?05), pages 99?
106.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-Based Dependency Parsing. In HLT-
NAACL 2004 Workshop: Eighth Conference
on Computational Natural Language Learning
(CoNLL-2004), pages 49?56.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 Shared Task
on Dependency Parsing. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL
2007, pages 915?932.
Ga?bor Pro?sze?ky, La?szlo? Tihanyi, and Ga?bor L. Ugray.
2004. Moose: A Robust High-Performance Parser
and Generator. In Proceedings of the 9th Workshop
of the European Association for Machine Transla-
tion.
John R. Ross. 1986. Infinite syntax! ABLEX, Nor-
wood, NJ.
Lucien Tesnie`re. 1959. E?le?ments de syntaxe struc-
turale. Klincksieck, Paris.
64
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-
of-speech tagging with a cyclic dependency net-
work. In Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology - Volume 1, pages 173?180.
Viktor Tro?n, Pe?ter Hala?csy, Pe?ter Rebrus, Andra?s
Rung, Eszter Simon, and Pe?ter Vajda. 2006. Mor-
phdb.hu: Hungarian lexical database and morpho-
logical grammar. In Proceedings of 5th Inter-
national Conference on Language Resources and
Evaluation (LREC ?06).
Da?niel Varga, Pe?ter Hala?csy, Andra?s Kornai, Viktor
Nagy, La?szlo? Ne?meth, and Viktor Tro?n. 2005. Par-
allel corpora for medium density languages. In Pro-
ceedings of the RANLP, pages 590?596.
Veronika Vincze, Do?ra Szauter, Attila Alma?si, Gyo?rgy
Mo?ra, Zolta?n Alexin, and Ja?nos Csirik. 2010. Hun-
garian Dependency Treebank. In Proceedings of the
Seventh Conference on International Language Re-
sources and Evaluation (LREC?10).
65
Knowledge Sources for Constituent Parsing
of German, a Morphologically Rich and
Less-Configurational Language
Alexander Fraser?
Institute for NLP, University of Stuttgart
Helmut Schmid??
Institute for NLP, University of Stuttgart
Richa?rd Farkas?
Institute for NLP, University of Stuttgart
Renjing Wang?
Institute for NLP, University of Stuttgart
Hinrich Schu?tze?
Institute for NLP, University of Stuttgart
We study constituent parsing of German, a morphologically rich and less-configurational
language. We use a probabilistic context-free grammar treebank grammar that has been adapted
to the morphologically rich properties of German by markovization and special features added
to its productions. We evaluate the impact of adding lexical knowledge. Then we examine both
monolingual and bilingual approaches to parse reranking. Our reranking parser is the new state
of the art in constituency parsing of the TIGER Treebank. We perform an analysis, concluding
with lessons learned, which apply to parsing other morphologically rich and less-configurational
languages.
? Institute for Natural Language Processing, University of Stuttgart, Pfaffenwaldring 5b, 70569 Stuttgart,
Germany. E-mail: fraser@ims.uni-stuttgart.de.
?? Institute for Natural Language Processing, University of Stuttgart, Pfaffenwaldring 5b, 70569 Stuttgart,
Germany. E-mail: schmid@ims.uni-stuttgart.de.
? Institute for Natural Language Processing, University of Stuttgart, Pfaffenwaldring 5b, 70569 Stuttgart,
Germany. E-mail: farkas@ims.uni-stuttgart.de.
? Institute for Natural Language Processing, University of Stuttgart, Pfaffenwaldring 5b, 70569 Stuttgart,
Germany.
? Institute for Natural Language Processing, University of Stuttgart, Pfaffenwaldring 5b, 70569 Stuttgart,
Germany.
Submission received: October 1, 2011; revised submission received: May 30, 2012; accepted for publication:
August 3, 2012
? 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 1
1. Introduction
A large part of the methodology for parsing in natural language processing has been
developed for English and a majority of publications on parsing are about parsing
of English. English is a strongly configurational language. Nearly all of the syntactic
information needed by anyNLP application can be obtained by configurational analysis
(e.g., by having a correct constituent parse).
Many other languages of the world are fundamentally different from English in this
respect. At the other end of the configurational?nonconfigurational spectrum we find a
language like Hungarian that has very little fixed structure on the level of the sentence.
Leaving aside the issue of the internal structure of NPs, most sentence-level syntactic
information in Hungarian is conveyed by morphology, not by configuration.
In this paper, we address German, a third type of language that is intermediate
between English and Hungarian. German has strong configurational constraints (e.g.,
main clauses are verb-second) as well as rich derivational and inflectional morphology,
all of which must be modeled for high-quality parsing. German?s intermediate status
raises a number of interesting issues in parsing that are of particular prominence for a
mixed configurational/morphological language, but are?as we will argue?of general
relevance for morphologically rich languages. Partly this is the case because there are
few (if any) languages archetypical of being purely configurational and purely noncon-
figurational (e.g., morphology is also important for English and even Hungarian has
configurational constraints). For lack of a better termwe refer to intermediate languages
as typified by German as MR&LC for morphologically rich and less-configurational.
Part of the motivation for this special issue is that most work on parsing to date
has been done on English, a morphologically simple language. As computational lin-
guistics broadens its focus beyond English it becomes important to take a more general
approach to parsing that can handle languages that are typologically very different from
English. Rich morphology (RM) is one very salient characteristic of a language that
affects the design of parsing methods. We argue that there are two other properties
of languages that are relevant in a discussion of parsing RM languages: syncretism
and configurationality. These two properties are correlated typologically with RM and
should therefore be taken into account when we address parsing RM languages.1
We first define the three properties and explain their relevance for parsing. The
large number of languages for which this correlation holds can be ordered along a
single dimension that can be interpreted as degree of morphological complexity. We
give examples for a number of languages that are positioned at different points on this
scale. Finally, we argue that just as languages that are at the opposite end of the spectrum
from English (prototypical examples of morphological richness like Hungarian) require
parsing methods that can be quite different from those optimal for English, the same
is true for a language like German that is in the middle of the spectrum?and what is
required is in some respects different from what is optimal for one extreme (English) or
the other (Hungarian).
The three correlated properties are rich morphology, syncretism, and configura-
tionality. Morphological richness can be roughly measured by the number of different
morphological forms a word of a particular syntactic category can have; for example,
1 We note, however, that this relationship is not a language universal. It is instead a frequently observed
correlation; for Chinese, for instance, the correlation does not seem to hold as strongly.
58
Fraser et al Knowledge Sources for Parsing German
a typical English noun has two forms (singular and plural), a typical German noun
has eight forms (singular and plural in four different cases), and a typical Hungarian
noun has several hundreds of forms. Syncretism refers to the fact that different mor-
phological forms have identical surface realization; for example, the formMann (?man?
in German) can be the nominative, dative, or accusative singular of Mann depending
on context. Configurationality refers to the degree to which the arrangement of words
and phrases of a particular syntactic function in a sentence is fixed. English is highly
configurational: it has limited flexibility in how the major phrases in a sentence (subject,
verb, direct object, indirect object, etc.) can be ordered. Hungarian and Latin are highly
flexible: Even though there are pragmatic constraints, in principle a large number of
possible orderings are grammatical. German is less configurational. It has some strict
constraints (verb second in main clauses, verb final in subordinate clauses), but also
some properties of a nonconfigurational language; for example, ordering of phrases
in the mittelfeld (the part of the main clause enclosed by the two parts of the verbal
complex) is very flexible.
It is obvious why configurationality and rich morphology are typologically (neg-
atively) correlated. Rich morphology specifies the syntactic role of a phrase in the
sentence, so fixing a position is not required, and many morphologically rich languages
therefore do not fix the position. Conversely, simple morphology gives little specific
information about the role of words and phrases in the sentence. One device often used
by morphologically simple languages to address this problem and reduce widespread
ambiguity is to fix the order of words and phrases in the sentence.
Syncretism has an effect that is similar to simplification of complex morphology.
Simple morphology is unspecific about grammatical function because it uses a small
number of morphological categories. Syncretism is unspecific about grammatical func-
tion because it suffers from a high degree of ambiguity. Even though the number of
different morphological categories is potentially large, syncretic forms conflate many of
these categories, so that these forms are much less helpful in determining grammatical
function than forms in a nonsyncretic language with the same number of categories.
Again, to counteract the communicative difficulties that lack of morphological speci-
ficity would create, stricter constraints on ordering and configuration are often used by
syncretic languages.
We have used English and Hungarian as examples for the extremes and German
for the middle of the spectrum. We now give examples of other languages and their
positions on the scale. Dutch is similar to German in that it also is verb second in main
clauses and verb final in subordinate clauses. The order of arguments in the mittelfeld is
much more restricted than in German, however. At the same time, Dutch morphology
has been much more simplified in the last centuries than German morphology. This
nicely confirms the correlation between RM and configurationality. Thus, Dutch is
positioned between English and German on the scale.
Classical Arabic is somewhat similar to German: The number of different morpho-
logical forms is roughly comparable to German and it allows a number of different
word orders. Modern Standard Arabic speakers rarely mark case, however, at least not
in spontaneous speech. At the same time, Modern Standard Arabic speakers use SVO
order much more frequently and consistently than is the case in Classical Arabic. Thus,
Classical Arabic is roughly at the same position as German on the scale whereas spoken
Modern Standard Arabic may be more comparable to Dutch.
Finally, Modern Greek is a language that is intermediate between German and
Hungarian. It has richer morphology than German, but it has a fair amount of syn-
cretism and therefore more morphological ambiguity than Hungarian. SVO is the
59
Computational Linguistics Volume 39, Number 1
predominant word order in modern Greek, but other word orders can be used. The
order within the noun phrase is more flexible than in German: Adjectives can precede
or follow the noun.
In the examples we have given, the amount of information conveyed by a mor-
phological form is negatively correlated with the amount of information conveyed by
configuration. If morphology conveys a lot of information (due to a large number of
distinctions and the lack of syncretism), then word order is freer and conveys less
information. If morphology conveys less information (due to fewer distinctions or more
syncretism), then configuration is fixed and provides more information to the speaker.
This suggests that RM and configuration are important variables that should be taken
into account in the design of parsing methods. In addition to looking at the extremes of
the spectrum that are exemplified by English andHungarian, we should also investigate
the middle: morphologically (somewhat) rich languages that are less configurational. In
this article, we look at the example of German.
One key question for MR&LC parsing is which type of parsing formalism to adopt,
constituency or dependency. It is a widely held belief that dependency structures are
better suited to represent syntactic analyses for morphologically rich languages because
they allow non-projective structures (the equivalent of discontinuous constituents in
constituency parsing). As Tsarfaty et al (2010) point out, however, this is not the same
as proving that dependency parsers function better than constituency parsers for pars-
ing morphologically rich languages. In fact, most state-of-the-art dependency parsers
(McDonald and Pereira 2006; Hall and Nivre 2008; Seeker et al 2010a) generate purely
projective dependency structures that are optionally transformed into non-projective
structures in a post-processing step. Comparable post-processing techniques have been
used in English constituency parsing (Gabbard, Marcus, and Kulick 2006; Schmid 2006;
Cai, Chiang, and Goldberg 2011) to identify discontinuous constituents andmight work
for other languages, as well.
The overview paper of the Parsing German Shared Task (Ku?bler 2008) reports
higher accuracies for detecting grammatical functions with dependency parsers than
with constituent parsers, but the direct comparison is not fair as it required phrase
boundaries to be correct on the constituent side while the tokens were the unit of
evaluation on the dependency side.2 How to carry out an absolutely fair comparison
of the two representations is still an open research question.3
Constituent parses often provide more information than dependency parses. An
example is the coordination ambiguity in old men and women versus old men and children.
The correct constituent parse for the first expression contains a coordination at the
noun level whereas the parse for the second expression coordinates at the level of
NPs. The dependency structures of both expressions, on the other hand, are usually
identical and thus unable to reflect the fact that oldmodifies women but not children. It is
possible, in principle, to encode the difference in dependency trees (cf. Rambow 2010),
2 This is due to how the evalb tool used to calculate PARSEVAL works. If a constituent is not perfectly
matched, the grammatical function is considered to be wrong, even if there was a partial match (at the
token level). This is not a problem with dependency-based evaluation. For further discussion of the
PARSEVAL metric and dependency-based evaluation see, for example, Rehbein and van Genabith (2007)
and Tsarfaty, Nivre, and Andersson (2012).
3 Two possible solutions are to use TedEval (Tsarfaty, Nivre, and Andersson 2012), or to conduct an analysis
of grammatical functions at the token level in a consistent fashion for both dependency and constituent
parsers. In our case, the latter would require a high quality conversion from the Tiger constituency
representation to a dependency representation, which we hope to implement in future work.
60
Fraser et al Knowledge Sources for Parsing German
for example, by enriching the edge labels, but the constituent representation is simpler
for this phenomenon.
Finally, there are some applications that need constituent parses rather than depen-
dency parses. For instance, many hierarchical statistical machine translation systems
use constituency parses, requiring the output of a dependency parser to be transformed
into a constituent parse.4 We conclude that there is no clear evidence for preferring
dependency parsing over constituency parsing in analyzing languages with RM and
instead argue that research in both frameworks is important.
We view the detailed description of a constituency parsing system for a mor-
phologically rich language, a system that addresses the major problems that arise in
constituency parsing for MR&LC, as one of our main contributions in this paper.
The first problem we address is the proliferation of phrase structure rules in
MR&LC languages. For example, there are a large number of possible orderings of the
phrases in the German mittelfeld, and many orderings are exceedingly rare. A standard
constituency parser cannot estimate probabilities for the corresponding rules reliably.
The solution we adopt here is markovization?complex rules are decomposed
into small unidirectional rules that can be modeled and estimated more reliably than
complex rules. Although markovization in itself is not new, we stress its importance for
MR&LC languages here and present a detailed, reproducible account of how we use it
for German. Markovization combines the best of both worlds for MR&LC languages:
Preferential configurational information can be formalized and exploited by the parser
without incurring too large of a performance penalty due to sparse data problems.
The second problem that needs to be addressed in parsingmanyMR&LC languages
is widespread syncretism. We mainly address syncretism by using a high performance
finite-state automata-based morphological analyzer. Such an analyzer is of obvious
importance for any morphologically rich language because the productivity of mor-
phologically rich languages significantly increases the unknown-word rate in new text
versus morphologically poor languages. So the parser cannot simply memorize the
grammatical properties of words in the Treebank used for training. Instead we incorpo-
rate a complex guesser into our parser that, based on the input from the morphological
analyzer, predicts the grammatical properties of new words and (equally important)
unobserved grammatical properties of known words. With prevailing syncretism, this
task is muchmore complex than in a language where case, gender, number, and so forth,
can be deterministically deduced from morphology.
The morphological analyzer is based on (i) a finite state formalization of German
morphology and (ii) a large lexicon of morphologically analyzed German words. We
refer to these two components together as lexical knowledge. We show that lexical
knowledge is beneficial for parsing performance for an MR&LC language like German.
In addition to lexical knowledge, there is a second important aspect of syncretism
that needs to be addressed in MR&LC languages. Syntactic disambiguation in these
languages must always involve both systems of grammatical encoding, morphology
and configuration, acting together. The most natural way of doing this in a language
like German is to perform this integration of the two knowledge sources directly as part
of parsing.We do this by annotating constituent labels with grammatical functionwhere
appropriate. In contrast with syntactic parses of strongly configurational languages
like English, syntactic parses of German are not useful for most tasks without having
4 We do note, however, that there are a few translation systems which use a dependency representation
directly (e.g., Quirk, Menezes, and Cherry 2005; Shen, Xu, and Weischedel 2008; Tu et al 2010).
61
Computational Linguistics Volume 39, Number 1
grammatical functions indicated. It is not even possible to access the basic subcatego-
rization of the verb (such as determining the subject) without grammatical functions.
We argue that MR&LC languages like German should always be evaluated on labels-
cum-grammatical-function.
Our last main contribution in this paper concerns the fact that we believe that
MR&LC languages give rise to more ambiguity than languages that are predominantly
configurational or morphological. As an example consider the German sentence ?Die
[the] Katze [cat] jagt [hunts] die [the] Schlange [snake].? In German either the cat or the
snake can be the hunter. This type of ambiguity neither occurs in a strongly configu-
rational language like English (where configuration determines grammatical function)
nor in a morphologically rich language like Hungarian that has no or little syncretism
(where morphology determines grammatical function). Although morphology and
configuration in MR&LC languages often work hand in hand for complete disambigua-
tion, there are also many sentences where neither of the two provides the necessary
information for disambiguation. We believe that this distinguishing characteristic of
MR&LC languages makes it necessary to tap additional knowledge sources. In this
paper, we look at two such knowledge sources: monolingual reranking (which captures
global properties of well-formed parses for additional disambiguation) and bilingual
reranking (which exploits parallel text in a different language for disambiguation).
For monolingual reranking, we define a novel set of rich features based on sub-
categorization frames. We compare our compact feature set with a sparse feature set
designed for German previously by Versley and Rehbein (2009). We show that the
richer subcategorization-based framework for monolingual reranking is effective; it has
comparable performance to the sparse feature set?moreover, they complement each
other.
For bilingual reranking, we present our approach to bitext parsing, where a German
parse is found that minimizes syntactic divergence with an automatically generated
parse of its English translation. We pursue this approach for a number of reasons. First,
one limiting factor for syntactic approaches to statistical machine translation is parse
quality (Quirk and Corston-Oliver 2006). Improved parses of bitext should result in
improved machine translation. Second, as more and more texts are available in several
languages, it will be increasingly the case that a text to be parsed is itself part of a
bitext. Third, we hope that the improved parses of bitext can serve as higher quality
training data for improving monolingual parsing using a process similar to self-training
(McClosky, Charniak, and Johnson 2006a).
We show that the three different knowledge sources we use in this paper (lexical
knowledge, monolingual features, and bilingual features) are valuable separately. We
also show that the gain of the two sets of reranking features (monolingual and bilingual)
is additive, suggesting that they capture different types of information.
The resulting parser is currently the best constituent parser for German (with or
without bilingual features). In particular, we show that the baseline parser without
reranking is competitive with the previous state of the art (the Berkeley parser) and
that the re-ranking can add an important gain.
2. Previous Work
Constituent parsing for English is well studied. The best generative constituent parsers
are currently the Brown reranking parser (Charniak and Johnson 2005), the exten-
sion of this parser with self training by McClosky, Charniak, and Johnson (2006b),
and the parser of Petrov and Klein (2007), which is an unlexicalized probabilistic
62
Fraser et al Knowledge Sources for Parsing German
context-free grammar (PCFG) parser with latent feature annotations. Charniak and
Johnson (2005) and Huang (2008) have introduced a significant improvement by
feature-rich discriminative reranking as well.
The number of treebank constituent parsers for German is smaller. Dubey and
Keller (2003) adapted Collins?s (1997) lexicalized parser to German. An unlexicalized
PCFG parser similar to our generative parser was presented by Schiehlen (2004). The
best constituent parser participating in the ACL-08 Workshop on Parsing German
(Ku?bler 2008) was the Berkeley parser (Petrov and Klein 2008). The Stanford parser
was also adapted to German (Rafferty andManning 2008). German dependency parsers
have been developed by Menzel and Schro?der (1998), Duchier and Debusmann (2001),
Hall and Nivre (2008), Henderson et al (2008), and Seeker et al (2010a), to name
a few.
There is also some previous work on German parse reranking. Forst (2007) pre-
sented a reranker for German LFG parsing, and Dreyer, Smith, and Smith (2006) applied
reranking to German dependency parsing. Versley and Rehbein (2009) developed a
reranking method for German constituent parsers. The work by Versley and Rehbein
and by Schiehlen (2004) is closest to ours. Like them, we rerank the unlexicalized BitPar
parser. We also refine treebank labels to increase parsing performance, but add more
information and achieve a larger improvement. We use the monolingual feature set of
Versley and Rehbein in our reranker, but add further monolingual features as well as
bilingual features.
3. Generative Parsing Framework
Our generative parser is an unlexicalized PCFG parser which is based on the BitPar
parser (Schmid 2004). BitPar uses a fast bitvector-based implementation of the well-
known Cocke-Younger-Kasami algorithm and stores the chart as a large bit vector.
This representation is memory efficient and allows full parsing (without search space
pruning) with large treebank grammars. BitPar is also quite fast because the basic
parsing operations are parallelized by means of (single-instruction) and-operations on
bitvectors. BitPar can either be used to compute the most likely parse (Viterbi parse), or
the full set of parses in the form of a parse forest, or the n-best parse trees.
3.1 Grammar
The grammar and lexicon used by our generative parser are extracted from the Tiger2
Treebank (Brants et al 2002). Similar to Johnson (1998) andKlein andManning (2003) we
improve the accuracy of the unlexicalized parser by refining the non-terminal symbols
of the grammar to encode relevant contextual information. This refinement weakens
the strong independence assumptions of PCFGs and improves parsing accuracy. The
extraction of the grammar and lexicon involves the following steps:
1. Discontinuous constituents are eliminated (Section 3.2).
2. Treebank annotations are transformed (Section 3.4) and augmented
(Section 3.5).
3. Grammar rules, lexical rules, and their frequencies are extracted from the
annotated parse trees.
4. The grammar is markovized (Section 3.6).
63
Computational Linguistics Volume 39, Number 1
.
S-TOP
PROPAV-OP-1
Daraus
This-from
VMFIN-HD
kann
can
VP-OC
VP-OC
PROAV-OP
*T*-1
VVPP-HD
gefolgert
concluded
VAINF-HD
werden
be
Figure 1
Projectivized parse tree for the sentence: Daraus kann gefolgert werden [From this can be
concluded].
3.2 Raising for Non-Projectivity
The Tiger2 Treebank that we used in our experiments contains discontinuous con-
stituents. As in other work on German parsing using the Tiger2 Treebank (Dubey
and Keller 2003; Schiehlen 2004; Ku?bler, Hinrichs, and Maier 2006), we eliminated
discontinuous constituents by raising those parts of the discontinuous constituent that
do not contain the head to the child position of an ancestor node of the discontinuous
constituent. Hsu (2010) compared three different Tiger2 conversion schemes and found
raising to be the most effective. The projective parse tree in Figure 1, for instance, is
obtained from a Tiger parse tree where the pronominal adverb Daraus was a dis-
continuous child of the lower VP-OC node.
The parse tree in Figure 1 shows a trace node and coreference indices (similar to
the Penn Treebank annotation style for discontinuous constituents). If slash features
are added to the nodes on the path between the PROAV node and its trace within the
VP, it is possible to restore discontinuous constituents (Schmid 2006). Due to sparse
data problems caused by the added slash features, however, the parsing accuracy
drops by 1.5% compared with the version without slash features (when evaluated on
projectivized parse trees). Traces are recognized with a precision of 53% and a recall of
33%. The correct antecedents are identified with a precision of 48% and a recall of 30%.
These figures indicate that the identification of discontinuous constituents in Tiger parse
trees is a harder task than in English Penn Treebank parses, considering the 84% F-score
for the recognition of empty constituents and the 77% F-score for the identification of
antecedents reported in Schmid (2006) for an analogous approach.
As the example in Figure 1 shows, the precise attachment point of constituents
is often not required: We can simply assume that all constituents appearing at the S
level are dependents of the main verb of the clause. Only for modifiers with scope
ambiguities (e.g., negation particles) is it relevant whether they are attached at the S
or VP level. These considerations suggest that it is better to recognize discontinuous
constituents in a post-processing step as in Johnson (2001), Campbell (2004), and Levy
and Manning (2004). In the rest of the paper, we will only work with parse trees from
which coreference indices and trace nodes have been removed.
3.3 Morphological Features and Grammatical Functions
The Tiger2 Treebank annotates non-terminals not only with syntactic categories but
also with grammatical function labels such as SB (subject), OA (accusative object), or
64
Fraser et al Knowledge Sources for Parsing German
MO (modifier). These labels provide important information that is necessary in order to
derive a semantic representation from a parse. It is not possible to infer the grammatical
role of a constituent from its position in the parse tree alone (as can be done in English,
for instance). Case information is needed in addition in order to help determine the
correct grammatical role. The Tiger2 Treebank provides case, number, degree (positive,
comparative, superlative), and gender information at the part-of-speech (POS) level.
Our parser concatenates the grammatical function labels as well as the case infor-
mation of the POS tags to the base labels similarly to Dubey (2004) and Versley (2005).
Our earlier experiments showed that adding case information increases F-score by 2.1%
absolute. Further enriching the grammar with morphological features, however, hurts
performance. Adding number features decreased F-score by 0.5%. Adding number,
gender, and degree decreased F-score by 1.6%. When grammatical functions are taken
into account in the evaluation, the performance drops by 1.5% when number, gender,
and degree features are incorporated. It seems that the additional information supplied
by the agreement features is not useful enough to outweigh sparse data problems
caused by the more fine-grained label set. Therefore we only use case, but designing
a smoothing procedure allowing us to use number, gender, and degree is interesting
future work.
3.4 Tree Transformations
Similarly to Schiehlen (2004), we automatically augment the Tiger2 annotation with ad-
ditional feature annotations. Our feature annotation set is larger than that of Schiehlen.
In addition to making feature annotations, we also perform some tree transformations
that reduce the complexity of the grammar. In all evaluations, we use the original
(projectivized) Tiger parse trees as gold standard and convert the parse trees generated
by our parser to the same format by undoing the transformations and removing the
additional features. In the rest of this section, we explain the tree transformations that
we used. The following section describes the feature annotations.5
Unary branching rules. The Tiger Treebank avoids unary branching nodes. NPs
and other phrasal categories which dominate just a single node are usually omitted.
The sentence Sie zo?gern [They hesitate], for instance, is analyzed as (S-TOP (PPER-SB
Sie) (VVFIN-HD zo?gern)) without an explicit NP or VP. The lack of unary branching
nodes increases the number of rules because now a rule S-TOP? PPER-SB VVFIN-HD
is needed in addition to the rule S-TOP? NP-SB VVFIN-HD, for instance.
In order to reduce sparse-data problems, we insert unary branching nodes and
transform this parse to (S-TOP (NP-SB (PPER-HD Sie)) (VVFIN-HD zo?gern)) by adding
an NP node with the grammatical function (GF) of the pronoun. The GF of the pronoun,
in turn, is replaced by HD (head). Such unary branching NPs are added on top of nouns
(NN), pronouns (PPER, PDS, PIS, PRELS), cardinals (CARD), and complex proper
names (PN) that are dominated by S, VP, TOP, or DL6 nodes.7 The transformation is
reversible, which allows the original annotation to be restored.
5 Descriptions of the different symbols used in the Tiger annotation scheme are available at
http://www.ims.uni-stuttgart.de/tcl/RESOURCES/CL.html.
6 DL is a discourse level constituent.
7 If a single proper name (NE) forms a noun phrase, we first add a PN node and then an NP node on top.
If a simple noun (NN) with a GF other than NK appears inside of an NP, PP, CNP, CO, or AP, we also add
an NP node on top of it. Similarly, we add a PN node on top of proper names (NE) in the same context.
65
Computational Linguistics Volume 39, Number 1
.
CPP-MO
KON-CD
weder
neither
PP-CJ
APPR-AC
in
in
NE-NK
Berlin
Berlin
KON-CD
noch
nor
PP-CJ
APPR-AC
in
in
NE-NK
Frankfurt
Frankfurt
.
CPP-MO
KON-CD/weder
weder
neither
PP-MO
APPR-AC/in
in
in
NE-HD
Berlin
Berlin
KON-CD/noch
noch
nor
PP-MO
APPR-AC/in
in
in
NE-HD
Frankfurt
Frankfurt
Figure 2
Parse of the phrase weder in Berlin noch in Frankfurt [neither in Berlin nor in Frankfurt] before
and after selective lexicalization of prepositions and conjunctions. This example also shows the
replacement of the grammatical function features CJ and NK discussed in the previous section.
The modified parts are printed in boldface.
By adding a unary-branching NP-SB node, for instance, we introduce an additional
independence assumption, namely, we assume that the expansion of the subject NP is
independent of the other arguments and adjuncts of the verb (a plausible assumption
that is confirmed by a performance improvement).
Elimination of NK. Tiger normally uses the grammatical function HD to mark the
head of a phrase. In case of NPs and PPs, however, the GF of the head is NK (noun
kernel). The same GF is also assigned to the adjectives and determiners of the noun
phrase. We replace NK by HD in order to reduce the set of symbols.8
Elimination of CJ. Tiger annotates each conjunct in a coordination with the spe-
cial grammatical function label CJ. We replace CJ by the grammatical function of the
coordinated phrase. This transformation is also reversible.
3.5 Additional Feature Annotations
Selective lexicalization. We mark the POS tags of the frequent prepositions in [in], von
[from, of], auf [on], durch [through, by means of], unter [under], um [around, at] and
their variants regarding capitalization (e.g., Unter) and incorporation of articles (e.g.,
unters, unterm) with a feature which identifies the preposition. This can be seen as a
restricted form of lexicalization. In the same way, we also ?lexicalize? the coordinating
conjunctions (KON-CD) sowohl [as well], als [as], weder [neither], noch [nor], entweder
[either], and oder [or] if preceded by entweder. Figure 2 shows an example.
Sentence punctuation. If a clause node (S) has a sibling node labeled with POS tag
?$.? that dominates a question mark or exclamation mark, then the clause node and the
POS tag are annotated with quest or excl, so the grammar models different clause types.
8 The original annotation can be restored: HD never occurs in NP or PP children in original Tiger parses.
66
Fraser et al Knowledge Sources for Parsing German
.
CS-RC
S-RC
NP-SB/rel
PRELS-HD-Nom
die
who
NP-OA
NN-HD-ACC
Surfen
surfing
VVFIN-HD
sagen
say
KON-CD
und
and
S-RC/norel/nosubj
NP-OA
NN-HD-Acc
Freiheit
freedom
VVFIN-HD
meinen
mean
Figure 3
Parse of the phrase die Surfen sagen und Freiheit meinen [who say surfing and mean freedom] before
and after annotation with relative clause features. This example also shows the nosubj feature,
which will be discussed later.
Adjunct attachment. Adjuncts often differ with respect to their preferred attach-
ment sites. Therefore, we annotate PPs and adverbials (AVP, ADV, ADJD) with one of
the features N, V, or 0 which indicate a nominal parent (NP or PP), a verbal parent
(VP, S), or anything else, respectively. In case of adverbial phrases (AVP), the label is
propagated to the head child.
Relative clause features. In many relative clauses (S-RC), the relative pronoun
(PRELS, PRELAT, PWAV, PWS) is embedded inside of another constituent. In this case,
all nodes on the path between the pronoun and the clause node are marked with the
feature rel. Furthermore, we add a feature norel to relative clauses if no relative pronoun
is found. Figure 3 shows an example.
Wh features. Similar to the feature rel assigned to phrases that dominate a relative
pronoun, we use a feature wh which is assigned to all NPs and PPs which immediately
dominate a wh-pronoun (PWAT, PWS, PWAV). This feature better restricts the positions
where such NPs and PPs can occur.
Noun sequence feature. If two nouns occur together within a GermanNP (as in drei
Liter Milch [three liters (of) milk] or Ende Januar [end (of) January]), then the first noun
is usually a kind of measure noun. We mark it with the feature seq.
Proper name chunks. Some noun phrases such as Frankfurter Rundschau, Junge
Union, Die Zeit are used as proper names. In this case, the grammatical function of the
NP is PNC. In order to restrict the nouns and adjectives that can occur inside of such
proper name chunks, we mark their POS tags with the feature name.
Predicative APs. Complex adjectival phrases (AP) are either attributively used as
noun modifiers inside of an NP or PP, or predicatively elsewhere. In order to better
model the two types of APs, we mark APs that dominate a predicative adjective (ADJD)
with the feature pred.9
Nominal heads of APs. Sometimes the head of an AP is a noun as in (AP drei
Millionen) Mark [three million Marks] or ein (AP politisch Verfolgter) [a politically
persecuted-person]. We mark these APs with the feature nom.
Year numbers.Years such as 1998 can appear in places where other numbers cannot.
Therefore POS tags of numbers between 1900 and 2019 are marked with year.10
Clause type feature for conjunctions. The type of a subordinate clause and the
subordinating conjunction are highly correlated. German object clauses (S-OC) usually
9 We also mark an AP parent of a node with the label AP-HD/pred in the same way.
10 For some texts, it might be advantageous to use a broader definition of year numbers.
67
Computational Linguistics Volume 39, Number 1
start with dass [that] or ob [whether]; modifier clauses (S-MO) often start with wenn
[if], weil [because], or als [when]. We mark subordinating conjunctions of argument
clauses (S-OC), modifier clauses (S-MO), subject clauses (S-SB), and dislocated clauses
(S-RE) with a feature (OC,MO, SB, or RE) identifying clause type. Without this feature,
argument clauses of nouns, for instance, are often misanalyzed as modifiers of the main
clause.
VP features. VPs that are headed by finite verbs, infinitives, past participles, imper-
atives, and zu infinitives are all used in different contexts. Therefore we mark object VPs
(VP-OC) with a corresponding feature. When parsing the sentence Alle Ra?ume mu?ssen
mehrfach gesa?ubert und desinfiziert werden [all rooms must multiply cleaned and disin-
fected be; all rooms must be ...], this feature allows the parser to correctly coordinate the
two past participle VPs mehrfach gesa?ubert and desinfiziert instead of the past participle
VP mehrfach gesa?ubert and the infinitival VP desinfiziert werden.
Phrases without a head. Some phrases in the Tiger corpus lack a head. This is
frequent in coordinations. All phrases that do not have a child node with one of the
grammatical functions HD, PNC, AC, AVC, NMC, PH, PD, ADC, UC, or DH aremarked
with the feature nohead.
Clauses without a subject. We also mark conjunct clauses with the feature nosubj
if they are neither headed by an imperative nor contain a child node with the gram-
matical function SB (subject) or EP (expletive). This is useful in order to correctly parse
coordinations where the subject is dropped in the second conjunct.
3.6 Markovization
The Tiger Treebank uses rather flat structures where nodes have up to 25 child nodes.
This causes sparse data problems because only some of the possible rules of that length
actually appear in the training corpus. The sparse data problem is solved bymarkoviza-
tion (Collins 1997; Klein and Manning 2003), which splits long rules into a set of shorter
rules. The shorter rules generate the child nodes of the original rule one by one. First,
the left siblings of the head child of the rule are generated from left to right, then the
right siblings are generated from right to left. Finally, the head is generated. Figure 4
shows the markovization of the rule NP? NMNN PP PP.
The auxiliary symbols that are used here encode information about the parent cat-
egory, the head child, and previously generated children. Because all auxiliary symbols
encode the head category, the head is already selected by the first rule, but only later
actually generated by the last rule.
.
NP
NM ?L:NP[NN]NN|NM?
?M:NP[NN]?
?R:NP[NN]PP|PP?
?R:NP[NN]NN|PP?
NN
PP
PP
Figure 4
Markovization of the rule NP? NMNN PP PP.
68
Fraser et al Knowledge Sources for Parsing German
The general form of the auxiliary symbols is ?direction:parent[head]next|previous?
where direction is either L, M, or R, parent is the symbol on the left hand side of the
rule, head is the head on the right hand side of the rule, next is the symbol which will
be generated next, and previous is the symbol that was generated before. Auxiliaries
starting with L generate the children to the left of the head. Auxiliaries starting with
R similarly generate the children to the right of the head and the head itself. The
auxiliary starting with M is used to switch from generating left children to generating
right children. Each rule contains information about the parent, the head, and (usually)
three child symbols (which may include an imaginary boundary symbol). The first rule
encodes the trigram left-boundary NM NN. The second rule is an exception which only
encodes the bigram NM NN. The third rule encodes the trigram PP PP right-boundary.
The last rule is an exception, again, and only encodes NN PP. There is no rule which
covers the trigram consisting of the head and its two immediate neighbors.
Our markovization strategy only transforms rules that occur less than 10 times in
the training data. If one of the auxiliary symbols introduced by the markovization (such
as ?L:NP[NN]NN?NM?) is used less than 20 times (the values of the two thresholds
were optimized on part of the development data) overall, it is replaced by a simpler
symbol ?L:NP[NN]NN? that encodes less context. In this way, we switch from a trigram
model (where the next child depends on the two preceding children) to a bigrammodel
(where it only depends on the preceding child) in order to avoid sparse data problems.
Themethod is similar to the markovization strategy of Klein andManning (2003) except
that they markovize all rules. We simulated their strategy by raising the rule frequency
threshold to a larger value, but obtained worse results. We also tried an alternative
markovization strategy that generates all children left to right (the auxiliary symbols
now lack the direction flag, and the rules cover all possible trigrams), but again obtained
worse results. A disadvantage of our markovization method are spurious ambiguities.
They arise because some of the rules which are not markovized are also covered by
markovization rules.
3.7 Dealing with Unknown Words and Unseen POS Tags
BitPar includes a sophisticated POS guesser that uses several strategies to deal with
unknown words and unseen POS tags of known words. Unknown words are divided
into eight classes11 based on regular expressions that are manually defined. These
classes distinguish between lower-case words, capitalized words, all upper-case words,
hyphenated words, numbers, and so forth. For each word class, BitPar builds a suffix
tree (Weischedel et al 1993; Schmid 1995; Brants 2000) from the suffixes of all words in
the lexicon up to a length of 7. At each node of the suffix tree, it sums up the conditional
POS probabilities (given the word) over all known words with that suffix. By summing
POS probabilities rather than frequencies, all words have the same weight, which is
appropriate here because we need to model the POS probabilities of infrequent words.
BitPar computes POS probability estimates for each node using the sum of probabilities
as a pseudo-frequency for each tag. The estimates are recursively smoothed with the
Witten-Bell method using the smoothed POS probabilities of the parent node as a
backoff probability distribution.12 The suffix trees are pruned by recursively removing
11 We also experimented with more complex classifications, but they failed to improve the results.
12 The number of ?observed? POS tags, which is needed by Witten-Bell smoothing, is defined as the
number of POS tags with a pseudo-frequency larger than 0.5.
69
Computational Linguistics Volume 39, Number 1
leaf nodes whose pseudo-frequency is below 5 or whose weighted information gain13
is below a threshold of 1.
Whenever an unknown word is encountered during parsing, BitPar determines the
word class and obtains the tag probability distribution from the corresponding suffix
tree. BitPar assumes that function words are completely covered by the lexicon and
never guesses function word POS classes for unknown words.
BitPar uses information from the unknown word POS guesser and (if available)
information from an external lexicon (generated by a computational morphology, for
instance, as we will discuss in Section 5.1) in order to predict unobserved POS tags
for known words. First the external lexicon and the lexicon extracted from the training
corpus are merged. Then smoothed probabilities are estimated using Witten-Bell
smoothing with a backoff distribution. The backoff distribution is the average of:
(1) the probability distribution returned by the unknown word POS guesser
if at least one possible POS tag of the word according to the lexicon is an
open-class POS tag,
(2) the average POS probability distribution of all words with exactly the same
set of possible POS tags as the given word14 if at least one of the possible
tags is unseen, and
(3) the prior POS probability distribution if no other word in the lexicon has
the same set of possible POS tags and at least one of the word?s possible
POS tags is unseen.
4. Evaluation of the Generative Parser
As we present each knowledge source, we would like to evaluate it against manually
annotated Treebanks. Our first evaluation shows that our generative parser introduced
in the previous section is comparable with the Berkeley generative parser. Before we
present this comparison in Section 4.1 we discuss evaluating parse accuracy.
In our evaluations, we use the Tiger Treebank (Brants et al 2002) and a small
Europarl Treebank (Pado? and Lapata 2009). We take the first 40,474 sentences of the
Tiger Treebank as training data (Tiger train), the next 5,000 sentences as development
data (Tiger dev), and the last 5,000 sentences as test data (Tiger test). The Europarl
data consists of 662 sentences15 and are either completely used as test data and not di-
vided up or we carried out seven-fold cross-validation experiments with our reranking
models.
All parsers are evaluated on projectivized parse trees. This means that we apply
step 1 of the grammar extraction process described in Section 3.1 to the test parses
and use the result as the gold standard (except for the Pado? set, which is already
projectivized). The test sentences are parsed and the resulting parse trees are converted
13 The weighted information gain is the difference between the entropy of the parent node and the entropy
of the current node, multiplied by the total frequency of the current node and divided by the number of
?observed? POS tags of the current node.
14 A similar pooling of lexicon entries was previously used in the POS tagger of Cutting et al (1992).
15 We use only the sentences in this set which had a single sentence as a translation, so that they could
be used in bilingual reranking, which will be discussed later.
70
Fraser et al Knowledge Sources for Parsing German
to the same format as the gold standard trees by undoing Steps 2, 3, and 4 of Section 3.1.
This conversion involves four steps:
1. Demarkovization removes all the auxiliary nodes introduced by
markovization and raises their children to the next non-auxiliary node.
2. The added unary-branching nodes are eliminated.
3. The original grammatical function labels NK inside of NPs and PPs,
and CJ inside of coordinated phrases, are restored.
4. All feature annotations are deleted.
We use PARSEVAL scores (Black et al 1991) and the standard evaluation tool evalb16
to compare the converted parse trees with the gold standard parse trees using labeled
F-score. We report accuracies for all test sentences and not just sentences of length up to
40. We do not evaluate parsers with gold standard POS tags, but instead automatically
infer them. These considerations make our evaluation setting as close to the real-world
setting as possible.
We report results for evaluations with and without grammatical functions. We
report PARSEVAL scores with grammatical functions inside parentheses after the
results using only basic constituent categories. We believe that grammatical functions
are an important part of the syntactic analysis for any downstream applications in less-
configurational languages such as German because crucial distinctions (e.g., the distinc-
tion between subject and object) are not feasible without them. We should mention that
our results are not directly comparable to previously published results on the Tiger2
corpus (Ku?bler 2008; Versley and Rehbein 2009; Seeker et al 2010b), because each of
the previous studies used different portions of the corpus and there are differences in
the evaluation metric as well. The transformed corpus (in our train, development, and
test split format) and the evaluation scripts we used are available,17 which we hope will
enable direct comparison with our results.
4.1 Comparison of BitPar and Berkeley
The best constituent parser participating in the Parsing German Shared Task (Ku?bler
2008) was the Berkeley parser (Petrov and Klein 2008) and to the best of our knowledge
it has achieved the best published accuracy for German constituency parsing so far.
The Berkeley parser takes an automated approach, in which each constituent symbol is
split into subsymbols applying an expectation-maximization method. We compare our
manually enriched grammar to this automatic approach.
We trained the Berkeley parser on Tiger train using the basic constituent categories
concatenated to the grammatical function labels as starting symbols. We found that it
achieved the best PARSEVAL scores on Tiger dev after the fourth iteration. This model
was used for parsing Tiger dev, Tiger test, and the Europarl corpus.
BitPar achieved 82.51 (72.46), 76.67 (65.61), and 77.13 (66.06), and the Berkeley
parser achieved 82.76 (73.20), 76.37 (65.66), and 75.51 (63.3) on the three corpora,
respectively. In general, these results indicate that these two parsers are competitive.
On the other hand, the fact that the results of the Berkeley parser are much worse than
16 http://nlp.cs.nyu.edu/evalb/, 2008 version.
17 See http://www.ims.uni-stuttgart.de/tcl/RESOURCES/CL.html.
71
Computational Linguistics Volume 39, Number 1
BitPar on the out-of-domain Europarl corpus indicates that it overfits to the domain
of the training corpus (Tiger2). Following a reviewer suggestion, we looked at the
sentences containing many words not occurring in the training data, and observed that
our lexical resource is strongly helpful for these sentences. Another disadvantage of
the automatic approach of the Berkeley parser is that the resulting subsymbols are not
easily interpretable, which can hinder defining features for parse reranking using them.
Based on these considerations, we decided to use BitPar in our reranking experiments.
The combination of the two radically different approaches (linguistically motivated
grammar extensions and automatic symbol splitting) is a rather promising area of
research for improving parsing accuracy, which we plan to address in future work.
5. Impact of Our Lexical Resource
5.1 Integration of SMOR with BitPar
There are a large number of inflectedword forms formanyGerman lemmas. This causes
sparse data problems if some forms are not observed in the training data. BitPar applies
the heuristics described in Section 3.7 to obtain POS probabilities for unseen words.
Although these heuristics seem to work quite well, we expect better results if the parser
has access to information from a morphological analyzer.
We use the German finite-state morphology SMOR (Schmid, Fitschen, and Heid
2004) to provide sets of possible POS tags for all words. SMOR covers inflection, deriva-
tion, and compounding and achieves good coverage in combination with the stem
lexicon IMSLex (Lezius, Dipper, and Fitschen 2000). SMOR is integrated into the parser
in the following way. We create a combined word list from the training and testing
data18 and analyze it with SMOR. The SMOR analyses are then mapped to the POS
tag set used by the parser, and supplied to BitPar as an external lexicon (see Section 3.7).
Consider the example word erlischt [goes out], which did not appear in the train-
ing corpus. SMOR produces the analysis erlo?schen.V.3.Sg.Pres.Ind, which is mapped
to VVFIN-HD and added to the lexicon. Using this entry, BitPar correctly parsed
the sentence Die Anzeige erlischt [The display goes out]. Without using SMOR, the
parser analysed erlischt as a past participle because scht is a frequent past participle
ending.
5.2 Effect on In-Domain and Out-of-Domain Parsing
In order to measure the effect of the integration of a German morphology on parsing
accuracy (see Section 5.1), we tested the BitPar parser on the Tiger data and on Europarl
data. The results are summarized in Table 1. They show that the morphology helps on
out-of-domain data (Europarl), but not so much on in-domain data (Tiger). The POS
tagging accuracy, however, also increases on Tiger data by 0.13%. When grammatical
functions are included in the evaluation, the performance improvement more than
doubles on Europarl data. As a result, we decided to use the finite-state morphology
in the rest of the experiments we conducted.
Table 1 also shows that the Tiger test data is harder to parse than the dev data. We
examined the two subcorpora and found that the test data contains longer sentences
18 Because we are only using the words here, and not their POS labels, this approach is methodologically
sound and could be applied to any unparsed data in the same way.
72
Fraser et al Knowledge Sources for Parsing German
Table 1
Effect of using finite-state morphology on parsing accuracy. The values in parentheses are
labeled F-scores from the evaluation with grammatical functions.
morphology Tiger dev Tiger test Europarl
without 82.51 (72.46) 76.67 (65.61) 76.81 (65.31)
with 82.42 (72.36) 76.84 (65.91) 77.13 (66.06)
difference ?0.09 (?0.10) +0.17 (+0.30) +0.32 (+0.75)
(18.4 vs. 15.3 words on average) and that the ratio of unknown words is higher (10.0%
vs. 7.6%).
6. Parse Reranking
The most successful supervised phrase-structure parsers are feature-rich discriminative
parsers that heavily depend on an underlying PCFG grammar (Charniak and Johnson
2005; Huang 2008). These approaches consist of two stages. At the first stage they apply
a PCFG grammar to extract possible parses. The full set of possible parses cannot be
iterated through in practice, and is usually pruned as a consequence. The n-best list
parsers keep just the 50?100 best parses according to the PCFG. Other methods remove
nodes and edges from the packed parse forest whose posterior probability is under a
pre-defined threshold (Charniak and Johnson 2005).
The task of the second stage is to select the best parse from the set of possible
parses (i.e., rerank this set). These methods use a large feature set (usually a few
million features) (Collins 2000; Charniak and Johnson 2005). The n-best list approaches
can straightforwardly use local and non-local features as well because they decide at
the sentence-level (Charniak and Johnson 2005). Involving non-local features is more
complicated in the forest-based approaches. The conditional random field methods
usually use only local features (Yusuke and Jun?ichi 2002; Finkel, Kleeman, and
Manning 2008). Huang (2008) introduced a beam-search and average perceptron-based
procedure incorporating non-local features in a forest-based approach. His empirical
results show only a minor improvement from incorporating non-local features,
however.
In this study, we experiment with n-best list reranking using a maximum entropy
machine learning model for (re)ranking along with local and non-local features. Our
reranking framework follows Charniak and Johnson (2005). At the first-stage of parsing,
we extract the 100 best parses for a sentence according to BitPar?s probability model.
At parsing time, a weight vector w is given for the feature vectors (which numerically
represent one possible parse) and we select the parse with the highest inner product
of these two vectors. The goal of training is to adjust w. In the maximum entropy
framework, this is achieved by solving the optimization problem of maximizing the
posterior probability of the oracle parse?the parse with the highest F-score.19 Our
method aims to select the oracle, as the gold standard parse is often not present in
the 100-best parses.20 Our preliminary experiments showed that parse candidates close
19 Ties are broken using the PCFG probabilities of the parses.
20 The oracle F-score (i.e., the upper limit of 100-best reranking on the Tiger development corpus) is 90.17.
73
Computational Linguistics Volume 39, Number 1
to the oracle confuse training. Hence during training, we removed all parses whose
F-score is closer than 1.0 to the score of the oracle.21
As we discussed in Section 1, the parsing output of morphologically rich languages
is useful only when it is additionally annotated with grammatical functions. The oracle
parses often change if the grammatical function labels are also taken into consideration
at the PARSEVAL score calculation. Hence slightly different objective functions are used
in the two cases. We will report results achieved by reranking models where the oracle
selection for training agrees with the evaluation metric utilized?that is, we trained
different models (which differ in the oracle selection) for the basic constituent label
evaluation and for the evaluation on grammatical functions.
During training we followed an eight-fold cross validation technique for candidate
extraction (Collins 2000). Here, one-eighth of the training corpus was parsed with a
PCFG extracted from seven-eighths of the data set. This provides realistic training
examples for the reranker as these parses were not seen during grammar extraction. We
used the ranking MaxEnt implementation of MALLET (McCallum 2002) with default
parameters.
7. Monolingual Reranking
7.1 Subcategorization-Based Monolingual Reranking Features
We introduce here several novel subcategorization-based features for monolingual
reranking. For this, we first describe our algorithm for extracting subcategorization
(subcat) information. We use our enriched version of the Tiger2 training set. In order
to extract verbal subcat frames we find all nodes labeled with the category S (clause)
or VP-MO (modifying VP) and extract their arguments. Arguments22 are nodes of the
categories shown in Table 2. The arguments of nouns are obtained by looking for NN
nodes which are either dominated by an NP or a PP, and which take a following node
of category PP, VP-OC, or S-OC as argument.
The feature functions we present are mostly lexicalized. This means we need access
to the head words of the arguments. The argument heads are extracted as follows: As
NP headwe take the last nodewhose function label is either HD,NK, or PH. If this node
is of category NP or PN, we recursively select the head of that constituent. Similarly, the
head of an AP is the last node with functional label HD. If it is an AP, the head is
searched inside of it. In the case of PPs, we extract two heads, namely, the preposition
(or postposition) as well as the nominal head of the PP, which is found using similar
rules as for NPs. We also extract the case of the nominal head.
The extraction of verbal heads is somewhat more complicated. In order to obtain
the correct verbal head of a clause irrespective of the verb position (verb-first, verb-
second, verb-final), we extract all verbs that are dominated by the clause and a possibly
empty sequence of VP-OC or VP-PD (statal passive) nodes and an optional VZ-HD
node. Then we take the first non-finite verb, or alternatively the first finite verb if all
verbs were finite. In order to avoid sparse data problems caused by the many different
inflections of German verbs, we lemmatize the verbs.
21 In Fraser, Wang, and Schu?tze (2009) we used Minimum Error Rate Training. Once we made this change
to maximum entropy the results on small feature sets became similar (details omitted).
22 An exception to this is that if a PP argument dominates a node of category PROAV-PH, it is considered
a PROAV-PH argument. An example is the sentence Er [he] wartet [waits] (PP-OP (PROAV-PH darauf
[for this]), (S-RE dass [that] sie [she] kommt [comes])).
74
Fraser et al Knowledge Sources for Parsing German
Table 2
Arguments used in extracted subcategorization frames.
NP-SB, PN-SB, CNP-SB, S-SB, VP-SB subjects
NP-OA, PN-OA, CNP-OA direct objects
NP-DA, PN-DA, CNP-DA indirect objects
PRF-OA reflexive direct objects
PRF-DA reflexive indirect objects
NP-PD, CNP-PD predicative NPs
ADJD-PD, AP-PD, CAP-PD predicative adjectives
S-OC, CS-OC argument clauses
PP-OP, CPP-OP PP arguments
VP-OC/zu infinitival complement clauses
PROAV-OP pronominal adverbs serving as PP proxies such as
daraus [out of this]
NP-EP expletive subjects
VP-RE, NP-RE VP/NP appearing in expletive constructions
In the case of coordinated phrases, we take the head of the first conjunct. Arguments
are sorted to put them in a well-defined order. An example is that given the correct
parse of the sentence Statt [instead of] Details [details] zu [to] nennen [name], hat [has]
er [he] unverdrossen [assiduously] die [the] ?Erfolgsformel? [formula of success] wiederholt
[repeated], meaning ?instead of naming the details, he assiduously repeated the formula
of success,? we extract the two subcat frames:
VP-MO OBJ:Details VZ-HD:zu:nennen
S-TOP VP-MO SUBJ:er OBJ:Erfolgsformel VVPP-HD:wiederholt
We can now describe our features. The features focus on subcat frames taken from
S nodes (VP-MO is treated as S), and on attachment of prepositions and conjunctions to
nouns. We define conditional probability and mutual information (MI) features.
The two conditional probability features are ProbPrepAttach and ProbAdverb-
Attach, which calculate the probability for each preposition or adverb to be attached
to its governor, given the label of the governor. We estimate this from the training
data as follows, for the example of the PP feature. In the feature scoring, we give
each preposition attachment a score which is the negative log10 of the probabil-
ity p(lex prep|label governor) = f (lex prep, label governor)/f (label governor) (with a
cutoff of 5).
For all of our other monolingual features, we use (negative) pointwise mutual
information: ?log10(p(a, b)/p(a)p(b)) (here we use cutoffs of 5 and ?5).
MI NounP and MI NounConj give an assessment of a preposition or a conjunction
being attached to a noun (given the lexicalized preposition and the lexicalized noun).
For the MI VSubcat feature, we use as a the frame (without lexicalization), and as
b the head verb. p(a) is estimated as the relative frequency of this frame over all frames
extracted from Tiger2 train. MI VSimpleSubcat is a simpler version of MI VSubcat.
PP is excluded from frames because PP is often an adjunct rather than an argument.
For the MI VArg feature, we use as a the argument function and the head word
of the argument (e.g., OBJ:Buch, which is ?book? used as an object). As b we again
use the head verb. The estimate of p(a) is frequency(OBJ:Buch)/(total number of
extracted frames).23 In addition, this feature is refined into individual features for
23 We make the assumption that every frame has an object, but that this object can be NULL.
75
Computational Linguistics Volume 39, Number 1
different kinds of arguments: MI VSubj, MI VObj, MI VIobj, MI VPP, MI VPRF,
MI VS OC, MI VVP, and MI VerbPROAV. As an example, the MI of ?lesen, OBJ:Buch?
(reading, object:Book) would be used for the MI VArg features and for the MI VObj
feature. For functions such as MI VPP which are headed by both a function word (here,
a preposition) and a content word, only the function word is used (and no case).
The last MI feature is MI VParticle. Some German verbs contain a separable parti-
cle, which can also be analyzed as an adverb but will then have a different meaning. For
the sentence ?Und [and] Frau [Mrs.] Ku?nast [(proper name)] bringt [brings] das [that] auch
[also] nicht [not] ru?ber [across],? if ?ru?ber? is analyzed as an adverb, the verb means to
carry/take/bring over [to another physical location], but if it is viewed as a particle, the
sentence means Frau Ku?nast is not able to explain this. The feature MI VParticle helps
with this kind of disambiguation.
7.2 The Versley and Rehbein Feature Set
We also carried out experiments with the feature set of Versley and Rehbein (2009),
which is specially designed for German. It consists of features constructed from the
lexicalized parse tree along with features based on external statistical information.
The features here are local in the sense that their values can be computed at the
constituent in question, its daughters, and its spanning words. All features except
the external statistical information are binary and indicate that a lexicalized pattern is
present in the parse. They were originally designed for forest-based reranking (Versley
and Rehbein 2009). Following Charniak and Johnson (2005) we sum up these local
feature values in the parse tree. Thus our versions count the number of times that a
particular pattern occurs in the entire parse tree.
The patterns used can be further subcategorized into three groups. The wordform-
based patterns are token?POS (e.g., one pattern is ?lesen-VVINF?) and the word class
of the token in question (word class comes from an automatic clustering of words based
on contextual features). The constituent-based patterns are the size of the constituent,
the constituent label, and the right-hand side of the derivational rule applied at the node
in question. The last and biggest group of the pattern features is formed by the bilexical
dependencies. They are based on the head word of the constituent node in question
and its daughters. Versley and Rehbein (2009) have also introduced features that exploit
statistical information gathered from an external data set and aim to resolve PP attach-
ment ambiguity. Mutual information values were gathered on the association between
nouns and immediately following prepositions, as well as between prepositions and
closely following verbs on the DE-WaC corpus (Baroni and Kilgarriff 2006). These
feature values were then used at NP?PP and VP?PP daughter attachments.
A total of 2.7 million features fired in the Tiger train. We ignored features firing
in less than five sentences for computational efficiency, resulting in 117,000 extremely
sparse features.
7.3 Monolingual Reranking Experiments
We rerank 100-best lists from BitPar (Schmid 2004), which uses the grammar extraction
procedure and lexical resources introduced in Section 3. In each of the experiments we
extracted the grammar from the Tiger train and used it to obtain the 100-best parses for
the sentences of the evaluation corpus.
We trained reranking models on the Tiger train as described in Section 6 using our
subcategorization-based features, the Versley09 feature set, and the union of these two
76
Fraser et al Knowledge Sources for Parsing German
Table 3
The PARSEVAL score of monolingual features to rerank the parses of Europarl (seven-way
cross-validation on 662 sentences) and Tiger2 (development and test sets).
Tiger dev Tiger test Europarl CROSS Europarl IN
Baseline 82.42 (72.36) 76.84 (65.91) 77.13 (66.06)
subcat 83.19 (73.63) 77.65 (67.21) 77.23 (66.13) 77.73 (66.95)
Versley09 83.56 (73.89) 78.57 (68.42) 77.82 (66.87) 77.62 (66.05)
subcat+Versley09 84.19 (74.96) 78.86 (69.04) 77.76 (66.84) 77.93 (66.75)
sets. We evaluated the models on Tiger dev, Tiger test, and Europarl. As the domains
of Tiger and Europarl are quite different, besides this cross-domain parser evaluation
(CROSS) we carried out an in-domain (IN) evaluation as well. In the latter we followed
the seven-fold cross-validation approach, that is, the reranking models were trained on
six-sevenths of Europarl. The results are presented in Table 3.
The results presented in Table 3 show that the reranking models achieve an im-
provement over the baseline parser using both our and the Versley09 feature sets. The
Versley09 feature set achieved better results than our monolingual features when a
training dataset with sufficient size is given (Tiger). On the other hand using our 16
rich features (compared with 117,000 sparse features) is more suitable for the settings
where only a limited amount of training instances are available (the training sets consist
of 567 sentences of Europarl in seven-fold cross-validation). The rerankingmodels using
the union of the feature sets obtain close to the sum of the improvements of the two in-
dividual feature sets. The subcategorization features model rich non-local information,
and the fine-grained features capture local distinctions well and the features based on
the Web corpus access additional knowledge.
We performed an experiment adding one feature at a time, and found that the
most effective features were ProbAdverbAttach, MI VPP, MI VPRF, MI VSubj, and
MI VArg. After this the variation caused by numeric instability was too high to see a
consistent incremental gain from the rest of the features. We conclude that these features
can be robustly estimated and have more discriminative power than the others, but we
emphasize that we used all features in our experiments.
Figure 5 shows a parse tree produced by the BitPar parser in which the noun phrase
diese Finanzierung is incorrectly classified as an accusative object. The monolingual
subcategorization features MI VSubcat, MI VSimpleSubcat, and MI VArg enable the
reranker to correctly analyze the noun phrase as a subject and to move it from the VP
level to the S level.
.
S-TOP
PWAV-MO
Woher
where-from
VMFIN-HD
soll
should
VP-OC
NP-OA
PDAT-HD
diese
this
NN-HD
Finanzierung
financing
VVINF-HD
kommen
come
Figure 5
Erroneous parse produced by BitPar that is corrected by monolingual features.
77
Computational Linguistics Volume 39, Number 1
8. Bilingual Reranking
We now present our bilingual reranking framework. This follows our previous work
(Fraser, Wang, and Schu?tze 2009), which defined feature functions for reranking
English parses, but now we will use these same feature functions (and three additional
feature functions introduced to capture phenomena higher in the syntactic tree) to
rerank German parses. The intuition for using this type of bitext projection feature is
that ambiguous structures in one language often correspond to unambiguous structures
in another. Our feature functions are functions on the hypothesized English parse e,
the German parse g, and the word alignment a, and they assign a score (varying
between 0 and infinity) that measures syntactic divergence. The alignment of a sentence
pair is a function that, for each English word, returns a set of German words with
which the English word is aligned. Feature function values are calculated either by
taking the negative log of a probability, or by using a heuristic function which scales
similarly.24
The bilingual feature functions we define are functions that measure differ-
ent types of syntactic divergence between an English parse and a German parse.
Charniak and Johnson (2005) defined the state of the art in discriminative n-best
constituency parsing of English syntax (without the use of self-training). The n-best
output of their generative parser is reranked discriminatively by a reranker. We call
this CJRERANK. We will use an array of feature functions measuring the syntactic
divergence of candidate German parses with the projection of the English parse
obtained from CJRERANK.
In our experiments we use the English text of the parallel Treebank extracted from
the Europarl corpus and annotated by Pado? and Lapata (2009). There are 662 German
sentences that are aligned to single English sentences; this is the set that we use. Due to
the limited number of trees, we perform cross-validation to measure performance.
The basic idea behind our feature functions is that any constituent in a sentence
should play approximately the same syntactic role and have a similar span as the corre-
sponding constituent in a translation. If there is an obvious disagreement, it is probably
caused by wrong attachment or other syntactic mistakes in parsing. Sometimes in
translation the syntactic role of a given semantic constituent changes; we assume that
our model penalizes all hypothesized parses equally in this case.
To determine which features to describe here we conducted a greedy feature addi-
tion experiment (adding one feature at a time), on top of our best monolingual system
(combining both subcat and Versley09 feature sets). All bilingual experiments use all of
the features (not just the features we describe here). Definitions are available.25
BitParLogProb (the only monolingual feature used in the bilingual-only experi-
ment) is the negative log probability assigned by BitPar to the German parse.
8.1 Count Feature Functions
Count feature functions count projection constraint violations.
Feature CrdBin counts binary events involving the heads of coordinated phrases. If
we have a coordination where the English CC is aligned only with a German KON, and
24 A probability of 1 is a feature value of 0, whereas a low probability is a feature value which is
 0.
25 See http://www.ims.uni-stuttgart.de/tcl/RESOURCES/CL.html.
78
Fraser et al Knowledge Sources for Parsing German
Table 4
Other projection features selected; see the previously mentioned Web page25 for precise
definitions.
POSParentPrjWordPerG2E Computes the span difference between all the parent constituents
of POS tags in a German parse and their respective coverage
in the corresponding English parse, measured using percentage
coverage of the sentence in words. The feature value is the sum
of all the differences. The projection direction is from German to
English.
AbovePOSPrjPer Projection direction is from English to German, and measured in
percentage sentence coverage using characters, not words. The
feature value is calculated over all constituents above the POS
level in the English tree.
AbovePOSPrjWord Calculates a length-based difference using words.
POSPar2Prj Only applies when the POS tag?s parent has two children (the
POS tag has only one sibling). Projects from English to German
and calculates a length-based difference in characters.
POSPar2PrjPer Calculates a percentage-based difference based on characters.
POSPar2PrjG2E Like POSPar2Prj except projects from German to English.
POSPar2PrjWordG2E Like POSPar2PrjG2E except uses word-based differences.
both have two siblings, then the value contributed toCrdBin is 1 (indicating a constraint
violation) unless the head of the English left conjunct is aligned with the head of the
German left conjunct and likewise the right conjuncts are aligned.
Feature Q simply captures a mismatch between questions and statements. If a
German sentence is parsed as a question but the parallel English sentence is not, or
vice versa, the feature value is 1; otherwise the value is 0.
Feature S-OC considers that a clausal object (OC) in a German parse should be
projected to a simple declarative clause in English. This feature counts violations.
EngPPinSVP checks whether a PP inside of a S or VP in English attaches to the
same (projected) constituent in German. If an English PP follows immediately a VP or
a single verb, and the whole constituent is labeled ?S? or ?VP,? then the PP should be
identified as governed by the VP. In this case the corresponding German PP should
attach as well to the German VP to which the English VP is projected (attachment in
German can be to the left or to the right). If the governor in German does not turn out to
be a VP or have a tag starting with ?V,? a value of 1 will be added to the feature for this
German parse.
EngLeftSVP checks whether the left sibling of S or VP in English attaches to the
same (projected) constituent in German (where attachment can be left or right). This
feature counts violations.
Span Projection Feature Functions. Span projection features calculate an absolute or
percentage difference between a constituent?s span and the span of its projection. Span
size is measured in characters or words. To project a constituent in a parse, we use the
word alignment to project all word positions covered by the constituent and then look
for the smallest covering constituent in the parse of the parallel sentence.
PPParentPrjWord checks the correctness of PP attachment. It projects all the parents
of PP constituents in an English parse to German, and sums all the span differences. It is
measured in words. In addition to PPParentPrjWord we implement two bonus features,
NonPPWord and NonPPPer. The former simply calculates the number of words that
79
Computational Linguistics Volume 39, Number 1
do not belong to PP phrases in the sentence, and the latter computes the non-PP
proportion in a character-based fashion. These can be thought of as tunable parameters
which adjust PPParentPrjWord to not disfavor large PPs. The other selected projection
features are described in Table 4.
Probabilistic Feature Functions. We use Europarl (Koehn 2005), from which we
extract a parallel corpus of approximately 1.22 million sentence pairs, to estimate
the probabilistic feature functions described in this section.
We describe the feature PTag, despite the fact that it was not selected by the feature
analysis, because several variations (described next) were selected. PTag measures
tagging inconsistency based on estimating the probability for each English word that
it has a particular POS tag, given the aligned German word?s POS tag. To avoid noisy
feature values due to outliers and parse errors, we bound the value of PTag at 5.26 We
use relative frequency to estimate this feature. When an English word is aligned with
two words, estimation is more complex. We heuristically give each English and German
pair one count. The value calculated by the feature function is the geometric mean27 of
the pairwise probabilities.
The feature PTagEParent measures tagging inconsistency based on estimating the
probability that the parent of the English word at position i has a particular tag, given
the aligned German word?s POS label. PTagBiGLeft measures tagging inconsistency
based on estimating the probability for each English word that it has a particular POS
tag, given the aligned German word?s label and the word to the left of the aligned
German word?s label. PTagBiGParent measures tagging inconsistency based on esti-
mating the probability for each English word that it has a particular POS tag, given the
aligned German word?s label and the German word?s parent?s label.
8.2 Bilingual Reranking Experiments
We performed experiments looking at bilingual reranking performance. To train the
parameters of the probabilistic feature functions, we use 1-best parses of the large
Europarl parallel corpus (from CJRERANK and BitPar). We work on the same 100-best
list (of the German sentences in the small Pado? set) as was used in the previous section.
We parse the English sentences of the small Europarl set with CJRERANK; this parse is
used as our bilingual knowledge source. Finally we rerank using the bilingual features
(results in the first row of Table 5).
We then combine the monolingual features with the bilingual features. We rerank
using both the monolingual and the bilingual features together, and the results are
presented in Table 5. The bilingual feature-based reranker achieved 1 percentage point
improvement over the baseline. This advantage was just slightly decreasedwhenmono-
lingual features are also present. This indicates again that themonolingual and bilingual
features can capture different linguistic phenomena and their information content is
rather different. As in the Europarl IN setting, using the large sparse Versley09 feature
set the reranker could not learn a meaningful model from a moderate-sized training
data set.
26 Throughout this paper, assume log(0) = ??.
27 Each English word has the same weight regardless of whether it was aligned with one or with more
German words.
80
Fraser et al Knowledge Sources for Parsing German
Table 5
PARSEVAL scores of bi+monolingual features to rerank the parses of Europarl (seven-way
cross-validation) and the added value of bilingual features over the results achieved by the
corresponding monolingual feature set.
Mono features without bilingual with bilingual added value
NONE 77.13 (66.06) 78.10 (67.12) +0.97 (+1.06)
subcat 77.73 (66.95) 78.54 (67.95) +0.78 (+1.00)
Versley09 77.62 (66.05) 77.71 (66.06) +0.09 (+0.01)
subcat+Versley09 77.93 (66.75) 78.70 (67.45) +0.78 (+0.70)
The parse tree in Figure 6 demonstrates the value of bilingual features. It was
produced by the monolingual reranker and it incorrectly combines the two adverbs aber
and ebenso into an adverbial phrase and places this under the VP. The bilingual reranker
instead attaches the two adverbs separately at the S level. The attachment to the S node
indicates that the two adverbs modify the modal verb kann and not the full verb sagen.
This is triggered by the feature POSPar2Prj.
8.3 Previous Work on Bitext Parsing
Bitext parsing was also addressed by Burkett and Klein (2008). In that work, they use
feature functions defined on triples of (English parse tree, Chinese parse tree, alignment)
which are combined in a log-linear model, much as we do. In later work (Burkett,
Blitzer, and Klein 2010), they developed a unified joint model for solving the same
problem using a weakly synchronized grammar. To train these models they use a small
parallel Treebank that contains gold standard trees for parallel sentences in Chinese
and English, whereas we only require gold standard trees for the language we are
reranking. Another important difference is that Burkett and Klein (2008) use a large
number of automatically generated features (defined in terms of feature generation
templates) whereas we use a small number of carefully designed features that we found
by linguistic analysis of parallel corpora. Burkett, Blitzer, and Klein (2010) use a subset
of the features of Burkett and Klein (2008) for synchronization, along with monolin-
gual parsing and alignment based features. Finally, self-training (McClosky, Charniak,
and Johnson 2006b) is another differentiator of our work. We use probabilities esti-
mated from aligned English CJRERANK parses and German BitPar parses of the large
Europarl corpus in our bilingual feature functions. These feature functions are used to
.
S-TOP
PIS-SB
Man
one
VMFIN-HD
kann
can
VP-OC
AVP-MO
ADV-MO
aber
but
ADV-HD
ebenso
just-as-well
VVINF-HD
sagen
say
,
,
S-OC
KOUS-CP
dass
that
PPER-SB
sie
they
ADJD-PD
anspruchsvoll
demanding
VAFIN-HD
sind
are
Figure 6
Erroneous parse produced by the reranker using only monolingual features, which is corrected
by bilingual features. The sentence means One can, however, just as well say that they are demanding.
81
Computational Linguistics Volume 39, Number 1
improve ranking of German BitPar parses in the held-out test sets, which is a form of
self-training.
Two other interesting studies in this area are those of Fossum and Knight (2008)
and of Huang, Jiang, and Liu (2009). They improve English prepositional phrase at-
tachment using features from a Chinese sentence. Unlike our approach, however, they
do not require a Chinese syntactic parse as the word order in Chinese is sufficient to
unambiguously determine the correct attachment point of the prepositional phrase in
the English sentence without using a Chinese syntactic parse.
We know of no other work that has investigated to what extent monolingual and
bilingual features in parse reranking are complementary. In particular, the work on bi-
text parsing by Burkett and Klein (2008) does not address the question as to whether the
effect of monolingual and bilingual features in parse reranking is (partially) additive.
We demonstrate bilingual improvement for a strong parser of German. Previously,
we showed bilingual improvement for parsing English with an unlexicalized parser
(Fraser, Wang, and Schu?tze 2009), using 34 of the 37 bilingual feature functions we use
in this work.
9. Conclusion
In this paper, we have focused on MR&LC languages like German?languages that
are morphologically rich, but also have a strong configurational component. We have
argued that constituency parsing is, perhaps contrary to conventional wisdom, an ap-
propriate parsing formalism for MR&LC because constituents capture configurational
constraints in a transparent way and because for many applications constituency pars-
ing is preferable to dependency parsing. Our detailed description of a constituency
parsing system for a morphologically rich language, a system that addresses the major
problems that arise in constituency parsing for MR&LC, is one main contribution of this
paper. Two of these problems are rule proliferation and syncretism. We have addressed
rule proliferation bymarkovization and syncretism by (i) deploying a high performance
finite-state-based morphological analyzer that is based on rich lexical knowledge and
(ii) encoding grammatical functions directly as part of the phrase labels. This direct
encoding allows us to directly combine morphological and configurational informa-
tion in parsing and arrive at a maximally disambiguated parse. We argued that this
is the right setup for MR&LC languages because applications must have access to
grammatical functions.
A large part of this paper was concerned with making available and evaluating
additional knowledge sources for improved parsing of the MR&LC language German.
Our motivation was that (as we argued) MR&LC languages have in general higher am-
biguity than purely configurational and purely morphological languages, in particular
with respect to grammatical functions. Apart from the lexical knowledge embedded
in the morphological analyzer, we presented work on two other knowledge sources to
address this type of additional ambiguity: monolingual reranking (which looks at global
sentence-wide constraints for disambiguation) and bitext reranking (which exploits
parallel text for disambiguation). We were able to improve the performance of a strong
baseline parser using these three knowledge sources and we showed that they are
largely complementary: Performance improvements were additive when we used them
together. The resulting parser is currently the best constituent parser for German (with
or without bilingual features).
New languages and even new domains can require new treebanks. To create such
a treebank for a MR&LC language, we would first annotate a small number of gold
82
Fraser et al Knowledge Sources for Parsing German
standard trees, using parallel text with English or another language if such text is
available. Next, wewould consider how to quickly differentiate constituents of the same
type using constituent labels plus grammatical functions, as we outlined in Section 3.
Following this, we would use BitPar to build a parser in the same way as we presented
here, and to determine the optimal level of markovization, which we assume would be
very high with a small number of gold standard training trees. Next, as more trees are
annotated in an active learning framework, we would begin to develop morphological
analysis. We would implement the bilingual framework following this (if we have
access to bitext). Then we would implement basic subcategorization extraction and add
monolingual features. Finally, as more gold standard trees are annotated, the reranking
framework should be constantly retrained. In particular, we expect that the effect of the
knowledge sources we have presented will be much stronger when starting with less
training data.
Our work in this paper will be of use to developers of German syntactic parsers
as we have state-of-the-art performance using linguistically motivated features that are
easy to understand. We also hope that our work can serve as a cookbook of ideas to try
for others working on parsers for other morphologically rich languages.
Acknowledgments
We would like to thank Sandra Ku?bler and
Yannick Versley. We gratefully acknowledge
Deutsche Forschungsgemeinschaft (DFG)
for funding this work (grants SCHU 2246/
6-1Morphosyntax for MT and SFB 732
D4Modular lexicalization of PCFGs). This
work was supported in part by the IST
Programme of the European Community,
under the PASCAL2 Network of Excellence,
IST-2007-216886. This publication only
reflects the authors? views.
References
Baroni, Marco and Adam Kilgarriff. 2006.
Large linguistically processed Web
corpora for multiple languages.
In EACL: Posters & Demonstrations,
pages 87?90, Trento.
Black, E., S. Abney, S. Flickenger,
C. Gdaniec, C. Grishman, P. Harrison,
D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus,
S. Roukos, B. Santorini, and
T. Strzalkowski. 1991. Procedure for
quantitatively comparing the syntactic
coverage of English grammars. In
Proceedings of the Workshop on Speech
and Natural Language, HLT ?91,
pages 306?311, Pacific Grove, CA.
Brants, Sabine, Stefanie Dipper, Silvia
Hansen, Wolfgang Lezius, and George
Smith. 2002. The TIGER Treebank.
In Proceedings of the Workshop on
Treebanks and Linguistic Theories,
pages 24?41, Sozopol.
Brants, Thorsten. 2000. TnT?a statistical
part-of-speech tagger. In ANLP,
pages 224?231, Seattle, WA.
Burkett, David, John Blitzer, and Dan Klein.
2010. Joint parsing and alignment
with weakly synchronized grammars.
In HLT-NAACL, pages 127?135,
Los Angeles, CA.
Burkett, David and Dan Klein. 2008. Two
languages are better than one (for syntactic
parsing). In EMNLP, pages 877?886,
Honolulu, HI.
Cai, Shu, David Chiang, and Yoav Goldberg.
2011. Language-independent parsing with
empty elements. In ACL, pages 212?216,
Portland, OR.
Campbell, Richard. 2004. Using linguistic
principles to recover empty categories.
In ACL, pages 645?652, Barcelona.
Charniak, Eugene and Mark Johnson. 2005.
Coarse-to-fine n-best parsing and MaxEnt
discriminative reranking. In ACL,
pages 173?180, Ann Arbor, MI.
Collins, Michael. 1997. Three generative,
lexicalized models for statistical parsing.
In ACL, pages 16?23, Madrid.
Collins, Michael. 2000. Discriminative
reranking for natural language parsing.
In ICML, pages 25?70, Stanford, CA.
Cutting, Doug, Julian Kupiec, Jan Pedersen,
and Penelope Sibun. 1992. A practical
part-of-speech tagger. In ANLP,
pages 133?140, Trento.
Dreyer, Markus, David A. Smith, and
Noah A. Smith. 2006. Vine parsing and
minimum risk reranking for speed and
precision. In CoNLL, pages 201?205,
New York, NY.
83
Computational Linguistics Volume 39, Number 1
Dubey, Amit. 2004. Statistical Parsing for
German: Modeling Syntactic Properties
and Annotation Differences. Ph.D. thesis,
Saarland University.
Dubey, Amit and Frank Keller. 2003.
Probabilistic parsing for German using
sister-head dependencies. In ACL,
pages 96?103, Sapporo.
Duchier, Denys and Ralph Debusmann.
2001. Topological dependency trees:
a constraint-based account of linear
precedence. In ACL, pages 180?187,
Toulouse.
Finkel, Jenny Rose, Alex Kleeman, and
Christopher D. Manning. 2008. Efficient,
feature-based, conditional random
field parsing. In ACL, pages 959?967,
Columbus, OH.
Forst, Martin. 2007. Filling statistics
with linguistics?property design
for the disambiguation of German
LFG parses. In Proceedings of the ACL
Workshop on Deep Linguistic Processing,
pages 17?24, Prague.
Fossum, Victoria and Kevin Knight. 2008.
Using bilingual Chinese?English word
alignments to resolve PP-attachment
ambiguity in English. In AMTA,
pages 48?53, Honolulu, HI.
Fraser, Alexander, Renjing Wang,
and Hinrich Schu?tze. 2009. Rich
bitext projection features for parse
reranking. In EACL, pages 282?290,
Athens.
Gabbard, Ryan, Mitchell Marcus, and Seth
Kulick. 2006. Fully parsing the Penn
Treebank. In HLT-NAACL, pages 184?191,
Morristown, NJ.
Hall, Johan and Joakim Nivre. 2008.
A dependency-driven parser for
German dependency and constituency
representations. In Proceedings of the
Workshop on Parsing German, pages 47?54,
Columbus, OH.
Henderson, James, Paola Merlo, Gabriele
Musillo, and Ivan Titov. 2008. A latent
variable model of synchronous parsing
for syntactic and semantic dependencies.
In CoNLL, pages 178?182, Manchester.
Hsu, Yu-Yin. 2010. Comparing conversions
of discontinuity in PCFG parsing. In TLT,
pages 103?113, Tartu.
Huang, Liang. 2008. Forest reranking:
Discriminative parsing with non-local
features. In ACL, pages 586?594,
Columbus, OH.
Huang, Liang, Wenbin Jiang, and
Qun Liu. 2009. Bilingually constrained
(monolingual) shift-reduce parsing.
In EMNLP, pages 1,222?1,231,
Singapore.
Johnson, Mark. 1998. PCFG models
of linguistic tree representations.
Computational Linguistics, 24(4):613?632.
Johnson, Mark. 2001. A simple pattern-
matching algorithm for recovering empty
nodes and their antecedents. In ACL,
pages 136?143, Philadelphia, PA.
Klein, Dan and Christopher D. Manning.
2003. Accurate unlexicalized parsing.
In ACL, pages 423?430, Sapporo.
Koehn, Philipp. 2005. Europarl: a parallel
corpus for statistical machine translation.
InMT Summit X, pages 79?86, Phuket.
Ku?bler, Sandra. 2008. The PaGe 2008 shared
task on parsing German. In Proceedings
of the Workshop on Parsing German,
pages 55?63, Columbus, OH.
Ku?bler, Sandra, Erhard W. Hinrichs, and
Wolfgang Maier. 2006. Is it really that
difficult to parse German? In EMNLP,
pages 111?119, Sydney.
Levy, Roger and Christopher D. Manning.
2004. Deep dependencies from context-free
statistical parsers: Correcting the surface
dependency approximation. In ACL,
pages 327?334, Barcelona.
Lezius, Wolfgang, Stefanie Dipper, and Arne
Fitschen. 2000. IMSLex?representing
morphological and syntactical information
in a relational database. In EURALEX,
pages 133?139, Stuttgart.
McCallum, Andrew Kachites. 2002. Mallet:
A machine learning for language toolkit.
http://mallet.cs.umass.edu.
McClosky, David, Eugene Charniak,
and Mark Johnson. 2006a. Effective
self-training for parsing. In HLT-NAACL,
pages 152?159, Morristown, NJ.
McClosky, David, Eugene Charniak,
and Mark Johnson. 2006b. Reranking
and self-training for parser adaptation.
In COLING-ACL, pages 337?344,
Sydney.
McDonald, Ryan and Fernando Pereira.
2006. Online learning of approximate
dependency parsing algorithms.
In EACL, pages 81?88, Trento.
Menzel, Wolfgang and Ingo Schro?der.
1998. Decision procedures for dependency
parsing using graded constraints.
In COLING-ACL Workshop on Processing
of Dependency-Based Grammars,
pages 78?87, Montreal.
Pado?, Sebastian and Mirella Lapata. 2009.
Cross-lingual annotation projection of
semantic roles. Journal of Artificial
Intelligence Research, 36:307?340.
84
Fraser et al Knowledge Sources for Parsing German
Petrov, Slav and Dan Klein. 2007. Improved
inference for unlexicalized parsing.
In HLT-NAACL, pages 404?411,
Rochester, NY.
Petrov, Slav and Dan Klein. 2008. Parsing
German with latent variable grammars.
In Proceedings of the Workshop on Parsing
German, pages 33?39, Columbus, OH.
Quirk, Chris and Simon Corston-Oliver.
2006. The impact of parse quality on
syntactically-informed statistical
machine translation. In EMNLP,
pages 62?69, Sydney.
Quirk, Chris, Arul Menezes, and
Colin Cherry. 2005. Dependency treelet
translation: Syntactically informed
phrasal SMT. In ACL, pages 271?279,
Oxford.
Rafferty, Anna and Christopher D. Manning.
2008. Parsing three German Treebanks:
Lexicalized and unlexicalized baselines.
In Proceedings of the Workshop on Parsing
German, pages 40?46, Columbus, OH.
Rambow, Owen. 2010. The simple truth
about dependency and phrase structure
representations: an opinion piece.
In HLT-NAACL, pages 337?340,
Los Angeles, CA.
Rehbein, Ines and Josef van Genabith.
2007. Evaluating evaluation measures.
In NODALIDA, pages 372?379, Tartu.
Schiehlen, Michael. 2004. Annotation
strategies for probabilistic parsing in
German. In COLING, pages 390?396,
Geneva.
Schmid, Helmut. 1995. Improvements in
part-of-speech tagging with an application
to German. In Proceedings of the ACL
SIGDAT-Workshop, pages 47?50, Dublin.
Schmid, Helmut. 2004. Efficient parsing
of highly ambiguous context-free
grammars with bit vectors. In COLING,
pages 162?168, Geneva.
Schmid, Helmut. 2006. Trace prediction
and recovery with unlexicalized PCFGs
and slash features. In COLING-ACL,
pages 177?184, Sydney.
Schmid, Helmut, Arne Fitschen, and
Ulrich Heid. 2004. SMOR: A German
computational morphology covering
derivation, composition and inflection.
In LREC, pages 1,263?1,266, Lisbon.
Seeker, Wolfgang, Bernd Bohnet, Lilja
?vrelid, and Jonas Kuhn. 2010a.
Informed ways of improving data-driven
dependency parsing for German. In
COLING: Posters, pages 1,122?1,130,
Beijing.
Seeker, Wolfgang, Ines Rehbein, Jonas Kuhn,
and Josef Van Genabith. 2010b. Hard
constraints for grammatical function
labelling. In ACL, pages 1,087?1,097,
Uppsala.
Shen, Libin, Jinxi Xu, and Ralph Weischedel.
2008. A new string-to-dependency
machine translation algorithm with a
target dependency language model. In
ACL-HLT, pages 577?585, Columbus, OH.
Tsarfaty, Reut, Joakim Nivre, and Evelina
Andersson. 2012. Cross-framework
evaluation for statistical parsing.
In EACL, pages 44?54, Avignon.
Tsarfaty, Reut, Djame? Seddah, Yoav
Goldberg, Sandra Kuebler, Yannick
Versley, Marie Candito, Jennifer Foster,
Ines Rehbein, and Lamia Tounsi. 2010.
Statistical parsing of morphologically
rich languages (SPMRL) what, how and
whither. In Proceedings of the NAACL HLT
2010 First Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 1?12,
Los Angeles, CA.
Tu, Zhaopeng, Yang Liu, Young-Sook
Hwang, Qun Liu, and Shouxun Lin. 2010.
Dependency forest for statistical machine
translation. In COLING, pages 1,092?1,100,
Beijing.
Versley, Yannick. 2005. Parser evaluation
across text types. In Fourth Workshop on
Treebanks and Linguistic Theories (TLT),
pages 209?220, Barcelona.
Versley, Yannick and Ines Rehbein. 2009.
Scalable discriminative parsing for
German. In IWPT, pages 134?137, Paris.
Weischedel, Ralph, Marie Meteer, Richard
Schwartz, Lance Ramshaw, and Jeff
Palmucci. 1993. Coping with ambiguity
and unknown words through probabilistic
models. Computational Linguistics,
19(2):359?382.
Yusuke, Miyao and Tsujii Jun?ichi. 2002.
Maximum entropy estimation for
feature forests. In HLT, pages 292?297,
San Diego, CA.
85

2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 386?395,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
A Comparative Investigation of Morphological Language Modeling for the
Languages of the European Union
Thomas Mu?ller, Hinrich Schu?tze and Helmut Schmid
Institute for Natural Language Processing
University of Stuttgart, Germany
{muellets,schmid}@ims.uni-stuttgart.de
Abstract
We investigate a language model that com-
bines morphological and shape features with
a Kneser-Ney model and test it in a large
crosslingual study of European languages.
Even though the model is generic and we use
the same architecture and features for all lan-
guages, the model achieves reductions in per-
plexity for all 21 languages represented in the
Europarl corpus, ranging from 3% to 11%. We
show that almost all of this perplexity reduc-
tion can be achieved by identifying suffixes by
frequency.
1 Introduction
Language models are fundamental to many natural
language processing applications. In the most com-
mon approach, language models estimate the proba-
bility of the next word based on one or more equiv-
alence classes that the history of preceding words is
a member of. The inherent productivity of natural
language poses a problem in this regard because the
history may be rare or unseen or have unusual prop-
erties that make assignment to a predictive equiva-
lence class difficult.
In many languages, morphology is a key source
of productivity that gives rise to rare and unseen
histories. For example, even if a model can learn
that words like ?large?, ?dangerous? and ?serious?
are likely to occur after the relatively frequent his-
tory ?potentially?, this knowledge cannot be trans-
ferred to the rare history ?hypothetically? without
some generalization mechanism like morphological
analysis.
Our primary goal in this paper is not to de-
velop optimized language models for individual lan-
guages. Instead, we investigate whether a simple
generic language model that uses shape and mor-
phological features can be made to work well across
a large number of languages. We find that this is
the case: we achieve considerable perplexity reduc-
tions for all 21 languages in the Europarl corpus.
We see this as evidence that morphological language
modeling should be considered as a standard part of
any language model, even for languages like English
that are often not viewed as a good application of
morphological modeling due to their morphological
simplicity.
To understand which factors are important for
good performance of the morphological compo-
nent of a language model, we perform an exten-
sive crosslingual analysis of our experimental re-
sults. We look at three parameters of the morpho-
logical model we propose: the frequency threshold
? that divides words subject to morphological clus-
tering from those that are not; the number of suffixes
used ?; and three different morphological segmen-
tation algorithms. We also investigate the differen-
tial effect of morphological language modeling on
different word shapes: alphabetical words, punctua-
tion, numbers and other shapes.
Some prior work has used morphological models
that require careful linguistic analysis and language-
dependent adaptation. In this paper we show that
simple frequency analysis performs only slightly
worse than more sophisticated morphological anal-
ysis. This potentially removes a hurdle to using
morphological models in cases where sufficient re-
sources to do the extra work required for sophisti-
cated morphological analysis are not available.
The motivation for using morphology in lan-
guage modeling is similar to distributional clustering
386
(Brown et al, 1992). In both cases, we form equiv-
alence classes of words with similar distributional
behavior. In a preliminary experiment, we find that
morphological equivalence classes reduce perplex-
ity as much as traditional distributional classes ? a
surprising result we intend to investigate in future
work.
The main contributions of this paper are as fol-
lows. We present a language model design and a
set of morphological and shape features that achieve
reductions in perplexity for all 21 languages rep-
resented in the Europarl corpus, ranging from 3%
to 11%, compared to a Kneser-Ney model. We
show that identifying suffixes by frequency is suf-
ficient for getting almost all of this perplexity reduc-
tion. More sophisticated morphological segmenta-
tion methods do not further increase perplexity or
just slightly. Finally, we show that there is one pa-
rameter that must be tuned for good performance for
most languages: the frequency threshold ? above
which a word is not subject to morphological gen-
eralization because it occurs frequently enough for
standard word n-gram language models to use it ef-
fectively for prediction.
The paper is organized as follows. In Section 2
we discuss related work. In Section 3 we describe
the morphological and shape features we use. Sec-
tion 4 introduces language model and experimental
setup. Section 5 discusses our results. Section 6
summarizes the contributions of this paper.
2 Related Work
Whittaker and Woodland (2000) apply language
modeling to morpheme sequences and investigate
data-driven segmentation methods. Creutz et al
(2007) propose a similar method that improves
speech recognition for highly inflecting languages.
They use Morfessor (Creutz and Lagus, 2007) to
split words into morphemes. Both approaches are
essentially a simple form of a factored language
model (FLM) (Bilmes and Kirchhoff, 2003). In a
general FLM a number of different back-off paths
are combined by a back-off function to improve the
prediction after rare or unseen histories. Vergyri et
al. (2004) apply FLMs and morphological features
to Arabic speech recognition.
These papers and other prior work on using mor-
phology in language modeling have been language-
specific and have paid less attention to the ques-
tion as to how morphology can be useful across
languages and what generic methods are appropri-
ate for this goal. Previous work also has concen-
trated on traditional linguistic morphology whereas
we compare linguistically motivated morphologi-
cal segmentation with frequency-based segmenta-
tion and include shape features in our study.
Our initial plan for this paper was to use com-
plex language modeling frameworks that allow ex-
perimenters to include arbitrary features (including
morphological and shape features) in the model. In
particular, we looked at publicly available imple-
mentations of maximum entropy models (Rosen-
feld, 1996; Berger et al, 1996) and random forests
(Xu and Jelinek, 2004). However, we found that
these methods do not currently scale to running a
large set of experiments on a multi-gigabyte parallel
corpus of 21 languages. Similar considerations ap-
ply to other sophisticated language modeling tech-
niques like Pitman-Yor processes (Teh, 2006), re-
current neural networks (Mikolov et al, 2010) and
FLMs in their general, more powerful form. In ad-
dition, perplexity reductions of these complex mod-
els compared to simpler state-of-the-art models are
generally not large.
We therefore decided to conduct our study in the
framework of smoothed n-gram models, which cur-
rently are an order of magnitude faster and more
scalable. More specifically, we adopt a class-based
approach, where words are clustered based on mor-
phological and shape features. This approach has the
nice property that the number of features used to es-
timate the classes does not influence the time needed
to train the class language model, once the classes
have been found. This is an important consideration
in the context of the questions asked in this paper as
it allows us to use large numbers of features in our
experiments.
3 Modeling of morphology and shape
Our basic approach is to define a number of morpho-
logical and shape features and then assign all words
with identical feature values to one class. For the
morphological features, we investigate three differ-
ent automatic suffix identification algorithms: Re-
387
s, e, d, ed, n, g, ng, ing, y, t, es, r, a, l, on, er, ion,
ted, ly, tion, rs, al, o, ts, ns, le, i, ation, an, ers, m, nt,
ting, h, c, te, sed, ated, en, ty, ic, k, ent, st, ss, ons, se,
ity, ble, ne, ce, ess, ions, us, ry, re, ies, ve, p, ate, in,
tions, ia, red, able, is, ive, ness, lly, ring, ment, led,
ned, tes, as, ls, ding, ling, sing, ds, ded, ian, nce, ar,
ating, sm, ally, nts, de, nd, ism, or, ge, ist, ses, ning,
u, king, na, el
Figure 1: The 100 most frequent English suffixes in Eu-
roparl, ordered by frequency
ports (Keshava and Pitler, 2006), Morfessor (Creutz
and Lagus, 2007) and Frequency, where Frequency
simply selects the most frequent word-final letter se-
quences as suffixes. The 100 most frequent suffixes
found by Frequency for English are given in Fig-
ure 1.
We use the ? most frequent suffixes for all three
algorithms, where ? is a parameter. The focus of our
work is to evaluate the utility of these algorithms for
language modeling; we do not directly evaluate the
quality of the suffixes.
A word is segmented by identifying the longest of
the ? suffixes that it ends with. Thus, each word has
one suffix feature if it ends with one of the ? suffixes
and none otherwise.
In addition to suffix features, we define features
that capture shape properties: capitalization, special
characters and word length. If a word in the test set
has a combination of feature values that does not oc-
cur in the training set, then it is assigned to the class
whose features are most similar. We described the
similarity measure and details of the shape features
in prior work (Mu?ller and Schu?tze, 2011). The shape
features are listed in Table 1.
4 Experimental Setup
Experiments are performed using srilm (Stolcke,
2002), in particular the Kneser-Ney (KN) and
generic class model implementations. Estimation of
optimal interpolation parameters is based on (Bahl
et al, 1991).
4.1 Baseline
Our baseline is a modified KN model (Chen and
Goodman, 1999).
4.2 Morphological class language model
We use a variation of the model proposed by Brown
et al (1992) that we developed in prior work on En-
glish (Mu?ller and Schu?tze, 2011). This model is a
class-based language model that groups words into
classes and replaces the word transition probability
by a class transition probability and a word emission
probability:
PC(wi|wi?1i?N+1) =
P (g(wi)|g(wi?1i?N+1)) ? P (wi|g(wi))
where g(w) is the class of word w and we write
g(wi . . . wj) for g(wi) . . . g(wj).
Our approach targets rare and unseen histories.
We therefore exclude all frequent words from clus-
tering on the assumption that enough training data
is available for them. Thus, clustering of words is
restricted to those below a certain token frequency
threshold ?. As described above, we simply group
all words with identical feature values into one class.
Words with a training set frequency above ? are
added as singletons. The class transition probabil-
ity P (g(wi)|g(wi?1i?N+1)) is estimated using Witten-
Bell smoothing.1
The word emission probability is defined as fol-
lows:
P (w|c) =
?
??
??
1 , N(w) > ?
N(w)P
w?c N(w) ?
?(c)
|c|?1 , ??N(w)>0
?(c) , N(w) = 0
where c = g(w) is w?s class and N(w) is the fre-
quency of w in the training set. The class-dependent
out-of-vocabulary (OOV) rate ?(c) is estimated on
held-out data. Our final model PM interpolates PC
with a modified KN model:
PM (wi|wi?N+1i?1 ) =
?(g(wi?1)) ? PC(wi|wi?N+1i?1 )
+(1? ?(g(wi?1))) ? PKN(wi|wi?N+1i?1 ) (1)
This model can be viewed as a generalization of
the simple interpolation ?PC + (1? ?)PW used by
Brown et al (1992) (where PW is a word n-gram
1Witten-Bell smoothing outperformed modified Kneser-Ney
(KN) and Good-Turing (GT).
388
is capital(w) first character of w is an uppercase letter
is all capital(w) ? c ? w : c is an uppercase letter
capital character(w) ? c ? w : c is an uppercase letter
appears in lowercase(w) ?capital character(w) ? w? ? ?T
special character(w) ? c ? w : c is not a letter or digit
digit(w) ? c ? w : c is a digit
is number(w) w ? L([+? ?][0? 9] (([., ][0? 9])|[0? 9]) ?)
Table 1: Shape features as defined by Mu?ller and Schu?tze (2011). ?T is the vocabulary of the training corpus T , w? is
obtained from w by changing all uppercase letters to lowercase and L(expr) is the language generated by the regular
expression expr.
model and PC a class n-gram model). For the set-
ting ? = ? (clustering of all words), our model is
essentially a simple interpolation of a word n-gram
and a class n-gram model except that the interpola-
tion parameters are optimized for each class instead
of using the same interpolation parameter ? for all
classes. We have found that ? = ? is never optimal;
it is always beneficial to assign the most frequent
words to their own singleton classes.
Following Yuret and Bic?ici (2009), we evaluate
models on the task of predicting the next word from
a vocabulary that consists of all words that occur
more than once in the training corpus and the un-
known word UNK. Performing this evaluation for
KN is straightforward: we map all words with fre-
quency one in the training set to UNK and then com-
pute PKN(UNK |h) in testing.
In contrast, computing probability estimates for
PC is more complicated. We define the vocabulary
of the morphological model as the set of all words
found in the training corpus, including frequency-1
words, and one unknown word for each class. We
do this because ? as we argued above ? morpholog-
ical generalization is only expected to be useful for
rare words, so we are likely to get optimal perfor-
mance for PC if we include all words in clustering
and probability estimation, including hapax legom-
ena. Since our testing setup only evaluates on words
that occur more than once in the training set, we ide-
ally would want to compute the following estimate
when predicting the unknown word:
PC(UNKKN |h) =?
{w:N(w)=1}
PC(w|h) +
?
c
PC(UNKc |h) (2)
where we distinguish the unknown words of the
morphological classes from the unknown word used
in evaluation and by the KN model by giving the lat-
ter the subscript KN.
However, Eq. 2 cannot be computed efficiently
and we would not be able to compute it in practical
applications that require fast language models. For
this reason, we use the modified class model P ?C in
Eq. 1 that is defined as follows:
P ?C(w|h) =
{ PC(w|h) , N(w) ? 1
PC(UNKg(w) |h), N(w) = 0
P ?C and ? by extension ? PM are deficient. This
means that the evaluation of PM we present below
is pessimistic in the sense that the perplexity reduc-
tions would probably be higher if we were willing to
spend additional computational resources and com-
pute Eq. 2 in its full form.
4.3 Distributional class language model
The most frequently used type of class-based lan-
guage model is the distributional model introduced
by Brown et al (1992). To understand the dif-
ferences between distributional and morphological
class language models, we compare our morpholog-
ical model PM with a distributional model PD that
has exactly the same form as PM; in particular, it
is defined by Equations (1) and (2). The only dif-
ference is that the classes are morphological for PM
and distributional for PD.
The exchange algorithm that was used by Brown
et al (1992) has very long running times for large
corpora in standard implementations like srilm. It
is difficult to conduct the large number of cluster-
ings necessary for an extensive study like ours using
standard implementations.
389
Language T/T ? #Sentences
S bg Bulgarian .0183 .0094 181,415
S cs Czech .0185 .0097 369,881
S pl Polish .0189 .0096 358,747
S sk Slovak .0187 .0088 368,624
S sl Slovene .0156 .0090 365,455
G da Danish .0086 .0077 1,428,620
G de German .0091 .0073 1,391,324
G en English .0028 .0023 1,460,062
G nl Dutch .0061 .0048 1,457,629
G sv Swedish .0090 .0095 1,342,667
E el Greek .0081 .0079 851,636
R es Spanish .0040 .0031 1,429,276
R fr French .0029 .0024 1,460,062
R it Italian .0040 .0030 1,389,665
R pt Portuguese .0042 .0032 1,426,750
R ro Romanian .0142 .0079 178,284
U et Estonian .0329 .0198 375,698
U fi Finnish .0231 .0183 1,394,043
U hu Hungarian .0312 .0163 364,216
B lt Lithuanian .0265 .0147 365,437
B lv Latvian .0182 .0086 363,104
Table 2: Statistics for the 21 languages. S = Slavic, G
= Germanic, E = Greek, R = Romance, U = Uralic, B
= Baltic. Type/token ratio (T/T) and # sentences for the
training set and OOV rate ? for the validation set. The
two smallest and largest values in each column are bold.
We therefore induce the distributional classes
as clusters in a whole-context distributional vector
space model (Schu?tze and Walsh, 2011), a model
similar to the ones described by Schu?tze (1992)
and Turney and Pantel (2010) except that dimension
words are immediate left and right neighbors (as op-
posed to neighbors within a window or specific types
of governors or dependents). Schu?tze and Walsh
(2011) present experimental evidence that suggests
that the resulting classes are competitive with Brown
classes.
4.4 Corpus
Our experiments are performed on the Europarl cor-
pus (Koehn, 2005), a parallel corpus of proceed-
ings of the European Parliament in 21 languages.
The languages are members of the following fam-
ilies: Baltic languages (Latvian, Lithuanian), Ger-
manic languages (Danish, Dutch, English, Ger-
man, Swedish), Romance languages (French, Ital-
ian, Portuguese, Romanian, Spanish), Slavic lan-
guages (Bulgarian, Czech, Polish, Slovak, Slovene),
Uralic languages (Estonian, Finnish, Hungarian)
and Greek. We only use the part of the corpus that
can be aligned to English sentences. All 21 corpora
are divided into training set (80%), validation set
(10%) and test set (10%). The training set is used for
morphological and distributional clustering and esti-
mation of class and KN models. The validation set
is used to estimate the OOV rates ? and the optimal
parameters ?, ? and ?. Table 2 gives basic statistics
about the corpus. The sizes of the corpora of lan-
guages whose countries have joined the European
community more recently are smaller than for coun-
tries who have been members for several decades.
We see that English and French have the lowest
type/token ratios and OOV rates; and the Uralic lan-
guages (Estonian, Finnish, Hungarian) and Lithua-
nian the highest. The Slavic languages have higher
values than the Germanic languages, which in turn
have higher values than the Romance languages ex-
cept for Romanian. Type/token ratio and OOV
rate are one indicator of how much improvement
we would expect from a language model with
a morphological component compared to a non-
morphological language model.2
5 Results and Discussion
We performed all our experiments with an n-gram
order of 4; this was the order for which the KN
model performs best for all languages on the vali-
dation set.
5.1 Morphological model
Using grid search, we first determined on the vali-
dation set the optimal combination of three param-
eters: (i) ? ? {100, 200, 500, 1000, 2000, 5000},
(ii) ? ? {50, 100, 200, 500} and (iii) segmentation
method. Recall that we only cluster words whose
frequency is below ? and only consider the ? most
2The tokenization of the Europarl corpus has a preference
for splitting tokens in unclear cases. OOV rates would be higher
for more conservative tokenization strategies.
4A two-tailed paired t-test on the improvements by language
shows that the morphological model significantly outperforms
the distributional model with p=0.0027. A test on the Germanic,
Romance and Greek languages yields p=0.19.
390
PPKN ??M ?? M? PPC PPM ?M ??D PPWC PPD ?D
S bg 74 200 50 f 103 69 0.07 500 141 71 0.04
S cs 141 500 100 f 217 129 0.08 1000 298 134 0.04
S pl 148 500 100 m 241 134 0.09 1000 349 141 0.05
S sk 123 500 200 f 186 111 0.10 1000 261 116 0.06
S sl 118 500 100 m 177 107 0.09 1000 232 111 0.06
G da 69 1000 100 r 89 65 0.05 2000 103 65 0.05
G de 100 2000 50 m 146 94 0.06 2000 150 94 0.06
G en 55 2000 50 f 73 53 0.03 5000 87 53 0.04
G nl 70 2000 50 r 100 67 0.04 5000 114 67 0.05
G sv 98 1000 50 m 132 92 0.06 2000 154 92 0.06
E el 80 1000 100 f 108 73 0.08 2000 134 74 0.07
R es 57 2000 100 m 77 54 0.05 5000 93 54 0.05
R fr 45 1000 50 f 56 43 0.04 5000 71 42 0.05
R it 69 2000 100 m 101 66 0.04 2000 100 66 0.05
R pt 62 2000 50 m 88 59 0.05 2000 87 59 0.05
R ro 76 500 100 m 121 70 0.07 1000 147 71 0.07
U et 256 500 100 m 422 230 0.10 1000 668 248 0.03
U fi 271 1000 500 f 410 240 0.11 2000 706 261 0.04
U hu 151 200 200 m 222 136 0.09 1000 360 145 0.03
B lt 175 500 200 m 278 161 0.08 1000 426 169 0.03
B lv 154 500 200 f 237 142 0.08 1000 322 147 0.05
Table 3: Perplexities on the test set for N = 4. S = Slavic, G = Germanic, E = Greek, R = Romance, U =
Uralic, B = Baltic. ??x, ?? and M? denote frequency threshold, suffix count and segmentation method optimal on the
validation set. The letters f, m and r stand for the frequency-based method, Morfessor and Reports. PPKN, PPC,
PPM, PPWC, PPD are the perplexities of KN, morphological class model, interpolated morphological class model,
distributional class model and interpolated distributional class model, respectively. ?x denotes relative improvement:
(PPKN?PPx)/PPKN. Bold numbers denote maxima and minima in the respective column.4
frequent suffixes. An experiment with the optimal
configuration was then run on the test set. The re-
sults are shown in Table 3. The KN perplexities vary
between 45 for French and 271 for Finnish.
The main result is that the morphological model
PM consistently achieves better performance than
KN (columns PPM and ?M), in particular for
Slavic, Uralic and Baltic languages and Greek. Im-
provements range from 0.03 for English to 0.11 for
Finnish.
Column ??M gives the threshold that is optimal for
the validation set. Values range from 200 to 2000.
Column ?? gives the optimal number of suffixes. It
ranges from 50 to 500. The morphologically com-
plex language Finnish seems to benefit from more
suffixes than morphologically simple languages like
Dutch, English and German, but there are a few lan-
guages that do not fit this generalization, e.g., Esto-
nian for which 100 suffixes are optimal.
The optimal morphological segmenter is given in
column M?: f = Frequency, r = Reports, m = Mor-
fessor. The most sophisticated segmenter, Morfes-
sor is optimal for about half of the 21 languages, but
Frequency does surprisingly well. Reports is opti-
mal for two languages, Danish and Dutch. In gen-
eral, Morfessor seems to have an advantage for com-
plex morphologies, but is beaten by Frequency for
Finnish and Latvian.
5.2 Distributional model
Columns PPD and ?D show the performance of the
distributional class language model. As one would
perhaps expect, the morphological model is superior
to the distributional model for morphologically com-
plex languages like Estonian, Finnish and Hungar-
ian. These languages have many suffixes that have
391
??+ ???? ?+ ?? ??+ ???? ?+ ?? ?M+ ??M? M+ M?
S bg 0.03 200 5000 0.01 50 500 f m
S cs 0.03 500 5000 100 500 f r
S pl 0.03 500 5000 0.01 100 500 m r
S sk 0.02 500 5000 200 500 0.01 f r
S sl 0.03 500 5000 0.01 100 500 m r
G da 0.02 1000 100 100 50 r f
G de 0.02 2000 100 50 500 m f
G en 0.01 2000 100 50 500 f r
G nl 0.01 2000 100 50 500 r f
G sv 0.02 1000 100 50 500 m f
E el 0.02 1000 100 100 500 0.01 f r
R es 0.02 2000 100 100 500 m r
R fr 0.01 1000 100 50 500 f r
R it 0.01 2000 100 100 500 m r
R pt 0.02 2000 100 50 500 m r
R ro 0.03 500 5000 100 500 m r
U et 0.02 500 5000 0.01 100 50 0.01 m r
U fi 0.03 1000 100 0.03 500 50 0.02 f r
U hu 0.03 200 5000 0.01 200 50 m r
B lt 0.02 500 5000 200 50 m r
B lv 0.02 500 5000 200 500 f r
Table 4: Sensitivity of perplexity values to the parameters (on the validation set). S = Slavic, G = Germanic, E =
Greek, R = Romance, U = Uralic, B = Baltic. ?x+ and ?x? denote the relative improvement of PM over the KN
model when parameter x is set to the best (x+) and worst value (x?), respectively. The remaining parameters are set
to the optimal values of Table 3. Cells with differences of relative improvements that are smaller than 0.01 are left
empty.
high predictive power for the distributional contexts
in which a word can occur. A morphological model
can exploit this information even if a word with an
informative suffix did not occur in one of the lin-
guistically licensed contexts in the training set. For
a distributional model it is harder to learn this type
of generalization.
What is surprising about the comparative perfor-
mance of morphological and distributional models is
that there is no language for which the distributional
model outperforms the morphological model by a
wide margin. Perplexity reductions are lower than
or the same as those of the morphological model
in most cases, with only four exceptions ? English,
French, Italian, and Dutch ? where the distributional
model is better by one percentage point than the
morphological model (0.05 vs. 0.04 and 0.04 vs.
0.03).
Column ??D gives the frequency threshold for the
distributional model. The optimal threshold ranges
from 500 to 5000. This means that the distributional
model benefits from restricting clustering to less fre-
quent words ? and behaves similarly to the morpho-
logical class model in that respect. We know of no
previous work that has conducted experiments on
frequency thresholds for distributional class models
and shown that they increase perplexity reductions.
5.3 Sensitivity analysis of parameters
Table 3 shows results for parameters that were opti-
mized on the validation set. We now want to analyze
how sensitive performance is to the three parame-
ters ?, ? and segmentation method. To this end, we
present in Table 4 the best and worst values of each
parameter and the difference in perplexity improve-
ment between the two.
Differences of perplexity improvement between
best and worst values of ?M range between 0.01
392
and 0.03. The four languages with the smallest
difference 0.01 are morphologically simple (Dutch,
English, French, Italian). The languages with the
largest difference (0.03) are morphologically more
complex languages. In summary, the frequency
threshold ?M has a comparatively strong influence
on perplexity reduction. The strength of the effect is
correlated with the morphological complexity of the
language.
In contrast to ?, the number of suffixes ? and
the segmentation method have negligible effect on
most languages. The perplexity reductions for dif-
ferent values of ? are 0.03 for Finnish, 0.01 for Bul-
garian, Estonian, Hungarian, Polish and Slovenian,
and smaller than 0.01 for the other languages. This
means that, with the exception of Finnish, we can
use a value of ? = 100 for all languages and be very
close to the optimal perplexity reduction ? either be-
cause 100 is optimal or because perplexity reduction
is not sensitive to choice of ?. Finnish is the only
language that clearly benefits from a large number
of suffixes.
Surprisingly, the performance of the morphologi-
cal segmentation methods is very close for 17 of the
21 languages. For three of the four where there is
a difference in improvement of ? 0.01, Frequency
(f) performs best. This means that Frequency is a
good segmentation method for all languages, except
perhaps for Estonian.
5.4 Impact of shape
The basic question we are asking in this paper is
to what extent the sequence of characters a word
is composed of can be exploited for better predic-
tion in language modeling. In the final analysis in
Table 5 we look at four different types of character
sequences and their contributions to perplexity re-
duction. The four groups are alphabetic character
sequences (W), numbers (N), single special charac-
ters (P = punctuation), and other (O). Examples for
O would be ?751st? and words containing special
characters like ?O?Neill?. The parameters used are
the optimal ones of Table 3. Table 5 shows that the
impact of special characters on perplexity is similar
across languages: 0.04 ? ?P ? 0.06. The same is
true for numbers: 0.23 ? ?N ? 0.33, with two out-
liers that show a stronger effect of this class: Finnish
?N = 0.38 and German ?N = 0.40.
?W ?P ?N ?O
S bg 0.07 0.04 0.28 0.16
S cs 0.09 0.04 0.26 0.33
S pl 0.10 0.05 0.23 0.22
S sk 0.10 0.05 0.25 0.28
S sl 0.10 0.04 0.28 0.28
G da 0.05 0.05 0.31 0.18
G de 0.06 0.05 0.40 0.18
G en 0.03 0.04 0.33 0.14
G nl 0.04 0.05 0.31 0.26
G sv 0.06 0.05 0.31 0.35
E el 0.08 0.05 0.33 0.14
R es 0.05 0.04 0.26 0.14
R fr 0.04 0.04 0.29 0.01
R it 0.04 0.05 0.33 0.02
R pt 0.05 0.05 0.28 0.39
R ro 0.08 0.04 0.25 0.17
U et 0.11 0.05 0.26 0.26
U fi 0.12 0.06 0.38 0.36
U hu 0.10 0.04 0.32 0.23
B lt 0.08 0.06 0.27 0.05
B lv 0.08 0.05 0.26 0.19
Table 5: Relative improvements of PM on the valida-
tion set compared to KN for histories wi?1i?N+1 grouped
by the type of wi?1. The possible types are alphabetic
word (W), punctuation (P), number (N) and other (O).
The fact that special characters and numbers be-
have similarly across languages is encouraging as
one would expect less crosslinguistic variation for
these two classes of words.
In contrast, ?true? words (those exclusively com-
posed of alphabetic characters) show more variation
from language to language: 0.03 ? ?W ? 0.12.
The range of variation is not necessarily larger than
for numbers, but since most words are alphabetical
words, class W is responsible for most of the differ-
ence in perplexity reduction between different lan-
guages. As before we observe a negative correlation
between morphological complexity and perplexity
reduction; e.g., Dutch and English have small ?W
and Estonian and Finnish large values.
We provide the values of ?O for completeness.
The composition of this catch-all group varies con-
siderably from language to language. For exam-
ple, many words in this class are numbers with al-
phabetic suffixes like ?2012-ben? in Hungarian and
393
words with apostrophes in French.
6 Summary
We have investigated an interpolation of a KN model
with a class language model whose classes are de-
fined by morphology and shape features. We tested
this model in a large crosslingual study of European
languages.
Even though the model is generic and we use
the same architecture and features for all languages,
the model achieves reductions in perplexity for all
21 languages represented in the Europarl corpus,
ranging from 3% to 11%, when compared to a KN
model. We found perplexity reductions across all
21 languages for histories ending with four different
types of word shapes: alphabetical words, special
characters, and numbers.
We looked at the sensitivity of perplexity reduc-
tions to three parameters of the model: ?, a thresh-
old that determines for which frequencies words are
given their own class; ?, the number of suffixes used
to determine class membership; and morphological
segmentation. We found that ? has a considerable
influence on the performance of the model and that
optimal values vary from language to language. This
parameter should be tuned when the model is used
in practice.
In contrast, the number of suffixes and the mor-
phological segmentation method only had a small
effect on perplexity reductions. This is a surprising
result since it means that simple identification of suf-
fixes by frequency and choosing a fixed number of
suffixes ? across languages is sufficient for getting
most of the perplexity reduction that is possible.
7 Future Work
A surprising result of our experiments was that the
perplexity reductions due to morphological classes
were generally better than those due to distributional
classes even though distributional classes are formed
directly based on the type of information that a lan-
guage model is evaluated on ? the distribution of
words or which words are likely to occur in se-
quence. An intriguing question is to what extent the
effect of morphological and distributional classes is
additive. We ran an exploratory experiment with
a model that interpolates KN, morphological class
model and distributional class model. This model
only slightly outperformed the interpolation of KN
and morphological class model (column PPM in Ta-
ble 3). We would like to investigate in future work if
the information provided by the two types of classes
is indeed largely redundant or if a more sophisticated
combination would perform better than the simple
linear interpolation we have used here.
Acknowledgments. This research was funded by
DFG (grant SFB 732). We would like to thank the
anonymous reviewers for their valuable comments.
References
Lalit R. Bahl, Peter F. Brown, Peter V. de Souza,
Robert L. Mercer, and David Nahamoo. 1991. A fast
algorithm for deleted interpolation. In Eurospeech.
Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Comput. Linguist.
Jeff A. Bilmes and Katrin Kirchhoff. 2003. Factored
language models and generalized parallel backoff. In
NAACL-HLT.
Peter F. Brown, Peter V. de Souza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Comput.
Linguist.
Stanley F. Chen and Joshua Goodman. 1999. An empir-
ical study of smoothing techniques for language mod-
eling. Computer Speech & Language.
Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphology
learning. ACM TSLP.
Mathias Creutz, Teemu Hirsima?ki, Mikko Kurimo, Antti
Puurula, Janne Pylkko?nen, Vesa Siivola, Matti Var-
jokallio, Ebru Arisoy, Murat Sarac?lar, and Andreas
Stolcke. 2007. Morph-based speech recognition
and modeling of out-of-vocabulary words across lan-
guages. ACM TSLP.
Samarth Keshava and Emily Pitler. 2006. A simpler,
intuitive approach to morpheme induction. In PASCAL
Morpho Challenge.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT summit.
Toma?s? Mikolov, Martin Karafia?t, Luka?s? Burget, Jan
C?ernocky?, and Sanjeev Khudanpur. 2010. Recurrent
neural network based language model. In ICSLP.
Thomas Mu?ller and Hinrich Schu?tze. 2011. Improved
modeling of out-of-vocabulary words using morpho-
logical classes. In ACL.
394
Ronald Rosenfeld. 1996. A maximum entropy approach
to adaptive statistical language modelling. Computer
Speech & Language.
Hinrich Schu?tze and Michael Walsh. 2011. Half-context
language models. Comput. Linguist.
Hinrich Schu?tze. 1992. Dimensions of meaning.
In ACM/IEEE Conference on Supercomputing, pages
787?796.
Andreas Stolcke. 2002. SRILM - An extensible lan-
guage modeling toolkit. In Interspeech.
Yee Whye Teh. 2006. A hierarchical bayesian language
model based on Pitman-Yor processes. In ACL.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of semantics.
JAIR.
Dimitra Vergyri, Katrin Kirchhoff, Kevin Duh, and An-
dreas Stolcke. 2004. Morphology-based language
modeling for Arabic speech recognition. In ICSLP.
E.W.D. Whittaker and P.C. Woodland. 2000. Particle-
based language modelling. In ICSLP.
Peng Xu and Frederick Jelinek. 2004. Random forests in
language modeling. In EMNLP.
Deniz Yuret and Ergun Bic?ici. 2009. Modeling morpho-
logically rich languages using split words and unstruc-
tured dependencies. In ACL-IJCNLP.
395
Proceedings of NAACL-HLT 2013, pages 1?11,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Model With Minimal Translation Units, But Decode With Phrases
Nadir Durrani?
University of Edinburgh
dnadir@inf.ed.ac.uk
Alexander Fraser Helmut Schmid
University of Stuttgart
fraser,schmid@ims.uni-stuttgart.de
Abstract
N-gram-based models co-exist with their
phrase-based counterparts as an alternative
SMT framework. Both techniques have pros
and cons. While the N-gram-based frame-
work provides a better model that captures
both source and target contexts and avoids
spurious phrasal segmentation, the ability to
memorize and produce larger translation units
gives an edge to the phrase-based systems dur-
ing decoding, in terms of better search per-
formance and superior selection of transla-
tion units. In this paper we combine N-gram-
based modeling with phrase-based decoding,
and obtain the benefits of both approaches.
Our experiments show that using this combi-
nation not only improves the search accuracy
of the N-gram model but that it also improves
the BLEU scores. Our system outperforms
state-of-the-art phrase-based systems (Moses
and Phrasal) and N-gram-based systems by
a significant margin on German, French and
Spanish to English translation tasks.
1 Introduction
Statistical Machine Translation advanced from
word-based models (Brown et al, 1993) towards
more sophisticated models that take contextual in-
formation into account. Phrase-based (Och and
Ney, 2004; Koehn et al, 2003) and N-gram-based
(Marin?o et al, 2006) models are two instances of
such frameworks. While the two models have some
common properties, they are substantially different.
?Much of the work presented here was carried out while the
first author was at the University of Stuttgart.
Phrase-based systems employ a simple and effec-
tive machinery by learning larger chunks of trans-
lation called phrases1. Memorizing larger units en-
ables the phrase-based model to learn local depen-
dencies such as short reorderings, idioms, insertions
and deletions, etc. The model however, has the fol-
lowing drawbacks: i) it makes independence as-
sumptions over phrases ignoring the contextual in-
formation outside of phrases ii) it has issues han-
dling long-distance reordering iii) it has the spurious
phrasal segmentation problem which allows multi-
ple derivations of a bilingual sentence pair having
different model scores for each segmentation.
Modeling with minimal translation units helps ad-
dress some of these issues. The N-gram-based SMT
framework is based on tuples. Tuples are mini-
mal translation units composed of source and target
cepts2. N-gram-based models are Markov models
over sequences of tuples (Marin?o et al, 2006; Crego
and Marin?o, 2006) or operations encapsulating tu-
ples (Durrani et al, 2011). This mechanism has sev-
eral useful properties. Firstly, no phrasal indepen-
dence assumption is made. The model has access
to both source and target context outside of phrases.
Secondly the model learns a unique derivation of a
bilingual sentence given its alignment, thus avoiding
the spurious segmentation problem.
Using minimal translation units, however, results
in a higher number of search errors because of i)
1A phrase-pair in PBSMT is a sequence of source and target
words that is translation of each other, and is not necessarily a
linguistic constituent. Phrases are built by combining minimal
translation units and ordering information.
2A cept is a group of words in one language that is translated
as a minimal unit in one specific context (Brown et al, 1993).
1
poor translation selection, ii) inaccurate future-cost
estimates and iii) incorrect early pruning of hypothe-
ses that would produce better model scores if al-
lowed to continue. In order to deal with these
problems, search is carried out only on a graph
of pre-calculated orderings, and ad-hoc reordering
limits are imposed to constrain the search space
(Crego et al, 2005; Crego and Marin?o, 2006), or
a higher beam size is used in decoding (Durrani
et al, 2011). The ability to memorize and pro-
duce larger translation chunks during decoding, on
the other hand, gives a distinct advantage to the
phrase-based system during search. Phrase-based
systems i) have access to uncommon translations,
ii) do not require higher beam sizes, iii) have more
accurate future-cost estimates because of the avail-
ability of phrase-internal language model context
before search is started. To illustrate this consider
the German-English phrase-pair ?scho? ein Tor ?
scored a goal?, composed from the tuples (cept-
pairs) ?scho? ? scored?, ?ein ? a? and ?Tor ? goal?.
It is likely that the N-gram system does not have
the tuple ?scho? ? scored? in its n-best translation
options because ?scored? is an uncommon transla-
tion for ?scho?? outside the sports domain. Even if
?scho? ? scored? is hypothesized, it will be ranked
quite low in the stack until ?ein? and ?Tor? are gen-
erated in the next steps. A higher beam is required
to prevent it from getting pruned. Phrase-based sys-
tems, on the other hand, are likely to have access to
the phrasal unit ?scho? ein Tor ? scored a goal? and
can generate it in a single step. Moreover, a more ac-
curate future-cost estimate can be computed because
of the available context internal to the phrase.
In this work, we extend the N-gram model, based
on operation sequences (Durrani et al, 2011), to
use phrases during decoding. The main idea is to
study whether a combination of modeling with min-
imal translation units and using phrasal information
during decoding helps to solve the above-mentioned
problems.
The remainder of this paper is organized as fol-
lows. The next two sections review phrase-based
and N-gram-based SMT. Section 2 provides a com-
parison of phrase-based and N-gram-based SMT.
Section 3 summarizes the operation sequence model
(OSM), the main baseline for this work. Section
4 analyzes the search problem when decoding with
Figure 1: Different Segmentations of a Bilingual Sen-
tence Pair
minimal units. Section 5 discusses how information
available in phrases can be used to improve search
performance. Section 6 presents the results of this
work. We conducted experiments on the German-to-
English and French-to-English translation tasks and
found that using phrases in decoding improves both
search accuracy and BLEU scores. Finally we com-
pare our system with two state-of-the-art phrase-
based systems (Moses and Phrasal) and two state-
of-the-art N-gram-based systems (Ncode and OSM)
on standard translation tasks.
2 Previous Work
Phrase-based and N-gram-based SMT are alter-
native frameworks for string-to-string translation.
Phrase-based SMT segments a bilingual sentence
pair into phrases that are continuous sequences of
words (Och and Ney, 2004; Koehn et al, 2003)
or discontinuous sequences of words (Galley and
Manning, 2010). These phrases are then reordered
through a lexicalized reordering model that takes
into account the orientation of a phrase with respect
to its previous phrase (Tillmann and Zhang, 2005)
or block of phrases (Galley and Manning, 2008).
There are several drawbacks of the phrase-based
model. Firstly it makes an independence assump-
tion over phrases, according to which phrases are
translated independently of each other, thus ignor-
ing the contextual information outside of the phrasal
boundary. This problem is corrected by the monolin-
gual language model that takes context into account.
But often the language model cannot compensate for
the dispreference of the translation model for non-
local dependencies. The second problem is that the
model is unaware of the actual phrasal segmentation
of a sentence during training. It therefore learns all
possible ways of segmenting a bilingual sentence.
Different segmentations of a bilingual sentence re-
2
sult in different probability scores for the translation
and reordering models, causing spurious ambiguity
in the model. See Figure 1. In the first segmentation,
the model learns the lexical and reordering proba-
bilities of the phrases ?sie wu?rden ? they would?
and ?gegen ihre kampagne abstimmen ? vote against
your campaign?. In the second segmentation, the
model learns the lexical and reordering probabilities
of the phrases ?sie ? they? ?wu?rden ? would?, ?ab-
stimmen ? vote?, ?gegen ihre kampagne ? against
your campaign?. Both segmentations result in dif-
ferent translation and reordering scores. This kind
of ambiguity in the model subsequently results in
the presence of many different equivalent segmen-
tations in the search space. Also note that the two
segmentations contain different information. From
the first segmentation the model learns the depen-
dency between the verb ?abstimmen ? vote? and the
phrase ?gegen ihre kampagne ? against your cam-
paign?. The second segmentation allows the model
to capture the reordering of the complex verb pred-
icate ?wu?rden ? would? and ?abstimmen ? vote? by
learning that the verb ?abstimmen ? vote? is discon-
tinuous with respect to the auxiliary. This informa-
tion cannot be captured in the first segmentation be-
cause of the phrasal independence assumption and
stiff phrasal boundaries. The model loses one of the
dependencies depending upon which segmentation
it chooses during decoding.
N-gram-based SMT is an instance of a joint
model that generates source and target strings to-
gether in bilingual translation units called tuples.
Tuples are essentially phrases but they are atomic
units that cannot be decomposed any further. This
condition of atomicity results in a unique segmen-
tation of the bilingual sentence pair given its align-
ments. The model does not make any phrasal inde-
pendence assumption and generates a tuple by look-
ing at a context of n ? 1 previous tuples (or opera-
tions). This allows the N-gram model to model all
the dependencies through a single derivation.
The main drawback of N-gram-based SMT is its
poor search mechanism which is inherent from us-
ing minimal translation units during search. Decod-
ing with tuples has problems with a high number
of search errors caused by lower translation cover-
age, inaccurate future-cost estimation and pruning
of correct hypotheses (see Section 4.2 for details).
Crego and Marin?o (2006) proposed a way to couple
reordering and search through POS-based rewrite
rules. These rules are learned during training when
units with crossing alignments are unfolded through
source linearization to form minimal tuples. For ex-
ample, in Figure 1, the N-gram-based MT will lin-
earize the word sequence ?gegen ihre kampagne ab-
stimmen? to ?abstimmen gegen ihre kampagne?, so
that it is in the same order as the English words.
It also learns a POS-rule ?IN PRP NN VB ? VB
IN PRP NN?. The POS-based rewrite rules serve
to precompute the orderings that are hypothesized
during decoding. Coupling reordering and search
allows the N-gram model to arrange hypotheses in
2m stacks (for an m word source sentence), each
containing hypotheses that cover exactly the same
foreign words. This removes the need for future-
cost estimation3. Secondly, memorizing POS-based
rules enables phrase-based like reordering, however
without lexical selection. There are three drawbacks
of this approach. Firstly, lexical generation and re-
ordering are decoupled. Search is only performed on
a small number of reorderings, pre-calculated using
the source side and completely ignoring the target-
side. And lastly, the POS-based rules face data spar-
sity problems especially in the case of long distance
reorderings.
Durrani et al (2011) recently addressed these
problems by proposing an operation sequence N-
gram model which strongly couples translation and
reordering, hypothesizes all possible reorderings
and does not require POS-based rules. Represent-
ing bilingual sentences as a sequence of operations
enables them to memorize phrases and lexical re-
ordering triggers like PBSMT. However, using min-
imal units during decoding and searching over all
possible reorderings means that hypotheses can no
longer be arranged in 2m stacks. The problem of
inaccurate future-cost estimates resurfaces resulting
in more search errors. A higher beam size of 500 is
therefore used to produce translation units in com-
parison to phrase-based systems. This, however,
still does not eliminate all search errors. This pa-
per shows that using phrases instead of cepts in de-
3Using m stacks with future-cost estimation is a more effi-
cient solution but is not used ?due to the complexity of accu-
rately computing these estimations in the N-gram architecture?
(Crego et al, 2011).
3
coding improves the search accuracy and translation
quality. It also shows that using some phrasal in-
formation in cept-based decoding captures some of
these improvements.
3 Operation Sequence Model
The N-gram model with integrated reordering mod-
els a sequence of operations obtained through the
transformation of a bilingual sentence pair. An op-
eration can either be to i) generate a sequence of
source and target words, ii) to insert a gap as a place-
holder for skipped words, iii) or to jump forward and
backward in a sentence to translate words discon-
tinuously. The translate operation Generate(X,Y)
encapsulates the translation tuple (X,Y). It gener-
ates source and target translations simultaneously4.
This is similar to N-gram-based SMT except that
the tuples in the N-gram-based model are generated
monotonically, whereas in this case lexical genera-
tion and reordering information is strongly coupled
in an operation sequence.
Consider the phrase pair:
The model memorizes it
through the sequence:
Generate(Wie, What is)? Gap? Generate (Sie,
your)? Jump Back (1)?Generate (heissen, name)
Let O = o1, . . . , oj?1 be a sequence of opera-
tions as hypothesized by the translator to generate
the bilingual sentence pair ?F,E? with an alignment
function A. The translation model is defined as:
p(F,E,A) = p(oJ1 ) =
J?
j=1
p(oj |oj?n+1, ..., oj?1)
where n indicates the amount of context used. The
translation model is implemented as an N-gram
model of operations using SRILM-Toolkit (Stol-
cke, 2002) with Kneser-Ney smoothing. A 9-gram
model is used. Several count-based features such as
gap and open gap penalties and distance-based fea-
tures such as gap-width and reordering distance are
added to the model, along with the lexical weighting
and length penalty features in a standard log-linear
framework (Durrani et al, 2011).
4The generation is carried out in the order of the target lan-
guage E.
4 Search
4.1 Overview of Decoding Framework
The decoding framework used in the operation se-
quence model is based on Pharaoh (Koehn, 2004a).
The decoder uses beam search to build up the trans-
lation from left to right. The hypotheses are ar-
ranged in m stacks such that stack i maintains hy-
potheses that have already translated i many foreign
words. The ultimate goal is to find the best scor-
ing hypothesis, that has translated all the words in
the foreign sentence. The overall process can be
roughly divided into the following steps: i) extrac-
tion of translation units ii) future-cost estimation, iii)
hypothesis extension iv) recombination and pruning.
During the hypothesis extension each extracted
phrase is translated into a sequence of operations.
The reordering operations (gaps and jumps) are gen-
erated by looking at the position of the translator,
the last foreign word generated etc. (Refer to Algo-
rithm 1 in Durrani et al (2011)). The probability of
an operation depends on the n ? 1 previous opera-
tions. The model backs-off to the smaller n-grams
of operations if the full history is unknown. We use
Kneser-Ney smoothing to handle back-off5.
4.2 Drawbacks of Cept-based Decoding
One of the main drawbacks of the operation se-
quence model is that it has a more difficult search
problem than the phrase-based model. The opera-
tion model, although based on minimal translation
units, can learn larger translation chunks by mem-
orizing a sequence of operations. However, using
cepts during decoding has the following drawbacks:
i) the cept-based decoder does not have access to
all the translation units that a phrase-based decoder
uses as part of a larger phrase. ii) it requires a higher
beam size to prevent early pruning of better hypothe-
ses that lead toward higher model scores when al-
lowed to continue and iii) it uses worse future-cost
estimates than the phrase-based decoder.
Recall the example from the last section. For
the cept-based decoder to generate the same phrasal
translation, it requires three separate tuple transla-
tions ?Wie ? what is?, ?Sie ? your? and ?hei?en ?
name?. Here we are faced with three challenges.
5We also tried Witten-Bell and Good Turing methods of dis-
counting and found Kneser-Ney smoothing to produce the best
results.
4
Translation Coverage: The first problem is that
the N-gram model does not have the same cov-
erage of translation options. The English cepts
?what is?, ?your? and ?name? are not good candi-
date translations for the German cepts ?Wie?, ?Sie?
and ?hei?en?, respectively. When extracting tuple
translations for these cepts from the Europarl data
for our system, the tuple ?Wie ? what is? is ranked
124th, ?hei?en ? name? is ranked 56th, and ?Sie ?
your? is ranked 9th in the list of n-best translation
candidates. Typically only the 20 best translation
options are used, to reduce the decoding time, and
such phrasal units with less frequent cept transla-
tions are never hypothesized in the N-gram-based
systems. The phrase-based system on the other hand
can extract the phrase ?Wie hei?en Sie ? what is
your name? even if it is observed only once dur-
ing training. A similar problem is also reported in
Costa-jussa` et al (2007). When trying to repro-
duce the sentences in the n-best translation output
of the phrase-based system, the N-gram-based sys-
tem was only able to produce 37.5% of the sen-
tences in the Spanish-to-English and 37.2% in the
English-to-Spanish translation tasks. In compar-
ison the phrase-based system was able to repro-
duce 57.5% and 48.6% of the sentences in the n-
best translation output of the Spanish-to-English and
English-to-Spanish N-gram-based systems.
Larger Beam Size: A related problem is that a
higher beam size is required in cept-based decod-
ing to prevent uncommon translations from getting
pruned. The phrase-based system can generate the
phrase-pair ?Wie hei?en Sie ? what is your name?
in a single step placing it directly into the stack three
words to the right. The cept-based decoder generates
this phrase in three stacks with the tuple translations
?Wie ? What is?, ?Sie ? your? and ?hei?en ? name?.
A very large stack size is required during decoding
to prevent the pruning of ?Wie ? What is? which is
ranked quite low in the stack until the tuple ?Sie ?
your? is hypothesized in the next stack. Costa-jussa`
et al (2007) reports a significant drop in the perfor-
mance of N-gram-based SMT when a beam size of
10 is used instead of 50 in their experiments. For the
(cept-based) operation sequence model, Durrani et
al. (2011) required a stack size of 500. In compari-
son, the translation quality achieved by phrase-based
SMT remains the same when varying the beam size
between 5 and 50.
Future-Cost Estimation: A third problem is
caused by inaccurate future-cost estimation. Using
phrases helps phrase-based SMT to better estimate
the future language model cost because of the larger
context available, and allows the decoder to capture
local (phrase-internal) reorderings in the future cost.
In comparison the future cost for tuples is mostly un-
igram probabilities. The future-cost estimate for the
phrase pair ?Wie hei?en Sie ? What is your name?
is estimated by calculating the cost of each feature.
The language model cost, for example, is estimated
in the phrase-based system as follows:
plm = p(What)? p(is|What)? p(your|What is)
? p(name|What is your)
The cost of the direct phrase translation probabil-
ity, one of the features used in the phrase translation
model, is estimated as:
ptm = p(What is your name|Wie hei?en Sie)
Phrase-based SMT is aware during the prepro-
cessing step that the words ?Wie hei?en Sie? may
be translated as a phrase. This is helpful for estimat-
ing a more accurate future cost because the phrase-
internal context is already available. The same is not
true for the operation sequence model, to which only
minimal units are available. The operation model
does not have the information that ?Wie hei?en Sie?
may be translated as a phrase during decoding. The
future-cost estimate available to the operation model
for the span covering ?Wie hei?en Sie? will have un-
igram probabilities for both the translation and lan-
guage model:
plm = p(What)? p(is|What)? p(your)? p(name)
ptm = p(Generate(Wie, What is))? p(Generate
(hei?en,name))? p(Generate(Sie, your))
Thus the future-cost estimate in the operation
model is much worse than that of the phrase-based
model. The poor future-cost estimation leads to
search errors, causing a drop in the translation qual-
ity. A more accurate future-cost estimate for the
translation model cost would be:
5
ptm = p(Generate(Wie,What is))? p(Insert Gap|C)
? p(Generate(Sie,your)|C)? p(Jump Back(1)|C)
p(Generate(hei?en,name)|C)
where C is the context, i.e., the n?1 previously gen-
erated operations. The future-cost estimates com-
puted in this manner are much more accurate be-
cause they not only consider context, but also take
the reordering operations into account.
5 N-gram Model with Phrase-based
Decoding
In the last section we discussed the disadvantages of
using cepts during search in a left-to-right decoding
framework. We now define a method to empirically
study the mentioned drawbacks and whether using
information available in phrase-pairs during decod-
ing can help improve search accuracy and translation
quality.
5.1 Training
We extended the training steps in Durrani et al
(2011) to extract a phrase lexicon from the paral-
lel data. We extract all phrase pairs of length 6 and
below, that are consistent (Och et al, 1999) with
the word alignments. Only continuous phrases as
used in a traditional phrase-based system are ex-
tracted thus allowing only inside-out (Wu, 1997)
type of alignments. The future cost of each fea-
ture component used in the log-linear model is cal-
culated. The operation sequence required to hypoth-
esize each phrase is generated and its future cost is
calculated. The future costs of other features such
as language models, lexicalized probability features,
etc. are also estimated. The estimates of the count-
based reordering penalties (gap penalty and open
gap penalty) and the distance-based features (gap-
width and reordering distance) could not be esti-
mated previously with cepts but are available when
using phrases.
5.2 Decoding
We extended the decoder developed by Durrani et al
(2011) and tried three ideas. In our primary experi-
ments we enabled the decoder to use phrases instead
of cepts. This allows the decoder to i) use phrase-
internal context when computing the future-cost es-
timates, ii) hypothesize translation options not avail-
able to the cept-based decoder iii) cover multiple
source words in a single step subsequently improv-
ing translation coverage and search. Note that us-
ing phrases instead of cepts during decoding, does
not reintroduce the spurious phrasal segmentation
problem as is present in the phrase-based system,
because the model is built on minimal units which
avoids segmentation ambiguity. Different compo-
sitions of the same phrasal unit lead to exactly the
same model score. We therefore do not create any
alternative compositions of the same phrasal unit
during decoding. This option is not available in
phrase-based decoding, because an alternative com-
position may lead towards a better model score.
In our secondary set of experiments, we used
cept-based decoding but modified the decoder to
use information available from the phrases extracted
for the test sentences. Firstly, we used future-cost
estimates from the extracted phrases (see system
cept.500.fc in Table1). This however, leads to in-
consistency in the cases where the future cost is es-
timated from some phrasal unit that cannot be gen-
erated through the available cept translations. For
example, say the best cost to cover the sequence
?Wie hei?en Sie? is given by the phrase ?What is
your name?. The 20-best translation options in cept-
based system, however, do not have tuples ?Wie ?
What? and ?hei?en ? name?. To remove this dis-
crepancy, we add all such tuples that are used in
the extracted phrases, to the list of extracted cepts
(system cept.500.fc.t). We also studied how much
gain we obtain by only adding tuples from phrases
and using cept-based future-cost estimates (system
cept.500.t).
5.3 Evaluation Method
To evaluate our modifications we apply a simple
strategy. We hold the model constant and change
the search to use the baseline decoder, which uses
minimal translation units, or the modified decoders
that use phrasal information during decoding. The
model parameters are optimized by running MERT
(minimum error rate training) for the baseline de-
coder on the dev set. After we have the optimized
weights, we run the baseline decoder and our mod-
ifications on the test. Note that because all the de-
coding runs use the same feature vector, the model
6
stays constant, only search changes. This allows us
to compare different decoding runs, obtained using
the same parameters, but different search strategies,
in terms of model scores. We compute a search ac-
curacy and translation quality for each run.
Search accuracy is computed by comparing trans-
lation hypotheses from the different decoding runs.
We form a collection of the best scoring hypotheses
by traversing through all the runs and selecting the
sentences with highest model score. For each input
sentence we select a single best scoring hypothesis.
The best scoring hypothesis can be contributed from
several runs. In this case all these runs will be given
a credit for that particular sentence when computing
the search accuracy. The search accuracy of a decod-
ing run is defined as the percentage of hypotheses
that were contributed from this run, when forming a
list of best scoring hypotheses. For example, for a
test set of 1000 sentences, the accuracy of a decod-
ing run would be 30% if it was able to produce the
best scoring hypothesis for 300 sentences in the test
set. Translation quality is measured through BLEU
(Papineni et al, 2002).
6 Experimental Setup
We initially experimented with two language pairs:
German-to-English (G-E) and French-to-English (F-
E). We trained our system and the baseline sys-
tems on most of the data6 made available for the
translation task of the Fourth Workshop on Statis-
tical Machine Translation.7 We used 1M bilin-
gual sentences, for the estimation of the transla-
tion model and 2M sentences from the monolingual
corpus (news commentary) which also contains the
English part of the bilingual corpus. Word align-
ments are obtained by running GIZA++ (Och and
Ney, 2003) with the grow-diag-final-and (Koehn et
al., 2005) symmetrization heuristic. We follow the
training steps described in Durrani et al (2011), con-
sisting of i) post-processing the alignments to re-
move discontinuous and unaligned target cepts, ii)
conversion of bilingual alignments into operation
sequences, iii) estimation of the n-gram language
models.
6We did not use all the available data due to scalability is-
sues. The scores reported are therefore well below those ob-
tained by the systems submitted to the WMT evaluation series.
7http://www.statmt.org/wmt09/translation-task.html
6.1 Search Accuracy Results
We divided our evaluation into two halves. In
the first half we carried out experiments to mea-
sure search accuracy and translation quality of
our decoders against the baseline cept-based OSM
(cept.500) that uses minimal translation units with a
stack size of 500. We used the version of the cept-
based OSM system that does not allow discontinu-
ous8 source cepts. To increase the speed of the sys-
tem we used a hard reordering limit of 159, in the
baseline decoder and our modifications, disallowing
jumps that are beyond 15 words from the first open
gap. For each extracted cept or phrase 10-best trans-
lation options are extracted.
Using phrases in search reduces the decoding
speed. In order to make a fair comparison, both the
phrase-based and the baseline cept-based decoders
should be allowed to run for the same amount of
time. We therefore reduced the stack size in the
phrase-based decoder so that it runs in the same
amount of time as the cept-based decoder. We found
that using a stack size of 20010 for the phrase-based
decoder was comparable in speed to using a stack-
size of 500 in the cept-based decoding.
We first tuned the baseline on dev11 to obtain an
optimized weight vector. We then ran the baseline
and our decoders as discussed in Section 5.2 on the
dev-test. Then we repeated this experiment by tun-
ing the weights with our phrase-based decoder (us-
ing a stack size of 100) and ran all the decoders again
using the new weights.
Table 1 shows the average search accuracies and
BLEU scores of the two experiments. Using phrases
during decoding in the G-E experiments resulted
in a statistically significant12 0.69 BLEU points
gain comparing our best system phrase.200 with the
baseline system cept.500. We mark a result as sig-
8Discontinuous source-side units did not lead to any im-
provements in (Durrani et al, 2011) and increased the decoding
times by multiple folds. We also found these to be less useful.
9Imposing a hard reordering limit significantly reduced the
decoding time and also slightly increased the BLEU scores.
10Higher stack sizes leads to improvement in model scores
for both German-English and French-English and slight im-
provement of BLEU in the case of the former.
11We used news-dev2009a as dev and news-dev2009b as dev-
test and tuned the weights with Z-MERT (Zaidan, 2009).
12We use bootstrap resampling (Koehn, 2004b) to test our
results against the baseline result.
7
System German French
Acc. BLEU Acc. BLEU
Baseline System cept.stack-size
cept.50 25.95% 19.50 42.10% 21.44
cept.100 30.04% 19.79 47.32% 21.70
cept.200 35.17% 19.98 51.47% 21.82
cept.500 41.56% 20.14 54.93% 21.87
Our Cept-based Decoders
cept.500.fc 48.44% 20.52* 54.73% 21.86
cept.500.t 52.24% 20.34 67.95% 22.00
cept.500.fc.t 61.81% 20.53* 67.76% 21.96
Our Phrase-based Decoders
phrase.50 58.88% 20.58* 80.83% 22.04
phrase.100 69.85% 20.73* 88.34% 22.13
phrase.200 79.71% 20.83* 92.93% 22.17*
Table 1: Search Accuracies (Acc.) and BLEU scores of
the Baseline and Our Decoders with different Stack Sizes
(fc = Future Cost Estimated from Phrases, t = Cept Trans-
lation Options enriched from Phrases)
nificant if the improvement shown by our decoder
over the baseline decoder (cept.500) is significant at
the p ? 0.05 level, in both the runs. All the out-
puts that show statistically significant improvements
over the baseline decoder (cept.500) in Table 1 are
marked with an asterisk.
The search accuracy of our best system
(phrase.200), in G-E experiments is roughly
80%, which means that 80% of the times the
phrase-based decoder (using stack size 200) was
able to produce the same model score or a better
model score than the cept-based decoders (using
a stack size of 500). Our F-E experiments also
showed improvements in BLEU and model scores.
The search accuracy of our best system phrase.200
is roughly 93% as compared with 55% in the
baseline decoder (cept.500) giving a BLEU point
gain of +0.30 over the baseline.
Our modifications to the cept-based decoder also
showed improvements. We found that extending
the cept translation table (cept.500.t) using phrases
helps both in G-E and F-E experiments by extend-
ing the list of n-best translation options by 18% and
18.30% respectively. Using future costs estimated
from phrases (cept.500.fc) improved both search ac-
curacy and BLEU scores in G-E experiments, but
does not lead to any improvements in the F-E ex-
periments, as both BLEU and model scores drop
slightly. We looked at a few examples where the
model score dropped and found that in these cases,
the best scoring hypotheses are ranked very low ear-
lier in the decoding and make their way to the top
gradually in subsequent steps. A slight difference in
the future-cost estimate prunes these hypotheses in
one or the other decoder. We found future cost to
be more critical in G-E than F-E experiments. This
can be explained by the fact that more reordering is
required in G-E and it is necessary to account for the
reordering operations and jump-based features (gap-
based penalties, reordering distance and gap-width)
in the future-cost estimation. F-E on the other hand
is largely monotonic except for a few short distance
reorderings such as flipping noun and adjective.
6.2 Comparison with other Baseline Systems
In the second half of our evaluation we compared
our best system phrase.200 with the baseline sys-
tem cept.500, and other state-of-the-art phrase-based
and N-gram-based systems on German-to-English,
French-to-English, and Spanish-to-English tasks13.
We used the official evaluation data (news-test sets)
from the Statistical Machine Translation Workshops
2009-2011 for all three language pairs (German,
Spanish and French). The feature weights for all the
systems are tuned using the dev set news-dev2009a.
We separately tune the baseline system (cept.500)
and the phrase-based system (phrase.200) and do not
hold the lambda vector constant like before.
Baseline Systems: We also compared our system
with i) Moses (Koehn et al, 2007), ii) Phrasal14 (Cer
et al, 2010), and iii) Ncode (Crego et al, 2011).
We used the default stack sizes of 100 for
Moses15, 200 for Phrasal, 25 for Ncode (with 2m
stacks). A 5-gram English language model is used.
Both phrase-based systems use 20-best phrases for
translation, Ncode uses 25-best tuple translations.
The training and test data for Ncode was tagged us-
ing TreeTagger (Schmid, 1994). All the baseline
systems used lexicalized reordering model. A hard
reordering limit16 of 6 words is used as a default in
13We did not include the results of Spanish in the previous
section due to space limitations but these are similar to those of
the French-to-English translation task.
14Phrasal provides two extensions to Moses: i) hierarchical
reordering model (Galley and Manning, 2008) and ii) discon-
tinuous phrases (Galley and Manning, 2010).
15Using stacks sizes from 200?1000 did not improve results.
16We tried to increase the distortion limit in the baseline sys-
8
both the baseline phrase-based systems. Amongst
the other defaults we retained the hard source gap
penalty of 15 and a target gap penalty of 7 in Phrasal.
We provide Moses and Ncode with the same post-
edited alignments17 from which we removed target-
side discontinuities. We feed the original alignments
to Phrasal because of its ability to learn discontinu-
ous source and target phrases. All the systems use
MERT for the optimization of the weight vector.
Ms Pd Nc C500 P200
German-to-English
MT09 18.73* 19.00* 18.37* 19.06* 19.66
MT10 18.58* 18.96* 18.64* 19.12* 19.70
MT11 17.38* 17.58* 17.49* 17.87* 18.19
French-to-English
MT09 24.61* 24.73* 24.28* 24.94* 25.27
MT10 23.69* 23.09* 23.96 23.90* 24.25
MT11 25.17* 25.55* 24.92* 25.40* 25.92
Spanish-to-English
MT09 24.38* 24.63 24.72 24.48* 24.72
MT10 25.55* 25.66* 25.87 25.68* 26.10
MT11 25.72* 26.17* 26.36* 26.48 26.67
Table 2: Comparison on 3-Test Sets ? Ms = Moses, Pd
= Phrasal (Discontinuous Phrases), Nc = Ncode, C500 =
Cept.500, P200 = Phrase.200
Table 2 compares the performance of our phrase-
based decoder against the baselines. Our system
shows an improvement over all the baseline systems
for the G-E pair, in 11 out of 12 cases in the F-E
pair and in 8 out of 12 cases in the S-E language
pair. We mark a baseline with ?*? to indicate that
our decoder shows an improvement over this base-
line result which is significant at the p ? 0.05 level.
7 Conclusion and Future Work
We proposed a combination of using a model based
on minimal units and decoding with phrases. Mod-
eling with minimal units enables us to learn local
and non-local dependencies in a unified manner and
avoid spurious segmentation ambiguities. However,
using minimal units also in the search presents a
significant challenge because of the poor transla-
tion coverage, inaccurate future-cost estimates and
tems to 15 (in G-E experiments) as used in our systems but the
results dropped significantly in case of Moses and slightly for
Phrasal so we used the default limits for both decoders.
17Using post-processed alignments gave slightly better re-
sults than the original alignments for these baseline systems.
Details are omitted due to space limitation.
the pruning of the correct hypotheses. Phrase-based
SMT on the other hand overcomes these drawbacks
by using larger translation chunks during search.
However, the drawback of the phrase-based model is
the phrasal independence assumption, spurious am-
biguity in segmentation and a weak mechanism to
handle non-local reorderings. We showed that com-
bining a model based on minimal units with phrase-
based decoding can improve both search accuracy
and translation quality. We also showed that the
phrasal information can be indirectly used in cept-
based decoding with improved results. We tested
our system against the state-of-the-art phrase-based
and N-gram-based systems, for German-to-English,
French-to-English, and Spanish-to-English for three
standard test sets. Our system showed statistically
significant improvements over all the baseline sys-
tems in most of the cases. We have shown the bene-
fits of using phrase-based search with a model based
on minimal units. In future work, we would like to
study whether a phrase-based system like Moses or
Phrasal can profit from an OSM-style or N-gram-
style feature. Feng et al (2010) previously showed
that adding a linearized source-side language model
in a phrase-based system helped. It would also
be interesting to study whether the insight of us-
ing minimal units for modeling and phrase-based
search would hold for hierarchical SMT. Vaswani et
al. (2011) recently showed that a Markov model over
the derivation history of minimal rules can obtain the
same translation quality as using grammars formed
with composed rules.
Acknowledgments
We would like to thank the anonymous reviewers
for their helpful feedback and suggestions. Nadir
Durrani and Alexander Fraser were funded by
Deutsche Forschungsgemeinschaft grant Models of
Morphosyntax for Statistical Machine Translation.
Nadir Durrani was partially funded by the European
Union Seventh Framework Programme (FP7/2007-
2013) under grant agreement n ? 287658. Helmut
Schmid was supported by Deutsche Forschungsge-
meinschaft grant SFB 732. This work was sup-
ported in part by the IST Programme of the Eu-
ropean Community, under the PASCAL2 Network
of Excellence, IST-2007-216886. This publication
only reflects the authors? views.
9
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and R. L. Mercer. 1993. The Mathematics
of Statistical Machine Translation: Parameter Estima-
tion. Computational Linguistics, 19(2):263?311.
Daniel Cer, Michel Galley, Daniel Jurafsky, and Christo-
pher D. Manning. 2010. Phrasal: A Statistical Ma-
chine Translation Toolkit for Exploring New model
Features. In Proceedings of the NAACL HLT 2010
Demonstration Session, pages 9?12, Los Angeles,
California, June.
Marta R. Costa-jussa`, Josep M. Crego, David Vilar,
Jose? A.R. Fonollosa, Jose? B. Marin?o, and Hermann
Ney. 2007. Analysis and System Combination of
Phrase- and N-Gram-Based Statistical Machine Trans-
lation Systems. In Human Language Technologies
2007: The Conference of the North American Chap-
ter of the Association for Computational Linguistics;
Companion Volume, Short Papers, pages 137?140,
Rochester, New York, April.
Josep M. Crego and Jose? B. Marin?o. 2006. Improving
Statistical MT by Coupling Reordering and Decoding.
Machine Translation, 20(3):199?215.
Josep M. Crego, Jose? B. Marin?o, and Adria` de Gispert.
2005. Reordered Search and Unfolding Tuples for N-
Gram-Based SMT. In Proceedings of the 10th Ma-
chine Translation Summit (MT Summit X), pages 283?
289, Phuket, Thailand.
Josep M. Crego, Franc?ois Yvon, and Jose? B. Marin?o.
2011. Ncode: an Open Source Bilingual N-gram SMT
Toolkit. The Prague Bulletin of Mathematical Lin-
guistics, (96):49?58.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A Joint Sequence Translation Model with Inte-
grated Reordering. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies, pages 1045?
1054, Portland, Oregon, USA, June.
Minwei Feng, Arne Mauser, and Hermann Ney. 2010.
A Source-side Decoding Sequence Model for Statisti-
cal Machine Translation. In Conference of the Associ-
ation for Machine Translation in the Americas 2010,
Denver, Colorado, USA, October.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reordering
Model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 848?856, Honolulu, Hawaii, October.
Michel Galley and Christopher D. Manning. 2010. Ac-
curate Non-Hierarchical Phrase-Based Translation. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 966?
974, Los Angeles, California, June. Association for
Computational Linguistics.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proceedings
of HLT-NAACL, pages 127?133, Edmonton, Canada.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh System Description
for the 2005 IWSLT Speech Translation Evaluation. In
International Workshop on Spoken Language Transla-
tion 2005.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL 2007
Demonstrations, Prague, Czech Republic.
Philipp Koehn. 2004a. Pharaoh: A Beam Search De-
coder for Phrase-Based Statistical Machine Transla-
tion Models. In AMTA, pages 115?124.
Philipp Koehn. 2004b. Statistical Significance Tests
for Machine Translation Evaluation. In Dekang Lin
and Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July.
Jose? B. Marin?o, Rafael E. Banchs, Josep M. Crego, Adria`
de Gispert, Patrik Lambert, Jose? A. R. Fonollosa, and
Marta R. Costa-jussa`. 2006. N-gram-Based Machine
Translation. Computational Linguistics, 32(4):527?
549.
Franz J. Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Franz J. Och and Hermann Ney. 2004. The Alignment
Template Approach to Statistical Machine Translation.
Computational Linguistics, 30(1):417?449.
Franz J. Och, Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical ma-
chine translation. In Joint SIGDAT Conf. on Empiri-
cal Methods in Natural Language Processing and Very
Large Corpora, pages 20?28, University of Maryland,
College Park, MD.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318, Morris-
town, NJ, USA.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In International Conference
on New Methods in Language Processing, pages 44?
49, Manchester, UK.
10
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Intl. Conf. Spoken Lan-
guage Processing, Denver, Colorado.
Christoph Tillmann and Tong Zhang. 2005. A Local-
ized Prediction Model for Statistical Machine Transla-
tion. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL?05),
pages 557?564, Ann Arbor, Michigan, June.
Ashish Vaswani, Haitao Mi, Liang Huang, and David
Chiang. 2011. Rule Markov Models for Fast Tree-to-
String Translation. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 856?864,
Portland, Oregon, USA, June.
Dekai Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Corpora.
Computational Linguistics, 23(3):377?403.
Omar F. Zaidan. 2009. Z-MERT: A Fully Configurable
Open Source Tool for Minimum Error Rate Training
of Machine Translation Systems. The Prague Bulletin
of Mathematical Linguistics, 91:79?88.
11
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 465?474,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Hindi-to-Urdu Machine Translation Through Transliteration
Nadir Durrani Hassan Sajjad Alexander Fraser Helmut Schmid
Institute for Natural Language Processing
University of Stuttgart
{durrani,sajjad,fraser,schmid}@ims.uni-stuttgart.de
Abstract
We present a novel approach to integrate
transliteration into Hindi-to-Urdu statisti-
cal machine translation. We propose two
probabilistic models, based on conditional
and joint probability formulations, that are
novel solutions to the problem. Our mod-
els consider both transliteration and trans-
lation when translating a particular Hindi
word given the context whereas in pre-
vious work transliteration is only used
for translating OOV (out-of-vocabulary)
words. We use transliteration as a tool
for disambiguation of Hindi homonyms
which can be both translated or translit-
erated or transliterated differently based
on different contexts. We obtain final
BLEU scores of 19.35 (conditional prob-
ability model) and 19.00 (joint probability
model) as compared to 14.30 for a base-
line phrase-based system and 16.25 for a
system which transliterates OOV words in
the baseline system. This indicates that
transliteration is useful for more than only
translating OOV words for language pairs
like Hindi-Urdu.
1 Introduction
Hindi is an official language of India and is writ-
ten in Devanagari script. Urdu is the national lan-
guage of Pakistan, and also one of the state lan-
guages in India, and is written in Perso-Arabic
script. Hindi inherits its vocabulary from Sanskrit
while Urdu descends from several languages in-
cluding Arabic, Farsi (Persian), Turkish and San-
skrit. Hindi and Urdu share grammatical structure
and a large proportion of vocabulary that they both
inherited from Sanskrit. Most of the verbs and
closed-class words (pronouns, auxiliaries, case-
markers, etc) are the same. Because both lan-
guages have lived together for centuries, some
Urdu words which originally came from Arabic
and Farsi have also mixed into Hindi and are now
part of the Hindi vocabulary. The spoken form of
the two languages is very similar.
The extent of overlap between Hindi and Urdu
vocabulary depends upon the domain of the text.
Text coming from the literary domain like novels
or history tend to have more Sanskrit (for Hindi)
and Persian/Arabic (for Urdu) vocabulary. How-
ever, news wire that contains text related to me-
dia, sports and politics, etc., is more likely to have
common vocabulary.
In an initial study on a small news corpus of
5000 words, randomly selected from BBC1 News,
we found that approximately 62% of the Hindi
types are also part of Urdu vocabulary and thus
can be transliterated while only 38% have to be
translated. This provides a strong motivation to
implement an end-to-end translation system which
strongly relies on high quality transliteration from
Hindi to Urdu.
Hindi and Urdu have similar sound systems but
transliteration from Hindi to Urdu is still very hard
because some phonemes in Hindi have several or-
thographic equivalents in Urdu. For example the
?z? sound2 can only be written as whenever it
occurs in a Hindi word but can be written as ,
, and in an Urdu word. Transliteration
becomes non-trivial in cases where the multiple
orthographic equivalents for a Hindi word are all
valid Urdu words. Context is required to resolve
ambiguity in such cases. Our transliterator (de-
scribed in sections 3.1.2 and 4.1.3) gives an accu-
racy of 81.6% and a 25-best accuracy of 92.3%.
Transliteration has been previously used only as
a back-off measure to translate NEs (Name Enti-
ties) and OOV words in a pre- or post-processing
step. The problem we are solving is more difficult
than techniques aimed at handling OOV words,
1http://www.bbc.co.uk/hindi/index.shtml
2All sounds are represented using SAMPA notation.
465
Hindi Urdu SAMPA Gloss
/ Am Mango/Ordinary
/ d ZAli Fake/Net
/ Ser Lion/Verse
Table 1: Hindi Words That Can Be Transliterated
Differently in Different Contexts
Hindi Urdu SAMPA Gloss
/ simA Border/Seema
/ Amb@r Sky/Ambar
/ vId Ze Victory/Vijay
Table 2: Hindi Words That Can Be Translated or
Transliterated in Different Contexts
which focus primarily on name transliteration, be-
cause we need different transliterations in differ-
ent contexts; in their case context is irrelevant. For
example: consider the problem of transliterating
the English word ?read? to a phoneme represen-
tation in the context ?I will read? versus the con-
text ?I have read?. An example of this for Hindi
to Urdu transliteration: the two Urdu words
(face/condition) and (chapter of the Koran)
are both written as (sur@t d) in Hindi. The
two are pronounced identically in Urdu but writ-
ten differently. In such cases we hope to choose
the correct transliteration by using context. Some
other examples are shown in Table 1.
Sometimes there is also an ambiguity of
whether to translate or transliterate a particular
word. The Hindi word , for example, will
be translated to (peace, s@kun) when it is a
common noun but transliterated to (Shanti,
SAnt di) when it is a proper name. We try to
model whether to translate or transliterate in a
given situation. Some other examples are shown
in Table 2.
The remainder of this paper is organized as fol-
lows. Section 2 provides a review of previous
work. Section 3 introduces two probabilistic mod-
els for integrating translations and transliterations
into a translation model which are based on condi-
tional and joint probability distributions. Section 4
discusses the training data, parameter optimization
and the initial set of experiments that compare our
two models with a baseline Hindi-Urdu phrase-
based system and with two transliteration-aided
phrase-based systems in terms of BLEU scores
(Papineni et al, 2001). Section 5 performs an er-
ror analysis showing interesting weaknesses in the
initial formulations. We remedy the problems by
adding some heuristics and modifications to our
models which show improvements in the results as
discussed in section 6. Section 7 gives two exam-
ples illustrating how our model decides whether
to translate or transliterate and how it is able to
choose among different valid transliterations given
the context. Section 8 concludes the paper.
2 Previous Work
There has been a significant amount of work on
transliteration. We can break down previous work
into three groups. The first group is generic
transliteration work, which is evaluated outside of
the context of translation. This work uses either
grapheme or phoneme based models to translit-
erate words lists (Knight and Graehl, 1998; Li
et al, 2004; Ekbal et al, 2006; Malik et al,
2008). The work by Malik et al addresses Hindi to
Urdu transliteration using hand-crafted rules and
a phonemic representation; it ignores translation
context.
A second group deals with out-of-vocabulary
words for SMT systems built on large parallel cor-
pora, and therefore focuses on name translitera-
tion, which is largely independent of context. Al-
Onaizan and Knight (2002) transliterate Arabic
NEs into English and score them against their re-
spective translations using a modified IBM Model
1. The options are further re-ranked based on dif-
ferent measures such as web counts and using co-
reference to resolve ambiguity. These re-ranking
methodologies can not be performed in SMT at
the decoding time. An efficient way to compute
and re-rank the transliterations of NEs and inte-
grate them on the fly might be possible. However,
this is not practical in our case as our model con-
siders transliterations of all input words and not
just NEs. A log-linear block transliteration model
is applied to OOV NEs in Arabic to English SMT
by Zhao et al (2007). This work is also translit-
erating only NEs and not doing any disambigua-
tion. The best method proposed by Kashani et
al. (2007) integrates translations provided by ex-
ternal sources such as transliteration or rule-base
translation of numbers and dates, for an arbitrary
number of entries within the input text. Our work
is different from Kashani et al (2007) in that our
model compares transliterations with translations
466
on the fly whereas transliterations in Kashani et al
do not compete with internal phrase tables. They
only compete amongst themselves during a sec-
ond pass of decoding. Hermjakob et al (2008) use
a tagger to identify good candidates for translit-
eration (which are mostly NEs) in input text and
add transliterations to the SMT phrase table dy-
namically such that they can directly compete with
translations during decoding. This is closer to
our approach except that we use transliteration as
an alternative to translation for all Hindi words.
Our focus is disambiguation of Hindi homonyms
whereas they are concentrating only on translit-
erating NE?s. Moreover, they are working with
a large bitext so they can rely on their transla-
tion model and only need to transliterate NEs and
OOVs. Our translation model is based on data
which is both sparse and noisy. Therefore we pit
transliterations against translations for every input
word. Sinha (2009) presents a rule-based MT sys-
tem that uses Hindi as a pivot to translate from En-
glish to Urdu. This work also uses transliteration
only for the translation of unknown words. Their
work can not be used for direct translation from
Hindi to Urdu (independently of English) ?due to
various ambiguous mappings that have to be re-
solved?.
The third group uses transliteration models in-
side of a cross-lingual IR system (AbdulJaleel and
Larkey, 2003; Virga and Khudanpur, 2003; Pirkola
et al, 2003). Picking a single best transliteration
or translation in context is not important in an IR
system. Instead, all the options are used by giv-
ing them weights and context is typically not taken
into account.
3 Our Approach
Both of our models combine a character-based
transliteration model with a word-based transla-
tion model. Our models look for the most probable
Urdu token sequence un1 for a given Hindi token
sequence hn1 . We assume that each Hindi token is
mapped to exactly one Urdu token and that there is
no reordering. The assumption of no reordering is
reasonable given the fact that Hindi and Urdu have
identical grammar structure and the same word or-
der. An Urdu token might consist of more than one
Urdu word3. The following sections give a math-
3This occurs frequently in case markers with nouns,
derivational affixes and compounds etc. These are written
as single words in Hindi as opposed to Urdu where they are
ematical formulation of our two models, Model-1
and Model-2.
3.1 Model-1 : Conditional Probability Model
Applying a noisy channel model to compute the
most probable translation u?n1 , we get:
argmax
un1
p(un1 |h
n
1 ) = argmax
un1
p(un1 )p(h
n
1 |u
n
1 )
(1)
3.1.1 Language Model
The language model (LM) p(un1 ) is implemented
as an n-gram model using the SRILM-Toolkit
(Stolcke, 2002) with Kneser-Ney smoothing. The
parameters of the language model are learned from
a monolingual Urdu corpus. The language model
is defined as:
p(un1 ) =
n?
i=1
pLM (ui|u
i?1
i?k) (2)
where k is a parameter indicating the amount of
context used (e.g., k = 4 means 5-gram model).
ui can be a single or a multi-word token. A
multi-word token consists of two or more Urdu
words. For a multi-word ui we do multiple lan-
guage model look-ups, one for each uix in ui =
ui1 , . . . , uim and take their product to obtain the
value pLM (ui|u
i?1
i?k).
Language Model for Unknown Words: Our
model generates transliterations that can be known
or unknown to the language model and the trans-
lation model. We refer to the words known to
the language model and to the translation model
as LM-known and TM-known words respectively
and to words that are unknown as LM-unknown
and TM-unknown respectively.
We assign a special value ? to the LM-unknown
words. If one or more uix in a multi-word ui are
LM-unknown we assign a language model score
pLM (ui|u
i?1
i?k) = ? for the entire ui, meaning
that we consider partially known transliterations
to be as bad as fully unknown transliterations. The
parameter ? controls the trade-off between LM-
known and LM-unknown transliterations. It does
not influence translation options because they are
always LM-known in our case. This is because our
monolingual corpus also contains the Urdu part of
translation corpus. The optimization of ? is de-
scribed in section 4.2.1.
written as two words. For example (beautiful ; xub-
sur@t d) and (your?s ; ApkA) are written as
and respectively in Urdu.
467
3.1.2 Translation Model
The translation model (TM) p(hn1 |u
n
1 ) is approx-
imated with a context-independent model:
p(hn1 |u
n
1 ) =
n?
i=1
p(hi|ui) (3)
where hi and ui are Hindi and Urdu tokens re-
spectively. Our model estimates the conditional
probability p(hi|ui) by interpolating a word-
based model and a character-based (translitera-
tion) model.
p(hi|ui) = ?pw(hi|ui) + (1? ?)pc(hi|ui) (4)
The parameters of the word-based translation
model pw(h|u) are estimated from the word align-
ments of a small parallel corpus. We only retain
1-1/1-N (1 Hindi word, 1 or more Urdu words)
alignments and throw away N-1 and M-N align-
ments for our models. This is further discussed in
section 4.1.1.
The character-based transliteration model
pc(h|u) is computed in terms of pc(h, u), a joint
character model, which is also used for Chinese-
English back-transliteration (Li et al, 2004) and
Bengali-English name transliteration (Ekbal et al,
2006). The character-based transliteration proba-
bility is defined as follows:
pc(h, u) =
?
an1?align(h,u)
p(an1 )
=
?
an1?align(h,u)
n?
i=1
p(ai|a
i?1
i?k) (5)
where ai is a pair consisting of the i-th Hindi char-
acter hi and the sequence of 0 or more Urdu char-
acters that it is aligned with. A sample alignment
is shown in Table 3(b) in section 4.1.3. Our best
results are obtained with a 5-gram model. The
parameters p(ai|a
i?1
i?k) are estimated from a small
transliteration corpus which we automatically ex-
tracted from the translation corpus. The extrac-
tion details are also discussed in section 4.1.3. Be-
cause our overall model is a conditional probabil-
ity model, joint-probabilities are marginalized us-
ing character-based prior probabilities:
pc(h|u) =
pc(h, u)
pc(u)
(6)
The prior probability pc(u) of the character se-
quence u = cm1 is defined with a character-based
language model:
pc(u) =
m?
i=1
p(ci|c
i?1
i?k) (7)
The parameters p(ci|c
i?1
i?k) are estimated from
the Urdu part of the character-aligned translitera-
tion corpus. Replacing (6) in (4) we get:
p(hi|ui) = ?pw(hi|ui) + (1? ?)
pc(hi, ui)
pc(ui)
(8)
Having all the components of our model defined
we insert (8) and (2) in (1) to obtain the final equa-
tion:
u?n1 = argmax
un1
n?
i=1
pLM (ui|u
i?1
i?k)[?pw(hi|ui)
+ (1? ?)
pc(hi, ui)
pc(ui)
] (9)
The optimization of the interpolating factor ? is
discussed in section 4.2.1.
3.2 Model-2 : Joint Probability Model
This section briefly defines a variant of our model
where we interpolate joint probabilities instead of
conditional probabilities. Again, the translation
model p(hn1 |u
n
1 ) is approximated with a context-
independent model:
p(hn1 |u
n
1 ) =
n?
i=1
p(hi|ui) =
n?
i=1
p(hi, ui)
p(ui)
(10)
The joint probability p(hi, ui) of a Hindi and an
Urdu word is estimated by interpolating a word-
based model and a character-based model.
p(hi, ui) = ?pw(hi, ui)+(1??)pc(hi, ui) (11)
and the prior probability p(ui) is estimated as:
p(ui) = ?pw(ui) + (1? ?)pc(ui) (12)
The parameters of the translation model pw(hi, ui)
and the word-based prior probabilities pw(ui) are
estimated from the 1-1/1-N word-aligned corpus
(the one that we also used to estimate translation
probabilities pw(hi|ui) previously).
The character-based transliteration probability
pc(hi, ui) and the character-based prior probabil-
ity pc(ui) are defined by (5) and (7) respectively in
468
the previous section. Putting (11) and (12) in (10)
we get
p(hn1 |u
n
1 ) =
n?
i=1
?pw(hi, ui) + (1? ?)pc(hi, ui)
?pw(ui) + (1? ?)pc(ui)
(13)
The idea is to interpolate joint probabilities and di-
vide them by the interpolated marginals. The final
equation for Model-2 is given as:
u?n1 = argmax
un1
n?
i=1
pLM (ui|u
i?1
i?k)?
?pw(hi, ui) + (1? ?)pc(hi, ui)
?pw(ui) + (1? ?)pc(ui)
(14)
3.3 Search
The decoder performs a stack-based search using
a beam-search algorithm similar to the one used
in Pharoah (Koehn, 2004a). It searches for an
Urdu string that maximizes the product of trans-
lation probability and the language model proba-
bility (equation 1) by translating one Hindi word
at a time. It is implemented as a two-level pro-
cess. At the lower level, it computes n-best
transliterations for each Hindi word hi accord-
ing to pc(h, u). The joint probabilities given by
pc(h, u) are marginalized for each Urdu transliter-
ation to give pc(h|u). At the higher level, translit-
eration probabilities are interpolated with pw(h|u)
and then multiplied with language model probabil-
ities to give the probability of a hypothesis. We use
20-best translations and 25-best transliterations for
pw(h|u) and pc(h|u) respectively and a 5-gram
language model.
To keep the search space manageable and time
complexity polynomial we apply pruning and re-
combination. Since our model uses monotonic de-
coding we only need to recombine hypotheses that
have the same context (last n-1 words). Next we
do histogram-based pruning, maintaining the 100-
best hypotheses for each stack.
4 Evaluation
4.1 Training
This section discusses the training of the different
model components.
4.1.1 Translation Corpus
We used the freely available EMILLE Corpus
as our bilingual resource which contains roughly
13,000 Urdu and 12,300 Hindi sentences. From
these we were able to sentence-align 7000 sen-
tence pairs using the sentence alignment algorithm
given by Moore (2002).
The word alignments for this task were ex-
tracted by using GIZA++ (Och and Ney, 2003) in
both directions. We extracted a total of 107323
alignment pairs (5743 N-1 alignments, 8404 M-
N alignments and 93176 1-1/1-N alignments). Of
these alignments M-N and N-1 alignment pairs
were ignored. We manually inspected a sample of
1000 instances of M-N/N-1 alignments and found
that more than 70% of these were (totally or par-
tially) wrong. Of the 30% correct alignments,
roughly one-third constitute N-1 alignments. Most
of these are cases where the Urdu part of the align-
ment actually consists of two (or three) words
but was written without space because of lack of
standard writing convention in Urdu. For exam-
ple (can go ; d ZA s@kt de) is alterna-
tively written as (can go ; d ZAs@kt de)
i.e. without space. We learned that these N-1
translations could be safely dropped because we
can generate a separate Urdu word for each Hindi
word. For valid M-N alignments we observed that
these could be broken into 1-1/1-N alignments in
most of the cases. We also observed that we usu-
ally have coverage of the resulting 1-1 and 1-N
alignments in our translation corpus. Looking at
the noise in the incorrect alignments we decided
to drop N-1 and M-N cases. We do not model
deletions and insertions so we ignored null align-
ments. Also 1-N alignments with gaps were ig-
nored. Only the alignments with contiguous words
were kept.
4.1.2 Monolingual Corpus
Our monolingual Urdu corpus consists of roughly
114K sentences. This comprises 108K sentences
from the data made available by the University of
Leipzig4 + 5600 sentences from the training data
of each fold during cross validation.
4.1.3 Transliteration Corpus
The training corpus for transliteration is extracted
from the 1-1/1-N word-alignments of the EMILLE
corpus discussed in section 4.1.1. We use an edit
distance algorithm to align this training corpus at
the character level and we eliminate translation
pairs with high edit distance which are unlikely to
be transliterations.
4http://corpora.informatik.uni-leipzig.de/
469
We used our knowledge of the Hindi and Urdu
scripts to define the initial character mapping. The
mapping was further extended by looking into
available Hindi-Urdu transliteration systems[5,6]
and other resources (Gupta, 2004; Malik et al,
2008; Jawaid and Ahmed, 2009). Each pair in the
character map is assigned a cost. A Hindi charac-
ter that always map to only one Urdu character is
assigned a cost of 0 whereas the Hindi characters
that map to different Urdu characters are assigned
a cost of 0.2. The edit distance metric allows
insert, delete and replace operations. The hand-
crafted pairs define the cost of replace operations.
We set a cost of 0.6 for deletions and insertions.
These costs were optimized on held out data. The
details of optimization are not mentioned due to
limited space. Using this metric we filter out the
word pairs with high edit-distance to extract our
transliteration corpus. We were able to extract
roughly 2100 unique pairs along with their align-
ments. The resulting alignments are modified by
merging unaligned ? ? 1 (no character on source
side, 1 character on target side) or ? ? N align-
ments with the preceding alignment pair. If there
is no preceding alignment pair then it is merged
with the following pair. Table 3 gives an example
showing initial alignment (a) and the final align-
ment (b) after applying the merge operation. Our
model retains 1 ? ? and N ? ? alignments as
deletion operations.
a) Hindi ? b c ? e f
Urdu A XY C D ? F
b) Hindi b c e f
Urdu AXY CD ? F
Table 3: Alignment (a) Before (b) After Merge
The parameters pc(h, u) and pc(u) are trained
on the aligned corpus using the SRILM toolkit.
We use Add-1 smoothing for unigrams and
Kneser-Ney smoothing for higher n-grams.
4.1.4 Diacritic Removal and Normalization
In Urdu, short vowels are represented with diacrit-
ics but these are rarely written in practice. In or-
der to keep the data consistent, all diacritics are
removed. This loss of information is not harm-
ful when transliterating/translating from Hindi to
Urdu because undiacritized text is equally read-
5CRULP: http://www.crulp.org/software/langproc.htm
6Malerkotla.org: http://translate.malerkotla.co.in
able to native speakers as its diacritized counter
part. However leaving occasional diacritics in the
corpus can worsen the problem of data sparsity by
creating spurious ambiguity7.
There are a few Urdu characters that have mul-
tiple equivalent Unicodes. All such forms are nor-
malized to have only one representation8.
4.2 Experimental Setup
We perform a 5-fold cross validation taking 4/5 of
the data as training and 1/5 as test data. Each fold
comprises roughly 1400 test sentences and 5600
training sentences.
4.2.1 Parameter Optimization
Our model contains two parameters ? (the inter-
polating factor between translation and transliter-
ation modules) and ? (the factor that controls the
trade-off between LM-known and LM-unknown
transliterations). The interpolating factor ? is ini-
tialized, inspired by Written-Bell smoothing, with
a value of NN+B
9. We chose a very low value
1e?40 for the factor ? initially, favoring LM-
known transliterations very strongly. Both of these
parameters are optimized as described below.
Because our training data is very sparse we do
not use held-out data for parameter optimization.
Instead we optimize these parameters by perform-
ing a 2-fold optimization for each of the 5 folds.
Each fold is divided into two halves. The param-
eters ? and ? are optimized on the first half and
the other half is used for testing, then optimiza-
tion is done on the second half and the first half is
used for testing. The optimal value for parameter
? occurs between 0.7-0.84 and for the parameter
? between 1e?5 and 1e?10.
4.2.2 Results
Baseline Pb0: We ran Moses (Koehn et al, 2007)
using Koehn?s training scripts10, doing a 5-fold
cross validation with no reordering11. For the
other parameters we use the default values i.e.
5-gram language model and maximum phrase-
length= 6. Again, the language model is imple-
7It should be noted though that diacritics play a very im-
portant role when transliterating in the reverse direction be-
cause these are virtually always written in Hindi as dependent
vowels.
8www.crulp.org/software/langproc/urdunormalization.htm
9N is the number of aligned word pairs (tokens) and B is
the number of different aligned word pairs (types).
10http://statmt.org/wmt08/baseline.html
11Results are worse with reordering enabled.
470
M Pb0 Pb1 Pb2 M1 M2
BLEU 14.3 16.25 16.13 18.6 17.05
Table 4: Comparing Model-1 and Model-2 with
Phrase-based Systems
mented as an n-gram model using the SRILM-
Toolkit with Kneser-Ney smoothing. Each fold
comprises roughly 1400 test sentences, 5000 in
training and 600 in dev12. We also used two meth-
ods to incorporate transliterations in the phrase-
based system:
Post-process Pb1: All the OOV words in the
phrase-based output are replaced with their top-
candidate transliteration as given by our translit-
eration system.
Pre-process Pb2: Instead of adding translit-
erations as a post process we do a second pass
by adding the unknown words with their top-
candidate transliteration to the training corpus and
rerun Koehn?s training script with the new training
corpus. Table 4 shows results (taking arithmetic
average over 5 folds) from Model-1 and Model-
2 in comparison with three baselines discussed
above.
Both our systems (Model-1 and Model-2) beat
the baseline phrase-based system with a BLEU
point difference of 4.30 and 2.75 respectively. The
transliteration aided phrase-based systems Pb1
and Pb2 are closer to our Model-2 results but are
way below Model-1 results. The difference of
2.35 BLEU points between M1 and Pb1 indicates
that transliteration is useful for more than only
translating OOV words for language pairs like
Hindi-Urdu. Our models choose between trans-
lations and transliterations based on context un-
like the phrase-based systems Pb1 and Pb2 which
use transliteration only as a tool to translate OOV
words.
5 Error Analysis
Based on preliminary experiments we found three
major flaws in our initial formulations. This sec-
tion discusses each one of them and provides some
heuristics and modifications that we employ to try
to correct deficiencies we found in the two models
described in section 3.1 and 3.2.
12After having the MERT parameters, we add the 600 dev
sentences back into the training corpus, retrain GIZA, and
then estimate a new phrase table on all 5600 sentences. We
then use the MERT parameters obtained before together with
the newer (larger) phrase-table set.
5.1 Heuristic-1
A lot of errors occur because our translation model
is built on very sparse and noisy data. The moti-
vation for this heuristic is to counter wrong align-
ments at least in the case of verbs and functional
words (which are often transliterations). This
heuristic favors translations that also appear in the
n-best transliteration list over only-translation and
only-transliteration options. We modify the trans-
lation model for both the conditional and the joint
model by adding another factor which strongly
weighs translation+transliteration options by tak-
ing the square-root of the product of the translation
and transliteration probabilities. Thus modifying
equations (8) and (11) in Model-1 and Model-2
we obtain equations (15) and (16) respectively:
p(hi|ui) = ?1pw(hi|ui) + ?2
pc(hi, ui)
pc(ui)
+ ?3
?
pw(hi|ui)
pc(hi, ui)
pc(ui)
(15)
p(hi, ui) = ?1pw(hi, ui) + ?2pc(hi, ui)
+ ?3
?
pw(hi, ui)pc(hi, ui) (16)
For the optimization of lambda parameters we
hold the value of the translation coefficient ?113
and the transliteration coefficient ?2 constant (us-
ing the optimized values as discussed in section
4.2.1) and optimize ?3 again using 2-fold opti-
mization on all the folds as described above14.
5.2 Heuristic-2
When an unknown Hindi word occurs for which
all transliteration options are LM-unknown then
the best transliteration should be selected. The
problem in our original models is that a fixed LM
probability ? is used for LM-unknown transliter-
ations. Hence our model selects the translitera-
tion that has the best pc(hi,ui)pc(ui) score i.e. we max-
imize pc(hi|ui) instead of pc(ui|hi) (or equiva-
lently pc(hi, ui)). The reason is an inconsistency
in our models. The language model probabil-
ity of unknown words is uniform (and equal to
?) whereas the translation model uses the non-
uniform prior probability pc(ui) for these words.
There is another reason why we can not use the
13The translation coefficient ?1 is same as ? used in previ-
ous models and the transliteration coefficient ?2 = 1? ?
14After optimization we normalize the lambdas to make
their sum equal to 1.
471
value ? in this case. Our transliterator model also
produces space inserted words. The value of ? is
very small because of which transliterations that
are actually LM-unknown, but are mistakenly bro-
ken into constituents that are LM-known, will al-
ways be preferred over their counter parts. An ex-
ample of this is (America) for which two
possible transliterations as given by our model are
(AmerIkA, without space) and (AmerI
kA, with space). The latter version is LM-known
as its constituents are LM-known. Our models al-
ways favor the latter version. Space insertion is an
important feature of our transliteration model. We
want our transliterator to tackle compound words,
derivational affixes, case-markers with nouns that
are written as one word in Hindi but as two or more
words in Urdu. Examples were already shown in
section 3?s footnote.
We eliminate the inconsistency by using pc(ui)
as the 0-gram back-off probability distribution in
the language model. For an LM-unknown translit-
erations we now get in Model-1:
p(ui|u
i?1
i?k)[?pw(hi|ui) + (1? ?)
pc(hi, ui)
pc(ui)
]
= p(ui|u
i?1
i?k)[(1? ?)
pc(hi, ui)
pc(ui)
]
=
k?
j=0
?(ui?1i?j )pc(ui)[(1? ?)
pc(hi, ui)
pc(ui)
]
=
k?
j=0
?(ui?1i?j )[(1? ?)pc(hi, ui)]
where
?k
j=0 ?(u
i?1
i?j ) is just the constant that
SRILM returns for unknown words. The last
line of the calculation shows that we simply drop
pc(ui) if ui is LM-unknown and use the constant?k
j=0 ?(u
i?1
i?j ) instead of ?. A similar calculation
for Model-2 gives
?k
j=0 ?(u
i?1
i?j )pc(hi, ui).
5.3 Heuristic-3
This heuristic discusses a flaw in Model-2. For
transliteration options that are TM-unknown, the
pw(h, u) and pw(u) factors becomes zero and the
translation model probability as given by equation
(13) becomes:
(1? ?)pc(hi, ui)
(1? ?)pc(ui)
=
pc(hi, ui)
pc(ui)
In such cases the ? factor cancels out and no
weighting of word translation vs. transliteration
H1 H2 H12
M1 18.86 18.97 19.35
M2 17.56 17.85 18.34
Table 5: Applying Heuristics 1 and 2 and their
Combinations to Model-1 and Model-2
H3 H13 H23 H123
M2 18.52 18.93 18.55 19.00
Table 6: Applying Heuristic 3 and its Combina-
tions with other Heuristics to Model-2
occurs anymore. As a result of this, translitera-
tions are sometimes incorrectly favored over their
translation alternatives.
In order to remedy this problem we assign a
minimal probability ? to the word-based prior
pw(ui) in case of TM-unknown transliterations,
which prevents it from ever being zero. Because
of this addition the translation model probability
for LM-unknown words becomes:
(1? ?)pc(hi, ui)
?? + (1? ?)pc(ui)
where ? =
1
Urdu Types in TM
6 Final Results
This section shows the improvement in BLEU
score by applying heuristics and combinations of
heuristics in both the models. Tables 5 and 6 show
the improvements achieved by using the differ-
ent heuristics and modifications discussed in sec-
tion 5. We refer to the results as MxHy where x
denotes the model number, 1 for the conditional
probability model and 2 for the joint probability
model and y denotes a heuristic or a combination
of heuristics applied to that model15.
Both heuristics (H1 and H2) show improve-
ments over their base models M1 and M2.
Heuristic-1 shows notable improvement for both
models in parts of test data which has high num-
ber of common vocabulary words. Using heuris-
tic 2 we were able to properly score LM-unknown
transliterations against each other. Using these
heuristics together we obtain a gain of 0.75 over
M-1 and a gain of 1.29 over M-2.
Heuristic-3 remedies the flaw in M2 by assign-
ing a special value to the word-based prior pw(ui)
for TM-unknown words which prevents the can-
celation of interpolating parameter ?. M2 com-
bined with heuristic 3 (M2H3) results in a 1.47
15For example M1H1 refers to the results when heuristic-
1 is applied to model-1 whereas M2H12 refers to the results
when heuristics 1 and 2 are together applied to model 2.
472
BLEU point improvement and combined with all
the heuristics (M2H123) gives an overall gain of
1.95 BLEU points and is close to our best results
(M1H12). We also performed significance test
by concatenating all the fold results. Both our best
systems M1H12 and M2H123 are statistically sig-
nificant (p < 0.05)16 over all the baselines dis-
cussed in section 4.2.2.
One important issue that has not been investi-
gated yet is that BLEU has not yet been shown
to have good performance in morphologically rich
target languages like Urdu, but there is no metric
known to work better. We observed that some-
times on data where the translators preferred to
translate rather than doing transliteration our sys-
tem is penalized by BLEU even though our out-
put string is a valid translation. For other parts of
the data where the translators have heavily used
transliteration, the system may receive a higher
BLEU score. We feel that this is an interesting
area of research for automatic metric developers,
and that a large scale task of translation to Urdu
which would involve a human evaluation cam-
paign would be very interesting.
7 Sample Output
This section gives two examples showing how our
model (M1H2) performs disambiguation. Given
below are some test sentences that have Hindi
homonyms (underlined in the examples) along
with Urdu output given by our system. In the first
example (given in Figure 1) Hindi word can be
transliterated to ( Lion) or (Verse) depend-
ing upon the context. Our model correctly identi-
fies which transliteration to choose given the con-
text.
In the second example (shown in Figure 2)
Hindi word can be translated to (peace,
s@kun) when it is a common noun but transliter-
ated to (Shanti, SAnt di) when it is a proper
name. Our model successfully decides whether to
translate or transliterate given the context.
8 Conclusion
We have presented a novel way to integrate
transliterations into machine translation. In
closely related language pairs such as Hindi-Urdu
with a significant amount of vocabulary overlap,
16We used Kevin Gimpel?s tester
(http://www.ark.cs.cmu.edu/MT/) which uses bootstrap
resampling (Koehn, 2004b), with 1000 samples.
Ser d Z@ngl kA rAd ZA he
?Lion is the king of jungle?
AIqbAl kA Aek xub sur@t d Ser he
?There is a beautiful verse from Iqbal?
Figure 1: Different Transliterations in Different
Contexts
p hIr b hi vh s@kun se n@her?h s@kt dA
?Even then he can?t live peacefully?
Aom SAnt di Aom frhA xAn ki d dusri fIl@m he
?Om Shanti Om is Farah Khan?s second film?
Figure 2: Translation or Transliteration
transliteration can be very effective in machine
translation for more than just translating OOV
words. We have addressed two problems. First,
transliteration helps overcome the problem of data
sparsity and noisy alignments. We are able to gen-
erate word translations that are unseen in the trans-
lation corpus but known to the language model.
Additionally, we can generate novel translitera-
tions (that are LM-Unknown). Second, generat-
ing multiple transliterations for homograph Hindi
words and using language model context helps us
solve the problem of disambiguation. We found
that the joint probability model performs almost as
well as the conditional probability model but that
it was more complex to make it work well.
Acknowledgments
The first two authors were funded by the Higher
Education Commission (HEC) of Pakistan. The
third author was funded by Deutsche Forschungs-
gemeinschaft grants SFB 732 and MorphoSynt.
The fourth author was funded by Deutsche
Forschungsgemeinschaft grant SFB 732.
473
References
Nasreen AbdulJaleel and Leah S. Larkey. 2003. Sta-
tistical transliteration for English-Arabic cross lan-
guage information retrieval. In CIKM 03: Proceed-
ings of the twelfth international conference on In-
formation and knowledge management, pages 139?
146.
Yaser Al-Onaizan and Kevin Knight. 2002. Translat-
ing named entities using monolingual and bilingual
resources. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 400?408.
Asif Ekbal, Sudip Kumar Naskar, and Sivaji Bandy-
opadhyay. 2006. A modified joint source-channel
model for transliteration. In Proceedings of the
COLING/ACL poster sessions, pages 191?198, Syd-
ney, Australia. Association for Computational Lin-
guistics.
Swati Gupta. 2004. Aligning Hindi and Urdu bilin-
gual corpora for robust projection. Masters project
dissertation, Department of Computer Science, Uni-
versity of Sheffield.
Ulf Hermjakob, Kevin Knight, and Hal Daume? III.
2008. Name translation in statistical machine trans-
lation - learning when to transliterate. In Proceed-
ings of ACL-08: HLT, pages 389?397, Columbus,
Ohio. Association for Computational Linguistics.
Bushra Jawaid and Tafseer Ahmed. 2009. Hindi to
Urdu conversion: beyond simple transliteration. In
Conference on Language and Technology 2009, La-
hore, Pakistan.
Mehdi M. Kashani, Eric Joanis, Roland Kuhn, George
Foster, and Fred Popowich. 2007. Integration of an
Arabic transliteration module into a statistical ma-
chine translation system. In Proceedings of the Sec-
ond Workshop on Statistical Machine Translation,
pages 17?24, Prague, Czech Republic. Association
for Computational Linguistics.
Kevin Knight and Jonathan Graehl. 1998. Ma-
chine transliteration. Computational Linguistics,
24(4):599?612.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation for Computational Linguistics, Demonstra-
tion Program, Prague, Czech Republic.
Philipp Koehn. 2004a. Pharaoh: A beam search de-
coder for phrase-based statistical machine transla-
tion models. In AMTA, pages 115?124.
Philipp Koehn. 2004b. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
Haizhou Li, Zhang Min, and Su Jian. 2004. A joint
source-channel model for machine transliteration.
In ACL ?04: Proceedings of the 42nd Annual Meet-
ing on Association for Computational Linguistics,
pages 159?166, Barcelona, Spain. Association for
Computational Linguistics.
M G Abbas Malik, Christian Boitet, and Pushpak Bhat-
tacharyya. 2008. Hindi Urdu machine translitera-
tion using finite-state transducers. In Proceedings
of the 22nd International Conference on Computa-
tional Linguistics, Manchester, UK.
Robert C. Moore. 2002. Fast and accurate sentence
alignment of bilingual corpora. In Conference of the
Association for Machine Translation in the Ameri-
cas (AMTA).
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Kishore A. Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2001. BLEU: a method for auto-
matic evaluation of machine translation. Technical
Report RC22176 (W0109-022), IBM Research Di-
vision, Thomas J. Watson Research Center, York-
town Heights, NY.
Ari Pirkola, Jarmo Toivonen, Heikki Keskustalo, Kari
Visala, and Kalervo Ja?rvelin. 2003. Fuzzy trans-
lation of cross-lingual spelling variants. In SIGIR
?03: Proceedings of the 26th annual international
ACM SIGIR conference on Research and develop-
ment in informaion retrieval, pages 345?352, New
York, NY, USA. ACM.
R. Mahesh K. Sinha. 2009. Developing English-Urdu
machine translation via Hindi. In Third Workshop
on Computational Approaches to Arabic Script-
based Languages (CAASL3), MT Summit XII, Ot-
tawa, Canada.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Intl. Conf. Spoken Lan-
guage Processing, Denver, Colorado.
Paola Virga and Sanjeev Khudanpur. 2003. Translit-
eration of proper names in cross-lingual information
retrieval. In Proceedings of the ACL 2003 workshop
on Multilingual and mixed-language named entity
recognition, pages 57?64, Morristown, NJ, USA.
Association for Computational Linguistics.
Bing Zhao, Nguyen Bach, Ian Lane, and Stephan Vo-
gel. 2007. A log-linear block transliteration model
based on bi-stream HMMs. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 364?371, Rochester, New York. Associ-
ation for Computational Linguistics.
474
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 430?439,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
An Algorithm for Unsupervised Transliteration Mining with an Application
to Word Alignment
Hassan Sajjad Alexander Fraser Helmut Schmid
Institute for Natural Language Processing
University of Stuttgart
{sajjad,fraser,schmid}@ims.uni-stuttgart.de
Abstract
We propose a language-independent method
for the automatic extraction of transliteration
pairs from parallel corpora. In contrast to
previous work, our method uses no form of
supervision, and does not require linguisti-
cally informed preprocessing. We conduct
experiments on data sets from the NEWS
2010 shared task on transliteration mining and
achieve an F-measure of up to 92%, out-
performing most of the semi-supervised sys-
tems that were submitted. We also apply our
method to English/Hindi and English/Arabic
parallel corpora and compare the results with
manually built gold standards which mark
transliterated word pairs. Finally, we integrate
the transliteration module into the GIZA++
word aligner and evaluate it on two word
alignment tasks achieving improvements in
both precision and recall measured against
gold standard word alignments.
1 Introduction
Most previous methods for building transliteration
systems were supervised, requiring either hand-
crafted rules or a clean list of transliteration pairs,
both of which are expensive to create. Such re-
sources are also not applicable to other language
pairs.
In this paper, we show that it is possible to ex-
tract transliteration pairs from a parallel corpus us-
ing an unsupervised method. We first align a bilin-
gual corpus at the word level using GIZA++ and
create a list of word pairs containing a mix of non-
transliterations and transliterations. We train a sta-
tistical transliterator on the list of word pairs. We
then filter out a few word pairs (those which have
the lowest transliteration probabilities according to
the trained transliteration system) which are likely
to be non-transliterations. We retrain the translitera-
tor on the filtered data set. This process is iterated,
filtering out more and more non-transliteration pairs
until a nearly clean list of transliteration word pairs
is left. The optimal number of iterations is automat-
ically determined by a novel stopping criterion.
We compare our unsupervised transliteration min-
ing method with the semi-supervised systems pre-
sented at the NEWS 2010 shared task on translit-
eration mining (Kumaran et al, 2010) using four
language pairs. We refer to this task as NEWS10.
These systems used a manually labelled set of data
for initial supervised training, which means that
they are semi-supervised systems. In contrast, our
system is fully unsupervised. We achieve an F-
measure of up to 92% outperforming most of the
semi-supervised systems.
The NEWS10 data sets are extracted Wikipedia
InterLanguage Links (WIL) which consist of par-
allel phrases, whereas a parallel corpus consists of
parallel sentences. Transliteration mining on the
WIL data sets is easier due to a higher percentage
of transliterations than in parallel corpora. We also
do experiments on parallel corpora for two language
pairs. To this end, we created gold standards in
which sampled word pairs are annotated as either
transliterations or non-transliterations. These gold
standards have been submitted with the paper as sup-
plementary material as they are available to the re-
search community.
430
Finally we integrate a transliteration module into
the GIZA++ word aligner and show that it improves
word alignment quality. The transliteration mod-
ule is trained on the transliteration pairs which our
mining method extracts from the parallel corpora.
We evaluate our word alignment system on two lan-
guage pairs using gold standard word alignments
and achieve improvements of 10% and 13.5% in pre-
cision and 3.5% and 13.5% in recall.
The rest of the paper is organized as follows. In
section 2, we describe the filtering model and the
transliteration model. In section 3, we present our
iterative transliteration mining algorithm and an al-
gorithm which computes a stopping criterion for the
mining algorithm. Section 4 describes the evaluation
of our mining method through both gold standard
evaluation and through using it to improve word
alignment quality. In section 5, we present previous
work and we conclude in section 6.
2 Models
Our algorithms use two different models. The first
model is a joint character sequence model which
we apply to transliteration mining. We use the
grapheme-to-phoneme converter g2p to implement
this model. The other model is a standard phrase-
based MT model which we apply to transliteration
(as opposed to transliteration mining). We build it
using the Moses toolkit.
2.1 Joint Sequence Model Using g2p
Here, we briefly describe g2p using notation from
Bisani and Ney (2008). The details of the model,
its parameters and the utilized smoothing techniques
can be found in Bisani and Ney (2008).
The training data is a list of word pairs (a source
word and its presumed transliteration) extracted
from a word-aligned parallel corpus. g2p builds a
joint sequence model on the character sequences of
the word pairs and infers m-to-n alignments between
source and target characters with Expectation Maxi-
mization (EM) training. The m-to-n character align-
ment units are referred to as ?multigrams?.
The model built on multigrams consisting of
source and target character sequences greater than
one learns too much noise (non-transliteration infor-
mation) from the training data and performs poorly.
In our experiments, we use multigrams with a maxi-
mum of one character on the source and one charac-
ter on the target side (i.e., 0,1-to-0,1 character align-
ment units).
The N-gram approximation of the joint probabil-
ity can be defined in terms of multigrams qi as:
p(qk1 ) ?
k+1?
j=1
p(qj |q
j?1
j?N+1) (1)
where q0, qk+1 are set to a special boundary symbol.
N-gram models of order > 1 did not work well
because these models tended to learn noise (infor-
mation from non-transliteration pairs) in the training
data. For our experiments, we only trained g2p with
the unigram model.
In test mode, we look for the best sequence of
multigrams given a fixed source and target string and
return the probability of this sequence.
For the mining process, we trained g2p on
lists containing both transliteration pairs and non-
transliteration pairs.
2.2 Statistical Machine Transliteration System
We build a phrase-based MT system for translitera-
tion using the Moses toolkit (Koehn et al, 2003). We
also tried using g2p for implementing the translit-
eration decoder but found Moses to perform bet-
ter. Moses has the advantage of using Minimum Er-
ror Rate Training (MERT) which optimizes translit-
eration accuracy rather than the likelihood of the
training data as g2p does. The training data con-
tains more non-transliteration pairs than transliter-
ation pairs. We don?t want to maximize the like-
lihood of the non-transliteration pairs. Instead we
want to optimize the transliteration performance for
test data. Secondly, it is easy to use a large language
model (LM) with Moses. We build the LM on the
target word types in the data to be filtered.
For training Moses as a transliteration system, we
treat each word pair as if it were a parallel sentence,
by putting spaces between the characters of each
word. The model is built with the default settings
of the Moses toolkit. The distortion limit ?d? is set
to zero (no reordering). The LM is implemented as
a five-gram model using the SRILM-Toolkit (Stol-
cke, 2002), with Add-1 smoothing for unigrams and
Kneser-Ney smoothing for higher n-grams.
431
3 Extraction of Transliteration Pairs
Training of a supervised transliteration system re-
quires a list of transliteration pairs which is expen-
sive to create. Such lists are usually either built man-
ually or extracted using a classifier trained on man-
ually labelled data and using other language depen-
dent information. In this section, we present an it-
erative method for the extraction of transliteration
pairs from parallel corpora which is fully unsuper-
vised and language pair independent.
Initially, we extract a list of word pairs from a
word-aligned parallel corpus using GIZA++. The
extracted word pairs are either transliterations, other
kinds of translations, or misalignments. In each it-
eration, we first train g2p on the list of word pairs.
Then we delete those 5% of the (remaining) train-
ing data which are least likely to be transliterations
according to g2p.1 We determine the best iteration
according to our stopping criterion and return the fil-
tered data set from this iteration. The stopping crite-
rion uses unlabelled held-out data to predict the opti-
mal stopping point. The following sections describe
the transliteration mining method in detail.
3.1 Methodology
We will first describe the iterative filtering algorithm
(Algorithm 1) and then the algorithm for the stop-
ping criterion (Algorithm 2). In practice, we first
run Algorithm 2 for 100 iterations to determine the
best number of iterations. Then, we run Algorithm 1
for that many iterations.
Initially, the parallel corpus is word-aligned using
GIZA++ (Och and Ney, 2003), and the alignments
are refined using the grow-diag-final-and heuristic
(Koehn et al, 2003). We extract all word pairs which
occur as 1-to-1 alignments in the word-aligned cor-
pus. We ignore non-1-to-1 alignments because they
are less likely to be transliterations for most lan-
guage pairs. The extracted set of word pairs will be
called ?list of word pairs? later on. We use the list
of word pairs as the training data for Algorithm 1.
Algorithm 1 builds a joint sequence model using
g2p on the training data and computes the joint prob-
ability of all word pairs according to g2p. We nor-
malize the probabilities by taking the nth square root
1Since we delete 5% from the filtered data, the number of
deleted data items decreases in each iteration.
Algorithm 1 Mining of transliteration pairs
1: training data?list of word pairs
2: I? 0
3: repeat
4: Build a joint source channel model on the training
data using g2p and compute the joint probability
of every word pair.
5: Remove the 5% word pairs with the lowest length-
normalized probability from the training data.
{and repeat the process with the filtered training
data}
6: I? I+1
7: until I = Stopping iteration from Algorithm 2
where n is the average length of the source and the
target string. The training data contains mostly non-
transliteration pairs and a few transliteration pairs.
Therefore the training data is initially very noisy and
the joint sequence model is not very accurate. How-
ever it can successfully be used to eliminate a few
word pairs which are very unlikely to be translitera-
tions.
On the filtered training data, we can train a model
which is slightly better than the previous model. Us-
ing this improved model, we can eliminate further
non-transliterations.
Our results show that at the iteration determined
by our stopping criterion, the filtered set mostly
contains transliterations and only a small number
of transliterations have been mistakenly eliminated
(see section 4.2).
Algorithm 2 automatically determines the best
stopping point of the iterative transliteration min-
ing process. It is an extension of Algorithm 1. It
runs the iterative process of Algorithm 1 on half of
the list of word pairs (training data) for 100 itera-
tions. For every iteration, it builds a transliteration
system on the filtered data. The transliteration sys-
tem is tested on the source side of the other half of
the list of word pairs (held-out). The output of the
transliteration system is matched against the target
side of the held-out data. (These target words are ei-
ther transliterations, translations or misalignments.)
We match the target side of the held-out data under
the assumption that all matches are transliterations.
The iteration where the output of the transliteration
system best matches the held-out data is chosen as
the stopping iteration of Algorithm 1.
432
Algorithm 2 Selection of the stopping iteration for
the transliteration mining algorithm
1: Create clusters of word pairs from the list of word
pairs which have a common prefix of length 2 both
on the source and target language side.
2: Randomly add each cluster either to the training data
or to the held-out data.
3: I? 0
4: while I < 100 do
5: Build a joint sequence model on the training
data using g2p and compute the length-normalized
joint probability of every word pair in the training
data.
6: Remove the 5% word pairs with the lowest prob-
ability from the training data. {The training data
will be reduced by 5% of the rest in each iteration}
7: Build a transliteration system on the filtered train-
ing data and test it using the source side of the
held-out and match the output against the target
side of the held-out.
8: I? I+1
9: end while
10: Collect statistics of the matching results and take the
median from 9 consecutive iterations (median9).
11: Choose the iteration with the best median9 score for
the transliteration mining process.
We will now describe Algorithm 2 in detail. Al-
gorithm 2 initially splits the word pairs into training
and held-out data. This could be done randomly, but
it turns out that this does not work well for some
tasks. The reason is that the parallel corpus con-
tains inflectional variants of the same word. If two
variants are distributed over training and held-out
data, then the one in the training data may cause the
transliteration system to produce a correct transla-
tion (but not transliteration) of its variant in the held-
out data. This problem is further discussed in section
4.2.2. Instead of randomly splitting the data, we first
create clusters of word pairs which have a common
prefix of length 2 both on the source and target lan-
guage side. We randomly add each cluster either to
the training data or to the held-out data.
We repeat the mining process (described in Algo-
rithm 1) to eliminate non-transliteration pairs from
the training data. For each iteration of Algorithm 2,
i.e., steps 4 to 9, we build a transliteration system on
the filtered training data and test it on the source side
of the held-out. We collect statistics on how well the
output of the system matches the target side of the
held-out. The matching scores on the held-out data
often make large jumps from iteration to iteration.
We take the median of the results from 9 consecutive
iterations (the 4 iterations before, the current and the
4 iterations after the current iteration) to smooth the
scores. We call this median9. We choose the iter-
ation with the best smoothed score as the stopping
point for the filtering process. In our tests, the me-
dian9 heuristic indicated an iteration close to the op-
timal iteration.
Sometimes several nearby iterations have the
same maximal smoothed score. In that case, we
choose the one with the highest unsmoothed score.
Section 4.2 explains the median9 heuristic in more
detail and presents experimental results showing that
it works well.
4 Experiments
We evaluate our transliteration mining algorithm on
three tasks: transliteration mining from Wikipedia
InterLanguage Links, transliteration mining from
parallel corpora, and word alignment using a word
aligner with a transliteration component. On the
WIL data sets, we compare our fully unsupervised
system with the semi-supervised systems presented
at the NEWS10 (Kumaran et al, 2010). In the eval-
uation on parallel corpora, we compare our min-
ing results with a manually built gold standard in
which each word pair is either marked as a translit-
eration or as a non-transliteration. In the word align-
ment experiment, we integrate a transliteration mod-
ule which is trained on the transliterations pairs ex-
tracted by our method into a word aligner and show
a significant improvement. The following sections
describe the experiments in detail.
4.1 Experiments Using Parallel Phrases of
Wikipedia InterLanguage Links
We conduct transliteration mining experiments on
the English/Arabic, English/Hindi, English/Tamil
and English/Russian Wikipedia InterLanguage
Links (WIL) used in the NEWS10.2 All data sets
2We do not evaluate on the English/Chinese data because
the Chinese data requires word segmentation which is beyond
the scope of our work. Another problem is that our extraction
method was developed for alphabetic languages and probably
needs to be adapted before it is applicable to logographic lan-
guages such as Chinese.
433
Our S-Best S-Worst Systems Rank
EA 87.4 91.5 70.2 16 3
ET 90.1 91.4 57.5 14 3
EH 92.2 94.4 71.4 14 3
Table 1: Summary of results on NEWS10 data sets where
?EA? is English/Arabic, ?ET? is English/Tamil and ?EH?
is English/Hindi. ?Our? shows the F-measure of our fil-
tered data against the gold standard using the supplied
evaluation tool, ?Systems? is the total number of partic-
ipants in the subtask, and ?Rank? is the rank we would
have obtained if our system had participated.
contain training data, seed data and reference data.
We make no use of the seed data since our system is
fully unsupervised. We calculate the F-measure of
our filtered transliteration pairs against the supplied
gold standard using the supplied evaluation tool.
For English/Arabic, English/Hindi and En-
glish/Tamil, our system is better than most of the
semi-supervised systems presented at the NEWS
2010 shared task for transliteration mining. Table 1
summarizes the F-scores on these data sets.
On the English/Russian data set, our system
achieves 76% F-measure which is not good com-
pared with the systems that participated in the shared
task. The English/Russian corpus contains many
cognates which ? according to the NEWS10 defi-
nition ? are not transliterations of each other. Our
system learns the cognates in the training data and
extracts them as transliterations (see Table 2).
The two best teams on the English/Russian task
presented various extraction methods (Jiampoja-
marn et al, 2010; Darwish, 2010). Their sys-
tems behave differently on English/Russian than on
other language pairs. Their best systems for En-
glish/Russian are only trained on the seed data and
the use of unlabelled data does not help the perfor-
mance. Since our system is fully unsupervised, and
the unlabelled data is not useful, we perform badly.
4.2 Experiments Using Parallel Corpora
The Wikipedia InterLanguage Links shared task
data contains a much larger proportion of translitera-
tions than a parallel corpus. In order to examine how
well our method performs on parallel corpora, we
apply it to parallel corpora of English/Hindi and En-
glish/Arabic, and compare the transliteration mining
results with a gold standard.
Table 2: Cognates from English/Russian corpus extracted
by our system as transliteration pairs. None of them are
correct transliteration pairs according to the gold stan-
dard.
We use the English/Hindi corpus from the shared
task on word alignment, organized as part of the
ACL 2005 Workshop on Building and Using Par-
allel Texts (WA05) (Martin et al, 2005). For En-
glish/Arabic, we use a freely available parallel cor-
pus from the United Nations (UN) (Eisele and Chen,
2010). We randomly take 200,000 parallel sentences
from the UN corpus of the year 2000. We cre-
ate gold standards for both language pairs by ran-
domly selecting a few thousand word pairs from the
lists of word pairs extracted from the two corpora.
We manually tag them as either transliterations or
non-transliterations. The English/Hindi gold stan-
dard contains 180 transliteration pairs and 2084
non-transliteration pairs and the English/Arabic gold
standard contains 288 transliteration pairs and 6639
non-transliteration pairs. We have submitted these
gold standards with the paper. They are available to
the research community.
In the following sections, we describe the me-
dian9 heuristic and the splitting method of Algo-
rithm 2. The splitting method is used to avoid early
peaks in the held-out statistics, and the median9
heuristic smooths the held-out statistics in order to
obtain a single peak.3
4.2.1 Motivation for Median9 Heuristic
Algorithm 2 collects statistics from the held-out data
(step 10) and selects the stopping iteration. Due to
the noise in the held-out data, the transliteration ac-
curacy on the held-out data often jumps from itera-
tion to iteration. The dotted line in figure 1 (right)
shows the held-out prediction accuracy for the En-
3We do not use the seed data in our system. However,
to check the correctness of the stopping point, we tested
the transliteration system on the seed data (available with
NEWS10) for every iteration of Algorithm 2. We verified that
the median9 held-out statistics and accuracy on the seed data
have their peaks at the same iteration.
434
glish/Hindi parallel corpus. The curve is very noisy
and has two peaks. It is difficult to see the effect of
the filtering. We take the median of the results from
9 consecutive iterations to smooth the scores. The
solid line in figure 1 (right) shows a smoothed curve
built using the median9 held-out scores. A compari-
son with the gold standard (section 4.2.3) shows that
the stopping point (peak) reached using the median9
heuristic is better than the stopping point obtained
with unsmoothed scores.
4.2.2 Motivation for Splitting Method
Algorithm 2 initially splits the list of word pairs into
training and held-out data. A random split worked
well for the WIL data, but failed on the parallel cor-
pora. The reason is that parallel corpora contain in-
flectional variants of the same word. If these vari-
ants are randomly distributed over training and held-
out data, then a non-transliteration word pair such as
the English-Hindi pair ?change ? badlao? may end
up in the training data and the related pair ?changes
? badlao? in the held-out data. The Moses system
used for transliteration will learn to ?transliterate?
(or actually translate) ?change? to ?badlao?. From
other examples, it will learn that a final ?s? can be
dropped. As a consequence, the Moses transliterator
may produce the non-transliteration ?badlao? for the
English word ?changes? in the held-out data. Such
matching predictions of the transliterator which are
actually translations lead to an overestimate of the
transliteration accuracy and may cause Algorithm 2
to predict a stopping iteration which is too early.
By splitting the list of word pairs in such a way
that inflectional variants of a word are placed either
in the training data, or in the held-out, but not in
both, this problem can be solved.4
The left graph in Figure 1 shows that the median9
held-out statistics obtained after a random data split
of a Hindi/English corpus contains two peaks which
occur too early. These peaks disappear in the right
graph of Figure 1 which shows the results obtained
after a split with the clustering method.
The overall trend of the smoothed curve in fig-
ure 1 (right) is very clear. We start by filtering out
non-transliteration pairs from the data, so the results
4This solution is appropriate for all of the language pairs
used in our experiments, but should be revisited if there is in-
flection realized as prefixes, etc.
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0 10 20 30 40 50 60 70 80 90
accu
racy
iterations
held outmedian9
0
0.1
0.2
0.3
0.4
0.5
0.6
0 10 20 30 40 50 60 70 80 90
accu
racy
iterations
held outmedian 9
Figure 1: Statistics of held-out prediction of En-
glish/Hindi data using modified Algorithm 2 with random
division of the list of word pairs (left) and using Algo-
rithm 2 (right). The dotted line shows unsmoothed held-
out scores and solid line shows median9 held-out scores
of the transliteration system go up. When no more
non-transliteration pairs are left, we start filtering
out transliteration pairs and the results of the system
go down. We use this stopping criterion for all lan-
guage pairs and achieve consistently good results.
4.2.3 Results on Parallel Corpora
According to the gold standard, the English/Hindi
and English/Arabic data sets contain 8% and 4%
transliteration pairs respectively. We repeat the same
mining procedure ? run Algorithm 2 up to 100 itera-
tions and return the stopping iteration. Then, we run
Algorithm 1 up to the stopping iteration returned by
Algorithm 2 and obtain the filtered data.
TP FN TN FP
EH Filtered 170 10 2039 45
EA Filtered 197 91 6580 59
Table 3: Transliteration mining results using the parallel
corpus of English/Hindi (EH) and English/Arabic (EA)
against the gold standard
Table 3 shows the mining results on the En-
glish/Hindi and English/Arabic corpora. The gold
standard is a subset of the data sets. The En-
glish/Hindi gold standard contains 180 translitera-
tion pairs and 2084 non-transliteration pairs. The
English/Arabic gold standard contains 288 translit-
eration pairs and 6639 non-transliteration pairs.
From the English/Hindi data, the mining system has
mined 170 transliteration pairs out of 180 transliter-
ation pairs. The English/Arabic mined data contains
197 transliteration pairs out of 288 transliteration
pairs. The mining system has wrongly identified a
few non-transliteration pairs as transliterations (see
435
table 3, last column). Most of these word pairs are
close transliterations and differ by only one or two
characters from perfect transliteration pairs. The
close transliteration pairs provide many valid multi-
grams which may be helpful for the mining system.
4.3 Integration into Word Alignment Model
In the previous section, we presented a method for
the extraction of transliteration pairs from a parallel
corpus. In this section, we will explain how to build
a transliteration module on the extracted transliter-
ation pairs and how to integrate it into MGIZA++
(Gao and Vogel, 2008) by interpolating it with the t-
table probabilities of the IBM models and the HMM
model. MGIZA++ is an extension of GIZA++. It
has the ability to resume training from any model
rather than starting with Model1.
4.3.1 Modified EM Training of the Word
Alignment Models
GIZA++ applies the IBM models (Brown et al,
1993) and the HMM model (Vogel et al, 1996)
in both directions, i.e., source to target and target
to source. The alignments are refined using the
grow-diag-final-and heuristic (Koehn et al, 2003).
GIZA++ generates a list of translation pairs with
alignment probabilities, which is called the t-table.
In this section, we propose a method to modify the
translation probabilities of the t-table by interpolat-
ing the translation counts with transliteration counts.
The interpolation is done in both directions. In the
following, we will only consider the e-to-f direction.
The transliteration module which is used to calcu-
late the conditional transliteration probability is de-
scribed in Algorithm 3.
We build a transliteration system by training
Moses on the filtered transliteration corpus (using
Algorithm 1) and apply it to the e side of the list
of word pairs. For every source word, we gener-
ate the list of 10-best transliterations nbestTI(e).
Then, we extract every f that cooccurs with e in a
parallel sentence and add it to nbestTI(e) which
gives us the list of candidate transliteration pairs
candidateTI(e). We use the sum of transliteration
probabilities
?
f ??CandidateTI(e) pmoses(f
?, e) as an
approximation for the prior probability pmoses(e) =?
f ? pmoses(f
?, e) which is needed to convert the
joint transliteration probability into a conditional
Algorithm 3 Estimation of transliteration probabili-
ties, e-to-f direction
1: unfiltered data?list of word pairs
2: filtered data ?transliteration pairs extracted using
Algorithm 1
3: Train a transliteration system on the filtered data
4: for all e do
5: nbestTI(e) ? 10 best transliterations for e ac-
cording to the transliteration system
6: cooc(e) ? set of all f that cooccur with e in a
parallel sentence
7: candidateTI(e)? cooc(e) ? nbestTI(e)
8: end for
9: for all f do
10: pmoses(f, e)? joint transliteration probability of
e and f according to the transliterator
11: pti(f |e)?
pmoses(f,e)P
f??CandidateTI(e) pmoses(f
?,e)
12: end for
probability. We use the constraint decoding option
of Moses to compute the joint probability of e and f.
It computes the probability by dividing the transla-
tion score of the best target sentence given a source
sentence by the normalization factor.
We combine the transliteration probabilities with
the translation probabilities of the IBM models and
the HMM model. The normal translation probability
pta(f |e) of the word alignment models is computed
with relative frequency estimates.
We smooth the alignment frequencies by adding
the transliteration probabilities weighted by the fac-
tor ? and get the following modified translation
probabilities
p?(f |e) =
fta(f, e) + ?pti(f |e)
fta(e) + ?
(2)
where fta(f, e) = pta(f |e)f(e). pta(f |e) is ob-
tained from the original t-table of the alignment
model. f(e) is the total corpus frequency of e. ?
is the transliteration weight which is optimized for
every language pair (see section 4.3.2). Apart from
the definition of the weight ?, our smoothing method
is equivalent to Witten-Bell smoothing.
We smooth after every iteration of the IBM mod-
els and the HMM model except the last iteration of
each model. Algorithm 4 shows the smoothing for
IBM Model4. IBM Model1 and the HMM model
are smoothed in the same way. We also apply Algo-
rithm 3 and Algorithm 4 in the alignment direction
436
Algorithm 4 Interpolation with the IBM Model4, e-
to-f direction
1: {We want to run four iterations of Model4}
2: f(e)? total frequency of e in the corpus
3: Run MGIZA++ for one iteration of Model4
4: I ? 1
5: while I < 4 do
6: Look up pta(f |e) in the t-table of Model4
7: fta(f, e)? pta(f |e)f(e) for all (f, e)
8: p?(f |e)? fta(f,e)+?pti(f |e)fta(e)+? for all (f, e)
9: Resume MGIZA++ training for 1 iteration using
the modified t-table probabilities p?(f |e)
10: I ? I + 1
11: end while
f to e. The final alignments are generated using the
grow-diag-final-and heuristic (Koehn et al, 2003).
4.3.2 Evaluation
The English/Hindi corpus available from WA05
consists of training, development and test data. As
development and test data for English/Arabic, we
use manually created gold standard word alignments
for 155 sentences extracted from the Hansards cor-
pus released by LDC. We use 50 sentences for de-
velopment and 105 sentences for test.
Baseline: We align the data sets using GIZA++
(Och and Ney, 2003) and refine the alignments us-
ing the grow-diag-final-and heuristic (Koehn et al,
2003). We obtain the baseline F-measure by com-
paring the alignments of the test corpus with the gold
standard alignments.
Experiments We use GIZA++ with 5 iterations of
Model1, 4 iterations of HMM and 4 iterations of
Model4. We interpolate translation and translitera-
tion probabilities at different iterations (and different
combinations of iterations) of the three models and
always observe an improvement in alignment qual-
ity. For the final experiments, we interpolate at every
iteration of the IBM models and the HMM model
except the last iteration of every model where we
could not interpolate for technical reasons.5 Algo-
5We had problems in resuming MGIZA++ training when
training was supposed to continue from a different model, such
as if we stopped after the 5th iteration of Model1 and then
tried to resume MGIZA++ from the first iteration of the HMM
model. In this case, we ran the 5th iteration of Model1, then the
first iteration of the HMM and only then stopped for interpola-
rithm 4 shows the interpolation of the transliteration
probabilities with IBM Model4. We used the same
procedure with IBM Model1 and the HMM model.
The parameter ? is optimized on development
data for every language pair. The word alignment
system is not very sensitive to ?. Any ? in the
range between 50 and 100 works fine for all lan-
guage pairs. The optimization helps to maximize the
improvement in word alignment quality. For our ex-
periments, we use ? = 80.
On test data, we achieve an improvement of
approximately 10% and 13.5% in precision and
3.5% and 13.5% in recall on English/Hindi and En-
glish/Arabic word alignment, respectively. Table 4
shows the scores of the baseline and our word align-
ment model.
Lang Pb Rb Fb Pti Rti Fti
EH 49.1 48.5 51.2 59.1 52.1 55.4
EA 50.8 49.9 50.4 64.4 63.6 64
Table 4: Word alignment results on the test data of En-
glish/Hindi (EH) and English/Arabic (EA) where Pb is
the precision of baseline GIZA++ and Pti is the precision
of our word alignment system
We compared our word alignment results with the
systems presented at WA05. Three systems, one
limited and two un-limited, participated in the En-
glish/Hindi task. We outperform the limited system
and one un-limited system.
5 Previous Research
Previous work on transliteration mining uses a man-
ually labelled set of training data to extract translit-
eration pairs from a parallel corpus or comparable
corpora. The training data may contain a few hun-
dred randomly selected transliteration pairs from a
transliteration dictionary (Yoon et al, 2007; Sproat
et al, 2006; Lee and Chang, 2003) or just a few
carefully selected transliteration pairs (Sherif and
Kondrak, 2007; Klementiev and Roth, 2006). Our
work is more challenging as we extract translitera-
tion pairs without using transliteration dictionaries
or gold standard transliteration pairs.
Klementiev and Roth (2006) initialize their
transliteration model with a list of 20 transliteration
tion; so we did not interpolate in just those iterations of training
where we were transitioning from one model to the next.
437
pairs. Their model makes use of temporal scoring
to rank the candidate transliterations. A lot of work
has been done on discovering and learning translit-
erations from comparable corpora by using temporal
and phonetic information (Tao et al, 2006; Klemen-
tiev and Roth, 2006; Sproat et al, 2006). We do not
have access to this information.
Sherif and Kondrak (2007) train a probabilistic
transducer on 14 manually constructed translitera-
tion pairs of English/Arabic. They iteratively extract
transliteration pairs from the test data and add them
to the training data. Our method is different from the
method of Sherif and Kondrak (2007) as our method
is fully unsupervised, and because in each iteration,
they add the most probable transliteration pairs to
the training data, while we filter out the least proba-
ble transliteration pairs from the training data.
The transliteration mining systems of the four
NEWS10 participants are either based on discrim-
inative or on generative methods. All systems use
manually labelled (seed) data for the initial training.
The system based on the edit distance method sub-
mitted by Jiampojamarn et al (2010) performs best
for the English/Russian task. Jiampojamarn et al
(2010) submitted another system based on a stan-
dard n-gram kernel which ranked first for the En-
glish/Hindi and English/Tamil tasks.6 For the En-
glish/Arabic task, the transliteration mining system
of Noeman and Madkour (2010) was best. They
normalize the English and Arabic characters in the
training data which increases the recall.7
Our transliteration extraction method differs in
that we extract transliteration pairs from a paral-
lel corpus without supervision. The results of the
NEWS10 experiments (Kumaran et al, 2010) show
that no single system performs well on all language
pairs. Our unsupervised method seems robust as its
performance is similar to or better than many of the
semi-supervised systems on three language pairs.
We are only aware of one previous work which
uses transliteration information for word alignment.
6They use the seed data as positive examples. In order to
obtain also negative examples, they generate all possible word
pairs from the source and target words in the seed data and ex-
tract the ones which are not transliterations but have a common
substring of some minimal length.
7They use the phrase table of Moses to build a mapping table
between source and target characters. The mapping table is then
used to construct a finite state transducer.
Hermjakob (2009) proposed a linguistically focused
word alignment system which uses many features
including hand-crafted transliteration rules for Ara-
bic/English alignment. His evaluation did not ex-
plicitly examine the effect of transliteration (alone)
on word alignment. We show that the integration
of a transliteration system based on unsupervised
transliteration mining increases the word alignment
quality for the two language pairs we tested.
6 Conclusion
We proposed a method to automatically extract
transliteration pairs from parallel corpora without
supervision or linguistic knowledge. We evaluated
it against the semi-supervised systems of NEWS10
and achieved high F-measure and performed bet-
ter than most of the semi-supervised systems. We
also evaluated our method on parallel corpora and
achieved high F-measure. We integrated the translit-
eration extraction module into the GIZA++ word
aligner and showed gains in alignment quality. We
will release our transliteration mining system and
word alignment system in the near future.
Acknowledgments
The authors wish to thank the anonymous re-
viewers for their comments. We would like to
thank Christina Lioma for her valuable feedback
on an earlier draft of this paper. Hassan Sajjad
was funded by the Higher Education Commission
(HEC) of Pakistan. Alexander Fraser was funded
by Deutsche Forschungsgemeinschaft grant Models
of Morphosyntax for Statistical Machine Transla-
tion. Helmut Schmid was supported by Deutsche
Forschungsgemeinschaft grant SFB 732.
References
Maximilian Bisani and Hermann Ney. 2008. Joint-
sequence models for grapheme-to-phoneme conver-
sion. Speech Communication, 50(5).
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and R. L. Mercer. 1993. The mathematics of
statistical machine translation: parameter estimation.
Computational Linguistics, 19(2):263?311.
Kareem Darwish. 2010. Transliteration mining with
phonetic conflation and iterative training. In Proceed-
ings of the 2010 Named Entities Workshop, Uppsala,
Sweden. Association for Computational Linguistics.
438
Andreas Eisele and Yu Chen. 2010. MultiUN: A multi-
lingual corpus from United Nation documents. In Pro-
ceedings of the Seventh conference on International
Language Resources and Evaluation (LREC?10), Val-
letta, Malta.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engineer-
ing, Testing, and Quality Assurance for Natural Lan-
guage Processing, Columbus, Ohio, June. Association
for Computational Linguistics.
Ulf Hermjakob. 2009. Improved word alignment with
statistics and linguistic heuristics. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing: Volume 1 - Volume 1, EMNLP
?09, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Sittichai Jiampojamarn, Kenneth Dwyer, Shane Bergsma,
Aditya Bhargava, Qing Dou, Mi-Young Kim, and
Grzegorz Kondrak. 2010. Transliteration generation
and mining with limited training resources. In Pro-
ceedings of the 2010 Named Entities Workshop, Upp-
sala, Sweden. Association for Computational Linguis-
tics.
Alexandre Klementiev and Dan Roth. 2006. Weakly
supervised named entity transliteration and discovery
from multilingual comparable corpora. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and the 44th annual meeting of the
ACL, Morristown, NJ, USA.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
the Human Language Technology and North Ameri-
can Association for Computational Linguistics Con-
ference, pages 127?133, Edmonton, Canada.
A Kumaran, Mitesh M. Khapra, and Haizhou Li. 2010.
Whitepaper of news 2010 shared task on translitera-
tion mining. In Proceedings of the 2010 Named En-
tities Workshop the 48th Annual Meeting of the ACL,
Uppsala, Sweden.
Chun-Jen Lee and Jason S. Chang. 2003. Acqui-
sition of English-Chinese transliterated word pairs
from parallel-aligned texts using a statistical machine
transliteration model. In Proceedings of the HLT-
NAACL 2003Workshop on Building and using parallel
texts, Morristown, NJ, USA. ACL.
Joel Martin, Rada Mihalcea, and Ted Pedersen. 2005.
Word alignment for languages with scarce resources.
In ParaText ?05: Proceedings of the ACL Workshop
on Building and Using Parallel Texts, Morristown, NJ,
USA. Association for Computational Linguistics.
Sara Noeman and Amgad Madkour. 2010. Language
independent transliteration mining system using finite
state automata framework. In Proceedings of the 2010
Named Entities Workshop, Uppsala, Sweden. Associ-
ation for Computational Linguistics.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Tarek Sherif and Grzegorz Kondrak. 2007. Boot-
strapping a stochastic transducer for Arabic-English
transliteration extraction. In ACL, Prague, Czech Re-
public.
Richard Sproat, Tao Tao, and ChengXiang Zhai. 2006.
Named entity transliteration with comparable corpora.
In ACL.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Intl. Conf. Spoken Language Pro-
cessing, Denver, Colorado.
Tao Tao, Su-Yoon Yoon, Andrew Fister, Richard Sproat,
and ChengXiang Zhai. 2006. Unsupervised named
entity transliteration using temporal and phonetic
correlation. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), Sydney.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In 16th International Conference on Computa-
tional Linguistics, pages 836?841, Copenhagen, Den-
mark.
Su-Youn Yoon, Kyoung-Young Kim, and Richard Sproat.
2007. Multilingual transliteration using feature based
phonetic method. In Proceedings of the 45th Annual
Meeting of the ACL, Prague, Czech Republic.
439
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1045?1054,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
A Joint Sequence Translation Model with Integrated Reordering
Nadir Durrani Helmut Schmid Alexander Fraser
Institute for Natural Language Processing
University of Stuttgart
{durrani,schmid,fraser}@ims.uni-stuttgart.de
Abstract
We present a novel machine translation model
which models translation by a linear sequence
of operations. In contrast to the ?N-gram?
model, this sequence includes not only trans-
lation but also reordering operations. Key
ideas of our model are (i) a new reordering
approach which better restricts the position to
which a word or phrase can be moved, and
is able to handle short and long distance re-
orderings in a unified way, and (ii) a joint
sequence model for the translation and re-
ordering probabilities which is more flexi-
ble than standard phrase-based MT. We ob-
serve statistically significant improvements in
BLEU over Moses for German-to-English and
Spanish-to-English tasks, and comparable re-
sults for a French-to-English task.
1 Introduction
We present a novel generative model that explains
the translation process as a linear sequence of oper-
ations which generate a source and target sentence
in parallel. Possible operations are (i) generation of
a sequence of source and target words (ii) insertion
of gaps as explicit target positions for reordering op-
erations, and (iii) forward and backward jump oper-
ations which do the actual reordering. The probabil-
ity of a sequence of operations is defined according
to an N-gram model, i.e., the probability of an op-
eration depends on the n ? 1 preceding operations.
Since the translation (generation) and reordering op-
erations are coupled in a single generative story,
the reordering decisions may depend on preceding
translation decisions and translation decisions may
depend on preceding reordering decisions. This pro-
vides a natural reordering mechanism which is able
to deal with local and long-distance reorderings in a
consistent way. Our approach can be viewed as an
extension of the N-gram SMT approach (Marin?o et
al., 2006) but our model does reordering as an inte-
gral part of a generative model.
The paper is organized as follows. Section 2 dis-
cusses the relation of our work to phrase-based and
the N-gram SMT. Section 3 describes our genera-
tive story. Section 4 defines the probability model,
which is first presented as a generative model, and
then shifted to a discriminative framework. Section
5 provides details on the search strategy. Section 6
explains the training process. Section 7 describes
the experimental setup and results. Section 8 gives
a few examples illustrating different aspects of our
model and Section 9 concludes the paper.
2 Motivation and Previous Work
2.1 Relation of our work to PBSMT
Phrase-based SMT provides a powerful translation
mechanism which learns local reorderings, transla-
tion of short idioms, and the insertion and deletion of
words sensitive to local context. However, PBSMT
also has some drawbacks. (i) Dependencies across
phrases are not directly represented in the translation
model. (ii) Discontinuous phrases cannot be used.
(iii) The presence of many different equivalent seg-
mentations increases the search space.
Phrase-based SMT models dependencies between
words and their translations inside of a phrase well.
However, dependencies across phrase boundaries
are largely ignored due to the strong phrasal inde-
1045
German English
hat er ein buch gelesen he read a book
hat eine pizza gegessen has eaten a pizza
er he
hat has
ein a
eine a
menge lot of
butterkekse butter cookies
gegessen eaten
buch book
zeitung newspaper
dann then
Table 1: Sample Phrase Table
pendence assumption. A phrase-based system us-
ing the phrase table1 shown in Table 1, for exam-
ple, correctly translates the German sentence ?er
hat eine pizza gegessen? to ?he has eaten a pizza?,
but fails while translating ?er hat eine menge but-
terkekse gegessen? (see Table 1 for a gloss) which
is translated as ?he has a lot of butter cookies eaten?
unless the language model provides strong enough
evidence for a different ordering. The generation of
this sentence in our model starts with generating ?er
? he?, ?hat ? has?. Then a gap is inserted on the Ger-
man side, followed by the generation of ?gegessen ?
eaten?. At this point, the (partial) German and En-
glish sentences look as follows:
er hat gegessen
he has eaten
We jump back to the gap on the German side
and fill it by generating ?eine ? a? and ?pizza ?
pizza?, for the first example and generating ?eine ?
a?, ?menge ? lot of?, ?butterkekse ? butter cookies?
for the second example, thus handling both short
and long distance reordering in a unified manner.
Learning the pattern ?hat gegessen ? has eaten?
helps us to generalize to the second example with
unseen context. Notice how the reordering deci-
sion is triggered by the translation decision in our
model. The probability of a gap insertion operation
after the generation of the auxiliaries ?hat ? has? will
be high because reordering is necessary in order to
move the second part of the German verb complex
(?gegessen?) to its correct position at the end of the
clause. This mechanism better restricts reordering
1The examples given in this section are not taken from the
real data/system, but made-up for the sake of argument.
Figure 1: (a) Known Context (b) Unknown Context
than traditional PBSMT and is able to deal with local
and long-distance reorderings in a consistent way.
Another weakness of the traditional phrase-based
system is that it can only capitalize on continuous
phrases. Given the phrase inventory in Table 1,
phrasal MT is able to generate example in Figure
1(a). The information ?hat...gelesen ? read? is inter-
nal to the phrase pair ?hat er ein buch gelesen ? he
read a book?, and is therefore handled conveniently.
On the other hand, the phrase table does not have
the entry ?hat er eine zeitung gelesen ? he read a
newspaper? (Figure 1(b)). Hence, there is no option
but to translate ?hat...gelesen? separately, translat-
ing ?hat? to ?has? which is a common translation for
?hat? but wrong in the given context. Context-free
hierarchical models (Chiang, 2007; Melamed, 2004)
have rules like ?hat er X gelesen ? he read X? to han-
dle such cases. Galley and Manning (2010) recently
solved this problem for phrasal MT by extracting
phrase pairs with source and target-side gaps. Our
model can also use tuples with source-side discon-
tinuities. The above sentence would be generated
by the following sequence of operations: (i) gener-
ate ?dann ? then? (ii) insert a gap (iii) generate ?er
? he? (iv) backward jump to the gap (v) generate
?hat...[gelesen] ? read? (only ?hat? and ?read? are
added to the sentences yet) (vi) jump forward to the
right-most source word so far generated (vii) insert
a gap (viii) continue the source cept (?gelesen? is in-
serted now) (ix) backward jump to the gap (x) gen-
erate ?ein ? a? (xi) generate ?buch ? book?.
Figure 2: Pattern
From this operation se-
quence, the model learns a
pattern (Figure 2) which al-
lows it to generalize to the
example in Figure 1(b). The open gap represented
by serves a similar purpose as the non-terminal
categories in a hierarchical phrase-based system
such as Hiero. Thus it generalizes to translate ?eine
zeitung? in exactly the same way as ?ein buch?.
1046
Another problem of phrasal MT is spurious
phrasal segmentation. Given a sentence pair and
a corresponding word alignment, phrasal MT can
learn an arbitrary number of source segmentations.
This is problematic during decoding because differ-
ent compositions of the same minimal phrasal units
are allowed to compete with each other.
2.2 Relation of our work to N-gram SMT
N-gram based SMT is an alternative to hierarchi-
cal and non-hierarchical phrase-based systems. The
main difference between phrase-based and N-gram
SMT is the extraction procedure of translation units
and the statistical modeling of translation context
(Crego et al, 2005a). The tuples used in N-gram
systems are much smaller translation units than
phrases and are extracted in such a way that a unique
segmentation of each bilingual sentence pair is pro-
duced. This helps N-gram systems to avoid the
spurious phrasal segmentation problem. Reorder-
ing works by linearization of the source side and tu-
ple unfolding (Crego et al, 2005b). The decoder
uses word lattices which are built with linguistically
motivated re-write rules. This mechanism is further
enhanced with an N-gram model of bilingual units
built using POS tags (Crego and Yvon, 2010). A
drawback of their reordering approach is that search
is only performed on a small number of reorderings
that are pre-calculated on the source side indepen-
dently of the target side. Often, the evidence for
the correct ordering is provided by the target-side
language model (LM). In the N-gram approach, the
LM only plays a role in selecting between the pre-
calculated orderings.
Our model is based on the N-gram SMT model,
but differs from previous N-gram systems in some
important aspects. It uses operation n-grams rather
than tuple n-grams. The reordering approach is en-
tirely different and considers all possible orderings
instead of a small set of pre-calculated orderings.
The standard N-gram model heavily relies on POS
tags for reordering and is unable to use lexical trig-
gers whereas our model exclusively uses lexical trig-
gers and no POS information. Linearization and un-
folding of the source sentence according to the target
sentence enables N-gram systems to handle source-
side gaps. We deal with this phenomenon more di-
rectly by means of tuples with source-side discon-
tinuities. The most notable feature of our work is
that it has a complete generative story of transla-
tion which combines translation and reordering op-
erations into a single operation sequence model.
Like the N-gram model2, our model cannot deal
with target-side discontinuities. These are elimi-
nated from the training data by a post-editing pro-
cess on the alignments (see Section 6). Galley and
Manning (2010) found that target-side gaps were not
useful in their system and not useful in the hierarchi-
cal phrase-based system Joshua (Li et al, 2009).
3 Generative Story
Our generative story is motivated by the complex re-
orderings in the German-to-English translation task.
The German and English sentences are jointly gen-
erated through a sequence of operations. The En-
glish words are generated in linear order3 while
the German words are generated in parallel with
their English translations. Occasionally the trans-
lator jumps back on the German side to insert some
material at an earlier position. After this is done, it
jumps forward again and continues the translation.
The backward jumps always end at designated land-
ing sites (gaps) which were explicitly inserted be-
fore. We use 4 translation and 3 reordering opera-
tions. Each is briefly discussed below.
Generate (X,Y): X and Y are German and English
cepts4 respectively, each with one or more words.
Words in X (German) may be consecutive or discon-
tinuous, but the words in Y (English) must be con-
secutive. This operation causes the words in Y and
the first word in X to be added to the English and
German strings respectively, that were generated so
far. Subsequent words in X are added to a queue to
be generated later. All the English words in Y are
generated immediately because English is generated
in linear order. The generation of the second (and
subsequent) German word in a multi-word cept can
be delayed by gaps, jumps and the Generate Source
Only operation defined below.
Continue Source Cept: The German words added
2However, Crego and Yvon (2009), in their N-gram system,
use split rules to handle target-side gaps and show a slight im-
provement on a Chinese-English translation task.
3Generating the English words in order is also what the de-
coder does when translating from German to English.
4A cept is a group of words in one language translated as a
minimal unit in one specific context (Brown et al, 1993).
1047
to the queue by the Generate (X,Y) operation are
generated by the Continue Source Cept operation.
Each Continue Source Cept operation removes one
German word from the queue and copies it to the
German string. If X contains more than one German
word, say n many, then it requires n translation op-
erations, an initial Generate (X1...Xn, Y ) operation
and n ? 1 Continue Source Cept operations. For
example ?hat...gelesen ? read? is generated by the
operation Generate (hat gelesen, read), which adds
?hat? and ?read? to the German and English strings
and ?gelesen? to a queue. A Continue Source Cept
operation later removes ?gelesen? from the queue
and adds it to the German string.
Generate Source Only (X): The string X is added
at the current position in the German string. This op-
eration is used to generate a German word X with no
corresponding English word. It is performed imme-
diately after its preceding German word is covered.
This is because there is no evidence on the English-
side which indicates when to generate X. Generate
Source Only (X) helps us learn a source word dele-
tion model. It is used during decoding, where a Ger-
man word (X) is either translated to some English
word(s) by a Generate (X,Y) operation or deleted
with a Generate Source Only (X) operation.
Generate Identical: The same word is added at
the current position in both the German and En-
glish strings. The Generate Identical operation is
used during decoding for the translation of unknown
words. The probability of this operation is estimated
from singleton German words that are translated to
an identical string. For example, for a tuple ?Port-
land ? Portland?, where German ?Portland? was ob-
served exactly once during training, we use a Gen-
erate Identical operation rather than Generate (Port-
land, Portland).
We now discuss the set of reordering operations
used by the generative story. Reordering has to be
performed whenever the German word to be gen-
erated next does not immediately follow the previ-
ously generated German word. During the genera-
tion process, the translator maintains an index which
specifies the position after the previously covered
German word (j), an index (Z) which specifies the
index after the right-most German word covered so
far, and an index of the next German word to be cov-
ered (j?). The set of reordering operations used in
Table 2: Step-wise Generation of Example 1(a). The ar-
row indicates position j.
generation depends upon these indexes.
Insert Gap: This operation inserts a gap which acts
as a place-holder for the skipped words. There can
be more than one open gap at a time.
Jump Back (W): This operation lets the translator
jump back to an open gap. It takes a parameter W
specifying which gap to jump to. Jump Back (1)
jumps to the closest gap to Z, Jump Back (2) jumps
to the second closest gap to Z, etc. After the back-
ward jump the target gap is closed.
Jump Forward: This operation makes the transla-
tor jump to Z. It is performed if some already gen-
erated German word is between the previously gen-
erated word and the word to be generated next. A
Jump Back (W) operation is only allowed at position
Z. Therefore, if j 6= Z, a Jump Forward operation
has to be performed prior to a Jump Back operation.
Table 2 shows step by step the generation of a
German/English sentence pair, the corresponding
translation operations, and the respective values of
the index variables. A formal algorithm for convert-
ing a word-aligned bilingual corpus into an opera-
tion sequence is presented in Algorithm 1.
4 Model
Our translation model p(F,E) is based on opera-
tion N-gram model which integrates translation and
reordering operations. Given a source string F , a
sequence of tuples T = (t1, . . . , tn) as hypothe-
sized by the decoder to generate a target string E,
the translation model estimates the probability of a
1048
Algorithm 1 Corpus Conversion Algorithm
i Position of current English cept
j Position of current German word
j? Position of next German word
N Total number of English cepts
fj German word at position j
Ei English cept at position i
Fi Sequence of German words linked to Ei
Li Number of German words linked with Ei
k Number of already generated German words for Ei
aik Position of kth German translation of Ei
Z Position after right-most generated German word
S Position of the first word of a target gap
i := 0; j := 0; k := 0
while fj is an unaligned word do
Generate Source Only (fj)
j := j + 1
Z := j
while i < N do
j? := aik
if j < j? then
if fj was not generated yet then
Insert Gap
if j = Z then
j := j?
else
Jump Forward
if j? < j then
if j < Z and fj was not generated yet then
Insert Gap
W := relative position of target gap
Jump Back (W)
j := S
if j < j? then
Insert Gap
j := j?
if k = 0 then
Generate (Fi, Ei) {or Generate Identical}
else
Continue Source Cept
j := j + 1; k := k + 1
while fj is an unaligned word do
Generate Source Only (fj)
j := j + 1
if Z < j then
Z := j
if k = Li then
i := i + 1; k := 0
Remarks:
We use cept positions for English (not word positions) because
English cepts are composed of consecutive words. German po-
sitions are word-based.
The relative position of the target gap is 1 if it is closest to Z, 2
if it is the second closest gap etc.
The operation Generate Identical is chosen if Fi = Ei and the
overall frequency of the German cept Fi is 1.
generated operation sequence O = (o1, . . . , oJ) as:
p(F,E) ?
J?
j=1
p(oj |oj?m+1...oj?1)
where m indicates the amount of context used. Our
translation model is implemented as an N-gram
model of operations using SRILM-Toolkit (Stolcke,
2002) with Kneser-Ney smoothing. We use a 9-gram
model (m = 8).
Integrating the language model the search is de-
fined as:
E? = argmax
E
pLM (E)p(F,E)
where pLM (E) is the monolingual language model
and p(F,E) is the translation model. But our trans-
lation model is a joint probability model, because of
which E is generated twice in the numerator. We
add a factor, prior probability ppr(E), in the denom-
inator, to negate this effect. It is used to marginalize
the joint-probability model p(F,E). The search is
then redefined as:
E? = argmax
E
pLM (E)
p(F,E)
ppr(E)
Both, the monolingual language and the prior
probability model are implemented as standard
word-based n-gram models:
px(E) ?
J?
j=1
p(wj |wj?m+1, . . . , wj?1)
where m = 4 (5-gram model) for the standard
monolingual model (x = LM ) and m = 8 (same
as the operation model5) for the prior probability
model (x = pr).
In order to improve end-to-end accuracy, we in-
troduce new features for our model and shift from
the generative6 model to the standard log-linear ap-
proach (Och and Ney, 2004) to tune7 them. We
search for a target stringE which maximizes a linear
combination of feature functions:
5In decoding, the amount of context used for the prior prob-
ability is synchronized with the position of back-off in the op-
eration model.
6Our generative model is about 3 BLEU points worse than
the best discriminative results.
7We tune the operation, monolingual and prior probability
models as separate features. We expect the prior probability
model to get a negative weight but we do not force MERT to
assign a negative weight to this feature.
1049
E? = argmax
E
?
?
?
J?
j=1
?jhj(F,E)
?
?
?
where ?j is the weight associated with the feature
hj(F,E). Other than the 3 features discussed above
(log probabilities of the operation model, monolin-
gual language model and prior probability model),
we train 8 additional features discussed below:
Length Bonus The length bonus feature counts the
length of the target sentence in words.
Deletion Penalty Another feature for avoiding too
short translations is the deletion penalty. Deleting a
source word (Generate Source Only (X)) is a com-
mon operation in the generative story. Because there
is no corresponding target-side word, the monolin-
gual language model score tends to favor this op-
eration. The deletion penalty counts the number of
deleted source words.
Gap Bonus and Open Gap Penalty These features
are introduced to guide the reordering decisions. We
observe a large amount of reordering in the automat-
ically word aligned training text. However, given
only the source sentence (and little world knowl-
edge), it is not realistic to try to model the reasons
for all of this reordering. Therefore we can use a
more robust model that reorders less than humans.
The gap bonus feature sums to the total number of
gaps inserted to produce a target sentence. The open
gap penalty feature is a penalty (paid once for each
translation operation performed) whose value is the
number of open gaps. This penalty controls how
quickly gaps are closed.
Distortion and Gap Distance Penalty We have
two additional features to control the reordering de-
cisions. One of them is similar8 to the distance-
based reordering model used by phrasal MT. The
other feature is the gap distance penalty which calcu-
lates the distance between the first word of a source
ceptX and the start of the left-most gap. This cost is
paid once for each Generate, Generate Identical and
Generate Source Only. For a source cept coverd by
indexes X1, . . . , Xn, we get the feature value gj =
X1?S, where S is the index of the left-most source
word where a gap starts.
8Let X1, . . . , Xn and Y1, . . . , Ym represent indexes of the
source words covered by the tuples tj and tj?1 respectively.
The distance between tj and tj?1 is given as dj = min(|Xk ?
Yl| ? 1) ?Xk ? {X1, . . . , Xn} and ? Yl ? {Y1, . . . , Ym}
Lexical Features We also use source-to-target
p(e|f) and target-to-source p(f |e) lexical transla-
tion probabilities. Our lexical features are standard
(Koehn et al, 2003). The estimation is motivated by
IBM Model-1. Given a tuple ti with source words
f = f1, f2, . . . , fn, target words e = e1, e2, . . . , em
and an alignment a between the source word posi-
tions x = 1, . . . , n and the target word positions
y = 1, . . . ,m, the lexical feature pw(f |e) is com-
puted as follows:
pw(f |e, a) =
n?
x=1
1
|{y : (x, y) ? a}|
?
?(x,y)?a
w(fx|ey)
pw(e|f, a) is computed in the same way.
5 Decoding
Our decoder for the new model performs a stack-
based search with a beam-search algorithm similar
to that used in Pharoah (Koehn, 2004a). Given an
input sentence F , it first extracts a set of match-
ing source-side cepts along with their n-best trans-
lations to form a tuple inventory. During hypoth-
esis expansion, the decoder picks a tuple from the
inventory and generates the sequence of operations
required for the translation with this tuple in light
of the previous hypothesis.9 The sequence of op-
erations may include translation (generate, continue
source cept etc.) and reordering (gap insertions,
jumps) operations. The decoder also calculates the
overall cost of the new hypothesis. Recombination
is performed on hypotheses having the same cov-
erage vector, monolingual language model context,
and operation model context. We do histogram-
based pruning, maintaining the 500 best hypotheses
for each stack.10
9A hypothesis maintains the index of the last source word
covered (j), the position of the right-most source word covered
so far (Z), the number of open gaps, the number of gaps so
far inserted, the previously generated operations, the generated
target string, and the accumulated values of all the features dis-
cussed in Section 4.
10We need a higher beam size to produce translation units
similar to the phrase-based systems. For example, the phrase-
based system can learn the phrase pair ?zum Beispiel ? for ex-
ample? and generate it in a single step placing it directly into the
stack two words to the right. Our system generates this example
with two separate tuple translations ?zum ? for? and ?Beispiel
? example? in two adjacent stacks. Because ?zum ? for? is not
a frequent translation unit, it will be ranked quite low in the first
stack until the tuple ?Beispiel ? example? appears in the second
stack. Koehn and his colleagues have repeatedly shown that in-
1050
Figure 3: Post-editing of Alignments (a) Initial (b) No
Target-Discontinuities (c) Final Alignments
6 Training
Training includes: (i) post-editing of the alignments,
(ii) generation of the operation sequence (iii) estima-
tion of the n-gram language models.
Our generative story does not handle target-side
discontinuities and unaligned target words. There-
fore we eliminate them from the training corpus in a
3-step process: If a source word is aligned with mul-
tiple target words which are not consecutive, first
the link to the least frequent target word is iden-
tified, and the group of links containing this word
is retained while the others are deleted. The in-
tuition here is to keep the alignments containing
content words (which are less frequent than func-
tional words). The new alignment has no target-
side discontinuities anymore, but might still contain
unaligned target words. For each unaligned target
word, we determine the (left or right) neighbour that
it appears more frequently with and align it with the
same source word as the neighbour. The result is
an alignment without target-side discontinuities and
unaligned target words. Figure 3 shows an illustra-
tive example of the process. The tuples in Figure 3c
are ?A ? U V?, ?B ? W X Y?, ?C ? NULL?, ?D ? Z?.
We apply Algorithm 1 to convert the preprocessed
aligned corpus into a sequence of translation opera-
tions. The resulting operation corpus contains one
sequence of operations per sentence pair.
In the final training step, the three language mod-
els are trained using the SRILM Toolkit. The oper-
ation model is estimated from the operation corpus.
The prior probability model is estimated from the
target side part of the bilingual corpus. The mono-
lingual language model is estimated from the target
side of the bilingual corpus and additional monolin-
gual data.
creasing the Moses stack size from 200 to 1000 does not have
a significant effect on translation into English, see (Koehn and
Haddow, 2009) and other shared task papers.
7 Experimental Setup
7.1 Data
We evaluated the system on three data sets with
German-to-English, Spanish-to-English and French-
to-English news translations, respectively. We used
data from the 4th version of the Europarl Corpus
and the News Commentary which was made avail-
able for the translation task of the Fourth Workshop
on Statistical Machine Translation.11 We use 200K
bilingual sentences, composed by concatenating the
entire news commentary (? 74K sentences) and Eu-
roparl (? 126K sentence), for the estimation of the
translation model. Word alignments were generated
with GIZA++ (Och and Ney, 2003), using the grow-
diag-final-and heuristic (Koehn et al, 2005). In or-
der to obtain the best alignment quality, the align-
ment task is performed on the entire parallel data and
not just on the training data we use. All data is low-
ercased, and we use the Moses tokenizer and recap-
italizer. Our monolingual language model is trained
on 500K sentences. These comprise 300K sentences
from the monolingual corpus (news commentary)
and 200K sentences from the target-side part of the
bilingual corpus. The latter part is also used to train
the prior probability model. The dev and test sets
are news-dev2009a and news-dev2009b which con-
tain 1025 and 1026 parallel sentences. The feature
weights are tuned with Z-MERT (Zaidan, 2009).
7.2 Results
Baseline: We compare our model to a recent ver-
sion of Moses (Koehn et al, 2007) using Koehn?s
training scripts and evaluate with BLEU (Papineni
et al, 2002). We provide Moses with the same ini-
tial alignments as we are using to train our system.12
We use the default parameters for Moses, and a 5-
gram English language model (the same as in our
system).
We compare two variants of our system. The first
system (Twno?rl) applies no hard reordering limit
and uses the distortion and gap distance penalty fea-
tures as soft constraints, allowing all possible re-
orderings. The second system (Twrl?6) uses no dis-
tortion and gap distance features, but applies a hard
constraint which limits reordering to no more than 6
11http://www.statmt.org/wmt09/translation-task.html
12We tried applying our post-processing to the alignments
provided to Moses and found that this made little difference.
1051
Source German Spanish French
Blno?rl 17.41 19.85 19.39
Blrl?6 18.57 21.67 20.84
Twno?rl 18.97 22.17 20.94
Twrl?6 19.03 21.88 20.72
Table 3: This Work(Tw) vs Moses (Bl), no-rl = No Re-
ordering Limit, rl-6 = Reordering limit 6
positions. Specifically, we do not extend hypotheses
that are more than 6 words apart from the first word
of the left-most gap during decoding. In this exper-
iment, we disallowed tuples which were discontin-
uous on the source side. We compare our systems
with two Moses systems as baseline, one using no
reordering limit (Blno?rl) and one using the default
distortion limit of 6 (Blrl?6).
Both of our systems (see Table 3) outperform
Moses on the German-to-English and Spanish-to-
English tasks and get comparable results for French-
to-English. Our best system (Twno?rl), which uses
no hard reordering limit, gives statistically signifi-
cant (p < 0.05)13 improvements over Moses (both
baselines) for the German-to-English and Spanish-
to-English translation task. The results for Moses
drop by more than a BLEU point without the re-
ordering limit (see Blno?rl in Table 3). All our
results are statistically significant over the baseline
Blno?rl for all the language pairs.
In another experiment, we tested our system also
with tuples which were discontinuous on the source
side. These gappy translation units neither improved
the performance of the system with hard reordering
limit (Twrl?6?asg) nor that of the system without
reordering limit (Twno?rl?asg) as Table 4 shows.
In an analysis of the output we found two reasons
for this result: (i) Using tuples with source gaps in-
creases the list of extracted n-best translation tuples
exponentially which makes the search problem even
more difficult. Table 5 shows the number of tuples
(with and without gaps) extracted when decoding
the test file with 10-best translations. (ii) The fu-
ture cost14 is poorly estimated in case of tuples with
gappy source cepts, causing search errors.
In an experiment, we deleted gappy tuples with
13We used Kevin Gimpel?s implementation of pairwise boot-
strap resampling (Koehn, 2004b), 1000 samples.
14The dynamic programming approach of calculating future
cost for bigger spans gives erroneous results when gappy cepts
can interleave. Details omitted due to space limitations.
Source German Spanish French
Twno?rl?asg 18.61 21.60 20.59
Twrl?6?asg 18.65 21.40 20.47
Twno?rl?hsg 18.91 21.93 20.87
Twrl?6?hsg 19.23 21.79 20.85
Table 4: Our Systems with Gappy Units, asg = All Gappy
Units, hsg = Heuristic for pruning Gappy Units
Source German Spanish French
Gaps 965515 1705156 1473798
No-Gaps 256992 313690 343220
Heuristic (hsg) 281618 346993 385869
Table 5: 10-best Translation Options With & Without
Gaps and using our Heuristic
a score (future cost estimate) lower than the sum of
the best scores of the parts. This heuristic removes
many useless discontinuous tuples. We found that
results improved (Twno?rl?hsg and Twrl?6?hsg in
Table 4) compared to the version using all gaps
(Twno?rl?asg, Twrl?6?asg), and are closer to the
results without discontinuous tuples (Twno?rl and
Twrl?6 in Table 3).
8 Sample Output
In this section we compare the output of our sys-
tems and Moses. Example 1 in Figure 4 shows
the powerful reordering mechanism of our model
which moves the English verb phrase ?do not want
to negotiate? to its correct position between the sub-
ject ?they? and the prepositional phrase ?about con-
crete figures?. Moses failed to produce the correct
word order in this example. Notice that although
our model is using smaller translation units ?nicht
? do not?, ?verhandlen ? negotiate? and ?wollen ?
want to?, it is able to memorize the phrase transla-
tion ?nicht verhandlen wollen ? do not want to ne-
gotiate? as a sequence of translation and reordering
operations. It learns the reordering of ?verhandlen ?
negotiate? and ?wollen ? want to? and also captures
dependencies across phrase boundaries.
Example 2 shows how our system without a re-
ordering limit moves the English translation ?vote?
of the German clause-final verb ?stimmen? across
about 20 English tokens to its correct position be-
hind the auxiliary ?would?.
Example 3 shows how the system with gappy tu-
ples translates a German sentence with the particle
verb ?kehrten...zuru?ck? using a single tuple (dashed
lines). Handling phenomena like particle verbs
1052
Figure 4: Sample Output Sentences
strongly motivates our treatment of source side gaps.
The system without gappy units happens to pro-
duce the same translation by translating ?kehrten? to
?returned? and deleting the particle ?zuru?ck? (solid
lines). This is surprising because the operation for
translating ?kehrten? to ?returned? and for deleting
the particle are too far apart to influence each other
in an n-gram model. Moses run on the same exam-
ple deletes the main verb (?kehrten?), an error that
we frequently observed in the output of Moses.
Our last example (Figure 5) shows that our model
learns idioms like ?meiner Meinung nach ? In my
opinion ,? and short phrases like ?gibt es ? there
are? showing its ability to memorize these ?phrasal?
translations, just like Moses.
9 Conclusion
We have presented a new model for statistical MT
which can be used as an alternative to phrase-
based translation. Similar to N-gram based MT,
it addresses three drawbacks of traditional phrasal
MT by better handling dependencies across phrase
boundaries, using source-side gaps, and solving the
phrasal segmentation problem. In contrast to N-
gram based MT, our model has a generative story
which tightly couples translation and reordering.
Furthermore it considers all possible reorderings un-
like N-gram systems that perform search only on
Figure 5: Learning Idioms
a limited number of pre-calculated orderings. Our
model is able to correctly reorder words across
large distances, and it memorizes frequent phrasal
translations including their reordering as probable
operations sequences. Our system outperformed
Moses on standard Spanish-to-English and German-
to-English tasks and achieved comparable results for
French-to-English. A binary version of the corpus
conversion algorithm and the decoder is available.15
Acknowledgments
The authors thank Fabienne Braune and the re-
viewers for their comments. Nadir Durrani was
funded by the Higher Education Commission (HEC)
of Pakistan. Alexander Fraser was funded by
Deutsche Forschungsgemeinschaft grant Models
of Morphosyntax for Statistical Machine Transla-
tion. Helmut Schmid was supported by Deutsche
Forschungsgemeinschaft grant SFB 732.
15http://www.ims.uni-stuttgart.de/?durrani/resources.html
1053
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and R. L. Mercer. 1993. The mathematics of
statistical machine translation: parameter estimation.
Computational Linguistics, 19(2):263?311.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Josep Maria Crego and Franois Yvon. 2009. Gappy
translation units under left-to-right smt decoding. In
Proceedings of the meeting of the European Associa-
tion for Machine Translation (EAMT), pages 66?73,
Barcelona, Spain.
Josep Maria Crego and Franc?ois Yvon. 2010. Improv-
ing reordering with linguistically informed bilingual
n-grams. In Coling 2010: Posters, pages 197?205,
Beijing, China, August. Coling 2010 Organizing Com-
mittee.
Josep M. Crego, Marta R. Costa-juss, Jos B. Mario,
and Jos A. R. Fonollosa. 2005a. Ngram-based ver-
sus phrasebased statistical machine translation. In In
Proceedings of the International Workshop on Spoken
Language Technology (IWSLT05, pages 177?184.
Josep M. Crego, Jose? B. Marin?o, and Adria` de Gispert.
2005b. Reordered search and unfolding tuples for
ngram-based SMT. In Proceedings of the 10th Ma-
chine Translation Summit (MT Summit X), pages 283?
289, Phuket, Thailand.
Michel Galley and Christopher D. Manning. 2010. Ac-
curate non-hierarchical phrase-based translation. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 966?
974, Los Angeles, California, June. Association for
Computational Linguistics.
Philipp Koehn and Barry Haddow. 2009. Edinburgh?s
submission to all tracks of the WMT 2009 shared task
with reordering and speed improvements to Moses.
In Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 160?164, Athens, Greece,
March. Association for Computational Linguistics.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
the Human Language Technology and North Ameri-
can Association for Computational Linguistics Con-
ference, pages 127?133, Edmonton, Canada.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description for
the 2005 iwslt speech translation evaluation. In Inter-
national Workshop on Spoken Language Translation
2005.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics, Demonstration Program,
Prague, Czech Republic.
Philipp Koehn. 2004a. Pharaoh: A beam search decoder
for phrase-based statistical machine translation mod-
els. In AMTA, pages 115?124.
Philipp Koehn. 2004b. Statistical significance tests
for machine translation evaluation. In Dekang Lin
and Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
Zhifei Li, Chris Callison-burch, Chris Dyer, Juri Ganitke-
vitch, Sanjeev Khudanpur, Lane Schwartz, Wren N. G.
Thornton, Jonathan Weese, and Omar F. Zaidan. 2009.
Joshua: An open source toolkit for parsing-based ma-
chine translation.
J.B. Marin?o, R.E. Banchs, J.M. Crego, A. de Gispert,
P. Lambert, J.A.R. Fonollosa, and M.R. Costa-jussa`.
2006. N-gram-based machine translation. Computa-
tional Linguistics, 32(4):527?549.
I. Dan Melamed. 2004. Statistical machine translation
by parsing. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics,
Barcelona, Spain.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(1):417?449.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Intl. Conf. Spoken Language Pro-
cessing, Denver, Colorado.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
1054
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 469?477,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Statistical Model for Unsupervised and Semi-supervised Transliteration
Mining
Hassan Sajjad Alexander Fraser Helmut Schmid
Institute for Natural Language Processing
University of Stuttgart
{sajjad,fraser,schmid}@ims.uni-stuttgart.de
Abstract
We propose a novel model to automatically
extract transliteration pairs from parallel cor-
pora. Our model is efficient, language pair
independent and mines transliteration pairs in
a consistent fashion in both unsupervised and
semi-supervised settings. We model transliter-
ation mining as an interpolation of translitera-
tion and non-transliteration sub-models. We
evaluate on NEWS 2010 shared task data and
on parallel corpora with competitive results.
1 Introduction
Transliteration mining is the extraction of translit-
eration pairs from unlabelled data. Most transliter-
ation mining systems are built using labelled train-
ing data or using heuristics to extract transliteration
pairs. These systems are language pair dependent or
require labelled information for training. Our sys-
tem extracts transliteration pairs in an unsupervised
fashion. It is also able to utilize labelled information
if available, obtaining improved performance.
We present a novel model of transliteration min-
ing defined as a mixture of a transliteration model
and a non-transliteration model. The transliteration
model is a joint source channel model (Li et al,
2004). The non-transliteration model assumes no
correlation between source and target word charac-
ters, and independently generates a source and a tar-
get word using two fixed unigram character models.
We use Expectation Maximization (EM) to learn pa-
rameters maximizing the likelihood of the interpola-
tion of both sub-models. At test time, we label word
pairs as transliterations if they have a higher proba-
bility assigned by the transliteration sub-model than
by the non-transliteration sub-model.
We extend the unsupervised system to a semi-
supervised system by adding a new S-step to the
EM algorithm. The S-step takes the probability es-
timates from unlabelled data (computed in the M-
step) and uses them as a backoff distribution to
smooth probabilities which were estimated from la-
belled data. The smoothed probabilities are then
used in the next E-step. In this way, the parame-
ters learned by EM are constrained to values which
are close to those estimated from the labelled data.
We evaluate our unsupervised and semi-
supervised transliteration mining system on the
datasets available from the NEWS 2010 shared task
on transliteration mining (Kumaran et al, 2010b).
We call this task NEWS10 later on. Compared with
a baseline unsupervised system our unsupervised
system achieves up to 5% better F-measure. On
the NEWS10 dataset, our unsupervised system
achieves an F-measure of up to 95.7%, and on three
language pairs, it performs better than all systems
which participated in NEWS10. We also evaluate
our semi-supervised system which additionally uses
the NEWS10 labelled data for training. It achieves
an improvement of up to 3.7% F-measure over our
unsupervised system. Additional experiments on
parallel corpora show that we are able to effectively
mine transliteration pairs from very noisy data.
The paper is organized as follows. Section 2 de-
scribes previous work. Sections 3 and 4 define our
unsupervised and semi-supervised models. Section
5 presents the evaluation. Section 6 concludes.
469
2 Previous Work
We first discuss the literature on semi-supervised
and supervised techniques for transliteration min-
ing and then describe a previously defined unsuper-
vised system. Supervised and semi-supervised sys-
tems use a manually labelled set of training data to
learn character mappings between source and tar-
get strings. The labelled training data either con-
sists of a few hundred transliteration pairs or of
just a few carefully selected transliteration pairs.
The NEWS 2010 shared task on transliteration min-
ing (NEWS10) (Kumaran et al, 2010b) is a semi-
supervised task conducted on Wikipedia InterLan-
guage Links (WIL) data. The NEWS10 dataset con-
tains 1000 labelled examples (called the ?seed data?)
for initial training. All systems which participated
in the NEWS10 shared task are either supervised or
semi-supervised. They are described in (Kumaran
et al, 2010a). Our transliteration mining model
can mine transliterations without using any labelled
data. However, if there is some labelled data avail-
able, our system is able to use it effectively.
The transliteration mining systems evaluated on
the NEWS10 dataset generally used heuristic meth-
ods, discriminative models or generative models for
transliteration mining (Kumaran et al, 2010a).
The heuristic-based system of Jiampojamarn et
al. (2010) is based on the edit distance method
which scores the similarity between source and tar-
get words. They presented two discriminative meth-
ods ? an SVM-based classifier and alignment-based
string similarity for transliteration mining. These
methods model the conditional probability distribu-
tion and require supervised/semi-supervised infor-
mation for learning. We propose a flexible genera-
tive model for transliteration mining usable for both
unsupervised and semi-supervised learning.
Previous work on generative approaches uses
Hidden Markov Models (Nabende, 2010; Darwish,
2010; Jiampojamarn et al, 2010), Finite State Au-
tomata (Noeman and Madkour, 2010) and Bayesian
learning (Kahki et al, 2011) to learn transliteration
pairs from labelled data. Our method is different
from theirs as our generative story explains the un-
labelled data using a combination of a transliteration
and a non-transliteration sub-model. The translit-
eration model jointly generates source and target
strings, whereas the non-transliteration system gen-
erates them independently of each other.
Sajjad et al (2011) proposed a heuristic-based un-
supervised transliteration mining system. We later
call it Sajjad11. It is the only unsupervised mining
system that was evaluated on the NEWS10 dataset
up until now, as far as we know. That system is com-
putationally expensive. We show in Section 5 that its
runtime is much higher than that of our system.
In this paper, we propose a novel model-based
approach to transliteration mining. Our approach
is language pair independent ? at least for alpha-
betic languages ? and efficient. Unlike the pre-
vious unsupervised system, and unlike the super-
vised and semi-supervised systems we mentioned,
our model can be used for both unsupervised and
semi-supervised mining in a consistent way.
3 Unsupervised Transliteration Mining
Model
A source word and its corresponding target word can
be character-aligned in many ways. We refer to a
possible alignment sequence which aligns a source
word e and a target word f as ?a?. The function
Align(e, f) returns the set of all valid alignment se-
quences a of a word pair (e, f). The joint transliter-
ation probability p1(e, f) of a word pair is the sum
of the probabilities of all alignment sequences:
p1(e, f) =
?
a?Align(e,f)
p(a) (1)
Transliteration systems are trained on a list of
transliteration pairs. The alignment between the
transliteration pairs is learned with Expectation
Maximization (EM). We use a simple unigram
model, so an alignment sequence from function
Align(e, f) is a combination of 0?1, 1?1, and 1?
0 character alignments between a source word e and
its transliteration f . We refer to a character align-
ment unit as ?multigram? later on and represent it
by the symbol ?q?. A sequence of multigrams forms
an alignment of a source and target word. The prob-
ability of a sequence of multigrams a is the product
of the probabilities of the multigrams it contains.
p(a) = p(q1, q2, ..., q|a|) =
|a|?
j=1
p(qj) (2)
470
While transliteration systems are trained on a
clean list of transliteration pairs, our translitera-
tion mining system has to learn from data con-
taining both transliterations and non-transliterations.
The transliteration model p1(e, f) handles only the
transliteration pairs. We propose a second model
p2(e, f) to deal with non-transliteration pairs (the
?non-transliteration model?). Interpolation with the
non-transliteration model allows the transliteration
model to concentrate on modelling transliterations
during EM training. After EM training, transliter-
ation word pairs are assigned a high probability by
the transliteration submodel and a low probability by
the non-transliteration submodel, and vice versa for
non-transliteration pairs. This property is exploited
to identify transliterations.
In a non-transliteration word pair, the characters
of the source and target words are unrelated. We
model them as randomly seeing a source word and a
target word together. The non-transliteration model
uses random generation of characters from two uni-
gram models. It is defined as follows:
p2(e, f) = pE(e) pF (f) (3)
pE(e) =
?|e|
i=1 pE(ei) and pF (f) =
?|f |
i=1 pF (fi).
The transliteration mining model is an interpo-
lation of the transliteration model p1(e, f) and the
non-transliteration model p2(e, f):
p(e, f) = (1? ?)p1(e, f) + ?p2(e, f) (4)
? is the prior probability of non-transliteration.
3.1 Model Estimation
In this section, we discuss the estimation of the pa-
rameters of the transliteration model p1(e, f) and the
non-transliteration model p2(e, f).
The non-transliteration model consists of two un-
igram character models. Their parameters are esti-
mated from the source and target words of the train-
ing data, respectively, and the parameters do not
change during EM training.
For the transliteration model, we implement a
simplified form of the grapheme-to-phoneme con-
verter, g2p (Bisani and Ney, 2008). In the follow-
ing, we use notations from Bisani and Ney (2008).
g2p learns m-to-n character alignments between a
source and a target word. We restrict ourselves to
0?1,1?1,1?0 character alignments and to a unigram
model.1 The Expectation Maximization (EM) algo-
rithm is used to train the model. It maximizes the
likelihood of the training data. In the E-step the EM
algorithm computes expected counts for the multi-
grams and in the M-step the multigram probabilities
are reestimated from these counts. These two steps
are iterated. For the first EM iteration, the multigram
probabilities are initialized with a uniform distribu-
tion and ? is set to 0.5.
The expected count of a multigram q (E-step) is
computed by multiplying the posterior probability
of each alignment a with the frequency of q in a and
summing these weighted frequencies over all align-
ments of all word pairs.
c(q) =
N?
i=1
?
a?Align(ei,fi)
(1? ?)p1(a, ei, fi)
p(ei, fi)
nq(a)
nq(a) is here the number of times the multigram q
occurs in the sequence a and p(ei, fi) is defined in
Equation 4. The new estimate of the probability of a
multigram is given by:
p(q) =
c(q)
?
q? c(q
?)
(5)
Likewise, we calculate the expected count of non-
transliterations by summing the posterior probabili-
ties of non-transliteration given each word pair:
cntr =
N?
i=1
pntr(ei, fi) =
N?
i=1
?p2(ei, fi)
p(ei, fi)
(6)
? is then reestimated by dividing the expected count
of non-transliterations by N .
3.2 Implementation Details
We use the Forward-Backward algorithm to estimate
the counts of multigrams. The algorithm has a for-
ward variable? and a backward variable ? which are
calculated in the standard way (Deligne and Bimbot,
1995). Consider a node r which is connected with
a node s via an arc labelled with the multigram q.
The expected count of a transition between r and s
is calculated using the forward and backward prob-
abilities as follows:
??rs =
?(r) p(q) ?(s)
?(E)
(7)
1In preliminary experiments, using an n-gram order of
greater than one or more than one character on the source side or
the target side or both sides of the multigram caused the translit-
eration model to incorrectly learn non-transliteration informa-
tion from the training data.
471
where E is the final node of the graph.
We multiply the expected count of a transition
by the posterior probability of transliteration (1 ?
pntr(e, f)) which indicates how likely the string pair
is to be a transliteration. The counts ?rs are then
summed for all multigram types q over all training
pairs to obtain the frequencies c(q) which are used
to reestimate the multigram probabilities according
to Equation 5.
4 Semi-supervised Transliteration Mining
Model
Our unsupervised transliteration mining system can
be applied to language pairs for which no labelled
data is available. However, the unsupervised sys-
tem is focused on high recall and also mines close
transliterations (see Section 5 for details). In a task
dependent scenario, it is difficult for the unsuper-
vised system to mine transliteration pairs according
to the details of a particular definition of what is con-
sidered a transliteration (which may vary somewhat
with the task). In this section, we propose an exten-
sion of our unsupervised model which overcomes
this shortcoming by using labelled data. The idea
is to rely on probabilities from labelled data where
they can be estimated reliably and to use probabili-
ties from unlabelled data where the labelled data is
sparse. This is achieved by smoothing the labelled
data probabilities using the unlabelled data probabil-
ities as a backoff.
4.1 Model Estimation
We calculate the unlabelled data probabilities in the
E-step using Equation 4. For labelled data (contain-
ing only transliterations) we set ? = 0 and get:
p(e, f) =
?
a?Align(e,f)
p1(e, f, a) (8)
In every EM iteration, we smooth the probability
distribution in such a way that the estimates of the
multigrams of the unlabelled data that do not occur
in the labelled data would be penalized. We obtain
this effect by smoothing the probability distribution
of unlabelled and labelled data using a technique
similar to Witten-Bell smoothing (Witten and Bell,
1991), as we describe below.
Figure 1: Semi-supervised training
4.2 Implementation Details
We divide the training process of semi-supervised
mining in two steps as shown in Figure 1. The first
step creates a reasonable alignment of the labelled
data from which multigram counts can be obtained.
The labelled data is a small list of transliteration
pairs. Therefore we use the unlabelled data to help
correctly align it and train our unsupervised min-
ing system on the combined labelled and unlabelled
training data. In the expectation step, the prior prob-
ability of non-transliteration ? is set to zero on the
labelled data since it contains only transliterations.
The first step passes the resulting multigram proba-
bility distribution to the second step.
We start the second step with the probability es-
timates from the first step and run the E-step sepa-
rately on labelled and unlabelled data. The E-step
on the labelled data is done using Equation 8, which
forces the posterior probability of non-transliteration
to zero, while the E-step on the unlabelled data uses
Equation 4. After the two E-steps, we estimate
a probability distribution from the counts obtained
from the unlabelled data (M-step) and use it as a
backoff distribution in computing smoothed proba-
bilities from the labelled data counts (S-step).
The smoothed probability estimate p?(q) is:
p?(q) =
cs(q) + ?sp(q)
Ns + ?s
(9)
where cs(q) is the labelled data count of the multi-
gram q, p(q) is the unlabelled data probability es-
timate, and Ns =
?
q cs(q), and ?s is the number
of different multigram types observed in the Viterbi
alignment of the labelled data.
472
5 Evaluation
We evaluate our unsupervised system and semi-
supervised system on two tasks, NEWS10 and paral-
lel corpora. NEWS10 is a standard task on translit-
eration mining from WIL. On NEWS10, we com-
pare our results with the unsupervised mining sys-
tem of Sajjad et al (2011), the best supervised
and semi-supervised systems presented at NEWS10
(Kumaran et al, 2010b) and the best supervised and
semi-supervised results reported in the literature for
the NEWS10 task. For the challenging task of min-
ing from parallel corpora, we use the English/Hindi
and English/Arabic gold standard provided by Saj-
jad et al (2011) to evaluate our results.
5.1 Experiments using the NEWS10 Dataset
We conduct experiments on four language pairs: En-
glish/Arabic, English/Hindi, English/Tamil and En-
glish/Russian using data provided at NEWS10. Ev-
ery dataset contains training data, seed data and ref-
erence data. The NEWS10 data consists of pairs of
titles of the same Wikipedia pages written in dif-
ferent languages, which may be transliterations or
translations. The seed data is a list of 1000 transliter-
ation pairs provided to semi-supervised systems for
initial training. We use the seed data only in our
semi-supervised system, and not in the unsupervised
system. The reference data is a small subset of the
training data which is manually annotated with pos-
itive and negative examples.
5.1.1 Training
We word-aligned the parallel phrases of the train-
ing data using GIZA++ (Och and Ney, 2003), and
symmetrized the alignments using the grow-diag-
final-and heuristic (Koehn et al, 2003). We extract
all word pairs which occur as 1-to-1 alignments (like
Sajjad et al (2011)) and later refer to them as the
word-aligned list. We compared the word-aligned
list with the NEWS10 reference data and found that
the word-aligned list is missing some transliteration
pairs because of word-alignment errors. We built an-
other list by adding a word pair for every source
word that cooccurs with a target word in a paral-
lel phrase/sentence and call it the cross-product list
later on. The cross-product list is noisier but con-
tains almost all transliteration pairs in the corpus.
Word-aligned Cross-product
P R F P R F
EA 27.8 97.1 43.3 14.3 98.0 25.0
EH 42.5 98.7 59.4 20.5 99.6 34.1
ET 32.0 98.1 48.3 17.2 99.6 29.3
ER 25.5 95.6 40.3 12.8 99.0 22.7
Table 1: Statistics of word-aligned and cross-product
list calculated from the NEWS10 dataset, before min-
ing. EA is English/Arabic, EH is English/Hindi, ET is
English/Tamil and ER is English/Russian
Table 1 shows the statistics of the word-aligned
list and the cross-product list calculated using the
NEWS10 reference data.2 The word-aligned list cal-
culated from the NEWS10 dataset is used to com-
pare our unsupervised system with the unsupervised
system of Sajjad et al (2011) on the same training
data. All the other experiments on NEWS10 use
cross-product lists. We remove numbers from both
lists as they are defined as non-transliterations (Ku-
maran et al, 2010b).
5.1.2 Unsupervised Transliteration Mining
We run our unsupervised transliteration mining
system on the word-aligned list and the cross-
product list. The word pairs with a posterior prob-
ability of transliteration 1 ? pntr(e, f) = 1 ?
?p2(ei, fi)/p(ei, fi) greater than 0.5 are selected as
transliteration pairs.
We compare our unsupervised system with the
unsupervised system of Sajjad11. Our unsupervised
system trained on the word-aligned list shows F-
measures of 91.7%, 95.5%, 92.9% and 77.7% which
is 4.3%, 3.3%, 2.8% and 1.7% better than the sys-
tem of Sajjad11 on English/Arabic, English/Hindi,
English/Tamil and English/Russian respectively.
Sajjad11 is computationally expensive. For in-
stance, a phrase-based statistical MT system is
built once in every iteration of the heuristic proce-
dure. We ran Sajjad11 on the English/Russian word-
aligned list using a 2.4 GHz Dual-Core AMD ma-
chine, which took almost 10 days. On the same ma-
chine, our transliteration mining system only takes
1.5 hours to finish the same experiment.
2Due to inconsistent word definition used in the reference
data, we did not achieve 100% recall in our cross-product list.
For example, the underscore is defined as a word boundary for
English WIL phrases. This assumption is not followed for cer-
tain phrases like ?New York? and ?New Mexico?.
473
Unsupervised Semi-supervised/Supervised
SJD OU OS SBest GR DBN
EA 87.4 92.4 92.7 91.5 94.1 -
EH 92.2 95.7 96.3 94.4 93.2 95.5
ET 90.1 93.2 94.6 91.4 95.5 93.9
ER 76.0 79.4 83.1 87.5 92.3 82.5
Table 2: F-measure results on NEWS10 datasets where
SJD is the unsupervised system of Sajjad11, OU is
our unsupervised system built on the cross-product list,
OS is our semi-supervised system, SBest is the best
NEWS10 system, GR is the supervised system of Kahki
et al (2011) and DBN is the semi-supervised system of
Nabende (2011)
Our unsupervised mining system built on the
cross-product list consistently outperforms the one
built on the word-aligned list. Later, we consider
only the system built on the cross-product list. Ta-
ble 2 shows the results of our unsupervised sys-
tem OU in comparison with the unsupervised sys-
tem of Sajjad11 (SJD), the best semi-supervised sys-
tems presented at NEWS10 (SBEST ) and the best
semi-supervised results reported on the NEWS10
dataset (GR, DBN ). On three language pairs, our
unsupervised system performs better than all semi-
supervised systems which participated in NEWS10.
It has competitive results with the best supervised
results reported on NEWS10 datasets. On En-
glish/Hindi, our unsupervised system outperforms
the state-of-the-art supervised and semi-supervised
systems. Kahki et al (2011) (GR) achieved
the best results on English/Arabic, English/Tamil
and English/Russian. For the English/Arabic task,
they normalized the data using language dependent
heuristics3 and also used a non-standard evaluation
method (discussed in Section 5.1.4).
On the English/Russian dataset, our unsupervised
system faces the problem that it extracts cognates
as transliterations. The same problem was reported
in Sajjad et al (2011). Cognates are close translit-
erations which differ by only one or two characters
from an exact transliteration pair. The unsupervised
system learns to delete the additional one or two
characters with a high probability and incorrectly
mines such word pairs as transliterations.
3They applied an Arabic word segmenter which uses lan-
guage dependent information. Arabic long vowels which have
identical sound but are written differently were merged to one
form. English characters were normalized by dropping accents.
Unsupervised Semi-supervised
P R F P R F
EA 89.2 95.7 92.4 92.9 92.4 92.7
EH 92.6 99.0 95.7 95.5 97.0 96.3
ET 88.3 98.6 93.2 93.4 95.8 94.6
ER 67.2 97.1 79.4 74.0 94.9 83.1
Table 3: Precision(P), Recall(R) and F-measure(F) of our
unsupervised and semi-supervised transliteration mining
systems on NEWS10 datasets
5.1.3 Semi-supervised Transliteration Mining
Our semi-supervised system uses similar initial-
ization of the parameters as used for unsupervised
system. Table 2 shows on three language pairs, our
semi-supervised system OS only achieves a small
gain in F-measure over our unsupervised system
OU . This shows that the unlabelled training data is
already providing most of the transliteration infor-
mation. The seed data is used to help the translit-
eration mining system to learn the right definition
of transliteration. On the English/Russian dataset,
our semi-supervised system achieves almost 7% in-
crease in precision with a 2.2% drop in recall com-
pared to our unsupervised system. This provides a
3.7% gain on F-measure. The increase in precision
shows that the seed data is helping the system in dis-
ambiguating transliteration pairs from cognates.
5.1.4 Discussion
The unsupervised system produces lists with high
recall. The semi-supervised system tends to better
balance out precision and recall. Table 3 compares
the precision, recall and F-measure of our unsuper-
vised and semi-supervised mining systems.
The errors made by our semi-supervised system
can be classified into the following categories:
Pronunciation differences: English proper
names may be pronounced differently in other lan-
guages. Sometimes, English short vowels are con-
verted to long vowels in Hindi such as the English
word ?Lanthanum? which is pronounced ?Laan-
thanum? in Hindi. Our transliteration mining system
wrongly extracts such pairs as transliterations.
In some cases, different vowels are used in two
languages. The English word ?January? is pro-
nounced as ?Janvary? in Hindi. Such word pairs are
non-transliterations according to the gold standard
but our system extracts them as transliterations. Ta-
474
Table 4: Word pairs with pronunciation differences
Table 5: Examples of word pairs which are wrongly an-
notated as transliterations in the gold standard
ble 4 shows a few examples of such word pairs.
Inconsistencies in the gold standard: There are
several inconsistencies in the gold standard where
our transliteration system correctly identifies a word
pair as a transliteration but it is marked as a non-
transliteration or vice versa. Consider the example
of the English word ?George? which is pronounced
as ?Jaarj? in Hindi. Our semi-supervised system
learns this as a non-transliteration but it is wrongly
annotated as a transliteration in the gold standard.
Arabic nouns have an article ?al? attached to them
which is translated in English as ?the?. There are
various cases in the training data where an English
noun such as ?Quran? is matched with an Arabic
noun ?alQuran?. Our mining system classifies such
cases as non-transliterations, but 24 of them are in-
correctly annotated as transliterations in the gold
standard. We did not correct this, and are there-
fore penalized. Kahki et al (2011) preprocessed
such Arabic words and separated ?al? from the noun
?Quran? before mining. They report a match if the
version of the Arabic word with ?al? appears with
the corresponding English word in the gold stan-
dard. Table 5 shows examples of word pairs which
are wrongly annotated as transliterations.
Cognates: Sometimes a word pair differs by only
one or two ending characters from a true translit-
eration. For example in the English/Russian train-
ing data, the Russian nouns are marked with cases
whereas their English counterparts do not mark the
case or translate it as a separate word. Often the
Russian word differs only by the last character from
a correct transliteration of the English word. Due
to the large amount of such word pairs in the En-
glish/Russian data, our mining system learns to
delete the final case marking characters from the
Russian words. It assigns a high transliteration prob-
Table 6: A few examples of English/Russian cognates
ability to these word pairs and extracts them as
transliterations. Table 6 shows some examples.
There are two English/Russian supervised sys-
tems which are better than our semi-supervised sys-
tem. The Kahki et al (2011) system is built on seed
data only. Jiampojamarn et al (2010)?s best sys-
tem on English/Russian is based on the edit distance
method. Both of these systems are focused on high
precision. Our semi-supervised system is focused
on high recall at the cost of lower precision.4
5.2 Transliteration Mining using Parallel
Corpora
The percentage of transliteration pairs in the
NEWS10 datasets is high. We further check the ef-
fectiveness of our unsupervised and semi-supervised
mining systems by evaluating them on parallel cor-
pora with as few as 2% transliteration pairs.
We conduct experiments using two language
pairs, English/Hindi and English/Arabic. The En-
glish/Hindi corpus is from the shared task on word
alignment organized as part of the ACL 2005 Work-
shop on Building and Using Parallel Texts (WA05)
(Martin et al, 2005). For English/Arabic, we use
200,000 parallel sentences from the United Nations
(UN) corpus (Eisele and Chen, 2010). The En-
glish/Hindi and English/Arabic transliteration gold
standards were provided by Sajjad et al (2011).
5.2.1 Experiments
We follow the procedure for creating the training
data described in Section 5.1.1 and build a word-
aligned list and a cross-product list from the parallel
corpus. We first train and test our unsupervised min-
ing system on the word-aligned list and compare our
results with Sajjad et al Table 7 shows the results.
Our unsupervised system achieves 0.6% and 1.8%
higher F-measure than Sajjad et al respectively.
The cross-product list is huge in comparison to
the word-aligned list. It is noisier than the word-
4We implemented a bigram version of our system to learn
the contextual information at the end of the word pairs, but only
achieved a gain of less than 1% F-measure over our unigram
semi-supervised system. Details are omitted due to space.
475
TP FN TN FP P R F
EHSJD 170 10 2039 45 79.1 94.4 86.1
EHO 176 4 2034 50 77.9 97.8 86.7
EASJD 197 91 6580 59 77.0 68.4 72.5
EAO 288 0 6440 199 59.1 100 74.3
Table 7: Transliteration mining results of our unsuper-
vised system and Sajjad11 system trained and tested
on the word-aligned list of English/Hindi and En-
glish/Arabic parallel corpus
TP FN TN FP P R F
EHU 393 19 12279 129 75.3 95.4 84.2
EHS 365 47 12340 68 84.3 88.6 86.4
EAU 277 11 6444 195 58.7 96.2 72.9
EAS 272 16 6497 142 65.7 94.4 77.5
Table 8: Transliteration mining results of our unsuper-
vised and semi-supervised systems trained on the word-
aligned list and tested on the cross-product list of En-
glish/Hindi and English/Arabic parallel corpus
aligned list but has almost 100% recall of transliter-
ation pairs. The English-Hindi cross-product list has
almost 55% more transliteration pairs (412 types)
than the word-aligned list (180 types). We can not
report these numbers on the English/Arabic cross-
product list since the English/Arabic gold standard
is built on the word-aligned list.
In order to keep the experiment computationally
inexpensive, we train our mining systems on the
word-aligned list and test them on the cross-product
list.5 We also perform the first semi-supervised eval-
uation on this task. For our semi-supervised sys-
tem, we additionally use the English/Hindi and En-
glish/Arabic seed data provided by NEWS10.
Table 8 shows the results of our unsupervised
and semi-supervised systems on the English/Hindi
and English/Arabic parallel corpora. Our unsu-
pervised system achieves higher recall than our
semi-supervised system but lower precision. The
semi-supervised system shows an improvement in
F-measure for both language pairs. We looked
into the errors made by our systems. The mined
transliteration pairs of our unsupervised system con-
tains 65 and 111 close transliterations for the En-
glish/Hindi and English/Arabic task respectively.
5There are some multigrams of the cross-product list which
are unknown to the model learned on the word-aligned list. We
define their probability as the inverse of the number of multi-
gram tokens in the Viterbi alignment of the labelled and unla-
belled data together.
The close transliterations only differ by one or two
characters from correct transliterations. We think
these pairs provide transliteration information to
the systems and help them to avoid problems with
data sparseness. Our semi-supervised system uses
the seed data to identify close transliterations as
non-transliterations and decreases the number of
false positives. They are reduced to 35 and 89
for English/Hindi and English/Arabic respectively.
The seed data and the training data used in the
semi-supervised system are from different domains
(Wikipedia and UN). Seed data extracted from the
same domain is likely to work better, resulting in
even higher scores than we have reported.
6 Conclusion and Future Work
We presented a novel model to automatically
mine transliteration pairs. Our approach is ef-
ficient and language pair independent (for alpha-
betic languages). Both the unsupervised and semi-
supervised systems achieve higher accuracy than the
only unsupervised transliteration mining system we
are aware of and are competitive with the state-
of-the-art supervised and semi-supervised systems.
Our semi-supervised system outperformed our un-
supervised system, in particular in the presence of
prevalent cognates in the Russian/English data.
In future work, we plan to adapt our approach
to language pairs where one language is alphabetic
and the other language is non-alphabetic such as En-
glish/Japanese. These language pairs require one-
to-many character mappings to learn transliteration
units, while our current system only learns unigram
character alignments.
Acknowledgments
The authors wish to thank the anonymous review-
ers. We would like to thank Syed Aoun Raza for
discussions of implementation efficiency. Hassan
Sajjad was funded by the Higher Education Com-
mission of Pakistan. Alexander Fraser was funded
by Deutsche Forschungsgemeinschaft grant Models
of Morphosyntax for Statistical Machine Transla-
tion. Helmut Schmid was supported by Deutsche
Forschungsgemeinschaft grant SFB 732. This work
was supported in part by the IST Programme of
the European Community, under the PASCAL2 Net-
work of Excellence, IST-2007-216886. This publi-
cation only reflects the authors? views.
476
References
Maximilian Bisani and Hermann Ney. 2008. Joint-
sequence models for grapheme-to-phoneme conver-
sion. Speech Communication, 50(5).
Kareem Darwish. 2010. Transliteration mining with
phonetic conflation and iterative training. In Proceed-
ings of the 2010 Named Entities Workshop, Uppsala,
Sweden.
Sabine Deligne and Fre?de?ric Bimbot. 1995. Language
modeling by variable length sequences : Theoreti-
cal formulation and evaluation of multigrams. In
Proceedings of the IEEE International Conference on
Acoustics, Speech, and Signal Processing, volume 1,
Los Alamitos, CA, USA.
Andreas Eisele and Yu Chen. 2010. MultiUN: A multi-
lingual corpus from United Nation documents. In Pro-
ceedings of the Seventh conference on International
Language Resources and Evaluation (LREC?10), Val-
letta, Malta.
Sittichai Jiampojamarn, Kenneth Dwyer, Shane Bergsma,
Aditya Bhargava, Qing Dou, Mi-Young Kim, and
Grzegorz Kondrak. 2010. Transliteration generation
and mining with limited training resources. In Pro-
ceedings of the 2010 Named Entities Workshop, Upp-
sala, Sweden.
Ali El Kahki, Kareem Darwish, Ahmed Saad El Din,
Mohamed Abd El-Wahab, Ahmed Hefny, and Waleed
Ammar. 2011. Improved transliteration mining using
graph reinforcement. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP), Edinburgh, UK.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
the Human Language Technology and North Ameri-
can Association for Computational Linguistics Con-
ference, Edmonton, Canada.
A Kumaran, Mitesh M. Khapra, and Haizhou Li. 2010a.
Report of NEWS 2010 transliteration mining shared
task. In Proceedings of the 2010 Named Entities Work-
shop, Uppsala, Sweden.
A Kumaran, Mitesh M. Khapra, and Haizhou Li. 2010b.
Whitepaper of NEWS 2010 shared task on translitera-
tion mining. In Proceedings of the 2010 Named Enti-
ties Workshop, Uppsala, Sweden.
Haizhou Li, Zhang Min, and Su Jian. 2004. A joint
source-channel model for machine transliteration. In
ACL ?04: Proceedings of the 42nd Annual Meeting on
Association for Computational Linguistics, Barcelona,
Spain.
Joel Martin, Rada Mihalcea, and Ted Pedersen. 2005.
Word alignment for languages with scarce resources.
In ParaText ?05: Proceedings of the ACL Workshop
on Building and Using Parallel Texts, Morristown, NJ,
USA.
Peter Nabende. 2010. Mining transliterations from
wikipedia using pair hmms. In Proceedings of the
2010 Named Entities Workshop, Uppsala, Sweden.
Peter Nabende. 2011. Mining transliterations from
Wikipedia using dynamic bayesian networks. In Pro-
ceedings of the International Conference Recent Ad-
vances in Natural Language Processing 2011, Hissar,
Bulgaria.
Sara Noeman and Amgad Madkour. 2010. Language
independent transliteration mining system using finite
state automata framework. In Proceedings of the 2010
Named Entities Workshop, Uppsala, Sweden.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1).
Hassan Sajjad, Alexander Fraser, and Helmut Schmid.
2011. An algorithm for unsupervised transliteration
mining with an application to word alignment. In Pro-
ceedings of the 49th Annual Conference of the Associ-
ation for Computational Linguistics, Portland, USA.
Ian H. Witten and Timothy C. Bell. 1991. The zero-
frequency problem: Estimating the probabilities of
novel events in adaptive text compression. In IEEE
Transactions on Information Theory, volume 37.
477
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 399?405,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Can Markov Models Over Minimal Translation Units Help Phrase-Based
SMT?
Nadir Durrani
University of Edinburgh
dnadir@inf.ed.ac.uk
Hieu Hoang Philipp Koehn
University of Edinburgh
hieu.hoang,pkoehn@inf.ed.ac.uk
Alexander Fraser Helmut Schmid
Ludwig Maximilian University Munich
fraser,schmid@cis.uni-muenchen.de
Abstract
The phrase-based and N-gram-based
SMT frameworks complement each other.
While the former is better able to memo-
rize, the latter provides a more principled
model that captures dependencies across
phrasal boundaries. Some work has been
done to combine insights from these two
frameworks. A recent successful attempt
showed the advantage of using phrase-
based search on top of an N-gram-based
model. We probe this question in the
reverse direction by investigating whether
integrating N-gram-based translation and
reordering models into a phrase-based
decoder helps overcome the problematic
phrasal independence assumption. A large
scale evaluation over 8 language pairs
shows that performance does significantly
improve.
1 Introduction
Phrase-based models (Koehn et al, 2003; Och
and Ney, 2004) learn local dependencies such as
reorderings, idiomatic collocations, deletions and
insertions by memorization. A fundamental draw-
back is that phrases are translated and reordered
independently of each other and contextual infor-
mation outside of phrasal boundaries is ignored.
The monolingual language model somewhat re-
duces this problem. However i) often the language
model cannot overcome the dispreference of the
translation model for nonlocal dependencies, ii)
source-side contextual dependencies are still ig-
nored and iii) generation of lexical translations and
reordering is separated.
The N-gram-based SMT framework addresses
these problems by learning Markov chains over se-
quences of minimal translation units (MTUs) also
known as tuples (Marin?o et al, 2006) or over op-
erations coupling lexical generation and reorder-
ing (Durrani et al, 2011). Because the mod-
els condition the MTU probabilities on the previ-
ous MTUs, they capture non-local dependencies
and both source and target contextual information
across phrasal boundaries.
In this paper we study the effect of integrating
tuple-based N-gram models (TSM) and operation-
based N-gram models (OSM) into the phrase-
based model in Moses, a state-of-the-art phrase-
based system. Rather than using POS-based
rewrite rules (Crego and Marin?o, 2006) to form
a search graph, we use the ability of the phrase-
based system to memorize larger translation units
to replicate the effect of source linearization as
done in the TSM model.
We also show that using phrase-based search
with MTU N-gram translation models helps to ad-
dress some of the search problems that are non-
trivial to handle when decoding with minimal
translation units. An important limitation of the
OSM N-gram model is that it does not handle un-
aligned or discontinuous target MTUs and requires
post-processing of the alignment to remove these.
Using phrases during search enabled us to make
novel changes to the OSM generative story (also
applicable to the TSM model) to handle unaligned
target words and to use target linearization to deal
with discontinuous target MTUs.
We performed an extensive evaluation, carrying
out translation experiments from French, Spanish,
Czech and Russian to English and in the opposite
direction. Our integration of the OSM model into
Moses and our modification of the OSM model to
deal with unaligned and discontinuous target to-
kens consistently improves BLEU scores over the
399
baseline system, and shows statistically significant
improvements in seven out of eight cases.
2 Previous Work
Several researchers have tried to combine the ideas
of phrase-based and N-gram-based SMT. Costa-
jussa` et al (2007) proposed a method for combin-
ing the two approaches by applying sentence level
reranking. Feng et al (2010) added a linearized
source-side language model in a phrase-based sys-
tem. Crego and Yvon (2010) modified the phrase-
based lexical reordering model of Tillman (2004)
for an N-gram-based system. Niehues et al (2011)
integrated a bilingual language model based on
surface word forms and POS tags into a phrase-
based system. Zhang et al (2013) explored multi-
ple decomposition structures for generating MTUs
in the task of lexical selection, and to rerank the
N-best candidate translations in the output of a
phrase-based. A drawback of the TSM model is
the assumption that source and target information
is generated monotonically. The process of re-
ordering is disconnected from lexical generation
which restricts the search to a small set of precom-
puted reorderings. Durrani et al (2011) addressed
this problem by coupling lexical generation and
reordering information into a single generative
process and enriching the N-gram models to learn
lexical reordering triggers. Durrani et al (2013)
showed that using larger phrasal units during de-
coding is superior to MTU-based decoding in an
N-gram-based system. However, they do not use
phrase-based models in their work, relying only
on the OSM model. This paper combines insights
from these recent pieces of work and show that
phrase-based search combined with N-gram-based
and phrase-based models in decoding is the over-
all best way to go. We integrate the two N-gram-
based models, TSM and OSM, into phrase-based
Moses and show that the translation quality is im-
proved by taking both translation and reordering
context into account. Other approaches that ex-
plored such models in syntax-based systems used
MTUs for sentence level reranking (Khalilov and
Fonollosa, 2009), in dependency translation mod-
els (Quirk and Menezes, 2006) and in target lan-
guage syntax systems (Vaswani et al, 2011).
3 Integration of N-gram Models
We now describe our integration of TSM and
OSM N-gram models into the phrase-based sys-
Figure 1: Example (a) Word Alignments (b) Un-
folded MTU Sequence (c) Operation Sequence (d)
Step-wise Generation
tem. Given a bilingual sentence pair (F,E) and
its alignment (A), we first identify minimal trans-
lation units (MTUs) from it. An MTU is defined
as a translation rule that cannot be broken down
any further. The MTUs extracted from Figure 1(a)
are A ? a,B ? b, C . . .H ? c1 and D ? d.
These units are then generated left-to-right in two
different ways, as we will describe next.
3.1 Tuple Sequence Model (TSM)
The TSM translation model assumes that MTUs
are generated monotonically. To achieve this ef-
fect, we enumerate the MTUs in the target left-
to-right order. This process is also called source
linearization or tuple unfolding. The resulting se-
quence of monotonic MTUs is shown in Figure
1(b). We then define a TSM model over this se-
quence (t1, t2, . . . , tJ ) as:
ptsm(F,E,A) =
J?
j=1
p(tj |tj?n+1, ..., tj?1)
where n indicates the amount of context used. A
4-gram Kneser-Ney smoothed language model is
trained with SRILM (Stolcke, 2002).
Search: In previous work, the search graph in
TSM N-gram SMT was not built dynamically
like in the phrase-based system, but instead con-
structed as a preprocessing step using POS-based
rewrite rules (learned when linearizing the source
side). We do not adopt this framework. We use
1We use . . . to denote discontinuous MTUs.
400
phrase-based search which builds up the decoding
graph dynamically and searches through all pos-
sible reorderings within a fixed window. During
decoding we use the phrase-internal alignments to
perform source linearization. For example, if dur-
ing decoding we would like to apply the phrase
pair ?C D H ? d c?, a combination of t3 and t4 in
Figure 1(b), then we extract the MTUs from this
phrase-pair and linearize the source to be in the
order of the target. We then compute the TSM
probability given the n ? 1 previous MTUs (in-
cluding MTUs occurring in the previous source
phrases). The idea is to replicate rewrite rules
with phrase-pairs to linearize the source. Previ-
ous work on N-gram-based models restricted the
length of the rewrite rules to be 7 or less POS tags.
We use phrases of length 6 and less.
3.2 Operation Sequence Model (OSM)
The OSM model represents a bilingual sentence
pair and its alignment through a sequence of oper-
ations that generate the aligned sentence pair. An
operation either generates source and target words
or it performs reordering by inserting gaps and
jumping forward and backward. The MTUs are
generated in the target left-to-right order just as in
the TSM model. However rather than linearizing
the source-side, reordering operations (gaps and
jumps) are used to handle crossing alignments.
During training, each bilingual sentence pair is de-
terministically converted to a unique sequence of
operations.2 The example in Figure 1(a) is con-
verted to the sequence of operations shown in Fig-
ure 1(c). A step-wise generation of MTUs along
with reordering operations is shown in Figure 1(d).
We learn a Markov model over a sequence of oper-
ations (o1, o2, . . . , oJ ) that encapsulate MTUs and
reordering information which is defined as fol-
lows:
posm(F,E,A) =
J?
j=1
p(oj |oj?n+1, ..., oj?1)
A 9-gram Kneser-Ney smoothed language model
is trained with SRILM.3 By coupling reorder-
ing with lexical generation, each (translation or
reordering) decision conditions on n ? 1 previ-
ous (translation and reordering) decisions span-
ning across phrasal boundaries. The reordering
decisions therefore influence lexical selection and
2Please refer to Durrani et al (2011) for a list of opera-
tions and the conversion algorithm.
3We also tried a 5-gram model, the performance de-
creased slightly in some cases.
vice versa. A heterogeneous mixture of translation
and reordering operations enables the OSM model
to memorize reordering patterns and lexicalized
triggers unlike the TSM model where translation
and reordering are modeled separately.
Search: We integrated the generative story of
the OSM model into the hypothesis extension pro-
cess of the phrase-based decoder. Each hypothesis
maintains the position of the source word covered
by the last generated MTU, the right-most source
word generated so far, the number of open gaps
and their relative indexes, etc. This information
is required to generate the operation sequence for
the MTUs in the hypothesized phrase-pair. After
the operation sequence is generated, we compute
its probability given the previous operations. We
define the main OSM feature, and borrow 4 sup-
portive features, the Gap, Open Gap, Gap-width
and Deletion penalties (Durrani et al, 2011).
3.3 Problem: Target Discontinuity and
Unaligned Words
Two issues that we have ignored so far are the han-
dling of MTUs which have discontinuous targets,
and the handling of unaligned target words. Both
TSM and OSM N-gram models generate MTUs
linearly in left-to-right order. This assumption be-
comes problematic in the cases of MTUs that have
target-side discontinuities (See Figure 2(a)). The
MTU A? g . . . a can not be generated because of
the intervening MTUs B ? b, C . . .H ? c and
D ? d. In the original TSM model, such cases are
dealt with by merging all the intervening MTUs
to form a bigger unit t?1 in Figure 2(c). A solu-
tion that uses split-rules is proposed by Crego and
Yvon (2009) but has not been adopted in Ncode
(Crego et al, 2011), the state-of-the-art TSM N-
gram system. Durrani et al (2011) dealt with
this problem by applying a post-processing (PP)
heuristic that modifies the alignments to remove
such cases. When a source word is aligned to a
discontinuous target-cept, first the link to the least
frequent target word is identified, and the group
of links containing this word is retained while the
others are deleted. The alignment in Figure 2(a),
for example, is transformed to that in Figure 2(b).
This allows OSM to extract the intervening MTUs
t2 . . . t5 (Figure 2(c)). Note that this problem does
not exist when dealing with source-side disconti-
nuities: the TSM model linearizes discontinuous
source-side MTUs such as C . . .H ? c. The
401
Figure 2: Example (a) Original Alignments (b)
Post-Processed Alignments (c) Extracted MTUs ?
t?1 . . . t?3 (from (a)) and t1 . . . t7 (from (b))
OSM model deals with such cases through Insert
Gap and Continue Cept operations.
The second problem is the unaligned target-side
MTUs such as ? ? f in Figure 2(a). Inserting
target-side words ?spuriously? during decoding is
a non-trival problem because there is no evidence
of when to hypothesize such words. These cases
are dealt with in N-gram-based SMT by merging
such MTUs to the MTU on the left or right based
on attachment counts (Durrani et al, 2011), lexical
probabilities obtained from IBM Model 1 (Marin?o
et al, 2006), or POS entropy (Gispert and Marin?o,
2006). Notice how ?? f (Figure 2(a)) is merged
with the neighboring MTU E ? e to form a new
MTU E ? ef (Figure 2 (c)). We initially used the
post-editing heuristic (PP) as defined by Durrani et
al. (2011) for both TSM and OSM N-gram mod-
els, but found that it lowers the translation quality
(See Row 2 in Table 2) in some language pairs.
3.4 Solution: Insertion and Linearization
To deal with these problems, we made novel modi-
fications to the generative story of the OSM model.
Rather than merging the unaligned target MTU
such as ? ? f , to its right or left MTU, we gen-
erate it through a new Generate Target Only (f)
operation. Orthogonal to its counterpart Generate
Source Only (I) operation (as used for MTU t7 in
Figure 2 (c)), this operation is generated as soon
as the MTU containing its previous target word
is generated. In Figure 2(a), ? ? f is generated
immediately after MTU E ? e is generated. In
a sequence of unaligned source and target MTUs,
unaligned source MTUs are generated before the
unaligned target MTUs. We do not modify the de-
coder to arbitrarily generate unaligned MTUs but
hypothesize these only when they appear within
an extracted phrase-pair. The constraint provided
by the phrase-based search makes the Generate
Target Only operation tractable. Using phrase-
based search therefore helps addressing some of
the problems that exist in the decoding framework
of N-gram SMT.
The remaining problem is the discontinuous tar-
get MTUs such as A? g . . . a in Figure 2(a). We
handle this with target linearization similar to the
TSM source linearization. We collapse the target
words g and a in the MTU A ? g . . . a to occur
consecutively when generating the operation se-
quence. The conversion algorithm that generates
the operations thinks that g and a occurred adja-
cently. During decoding we use the phrasal align-
ments to linearize such MTUs within a phrasal
unit. This linearization is done only to compute
the OSM feature. Other features in the phrase-
based system (e.g., language model) work with the
target string in its original order. Notice again how
memorizing larger translation units using phrases
helps us reproduce such patterns. This is achieved
in the tuple N-gram model by using POS-based
split and rewrite rules.
4 Evaluation
Corpus: We ran experiments with data made
available for the translation task of the Eighth
Workshop on Statistical Machine Translation. The
sizes of bitext used for the estimation of translation
and monolingual language models are reported in
Table 1. All data is true-cased.
Pair Parallel Monolingual Lang
fr?en ?39 M ?91 M fr
cs?en ?15.6 M ?43.4 M cs
es?en ?15.2 M ?65.7 M es
ru?en ?2 M ?21.7 M ru
?287.3 M en
Table 1: Number of Sentences (in Millions) used
for Training
We follow the approach of Schwenk and Koehn
(2008) and trained domain-specific language mod-
els separately and then linearly interpolated them
using SRILM with weights optimized on the held-
out dev-set. We concatenated the news-test sets
from four years (2008-2011) to obtain a large dev-
setin order to obtain more stable weights (Koehn
and Haddow, 2012). For Russian-English and
English-Russian language pairs, we divided the
tuning-set news-test 2012 into two halves and used
402
No. System fr-en es-en cs-en ru-en en-fr en-es en-cs en-ru
1. Baseline 31.89 35.07 23.88 33.45 29.89 35.03 16.22 23.88
2. 1+pp 31.87 35.09 23.64 33.04 29.70 35.00 16.17 24.05
3. 1+pp+tsm 31.94 35.25 23.85 32.97 29.98 35.06 16.30 23.96
4. 1+pp+osm 32.17 35.50 24.14 33.21 30.35 35.34 16.49 24.22
5. 1+osm* 32.13 35.65 24.23 33.91 30.54 35.49 16.62 24.25
Table 2: Translating into and from English. Bold: Statistically Significant (Koehn, 2004) w.r.t Baseline
the first half for tuning and second for test. We test
our systems on news-test 2012. We tune with the
k-best batch MIRA algorithm (Cherry and Foster,
2012).
Moses Baseline: We trained a Moses system
(Koehn et al, 2007) with the following settings:
maximum sentence length 80, grow-diag-final-
and symmetrization of GIZA++ alignments, an
interpolated Kneser-Ney smoothed 5-gram lan-
guage model with KenLM (Heafield, 2011) used at
runtime, msd-bidirectional-fe lexicalized reorder-
ing, sparse lexical and domain features (Hasler
et al, 2012), distortion limit of 6, 100-best
translation options, minimum bayes-risk decoding
(Kumar and Byrne, 2004), cube-pruning (Huang
and Chiang, 2007) and the no-reordering-over-
punctuation heuristic.
Results: Table 2 shows uncased BLEU scores
(Papineni et al, 2002) on the test set. Row 2 (+pp)
shows that the post-editing of alignments to re-
move unaligned and discontinuous target MTUs
decreases the performance in the case of ru-en, cs-
en and en-fr. Row 3 (+pp+tsm) shows that our in-
tegration of the TSM model slightly improves the
BLEU scores for en-fr, and es-en. Results drop
in ru-en and en-ru. Row 4 (+pp+osm) shows that
the OSM model consistently improves the BLEU
scores over the Baseline systems (Row 1) giving
significant improvements in half the cases. The
only result that is lower than the baseline system
is that of the ru-en experiment, because OSM is
built with PP alignments which particularly hurt
the performance for ru-en. Finally Row 5 (+osm*)
shows that our modifications to the OSM model
(Section 3.4) give the best result ranging from
[0.24?0.65] with statistically significant improve-
ments in seven out of eight cases. It also shows im-
provements over Row 4 (+pp+osm) even in some
cases where the PP heuristic doesn?t hurt. The
largest gains are obtained in the ru-en translation
task (where the PP heuristic inflicted maximum
damage).
5 Conclusion and Future Work
We have addressed the problem of the indepen-
dence assumption in PBSMT by integrating N-
gram-based models inside a phrase-based system
using a log-linear framework. We try to replicate
the effect of rewrite and split rules as used in the
TSM model through phrasal alignments. We pre-
sented a novel extension of the OSM model to
handle unaligned and discontinuous target MTUs
in the OSM model. Phrase-based search helps us
to address these problems that are non-trivial to
handle in the decoding frameworks of the N-gram-
based models. We tested our extentions and modi-
fications by evaluating against a competitive base-
line system over 8 language pairs. Our integra-
tion of TSM shows small improvements in a few
cases. The OSM model which takes both reorder-
ing and lexical context into consideration consis-
tently improves the performance of the baseline
system. Our modification to the OSM model pro-
duces the best results giving significant improve-
ments in most cases. Although our modifications
to the OSM model enables discontinuous MTUs,
we did not fully utilize these during decoding, as
Moses only uses continous phrases. The discon-
tinuous MTUs that span beyond a phrasal length
of 6 words are therefore never hypothesized. We
would like to explore this further by extending the
search to use discontinuous phrases (Galley and
Manning, 2010).
Acknowledgments
We would like to thank the anonymous reviewers
for their helpful feedback and suggestions. The re-
search leading to these results has received fund-
ing from the European Union Seventh Framework
Programme (FP7/2007-2013) under grant agree-
ment n ? 287658. Alexander Fraser was funded by
Deutsche Forschungsgemeinschaft grant Models
of Morphosyntax for Statistical Machine Transla-
tion. Helmut Schmid was supported by Deutsche
Forschungsgemeinschaft grant SFB 732. This
publication only reflects the authors views.
403
References
Colin Cherry and George Foster. 2012. Batch Tun-
ing Strategies for Statistical Machine Translation. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 427?436, Montre?al, Canada, June. Associa-
tion for Computational Linguistics.
Marta R. Costa-jussa`, Josep M. Crego, David Vilar,
Jose? A.R. Fonollosa, Jose? B. Marin?o, and Her-
mann Ney. 2007. Analysis and System Combina-
tion of Phrase- and N-Gram-Based Statistical Ma-
chine Translation Systems. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Compu-
tational Linguistics; Companion Volume, Short Pa-
pers, pages 137?140, Rochester, New York, April.
Josep M. Crego and Jose? B. Marin?o. 2006. Improving
Statistical MT by Coupling Reordering and Decod-
ing. Machine Translation, 20(3):199?215.
Josep M. Crego and Franc?ois Yvon. 2009. Gappy
Translation Units under Left-to-Right SMT Decod-
ing. In Proceedings of the Meeting of the European
Association for Machine Translation (EAMT), pages
66?73, Barcelona, Spain.
Josep M. Crego and Franc?ois Yvon. 2010. Improv-
ing Reordering with Linguistically Informed Bilin-
gual N-Grams. In Coling 2010: Posters, pages 197?
205, Beijing, China, August. Coling 2010 Organiz-
ing Committee.
Josep M. Crego, Franc?ois Yvon, and Jose? B. Marin?o.
2011. Ncode: an Open Source Bilingual N-gram
SMT Toolkit. The Prague Bulletin of Mathematical
Linguistics, 96:49?58.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A Joint Sequence Translation Model with In-
tegrated Reordering. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
1045?1054, Portland, Oregon, USA, June.
Nadir Durrani, Alexander Fraser, and Helmut Schmid.
2013. Model With Minimal Translation Units, But
Decode With Phrases. In The 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Atlanta, Georgia, USA, June. Association
for Computational Linguistics.
Minwei Feng, Arne Mauser, and Hermann Ney. 2010.
A Source-side Decoding Sequence Model for Statis-
tical Machine Translation. In Conference of the As-
sociation for Machine Translation in the Americas
2010, Denver, Colorado, USA, October.
Michel Galley and Christopher D. Manning. 2010.
Accurate Non-Hierarchical Phrase-Based Transla-
tion. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 966?974, Los Angeles, California, June. As-
sociation for Computational Linguistics.
Adria` Gispert and Jose? B. Marin?o. 2006. Linguis-
tic Tuple Segmentation in N-Gram-Based Statistical
Machine Translation. In INTERSPEECH.
Eva Hasler, Barry Haddow, and Philipp Koehn. 2012.
Sparse Lexicalised Features and Topic Adaptation
for SMT. In Proceedings of the seventh Interna-
tional Workshop on Spoken Language Translation
(IWSLT), pages 268?275.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 187?197, Edinburgh, Scotland, United King-
dom, 7.
Liang Huang and David Chiang. 2007. Forest Rescor-
ing: Faster Decoding with Integrated Language
Models. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 144?151, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Maxim Khalilov and Jose? A. R. Fonollosa. 2009. N-
Gram-Based Statistical Machine Translation Versus
Syntax Augmented Machine Translation: Compar-
ison and System Combination. In Proceedings of
the 12th Conference of the European Chapter of the
ACL (EACL 2009), pages 424?432, Athens, Greece,
March. Association for Computational Linguistics.
Philipp Koehn and Barry Haddow. 2012. Towards Ef-
fective Use of Training Data in Statistical Machine
Translation. In Proceedings of the Seventh Work-
shop on Statistical Machine Translation, pages 317?
321, Montre?al, Canada, June. Association for Com-
putational Linguistics.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proceed-
ings of HLT-NAACL, pages 127?133, Edmonton,
Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In ACL 2007 Demonstrations, Prague, Czech Re-
public.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July.
Shankar Kumar and William J. Byrne. 2004. Mini-
mum Bayes-Risk Decoding for Statistical Machine
Translation. In HLT-NAACL, pages 169?176.
404
Jose? B. Marin?o, Rafael E. Banchs, Josep M. Crego,
Adria` de Gispert, Patrik Lambert, Jose? A. R. Fonol-
losa, and Marta R. Costa-jussa`. 2006. N-gram-
Based Machine Translation. Computational Lin-
guistics, 32(4):527?549.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel. 2011. Wider Context by Using Bilin-
gual Language Models in Machine Translation. In
Proceedings of the Sixth Workshop on Statistical
Machine Translation, pages 198?206, Edinburgh,
Scotland, July. Association for Computational Lin-
guistics.
Franz J. Och and Hermann Ney. 2004. The Alignment
Template Approach to Statistical Machine Transla-
tion. Computational Linguistics, 30(1):417?449.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ?02, pages 311?318,
Morristown, NJ, USA.
Christopher Quirk and Arul Menezes. 2006. Do We
Need Phrases? Challenging the Conventional Wis-
dom in Statistical Machine Translation. In HLT-
NAACL.
Holger Schwenk and Philipp Koehn. 2008. Large and
Diverse Language Models for Statistical Machine
Translation. In International Joint Conference on
Natural Language Processing, pages 661?666, Jan-
uary 2008.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Intl. Conf. Spoken Lan-
guage Processing, Denver, Colorado.
Christoph Tillman. 2004. A Unigram Orienta-
tion Model for Statistical Machine Translation. In
HLT-NAACL 2004: Short Papers, pages 101?104,
Boston, Massachusetts.
Ashish Vaswani, Haitao Mi, Liang Huang, and David
Chiang. 2011. Rule Markov Models for Fast Tree-
to-String Translation. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 856?864, Portland, Oregon, USA, June.
Hui Zhang, Kristina Toutanova, Chris Quirk, and Jian-
feng Gao. 2013. Beyond Left-to-Right: Multi-
ple Decomposition Structures for SMT. In The
2013 Conference of the North American Chapter
of the Association for Computational Linguistics:
Human Language Technologies, Atlanta, Georgia,
USA, June. Association for Computational Linguis-
tics.
405
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 122?127,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Munich-Edinburgh-Stuttgart Submissions of OSM Systems at WMT13
Nadir Durrani1, Helmut Schmid2, Alexander Fraser2,
Hassan Sajjad3, Richa?rd Farkas4
1University of Edinburgh ? dnadir@inf.ed.ac.uk
2Ludwig Maximilian University Munich ? schmid,fraser@cis.uni-muenchen.de
3Qatar Computing Research Institute ? hsajjad@qf.org.qa
4University of Szeged ? rfarkas@inf.u-szeged.hu
Abstract
This paper describes Munich-Edinburgh-
Stuttgart?s submissions to the Eighth
Workshop on Statistical Machine Transla-
tion. We report results of the translation
tasks from German, Spanish, Czech and
Russian into English and from English to
German, Spanish, Czech, French and Rus-
sian. The systems described in this paper
use OSM (Operation Sequence Model).
We explain different pre-/post-processing
steps that we carried out for different
language pairs. For German-English we
used constituent parsing for reordering
and compound splitting as preprocessing
steps. For Russian-English we transliter-
ated the unknown words. The translitera-
tion system is learned with the help of an
unsupervised transliteration mining algo-
rithm.
1 Introduction
In this paper we describe Munich-Edinburgh-
Stuttgart?s1 joint submissions to the Eighth Work-
shop on Statistical Machine Translation. We use
our in-house OSM decoder which is based on
the operation sequence N-gram model (Durrani
et al, 2011). The N-gram-based SMT frame-
work (Marin?o et al, 2006) memorizes Markov
chains over sequences of minimal translation units
(MTUs or tuples) composed of bilingual transla-
tion units. The OSM model integrates reordering
operations within the tuple sequences to form a
heterogeneous mixture of lexical translation and
1Qatar Computing Research Institute and University of
Szeged were partnered for RU-EN and DE-EN language pairs
respectively.
reordering operations and learns a Markov model
over a sequence of operations.
Our decoder uses the beam search algorithm in
a stack-based decoder like most sequence-based
SMT frameworks. Although the model is based
on minimal translation units, we use phrases dur-
ing search because they improve the search accu-
racy of our system. The earlier decoder (Durrani
et al, 2011) was based on minimal units. But we
recently showed that using phrases during search
gives better coverage of translation, better future
cost estimation and lesser search errors (Durrani
et al, 2013a) than MTU-based decoding. We have
therefore shifted to phrase-based search on top of
the OSM model.
This paper is organized as follows. Section 2
gives a short description of the model and search
as used in the OSM decoder. In Section 3 we
give a description of the POS-based operation se-
quence model that we test for our German-English
and English-German experiments. Section 4 de-
scribes our processing of the German and English
data for German-English and English-German ex-
periments. In Section 5 we describe the unsuper-
vised transliteration mining that has been done for
the Russian-English and English-Russian experi-
ments. In Section 6 we describe the sub-sampling
technique that we have used for several language
pairs. In Section 7 we describe the experimental
setup followed by the results. Finally we summa-
rize the paper in Section 8.
2 System Description
2.1 Model
Our systems are based on the OSM (Operation Se-
quence Model) that simultaneously learns trans-
lation and reordering by representing a bilingual
122
Figure 1: Bilingual Sentence with Alignments
sentence pair and its alignments as a unique se-
quence of operations. An operation either jointly
generates source and target words, or it performs
reordering by inserting gaps or jumping to gaps.
We then learn a Markov model over a sequence of
operations o1, o2, . . . , oJ that encapsulate MTUs
and reordering information as:
posm(o1, ..., oJ) =
J?
j=1
p(oj |oj?n+1, ..., oj?1)
By coupling reordering with lexical generation,
each (translation or reordering) decision depends
on n? 1 previous (translation and reordering) de-
cisions spanning across phrasal boundaries. The
reordering decisions therefore influence lexical se-
lection and vice versa. A heterogeneous mixture
of translation and reordering operations enables us
to memorize reordering patterns and lexicalized
triggers unlike the classic N-gram model where
translation and reordering are modeled separately.
2.2 Training
During training, each bilingual sentence pair is de-
terministically converted to a unique sequence of
operations.2 The example in Figure 1(a) is con-
verted to the following sequence of operations:
Generate(Beide, Both)? Generate(La?nder, coun-
tries)? Generate(haben, have)? Insert Gap?
Generate(investiert, invested)
At this point, the (partial) German and English
sentences look as follows:
Beide La?nder haben investiert
Both countries have invested
The translator then jumps back and covers the
skipped German words through the following se-
quence of operations:
Jump Back(1)?Generate(Millionen, millions)?
Generate(von, of)? Generate(Dollar, dollars)
2Please refer to Durrani et al (2011) for a list of opera-
tions and the conversion algorithm.
The generative story of the OSM model also
supports discontinuous source-side cepts and
source-word deletion. However, it doesn?t provide
a mechanism to deal with unaligned and discon-
tinuous target cepts. These are handled through
a 3-step process3 in which we modify the align-
ments to remove discontinuous and unaligned tar-
get MTUs. Please see Durrani et al (2011) for
details. After modifying the alignments, we con-
vert each bilingual sentence pair and its align-
ments into a sequence of operations as described
above and learn an OSM model. To this end,
a Kneser-Ney (Kneser and Ney, 1995) smoothed
9-gram model is trained with SRILM (Stolcke,
2002) while KenLM (Heafield, 2011) is used at
runtime.
2.3 Feature Functions
We use additional features for our model and em-
ploy the standard log-linear approach (Och and
Ney, 2004) to combine and tune them. We search
for a target string E which maximizes a linear
combination of feature functions:
E? = argmax
E
?
?
?
J?
j=1
?jhj(o1, ..., oJ)
?
?
?
where ?j is the weight associated with the fea-
ture hj(o1, ..., oj). Apart from the main OSM
feature we train 9 additional features: A target-
language model (see Section 7 for details), 2 lex-
ical weighting features, gap and open gap penalty
features, two distance-based distortion models and
2 length-based penalty features. Please refer to
Durrani et al (2011) for details.
2.4 Phrase Extraction
Phrases are extracted in the following way: The
aligned training corpus is first converted to an op-
eration sequence. Each subsequence of operations
that starts and ends with a translation operation, is
considered a ?phrase?. The translation operations
include Generate Source Only (X) operation which
deletes unaligned source word. Such phrases may
be discontinuous if they include reordering opera-
tions. We replace each subsequence of reordering
operations by a discontinuity marker.
3Durrani et al (2013b) recently showed that our post-
processing of alignments hurt the performance of the Moses
Phrase-based system in several language pairs. The solu-
tion they proposed has not been incorporated into the current
OSM decoder yet.
123
During decoding, we match the source tokens
of the phrase with the input. Whenever there is
a discontinuity in the phrase, the next source to-
ken can be matched at any position of the input
string. If there is no discontinuity marker, the next
source token in the phrase must be to the right of
the previous one. Finally we compute the number
of uncovered input tokens within the source span
of the hypothesized phrase and reject the phrase
if the number is above a threshold. We use a
threshold value of 2 which had worked well in
initial experiments. Once the positions of all the
source words of a phrase are known, we can com-
pute the necessary reordering operations (which
may be different from the ones that appeared in
the training corpus). This usage of phrases al-
lows the decoder to generalize from a seen trans-
lation ?scored a goal ? ein Tor schoss? (where
scored/a/goal and schoss/ein/Tor are aligned, re-
spectively) to ?scored a goal ? schoss ein Tor?.
The phrase can even be used to translate ?er schoss
heute ein Tor ? he scored a goal today? although
?heute? appears within the source span of the
phrase ?ein Tor schoss?. Without phrase-based
decoding, the unusual word translations ?schoss?
scored? and ?Tor?goal? (at least outside of the soc-
cer literature) are likely to be pruned.
The phrase tables are further filtered with
threshold pruning. The translation options with
a frequency less than x times the frequency of
the most frequent translation are deleted. We use
x = 0.02. We use additional settings to increase
this threshold for longer phrases. The phrase fil-
tering heuristic was used to speed up decoding. It
did not lower the BLEU score in our small scale
experiments (Durrani et al, 2013a), however we
could not test whether this result holds in a large
scale evaluation.
2.5 Decoder
The decoding framework used in the operation se-
quence model is based on Pharaoh (Koehn, 2004).
The decoder uses beam search to build up the
translation from left to right. The hypotheses are
arranged in m stacks such that stack i maintains
hypotheses that have already translated imany for-
eign words. The ultimate goal is to find the best
scoring hypothesis, that translates all the words
in the foreign sentence. During the hypothesis
extension each extracted phrase is translated into
a sequence of operations. The reordering opera-
tions (gaps and jumps) are generated by looking at
the position of the translator, the last foreign word
generated etc. (Please refer to Algorithm 1 in Dur-
rani et al (2011)). The probability of an opera-
tion depends on the n?1 previous operations. The
model is smoothed with Kneser-Ney smoothing.
3 POS-based OSM Model
Part-of-speech information is often relevant for
translation. The word ?stores? e.g. should be
translated to ?La?den? if it is a noun and to ?spei-
chert? when it is a verb. The sentence ?The small
child cries? might be incorrectly translated to ?Die
kleinen Kind weint? where the first three words
lack number, gender and case agreement.
In order to better learn such constraints which
are best expressed in terms of part of speech, we
add another OSM model as a new feature to the
log-linear model of our decoder, which is identi-
cal to the regular OSM except that all the words
have been replaced by their POS tags. The input
of the decoder consists of the input sentence with
automatically assigned part-of-speech tags. The
source and target part of the training data are also
automatically tagged and phrases with words and
POS tags on both sides are extracted. The POS-
based OSM model is only used in the German-to-
English and English-to-German experiments.4 So
far, we only used coarse POS tags without gender
and case information.
4 Constituent Parse Reordering
Our German-to-English system used constituent
parses for pre-ordering of the input. We parsed all
of the parallel German to English data available,
and the tuning, test and blind-test sets. We then
applied reordering rules to these parses. We used
the rules for reordering German constituent parses
of Collins et al (2005) together with the additional
rules described by Fraser (2009). These are ap-
plied as a preprocess to all German data (training,
tuning and test data). To produce the parses, we
started with the generative BitPar parser trained on
the Tiger treebank with optimizations of the gram-
mar, as described by (Fraser et al, 2013). We then
performed self-training using the high quality Eu-
roparl corpus - we parsed it, and then retrained the
parser on the output.
4This work is ongoing and we will present detailed exper-
iments in the future.
124
Following this, we performed linguistically-
informed compound splitting, using the system of
Fritzinger and Fraser (2010), which disambiguates
competing analyses from the high-recall Stuttgart
Morphological Analyzer SMOR (Schmid et al,
2004) using corpus statistics (Koehn and Knight,
2003). We also split portmanteaus like German
?zum? formed from ?zu dem? meaning ?to the?.
Due to time constraints, we did not address Ger-
man inflection. See Weller et al (2013) for further
details of the linguistic processing involved in our
German-to-English system.
5 Transliteration Mining/Handling
OOVs
The machine translation system fails to translate
out-of-vocabulary words (OOVs) as they are un-
known to the training data. Most of the OOVs
are named entities and simply passing them to
the output often produces correct translations if
source and target language use the same script.
If the scripts are different transliterating them to
the target language script could solve this prob-
lem. However, building a transliteration system
requires a list of transliteration pairs for training.
We do not have such a list and making one is a
cumbersome process. Instead, we use the unsu-
pervised transliteration mining system of Sajjad et
al. (2012) that takes a list of word pairs for train-
ing and extracts transliteration pairs that can be
used for the training of the transliteration system.
The procedure of mining transliteration pairs and
transliterating OOVs is described as follows:
We word-align the parallel corpus using
GIZA++ in both direction and symmetrize the
alignments using the grow-diag-final-and heuris-
tic. We extract all word pairs which occur as 1-
to-1 alignments (like Sajjad et al (2011)) and later
refer to them as the list of word pairs. We train the
unsupervised transliteration mining system on the
list of word pairs and extract transliteration pairs.
We use these mined pairs to build a transliteration
system using the Moses toolkit. The translitera-
tion system is applied in a post-processing step
to transliterate OOVs. Please refer to Sajjad et
al. (2013) for further details on our transliteration
work.
6 Sub-sampling
Because of scalability problems we were not able
to use the entire data made available for build-
ing the translation model in some cases. We used
modified Moore-Lewis sampling (Axelrod et al,
2011) for the language pairs es-en, en-es, en-fr,
and en-cs. In each case we included the News-
Commentary and Europarl corpora in their en-
tirety, and scored the sentences in the remaining
corpora (the selection corpus) using a filtering cri-
terion, adding 10% of the selection corpus to
the training data. We can not say with certainty
whether using the entire data will produce better
results with the OSM decoder. However, we know
that the same data used with the state-of-the-art
Moses produced worse results in some cases. The
experiments in Durrani et al (2013c) showed that
MML filtering decreases the BLEU scores in es-
en (news-test13: Table 19) and en-cs (news-test12:
Table 14). We can therefore speculate that being
able to use all of the data may improve our results
somewhat.
7 Experiments
Parallel Corpus: The amount of bitext used for
the estimation of the translation models is: de?en
? 4.5M and ru?en ? 2M parallel sentences. We
were able to use all the available data for cs-to-en
(? 15.6M sentences). However, sub-sampled data
was used for en-to-cs (? 3M sentences), en-to-fr
(? 7.8M sentences) and es?en (? 3M sentences).
Monolingual Language Model: We used all
the available training data (including LDC Giga-
word data) for the estimation of monolingual lan-
guage models: en? 287.3M sentences, fr? 91M,
es ? 65.7M, cs ? 43.4M and ru ? 21.7M sen-
tences. All data except for ru-en and en-ru was
true-cased. We followed the approach of Schwenk
and Koehn (2008) by training language models
from each sub-corpus separately and then linearly
interpolated them using SRILM with weights op-
timized on the held-out dev-set. We concatenated
the news-test sets from four years (2008-2011) to
obtain a large dev-set5 in order to obtain more sta-
ble weights (Koehn and Haddow, 2012).
Decoder Settings: For each extracted input
phrase only 15-best translation options were used
during decoding.6 We used a hard reordering limit
5For Russian-English and English-Russian language
pairs, we divided the tuning-set news-test 2012 into two
halves and used the first half for tuning and second for test.
6We could not experiment with higher n-best translation
options due to a bug that was not fixed in time and hindered
us from scaling.
125
of 16 words which disallows a jump beyond 16
source words. A stack size of 100 was used during
tuning and 200 for decoding the test set.
Results: Table 1 shows the uncased BLEU
scores along with the rank obtained on the sub-
mission matrix.7 We also show the results from
human evaluation.
Lang Evaluation
Automatic Human
BLEU Rank Win Ratio Rank
de-en 27.6 9/31 0.562 6-8
es-en 30.4 6/12 0.569 3-5
cs-en 26.4 3/11 0.581 2-3
ru-en 24.5 8/22 0.534 7-9
en-de 20.0 6/18
en-es 29.5 3/13 0.544 5-6
en-cs 17.6 14/22 0.517 4-6
en-ru 18.1 6/15 0.456 9-10
en-fr 30.0 7/26 0.541 5-9
Table 1: Translating into and from English
8 Conclusion
In this paper, we described our submissions to
WMT 13 in all the shared-task language pairs
(except for fr-en). We used an OSM-decoder,
which implements a model on n-gram of opera-
tions encapsulating lexical generation and reorder-
ing. For German-to-English we used constituent
parsing and applied linguistically motivated rules
to these parses, followed by compound splitting.
We additionally used a POS-based OSM model for
German-to-English and English-to-German exper-
iments. For Russian-English language pairs we
used unsupervised transliteration mining. Because
of scalability issues we could not use the entire
data in some language pairs and used only sub-
sampled data. Our Czech-to-English system that
was built from the entire data did better in both
automatic and human evaluation compared to the
systems that used sub-sampled data.
Acknowledgments
We would like to thank the anonymous reviewers
for their helpful feedback and suggestions. We
would like to thank Philipp Koehn and Barry Had-
dow for providing data and alignments. Nadir
7http://matrix.statmt.org/
Durrani was funded by the European Union Sev-
enth Framework Programme (FP7/2007-2013) un-
der grant agreement n ? 287658. Alexander
Fraser was funded by Deutsche Forschungsge-
meinschaft grant Models of Morphosyntax for
Statistical Machine Translation. Helmut Schmid
was supported by Deutsche Forschungsgemein-
schaft grant SFB 732. Richa?rd Farkas was
partially funded by the Hungarian National Ex-
cellence Program (TA?MOP 4.2.4.A/2-11-1-2012-
0001). This publication only reflects the authors?
views.
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 355?362, Edinburgh, Scotland, UK., July.
Association for Computational Linguistics.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause Restructuring for Statistical Machine
Translation. In ACL05, pages 531?540, Ann Arbor,
MI.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A Joint Sequence Translation Model with In-
tegrated Reordering. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
1045?1054, Portland, Oregon, USA, June.
Nadir Durrani, Alexander Fraser, and Helmut Schmid.
2013a. Model With Minimal Translation Units, But
Decode With Phrases. In The 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Atlanta, Georgia, USA, June. Association
for Computational Linguistics.
Nadir Durrani, Alexander Fraser, Helmut Schmid,
Hieu Hoang, and Philipp Koehn. 2013b. Can
Markov Models Over Minimal Translation Units
Help Phrase-Based SMT? In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics, Sofia, Bulgaria, August. Asso-
ciation for Computational Linguistics.
Nadir Durrani, Barry Haddow, Kenneth Heafield, and
Philipp Koehn. 2013c. Edinburgh?s Machine Trans-
lation Systems for European Language Pairs. In
Proceedings of the Eighth Workshop on Statistical
Machine Translation, Sofia, Bulgaria, August. As-
sociation for Computational Linguistics.
Alexander Fraser, Helmut Schmid, Richa?rd Farkas,
Renjing Wang, and Hinrich Schu?tze. 2013. Knowl-
edge sources for constituent parsing of German, a
morphologically rich and less-configurational lan-
guage. Computational Linguistics - to appear.
126
Alexander Fraser. 2009. Experiments in Morphosyn-
tactic Processing for Translating to and from Ger-
man. In Proceedings of the EACL 2009 Fourth
Workshop on Statistical Machine Translation, pages
115?119, Athens, Greece, March.
Fabienne Fritzinger and Alexander Fraser. 2010. How
to Avoid Burning Ducks: Combining Linguistic
Analysis and Corpus Statistics for German Com-
pound Processing. In Proceedings of the ACL 2010
Fifth Workshop on Statistical Machine Translation,
Uppsala, Sweden.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 187?197, Edinburgh, Scotland, United King-
dom, 7.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In In
Proceedings of the IEEE International Conference
on Acoustics, Speech and Signal Processing, vol-
ume I, pages 181?184, Detroit, Michigan, May.
Philipp Koehn and Barry Haddow. 2012. Towards Ef-
fective Use of Training Data in Statistical Machine
Translation. In Proceedings of the Seventh Work-
shop on Statistical Machine Translation, pages 317?
321, Montre?al, Canada, June. Association for Com-
putational Linguistics.
Philipp Koehn and Kevin Knight. 2003. Empirical
methods for compound splitting. In Proceedings of
the 10th Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 187?193, Morristown, NJ.
Philipp Koehn. 2004. Pharaoh: A Beam Search De-
coder for Phrase-Based Statistical Machine Transla-
tion Models. In AMTA, pages 115?124.
Jose? B. Marin?o, Rafael E. Banchs, Josep M. Crego,
Adria` de Gispert, Patrik Lambert, Jose? A. R. Fonol-
losa, and Marta R. Costa-jussa`. 2006. N-gram-
Based Machine Translation. Computational Lin-
guistics, 32(4):527?549.
Franz J. Och and Hermann Ney. 2004. The Alignment
Template Approach to Statistical Machine Transla-
tion. Computational Linguistics, 30(1):417?449.
Hassan Sajjad, Alexander Fraser, and Helmut Schmid.
2011. An algorithm for unsupervised transliteration
mining with an application to word alignment. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies - Volume 1, Portland, USA.
Hassan Sajjad, Alexander Fraser, and Helmut Schmid.
2012. A statistical model for unsupervised and
semi-supervised transliteration mining. In Proceed-
ings of the 50th Annual Meeting of the Association
for Computational Linguistics: Long Papers - Vol-
ume 1, Jeju, Korea.
Hassan Sajjad, Svetlana Smekalova, Nadir Durrani,
Alexander Fraser, and Helmut Schmid. 2013.
QCRI-MES Submission at WMT13: Using Translit-
eration Mining to Improve Statistical Machine
Translation. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Helmut Schmid, Arne Fitschen, and Ulrich Heid.
2004. SMOR: A German Computational Morphol-
ogy Covering Derivation, Composition, and Inflec-
tion. In Proceedings of the Fourth International
Conference on Language Resources and Evaluation
(LREC).
Holger Schwenk and Philipp Koehn. 2008. Large and
Diverse Language Models for Statistical Machine
Translation. In International Joint Conference on
Natural Language Processing, pages 661?666, Jan-
uary 2008.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Intl. Conf. Spoken Lan-
guage Processing, Denver, Colorado.
Marion Weller, Max Kisselew, Svetlana Smekalova,
Alexander Fraser, Helmut Schmid, Nadir Durrani,
Hassan Sajjad, and Richa?rd Farkas. 2013. Munich-
Edinburgh-Stuttgart Submissions at WMT13: Mor-
phological and Syntactic Processing for SMT. In
Proceedings of the Eighth Workshop on Statistical
Machine Translation, Sofia, Bulgaria, August. As-
sociation for Computational Linguistics.
127
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 219?224,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
QCRI-MES Submission at WMT13: Using Transliteration Mining to
Improve Statistical Machine Translation
Hassan Sajjad1, Svetlana Smekalova2, Nadir Durrani3,
Alexander Fraser4, Helmut Schmid4
1Qatar Computing Research Institute ? hsajjad@qf.org.qa
2University of Stuttgart ? smekalsa@ims.uni-stuttgart.de
3University of Edinburgh ? dnadir@inf.ed.ac.uk
4Ludwig-Maximilians University Munich ? (fraser|schmid)@cis.uni-muenchen.de
Abstract
This paper describes QCRI-MES?s sub-
mission on the English-Russian dataset to
the Eighth Workshop on Statistical Ma-
chine Translation. We generate improved
word alignment of the training data by
incorporating an unsupervised translitera-
tion mining module to GIZA++ and build
a phrase-based machine translation sys-
tem. For tuning, we use a variation of PRO
which provides better weights by optimiz-
ing BLEU+1 at corpus-level. We translit-
erate out-of-vocabulary words in a post-
processing step by using a transliteration
system built on the transliteration pairs
extracted using an unsupervised translit-
eration mining system. For the Russian
to English translation direction, we apply
linguistically motivated pre-processing on
the Russian side of the data.
1 Introduction
We describe the QCRI-Munich-Edinburgh-
Stuttgart (QCRI-MES) English to Russian and
Russian to English systems submitted to the
Eighth Workshop on Statistical Machine Trans-
lation. We experimented using the standard
Phrase-based Statistical Machine Translation
System (PSMT) as implemented in the Moses
toolkit (Koehn et al, 2007). The typical pipeline
for translation involves word alignment using
GIZA++ (Och and Ney, 2003), phrase extraction,
tuning and phrase-based decoding. Our system is
different from standard PSMT in three ways:
? We integrate an unsupervised transliteration
mining system (Sajjad et al, 2012) into the
GIZA++ word aligner (Sajjad et al, 2011).
So, the selection of a word pair as a correct
alignment is decided using both translation
probabilities and transliteration probabilities.
? The MT system fails when translating out-of-
vocabulary (OOV) words. We build a statis-
tical transliteration system on the translitera-
tion pairs mined by the unsupervised translit-
eration mining system and transliterate them
in a post-processing step.
? We use a variation of Pairwise Ranking Op-
timization (PRO) for tuning. It optimizes
BLEU at corpus-level and provides better
feature weights that leads to an improvement
in translation quality (Nakov et al, 2012).
We participate in English to Russian and Rus-
sian to English translation tasks. For the Rus-
sian/English system, we present experiments with
two variations of the parallel corpus. One set of
experiments are conducted using the standard par-
allel corpus provided by the workshop. In the sec-
ond set of experiments, we morphologically re-
duce Russian words based on their fine-grained
POS tags and map them to their root form. We
do this on the Russian side of the parallel corpus,
tuning set, development set and test set. This im-
proves word alignment and learns better transla-
tion probabilities by reducing the vocabulary size.
The paper is organized as follows. Section
2 talks about unsupervised transliteration mining
and its incorporation to the GIZA++ word aligner.
In Section 3, we describe the transliteration sys-
tem. Section 4 describes the extension of PRO
that optimizes BLEU+1 at corpus level. Section
5 and Section 6 present English/Russian and Rus-
sian/English machine translation experiments re-
spectively. Section 7 concludes.
219
2 Transliteration Mining
Consider a list of word pairs that consists of either
transliteration pairs or non-transliteration pairs.
A non-transliteration pair is defined as a word
pair where words are not transliteration of each
other. They can be translation, misalignment,
etc. Transliteration mining extracts transliteration
pairs from the list of word pairs. Sajjad et al
(2012) presented an unsupervised transliteration
mining system that trains on the list of word pairs
and filters transliteration pairs from that. It models
the training data as the combination of a translit-
eration sub-model and a non-transliteration sub-
model. The transliteration model is a joint source
channel model. The non-transliteration model as-
sumes no correlation between source and target
word characters, and independently generates a
source and a target word using two fixed uni-
gram character models. The transliteration mining
model is defined as an interpolation of the translit-
eration model and the non-transliteration model.
We apply transliteration mining to the list of
word pairs extracted from English/Russian paral-
lel corpus and mine transliteration pairs. We use
the mined pairs for the training of the translitera-
tion system.
2.1 Transliteration Augmented-GIZA++
GIZA++ aligns parallel sentences at word level. It
applies the IBM models (Brown et al, 1993) and
the HMM model (Vogel et al, 1996) in both direc-
tions i.e. source to target and target to source. It
generates a list of translation pairs with translation
probabilities, which is called the t-table. Sajjad
et al (2011) used a heuristic-based transliteration
mining system and integrated it into the GIZA++
word aligner. We follow a similar procedure but
use the unsupervised transliteration mining system
of Sajjad et al (2012).
We define a transliteration sub-model and train
it on the transliteration pairs mined by the unsuper-
vised transliteration mining system. We integrate
it into the GIZA++ word aligner. The probabil-
ity of a word pair is calculated as an interpolation
of the transliteration probability and the transla-
tion probability stored in the t-table of the differ-
ent alignment models used by the GIZA++ aligner.
This interpolation is done for all iterations of all
alignment models.
2.1.1 Estimating Transliteration Probabilities
We use the algorithm for the estimation of translit-
eration probabilities of Sajjad et al (2011). We
modify it to improve efficiency. In step 6 of Al-
gorithm 1 instead of taking all f that coocur with
e, we take only those that have a word length ra-
tio in range of 0.8-1.2.1 This reduces cooc(e) by
more than half and speeds up step 9 of Algorithm
1. The word pairs that are filtered out from cooc(e)
won?t have transliteration probability pti(f |e). We
do not interpolate in these cases and use the trans-
lation probability as it is.
Algorithm 1 Estimation of transliteration proba-
bilities, e-to-f direction
1: unfiltered data? list of word pairs
2: filtered data?transliteration pairs extracted using unsu-
pervised transliteration mining system
3: Train a transliteration system on the filtered data
4: for all e do
5: nbestTI(e) ? 10 best transliterations for e accord-
ing to the transliteration system
6: cooc(e)? set of all f that cooccur with e in a parallel
sentence with a word length in ratio of 0.8-1.2
7: candidateTI(e)? cooc(e) ? nbestTI(e)
8: for all f do
9: pmoses(f, e) ? joint transliteration probability of e
and f according to the transliterator
10: Calculate conditional transliteration probability
pti(f |e)? pmoses(f,e)?
f??CandidateTI(e) pmoses(f ?,e)
2.1.2 Modified EM Training
Sajjad et al (2011) modified the EM training of
the word alignment models. They combined the
translation probabilities of the IBM models and
the HMM model with the transliteration proba-
bilities. Consider pta(f |e) = fta(f, e)/fta(e) is
the translation probability of the word alignment
models. The interpolated probability is calcu-
lated by adding the smoothed alignment frequency
fta(f, e) to the transliteration probability weight
by the factor ?. The modified translation probabil-
ities is given by:
p?(f |e) = fta(f, e) + ?pti(f |e)fta(e) + ?
(1)
where fta(f, e) = pta(f |e)fta(e). pta(f |e) is ob-
tained from the original t-table of the alignment
model. fta(e) is the total corpus frequency of e.
? is the transliteration weight which is defined as
the number of counts the transliteration model gets
versus the translation model. The model is not
1We assume that the words with very different character
counts are less likely to be transliterations.
220
very sensitive to the value of ?. We use ? = 50
for our experiments. The procedure we described
of estimation of transliteration probabilities and
modification of EM is also followed in the oppo-
site direction f-to-e.
3 Transliteration System
The unsupervised transliteration mining system
(as described in Section 2) outputs a list of translit-
eration pairs. We consider transliteration word
pairs as parallel sentences by putting a space af-
ter every character of the words and train a PSMT
system for transliteration. We apply the transliter-
ation system to OOVs in a post-processing step on
the output of the machine translation system.
Russian is a morphologically rich language.
Different cases of a word are generally represented
by adding suffixes to the root form. For OOVs
that are named entities, transliterating the inflected
forms generates wrong English transliterations as
inflectional suffixes get transliterated too. To han-
dle this, first we need to identify OOV named en-
tities (as there can be other OOVs that are not
named entities) and then transliterate them cor-
rectly. We tackle the first issue as follows: If
an OOV word is starting with an upper case let-
ter, we identify it as a named entity. To correctly
transliterate it to English, we stem the named en-
tity based on a list of suffixes ( , , , , , )
and transliterate the stemmed form. For morpho-
logically reduced Russian (see Section 6.1), we
follow the same procedure as OOVs are unknown
to the POS tagger too and are (incorrectly) not re-
duced to their root forms. For OOVs that are not
identified as named entities, we transliterate them
without any pre-processing.
4 PRO: Corpus-level BLEU
Pairwise Ranking Optimization (PRO) (Hopkins
and May, 2011) is an extension of MERT (Och,
2003) that can scale to thousands of parameters.
It optimizes sentence-level BLEU+1 which is an
add-one smoothed version of BLEU (Lin and Och,
2004). The sentence-level BLEU+1 has a bias
towards producing short translations as add-one
smoothing improves precision but does not change
the brevity penalty. Nakov et al (2012) fixed this
by using several heuristics on brevity penalty, ref-
erence length and grounding the precision length.
In our experiments, we use the improved version
of PRO as provided by Nakov et al (2012). We
call it PROv1 later on.
5 English/Russian Experiments
5.1 Dataset
The amount of bitext used for the estimation of the
translation model is ? 2M parallel sentences. We
use newstest2012a for tuning and newstest2012b
(tst2012) as development set.
The language model is estimated using large
monolingual corpus of Russian ? 21.7M sen-
tences. We follow the approach of Schwenk and
Koehn (2008) by training domain-specific lan-
guage models separately and then linearly inter-
polate them using SRILM with weights optimized
on the held-out development set. We divide the
tuning set newstest2012a into two halves and use
the first half for tuning and second for test in or-
der to obtain stable weights (Koehn and Haddow,
2012).
5.2 Baseline Settings
We word-aligned the parallel corpus using
GIZA++ (Och and Ney, 2003) with 5 iterations
of Model1, 4 iterations of HMM and 4 iterations
of Model4, and symmetrized the alignments us-
ing the grow-diag-final-and heuristic (Koehn et al,
2003). We built a phrase-based machine transla-
tion system using the Moses toolkit. Minimum er-
ror rate training (MERT), margin infused relaxed
algorithm (MIRA) and PRO are used to optimize
the parameters.
5.3 Main System Settings
Our main system involves a pre-processing step
? unsupervised transliteration mining, and a post-
processing step ? transliteration of OOVs. For the
training of the unsupervised transliteration min-
ing system, we take the word alignments from
our baseline settings and extract all word pairs
which occur as 1-to-1 alignments (like Sajjad et
al. (2011)) and later refer to them as a list of
word pairs. The unsupervised transliteration min-
ing system trains on the list of word pairs and
mines transliteration pairs. We use the mined pairs
to build a transliteration system using the Moses
toolkit. The transliteration system is used in Algo-
rithm 1 to generate transliteration probabilities of
candidate word pairs and is also used in the post-
processing step to transliterate OOVs.
We run GIZA++ with identical settings as de-
scribed in Section 5.2. We interpolate for ev-
221
GIZA++ TA-GIZA++ OOV-TI
MERT 23.41 23.51 23.60
MIRA 23.60 23.73 23.85
PRO 23.57 23.68 23.70
PROv1 23.65 23.76 23.87
Table 1: BLEU scores of English to Russian ma-
chine translation system evaluated on tst2012 us-
ing baseline GIZA++ alignment and translitera-
tion augmented-GIZA++. OOV-TI presents the
score of the system trained using TA-GIZA++ af-
ter transliterating OOVs
ery iteration of the IBM Model1 and the HMM
model. We had problem in applying smoothing
for Model4 and did not interpolate transliteration
probabilities for Model4. The alignments are re-
fined using the grow-diag-final-and heuristic. We
build a phrase-based system on the aligned pairs
and tune the parameters using PROv1. OOVs are
transliterated in the post-processing step.
5.4 Results
Table 1 summarizes English/Russian results on
tst2012. Improved word alignment gives up to
0.13 BLEU points improvement. PROv1 improves
translation quality and shows 0.08 BLEU point
increase in BLEU in comparison to the parame-
ters tuned using PRO. The transliteration of OOVs
consistently improve translation quality by at least
0.1 BLEU point for all systems.2 This adds to a
cumulative gain of up to 0.2 BLEU points.
We summarize results of our systems trained on
GIZA++ and transliteration augmented-GIZA++
(TA-GIZA++) and tested on tst2012 and tst2013
in Table 2. Both systems use PROv1 for tuning
and transliteration of OOVs in the post-processing
step. The system trained on TA-GIZA++ per-
formed better than the system trained on the base-
line aligner GIZA++.
6 Russian/English Experiments
In this section, we present translation experiments
in Russian to English direction. We morphologi-
cally reduce the Russian side of the parallel data in
a pre-processing step and train the translation sys-
tem on that. We compare its result with the Rus-
sian to English system trained on the un-processed
parallel data.
2We see similar gain in BLEU when using operation se-
quence model (Durrani et al, 2011) for decoding and translit-
erating OOVs in a post-processing step (Durrani et al, 2013).
SYS tst2012 tst2013
GIZA++ 23.76 18.4
TA-GIZA++ 23.87 18.5*
Table 2: BLEU scores of English to Russian ma-
chine translation system evaluated on tst2012 and
tst2013 using baseline GIZA++ alignment and
transliteration augmented-GIZA++ alignment and
post-processed the output by transliterating OOVs.
Human evaluation in WMT13 is performed on
TA-GIZA++ tested on tst2013 (marked with *)
6.1 Morphological Processing
The linguistic processing of Russian involves POS
tagging and morphological reduction. We first tag
the Russian data using a fine grained tagset. The
tagger identifies lemmas and the set of morpholog-
ical attributes attached to each word. We reduce
the number of these attributes by deleting some
of them, that are not relevant for English (for ex-
ample, gender agreement of verbs). This gener-
ates a morphologically reduced Russian which is
used in parallel with English for the training of
the machine translation system. Further details on
the morphological processing of Russian are de-
scribed in Weller et al (2013).
6.1.1 POS Tagging
We use RFTagger (Schmid and Laws, 2008) for
POS tagging. Despite the good quality of tagging
provided by RFTagger, some errors seem to be un-
avoidable due to the ambiguity of certain gram-
matical forms in Russian. A good example of
this is neuter nouns that have the same form in
all cases, or feminine nouns, which have identi-
cal forms in singular genitive and plural nomina-
tive (Sharoff et al, 2008). Since Russian sentences
have free word order, and the case of nouns can-
not be determined on that basis, this imperfection
can not be corrected during tagging or by post-
processing the tagger output.
6.1.2 Morphological Reduction
English in comparison to Slavic group of lan-
guages is morphologically poor. For example, En-
glish has no morphological attributes for nouns
and adjectives to express gender or case; verbs in
English have no gender either. Russian, on the
contrary, has rich morphology. It suffices to say
that the Russian has 6 cases and 3 grammatical
genders, which manifest themselves in different
222
suffixes for nouns, pronouns, adjectives and some
verb forms.
When translating from Russian into English, a
lot of these attributes become meaningless and ex-
cessive. It makes sense to reduce the number of
morphological attributes before the text is sup-
plied for the training of the MT system. We ap-
ply morphological reduction to nouns, pronouns,
verbs, adjectives, prepositions and conjunctions.
The rest of the POS (adverbs, particles, interjec-
tions and abbreviations) have no morphological at-
tributes and are left unchanged.
We apply morphological reduction to train,
tune, development and test data. We refer to this
data set as morph-reduced later on.
6.2 Dataset
We use two variations of the parallel corpus to
build and test the Russian to English system. One
system is built on the data provided by the work-
shop. For the second system, we preprocess the
Russian side of the data as described in Section
6.1. Both the provided parallel corpus and the
morph-reduced parallel corpus consist of 2M par-
allel sentences each. We use them for the estima-
tion of the translation model. We use large train-
ing data for the estimation of monolingual lan-
guage model ? en? 287.3M sentences. We follow
the identical procedure of interpolated language
model as described in Section 5.1. We use new-
stest2012a for tuning and newstest2012b (tst2012)
for development.
6.3 System Settings
We use identical system settings to those described
in Section 5.3. We trained the systems sepa-
rately on GIZA++ and transliteration augmented-
GIZA++ to compare their results. All systems are
tuned using PROv1. The translation output is post-
processed to transliterate OOVs.
6.4 Results
Table 3 summarizes results of Russian to English
machine translation systems trained on the orig-
inal parallel corpus and on the morph-reduced
corpus and using GIZA++ and transliteration
augmented-GIZA++ for word alignment. The sys-
tem using TA-GIZA++ for alignment shows the
best results for both tst2012 and tst2013. The im-
proved alignment gives a BLEU improvement of
up to 0.4 points.
Original corpus
SYS tst2012 tst2013
GIZA++ 32.51 25.5
TA-GIZA++ 33.40 25.9*
Morph-reduced
SYS tst2012 tst2013
GIZA++ 31.22 24.30
TA-GIZA++ 31.40 24.45
Table 3: Russian to English machine translation
system evaluated on tst2012 and tst2013. Human
evaluation in WMT13 is performed on the system
trained using the original corpus with TA-GIZA++
for alignment (marked with *)
The system built on the morph-reduced data
shows degradation in results by 1.29 BLEU points.
However, the percentage of OOVs reduces for
both test sets when using the morph-reduced data
set compared to the original parallel corpus. We
analyze the output of the system and find that the
morph-reduced system makes mistakes in choos-
ing the right tense of the verb. This might be one
reason for poor performance. This implies that the
morphological reduction is slightly damaging the
data, perhaps for specific parts of speech. In the
future, we would like to investigate this issue in
detail.
7 Conclusion
In this paper, we described the QCRI-Munich-
Edinburgh-Stuttgart machine translation systems
submitted to the Eighth Workshop on Statistical
Machine Translation. We aligned the parallel cor-
pus using transliteration augmented-GIZA++ to
improve the word alignments. We built a phrase-
based system using the Moses toolkit. For tun-
ing the feature weights, we used an improvement
of PRO that optimizes for corpus-level BLEU. We
post-processed the output of the machine transla-
tion system to transliterate OOV words.
For the Russian to English system, we mor-
phologically reduced the Russian data in a pre-
processing step. This reduced the vocabulary size
and helped to generate better word alignments.
However, the performance of the SMT system
dropped by 1.29 BLEU points in decoding. We
will investigate this issue further in the future.
223
Acknowledgments
We would like to thank the anonymous reviewers
for their helpful feedback and suggestions. We
would like to thank Philipp Koehn and Barry Had-
dow for providing data and alignments. Nadir
Durrani was funded by the European Union Sev-
enth Framework Programme (FP7/2007-2013) un-
der grant agreement n ? 287658. Alexander Fraser
was funded by Deutsche Forschungsgemeinschaft
grant Models of Morphosyntax for Statistical Ma-
chine Translation. Helmut Schmid was supported
by Deutsche Forschungsgemeinschaft grant SFB
732. This publication only reflects the authors
views.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and R. L. Mercer. 1993. The mathe-
matics of statistical machine translation: parameter
estimation. Computational Linguistics, 19(2).
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A joint sequence translation model with in-
tegrated reordering. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, Port-
land, USA.
Nadir Durrani, Helmut Schmid, Alexander Fraser, Has-
san Sajjad, and Richa?rd Farkas. 2013. Munich-
Edinburgh-Stuttgart submissions of OSM systems at
WMT13. In Proceedings of the Eighth Workshop on
Statistical Machine Translation, Sofia, Bulgaria.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
Edinburgh, United Kingdom.
Philipp Koehn and Barry Haddow. 2012. Towards
effective use of training data in statistical machine
translation. In Proceedings of the Seventh Work-
shop on Statistical Machine Translation, Montre?al,
Canada.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceed-
ings of the Human Language Technology and North
American Association for Computational Linguis-
tics Conference, Edmonton, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation for Computational Linguistics, Demonstra-
tion Program, Prague, Czech Republic.
Chin-Yew Lin and Franz Josef Och. 2004. OR-
ANGE: a method for evaluating automatic evalua-
tion metrics for machine translation. In Proceed-
ings of the 20th international conference on Compu-
tational Linguistics, Geneva, Switzerland.
Preslav Nakov, Francisco Guzma?n, and Stephan Vo-
gel. 2012. Optimizing for sentence-level BLEU+1
yields short translations. In Proceedings of the
24th International Conference on Computational
Linguistics, Mumbai, India.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1).
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, Sapporo, Japan.
Hassan Sajjad, Alexander Fraser, and Helmut Schmid.
2011. An algorithm for unsupervised translitera-
tion mining with an application to word alignment.
In Proceedings of the 49th Annual Conference of
the Association for Computational Linguistics, Port-
land, USA.
Hassan Sajjad, Alexander Fraser, and Helmut Schmid.
2012. A statistical model for unsupervised and
semi-supervised transliteration mining. In Proceed-
ings of the 50th Annual Conference of the Associa-
tion for Computational Linguistics, Jeju, Korea.
Helmut Schmid and Florian Laws. 2008. Estimation
of conditional probabilities with decision trees and
an application to fine-grained pos tagging. In Pro-
ceedings of the 22nd International Conference on
Computational Linguistics - Volume 1, Manchester,
United Kingdom.
Holger Schwenk and Philipp Koehn. 2008. Large and
Diverse Language Models for Statistical Machine
Translation. In International Joint Conference on
Natural Language Processing, Hyderabad, India.
Serge Sharoff, Mikhail Kopotev, Tomaz Erjavec, Anna
Feldman, and Dagmar Divjak. 2008. Designing
and evaluating a russian tagset. In Proceedings of
the Sixth International Conference on Language Re-
sources and Evaluation.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical
translation. In 16th International Conference on
Computational Linguistics, Copenhagen, Denmark.
Marion Weller, Max Kisselew, Svetlana Smekalova,
Alexander Fraser, Helmut Schmid, Nadir Durrani,
Hassan Sajjad, and Richa?rd Farkas. 2013. Munich-
Edinburgh-Stuttgart submissions at WMT13: Mor-
phological and syntactic processing for SMT. In
Proceedings of the Eighth Workshop on Statistical
Machine Translation, Sofia, Bulgaria.
224
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 232?239,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Munich-Edinburgh-Stuttgart Submissions at WMT13:
Morphological and Syntactic Processing for SMT
Marion Weller1, Max Kisselew1, Svetlana Smekalova1, Alexander Fraser2,
Helmut Schmid2, Nadir Durrani3, Hassan Sajjad4, Richa?rd Farkas5
1University of Stuttgart ? (wellermn|kisselmx|smekalsa)@ims.uni-stuttgart.de
2Ludwig-Maximilian University of Munich ? (schmid|fraser)@cis.uni-muenchen.de
3University of Edinburgh ? dnadir@inf.ed.ac.uk
4Qatar Computing Research Institute ? hsajjad@qf.org.qa
5University of Szeged ? rfarkas@inf.u-szeged.hu
Abstract
We present 5 systems of the Munich-
Edinburgh-Stuttgart1 joint submissions to
the 2013 SMT Shared Task: FR-EN, EN-
FR, RU-EN, DE-EN and EN-DE. The
first three systems employ inflectional gen-
eralization, while the latter two employ
parser-based reordering, and DE-EN per-
forms compound splitting. For our ex-
periments, we use standard phrase-based
Moses systems and operation sequence
models (OSM).
1 Introduction
Morphologically complex languages often lead to
data sparsity problems in statistical machine trans-
lation. For translation pairs with morphologically
rich source languages and English as target lan-
guage, we focus on simplifying the input language
in order to reduce the complexity of the translation
model. The pre-processing of the source-language
is language-specific, requiring morphological anal-
ysis (FR, RU) as well as sentence reordering (DE)
and dealing with compounds (DE). Due to time
constraints we did not deal with inflection for DE-
EN and EN-DE.
The morphological simplification process con-
sists in lemmatizing inflected word forms and deal-
ing with word formation (splitting portmanteau
prepositions or compounds). This needs to take
into account translation-relevant features (e.g. num-
ber) which vary across the different language pairs:
while French only has the features number and
gender, a wider array of features needs to be con-
sidered when modelling Russian (cf. table 6). In
addition to morphological reduction, we also apply
transliteration models learned from automatically
1The language pairs DE-EN and RU-EN were developed
in collaboration with the Qatar Computing Research Institute
and the University of Szeged.
mined transliterations to handle out-of-vocabulary
words (OOVs) when translating from Russian.
Replacing inflected word forms with simpler
variants (lemmas or the components of split com-
pounds) aims not only at reducing the general com-
plexity of the translation model, but also at decreas-
ing the amount of out-of-vocabulary words in the
input data. This is particularly the case with Ger-
man compounds, which are very productive and
thus often lack coverage in the parallel training
data, whereas the individual components can be
translated. Similarly, inflected word forms (e.g. ad-
jectives) benefit from the reduction to lemmas if
the full inflection paradigm does not occur in the
parallel training data.
For EN-FR, a translation pair with a morpho-
logically complex target language, we describe a
two-step translation system built on non-inflected
word stems with a post-processing component for
predicting morphological features and the genera-
tion of inflected forms. In addition to the advantage
of a more general translation model, this method
also allows the generation of inflected word forms
which do not occur in the training data.
2 Experimental setup
The translation experiments in this paper are car-
ried out with either a standard phrase-based Moses
system (DE-EN, EN-DE, EN-FR and FR-EN) or
with an operation sequence model (RU-EN, DE-
EN), cf. Durrani et al (2013b) for more details.
An operation sequence model (OSM) is a state-
of-the-art SMT-system that learns translation and
reordering patterns by representing a sentence pair
and its word alignment as a unique sequence of
operations (see e.g. Durrani et al (2011), Durrani
et al (2013a) for more details). For the Moses sys-
tems we used the old train-model perl scripts rather
than the EMS, so we did not perform Good-Turing
smoothing; parameter tuning was carried out with
batch-mira (Cherry and Foster, 2012).
232
1 Removal of empty lines
2 Conversion of HTML special characters like
&quot; to the corresponding characters
3 Unification of words that were written both
with an ? or with an oe to only one spelling
4 Punctuation normalization and tokenization
5 Putting together clitics and apostrophes like
l ? or d ? to l? and d?
Table 1: Text normalization for FR-EN.
Definite determiners la / l? / les ? le
Indefinite determiners un / une ? un
Adjectives Infl. form ? lemma
Portmanteaus e. g. au ? a` le
Verb participles Reduced to
inflected for gender non-inflected
and number verb participle form
ending in e?e/e?s/e?es ending in e?
Clitics and apostroph- d? ? de,
ized words are converted qu? ? que,
to their lemmas n? ? ne, ...
Table 2: Rules for morphological simplification.
The development data consists of the concate-
nated news-data sets from the years 2008-2011.
Unless otherwise stated, we use all constrained data
(parallel and monolingual). For the target-side lan-
guage models, we follow the approach of Schwenk
and Koehn (2008) and train a separate language
model for each corpus and then interpolate them
using weights optimized on development data.
3 French to English
French has a much richer morphology than English;
for example, adjectives in French are inflected with
respect to gender and number whereas adjectives
in English are not inflected at all. This causes data
sparsity in coverage of French inflected forms. We
try to overcome this problem by simplifying French
inflected forms in a pre-processing step in order to
adapt the French input better to the English output.
Processing of the training and test data The
pre-processing of the French input consists of two
steps: (1) normalizing not well-formed data (cf.
table 1) and (2) morphological simplification.
In the second step, the normalized training data
is annotated with Part-of-Speech tags (PoS-tags)
and word lemmas using RFTagger (Schmid and
Laws, 2008) which was trained on the French tree-
bank (Abeille? et al, 2003). French forms are then
simplified according to the rules given in table 2.
Data and experiments We trained a French to
English Moses system on the preprocessed and
System BLEU (cs) BLEU (ci)
Baseline 29.90 31.02
Simplified French* 29.70 30.83
Table 3: Results of the French to English system
(WMT-2012). The marked system (*) corresponds
to the system submitted for manual evaluation. (cs:
case-sensitive, ci: case-insensitive)
simplified constrained parallel data.
Due to tractability problems with word align-
ment, the 109 French-English corpus and the UN
corpus were filtered to a more manageable size.
The filtering criteria are sentence length (between
15 and 25 words), as well as strings indicating that
a sentence is neither French nor English, or other-
wise not well-formed, aiming to obtain a subset of
good-quality sentences. In total, we use 9M par-
allel sentences. For the English language model
we use large training data with 287.3M true-cased
sentences (including the LDC Giga-word data).
We compare two systems: a baseline with reg-
ular French text, and a system with the described
morphological simplifications. Results for the
WMT-2012 test set are shown in table 3. Even
though the baseline is better than the simplified
system in terms of BLEU, we assume that the trans-
lation model of the simplified system benefits from
the overall generalization ? thus, human annotators
might prefer the output of the simplified system.
For the WMT-2013 set, we obtain BLEU scores
of 29,97 (cs) and 31,05 (ci) with the system built
on simplified French (mes-simplifiedfrench).
4 English to French
Translating into a morphologically rich language
faces two problems: that of asymmetry of mor-
phological information contained in the source and
target language and that of data sparsity.
In this section we describe a two-step system de-
signed to overcome these types of problems: first,
the French data is reduced to non-inflected forms
(stems) with translation-relevant morphological fea-
tures, which is used to built the translation model.
The second step consists of predicting all neces-
sary morphological features for the translation out-
put, which are then used to generate fully inflected
forms. This two-step setup decreases the complex-
ity of the translation task by removing language-
specific features from the translation model. Fur-
thermore, generating inflected forms based on word
stems and morphological features allows to gener-
233
ate forms which do not occur in the parallel training
data ? this is not possible in a standard SMT setup.
The idea of separating the translation into two
steps to deal with complex morphology was in-
troduced by Toutanova et al (2008). Fraser et
al. (2012) applied this method to the language
pair English-German with an additional special
focus on word formation issues such as the split-
ting and merging of portmanteau prepositions and
compounds. The presented inflection prediction
systems focuses on nominal inflection; verbal in-
flection is not addressed.
Morphological analysis and resources The
morphological analysis of the French training data
is obtained using RFTagger, which is designed
for annotating fine-grained morphological tags
(Schmid and Laws, 2008). For generating inflected
forms based on stems and morphological features,
we use an extended version of the finite-state mor-
phology FRMOR (Zhou, 2007). Additionally, we
use a manually compiled list of abbreviations and
named entities (names of countries) and their re-
spective grammatical gender.
Stemming For building the SMT system, the
French data (parallel and monolingual) is trans-
formed into a stemmed representation. Nouns,
i.e. the heads of NPs or PPs, are marked with
inflection-relevant features: gender is considered
as part of the stem, whereas number is determined
by the source-side input: for example, we expect
source-language words in plural to be translated by
translated by stems with plural markup. This stem-
markup is necessary in order to guarantee that the
number information is not lost during translation.
For a better generalization, portmanteaus are split
into separate parts: au? a`+le (meaning, ?to the?).
Predicting morphological features For predict-
ing the morphological features of the SMT output
(number and gender), we use a linear chain CRF
(Lavergne et al, 2010) trained on data annotated
with these features using n-grams of stems and part-
of-speech tags within a window of 4 positions to
each side of the current word. Through the CRF,
the values specified in the stem-markup (number
and gender on nouns) are propagated over the rest
of the linguistic phrase, as shown in column 2 of
table 4. Based on the stems and the morphological
features, inflected forms can be generated using
FRMOR (column 3).
Post-processing As the French data has been
normalized, a post-processing step is needed in or-
der to generate correct French surface forms: split
portmanteaus are merged into their regular forms
based on a simple rule set. Furthermore, apostro-
phes are reintroduced for words like le, la, ne, ... if
they are followed by a vowel. Column 4 in table 4
shows post-processing including portmanteau for-
mation. Since we work on lowercased data, an
additional recasing step is required.
Experiments and evaluation We use the same
set of reduced parallel data as the FR-EN system;
the language model is built on 32M French sen-
tences. Results for the WMT-2012 test set are given
in table 5. Variant 1 shows the results for a small
system trained only on a part of the training data
(Europarl+News Commentary), whereas variant 2
corresponds to the submitted system. A small-scale
analysis indicated that the inflection prediction sys-
tem tends to have problems with subject-verb agree-
ment. We trained a factored system using addi-
tional PoS-tags with number information which
lead to a small improvement on both variants.
While the small model is significantly better than
the baseline2 as it benefits more from the general-
ization, the result for the full system is worse than
the baseline3. Here, given the large amount of
data, the generalization effect has less influence.
However, we assume that the more general model
from the inflection prediction system produces bet-
ter translations than a regular model containing a
large amount of irrelevant inflectional information,
particularly when considering that it can produce
well-formed inflected sequences that are inaccessi-
ble to the baseline. Even though this is not reflected
in terms of BLEU, humans might prefer the inflec-
tion prediction system.
For the WMT-2013 set, we obtain BLEU scores
of 29.6 (ci) and 28.30 (cs) with the inflection pre-
diction system mes-inflection (marked in table 5).
5 Russian-English
The preparation of the Russian data includes the
following stages: (1) tokenization and tagging and
(2) morphological reduction.
Tagging and tagging errors For tagging, we use
a version of RFTagger (Schmid and Laws, 2008)
2Pairwise bootstrap resampling with 1000 samples.
3However, the large inflection-prediction system has a
slightly better NIST score than the baseline (7.63 vs. 7.61).
234
SMT-output predicted generated after post- gloss
with stem-markup in bold print features forms processing
avertissement<Masc><Pl>[N] Masc.Pl avertissements avertissements warnings
sinistre[ADJ] Masc.Pl sinistres sinistres dire
de[P] ? de du from
le[ART] Masc.Sg le the
pentagone<Masc><Sg>[N] Masc.Sg pentagone pentagone pentagon
sur[P] ? sur sur over
de[P] ? de d? of
e?ventuel[ADJ] Fem.Pl e?ventuelles e?ventuelles potential
re?duction<Fem><Pl>[N] Fem.Pl re?ductions re?ductions reductions
de[P] ? de du of
le[ART] Masc.Sg le the
budget<Masc><Sg>[N] Masc.Sg budget budget budget
de[P] ? de de of
le[ART] Fem.Sg la la the
de?fense<Fem><Sg>[N] Fem.Sg de?fense de?fense de?fense
Table 4: Processing steps for the input sentence dire warnings from pentagon over potential defence cuts.
that has been developed based on data tagged with
TreeTagger (Schmid, 1994) using a model from
Sharoff et al (2008). The data processed by Tree-
Tagger contained errors such as wrong definition
of PoS for adverbs, wrong selection of gender for
adjectives in plural and missing features for pro-
nouns and adverbs. In order to train RFTagger, the
output of TreeTagger was corrected with a set of
empirical rules. In particular, the morphological
features of nominal phrases were made consistent
to train RFTagger: in contrast to TreeTagger, where
morphological features are regarded as part of the
PoS-tag, RFTagger allows for a separate handling
of morphological features and POS tags.
Despite a generally good tagging quality, some
errors seem to be unavoidable due to the ambiguity
of certain grammatical forms in Russian. A good
example of this are neuter nouns that have the same
form in all cases, or feminine nouns, which have
identical forms in singular genitive and plural nom-
inative (Sharoff et al, 2008). Since Russian has no
binding word order, and the case of nouns cannot
be determined on that basis, such errors cannot be
corrected with empirical rules implemented as post-
System BLEU (ci) BLEU (cs)
1 Baseline 24.91 23.40
InflPred 25.31 23.81
InflPred-factored 25.53 24.04
2 Baseline 29.32 27.65
InflPred* 29.07 27.40
InflPred-factored 29.17 27.46
Table 5: Results for French inflection prediction
on the WMT-2012 test set. The marked system (*)
corresponds to the system submitted for manual
evaluation.
processing. Similar errors occur when specifying
the case of adjectives, since the suffixes of adjec-
tives are even less varied as compared to the nouns.
In our application, we hope that this type of error
does not affect the result due to the following sup-
pression of a number of morphological attributes
including the case of adjectives.
Morphological reduction In comparison to
Slavic languages, English is morphologically poor.
For example, English has no morphological at-
tributes for nouns and adjectives to express gender
or case; verbs have no gender either. In contrast,
Russian is morphologically very rich ? there are
e.g. 6 cases and 3 grammatical genders, which
manifest themselves in different suffixes for nouns,
pronouns, adjectives and some verb forms. When
translating from Russian into English, many of
these attributes are (hopefully) redundant and are
therefore deleted from the training data. The mor-
phological reduction in our system was applied to
nouns, pronouns, verbs, adjectives, prepositions
and conjunctions. The rest of the POS (adverbs,
particles, interjections and abbreviations) have no
morphological attributes. The list of the original
and the reduced attributes is given in Table 6.
Transliteration mining to handle OOVs The
machine translation system fails to translate out-of-
vocabulary words (OOVs) as they are unknown to
the training data. Most of the OOVs are named en-
tities and transliterating them to the target language
script could solve this problem. The transliteration
system requires a list of transliteration pairs for
training. As we do not have such a list, we use
the unsupervised transliteration mining system of
Sajjad et al (2012) that takes a list of word pairs for
235
Part of Attributes Reduced
Speech RFTagger attributes
Noun Type Type
Gender Gender
Number Number
Case Case
nom,gen,dat,acc,instr,prep gen,notgen
Animate
Case 2
Pronoun Person Person
Gender Gender
Number Number
Case Case
nom,gen,dat,acc,instr,prep nom,notnom
Syntactic type
Animated
Verb Type Type
VForm VForm
Tense Tense
Person Person
Number Number
Gender
Voice Voice
Definiteness
Aspect Aspect
Case
Adjec- Type Type
tive Degree Degree
Gender
Number
Case
Definiteness
Prep- Type
osition Formation
Case
Conjunc- Type Type
tion Formation Formation
Table 6: Rules for simplifying the morphological
complexity for RU.
training and extracts transliteration pairs that can
be used for the training of the transliteration system.
The procedure of mining transliteration pairs and
transliterating OOVs is described as follows: We
word-align the parallel corpus using GIZA++ and
symmetrize the alignments using the grow-diag-
final-and heuristic. We extract all word pairs which
occur as 1-to-1 alignments (Sajjad et al, 2011) and
later refer to them as a list of word pairs. We train
the unsupervised transliteration mining system on
the list of word pairs and extract transliteration
pairs. We use these mined pairs to build a transliter-
ation system using the Moses toolkit. The translit-
eration system is applied as a post-processing step
to transliterate OOVs.
The morphological reduction of Russian (cf. sec-
tion 5) does not process most of the OOVs as they
are also unknown to the POS tagger. So OOVs that
we get are in their original form. When translit-
Original corpus
SYS WMT-2012 WMT-2013
GIZA++ 32.51 25.5
TA-GIZA++ 33.40 25.9*
Morph-reduced
SYS WMT-2012 WMT-2013
GIZA++ 31.22 24.3
TA-GIZA++ 31.40 24.45
Table 7: Russian to English machine translation
system evaluated on WMT-2012 and WMT-2013.
Human evaluation in WMT13 is performed on the
system trained using the original corpus with TA-
GIZA++ for alignment (marked with *).
erating them, the inflected forms generate wrong
English transliterations as inflectional suffixes get
transliterated too, specially OOV named entities.
We solved this problem by stemming the OOVs
based on a list of suffixes ( , , , , , ) and
transliterating the stemmed forms.
Experiments and results We trained the sys-
tems separately on GIZA++ and transliteration
augmented-GIZA++ (TA-GIZA++) to compare
their results; for more details see Sajjad et al
(2013). All systems are tuned using PROv1 (Nakov
et al, 2012). The translation output is post-
processed to transliterate OOVs.
Table 7 summarizes the results of RU-EN trans-
lation systems trained on the original corpus and
on the morph-reduced corpus. Using TA-GIZA++
alignment gives the best results for both WMT-
2012 and WMT-2013, leading to an improvement
of 0.4 BLEU points.
The system built on the morph-reduced data
leads to decreased BLEU results. However, the per-
centage of OOVs is reduced for both test sets when
using the morph-reduced data set compared to the
original data. An analysis of the output showed
that the morph-reduced system makes mistakes in
choosing the right tense of the verb, which might
be one reason for this outcome. In the future, we
would like to investigate this issue in detail.
6 German to English and English to
German
We submitted systems for DE-EN and EN-DE
which used constituent parses for pre-reordering.
For DE-EN we also deal with word formation is-
sues such as compound splitting. We did not per-
form inflectional normalization or generation for
German due to time constraints, instead focusing
236
our efforts on these issues for French and Russian
as previously described.
German to English German has a wider diver-
sity of clausal orderings than English, all of which
need to be mapped to the English SVO order. This
is a difficult problem to solve during inference, as
shown for hierarchical SMT by Fabienne Braune
and Fraser (2012) and for phrase-based SMT by
Bisazza and Federico (2012).
We syntactically parsed all of the source side
sentences of the parallel German to English data
available, and the tuning, test and blindtest sets.
We then applied reordering rules to these parses.
We use the rules for reordering German constituent
parses of Collins et al (2005) together with the
additional rules described by Fraser (2009). These
are applied as a preprocess to all German data.
For parsing the German sentences, we used the
generative phrase-structure parser BitPar with opti-
mizations of the grammar, as described by Fraser
et al (2013). The parser was trained on the Tiger
Treebank (Brants et al, 2002) along with utilizing
the Europarl corpus as unlabeled data. At the train-
ing of Bitpar, we followed the targeted self-training
approach (Katz-Brown et al, 2011) as follows. We
parsed the whole Europarl corpus using a grammar
trained on the Tiger corpus and extracted the 100-
best parse trees for each sentence. We selected the
parse tree among the 100 candidates which got the
highest usefulness scores for the reordering task.
Then we trained a new grammar on the concatena-
tion of the Tiger corpus and the automatic parses
from Europarl.
The usefulness score estimates the value of a
parse tree for the reordering task. We calculated
this score as the similarity between the word order
achieved by applying the parse tree-based reorder-
ing rules of Fraser (2009) and the word order indi-
cated by the automatic word alignment between
the German and English sentences in Europarl.
We used the Kendall?s Tau Distance as the simi-
larity metric of two word orderings (as suggested
by Birch and Osborne (2010)).
Following this, we performed linguistically-
informed compound splitting, using the system of
Fritzinger and Fraser (2010), which disambiguates
competing analyses from the high-recall Stuttgart
Morphological Analyzer SMOR (Schmid et al,
2004) using corpus statistics. We also split German
portmanteaus like zum? zu dem (meaning to the).
system BLEU BLEU system name
(ci) (cs)
DE-EN (OSM) 27.60 26.12 MES
DE-EN (OSM) 27.48 25.99 not submitted
BitPar not self-trained
DE-EN (Moses) 27.14 25.65 MES-Szeged-
reorder-split
DE-EN (Moses) 26.82 25.36 not submitted
BitPar not self-trained
EN-DE (Moses) 19.68 18.97 MES-reorder
Table 8: Results on WMT-2013 (blindtest)
English to German The task of mapping En-
glish SVO order to the different clausal orders in
German is difficult. For our English to German
systems, we solved this by parsing the English and
applying the system of Gojun and Fraser (2012) to
reorder English into the correct German clausal or-
der (depending on the clause type which is detected
using the English parse, see (Gojun and Fraser,
2012) for further details).
We primarily used the Charniak-Johnson gener-
ative parser (Charniak and Johnson, 2005) to parse
the English Europarl data and the test data. How-
ever, due to time constraints we additionally used
Berkeley parses of about 400K Europarl sentences
and the other English parallel training data. We
also left a small amount of the English parallel
training data unparsed, which means that it was
not reordered. For tune, test and blindtest (WMT-
2013), we used the Charniak-Johnson generative
parser.
Experiments and results We used all available
training data for constrained systems; results for
the WMT-2013 set are given in table 8. For the
contrastive BitPar results, we reparsed WMT-2013.
7 Conclusion
We presented 5 systems dealing with complex mor-
phology. For two language pairs with a morpho-
logically rich source language (FR and RU), the
input was reduced to a simplified representation
containing only translation-relevant morphologi-
cal information (e.g. number on nouns). We also
used reordering techniques for DE-EN and EN-DE.
For translating into a language with rich morphol-
ogy (EN-FR), we applied a two-step method that
first translates into a stemmed representation of
the target language and then generates inflected
forms based on morphological features predicted
on monolingual data.
237
Acknowledgments
We would like to thank the anonymous reviewers
for their helpful feedback and suggestions, Daniel
Quernheim for providing Berkeley parses of some
of the English data, Stefan Ru?d for help with the
manual evalution, and Philipp Koehn and Barry
Haddow for providing data and alignments.
Nadir Durrani was funded by the European
Union Seventh Framework Programme (FP7/2007-
2013) under grant agreement n. 287658. Alexan-
der Fraser was funded by Deutsche Forschungs-
gemeinschaft grant Models of Morphosyntax for
Statistical Machine Translation and from the Eu-
ropean Community?s Seventh Framework Pro-
gramme (FP7/2007-2013) under Grant Agreement
n. 248005. Marion Weller was funded from the
European Community?s Seventh Framework Pro-
gramme (FP7/2007-2013) under Grant Agreement
n. 248005. Svetlana Smekalova was funded by
Deutsche Forschungsgemeinschaft grant Models
of Morphosyntax for Statistical Machine Trans-
lation. Helmut Schmid and Max Kisselew were
supported by Deutsche Forschungsgemeinschaft
grant SFB 732. Richa?rd Farkas was supported by
the European Union and the European Social Fund
through project FuturICT.hu (grant n. TA?MOP-
4.2.2.C-11/1/KONV-2012-0013). This publication
only reflects the authors? views.
References
A. Abeille?, L. Cle?ment, and F. Toussenel. 2003. Build-
ing a treebank for french. In A. Abeille?, editor, Tree-
banks. Kluwer, Dordrecht.
Alexandra Birch and Miles Osborne. 2010. Lrscore for
evaluating lexical and reordering quality in mt. In
Proceedings of ACL WMT and MetricsMATR, Upp-
sala, Sweden.
Arianna Bisazza and Marcello Federico. 2012. Mod-
ified distortion matrices for phrase-based statistical
machine translation. In ACL, pages 478?487.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In ACL, pages 173?180, Ann Arbor, MI,
June. Association for Computational Linguistics.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL).
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Porceedings of ACL 2005.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A Joint Sequence Translation Model with In-
tegrated Reordering. In Proceedings of ACL-HLT
2011, Portland, Oregon, USA.
Nadir Durrani, Alexander Fraser, and Helmut Schmid.
2013a. Model With Minimal Translation Units, But
Decode With Phrases. In Proceedings of NAACL
2013, Atlanta, Georgia, USA.
Nadir Durrani, Helmut Schmid, Alexander Fraser, Has-
san Sajjad, and Richa?rd Farkas. 2013b. Munich-
Edinburgh-Stuttgart Submissions of OSM Systems
at WMT13. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, Sofia, Bulgaria.
Anita Gojun Fabienne Braune and Alexander Fraser.
2012. Long-distance reordering during search for
hierarchical phrase-based SMT. In Proceedings of
EAMT 2012.
Alexander Fraser, Marion Weller, Aoife Cahill, and Fa-
bienne Cap. 2012. Modeling Inflection and Word-
Formation in SMT. In Proceedings of EACL 2012,
Avignon, France.
Alexander Fraser, Helmut Schmid, Richa?rd Farkas,
Renjing Wang, and Hinrich Schu?tze. 2013. Knowl-
edge sources for constituent parsing of German, a
morphologically rich and less-configurational lan-
guage. Computational Linguistics - to appear.
Alexander Fraser. 2009. Experiments in morphosyn-
tactic processing for translating to and from German.
In EACL WMT.
Fabienne Fritzinger and Alexander Fraser. 2010. How
to avoid burning ducks: Combining linguistic analy-
sis and corpus statistics for German compound pro-
cessing. In ACL WMT and Metrics MATR.
Anita Gojun and Alexander Fraser. 2012. Determin-
ing the placement of German verbs in English-to-
German SMT. In Proceedings of EACL 2012.
Jason Katz-Brown, Slav Petrov, Ryan McDon-
ald, Franz Och, David Talbot, Hiroshi Ichikawa,
Masakazu Seno, and Hideto Kazawa. 2011. Train-
ing a parser for machine translation reordering. In
Proceedings of EMNLP 2011, Edinburgh, Scotland.
Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.
2010. Practical very large scale CRFs. In Proceed-
ings of ACL 2010, pages 504?513.
Preslav Nakov, Francisco Guzma?n, and Stephan Vo-
gel. 2012. Optimizing for sentence-level BLEU+1
yields short translations. Mumbai, India.
238
Hassan Sajjad, Alexander Fraser, and Helmut Schmid.
2011. An algorithm for unsupervised transliteration
mining with an application to word alignment. In
Proceedings of ACL 2011, Portland, USA.
Hassan Sajjad, Alexander Fraser, and Helmut Schmid.
2012. A statistical model for unsupervised and semi-
supervised transliteration mining. In Proceedings of
ACL 2012, Jeju, Korea.
Hassan Sajjad, Svetlana Smekalova, Nadir Durrani,
Alexander Fraser, and Helmut Schmid. 2013.
QCRI-MES Submission at WMT13: Using Translit-
eration Mining to Improve Statistical Machine
Translation. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, Sofia, Bulgaria.
Helmut Schmid and Florian Laws. 2008. Estimation
of conditional probabilities with decision trees and
an application to fine-grained pos tagging. In Pro-
ceedings of COLING 2008, Stroudsburg, PA, USA.
Helmut Schmid, Arne Fitschen, and Ulrich Heid. 2004.
SMOR: a German Computational Morphology Cov-
ering Derivation, Composition, and Inflection. In
Proceedings of LREC 2004.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing.
Holger Schwenk and Philipp Koehn. 2008. Large
and diverse language models for statistical machine
translation. In Proceedings of IJCNLP 2008.
Serge Sharoff, Mikhail Kopotev, Tomaz Erjavec, Anna
Feldman, and Dagmar Divjak. 2008. Designing and
evaluating russian tagsets. In Proceedings of LREC
2008.
Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.
2008. Applying Morphology Generation Models to
Machine Translation. In Proceedings of ACL-HLT
2008.
Zhenxia Zhou. 2007. Entwicklung einer franzo?sischen
Finite-State-Morphologie. Diploma Thesis, Insti-
tute for Natural Language Processing, University of
Stuttgart.
239
