	 
 
  	 
 	
 	 	   	
	 
 
	 


  
 



  Generation of Relative Referring Expressions based on Perceptual Grouping
Kotaro FUNAKOSHI
Department of Computer Science
Tokyo Institute of Technology
Meguro O?okayama 2-12-1,
Tokyo 152-8552, Japan
koh@cl.cs.titech.ac.jp
Satoru WATANABE
Department of Computer Science
Tokyo Institute of Technology
Meguro O?okayama 2-12-1,
Tokyo 152-8552, Japan
satoru w@cl.cs.titech.ac.jp
Naoko KURIYAMA
Department of Human System Science
Tokyo Institute of Technology
Meguro O?okayama 2-12-1,
Tokyo 152-8552, Japan
kuriyama@hum.titech.ac.jp
Takenobu TOKUNAGA
Department of Computer Science
Tokyo Institute of Technology
Meguro O?okayama 2-12-1,
Tokyo 152-8552, Japan
take@cl.cs.titech.ac.jp
Abstract
Past work of generating referring expressions
mainly utilized attributes of objects and bi-
nary relations between objects. However, such
an approach does not work well when there
is no distinctive attribute among objects. To
overcome this limitation, this paper proposes a
method utilizing the perceptual groups of ob-
jects and n-ary relations among them. The key
is to identify groups of objects that are natu-
rally recognized by humans. We conducted psy-
chological experiments with 42 subjects to col-
lect referring expressions in such situations, and
built a generation algorithm based on the re-
sults. The evaluation using another 23 subjects
showed that the proposed method could effec-
tively generate proper referring expressions.
1 Introduction
In the last two decades, many researchers have stud-
ied the generation of referring expressions to enable
computers to communicate with humans about con-
crete objects in the world.
For that purpose, most past work (Appelt, 1985;
Dale and Haddock, 1991; Dale, 1992; Dale and
Reiter, 1995; Heeman and Hirst, 1995; Horacek,
1997; Krahmer and Theune, 2002; van Deemter,
2002; Krahmer et al, 2003) makes use of attributes
of an intended object (the target) and binary rela-
tions between the target and others (distractors) to
distinguish the target from distractors. Therefore,
these methods cannot generate proper referring ex-
pressions in situations where no significant surface
difference exists between the target and distractors,
and no binary relation is useful to distinguish the
target. Here, a proper referring expression means
a concise and natural linguistic expression enabling
hearers to distinguish the target from distractors.
For example, consider indicating object b to per-
son P in the situation shown in Figure 1. Note that
person P does not share the label information such
as a and b with the speaker. Because object b is
not distinguishable from objects a or c by means of
their appearance, one would try to use a binary re-
lation between object b and the table, i.e., ?A ball
to the right of the table?. 1 However, ?to the right
of ? is not a discriminatory relation, for objects a
and c are also located to the right of the table. Us-
ing a and c as a reference object instead of the ta-
ble does not make sense, since a and c cannot be
uniquely identified because of the same reason that
b cannot be identified. Such situations have never
drawn much attention, but can occur easily and fre-
quently in some domains such as object arrange-
ment (Tanaka et al, 2004).
van der Sluis and Krahmer (2000) proposed us-
ing gestures such as pointing in situations like those
shown in Figure 1. However, pointing and gazing
are not always available depending on the positional
relation between the speaker and the hearer.
In the situation shown in Figure 1, a speaker can
indicate object b to person P with a simple expres-
sion ?the front ball? without using any gesture. In
order to generate such an expression, one must be
able to recognize the salient perceptual group of the
objects and use the n-ary relative relations in the
group. 2
In this paper, we propose a method of generat-
1In this paper, we simply assume that all participants share
the appropriate reference frame (Levinson, 2003). We mention
this issue in the last section.
2Although Krahmer et al claim that their method can han-
dle n-ary relations (Krahmer et al, 2003), they provide no de-
tails. We think their method cannot directly handle situations
we discuss here.
ing referring expressions that utilizes n-ary relations
among members of a group. Our method recognizes
groups by using Tho?risson?s algorithm (Tho?risson,
1994). As the first step of our research project, we
deal with the limited situations where only homoge-
neous objects are randomly arranged (see Figure 2).
Therefore, we handle positional n-ary relation only,
and other types of n-ary relation such as size, e.g.,
?the biggest one?, are not mentioned.
Speakers often refer to multiple groups in the
course of referring to the target. In these cases, we
can observe two types of relations: the intra-group
relation such as ?the front two among the five near
the desk?, and the inter-group relation such as ?the
two to the right of the five?. We define that a sub-
sumption relation between two groups is an intra-
group relation.
In what follows, Section 2 explains the exper-
iments conducted to collect expressions in which
perceptual groups are used. The proposed method is
described and evaluated in Section 3. In Section 4,
we examine a possibility to predict the adequacy of
an expression in terms of perceptual grouping. Fi-
nally, we conclude the paper in Section 5.
P
a
b
c
Table
Figure 1: An example of problematic situations
2 Data Collection
We conducted a psychological experiment with 42
Japanese undergraduate students to collect referring
expressions in which perceptual groups are used. In
order to evaluate the collected expressions, we con-
ducted another experiment with a different group of
44 Japanese undergraduate students. There is no
overlap between the subjects of those two experi-
ments. Details of this experiment are described in
the following subsections.
2.1 Collecting Referring Expressions
Method Subjects were presented 2-dimensional
bird?s-eye images in which several objects of the
same color and the same size were arranged and the
subjects were requested to convey a target object to
the third person drawn in the same image. We used
12 images of arrangements. In each image, three
to nine objects were arranged manually so that the
objects distributes non-uniformly. An example of
images presented to subjects is shown in Figure 2.
Labels a, . . . , f, x in the image are assigned for pur-
poses of illustration and are not assigned in the ac-
tual images presented to the subjects. Each subject
was asked to describe a command so that the person
in the image picks a target object that is enclosed
with dotted lines. When a subject could not think
of a proper expression, she/he was allowed to aban-
don that arrangement and proceed to the next one.
Referring expressions designating the target object
were collected from these subjects? commands.
P
a
b
e
f
c d
x
Figure 2: A visual stimulus of the experiment
Analysis We presented 12 arrangements to 42
subjects and obtained 476 referring expressions.
Twenty eight judgments were abandoned in the ex-
periment. Observing the collected expressions, we
found that starting from a group with all of the ob-
jects, subjects generally narrow down the group to
a singleton group that has the target object. There-
fore, a referring expression can be formalized as a
sequence of groups (SOG) reflecting the subject?s
narrowing down process.
The following example shows an observed ex-
pression describing the target x in Figure 2 with the
corresponding SOG representation below it.
?hidari oku ni aru mittu no tama no uti no
itiban migi no tama.?
(the rightmost ball among the three balls
at the back left)
SOG:[{a, b, c, d, e, f, x}, {a, b, x}, {x}] 3
where
{a, b, c, d, e, f, x} denotes all objects in
the image (total set),
{a, b, x} denotes the three objects at the
back left, and
{x} denotes the target.
3We denote an SOG representation by enclosing groups
with square brackets.
Since narrowing down starts from the total set,
the SOG representation starts with a set of all ob-
jects and ends with a singleton group with the tar-
get. Translating the collected referring expressions
into the SOG representation enables us to abstract
and classify the expressions. On average, we ob-
tained about 40 expressions for each arrangement,
and classified them into 8.4 different SOG represen-
tations.
Although there are two types of relations be-
tween groups as we mentioned in Section 1, the ex-
pressions using only intra-group relations made up
about 70% of the total.
2.2 Evaluating the Collected Expressions
Method Subjects were presented expressions col-
lected in the experiment described in Section 2.1 to-
gether with the corresponding images, and were re-
quested to indicate objects referred to by the expres-
sions. The presented images are the same as those
used in the previous experiment except that there are
no marks on the targets. At the same time, subjects
were requested to express their confidence in select-
ing the target, and evaluate the conciseness, and the
naturalness of the given expressions on a scale of 1
to 8.
Because the number of expressions that we could
evaluate with subjects was limited, we chose a max-
imum of 10 frequent expressions for each arrange-
ment. The expressions were chosen so that as many
different SOG representations were included as pos-
sible. If an arrangement had SOGs less than 10,
several expressions that had the same SOG but dif-
ferent surface realizations were chosen. The resul-
tant 117 expressions were evaluated by 49 subjects.
Each subject evaluated about 29.5 expressions.
Analysis Discarding incomplete answers, we ob-
tained 1,429 evaluations in total. 12.2 evaluations
were obtained for each expression on average.
We measured the quality of each expression in
terms of an evaluation value that is defined in (1).
This measure is used to analyze what kind of ex-
pressions are preferred and to set up a scoring func-
tion (6) for machine-generated expressions as de-
scribed in Section 3.1.
(evaluation value)
= (accuracy)? (confidence)
?
(naturalness) + (conciseness)
2
(1)
According to our analysis, the expressions with
only intra-group relations (84 samples) obtained
high accuracies (Ave. 79.3%) and high evaluation
values (Ave. 33.1), while the expressions with inter-
group relations (33 samples) obtained lower accura-
cies (Ave. 69.1%) and lower evaluation values (Ave.
19.7).
The expressions with only intra-group relations
are observed more than double as many as the ex-
pressions with inter-group relations. We provide a
couple of example expressions indicating object x
in Figure 2 to contrast those two types of expres-
sions below.
? without inter-group relations
? ?the rightmost ball among the three balls
at the back left?
? with inter-group relations
? ?the ball behind the two front balls?
In addition, expressions explicitly mentioning all
the objects obtained lower evaluation values. Con-
sidering these observations, we built a generation
algorithm using only intra-group relations and did
not mention all the objects explicitly.
Among these expressions, we selected those with
which the subjects successfully identified the target
with more than 90% accuracy. These expressions
are used to extract parameters of our generation al-
gorithm in the following sections.
3 Generating Referring Expressions
3.1 Generation Algorithm
Given an arrangement of objects and a target, our al-
gorithm generates referring expressions by the fol-
lowing three steps:
Step 1: enumerate perceptual groups based on the
proximity between objects
Step 2: generate the SOG representations by com-
bining the groups
Step 3: translate the SOG representations into lin-
guistic expressions
In the rest of this section, we illustrate how these
three steps generate referring expressions in the sit-
uation shown in Figure 2.
Step 1: Enumerating Perceptual Groups.
To generate perceptual groups from an arrangement,
Tho?risson?s algorithm (Tho?risson, 1994) is adopted.
Given a list of objects in an arrangement, the al-
gorithm generates groups based on the proximity of
the objects and returns a list of groups. Only groups
containing the target, that is x, are chosen because
SOG: [{a, b, c, d, e, f, x}, {a, b, x}, {x}]
? E(R({a, b, c, d, e, f, x}, {a, b, x})) + E({a, b, x}) + E(R({a, b, x}, {x})) + E({x})
? ?hidari oku no?+?mittu no tama?+?no uti no migihasi no?+?tama?
(at the back left) (three balls) (rightmost . . . among) (ball)
Figure 3: An example of surface realization
we handle intra-group relations only as mentioned
before, and that implies that all groups mentioned
in an expression must include the target. Then, the
groups are sorted in descending order of the group
size. Finally a singleton group consisting of the tar-
get is added to the end of the list if such a group is
missing in the list. The resultant group list, GL, is
the output of Step 1.
For example, the algorithm recognizes the fol-
lowing groups given the arrangement shown in Fig-
ure 2:
{{a, b, c, d, e, f, x}, {a, b, c, d, x},
{a, b, x}, {c, d}, {e, f}}.
After filtering out the groups without the target and
adding a singleton group with the target, we obtain
the following list:
{{a, b, c, d, e, f, x}, {a, b, c, d, x}, {a, b, x}, {x}}.
(2)
Step 2: Generating the SOG Representations.
In this step, the SOG representations introduced in
Section 2 are generated from the GL of Step 1,
which generally has a form like (3), where G
i
de-
notes a group, and G
0
is a group of all the objects.
Here, we narrow down the objects starting from the
total set (G
0
) to the target ({x}).
[G
0
, G
1
, . . . , G
m?2
, {x}] (3)
Given a group list GL, all possible SOGs are gen-
erated. From a group list of size m, 2m?2 SOG
representations can be generated since G
0
and {x}
should be included in the SOG representation. For
example, from a group list of {G
0
, G
1
, G
2
, {x}},
we obtain four SOGs: [G
0
, {x}], [G
0
, G
1
, {x}],
[G
0
, G
2
, {x}], and [G
0
, G
1
, G
2
, {x}].
For example, one of the SOG representations
generated from list (2) is
[{a, b, c, d, e, f, x}, {a, b, x}, {x}]. (4)
Note that any two groups G
i
and G
j
in a list of
groups generated by Tho?risson?s algorithm with re-
gard to one feature, e.g., proximity in this paper, are
mutually disjoint (G
i
?G
j
= ?), otherwise one sub-
sumes the other (G
i
? G
j
or G
j
? G
i
). No inter-
secting groups without a subsumption relation are
generated.
Step 3: Generating Linguistic Expressions.
In the last step, the SOG representations are trans-
lated into linguistic expressions. Since Japanese is
a head-final language, the order of linguistic ex-
pressions for groups are retained in the final lin-
guistic expression for the SOG representation. That
is, an SOG representation [G
0
, G
1
, . . . , G
n?2
, {x}]
can be realized as shown in (5), where E(X) de-
notes a linguistic expression for X, R(X,Y ) de-
notes a relation between X and Y , and ?+? is a
string concatenation operator.
E(G
0
) + E(R(G
0
, G
1
)) + E(G
1
) + . . .
+E(R(G
n?2
, {x})) + E({x}) (5)
As described in Section 2.2, expressions that ex-
plicitly mention all the objects obtain lower evalu-
ation values, and expressions using intra-group re-
lations obtain high evaluation values. Considering
these observations, our algorithm does not use the
linguistic expression corresponding to all the ob-
jects, that is E(G
0
), and only uses intra-group re-
lations for R(X,Y ).
Possible expressions of X are collected from the
experimental data in Section 2.1, and the first ap-
plicable expression is selected when realizing a lin-
guistic expression for X, i.e., E(X). Therefore,
this algorithm produces one linguistic expression
for each SOG even though there are some other pos-
sible expressions.
For example, the SOG representation (4) is real-
ized as shown in Figure 3.
Note that there is no mention of all the objects,
{a, b, c, d, e, f, x}, in the linguistic expression.
3.2 Evaluation of Generated Expressions
We implemented the algorithm described in Sec-
tion 3.1, and evaluated the output with 23 under-
graduate students. The subjects were different from
those of the previous experiments but were of the
same age group, and the experimental environment
Accuracy (%) Naturalness Conciseness Confidence Eval. val.
Human-12-all 87.3 4.82 5.27 6.14 29.3
Human-12-90 97.9 5.20 5.62 6.50 35.0
Human-12-100 100 5.36 5.73 6.65 37.2
System-12 91.0 5.60 6.25 6.32 40.1
System-20 88.4 5.09 5.65 6.25 35.2
System-Average 89.2 5.24 5.82 6.27 36.6
Table 1: Summary of evaluation
was the same. The evaluation of the output was per-
formed in the same manner as that of Section 2.2.
The results are shown in Table 1. ?Human-
12-all? shows the average values of all expres-
sions collected from humans with 12 arrangements
as described in Section 2.2. ?Human-12-90? and
?Human-12-100? show the average values of ex-
pressions by humans that gained more than 90%
and 100% in accuracy in the same evaluation ex-
periment respectively.
?System-12? shows the average values of expres-
sions generated by the algorithm for the 12 arrange-
ments used in the data collection experiment de-
scribed in Section 2.1. The algorithm generated 18
expressions for the 12 arrangements, which were
presented to each subject in random order for eval-
uation.
?System-20? shows the average values of expres-
sions generated by the algorithm for 20 randomly
generated arrangements that generate at least two
linguistic expressions each. The algorithm gen-
erated 48 expressions for these 20 arrangements,
which were evaluated in the same manner as that
of ?System-12?.
?System-Average? shows the micro average of
expressions of both ?System-12? and ?System-20?.
?Accuracy? shows the rates at which the sub-
jects could identify the correct target objects from
the given expressions. Comparing the accuracies of
?Human-12-*? and ?System-12?, we find that the
algorithm generates good expressions. Moreover,
the algorithm is superior to human in terms of ?Nat-
uralness? and ?Conciseness?. However, this result
should be interpreted carefully. Further investiga-
tion of the expressions revealed that humans often
sacrificed naturalness and conciseness in order to
describe the target as precisely as possible for com-
plex arrangements.
4 Scoring SOG Representations
The algorithm presented in the previous section out-
puts several possible expressions. Therefore, we
have to choose one of the expressions by calculat-
ing their scores.
The scores can be computed using various mea-
sures, such as complexity of expressions, and
salience of referent objects. In this section, we in-
vestigate whether the adequacies of the courses of
narrowing down can be predicted: that is, whether
meaningful scores of SOG representations can be
calculated.
4.1 Method for SOG Scoring
An SOG representation has a form as stated in (3).
We presumed that, when a speaker tries to narrow
down an object group from G
i
to G
i+1
, there is
an optimal ratio between the dimensions of G
i
and
G
i+1
. In other words, narrowing down a group from
a very big one to a very small one might cause hear-
ers to become confused.
For example, consider the following two expres-
sions that both indicate object x in Figure 2. Hearers
would prefer (i) to (ii) though (ii) is simpler than (i).
(i) ?the rightmost ball among the three balls at the
back left?
(ii) ?the fourth ball from the right?
In fact, we found (i) among the expressions col-
lected in Section 2.1, but did not find (ii) among
them. Our algorithm generated both (i) and (ii)
in Section 3.2, and the two expressions gained the
evaluation values of 44.4 and 32.1 respectively.
If our presumption is correct, we can expect
to choose better expressions by choosing expres-
sions that have adequate dimension ratios between
groups.
Calculation Formula
The total score of an SOG representation is calcu-
lated by averaging the scores given by functions f
1
and f
2
whose parameters are dimension ratios be-
tween two consecutive groups as given in (6), where
n is the number of groups in the SOG.
score(SOG) = 1
n ? 1
{
n?3
?
i=0
f
1
(
dim(G
i+1
)
dim(G
i
)
)
+ f
2
(
dim({x})
dim(G
n?2
)
)
} (6)
The dimension of a group dim is defined as the
average distance between the centroid of the group
and that of each object. The dimension of the sin-
gleton group {x} is defined as a constant value. Be-
cause of this idiosyncrasy of the singleton group
{x} compared to other groups, f
2
was introduced
separately from f
1
even though both functions rep-
resent the same concept, as described below.
The optimal ratio between two groups, and that
from a group to the target were found through the
quadratic regression analysis of data collected in the
experiment described in Section 2.2. f
1
and f
2
are
the two regression curves found through analysis
representing correlations between dimension ratios
and values calculated based on human evaluation as
in formula (1).We could not find direct correlations
between dimension ratios and accuracies.
4.2 Results
We checked to what extent the scores of generated
expressions given by formula (6) conformed with
the human evaluation given by formula (1) as agree-
ment. Agreement was calculated as follows using
20 randomly generated arrangements described in
Section 3.2.
First, the generated expressions were ordered ac-
cording to the score given by formula (6) and the
human evaluation given by formula (1). All binary
order relations between two expressions were ex-
tracted from these two ordered lists of expressions.
The agreement was defined as the ratio of the same
binary order relations among all binary order rela-
tions.
The agreement between scores and the human
evaluation was 45.8%. The score did not predict
SOG representations that would generate better ex-
pressions very well. Further research is required to
conclusively rule out the use of dimension ratios for
prediction or whether other factors are involved.
5 Concluding Remarks and Future Work
This paper proposed an algorithm that generates re-
ferring expressions using perceptual groups and n-
ary relations among them. The algorithm was built
on the basis of the analysis of expressions that were
collected through psychological experiments. The
performance of the algorithm was evaluated by 23
subjects and it generated promising results.
In the following, we look at future work to be
done.
Recognizing salient geometric formations:
Tho?risson?s algorithm (Tho?risson, 1994) cannot
recognize a linear arrangement of objects as a
group, although such arrangements are quite salient
for humans. This is one of the reasons for the
disconformity between the evaluations given by the
algorithm and those of the humans subjects.
We can enumerate most of such geometric ar-
rangements salient for human subject by referring to
geometric terms found in lexicons and thesauri such
as ?line?, ?circle?, ?square? and so on. Tho?risson?s
algorithm should be extended to recognize these ar-
rangements.
Using relations other than positional relations:
In this paper, we focused on positional relations of
perceptual groups. Other relations such as degree of
color and size should be treated in the same manner.
Tho?risson?s original algorithm (Tho?risson, 1994)
takes into account these relations as well as posi-
tional relations of objects when calculating similar-
ity between objects to generate groups. However, if
we generate groups using multiple relations simul-
taneously, the assumption used in Step 1 of our al-
gorithm that any pair of groups in an output list do
not intersect without a subsumption relation cannot
be held. Therefore, the mechanism generating SOG
representations (Step 2 in Section 3.1) must be re-
considered.
Resolving reference frames and differences of
perspective: We assumed that all participants in
a conversation shared the same reference frame.
However, when we apply our method to conversa-
tional agent systems, e.g., (Cavazza et al, 2002;
Tanaka et al, 2004), reference frames must be prop-
erly determined each time to generate referring ex-
pressions. Although there are many studies con-
cerning reference frames, e.g., (Clark, 1973; Her-
skovits, 1986; Levinson, 2003), little attention has
been paid to how reference frames are determined in
terms of the perceptual groups and their elements.
In addition to reference frames, differences of
perspective also have to be taken into account to
produce proper referring expressions since humans
often view spatial relations between objects in a
3-dimensional space by projecting them on a 2-
dimensional plane. In the experiments, we pre-
sented the subjects with 2-dimensional bird?s-eye
images. The result might have been different if we
had used 3-dimensional images instead, because the
projection changes the sizes of objects and spatial
relations among them.
Integration with conventional methods: In this
paper, we focused on a limited situation where in-
herent attributes of objects do not serve any identi-
fying function, but this is not the case in general. An
algorithm integrating conventional attribute-based
methods and the proposed method should be formu-
lated to achieve the end goal.
A possible direction would be to enhance the al-
gorithm proposed by Krahmer et al (Krahmer et
al., 2003). They formalize an object arrangement
(scene) as a labeled directed graph in which ver-
tices model objects and edges model attributes and
binary relations, and regard content selection as a
subgraph construction problem. Their algorithm
performs searches directed by a cost function on a
graph to find a unique subgraph.
If we consider a perceptual group as an ordinary
object as shown in Figure 4, their algorithm is appli-
cable. It will be able to handle not only intra-group
relations (e.g., the edges with labels ?front?, ?mid-
dle?, and ?back? in Figure 4) but also inter-group re-
lations (e.g., the edge from ?Group 1? to ?Table? in
Figure 4). However, introducing perceptual groups
as vertices makes it difficult to design the cost func-
tion. A well-designed cost function is indispensable
for generating concise and comprehensible expres-
sions. Otherwise, an expression like ?a ball in front
of a ball in front of a ball? for the situation shown in
Figure 1 would be generated.
Group 1
b c a
Table
front_of front_of right_of
back_of back_of left_of
front
middle back
right_of
right_of
right_of
Figure 4: A simplified graph with a group vertex for
the situation shown in Figure 1
References
Douglas E. Appelt. 1985. Planning English refer-
ring expressions. Artificial Intelligence, 26:1?33.
Mark Cavazza, Fred Charles, and Steven J. Mead.
2002. Character-based interactive stroytelling.
IEEE Intelligent Systems, 17(4):17?24.
Herbert H. Clark. 1973. Space, time, semantics,
and the child. In T. E. Moore, editor, Cogni-
tive development and the acquisition of language,
pages 65?110. Academic Press.
Robert Dale and Nicholas Haddock. 1991. Gener-
ating referring expressions involving relations. In
Proceedings of the Fifth Conference of the Eu-
ropean Chapter of the Association for Computa-
tional Linguistics(EACL?91), pages 161?166.
Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the gricean maxims in the gen-
eration of referring expressions. Cognitive Sci-
ence, 19(2):233?263.
Robert Dale. 1992. Generating referring expres-
sions: Constructing descriptions in a domain of
objects and processes. MIT Press, Cambridge.
Peter Heeman and Graem Hirst. 1995. Collabo-
rating referring expressions. Computational Lin-
guistics, 21(3):351?382.
Annette Herskovits. 1986. Language and Spa-
tial cognition: an interdisciplinary study of the
prepositions in English. Cambridge University
Press.
Helmut Horacek. 1997. An algorithm for gener-
ating referential descriptions with flexible inter-
faces. In Proceedings of the 35th Annual Meeting
of the Association for Computational Linguistics,
pages 206?213.
Emiel Krahmer and Marie?t Theune. 2002. Efficient
context-sensitive generation of descriptions. In
Kees van Deemter and Rodger Kibble, editors,
Information Sharing: Givenness and Newness in
Language Processing. CSLI Publications, Stan-
ford, California.
Emiel Krahmer, Sebastiaan van Erk, and Andre?
Verleg. 2003. Graph-based generation of re-
ferring expressions. Computational Linguistics,
29(1):53?72.
Stephen C. Levinson, editor. 2003. Space in Lan-
guage and Cognition. Cambridge University
Press.
Hozumi Tanaka, Takenobu Tokunaga, and Yusuke
Shinyama. 2004. Animated agents capable of
understanding natural language and perform-
ing actions. In Helmut Prendinger and Mituru
Ishizuka, editors, Life-Like Characters, pages
429?444. Springer.
Kristinn R. Tho?risson. 1994. Simulated perceptual
grouping: An application to human-computer in-
teraction. In Proceedings of the Sixteenth An-
nual Conference of the Cognitive Science Society,
pages 876?881.
Kees van Deemter. 2002. Generating referring ex-
pressions: Boolean extensions of the incremental
algorithm. Computational Linguistics, 28(1):37?
52.
Ielka van der Sluis and Emiel Krahmer. 2000.
Generating referring expressions in a multimodal
context: An empirically oriented approach. Pre-
sented at the CLIN meeting 2000, Tilburg.
Coling 2008: Companion volume ? Posters and Demonstrations, pages 115?118
Manchester, August 2008
On ?redundancy? in selecting attributes for generating referring
expressions
Philipp Spanger Kurosawa Takehiro
Department of Computer Science
Tokyo Institute of Technology
Tokyo Meguro
?
Ookayama 2-12-1, 152-8550 Japan
{philipp, kurosawa, take}@cl.cs.titech.ac.jp
Tokunaga Takenobu
Abstract
We seek to develop an efficient algorithm
selecting attributes that approximates hu-
man selection. In contrast to previous work
we sought to combine the strengths of cog-
nitive theories and simple learning algo-
rithms. We then developed a new algo-
rithm for attribute selection based on ob-
servations from a corpus, which outper-
formed a simple base algorithm by a sig-
nificant margin. We then carried out a de-
tailed comparison between our algorithm
and Reiter & Dale?s ?Incremental Algo-
rithm?. In terms of achieving a human-like
attribute selection, the overall performance
of both algorithms is fundamentally equiv-
alent, while differing in the handling of re-
dundancy in selected attributes. We further
investigated this phenomenon and draw
some conclusions for further improvement
of attribute-selection algorithms.
1 Introduction
Referring expressions are a key research area in
human-agent communication. In the generation of
referring expressions humans do not necessarily
produce the most effective (i.e. minimal) expres-
sions in a computational sense. Given evolution-
ary development of human linguistic capabilities,
we can assume that human-produced expressions
are generally optimal to identify a target for other
human subjects. Thus the generation of human-
like referring expressions is an important task as
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
the generation of those expressions that are most
easily understandable for humans.
The seminal work in this field is the ?Incremen-
tal algorithm? (IA) (Dale and Reiter, 1995). Their
work is based on an analysis of the overall cog-
nitive tendencies of humans in the selection of at-
tributes. In recent years, there have been a number
of important extensions to this algorithm, dealing
with very specific problems. This need for a sys-
tematic approach and unified evaluation of those
vastly differing algorithms provided the motiva-
tion for the creation of the TUNA-corpus
1
that was
developed at Aberdeen University as part of the
TUNA project (van Deemter, 2007). Work has be-
gun to use this corpus for evaluating different al-
gorithms for attribute selection.
Our research is carried out within this general
trend, seeking to take advantage of common re-
sources (e.g. TUNA-corpus). A critical question is
how to combine the generic human cognitive ten-
dencies and the dependency of attribute selection
on a specific distribution of attributes in a specific
case. In this research we tackle this question in
a corpus-based approach. Specifically, in a given
environment, we seek to develop an efficient algo-
rithm for selection of attributes that approximates
human selection.
2 The corpus
We utilized a simplified version of the TUNA-
corpus, which was also the basis for the GRE-
challenge held as part of the UCNLG+MT work-
shop in 2007 (Belz and Gatt, 2007). The corpus
consists of a collection of paired pictures of objects
and human-produced referring expressions anno-
tated with attribute sets. Figure 1 shows an image
1
TUNA-corpus: www.csd.abdn.ac.uk/research/tuna
115
green
green
greengreen
red
red blue
target
Figure 1: Image of a TUNA-corpus picture
of such a case
2
. This corpus provides information
on the attribute-value pairs of the target and the
distractors as well as of the referring expressions
humans produced. Every item in our corpus con-
sists of an input part (?case?) and an output part
(?description?). Each individual case consists of
seven case entities: one target referent and six dis-
tractors. Every entity consists of a set of attribute-
value pairs and all descriptions consist of a subset
of the attribute-value pairs of the target referent in
the same format as any entity. This corpus com-
prises two domains: a ?Furniture? and a ?Person?
- domain. We note that within the corpus there
were some cases that we judged as inappropriate
for this study and thus excluded from the overall
evaluation. This included cases where attribute-
values were unspecified and/or inconsistent.
3 The base algorithm
We developed a base algorithm as a baseline for
evaluation. We define ?discriminative power? of a
specific attribute as the number of entities in the
case that have a different value from the target for
this attribute.
We add attributes in descending order of dis-
criminative power until the target can be identified
uniquely. The generated attribute set is the output.
Every time an attribute is selected, we recalcu-
late the discriminative power of the attributes of
exclusively those distractors that could not be ex-
cluded by this stage.
4 Analysis of human-produced referring
expressions
Our hypothesis is that in human generation of re-
ferring expressions, a combination of generic cog-
2
Actual pictures in the TUNA-corpus do neither show
colour labels nor a target-marker.
nitive factors as well as case-dependent factors
have to be dealt with. In order to account for the
cognitive factor, we define a ?selection probabil-
ity? over a whole domain (i.e. independent from a
specific case) and calculate the differences of this
selection probability over the different attributes.
We define the selection probability of a specific at-
tribute a in a specific domain as equation (1).
SP (a) =
C(a)
?
x?X
C(x)
(1)
where C(x) denotes the number of occurrences of
attribute x in the corpus.
We observe that in the Furniture-domain the at-
tributes colour and type have extraordinarily high
selection probabilities and in particular the at-
tribute type is selected virtually unconditionally.
We observe the same tendency of a very high selec-
tion probability for the attribute type in the Person-
domain, even though all distractors as well as the
target are of same type ?person?. Since the at-
tribute type becomes the head of the noun phrase
in the linguistic realisation of a referring expres-
sion, it is natural to mention the type. Overall, we
can conclude that the different values for the selec-
tion probabilities reflect the cognitive load humans
assign different attributes in a given domain.
4.1 Co-occurrence of attributes
We hypothesize that the selection of attributes is
limited by co-occurence - dependencies between
attributes.
In order to measure this degree of co-
occurrence, we defined a ?degree of dependency?
between attributes as in equation (2). If the degree
of dependency approaches 1, there is practically no
dependency in the occurrence of attributes a and b.
If this factor grows above 1, the two attributes eas-
ily occur jointly in the referring expression, on the
other hand, the further it decreases below 1, the
less likely are the two attributes to occur jointly.
In the equation P(a, b) is the probability that the
two attributes will be selected together, P(x) is
the probability that the attribute x will be selected.
D(a, b) is the degree of dependency between at-
tributes
D(a, b) =
P(a, b)
P(a)? P(b)
(2)
We observed that in the Furniture-domain, size
or orientation and dimension are less likely to oc-
116
cur together in a referring expression. Further-
more, in the Person - domain, hairColour and
hasHair or hasBeard have a high degree of depen-
dency, i.e. they likely occur together.
4.2 Redundancy of attributes
Even though in many referring expressions unique
identification with few attributes is possible, hu-
mans show a tendency to add ?redundant? at-
tributes, i.e. that are in a strict sense not necessary
for identification. By adding redundancy, humans
add robustness to the expression as well as pos-
sibly reducing the cognitive load for humans in a
specific context. Within the corpus, we counted the
number of expressions containing redundancy. In
the Furniture-domain there were 220 out of all 278
expressions and in the Person-domain there were
213 out of 230.
Table 1: Number of selected redundant attributes
Furniture (278 cases) Person (230 cases)
attribute occurrences attribute occurrences
colour 110 type 201
orientation 15 x-dimension 4
size 10 hasBeard 42
type 210 hasGlasses 41
x-dimension 18 hasHair 32
This level of redundancy indicates that in or-
der to produce human-like sets of attributes for the
generation of referring expressions, it is not neces-
sary to aim for a minimal set.
5 Our proposed algorithm for effective
attribute selection
Based on our analysis of co-occurrence and redun-
dancy of attributes, we centrally implemented the
following improvements of the base algorithm.
Co-occurrence Based on the results from sec-
tion 4.1, when a certain attribute is selected, we
raise the selection probabilities of those attributes
that have a tendency to co-occur with it, on the
other hand we lower the selection probabilities of
those attributes that have a tendency not to co-
occur with this attribute.
Redundancy Based on the results in section 4.2,
having selected the attributes to uniquely deter-
mine the target, we add the next candidate in the
list of attributes as a redundant attribute .
Combination We combine both individual im-
provements. First of all, we add the type-attribute
and then score the result based on the selec-
tion probability. With each selection of a spe-
cific attribute, we change the scores based on co-
occurrence, and at the end we add a redundant at-
tribute.
6 Evaluation of proposed algorithm
We measured the proximity of the sets of attributes
by our system to the human-produced set of at-
tributes. We utilize the Dice-coefficient (DC) ?
a measure of proximity for sets. For purposes of
Table 2: Average DC for key improvements
Furniture Person
Base algorithm 0.305 0.314
Base+selection probability 0.784 0.669
Base+co-occurrence 0.254 0.314
Base+redundancy 0.401 0.341
Combination 0.811 0.703
Incremental algorithm 0.811 0.705
comparison, we implemented a version of the In-
cremental algorithm, where we calculated the or-
der of selection of attributes according to the se-
lection probabilities of attributes in the overall do-
main (Furniture or Person). It is of note that our
algorithm (combination of all individual improve-
ments) performs almost equivalent to the IA.
6.1 Comparison with Incremental Algorithm
We carried out a detailed analysis of the results of
our algorithm and those of the IA. We found that
the results of both algorithms in the Furniture - do-
main are exactly the same; however the results of
the Person - domain show significant differences.
Thus we concentrate on further analysis of the re-
sults in the Person - domain.
We divided all cases from the Person - domain
into three sets; a set of cases where our algorithm
performs better than the IA (sys-superior cases: 27
cases), a set of cases where the opposite is true (IA-
superior cases: 24 cases) and a set of tie cases. We
then compared the first two sets.
Investigating these sets, we observed that the
key difference between these two algorithms lay
in the treatment of redundancy. The IA often fails
in the case where humans use fewer attributes and
add only type as redundant attribute. On the other
hand, our algorithm fails in the case where humans
use more complex expressions, that is, more at-
tributes including several redundant ones.
We investigated the redundant attributes which
are selected by humans but not by the algorithms.
117
In the IA-superior cases, our system fails to se-
lect the hasBeard attribute compared with the IA
in 20 out of 24 cases, while in the sys-superior
cases both algorithms fail to select almost the same
redundant attributes. We investigated for both al-
gorithms, which attributes the algorithms wrongly
select; i.e. which are not selected by humans. In
the sys-superior cases, the IA wrongly selects at-
tributes in all 27 cases, with 23 out of those in-
cluding the wrongly-selected hasBeard attribute.
In the IA-superior cases, the number of cases
with wrongly selected attributes is much smaller
(9 cases for each) and they are largely equiavalent.
Thus, our detailed analysis showed an over-
all opposite tendency in one attribute; hasBeard.
While in sys-superior cases about 85% of the cases
in which the IA output wrong attributes included
hasBeard, in IA-superior cases our system failed
to select exactly hasBeard at a largely equivalent
rate (about 83%). At this moment, we do not have
any reasonable explanation for this peculiarity of
hasBeard, but suspect it might possibly be related
to the characteristics of the corpus.
However, from the overall observation that our
algorithm achieved an equivalent level of human-
likeness to the IA while being weaker in cases of
more complex redundancy, we conclude that fur-
ther improvement in selecting redundant attributes
is crucial to outperform the IA.
7 Concluding Remarks
Based on observations from the TUNA-corpus,
we developed an algorithm for attribute-selection
modeling human referring expressions. Our
corpus-based algorithm sought to combine human
generic tendencies of attribute selection in a cer-
tain domain with case-dependent variation of the
salience of specific attributes. Our improved algo-
rithm outperformed the base algorithm by a signif-
icant margin. However, we got qualitatively equiv-
alent results to our implementation of the IA.
A detailed analysis of the characteristics of our
algorithm in comparison to the IA pointed to the
importance of the phenomenon of redundancy as
possibly a central aspect that needs to be further
investigated to achieve a qualitative improvement
over the IA.
Our investigations into redundancy show that in
those cases where our algorithm outperformed the
IA, our algorithm almost exclusively added solely
the type-attribute. In contrast in more complex
cases of redundancy in referring expressions, the
IA has shown to be superior. Since we achieved
overall parity to the IA even though generally per-
forming worse than the IA in cases of more com-
plex redundancy, we can conclude that outside of
this phenomenon our algorithm performs better
than the IA in terms of human-likeness.
In previous research there has been some discus-
sion on ?redundancy? vs. ?minimality? in refer-
ring expressions (e.g. (Viethen and Dale, 2006)).
Through our research we have identified the phe-
nomenon of redundancy as a critical topic for fur-
ther research and for achieving further progress
in the generation of human-like referring expres-
sions.
Our algorithm includes some strong simplifica-
tions, e.g. our treatment of attributes did not take
account of the fact that attribute-values are also of
different type and did not explore what implica-
tions this has for the process of producing refer-
ring expressions; binary (hasHair), discrete (hair-
Colour) or graded (x-dim). In future these factors
should be integrated into attribute selection algo-
rithms.
In future work, we will seek to provide a more
detailed investigation of the phenomenon of re-
dundancy, including its variation over different do-
mains. Such an analysis should also contribute to
further our understanding of the human cognitive
process in the selection of attributes for the gener-
ation of referring expressions.
References
Belz, Anja and Albert Gatt. 2007. The attribute se-
lection for GRE challenge: Overview and evaluation
results. In Proceedings of the MT Summit XI Work-
shop Using Corpora for Natural Language Gener-
ation: Language Generation and Machine Transla-
tion (UCNLG+MT), pages 75?83.
Dale, Robert. and Ehud Reiter. 1995. Computational
interpretation of the gricean maxims in the gener-
ation of referring expressions. Cognitive Science,
19(2):233?263.
van Deemter, Kees. 2007. TUNA: To-
wards a unified algorithm for the genera-
tion of referring expressions - Final Report -.
http://www.csd.abdn.ac.uk/research/tuna/pubs/TUNA-
final-report.pdf.
Viethen, Jette and Robert Dale. 2006. Algorithms for
generating referring expressions: Do they do what
people do? In Proceedings of the Fourth Inter-
national Natural Language Generation Conference,
pages 63?70.
118
Identifying Repair Targets in Action Control Dialogue
Kotaro Funakoshi and Takenobu Tokunaga
Department of Computer Science,
Tokyo Institute of Technology
2-12-1 Oookayama Meguro, Tokyo, JAPAN
{koh,take}@cl.cs.titech.ac.jp
Abstract
This paper proposes a method for deal-
ing with repairs in action control dialogue
to resolve participants? misunderstanding.
The proposed method identifies the re-
pair target based on common grounding
rather than surface expressions. We extend
Traum?s grounding act model by introduc-
ing degree of groundedness, and partial
and mid-discourse unit grounding. This
paper contributes to achieving more natu-
ral human-machine dialogue and instanta-
neous and flexible control of agents.
1 Introduction
In natural language dialogue, misunderstanding
and its resolution is inevitable for the natural
course of dialogue. The past research dealing
with misunderstanding has been focused on the di-
alogue involving only utterances. In this paper,
we discuss misunderstanding problem in the di-
alogue involving participant?s actions as well as
utterances. In particular, we focus on misunder-
standing in action control dialogue.
Action control dialogue is a kind of task-
oriented dialogue in which a commander con-
trols the actions1 of other agents called followers
through verbal interaction.
This paper deals with disagreement repair ini-
tiation utterances2 (DRIUs) which are used by
commanders to resolve followers? misunderstand-
ings3, or to correct commanders? previous erro-
neous utterances. These are so called third-turn
1We use the term ?action? for the physical behavior of
agents except for speaking.
2This denomination is lengthy and may be still controver-
sial. However we think this is most descriptively adequate for
the moment.
3Misunderstanding is a state where miscommunication
has occurred but participants are not aware of this, at least
initially (Hirst et al, 1994).
repair (Schegloff, 1992). Unlike in ordinary dia-
logue consisting of only utterances, in action con-
trol dialogue, followers? misunderstanding could
be manifested as their inappropriate actions in re-
sponse to a given command.
Let us look at a sample dialogue (1.1 ? 1.3). Ut-
terance (1.3) is a DRIU for repairing V?s mis-
understanding of command (1.1) which is mani-
fested by his action performed after saying ?OK?
in (1.2).
(1.1) U: Put the red book on the shelf to the right.
(1.2) V: OK. <V performs the action>
(1.3) U: Not that.
It is not easy for machine agents to under-
stand DRIUs because they can sometimes be so
elliptical and context-dependent that it is difficult
to apply traditional interpretation methodology to
DRIUs.
In the rest of this paper, we describe the dif-
ficulty of understanding DRIUs and propose a
method to identify repair targets. The identifica-
tion of repair targets plays a key role in under-
standing DRIUs and this paper is intensively fo-
cused on this issue.
2 Difficulty of Understanding DRIUs
Understanding a DRIU consists of repair tar-
get identification and repair content interpretation.
Repair target identification identifies a target to be
repaired by the speaker?s utterance. Repair con-
tent interpretation recovers the speaker?s intention
by replacing the identified repair target with the
correct one.
One of the major source of difficulties in un-
derstanding DRIUs is that they are often elliptical.
Repair content interpretation depends heavily on
repair targets but the information to identify re-
pair targets is not always mentioned explicitly in
DRIUs.
Let us look at dialogue (1.1 ? 1.3) again. The
DRIU (1.3) indicates that V failed to identify U?s
intended object in utterance (1.1). However, (1.3)
does not explicitly mention the repair target, i.e.,
either book or shelf in this case.
The interpretation of (1.3) changes depending
on when it is uttered. More specifically, the inter-
pretation depends on the local context and the sit-
uation when the DRIU is uttered. If (1.3) is uttered
when V is reaching for a book, it would be natu-
ral to consider that (1.3) is aimed at repairing V?s
interpretation of ?the book?. On the other hand,
if (1.3) is uttered when V is putting the book on a
shelf, it would be natural to consider that (1.3) is
aimed at repairing V?s interpretation of ?the shelf
to the right?.
Assume that U uttered (1.3) when V was putting
a book in his hand on a shelf, how can V identify
the repair target as shelf instead of book? This pa-
per explains this problem on the basis of common
grounding (Traum, 1994; Clark, 1996). Common
grounding or shortly grounding is the process of
building mutual belief among a speaker and hear-
ers through dialogue. Note that in action control
dialogue, we need to take into account not only
utterances but also followers? actions. To identify
repair targets, we keep track of states of grounding
by treating followers? actions as grounding acts
(see Section 3). Suppose V is placing a book in
his hand on a shelf. At this moment, V?s inter-
pretation of ?the book? in (1.1) has been already
grounded, since U did not utter any DRIU when
V was taking the book. This leads to the interpre-
tation that the repair target of (1.1) is shelf rather
than already grounded book.
3 Grounding
This section briefly reviews the grounding acts
model (Traum, 1994) which we adopted in our
framework. We will extend the grounding act
model by introducing degree of groundedness that
have a quaternary distinction instead of the orig-
inal binary distinction. The notions of partial
grounding and mid-discourse unit grounding are
also introduced for dealing with action control di-
alogue.
3.1 Grounding Acts Model
The grounding acts model is a finite state transi-
tion model to dynamically compute the state of
grounding in a dialogue from the viewpoint of
each participant.
This theory models the process of grounding
with a theoretical construct, namely the discourse
unit (DU). A DU is a sequence of utterance units
(UUs) assigned grounding acts (GAs). Each UU
in a dialogue has at least one GA, except fillers or
several cue phrases, which are considered useful
for turn taking but not for grounding. Each DU
has an initiator (I) who opened it, and other par-
ticipants of that DU are called responders (R).
Each DU is in one of seven states listed in Ta-
ble 1 at a time. Given one of GAs shown in Table 2
as an input, the state of DU changes according to
the current state and the input. A DU starts with
a transition from initial state S to state 1, and fin-
ishes at state F or D. DUs in state F are regarded
as grounded.
Analysis of the grounding process for a sam-
ple dialogue is illustrated in Figure 1. Speaker B
can not understand the first utterance by speaker
A and requests a repair (ReqRep-R) with his ut-
terance. Responding to this request, A makes a
repair (Repair-I). Finally, B acknowledges to
show he has understood the first utterance and the
discourse unit reaches the final state, i.e., state F.
State Description
S Initial state
1 Ongoing
2 Requested a repair by a responder
3 Repaired by a responder
4 Requested a repair by the initiator
F Finished
D Canceled
Table 1: DU states
Grounding act Description
Initiate Begin a new DU
Continue Add related content
Ack Present evidences of understanding
Repair Correct misunderstanding
ReqRepair Request a repair act
ReqAck Request an acknowledge act
Cancel Abandon the DU
Table 2: Grounding acts
UU DU1
A : Can I speak to Jim Johnstone
please?
Init-I 1
B : Senior? ReqRep-R 2
A : Yes Repair-I 1
B : Yes Ack-R F
Figure 1: An example of grounding (Ishizaki and
Den, 2001)
178
3.2 Degree of Groundedness and Evidence
Intensity
As Traum admitted, the binary distinction between
grounded and ungrounded in the grounding acts
model is an oversimplification (Traum, 1999). Re-
pair target identification requires more finely de-
fined degree of groundedness. The reason for this
will be elucidated in Section 5.
Here, we will define the four levels of evidence
intensity and equate these with degrees of ground-
edness, i.e., if an utterance is grounded with evi-
dence of level N intensity, the degree of ground-
edness of the utterance is regarded as level N .
(2) Levels of evidence intensity
Level 0: No evidence (i.e., not grounded).
Level 1: The evidence shows that the re-
sponder thinks he understood the utter-
ance. However, it does not necessar-
ily mean that the responder understood
it correctly. E.g., the acknowledgment
?OK? in response to the request ?turn to
the right.?
Level 2: The evidence shows that the re-
sponder (partially) succeeded in trans-
ferring surface level information. It does
not yet ensure that the interpretation of
the surface information is correct. E.g.,
the repetition ?to the right? in response
to the request ?turn to the right.?
Level 3: The evidence shows that the re-
sponder succeeded in interpretation.
E.g., turning to the right as the speaker
intended in response to the request ?turn
to the right.?
3.3 Partial and mid-DU Grounding
In Traum?s grounding model, the content of a DU
is uniformly grounded. However, things in the
same DU should be more finely grounded at var-
ious levels individually. For example, if one ac-
knowledged by saying ?to the right? in response
to the command ?put the red chair to the right of
the table?, to the right of should be regarded as
grounded at Level 2 even though other parts of the
request are grounded at Level 1.
In addition, in Traum?s model, the content of a
DU is grounded all at once when the DU reaches
the final state, F. However, some elements in a DU
can be grounded even though the DU has not yet
reached state F. For example, if one requested a
repair as ?to the right of what?? in response to
the command ?put the red chair to the right of
the table?, to the right of should be regarded as
grounded at level 2 even though table has not yet
been grounded.
Although Traum admitted these problems ex-
isted in his model, he retained it for the sake of
simplicity. However, such partial and mid-DU
grounding is necessary to identify repair targets.
We will describe the usage of these devices to
identify repair targets in Section 5. In brief, when
a level 3 evidence is presented by the follower and
negative feedback (i.e., DRIUs) is not provided by
the commander, only propositions supported by
the evidence are considered to be grounded even
though the DU has not yet reached state F.
4 Treatment of Actions in Dialogue
In general, past work on discourse has targeted di-
alogue consisting of only utterances, or has con-
sidered actions as subsidiary elements. In contrast,
this paper targets action control dialogue, where
actions are considered to be primary elements of
dialogue as well as utterances.
Two issues have to be mentioned for handling
action control dialogue in the conventional se-
quential representation as in Figure 1. We will in-
troduce assumptions (3) and (4) as shown below.
Overlap between utterances and actions
Actions in dialogue do not generally obey turn
allocation rules as Clark pointed out (Clark, 1996).
In human-human action control dialogue, follow-
ers often start actions in the middle of a comman-
der?s utterance. This makes it difficult to analyze
discourse in sequential representation. Given this
fact, we impose the three assumptions on follow-
ers as shown in (3) so that followers? actions will
not overlap the utterances of commanders. These
requirements are not unreasonable as long as fol-
lowers are machine agents.
(3) Assumptions on follower?s actions
(a) The follower will not commence action
until turn taking is allowed.
(b) The follower immediately stops the ac-
tion when the commander interrupts
him.
(c) The follower will not make action as pri-
mary elements while speaking. 4
4We regard gestures such as pointing as secondary ele-
179
Hierarchy of actions
An action can be composed of several sub-
actions, thus has a hierarchical structure. For ex-
ample, making tea is composed of boiling the wa-
ter, preparing the tea pot, putting tea leaves in the
pot, and pouring the boiled water into it, and so
on. To analyze actions in dialogue as well as ut-
terances in the traditional way, a unit of analysis
should be determined. We assume that there is a
certain granularity of action that human can recog-
nize as primitive. These actions would correspond
to basic verbs common to humans such as ?walk?,
?grasp?, ?look?, etc.We call these actions funda-
mental actions and consider them as UUs in action
control dialogue.
(4) Assumptions on fundamental actions
In the hierarchy of actions, there is a cer-
tain level consisting of fundamental actions
that human can commonly recognize as prim-
itives. Fundamental actions can be treated as
units of primary presentations in an analogy
with utterance units .
5 Repair Target Identification
In this section, we will discuss how to identify the
repair target of a DRIU based on the notion of
grounding. The following discussion is from the
viewpoint of the follower.
Let us look at a sample dialogue (5.1 ? 5.5),
where U is the commander and V is the fol-
lower. The annotation Ack1-R:F in (5.2) means
that (5.2) has grounding act Ack by the respon-
der (R) for DU1 and the grounding act made DU1
enter state F. The angle bracketed descriptions in
(5.3) and (5.4) indicate the fundamental actions by
V.
Note that thanks to assumption (4) in Section 4,
a fundamental action itself can be considered as a
UU even though the action is performed without
any utterances.
(5.1) U: Put the red ball on the left box. (Init1-I:1)
(5.2) V: Sure. (Ack1-R:F)
(5.3) V: <V grasps the ball> (Init2-I:1)
(5.4) V: <V moves the ball> (Cont2-I:1)
(5.5) U: Not that. (Repair1-R:3)
The semantic content of (5.1) can be repre-
sented as a set of propositions as shown in (6).
ments when they are presented in parallel with speech. There-
fore, this constraint does not apply to them.
(6) ? = Request(U, V, Put(#Agt1, #Obj1, #Dst1))
(a) speechActType(?)=Request
(b) presenter(?)=U
(c) addressee(?)=V
(d) actionType(content(?))=Put
(e) agent(content(?))=#Agt1,
referent(#Agt1)=V
(f) object(content(?))=#Obj1,
referent(#Obj1)=Ball1
(g) destination(content(?))=#Dst1,
referent(#Dst1)=Box1
? represents the entire content of (5.1). Sym-
bols beginning with a lower case letter are func-
tion symbols. For example, (6a) means the speech
act type for ? is ?Request?. Symbols beginning
with an upper case letter are constants. ?Request?
is the name of a speech act type and ?Move? is
that of fundamental action respectively. U and V
represents dialogue participants and ?Ball1? rep-
resents an entity in the world. Symbols beginning
with # are notional entities introduced in the dis-
course and are called discourse referents. A dis-
course referent represents something referred to
linguistically. During a dialogue, we need to con-
nect discourse referents to entities in the world, but
in the middle of the dialogue, some discourse ref-
erents might be left unconnected. As a result we
can talk about entities that we do not know. How-
ever, when one takes some actions on a discourse
referent, he must identify the entity in the world
(e.g., an object or a location) corresponding to the
discourse referent. Many problems in action con-
trol dialogue are caused by misidentifying entities
in the world.
Follower V interprets (5.1) to obtain (6), and
prepares an action plan (7) to achieve ?Put(#Agt1,
#Obj1, #Dst1)?. Plan (7) is executed downward
from the top.
(7) Plan for Put(#Agt1, #Obj1, #Dst1)
Grasp(#Agt1, #Obj1),
Move(#Agt1, #Obj1, #Dst1),
Release(#Agt1, #Obj1)
Here, (5.1 ? 5.5) are reformulated as in (8.1 ?
8.5). ?Perform? represents performing the action.
(8.1) U: Request(U, V, Put(#Agt1, #Obj1, #Dst1))
(8.2) V: Accept(V, U, ?)
(8.3) V: Perform(V, U, Grasp(#Agt1, #Obj1))
180
(8.4) V: Perform(V, U,Move(#Agt1, #Obj1, #Dst1))
(8.5) U: Inform(U, V, incorrect(X))
To understand DRIU (5.5), i.e., (8.5), follower
V has to identify repair target X in (8.5) referred
to as ?that? in (5.5). In this case, the repair target
of (5.5) X is ?the left box?, i.e., #Dst1.5 However,
the pronoun ?that? cannot be resolved by anaphora
resolution only using textual information.
We treat propositions, or bindings of variables
and values, such as (6a ? 6g), as the minimum
granularity of grounding because the identification
of repair targets requires that granularity. We then
make the following assumptions concerning repair
target identification.
(9) Assumptions on repair target identification
(a) Locality of elliptical DRIUs: The target
of an elliptical DRIU that interrupted the
follower?s action is a proposition that is
given an evidence of understanding by
the interrupted action.
(b) Instancy of error detection: A dialogue
participant observes his dialogue con-
stantly and actions presenting strong ev-
idence (Level 3). Thus, when there is an
error, the commander detects it immedi-
ately once an action related to that error
occurs.
(c) Instancy of repairs: If an error is
found, the commander immediately in-
terrupts the dialogue and initiates a re-
pair against it.
(d) Lack of negative evidence as positive
evidence: The follower can determine
that his interpretation is correct if the
commander does not initiates a repair
against the follower?s action related to
the interpretation.
(e) Priority of repair targets: If there are
several possible repair targets, the least
grounded one is chosen.
(9a) assumes that a DRIU can only be ellipti-
cal when it presupposes the use of local context to
identify its target. It also predicts that if the target
of a repair is neither local nor accessible within
local information, the DRIU will not be elliptical
depending on local context but contain explicit and
5We assume that there is a sufficiently long interval be-
tween the initiations of (5.4) and (5.5).
sufficient information to identify the target. (9b)
and (9c) enable (9a).
Nakano et al (2003) experimentally confirmed
that we observe negative responses as well as pos-
itive responses in the process of grounding. Ac-
cording to their observations, speakers continue
dialogues if negative responses are not found even
when positive responses are not found. This evi-
dence supports (9d).
An intuitive rationale for (9e) is that an issue
with less proof would more probably be wrong
than one with more proof.
Now let us go through (8.2) to (8.5) again ac-
cording to the assumptions in (9). First, ? is
grounded at intensity level 1 by (8.2). Second, V
executes Grasp(#Agt1, #Obj1) at (8.3). Because
V does not observe any negative response from U
even after this action is completed, V considers
that the interpretations of #Agt1 and #Obj1 have
been confirmed and grounded at intensity level 3
according to (9d) (this is the partial and mid-DU
grounding mentioned in Section 3.3). After initiat-
ing Move(#Agt1, #Obj1, #Dst1), V is interrupted
by commander U with (8.5) in the middle of the
action.
V interprets elliptical DRIU (5.5) as ?Inform(S,
T, incorrect(X))?, but he cannot identify repair tar-
get X. He tries to identify this from the discourse
state or context. According to (9a), V assumes that
the repair target is a proposition that its interpre-
tation is demonstrated by interrupted action (8.4).
Due to the nature of the word ?that?, V knows that
possible candidates are not types of action or the
speech act but discourse referents #Agt1, #Obj1
and #Dst16. Here, #Agt1 and #Obj1 have been
grounded at intensity level 3 by the completion of
(8.3). Now, (9e) tells V that the repair target is
#Dst1, which has only been grounded at intensity
level 1 7.
(10) below summarizes the method of repair tar-
get identification based on the assumptions in (9).
(10) Repair target identification
6We have consistently assumed Japanese dialogues in this
paper although examples have been translated into English.
?That? is originally the pronoun ?sotti? in Japanese, which
can only refer to objects, locations, or directions, but cannot
refer to actions.
7There are two propositions concerned with #Dst1:
destination(content(?)) = #Dst1 and referent(#Dst1) = Box1.
However if dest(content(?)) = #Dst1 is not correct, this
means that V grammatically misinterpreted (8.1). It seems
hard to imagine for participants speaking in their mother
tongue and thus one can exclude dest(content(?)) = #Dst1
from the candidates of the repair target.
181
(a) Specify the possible types of the repair
target from the linguistic expression.
(b) List the candidates matching the types
determined in (10a) from the latest pre-
sented content.
(c) Rank candidates based on groundedness
according to (9e) and choose the top
ranking one.
Dependencies between Parameters
The follower prepares an action plan to achieve
the commander?s command as in plan (7). Here,
the planned actions can contain parameters not di-
rectly corresponding to the propositions given by
the commander. Sometimes a selected parameter
by using (10) is not the true target but the depen-
dent of the target. Agents must retrieve the true
target by recognizing dependencies of parameters.
For example, assume a situation where objects
are not within the follower?s reach as shown in
Figure 2. Then, the commander issues command
(6) to the follower (Agent1 in Figure 2) and he
prepares an action plan (11).
(11) Agent1?s plan (partial) for (6) in Figure 2.
Walk(#Agt1, #Dst1),
Grasp(#Agt1, #Obj1),
. . .
The first Walk is a prerequisite action for Grasp
and #Dst1 depends on #Obj1. In this case, if refer-
ent(#Obj1) is Object1 then referent(#Dst1) is Po-
sition1, or if referent(#Obj1) is Object2 then ref-
erent(#Dst1) is Position2. Now, assume that the
commander intends referent(#Obj1) to be Object2
with (6), but the follower interprets this as refer-
ent(#Obj1) = Object1 (i.e., referent(#Dst1) = Po-
sition1) and performs Walk(#Agt1, #Dst1). The
commander then observes the follower moving to-
ward a direction different from his expectation and
infers the follower has misunderstood the target
object. He, then, interrupts the follower with the
utterance ?not that? at the timing illustrated in Fig-
ure 3. Because (10c) chooses #Dst2 as the repair
target, the follower must be aware of the depen-
dencies between parameters #Dst1 and #Obj1 to
notice his misidentification of #Obj1.
6 Implementation and Some Problems
We implemented the repair target identification
method described in Section 5 into our prototype
Position1
?Agent1 Object1 (wrong)
Object2 (correct)
?Position2
Figure 2: Situation with dependent parameters
Time
Walk(#Agt1, #Dst1) Grasp(#Agt1, #Obj1)
  " Not that "
Figure 3: Dependency between parameters
dialogue system (Figure 4). The dialogue system
has animated humanoid agents in its visualized 3D
virtual world. Users can command the agent by
speech to move around and relocate objects.
Figure 4: Snapshot of the dialogue system
Because our domain is rather small, current pos-
sible repair targets are agents, objects and goals
of actions. According to the qualitative evalua-
tion of the system through interaction with sev-
eral subjects, most of the repair targets were cor-
rectly identified by the proposed method described
in Section 5. However, through the evaluation, we
found several important problems to be solved as
below.
6.1 Feedback Delay
In a dialogue where participants are paying atten-
tion to each other, the lack of negative feedback
can be considered as positive evidence (see (9d)).
However, it is not clear how long the system needs
to wait to consider the lack of negative feedback as
positive evidence. In some cases, it will be not ap-
propriate to consider the lack of negative feedback
182
as positive evidence immediately after an action
has been completed. Non-linguistic information
such as nodding and gazing should be taken into
consideration to resolve this problem as (Nakano
et al, 2003) proposed.
Positive feedback is also affected by delay.
When one receives feedback shortly after an action
is completed and begins the next action, it may be
difficult to determine whether the feedback is di-
rected to the completed action or to the just started
action.
6.2 Visibility of Actions
The visibility of followers? actions must be con-
sidered. If the commander cannot observe the fol-
lower?s action due to environmental conditions,
the lack of negative feedback cannot be positive
evidence for grounding.
For example, assume the command ?bring me
a big red cup from the next room? is given and
assume that the commander cannot see the inside
of the next room. Because the follower?s funda-
mental action of taking a cup in the next room is
invisible to the commander, it cannot be grounded
at that time. They have to wait for the return of the
follower with a cup.
6.3 Time-dependency of Grounding
Utterances are generally regarded as points on the
time-line in dialogue processing. However, this
approximation cannot be applied to actions. One
action can present evidences for multiple propo-
sitions but it will present these evidences at con-
siderably different time. This affects repair target
identification.
Let us look at an action Walk(#Agt, #Dst),
where agent #Agt walks to destination #Dst. This
action will present evidence for ?who is the in-
tended agent (#Agt)? at the beginning. However,
the evidence for ?where is the intended position
(#Dst)? will require the action to be completed.
However, if the position intended by the follower
is in a completely different direction from the one
intended by the commander, his misunderstanding
will be evident at a fairly early stage of the action.
6.4 Differences in Evidence Intensities
between Actions
Evidence intensities vary depending on the char-
acteristics of actions. Although the symbolic de-
scription of actions such as (12) and (13) does not
explicitly represent differences in intensity, there
is a significant difference between (12) where
#Agent looks at #Object at a distance, and (13)
where #Agent directly contacts #Object. Agents
must recognize these differences to conform with
human recognition and share the same state of
grounding with participants.
(12) LookAt(#Agent, #Object)
(13) Grasp(#Agent, #Object)
6.5 Other Factors of Confidence in
Understanding
Performing action can provide strong evidence of
understanding and such evidence enables partic-
ipants to have strong confidence in understand-
ing. However, other factors such as linguistic con-
straints (not limited to surface information) and
plan/goal inference can provide confidence in un-
derstanding without grounding. Such factors of
confidence also must be incorporated to explain
some repairs.
Let us see a sample dialogue below, and assume
that follower V missed the word red in (14.3).
(14.1) U: Get the white ball in front of the table.
(14.2) V: OK. <V takes a white ball>
(14.3) U: Put it on the (red) table.
(14.4) V: Sure. <V puts the white ball holding in
his hand on a non-red table>
(14.5) U: I said red.
When commander U repairs V?s misunder-
standing by (14.5), V cannot correctly decide that
the repair target is not ?it? but ?the (red) table? in
(14.3) by using the proposed method, because the
referent of ?it? had already been in V?s hand and
no explicit action choosing a ball was performed
after (14.3). However, in such a situation we seem
to readily doubt misunderstanding of ?the table?
because of strong confidence in understanding of
?it? that comes from outside of grounding process.
Hence, we need a unified model of confidence in
understanding that can map different sources of
confidence into one dimension. Such a model is
also useful for clarification management of dia-
logue systems.
7 Discussion
7.1 Advantage of Proposed Method
The method of repair target identification pro-
posed in this paper less relies on surface infor-
mation to identify targets. This is advantageous
183
against some sort of misrecognitions by automatic
speech recognizers and contributes to the robust-
ness of spoken dialogue systems.
Only surface information is generally insuffi-
cient to identify repair targets. For example, as-
sume that there is an agent acting in response to
(15) and his commander interrupts him with (16).
(15) Put the red ball on the table
(16) Sorry, I meant blue
If one tries to identify the repair target with sur-
face information, the most likely candidate will
be ?the red ball? because of the lexical similar-
ity. Such methods easily break down. They can-
not deal with (16) after (17). If, however, one pays
attention to the state of grounding as our proposed
method, he can decide which one is likely to be re-
paired ?the red ball? or ?the green table? depend-
ing on the timing of the DRIU.
(17) Put the red ball on the green table
7.2 Related Work
McRoy and Hirst (1995) addressed the detection
and resolution of misunderstandings on speech
acts using abduction. Their model only dealt with
speech acts and did not achieve our goals.
Ardissono et al (1998) also addressed the same
problem but with a different approach. Their
model could also handle misunderstanding regard-
ing domain level actions. However, we think that
their model using coherence to detect and resolve
misunderstandings cannot handle DRIUs such as
(8.5), since both possible repairs for #Obj1 and
#Dst1 have the same degree of coherence in their
model.
Although we did not adopt this, the notion of
QUD (questions under discussion) proposed by
Ginzburg (Ginzburg, 1996) would be another pos-
sible approach to explaining the problems ad-
dressed in this paper. It is not yet clear whether
QUD would be better or not.
8 Conclusion
Identifying repair targets is a prerequisite to un-
derstand disagreement repair initiation utterances
(DRIUs). This paper proposed a method to iden-
tify the target of a DRIU for conversational agents
in action control dialogue. We explained how a re-
pair target is identified by using the notion of com-
mon grounding. The proposed method has been
implemented in our prototype system and eval-
uated qualitatively. We described the problems
found in the evaluation and looked at the future
directions to solve these problems.
Acknowledgment
This work was supported in part by the Ministry of
Education, Science, Sports and Culture of Japan as
the Grant-in-Aid for Creative Basic Research No.
13NP0301.
References
L. Ardissono, G. Boella, and R. Damiano. 1998. A
plan based model of misunderstandings in cooper-
ative dialogue. International Journal of Human-
Computer Studies, 48:649?679.
Herbert H. Clark. 1996. Using Language. Cambridge
University Press.
Jonathan Ginzburg. 1996. Interrogatives: ques-
tions, facts and dialogue. In Shalom Lappin, editor,
The Handbook of Contemporary Semantic Theory.
Blackwell, Oxford.
G. Hirst, S. McRoy, P. Heeman, P. Edmonds, and
D. Horton. 1994. Repairing conversational misun-
derstandings and non-understandings. Speech Com-
munication, 15:213?230.
Masato Ishizaki and Yasuharu Den. 2001. Danwa
to taiwa (Discourse and Dialogue). University of
Tokyo Press. (In Japanese).
Susan Weber McRoy and Graeme Hirst. 1995. The re-
pair of speech act misunderstandings by abductive
inference. Computational Linguistics, 21(4):435?
478.
Yukiko Nakano, Gabe Reinstein, Tom Stocky, and Jus-
tine Cassell. 2003. Towards a model of face-to-face
grounding. In Erhard Hinrichs and Dan Roth, edi-
tors, Proceedings of the 41st Annual Meeting of the
Association for Computational Linguistics, pages
553?561.
E. A Schegloff. 1992. Repair after next turn: The
last structurally provided defense of intersubjectiv-
ity in conversation. American Journal of Sociology,
97(5):1295?1345.
David R. Traum. 1994. Toward a Computational
Theory of Grounding. Ph.D. thesis, University of
Rochester.
David R. Traum. 1999. Computational models of
grounding in collaborative systems. In Working
Papers of AAAI Fall Symbosium on Psychological
Models of Communication in Collaborative Systems,
pages 137?140.
184
Controlling Animated Agents in Natural Language
Kotaro Funakoshi
Department of Computer Science,
Tokyo Institute of Technology
2-12-1 Oookayama, Meguro,
Tokyo 152-8552, Japan
koh@cl.cs.titech.ac.jp
Takenobu Tokugana
Department of Computer Science,
Tokyo Institute of Technology
2-12-1 Oookayama, Meguro,
Tokyo 152-8552, Japan
take@cl.cs.titech.ac.jp
Abstract
This paper presents a prototype dia-
logue system, K3 , in which a user can
instruct agents through speech input to
manipulate various objects in a 3-D vir-
tual world. In this paper, we focus
on two distinctive features of the K3
system: plan-based anaphora resolution
and handling vagueness in spatial ex-
pressions. After an overview of the sys-
tem architecture, each of these features
is described. We also look at the future
research agenda of this system.
1 Introduction
SHRDLU (?) can be considered as the most im-
portant natural language understanding system.
Although SHRDLU was not ?embodied?, hav-
ing had only a small stick to manipulate objects,
it certainly had several features that a conversa-
tional agent is supposed to have. It had a great
potential, and it was very promising for future re-
search on natural language understanding.
Recently better technologies have become
available in speech recognition and natural lan-
guage processing. Major breakthroughs in the
area of computer graphics have enabled us to gen-
erate complex, yet realistic 3-D animated agents
or embodied life-like agents in a virtual environ-
ment. Researchers are now in a good position to
go beyond SHRDLU by combining these tech-
nologies (?). This paper presents a conversational
animated agent system, K3 .
Since all the actions carried out by an agent of
the K3 system are visible, we can evaluate the
performance of the system by observing its an-
imation. Visualizing the agents? actions yields
many interesting issues from a cognitive science
point of view; more complex processes are in-
volved than those found in most conventional nat-
ural language understanding systems.
After sketching out the overview of the K3 sys-
tem in section 2, Two distinctive features of K3
are discussed in section 3, and 4. Finally, sec-
tion 5 concludes the paper and looks at future re-
search agenda.
2 System Overview
A screen shot of K3 is shown in Fig. 1. There are
two agents and several objects in a virtual world.
The current system accepts simple Japanese utter-
ances with anaphoric and elliptical expressions,
such as ?Walk to the desk.? and ?Further?. The
size of the lexicon is about 100 words.
Figure 1: A screenshot of K3
91
The architecture of the K3 is illustrated in
Fig. 2. system. The speech recognition module
receives the user?s speech input and generates a
sequence of words. The syntactic/semantic anal-
ysis module analyzes the word sequence to ex-
tract a case frame. This module accepts ill-formed
speech input including postposition omission, in-
version, and self-correction. At this stage, not
all case slots are necessarily filled, because of el-
lipses in the utterance. Even in cases where there
is no ellipsis, instances of objects are not identi-
fied at this stage.
Resolving ellipses and anaphora, and identify-
ing instances in the world are performed by the
discourse analysis module. Anaphora resolution
and instance identification are achieved by using
plan-knowledge, which will be described in sec-
tion 3.
The discourse analysis module extracts the
user?s goal as well and hands it over to the plan-
ning modules, which build a plan to generate the
appropriate animation. In other words, the plan-
ning modules translate the user?s goal into anima-
tion data. However, the properties of these two
ends are very different and straightforward trans-
lation is rather difficult. The user?s goal is repre-
sented in terms of symbols, while the animation
data is a sequence of numeric values. To bridge
this gap, we take a two-stage approach ? macro-
and micro-planning.
During the macro-planning, the planner needs
to know the physical properties of objects, such
as their size, location and so on. For example, to
pick up a ball, the agent first needs to move to the
location at which he can reach the ball. In this
planning process, the distance between the ball
and the agent needs to be calculated. This sort
of information is represented in terms of coordi-
nate values of the virtual space and handled by the
micro-planner.
To interface the macro- and micro-planning,
we introduced the SPACE object to represent a lo-
cation in the virtual space by its symbolic and nu-
meric character. The SPACE object is described in
section 4.
3 Plan-based Anaphora Resolution
3.1 Surface-clue-based Resolution vs.
Plan-based Resolution
Consider the following two dialogue examples.
(1-1) ?Agent X, push the red ball.?
(1-2) ?Move to the front of the blue ball.?
(1-3) ?Push it.?
(2-1) ?Agent X, pick up the red ball.?
(2-2) ?Move to the front of the blue ball.?
(2-3) ?Put it down.?
The second dialogue is different from the first
one only in terms of the verbs in the first and third
utterances. The syntactic structure of each sen-
tence in the second dialogue (2-1)?(2-3) is the
same as the corresponding sentence in the first
dialogue (1-1)?(1-3). However, pronoun ?it? in
(1-3) refers to ?the blue ball? in (1-2), and pro-
noun ?it? in (2-3) refers to ?the red ball? in (2-1).
The difference between these two examples is not
explained by the theories based on surface clues
such as the centering theory (?; ?).
In the setting of SHRDLU-like systems, the
user has a certain goal of arranging objects in the
world, and constructs a plan to achieve it through
interaction with the system. As Cohen pointed
out, users tend to break up the referring and pred-
icating functions in speech dialogue (?). Thus,
each user?s utterance suggests a part of plan rather
than a whole plan that the user tries to perform.
To avoid redundancy, users need to use anaphora.
From these observations, we found that consid-
ering a user?s plan is indispensable in resolving
anaphora in this type of dialogue system and de-
veloped an anaphora resolution algorithm using
the relation between utterances in terms of partial
plans (plan operators) corresponding to them.
The basic idea is to identify a chain of plan op-
erators based on their effects and preconditions.
Our method explained in the rest of this section
finds preceding utterances sharing the same goal
as the current utterance with respect to their cor-
responding plan operators as well as surface lin-
guistic clues.
92
!"#$%&?()**"*+
,-.)*/"#
0"#/"%*)$1
2-*0-$"*+
,1*/)#/"#3
,-.)*/"#&)*)(14"4
5()*("6$)$1
7"$/8)(
9%$(0
://-$)*#-
;"4/%$1
<*/%(%+1=%$00"#/"%*)$1>)*+8)+-.%0-(
=%$0&4-?8-*#- @)4-&A$).- B%)(
C)4"#
.%D-.-*/
!"#$%
&?()*+
@%%$0"*)/-&D)(8-
,?--#;&"*?8/
E*".)/"%*
F"4#%8$4-)*)(14"4
,?)#-$-#%+*"/"%*
!)#$%
?()**"*+
!%D-.-*/
+-*-$)/"%*
,?--#;$-#%+*"/"%*
Figure 2: The system architecture of K3
3.2 Resolution Algorithm
Recognized speech input is transformed into a
case frame. At this stage, anaphora is not re-
solved. Based on this case frame, a plan opera-
tor is retrieved in the plan library. This process
is generally called ?plan recognition.? A plan
operator used in our system is similar to that of
STRIPS (?), which consists of precondition, ef-
fect and action description.
Variables in the retrieved plan operator are
filled with case fillers in the utterance. There
might be missing case fillers when anaphora (zero
pronoun) is used in the utterance. The system
tries to resolve these missing elements in the plan
operator. To resolve the missing elements, the
system again uses clue words and the plan library.
An overview of the anaphora resolution algorithm
is shown in Figure 3.
When the utterance includes clue words, the
system uses them to search the history database
for the preceding utterance that shares the same
goal as the current utterance. Then, it identifies
the referent on the basis of case matching.
There are cases in which the proper preceding
utterance cannot be identified even with the clue
words. These cases are sent to the left branch in
Fig. 3 where the plan library is used to resolve
anaphora.
When there is no clue word or the clue word
does not help to resolve the anaphora, the process
goes through the left branch in Fig. 3. First, the
system enumerates the candidates of referents us-
ing the surface information, then filters them out
with linguistic clues and the plan library. For ex-
ample, demonstratives such as ?this?, ?that? are
usually used for objects that are in the user?s view.
Therefore, the referent of anaphora with demon-
stratives is restricted to the objects in the current
user?s view.
If the effect of a plan operator satisfies the pre-
condition of another plan operator, and the utter-
ances corresponding to these plan operators are
uttered in discourse, they can be considered to
intend the same goal. Thus, identifying a chain
of effect-precondition relations gives important
information for grouping utterances sharing the
same goal. We can assume an anaphor and its
referent appear within the same utterance group.
Once the utterance group is identified, the sys-
tem finds the referent based on matching variables
between plan operators.
After filtering out the candidates, there still
might be more than one candidate left. In such a
case, each candidate is assigned a score that is cal-
culated based on the following factors: saliency,
agent?s view, and user?s view.
4 Handling Spatial Vagueness
To interface the macro- and micro-planning, we
introduced the SPACE object which represents a
location in the virtual world. Because of space
limitations, we briefly explain the SPACE object.
The macro planner uses plan operators de-
scribed in terms of the logical forms. Thus, the
93
Utterance 
includes clue 
word?
Enumerate 
candidates by 
surface information
Identify utterance 
including referent
by clue word
Anaphora
resolved?
Unique
candidate?
Filtering
candidates
Unique
candidate?
Scoring
Referent 
identified
no
yes
no
no
yes
Resolve anaphora
by case matching
no
yes
yes
Figure 3: Anaphora resolution algorithm
SPACE object is designed to behave as a sym-
bolic object in the macro-planning by referring to
its unique identifier. On the other hand, a loca-
tion could be vague and the most plausible place
changes depending on the situation. Therefore, it
should be treated as a certain region rather than a
single point. To fulfill this requirement, we adopt
the idea of the potential model proposed by Ya-
mada et al (?). Vagueness of a location is nat-
urally realized as a potential function embedded
in the SPACE object. The most plausible point is
calculated by using the potential function with the
Steepest Descent Method on request.
Consider the following short conversation be-
tween a human (H) and a virtual agent (A).
H: Do you see a ball in front of the desk?
A: Yes.
H: Put it on the desk.
When the first utterance is given in the situation
shown in Fig. 1, the discourse analysis module
identifies an instance of ?a ball? in the following
steps.
back
left
right
Desk
front
Viewpoint
Ball(1)
(2)
(3)
(4)
Figure 4: Adjustment of axis
(A) space#1 := new inFrontOf(desk#1, viewpoint#1,
MIRROR)
(B) list#1 := space#1.findObjects()
(C) ball#1 := list#1.getFirstMatch(kindOf(BALL))
In step (A), an instance of SPACE is created as
an instance of the class inFrontOf. The construc-
tor of inFrontOf takes three arguments: the ref-
erence object, the viewpoint, and the axis order.
Although it is necessary to identify the reference
frame, we focus on the calculation of potential
functions given a reference frame.
Suppose the parameters of inFrontOf have been
resolved in the preceding steps, and the discourse
analysis module chooses the axis mirror order and
the orientation of the axis based on the viewpoint
of the light-colored arrows in Fig. 4. The closest
arrow to the viewpoint-based ?front? axis ((1) in
Fig. 4) is chosen as the ?front? of the desk. Then,
the parameters of potential function correspond-
ing to ?front? are set.
In step (B), the method matchObjects() returns
a list of objects located in the potential field of
space#1 shown in Fig. 5. The objects in the list are
sorted in descending order of the potential value
of their location.
In step (C), the most plausible object satisfy-
ing the type constraint (BALL) is selected by the
method getFirstMatch().
When receiving the next utterance, ?Put it on
the desk.?, the discourse analysis module resolves
the referent of the pronoun ?it? and extracts the
user?s goal.
walk(inFrontOf(ball#1, viewpoint#1, MIRROR)
AND reachableByHand(ball#1)
AND NOT(occupied(ball#1)))
The movement walk takes a SPACE object rep-
resenting its destination as an argument. In this
example, the conjunction of three SPACE objects
94
Viewpoint
Figure 5: Potential field of space#1
is given as the argument. The potential function
of the resultant SPACE is calculated by multiply-
ing the values of the corresponding three potential
functions at each point.
As this example illustrates, the SPACE object
effectively plays a role as a mediator between the
macro and micro planning.
5 Conclusions and Future Work
We have introduced our prototype systemK3 , two
distinctive features of which are described in this
paper. Plan-based anaphora resolution enables
K3 to interpret the user?s intention more pre-
cisely than the previous, surface-cue-based reso-
lution algorithms. The SPACE object is designed
to bridge the gap between the symbolic system
(language processing) and the continuous system
(animation generation) . In what follows, we de-
scribe the research agenda of our project.
One-to-many Conversation. Conversational
agent systems should deal with one-to-many con-
versations as well as one-to-one conversations.
In a one-to-many conversation, it is not easy to
decide who is the intended listener. The situation
gets worse when a speaker is concerned with
only performing an action without caring who
does it. In such cases, agents have to request
clarifications or negotiate among themselves.
Agent Coordination. In one-to-many conver-
sations, agents must coordinate each other. Some
sorts of coordination are explicitly requested by
user, e.g., ?Agent A and B tidy up the table,
please.? But other kinds of coordination are im-
plicitly requested, e.g., ?Agent A hands agent B
the box, please.? In this case, the speaker asks
agent B nothing explicitly. However, agent B
must react to the request for agent A and coor-
dinate with agent A to receive a box.
Parallel Actions. Most intelligent agent sys-
tems perform only one action at a time. Yet, if we
want to make systems become more flexible, we
must enable them to handle more than one action
at a time. Hence, they must speak while walking,
wave while nodding, and so on.
Memory System. A history database is not
enough to serve realistic dialogue in the domain
of K3 . In such a domain, people often mention a
previous state, e.g., ?Put the ball back to the place
where it was.? To comply such a request, agents
must have a human-like memory system.
Interruption Handling. Agents sometimes
misunderstand requests and perform not intended
actions. In case of human conversations, a
speaker usually interrupts hearer and try to repair
misunderstanding. Conversational agents also
should be able to accept such interruptions.
Interruption handling is also essential to request
a next action before agents finish actions.
Acknowledgment
This work is partially supported by a Grant-in-
Aid for Creative Scientific Research 13NP0301,
the Ministry of Education, Culture, Sports, Sci-
ence and Technology of Japan. The URL of
the project is http://www.cl.cs.titech.ac.
jp/sinpro/en/index.html.
References
P. R. Cohen. 1984. The pragmatics of referring and
the modality of communication. Computational
Linguistics, 10(2):97?146.
R. E. Fikes. 1971. STRIPS: A new approach to the
application of theorem problem solving. Artificial
Intelligence, 2:189?208.
B. J. Grosz, A. K. Joshi, and P. Weinstein. 1995.
Centering: A framework for modeling the local co-
herence of discourse. Computational Linguistics,
21(2):203?226.
H. Tanaka, T. Tokunaga, and Y. Shinyama. 2004. An-
imated agents capable of understanding natural lan-
guage and performing actions. In Life-Like Char-
acters, pages 429?444. Springer.
95
M. A. Walker, A. K. Joshi, and E. F. Prince, editors.
1998. Centering Theory in Discourse. Clarendon
Press Oxford.
T. Winograd. 1972. Understanding Natural Lan-
guage. Academic Press.
A. Yamada, T. Nishida, and S. Doshita. 1988. Fig-
uring out most plausible interpretation from spa-
tial description. In the 12th International Con-
ference on Computational Linguistics (COLING),
pages 764?769.
96
eBonsai: An integrated environment for annotating treebanks
Ichikawa Hiroshi, Noguchi Masaki, Hashimoto Taiichi, Tokunaga Takenobu, Tanaka Hozumi
Department of Computer Science, Tokyo Institute of Technology
Tokyo Meguro ?Ookayama 2-12-1, Japan
ichikawa@cl.cs.titech.ac.jp
Abstract
Syntactically annotated corpora (tree-
banks) play an important role in re-
cent statistical natural language pro-
cessing. However, building a large tree-
bank is labor intensive and time con-
suming work. To remedy this prob-
lem, there have been many attempts to
develop software tools for annotating
treebanks.
This paper presents an integrated en-
vironment for annotating a treebank,
called eBonsai. eBonsai helps annota-
tors to choose a correct syntactic struc-
ture of a sentence from outputs of a
parser, allowing the annotators to re-
trieve similar sentences in the treebank
for referring to their structures.
1 Introduction
Statistical approach has been a main stream of
natural language processing research for the last
decade. Particularly, syntactically annotated cor-
pora (treebanks), such as Penn Treebank (Marcus
et al, 1993), Negra Corpus (Skut et al, 1997)
and EDR Corpus (Jap, 1994), contribute to im-
prove the performance of morpho-syntactic anal-
ysis systems. It is notorious, however, that build-
ing a large treebank is labor intensive and time
consuming work. In addition, it is quite difficult
to keep quality and consistency of a large tree-
bank. To remedy this problem, there have been
many attempts to develop software tools for anno-
tating treebanks (Plaehn and Brants, 2000; Bird et
al., 2002).
This paper presents an integrated environment
for annotating treebanks, called eBonsai. Fig-
ure 1 shows a snapshot of eBonsai. eBonsai
first performs syntactic analysis of a sentence us-
ing a parser based on GLR algorithm (MSLR
parser) (Tanaka et al, 1993), and provides can-
didates of its syntactic structure. An annotator
chooses a correct structure from these candidates.
When choosing a correct structure, the annotator
can consult the system to retrieve already anno-
tated similar sentences to make the current deci-
sion. Integration of annotation and retrieval is a
significant feature of eBonsai.
To realize the tight coupling of annotation and
retrieval, eBonsai has been implemented as the
following two plug-in modules of an universal
tool platform: Eclipse (The Eclipse Foundation,
2001).
? Annotation plug-in module: This module
helps to choose a correct syntactic structure
from candidate structures.
? Retrieval plug-in module: This module re-
trieves similar sentences to a sentence in
question from already annotated sentences in
the treebank.
These two plug-in modules work cooperatively
in the Eclipse framework. For example, infor-
mation can be transferred easily between these
two modules in a copy-and-past manner. Further-
more, since they are implemented as Eclipse plug-
in modules, these functionalities can also inter-
act with other plug-in modules and Eclipse native
features such as CVS.
108
Figure 1: A snapshot of eBonsai
	

	

	
 
   


   	       
 
  
   
                    
 Evaluation of a Japanese CFG Derived from a Syntactically Annotated
Corpus with Respect to Dependency Measures
Tomoya Noro  Chimato Koike Taiichi Hashimoto 
Takenobu Tokunaga  Hozumi Tanaka 
  Graduate School of Information Science and Engineering
Tokyo Institute of Technology, Tokyo
 noro@tt,taiichi@cl,take@cl.cs.titech.ac.jp
 Graduate School of Science and Engineering, Tokyo Institute of Technology, Tokyo
chimato@it.ss.titech.ac.jp
  School of Computer and Cognitive Sciences, Chukyo University, Nagoya
htanaka@sccs.chukyo-u.ac.jp
Abstract
Parsing is one of the important
processes for natural language process-
ing and, in general, a large-scale CFG
is used to parse a wide variety of
sentences. For many languages, a
CFG is derived from a large-scale
syntactically annotated corpus, and
many parsing algorithms using CFGs
have been proposed. However, we
could not apply them to Japanese since
a Japanese syntactically annotated
corpus has not been available as of yet.
In order to solve the problem, we have
been building a large-scale Japanese
syntactically annotated corpus. In this
paper, we show the evaluation results
of a CFG derived from our corpus
and compare it with results of some
Japanese dependency analyzers.
1 Introduction
Parsing is one of the important processes for nat-
ural language processing and, in general, a large-
scale CFG is used to parse a wide variety of sen-
tences. Although it is difficult to build a large-
scale CFG manually, a CFG can be derived from
a large-scale syntactically annotated corpus. For
many languages, large-scale syntactically anno-
tated corpora have been built (e.g. the Penn Tree-
bank (Marcus et al, 1993)), and many parsing al-
gorithms using CFGs have been proposed.
However, such a syntactically annotated corpus
has not been built for Japanese as of yet. De-
pendency analysis is preferred in order to analyze
Japanese sentences (dependency relation between
Japanese phrasal unit, called bunsetsu) (Kuro-
hashi and Nagao, 1998; Uchimoto et al, 2000;
Kudo and Matsumoto, 2002), and only a few stud-
ies about Japanese CFG have been conducted.
Since many efficient parsing algorithms for CFG
have been proposed, a Japanese CFG is necessary
to apply the algorithms to Japanese.
We have been building a large-scale Japanese
syntactically annotated corpus to derive a
Japanese CFG for syntactic parsing (Noro et al,
2004a; Noro et al, 2004b). According to the re-
sult, a CFG derived from the corpus can parse
sentences with high accuracy and coverage. How-
ever, as mentioned previously, dependency analy-
sis is usually adopted in Japanese NLP, and it
is difficult to compare our result with results of
other dependency analysis since we evaluated our
CFG with respect to phrase structure based mea-
sure. Although we evaluated with respect to de-
pendency measure as a preliminary experiment in
order to compare, the scale was quite small (eval-
uated on only 100 sentences) and the comparison
was unfair since we did not use the same evalua-
tion data.
In this paper, we show an evaluation result of a
CFG derived from our corpus and compare it with
results of other Japanese dependency analyzers.
We used the Kyoto corpus (Kurohashi and Nagao,
1997) for evaluation data, and chose KNP (Kuro-
hashi and Nagao, 1998) and CaboCha (Kudo and
Matsumoto, 2002) for comparison.
9
Syntactically
Annotated Corpus
CFG Annotation
Policy
Deriving a CFG
Analyzing Causes of Ambiguity,
Deciding on an Annotation Policy
Modifying the Corpus
Figure 1: Procedure of building a syntactically annotated corpus
2 Annotation Policy
In this section, we start by introducing our policy
for annotating a Japanese syntactically annotated
corpus briefly. The details are given in (Noro et
al., 2004a; Noro et al, 2004b)
Although a large-scale CFG can be easily de-
rived from a syntactically annotated corpus, such
a CFG has a problem that it creates a large-
number of parse results during syntactic parsing
(i.e. high ambiguity). A syntactically annotated
corpus should be built so that the derived CFG
would create less ambiguity.
We have been building such Japanese corpus
by using the following method (Figure 1):
1. Derive a CFG from an existing corpus.
2. Analyze major causes of ambiguity.
3. Determine a policy for modifying the cor-
pus.
4. Modify the corpus according to the policy
and derive a CFG from it again.
5. Repeat steps (2) - (4) until most problems are
solved.
We focused on two major causes of ambiguity:
Lack of Syntactic Information: Some syntac-
tic information which is important for syn-
tactic parsing might be lost during the CFG
derivation since CFG rules generally repre-
sent only structures of subtree with the depth
of 1 (relation between a parent node and
some child nodes).
Need for Semantic Information: Not only
syntactic information but also semantic
information is necessary for disambiguation
in some cases.
To avoid the first cause, we considered which syn-
tactic information is necessary for syntactic pars-
ing and added the information to each interme-
diate node in the structure. On the other hand,
we considered ambiguity due to the second cause
better be left to the subsequent semantic process-
ing since it is difficult to reduce such ambiguity
without recourse to semantic information during
syntactic parsing. This can be achieved by rep-
resenting the ambiguous cases as the same struc-
ture. We assume that syntactic analysis based on a
large-scale CFG is followed by semantic analysis,
and the second cause of ambiguity is supposed
to be disambiguated in the subsequent semantic
processing.
The main aspects of our policy are as follows:
Verb Conjugation: Information about verb
conjugation is added to each intermediate
node related to the verb (cf. ?SPLIT-VP?
in (Klein and Manning, 2003) and ?Verb
Form? in (Schiehlen, 2004)).
Compound Noun Structure: Structure ambi-
guity of compound noun is represented as
the same structure regardless of the meaning
or word-formation as Shirai et al described
in (Shirai et al, 1995).
Adnominal and Adverbial Phrase Attachment:
Structure ambiguity of adnominal phrase
attachment is represented as the same
structure regardless of the meaning while
structure ambiguity of adverbial phrase
attachment is distinguished by meaning.
In case of a phrase like ?watashi no chichi
no hon (my father?s book)?, the structure
is same whether the adnominal phrase
?watashi no (my)? attaches to the noun
?chichi (father)? or the noun ?hon (book)?.
On the other hand, in case of a sentence
10
Input
Producing
Possible Parse Trees
Using a CFG
Top-n Possible Parse Trees
Using PCFG, PGLR model, etc.
Final Interpretation
Disambiguation of 
Adnominal Phrase Attachment
One Parse Tree
Disambiguation of
Adverbial Phrase Attachment
Figure 2: Procedure in the subsequent processing
Segmentation Accuracy   # sentences segmented into bunsetsu correctly
# all sentences
Dependency Accuracy   # correct dependency relations
# all dependency relations
Sentence Accuracy   # sentences determined all relations correctly
# sentences segmented in bunsetsu correctly
Figure 3: Dependency measures
like ?kare ga umi wo egaita e wo katta?,
we distinguish the structure according to
whether the adverbial phrase ?kare ga (he)?
attaches to the verb ?egaita (paint)? (it
means ?I bought a picture of a sea painted
by him?) or the verb ?katta (buy)? (it means
?he bought a picture of a sea?).
Conjunctive Structure: Conjunctive structure
is not specified during syntactic parsing, in-
stead their analysis is left for the subsequent
processing (contrary to (Kurohashi and Na-
gao, 1994)).
We have decided to deal with adnominal phrase
attachment and adverbial phrase attachment sep-
arately in our policy since we believe that a dif-
ferent algorithm should be used to disambiguate
them. In the subsequent processing, we assume
that adverbial phrase attachment would be disam-
biguated by choosing one parse tree among the
results at first, and adnominal phrase attachment
would be disambiguated by choosing one inter-
pretation among all of interpretations which the
parse tree represents (Figure 2).
We used the EDR corpus (EDR, 1994) for
developing our annotation policy, and annotated
8,911 sentences in the corpus and 20,190 sen-
tences in the RWC corpus (Hashida et al, 1998).
In the following evaluation, we used the latter
one.
3 Experimental Setup
As mentioned previously, in general, analyzing
dependency relations between bunsetsu is pre-
ferred in Japanese, which makes it difficult to
compare the result by the CFG with the result
by dependency analysis. In order to compare
with other dependency analysis, we evaluated our
derived CFG with respect to dependency mea-
sures shown in Figure 3. Note that sentences
which are not segmented into bunsetsu correctly
are dropped from the evaluation data when we
evaluate dependency accuracy and sentence accu-
racy.
A CFG is derived from all sentences in our cor-
pus, with which we parsed 6,931 sentences (POS
sequences) in the Kyoto corpus 1 by MSLR parser
(Shirai et al, 2000). The Kyoto corpus has an-
1On average, 8.89 bunsetsu in a sentence.
11
Syntactically
Annotated
Corpus
CFG
CFG
Derivation
Top-n Parse Results
(Phrase Structure)
Top-n Parse Results
(Dependency Relations)Extract
Dependency Relations
POS
Sequence
Kyoto Corpus
(Dependency Relations)
Parsing & Ranking
POS Conversion
Evaluation
Figure 4: Evaluation with respect to dependency measure
notation in terms of dependency relations among
bunsetsu, and it is usually used for evaluation of
dependency analysis. The parser is trained ac-
cording to probabilistic generalized LR (PGLR)
model (Inui et al, 2000) (all sentences are used
for training), and parse results are ranked by the
model.
The experiment was carried out as follows
(Figure 4):
1. Convert POS tags automatically to the RWC
tag set.
2. Parse the POS sequence using a CFG derived
from our corpus.
3. Rank the parse results by PGLR model and
pick up the top-  parse results.
4. Extract dependency relations among bun-
setsu for each result.
5. Choose the result which is closest to the
gold-standard and evaluate it.
Since the tag set of the Kyoto corpus is different
from that of the RWC corpus, a POS conversion
in step (1) is necessary. It is a rule-based con-
version, and the accuracy is about 80%. It seems
that the low conversion accuracy would damage
the evaluation result. We will discuss this issue in
the next section.
In the 4th step of the experimental procedure,
we determine boundaries of bunsetsu and depen-
dency relations among the bunsetsu in a sentence
with the CFG rules included in the phrase struc-
ture of the sentence. Some CFG rules in our CFG
indicate positions of bunsetsu boundaries. For ex-
ample, a CFG rule ?NP   AdnP NP? (?NP? and
?AdnP? stand for a noun phrase and an adnom-
inal phrase respectively) indicates that there is a
boundary of bunsetsu between the two phrases in
the right-hand side of the CFG rule (i.e. between
the noun phrase and the adnominal phrase), and
that a bunsetsu including the head word of the ad-
nominal phrase depends on a bunsetsu including
the head word of the noun phrase. An example of
?Nihon teien no nagame ga subarashii (The view
of the Japanese garden is wonderful)? is shown in
Figure 5.
Structure ambiguity of adnominal phrase at-
tachment needs to be disambiguated in extracting
dependency relations in step (4) since it is repre-
sented as the same structure according to our pol-
icy 2. We disambiguate adnominal phrase attach-
ment based on one of the following assumptions:
NEAREST: Every ambiguous adnominal
phrase attaches to the nearest noun among
the nouns which the phrase could attach to.
BEST: Choose the best noun among the nouns
which could be attached to (assume that
disambiguation of adnominal phrase attach-
ment was done correctly) 3.
2Since dependency relations are not categorized in the
Kyoto corpus, it is difficult to know how many relations rep-
resenting adnominal phrase attachment are included in the
evaluation data. On the other hand, among the top parse re-
sults ranked by PGLR model (i.e in case of      in section
4), about 34.1% of all dependency relations represent ad-
nominal phrase attachment, and about 23.4% of them (i.e.
about 8.0% of all relations) remain ambiguous.
3We choose the best noun automatically by referring to
12
Nihon
(Japanese)
teien
(garden)
no nagame
(view)
ga subarashii
(wonderful)
n n p n p adj
<comp.n>
<NP>
<AdnP>
<NP>
<NP>
<PP>
<AdjP>
<AdjP>
<S>
bunsetsu #1 bunsetsu #2 bunsetsu #3
Bunsetsu No. Word Sequence Bunsetsu Which is Depended on
1 nihon teien no 2
2 nagame ga 3
3 subarashii ?
Figure 5: Extracting Dependency Relations from a Pharse Structure
?NEAREST? is a quite simple way for disam-
biguation, and it would be the baseline model.
On the other hand, since we assume that struc-
ture ambiguity of adnominal phrase attachment is
supposed to be disambiguated in the subsequent
semantic processing, ?BEST? would be the upper
bound and we could not overcome the accuracy
even if the disambiguation was done perfectly in
the subsequent processing.
To take two noun phrases ?watashi no chichi
no hon (my father?s book)? and ?watashi no ka-
gaku no hon (my book on science)? as examples
(the correct answer is that the adnominal phrase
?watashi no (my)? attaches to the noun ?chichi
(father)? in the former case, and attaches to the
noun ?hon (book)? in the latter case), ?NEAR-
EST? attaches to the adnominal phrase ?watashi
no? to the nouns ?chichi? and ?kagaku (science)?
regardless of their meanings. ?BEST? attaches
the adnominal phrase to the noun ?chichi? in the
former case, and attaches to the noun ?hon? in the
latter case.
Although structure ambiguity of compound
noun is also represented as the same structure re-
the Kyoto corpus. If the noun which is attached to in the
Kyoto corpus is not in the candidates, we choose the nearest
noun (i.e. ?NEAREST?).
gardless of the meaning or word-formation, we
have nothing to do with the structure ambiguity
since a bunsetsu is a larger unit than a compound
noun. Furthermore, since dependency relations
are not categorized, we do not have to care about
whether two bunsetsu have conjunctive relation
with each other or not.
In order to compare our result with that of
other dependency analyzers, we used two well-
known Japanese dependency analyzers, KNP and
CaboCha, and analyzed dependency structure of
the sentences in the same evaluation data set. In
both cases, POS tagged sentences are used as the
input. Since CaboCha uses the same tagset as the
RWC corpus, we converted POS tags in the same
way as step (1) in our experimental procedure. On
the other hand, since KNP uses the tagset adopted
by the Kyoto corpus, POS tags do not have to be
converted in case of analyzing by KNP.
4 Results
Table 1 shows the results when     , which
means the top parse result of each sentence is used
for evaluation. In this case, ?NEAREST? means
only PGLR model was used for disambiguation
without any other information (e.g. lexical infor-
13
Table 1: Segmentation, dependency, and sentence accuracy (    )
Segmentation Dependency Sentence
NEAREST 65.68% 87.88% 50.47%
BEST 65.68% 90.27% 57.73%
KNP 96.90% 91.32% 60.07%
CaboCha 84.88% 92.88% 64.48%
 45
 50
 55
 60
 65
 70
 75
 80
 85
 90
 95
 100
 0  10  20  30  40  50  60  70  80  90  100
Se
gm
en
ta
tio
n 
/ D
ep
en
de
nc
y 
/ S
en
te
nc
e 
Ac
cu
ra
cy
 (%
)
Rank by PGLR model (top-n parse results)
76.53
93.10
66.85
95.24
75.72
Segmentation Accuracy
Dependency Accuracy (NEAREST)
Dependency Accuracy (BEST)
Sentence Accuracy (NEAREST)
Sentence Accuracy (BEST)
Figure 6: Segmentation, dependency, and sentence accuracy (        )
mation, semantic information, etc.) On the other
hand, ?BEST? means only disambiguation of ad-
nominal phrase attachment was done in the subse-
quent processing. Results by KNP and CaboCha
are shown in the same table for comparison.
As seen from Table 1, accuracy is still lower
than KNP and CaboCha even if disambiguation
of adnominal phrase attachment was done cor-
rectly in the subsequent processing. However,
in this case, we do not use any information but
PGLR model for disambiguation of any relations
except adnominal phrase attachment (i.e. adver-
bial phrase attachment).
Next, assuming that disambiguation of other
relations, we carried out another evaluation
changing   from 1 to 100. The result is shown
in Figure 6. Dependency accuracy could achieve
about 95.24% for ?BEST?, which exceeds the
dependency accuracy by KNP and CaboCha, if
choosing the best result among top-100 parse re-
sults ranked by PGLR model would be done cor-
rectly in the subsequent processing 4. From the
results, we can conclude the accuracy will in-
crease as soon as lexical and semantic informa-
tion is incorporated in the subsequent processing
5
.
However, segmentation accuracy is still signif-
icantly lower. The main reasons are as follows:
POS Conversion Error: As mentioned previ-
ously, we converted POS tags automatically
since the POS system of the Kyoto corpus is
4Even if only top-10 parse results are considered, our
CFG have a possibility to outperform KNP and CaboCha
5In some studies, it is said that lexical information has
little impact on accuracy (Bikel, 2004). However, we think
some lexical information is useful for disambiguation, and
it is necessary to consider what kind of lexical information
could improve the accuracy.
14
different from that of the RWC corpus. How-
ever, accuracy of the conversion is not high
(about 80%). Since we used only POS in-
formation and did not use any word infor-
mation for parsing, the result can be easily
affected by the conversion error. Segmen-
tation accuracy by CaboCha is also a little
lower than accuracy by KNP. Since POS tags
were converted in the same way, we think
the reason is same. However, the difference
between the accuracy by KNP and CaboCha
is smaller since CaboCha uses not only POS
information but also word information.
Difference in Segmentation Policy: There is
difference in bunsetsu segmentation policy
between the Kyoto corpus and our corpus.
For example:
1. 3 gatsu 31 nichi gogo 9 ji 43 fun goro,
jishin ga atta
(An earthquake occurred at around
9:43 p.m., March 1st.)
2. gezan suru no wo miokutta
(We gave up going down the moun-
tain.)
In the former case, the underlined part is
segmented into 5 bunsetsu (?3 gatsu?, ?31
nichi?, ?gogo?, ?9 ji?, and ?43 fun goro,?) in
the Kyoto corpus, while it is not segmented
in our corpus. On the other hand, in the latter
case, the underlined part is segmented into 2
bunsetsu (?gezan suru? and ?no wo?) in our
corpus, while it is not segmented in the Ky-
oto corpus. By correction of these two types
of error, segmentation accuracy improved by
4.35% (76.53%  80.88%) and dependency
accuracy improved by 0.61% (95.24%  
95.85%).
5 Conclusion
We have been building a large-scale Japanese syn-
tactically annotated corpus. In this paper, we eval-
uated a CFG derived from the corpus with re-
spect to dependency measure. We assume that
parse results created by our CFG is supposed to
be re-analyzed in the subsequent processing using
semantic information, and the result shows that
parsing accuracy will increase when semantic in-
formation is incorporated.
We also compared our result with other depen-
dency analyzers, KNP and CaboCha. Although
dependency accuracy of our CFG cannot reach
those of KNP and CaboCha if only PGLR model
is used for disambiguation, it would exceed if
disambiguation in the subsequent processing was
done correctly.
As future work, since we assume that the
parse results created by our CFG are re-analyzed
in the subsequent processing, we need to inte-
grate the subsequent processing into the current
framework. Collins proposed a method for re-
ranking the output from an initial statistical parser
(Collins, 2000). However, it is not enough for us
since we represent some ambiguous cases as the
same structure (we need to consider the ambigu-
ity included in each parse result). Our policy has
been considered with several types of ambiguity:
structure of compound noun, adnominal phrase
attachment, adverbial phrase attachment and con-
junctive structure. We are planning to provide
each method individually and integrate them into
a single process.
Although we attempt to re-analyze after pars-
ing, it seems that some problem should be solved
before parsing. For example, ellipsis often occurs
in Japanese. It is difficult to deal with ellipsis (es-
pecially, postpositions and verbs) in a CFG frame-
work, resulting in higher ambiguity. It would
be helpful if the positions where some words are
omitted in a sentence were detected and marked
in advance.
References
Daniel M. Bikel. 2004. A distributional analysis of a
lexicalized statistical parsing model. In 2004 Con-
ference on Empirical Methods in Natural Language
Processing, pages 182?189.
Michael Collins. 2000. Disriminative reranking for
natural language parsing. In 17th International
Conference on Machine Learning, pages 175?182.
EDR, 1994. EDR Electronic Dictionary User?s Man-
ual, 2.1 edition. In Japanese.
Koichi Hashida, Hitoshi Isahara, Takenobu Tokunaga,
Minako Hashimoto, Shiho Ogino, and Wakako
Kashino. 1998. The RWC text databases. In
15
The First International Conference on Language
Resource and Evaluation, pages 457?461.
Kentaro Inui, Virach Sornlertamvanich, Hozumi
Tanaka, and Takenobu Tokunaga. 2000. Proba-
bilistic GLR parsing. In Harry Bunt and Anton Ni-
jholt, editors, Advances in Probabilistic and Other
Parsing Technologies, pages 85?104. Kluwer Aca-
demic Publishers.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In the 41st Annual
Meeting of the Association for Computational Lin-
guistics, pages 423?430.
Taku Kudo and Yuji Matsumoto. 2002. Japanese
dependency analysis using cascaded chunking. In
CONLL 2002.
Sadao Kurohashi and Makoto Nagao. 1994. A syn-
tactic analysis method of long Japanese sentences
based on the detection of conjunctive structures.
Computational Linguistic, 20(4):507?534.
Sadao Kurohashi and Makoto Nagao. 1997. Kyoto
university text corpus project. In the 3rd Confer-
ence for Natural Language Processing, pages 115?
118. In Japanese.
Sadao Kurohashi and Makoto Nagao. 1998. Build-
ing a Japanese parsed corpus while improving the
parsing system. In the first International Confer-
ence on Language Resources and Evaluation, pages
719?724.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Tomoya Noro, Taiichi Hashimoto, Takenobu Toku-
naga, and Hozumi Tanaka. 2004a. Building a
large-scale japanese CFG for syntactic parsing. In
The 4th Workshop on Asian Language Processing,
pages 71?78.
Tomoya Noro, Taiichi Hashimoto, Takenobu Toku-
naga, and Hozumi Tanaka. 2004b. A large-scale
japanese CFG derived from a syntactically anno-
tated corpus and its evaluation. In The 3rd Work-
shop on Treebanks and Linguistic Theories, pages
115?126.
Michcael Schiehlen. 2004. Annotation strategies for
probabilistic parsing in German. In the 20th Inter-
national Conference on Computational Linguistics,
pages 390?396.
Kiyoaki Shirai, Takenobu Tokunaga, and Hozumi
Tanaka. 1995. Automatic extraction of Japanese
grammar from a bracketed corpus. In Natural Lan-
guage Processing Pacific Rim Symposium, pages
211?216.
Kiyoaki Shirai, Masahiro Ueki, Taiichi Hashimoto,
Takenobu Tokunaga, and Hozumi Tanaka. 2000.
MSLR parser ? tools for natural language analysis.
Journal of Natural Language Processing, 7(5):93?
112. In Japanese.
Kiyotaka Uchimoto, Masaki Murata, Satoshi Sekine,
and Hitoshi Isahara. 2000. Dependency model us-
ing posterior context. In 6th International Work-
shop on Parsing Technologies.
16
Constructing Taxonomy of Numerative Classifiers for Asian Languages
Kiyoaki Shirai
JAIST
kshirai@jaist.ac.jp
Takenobu Tokunaga
Tokyo Inst. of Tech.
take@cl.cs.titech.ac.jp
Chu-Ren Huang
Academia Sinica
churenhuang@gmail.com
Shu-Kai Hsieh
National Taiwan Normal Univ.
shukai@gmail.com
Tzu-Yi Kuo
Academia Sinica
ivykuo@gate.sinica.edu.tw
Virach Sornlertlamvanich
TCL, NICT
virach@tcllab.org
Thatsanee Charoenporn
TCL, NICT
thatsanee@tcllab.org
Abstract
Numerative classifiers are ubiquitous in
many Asian languages. This paper pro-
poses a method to construct a taxonomy
of numerative classifiers based on a noun-
classifier agreement database. The taxon-
omy defines superordinate-subordinate rela-
tion among numerative classifiers and rep-
resents the relations in tree structures. The
experiments to construct taxonomies were
conducted for evaluation by using data from
three different languages: Chinese, Japanese
and Thai. We found that our method was
promising for Chinese and Japanese, but in-
appropriate for Thai. It confirms that there
really is no hierarchy among Thai classifiers.
1 Introduction
Many Asian languages do not mark grammatical
numbers (singular/plural) in noun form, but use nu-
merative classifiers together with numerals instead
when describing the number of nouns. Numerative
classifiers (hereafter ?classifiers?) are used with a
limited group of nouns, in particular material nouns.
In English, for example: ?three pieces of paper?. In
Asian languages these classifiers are ubiquitous and
used with common nouns. Therefore the number of
classifiers is much larger than in Western languages.
An agreement between nouns and classifiers is also
necessary, i.e., a certain noun specifies possible clas-
sifiers. The agreement is determined based on var-
ious aspects of a noun, such as its meaning, shape,
pragmatic aspect and so on.
This paper proposes a method to automati-
cally construct a taxonomy of numerative classi-
fiers for Asian languages. The taxonomy defines
superordinate-subordinate relations between classi-
fiers. For instance, the Japanese classifier ?? (to?)?
is used for counting big animals such as elephants
and tigers, while ?? (hiki)? is used for all animals.
Since ??? can be considered more general than ?
??, ??? is the superordinate classifier of ???, rep-
resented as ???  ??? in this paper. The taxon-
omy represents such superordinate-subordinate rela-
tions between classifiers in the form of a tree struc-
ture. A taxonomy of classifiers would be fundamen-
tal knowledge for natural language processing. In
addition, it will be useful for language learners, be-
cause learning usage of classifiers is rather difficult,
especially for Western language speakers.
We evaluate the proposed method by using the
data of three Asian languages: Chinese, Japanese
and Thai.
2 Noun-classifier agreement database
First, let us introduce usages of classifiers in Asian
languages. In the following examples, ?CL? stands
for classifier.
? Chinese: yi-ju
(CL)
dian-hua
(telephone)
? ? ? a telephone
? Japanese: inu
(dog)
2 hiki
(CL)
? ? ? 2 dogs
? Thai: nakrian
(student)
3 khon
(CL)
? ? ? 3 students
397
As mentioned earlier, the agreement between nouns
and classifiers is observed. For instance, the
Japanese classifier ?hiki? in the above example
agrees with only animals. The agreement is also
found in Chinese and Thai.
The proposed method to construct a classifier tax-
onomy is based on agreement between nouns and
classifiers. First we prepare a collection of pairs
(n, c) of a noun n and a classifier c which agrees
with n for a language. The statistics of our Chinese,
Japanese, and Thai database are summarized in Ta-
ble 1.
Table 1: Noun-classifier agreement database
Chinese Japanese Thai
No. of (n,c) pairs 28,202 9,582 9,618
No. of nouns (type) 10,250 4,624 8,224
No. of CLs (type) 205 331 608
The Japanese database was built by extracting
noun-classifier pairs from a dictionary (Iida, 2004)
which enumerates nouns and their corresponding
classifiers. The Chinese database was derived from
a dictionary (Huang et al, 1997). The Thai database
consists of a mixture of two kinds of noun-classifier
pairs: 8,024 nouns and their corresponding classi-
fiers from a dictionary of a machine translation sys-
tem (CICC, 1995) and 200 from a corpus. The pairs
from the corpus were manually checked for their va-
lidity.
3 Proposed Method
3.1 Extracting superordinate-subordinate
relations of classifiers
We extracted superordinate-subordinate classifier
pairs based on inclusive relations of sets of nouns
agreeing with those classifiers. Suppose that Nk is
a set of nouns that agrees with a classifier ck. If Ni
subsumes Nj (Ni ? Nj), we can estimate that ci
subsumes cj (ci  cj). For instance, in our Japanese
database, the classifier ?? (ten)? agrees with shops
such as ?drug store?, ?kiosk? and ?restaurant?, and
these nouns also agree with ?? (ken)?, since ??? is
a classifier which agrees with any kind of building.
Thus, we can estimate the relation ???  ???.
Given a certain classifier cj , ci satisfying the fol-
lowing two conditions (1) and (2) is considered as a
N
j
N
i
Figure 1: Relation of sets of nouns agreeing with
classifiers
superordinate classifier of cj .
|Ni| > |Nj | (1)
IR(ci, cj) ? Tir
where IR(ci, cj)
def
=
|N
i
?N
j
|
|N
j
|
(2)
Condition (1) requires that a superordinate classifier
agrees with more nouns than a subordinate classifier.
IR(ci, cj) is an inclusion ratio representing to what
extent nouns in Nj are also included in Ni (the ratio
of the light gray area to the area of the small circle
in Figure 1).
Condition (2) means that if IR(ci, cj) is greater
than a certain threshold T
ir
, we estimate a
superordinate-subordinate relation between ci and
cj . The basic idea is that superordinate-subordinate
relations are extracted when Nj is a proper subset
of Ni, i.e. IR(ci, cj) = 1, but this is too strict. In
order to extract more relations, we loosen this condi-
tion such that relations are extracted when IR(ci, cj)
is large enough. If we set Tir lower, more relations
can be acquired, but they may be less reliable.
Table 2: Extraction of superordinate-subordinate re-
lations
Chinese Japanese Thai
T
ir
0.7 0.6 0.6
No. of extracted relations 251 322 239
No. of CLs not in 36 76 395
the extracted relations (18%) (23%) (61%)
Table 2 shows the results of our experiments to
extract superordinate-subordinate relations of classi-
fiers. The threshold T
ir
was determined in an ad hoc
manner for each language. The numbers of extracted
superordinate-subordinate relations are shown in the
second row in the table. Manual inspection of the
sampled relations revealed that many reasonable re-
lations were extracted. The objective evaluation of
these extracted relations will be discussed in 4.2.
398
The third row in Table 2 indicates the numbers of
classifiers which were not included in the extracted
superordinate-subordinate relations with its ratio to
the total number of classifiers in the database in
parentheses. We found that no relation is extracted
for a large number of Thai classifiers.
3.2 Constructing structure
The structure of a taxonomy is constructed based
on a set of superordinate-subordinate relations be-
tween classifiers. Currently we adopt a very naive
approach to construct structures, i.e., starting from
the most superordinate classifiers as roots, we ex-
tend trees downward to less general classifiers by
using the extracted superordinate-subordinate rela-
tions. Note that since there is more than one classi-
fier that does not have any superordinate classifiers,
we will have a set of trees rather than a single tree.
When constructing structures, redundant relations
are ignored in order to make the structures as concise
as possible. A relation is considered redundant if the
relation can be inferred by using other relations and
transitivity of the relations. The formal definition of
redundant relations is given below:
ca  cb is redundant iff ?cm : ca  cm, cm  cb
Statistics of constructed structures for each lan-
guage are shown in Table 3. More than 50 iso-
lated structures (trees) were obtained for Chinese
and Japanese, while more than 100 for Thai. We ob-
tained several large structures, the largest containing
45, 85 and 23 classifiers for Chinese, Japanese and
Thai, respectively. As indicated in the fifth row in
Table 3, however, many structures consisting of only
2 classifiers were also constructed.
Table 3: Construction of structures
Chinese Japanese Thai
No. of structures 52 54 102
No. of CLs in a structure
Average 4.9 6.3 3.3
Maximum 45 85 23
Max. depth of structures 4 3 3
No. of structures with 2 CLs 18 24 54
4 Discussion
In this section, we will discuss the results of our
experiments. First 4.1 discusses appropriateness of
our method for the three languages. Then we eval-
uate our method in more detail. The evaluation of
extracted superordinate-subordinate relations is de-
scribed in 4.2, and the evaluation of structures in 4.3.
4.1 Comparison of different languages
According to the results of our experiments, the
proposed method seems promising for Chinese and
Japanese, but not for Thai. From the Thai data,
no relation was obtained for about 60% of classi-
fiers (Table 2), and many small fragmented struc-
tures were created (Table 3).
This is because of the characteristic that nouns
and classifiers are strongly coupled in Thai, i.e.,
many classifiers agree with only one noun. In our
Thai database, 252 (41.5%) classifiers agree with
only one noun. This means that the overlap between
two noun sets Ni and Nj can be quite small, making
the inclusion ratio IR(ci, cj) very small. Out basic
idea is that we can extract superordinate-subordinate
relations between two classifiers when the overlap of
their corresponding noun sets is large. However, this
assumption does not hold in Thai classifiers. The
above facts suggest that there seems to be no hierar-
chical taxonomy of classifiers in Thai.
4.2 Evaluation of extracted relations
4.2.1 Analysis of Nouns in Nj \ Ni
As explained in 3.1, our method extracts a relation
ci  cj even when Ni does not completely subsume
Nj . We analysed nouns in the relative complement
of Ni in Nj (Nj \Ni), i.e., the dark gray area in Fig-
ure 1. The relation ci  cj implies that all nouns
which are countable with a subordinate classifier cj
are also countable with its superordinate classifier ci,
but there is no guarantee of this for nouns in Nj \Ni,
since we loosened the condition as in (2) by intro-
ducing a threshold.
To see to what extent nouns in Nj \ Ni agree
with ci as well, we manually verified the agreement
of nouns in Nj \ Ni and ci for all extracted rela-
tions ci  cj . The verification was done by native
speakers of each language. Results of the valida-
tion are summarized in Table 4. For Japanese and
Chinese, multiple judges verified the results. When
judgments conflicted, we decided the final decision
by a discussion of two judges for Japanese, and by
majority voting for Chinese. The 4th and 5th rows
399
in Table 4 show the agreement of judgments. The
?Agreement ratio? is the ratio of cases that judg-
ments agree. Since three judges verified nouns for
Chinese, we show the average of the agreement ra-
tios for two judges out of the three. The agreement
ratio and Cohen?s ? is relatively high for Japanese,
but not for Chinese. We found many uncertain cases
for Chinese nouns. For example, ?? (wei)? is a clas-
sifier used when counting people with honorific per-
spective. However, judgement if ??? can modify
nouns such as ?political prisoner? or ?local villain?
is rather uncertain.
Table 4: Analysis of nouns in Nj \ Ni
Chinese Japanese Thai
No. of nouns in N
j
\N
i
1,650 579 43
No. of nouns countable 1,195 241 24
with c
i
as well 72% 42% 56%
No. of judges 3 2 1
Agreement ratio 0.677 0.936 ?
Cohen?s ? 0.484 0.868 ?
Table 4 reveals that a considerable number of
nouns in Nj \ Ni are actually countable with ci,
meaning that our databases do not include noun-
classifier agreement exhaustively.
4.2.2 Reliability of relations ??
Based on the analysis in 4.2.1, we evaluate ex-
tracted superordinate-subordinate relations. We de-
fine the reliability R of the relation ci  cj as
R(ci  cj) =
|Ni ? Nj |+ |NCj,i|
|Nj |
, (3)
where, NCj,i is a subset of Nj \ Ni consisting of
nouns which are manually judged to agree with ci.
We can consider that the more strictly this statement
holds, the more reliable the extracted relations will
be.
Figure 2 shows the relations between the thresh-
old T
ir
and both the number of extracted relations
and their reliability. The horizontal axis indicates
the threshold T
ir
in (2). The bar charts indicate the
number of extracted relations, while the line graphs
indicate the averages of reliability of all extracted re-
lations. Of course, if we set T
ir
lower, we can extract
more relations at the cost of their reliability. How-
ever, even when T
ir
is set to the lowest value, the
averages of reliability are relatively high, i.e. 0.98
(Chinese), 0.91 (Japanese) and 0.99 (Thai). Thus
we can conclude that the extracted superordinate-
subordinate relations are reliable enough.
4.3 Evaluation of structures
As in ordinary ontologies, we will assume that prop-
erties of superordinate classifiers can be inherited to
their subordinate classifiers. In other words, a clas-
sifier taxonomy suggests transitivity of agreement
with nouns over superordinate-subordinate relations
as
c
1
 c
2
? c
2
 c
3
? c
1
 c
3
.
In order to evaluate the structures of our taxonomy,
we verify the validity of transitivity.
First, we extracted all pairs of classifiers having
an ancestor-descendant relation from our classifier
taxonomy. Hereafter we denote ancestor-descendant
pairs of classifiers as (ca, cd), where ca is an ances-
tor and cd an descendant. The path from ca to cd on
the taxonomy can be represented as
c
0
(= ca)  c1  ...  cn(= cd). (4)
We denote a superordinate-subordinate relation de-
rived by transitivity as
?
, such as c
0
?
 cn. Among
all ancestor-descendant relations, we extracted ones
with a path length of more than one, or n > 1
in (4). Then we compare R(ca
?
 cd), the re-
liability of a relation derived by transitivity, with
R(ci  ci+1) (0 ? i < n), the reliability of di-
rect relations in the path from ca to cd. If these are
comparable, we can conclude that transitivity in the
taxonomy is valid.
Table 5 shows the results of the analysis of transi-
tivity. As indicated in the column ?all? in Table 5, 78
and 86 ancestor-descendant pairs (ca, cd) were ex-
tracted from the Chinese and Japanese classifier tax-
onomy, respectively. In contrast, only 6 pairs were
extracted from the Thai taxonomy, since each struc-
ture of the Thai taxonomy is rather small as we al-
ready discussed with Table 3. Thus we have omit-
ted further analysis of Thai. The extracted ancestor-
descendant pairs of classifiers are then classified into
three cases, (A), (B) and (C). Their numbers are
shown in the last three rows in Table 5, where mini
and maxi denote the minimum and maximum of re-
liability among all direct relations R(ci  ci+1) in
the path from ca to cd.
400
Chinese Japanese Thai
0
50
100
150
200
250
300
350
0.9
0.92
0.94
0.96
0.98
1
1.0 0.9 0.8 0.7 irT
# of Rel. Ave. of R
0
50
100
150
200
250
300
350
0.9
0.92
0.94
0.96
0.98
1
1.0 0.9 0.8 0.7 0.6 irT
# of Rel. Ave. of R
0
50
100
150
200
250
300
350
0.9
0.92
0.94
0.96
0.98
1
1.0 0.9 0.8 0.7 0.6 irT
# of Rel. Ave. of R
Figure 2: Reliability of extracted superordinate-subordinate relations
Table 5: Verification of transitivity
Chinese Japanese
all direct indirect all direct indirect
No. of (c
a
, c
d
) 78 58 20 86 55 31
Average of R(c
a
?
c
d
) 0.88 0.98 0.61 0.77 0.93 0.48
(A) min
i
> R(c
a
?
c
d
) 16 (21%) 4 (7%) 12 (60%) 24 (28%) 3 (5%) 21 (68%)
(B) min
i
? R(c
a
?
c
d
) < max
i
39 (50%) 34 (59%) 5 (25%) 27 (31%) 24 (44%) 3 (9%)
(C) max
i
? R(c
a
?
c
d
) 23 (29%) 20 (34%) 3 (15%) 35 (41%) 28 (51%) 7 (23%)
In case (A), reliability of a relation derived by
transitivity, R(ca
?
 cd), is less than that of any di-
rect relations, R(ci  ci+1). In case (B), reliability
of a transitive relation is comparable with that of di-
rect relations, i.e. R(ca
?
 cd) is greater or equal to
mini and less than maxi. In case (C), the transitive
relation is more reliable than direct relations.
The average of the reliability of ca
?
 cd is rela-
tively high, 0.88 for Chinese and 0.77 for Japanese.
We also found that more than 70% of derived rela-
tions (case (B) and case (C)) are comparable to or
greater than direct relations. The above facts indi-
cate transitivity on our structural taxonomy is valid
to some degree.
From a different point of view, we divided pairs
of (ca, cd) into two other cases, ?direct? and ?indi-
rect? as shown in the columns of Table 5. The ?di-
rect? case includes the relations which are also ex-
tracted by our method. Note that such relations are
discarded as redundant ones. On the other hand, the
?indirect? case includes the relations which can not
be extracted from the database but only inferred by
using transitivity on the taxonomy. That is, they are
truly new relations. In order to calculate reliability
of ?indirect? cases, we performed additional manual
validation of nouns in Nd\Na.
However, the average of R(ca
?
 cd) in ?in-
direct? cases is not so high for both Chinese and
Japanese, as a large amount of pairs are classi-
fied into case (A). Thus it is not effective to infer
new superordinate-subordinate relations by transi-
tivity. Since we currently only adopted a very naive
method to construct a classifier taxonomy, more so-
phisticated methods should be explored in order to
prevent inferring irrelevant relations.
5 Related Work
Bond (2000) proposed a method to choose an appro-
priate classifier for a noun by referring its seman-
tic class. This method is implemented in a sentence
generation module of a machine translation system.
Similar attempts to generate both Japanese and Ko-
rean classifiers were also reported (Paik and Bond,
2001). Bender and Siegel (2004) implemented a
HPSG that handles several intricate structures in-
cluding Japanese classifiers. Matsumoto (1993)
reported his close analysis of Japanese classi-
fiers based on prototype semantics. Sornlertlam-
vanich (1994) presented an algorithm for selecting
an adequate classifier for a noun by using a cor-
pus. Their research can be regarded as a method to
construct a noun-classifier agreement database au-
401
tomatically from corpora. We used databases de-
rived from dictionaries except for a small number
of noun-classifier pairs in Thai, because we believe
dictionaries provide more reliable and stable infor-
mation than corpora, and in addition they were avail-
able and on hand. Note that we are not concerned
with frequencies of noun-classifier coocurrence in
this study. Huang (1998) proposed a method to
construct a noun taxonomy based on noun-classifier
agreement that is very similar to ours, but aims at
developing a taxonomy for nouns rather than one for
classifiers. There has not been very much work on
building resources concerning noun-classifier agree-
ment. To our knowledge, this is the first attempt to
construct a classifier taxonomy.
6 Conclusion
This paper proposed a method to construct a tax-
onomy of numerative classifiers based on a noun-
classifier agreement database. First, superordinate-
subordinate relations of two classifiers are extracted
by measuring the overlap of two sets of nouns agree-
ing with each classifier. Then these relations are
used as building blocks to build a taxonomy of
tree structures. We conducted experiments to build
classifier taxonomies for three languages: Chinese,
Japanese and Thai. The effectiveness of our method
was evaluated by measuring reliability of extracted
relations, and verifying validity of transitivity in the
taxonomy. We found that extracted relations are re-
liable, and the transitivity in the taxonomy relatively
valid. Relations inferred by transitivity, however, are
less reliable than those directly derived from noun-
classifier agreement.
Future work includes investigating a way to en-
large classifier taxonomies. Currently, not all clas-
sifiers are included in our taxonomy, and it con-
sists of a set of fragmented structures. A more so-
phisticated method to build a large taxonomy in-
cluding more classifiers should be examined. Our
method should also be refined in order to make
superordinate-subordinate relations inferred by the
transitivity more reliable. We are now investigat-
ing a stepwise method to construct taxonomies that
prefers more reliable relations, i.e. an initial tax-
onomy is built with a small number of highly reli-
able relations, and is then expanded with less reli-
able ones.
Acknowledgment
This research was carried out through financial sup-
port provided under the NEDO International Joint
Research Grant Program (NEDO Grant).
References
Emily M. Bender and Melanie Siegel. 2004. Imple-
menting the syntax of Japanese numeral classifiers. In
Proceedings of the the First International Joint Con-
ference on Natural Language Processing, pages 398?
405.
Francis Bond and Kyonghee Paik. 2000. Reusing an on-
tology to generate numeral classifiers. In Proceedings
of the COLING, pages 90?96.
CICC. 1995. CICC Thai basic dictionary. (developed by
Center of the International Cooperation for Computer-
ization).
Chu-Ren Huang, Keh-Jian Chen, and Chin-Hsiung Lai,
editors. 1997. Mandarin Daily News Dictionary of
Measure Words. Mandarin Daily News Publisher.
Chu-Ren Huang, Keh-jiann Chen, and Zhao-ming Gao.
1998. Noun class extraction from a corpus-based col-
location dictionary: An integration of computational
and qualitative approaches. In Quantitative and Com-
putational Studies of Chinese Linguistics, pages 339?
352.
Asako Iida. 2004. Kazoekata no Ziten (Dictionary for
counting things). Sho?gakukan. (in Japanese).
Yo Matsumoto. 1993. The Japanese numeral classifiers:
A study of semantic categories and lexical organiza-
tion. Linguistics, 31:667?713.
Kyonghee Paik and Francis Bond. 2001. Multilin-
gual generation of numeral classifiers using a common
ontology. In Proceedings of the 19th International
Conference on Computer Processing of Oriental Lan-
guages (ICCPOL), pages 141?147.
Virach Sornlertlamvanich, Wantanee Pantachat, and
Surapant Meknavin. 1994. Classifier assignment by
corpus-based approach. In Proceedings of the COL-
ING, pages 556?561.
402
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 399?406,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Efficient sentence retrieval based on syntactic structure
Ichikawa Hiroshi, Hakoda Keita, Hashimoto Taiichi and Tokunaga Takenobu
Department of Computer Science, Tokyo Institute of Technology
{ichikawa,hokoda,taiichi,take}@cl.cs.titech.ac.jp
Abstract
This paper proposes an efficient method
of sentence retrieval based on syntactic
structure. Collins proposed Tree Kernel
to calculate structural similarity. However,
structual retrieval based on Tree Kernel
is not practicable because the size of the
index table by Tree Kernel becomes im-
practical. We propose more efficient al-
gorithms approximating Tree Kernel: Tree
Overlapping and Subpath Set. These algo-
rithms are more efficient than Tree Kernel
because indexing is possible with practical
computation resources. The results of the
experiments comparing these three algo-
rithms showed that structural retrieval with
Tree Overlapping and Subpath Set were
faster than that with Tree Kernel by 100
times and 1,000 times respectively.
1 Introduction
Retrieving similar sentences has attracted much
attention in recent years, and several methods
have been already proposed. They are useful for
many applications such as information retrieval
and machine translation. Most of the methods
are based on frequencies of surface information
such as words and parts of speech. These methods
might work well concerning similarity of topics or
contents of sentences. Although the surface infor-
mation of two sentences is similar, their syntactic
structures can be completely different (Figure 1).
If a translation system regards these sentences as
similar, the translation would fail. This is because
conventional retrieval techniques exploit only sim-
ilarity of surface information such as words and
parts-of-speech, but not more abstract information
such as syntactic structures.
He beats a dog with a
V DET NN P DET
NP
PP
NP
S
stick
N
VP
VP
NP
He knows the girl with a
V DET NN P DET
NP
PP
NP
S
ribbon
N
NP
VP
NP
Figure 1: Sentences similar in appearance but dif-
fer in syntactic structure
Collins et al (Collins, 2001a; Collins, 2001b)
proposed Tree Kernel, a method to calculate a sim-
ilarity between syntactic structures. Tree Kernel
defines the similarity between two syntactic struc-
tures as the number of shared subtrees. Retrieving
similar sentences in a huge corpus requires cal-
culating the similarity between a given query and
each of sentences in the corpus. Building an index
table in advance could improve retrieval efficiency,
but indexing with Tree Kernel is impractical due to
the size of its index table.
In this paper, we propose two efficient algo-
399
rithms to calculate similarity of syntactic struc-
tures: Tree Overlapping and Subpath Set. These
algorithms are more efficient than Tree Kernel be-
cause it is possible to make an index table in rea-
sonable size. The experiments comparing these
three algorithms showed that Tree Overlapping is
100 times faster and Subpath Set is 1,000 times
faster than Tree Kernel when being used for struc-
tural retrieval.
After briefly reviewing Tree Kernel in section 2,
in what follows, we describe two algorithms in
section 3 and 4. Section 5 describes experiments
to compare these three algorithms and discussion
on the results. Finally, we conclude the paper and
look at the future direction of our research in sec-
tion 6.
2 Tree Kernel
2.1 Definition of similarity
Tree Kernel is proposed by Collins et al (Collins,
2001a; Collins, 2001b) as a method to calculate
similarity between tree structures. Tree Kernel de-
fines similarity between two trees as the number
of shared subtrees. Subtree S of tree T is defined
as any tree subsumed by T , and consisting of more
than one node, and all child nodes are included if
any.
Tree Kernel is not always suitable because the
desired properties of similarity are different de-
pending on applications. Takahashi et al pro-
posed three types of similarity based on Tree Ker-
nel (Takahashi, 2002). We use one of the similar-
ity measures (equation (1)) proposed by Takahashi
et al
KC(T1, T2) = maxn1?N1, n2?N2
C(n1, n2) (1)
where C(n1, n2) is the number of shared subtrees
by two trees rooted at nodes n1 and n2.
2.2 Algorithm to calculate similarity
Collins et al (Collins, 2001a; Collins, 2001b)
proposed an efficient method to calculate Tree
Kernel by using C(n1, n2) as follows.
? If the productions at n1 and n2 are different
C(n1, n2) = 0
? If the productions at n1 and n2 are the
same, and n1 and n2 are pre-terminals, then
C(n1, n2) = 1
? Else if the productions at n1 and n2 are the
same and n1 and n2 are not pre-terminals,
C(n1, n2) =
nc(n1)
?
i=1
(1 + C(ch(n1, i), ch(n2, i)))
(2)
where nc(n) is the number of children of node n
and ch(n, i) is the i?th child node of n. Equa-
tion (2) recursively calculates C on its child node,
and calculating Cs in postorder avoids recalcula-
tion. Thus, the time complexity of KC(T1, T2) is
O(mn), where m and n are the numbers of nodes
in T1 and T2 respectively.
2.3 Algorithm to retrieve sentences
Neither Collins nor Takahashi discussed retrieval
algorithms using Tree Kernel. We use the follow-
ing simple algorithm. First we calculate the simi-
larity KC(T1, T2) between a query tree and every
tree in the corpus and rank them in descending or-
der of KC .
Tree Kernel exploits all subtrees shared by trees.
Therefore, it requires considerable amount of time
in retrieval because similarity calculation must be
performed for every pair of trees. To improve re-
trieval time, an index table can be used in general.
However, indexing by all subtrees is difficult be-
cause a tree often includes millions of subtrees.
For example, one sentence in Titech Corpus (Noro
et al, 2005) with 22 words and 87 nodes includes
8,213,574,246 subtrees. The number of subtrees
in a tree with N nodes is bounded above by 2N .
3 Tree Overlapping
3.1 Definition of similarity
When putting an arbitrary node n1 of tree T1 on
node n2 of tree T2, there might be the same pro-
duction rule overlapping in T1 and T2. We define
CTO(n1, n2) as the number of such overlapping
production rules when n1 overlaps n2 (Figure 2).
We will define CTO(n1, n2) more precisely.
First we define L(n1, n2) of node n1 of T1 and
node n2 of T2. L(n1, n2) represents a set of pairs
of nodes which overlap each other when putting
n1 on n2. For example in Figure 2, L(b11, b21) =
{(b11, b21), (d11, d21), (e11, e21), (g11, g21), (i11, j21)}.
L(n1, n2) is defined as follows. Here ni and mi
are nodes of tree Ti, ch(n, i) is the i?th child of
node n.
1. (n1, n2) ? L(n1, n2)
400
 (1) aT2
b
d e
g
j
g
i
a
b c
d
(2)
e
g
i
b
d e
g
j
a
b c
d e
g
i
a
b c
d e
g
i
(3)
g
i
CTO(b11,b21) = 2
a
g
i
b
d e
g
j
T1
a
CTO(g11,g21) = 1
11
11
11 11
11
11
11
21
21
21
21
21 21
22
21
11
11
11 11
11
11
11
21
21
21
21
21 21
22
21
11
11
11 11
11
11
11
21
21
21
21
21 21
22
21
Figure 2: Example of similarity calculation
2. If (m1,m2) ? L(n1, n2),
(ch(m1, i), ch(m2, i)) ? L(n1, n2)
3. If (ch(m1, i), ch(m2, i)) ? L(n1, n2),
(m1,m2) ? L(n1, n2)
4. L(n1, n2) includes only pairs generated by
applying 2. and 3. recursively.
CTO(n1, n2) is defined by using L(n1, n2) as
follows.
CTO(n1, n2)
=
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
(m1,m2)
?
?
?
?
?
?
?
?
?
m1 ? NT (T1)
? m2 ? NT (T2)
? (m1,m2) ? L(n1, n2)
? PR(m1) = PR(m2)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
,
(3)
where NT (T ) is a set of nonterminal nodes in tree
T , PR(n) is a production rule rooted at node n.
Tree Overlapping similarity STO(T1, T2) is de-
fined as follows by using CTO(n1, n2).
STO(T1, T2) = max
n1?NT (T1) n2?NT (T2)
CTO(n1, n2)
(4)
This formula corresponds to equation (1) of Tree
Kernel.
As an example, we calculate STO(T1, T2) in
Figure 2 (1). Putting b11 on b21 gives Figure 2 (2)
in which two production rules b ? d e and e ? g
overlap respectively. Thus, CTO(b11, b21) becomes
2. While overlapping g11 and g21 gives Figure 2 (3)
in which only one production rule g ? i overlaps.
Thus, CTO(g11, g21) becomes 1. Since there are no
other node pairs which gives larger CTO than 2,
STO(T1, T2) becomes 2.
Table 1: Example of the index table
p I[p]
a ? b c {a11}
b ? d e {b11, b21}
e ? g {e11, e21}
g ? i {g11, g21}
a ? g b {a21}
g ? j {g21}
3.2 Algorithm
Let us take an example in Figure 3 to explain the
algorithm. Suppose that T0 is a query tree and the
corpus has only two trees, T1 and T2.
The method to find the most similar tree to a
given query tree is basically the same as Tree Ker-
nel?s (section 2.2). However, unlike Tree Kernel,
Tree Overlapping-based retrieval can be acceler-
ated by indexing the corpus in advance. Thus,
given a tree corpus, we build an index table I[p]
which maps a production rule p to its occurrences.
Occurrences of production rules are represented
by their left-hand side symbols, and are distin-
guished with respect to trees including the rule and
401
(1) T0 a(2)
b
d e
g
j
g
i
(3)a
b c
d e
Score: 2 pt. Score: 1 pt.
a
b c
d e
g
i
a
b c
d e
a
b c
d e
aT2
b
d e
g
j
g
i
a
b c
d e
g
i
T101
01
01
01
01
11
11
11 11
11
11
11
21
21
21
21
21
22
21
01
01
01
01
01
11
11
11 11
11
11
11
01
01
01
01
0121
21
21
21
21
21
22
21
21
 
Figure 3: Example of Tree Overlapping-based retrieval
the position in the tree. I[p] is defined as follows.
I[p] =
?
?
?
?
?
m
?
?
?
?
?
?
?
T ? F
? m ? NT (T )
? p = PR(m)
?
?
?
?
?
(5)
where F is the corpus (here {T1, T2}) and the
meaning of other symbols is the same as the defi-
nition of CTO (equation (3)).
Table 1 shows an example of the index table
generated from T1 and T2 in Figure 3 (1). In Ta-
ble 1, a superscript of a nonterminal symbol iden-
tifies a tree, and a subscript identifies a position in
the tree.
By using the index table, we calculate C[n,m]
with the following algorithm.
for all (n,m) do C[n,m] := 0 end
foreach n in NT (T0) do
foreach m in I[PR(n)] do
(n?,m?) := top(n,m)
C[n?,m?] := C[n?,m?] + 1
end
end
where top(n,m) returns the upper-most pair of
overlapped nodes when node n and m overlap.
The value of top uniquely identifies a situation of
overlapping two trees. Function top(n,m) is cal-
culated by the following algorithm.
function top(n,m);
begin
(n?,m?) := (n,m)
while order(n?) = order(m?) do
n? := parent(n?)
m? := parent(m?)
end
return (n?,m?)
end
where parent(n) is the parent node of n, and
order(n) is the order of node n among its siblings.
Table 2 shows example values of top(n,m) gen-
erated by overlapping T0 and T1 in Figure 3. Note
that top maps every pair of corresponding nodes
in a certain overlapping situation to a pair of the
upper-most nodes of that situation. This enables
us to use the value of top as an identifier of a situ-
ation of overlap.
Table 2: Examples of top(n,m)
(n,m) top(n,m)
(a01, a11) (a01, a11)
(b01, b11) (a01, a11)
(c01, c11) (a01, a11)
Now C[top(n,m)] = CTO(n,m), therefore the
tree similarity between a query tree T0 and each
tree T in the corpus STO(T0, T )can be calculated
by:
STO(T0, T ) = max
n?NT (T0), m?NT (T )
C[top(n,m)]
(6)
3.3 Comparison with Tree Kernel
The value of STO(T1, T2) roughly corresponds to
the number of production rules included in the
largest sub-tree shared by T1 and T2. Therefore,
this value represents the size of the subtree shared
402
by both trees, like Tree Kernel?s KC , though the
definition of the subtree size is different.
One difference is that Tree Overlapping consid-
ers shared subtrees even though they are split by a
nonshared node as shown in Figure 4. In Figure 4,
T1 and T2 share two subtrees rooted at b and c, but
their parent nodes are not identical. While Tree
Kernel does not consider the superposition putting
node a on h, Tree Overlapping considers putting a
on h and assigns count 2 to this superposition.
 a
b c
f g
(3)
d e
h
b c
f gd e
a
b c
f gd e
h
b c
f gd e
STO(T1,T2) = 2
(1) T1 (2) T2
Figure 4: Example of counting two separated
shared subtrees as one
Another, more important, difference is that Tree
Overlapping retrieval can be accelerated by index-
ing the corpus in advance. The number of indexes
is bounded above by the number of production
rules, which is within a practical index size.
4 Subpath Set
4.1 Definition of similarity
Subpath Set similarity between two trees is de-
fined as the number of subpaths shared by the
trees. Given a tree, its subpaths is defined as a
set of every path from the root node to leaves and
their partial paths.
Figure 5 (2) shows all subpaths in T1 and T2 in
Figure 5(1). Here we denotes a path as a sequence
of node names such as (a, b, d). Therefore, Sub-
path Set similarity of T1 and T2 becomes 15.
4.2 Algorithm
Suppose T0 is a query tree, TS is a set of trees in
the corpus and P (T ) is a set of subpaths of T . We
can build an index table I[p] for each production
rule p as follows.
I[p] = {T |T ? TS ? p ? P (T )} (7)
Using the index table, we can calculate the num-
ber of shared subpaths by T0 and T , S[T ], by the
following algorithm:
for all T S[T ] := 0;
foreach p in P (T0) do
foreach T in I[p] do
S[T ] := S[T ] + 1
end
end
4.3 Comparison with Tree Kernel
As well as Tree Overlapping, Subpath Set retrieval
can be accelerated by indexing the corpus. The
number of indexes is bounded above by L ? D2
where L is the maximum number of leaves of trees
(the number of words in a sentence) and D is the
maximum depth of syntactic trees. Moreover, con-
sidering a subpath as an index term, we can use
existing retrieval tools.
Subpath Set uses less structural information
than Tree Kernel and Tree Overlapping. It does
not distinguish the order and number of child
nodes. Therefore, the retrieval result tends to be
noisy. However, Subpath Set is faster than Tree
Overlapping, because the algorithm is simpler.
5 Experiments
This section describes the experiments which were
conducted to compare the performance of struc-
ture retrieval based on Tree Kernel, Tree Overlap-
ping and Subpath Set.
5.1 Data
We conducted two experiments using different an-
notated corpora. Titech corpus (Noro et al, 2005)
consists of about 20,000 sentences of Japanese
newspaper articles (Mainiti Shimbun). Each sen-
tence has been syntactically annotated by hand.
Due to the limitation of computational resources,
we used randomly selected 2,483 sentences as a
data collection.
Iwanami dictionary (Nishio et al, 1994) is a
Japanese dictionary. We extracted 57,982 sen-
tences from glosses in the dictionary. Each sen-
tences was analyzed with a morphological an-
alyzer, ChaSen (Asahara et al, 1996) and the
MSLR parser (Shirai et al, 2000) to obtain syntac-
tic structure candidates. The most probable struc-
ture with respect to PGLR model (Inui et al, 1996)
was selected from the output of the parser. Since
they were not investigated manually, some sen-
tences might have been assigned incorrect struc-
tures.
5.2 Method
We conducted two experiments Experiment I and
Experiment II with different corpora. The queries
403
(1) aT2
b
d e
g
j
g
i
a
b c
d e
g
i
T1
(c),
(a,c),
(e,g,i),
(b,e,g,i),
(a,b,e,g,i)
(2) Subpaths of T1
Subpaths of T2SSS(T1,T2) = 15
(a), (b), (d), (e), (g), (i),
(a,b), (b,d), (b,e), (e,g), (g,i),
(a,b,d), (a, b, e), (b,e,g),
(a,b,e,g)
(j),
(a,g), (g,j),
(a,g,i), (e,g,j),
(b,e,g,j),
(a,b,e,g,j)
Figure 5: Example of subpaths
were extracted from these corpora. The algorithms
described in the preceding sections were imple-
mented with Ruby 1.8.2. Table 3 outlines the ex-
periments.
Table 3: Summary of experiments
Experiment I II
Target corpus Titech Corpus Iwanami dict.
Corpus size 2,483 sent. 57,982 sent.
No. of queries 100 1,000
CPU Intel Xeon PowerPC G5
(2.4GHz) (2.3GHz)
Memory 2GB 2GB
5.3 Results and discussion
Since we select a query from the target corpus,
the query is always ranked in the first place in the
retrieval result. In what follows, we exclude the
query tree as an answer from the result.
We evaluated the algorithms based on the fol-
lowing two factors: average retrieval time (CPU
time) (Table 4) and the rank of the tree which was
top-ranked in other algorithm (Table 5). For ex-
ample, in Experiment I of Table 5, the column
??5th? of the row ?TO/TK? means that there were
73 % of the cases in which the top-ranked tree by
Tree Kernel (TK) was ranked 5th or above by Tree
Overlapping (TO).
We consider Tree Kernel (TK) as the baseline
method because it is a well-known existing simi-
larity measure and exploits more information than
others. Table 4 shows that in both corpora, the
retrieval speed of Tree Overlapping (TO) is about
Table 4: Average retrieval time per query [sec]
Algorithm Experiment I Experiment II
TK 529.42 3796.1
TO 6.29 38.3
SS 0.47 5.1
100 times faster than that of Tree Kernel, and the
retrieval speed of Subpath Set (SS) is about 1,000
times faster than that of Tree Kernel. This re-
sults show we have successfully accelerated the
retrieval speed.
The retrieval time of Tree Overlapping, 6.29
and 38.3 sec./per query, seems be a bit long. How-
ever, we can shorten this time if we tune the im-
plementation by using a compiler-type language.
Note that the current implementation uses Ruby,
an interpreter-type language.
Comparing Tree Overlapping and Subpath Set
with respect to Tree Kernel (see rows ?TK/TO?
and ?TK/SS?), the top-ranked trees by Tree Kernel
are ranked in higher places by Tree Overlapping
than by Subpath Set. This means Tree Overlap-
ping is better than Subpath Set in approximating
Tree Kernel.
Although the corpus of Experiment II is 20
times larger than that of Experiment I, the figures
of Experiment II is better than that of Experiment I
in Table 5. This could be explained as follows.
In Experiment II, we used sentences from glosses
in the dictionary, which tend to be formulaic and
short. Therefore we could find similar sentences
easier than in Experiment I.
To summarize the results, when being used in
404
Table 5: The rank of the top-ranked tree by other
algorithm [%]
Experiment I
A/B ? 1st? ? 5th ? 10th
TO/TK 34.0 73.0 82.0
SS/TK 16.0 35.0 45.0
TK/TO 29.0 41.0 51.0
SS/TO 27.0 49.0 58.0
TK/SS 17.0 29.0 37.0
TO/SS 29.0 58.0 69.0
Experiment II
A/B ? 1st? ? 5th ? 10th
TO/TK 74.6 88.0 92.0
SS/TK 65.3 78.8 84.1
TK/TO 71.1 81.0 84.6
SS/TO 73.4 86.0 89.8
TK/SS 65.5 75.9 79.7
TO/SS 76.1 87.7 92.0
similarity calculation of tree structure retrieval,
Tree Overlapping approximates Tree Kernel bet-
ter than Subpath Set, while Subpath Set is faster
than Tree Overlapping.
6 Conclusion
We proposed two fast algorithms to retrieve sen-
tences which have a similar syntactic structure:
Tree Overlapping (TO) and Subpath Set (SS). And
we compared them with Tree Kernel (TK) to ob-
tain the following results.
? Tree Overlapping-based retrieval outputs
similar results to Tree Kernel-based retrieval
and is 100 times faster than Tree Kernel-
based retrieval.
? Subpath Set-based retrieval is not so good
at approximating Tree Kernel-based retrieval,
but is 1,000 times faster than Tree Kernel-
based retrieval.
Structural retrieval is useful for annotationg cor-
pora with syntactic information (Yoshida et al,
2004). We are developing a corpus annotation tool
named ?eBonsai? which supports human to anno-
tate corpora with syntactic information and to re-
trieve syntactic structures. Integrating annotation
and retrieval enables annotators to annotate a new
instance with looking back at the already anno-
tated instances which share the similar syntactic
structure with the current one. For such purpose,
Tree Overlapping and Subpath Set alorithms con-
tribute to speed up the retrieval process, thus make
the annotation process more efficient.
However, ?similarity? of sentences is affected
by semantic aspects as well as structural aspects.
The output of the algorithms do not always con-
form with human?s intuition. For example, the
two sentences in Figure 6 have very similar struc-
tures including particles, but they are hardly con-
sidered similar from human?s viewpoint. With this
respect, it is hardly to say which algorithm is su-
perior to others.
As a future work, we need to develop a method
to integrate both content-based and structure-
based similarity measures. To this end, we have
to evaluate the algorithms in real application envi-
ronments (e.g. information retrieval and machine
translation) because desired properties of similar-
ity are different depending on applications.
References
Asahara, M. and Matsumoto, Y., Extended Models and
Tools for High-performance Part-of-Speech Tagger.
Proceedings of COLING 2000, 2000.
Collins, M. and Duffy, N. Parsing with a Single Neu-
ron: Convolution Kernels for Natural Language
Problems. Technical report UCSC-CRL-01-01, Uni-
versity of California at Santa Cruz, 2001.
Collins, M. and Duffy, N. Convolution Kernels for Nat-
ural Language. In Proceedings of NIPS 2001, 2001.
Inui, K., Shirai, K., Tokunaga T. and Tanaka H., The In-
tegration of Statistics-based Techniques in the Anal-
ysis of Japanese Sentences. Special Interest Group
of Natural Language Processing, Information Pro-
cessing Society of Japan, Vol. 96, No. 114, 1996.
Nagao, M. A framework of a mechanical translation
between Japanese and English by analogy principle.
In Alick Elithorn and Ranan Banerji, editors, Artif-
ical and Human Intelligence, pages 173-180. Ams-
terdam, 1984.
Noro, T., Koike, C., Hashimoto, T., Tokunaga, T. and
Tanaka, H. Evaluation of a Japanese CFG Derived
from a Syntactically Annotated Corpus with respect
to Dependency Measures, The 5th Workshop on
Asian Language Resources, pp.9-16, 2005.
Nishio, M., Iwabuchi, E. and Mizutani, S. (ed.)
Iwanami Kokugo Jiten, Iwanamishoten, 5th Edition,
1994.
Shirai, K., Ueki, M. Hashimoto, T., Tokunaga, T. and
Tanaka, H., MSLR Parser Tool Kit - Tools for Natu-
ral Language Analysis. Journal of Natural Language
405
P ADJ NN P N
PP
PP
S
P
VP
NP
V
NP
(to) (young) (a teaching material company) (of) (man) (SBJ ) (came)(classroom)
P ADJ NN P N
PP
PP
S
P
VP
NP
V
NP
(to) (exploded) (bombshell) (of) (piece) (SBJ ) (hit)(head)
"A young man of a teaching material company came to the classroom"
"A piece of the exploded bombshell hit his head"
Query
Top- ranked
Figure 6: Example of a retrieved similar sentence
Processing, Vol. 7, No. 5, pp. 93-112, 2000. (in
Japanese)
Somers, H., McLean, I., Jones, D. Experiments in mul-
tilingual example-based generation. CSNLP 1994:
3rd conference on the Cognitive Science of Natural
Language Processing, Dublin, 1994.
Takahashi, T., Inui K., and Matsumoto, Y.. Methods
of Estimating Syntactic Similarity. Special Interest
Group of Natural Language Processing, Information
Processing Society of Japan, NL-150-7, 2002. (in
Japanese)
Yoshida, K., Hashimoto, T., Tokunaga, T. and Tanaka,
H.. Retrieving annotated corpora for corpus annota-
tion. Proceedings of 4th International Conference on
Language Resources and Evaluation: LREC 2004.
pp.1775 ? 1778. 2004.
406
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 827?834,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Infrastructure for standardization of Asian language resources
Tokunaga Takenobu
Tokyo Inst. of Tech.
Virach Sornlertlamvanich
TCL, NICT
Thatsanee Charoenporn
TCL, NICT
Nicoletta Calzolari
ILC/CNR
Monica Monachini
ILC/CNR
Claudia Soria
ILC/CNR
Chu-Ren Huang
Academia Sinica
Xia YingJu
Fujitsu R&D Center
Yu Hao
Fujitsu R&D Center
Laurent Prevot
Academia Sinica
Shirai Kiyoaki
JAIST
Abstract
As an area of great linguistic and cul-
tural diversity, Asian language resources
have received much less attention than
their western counterparts. Creating a
common standard for Asian language re-
sources that is compatible with an interna-
tional standard has at least three strong ad-
vantages: to increase the competitive edge
of Asian countries, to bring Asian coun-
tries to closer to their western counter-
parts, and to bring more cohesion among
Asian countries. To achieve this goal, we
have launched a two year project to create
a common standard for Asian language re-
sources. The project is comprised of four
research items, (1) building a description
framework of lexical entries, (2) building
sample lexicons, (3) building an upper-
layer ontology and (4) evaluating the pro-
posed framework through an application.
This paper outlines the project in terms of
its aim and approach.
1 Introduction
There is a long history of creating a standard
for western language resources. The human
language technology (HLT) society in Europe
has been particularly zealous for the standardiza-
tion, making a series of attempts such as EA-
GLES1, PAROLE/SIMPLE (Lenci et al, 2000),
ISLE/MILE (Calzolari et al, 2003) and LIRICS2.
These continuous efforts has been crystallized as
activities in ISO-TC37/SC4 which aims to make
an international standard for language resources.
1http://www.ilc.cnr.it/Eagles96/home.html
2lirics.loria.fr/documents.html
(1) Description 
framework of lexical 
entries
(2) Sample lexicons
(4) Evaluation 
through application
(3) Upper layer 
ontologyrefinement
description classification
refinement
evaluationevaluation
Figure 1: Relations among research items
On the other hand, since Asia has great lin-
guistic and cultural diversity, Asian language re-
sources have received much less attention than
their western counterparts. Creating a common
standard for Asian language resources that is com-
patible with an international standard has at least
three strong advantages: to increase the competi-
tive edge of Asian countries, to bring Asian coun-
tries to closer to their western counterparts, and to
bring more cohesion among Asian countries.
To achieve this goal, we have launched a two
year project to create a common standard for
Asian language resources. The project is com-
prised of the following four research items.
(1) building a description framework of lexical
entries
(2) building sample lexicons
(3) building an upper-layer ontology
(4) evaluating the proposed framework through
an application
Figure 1 illustrates the relations among these re-
search items.
Our main aim is the research item (1), building
a description framework of lexical entries which
827
fits with as many Asian languages as possible, and
contributing to the ISO-TC37/SC4 activities. As
a starting point, we employ an existing descrip-
tion framework, the MILE framework (Bertagna
et al, 2004a), to describe several lexical entries of
several Asian languages. Through building sam-
ple lexicons (research item (2)), we will find prob-
lems of the existing framework, and extend it so
as to fit with Asian languages. In this extension,
we need to be careful in keeping consistency with
the existing framework. We start with Chinese,
Japanese and Thai as target Asian languages and
plan to expand the coverage of languages. The re-
search items (2) and (3) also comprise the similar
feedback loop. Through building sample lexicons,
we refine an upper-layer ontology. An application
built in the research item (4) is dedicated to evalu-
ating the proposed framework. We plan to build an
information retrieval system using a lexicon built
by extending the sample lexicon.
In what follows, section 2 briefly reviews the
MILE framework which is a basis of our de-
scription framework. Since the MILE framework
is originally designed for European languages, it
does not always fit with Asian languages. We ex-
emplify some of the problems in section 3 and sug-
gest some directions to solve them. We expect
that further problems will come into clear view
through building sample lexicons. Section 4 de-
scribes a criteria to choose lexical entries in sam-
ple lexicons. Section 5 describes an approach
to build an upper-layer ontology which can be
sharable among languages. Section 6 describes
an application through which we evaluate the pro-
posed framework.
2 The MILE framework for
interoperability of lexicons
The ISLE (International Standards for Language
Engineering) Computational Lexicon Working
Group has consensually defined the MILE (Mul-
tilingual ISLE Lexical Entry) as a standardized
infrastructure to develop multilingual lexical re-
sources for HLT applications, with particular at-
tention toMachine Translation (MT) and Crosslin-
gual Information Retrieval (CLIR) application
systems.
The MILE is a general architecture devised
for the encoding of multilingual lexical informa-
tion, a meta-entry acting as a common representa-
tional layer for multilingual lexicons, by allowing
integration and interoperability between different
monolingual lexicons3.
This formal and standardized framework to en-
code MILE-conformant lexical entries is provided
to lexicon and application developers by the over-
all MILE Lexical Model (MLM). As concerns
the horizontal organization, the MLM consists of
two independent, but interlinked primary compo-
nents, the monolingual and the multilingual mod-
ules. The monolingual component, on the vertical
dimension, is organized over three different repre-
sentational layers which allow to describe differ-
ent dimensions of lexical entries, namely the mor-
phological, syntactic and semantic layers. More-
over, an intermediate module allows to define
mechanisms of linkage and mapping between the
syntactic and semantic layers. Within each layer, a
basic linguistic information unit is identified; basic
units are separated but still interlinked each other
across the different layers.
Within each of the MLM layers, different types
of lexical object are distinguished :
? the MILE Lexical Classes (MLC) represent
the main building blocks which formalize
the basic lexical notions. They can be seen
as a set of structural elements organized in
a layered fashion: they constitute an on-
tology of lexical objects as an abstraction
over different lexical models and architec-
tures. These elements are the backbone of
the structural model. In the MLM a defini-
tion of the classes is provided together with
their attributes and the way they relate to each
other. Classes represent notions like Inflec-
tionalParadigm, SyntacticFunction, Syntac-
ticPhrase, Predicate, Argument,
? the MILE Data Categories (MDC) which
constitute the attributes and values to adorn
the structural classes and allow concrete en-
tries to be instantiated. MDC can belong to
a shared repository or be user-defined. ?NP?
and ?VP? are data category instances of the
class SyntacticPhrase, whereas and ?subj?
and ?obj? are data category instances of the
class SyntacticFunction.
? lexical operations, which are special lexical
entities allowing the user to define multilin-
3MILE is based on the experience derived from exist-
ing computational lexicons (e.g. LE-PAROLE, SIMPLE, Eu-
roWordNet, etc.).
828
gual conditions and perform operations on
lexical entries.
Originally, in order to meet expectations placed
upon lexicons as critical resources for content pro-
cessing in the Semantic Web, the MILE syntactic
and semantic lexical objects have been formalized
in RDF(S), thus providing a web-based means to
implement the MILE architecture and allowing for
encoding individual lexical entries as instances of
the model (Ide et al, 2003; Bertagna et al, 2004b).
In the framework of our project, by situating our
work in the context of W3C standards and relying
on standardized technologies underlying this com-
munity, the original RDF schema for ISLE lexi-
cal entries has been made compliant to OWL. The
whole data model has been formalized in OWL by
using Prote?ge? 3.2 beta and has been extended to
cover the morphological component as well (see
Figure 2). Prote?ge? 3.2 beta has been also used as
a tool to instantiate the lexical entries of our sam-
ple monolingual lexicons, thus ensuring adherence
to the model, encoding coherence and inter- and
intra-lexicon consistency.
3 Existing problems with the MILE
framework for Asian languages
In this section, we will explain some problematic
phenomena of Asian languages and discuss pos-
sible extensions of the MILE framework to solve
them.
Inflection The MILE provides the powerful
framework to describe the information about in-
flection. InflectedForm class is devoted to de-
scribe inflected forms of a word, while Inflec-
tionalParadigm to define general inflection rules.
However, there is no inflection in several Asian
languages, such as Chinese and Thai. For these
languages, we do not use the Inflected Form and
Inflectional Paradigm.
Classifier Many Asian languages, such as
Japanese, Chinese, Thai and Korean, do not dis-
tinguish singularity and plurality of nouns, but use
classifiers to denote the number of objects. The
followings are examples of classifiers of Japanese.
? inu
(dog)
ni
(two)
hiki
(CL)
? ? ? two dogs
? hon
(book)
go
(five)
satsu
(CL)
? ? ? five books
?CL? stands for a classifier. They always follow
cardinal numbers in Japanese. Note that differ-
ent classifiers are used for different nouns. In the
above examples, classifier ?hiki? is used to count
noun ?inu (dog)?, while ?satsu? for ?hon (book)?.
The classifier is determined based on the semantic
type of the noun.
In the Thai language, classifiers are used in var-
ious situations (Sornlertlamvanich et al, 1994).
The classifier plays an important role in construc-
tion with noun to express ordinal, pronoun, for in-
stance. The classifier phrase is syntactically gener-
ated according to a specific pattern. Here are some
usages of classifiers and their syntactic patterns.
? Enumeration
(Noun/Verb)-(cardinal number)-(CL)
e.g. nakrian
(student)
3 khon
(CL)
? ? ? three students
? Ordinal
(Noun)-(CL)-/thi:/-(cardinal number)
e.g. kaew
(glass)
bai
(CL)
thi: 4
(4th)
? ? ? the 4th glass
? Determination
(Noun)-(CL)-(Determiner)
e.g. kruangkhidlek
(calculator)
kruang
(CL)
nii
(this)
? ? ? this calculator
Classifiers could be dealt as a class of the part-
of-speech. However, since classifiers depend on
the semantic type of nouns, we need to refer to
semantic features in the morphological layer, and
vice versa. Some mechanism to link between fea-
tures beyond layers needs to be introduced into the
current MILE framework.
Orthographic variants Many Chinese words
have orthographic variants. For instance, the con-
cept of rising can be represented by either char-
acter variants of sheng1: ? or ?. However,
the free variants become non-free in certain com-
pound forms. For instance, only? allowed for?
? ?liter?, and only? is allowed for?? ?to sub-
lime?. The interaction of lemmas and orthographic
variations is not yet represented in MILE.
Reduplication as a derivational process In
some Asian languages, reduplication of words de-
rives another word, and the derived word often has
a different part-of-speech. Here are some exam-
ples of reduplication in Chinese. Man4 ? ?to be
slow? is a state verb, while a reduplicated form
829
Inflectional
Paradigm
Lexical Entry SyntacticUnit
Form Lemmatized Form Stem
Inflected Form
Combiner
Calculator Mrophfeat
Operation Argument
Morph
DataCats
0..*
0..* 0..*
0..*
0..*
0..1
0..*
0..*
1..*
<LemmatizedForm rdf:ID="LFstar">
  <hasInflectedForm>
    <InflectedForm rdf:ID="stars">
      <hasMorphoFeat>
<MorphoFeat rdf:ID="pl">
  <number rdf:datatype="http://www.w3c.org/
2001/ XMLSchema#string">
    plural
  </number>
</MorphoFeat>
      </hasMorphoFeat>
    </InflectedForm>
  </hasInflectedForm>
  <hasInflectedForm>
    <InflectedForm rdf:ID="star">
      <hasMorphoFeat>
<MorphoFeat rdf:ID="sg">
  <number rdf:datatype="http://www.w3c.org/
2001/ XMLSchema#string">
    singular
  </number>
</MorphoFeat>
      </hasMorphoFeat>
    </InflectedForm>
  </hasInflectedForm>
</LemmatiedForm>
Figure 2: Formalization of the morphological layer and excerpt of a sample RDF instantiation
man4-man4 ?? is an adverb. Another example
of reduplication involves verbal aspect. Kan4 ?
?to look? is an activity verb, while the reduplica-
tive form kan4-kan4 ??, refers to the tentative
aspect, introducing either stage-like sub-division
or the event or tentativeness of the action of the
agent. This morphological process is not provided
for in the current MILE standard.
There are also various usages of reduplication in
Thai. Some words reduplicate themselves to add a
specific aspect to the original meaning. The redu-
plication can be grouped into 3 types according to
the tonal sound change of the original word.
? Word reduplication without sound change
e.g. /dek-dek/ ? ? ? (N) children, (ADV) child-
ishly, (ADJ) childish
/sa:w-sa:w/ ? ? ? (N) women
? Word reduplication with high tone on the first
word
e.g. /dam4-dam/ ? ? ? (ADJ) extremely black
/bo:i4-bo:i/ ? ? ? (ADV) really often
? Triple word reduplication with high tone on
the second word
e.g. /dern-dern4-dern/ ?? (V) intensively walk
/norn-norn4-norn/??(V) intensively sleep
In fact, only the reduplication of the same sound
is accepted in the written text, and a special sym-
bol, namely /mai-yamok/ is attached to the origi-
nal word to represent the reduplication. The redu-
plication occurs in many parts-of-speech, such as
noun, verb, adverb, classifier, adjective, preposi-
tion. Furthermore, various aspects can be added
to the original meaning of the word by reduplica-
tion, such as pluralization, emphasis, generaliza-
tion, and so on. These aspects should be instanti-
ated as features.
Change of parts-of-speech by affixes Af-
fixes change parts-of-speech of words in
Thai (Charoenporn et al, 1997). There are
three prefixes changing the part-of-speech of the
original word, namely /ka:n/, /khwa:m/, /ya:ng/.
They are used in the following cases.
? Nominalization
/ka:n/ is used to prefix an action verb and
/khwa:m/ is used to prefix a state verb
in nominalization such as /ka:n-tham-nga:n/
(working), /khwa:m-suk/ (happiness).
? Adverbialization
An adverb can be derived by using /ya:ng/ to
prefix a state verb such as /ya:ng-di:/ (well).
Note that these prefixes are also words, and form
multi-word expressions with the original word.
This phenomenon is similar to derivation which
is not handled in the current MILE framework.
Derivation is traditionally considered as a different
phenomenon from inflection, and current MILE
focuses on inflection. The MILE framework is al-
ready being extended to treat such linguistic phe-
nomenon, since it is important to European lan-
guages as well. It would be handled in either the
morphological layer or syntactic layer.
830
Function Type Function types of predicates
(verbs, adjectives etc.) might be handled in a
partially different way for Japanese. In the syn-
tactic layer of the MILE framework, Function-
Type class is prepared to denote subcategorization
frames of predicates, and they have function types
such as ?subj? and ?obj?. For example, the verb
?eat? has two FunctionType data categories of
?subj? and ?obj?. Function types basically stand
for positions of case filler nouns. In Japanese,
cases are usually marked by postpositions and case
filler positions themselves do not provide much in-
formation on case marking. For example, both of
the following sentences mean the same, ?She eats
a pizza.?
? kanojo
(she)
ga
(NOM)
piza
(pizza)
wo
(ACC)
taberu
(eat)
? piza
(pizza)
wo
(ACC)
kanojo
(she)
ga
(NOM)
taberu
(eat)
?Ga? and ?wo? are postpositions which mark
nominative and accusative cases respectively.
Note that two case filler nouns ?she? and ?pizza?
can be exchanged. That is, the number of slots is
important, but their order is not.
For Japanese, we might use the set of post-
positions as values of FunctionType instead of
conventional function types such as ?subj? and
?obj?. It might be an user defined data category or
language dependent data category. Furthermore,
it is preferable to prepare the mapping between
Japanese postpositions and conventional function
types. This is interesting because it seems more
a terminological difference, but the model can be
applied also to Japanese.
4 Building sample lexicons
4.1 Swadesh list and basic lexicon
The issue involved in defining a basic lexicon for a
given language is more complicated than one may
think (Zhang et al, 2004). The naive approach of
simply taking the most frequent words in a lan-
guage is flawed in many ways. First, all frequency
counts are corpus-based and hence inherit the bias
of corpus sampling. For instance, since it is eas-
ier to sample written formal texts, words used pre-
dominantly in informal contexts are usually under-
represented. Second, frequency of content words
is topic-dependent and may vary from corpus to
corpus. Last, and most crucially, frequency of a
word does not correlate to its conceptual necessity,
which should be an important, if not only, criteria
for core lexicon. The definition of a cross-lingual
basic lexicon is even more complicated. The first
issue involves determination of cross-lingual lexi-
cal equivalencies. That is, how to determine that
word a (and not a?) in language A really is word b
in language B. The second issue involves the deter-
mination of what is a basic word in a multilingual
context. In this case, not even the frequency of-
fers an easy answer since lexical frequency may
vary greatly among different languages. The third
issue involves lexical gaps. That is, if there is a
word that meets all criteria of being a basic word
in language A, yet it does not exist in language D
(though it may exist in languages B, and C). Is this
word still qualified to be included in the multilin-
gual basic lexicon?
It is clear not all the above issues can be un-
equivocally solved with the time frame of our
project. Fortunately, there is an empirical core lex-
icon that we can adopt as a starting point. The
Swadesh list was proposed by the historical lin-
guist Morris Swadesh (Swadesh, 1952), and has
been widely used by field and historical linguists
for languages over the world. The Swadesh list
was first proposed as lexico-statistical metrics.
That is, these are words that can be reliably ex-
pected to occur in all historical languages and can
be used as the metrics for quantifying language
variations and language distance. The Swadesh
list is also widely used by field linguists when
they encounter a new language, since almost all
of these terms can be expected to occur in any
language. Note that the Swadesh list consists of
terms that embody human direct experience, with
culture-specific terms avoided. Swadesh started
with a 215 items list, before cutting back to 200
items and then to 100 items. A standard list of
207 items is arrived at by unifying the 200 items
list and the 100 items list. We take the 207 terms
from the Swadesh list as the core of our basic lex-
icon. Inclusion of the Swadesh list also gives us
the possibility of covering many Asian languages
in which we do not have the resources to make a
full and fully annotated lexicon. For some of these
languages, a Swadesh lexicon for reference is pro-
vided by a collaborator.
4.2 Aligning multilingual lexical entries
Since our goal is to build a multilingual sample
lexicon, it is required to align words in several
831
Asian languages. In this subsection, we propose
a simple method to align words in different lan-
guages. The basic idea for multilingual alignment
is an intermediary by English. That is, first we
prepare word pairs between English and other lan-
guages, then combine them together to make cor-
respondence among words in several languages.
The multilingual alignment method currently we
consider is as follows:
1. Preparing the set of frequent words of each
language
Suppose that {Jw
i
}, {Cw
i
}, {Tw
i
} is the
set of frequent words of Japanese, Chinese
and Thai, respectively. Now we try to con-
struct a multilingual lexicon for these three
languages, however, our multilingual align-
ment method can be easily extended to han-
dle more languages.
2. Obtaining English translations
A word Xw
i
is translated into a set of En-
glish words EXw
ij
by referring to the bilin-
gual dictionary, where X denotes one of our
languages, J , C or T . We can obtain map-
pings as in (1).
Jw
1
: EJw
11
, EJw
12
, ? ? ?
Jw
2
: EJw
21
, EJw
22
, ? ? ?
...
Cw
1
: ECw
11
, ECw
12
, ? ? ?
Cw
2
: ECw
21
, ECw
22
, ? ? ?
...
Tw
1
: ETw
11
, ETw
12
, ? ? ?
Tw
2
: ETw
21
, ETw
22
, ? ? ?
...
(1)
Notice that this procedure is automatically
done and ambiguities would be left at this
stage.
3. Generating new mapping
From mappings in (1), a new mapping is gen-
erated by inverting the key. That is, in the
new mapping, a key is an English word Ew
i
and a correspondence for each key is sets
of translations XEw
ij
for 3 languages, as
shown in (2):
Ew
1
: (JEw
11
, JEw
12
, ? ? ?)
(CEw
11
, CEw
12
, ? ? ?)
(TEw
11
, TEw
12
, ? ? ?)
Ew
2
: (JEw
21
, JEw
22
, ? ? ?)
(CEw
21
, CEw
22
, ? ? ?)
(TEw
21
, TEw
22
, ? ? ?)
...
(2)
Notice that at this stage, correspondence be-
tween different languages is very loose, since
they are aligned on the basis of sharing only
a single English word.
4. Refinement of alignment
Groups of English words are constructed by
referring to the WordNet synset information.
For example, suppose that Ew
i
and Ew
j
be-
long to the same synset S
k
. We will make a
new alignment by making an intersection of
{XEw
i
} and {XEw
j
} as shown in (3).
Ew
i
: (JEw
i1
, ??) (CEw
i1
, ??) (TEw
i1
, ??)
Ew
j
: (JEw
j1
, ??)(CEw
j1
, ??)(TEw
j1
, ??)
? intersection
S
k
: (JEw?
k1
, ??)(CEw?
k1
, ??)(TEw?
k1
, ??)
(3)
In (3), the key is a synset S
k
, which is sup-
posed to be a conjunction of Ew
i
and Ew
j
,
and the counterpart is the intersection of set
of translations for each language. This oper-
ation would reduce the number of words of
each language. That means, we can expect
that the correspondence among words of dif-
ferent languages becomes more precise. This
new word alignment based on a synset is a
final result.
To evaluate the performance of this method,
we conducted a preliminary experiment using the
Swadesh list. Given the Swadesh list of Chi-
nese, Italian, Japanese and Thai as a gold stan-
dard, we tried to replicate these lists from the En-
glish Swadesh list and bilingual dictionaries be-
tween English and these languages. In this experi-
ment, we did not perform the refinement step with
WordNet. From 207 words in the Swadesh list,
we dropped 4 words (?at?, ?in?, ?with? and ?and?)
due to their too many ambiguities in translation.
As a result, we obtained 181 word groups
aligned across 5 languages (Chinese, English, Ital-
ian, Japanese and Thai) for 203 words. An
aligned word group was judged ?correct? when the
words of each language include only words in the
Swadesh list of that language. It was judged ?par-
tially correct? when the words of a language also
include the words which are not in the Swadesh
list. Based on the correct instances, we obtain
0.497 for precision and 0.443 for recall. These fig-
ures go up to 0.912 for precision and 0.813 for re-
call when based on the partially correct instances.
This is quite a promising result.
832
5 Upper-layer ontology
The empirical success of the Swadesh list poses
an interesting question that has not been explored
before. That is, does the Swadesh list instantiates a
shared, fundamental human conceptual structure?
And if there is such as a structure, can we discover
it?
In the project these fundamental issues are as-
sociated with our quest for cross-lingual interop-
erability. We must make sure that the items of
the basic lexicon are given the same interpreta-
tion. One measure taken to ensure this consists in
constructing an upper-ontology based on the ba-
sic lexicon. Our preliminary work of mapping the
Swadesh list items to SUMO (Suggested Upper
Merged Ontology) (Niles and Pease, 2001) has al-
ready been completed. We are in the process of
mapping the list to DOLCE (Descriptive Ontology
for Linguistic and Cognitive Engineering) (Ma-
solo et al, 2003). After the initial mapping, we
carry on the work to restructure the mapped nodes
to form a genuine conceptual ontology based on
the language universal basic lexical items. How-
ever one important observation that we have made
so far is that the success of the Swadesh list is
partly due to its underspecification and to the lib-
erty it gives to compilers of the list in a new lan-
guage. If this idea of underspecification is essen-
tial for basic lexicon for human languages, then we
must resolve this apparent dilemma of specifying
them in a formal ontology that requires fully spec-
ified categories. For the time being, genuine ambi-
guities resulted in the introduction of each disam-
biguated sense in the ontology. We are currently
investigating another solution that allows the in-
clusion of underspecified elements in the ontology
without threatening its coherence. More specifi-
cally we introduce a underspecified relation in the
structure for linking the underspecified meaning
to the different specified meaning. The specified
meanings are included in the taxonomic hierarchy
in a traditional manner, while a hierarchy of un-
derspecified meanings can be derived thanks to the
new relation. An underspecified node only inherits
from the most specific common mother of its fully
specified terms. Such distinction avoids the clas-
sical misuse of the subsumption relation for rep-
resenting multiple meanings. This method does
not reflect a dubious collapse of the linguistic and
conceptual levels but the treatment of such under-
specifications as truly conceptual. Moreover we
Internet
Query
Local 
DB
User interest
 model
Topic
Feedback
Search
engine
Crawler
Retrieval
results
Figure 3: The system architecture
hope this proposal will provide a knowledge rep-
resentation framework for the multilingual align-
ment method presented in the previous section.
Finally, our ontology will not only play the role
of a structured interlingual index. It will also serve
as a common conceptual base for lexical expan-
sion, as well as for comparative studies of the lex-
ical differences of different languages.
6 Evaluation through an application
To evaluate the proposed framework, we are build-
ing an information retrieval system. Figure 3
shows the system architecture.
A user can input a topic to retrieve the docu-
ments related to that topic. A topic can consist
of keywords, website URL?s and documents which
describe the topic. From the topic information, the
system builds a user interest model. The system
then uses a search engine and a crawler to search
for information related to this topic in WWW and
stores the results in the local database. Generally,
the search results include many noises. To filter
out these noises, we build a query from the user
interest model and then use this query to retrieve
documents in the local database. Those documents
similar to the query are considered as more related
to the topic and the user?s interest, and are returned
to the user. When the user obtains these retrieval
results, he can evaluate these documents and give
the feedback to the system, which is used for the
further refinement of the user interest model.
Language resources can contribute to improv-
ing the system performance in various ways.
Query expansion is a well-known technique which
expands user?s query terms into a set of similar and
related terms by referring to ontologies. Our sys-
tem is based on the vector space model (VSM) and
traditional query expansion can be applicable us-
ing the ontology.
There has been less research on using lexical in-
833
formation for information retrieval systems. One
possibility we are considering is query expansion
by using predicate-argument structures of terms.
Suppose a user inputs two keywords, ?hockey?
and ?ticket? as a query. The conventional query
expansion technique expands these keywords to
a set of similar words based on an ontology. By
referring to predicate-argument structures in the
lexicon, we can derive actions and events as well
which take these words as arguments. In the above
example, by referring to the predicate-argument
structure of ?buy? or ?sell?, and knowing that
these verbs can take ?ticket? in their object role,
we can add ?buy? and ?sell? to the user?s query.
This new type of expansion requires rich lexical
information such as predicate argument structures,
and the information retrieval system would be a
good touchstone of the lexical information.
7 Concluding remarks
This paper outlined a new project for creating a
common standard for Asian language resources
in cooperation with other initiatives. We start
with three Asian languages, Chinese, Japanese
and Thai, on top of the existing framework which
was designed mainly for European languages.
We plan to distribute our draft to HLT soci-
eties of other Asian languages, requesting for
their feedback through various networks, such
as the Asian language resource committee net-
work under Asian Federation of Natural Language
Processing (AFNLP)4, and Asian Language Re-
source Network project5. We believe our ef-
forts contribute to international activities like ISO-
TC37/SC46 (Francopoulo et al, 2006) and to the
revision of the ISO Data Category Registry (ISO
12620), making it possible to come close to the
ideal international standard of language resources.
Acknowledgment
This research was carried out through financial
support provided under the NEDO International
Joint Research Grant Program (NEDO Grant).
References
F. Bertagna, A. Lenci, M. Monachini, and N. Calzo-
lari. 2004a. Content interoperability of lexical re-
sources, open issues and ?MILE? perspectives. In
4http://www.afnlp.org/
5http://www.language-resource.net/
6http://www.tc37sc4.org/
Proceedings of the 4th International Conference on
Language Resources and Evaluation (LREC2004),
pages 131?134.
F. Bertagna, A. Lenci, M. Monachini, and N. Calzo-
lari. 2004b. The MILE lexical classes: Data cat-
egories for content interoperability among lexicons.
In A Registry of Linguistic Data Categories within
an Integrated Language Resources Repository Area
? LREC2004 Satellite Workshop, page 8.
N. Calzolari, F. Bertagna, A. Lenci, and M. Mona-
chini. 2003. Standards and best practice for mul-
tilingual computational lexicons. MILE (the mul-
tilingual ISLE lexical entry). ISLE Deliverable
D2.2&3.2.
T. Charoenporn, V. Sornlertlamvanich, and H. Isahara.
1997. Building a large Thai text corpus ? part-
of-speech tagged corpus: ORCHID?. In Proceed-
ings of the Natural LanguageProcessing Pacific Rim
Symposium.
G. Francopoulo, G. Monte, N. Calzolari, M. Mona-
chini, N. Bel, M. Pet, and C. Soria. 2006. Lex-
ical markup framework (LMF). In Proceedings of
LREC2006 (forthcoming).
N. Ide, A. Lenci, and N. Calzolari. 2003. RDF in-
stantiation of ISLE/MILE lexical entries. In Pro-
ceedings of the ACL 2003 Workshop on Linguistic
Annotation: Getting the Model Right, pages 25?34.
A. Lenci, N. Bel, F. Busa, N. Calzolari, E. Gola,
M. Monachini, A. Ogonowsky, I. Peters, W. Peters,
N. Ruimy, M. Villegas, and A. Zampolli. 2000.
SIMPLE: A general framework for the development
of multilingual lexicons. International Journal of
Lexicography, Special Issue, Dictionaries, Thesauri
and Lexical-Semantic Relations, XIII(4):249?263.
C. Masolo, A. Borgo, S.; Gangemi, N. Guarino, and
A. Oltramari. 2003. Wonderweb deliverable d18
?ontology library (final)?. Technical report, Labo-
ratory for Applied Ontology, ISTC-CNR.
I. Niles and A Pease. 2001. Towards a standard upper
ontology. In Proceedings of the 2nd International
Conference on Formal Ontology in Information Sys-
tems (FOIS-2001).
V. Sornlertlamvanich, W. Pantachat, and S. Mek-
navin. 1994. Classifier assignment by corpus-
based approach. In Proceedings of the 15th Inter-
national Conference on Computational Linguistics
(COLING-94), pages 556?561.
M. Swadesh. 1952. Lexico-statistical dating of pre-
historic ethnic contacts: With special reference to
north American Indians and Eskimos. In Proceed-
ings of the American Philo-sophical Society, vol-
ume 96, pages 452?463.
H. Zhang, C. Huang, and S. Yu. 2004. Distributional
consistency: A general method for defining a core
lexicon. In Proceedings of the 4th International
Conference on Language Resources and Evaluation
(LREC2004), pages 1119?1222.
834
Feature Selection in Categorizing Procedural Expressions
Mineki Takechi   , Takenobu Tokunaga

, Yuji Matsumoto  , Hozumi Tanaka 
 
Fujitsu Limited
17-25 Shinkamata 1-chome, Ota-ku, Tokyo 144-8588, Japan

Department of Computer Science, Tokyo Institute of Technology
2-12-2 Ookayama, Meguro-ku, Tokyo 152-8552, Japan
 Graduate School of Information Science, Nara Institute of Science and Technology
8916-5 Takayama-cho, Ikoma city, Nara 630-0101, Japan

mineki-t,matsu  @is.aist-nara.ac.jp,  take,tanaka  @cl.cs.titech.ac.jp
Abstract
Text categorization, as an essential com-
ponent of applications for user navigation
on the World Wide Web using Question-
Answering in Japanese, requires more ef-
fective features for the categorization of
documents and the efficient acquisition of
knowledge. In the questions addressed by
such navigation, we focus on those ques-
tions for procedures and intend to clarify
specification of the answers.
1 Introduction
Recent methodologies of text categorization as ap-
plied to Question-Answering(QA) and user naviga-
tion on the Web address new types of problems, such
as the categorization of texts based on the question
type in addition to one based on domain and genre.
For good performance in a shallow approach, which
exploits the shallow specification of texts to cate-
gorize them, requires a great deal of knowledge of
the expressions in the answers corresponding to the
questions. In most past QA research, the types of
question have been primarily restricted to fact-based
questions. However, in user navigation on the Web,
other types of questions should be supported. In this
paper, we focus on questions requiring a procedure
asking for such navigation and intend to study the
features necessary for its extraction by illustrating
the specification of its answer. In the above type of
QA, very few studies have aimed at answering ques-
tions by extracting procedural expressions from web
pages. Accordingly, a) representations in a web text
to indicate a procedure, b) the method of extracting
those representations, and c) the way to combine re-
lated texts as an answer, are issues that have not been
sufficiently clarified. Consequently, past studies do
not provide a general approach for solving this task.
In contrast, it has been reported that the texts re-
lated to QA in web pages contain many lists in the
descriptions. We decided to focus on lists including
procedural expressions and employed an approach
of extracting lists from web pages as answers. This
results in difficulty in extracting the answers written
in a different style. However, compared to seeking
answer candidates from a document set including
various web pages, it is expected that they will be
found relatively more often from the gathered lists.
In this study, our motivation is to provide users with
the means to navigate accurately and credibly to in-
formation on the Web, but not to give a complete
relevant document set with respect to user queries.
In addition, a list is a summarization made by hu-
mans, and thus it is edited to make it easy to under-
stand. Therefore, the restriction to itemized answers
doesn?t lose its effectiveness in our study. In the ini-
tial step of our work for this type of QA, we discuss a
text categorization task that divides a set of lists into
two groups: procedural and non-procedural. First,
we gathered web pages from a search engine and
extracted lists including the procedural expressions
tagged with any HTML(Hyper Text Markup Lan-
guage) list tags found, and observed their character-
istics. Then we examined Support Vector Machines
(SVMs) and sequential pattern mining relative to the
set of lists, and observed the obtained model to find
useful features for extraction of answers to explain
a relevant procedure. In the following section, we
introduce some related work. Section 3 presents the
list features including procedural expressions in the
web pages. Subsequently, we will apply our ma-
chine learning and sequential pattern mining tech-
niques to learn these features, which are briefly il-
lustrated in Section 4. Section 5 shows the results
of our categorization experiments. Finally, Section
6 presents our conclusions and Section 7 gives our
plans for future study.
2 Related Works
The questions related in all procedures were ad-
dressed by an expert system(Barr et al, 1989). How-
ever, in QA and information retrieval for open do-
main documents from the Web, the system requires a
more flexible and more machine-operable approach
because of the diversity and changeable nature of
the information resources. Many competitions, e.g.
TREC and NTCIR, are being held each year and
various studies have been presented (Eguchi et al,
2003; Voorhees, 2001). Recently, the most suc-
cessful approach has been to combine many shal-
low clues in the texts and occasionally in other lin-
guistic resources. In this approach, the performance
of passage retrieval and categorization is vital for
the performance of the entire system. In particular,
the productiveness of the knowledge of expressions
corresponding to each question type, which is prin-
cipally exploited in retrieval and categorization, is
important. In this perspective, that means that the
requirements for categorization in such applications
are different from those in previous categorizations.
Many studies have been made that are related to QA.
Fujii et al(2001) studied QA and knowledge acqui-
sition for definition type questions. Approaches by
seeking any answer text in the pages of FAQs or
newsgroups appeared in some studies(Hamada et al,
2002; Lai et al, 2002). Automatic QA systems in a
support center of organizations was addressed in a
study by Kurohashi et al(2000).
However, most of the previous studies targeting
QA address fact type or definition type questions,
such as ?When was Mozart born?? or ?What is plat-
inum??. Previous research addressing the type of
QA relevant to procedures in Japanese is inconclu-
Table 1: Result from a Search Engine.
Keyword Gathered Retrieved Vaild Pages
tejun 3,713 748 629
houhou 5,998 916 929
Table 2: Domain and Type of List.
Domain Procedures Non-Procedures All
Computer 558 ( 295 ) 1666 ( 724 ) 2224
Others 163 ( 64 ) 1733 ( 476 ) 1896
All 721 3399 4120
sive. In text categorization research, the feature se-
lection has been discussed(Taira and Haruno, 2000;
Yang and Pedersen, 1997). However, most of the
research addressed categorization into taxonomy re-
lated to domain and genre. The features that are
used are primarily content words, such as nouns,
verbs, and adjectives. Function words and frequent
formative elements were usually eliminated. How-
ever, some particular areas of text categorization,
for example, authorship identification, suggested a
feasibility of text categorization with functional ex-
pressions on a different axis of document topics.
From the perspective of seeking methods of domain-
independent categorization for QA, this paper inves-
tigates the feasibility of functional expressions as a
feature for the extraction of lists including procedu-
ral expressions.
3 Extraction of Procedural Expressions
3.1 Answering Procedures with Lists
We can easily imagine a situation in which people
ask procedural questions, for instance a user who
wants to know the procedure for installing the Red-
Hat Linux OS. When using a web search engine,
the user could employ a keyword related to the do-
main, such as ?RedHat,? ?install,? or the synonyms
of ?procedure,? such as ?method? or ?process.? In
conclusion, the search engine will often return a re-
sult that does not include the actual procedures, for
instance, only including the lists of hyperlinks to
some URLs or simple alternatives that have no in-
tentional order as is given.
This paper addresses the issue in the context of
the solution being to return to the actual procedure.
In the initial step of this study, we focused on the
case that the continuous answer candidate passage
is in the original text and furthermore restricted the
form of documentation in the list. The list could
be expected to contain important information, be-
cause it is a summarization done by a human. It
has certain benefits pertaining to computer process-
ing. These are: a) a large number of lists in FAQs or
homepages on web pages, b) some clues before and
after the lists such as title and leads, c) extraction
which is relatively easy by using HTML list tags,
e.g. <OL>,<UL>.
In this study, a binary categorization was con-
ducted, which divided a set of lists into two classes
of procedures and non-procedures. The purpose is
to reveal an effective set of features to extract a list
explaining the procedure by examining the results of
the categorization.
3.2 Collection of Lists from Web Pages
To study the features of lists contained in web pages,
the sets of lists were made according to the follow-
ing steps (see Table 1) :
Step 1 Enter tejun (procedure) and houhou
(method) to Google(Brin and Page, 1998) as
keywords, and obtain a list of URLs that are
to serve as the seeds of collection for the next
step (Gathered).
Step 2 Recursively search from the top page to the
next lower page in the hyperlink structure and
gather the HTML pages (Retrieved).
Step 3 Extract the passages from the pages in Step
2 that are tagged with <OL> or <UL>. If a list
has multiple layers with nested tags, each layer
is decomposed as an independent list (Valid
Pages).
Step 4 Collect lists including no less than two
items. The document is created in such a way
that an article is equal to a list.
Subsequently, the document set was categorized
into procedure type and non-procedure type subsets
by human judgment. For this categorization, the def-
inition of the list to explain the procedure was as
follows: a) The percentage of items including ac-
tions or operations in a list is more than or equal
to 50%. b) The contexts before and after the lists
are ignored in the judgment. An item means an ar-
ticle or an item that is prefixed by a number or a
mark such as a bullet. That generally involves mul-
tiple sentences. In this categorization, two people
categorized the same lists and a kappa test(Siegel
and Castellan, 1988) is applied to the result. We
obtained a kappa value of 0.87, i.e., a near-perfect
match, in the computer domain and 0.66, i.e., a sub-
stantial match, in the other domains. Next, the doc-
uments were categorized according to their domain
by referring to the page including a list. Table 2 lists
the results. The values in parentheses indicate the
number of lists before decomposition of nested tags.
The documents of the Computer domain were dom-
inant; those of the other domains consisted of only a
few documents and were lumped together into a doc-
ument set named ?Others.? This domain consists of
documents regarding education, medical treatment,
weddings, etc. The instructions of software usage or
operation on the home pages of web services were
also assigned to the computer domain.
3.3 Procedural Expressions in the Lists
From the observations of the categorized lists made
by humans, the following results were obtained: a)
The first sentence in an item often describes an ac-
tion or an operation. b) There are two types of items
that terminate the first sentence: nominalized and
nonnominalized. c) In the case of the nominalized
type, verbal nouns are very often used at the end
of sentence. d) Arguments marked by ga (a par-
ticle marking nominative) or ha (a particle mark-
ing topic) and negatives are rarely used, while ar-
guments marked by wo (a particle marking object)
appear frequently. e) At the end of sentences and
immediately before punctuation marks, the same ex-
pressions appear repeatedly. Verbal nouns are inher-
ent expressions verbified by being followed by the
light verb suru in Japanese. If the features above are
domain-independent characteristics, the lists in a mi-
nor domain can be categorized by using the features
that were learned from the lists in the other major
domain. The function words or flections appearing
at the ends of sentences and before punctuation are
known as markers, and specify the style of descrip-
Table 3: Types of Tags.
tag type object types
Document dv list
p item
su sentence
Part of Speech np noun[1]
prefix
snp verbal noun
vp verb
adp particle[2]
adverb
adnominal
conjunction
ajp adjuctive
aup sentece-final-particle
auxiliary verb
suffix
ij interjection
seg others (punctuation, etc.)
unknown unknown word
tion in Japanese. Thus, to explain a procedure, the
list can be expected to have inherent styles of de-
scription.
These features are very similar to those in an au-
thorship identification task(Mingzhe, 2002; Tsuboi
and Matsumoto, 2002). That task uses word n-gram,
distribution of part of speech, etc. In recent research
for web documents, frequent word sequences have
also been examined. Our approach is based on these
features.
4 Features
4.1 Baseline
In addition to the features based on the presence of
specific words, we examined sequences of words for
our task. Tsuboi et al(2002) used a method of se-
quential pattern mining, PrefixSpan, and an algo-
rithm of machine learning, Support Vector Machine
in addition to morphological N-grams. They pro-
posed making use of the frequent sequential patterns
of words in sentences. This approach is expected
to contribute to explicitly use the relationships of
1Except verbal nouns
2Except sentence-final particles
distant words in the categorization. The list con-
tains differences in the omissions of certain particles
and the frequency of a particle?s usage to determine
whether the list is procedural. Such sequential pat-
terns are anticipated to improve the accuracy of cat-
egorization. The words in a sentence are transferred
to PrefixSpan after preprocessing, as follows:
Step 1 By using ChaSen(Matsumoto et al, 1999), a
Japanese POS(Part Of Speech) tagger, we put
the document tags and the POS tags into the
list. Table 3 lists the tag set that was used.
These tags are only used for distinguishing ob-
jects. The string of tags was ignored in sequen-
tial pattern mining.
Step 2 After the first n sentences are extracted from
each list item, a sequence is made for each sen-
tence. Sequential pattern mining is performed
for an item (literal) in a sequence as a mor-
pheme.
By using these features, we conducted categoriza-
tion with SVM. It is one of the large margin classi-
fiers, which shows high generalization performance
even in high dimensional spaces(Vapnik, 1995).
SVM is beneficial for our task, because it is un-
known which features are effective, and we must use
many features in categorization to investigate their
effectiveness. The dimension of the feature space is
relatively high.
4.2 Sequential Pattern Mining
Sequential pattern mining consists of finding all fre-
quent subsequences, that are called sequential pat-
terns, in the database of sequences of literals. Apri-
ori(Agrawal and Srikant, 1994) and PrefixSpan(Pei
et al, 2001) are examples of sequential pattern min-
ing methods. The Apriori algorithm is one of the
most widely used methods, however there is a great
deal of room for improvement in terms of calcula-
tion cost. The PrefixSpan algorithm succeed in re-
ducing the cost of calculation by performing an op-
eration, called projection, which confines the range
of the search to sets of frequent subsequences. De-
tails of the PrefixSpan algorithm are provided in an-
other paper(Pei et al, 2001).
Table 4: Statistics of Data Sets.
Proc. Non-Proc. Comp. Others
Lists 721 3399 2224 1896
Items 4.6 / 2.8 4.9 / 5.7 4.8 / 6.1 4.9 / 4.4
Sen. 1.8 / 1.7 1.3 / 0.9 1.5 / 1.1 1.3 / 1.1
Char. 40.3 / 48.6 32.6 / 42.4 35.6 / 40.1 32.6 / 48.2
5 Experiments and Results
5.1 Experimental Settings
In the first experiment, to determine the categoriza-
tion capability of a domain, we employed a set of
lists in the Computer domain and conducted a cross-
validation procedure. The document set was divided
into five subsets of nearly equal size, and five dif-
ferent SVMs, the training sets of four of the sub-
sets, and the remaining one classified for testing. In
the second experiment, to determine the categoriza-
tion capability of an open domain, we employed a
set of lists from the Others domain with the docu-
ment set in the first experiment. Then, the set of the
lists from the Others domain was used in the test
and the one from the Computer domain was used
in the training, and their training and testing roles
were also switched. In both experiments, recall, pre-
cision, and, occasionally, F-measure value were cal-
culated to evaluate categorization performance. F-
measure is calculated with precision (P) and recall
(R) in formula 1.
  
	

(1)
The lists in the experiment were gathered from those
marked by the list tags in the pages. To focus on
the feasibility of the features in the lists for the cat-
egorization task, the contexts before and after each
list are not targeted. Table 4 lists four groups di-
vided by procedure and domain into columns, and
the numbers of lists, items, sentences, and charac-
ters in each group are in the respective rows. The
two values in each cell in Table 4 are the mean on
the left and the deviation on the right. We employed
Tiny-SVM1 and a implementation of PrefixSpan2 by
T. Kudo. To observe the direct effect of the fea-
tures, the feature vectors were binary, constructed
1http://cl.aist-nara.ac.jp/?taku-ku/software/TinySVM/
2http://cl.aist-nara.ac.jp/?taku-ku/software/prefixspan/
Table 5: POS Groups.
Combination of POS Computer Others
F1 all of words 9885 13031
F2 snp+np+vp+ajp 4570 7818
F3 snp+np+vp+ajp+unknown 9277 12169
F4 aup+adp+seg 608 862
F5 aup+adp+seg+unknown 5315 5213
F6 snp+aup+adp+seg 1493 2360
with word N-gram and patterns; polynomial kernel
degree d for the SVM was equal to one. Support
values for PrefixSpan were determined in an ad hoc
manner to produce a sufficient number of patterns in
our experimental conditions.
To investigate the effective features for list cate-
gorization, feature sets of the lists were divided into
five groups (see Table 5) with consideration given to
the difference of content word and function words
according to our observations (described in Section
3.3). The values in Table 5 indicate the numbers of
differences between words in each domain data set.
The notation of tags above, such as ?snp?, follows
the categories in Table 3. F2 and F3 consist of con-
tent words and F4 and F5 consist of function words.
F6 was a feature group, which added verbal nouns
based on our observations (described in Section 3.3).
To observe the performances of SVM, we com-
pared the results of categorizations in the conditions
of F3 and F5 with a decision tree. For decision tree
learning, j48.j48, which is an implementation of the
C4.5 algorithm by Weka3, was chosen.
In these experiments, only the first sentence in
each list item was used because in our preliminary
experiments, we obtained the best results when only
the first sentence was used in categorization. As
many as a thousand patterns from the top in the rank-
ing of frequencies were selected and used in condi-
tions from F1 to F6. For pattern selection, we ex-
amined the method based on frequency. In addition,
mutual information filtering was conducted in some
conditions for comparison with performances based
only on pattern frequency. By ranking these with the
mutual information filtering, we selected 100, 300,
3http://www.cs.waikato.ac.nz/?ml/weka/
Table 6: Result of Close-Domain.
Computer domain
1 1+2 1+2+3 pattern
F1 0.88/0.88 0.92/0.90 0.93/0.90 0.93/0.92
F2 0.85/0.86 0.90/0.87 0.91/0.85 0.89/0.88
F3 0.87/0.86 0.93/0.87 0.93/0.86 0.91/0.88
F4 0.81/0.81 0.85/0.85 0.86/0.86 0.86/0.86
F5 0.81/0.84 0.86/0.85 0.90/0.86 0.89/0.88
F6 0.85/0.87 0.90/0.89 0.91/0.89 0.89/0.89
Table 7: Results when Learning from Computer Do-
main.
Computer Domain - Others Domain
1 1+2 1+2+3 pattern
F1 0.60/0.46 0.69/0.45 0.72/0.45 0.66/0.48
F2 0.52/0.42 0.69/0.39 0.72/0.37 0.64/0.41
F3 0.56/0.46 0.68/0.44 0.70/0.42 0.63/0.45
F4 0.46/0.51 0.59/0.58 0.58/0.52 0.53/0.60
F5 0.43/0.50 0.52/0.48 0.61/0.48 0.53/0.53
F6 0.53/0.49 0.67/0.53 0.71/0.50 0.61/0.55
and 500 patterns from 1000 patterns. Furthermore,
the features of N-grams were varied to N=1, 1+2,
and 1+2+3 by incrementing N and adding new N-
grams to the features in the experiments.
5.2 Experimental Results
Table 6 lists the results of a 5-fold cross-validation
evaluation of the Computer domain lists. Gradu-
ally, N-grams and patterns were added to input fea-
ture vectors, thus N=1, 2, 3, and patterns. The fea-
ture group primarily constructed of content words
slightly overtook the function group, with the excep-
tion of recall, while trigram and patterns were added.
In the comparison of F2 and F4, differences in per-
formance are not as salient as differences in num-
bers of features. Incorporating verbal nouns into the
categorization slightly improved the results. How-
ever, the patterns didn?t work in this task. The same
experiment-switching the roles of the two list sets,
the Computer and the Others domain, was then per-
formed (see Tables 7 and 8).
Along with adding N-grams, the recall became
worse for the group of content words. In contrast,
the group of function words showed better perfor-
Table 8: Results when Learning from Others Do-
main.
Others Domain - Computer Domain
1 1+2 1+2+3 pattern
F1 0.90/0.52 0.95/0.60 0.97/0.56 0.95/0.64
F2 0.88/0.51 0.92/0.44 0.94/0.37 0.94/0.47
F3 0.90/0.46 0.95/0.48 0.97/0.41 0.96/0.49
F4 0.80/0.33 0.79/0.58 0.79/0.55 0.79/0.59
F5 0.83/0.51 0.85/0.54 0.88/0.51 0.87/0.53
F6 0.81/0.51 0.90/0.56 0.94/0.51 0.89/0.56
mance in the recall, and the overall balance of pre-
cision and recall were well-performed. Calculating
the F-measure with formula 1, in most evaluations of
open domain, the functional group overtook the con-
tent group. This deviation is more salient in the Oth-
ers domain. In the results of both the Computer do-
main and the Others domain, the model trained with
functions performed better than the model trained
with content. The function words in Japanese char-
acterize the descriptive style of the text, meaning
that this result shows a possibility of the acquisi-
tion of various procedural expressions. From an-
other perspective, when trigram was added as a fea-
ture, performance took decreased in recall. Adding
the patterns, however, improved performance. It is
assumed that there are dependencies between words
at a distance greater than three words, which is ben-
eficial in their categorization. Table 9 compares the
results of SVM and j48.j48 decision tree. Table 10
lists the effectiveness of mutual information filter-
ing. In both tables, values show the F-measure cal-
culated with formula 1. According to Table 9, SVM
overtook j48.j48 overall. j48.j48 scarcely changes
with an increase in the number of features, however,
SVM gradually improves performance. For mutual
information filtering, SVM marked the best results
with no-filter in the Computer domain. However,
in the case of learning from the Others domain, the
mutual information filtering appears effective.
5.3 Discussion
The comparison of SVM and decision tree shows the
high degree of generalization of SVM in a high di-
mensional feature space. From the results of mutual
information filtering, we can recognize that the sim-
Table 9: Comparison of SVM and Decision Tree.
1 1+2 1+2+3
SVM j48 SVM j48 SVM j48 #feature
F3 0.84 0.79 0.84 0.83 0.84 0.83 300
0.85 0.76 0.85 0.81 0.84 0.82 500
0.84 0.76 0.86 0.82 0.86 0.83 1000
0.87 0.76 0.87 0.82 0.87 0.83 5000
F5 0.84 0.79 0.84 0.82 0.82 0.81 300
0.85 0.80 0.85 0.81 0.83 0.82 500
0.86 0.80 0.86 0.81 0.84 0.81 1000
0.84 0.80 0.86 0.82 0.87 0.82 5000
Table 10: Results of Pattern Selection with Mutual
Information Filtering.
100 300 500 no-filter
Computer F3 0.53 0.53 0.53 0.52
- Others F5 0.53 0.52 0.50 0.53
Others F3 0.74 0.74 0.75 0.65
- Computer F5 0.75 0.76 0.77 0.66
ple methods of other pre-cleaning are not notably
effective when learning from documents of the same
domain. However, the simple methods work well in
our task when learning from documents consisting
of a variety of domains.
Patterns performed well with mutual information
filtering in a data set including different domains and
genres. It appears that N-grams and credible pat-
terns are effective in acquiring the common char-
acteristics of procedural expressions across differ-
ent domains. There is a possibility that the patterns
are effective for moderate narrowing of the range of
answer candidates in the early process of QA and
Web information retrieval. In the Computer domain,
categorization performed well overall in every POS
group. That is why it includes many instruction
documents, for instance software installation, com-
puter settings, online shopping, etc., and those usu-
ally use similar and restricted vocabularies. Con-
versely, the uniformity of procedural expressions in
the Computer domain causes poorer performance
when learning from the documents of the Computer
domain than when learning from the Others domain.
We also often found in their expressions that for a
Sentence :  ?   [ menyu ]    w o    s ent a k u   s h i ,
                   ?    Sel ect    [ m enu ]    a nd
                      [ h o z o n ]     w o     k ur i k k u    s ur u .   ?
                        cl i ck      th e    s w i tch    o f      [ s a v e]  .   ?
P a tter n 1  :  ? [ ?     ? ] ?     ? w o ?     ? , ?
P a tter n 2  :  ? [ ?     ? ] ?     ? w o ?     ? . ?
Figure 1: Example of Effective Patterns.
particular class of content word, special characters
were adjusted (see Figure 1). This type of pattern
occasionally contributed the correct classification in
our experiment. The movement of the performance
of content and function word along with the addition
of N-grams is notable. It is likely that making use
of the difference of their movement more directly is
useful in the categorization of procedural text.
By error analysis, the following patterns were ob-
tained: those that reflected common expressions,
including the multiple appearance of verbs with a
case-marking particle wo. This worked well for the
case in which the procedural statement partially oc-
cupied the items of the list. Where there were fewer
characters in a list and failing POS tagging, pattern
mismatch was observed.
6 Conclusion
The present work has demonstrated effective fea-
tures that can be used to categorize lists in web pages
by whether they explain a procedure. We show that
categorization to extract texts including procedural
expressions is different from traditional text catego-
rization tasks with respect to the features and behav-
iors related to co-occurrences of words. We also
show the possibility of filtering to extract lists in-
cluding procedural expressions in different domains
by exploiting those features that primarily consist of
function words and patterns with mutual informa-
tion filtering. Lists with procedural expressions in
the Computer domain can be extracted with higher
accuracy.
7 Future works
The augmentation of the volume of data sets within
the Others domain is a considerable task. In this re-
search, the number of lists in each specific domain
of the data set within the Others domain is too few to
reveal its precise nature. In more technical domains,
the categorization of lists by humans is difficult for
people who have no knowledge of the field. An-
other unresolved problem is the nested structure of
lists. In our current method, no list is nested because
it has already been decomposed during preprocess-
ing. In some cases, this treatment incorrectly cate-
gorizes lists that can be regarded as procedural types
into another group based on the condition of accept-
ing a combination of two or more different layers of
nested lists. Another difficult point is related to the
nominal list type. According to the observations of
the differences in categorization in the Others do-
main by humans, some failures are of the nominal
type. It is difficult to distinguish such cases by fea-
tures only in lists, and more clues to recognize the
type of list are required such as, for example, the
contexts before and after the list.
Acknowledgements
My deepest gratitude is to Taku Kudo who provided
Tiny-SVM and an implementation of PrefixSpan.
References
Rakesh Agrawal and Ramakrishnan Srikant. 1994. Fast
Algorithms for Mining Association Rulesr. In Pro-
ceedings of 20th International. Conference. Very Large
Data Bases (VLDB), pages 487?499.
A. Barr, P. R. Cohen, and E. A. Feigenbaum. 1989. The
Handbook of Artificial Intelligence. Kyoritsu Shup-
pan, Tokyo. Japanese Edition Translated by K. Tanaka
and K. Fuchi.
S. Brin and L. Page. 1998. The Anatomy of a Large-
Scale Hypertexual Web Search Engine. In Proceed-
ings of 7th International World Wide Web Conference.
Koji Eguchi, Keizo Oyama, Emi Ishida, Noriko Kando,
and Kazuko Kuriyama. 2003. Overview of the Web
Retrieval Task at the Third NTCIR Workshop. Tech-
nical Report NII-2003-002E, National Institute of In-
formatics.
Atsushi Fujii and Tetsuya Ishikawa. 2001. Organizing
Encyclopedic Knowledge based on the Web and its
Application to Question Answering. In Proceedings of
the 39th Annual Meeting of the Association for Com-
putational Linguistics (ACL-EACL 2001), pages 196?
203, July.
Reiko Hamada, Ichiro Ide, Shuichi Sakai, and Hidehiko
Tanaka. 2002. Structural Analysis of Cooking Prepa-
ration Steps. The Transactions of The Institute of
Electronics, D-II Vol.J85-D-II(1):79?89, January. (in
Japanese).
Sadao Kurohashi and Wataru Higasa. 2000. Dialogue
Helpsystem based on Flexible Matching of User Query
with Natural Language Knowledge Base. In Proceed-
ings of 1st ACL SIGdial Workshop on Discourse and
Dialogue, pages 141?149.
Y. Lai, K. Fung, and C. Wu. 2002. FAQ Mining via List
Detection. In Proceedings of Workshop on Multilin-
gual Summarization and Question Answering (COL-
ING).
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita,
Yoshitaka Hirano, Hiroshi Matsuda, and Tomoaki
Imamura. 1999. Japanese Morphological analy-
sis System ChaSen Manual. Naist Technical Report
NAIST-IS-TR99009, Nara Institute of Science and
Technology. (in Japanese).
Jin Mingzhe. 2002. Authorship Attribution Based on
N-gram Models in Postpositional Particle of Japanese.
Mathematical Linguistic, 23(5):225?240, June.
Jian Pei, Jiawei Han, et al 2001. Prefixspan: Mining
Sequential Patterns by Prefix-Projected Growth. In
Proceedings of International Conference of Data En-
gineering, pages 215?224.
S. Siegel and NJ. Castellan, Jr. 1988. Nonparamet-
ric Statistics for the Behavioral Sciences 2nd Edition.
McGraw-Hill, New York.
Hirotoshi Taira and Masahiko Haruno. 2000. Feature
Selection in SVM Text Categorization. IPSJ Journal,
41(4):1113?1123, April. (in Japanese).
Yuta Tsuboi and Yuji Matsumoto. 2002. Authorship
Identification for Heterogeneous Documents. In IPSJ
SIG Notes, NL-148-3, pages 17?24.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer, New York.
Ellen M. Voorhees. 2001. Overview of the TREC
2001Question Answering Track. In Proceedings of the
2001 Text Retrieval Conference (TREC 2001).
Yiming Yang and Jan O. Pedersen. 1997. A Compara-
tive Study on Feature Selection in Text Categorization.
In Proceedings of ICML-97 14th International Confer-
ence on Machine Learning, pages 412?420.
Paraphrasing Japanese noun phrases using character-based indexing
Tokunaga Takenobu Tanaka Hozumi
Department of Computer Science, Tokyo Institute of Technology
Tokyo Meguro ?Ookayama 2-12-1, 152-8552 Japan
take@cl.cs.titech.ac.jp
Kimura Kenji
Abstract
This paper proposes a novel method to extract
paraphrases of Japanese noun phrases from a
set of documents. The proposed method con-
sists of three steps: (1) retrieving passages us-
ing character-based index terms given a noun
phrase as an input query, (2) filtering the re-
trieved passages with syntactic and seman-
tic constraints, and (3) ranking the passages
and reformatting them into grammatical forms.
Experiments were conducted to evaluate the
method by using 53 noun phrases and three
years worth of newspaper articles. The ac-
curacy of the method needs to be further im-
proved for fully automatic paraphrasing but the
proposed method can extract novel paraphrases
which past approaches could not.
1 Introduction
We can use various linguistic expressions to denote a con-
cept by virtue of richness of natural language. However
this richness becomes a crucial obstacle when processing
natural language by computer. For example, mismatches
of index terms cause failure of retrieving relevant docu-
ments in information retrieval systems, in which docu-
ments are retrieved on the basis of surface string match-
ing. To remedy this problem, the current information re-
trieval system adopts query expansion techniques which
replace a query term with a set of its synonyms (Baeza-
Yates and Riberto-Neto, 1999). The query expansion
works well for single-word index terms, but more sophis-
ticated techniques are necessary for larger index units,
such as phrases. The effectiveness of phrasal indexing
has recently drawn researchers? attention (Lewis, 1992;
Mitra et al, 1997; Tokunaga et al, 2002). However,
query expansion of phrasal index terms has not been fully
investigated yet (Jacquemin et al, 1997).
To deal with variations of linguistic expressions, para-
phrasing has recently been studied for various applica-
tions of natural language processing, such as machine
translation (Mitamura, 2001; Shimohata and Sumita,
2002), dialog systems (Ebert et al, 2001), QA sys-
tems (Katz, 1997) and information extraction (Shinyama
et al, 2002). Paraphrasing is defined as a process of
transforming an expression into another while keeping its
meaning intact. However, it is difficult to define what
?keeping its meaning intact? means, although it is the
core of the definition. On what basis could we consider
different linguistic expressions denoting the same mean-
ing? This becomes a crucial question when finding para-
phrases automatically.
In past research, various types of clues have been used
to find paraphrases. For example, Shinyama et al tried
to find paraphrases assuming that two sentences sharing
many Named Entities and a similar structure are likely
to be paraphrases of each other (Shinyama et al, 2002).
Barzilay and McKeown assume that two translations
from the same original text contain paraphrases (Barzi-
lay and McKeown, 2001). Torisawa used subcategoriza-
tion information of verbs to paraphrase Japanese noun
phrase construction ?NP1 no NP2? into a noun phrase
with a relative clause (Torisawa, 2001). Most of previ-
ous work on paraphrasing took corpus-based approach
with notable exceptions of Jacquemin (Jacquemin et al,
1997; Jacquemin, 1999) and Katz (Katz, 1997). In par-
ticular, text alignment technique is generally used to find
sentence level paraphrases (Shimohata and Sumita, 2002;
Barzilay and Lee, 2002).
In this paper, we follow the corpus-based approach
and propose a method to find paraphrases of a Japanese
noun phrase in a large corpus using information retrieval
techniques. The significant feature of our method is
use of character-based indexing. Japanese uses four
types of writing; Kanzi (Chinese characters), Hiragana,
Katakana, and Roman alphabet. Among these, Hiragana
and Katakana are phonographic, and Kanzi is an ideo-
graphic writing. Each Kanzi character itself has a certain
meaning and provides a basis for rich word formation
ability for Japanese. We use Kanzi characters as index
terms to retrieve paraphrase candidates, assuming that
noun phrases sharing the same Kanzi characters could be
paraphrases of each other. For example, character-based
indexing enables us to retrieve a paraphrase ??????
? (a commuting child)? for ???????? (a child
going to school)?. Note that their head is the same, ??
? (child)?, and their modifiers are different but sharing
common characters ?? (commute)? and ?? (study)?. As
shown in this example, the paraphrases generated based
on Japanese word formation rule cannot be classified in
terms of the past paraphrase classification (Jacquemin et
al., 1997).
The proposed method is summarized as follows. Given
a Japanese noun phrase as input, the method finds its
paraphrases in a set of documents. In this paper, we used
a collection of newspaper articles as a set of documents,
from which paraphrases are retrieved. The process is de-
composed into following three steps:
1. retrieving paraphrase candidates,
2. filtering the retrieved candidates based on syntactic
and semantic constraints, and
3. ranking the resulting candidates.
Newspaper articles are segmented into passages at punc-
tuation symbols, then the passages are indexed based on
Kanzi characters and stored in a database. The database
is searched with a query, an input noun phrase, to obtain a
set of passages, which are paraphrase candidates. In gen-
eral, using smaller index units, such as characters, results
in gains in recall at the cost of precision. To remedy this,
we introduce a filtering step after retrieving paraphrase
candidates. Filtering is performed based on syntactic and
semantic constraints. The resulting candidates are ranked
and provided as paraphrases.
The following three sections 2, 3 and 4 describe each
of three steps in detail. Section 5 describes experiments
to evaluate the proposed method. Finally, section 6 con-
cludes the paper and looks at the future work.
2 Retrieving paraphrase candidates
2.1 Indexing and term expansion
In conventional information retrieval, a query is given to
the system to retrieve a list of documents which are ar-
ranged in descending order of relevance. Our aim is to
obtain paraphrases given a noun phrase as a query, where
retrieved objects should be smaller than documents. We
divide a document into a set of passages at punctuation
symbols. These passages are retrieved by a given query,
a noun phrase.
The input noun phrase and the passages are segmented
into words and they are assigned part of speech tags by
a morphological analyzer. Among these tagged words,
content words (nouns, verbs, adjectives, adverbs) and un-
known words are selected. Kanzi characters contained
in these words are extracted as index terms. In addi-
tion to Kanzi characters, words written in Katakana (most
of them are imported words) and numbers are also used
as index terms. Precisely speaking, different numbers
should be considered to denote different meaning, but to
avoid data sparseness problem, we abstract numbers into
a special symbol ?num?.
As mentioned in section 1, the query expansion tech-
nique is often used in information retrieval to solve the
surface notational difference between queries and docu-
ments. We also introduce query expansion for retrieving
passage. Since we use Kanzi characters as index terms,
we need linguistic knowledge defining groups of simi-
lar characters for query expansion. However this kind of
knowledge is not available at hand. We obtain similar-
ity of Kanzi characters from an ordinary thesaurus which
defines similarity of words.
If a word t is not a Katakana word, we expand it to
a set of Kanzi characters E(t) which is defined by (1),
where Ct is a semantic class including the word t, KC is
a set of Kanzi characters used in words of semantic class
C, fr(k,C) is a frequency of a Kanzi character k used
in words of semantic class C, and Kt is a set of Kanzi
characters in word t.
E(t) = {k|k ? KCt , k? = argmaxl?Kt fr(l, Ct),
fr(k,Ct) > fr(k?, Ct)} ?Kt?
{s|s ? Ct, s is a Katakana word}
(1)
E(t) consists of Kanzi characters which is used in words
of semantic class Ct more frequently, than the most fre-
quent Kanzi character in the word t. If the word t is a
Katakana word, it is not expanded.
Let us see an expansion example of word ??? (hot
spring)?. Here we have t = ???? to expand, and we
have two characters that make the word, i.e. Kt = {
?, ? }. Suppose ???? belongs to a semantic class
Ct in which we find a set of words {??? (hot sprint
place), ???? (lukewarm water), ?? (warm water),
?? (spa), ???? (oasis), . . . }. From this word set,
we extract characters and count their occurence to obtain
KCt = { ? (35), ? (22), ? (20), ? (8),. . . }, where
a number in parentheses denotes the frequency of char-
acters in the semantic class Ct. Since the most frequent
character of Kt in KCt is ??? in this case, more fre-
quently used character ??? is added to E(t). In addi-
tion, Katakana words ???? and ?????? are added
to E(t) as well.
2.2 Term weighting
An index term is usually assigned a certain weight ac-
cording to its importance in user?s query and documents.
There are many proposals of term weighting most of
which are based on term frequency (Baeza-Yates and
Riberto-Neto, 1999) in a query and documents. Term
frequency-based weighting resides on Luhn?s assump-
tion (Luhn, 1957) that a repeatedly mentioned expression
denotes an important concepts. However it is obvious that
this assumption does not hold when retrieving paraphrase
candidates from a set of documents. For term weighting,
we use character frequency in a semantic class rather than
that in a query and documents, assuming that a character
frequently used in words of a semantic class represents
the concept of that semantic class very well.
A weight of a term k in a word t is calculated by (2).
w(k) =
?
???????
???????
100
if k is Katakana word or ?num?
100? log fr(k,Ct)?
k?inE(t)
log fr(k?, Ct)
if k is a Kanzi
(2)
Katakana words and numbers are assigned a constant
value, 100, and a Kanzi character is assigned a weight ac-
cording to its frequency in the semantic class Ct, where
k is used in the word t.
In the previous example of ????, we have obtained
an expanded term set { ?, ?, ?, ??, ???? }.
Among this set, ???? and ?????? are assigned
weight 100 because these are Katakana words, and the
rest three characters are assigned weight according to its
frequency in the class. For example, ??? is assigned
weight 100? log 35log 35+log 22+log 8 = 40.7.
2.3 Similarity
Similarity between an input noun phrase (I) and a pas-
sage (D) is calculated by summing up the weights of
terms which are shared by I and D, as defined in (3). In
the equation, k takes values over the index terms shared
by I and D, w(k) is its weight calculated as described in
the previous section.
sim(I,D) =
?
k?I?k?D
w(k) (3)
Note that since we do not use term frequency in passages,
we do not introduce normalization of passage length.
3 Syntactic and semantic filtering
The proposed method utilizes Kanzi characters as index
terms. In general, making index terms smaller units in-
creases exhaustivity to gain recall, but, at the same time, it
decreases specificity to degrade precision (Sparck Jones,
1972). We aim to gain recall by using smaller units as in-
dex terms at the cost of precision. Even though Kanzi are
ideograms and have more specificity than phonograms,
they are still less specific than words. Therefore there
would be many irrelevant passages retrieved due to coin-
cidentally shared characters. In this section, we describe
a process to filter out irrelevant passages based on the fol-
lowing two viewpoints.
Semantic constraints : Retrieved passages should con-
tain all concepts mentioned in the input noun phrase.
Syntactic constraints : Retrieved passages should have
a syntactically proper structure corresponding to the
input noun phrase.
3.1 Semantic constraints
In the indexing phase, we have decomposed an input
noun phrase and passages into a set of Kanzi characters
for retrieval. In the filtering phase, from these charac-
ters, we reconstruct words denoting a concept and verify
if concepts mentioned in the input noun phrase are also
included in the retrieved passages.
To achieve this, a retrieved passage is syntactically an-
alyzed and dependencies between bunsetu (word phrase)
are identified. Then, the correspondence between words
of the input noun phrase and bunsetu of the passage is
verified. This matching is done on the basis of sharing
the same Kanzi characters or the same Katakana words.
Passages missing any of the concepts mentioned in the
input noun phrase are discarded in this phase.
3.2 Syntactic constraints
Since passages are generated on the basis of punctuation
symbols, each passage is not guaranteed to have a syntac-
tically proper structure. In addition, a part of the passage
tends to be a paraphrase of the input noun phrase rather
than the whole passage. In such cases, it is necessary to
extract a corresponding part from the retrieved passage
and transform it into a proper syntactic structure.
By applying semantic constraints above, we have iden-
tified a set of bunsetu covering the concepts mentioned
in the input noun phrase. We extract a minimum depen-
dency structure which covers all the identified bunsetu.
Finally the extracted structure is transformed into a
proper phrase or clause by changing the ending of the
head (the right most element) and deleting unnecessary
elements such as punctuation symbols, particles and so
on.
Figure 1 illustrates the matching and transforming pro-
cess described in this section. The input noun phrase
is ??? w1 ?? w2 ? w3 ???? w4 (reduction of
telephone rate)? which consists of four words w1 . . . w4.
Suppose a passage ?????????????????
(the company?s telephone rate reduction caused. . . ? is re-
trieved. This passage is syntactically analyzed to give the
dependency structure of four bunsetu b1 . . . b4 as shown
in Figure 1.
Input NP ?? ?? ? ????
 (telephone) (charge) (of) (reduction)w1 w2 w3 w4
Retrieved ??? ????? ????? ???
passage (the company's) (telephone charge) (reduction) (caused)b1 b2 b3 b4
??????????
??????????
Extract proper structure
Transform ending
Figure 1: An example of matching and transformation
Correspondence between word w1 and bunsetu b2 is
made bacause they share a common character ???. Word
w2 corresponds to bunsetu b2 as well due to characters ?
?? and ???. And word w4 corresponds to bunsetu b3.
Although there is no counterpart of word w3, this pas-
sage is not discarded because word w3 is a function word
(postposition). After making correspondences, a mini-
mum dependency structure, the shaded part in Figure 1,
is extracted. Then the ending auxiliary verb is deleted
and the verb is restored to the base form.
4 Ranking
Retrieved passages are ranked according to the similarity
with an input noun phrase as described in section 2. How-
ever this ranking is not always suitable from the view-
point of paraphrasing. Some of the retrieved passages are
discarded and others are transformed through processes
described in the previous section. In this section, we de-
scribe a process to rerank remaining passages according
to their appropriateness as paraphrases of the input noun
phrase. We take into account the following three factors
for reranking.
? Similarity score of passage retrieval
? Distance between words
? Contextual information
The following subsections describe each of these factors.
4.1 Similarity score of retrieval
The similarity score used in passage retrieval is not suffi-
cient for evaluating the quality of the paraphrases. How-
ever, it reflects relatedness between the input noun phrase
and retrieved passages. Therefore, the similarity score
calculated by (3) is taken into account when ranking para-
phrase candidates.
4.2 Distance between words
In general, distance between words which have a de-
pendency relation reflects the strength of their semantic
closeness. We take into account the distance between two
bunsetu which have a dependency relation and contain
adjacent two words in the input noun phrase respectively.
This factor is formalized as in (4), where ti is the ith word
in the input noun phrase, and dist(s, t) is the distance be-
tween two bunsetu each of which contains s and t. A
distance between two bunsetu is defined as the number of
bunsetu between them. When two words are contained in
the same bunsetu, the distance between them is defined
as 0.
Mdistance = 11 +
?
i
dist(ti, ti+1)
(4)
4.3 Contextual information
We assume that phrases sharing the same Kanzi char-
acters likely represent the same meaning. Therefore
they could be paraphrases of each other. However, even
though a Kanzi denotes a certain meaning, its meaning is
often ambiguous. This problem is similar to word sense
ambiguities, which have been studied for many years. To
solve this problem, we adopt an idea one sense per collo-
cation which was introduced in word sense disambigua-
tion research (Yarowsky, 1995). Considering a newspa-
per article in which the retrieved passage and the input
noun phrase is included as the context, the context sim-
ilarity is taken into account for ranking paraphrase can-
didates. More concretely, context similarity is calculated
by following procedure.
1. For each paraphrase candidate, a context vector is
constructed from the newspaper article containing
the passage from which the candidate is derived.
The article is morphologically analyzed and content
words are extracted to make the context vector. The
tf ? idf metric is used for term weighting.
2. Since the input is given in terms of a noun phrase,
there is no corresponding newspaper article for the
input. However there is a case where the retrieved
passages include the input noun phrase. Such pas-
sages are not useful for finding paraphrases, but use-
ful for constructing a context vector of the input
noun phrase. The context vector of the input noun
phrase is constructed in the same manner as that of
paraphrase candidates, except that all newspaper ar-
ticles including the noun phrase are used.
3. Context similarity Mcontext is calculated by cosine
measure of two context vectors as in (5), where
wi(k) and wd(k) are the weight of the k-th term of
the input context vector and the candidate context
vector, respectively.
Mcontext =
?
k wi(k)wd(k)??
k w2i (k)
??
k w2d(k)
(5)
4.4 Ranking paraphrase candidates
Paraphrase candidates are ranked in descending order of
the product of three measures, sim(I,D) (equation (3)),
Mdistance (equation (4)) and Mcontext (equation (5)).
5 Experiments
5.1 Data and preprocessing
As input noun phrases, we used 53 queries excerpted
from Japanese IR test collection BMIR-J21 (Kitani et al,
1998) based on the following criteria.
? A query has two or more index terms.
It is less likely to retrieve proper paraphrases with
only one index term, since we adopt character-based
indexing.
? A query does not contain proper names.
It is generally difficult to paraphrase proper names.
We do not deal with proper name paraphrasing.
? A query contains at most one Katakana word or
number.
The proposed method utilize characteristics of Kanzi
characters, ideograms. It is obvious that the method
does not work well for Kanzi -poor expressions.
We searched paraphrases in three years worth of news-
paper articles (Mainichi Shimbun) from 1991 to 1993. As
described in section 2, each article is segmented into pas-
sages at punctuation marks and symbols. These passages
are assigned a unique identifier and indexed, then stored
in the GETA retrieval engine (IPA, 2003). We used the
JUMAN morphological analyzer (Kurohashi and Nagao,
1998) for indexing the passages. As a result of prepro-
cessing described above, we obtained 6,589,537 passages
to retrieve. The average number of indexes of a passage
was 12.
5.2 Qualitative evaluation
Out of 53 input noun phrases, no paraphrase was obtained
for 7 cases. Output paraphrases could be classified into
the following categories.
1BMIR-2 contains 60 queries.
(1) The paraphrase has the same meaning as that of the
input noun phrase.
e.g. ????? (damage by cool summer) ???
(cool summer damage)2
Note that this example is hardly obtained by the ex-
isting approaches such as syntactic transformation
and word substitution with thesaurus.
(2) The paraphrase does not have exactly the same
meaning but has related meaning. This category is
further divided into three subcategories.
(2-a) The meaning of the paraphrase is more specific
than that of the input noun phrase.
e.g. ?? (agricultural chemicals)?????
?? (insecticide and herbicide)
(2-b) The meaning of the paraphrase is more general
than that of the input noun phrase.
e.g. ???? (stock movement)??????
????? (movement of stock and exchange
rate)
(2-c) The paraphrase has related meaning to the in-
put but is not categorized into above two.
e.g. ??? (drinks) ???????? (inter-
national drink exhibition)
(3) There is no relation between the paraphrase and the
input noun phrase.
Among these categories, (1) and (2-a) are useful from
a viewpoint of information retrieval. By adding the para-
phrase of these classes to a query, we can expect the ef-
fective phrase expansion in queries.
Since the paraphrase of (2-b) generalizes the concept
denoted by the input, using these paraphrases for query
expansion might degrade precision of the retrieval. How-
ever, they might be useful for the recall-oriented retrieval.
The paraphrases of (2-c) have the similar property, since
relatedness includes various viewpoints.
The main reason of retrieval failure and irrelevant re-
trieval (3) are summarized as follows:
? The system cannot generate a paraphrase, when
there is no proper paraphrase for the input. In partic-
ular, this tends to be the case for single-word inputs,
such as ??? (liquid crystal)? and ??? (movie)?.
But this does not imply the proposed method does
not work well for single-words inputs. We had sev-
eral interesting paraphrases for single-word inputs,
such as ??????? (chemicals for agriculture
and gardening)? for ??? (agricultural chemicals)?.
? We used only three years worth of newspaper ar-
ticles due to the limitation of computational re-
soruces. Sometimes, the system could not generate
2The left-hand side of the arrow is the input and the right-
hand side is its paraphrase.
the paraphrase of the input because of the limited
size of the corpus.
5.3 Quantitative evaluation
Since there is no test collection available to evaluate para-
phrasing, we asked three judges to evaluate the output of
the system subjectively. The judges classified the outputs
into the categories introduced in 5.2. The evaluation was
done on the 46 inputs which gave at least one output.
Table 1 shows the results of judgments. Column ?Q?
denotes the query identifier, ?Len.? denotes its length in
morphemes, ?#Para.? denotes the number of outputs and
the columns (1) through (3) denote the number of outputs
which are classified into each category by three judges.
Therefore, the sum of these columns makes a triple of the
number of outputs. The decimal numbers in the paren-
theses denote the generalized raw agreement indices of
each category, which are calculated as given in (6) (Ue-
bersax, 2001), where K is the number of judged cases, C
is the number of categories, njk is the number of times
category j is applied to case k, and nk is calculated by
summing up over categories on case k; nk =
?C
j=1 njk.
ps(j) =
?K
k=1 njk(njk ? 1)?K
k=1 nk ? 1
(6)
In our case, K is the number of outputs (column
?#Para.?), nk is the number of judges, 3, and j moves
over (1) through (3).
As discussed in 5.2, from the viewpoint of information
retrieval, paraphrases of category (1) and (2-a) are use-
ful for query expansion of phrasal index terms. Column
?Acc.? denotes the ratio of paraphrases of category (1)
and (2-a) to the total outputs. Column ?Prec.? denotes
non-interpolated average precision. Since the precision
differs depending on the judge, the column is showing
the average of the precisions given by three judges.
We could obtain 45 paraphrases on average for each
input. But the average accuracy is quite low, 10%, which
means only one tenth of output is useful. Even though
considering that all paraphrases not being in category (3)
are useful, the accuracy only doubled. This means filter-
ing conditions should be more rigid. However, looking
at the agreement indices, we see that category (3) ranks
very high. Therefore, we expect finding the paraphrases
in category (3) is easy for a human. From all this, we
conclude that the proposed method need to be improved
in accuracy to be used for automatic query expansion in
information retrieval, but it is usable to help users to mod-
ify their queries by suggesting possible paraphrases.
Seeing the column ?Len.?, we find that the proposed
method does not work for complex noun phrases. The
average length of input noun phrase is 4.5 morphemes.
The longer input often results in less useful paraphrases.
The number of outputs also decreases for longer inputs.
We require all concepts mentioned in the input to have
their counterparts in its paraphrases as described in 3.1.
This condition seems to be strict for longer inputs. In
addition, we need to take into account syntactic variations
of longer inputs. Integrating syntactic transformation into
the proposed method is one of the possible extensions to
explore when dealing with longer inputs (Yoshikane et
al., 2002).
6 Conclusions and future work
This paper proposed a novel approach to extract para-
phrases of a Japanese noun phrase from a corpus. The
proposed method adopts both information retrieval tech-
niques and natural language processing techniques. Un-
like past research, the proposed method uses Kanzi
(ideograms) characters as index terms and retrieves para-
phrase candidates in a set of passages. The retrieved can-
didates are then filtered out based on syntactic and se-
mantic constraints.
The method was evaluated by a test set of 53 noun
phrases, and paraphrases were extracted for 46 cases.
These paraphrases were evaluated subjectively by three
independent judges. The quantitative evaluation suggests
that the performance needs to be further improved for
fully automatic query expansion in information retrieval,
but is usable to help users modify their queries by sug-
gesting possible paraphrases.
From a qualitative point of view, the proposed method
could extract paraphrases which cannot be obtained by
previous approaches such as syntactic transformation
and word substitution. Considering characteristics of
Japanese word formation by using character-based index-
ing enables us to obtain novel paraphrases.
The performance of the current system needs to be im-
proved for fully automatic paraphrasing. One direction
is introducing more precise filtering criteria. The cur-
rent system adopts only dependency analysis of bunsetu.
We need case analysis as well, to capture relations among
the bunsetu. Integrating syntactic transformation into the
proposed method is another research direction to explore.
In this paper, we evaluated output paraphrases subjec-
tively. Task oriented evaluation should be also conducted.
For example, effectiveness of phrase expansion in infor-
mation retrieval systems should be investigated.
Q Len. #Para. (1) (2-a) (2-b) (2-c) (3) Acc. Prec.
3 1 17 0 (0.00) 7 (0.86) 0 (0.00) 15 (0.60) 29 (0.83) 0.14 0.33
4 1 60 1 (0.00) 61 (0.74) 2 (0.50) 38 (0.47) 78 (0.69) 0.34 0.33
5 1 68 4 (0.75) 8 (0.62) 16 (0.00) 56 (0.14) 120 (0.62) 0.06 0.13
6 1 81 0 (0.00) 0 (0.00) 3 (0.33) 2 (0.00) 238 (0.99) 0.00 0.00
7 2 61 5 (0.60) 20 (0.70) 44 (0.45) 58 (0.66) 56 (0.73) 0.14 0.24
8 1 93 3 (0.00) 22 (0.68) 11 (0.64) 24 (0.42) 218 (0.91) 0.09 0.21
9 2 64 4 (0.75) 6 (0.67) 2 (0.50) 3 (0.33) 177 (0.99) 0.05 0.07
10 3 68 24 (0.42) 37 (0.22) 14 (0.50) 83 (0.41) 45 (0.29) 0.30 0.29
11 2 68 0 (0.00) 12 (0.08) 9 (0.44) 20 (0.25) 163 (0.83) 0.06 0.08
12 2 53 7 (0.14) 54 (0.76) 1 (0.00) 60 (0.37) 37 (0.19) 0.38 0.38
13 2 89 22 (0.32) 23 (0.30) 3 (1.00) 9 (0.11) 210 (0.98) 0.17 0.24
14 3 62 13 (0.85) 0 (0.00) 16 (0.44) 8 (0.12) 149 (0.92) 0.07 0.06
15 3 77 41 (0.49) 18 (0.44) 7 (0.57) 32 (0.38) 133 (0.89) 0.26 0.29
18 2 76 13 (0.08) 18 (0.28) 9 (0.56) 55 (0.42) 133 (0.80) 0.14 0.21
20 3 51 11 (0.82) 19 (0.95) 14 (0.71) 29 (0.62) 80 (0.82) 0.20 0.20
21 2 50 0 (0.00) 4 (0.75) 3 (0.33) 0 (0.00) 143 (0.98) 0.03 0.04
22 3 70 18 (0.72) 7 (0.00) 2 (0.50) 14 (0.36) 169 (0.94) 0.12 0.16
24 3 64 8 (0.88) 1 (0.00) 3 (1.00) 1 (0.00) 179 (0.99) 0.05 0.04
26 4 58 2 (0.50) 22 (0.18) 1 (0.00) 22 (0.27) 127 (0.78) 0.14 0.13
27 6 13 1 (0.00) 7 (0.00) 0 (0.00) 0 (0.00) 31 (0.77) 0.21 0.30
28 4 56 20 (0.25) 8 (0.38) 3 (0.33) 53 (0.30) 83 (0.54) 0.17 0.22
29 6 34 0 (0.00) 3 (1.00) 0 (0.00) 1 (0.00) 97 (0.98) 0.03 0.25
30 4 16 0 (0.00) 12 (0.33) 1 (0.00) 7 (0.14) 28 (0.64) 0.25 0.27
31 6 4 0 (0.00) 0 (0.00) 0 (0.00) 0 (0.00) 12 (1.00) 0.00 0.00
32 4 60 15 (0.80) 19 (0.58) 4 (0.00) 31 (0.39) 111 (0.84) 0.19 0.24
33 4 67 15 (0.60) 58 (0.83) 2 (0.50) 20 (0.65) 105 (0.94) 0.36 0.51
34 4 54 1 (0.00) 12 (0.67) 0 (0.00) 7 (0.57) 142 (0.99) 0.08 0.19
36 7 13 0 (0.00) 1 (0.00) 0 (0.00) 1 (0.00) 37 (0.97) 0.03 0.06
37 5 7 1 (0.00) 1 (0.00) 0 (0.00) 1 (0.00) 18 (0.89) 0.10 0.22
38 5 64 2 (0.50) 1 (0.00) 6 (1.00) 8 (0.38) 175 (0.97) 0.02 1.00
39 4 59 2 (0.50) 4 (0.00) 0 (0.00) 9 (0.56) 162 (0.97) 0.03 0.04
40 4 54 0 (0.00) 11 (0.55) 30 (0.10) 2 (0.50) 119 (0.76) 0.07 0.09
41 5 51 0 (0.00) 4 (0.50) 4 (0.00) 2 (0.00) 143 (0.97) 0.03 0.07
43 5 65 1 (0.00) 1 (0.00) 4 (0.00) 5 (0.20) 184 (0.95) 0.01 0.01
44 7 54 3 (1.00) 0 (0.00) 34 (0.35) 3 (0.00) 122 (0.81) 0.02 0.03
45 6 7 0 (0.00) 0 (0.00) 0 (0.00) 0 (0.00) 21 (1.00) 0.00 0.00
46 7 1 0 (0.00) 0 (0.00) 0 (0.00) 0 (0.00) 3 (1.00) 0.00 0.00
47 9 5 0 (0.00) 0 (0.00) 0 (0.00) 1 (0.00) 14 (0.93) 0.00 0.00
48 7 10 1 (0.00) 1 (0.00) 3 (0.00) 3 (0.00) 22 (0.86) 0.07 0.21
49 8 1 0 (0.00) 0 (0.00) 0 (0.00) 0 (0.00) 3 (1.00) 0.00 0.00
50 8 58 1 (0.00) 1 (0.00) 2 (0.00) 3 (0.00) 167 (0.97) 0.01 0.06
51 6 18 1 (0.00) 13 (0.92) 1 (0.00) 9 (0.78) 30 (1.00) 0.26 0.33
52 7 21 4 (0.00) 1 (0.00) 1 (0.00) 1 (0.00) 56 (0.95) 0.08 0.13
55 7 26 2 (0.00) 1 (0.00) 0 (0.00) 4 (0.00) 71 (0.96) 0.04 0.03
59 10 21 0 (0.00) 0 (0.00) 0 (0.00) 4 (0.50) 59 (0.97) 0.00 0.00
60 12 2 0 (0.00) 0 (0.00) 0 (0.00) 0 (0.00) 6 (1.00) 0.00 0.00
Ave. 4.5 45 5.35 (0.24) 10.8 (0.30) 5.54 (0.23) 15.3 (0.24) 97.9 (0.87) 0.10 0.17
Table 1: Summary of judgment
References
R. Baeza-Yates and B. Riberto-Neto. 1999. Modern In-
formation Retrieval. Addison Wesley.
R. Barzilay and L. Lee. 2002. Bootstrapping lexical
choice via multiple-sequence alignment. In Proceed-
ings of 2002 Conference on Empirical Methods in Nat-
ural Language Processing, pages 164?171.
R. Barzilay and K. R. McKeown. 2001. Extracting para-
phrases from a parallel corpus. In Proceedings of 39th
Annual Meeting of the Association for Computational
Linguistics, pages 50?57.
C. Ebert, L. Shalom, G. Howard, and N. Nicolas. 2001.
Generating full paraphrases of fragments in a dialogue
interpretation. In Proceedings of the 2nd SIGdial
Workshop on Discourse and Dialouge.
IPA. 2003. GETA: Generic Engine for Transposable As-
sociation. http://geta.ex.nii.ac.jp.
C. Jacquemin, J. L. Klavans, and E. Tzoukermann. 1997.
Expansion of multi-word terms for indexing and re-
trieval using morphology and syntax. In Proceedings
of 35th Annual Meeting of the Assosiation for Compu-
tational Linguistics.
C. Jacquemin. 1999. Syntagmatic and paradigmatic rep-
resentation of term variation. In Proceedings of 37th
Annual Meeting of the Assosiation for Computational
Linguistics, pages 341?348.
B. Katz. 1997. Annotating the world wide web using nat-
ural language. In Proceedings of ?Computer-assisted
information searching on Internet? (RIAO ?97), pages
136?155.
T. Kitani, Y. Ogawa, T. Ishikawa, H. Kimoto, I. Keshi,
J. Toyoura, T. Fukushima, K. Matsui, Y. Ueda,
T. Sakai, T. Tokunaga, H. Tsuruoka, H. Nakawatase,
and T. Agata. 1998. Lessons from BMIR-J2: A test
collection for Japanese IR systems. In Proceedings of
the Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 345?346.
S. Kurohashi and M. Nagao. 1998. Building a Japanese
parsed corpus while improving the parsing system. In
Proceedings of the 1st International Conference on
Language Resources and Evaluation, pages 719?724.
D. D. Lewis. 1992. An evaluation of phrasal and clus-
tered representations of a text categorization task. In
Proceedings of the Annual International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, pages 37?50.
H. P. Luhn. 1957. A statistical approach to mechanized
encoding and searching of literary information. IBM
Journal of Research and Development, 1(4):390?317.
T. Mitamura. 2001. Automatic rewriting for con-
trolled language translation. In The Sixth Nat-
ural Language Processing Pacific Rim Symposium
(NLPRS2001) Post-Conference Workshop, Automatic
Paraphrasing: Theories and Applications, pages ???
M. Mitra, C. Buckley, A. Singhal, and C. Cardie. 1997.
An analysis of statistical and syntactic phrases. In Pro-
ceedings of RIAO ?97, pages 200?214.
M. Shimohata and E. Sumita. 2002. Automatic para-
phrasing based on parallel corpus for normalization.
In Third International Conference on Language Re-
sources and Evaluation, pages 453?457.
Y. Shinyama, S. Sekine, K. Sudo, and R. Grishman.
2002. Automatic paraphrase acquisition from news ar-
ticles. In Proceedings of Human Language Technology
Conference (HLT2002), pages 40?46.
K. Sparck Jones. 1972. A statistical interpretation of
term specificity and its application in retrieval. Journal
of Documentation, 28(1):11?21.
T. Tokunaga, K. Kenji, H. Ogibayashi, and H. Tanaka.
2002. Selecting effective index terms using a decision
tree. Natural Language Engineering, 8(2-3):193?207.
K. Torisawa. 2001. A nearly unsupervised learning
method for automatic paraphrasing of japanese noun
phrase. In The Sixth Natural Language Processing Pa-
cific Rim Symposium (NLPRS2001) Post-Conference
Workshop, Automatic Paraphrasing: Theories and Ap-
plications, pages 63?72.
J. Uebersax. 2001. Statistical methods for rater agree-
ment. http://ourworld.compuserve.com/
homepages/jsuebersax/agree.htm.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proceedings
of 33rd Annual Meeting of the Assosiation for Compu-
tational Linguistics, pages 189?196.
F. Yoshikane, K. Tsuji, K. Kageura, , and C. Jacquemin.
2002. Detecting Japanese term variation in textual
corpus. In Proceedings of 4th International Work-
shop on Information Retrieval with Asian Languages
(IRAL?99), pages 164?171.
Proceedings of the Fourth International Natural Language Generation Conference, pages 73?80,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Group-based Generation of Referring Expressions
Funakoshi Kotaro ? Watanabe Satoru ?
Department of Computer Science, Tokyo Institute of Technology
Tokyo Meguro O?okayama 2-12-1, 152-8552, Japan
take@cl.cs.titech.ac.jp
Tokunaga Takenobu
Abstract
Past work of generating referring expres-
sions mainly utilized attributes of objects
and binary relations between objects in or-
der to distinguish the target object from
others. However, such an approach does
not work well when there is no distinc-
tive attribute among objects. To over-
come this limitation, this paper proposes
a novel generation method utilizing per-
ceptual groups of objects and n-ary re-
lations among them. The evaluation us-
ing 18 subjects showed that the proposed
method could effectively generate proper
referring expressions.
1 Introduction
In the last two decades, many researchers have
studied the generation of referring expressions to
enable computers to communicate with humans
about objects in the world.
In order to refer to an intended object (the tar-
get) among others (distractors), most past work
(Appelt, 1985; Dale and Haddock, 1991; Dale,
1992; Dale and Reiter, 1995; Heeman and Hirst,
1995; Horacek, 1997; Krahmer and Theune, 2002;
van Deemter, 2002; Krahmer et al, 2003) utilized
attributes of the target and binary relations be-
tween the target and distractors. Therefore, these
methods cannot generate proper referring expres-
sions in situations where there is no significant
surface difference between the target and distrac-
tors, and no binary relation is useful to distinguish
the target. Here, a proper referring expression
?Currently at Honda Research Institute Japan Co., Ltd.
?Currently at Hitachi, Ltd.
means a concise and natural linguistic expression
enabling hearers to identify the target.
For example, consider indicating object b to per-
son P in the situation of Figure 1. Note that la-
bels a, b and c are assigned for explanation to the
readers, and person P does not share these labels
with the speaker. Because object b is not distin-
guishable from objects a or c by means of their
appearance, one would try to use a binary relation
between object b and the table, i.e., ?a ball to the
right of the table?. However, ?to the right of? is
not a discriminatory relation, for objects a and c
are also located to the right of the table. Using a
and c as a reference object instead of the table does
not make sense, since a and c cannot be uniquely
identified because of the same reason that b cannot
be identified. Such situations have drawn less at-
tention (Stone, 2000), but can frequently occur in
some domains such as object arrangement (Tanaka
et al, 2004).
P
a
b
c
Table
Figure 1: An example of problematic situations
In the situation of Figure 1, the speaker can indi-
cate object b to person P with a simple expression
?the front ball?. In order to generate such an ex-
pression, one must be able to recognize the salient
perceptual group of the objects and use the n-ary
relative relations in the group.
73
To overcome the problem described above, Fu-
nakoshi et al (2004) proposed a method of gen-
erating Japanese referring expressions that utilizes
n-ary relations among members of a group. They,
however, dealt with the limited situations where
only homogeneous objects are randomly arranged
(see Figure 2). Thus, their method could han-
dle only spatial n-ary relation, and could not han-
dle attributes and binary relations between objects
which have been the main concern of the past re-
search.
In this paper, we extend the generation method
proposed by (Funakoshi et al, 2004) so as to han-
dle object attributes and binary relations between
objects as well. In what follows, Section 2 shows
an extension of the SOG representation that was
proposed in (Funakoshi et al, 2004). Our new
method will be described in Section 3 and eval-
uated in Section 4. Finally we conclude the paper
in Section 5.
2 SOG representation
Funakoshi et al (2004) proposed an intermedi-
ate representation between a referring expression
and the situation that is referred to by the expres-
sion. The intermediate representation represents
a course of narrowing down to the target as a se-
quence of groups from the group of all objects to
the singleton group of the target object. Thus it is
called SOG (Sequence Of Groups).
The following example shows an expression de-
scribing the target x in Figure 2 with the cor-
responding SOG representation below it. Since
Japanese is a head-final language, the order of
groups in the SOG representation can be retained
in the linguistic expression.
hidari oku ni aru
(1)
mittu no tama no uti no
(2)
itiban migi no tama
(3)
(the rightmost ball
(3)
among the three balls
(2)
at the back left
(1)
)
SOG:[{a, b, c, d, e, f, x}, {a, b, x}, {x}],
where {a, b, c, d, e, f, x} denotes all objects in
the situation, {a, b, x} denotes the three objects
at the back left, and {x} denotes the target.
2.1 Extended SOG
As mentioned above, (Funakoshi et al, 2004) sup-
posed the limited situations where only homoge-
neous objects are randomly arranged, and consid-
ered only spatial subsumption relations between
consecutive groups. Therefore, relations between
P
a
b
e
f
c d
x
Figure 2: An example from (Funakoshi et al,
2004)
groups are not explicitly denoted in the original
SOGs as shown below.
SOG: [G
0
, G
1
, . . . , G
n
]
G
i
: a group
In this paper, however, other types of relations
between groups are also considered. We propose
an extended SOG representation where types of
relations are explicitly denoted as shown below. In
the rest of this paper, we will refer to this extended
SOG representation by simply saying ?SOG?.
SOG: [G
0
R
0
G
1
R
1
. . . G
i
R
i
. . . G
n
]
G
i
: a group
R
i
: a relation between G
i
and G
i+1
2.2 Relations between groups
R
i
, a relation between groups G
i
and G
i+1
, de-
notes a shift of attention from G
i
to G
i+1
with
a certain focused feature. The feature can be an
attribute of objects or a relation between objects.
There are two types of relations between groups:
intra-group relation and inter-group relation.
Intra-group relation When R
i
is an intra-group
relation, G
i
subsumes G
i+1
, that is, G
i
? G
i+1
.
Intra-group relations are further classified into the
following subcategories according to the feature
used to narrow down G
i
to G
i+1
. We denote these
subcategories with the following symbols.
space
?? : spatial subsumption
type
?? : the object type
shape
?? : the shape of objects
color
?? : the color of objects
size
?? : the size of objects
With respect to this classification, (Funakoshi et
al., 2004) dealt with only the
space
?? relation.
74
Inter-group relation When R
i
is an inter-group
relation, G
i
and G
i+1
are mutually exclusive, that
is, G
i
? G
i+1
= ?. An inter-group relation is a
spatial relation and denoted by symbol ?.
Example R
i
can be one of
space
?? ,
type
??,
shape
?? ,
color
??,
size
?? and ?. We show a referring expres-
sion indicating object b1 and the corresponding
SOG in the situation of Figure 3. In the SOG,
{all} denotes the total set of objects in the situ-
ation. The indexed underlines denote correspon-
dence between SOG and linguistic expressions.
As shown in the figure, we allow objects being on
the other objects.
marui
(1)
futatu no tukue no uti no
(2)
hidari no
(3)
tukue no
(4)
ue no
(5)
tama
(6)
(the ball
(6)
on
(5)
the left
(3)
table
(4)
among the two
(2)
round
(1)
tables
(2)
)
SOG: [{all}
type
?? {t1, t2, t3}
shape
??
(1)
{t1, t2}
(2)
space
??
(3)
{t1}
(4)
?
(5)
{b1}
(6)
]
b2
b1
b5
t3
t2
p1
t1
b3
b4
blue
black
red
Figure 3: An example situation
3 Generation
Our generation algorithm proposed in this section
consists of four steps: perceptual grouping, SOG
generation, surface realization and scoring. In the
rest of this section, we describe these four steps by
using Figure 3 as an example.
3.1 Step 1: Perceptual grouping
Our algorithm starts with identifying groups of
objects that are naturally recognized by humans.
We adopt Tho?risson?s perceptual grouping algo-
rithm (Tho?risson, 1994) for this purpose. Per-
ceptual grouping is performed with objects in the
situation with respect to each of the following
features: type, shape, color, size, and proxim-
ity. Three special features, total, singleton, and
closure are respectively used to recognize the to-
tal set of objects, groups containing each single
object, and objects bounded in perceptually sig-
nificant regions (table tops in the domain of this
paper). These three features are handled not by
Tho`risson?s algorithm but by individual proce-
dures.
Type is the most dominant feature because hu-
mans rarely recognize objects of different types as
a group. Thus, first we group objects with respect
to types, and then group objects of the same type
with respect to other features (except for total).
Although we adopt Tho?risson?s grouping algo-
rithm, we use different grouping strategies from
the original. Tho?risson (1994) lists the following
three combinations of features as possible strate-
gies of perceptual grouping.
? shape and proximity
? color and proximity
? size and proximity
However, these strategies are inappropriate to gen-
erate referring expressions. For example, because
two blue balls b1 and b2 in Figure 3 are too
much distant from each other, Tho?risson?s algo-
rithm cannot recognize the group consisting of b1
and b2 with the original strategies. However, the
expression like ?the left blue ball? can naturally
refer to b1. When using such an expression, we
assume an implicit group consisting of b1 and b2.
Hence, we do not combine features but use them
separately.
The results of perceptual grouping of the situa-
tion in Figure 3 are shown below. Relation labels
are assigned to recognized groups with respect to
features used in perceptual grouping. We define
six labels: all, type, shape, color, size, and
space. Features singleton, proximity and closure
share the same label space. A group may have
several labels.
feature label recognized groups
total all {t1, t2, t3, p1, b1, b2, b3, b4, b5}
singleton space {t1}, {t2}, {t3}, {p1}, {b1}, {b2},
{b3}, {b4}, {b5}
type type {t1, t2, t3}, {p1}, {b1, b2, b3, b4, b5}
shape shape {t1, t2}, {t3}
color color {b1, b2}, {b3}, {b4, b5}
size size {b1, b3, b4}, {b2, b5}
proximity space {t2, t3}, {b1, b3, b4, b5}, {b3, b4, b5}
closure space {b1}, {b3, b4}
75
Target # target object
AllGroups # all generated groups
SOGList # list of generated SOGs
01:makeSOG()
02: SOG = []; # list of groups and symbols
03: All = getAll(); # total set
04: add(All, SOG); # add All to SOG
05: TypeList = getAllTypes(All);
# list of all object types
06: TargetType = getType(Target);
# type of the target
07: TargetSailency = saliency(TargetType);
# saliency of the target type
08: for each Type in TypeList do
# {Table, Plant, Ball}
09: if saliency(Type) ?
TargetSaliency then
# saliency: Table > Plant > Ball
10: Group = getTypeGroup(Type);
# get the type group of Type
11: extend(SOG, Group);
12: end if
13: end for
14:return
Figure 4: Function makeSOG
3.2 Step 2: SOG generation
The next step is generating SOGs. This is so-
called content planning in natural language gen-
eration. Figure 4, Figure 5 and Figure 6 show the
algorithm of making SOGs.
Three variables Target, AllGroups, and
SOGList defined in Figure 4 are global variables.
Target holds the target object which the refer-
ring expression refers to. AllGroups holds the
set of all groups recognized in Step 1. Given
Target and AllGroups, function makeSOG
enumerates possible SOGs in the depth-first man-
ner, and stores them in SOGList.
makeSOG (Figure 4) makeSOG starts with a list
(SOG) that contains the total set of objects in the
domain. It chooses groups of objects that are more
salient than or equal to the target object and calls
function extend for each of the groups.
extend (Figure 5) Given an SOG and a group
to be added to the SOG, function extend extends
the SOG with the group for each label attached to
the group. This extension is done by creating a
copy of the given SOG and adding to its end an
intra-group relation symbol defined in Section 2.2
corresponding to the given label and group. Fi-
nally it calls search with the copy.
search (Figure 6) This function takes an SOG
as its argument. According to the last group in
01:extend(SOG, Group)
02: Labels = getLabels(Group);
03: for each Label in Labels do
04: SOGcopy = copy(SOG);
05: add(
Label
??, SOGcopy);
06: add(Group, SOGcopy);
07: search(SOGcopy);
08: end for
09:return
Figure 5: Function extend
the SOG (LastGroup), it extends the SOG as
described below.
1. If LastGroup is a singleton of the target
object, append SOG to SOGList and return.
2. If LastGroup is a singleton of a non-target
object, find the groups that contain the target
object and satisfy the following three condi-
tions: (a), (b) and (c).
(a) All objects in the group locate in
the same direction from the object of
LastGroup (the reference). Possi-
ble directions are one of ?back?, ?back
right?, ?right?, ?front right?, ?front?,
?front left?, ?left?, ?left back? and ?on?.
The direction is determined on the basis
of coordinate values of the objects, and
is assigned to the group for the use of
surface realization.
(b) There is no same type object located be-
tween the group and the reference.
(c) The group is not a total set of a certain
type of object.
Then, for each of the groups, make a copy
of the SOG, and concatenate ??? and the
group to the copy, and call search recur-
sively with the new SOG.
3. If LastGroup contains the target object
together with other objects, let the inter-
section of LastGroup and each group in
AllGroups be NewG, and copy the label
from each group to NewG. If NewG contains
the target object, call function extend un-
less Checked contains NewG.
4. If LastGroup contains only non-target ob-
jects, call function extend for each group
(Group) in AllGroupswhich is subsumed
by LastGroup.
Figure 7 shows the SOGs generated to refer to
object b1 in Figure 3.
76
1. [{all}
type
?? {t1, t2, t3}
space
?? {t1} ?{b1}]
2. [{all}
type
?? {t1, t2, t3}
shape
?? {t1, t2}
space
?? {t1} ?{b1}]
3. [{all}
type
?? {b1, b2, b3, b4, b5}
space
?? {b1}]
4. [{all}
type
?? {b1, b2, b3, b4, b5}
color
?? {b1, b2}
space
?? {b1}]
5. [{all}
type
?? {b1, b2, b3, b4, b5}
color
?? {b1, b2}
size
?? {b1}]
6. [{all}
type
?? {b1, b2, b3, b4, b5}
size
?? {b1, b4, b3}
space
?? {b1}]
7. [{all}
type
?? {b1, b2, b3, b4, b5}
size
?? {b1, b4, b3}
color
?? {b1}]
8. [{all}
type
?? {b1, b2, b3, b4, b5}
space
?? {b1, b3, b4, b5}
space
?? {b1}]
9. [{all}
type
?? {b1, b2, b3, b4, b5}
space
?? {b1, b3, b4, b5}
color
?? {b1}]
10. [{all}
type
?? {b1, b2, b3, b4, b5}
space
?? {b1, b3, b4, b5}
size
?? {b1, b3, b4}
space
?? {b1}]
11. [{all}
type
?? {b1, b2, b3, b4, b5}
space
?? {b1, b3, b4, b5}
size
?? {b1, b3, b4}
color
?? {b1}]
Figure 7: Generated SOGs from the situation in Figure 3
01:search(SOG)
02: LastGroup = getLastElement(SOG);
# get the rightmost group in SOG
03: Card = getCardinality(LastGroup);
04: if Card == 1 then
05: if containsTarget(LastGroup) then
# check if LastGroup contains
# the target
06: add(SOG, SOGList);
07: else
08: GroupList =
searchTargetGroups(LastGroup);
# find groups containing the target
09: for each Group in GroupList do
10: SOGcopy = copy(SOG);
11: add(?, SOGcopy);
12: add(Group, SOGcopy);
13: search(SOGcopy);
14: end for
15: end if
16: elsif containsTarget(LastGroup) then
17: Checked = [ ];
18: for each Group in AllGroups do
19: NewG = Intersect(Group, LastGroup);
# make intersection
20: Labels = getLabels(Group);
21: setLabels(Labels, NewG);
# copy labels from Group to NewG
22: if containsTarget(NewG) &
!contains(Checked, NewG) then
23: add(NewG, Checked);
24: extend(SOG, Group);
25: end if
26: end for
27: else
28: for each Group of AllGroups do
29: if contains(LastGroup, Group) then
30: extend(SOG, Group);
31: end if
32: end for
33: end if
34:return
Figure 6: Function search
3.3 Step 3: Surface realization
A referring expression is generated by determin-
istically assigning a linguistic expression to each
element in an SOG according to Rule 1 and 2.
As Japanese is a head-final language, simple con-
catenation of element expressions makes a well-
formed noun phrase1. Rule 1 generates expres-
sions for groups and Rule 2 does for relations.
Each rule consists of several subrules which are
applied in this order.
[Rule 1]: Realization of groups
Rule 1.1 The total set ({all}) is not realized.
(Funakoshi et al, 2004) collected referring
expressions from human subjects through ex-
periments and found that humans rarely men-
tioned the total set. According to their obser-
vation, we do not realize the total set.
Rule 1.2 Realize the type name for a singleton.
Type is realized as a noun and only for a sin-
gleton because the type feature is used first to
narrow down the group, and the succeeding
groups consist of the same type objects until
reaching the singleton. When the singleton is
not the last element of SOG, particle ?no? is
added.
Rule 1.3 The total set of the same type objects is
not realized.
This is because the same reason as Rule 1.1.
Rule 1.4 The group followed by the relation
space
??
is realized as ?[cardinality] [type] no-uti
(among)?, e.g., ?futatu-no (two) tukue (desk)
no-uti (among)?. The group followed by
1Although different languages require different surface
realization rules, we presume perceptual grouping and SOG
generation (Step 1 and 2) are applicable to other languages as
well.
77
the relation ? is realized as ?[cardinality]
[type] no?.
When consecutive groups are connected by
other than spatial relations (
space
?? and ?),
they can be realized as a sequence of relations
ahead of the noun (type name). For example,
expression ?the red ball among big balls? can
be simplified to ?the big red ball?.
Rule 1.5 Other groups are not realized.
[Rule 2]: Realization of relations
Rule 2.1 Relation
type
?? is not realized.
See Rule 1.2.
Rule 2.2 Relations
shape
?? ,
color
?? and
size
?? are real-
ized as the expressions corresponding to their
attribute values. Spatial relations (
space
?? and
?) are realized as follows, where |G
i
| de-
notes the cardinality of G
i
.
Intra-group relation (G
i
space
?? G
i+1
)
If |G
i
| = 2 (i.e., |G
i+1
| = 1), based on the
geometric relations among objects, generate
one of four directional expressions ?{migi,
hidari, temae, oku} no ({right, left, front,
back})?.
If |G
i
| ? 3 and |G
i+1
| = 1, based on the
geometric relations among objects, generate
one of eight directional expressions ?itiban
{migi, hidari, temae, oku, migi temae, hi-
dari temae, migi oku, hidari oku} no ({right,
left, front, back, front right, front left, back
right, back left}-most)? if applicable. If none
of these expressions is applicable, generate
expression ?mannaka no (middle)? if appli-
cable. Otherwise, generate one of four ex-
pressions ?{hidari, migi, temae, oku} kara
j-banme no (j-th from {left, right, front,
back})?.
If |G
i+1
| ? 2, based on the geometric rela-
tions among objects, generate one of eight di-
rectional expressions ?{migi, hidari, temae,
oku, migi temae, hidari temae, migi oku, hi-
dari oku} no ({right, left, front, back, front
right, front left, back right, back left})?.
Inter-group relation (G
i
?G
i+1
)
|G
i
| = 1 should hold because of search
in Step 2. According to the direction as-
signed by search, generate one of nine ex-
pressions : ?{migi, hidari, temae, oku, migi
temae, hidari temaen, migi oku, hidari oku,
ue} no ({right, left, front, back, front right,
front left, back right, back left, on})?.
Figure 8 shows the expressions generated from
the first three SOGs shown in Figure 7. The num-
bers in the parentheses denote coindexes of frag-
ments between the SOGs and the realized expres-
sions.
3.4 Step 4: Scoring
We assign a score to each expression by taking into
account the relations used in the expression, and
the length of the expression.
First we assign a cost ranging over [0, 1] to each
relation in the given SOG. Costs of relations are
decided as below. These costs conform to the pri-
orities of features described in (Dale and Reiter,
1995).
type
?? : No cost (to be neglected)
shape
?? : 0.2
color
?? : 0.4
size
?? : big(est): 0.6, small(est): 0.8, middle: 1.0
space
?? ,? : Cost functions are defined according to the
potential functions proposed in (Tokunaga
et al, 2005). The cost for relation ?on? is
fixed to 0.
Then, the average cost of the relations is calcu-
lated to obtain the relation cost, C
rel
. The cost of
surface length (C
len
) is calculated by
C
len
=
length(expression)
max
i
length(expression
i
)
,
where the length of an expression is measured by
the number of characters.
Using these costs, the score of an expression is
calculated by
score =
1
?? C
rel
+ (1? ?)? C
len
.
? was set to 0.5 in the following experiments.
4 Evaluation
4.1 Experiments
We conducted two experiments to evaluate expres-
sions generated by the proposed method.
Both experiments used the same 18 subjects and
the same 20 object arrangements which were gen-
erated automatically. For each arrangement, all
factors (number of objects, positions of objects, at-
tributes of objects, and the target object) were ran-
domly decided in advance to conform to the fol-
lowing conditions: (1) the proposed method can
generate more than five expressions for the given
target and (2) more than two other objects exist
which are the same type as the target.
78
1. SOG: [{all}
type
?? {t1, t2, t3}
space
??
(1)
{t1}
(2)
?
(3)
{b1}
(4)
]
itiban hidari no
(1)
tukue no
(2)
ue no
(3)
tama
(4)
(the ball
(4)
on
(3)
the leftmost
(1)
table
(2)
)
2. SOG: [{all}
type
?? {t1, t2, t3}
shape
??
(1)
{t1, t2}
(2)
space
??
(3)
{t1}
(4)
?
(5)
{b1}
(6)
]
marui
(1)
futatu no tukue no uti
(2)
hidari no
(3)
tukue no
(4)
ue no
(5)
tama
(6)
(the ball
(6)
on
(5)
the left
(3)
table
(4)
among
(2)
the round
(1)
two tables
(2)
)
3. SOG: [{all}
type
?? {b1, b2, b3, b4, b5}
space
??
(1)
{b1}
(2)
]
itiban hidari no
(1)
tama
(2)
(the leftmost
(1)
ball
(2)
)
Figure 8: Realized expressions
?20?
20/20
????????
t1
b3
b1
b4
t2
p1b2
Figure 9: An example stimulus of Experiment 1
Experiment 1 Experiment 1 was designed to
evaluate the ability of expressions to identify the
targets. The subjects were presented an arrange-
ment with a generated referring expression which
gained the highest score at a time, and were in-
structed to choose the object referred to by the ex-
pression. Figure 9 is an example of visual stimuli
used in Experiment 1. Each subject responded to
all 20 arrangements.
Experiment 2 Experiment 2 was designed to
evaluate validity of the scoring function described
in Section 3.4. The subjects were presented an
arrangement with a marked target together with
the best five generated expressions referring to the
target at a time. Then the subjects were asked
to choose the best one from the five expressions.
Figure 10 is an example of visual stimuli used in
Experiment 2. Each subject responded to the all
20 arrangements. The expressions used in Experi-
ment 2 include those used in Experiment 1.
4.2 Results
Table 1 shows the results of Experiment 1. The
average accuracy of target identification is 95%.
Figure 10: An example stimulus of Experiment 2
This shows a good performance of the generation
algorithm proposed in this paper.
The expression generated for arrangement
No. 20 (shown in Figure 9) resulted in the excep-
tionally poor accuracy. To refer to object b1, our
algorithm generated expression ?itiban temae no
tama (the most front ball)? because b1 is the most
close object to person P in terms of the vertical
axis. Humans, however, chose the object that is the
closest to P in terms of Euclidean distance. Some
psychological investigation is necessary to build
a more precise geometrical calculation model to
solve this problem (Landragin et al, 2001).
Table 2 shows the results of Experiment 2. The
first row shows the rank of expressions based on
their score. The second row shows the count of hu-
man votes for the expression. The third row shows
the ratio of the votes. The top two expressions oc-
cupy 72% of the total. This concludes that our
scoring function works well.
5 Conclusion
This paper extended the SOG representation pro-
posed in (Funakoshi et al, 2004) to generate refer-
79
Table 1: Result of Experiment 1
Arrangement No. 1 2 3 4 5 6 7 8 9 10
Accuracy 0.89 1.0 1.0 1.0 1.0 1.0 1.0 0.94 1.0 1.0
11 12 13 14 15 16 17 18 19 20 Ave.
1.0 0.94 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.17 0.95
Table 2: Result of Experiment 2
Rank 1 2 3 4 5 Total
Vote 134 125 59 22 20 360
Share 0.37 0.35 0.16 0.06 0.06 1
ring expressions in more general situations.
The proposed method was implemented and
evaluated through two psychological experiments
using 18 subjects. The experiments showed that
generated expressions had enough discrimination
ability and that the scoring function conforms to
human preference well.
The proposed method would be able to handle
other attributes and relations as far as they can be
represented in terms of features as described in
section 3. Corresponding surface realization rules
might be added in that case.
In the implementation, we introduced rather ad
hoc parameters, particularly in the scoring func-
tion. Although this worked well in our experi-
ments, further psychological validation is indis-
pensable.
This paper assumed a fixed reference frame is
shared by all participants in a situation. How-
ever, when we apply our method to conversational
agent systems, e.g., (Tanaka et al, 2004), refer-
ence frames change dynamically and they must
be properly determined each time when generat-
ing referring expressions.
In this paper, we focused on two dimensional
situations. To apply our method to three dimen-
sional worlds, more investigation on human per-
ception of spatial relations are required. We ac-
knowledge that a simple application of the current
method does not work well enough in three dimen-
sional worlds.
References
Douglas E. Appelt. 1985. Planning English referring expres-
sions. Artificial Intelligence, 26:1?33.
Robert Dale and Nicholas Haddock. 1991. Generating re-
ferring expressions involving relations. In Proceedings of
the Fifth Conference of the European Chapter of the As-
sociation for Computational Linguistics(EACL?91), pages
161?166.
Robert Dale and Ehud Reiter. 1995. Computational interpre-
tations of the Gricean maxims in the generation of refer-
ring expressions. Cognitive Science, 19(2):233?263.
Robert Dale. 1992. Generating referring expressions: Con-
structing descriptions in a domain of objects and pro-
cesses. MIT Press, Cambridge.
Kotaro Funakoshi, Satoru Watanabe, Naoko Kuriyama, and
Takenobu Tokunaga. 2004. Generating referring expres-
sions using perceptual groups. In Proceedings of the 3rd
International Conference on Natural Language Genera-
tion: INLG04, pages 51?60.
Peter Heeman and Graeme Hirst. 1995. Collaborating refer-
ring expressions. Computational Linguistics, 21(3):351?
382.
Helmut Horacek. 1997. An algorithm for generating refer-
ential descriptions with flexible interfaces. In Proceedings
of the 35th Annual Meeting of the Association for Compu-
tational Linguistics, pages 206?213.
Emiel Krahmer and Marie?t Theune. 2002. Efficient context-
sensitive generation of descriptions. In Kees van Deemter
and Rodger Kibble, editors, Information Sharing: Given-
ness and Newness in Language Processing. CSLI Publica-
tions, Stanford, California.
Emiel Krahmer, Sebastiaan van Erk, and Andre? Verleg. 2003.
Graph-based generation of referring expressions. Compu-
tational Linguistics, 29(1):53?72.
Fre?de?ric Landragin, Nadia Bellalem, and Laurent Romary.
2001. Visual salience and perceptual grouping in mul-
timodal interactivity. In Proceedings of International
Workshop on Information Presentation and Natural Mul-
timodal Dialogue (IPNMD), pages 151?155.
Matthew Stone. 2000. On identifying sets. In Proceedings
of the 1st International Conference on Natural Language
Generation: INLG00, pages 116?123.
Hozumi Tanaka, Takenobu Tokunaga, and Yusuke Shinyama.
2004. Animated agents capable of understanding natural
language and performing actions. In Helmut Prendinger
and Mituru Ishizuka, editors, Life-Like Characters, pages
429?444. Springer.
Kristinn R. Tho?risson. 1994. Simulated perceptual grouping:
An application to human-computer interaction. In Pro-
ceedings of the Sixteenth Annual Conference of the Cog-
nitive Science Society, pages 876?881.
Takenobu Tokunaga, Tomofumi Koyama, and Suguru Saito.
2005. Meaning of Japanese spatial nouns. In Proceedings
of the Second ACL-SIGSEM Workshop on the Linguistic
Dimentions of Prepositions and their Use in Computa-
tional Linguistics: Formalisms and Applications, pages
93?100.
Kees van Deemter. 2002. Generating referring expressions:
Boolean extensions of the incremental algorithm. Compu-
tational Linguistics, 28(1):37?52.
80
Proceedings of the 12th European Workshop on Natural Language Generation, pages 110?113,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
A Japanese corpus of referring expressions used in a situated
collaboration task
Philipp Spanger Yasuhara Masaaki Iida Ryu Tokunaga Takenobu
Department of Computer Science
Tokyo Institute of Technology
{philipp, yasuhara, ryu-i, take}@cl.cs.titech.ac.jp
Abstract
In order to pursue research on generating
referring expressions in a situated collab-
oration task, we set up a data-collection
experiment based on the Tangram puzzle.
For a pair of participants we recorded ev-
ery utterance in synchronisation with the
current state of the puzzle as well as all
operations by the participants. Referring
expressions were annotated with their ref-
erents in order to build a referring expres-
sion corpus in Japanese. We provide pre-
liminary results on the analysis of the cor-
pus from various standpoints, focussing on
action-mentioning expressions.
1 Introduction
Referring expressions are a linguistic device to re-
fer to a certain object, enabling smooth collabo-
ration between humans and agents where physical
operations are involved. Previous research often
either selectively focussed only on a limited num-
ber of expression-types or set up overly controlled
experiments. In contrast, we intend to work to-
wards analysing the whole breadth of referring ex-
pressions in a situated domain. For this purpose
we created a corpus (in Japanese) and analysed it
from various standpoints.
From very early on in referring expression re-
search, there has been some interest in the col-
laborative aspect of the reference process (Clark
and Wilkes-Gibbs, 1986). This has more recently
developed into creating situated corpora in order
to analyse the referring expressions occurring in
situated collaborative tasks. The COCONUT cor-
pus (Di Eugenio et al, 2000) is collected from
keyboard-input dialogues between two partici-
pants who are collaboratively working on a sim-
ple 2-D design task (buying and arranging furni-
ture for two rooms). In contrast, the QUAKE cor-
pus (Byron et al, 2005) ? as well as the more re-
cent SCARE corpus (Stoia et al, 2008), which is
an extension of QUAKE ? is based on an interac-
tion captured in a 3-D virtual reality (VR) world
where two participants collaboratively carry out
a treasure hunting task. There has been ongoing
work to exploit these two resources for research on
different aspects of referring expressions (Pamela
W. Jordan, 2005; Byron, 2005).
However, while these resources have inspired
new research into different aspects of referring ex-
pressions, at the same time they have clear limi-
tations. The COCONUT corpus is collected from
dialogues in which participants refer to symbol-
like objects in a 2-D world. It thus resem-
bles the more recent (non-collaborative) TUNA-
corpus (van Deemter, 2007) in tending to en-
courage very simple types of expressions. Fur-
thermore, limiting participants? interaction to key-
board input makes the dialogue less natural. While
the QUAKE corpus deals with a more complex do-
main (3-D virtual world), the participating sub-
jects were only able to carry out limited kinds of
actions (pushing buttons, picking up or dropping
objects) as compared with the complexity of the
three-dimensional target domain.
In contrast to these two corpora, we set up a
comparatively simple collaborative task (Tangram
Puzzle) allowing participants to freely communi-
cate via speech and to perform actions various
enough to accomplish the given task, e.g. pick-
ing, moving, turning and rotating pieces. All ut-
terances by participants were recorded in synchro-
nisation with operations on objects and the object
arrangement. The utterances were transcribed and
all referring expressions found were annotated to-
gether with their referents. Thus, this corpus al-
lows us to study in detail human-human interac-
tion, particularly referring expressions in a situ-
ated setting. In what follows, we first describe de-
tails of the building of the corpus and then provide
110
results of our preliminary analysis. This analysis
reveals a novel type of referring expression men-
tioning an action on objects, which we call action-
mentioning expressions.
2 Building the corpus
Figure 1: Screenshot of the Tangram simulator
2.1 Experimental setting
We recruited 12 Japanese graduate students (4 fe-
males, 8 males) and split them into 6 pairs. Each
pair was instructed to solve the Tangram puzzle
(an ancient Chinese geometrical puzzle) coopera-
tively. The goal of Tangram is to construct a given
shape by arranging seven pieces of simple figures
as shown in Figure 1.
In order to record detailed information of the
interaction (position of pieces, participants? ac-
tions), we implemented a Tangram simulator in
which the pieces on the computer display can be
moved, rotated and flipped with simple mouse op-
erations. Figure 1 shows the simulator interface in
which the left shows the goal shape area and the
right the working area. We assigned two differ-
ent roles to participants, a solver and an operator;
the solver thinks of the arrangement of the pieces
to make the goal shape and gives instructions to
the operator, while the operator manipulates the
pieces with the mouse according to the solver?s in-
structions.
A solver and an operator sit side by side in front
of their own computer display. Both participants
share the same working area of the simulator. The
operator can manipulate the pieces, but cannot see
the goal shape. In contrast, the solver sees the goal
shape but cannot move pieces. A shield screen was
set between the participants in order to prevent
them from peeking at their partner?s display. In
this asymmetrical interaction, we can expect many
referring expressions during the interaction.
Each pair is assigned four exercises and the par-
ticipants exchanged roles after two exercises. We
set a time limit of 15 minutes for an exercise.
Utterances by the participants are recorded sep-
arately in stereo through headset microphones in
synchronisation with the position of the pieces and
the mouse actions. In total, we collected 24 dia-
logues of about four hours. The average length of
a dialogue was 10 minutes 43 seconds.
2.2 Annotation
Recorded dialogues were transcribed with a time
code attached to each utterance. Since our main
concern is collecting referring expressions, we de-
fined an utterance to be a complete sentence to
prevent a referring expression being split into sev-
eral utterances. Referring expressions were an-
notated together with their referents by using the
multi-purpose annotation tool SLAT (Noguchi et
al., 2008). Two annotators (two of the authors) an-
notated four dialogue texts separately. We anno-
tated all 24 dialogue texts and corrected discrep-
ancies by discussion between the annotators.
3 Analysis of the corpus
We collected a total of 1,509 tokens and 449 types
of referring expressions in 24 dialogues. Our
asymmetric experimental setting tended to encour-
age referring expressions from the solver, while
the operator was constrained to confirming his un-
derstanding of the solver?s instructions. This is re-
flected in the number of referring expressions by
the solver (1,287) largely outnumbering those of
the operator (222). There are a number of expres-
sions (215 expressions; 15% of the total) referring
to multiple objects (referring to 2 or more pieces)
and we excluded them from our current analysis.
We exclusively deal here with expressions refer-
ring to a specific single piece or indefinite expres-
sions, i.e. those that have no definite referent (in
total 1,294 tokens).
We found the following syntactic/semantic fea-
tures used in the expressions: i) demonstratives
(adjectives and pronouns), ii) object attribute-
values, iii) spatial relations, iv) actions on an ob-
ject and v) others. The number of these features is
summarised in Table 1. (Note that multiple fea-
tures can be used in a single expression.) The
right-most column shows an example with its En-
111
Table 1: Features of referring expressions
Feature types tokens Example
i) demonstrative 118 745
adjective 100 196 ?ano migigawa no sankakkei (that triangle at the right side)?
pronoun 19 551 ?kore (this)?
ii) attribute 303 641
size 165 267 ?tittyai sankakkei (the small triangle)?
shape 271 605 ?o?kii sankakkei (the large triangle)?
direction 6 6 ?ano sita muiteru dekai sankakkei (that large triangle facing to the bottom)?
iii) spatial relations 129 148
projective 125 144 ?hidari no okkii sankakkei (the small triangle on the left)?
topological 2 2 ?o?kii hanareteiru yatu (the big distant one)?
overlapping 2 2 ?sono sita ni aru sankakkei (the triangle underneath it)?
iv) action-mentioning 78 85 ?migi ue ni doketa sankakkei (the triangle you put away to the top right)?
v) others 29 30
remaining 15 15 ?nokotteiru o?kii sankakkei (the remaining large triangle)?
similarity 14 15 ?sore to onazi katati no (the one of the same shape as that one)?
glish translation. The identified feature in the re-
ferring expression is underlined.
We note here a tendency to employ object at-
tributes, particularly the attribute ?shape? as well
as use of demonstratives, particularly demonstra-
tive pronouns. These kinds of referring expres-
sions are quite general and appear in a variety of
other non-situated settings as well. In addition,
we found another kind of expression not usually
employed by humans outside of situated collabo-
ration tasks; referring expressions mentioning an
action on an object. We have 85 expressions (over
6% of the total) of this type in our corpus.
4 Action-mentioning expressions
We further analysed those expressions that men-
tion an action on an object, which we call action-
mentioning expressions hereafter. Although there
was significant variation in usage of action-
mentioning expressions among the pairs, all 6
pairs of participants used at least one action-
mentioning expression, indicating that it is a fun-
damental type of expression for this task set-
ting. Action-mentioning expressions are different
from haptic-ostensive referring expressions (Fos-
ter et al, 2008) since action-mentioning expres-
sions are not necessarily accompanied by simulta-
neous physical operation on an object.
Action-mentioning expressions can be again di-
vided into three categories: i) combination of a
temporal adverbial with a verb indicating an ac-
tion (?turned?, ?put?, ?moved?, etc) (55 tokens or
about 65% of action-mentioning expression), ii)
use of temporal adverbials without a verb, i.e. verb
ellipsis (22 tokens or about 26%) and iii) expres-
sions with a verb without temporal adverbials (8
tokens or about 9%). The second category includ-
ing verb ellipsis would be rare in English, but it is
quite natural in Japanese.
Only less than 10% of this kind of expression
did not include any temporal adverbial, indicating
that humans tend to describe the temporal aspect
of an action. This needs to be integrated into any
generation algorithm for this task domain. The
temporal adverbials used by the participants were
the Japanese ?sakki no NP (the NP [verb-ed] just
before)? or ?ima no NP (the current NP/the NP
[you are verb-ing] now/the NP [verb-ed] just be-
fore)?. ?Ima? generally refers to the current time
point (?now?). It can, however, refer to a past time
point as well, thus it is ambiguous.
Participants tended to use ?ima? largely in its
perfect meaning (completed action). The fre-
quency of use of ?ima? in its perfect meaning in
comparison to its progressive meaning was about
2:1. The distribution of the two types of tempo-
ral adverbials ?sakki? and ?ima? was about 2:3.
The slight preference here for ?ima? might be ex-
plained by its dual meanings (progressive and per-
fect) in contrast to the exclusive use of ?sakki? for
past actions.
Figure 2 shows the distribution of ?sakki (just
before)? and past-cases of ?ima (now)? dependent
on the time-distance to the action they refer to. For
actions occurring within a timeframe of about 10
seconds previous to uttering an expression, partic-
ipants had an overwhelming preference for ?ima?.
The frequency of ?ima? decreases quickly for ac-
tions that occurred 10-20 seconds prior to the ut-
terance. In contrast, after 20 seconds from the ac-
112
04
8
12
16
0
?
1
0
1
0
?
2
0
2
0
?
3
0
3
0
?
4
0
4
0
?
5
0
5
0
?
6
0
6
0
?
7
0
7
0
?
8
0
8
0
?
9
0
9
0
?
1
0
0
1
0
0
?
f
r
e
q
u
e
n
c
y
time (sec)
"sakki (just before)"
"ima (now)"
Figure 2: Frequency of ?sakki? and ?ima? over the
time-distance to referenced action
tion, participants prefered ?sakki?.
In addition, we investigated what actions oc-
curred in between the utterance and the action
mentioned. The actions we take into account here
are basic manipulations of an object like ?move?,
?flip?, ?click? and so on. Referring to an immedi-
ately preceding action, participants had a strong
preference for using ?ima?. Interestingly, with
only one other action in between, the participants?
preference becomes opposite (i.e. ?sakki? is pre-
ferred.). For referring to actions further in the past
(i.e. more actions in between), there was a con-
tinous preference for ?sakki? over ?ima?. Further
analysis should also investigate the phenomenon
of the difference in use of temporal adverbials for
other languages and whether this is related to char-
acteristics of the Japanese language or rather an in-
herent property of the use of temporal adverbials
in natural language.
5 Conclusion and future work
We collected a corpus of Japanese referring ex-
pressions as a first step towards developing algo-
rithms for generating referring expressions in a sit-
uated collaboration. We carried out an initial anal-
ysis of the collected expressions, focussing on ex-
pressions that include a mention of an action on
an object. We noted that they are often combined
with temporal adverbials with participants seek-
ing to make a temporal ordering of actions. In
addition, we intend to further analyse other types
of expressisons (demonstratives, etc) and work on
developing generation algorithms for this domain.
In future work, we intend to generalise this exper-
iment in the Tangram-domain to other domains.
Furthermore, information such as gestures and eye
movements should be incorporated in data collec-
tion. This will lay the basis for the development of
more general models for the generation of refer-
ring expressions in a situated collaborative task.
References
Donna Byron, Thomas Mampilly, Vinay Sharma, and
Tianfang Xu. 2005. Utilizing visual attention for
cross-modal coreference interpretation. In CON-
TEXT 2005, pages 83?96.
Donna K. Byron. 2005. The OSU Quake 2004 cor-
pus of two-party situated problem-solving dialogs.
Technical report, Department of Computer Science
and Enginerring, The Ohio State University.
H. H. Clark and D. Wilkes-Gibbs. 1986. Referring as
a collaborative process. Cognition, 22:1?39.
B. Di Eugenio, P. W. Jordan, R. H. Thomason, and J. D
Moore. 2000. The agreement process: An empirical
investigation of human-human computer-mediated
collaborative dialogues. International Journal of
Human-Computer Studies, 53(6):1017?1076.
Mary Ellen Foster, Ellen Gurman Bard, Markus Guhe,
Robin L. Hill, Jon Oberlander, and Alois Knoll.
2008. The roles of haptic-ostensive referring expres-
sions in cooperative, task-based human-robot dia-
logue. In Proceedings of 3rd Human-Robot Inter-
action, pages 295?302.
Masaki Noguchi, Kenta Miyoshi, Takenobu Tokunaga,
Ryu Iida, Mamoru Komachi, and Kentaro Inui.
2008. Multiple purpose annotation using SLAT ?
Segment and link-based annotation tool. In Pro-
ceedings of 2nd Linguistic Annotation Workshop,
pages 61?64.
Marilyn A. Walker Pamela W. Jordan. 2005. Learning
content selection rules for generating object descrip-
tions in dialogue. Journal of Artificial Intelligence
Research, 24:157?194.
Laura Stoia, Darla Magdalene Shockley, Donna K. By-
ron, and Eric Fosler-Lussier. 2008. SCARE: A sit-
uated corpus with annotated referring expressions.
In Proceedings of the Sixth International Confer-
ence on Language Resources and Evaluation (LREC
2008).
Kees van Deemter. 2007. TUNA: Towards a UNified
Algorithm for the generation of referring expres-
sions. Technical report, Aberdeen University.
www.csd.abdn.ac.uk/research/tuna/pubs/TUNA-
final-report.pdf.
113
Proceedings of the 12th European Workshop on Natural Language Generation, pages 191?194,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
A Probabilistic Model of Referring Expressions for Complex Objects
Kotaro Funakoshi? Philipp Spanger?
?Honda Research Institute Japan Co., Ltd.
Saitama, Japan
funakoshi@jp.honda-ri.com
nakano@jp.honda-ri.com
Mikio Nakano? Takenobu Tokunaga?
?Tokyo Institute of Technology
Tokyo, Japan
philipp@cl.cs.titech.ac.jp
take@cl.cs.titech.ac.jp
Abstract
This paper presents a probabilistic model
both for generation and understanding of
referring expressions. This model intro-
duces the concept of parts of objects, mod-
elling the necessity to deal with the char-
acteristics of separate parts of an object in
the referring process. This was ignored or
implicit in previous literature. Integrating
this concept into a probabilistic formula-
tion, the model captures human character-
istics of visual perception and some type
of pragmatic implicature in referring ex-
pressions. Developing this kind of model
is critical to deal with more complex do-
mains in the future. As a first step in our
research, we validate the model with the
TUNA corpus to show that it includes con-
ventional domain modeling as a subset.
1 Introduction
Generation of referring expressions has been stud-
ied for the last two decades. The basic orientation
of this research was pursuing an algorithm that
generates a minimal description which uniquely
identifies a target object from distractors. Thus
the research was oriented and limited by two con-
straints: minimality and uniqueness.
The constraint on minimality has, however,
been relaxed due to the computational complexity
of generation, the perceived naturalness of redun-
dant expressions, and the easiness of understand-
ing them (e.g., (Dale and Reiter, 1995; Spanger et
al., 2008)). On the other hand, the other constraint
of uniqueness has not been paid much attention
to. One major aim of our research is to relax this
constraint on uniqueness because of the reason ex-
plained below.
The fundamental goal of our research is to deal
with multipartite objects, which have constituents
with different attribute values. Typical domain set-
tings in previous literature use uniform objects like
the table A shown in Figure 1. However, real life
is not so simple. Multipartite objects such as ta-
bles B and C can be found easily. Therefore this
paper introduces the concept of parts of objects to
deal with more complex domains containing such
objects. Hereby the constraint on uniqueness be-
comes problematic because people easily generate
and understand logically ambiguous expressions
in such domains.
For example, people often use an expression
such as ?the table with red corners? to identify
table B. Logically speaking, this expression is
equally applicable both to A and to B, that is, vio-
lating the constraint on uniqueness. And yet peo-
ple seem to have no problem identifying the in-
tended target correctly and have little reluctance to
use such an expression (Evidence is presented in
Section 3). We think that this reflects some type of
pragmatic implicature arising from human charac-
teristics of visual perception and that is important
both for understanding human-produced expres-
sions and for generating human-friendly expres-
sions in a real environment. This paper proposes a
model of referring expressions both for generation
and understanding. Our model uses probabilities
to solve ambiguity under the relaxed constraint on
uniqueness while considering human perception.
No adequate data is currently available in or-
der to provide a comprehensive evaluation of our
model. As a first step in our research, we validate
the model with the TUNA corpus to show that it
includes conventional domain modeling.
Figure 1: An example scene
191
2 Related work
Horacek (2005) proposes to introduce probabili-
ties to overcome uncertainties due to discrepan-
cies in knowledge and cognition between subjects.
While our model shares the same awareness of is-
sues with Horacek?s work, our focus is on rather
different issues (i.e., handling multipartite objects
and relaxing the constraint on uniqueness). In
addition, Horacek?s work is concerned only with
generation while our model is available both for
generation and understanding. Roy (2002) also
proposes a probabilistic model for generation but
presupposes uniform objects.
Horacek (2006) deals with references for struc-
tured objects such as documents. Although it con-
siders parts of objects, the motivation and focus of
the work are on quite different aspects from ours.
3 Evidence against logical uniqueness
We conducted two psycholinguistic experiments
using the visual stimulus shown in Figure 1.
In the first experiment, thirteen Japanese sub-
jects were presented with an expression ?kado no
akai tukue (the table with red corners)? and asked
to choose a table from the three in the figure.
Twelve out of the thirteen chose table B. Seven
out of the twelve subjects answered that the given
expression was not ambiguous.
In the second experiment, thirteen different
Japanese subjects were asked to make a descrip-
tion for table B without using positional relations.
Ten out of the thirteen made expressions seman-
tically equivalent to the expression used in the
first experiment. Only three subjects made log-
ically discriminative expressions such as ?asi to
yotu kado dake akai tukue (the table whose four
corners and leg only are red).?
These results show that people easily gener-
ate/understand logically ambiguous expressions.
4 Proposed model
We define pi = {p1, p2, . . . , pk} as the set of k
parts of objects (classes of sub-parts) that appears
in a domain. Here p1 is special and always means
the whole of an object. In a furniture domain, p1
means a piece of furniture regardless of the kind
of the object (chair, table, whatever). pi(i 6= 1)
means a sub-part class such as leg. Note that pi is
defined not for each object but for a domain. Thus,
objects may have no part corresponding to pi (e.g.,
some chairs have no leg.).
A referring expression e is represented as a set
of n pairs of an attribute value expression eaj and a
part expression epj modified by eaj as
e = {(ep1, e
a
1), (e
p
2, e
a
2), . . . , (epn, ean)}. (1)
For example, an expression ?the white table with
a red leg? is represented as
{(?table?, ?white?), (?leg?, ?red?)}.
Given a set of objects ? and a referring ex-
pression e, the probability with which the expres-
sion e refers to an object o ? ? is denoted as
Pr(O = o|E = e,? = ?). If we seek to provide
a more realistic model, we can model a probabilis-
tic distribution even for ?. In this paper, however,
we assume that ? is fixed to ? and it is shared by
interlocutors exactly. Thus, hereafter, Pr(o|e) is
equal to Pr(o|e, ?).
Following the definition (1), we estimate
Pr(o|e) as follows:
Pr(o|e) ? N
?
i
Pr(o|epi , e
a
i ). (2)
Here, N is a normalization coefficient. According
to Bayes? rule,
Pr(o|epi , e
a
i ) =
Pr(o)Pr(epi , eai |o)
Pr(epi , eai )
. (3)
Therefore,
Pr(o|e) ? N
?
i
Pr(o)Pr(epi , eai |o)
Pr(epi , eai )
. (4)
We decompose Pr(epi , eai |o) as
?
u
?
v
Pr(epi |pu, o)Pr(e
a
i |av, o)Pr(pu, av|o)
(5)
where pu is one of parts of objects that could be
expressed with epi , and av is one of attribute val-
ues1 that could be expressed with eai . Under the
simplifying assumption that epi and eai are not am-
biguous and are single possible expressions for
a part of objects and an attribute value indepen-
dently of objects 2,
Pr(o|e) ? N
?
i
Pr(o)Pr(pi, ai|o)
Pr(pi, ai)
(6)
? N
?
i
Pr(o|pi, ai) (7)
1Each attribute value belongs to an attribute ?, a set of
attribute values. E.g., ?color = {red, white, . . .}.
2That is, we ignore lexical selection matters in this paper,
although our model is potentially able to handle those matters
including training from corpora.
192
Pr(o|p, a) concerns attribute selection in gen-
eration of referring expressions. Most attribute
selection algorithms presented in past work are
based on set operations over multiple attributes
with discrete (i.e., symbolized) values such as col-
ors (red, brown, white, etc) to find a uniquely dis-
tinguishing description. The simplest estimation
of Pr(o|p, a) following this conventional Boolean
domain modeling is
Pr(o|p, a) ?
{
|??|?1 (p in o has a)
0 (p in o does not have a) (8)
where ?? is the subset of ?, each member of which
has attribute value a in its part of p.
As Horacek (2005) pointed out, however, this
standard approach is problematic in a real envi-
ronment because many physical attributes are non-
discrete and the symbolization of these continuous
attributes have uncertainties. For example, even
if two objects are blue, one can be more blueish
than the other. Some subjects may say it?s blue
but others may say it?s purple. Moreover, there
is the problem of logical ambiguity pointed out
in Section 1. That is, even if an attribute itself
is equally applicable to several objects in a logi-
cal sense, other available information (such as vi-
sual context) might influence the interpretation of
a given referring expression.
Such phenomena could be captured by estimat-
ing Pr(o|p, a) as
Pr(o|p, a) ? Pr(a|p, o)Pr(p|o)Pr(o)
Pr(p, a)
. (9)
Pr(a|p, o) represents the relevance of attribute
value a to part p in object o. Pr(p|o) represents
the salience of part p in object o. The underlying
idea to deal with the problem of logical ambiguity
is ?If some part of an object is mentioned, it should
be more salient than other parts.? This is related
to Grice?s maxims in a different way from mat-
ters discussed in (Dale and Reiter, 1995). Pr(p|o)
could be computed in some manner by using the
saliency map (Itti et al, 1998). Pr(o) is the prior
probability that object o is chosen. If potential
functions (such as used in (Tokunaga et al, 2005))
are used for computing Pr(o), we can naturally
rank objects, which are equally relevant to a given
referring expression, according to distances from
interlocutors.
5 Algorithms
5.1 Understanding
Understanding a referring expression e is identify-
ing the target object o? from a set of objects ?. This
is formulated in a straightforward way as
o? = argmax
o??
Pr(o|e). (10)
5.2 Generation
Generation of a referring expression is choosing
the best appropriate expression e? to discriminate a
given object o? from a set of distractors. A simple
formulation is
e? = argmax
e??
Pr(e)Pr(o?|e). (11)
? is a pre-generated set of candidate expressions
for o?. This paper does not explain how to generate
a set of candidates.
Pr(e) is the generation probability of an ex-
pression e independent of objects. This probabil-
ity can be learned from a corpus. In the evaluation
described in Section 6, we estimate Pr(e) as
Pr(e) ? Pr(|e|)
?
i
Pr(?i). (12)
Here, Pr(|e|) is the distribution of expression
length in terms of numbers of attributes used.
Pr(?) is the selection probability of a specific at-
tribute ? (SP (a) in (Spanger et al, 2008)).
6 Preliminary evaluation
As mentioned above, no adequate corpus is cur-
rently available in order to provide an initial vali-
dation of our model which we present in this pa-
per. In this section, we validate our model us-
ing the TUNA corpus (the ?Participant?s Pack?
available for download as part of the Generation
Challenge 2009) to show that it includes tradi-
tional domain modeling. We use the training-
part of the corpus for training our model and the
development-part for evaluation.
We note that we here assume a homogeneous
distribution of the probability Pr(o|p, a), i.e., we
are applying formula (8) here in order to calculate
this probability. We first implemented our proba-
bilistic model for the area of understanding. This
means our algorithm took as input the user?s selec-
tion of attribute?value pairs in the description and
calculated the most likely target object. This was
193
Table 1: Initial evaluation of proposed model for
generation in TUNA-domain
Furniture People
Total cases 80 68
Mean Dice-score 0.78 0.66
carried out for both the furniture and people do-
mains. Overall, outside of exceptional cases (e.g.,
human error), our algorithm was able to distin-
guish the target object for all human descriptions
(precision of 100%). This means it covers all the
cases the original approach dealt with.
We then implemented our model for the case of
generation. We measured the similarity of the out-
put of our algorithm with the human-produced sets
by using the Dice-coefficient (see (Belz and Gatt,
2007)). We evaluated this both for the Furniture
and People domain. The results are summarized
in Table 1.
Our focus was here to fundamentally show how
our model includes traditional modelling as a sub-
set, without much focus or effort on tuning in order
to achieve a maximum Dice-score. However, we
note that the Dice-score of our algorithm was com-
parable to the top 5-7 systems in the 2007 GRE-
Challenge (see (Belz and Gatt, 2007)) and thus
produced a relatively good result. This shows how
our algorithm ? providing a model of the referring
process in a more complex domain ? is applica-
ble as well to the very simple TUNA-domain as a
special case.
7 Discussion
In past work, parts of objects were ignored or im-
plicit. In case of the TUNA corpus, while the Fur-
niture domain ignores parts of objects, the People
domain contained parts of objects such as hair,
glasses, beard, etc. However, they were implic-
itly modeled by combining a pair of a part and its
attribute as an attribute such as hairColor. One
major advantage of our model is that, by explicitly
modelling parts of objects, it can handle the prob-
lem of logical ambiguity that is newly reported in
this paper. Although it might be possible to han-
dle the problem by extending previously proposed
algorithms in some ways, our formulation would
be clearer. Moreover, our model is directly avail-
able both for generation and understanding. Re-
ferring expressions using attributes (such as dis-
cussed in this paper) and those using discourse
contexts (such as ?it?) are separately approached
in past work. Our model possibly handles both of
them in a unified manner with a small extension.
This paper ignored relations between objects.
We, however, think that it is not difficult to prepare
algorithms handling relations using our model.
Generation using our model is performed in a
generate-and-test manner. Therefore computa-
tional complexity is a matter of concern. However,
that could be controlled by limiting the numbers
of attributes and parts under consideration accord-
ing to relevance and salience, because our model is
under the relaxed constraint of uniqueness unlike
previous work.
As future work, we have to gather data to eval-
uate our model and to statistically train lexical se-
lection in a new domain containing multipartite
objects.
References
Anja Belz and Albert Gatt. 2007. The attribute selec-
tion for GRE challenge: Overview and evaluation
results. In Proc. the MT Summit XI Workshop Using
Corpora for Natural Language Generation: Lan-
guage Generation and Machine Translation (UC-
NLG+MT), pages 75?83.
Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the Gricean maxims in the gener-
ation of referring expressions. Cognitive Science,
18:233?263.
Helmut Horacek. 2005. Generating referential de-
scriptions under conditions of uncertainty. In Proc.
ENLG 05.
Helmut Horacek. 2006. Generating references to parts
of recursively structured objects. In Proc. ACL 06.
L Itti, C. Koch, and E. Niebur. 1998. A model of
saliency-based visual attention for rapid scene anal-
ysis. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 20(11):1254?1259.
Deb Roy. 2002. Learning visually-grounded words
and syntax for a scene description task. Computer
Speech and Language, 16(3).
Philipp Spanger, Takehiro Kurosawa, and Takenobu
Tokunaga. 2008. On ?redundancy? in selecting
attributes for generating referring expressions. In
Proc. COLING 08.
Takenobu Tokunaga, Tomonori Koyama, and Suguru
Saito. 2005. Meaning of Japanese spatial nouns.
In Proc. the Second ACL-SIGSEM Workshop on The
Linguistic Dimensions of Prepositions and their Use
in Computational Linguistics Formalisms and Appli-
cations, pages 93 ? 100.
194
Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 145?152,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Query Expansion using LMF-Compliant Lexical Resources
Tokunaga Takenobu
Tokyo Inst. of Tech.
Dain Kaplan
Tokyo Inst. of Tech.
Nicoletta Calzolari
ILC/CNR
Monica Monachini
ILC/CNR
Claudia Soria
ILC/CNR
Virach Sornlertlamvanich
TCL, NICT
Thatsanee Charoenporn
TCL, NICT
Xia Yingju
Fujitsu R&D Center
Chu-Ren Huang
The Hong Kong Polytec. Univ.
Shu-Kai Hsieh
National Taiwan Normal Univ.
Shirai Kiyoaki
JAIST
Abstract
This paper reports prototype multilin-
gual query expansion system relying on
LMF compliant lexical resources. The
system is one of the deliverables of a
three-year project aiming at establish-
ing an international standard for language
resources which is applicable to Asian
languages. Our important contributions
to ISO 24613, standard Lexical Markup
Framework (LMF) include its robustness
to deal with Asian languages, and its ap-
plicability to cross-lingual query tasks, as
illustrated by the prototype introduced in
this paper.
1 Introduction
During the last two decades corpus-based ap-
proaches have come to the forefront of NLP re-
search. Since without corpora there can be no
corpus-based research, the creation of such lan-
guage resources has also necessarily advanced
as well, in a mutually beneficial synergetic re-
lationship. One of the advantages of corpus-
based approaches is that the techniques used
are less language specific than classical rule-
based approaches where a human analyses the
behaviour of target languages and constructs
rules manually. This naturally led the way
for international resource standardisation, and in-
deed there is a long standing precedent in the
West for it. The Human Language Technol-
ogy (HLT) society in Europe has been particu-
larly zealous in this regard, propelling the cre-
ation of resource interoperability through a se-
ries of initiatives, namely EAGLES (Sanfilippo et
al., 1999), PAROLE/SIMPLE (Lenci et al, 2000),
ISLE/MILE (Ide et al, 2003), and LIRICS1. These
1http://lirics.loria.fr/
continuous efforts have matured into activities in
ISO-TC37/SC42, which aims at making an inter-
national standard for language resources.
However, due to the great diversity of languages
themselves and the differing degree of technolog-
ical development for each, Asian languages, have
received less attention for creating resources than
their Western counterparts. Thus, it has yet to be
determined if corpus-based techniques developed
for well-computerised languages are applicable on
a broader scale to all languages. In order to effi-
ciently develop Asian language resources, utilis-
ing an international standard in this creation has
substantial merits.
We launched a three-year project to create an
international standard for language resources that
includes Asian languages. We took the following
approach in seeking this goal.
? Based on existing description frameworks,
each research member tries to describe sev-
eral lexical entries and find problems with
them.
? Through periodical meetings, we exchange
information about problems found and gen-
eralise them to propose solutions.
? Through an implementation of an application
system, we verify the effectiveness of the pro-
posed framework.
Below we summarise our significant contribution
to an International Standard (ISO24613; Lexical
Markup Framework: LMF).
1st year After considering many characteristics
of Asian languages, we elucidated the shortcom-
ings of the LMF draft (ISO24613 Rev.9). The
draft lacks the following devices for Asian lan-
guages.
2http://www.tc37sc4.org/
145
(1) A mapping mechanism between syntactic
and semantic arguments
(2) Derivation (including reduplication)
(3) Classifiers
(4) Orthography
(5) Honorifics
Among these, we proposed solutions for (1) and
(2) to the ISO-TC37 SC4 working group.
2nd year We proposed solutions for above the
(2), (3) and (4) in the comments of the Committee
Draft (ISO24613 Rev. 13) to the ISO-TC37 SC4
working group. Our proposal was included in DIS
(Draft International Standard).
(2?) a package for derivational morphology
(3?) the syntax-semantic interface resolving the
problem of classifiers
(4?) representational issues with the richness of
writing systems in Asian languages
3rd year Since ISO 24613 was in the FDIS stage
and fairly stable, we built sample lexicons in Chi-
nese, English, Italian, Japanese, and Thai based
on ISO24613. At the same time, we implemented
a query expansion system utilising rich linguis-
tic resources including lexicons described in the
ISO 24613 framework. We confirmed that a sys-
tem was feasible which worked on the tested lan-
guages (including both Western and Asian lan-
guages) when given lexicons compliant with the
framework. ISO 24613 (LMF) was approved by
the October 2008 ballot and published as ISO-
24613:2008 on 17th November 2008.
Since we have already reported our first 2 year
activities elsewhere (Tokunaga and others, 2006;
Tokunaga and others, 2008), we focus on the
above query expansion system in this paper.
2 Query expansion using
LMF-compliant lexical resources
We evaluated the effectiveness of LMF on a mul-
tilingual information retrieval system, particularly
the effectiveness for linguistically motivated query
expansion.
The linguistically motivated query expansion
system aims to refine a user?s query by exploiting
the richer information contained within a lexicon
described using the adapted LMF framework. Our
lexicons are completely complaint with this inter-
national standard. For example, a user inputs a
keyword ?ticket? as a query. Conventional query
expansion techniques expand this keyword to a
set of related words by using thesauri or ontolo-
gies (Baeza-Yates and Ribeiro-Neto, 1999). Using
the framework proposed by this project, expand-
ing the user?s query becomes a matter of following
links within the lexicon, from the source lexical
entry or entries through predicate-argument struc-
tures to all relevant entries (Figure 1). We focus
on expanding the user inputted list of nouns to rel-
evant verbs, but the reverse would also be possible
using the same technique and the same lexicon.
This link between entries is established through
the semantic type of a given sense within a lexical
entry. These semantic types are defined by higher-
level ontologies, such as MILO or SIMPLE (Lenci
et al, 2000) and are used in semantic predicates
that take such semantic types as a restriction ar-
gument. Since senses for verbs contain a link to
a semantic predicate, using this semantic type, the
system can then find any/all entries within the lexi-
con that have this semantic type as the value of the
restriction feature of a semantic predicate for any
of their senses. As a concrete example, let us con-
tinue using the ?ticket? scenario from above. The
lexical entry for ?ticket? might contain a semantic
type definition something like in Figure 2.
<LexicalEntry ...>
<feat att="POS" val="N"/>
<Lemma>
<feat att="writtenForm"
val="ticket"/>
</Lemma>
<Sense ...>
<feat att="semanticType"
val="ARTIFACT"/>
...
</Sense>
...
</LexicalEntry>
Figure 2: Lexical entry for ?ticket?
By referring to the lexicon, we can then derive
any actions and events that take the semantic type
?ARTIFACT? as an argument.
First all semantic predicates are searched for ar-
guments that have an appropriate restriction, in
this case ?ARTIFACT? as shown in Figure 3, and
then any lexical entries that refer to these predi-
cates are returned. An equally similar definition
would exist for ?buy?, ?find? and so on. Thus,
by referring to the predicate-argument structure of
related verbs, we know that these verbs can take
146
<LexicalEntry ...>
  <feat att="POS" val="Noun"/>
  <Lemma>
    <feat att="writtenForm" val="ticket"/>
  </Lemma>
  <Sense ...>
    <feat att="semanticType" val="ARTIFACT"/>
    ...
  </Sense>
  ...
</LexicalEntry>
User Inputs
ticket
<Sense>
<SemanticFeature>
Semantic Features of type 
"restriction" that take 
Sense's semanticType
All senses for 
matched nouns
<SemanticPredicate 
  id="pred-sell-1">
  <SemanticArgument>
    <feat att="label" val="X"/>
    <feat att="semanticRole" val="Agent"/>
    <feat att="restriction" val="Human"/>
  </SemanticArgument>
  ...
  <SemanticArgument>
    <feat att="label" val="Z"/>
    <feat att="semanticRole" val="Patient"/>
    <feat att="restriction" 
          val="ARTIFACT,LOCATION"/>
  </SemanticArgument>
</SemanticPredicate>
All Semantic Predicates 
that contain matched 
Semantic Features
<Sense>
Senses that use matched 
Semantic Predicates
<LexicalEntry ...>
  <feat att="POS" val="Verb"/>
  <Lemma>
    <feat att="writtenForm" val="sell"/>
  </Lemma>
  <Sense id="sell-1" ...>
    ...
    <PredicativeRepresentation
      predicate="pred-sell-1" ...>
  </Sense>
</LexicalEntry>
<LexicalEntry>
<SemanticPredicate>
<LexicalEntry>
System outputs
"sell", ...
For each <Sense> find all 
<SemanticArgument> that 
take this semanticType as 
a feature of type 
"restriction"
Find all verbs <LexicalEntry> 
that use these 
<SemanticPredicate>
All verbs that have 
matched Senses
Figure 1: QE Process Flow
147
<LexicalEntry ...>
<feat att="POS" val="V"/>
<Lemma>
<feat att="writtenForm"
val="sell"/>
</Lemma>
<Sense id="sell-1" ...>
<feat att="semanticType"
val="Transaction"/>
<PredicativeRepresentation
predicate="pred-sell-1"
correspondences="map-sell1">
</Sense>
</LexicalEntry>
<SemanticPredicate id="pred-sell-1">
<SemanticArgument ...>
...
<feat att="restriction"
val="ARTIFACT"/>
</SemanticArgument>
</SemanticPredicate>
Figure 3: Lexical entry for ?sell? with its semantic
predicate
?ticket? in the role of object. The system then re-
turns all relevant entries, here ?buy?, ?sell? and
?find?, in response to the user?s query. Figure 1
schematically shows this flow.
3 A prototype system in detail
3.1 Overview
To test the efficacy of the LMF-compliant lexi-
cal resources, we created a system implementing
the query expansion mechanism explained above.
The system was developed in Java for its ?com-
pile once, run anywhere? portability and its high-
availability of reusable off-the-shelf components.
On top of Java 5, the system was developed us-
ing JBoss Application Server 4.2.3, the latest stan-
dard, stable version of the product at the time of
development. To provide fast access times, and
easy traversal of relational data, a RDB was used.
The most popular free open-source database was
selected, MySQL, to store all lexicons imported
into the system, and the system was accessed, as a
web-application, via any web browser.
3.2 Database
The finalised database schema is shown in Fig-
ure 4. It describes the relationships between en-
tities, and more or less mirrors the classes found
within the adapted LMF framework, with mostly
only minor exceptions where it was efficacious for
querying the data. Due to space constraints, meta-
data fields, such as creation time-stamps have been
left out of this diagram. Since the system also al-
lows for multiple lexicons to co-exist, a lexicon id
resides in every table. This foreign key has been
highlighted in a different color, but not connected
via arrows to make the diagram easier to read. In
addition, though in actuality this foreign key is not
required for all tables, it has been inserted as a con-
venience for querying data more efficiently, even
within join tables (indicated in blue). Having mul-
tiple lexical resources co-existing within the same
database allows for several advantageous features,
and will be described later. Some tables also con-
tain a text id, which stores the original id attribute
for that element found within the XML. This is
not used in the system itself, and is stored only for
reference.
3.3 System design
As mentioned above, the application is deployed
to JBoss AS as an ear-file. The system it-
self is composed of java classes encapsulating
the data contained within the database, a Pars-
ing/Importing class for handling the LMF XML
files after they have been validated, and JSPs,
which contain HTML, for displaying the inter-
face to the user. There are three main sections
to the application: Search, Browse, and Config-
ure. Explaining last to first, the Configure section,
shown in Figure 5, allows users to create a new
lexicon within the system or append to an exist-
ing lexicon by uploading a LMF XML file from
their web browser, or delete existing lexicons that
are no longer needed/used. After import, the data
may be immediately queried upon with no other
changes to system configuration, from within both
the Browse and Search sections. Regardless of
language, the rich syntactic/semantic information
contained within the lexicon is sufficient for car-
rying out query expansion on its own.
The Browse section (Figure 6) allows the user to
select any available lexicon to see the relationships
contained within it, which contains tabs for view-
ing all noun to verb connections, a list of nouns, a
list of verbs, and a list of semantic types. Each has
appropriate links allowing the user to easily jump
to a different tab of the system. Clicking on a noun
takes them to the Search section (Figure 7). In this
section, the user may select many lexicons to per-
form query extraction on, as is visible in Figure 7.
148
semantic_link 
VARCHAR (64)
sense
sense_id
PRIMARY KEY
synset_id
FOREIGN KEY
syn_sem_correspondence_id
FOREIGN KEY
semantic_predicate_id
FOREIGN KEY
semantic_type
VARCHAR (64)
lexicon_id
FOREIGN KEY
text_id
VARCHAR (64)
lexicon_id
FOREIGN KEY
text_id
VARCHAR (100)
semantic_predicate_id
PRIMARY KEY
semantic_predicate
lexicon_id
FOREIGN KEY
text_id
VARCHAR (64)
semantic_argument_id
PRIMARY KEY
semantic_argument
value
VARCHAR (100)
attribute
VARCHAR (100)
lexicon_id
FOREIGN KEY
semantic_feature_id
PRIMARY KEY
semantic_feature
lexicon_id
FOREIGN KEY
semantic_argument_id
FOREIGN KEY
semantic_predicate_id
FOREIGN KEY
semantic_predicate_to_argument
lexicon_id
FOREIGN KEY
semantic_feature_id
FOREIGN KEY
semantic_argument_id 
FOREIGN KEY
semantic_argument_to_feature
description
TEXT
lexicon_id
FOREIGN KEY
text_id
VARCHAR (64)
synset_id
PRIMARY KEY
synset
written_form
VARCHAR (64) NOT NULL
part_of_speech
ENUM( 'Verb', 'Noun' , 'Unknown')
lexical_entry
text_id
VARCHAR (64)
entry_id 
PRIMARY KEY
lexicon_id 
FOREIGN KEY
semantic_feature
FOREIGN KEY
syntactic_feature
FOREIGN KEY
lexicon_id
FOREIGN KEY
argument_map_id
PRIMARY KEY
syn_sem_argument_map
lexicon_id
FOREIGN KEY
argument_map_id
FOREIGN KEY
syn_sem_correspondence_id 
FOREIGN KEY
syn_sem_correspondence_to_map
lexicon_id
FOREIGN KEY
text_id
VARCHAR (64)
syn_sem_correspondence_id
PRIMARY KEY
syn_sem_correspondence
lexicon_id
FOREIGN KEY
sense_id
FOREIGN KEY
entry_id
FOREIGN KEY
lexical_entry_to_sense
lexicon_id
FOREIGN KEY
text_id
VARCHAR (100)
frame_id
PRIMARY KEY
subcat_frame
lexicon_id
FOREIGN KEY
frame_id
FOREIGN KEY
sense_id
FOREIGN KEY
entry_id
FOREIGN KEY
lexical_entry_to_subcat_frame
lexicon_id
FOREIGN KEY
text_id
VARCHAR (64)
syntactic_argument_id
PRIMARY KEY
syntactic_argument
value
VARCHAR (100)
attribute
VARCHAR (100)
lexicon_id
FOREIGN KEY
syntactic_feature_id
PRIMARY KEY
syntactic_feature
lexicon_id
FOREIGN KEY
syntactic_argument_id
FOREIGN KEY
frame_id
FOREIGN KEY
subcat_frame_to_argument
lexicon_id
FOREIGN KEY
syntactic_feature_id
FOREIGN KEY
syntactic_argument_id 
FOREIGN KEY
syntactic_argument_to_feature
description
VARCHAR(128)
language
VARCHAR(64)
lexicon_id
PRIMARY KEY
lexicon
relation_type 
VARCHAR (64)
lexicon_id
FOREIGN KEY
related_sense_id
FOREIGN KEY
sense_id
FOREIGN KEY
sense_relation
Figure 4: Database schema
Figure 5: QE System - Configure Figure 6: QE System - Browse
149
Figure 7: QE System - Search
3.4 Semantic information
This new type of query expansion requires rich
lexical information. We augmented our data using
the SIMPLE ontology for semantic types, using
the same data for different languages. This had
the added benefit of allowing cross-language ex-
pansion as a result. In steps two and three of Fig-
ure 1 when senses are retrieved that take specific
semantic types as arguments, this process can be
done across all (or as many as are selected) lex-
icons in the database. Thus, results such as are
shown in Figure 7 are possible. In this figure the
Japanese word for ?nail? is entered, and results for
both selected languages, Japanese and Italian, are
returned. This feature requires the unification of
the semantic type ontology strata.
3.5 Possible extension
Next steps for the QE platform are to explore the
use of other information already defined within the
adapted framework, specifically sense relations.
Given to the small size of our sample lexicon, data
sparsity is naturally an issue, but hopefully by ex-
ploring and exploiting these sense relations prop-
erly, the system may be able to further expand a
user?s query to include a broader range of selec-
tions using any additional semantic types belong-
ing to these related senses. The framework also
contains information about the order in which syn-
tactic arguments should be placed. This informa-
tion should be used to format the results from the
user?s query appropriately.
4 An Additional Evaluation
We conducted some additional query expansion
experiments using a corpus that was acquired from
Chinese LDC (No. ?2004-863-009?) as a base (see
below). This corpus marked an initial achievement
in building a multi-lingual parallel corpus for sup-
porting development of cross-lingual NLP appli-
cations catering to the Beijing 2008 Olympics.
The corpus contains parallel texts in Chinese,
English and Japanese and covers 5 domains that
are closely related to the Olympics: traveling, din-
ing, sports, traffic and business. The corpus con-
sists of example sentences, typical dialogues and
articles from the Internet, as well as other language
teaching materials. To deal with the different lan-
guages in a uniform manner, we converted the cor-
pus into our proposed LMF-compliant lexical re-
sources framework, which allowed the system to
expand the query between all the languages within
the converted resources without additional modifi-
cations.
As an example of how this IR system func-
tioned, suppose that Mr. Smith will be visiting
Beijing to see the Olympic games and wants to
know how to buy a newspaper. Using this system,
he would first enter the query ?newspaper?. For
this query, with the given corpus, the system re-
turns 31 documents, fragments of the first 5 shown
below.
(1) I?ll bring an English newspaper immediately.
(2) Would you please hand me the newspaper.
(3) There?s no use to go over the newspaper ads.
(4) Let?s consult the newspaper for such a film.
(5) I have little confidence in what the newspa-
pers say.
Yet it can be seen that the displayed results are not
yet useful enough to know how to buy a newspa-
per, though useful information may in fact be in-
cluded within some of the 31 documents. Using
the lexical resources, the query expansion module
suggests ?buy?, ?send?, ?get?, ?read?, and ?sell?
as candidates to add for a revised query.
Mr. Smith wants to buy a newspaper, so he se-
lects ?buy? as the expansion term. With this query
the system returns 11 documents, fragments of the
first 5 listed below.
(6) I?d like some newspapers, please.
150
(7) Oh, we have a barber shop, a laundry, a store,
telegram services, a newspaper stand, table
tennis, video games and so on.
(8) We can put an ad in the newspaper.
(9) Have you read about the Olympic Games of
Table Tennis in today?s newspaper, Miss?
(10) newspaper says we must be cautious about
tidal waves.
This list shows improvement, as information about
newspapers and shopping is present, but still ap-
pears to lack any documents directly related to
how to buy a newspaper.
Using co-occurrence indexes, the IR system
returns document (11) below, because the noun
?newspaper? and the verb ?buy? appear in the
same sentence.
(11) You can make change at some stores, just buy
a newspaper or something.
From this example it is apparent that this sort
of query expansion is still too naive to apply to
real IR systems. It should be noted, however, that
our current aim of evaluation was in confirming
the advantage of LMF in dealing with multiple
languages, for which we conducted a similar run
with Chinese and Japanese. Results of these tests
showed that in following the LMF framework in
describing lexical resources, it was possibile to
deal with all three languages without changing the
mechanics of the system at all.
5 Discussion
LMF is, admittedly, a ?high-level? specification,
that is, an abstract model that needs to be fur-
ther developed, adapted and specified by the lex-
icon encoder. LMF does not provide any off-the-
shelf representation for a lexical resource; instead,
it gives the basic structural components of a lexi-
con, leaving full freedom for modeling the partic-
ular features of a lexical resource. One drawback
is that LMF provides only a specification manual
with a few examples. Specifications are by no
means instructions, exactly as XML specifications
are by no means instructions on how to represent
a particular type of data.
Going from LMF specifications to a true instan-
tiation of an LMF-compliant lexicon is a long way,
and comprehensive, illustrative and detailed ex-
amples for doing this are needed. Our prototype
system provides a good starting example for this
direction. LMF is often taken as a prescriptive
description, and its examples taken as pre-defined
normative examples to be used as coding guide-
lines. Controlled and careful examples of conver-
sion to LMF-compliant formats are also needed to
avoid too subjective an interpretation of the stan-
dard.
We believe that LMF will be a major base
for various SemanticWeb applications because it
provides interoperability across languages and di-
rectly contributes to the applications themselves,
such as multilingual translation, machine aided
translation and terminology access in different lan-
guages.
From the viewpoint of LMF, our prototype
demonstrates the adaptability of LMF to a rep-
resentation of real-scale lexicons, thus promoting
its adoption to a wider community. This project
is one of the first test-beds for LMF (as one of
its drawbacks being that it has not been tested on
a wide variety of lexicons), particularly relevant
since it is related to both Western and Asian lan-
guage lexicons. This project is a concrete attempt
to specify an LMF-compliant XML format, tested
for representative and parsing efficiency, and to
provide guidelines for the implementation of an
LMF-compliant format, thus contributing to the
reduction of subjectivity in interpretation of stan-
dards.
From our viewpoint, LMF has provided a for-
mat for exchange of information across differently
conceived lexicons. Thus LMF provides a stan-
dardised format for relating them to other lexical
models, in a linguistically controlled way. This
seems an important and promising achievement in
order to move the sector forward.
6 Conclusion
This paper described the results of a three-year
project for creating an international standard for
language resources in cooperation with other ini-
tiatives. In particular, we focused on query expan-
sion using the standard.
Our main contribution can be summarised as
follows.
? We have contributed to ISO TC37/SC4 ac-
tivities, by testing and ensuring the portabil-
ity and applicability of LMF to the devel-
opment of a description framework for NLP
lexicons for Asian languages. Our contribu-
tion includes (1) a package for derivational
151
morphology, (2) the syntax-semantic inter-
face with the problem of classifiers, and (3)
representational issues with the richness of
writing systems in Asian languages. As of
October 2008, LMF including our contribu-
tions has been approved as the international
standard ISO 26413.
? We discussed Data Categories necessary
for Asian languages, and exemplified sev-
eral Data Categories including reduplication,
classifier, honorifics and orthography. We
will continue to harmonise our activity with
that of ISO TC37/SC4 TDG2 with respect to
Data Categories.
? We designed and implemented an evaluation
platform of our description framework. We
focused on linguistically motivated query ex-
pansion module. The system works with lexi-
cons compliant with LMF and ontologies. Its
most significant feature is that the system can
deal with any language as far as the those lex-
icons are described according to LMF. To our
knowledge, this is the first working system
adopting LMF.
In this project, we mainly worked on three
Asian languages, Chinese, Japanese and Thai, on
top of the existing framework which was designed
mainly for European languages. We plan to dis-
tribute our results to HLT societies of other Asian
languages, requesting for their feedback through
various networks, such as the Asian language re-
source committee network under Asian Federation
of Natural Language Processing (AFNLP)3, and
the Asian Language Resource Network project4.
We believe our efforts contribute to international
activities like ISO-TC37/SC45 (Francopoulo et al,
2006).
Acknowledgments
This research was carried out through financial
support provided under the NEDO International
Joint Research Grant Program (NEDO Grant).
References
R. Baeza-Yates and B. Ribeiro-Neto. 1999. Modern
Information Retrieval. Addison-Wesley.
3http://www.afnlp.org/
4http://www.language-resource.net/
5http://www.tc37sc4.org/
G. Francopoulo, G. Monte, N. Calzolari, M. Mona-
chini, N. Bel, M. Pet, and C. Soria. 2006. Lex-
ical markup framework (LMF). In Proceedings of
LREC2006.
N. Ide, A. Lenci, and N. Calzolari. 2003. RDF in-
stantiation of ISLE/MILE lexical entries. In Pro-
ceedings of the ACL 2003 Workshop on Linguistic
Annotation: Getting the Model Right, pages 25?34.
A. Lenci, N. Bel, F. Busa, N. Calzolari, E. Gola,
M. Monachini, A. Ogonowsky, I. Peters, W. Peters,
N. Ruimy, M. Villegas, and A. Zampolli. 2000.
SIMPLE: A general framework for the development
of multilingual lexicons. International Journal of
Lexicography, Special Issue, Dictionaries, Thesauri
and Lexical-Semantic Relations, XIII(4):249?263.
A. Sanfilippo, N. Calzolari, S. Ananiadou,
R. Gaizauskas, P. Saint-Dizier, and P. Vossen.
1999. EAGLES recommendations on semantic
encoding. EAGLES LE3-4244 Final Report.
T. Tokunaga et al 2006. Infrastructure for standard-
ization of Asian language resources. In Proceedings
of the COLING/ACL 2006 Main Conference Poster
Sessions, pages 827?834.
T. Tokunaga et al 2008. Adapting international stan-
dard for asian language technologies. In Proceed-
ings of the Sixth International Language Resources
and Evaluation (LREC?08).
152
Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries, ACL-IJCNLP 2009, pages 88?95,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Automatic Extraction of Citation Contexts for Research Paper
Summarization: A Coreference-chain based Approach
Dain Kaplan Ryu Iida
Department of Computer Science
Tokyo Institute of Technology
{dain,ryu-i,take}@cl.cs.titech.ac.jp
Takenobu Tokunaga
Abstract
This paper proposes a new method based
on coreference-chains for extracting cita-
tions from research papers. To evaluate
our method we created a corpus of cita-
tions comprised of citing papers for 4 cited
papers. We analyze some phenomena of
citations that are present in our corpus,
and then evaluate our method against a
cue-phrase-based technique. Our method
demonstrates higher precision by 7?10%.
1 Introduction
Review and comprehension of existing research is
fundamental to the ongoing process of conducting
research; however, the ever increasing volume of
research papers makes accomplishing this task in-
creasingly more difficult. To mitigate this problem
of information overload, a form of knowledge re-
duction may be necessary.
Past research (Garfield et al, 1964; Small,
1973) has shown that citations contain a plethora
of latent information available and that much
can be gained by exploiting it. Indeed, there
is a wealth of literature on topic-clustering, e.g.
bibliographic coupling (Kessler, 1963), or co-
citation analysis (Small, 1973). Subsequent re-
search demonstrated that citations could be clus-
tered on their quality, using keywords that ap-
peared in the running-text of the citation (Wein-
stock, 1971; Nanba et al, 2000; Nanba et al,
2004; Teufel et al, 2006).
Similarly, other work has shown the utility in
the IR domain of ranking the relevance of cited pa-
pers by using supplementary index terms extracted
from the content of citations in citing papers,
including methods that search through a fixed
character-length window (O?Connor, 1982; Brad-
shaw, 2003), or that focus solely on the sentence
containing the citation (Ritchie et al, 2008) for
acquiring these terms. A prior case study (Ritchie
et al, 2006) pointed out the challenges in proper
identification of the full span of a citation in run-
ning text and acknowledged that fixed-width win-
dows have their limits. In contrast to this, en-
deavors have been made to extract the entire span
of a citation by using cue-phrases collected and
deemed salient by statistical merit (Nanba et al,
2000; Nanba et al, 2004). This has met in evalua-
tions with some success.
The Cite-Sum system (Kaplan and Tokunaga,
2008) also aims at knowledge reduction through
use of citations. It receives a paper title as a query
and attempts to generate a summary of the paper
by finding citing papers1 and extracting citations
in the running-text that refer to the paper. Before
outputting a summary, it also classifies extracted
citation text, and removes citations with redun-
dant content. Another similar study (Qazvinian
and Radev, 2008) aims at using the content of ci-
tations within citing papers to generate summaries
of fields of research.
It is clear that merit exists behind extraction
of citations in running text. This paper proposes
a new method for performing this task based on
coreference-chains. To evaluate our method we
created a corpus of citations comprised of citing
papers for 4 cited papers. We also analyze some
phenomena of citations that are present in our cor-
pus.
The paper organization is as follows. We first
define terminology, discuss the construction of our
corpus and the results found through its analysis,
and then move on to our proposed method us-
ing coreference-chains. We evaluate the proposed
method by using the constructed corpus, and then
conclude the paper.
1Papers are downloaded automatically from the web.
88
2 Terminology
So that we may dispense with convoluted explana-
tions for the rest of this paper, we introduce several
terms.
An anchor is the string of characters that marks
the occurrence of a citation in the running-text of a
paper, such as ?(Fakeman 2007)? or ?[57]?.2 The
sentence that this anchor resides within is then the
anchor sentence. The citation continues from be-
fore and after this anchor as long as the text con-
tinues to refer to the cited work; this block of text
may span more than a single sentence. We intro-
duce the citation-site, or c-site for short, to rep-
resent this block of text that discusses the cited
work. Since more than once sentence may discuss
the cited work, each of these sentences is called a
c-site sentence. For clarity will also call the an-
chor the c-site anchor henceforth. A citing paper
contains the c-site that refers to the cited paper.
Finally, the reference at the end of the paper pro-
vides details about a c-site anchor (and the c-site).
Figure 1 shows a sample c-site with the c-site
anchor wavy-underlined, and the c-site itself itali-
cized; the non-italicized text is unrelated to the c-
site. The reference for this c-site is also provided
below the dotted line. In all subsequent examples,
the c-site will be in italics and the current place of
emphasis wavy-underlined.
?. . . Our area of interest is plant growth. In past
research (
:::::::
Fakeman
::
et
:::
al.,
::::
2001), the relationship
between sunlight and plant growth was shown to
directly correlate. It was also shown to adhere
to simple equations for deducing this relation-
ship, the equation varying by plant. We propose
a method that . . . ?
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
J. Fakeman: Changing Plant Growth Factors
during Global Warming. In: Proceedings of
SCANLP 2001.
Figure 1: A sample c-site and its reference
3 Corpus Construction and Analysis
We created a corpus comprised of 38 papers citing
4 (cited) papers taken from Computational Lin-
guistics: Special Issue on the Web as Corpus, Vol-
ume 29, Number 3, 2003 as our data set and pre-
processed it to automatically mark c-site anchors
2In practice the anchor does not include brackets, though
the brackets do signal the start/end of the anchor. This is be-
cause multiple anchors may be present at once, e.g. (Fakeman
2007; Noman 2008).
to facilitate the annotation process. The citing pa-
pers were downloaded from CiteSeer-X;3 see Ta-
ble 1 for details.
We then proceeded to manually annotate the
corpus using SLAT (Noguchi et al, 2008), a
browser-based multi-purpose annotation tool. We
devised the following guidelines for annotation.
Since the tool allows for two types of annotation,
namely segments that demarcate a region of text,
and links, that allow an annotator to assign rela-
tionships between them, we created four segment
types and three link types. Segments were used
to mark c-site anchors, c-sites, background infor-
mation (explained presently), and references. We
used the term background information to refer to
any running-text that elaborates on a c-site but is
not strictly part of the c-site itself (refer to Fig-
ure 2 for an example). Even during annotation,
however, we encountered situations that felt am-
biguous, making this a rather contentious issue.
Our corpus had a limited number of background
information annotations, or we would likely have
experienced more issues. That being said, it is at
least important to recognize that such kinds of sup-
plementary content exist (that may not be part of
the c-site but is still beneficial to be included), and
needs to be considered more in the future.
We then linked each c-site to its anchor, each an-
chor to its reference, and any background informa-
tion to the c-site supplemented. We also decided
on annotating entire sentences, even if only part
of a sentence referred to the cited paper. Table 1
outlines our corpus.
Table 1: Corpus composition
Paper ID 1 2 3 4 Total
Citing papers 2 14 15 7 38
C-sites 3 17 18 12 50
C-site sentences 6 27 33 28 94
To our knowledge, this is the first corpus con-
structed in the context of paper summarization re-
lated to collections of citing papers.4
Analysis of the corpus provided some interest-
ing insights, though a larger corpus is required to
confirm the frequency and validity of such phe-
nomena. The more salient discoveries are item-
ized below. These phenomena may also co-occur.
3http://citeseerx.ist.psu.edu
4Though not specific to the task of summarization through
use of c-sites, citation corpora have been constructed in the
past, e.g. (Teufel et al, 2006).
89
Background Information Though not strictly
part of a c-site, background information may need
to be included for the citation to be comprehensi-
ble. Take Figure 2 for example (background infor-
mation is wavy-underlined) for the c-site anchor
?(Resnik & Smith 2003)?. The authors insert their
own research into the c-site (illustrated with wavy-
underlines); this information is important for un-
derstanding the following c-site sentence, but is
not strictly discussing the cited paper. Background
information is thus a form of ?meta-information?
about the c-site.
In well written papers, often the flow of content
is gradual, which can make distinguishing back-
ground information difficult.
?. . .Resnik and his colleagues (Resnik & Smith
2003) proposed a new approach, STRAND,
. . . The databases for parallel texts in several lan-
guages with download tools are available from
the STRAND webpage. Recently they also ap-
plied the same technique for collecting a set of
links to monolingual pages identified as Russian
by http://www.archive.org, and Internet archiv-
ing service.
::
We
:::::
have
:::::::::
evaluated
:::
the
:::::::
Russian
:::::::
database
::::::::
produced
::
by
:::
this
:::::::
method
:::
and
::::::::
identified
:
a
:::::::
number
::
of
::::::
serious
::::::::
problems
:::::
with
::
it. First, it
does not identify the time when the page was
downloaded and stored in the Internet archive
. . . ?
Figure 2: A non-contiguous c-site w/ background
information (from (Sharoff, 2006))
Contiguity C-sites are not necessarily contigu-
ous. We found in fact that authors tend to in-
sert opinions or comments related to their own
work with sentences/clauses in between actual c-
site sentences/clauses, that would be best omitted
from the c-site. In Figure 2 the wavy-underlined
text shows the author?s opinion portion. This cre-
ates problems for cue-phrase based techniques, as
though they detect the sentence following it, they
fail on the opinion sentence. Incorporation of a le-
niency for a gap in such techniques may be pos-
sible, but seems more problematic and likely to
misidentify c-site sentences altogether.
Related/Itemization Authors often list several
works (namely, insert several c-site anchors) in the
same sentence using connectives. The works may
likely be related, and though this may be useful
information for certain tasks, it is important to dif-
ferentiate which material is related to the c-site,
and which is the c-site itself.
In Figure 3 the second sentence discusses both
c-site anchors (and should be included in both
their c-sites); the first sentence, however, contains
two main clauses connected with a connective,
each clause a different c-site (one with the anchor
?[3]? and one with ?[4]?). Sub-clausal analysis is
necessary for resolving issues such as these. For
our current task, however, we annotated only sen-
tences, and so in this example the second c-site
anchor is included in the first.
?. . . STRAND system [4] searches the web for
parallel text
:::
and
:::
[3]
:::::::
extracts
::::::::::
translations
::::
pairs
:::::
among
::::::
anchor
::::
texts
:::::::
pointing
:::::::
together
::
to
:::
the
::::
same
:::::::
webpage. However they all suffered from the lack
of such bilingual resources available on the web
. . . ?
Figure 3: Itemized c-sites partially overlapping
(from (Zhang et al, 2005))
Nesting C-sites may be nested. In Figure 4
the nested citation (?[Lafferty and Zhai 2001,
Lavrenko and Croft 2001]?) should be included in
the parent one (?[Kraaij et al 2002]?). The wavy-
underlined portion shows the sentence needed for
full comprehension of the c-site.
?. . .
::
In
:::::
recent
:::::
years,
:::
the
:::
use
::
of
::::::::
language
::::::
models
::
in
::
IR
:::
has
::::
been
::
a
::::
great
::::::
success
::::::::
[Lafferty
:::
and
::::
Zhai
::::
2001,
::::::::
Lavrenko
::::
and
:::::
Croft
::::::
2001]. It is possible
to extend the approach to CLIR by integrating a
translation model. This is the approach proposed
in [Kraaij et al 2002] . . . ?
Figure 4: Separate c-site anchors does not mean
separate c-sites (from (Nie, 2002))
Aliases Figure 5 demonstrates another issue:
aliasing. The author redefines how they cite the
paper, in this case using the acronym ?K&L?.
?. . . To address the data-sparsity issue, we em-
ployed the technique used in Keller and Lapata
(2003, K&L) to get a more robust approxima-
tion of predicate-argument counts.
::::
K&L use this
technique to obtain frequencies for predicate-
argument bigrams that were unseen in a given
corpus, showing that the massive size of the web
outweighs the noisy and unbalanced nature of
searches performed on it to produce statistics
that correlate well with corpus data . . . ?
Figure 5: C-Site with Aliasing for anchor ?Keller
and Lapata (2003, K&L)? (from (Kehler, 2004))
4 Coreference Chain-based Extraction
Some of the issues found in our corpus, namely
identification of background information, non-
contiguous c-sites, and aliases, show promise of
90
Table 2: Evaluation results for coreference resolution against the MUC-7 formal corpus.
MUC-7 Task Sentence Eval.
System Setting R P F R P F
All Features 35.71 74.71 48.33 36.27 80.49 50.00
w/o SOON STR MATCH 48.35 83.81 61.32 48.35 88.00 62.41
w/o COSINE SIMILARITY 46.70 82.52 59.65 46.70 86.73 60.71
resolution with coreference-chains. This is be-
cause coreference-chains match noun phrases that
appear with other noun phrases to which they re-
fer, a characteristic present in these three cate-
gories. On the other hand, cue-phrases do not
detect any c-site sentence that does not use key-
words (e.g. ?In addition?). In the following sec-
tion we discuss our implementation of a corefer-
ence chain-based extraction technique, and how
we then applied it to the c-site extraction task. An
analysis of the results then follows.
4.1 Training the Coreference Resolver
To create and train our coreference resolver, we
used a combination of techniques as outlined orig-
inally by (Soon et al, 2001) and subsequently
extended by (Ng and Cardie, 2002). Mim-
icking their approaches, we used the corpora
provided for the MUC-7 coreference resolution
task (LDC2001T02, 2001), which includes sets of
newspaper articles, annotated with coreference re-
lations, for both training and testing. They also
outlined a list of features to extract for training
the resolver to recognize the coreference relations.
Specifically, (Soon et al, 2001) established a list
of 12 features that compare a given anaphor with
a candidate antecedent, e.g. gender agreement,
number agreement, both being pronouns, both part
of the same semantic class (i.e. WordNet synset
hyponyms/hypernyms), etc.
For training the resolver, a corpus annotated
with anaphors and their antecedents is processed,
and pairs of anaphor and candidate antecedents are
created so as to have only one positive instance
per anaphor (the annotated antecedent). Negative
examples are created by taking all occurrences of
noun phrases that occur between the anaphor and
its antecedent in the text. The antecedent in these
steps is also always considered to be to the left of,
or preceding, the anaphor; cataphors are not ad-
dressed in this technique.
We implemented, at least minimally, all 12 of
these features, with a few additions of what (Ng
and Cardie, 2002) hand selected as being most
salient for increased performance. We also ex-
tended this list by adding a cosine-similarity met-
ric between two noun phrases; it uses bag-of-
words to create a vector for each noun phrase
(where each word is a term in the vector) to com-
pute their similarity. The intuition behind this is
that noun phrases with more similar surface forms
should be more likely to corefer.
We further optimized string recognition and
plurality detection for handling citation-strings.
See Table 3 for the full list of our features. While
both (Soon et al, 2001) and (Ng and Cardie, 2002)
induced decision trees (C5 and C4.5, respectively)
we opted for using an SVM-based approach in-
stead (Vapnik, 1998; Joachims, 1999). SVMs are
known for being reliable and having good perfor-
mance.
4.2 Evaluating the Coreference Resolver
We ran our trained SVM classifier against the
MUC-7 formal evaluation corpus; the results are
shown in Table 2.
The results using all features listed in Table 3
are inferior to those set forth by (Soon et al,
2001; Ng and Cardie, 2002); likely this is due
to poorer selection of features. Upon analysis, it
seems that half of the misidentified antecedents
were still chosen within the correct sentence and
more than 10% identified the proper antecedent,
but selected the entire noun phrase (when that
antecedent was marked as, for example, only its
head); the majority of these cases involved the
antecedent being only one sentence away from
the anaphor. Since the former seemed suspect of
a partial string matching feature, we decided to
re-run the tests first excluding our implementa-
tion of the SOON STR MATCH feature, and then
our COSINE SIMILARITY feature. The results
for this are shown in Table 2. It can be seen
that using either of the two string comparison fea-
tures works substantially better than with both of
them in tandem, with the COSINE SIMILARITY
feature showing signs of overall better perfor-
mance which is competitive to (Soon et al,
91
Table 3: Features used for coreference resolution.
Feature Possible Values Brief Description (where necessary)
ANAPHOR IS PRONOUN T/F
ANAPHOR IS INDEFINITE T/F
ANAPHOR IS DEMONSTRATIVE T/F
ANTECEDENT IS PRONOUN T/F
ANTECEDENT IS EMBEDDED T/F Boolean indicating if the candidate antecedent is within another
NP.
SOON STR MATCH T/F As per (Soon et al, 2001). Articles and demonstrative pronouns
removed before comparing NPs. If any part of the NP matches
between candidate and anaphor set to true (T); false otherwise.
ALIAS MATCH T/F Creates abbreviations for organizations and proper names in an
attempt to find an alias.
BOTH PROPER NAMES T/F
BOTH PRONOUNS T/F/?
NUMBER AGREEMENT T/F/? Basic morphological rules applied to the words to see if they are
plural.
COSINE SIMILARITY NUM A cosine similarity score between zero and one is applied to the
head words of each NP.
GENDER AGREEMENT T/F/? If the semantic class is Male or Female, use that gender, other-
wise if a salutation is present, or lastly set to Unknown.
SEMANTIC CLASS AGREEMENT T/F/? Followed (Soon et al, 2001) specifications for using basic
WordNet synsets, specifically: Female and Male belonging to
Person, Organization, Location, Date, Time, Money, Percent
belonging to Object. Any other semantic classes mapped to
Unknown.
2001; Ng and Cardie, 2002). We exclude the
SOON STR MATCH feature in the following ex-
periments.
However, the MUC-7 task measures the ability
to identity the proper antecedent from a list of can-
didates; the c-site extraction task is less ambitious
in that it must only identify if a sentence contains
the antecedent, not which noun phrase it is. When
we evaluate our resolver using these loosened con-
ditions it is expected that it will perform better.
To accomplish this we reevaluate the results
from the resolver in a sentence-wise manner; we
group the test instances by anaphor, and then by
sentence. If any noun phrase within the sentence
is marked as positive when there is in fact a pos-
itive noun phrase in the sentence, the sentence is
marked as correct, and incorrect otherwise. The
results in Table 2 for this simplified task show
an increase in recall, and subsequently F-measure.
The numbers for the loosened constraints eval-
uation are counted by sentence; the original is
counted by noun phrase only.
Our system also generates many fewer training
instances than the previous research, which we at-
tribute to a more stringent noun phrase extraction
procedure, but have not investigated thoroughly
yet.
4.3 Application to the c-site extraction task
As outlined above, we used the resolver with the
loosened constraints, namely evaluating the sen-
tence a potential antecedent is in as likely or not,
and not which noun phrase within the sentence is
the actual antecedent. Using this principle as a
base, we devised an algorithm for scanning sen-
tences around a c-site anchor sentence to deter-
mine their likelihood of being part of the c-site.
The algorithm, shown in simplified form in Fig-
ure 6, is described below.
Starting at the beginning of a c-site anchor
sentence AS, scan left-to-right; for every noun
phrase encountered within AS, begin a right-to-
left sentence-by-sentence search; prepend any sen-
tence S containing an antecedent above a certain
likelihood THRESHOLD, until DISTANCE sen-
tences have been scanned and no suitable candi-
date sentences have been found. We set the like-
lihood score to 1.0, tested ad-hoc for best results,
and the distance-threshold to 5 sentences, having
noted in our corpus that no citation is discontinu-
ous by more than 4.
In a similar fashion, the algorithm then pro-
ceeds to scan text following AS; for every noun
phrase NP encountered (moving left-to-right), be-
gin a right-to-left search for a suitable antecedent.
If a sentence is not evaluated above THRESHOLD,
92
Table 4: Evaluation results for c-site extraction w/o background information
Sentence (Micro-average) C-site (Macro-average)
Method R P F R P F
Baseline 1 (anchor sentence) 53.2 100 69.4 74.6 100 85.5
Baseline 2 (random) 75.5 58.2 65.7 87.4 71.2 78.5
Cue-phrases (CP) 64.9 64.9 64.9 84.0 80.9 82.4
Coref-chains (CC)) 64.9 74.4 69.3 81.0 87.2 84.0
CP/CC Union 74.5 58.8 65.7 88.4 75.0 81.1
CP/CC Intersection 55.3 91.2 69.0 76.6 95.7 85.1
set CSITE to AS
pre:
foreach NP in AS
foreach sentence S preceding AS
if DISTANCE > MAX-DIST goto post
if likelihood > THRESHOLD then
set CSITE to S + CSITE
reset DISTANCE
end
end
end
post:
foreach sentence S after AS
foreach NP in S
foreach sentence S2 until S
if DISTANCE > MAX-DIST stop
if S2 has link then
if likelihood > THRESHOLD then
set S2 has link
end
end
end
end
end
Figure 6: Simplified c-site extraction algorithm
using coreference-chains
it will be ignored when the algorithm backtracks
to look for candidate noun phrases for a subse-
quent sentence, thus preserving the coreference-
chain and preventing additional spurious chains.
If more than DISTANCE sentences are scanned
without finding a c-site sentence, the process is
aborted and the collection of sentences returned.
4.4 Experiment Setup
To evaluate our coreference-chain extraction
method we compare it with a cue-phrases tech-
nique (Nanba et al, 2004) and two baselines.
Baseline 1 extracts only the c-site anchor sen-
tence as the c-site; baseline 2 includes sentences
before/after the c-site anchor sentence as part of
the c-site with a 50/50 probability ? it tosses
a coin for each consecutive sentence to decide
its inclusion. We also created two hybrid meth-
ods that combine the results of the cue-phrases
and coreference-chain techniques, one the union
of their results (includes the extracted sentences
of both methods), and the other the intersection
(includes sentences only for which both methods
agree), to measure their mutual compatibility.
The annotated corpus provided the locations of
c-site anchors for the cited paper within the citing
paper?s running-text. We then compared the ex-
tracted c-sites of each method to the c-sites of the
annotated corpus.
4.5 Evaluation
The results of our experiments are presented in Ta-
ble 4. We evaluated each method as follows. Re-
call and precision were measured for a c-site based
on the number of extracted sentences; if an ex-
tracted sentence was annotated as part of the c-site,
it counted as correct, and if an extracted sentence
was not part of a c-site, incorrect; sentences an-
notated as being part of the c-site not extracted by
the method counted as part of the total sentences
for that c-site. As an example, if an annotated c-
site has 3 sentences (including the c-site anchor
sentence), and the evaluated method extracted 2 of
these and 1 incorrect sentence, then the recall for
this c-site using this method would be 2/3, and the
precision 2/(2 + 1).
Since the evaluation is inherently sentence-
based, we provide two averages in Table 4. The
micro-average is for sentences across all c-sites;
in other words, we tallied the correct and incorrect
sentence count for the whole corpus and then di-
vided by the total number of sentences (94). This
average provides a clearer picture on the efficacy
of each method than does the macro-average. The
macro-average was computed per c-site (as ex-
plained above) and then averaged over the total
number of c-sites in the corpus (50).
With the exception of a 3% lead in macro-
average recall, coreference-chains outperform
cue-phrases in every way. We can see a substan-
93
tial difference in micro-average precision (74.4
vs. 64.9), which results in nearly a 5% higher
F-measure. The macro-average precision is also
higher by more than 6%. It matches more and
misses far less. The loss in the macro-average
recall can be attributed to the coreference-chain
method missing one of two sentences for several
c-sites, which would lower its overall recall score;
keep in mind that since in the macro-average all c-
sites are treated equally, even large c-sites in which
the coreference-chain method performs well, such
an advantage will be reduced with averaging and
is therefore misleading.
Baseline 2 performed as expected, i.e. higher
than baseline 1 for recall. Looking only at F-
measures for evaluating performance in this case
is misleading. This is particularly the case because
precision is more important than recall ? we want
accuracy. Coreference-chains achieved a precision
of over 87.2 compared to the 71.2 of baseline 2.
The combined methods also showed promise.
In particular, the intersection method had very
high precision (91.2 and 95.7), and marginally
managed to extract more sentences than base-
line 1. The union method has more conservative
scores.
We also understood from our corpus that only
about half of c-sites were represented by c-site an-
chor sentences. The largest c-site in the corpus
was 6 sentences, and the average 1.8. This means
using the c-site anchor sentence alone excludes on
average about half of the valuable data.
These results are promising, but a larger corpus
is necessary to validate the results presented here.
5 Conclusions and Future Work
The results demonstrate that a coreference-chain-
based approach may be useful to the c-site ex-
traction task. We can also see that there is still
much work to be done. The scores for the hy-
brid methods also indicate potential for a method
that more tightly couples these two tasks, such
as Rhetorical Structure Theory (RST) (Thompson
and Mann, 1987; Marcu, 2000). Though it has
demonstrated superior performance, coreference
resolution is not a light-weight task; this makes
real-time application more difficult than with cue-
phrase-based approaches.
Our plans for future work include the construc-
tion of a larger corpus of c-sites, investigation of
other features for improving our coreference re-
solver, and applying RST to c-site extraction.
Acknowledgments
The authors would like to express appreciation to
Microsoft for their contribution to this research by
selecting it as a recipient of the 2008 WEBSCALE
Grant (Web-Scale NLP 2008, 2008).
References
Shannon Bradshaw. 2003. Reference directed index-
ing: Redeeming relevance for subject search in cita-
tion indexes. In Proceedings of the 7th ECDL, pages
499?510.
Eugene Garfield, Irving H. Sher, and Richard J. Torpie.
1964. The use of citation data in writing the his-
tory of science. Institute for Scientific Information,
Philadelphia, Pennsylvania.
Thorsten Joachims. 1999. Making large-scale sup-
port vector machine learning practical. In Bernhard
Scho?lkopf, Christopher J. C. Burges, and Alexan-
der J. Smola, editors, Advances in kernel methods:
support vector learning, pages 169?184. MIT Press,
Cambridge, MA, USA.
Dain Kaplan and Takenobu Tokunaga. 2008. Sighting
citation sites: A collective-intelligence approach for
automatic summarization of research papers using
c-sites. In ASWC 2008 Workshops Proceedings.
Andrew Kehler. 2004. The (non)utility of predicate-
argument frequencies for pronoun interpretation. In
In: Proceedings of 2004 North American chapter of
the Association for Computational Linguistics an-
nual meeting, pages 289?296.
M. M. Kessler. 1963. Bibliographic coupling be-
tween scientific papers. American Documentation,
14(1):10?25.
LDC2001T02. 2001. Message understanding confer-
ence (MUC) 7.
Daniel Marcu. 2000. The rhetorical parsing of unre-
stricted texts: A surface-based approach. Computa-
tional Linguistics, 26(3):395?448.
Hidetsugu Nanba, Noriko Kando, and Manabu Oku-
mura. 2000. Classification of research papers using
citation links and citation types: Towards automatic
review article generation. In Proceedings of 11th
SIG/CR Workshop, pages 117?134.
Hidetsugu Nanba, Takeshi Abekawa, Manabu Oku-
mura, and Suguru Saito. 2004. Bilingual presri inte-
gration of multiple research paper databases. In Pro-
ceedings of RIAO 2004, pages 195?211, Avignon,
France.
94
Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In Proceedings of the 40th Annual Meeting on Asso-
ciation for Computational Linguistics, pages 104?
111.
J. Nie. 2002. Towards a unified approach to clir and
multilingual ir. In In: Workshop on Cross Language
Information Retrieval: A Research Roadmap in the
25th Annual International ACM SIGIR Conference
on Research and Development in Information Re-
trieval, pages 8?14.
Masaki Noguchi, Kenta Miyoshi, Takenobu Tokunaga,
Ryu Iida, Mamoru Komachi, and Kentaro Inui.
2008. Multiple purpose annotation using SLAT ?
Segment and link-based annotation tool ?. In Pro-
ceedings of 2nd Linguistic Annotation Workshop,
pages 61?64, May.
John O?Connor. 1982. Citing statements: Computer
recognition and use to improve retrieval. Informa-
tion Processing & Management., 18(3):125?131.
Vahed Qazvinian and Dragomir R. Radev. 2008. Sci-
entific paper summarization using citation summary
networks.
Anna Ritchie, Simone Teufel, and Stephen Robertson.
2006. How to find better index terms through cita-
tions. In Proceedings of the Workshop on How Can
Computational Linguistics Improve Information Re-
trieval?, pages 25?32, Sydney, Australia, July. As-
sociation for Computational Linguistics.
Anna Ritchie, Stephen Robertson, and Simone Teufel.
2008. Comparing citation contexts for informa-
tion retrieval. In CIKM ?08: Proceedings of the
17th ACM conference on Information and knowl-
edge management, pages 213?222, New York, NY,
USA. ACM.
Serge Sharoff. 2006. Creating general-purpose cor-
pora using automated search engine queries. In
WaCky! Working papers on the Web as Corpus.
Gedit.
H. Small. 1973. Co-citation in the scientific literature:
A newmeasure of the relationship between two doc-
uments. JASIS, 24:265?269.
Wee Meng Soon, Daniel Chung, Daniel Chung Yong
Lim, Yong Lim, and Hwee Tou Ng. 2001. A
machine learning approach to coreference resolu-
tion of noun phrases. Computational Linguistics,
27(4):521?544.
Simone Teufel, Advaith Siddharthan, and Dan Tidhar.
2006. Automatic classification of citation function.
In In Proceedings of EMNLP-06.
Sandra A. Thompson and William C. Mann. 1987.
Rhetorical structure theory: A framework for the
analysis of texts. Pragmatics, 1(1):79?105.
Vladimir N. Vapnik. 1998. Statistical Learning The-
ory. Adaptive and Learning Systems for Signal Pro-
cessing Communications, and control. JohnWiley &
Sons.
Web-Scale NLP 2008. 2008. http:
//research.microsoft.com/ur/asia/
research/NLP.aspx.
M. Weinstock. 1971. Citation indexes. Encyclopedia
of Library and Information Science, 5:16?41.
Ying Zhang, Fei Huang, and Stephan Vogel. 2005.
Mining translations of oov terms from the web
through. In International Conference on Natural
Language Processing and Knowledge Engineering
(NLP-KE ?03), pages 669?670.
95
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1259?1267,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Incorporating Extra-linguistic Information into Reference Resolution in
Collaborative Task Dialogue
Ryu Iida Shumpei Kobayashi Takenobu Tokunaga
Tokyo Institute of Technology
2-12-1, O?okayama, Meguro, Tokyo 152-8552, Japan
{ryu-i,skobayashi,take}@cl.cs.titech.ac.jp
Abstract
This paper proposes an approach to ref-
erence resolution in situated dialogues
by exploiting extra-linguistic information.
Recently, investigations of referential be-
haviours involved in situations in the real
world have received increasing attention
by researchers (Di Eugenio et al, 2000;
Byron, 2005; van Deemter, 2007; Spanger
et al, 2009). In order to create an accurate
reference resolution model, we need to
handle extra-linguistic information as well
as textual information examined by exist-
ing approaches (Soon et al, 2001; Ng and
Cardie, 2002, etc.). In this paper, we incor-
porate extra-linguistic information into an
existing corpus-based reference resolution
model, and investigate its effects on refer-
ence resolution problems within a corpus
of Japanese dialogues. The results demon-
strate that our proposed model achieves an
accuracy of 79.0% for this task.
1 Introduction
The task of identifying reference relations includ-
ing anaphora and coreferences within texts has re-
ceived a great deal of attention in natural language
processing, from both theoretical and empirical
perspectives. Recently, research trends for refer-
ence resolution have drastically shifted from hand-
crafted rule-based approaches to corpus-based ap-
proaches, due predominately to the growing suc-
cess of machine learning algorithms (such as Sup-
port Vector Machines (Vapnik, 1998)); many re-
searchers have examined ways for introducing var-
ious linguistic clues into machine learning-based
models (Ge et al, 1998; Soon et al, 2001; Ng
and Cardie, 2002; Yang et al, 2003; Iida et al,
2005; Yang et al, 2005; Yang et al, 2008; Poon
and Domingos, 2008, etc.). Research has contin-
ued to progress each year, focusing on tackling the
problem as it is represented in the annotated data
sets provided by the Message Understanding Con-
ference (MUC)1 and the Automatic Content Ex-
traction (ACE)2. In these data sets, coreference re-
lations are defined as a limited version of a typ-
ical coreference; this generally means that only
the relations where expressions refer to the same
named entities are addressed, because it makes
the coreference resolution task more information
extraction-oriented. In other words, the corefer-
ence task as defined by MUC and ACE is geared
toward only identifying coreference relations an-
chored to an entity within the text.
In contrast to this research trend, investigations
of referential behaviour in real world situations
have continued to gain interest in the language
generation community (Di Eugenio et al, 2000;
Byron, 2005; van Deemter, 2007; Foster et al,
2008; Spanger et al, 2009), aiming at applica-
tions such as human-robot interaction. Spanger
et al (2009) for example constructed a corpus by
recording dialogues of two participants collabo-
ratively solving the Tangram puzzle. The corpus
includes extra-lingustic information synchronised
with utterances (such as operations on the puzzle
pieces). They analysed the relations between re-
ferring expressions and the extra-linguistic infor-
mation, and reported that the pronominal usage of
referring expressions is predominant. They also
revealed that the multi-modal perspective of refer-
ence should be dealt with for more realistic refer-
ence understanding. Thus, a challenging issue in
reference resolution is to create a model bridging a
referring expression in the text and its object in the
real world. As a first step, this paper focuses on
incorporating extra-linguistic information into an
existing corpus-based approach, taking Spanger et
al. (2009)?s REX-J corpus3 as the data set. In our
1www-nlpir.nist.gov/related projects/muc/
2www.itl.nist.gov/iad/mig//tests/ace/
3The corpus was named REX-J after their publication of
1259
problem setting, a referent needs to be identified
by taking into account extra-linguistic informa-
tion, such as the spatiala relations of puzzle pieces
and the participants? operations on them, as well
as any preceding utterances in the dialogue. We
particularly focus on the participants? operation of
pieces and so introduce it as several features in a
machine learning-based approach.
This paper is organised as follows. We first ex-
plain the corpus of collaborative work dialogues
in Section 2, and then present our approach for
identifying a referent given a referring expres-
sion in situated dialogues in Section 3. Section 4
shows the results of our empirical evaluation.
In Section 5 we compare our work with exist-
ing work on reference resolution, and then con-
clude this paper and discuss future directions in
Section 6.
2 REX-J corpus: a corpus of
collaborative work dialogue
For investigating dialogue from the multi-modal
perspective, researchers have developed data sets
including extra-linguistic information, bridging
objects in the world and their referring expres-
sions. The COCONUT corpus (Di Eugenio et al,
2000) is collected from keyboard-dialogues be-
tween two participants, who are collaborating on
a simple 2D design task. The setting tends to en-
courage simple types of expressions by the partic-
ipants. The COCONUT corpus is also limited to
annotations with symbolic information about ob-
jects, such as object attributes and location in dis-
crete coordinates. Thus, in addition to the artifi-
cial nature of interaction, such as using keyboard
input, this corpus only records restricted types of
data.
On the other hand, though the annotated corpus
by Spanger et al (2009) focuses on a limited do-
main (i.e. collaborative work dialogues for solving
the Tangram puzzle using a puzzle simulator on
the computer), the required operations to solve the
puzzle, and the situation as it is updated by a series
of operations on the pieces are both recorded by
the simulator. The relationship between a referring
expression in a dialogue and its referent on a com-
puter display is also annotated. For this reason,
we selected the REX-J corpus for use in our em-
pirical evaluations on reference resolution. Before
explaining the details of our evaluation, we sketch
Spanger et al (2009), which describes its construction.
goal shape area
working area
Figure 1: Screenshot of the Tangram simulator
out the REX-J corpus and some of its prominent
statistics.
2.1 The REX-J corpus
In the process of building the REX-J corpus,
Spanger et al (2009) recruited 12 Japanese grad-
uate students (4 females and 8 males), and split
them into 6 pairs. All pairs knew each other previ-
ously and were of the same sex and approximately
the same age. Each pair was instructed to solve
the Tangram puzzle. The goal of the puzzle is to
construct a given shape by arranging seven pieces
of simple figures as shown in Figure 1. The pre-
cise position of every piece and every action that
the participants make are recorded by the Tangram
simulator in which the pieces on the computer dis-
play can be moved, rotated and flipped with sim-
ple mouse operations. The piece position and the
mouse actions were recorded at intervals of 10
msec. The simulator displays two areas: a goal
shape area (the left side of Figure 1) and a work-
ing area (the right side of Figure 1) where pieces
are shown and can be manipulated.
A different role was assigned to each participant
of a pair: a solver and an operator. Given a cer-
tain goal shape, the solver thinks of the necessary
arrangement of the pieces and gives instructions
to the operator for how to move them. The op-
erator manipulates the pieces with the mouse ac-
cording to the solver?s instructions. During this
interaction, frequent uttering of referring expres-
sions are needed to distinguish the pieces of the
puzzle. This collaboration is achieved by placing
a set of participants side by side, each with their
own display showing the work area, and a shield
screen set between them to prevent the operator
from seeing the goal shape, which is visible only
on the solver?s screen, and to further restrict their
1260
interaction to only speech.
2.2 Statistics
Table 1 lists the syntactic and semantic features of
the referring expressions in the corpus with their
respective frequencies. Note that multiple fea-
tures can be used in a single expression. This list
demonstrates that ?pronoun? and ?shape? features
are frequently uttered in the corpus. This is be-
cause pronominal expressions are often used for
pointing to a piece on a computer display. Expres-
sions representing ?shape? frequently appear in di-
alogues even though they may be relatively redun-
dant in the current utterance. From these statistics,
capturing these two features can be judged as cru-
cial as a first step toward accurate reference reso-
lution.
3 Reference Resolution using
Extra-linguistic Information
Before explaining the treatment of extra-linguistic
information, let us first describe the task defini-
tion, taking the REX-J corpus as target data. In
the task of reference resolution, the reference res-
olution model has to identify a referent (i.e. a
piece on a computer display)4. In comparison to
conventional problem settings for anaphora reso-
lution, where the model searches for an antecedent
out of a set of candidate antecedents from pre-
ceding utterances, expressions corresponding to
antecedents are sometimes omitted because refer-
ring expressions are used as deixis (i.e. physically
pointing to a piece on a computer display); they
may also refer to a piece that has just been manip-
ulated by an operator due to the temporal salience
in a series of operations. For these reasons, even
though the model checks all candidates in the pre-
ceding utterances, it may not find the antecedent
of a given referring expression. However, we do
know that each referent exists as a piece on the
display. We can therefore establish that when a re-
ferring expression is uttered by either a solver or
an operator, the model can choose one of seven
pieces as a referent of the current referring expres-
sion.
3.1 Ranking model to identify referents
To investigate the impact of extra-linguistic infor-
mation on reference resolution, we conduct an em-
4In the current task on reference resolution, we deal only
with referring expressions referring to a single piece to min-
imise complexity.
pirical evaluation in which a reference resolution
model chooses a referent (i.e. a piece) for a given
referring expression from the set of pieces illus-
trated on the computer display.
As a basis for our reference resolution model,
we adopt an existing model for reference res-
olution. Recently, machine learning-based ap-
proaches to reference resolution (Soon et al, 2001;
Ng and Cardie, 2002, etc.) have been developed,
particularly focussing on identifying anaphoric re-
lations in texts, and have achieved better perfor-
mance than hand-crafted rule-based approaches.
These models for reference resolution take into ac-
count linguistic factors, such as relative salience of
candidate antecedents, which have been modeled
in Centering Theory (Grosz et al, 1995) by rank-
ing candidate antecedents appearing in the preced-
ing discourse (Iida et al, 2003; Yang et al, 2003;
Denis and Baldridge, 2008). In order to take ad-
vantage of existing models, we adopt the ranking-
based approach as a basis for our reference resolu-
tion model.
In conventional ranking-based models, Yang et
al. (2003) and Iida et al (2003) decompose the
ranking process into a set of pairwise compar-
isons of two candidate antecedents. However, re-
cent work by Denis and Baldridge (2008) reports
that appropriately constructing a model for rank-
ing all candidates yields improved performance
over those utilising pairwise ranking.
Similarly we adopt a ranking-based model, in
which all candidate antecedents compete with
one another to decide the most likely candi-
date antecedent. Although the work by Denis
and Baldridge (2008) uses Maximum Entropy to
create their ranking-based model, we adopt the
Ranking SVM algorithm (Joachims, 2002), which
learns a weight vector to rank candidates for a
given partial ranking of each referent. Each train-
ing instance is created from the set of all referents
for each referring expression. To define the par-
tial ranking of referents, we simply rank referents
referred to by a given referring expression as first
place and other referents as second place.
3.2 Use of extra-linguistic information
Recent work on multi-modal reference resolution
or referring expression generation (Prasov and
Chai, 2008; Foster et al, 2008; Carletta et al,
2010) indicates that extra-linguistic information,
such as eye-gaze and manipulation of objects, is
1261
Table 1: Referring expressions in REX-J corpus
feature tokens example
demonstratives 742
adjective 194 ?ano migigawa no sankakkei (that triangle at the right side)?
pronoun 548 ?kore (this)?
attribute 795
size 223 ?tittyai sankakkei (the small triangle)?
shape 566 ?o?kii sankakkei (the large triangle)?
direction 6 ?ano sita muiteru dekai sankakkei (that large triangle facing to the bottom)?
spatial relations 147
projective 143 ?hidari no okkii sankakkei (the small triangle on the left)?
topological 2 ?o?kii hanareteiru yatu (the big distant one)?
overlapping 2 ? sono sita ni aru sankakkei (the triangle underneath it)?
action-mentioning 85 ?migi ue ni doketa sankakkei (the triangle you put away to the top right)?
one of essential clues for distinguishing deictic
reference from endophoric reference.
For instance, Prasov and Chai (2008) demon-
strated that integrating eye-gaze information (es-
pecially, relative fixation intensity, the amount of
time spent fixating a candidate object) into the
conventional dialogue history-based model im-
proved the performance of reference resolution.
Foster et al (2008) investigated the relationship of
referring expressions and the manupluation of ob-
jects on a collaborative construction task, which
is similar to our Tangram task5. They reported
about 36% of the initial mentioned referring ex-
pressions in their corpus were involved with par-
ticipant?s operations of objects, such as mouse ma-
nipulation.
From these background, in addition to the in-
formation about the history of the preceding dis-
course, which has been used in previous machine
learning-based approaches, we integrate extra-
linguistic information into the reference resolution
model shown in Section 3.1. More precisely, we
introduce the following extra-linguistic informa-
tion: the information with regards to the history
of a piece?s movement and the mouse cursor po-
sitions, and the information of the piece currently
manipulated by an operator. We next elaborate on
these three kinds of features. All the features are
summarised in Table 2.
3.2.1 Discourse history features
First, ?type of? features are acquired from the ex-
pressions of a given referring expression and its
antecedent in the preceding discourse if the an-
5Note that the task defined in Foster et al (2008) makes no
distinction between two roles; a operator and a solver. Thus,
two partipants both can mamipulate pieces on a computer dis-
play, but need to jointly construct to create a predefined goal
shape.
tecedent explicitly appears. These features have
been examined by approaches to anaphora or
coreference resolution (Soon et al, 2001; Ng and
Cardie, 2002, etc.) to capture the salience of a can-
didate antecedent. To capture the textual aspect
of dialogues for solving Tangram puzzle, we ex-
ploit the features such as a binary value indicating
whether a referring expression has no antecedent
in the preceding discourse and case markers fol-
lowing a candidate antecedent.
3.2.2 Action history features
The history of the operations may yield important
clues that indicate the salience in terms of the tem-
poral recency of a piece within a series of opera-
tions. To introduce this aspect as a set of features,
we can use, for example, the time distance of a
candidate referent (i.e. a piece in the Tangram puz-
zle) since the mouse cursor was moved over it. We
call this type of feature the action history feature.
3.2.3 Current operation features
The recency of operations of a piece is also an im-
portant factor on reference resolution because it is
directly associated with the focus of attention in
terms of the cognition in a series of operations.
For example, since a piece which was most re-
cently manipulated is most salient from cognitive
perspectives, it might be expected that the piece
tends to be referred to by unmarked referring ex-
pressions such as pronouns. To incorporate such
clues into the reference resolution model, we can
use, for example, the time distance of a candidate
referent since it was last manipulated in the pre-
ceding utterances. We call this type of feature the
current operation feature.
1262
Table 2: Feature set
(a) Discourse history features
DH1 : yes, no a binary value indicating that P is referred to by the most recent referring expression.
DH2 : yes, no a binary value indicating that the time distance to the last mention of P is less than or equal to 10 sec.
DH3 : yes, no a binary value indicating that the time distance to the last mention of P is more than 10 sec and less
than or equal to 20 sec.
DH4 : yes, no a binary value indicating that the time distance to the last mention of P is more than 20 sec.
DH5 : yes, no a binary value indicating that P has never been referred to by any mentions in the preceding utterances.
DH6 : yes, no, N/A a binary value indicating that the attributes of P are compatible with the attributes of R.
DH7 : yes, no a binary value indicating that R is followed by the case marker ?o (accusative)?.
DH8 : yes, no a binary value indicating that R is followed by the case marker ?ni (dative)?.
DH9 : yes, no a binary value indicating that R is a pronoun and the most recent reference to P is not a pronoun.
DH10 : yes, no a binary value indicating that R is not a pronoun and was most recently referred to by a pronoun.
(b) Action history features
AH1 : yes, no a binary value indicating that the mouse cursor was over P at the beginning of uttering R.
AH2 : yes, no a binary value indicating that P is the last piece that the mouse cursor was over when feature AH1 is
?no?.
AH3 : yes, no a binary value indicating that the time distance is less than or equal to 10 sec after the mouse cursor
was over P.
AH4 : yes, no a binary value indicating that the time distance is more than 10 sec and less than or equal to 20 sec
after the mouse cursor was over P.
AH5 : yes, no a binary value indicating that the time distance is more than 20 sec after the mouse cursor was over P.
AH6 : yes, no a binary value indicating that the mouse cursor was never over P in the preceding utterances.
(c) Current operation features
CO1 : yes, no a binary value indicating that P is being manipulated at the beginning of uttering R.
CO2 : yes, no a binary value indicating that P is the most recently manipulated piece when feature CO1 is ?no?.
CO3 : yes, no a binary value indicating that the time distance is less than or equal to 10 sec after P was most recently
manipulated.
CO4 : yes, no a binary value indicating that the time distance is more than 10 sec and less than or equal to 20 sec
after P was most recently manipulated.
CO5 : yes, no a binary value indicating that the time distance is more than 20 sec after P was most recently manipu-
lated.
CO6 : yes, no a binary value indicating that P has never been manipulated.
P stands for a piece of the Tangram puzzle (i.e. a candidate referent of a referring expression) and R stands for the target
referring expression.
4 Empirical Evaluation
In order to investigate the effect of the extra-
linguistic information introduced in this paper, we
conduct an empirical evaluation using the REX-J
corpus.
4.1 Models
As we see in Section 2.2, the feature testing
whether a referring expression is a pronoun or
not is crucial because it is directly related to the
?deictic? usage of referring expressions, whereas
other expressions tend to refer to an expression ap-
pearing in the preceding utterances. As described
in Denis and Baldridge (2008), when the size of
training instances is relatively small, the models
induced by learning algorithms (e.g. SVM) should
be separately created with regards to distinct fea-
tures. Therefore, focusing on the difference of
the pronominal usage of referring expressions, we
separately create the reference resolution models;
one is for identifying a referent of a given pro-
noun, and the other is for all other expressions.
We henceforth call the former model the pronoun
model and the latter one the non-pronoun model
respectively. At the training phase, we use only
training instances whose referring expressions are
pronouns for creating the pronoun model, and
all other training instances are used for the non-
pronoun model. The model using one of these
models depending on the referring expression to
be solved is called the separate model.
To verify Denis and Baldridge (2008)?s premise
mentioned above, we also create a model using all
training instances without dividing pronouns and
other. This model is called the combined model
hereafter.
4.2 Experimental setting
We used 40 dialogues in the REX-J corpus6, con-
taining 2,048 referring expressions. To facilitate
the experiments, we conduct 10-fold crossvalida-
tion using 2,035 referring expressions, each of
which refers to a single piece in a computer dis-
6Spanger et al (2009)?s original corpus contains only 24
dialogues. In addition to this, we obtained anothor 16 dia-
logues by favour of the authors.
1263
Table 3: Results on reference resolution: accuracy
model discourse history +action history* +current operation +action history,
(baseline) +current operation*
separated model (a+b) 0.664 (1352/2035) 0.790 (1608/2035) 0.685 (1394/2035) 0.780 (1587/2035)
a) pronoun model 0.648 (660/1018) 0.886 (902/1018) 0.692 (704/1018) 0.875 (891/1018)
b) non-pronoun model 0.680 (692/1017) 0.694 (706/1017) 0.678 (690/1017) 0.684 (696/1017)
combined model 0.664 (1352/2035) 0.749 (1524/2035) 0.650 (1322/2035) 0.743 (1513/2035)
?*? means the extra-lingustic features (or the combinations of them) significantly contribute to improving performance. For the
significant tests, we used McNemar test with Bonferroni?s correction for multiple comparisons, i.e. ?/K = 0.05/4 = 0.01.
play7.
As a baseline model, we adopted a model only
using the discourse history features. We utilised
SVMrank8 as an implementation of the Ranking
SVM algorithm, in which the parameter c was set
as 1.0 and the remaining parameters were set to
their defaults.
4.3 Results
The results of each model are shown in Table 3.
First of all, by comparing the models with and
without extra-linguistic information (i.e. the
model using all features shown in Table 2 and
the baseline model), we can see the effectiveness
of extra-linguistic information. The results typi-
cally show that the former achieved better perfor-
mance than the latter. In particular, it indicates that
exploiting the action history features are signifi-
cantly useful for reference resolution in this data
set.
Second, we can also see the impact of extra-
linguistic information (especially, the action his-
tory features) with regards to the pronoun and
non-pronoun models. In the former case, the
model with extra-linguistic information improved
by about 22% compared with the baseline model.
On the other hand, in the latter case, the accuracy
improved by only 7% over the baseline model.
The difference may be caused by the fact that pro-
nouns are more sensitive to the usage of the ac-
tion history features because pronouns are often
uttered as deixis (i.e. a pronoun tends to directly
refer to a piece shown in a computer display).
The results also show that the model using
the discourse history and action history features
achieved better performance than the model using
all the features. This may be due to the duplicated
definitions between the action history and current
7The remaining 13 instances referred to either more than
one piece or a class of pieces, thus were excluded in this ex-
periment.
8www.cs.cornell.edu/people/tj/svm light/svm rank.html
Table 4: Weights of the features in each model
pronoun model non-pronoun model
rank feature weight feature weight
1 AH1 0.6371 DH6 0.7060
2 AH3 0.2721 DH2 0.2271
3 DH1 0.2239 AH3 0.2035
4 DH2 0.2191 AH1 0.1839
5 CO1 0.1911 DH1 0.1573
6 DH9 0.1055 DH7 0.0669
7 AH2 0.0988 CO5 0.0433
8 CO3 0.0852 CO3 0.0393
9 DH6 0.0314 CO1 0.0324
10 CO2 0.0249 DH3 0.0177
11 DH10 0 AH4 0.0079
12 DH7 -0.0011 AH2 0.0069
13 DH3 -0.0088 CO4 0.0059
14 CO6 -0.0228 DH10 0.0059
15 CO4 -0.0308 DH9 0
16 CO5 -0.0317 CO2 -0.0167
17 DH8 -0.0371 DH8 -0.0728
18 AH6 -0.0600 CO6 -0.0885
19 AH4 -0.0761 DH4 -0.0924
20 DH5 -0.0910 AH5 -0.1042
21 DH4 -0.1193 AH6 -0.1072
22 AH5 -0.1361 DH5 -0.1524
operation features. As we can see in the feature
definitions of CO1 and AH1, some current opera-
tion features partially overlap with the action his-
tory features, which is effectively used in the rank-
ing process. However, the other current operation
features may have bad effects for ranking refer-
ents due to their ill-formed definitions. To shed
light on this problem, we need additional investi-
gation of the usage of features, and to refine their
definitions.
Finally, the results show that the performance
of the separated model is significantly better than
that of the combined model9, which indicates that
separately creating models to specialise in distinct
factors (i.e. whether a referring expression is a
pronoun or not) is important as suggested by Denis
and Baldridge (2008).
We next investigated the significance of each
9For the significant tests, we used McNemar test (? =
0.05).
1264
Table 5: Frequencies of REs relating to on-mouse
pronouns others total
# all REs 548 693 1,241
# on-mouse 452 155 607
(82.5%) (22.4%) (48.9%)
?# all REs? stands for the frequency of referring expressions
uttered in the corpus and ?# on-mouse? is the frequency of re-
ferring expressions in the situation when a referring expres-
sion is uttered and a mouse cursor is over the piece referred
to by the expression.
feature of the pronoun and non-pronoun models.
We calculate the weight of feature f shown in
Table 2 according to the following formula.
weight(f) =
?
x?SV s
wxzx(f) (1)
where SVs is a set of the support vectors in a ranker
induced by SVMrank, wx is the weight of the sup-
port vector x, zx(f) is the function that returns 1
if f occurs in x, respectively.
The feature weights are shown in Table 4. This
demonstrates that in the pronoun model the ac-
tion history features have the highest weight, while
with the non-pronoun model these features are less
significant. As we can see in Table 5, pronouns
are strongly related to the situation where a mouse
cursor is over a piece, directly causing the weights
of the features associated with the ?on-mouse? sit-
uation to become higher than other features.
On the other hand, in the non-pronoun model,
the discourse history features, such as DH6 and
DH2, are the most significant, indicating that the
compatibility of the attributes of a piece and a re-
ferring expression is more crucial than other ac-
tion history and current operation features. This is
compatible with the previous research concerning
textual reference resolution (Mitkov, 2002).
Table 4 shows that feature AH3 (aiming at cap-
turing the recency in terms of a series of oper-
ations) is also significant. It empirically proves
that the recent operation is strongly related to the
salience of reference as a kind of ?focus? by hu-
mans.
5 Related Work
There have been increasing concerns about ref-
erence resolution in dialogue. Byron and Allen
(1998) and Eckert and Strube (2000) reported
about 50% of pronouns had no antecedent in
TRAINS93 and Switchboard corpora respectively.
Strube and Mu?ller (2003) attempted to resolve
pronominal anaphora in the Switchboard corpus
by porting a corpus-based anaphora resolution
model focusing on written texts (e.g. Soon et al
(2001) and Ng and Cardie (2002)). They used
specialised features for spoken dialogues as well
as conventional features. They reported relatively
worse results than with written texts. The reason
is that the features in their work capture only in-
formation derived from transcripts of dialogues,
while it is also essential to bridge objects and con-
cepts in the real (or virtual) world and their expres-
sions (especially pronouns) for recognising refer-
ential relations intrinsically.
To improve performance on reference resolu-
tion in dialogue, researchers have focused on
anaphoricity determination, which is the task of
judging whether an expression explicitly has an
antecedent in the text (i.e. in the preceding ut-
terances) (Mu?ller, 2006; Mu?ller, 2007). Their
work presented implementations of pronominal
reference resolution in transcribed, multi-party di-
alogues. Mu?ller (2006) focused on the determina-
tion of non-referential it, categorising instances of
it in the ICSI Meeting Corpus (Janin et al, 2003)
into six classes in terms of their grammatical cat-
egories. They also took into account each charac-
teristic of these types by using a refined feature set.
In the work by Mu?ller (2007), they conducted an
empirical evaluation including antecedent identifi-
cation as well as anaphoricity determination. They
used the relative frequencies of linguistic patterns
as clues to introduce specific patterns for non-
referentials. They reported that their performance
for detecting non-referentials was relatively high
(80.0% in precision and 60.9% in recall), while
the overall performance was still low (18.2% in
precision and 19.1% in recall). These results indi-
cate the need for advancing research in reference
resolution in dialogue.
In contrast to the above mentioned research, our
task includes the treatment of entity disambigua-
tion (i.e. selecting a referent out of a set of pieces
on a computer display) as well as conventional
anaphora resolution. Although our task setting is
limited to the problem of solving the Tangram puz-
zle, we believe it is a good starting point for incor-
porating real (or virtual) world entities into coven-
tional anaphora resolution.
1265
6 Conclusion
This paper presented the task of reference reso-
lution bridging pieces in the real world and their
referents in dialogue. We presented an imple-
mentation of a reference resolution model ex-
ploiting extra-linguistic information, such as ac-
tion history and current operation features, to cap-
ture the salience of operations by a participant
and the arrangement of the pieces. Through our
empirical evaluation, we demonstrated that the
extra-linguistic information introduced in this pa-
per contributed to improving performance. We
also analysed the effect of each feature, showing
that while action history features were useful for
pronominal reference, discourse history features
made sense for the other references.
In order to enhance this kind of reference res-
olution, there are several possible future direc-
tions. First, in the current problem setting, we
exclude zero-anaphora (i.e. omitted expressions
refer to either an expression in the previous utter-
ances or an object on a display deictically). How-
ever, zero-anaphora is essential for precise mod-
eling and recognition of reference because it is
also directly related with the recency of referents,
either textually or situationally. Second, repre-
senting distractors in a reference resolution model
is also a key. Although, this paper presents an
implementation of a reference model considering
only the relationship between a referring expres-
sion and its candidate referents. However, there
might be cases when the occurrence of expressions
or manipulated pieces intervening between a refer-
ring expression and its referent need to be taken
into account. Finally, more investigation is needed
for considering other extra-linguistic information,
such as eye-gaze, for exploring what kinds of in-
formation is critical to recognising reference in di-
alogue.
References
D. K. Byron and J. F. Allen. 1998. Resolving demon-
strative pronouns in the trains93 corpus. In Proceed-
ings of the 2nd Colloquium on Discourse Anaphora
and Anaphor Resolution (DAARC2), pages 68?81.
D. K. Byron. 2005. Utilizing visual attention for
cross-model coreference interpretation. In CON-
TEXT 2005, pages 83?96.
J. Carletta, R. L. Hill, C. Nicol, T. Taylor, J. P.
de Ruiter, and E. G. Bard. 2010. Eyetracking
for two-person tasks with manipulation of a virtual
world. Behavior Research Methods, 42:254?265.
P. Denis and J. Baldridge. 2008. Specialized models
and ranking for coreference resolution. In Proceed-
ings of the 2008 Conference on Empirical Methods
in Natural Language Processing, pages 660?669.
B. P. W. Di Eugenio, R. H. Thomason, and J. D. Moore.
2000. The agreement process: An empirical investi-
gation of human-human computer-mediated collab-
orative dialogues. International Journal of Human-
Computer Studies, 53(6):1017?1076.
M. Eckert and M. Strube. 2000. Dialogue acts, syn-
chronising units and anaphora resolution. Journal
of Semantics, 17(1):51?89.
M. E. Foster, E. G. Bard, M. Guhe, R. L. Hill, J. Ober-
lander, and A. Knoll. 2008. The roles of haptic-
ostensive referring expressions in cooperative, task-
based human-robot dialogue. In Proceedings of the
3rd ACM/IEEE international conference on Human
robot interaction (HRI ?08), pages 295?302.
N. Ge, J. Hale, and E. Charniak. 1998. A statistical ap-
proach to anaphora resolution. In Proceedings of the
6th Workshop on Very Large Corpora, pages 161?
170.
B. J. Grosz, A. K. Joshi, and S. Weinstein. 1995.
Centering: A framework for modeling the local co-
herence of discourse. Computational Linguistics,
21(2):203?226.
R. Iida, K. Inui, H. Takamura, and Y. Matsumoto.
2003. Incorporating contextual cues in trainable
models for coreference resolution. In Proceedings
of the 10th EACL Workshop on The Computational
Treatment of Anaphora, pages 23?30.
R. Iida, K. Inui, and Y. Matsumoto. 2005. Anaphora
resolution by antecedent identification followed by
anaphoricity determination. ACM Transactions on
Asian Language Information Processing (TALIP),
4(4):417?434.
A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart,
N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stol-
cke, and C. Wooters. 2003. The ICSI meeting cor-
pus. In Proceedings of the IEEE International Con-
ference on Acoustics, Speech and Signal Processing,
pages 364?367.
T. Joachims. 2002. Optimizing search engines using
clickthrough data. In Proceedings of the ACM Con-
ference on Knowledge Discovery and Data Mining
(KDD), pages 133?142.
R. Mitkov. 2002. Anaphora Resolution. Studies in
Language and Linguistics. Pearson Education.
C. Mu?ller. 2006. Automatic detection of nonrefer-
ential It in spoken multi-party dialog. In Proceed-
ings of the 11th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 49?56.
1266
C. Mu?ller. 2007. Resolving It, This, and That in un-
restricted multi-party dialog. In Proceedings of the
45th Annual Meeting of the Association of Compu-
tational Linguistics, pages 816?823.
V. Ng and C. Cardie. 2002. Improving machine learn-
ing approaches to coreference resolution. In Pro-
ceedings of the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
104?111.
H. Poon and P. Domingos. 2008. Joint unsupervised
coreference resolution with Markov Logic. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 650?
659.
Z. Prasov and J. Y. Chai. 2008. What?s in a gaze?:
the role of eye-gaze in reference resolution in mul-
timodal conversational interfaces. In Proceedings of
the 13th international conference on Intelligent user
interfaces (IUI ?08), pages 20?29.
W. M. Soon, H. T. Ng, and D. C. Y. Lim. 2001. A
machine learning approach to coreference resolu-
tion of noun phrases. Computational Linguistics,
27(4):521?544.
P. Spanger, Y. Masaaki, R. Iida, and T. Takenobu.
2009. Using extra linguistic information for gen-
erating demonstrative pronouns in a situated collab-
oration task. In Proceedings of Workshop on Pro-
duction of Referring Expressions: Bridging the gap
between computational and empirical approaches to
reference.
M. Strube and C. Mu?ller. 2003. A machine learning
approach to pronoun resolution in spoken dialogue.
In Proceedings of the 41st Annual Meeting of the As-
sociation for Computational Linguistics, pages 168?
175.
K. van Deemter. 2007. TUNA: Towards a unified al-
gorithm for the generation of referring expressions.
Technical report, Aberdeen University.
V. N. Vapnik. 1998. Statistical Learning Theory.
Adaptive and Learning Systems for Signal Process-
ing Communications, and control. John Wiley &
Sons.
X. Yang, G. Zhou, J. Su, and C. L. Tan. 2003.
Coreference resolution using competition learning
approach. In Proceedings of the 41st Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 176?183.
X. Yang, J. Su, and C. L. Tan. 2005. Improving pro-
noun resolution using statistics-based semantic com-
patibility information. In Proceeding of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 165?172.
X. Yang, J. Su, J. Lang, C. L. Tan, T. Liu, and S. Li.
2008. An entity-mention model for coreference
resolution with inductive logic programming. In
Proceedings of Annual Meeting of the Association
for Computational Linguistics (ACL): Human Lan-
guage Technologies (HLT), pages 843?851.
1267
Proceedings of the 8th Workshop on Asian Language Resources, pages 38?46,
Beijing, China, 21-22 August 2010. c?2010 Asian Federation for Natural Language Processing
Construction of bilingual multimodal corpora of referring expressions in
collaborative problem solving
TOKUNAGA Takenobu IIDA Ryu YASUHARA Masaaki TERAI Asuka
{take,ryu-i,yasuhara}@cl.cs.titech.ac.jp asuka@nm.hum.titech.ac.jp
Tokyo Institute of Technology
David MORRIS Anja BELZ
D.Morris@brighton.ac.uk a.s.belz@itri.brighton.ac.uk
University of Brighton
Abstract
This paper presents on-going work on
constructing bilingual multimodal corpora
of referring expressions in collaborative
problem solving for English and Japanese.
The corpora were collected from dia-
logues in which two participants collab-
oratively solved Tangram puzzles with
a puzzle simulator. Extra-linguistic in-
formation such as operations on puzzle
pieces, mouse cursor position and piece
positions were recorded in synchronisa-
tion with utterances. The speech data
was transcribed and time-aligned with the
extra-linguistic information. Referring
expressions in utterances that refer to puz-
zle pieces were annotated in terms of their
spans, their referents and their other at-
tributes. The Japanese corpus has already
been completed, but the English counter-
part is still undergoing annotation. We
have conducted a preliminary comparative
analysis of both corpora, mainly with re-
spect to task completion time, task suc-
cess rates and attributes of referring ex-
pressions. These corpora showed signif-
icant differences in task completion time
and success rate.
1 Introduction
A referring expression (RE) is a linguistic de-
vice that refers to a certain object of interest (e.g.
used in describing where the object is located in
space). REs have attracted a great deal of atten-
tion in both language analysis and language gen-
eration research. In language analysis research,
reference resolution, particularly anaphora resolu-
tion (Mitkov, 2002), has a long research history as
far back as the mid-1970s (Hobbs, 1978). Much
research has been conducted from both theoretical
and empirical perspectives, mainly concerning the
identification of antecedents or entities mentioned
within the same text. This trend, targeting refer-
ence resolution in written text, is still dominant in
the language analysis, perhaps because such tech-
niques are intended for use in applications such as
information extraction.
In contrast, in language generation research in-
terest has recently shifted from the generation of
one-off references to entities to generation of REs
in discourse context (Belz et al, 2010) and inves-
tigating human referential behaviour in real world
situations, with the aim of using such techniques
in applications like human-robot interaction (Pi-
wek, 2007; Foster et al, 2008; Bard et al, 2009).
In both analysis and generation, machine-
learning approaches have come to replace rule-
based approaches as the predominant research
trend since the 1990s. This trend has made anno-
tated corpora an indispensable component of re-
search for training and evaluating proposed meth-
ods. In fact, research on reference resolution has
developed significantly as a result of large scale
corpora, e.g. those provided by the Message Un-
derstanding Conference (MUC)1 and the Auto-
matic Content Extraction (ACE)2 project. These
corpora were constructed primarily for informa-
tion extraction research, thus were annotated with
co-reference relations within texts. Also in the
language generation community, several corpora
1http://www.nlpir.nist.gov/related projects/muc/
2http://www.itl.nist.gov/iad/tests/ace/
38
have been developed (Di Eugenio et al, 2000; By-
ron, 2005; van Deemter et al, 2006; Foster and
Oberlander, 2007; Foster et al, 2008; Stoia et al,
2008; Spanger et al, 2009a; Belz et al, 2010).
Unlike the corpora of MUC and ACE, many are
collected from situated dialogues, and therefore
include multimodal information (e.g. gestures and
eye-gaze) other than just transcribed text (Martin
et al, 2007). Foster and Oberlander (2007) em-
phasised that any corpus for language generation
should include all possible contextual information
at the appropriate granularity. Since constructing
a dialogue corpus generally requires experiments
for data collection, this kind of corpus tends to be
small-scale compared with corpora for reference
resolution.
Against this background, we have been de-
veloping multimodal corpora of referring expres-
sions in collaborative problem-solving settings.
This paper presents on-going work of construct-
ing bilingual (English and Japanese) comparable
corpora in this domain. We achieve our goal by
replicating, for the English corpus, the same pro-
cess of data collection and annotation as we used
for our existing Japanese corpus (Spanger et al,
2009a). Our aim is to create bilingual multimodal
corpora collected from dialogues in dynamic situ-
ations. From the point of view of reference anal-
ysis, our corpora contribute to augmenting the re-
sources of multimodal dialogue corpora annotated
with reference relations which have been minor
in number compared to other types of text cor-
pora. From the point of view of reference gen-
eration, our corpora contribute to increasing the
resources available that can be used to further re-
search of this kind. In addition, our corpora con-
tribute to comparative studies of human referential
behaviour in different languages
The structure of the paper is as follows. Sec-
tion 2 describes the experimental set-up for data
collection which was introduced in our previous
work (Spanger et al, 2009a). The setting is basi-
cally the same for the construction of the English
corpus. Section 3 explains the annotation scheme
adopted in our corpora, followed by a description
of a preliminary analysis of the corpora in sec-
tion 4. Section 5 briefly mentions related work
to highlight the characteristics of our corpora. Fi-
nally, Section 6 concludes the paper and looks at
possible future directions.
!"#$%&'#()
*"+,-.!%#+)#
Figure 1: Screenshot of the Tangram simulator
2 Data collection
2.1 Experimental set-up
We recruited subjects in pairs of friends and col-
leagues. Each pair was instructed to solve Tan-
gram puzzles collaboratively. Tangram puzzles
are geometrical puzzles that originated in ancient
China. The goal of a Tangram puzzle is to con-
struct a given goal shape by arranging seven sim-
ple shapes, as shown in Figure 1. The pieces in-
clude two large triangles, a medium-sized trian-
gle, two small triangles, a parallelogram and a
square.
With the aim of recording the precise position
of every piece and every action the participants
made during the solving process, we implemented
a Tangram simulator in which the pieces can be
moved, rotated and flipped with simple mouse op-
erations on a computer display. The simulator dis-
plays two areas: a goal shape area and a work-
ing area where the pieces can be manupulated and
their movements are shown in real time.
We assigned a different role to each participant
of a pair: one acted as the solver and the other as
the operator. The operator has a mouse for manip-
ulating Tangram pieces, but does not have a goal
shape on the screen. The solver has a goal shape
on the screen but does not have a mouse. This set-
ting naturally leads to a situation where given a
certain goal shape, the solver thinks of the neces-
sary arrangement of the pieces and gives instruc-
tions to the operator how to move them, while the
operator manipulates the pieces with the mouse
39
according to the solver?s instructions.
Figure 2: Picture of the experiment setting
As we mentioned in our previous
study (Spanger et al, 2009a), this interaction
produces frequent use of referring expressions
intended to distinguish specific pieces of the
puzzle. In our Tangram simulator, all pieces
are of the same color, thus color is not useful
in identifying a specific piece, i.e. only size
and shape are discriminative object-intrinsic
attributes. Instead, we can expect other attributes
such as spatial relations and deictic reference to
be used more often.
Each pair of participants sat side by side as
shown in Figure 2. Each participant had his/her
own computer display showing the shared work-
ing area. A room-divider screen was set between
the solver (right side) and operator (left side) to
prevent the operator from seeing the goal shape on
the solver?s screen, and to restrict their interaction
to speech only.
!"#$%&'()"*(
!+#$,-"$.&/0
!.#$1/"(
!2#$34"*5$
Figure 3: The goal shapes given to the subjects
Each participant pair was assigned 4 trials con-
sisting of two symmetric and two asymmetric
goal shapes as shown in Figure 3. In Cogni-
tive Science, a wide variety of different kinds of
puzzles have been employed extensively in the
field of Insight Problem solving. This has been
termed the ?puzzle-problem approach? (Sternberg
and Davidson, 1996; Suzuki et al, 2001) and
in the case of physical puzzles has relatively of-
ten involved puzzle tasks of symmetric shapes
like the so-called T-puzzle, e.g. (Kiyokawa and
Nakazawa, 2006). In more recent work Tangram
puzzles have been used as a means to study var-
ious new aspects of human problem solving ap-
proaches, including collection of of eye-gaze in-
formation (Baran et al, 2007). In order to col-
lect data as broadly as possible in this context, we
set up puzzle-problems including both symmetri-
cal as well as asymmetrical ones as shown in Fig-
ure 3.
The participants exchanged their roles after two
trials, i.e. a participant first solves a symmetric and
then an asymmetric puzzle as the solver and then
does the same as the operator, and vice versa. The
order of the puzzle trials is the same for all pairs.
Before starting the first trial as the operator,
each participant had a short training exercise in
order to learn how to manipulate pieces with the
mouse. The initial arrangement of the pieces was
randomised every time. We set a time limit of 15
minutes for the completion of each trial (i.e. con-
struction of the goal shape). In order to prevent the
solver from getting into deep thought and keeping
silent, the simulator is designed to give a hint ev-
ery five minutes by showing a correct piece posi-
tion in the goal shape area. After 10 minutes have
passed, a second hint is provided, while the pre-
vious hint disappears. A trial ends when the goal
shape is complete or the time is up. Utterances by
the participants are recorded separately in stereo
through headset microphones in synchronisation
with the position of the pieces and the mouse op-
erations. Piece positions and mouse actions were
automatically recorded by the simulator at inter-
vals of 10 msec.
40
Table 1: The ELAN Tiers of the corpus
Tier meaning
OP-UT utterances by the operator
SV-UT utterances by the solver
OP-REX referring expressions by the operator
OP-Ref referents of OP-REX
OP-Attr attributes of OP-REX
SV-REX referring expressions by the solver
SV-Ref referents of SV-REX
SV-Attr attributes of SV-REX
Action action on a piece
Target the target piece of Action
Mouse the piece on which the mouse is hovering
? Indentation of Tier denotes parent-child relations.
2.2 Subjects and collected data
For our Japanese corpus, we recruited 12 Japanese
graduate students of the Cognitive Science depart-
ment, 4 females and 8 males, and split them into 6
pairs. All pairs knew each other previously and
were of the same sex and approximately same
age3. We collected 24 dialogues (4 trials by 6
pairs) of about 4 hours and 16 minutes. The av-
erage length of a dialogue was 10 minutes 40 sec-
onds (SD = 3 minutes 18 seconds).
For the comparable English corpus, we re-
cruited 12 native English speakers of various oc-
cupations, 6 males and 6 females. Their aver-
age age was 30. There were 6 pairs all of whom
knew each other beforehand except for one pair.
Whereas during the creation of the Japanese cor-
pus we had to give extra attention to ensuring that
social relationships did not have an impact on how
the subjects communicated with one another, for
the English corpus there was no such concern. We
collected 24 dialogues (4 trials by 6 pairs) of 5
hours and 7 minutes total length. The average
length of a dialogue was 12 minutes 47 seconds
(SD = 3 minutes 34 seconds).
3 Annotation
The recorded speech data was transcribed and
the referring expressions were annotated with
the Web-based multi-purpose annotation tool
3In Japan, the relationship of senior to junior or socially
higher to lower placed might affect the language use. We
carefully recruited pairs to avoid the effects of this social re-
lationship such as the possible use of overly polite and indi-
rect language, reluctance to correct mistakes etc.
Table 2: Attributes of referring expressions
dpr : demonstrative pronoun, e.g. ?the same one?,
?this?, ?that?, ?it?
dad : demonstrative adjective, e.g. ?that triangle?
siz : size, e.g. ?the large triangle?
typ : type, e.g. ?the square?
dir : direction of a piece, e.g. ?the triangle facing the
left?.
prj : projective spatial relation (including directional
prepositions or nouns such as ?right?, ?left?,
?above?. . . ) e.g. ?the triangle to the left of the
square?
tpl : topological spatial relation (including non-
directional prepositions or nouns such as ?near?,
?middle?. . . ), e.g. ?the triangle near the square?
ovl : overlap, e.g. ?the small triangle under the large
one?
act : action on pieces, e.g ?the triangle that you are
holding now?, ?the triangle that you just rotated?
cmp : complement, e.g. ?the other one?
sim : similarity, e.g. ?the same one?
num : number, e.g. ?the two triangle?
rpr : repair, e.g. ?the big, no, small triangle?
err : obvious erroneous expression, e.g. ?the square?
referring to a triangle
nest : nested expression; when a referring expression
includes another referring expression, only the
outermost expression is annotated with this at-
tribute, e.g. ?(the triangle to the left of (the small
triangle))?
meta: metaphorical expression, e.g. ?the leg?, ?the
head?
SLAT (Noguchi et al, 2008)4. Our target expres-
sions in this corpus are referring expressions re-
ferring to a puzzle piece or a set of puzzle pieces.
We do not deal with expressions referring to a lo-
cation, a part of a piece or a constructed shape.
These expressions are put aside for future work.
The annotation of referring expressions is three-
fold: (1) identification of the span of expressions,
(2) identification of their referents, and (3) assign-
ment of a set of attributes to each referring expres-
sion.
Using the multimodal annotation tool ELAN,5
the annotations of referring expressions were then
merged with extra-linguistic data recorded by the
Tangram simulator. The available extra-linguistic
information from the simulator consists of (1) the
action on a piece, (2) the coordinates of the mouse
cursor and (3) the position of each piece in the
4We did not use SLAT for English corpus annotation. In-
stead, ELAN was directly used for annotating referring ex-
pressions.
5http://www.lat-mpi.eu/tools/elan/
41
Table 3: Summary of trials
ID time success OP-REX SV-REX ID time success OP-REX SV-REX
E01 15:00 J01 8:40 o 10 48
E02 15:00 J02 11:49 o 7 55
E03 15:00 J03 11:36 o 5 26
E04 15:00 J04 7:31 o 2 21
E05 15:00 J05 15:00 23 78
E06 15:00 J06 11:12 o 5 60
E07 15:00 J07 12:11 o 3 59
E08 15:00 J08 11:20 o 4 61
E09 10:39 o J09 14:59 o 36 84
E10 15:00 J10 6:20 o 3 47
E11 15:00 J11 5:21 o 2 14
E12 8:30 o J12 13:40 o 37 77
E13 14:33 o 8 95 J13 15:00 8 56
E14 7:27 o 1 62 J14 4:48 o 1 29
E15 14:02 o 16 127 J15 9:30 o 20 39
E16 3:57 o 1 31 J16 5:07 o 3 17
E17 13:00 o J17 13:37 o 10 46
E18 6:40 o J18 8:57 o 4 51
E19 15:00 J19 8:02 o 0 37
E20 12:32 o J20 11:23 o 1 59
E21 15:00 J21 10:12 o 7 71
E22 15:00 J22 10:24 o 9 64
E23 15:00 J23 15:00 0 69
E24 5:36 o J24 14:22 o 0 76
Ave. 12:47 6.5 78.8 Ave. 10:40 8.3 51.8
SD 3:34 7.14 41.4 SD 3:18 10.4 20.1
Total 5:06:56 10 26 315 Total 4:16:01 21 200 1,244
working area. Actions and mouse cursor positions
are recorded at intervals of 10 msec, and are ab-
stracted into (1) a time span labeled with an action
symbol (?move?, ?rotate? or ?flip?) and its target
piece number (1?7), and (2) a time span labeled
with a piece number which is under the mouse
cursor during that span. The position of pieces is
updated and recorded with a timestamp when the
position of any piece changes. Information about
piece positions is not merged into the ELAN files
and is kept in separate files. As a result, we have
11 time-aligned ELAN Tiers as shown in Table 1.
Two annotators (two of the authors) first an-
notated four Japanese dialogues separately and
based on a discussion of discrepancies, decided
on the following criteria to identify a referring ex-
pression.
? The minimum span of a noun phrase in-
cluding necessary information to identify a
referent is annotated. The span might in-
clude repairs with their reparandum and dis-
fluency (Nakatani and Hirschberg, 1993) if
needed.
? Demonstrative adjectives are included in ex-
pressions.
? Erroneous expressions are annotated with a
special attribute.
? An expression without a definite referent (i.e.
a group of possible referents or none) is as-
signed a referent number sequence consist-
ing of a prefix, followed by the sequence of
possible referents as its referent, if any are
present.
? All expressions appearing in muttering to
oneself are excluded.
Table 2 shows a list of attributes of referring
expressions used in annotating the corpus.
The rest of the 20 Japanese dialogues were an-
notated by two of the authors and discrepancies
were resolved by discussion. Four English dia-
logues have been annotated so far by one of the
authors.
4 Preliminary corpus analysis
We have already completed the Japanese corpus,
which is named REX-J (2008-08), but only 4 out
of 24 dialogues have been annotated for the En-
glish counterpart (REX-E (2010-03)). Table 3
shows a summary of the trials. The horizontal
42
lines divide the trials by pairs, ?o? in the ?suc-
cess? column denotes that the trial was success-
fully completed in the time limit (15 minutes), and
the ?OP-REX? and ?SV-REX? columns show the
number of referring expressions used by the op-
erator and the solver respectively. The following
subsections describe a preliminary comparison of
the English and Japanese corpora.
Table 4: Task completion time
Lang.\Shape (a) (b) (c) (d)
English 832.0 741.2 890.3 605.8
(105.4) (246.5) (23.7) (287.2)
Japanese 774.7 535.0 571.7 633.8
(167.3) (168.5) (242.2) (215.2)
* Average (SD)
4.1 Task performance
We conducted a two-way ANOVA with the task
completion time as the dependent variable, and
the goal shape and the language as the indepen-
dent variables. Only the main effect of the lan-
guage was significant (F (1, 40) = 5.82, p <
0.05). Table 4 shows the average and the standard
deviation of the completion time. Note that we set
a time limit (15 minutes) for solving the puzzle.
We considered the completion time as 15 minutes
even when a puzzle was not actually solved in the
time limit. We also conducted a two-way ANOVA
using only the successful cases. Both main effects
and their interaction were not significant.
We then conducted an ANOVA with the num-
ber of successfully solved puzzles by each pair as
the dependent variable and the language as the in-
dependent variable. The main effect was signifi-
cant (F (1, 10) = 6.79, p < 0.05). Table 5 shows
the average number of success goals per pair and
the success rate with their standard deviations in
parentheses.
Finally, we conducted an ANOVA with the
number of pairs who succeeded in solving a goal
Table 5: The number of solved trials and success
rates
Lang. solved trials success rate [%]
Japanese 3.50 (0.55) 87.5 (13.7)
English 1.67 (1.63) 41.7 (40.8)
* Average (SD)
shape as the dependent variable and the goal shape
as the independent variable. The main effect was
not significant.
In summary, we found a difference in the task
performance between the languages in terms of
the task completion time and the success rate, but
no difference among the goal shapes. This dif-
ference could be explained by the diversity of the
subjects rather than the difference of languages.
The Japanese subject group consisted of univer-
sity graduate students from the same department
(Cognitive Science) and roughly of the same age
(Average = 23.3, SD = 1.5). In contrast, the En-
glish subjects have diverse backgrounds (e.g. high
school students, university faculty, writer, pro-
grammer, etc.) and age (Average = 30.8, SD =
11.7). In addition, a familiarity with this kind of
geometric puzzle might have some effect. How-
ever, we collected a familiarity with the puzzle
only from the English subjects, we could not con-
duct further analysis on this viewpoint. Anyhow,
in this respect, the independent variable should
have been named ?subject group? instead of ?lan-
guage?.
4.2 Referring expressions
It is important to note that since we have only
completed the annotation of four dialogs, all by
one pair of subjects, our analyses of referring ex-
pressions are tentative and pending further analy-
sis.
We have 200 and 1,243 referring expressions by
the operator and the solver respectively, 1,444 in
total in the 24 Japanese dialogues. On the other
hand we have 26 (operator) and 315 (solver) re-
ferring expressions in 4 English dialogues. The
average number of referring expressions per di-
alogue in Table 3 suggests that English subjects
use more referring expressions than Japanese sub-
jects. Since we have only the data from a single
pair, we cannot say whether this tendency applies
to the other pairs. We cannot draw a decisive con-
clusion until we complete the annotation of the
English corpus.
Table 6 shows the total frequencies of the at-
tributes and their frequencies per dialogue. The
table gives us an impression of significantly fre-
quent use of demonstrative pronouns (dpr) by the
43
Table 6: Comparison of attribute distribution
English Japanese
(4 dialogues) (24 dialogues)
attribute frq frq/dlg frq frq/dlg
dpr 226 56.5 678 28.3
dad 29 7.3 178 7.4
siz 68 17.0 288 12.0
typ 103 25.8 655 27.3
dir 0 0 7 0.3
prj 10 2.5 141 5.9
tpl 4 1 9 0.4
ovl 0 0 2 0.1
act 5 1.3 103 4.3
cmp 17 4.3 33 1.4
sim 0 0 7 0.3
num 22 5.5 35 1.5
rpr 0 0 1 0
err 0 0 1 0
nest 1 0.3 31 1.3
meta 1 0.3 6 0.3
English subjects. The Japanese subjects use more
attributes of projective spatial relations (prj) and
actions on the referent (act).6 The English subjects
use more complement attributes (cmp) as well as
more number attributes (num).
5 Related work
Over the last decade, with a growing recogni-
tion that referring expressions frequently appear
in collaborative task dialogues (Clark and Wilkes-
Gibbs, 1986; Heeman and Hirst, 1995), a num-
ber of corpora have been constructed to study the
nature of their use. This tendency also reflects
the recognition that this area yields both challeng-
ing research topics as well as promising applica-
tions such as human-robot interaction (Foster et
al., 2008; Kruijff et al, 2010).
The COCONUT corpus (Di Eugenio et al,
2000) was collected from keyboard-dialogs be-
tween two participants, who worked together on
a simple 2-D design task, buying and arranging
furniture for two rooms. The COCONUT cor-
pus is limited in annotations which describe sym-
bolic object information such as object intrinsic
attributes and location in discrete co-ordinates. As
an initial work of constructing a corpus for collab-
orative tasks, the COCONUT corpus can be char-
acterised as having a rather simple domain as well
6We called such expressions as action-mentioning expres-
sions (AME) in our previous work.
as limited annotation.
The QUAKE corpus (Byron, 2005) and its suc-
cessor, the SCARE corpus (Stoia et al, 2008) deal
with a more complex domain, where two partici-
pants collaboratively play a treasure hunting game
in a 3-D virtual world. Despite the complexity
of the domain, the participants were only allowed
limited actions, e.g. moving step forward, pushing
a button etc.
As a part of the JAST project, the Joint Con-
struction Task (JCT) corpus was created based on
dialogues in which two participants constructed a
puzzle (Foster et al, 2008). The setting of the
experiment is quite similar to ours except that
both participants have even roles. Since our main
concern is referring expressions, we believe our
asymmetric setting elicits more referring expres-
sions than the symmetric setting of the JCT cor-
pus.
In contrast to these previous corpora, our cor-
pora record a wide range of information useful
for analysis of human reference behaviour in situ-
ated dialogue. While the domain of our corpora is
simple compared to the QUAKE and SCARE cor-
pora, we allowed a comparatively large flexibil-
ity in the actions necessary for achieving the goal
shape (i.e. flipping, turning and moving of puzzle
pieces at different degrees), relative to the com-
plexity of the domain. Providing this relatively
larger freedom of actions to the participants to-
gether with the recording of detailed information
allows for research into new aspects of referring
expressions.
As for a multilingual aspect, all the above cor-
pora are English. There have been several recent
attempts at collecting multilingual corpora in situ-
ated domains. For instance, (Gargett et al, 2010)
collected German and English corpora in the same
setting. Their domain is similar to the QUAKE
corpus. Van der Sluis et al (2009) aim at a com-
parative study of referring expressions between
English and Japanese. Their domain is still static
at the moment. Our corpora aim at dealing with
the dynamic nature of situated dialogues between
very different languages, English and Japanese.
44
Table 7: The REX-J corpus family
name puzzle #pairs #dialg. #valid status
T2008-08 Tangram 6 24 24 completed
T2009-03 Tangram 10 40 16 completed
T2009-11 Tangram 10 36 27 validating
N2009-11 Tangram 5 20 8 validating
P2009-11 Polyomino 7 28 24 annotating
D2009-11 2-Tangram 7 42 24 annotating
6 Conclusion and future work
This paper presented an overview of our English-
Japanese bilingual multimodal corpora of refer-
ring expressions in a collaborative problem solv-
ing setting. The Japanese corpus was completed
and has already been used for research (Spanger et
al., 2009b; Spanger et al, 2010; Iida et al, 2010),
but the English counterpart is still undergoing an-
notation. We have also presented a preliminary
comparative analysis of these corpora in terms of
the task performance and usage of referring ex-
pressions. We found a significant difference of the
task performance, which could be attributed to the
difference in diversity of subjects. We have tenta-
tive results on the usage of referring expressions,
since only four English dialogues are available at
the moment.
The data collection experiments were con-
ducted in August 2008 for Japanese and in March
2010 for English. Between these periods, we
conducted various data collections to build differ-
ent types of Japanese corpora (March, 2009 and
November 2009). These experiments involve cap-
turing eye-gaze information of participants during
problem solving, and introducing variants of puz-
zles (Polyomino, Double Tangram and Tangram
without any hints7). They are also under prepa-
ration for publication. Table 7 gives an overview
of the REX-J corpus family, where ?#valid? de-
notes the number of dialogues with valid eye-
gaze data. Eye-gaze data is difficult to capture
cleanly throughout a dialogue. We discarded di-
alogues in which eye-gaze was captured success-
fully less than 70% of the total time of the dia-
logue. Namely, we annotated or will annotate di-
alogues with validated eye-gaze data only.
These corpora enable research on utilising eye-
gaze information in reference resolution and gen-
7N2009-11 in Table 7
eration, and evaluation in different tasks (puzzles)
as well. We are planning to distribute the REX-J
corpus family through GSK (Language Resources
Association in Japan)8, and the REX-E corpus
from both University of Brighton and GSK.
References
Baran, Bahar, Berrin Dogusoy, and Kursat Cagiltay.
2007. How do adults solve digital tangram prob-
lems? Analyzing cognitive strategies through eye
tracking approach. InHCI International 2007 - 12th
International Conference - Part III, pages 555?563.
Bard, Ellen Gurman, Robin Hill, Manabu Arai, and
Mary Ellen Foster. 2009. Accessibility and atten-
tion in situated dialogue: Roles and regulations. In
Proceedings of the Workshop on Production of Re-
ferring Expressions Pre-CogSci 2009.
Belz, Anja, Eric Kow, Jette Viethen, and Albert Gatt.
2010. Referring expression generation in context:
The GREC shared task evaluation challenges. In
Krahmer, Emiel and Marie?t Theune, editors, Empir-
ical Methods in Natural Language Generation, vol-
ume 5980 of Lecture Notes in Computer Science.
Springer-Verlag, Berlin/Heidelberg.
Byron, Donna K. 2005. The OSU Quake 2004 cor-
pus of two-party situated problem-solving dialogs.
Technical report, Department of Computer Science
and Enginerring, The Ohio State University.
Clark, H. Herbert. and Deanna Wilkes-Gibbs. 1986.
Referring as a collaborative process. Cognition,
22:1?39.
Di Eugenio, Barbara, Pamela W. Jordan, Richmond H.
Thomason, and Johanna. D. Moore. 2000. The
agreement process: An empirical investigation of
human-human computer-mediated collaborative di-
alogues. International Journal of Human-Computer
Studies, 53(6):1017?1076.
Foster, Mary Ellen and Jon Oberlander. 2007. Corpus-
based generation of head and eyebrow motion for
an embodied conversational agent. Language Re-
sources and Evaluation, 41(3?4):305?323, Decem-
ber.
Foster, Mary Ellen, Ellen Gurman Bard, Markus Guhe,
Robin L. Hill, Jon Oberlander, and Alois Knoll.
2008. The roles of haptic-ostensive referring ex-
pressions in cooperative, task-based human-robot
dialogue. In Proceedings of 3rd Human-Robot In-
teraction, pages 295?302.
8http://www.gsk.or.jp/index e.html
45
Gargett, Andrew, Konstantina Garoufi, Alexander
Koller, and Kristina Striegnitz. 2010. The give-
2 corpus of giving instructions in virtual environ-
ments. In Proceedings of the Seventh conference on
International Language Resources and Evaluation
(LREC 2010), pages 2401?2406.
Heeman, Peter A. and Graeme Hirst. 1995. Collabo-
rating on referring expressions. Computational Lin-
guistics, 21:351?382.
Hobbs, Jerry R. 1978. Resolving pronoun references.
Lingua, 44:311?338.
Iida, Ryu, Shumpei Kobayashi, and Takenobu Toku-
naga. 2010. Incorporating extra-linguistic informa-
tion into reference resolution in collaborative task
dialogue. In Proceedings of 48th Annual Meeting
of the Association for Computational Linguistics,
pages 1259?1267.
Kiyokawa, Sachiko and Midori Nakazawa. 2006. Ef-
fects of reflective verbalization on insight problem
solving. In Proceedings of 5th International Con-
ference of the Cognitive Science, pages 137?139.
Kruijff, Geert-Jan M., Pierre Lison, Trevor Ben-
jamin, Henrik Jacobsson, Hendrik Zender, and
Ivana Kruijff-Korbayova. 2010. Situated dialogue
processing for human-robot interaction. In Cogni-
tive Systems: Final report of the CoSy project, pages
311?364. Springer-Verlag.
Martin, Jean-Claude, Patrizia Paggio, Peter Kuehnlein,
Rainer Stiefelhagen, and Fabio Pianesi. 2007. Spe-
cial issue on Mulitmodal corpora for modeling hu-
man multimodal behaviour. Language Resources
and Evaluation, 41(3-4).
Mitkov, Ruslan. 2002. Anaphora Resolution. Long-
man.
Nakatani, Christine and Julia Hirschberg. 1993. A
speech-first model for repair identification and cor-
rection. In Proceedings of 31th Annual Meeting of
ACL, pages 200?207.
Noguchi, Masaki, Kenta Miyoshi, Takenobu Toku-
naga, Ryu Iida, Mamoru Komachi, and Kentaro
Inui. 2008. Multiple purpose annotation using
SLAT ? Segment and link-based annotation tool.
In Proceedings of 2nd Linguistic Annotation Work-
shop, pages 61?64.
Piwek, Paul L. A. 2007. Modality choise for gen-
eration of referring acts. In Proceedings of the
Workshop on Multimodal Output Generation (MOG
2007), pages 129?139.
Spanger, Philipp, Masaaki Yasuhara, Ryu Iida, and
Takenobu Tokunaga. 2009a. A Japanese corpus
of referring expressions used in a situated collab-
oration task. In Proceedings of the 12th European
Workshop on Natural Language Generation (ENLG
2009), pages 110 ? 113.
Spanger, Philipp, Masaaki Yasuhara, Ryu Iida, and
Takenobu Tokunaga. 2009b. Using extra linguistic
information for generating demonstrative pronouns
in a situated collaboration task. In Proceedings of
PreCogSci 2009: Production of Referring Expres-
sions: Bridging the gap between computational and
empirical approaches to reference.
Spanger, Philipp, Ryu Iida, Takenobu Tokunaga,
Asuka Teri, and Naoko Kuriyama. 2010. Towards
an extrinsic evaluation of referring expressions in
situated dialogs. In Kelleher, John, Brian Mac
Namee, and Ielka van der Sluis, editors, Proceed-
ings of the Sixth International Natural Language
Generation Conference (INGL 2010), pages 135?
144.
Sternberg, Robert J. and Janet E. Davidson, editors.
1996. The Nature of Insight. The MIT Press.
Stoia, Laura, Darla Magdalene Shockley, Donna K.
Byron, and Eric Fosler-Lussier. 2008. SCARE:
A situated corpus with annotated referring expres-
sions. In Proceedings of the Sixth International
Conference on Language Resources and Evaluation
(LREC 2008), pages 28?30.
Suzuki, Hiroaki, Keiga Abe, Kazuo Hiraki, and
Michiko Miyazaki. 2001. Cue-readiness in in-
sight problem-solving. In Proceedings of the 23rd
Annual Meeting of the Cognitive Science Society,
pages 1012 ? 1017.
van Deemter, Kees, Ielka van der Sluis, and Albert
Gatt. 2006. Building a semantically transparent
corpus for the generation of referring expressions.
In Proceedings of the Fourth International Natural
Language Generation Conference, pages 130?132.
van der Sluis, Ielka, Junko Nagai, and Saturnino Luz.
2009. Producing referring expressions in dialogue:
Insights from a translation exercise. In Proceedings
of PreCogSci 2009: Production of Referring Ex-
pressions: Bridging the gap between computational
and empirical approaches to reference.
46
Towards an Extrinsic Evaluation of Referring Expressions
in Situated Dialogs
Philipp SPANGER IIDA Ryu TOKUNAGA Takenobu
{philipp,ryu-i,take}@cl.cs.titech.ac.jp
TERAI Asuka KURIYAMA Naoko
asuka@nm.hum.titech.ac.jp kuriyama@hum.titech.ac.jp
Tokyo Institute of Technology
Abstract
In the field of referring expression gener-
ation, while in the static domain both in-
trinsic and extrinsic evaluations have been
considered, extrinsic evaluation in the dy-
namic domain, such as in a situated col-
laborative dialog, has not been discussed
in depth. In a dynamic domain, a cru-
cial problem is that referring expressions
do not make sense without an appropriate
preceding dialog context. It is unrealistic
for an evaluation to simply show a human
evaluator the whole period from the be-
ginning of a dialog up to the time point
at which a referring expression is used.
Hence, to make evaluation feasible it is
indispensable to determine an appropriate
shorter context. In order to investigate the
context necessary to understand a referring
expression in a situated collaborative dia-
log, we carried out an experiment with 33
evaluators and a Japanese referring expres-
sion corpus. The results contribute to find-
ing the proper contexts for extrinsic evalu-
tion in dynamic domains.
1 Introduction
In recent years, the NLG community has paid sig-
nificant attention to the task of generating referring
expressions, reflected in the seting-up of several
competitive events such as the TUNA and GIVE-
Challenges at ENLG 2009 (Gatt et al, 2009; By-
ron et al, 2009).
With the development of increasingly complex
generation systems, there has been heightened in-
terest in and an ongoing significant discussion on
different evaluation measures for referring expres-
sions. This discussion is carried out broadly in the
field of generation, including in the multi-modal
domain, e.g. (Stent et al, 2005; Foster, 2008).
!"#$%&
!"#$%&'()$)&'
'($)*$+%"&,#-+."/
*+),&#(&'
&#),&#(&'
-$,$./#&0!"#$%1
234456
78$#0!"#$%10
234496
:$#0!*,0;<=&(0!"#$%1
2344>6
;)/&$0!"#$%1
234456
?/,!$#0@A$<B*,
2344C6
D*<E0@0F$))0
2344G6
H8&(0I$I*,
234J46
K/()*,#!"#$%&#
234496
Figure 1: Overview of recent work on evaluation
of referring expressions
Figure 1 shows a schematic overview of recent
work on evaluation of referring expressions along
the two axes of evaluation method and domain in
which referring expressions are used.
There are two different evaluation methods cor-
responding to the bottom and the top of the verti-
cal axis in Figure 1: intrinsic and extrinsic eval-
uations (Sparck Jones and Galliers, 1996). In-
trinsic methods often measure similarity between
the system output and the gold standard corpora
using metrics such as tree similarity, string-edit-
distance and BLEU (Papineni et al, 2002). Intrin-
sic methods have recently become popular in the
NLG community. In contrast, extrinsic methods
evaluate generated expressions based on an exter-
nal metric, such as its impact on human task per-
formance.
While intrinsic evaluations have been widely
used in NLG, e.g. (Reiter et al, 2005), (Cahill
and van Genabith, 2006) and the competitive 2009
TUNA-Challenge, there have been a number of
criticisms against this type of evaluation. (Reiter
and Sripada, 2002) argue, for example, that gener-
ated text might be very different from a corpus but
still achieve the specific communicative goal.
An additional problem is that corpus-similarity
metrics measure how well a system reproduces
what speakers (or writers) do, while for most NLG
systems ultimately the most important considera-
tion is its effect on the human user (i.e. listener
or reader). Thus (Khan et al, 2009) argues that
?measuring human-likeness disregards effective-
ness of these expressions?.
Furthermore, as (Belz and Gatt, 2008) state
?there are no significant correlations between in-
trinsic and extrinsic evaluation measures?, con-
cluding that ?similarity to human-produced refer-
ence texts is not necessarily indicative of quality
as measured by human task performance?.
From early on in the NLG community, task-
based extrinsic evaluations have been considered
as the most meaningful evaluation, especially
when having to convince people in other commu-
nities of the usefulness of a system (Reiter and
Belz, 2009). Task performance evaluation is rec-
ognized as the ?only known way to measure the ef-
fectiveness of NLG systems with real users? (Re-
iter et al, 2003). Following this direction, the
GIVE-Challenges (Koller et al, 2009) at INLG
2010 (instruction generation) also include a task-
performance evaluation.
In contrast to the vertical axis of Figure 1, there
is the horizontal axis of the domain in which refer-
ring expressions are used. Referring expressions
can thus be distinguished according to whether
they are used in a static or a dynamic domain, cor-
responding to the left and right of the horizontal
axis of Figure 1. A static domain is one such as the
TUNA corpus (van Deemter, 2007), which col-
lects referring expressions based on a motionless
image. In contrast, a dynamic domain comprises a
constantly changing situation where humans need
context information to identify the referent of a re-
ferring expression.
Referring expressions in the static domain have
been evaluated relatively extensively. A recent ex-
ample of an intrinsic evaluation is (van der Sluis
et al, 2007), who employed the Dice-coefficient
measuring corpus-similarity. There have been a
number of extrinsic evaluations as well, such as
(Paraboni et al, 2006) and (Khan et al, 2009), re-
spectively measuring the effect of overspecifica-
tion on task performance and the impact of gener-
ated text on accuracy as well as processing speed.
They belong thus in the top-left quadrant of Fig-
ure 1.
Over a recent period, research in the generation
of referring expressions has moved to dynamic do-
mains such as situated dialog, e.g. (Jordan and
Walker, 2005) and (Stoia et al, 2006). However,
both of them carried out an intrinsic evaluation
measuring corpus-similarity or asking evaluators
to compare system output to expressions used by
human (the right bottom quadrant in Figure 1).
The construction of effective generation sys-
tems in the dynamic domain requires the imple-
mentation of an extrinsic task performance evalu-
ation. There has been work on extrinsic evalua-
tion of instructions in the dynamic domain on the
GIVE-2 challenge (Byron et al, 2009), which is a
task to generate instructions in a virtual world. It is
based on the GIVE-corpus (Gargett et al, 2010),
which is collected through keyboard interaction.
The evaluation measures used are e.g. the number
of successfully completed trials, completion time
as well as the numbers of instructions the system
sent to the user. As part of the JAST project, a
Joint Construction Task (JCT) puzzle construction
corpus (Foster et al, 2008) was created which is
similar in some ways in its set-up to the REX-
J corpus which we use in the current research.
There has been some work on evaluating gener-
ation strategies of instructions for a collaborative
construction task on this corpus, both considering
intrinsic as well as extrinsic measures (Foster et
al., 2009). Their main concern is, however, the in-
teraction between the text structure and usage of
referring expressions. Therefore, their ?context?
was given a priori.
However, as can be seen from Figure 1, in the
field of referring expression generation, while in
the static domain both intrinsic and extrinsic eval-
uations have been considered, the question of re-
alizing an extrinsic evaluation in the dynamic do-
main has not been dealt with in depth by previous
work. This paper addresses this shortcoming of
previous work and contributes to ?filling in? the
missing quadrant of Figure 1 (the top-right).
The realization of such an extrinsic evaluation
faces one key difficulty. In a static domain, an ex-
trinsic evaluation can be realized relatively easily
by showing evaluators the static context (e.g. any
image) and a referring expression, even though
this is still costly in comparison to intrinsic meth-
ods (Belz and Gatt, 2008).
In contrast, an extrinsic evaluation in the dy-
namic domain needs to present an evaluator with
the dynamic context (e.g. a certain length of the
recorded dialog) preceding a referring expression.
It is clearly not feasible to simply show the whole
preceding dialog; this would make even a very
small-scale evaluation much too costly. Thus, in
order to realize a cost-effective extrinsic evalua-
tion in a dynamic domain we have to deal with the
additional parameter of time length and content of
the context shown to evaluators.
This paper investigates the context necessary for
humans to understand different types of referring
expressions in a situated domain. This work thus
charts new territory and contributes to developing
a extrinsic evaluation in a dynamic domain. Sig-
nificantly, we consider not only linguistic but also
extra-linguistic information as part of the context,
such as the actions that have been carried out in the
preceding interaction. Our results indicate that, at
least in this domain, extrinsic evaluation results
in dynamic domains can depend on the specific
amount of context shown to the evaluator. Based
on the results from our evaluation experiments, we
discuss the broader conclusions to be drawn and
directions for future work.
2 Referring Expressions in the REX-J
Corpus
We utilize the REX-J corpus, a Japanese corpus
of referring expressions in a situated collaborative
task (Spanger et al, 2009a). It was collected by
recording the interaction of a pair of dialog partic-
ipants solving the Tangram puzzle cooperatively.
The goal of the Tangram puzzle is to construct a
given shape by arranging seven pieces of simple
figures as shown in Figure 2
!"#$%&'#()
*"+,-.!%#+)#
Figure 2: Screenshot of the Tangram simulator
In order to record the precise position of every
piece and every action by the participants, we im-
plemented a simulator. The simulator displays two
areas: a goal shape area, and a working area where
pieces are shown and can be manipulated.
We assigned different roles to the two partici-
pants of a pair: solver and operator. The solver
can see the goal shape but cannot manipulate the
pieces and hence gives instructions to the opera-
tor; by contrast, the operator can manipulate the
pieces but can not see the goal shape. The two
participants collaboratively solve the puzzle shar-
ing the working area in Figure 2.
In contrast to other recent corpora of refer-
ring expressions in situated collaborative tasks
(e.g. COCONUT corpus (Di Eugenio et al, 2000)
and SCARE corpora (Byron et al, 2005)), in
the REX-J corpus we allowed comparatively large
real-world flexibility in the actions necessary to
achieve the task (such as flipping, turning and
moving of puzzle pieces at different degrees), rel-
ative to the task complexity. The REX-J corpus
thus allows us to investigate the interaction of lin-
guistic and extra-linguistic information. Interest-
ingly, the GIVE-2 challenge at INLG 2010 notes
its ?main novelty? is allowing ?continuous moves
rather than discrete steps as in GIVE-1?. Our work
is in line with the broader research trend in the
NLG community of trying to get away from sim-
ple ?discrete? worlds to more realistic settings.
The REX-J corpus contains a total of 1,444 to-
kens of referring expressions in 24 dialogs with a
total time of about 4 hours and 17 minutes. The
average length of each dialog is 10 minutes 43
seconds. The asymmetric data-collection setting
encouraged referring expressions from the solver
(solver: 1,244 tokens, operator: 200 tokens). We
exclude from consideration 203 expressions refer-
ring to either groups of pieces or whose referent
cannot be determined due to ambiguity, thus leav-
ing us 1,241 expressions.
We identified syntactic/semantic features in the
collected referring expressions as listed in Table 1:
(a) demonstratives (adjectives and pronouns), (b)
object attribute-values, (c) spatial relations and (d)
actions on an object. The underlined part of the
examples denotes the feature in question.
3 Design of Evaluation Experiment
The aim of our experiment is to investigate the
?context? (content of the time span of the recorded
Table 1: Syntactic and semantic features of refer-
ring expressions in the REX-J corpus
Feature Tokens Example
(a) demonstrative 742 ano migigawa no sankakkei
(that triangle at the right side)
(b) attribute 795 tittyai sankakkei
(the small triangle)
(c) spatial relations 147 hidari no okkii sankakkei
(the small triangle on the left)
(d) action-mentioning 85 migi ue ni doketa sankakkei
(the triangle you put away to
the top right)
interaction prior to the uttering of the referring ex-
pression) necessary to enable successful identifi-
cation of the referent of a referring expression.
Our method is to vary the context presented to
evaluators and then to study the impact on human
referent identification. In order to realize this, for
each instance of a referring expression, we vary
the length of the video shown to the evaluator.
!"#$ %&'()*+,!-./0#
!1#$ 234*&,&5,678+*4,9&+:3(;,8+*8,3(,)7*,63<'=8)&+
!%#$ >))*+8(?*,3(?='43(;,)7*,+*5*++3(;,*@A+*663&(,)&,
*B8='8)*,!67&9(,3(,+*4#
!C#$ D)8+)E+*A*8),F'))&(
!G#$ D*=*?)3&(,F'))&(6,!-.H#,8(4,IJ,4&(K),:(&9I.F'))&(,,,,,,,,,,
!"#
!1#
!%#
!C#
!G#
Figure 3: The interface presented to evaluators
The basic procedure of our evaluation experi-
ment is as follows:
(1) present human evaluators with speech and
video from a dialog that captures shared
working area of a certain length previous to
the uttering of a referring expression,
(2) stop the video and display as text the next
solver?s utterance including the referring ex-
pression (shown in red),
(3) ask the evaluator to identify the referent
of the presented referring expression (if the
evaluator wishes, he/she can replay the video
as many times as he likes),
(4) proceed to the next referring expression (go
to (1)).
Figure 3 shows a screenshot of the interface pre-
pared for this experiment.
The test data consists of three types of referring
expressions: DPs (demonstrative pronouns),
AMEs (action-mentioning expressions), and
OTHERs (any other expression that is neither a
DP nor AME, e.g intrinsic attributes and spatial
relations). DPs are the most frequent type of
referring expression in the corpus. AMEs are
expressions that utilize an action on the referent
such as ?the triangle you put away to the top
right? (see Table 1)1. As we pointed out in our
previous paper (Spanger et al, 2009a), they are
also a fundamental type of referring expression in
this domain.
The basic question in investigating a suitable
context is what information to consider about the
preceding interaction; i.e. over what parameters to
vary the context. In previous work on the gener-
ation of demonstrative pronouns in a situated do-
main (Spanger et al, 2009b), we investigated the
role of linguistic and extra-linguistic information,
and found that time distance from the last action
(LA) on the referent as well as the last mention
(LM) to the referent had a significant influence on
the usage of referring expressions. Based on those
results, we focus on the information on the refer-
ent, namely LA and LM.
For both AMEs and OTHERs, we only consider
two possibilities of the order in which LM and LA
appear before a referring expression (REX), de-
pending on which comes first. These are shown in
Figure 4, context patterns (a) LA-LM and (b) LM-
LA. Towards the very beginning of a dialog, some
referring expressions have no LM and LA; those
expressions are not considered in this research.
All instances of AMEs and OTHERs in our test
data belong to either the LA-LM or the LM-LA
1An action on the referent is usually described by a verb
as in this example. However, there are cases with a verb el-
lipsis. While this would be difficult in English, it is natural
and grammatical in Japanese.
!"#$%&'()&*+'()&
,($%
-./%+ !"#$
%&
'"()
!"#$%&'%($)"**+,-
!"#*+'()&$%&'()&
,($%
-./%+ !"#$
%*
'"()
!.#$%('%&$)"**+,-
!"#
$%&'()&
,($%
-./%+
%*
'"()
!/#$%('%&0$)"**+,-
*+'()&
-./%+
-./%+
Figure 4: Schematic overview of the three context
Patterns
pattern. For each of these two context patterns,
there are three possible contexts2: Both (including
both LA and LM), LA/LM (including either LA or
LM) and None (including neither). Depending on
the order of LA and LM prior to an expression,
only one of the variations of LA/LM is possible
(see Figure 4 (a) and (b)).
In contrast, DPs tend to be utilized in a deic-
tic way in such situated dialogs (Piwek, 2007).
We further noted in (Spanger et al, 2009b), that
DPs in a collaborative task are also frequently used
when the referent is under operation. While they
belong neither to the LA-LM nor the LM-LA pat-
tern, it would be inappropriate to exclude those
cases. Hence, for DPs we consider another situa-
tion where the last action on the referent overlaps
with the utterance of the DP (Figure 4 (c) LM-LA?
pattern). In this case, we consider an ongoing op-
eration on the referent as a ?last action?. Another
peculiarity of the LM-LA? pattern is that we have
no None context in this case, since there is no way
to show a video without showing LA (the current
operation).
Given the three basic variations of context, we
recruited 33 university students as evaluators and
2To be more precise, we set a margin at the beginning of
contexts as shown in Figure 4.
divided them equally into three groups, i.e. 11
evaluators per group. As for the referring ex-
pressions to evaluate, we selected 60 referring ex-
pressions used by the solver from the REX-J cor-
pus (20 from each category), ensuring all were
correctly understood by the operator during the
recorded dialog. We selected those 60 instances
from expressions where both LM and LA ap-
peared within the last 30 secs previous to the re-
ferring expression. This selection excludes initial
mentions, as well as expressions where only LA
or only LM exists or they do not appear within 30
secs. Hence the data utilized for this experiment
is limited in this sense. We need further experi-
ments to investigate the relation between the time
length of contexts and the accuarcy of evaluators.
We will return to this issue in the conclusion.
We combined 60 referring expressions and the
three contexts to make the test instances. Follow-
ing the Latin square design, we divided these test
instances into three groups, distributing each of
the three contexts for every referring expression
to each group. The number of contexts was uni-
formly distributed over the groups. Each instance
group was assigned to each evaluator group.
For each referring expression instance, we
record whether the evaluator was able to correctly
identify the referent, how long it took them to
identify it and whether they repeated the video
(and if so how many times).
Reflecting the distribution of the data available
in our corpus, the number of instances per context
pattern differs for each type of referring expres-
sion. For AMEs, overwhelmingly the last action
on the referent was more recent than the last men-
tion. Hence we have only two LA-LM patterns
among the 20 AMEs in our data. For OTHERs, the
balance is 8 to 12, with a slight majority of LM-
LA patterns. For DPs, there is a strong tendency to
use a DP when a piece is under operation (Spanger
et al, 2009b). Of the 20 DPs in the data, 2 were
LA-LM, 5 were LM-LA pattern while 13 were of
the LM-LA? pattern (i.e. their referents were under
operation at the time of the utterance). For these
13 instances of LM-LA? we do not have a None
context.
The average stimulus times, i.e. time period of
presented context, were 7.48 secs for None, 11.04
secs for LM/LA and 18.10 secs for Both.
Table 2: Accuracy of referring expression identification per type and context
Type context pattern\Context None LM/LA Both Increase [None ? Both]
(LA-LM) 0.909 0.955 0.955 0.046
DP (20/22) (21/22) (21/22)
(LM-LA) 0.455 0.783 0.843 0.388
(25/55) (155/198) (167/198)
Total 0.584 0.800 0.855 0.271
(LA-LM) 0.227 0.455 0.682 0.455
AME (5/22) (10/22) (15/22)
(LM-LA) 0.530 0.859 0.879 0.349
(105/198) (170/198) (174/198)
Total 0.500 0.818 0.859 0.359
(LA-LM) 0.784 0.852 0.943 0.159
OTHER (69/88) (75/88) (83/88)
(LM-LA) 0.765 0.788 0.879 0.114
(101/132) (104/132) (116/132)
Total 0.773 0.814 0.905 0.132
Overall 0.629 0.811 0.903 0.274
(325/517) (535/660) (576/638)
4 Results and Analysis
In this section we discuss the results of our evalua-
tion experiment. In total 33 evaluators participated
in our experiment, each solving 60 problems of
referent identification. Taking into account the ab-
sence of the None context for the DPs of the LM-
LA? pattern (see (c) in Figure 4), we have 1,815
responses to analyze. We focus on the impact of
the three contexts on the three types of referring
expressions, considering the two context patterns
LA-MA and LM-LA.
4.1 Overview of Results
Table 2 shows the micro averages of the accura-
cies of referent identification of all evaluators over
different types of referring expressions with differ-
ent contexts. Accuracies increase with an increase
in the amount of information in the context; from
None to Both by between 13.2% (OTHERs) and
35.9% (AMEs). The average increase of accuracy
is 27.4%.
Overall, for AMEs the impact of the context is
the greatest, while for OTHERs it is the smallest.
This is not surprising given that OTHERs tend to
include intrinsic attributes of the piece and its spa-
tial relations, which are independent of the pre-
ceding context. We conducted ANOVA with the
context as the independent variable, testing its ef-
fect on identification accuracy. The main effect
of the context was significant on accuracy with
F (2, 1320) = 9.17, p < 0.01. Given that for
DPs we did not have an even distribution between
contexts, we only utilized the results of AMEs and
OTHERs.
There are differences between expression types
in terms of the impact of addition of LM/LA into
the context, which underlines that when studying
context, the relative role and contribution of LA
and LM (and their interaction) must be looked at in
detail for different types of referring expressions.
Over all referring expressions, the addition into
a None context of LM yields an average increase
in accuracy of 9.1% for all referring expression
types, while for the same conditions the addition
of LA yields an average increase of 21.3%. Hence,
interestingly for our test data, the addition of LA
to the context has a positive impact on accuracy by
more than two times over the addition of LM.
It is also notable that even with neither LA nor
LM present (i.e. the None context), the evaluators
were still able to correctly identify referents in be-
tween 50?68.6% (average: 62.9%) of the cases.
While this accuracy would be insufficient for the
evaluation of machine generated referring expres-
sions, it is still higher than one might expect and
further investigation of this case is necessary.
4.2 Demonstrative Pronouns
For DPs, there is a very clear difference between
the two patterns (LM-LA and LA-LM) in terms of
the increase of accuracy with a change of context.
While accuracy for the LA-LM pattern remains at
a high level (over 90%) for all three contexts (and
there is only a very small increase from None to
Both), for the LM-LA pattern there is a strong in-
crease from None to Both of 38.8%.
The difference in accuracy between the two
context patterns of DPs in the None context might
come from the mouse cursor effect. The two ex-
pressions of LA-LM pattern happened to have a
mouse cursor on the referent, when they were
used, resulting in high accuracy. On the other
hand, 4 out of 5 expressions of LM-LA pattern did
not have a mouse cursor on the referent. We have
currently no explanation for the relation between
context patterns and the mouse position. While
we have only 7 expressions in the None context
for DPs and hence cannot draw any decisive con-
clusions, we note that the impact of the mouse po-
sition is a likely factor.
For the LM-LA pattern, there is an increase
in accuracy of 32.8% from None to the LA-
context. Overwhelmingly, this represents in-
stances in which the referents are being operated
at the point in time when the solver utters a DP
(this is in fact the LM-LA? pattern, which has no
None context). For those instances, the current
operation information is sufficient to identify the
referents. In contrast, addition of LM leads only
to a small increase in accuracy of 5.6%. This re-
sult is in accordance with our previous work on the
generation of DPs, which stressed the importance
of extra-linguistic information in the framework of
considering the interaction between linguistic and
extra-linguistic information.
4.3 Action-mentioning Expressions
While for AMEs the number of instances is very
uneven between patterns (similar to the distribu-
tion for DPs), there is a strong increase in accuracy
from the None context to the Both context for both
patterns (between 30% to almost 50%). However,
there is a difference between the two patterns in
terms of the relative contribution of LM and LA to
this increase.
While for the LA-LM pattern the impact of
adding LM and LA is very similar, for the LM-LA
pattern the major increase in accuracy is due to
adding LA into the None context. This indicates
that for AMEs, LA has a stronger impact on ac-
curacy than LM, as is to be expected. The strong
increase for AMEs of the LM-LA pattern when
adding LA into the context is not surprising, given
that the evaluators were able to see the action men-
tioned in the AME.
For the opposite reason, it is not surprising that
AMEs show the lowest accuracy in the None con-
text, given that the last action on the referent is
not seen by the evaluators. However, accuracy
was still slightly over 50% in the LM-LA pattern.
Overall, of the 18 instances of AMEs of the LM-
LA pattern, in the None context a majority of eval-
uators correctly identified 9 and erred on the other
9. Further analysis of the difference between cor-
rectly and incorrectly identified AMEs led us to
note again the important role of the mouse cursor
also for AMEs.
Comparing to the LM-LA pattern, we had very
low accuracy even with the Both context. As we
mentioned in the previous section, we had very
skewed test instances for AME, i.e. 18 LM-LA
patterns vs. 2 LA-LM patterns. We need further
investigation on the LA-LM pattern of AME with
more large number of instances.
Of the 18 LM-LA instances of AMEs, there are
14 instances that mention a verb describing an ac-
tion on the referent. The referents of 6 of those
14 AMEs were correctly determined by the evalu-
ators and in all cases the mouse cursor played an
important role in enabling the evaluator to deter-
mine the referent. The evaluators seem to utilize
the mouse position at the time of the uttering of the
referring expression as well as mouse movements
in the video shown. In contrast, for 8 out of the
9 incorrectly determined AMEs no such informa-
tion from the mouse was available. There was a
very similar pattern for AMEs that did not include
a verb. These points indicate that movements and
the position of the mouse both during the video as
well as the time point of the uttering of the refer-
ring expression give important clues to evaluators.
4.4 Other Expressions
There is a relatively even gain in identification ac-
curacy from None to Both of between about 10?
15% for both patterns. However, there is a simi-
lar tendency as for AMEs, since there is a differ-
ence between the two patterns in terms of the rel-
ative contribution of LM and LA to this increase.
While for the LA-LM pattern the impact of adding
LM and LA is roughly equivalent, for the LM-LA
pattern the major increase in accuracy is due to
adding LM into the LA-context.
For this pattern of OTHERs, LM has a stronger
impact on accuracy than LA, which is exactly the
opposite tendency to AMEs. For OTHERs (e.g.
use of attributes for object identification), seeing
the last action on the target has a less positive im-
pact than listening to the last linguistic mention.
Furthermore, we note the relatively high accuracy
in the None context for OTHERs, underlining the
context-independence of expressions utilizing at-
tributes and spatial relations of the pieces.
4.5 Error Analysis
We analyzed those instances whose referents were
not correctly identified by a majority of evalua-
tors in the Both context. Among the three expres-
sion types, there were about 13?16% of wrong an-
swers. In total for 7 of the 60 expressions a ma-
jority of evaluators gave wrong answers (4 DPs, 2
AMEs and 1 OTHER). Analysis of these instances
indicates that some improvements of our concep-
tion of ?context? is needed.
For 3 out of the 4 DPs, the mouse was not over
the referent or was closer to another piece. In addi-
tion, these DPs included expressions that pointed
to the role of a piece in the overall construction of
the goal shape, e.g. ?soitu ga atama (that is the
head)?, or where a DP is used as part of a more
complex referring expression, e.g. ?sore to onazi
katati . . . (the same shape as this)?, intended to
identify a different piece. For a non-participant
of the task, such expressions might be difficult to
understand in any context. This phenomenon is
related to the ?overhearer-effect? (Schober et al,
1989).
The two AMEs that the majority of evaluators
failed to identify in the Both context were also
misidentified in the LA context. Both AMEs were
missing a verb describing an action on the referent.
While for AMEs including a verb the accuracy in-
creased from None to Both by 50%, for AMEs
without a verb there was an increase by slightly
over 30%, indicating that in the case where an
AME lacks a verb, the context has a smaller pos-
itive impact on accuracy than for AMEs that in-
clude a verb. In order to account for those cases,
further work is necessary, such as investigating
how to account for the information on the distrac-
tors.
5 Conclusions and Future Work
In order to address the task of designing a flexi-
ble experiment set-up with relatively low cost for
extrinsic evaluations of referring expressions, we
investigated the context that needs to be shown to
evaluators in order to correctly determine the ref-
erent of an expression.
The analysis of our results showed that the con-
text had a significant impact on referent identifi-
cation. The impact was strongest for AMEs and
DPs and less so for OTHERs. Interestingly, we
found for both DPs and AMEs that including LA
in the context had a stronger positive impact than
including LM. This emphasizes the importance of
taking into account extra-linguistic information in
a situated domain, as considered in this study.
Our analysis of those expressions whose refer-
ent was incorrectly identified in the Both context
indicated some directions for improving the ?con-
text? used in our experiments, for example look-
ing further into AMEs without a verb describing
an action on the referent. Generally, there is a
necessity to account for mouse movements during
the video shown to evaluators as well as the prob-
lem for extrinsic evaluations of how to address the
?overhearer?s effect?.
While likely differing in the specifics of the set-
up, the methodology in the experiment design dis-
cussed in this paper is applicable to other domains,
in that it allows a low-cost flexible design of eval-
uating referring expressions in a dynamic domain.
In order to avoid the additional effort of analyzing
cases in relation to LM and LA, in the future it will
be desirable to simply set a certain time period and
base an evaluation on such a set-up.
However, we cannot simply assume that a
longer context would yield a higher identification
accuracy, given that evaluators in our set-up are
not actively participating in the interaction. Thus
there is a possibility that identification accuracy
actually decreases with longer video segments,
due to a loss of the evaluator?s concentration. Fur-
ther investigation of this question is indicated.
Based on the work reported in this paper, we
plan to implement an extrinsic task-performance
evaluation in the dynamic domain. Even with
the large potential cost-savings based on the re-
sults reported in this paper, extrinsic evaluations
will remain costly. Thus one important future task
for extrinsic evaluations will be to investigate the
correlation between extrinsic and intrinsic evalua-
tion metrics. This in turn will enable the use of
cost-effective intrinsic evaluations whose results
are strongly correlated to task-performance eval-
uations. This paper made an important contribu-
tion by pointing the direction for further research
in extrinsic evaluations in the dynamic domain.
References
Anja Belz and Albert Gatt. 2008. Intrinsic vs. extrinsic
evaluation measures for referring expression genera-
tion. In Proceedings of ACL-08: HLT, Short Papers,
pages 197?200.
Donna Byron, Thomas Mampilly, Vinay Sharma, and
Tianfang Xu. 2005. Utilizing visual attention for
cross-modal coreference interpretation. In CON-
TEXT 2005, pages 83?96.
Donna Byron, Alexander Koller, Kristina Striegnitz,
Justine Cassell, Robert Dale, Johanna Moore, and
Jon Oberlander. 2009. Report on the First NLG
Challenge on Generating Instructions in Virtual En-
vironments (GIVE). In Proceedings of the 12th Eu-
ropean Workshop on Natural Language Generation
(ENLG 2009), pages 165?173.
Aoife Cahill and Josef van Genabith. 2006. Ro-
bust PCFG-based generation using automatically
acquired lfg approximations. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1033?
1040.
Barbara Di Eugenio, Pamela. W. Jordan, Richmond H.
Thomason, and Johanna. D Moore. 2000. The
agreement process: An empirical investigation of
human-human computer-mediated collaborative di-
alogues. International Journal of Human-Computer
Studies, 53(6):1017?1076.
Mary Ellen Foster, Ellen Gurman Bard, Markus Guhe,
Robin L. Hill, Jon Oberlander, and Alois Knoll.
2008. The roles of haptic-ostensive referring expres-
sions in cooperative, task-based human-robot dia-
logue. In Proceedings of 3rd Human-Robot Inter-
action, pages 295?302.
Mary Ellen Foster, Manuel Giuliani, Amy Isard, Colin
Matheson, Jon Oberlander, and Alois Knoll. 2009.
Evaluating description and reference strategies in a
cooperative human-robot dialogue system. In Pro-
ceedings of the 21st international jont conference
on Artifical intelligence (IJCAI 2009), pages 1818?
1823.
Mary Ellen Foster. 2008. Automated metrics that
agree with human judgements on generated output
for an embodied conversational agent. In Proceed-
ings of the 5th International Natural Language Gen-
eration Conference (INLG 2008), pages 95?103.
Andrew Gargett, Konstantina Garoufi, Alexander
Koller, and Kristina Striegnitz. 2010. The give-
2 corpus of giving instructions in virtual environ-
ments. In Proceedings of the Seventh conference on
International Language Resources and Evaluation
(LREC 2010), pages 2401?2406.
Albert Gatt, Anja Belz, and Eric Kow. 2009. The
TUNA-REG Challenge 2009: Overview and eval-
uation results. In Proceedings of the 12th European
Workshop on Natural Language Generation (ENLG
2009), pages 174?182.
Pamela W. Jordan and Marilyn A. Walker. 2005.
Learning content selection rules for generating ob-
ject descriptions in dialogue. Journal of Artificial
Intelligence Research, 24:157?194.
Imtiaz Hussain Khan, Kees van Deemter, Graeme
Ritchie, Albert Gatt, and Alexandra A. Cleland.
2009. A hearer-oriented evaluation of referring ex-
pression generation. In Proceedings of the 12th Eu-
ropean Workshop on Natural Language Generation
(ENLG 2009), pages 98?101.
Alexander Koller, Kristina Striegnitz, Donna Byron,
Justine Cassell, Robert Dale, Sara Dalzel-Job, Jon
Oberlander, and Johanna Moore. 2009. Validating
the web-based evaluation of nlg systems. In Pro-
ceedings of the ACL-IJCNLP 2009 Conference Short
Papers, pages 301?304.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics (ACL 2002), pages 311?318.
Ivandre? Paraboni, Judith Masthoff, and Kees van
Deemter. 2006. Overspecified reference in hierar-
chical domains: Measuring the benefits for readers.
In Proceedings of the 4th International Natural Lan-
guage Generation Conference (INLG 2006), pages
55?62.
Paul L.A. Piwek. 2007. Modality choise for generation
of referring acts. In Proceedings of the Workshop on
Multimodal Output Generation (MOG 2007), pages
129?139.
Ehud Reiter and Anja Belz. 2009. An investiga-
tion into the validity of some metrics for automat-
ically evaluating natural language generation sys-
tems. Computational Linguistics, 35(4):529?558.
Ehud Reiter and Somayajulu Sripada. 2002. Should
corpora texts be gold standards for NLG? In Pro-
ceesings of 2nd International Natural Language
Generation Conference (INLG 2002), pages 97?104.
Ehud Reiter, Roma Robertson, and Liesl M. Osman.
2003. Lessons from a failure: generating tailored
smoking cessation letters. Artificial Intelligence,
144(1-2):41?58.
Ehud Reiter, Somayajulu Sripada, Jim Hunter, Jin Yu,
and Ian Davy. 2005. Choosing words in computer-
generated weather forecasts. Artificial Intelligence,
167(1-2):137?169.
Michael F. Schober, Herbert, and H. Clark. 1989. Un-
derstanding by addressees and overhearers. Cogni-
tive Psychology, 21:211?232.
Philipp Spanger, Masaaki Yasuhara, Ryu Iida, and
Takenobu Tokunaga. 2009a. A Japanese corpus
of referring expressions used in a situated collabo-
ration task. In Proceedings of the 12th European
Workshop on Natural Language Generation (ENLG
2009), pages 110 ? 113.
Philipp Spanger, Masaaki Yasuhara, Iida Ryu, and
Tokunaga Takenobu. 2009b. Using extra linguistic
information for generating demonstrative pronouns
in a situated collaboration task. In Proceedings of
PreCogSci 2009: Production of Referring Expres-
sions: Bridging the gap between computational and
empirical approaches to reference.
Karen Sparck Jones and Julia R. Galliers. 1996. Eval-
uating Natural Language Processing Systems: An
Analysis and Review. Springer-Verlag.
Amanda Stent, Matthew Marge, and Mohit Singhai.
2005. Evaluating evaluation methods for generation
in the presence of variation. In Linguistics and In-
telligent Text Processing, pages 341?351. Springer-
Verlag.
Laura Stoia, Darla Magdalene Shockley, Donna K. By-
ron, and Eric Fosler-Lussier. 2006. Noun phrase
generation for situated dialogs. In Proceedings of
the 4th International Natural Language Generation
Conference (INLG 2006), pages 81?88.
Kees van Deemter. 2007. TUNA: Towards a unified
algorithm for the generation of referring expres-
sions. Technical report, Aberdeen University.
www.csd.abdn.ac.uk/research/tuna/pubs/TUNA-
final-report.pdf.
Ielka van der Sluis, Albert Gatt, and Kees van Deemter.
2007. Evaluating algorithms for the generation of
referring expressions: Going beyond toy domains.
In Proceedings of Recent Advances in Natural Lan-
guae Processing (RANLP 2007).
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 237?246,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
A Unified Probabilistic Approach to Referring Expressions
Kotaro Funakoshi Mikio Nakano
Honda Research Institute Japan Co., Ltd.
8-1 Honcho, Wako,
Saitama 351-0188, Japan
{funakoshi,nakano}@jp.honda-ri.com
Takenobu Tokunaga Ryu Iida
Tokyo Institute of Technology
2-12-1 Oookayama, Meguro,
Tokyo 152-8550, Japan
{take,ryu-i}@cl.cs.titech.ac.jp
Abstract
This paper proposes a probabilistic approach
to the resolution of referring expressions for
task-oriented dialogue systems. The approach
resolves descriptions, anaphora, and deixis in
a unified manner. In this approach, the notion
of reference domains serves an important role
to handle context-dependent attributes of enti-
ties and references to sets. The evaluation with
the REX-J corpus shows promising results.
1 Introduction
Referring expressions (REs) are expressions in-
tended by speakers to identify entities to hearers.
REs can be classified into three categories: descrip-
tions, anaphora, and deixis; and, in most cases,
have been studied within each category and with a
narrowly focused interest. Descriptive expressions
(such as ?the blue glass on the table?) exploit at-
tributes of entities and relations between them to
distinguish an entity from the rest. They are well
studied in natural language generation, e.g., (Dale
and Reiter, 1995; Krahmer et al, 2003; Dale and Vi-
ethen, 2009). Anaphoric expressions (such as ?it?)
refer to entities or concepts introduced in the pre-
ceding discourse and are studied mostly on textual
monologues, e.g., (Kamp and Reyle, 1993; Mitkov,
2002; Ng, 2010). Deictic (exophoric) expressions
(such as ?this one?) refer to entities outside the pre-
ceding discourse. They are often studied focusing
on pronouns accompanied with pointing gestures in
physical spaces, e.g., (Gieselmann, 2004).
Dialogue systems (DSs) as natural human-
machine (HM) interfaces are expected to han-
dle all the three categories of referring expres-
sions (Salmon-Alt and Romary, 2001). In fact, the
three categories are not mutually exclusive. To be
concrete, a descriptive expression in conversation is
either deictic or anaphoric. It is, however, not easy to
tell whether a RE is deictic or anaphoric in advance
of a resolution (regardless of whether the RE is de-
scriptive or not). Therefore, we propose a general
unified approach to the above three kinds of REs.
We employ a Bayesian network (BN) to model a
RE. Dealing with continuous information and vague
situations is critical to handle real world problems.
Probabilistic approaches enable this for reference re-
solvers. Each BN is dynamically constructed based
on the structural analysis result of a RE and contex-
tual information available at that moment. The BN
is used to estimate the probability with which the
corresponding RE refers to an entity.
One of the two major contributions of this paper is
our probabilistic formulation that handles the above
three kinds of REs in a unified manner. Previously
Iida et al (2010) proposed a quantitative approach
that handles anaphoric and deictic expressions in a
unified manner. However it lacks handling of de-
scriptive expressions. Our formulation subsumes
and extends it to handle descriptive REs. So far, no
previously proposed method for reference resolution
handles all three types of REs.
The other contribution is bringing reference
domains into that formulation. Reference do-
mains (Salmon-Alt and Romary, 2000) are sets of
referents implicitly presupposed at each use of REs.
By considering them, our approach can appropri-
ately interpret context-dependent attributes. In ad-
dition, by treating a reference domain as a referent,
REs referring to sets of entities are handled, too. As
far as the authors know, this work is the first that
takes a probabilistic approach to reference domains.
237
1.1 Reference domains
First, we explain reference domains concretely. Ref-
erence domains (RDs) (Salmon-Alt and Romary,
2000; Salmon-Alt and Romary, 2001; Denis, 2010)
are theoretical constructs, which are basically sets
of entities presupposed at each use of REs. RDs in
the original literature are not mere sets of entities
but mental objects equipped with properties such
as type, focus, or saliency and internally structured
with partitions. In this paper, while we do not ex-
plicitly handle partitions, reference domains can be
nested as an approximation of partitioning, that is,
an entity included in a RD is either an individual en-
tity or another RD. Each RD d has its focus and de-
gree of saliency (a non-negative real number). Here-
after, two of them are denoted as foc(d) and sal(d)
respectively. RDs are sorted in descending order ac-
cording to saliency.
We illustrate reference domains with figure 1. It
shows a snapshot of solving a Tangram puzzle (the
puzzle and corpus are explained in section 3.1). RDs
are introduced into our mental spaces either linguis-
tically (by hearing a RE) or visually (by observing
a physical situation). If one says ?the two big tri-
angles? in the situation shown in figure 1, we will
recognize a RD consisting of pieces 1 and 2. If we
observe one moves piece 1 and attaches it to piece
2, we will perceptually recognize a RD consisting
of pieces 1, 2, and 6 due to proximity (Tho?risson,
1994). In a similar way, a RD consisting of pieces 5
and 7 also can be recognized. Hereafter, we indicate
a RD with the mark @ with an index, and denote
its elements by enclosing them with [ ]. E.g., @1 =
[1, 2], @2 = [1, 2, 6], @3 = [5, 7]. The focused en-
tity is marked by ?*?. Thus, foc([1?, 2]) = 1.
The referent of a RE depends on which RD is pre-
supposed. That is, if one presupposes @1 or @2, the
referent of ?the right piece? should be piece 1. If
one presupposes @3, the referent of the same RE
should be piece 5. This is the context-dependency
mentioned above.
Previous work on RDs (Salmon-Alt and Romary,
2000; Salmon-Alt and Romary, 2001; Denis, 2010)
employ not probabilistic but formal approaches.
1.2 Probabilistic approaches to REs
Here, previous probabilistic approaches to REs are
explained and differences between ours and theirs
Figure 1: Tangram puzzle. (The labels 1 to 7 are for il-
lustration purposes and not visible to participants.)
are highlighted. Bayesian networks (Pearl, 1988;
Jensen and Nielsen, 2007) have been not often but
occasionally applied to problems in natural language
processing/computational linguistics since (Char-
niak and Goldman, 1989). With regard to REs,
Burger and Connolly (1992) proposed a BN special-
ized for anaphora resolution. Weissenbacher (2005;
2007) proposed a BN for the resolution of non-
anaphoric ?it? and also a BN for the resolution of
pronominal anaphora. They used pre-defined fixed
BNs for their tasks while our approach dynamically
tailors a BN for each RE.
Cho and Maida (1992) and Roy (2002) adopted
not exactly BNs but similar probabilistic approaches
for reference resolution and generation respectively.
However, their foci are only on descriptions.
Lison et al (2010) proposed an approach using
Markov logic networks (MLNs) (Richardson and
Domingos, 2006) to reference resolution. They
dealt with only deictic and descriptive REs. Even
though MLNs are also a probabilistic framework, it
is difficult for DS developers to provide quantitative
domain knowledge needed to resolve REs because
MLNs accept domain knowledge in the form of for-
mal logic rules with weights, which must be deter-
mined globally. In contrast, BNs are more flexible
and easy in providing quantitative knowledge to DSs
in the form of conditional probability tables, which
can be determined locally.
As just described, there are several probabilis-
tic approaches to REs but none of them incorpo-
rates reference domains. In the next section, we in-
troduce our REBNs (Referring Expression Bayesian
Networks), a novel Bayesian network-based model-
ing approach to REs that incorporates reference do-
mains.
238
W C X D
Figure 2: WCXD fundamental structure.
2 Bayesian Network-based Modeling of
Referring Expressions
Each REBN is dedicated for a RE in the context at
the moment. Its structure is determined by the syn-
tactic and semantic information in the RE and prob-
ability tables are determined by the context.
2.1 Structures
Figure 2 shows the fundamental network structure
of REBNs. We call this structure WCXD. The four
nodes (random variables)W ,C,X , andD represent
an observed word, the concept denoted by the word,
the referent of the RE, and the presupposed RD, re-
spectively. Here, a word means a lexical entry in
the system dictionary defined by the DS developer
(concept dictionary; section 3.2.1).
Each REBN is constructed by modifying or mul-
tiply connecting the WCXD structure as shown in
figures 3 and 4. Figure 3 shows the network for REs
indicating one referent such as ?that table.? EachWi
node has a corresponding word wi. Figure 4 shows
the network for REs indicating two referents such as
?his table.? We call the class of the former REs s-
REX (simple Referring EXpression) and the class of
the latter REs c-REX (compound Referring EXpres-
sion). Although REBNs have the potential to deal
with c-REX, hereafter we concentrate on s-REX be-
cause the page space is limited and the corpus used
for evaluation contains very few c-REX instances.
Although, in section 1, we explained that (Iida et
al., 2010) handles anaphoric and deictic expressions
in a unified manner, it handles anaphora to instances
only and does not handle that to concepts. There-
fore, it cannot satisfactorily resolve such an expres-
sion ?Bring me the red box, and the blue one, too.?
Here, ?one? does not refer to the physical referent
of ?the red box? but refers to the concept of ?box?.
TheC nodes will enable handling of such references
to concepts. This is one of the important features of
REBNs but will be investigated in future work.
W
1
C
1
X D
W
2
C
2
Figure 3: BN for two-word REs indicating one referent.
W
1
C
1
X
1
D
1
W
2
C
2
X
2
D
2
Figure 4: BN for two-word REs indicating two referents.
2.2 Domains of random variables
A REBN for an s-REX instance of N words
has 2N + 2 discrete random variables:
W1, . . . ,WN , C1, . . . , CN , X , and D. The do-
main of each variable depends on the corresponding
RE and the context at the moment. Here, D(V )
denotes the domain of a random variable V .
D(Wi) contains the corresponding observed word
wi and a special symbol ? that represents other pos-
sibilities, i.e., D(Wi) = {wi,?}. Each Wi has a
corresponding node Ci.
D(Ci) containsM concepts that can be expressed
by wi and a special concept ? that represents other
possibilities, i.e., D(Ci) = {c1i , . . . , cMi ,?}. cji
(j = 1 . . .M ) are looked up from the concept dic-
tionary (see section 3.2.1, table 2).
D(D) contains L + 1 RDs recognized up to that
point in time, i.e., D(D) = {@0,@1, . . . ,@L}. @0
is the ground domain that contains all the individ-
ual entities to be referred to in a dialogue. At the
beginning of the dialogue, D(D) = {@0}. Other
L RDs are incrementally added in the course of the
dialogue.
D(X) contains all the possible referents, i.e., K
individual entities and L + 1 RDs. Thus, D(X) =
{x1, . . . , xK ,@0, . . . ,@L}. Including RDs enables
handling of references to sets.
Then reference resolution is formalized as below:
x? = argmax
x?D(X)
P (X = x|W1 = w1, . . . ,WN = wN ). (1)
P (X|W1, . . . ,WN ) is obtained by marginalizing
the joint probabilities that are computed with the
probability tables described in the next subsection.
239
2.3 Probability tables
Probability distributions are given as (conditional)
probability tables since all the random variables
used in a REBN are discrete. Here, four types of
probability tables used by REBNs are described.
2.3.1 P (Wi|Ci, X)
P (Wi = w|Ci = c,X = x) is the probability that
a hearer observes w from c and x which the speaker
intends to indicate.
In most cases, Wi does not depend on X , i.e.,
P (Wi|Ci, X) ? P (Wi|Ci). X is, however, nec-
essary to handle individualized terms (names).
There are several conceivable ways of probabil-
ity assignment. One simple way is: for each cji ,
P (W = wi|C = cji ) = 1/T, P (W = ?|C =
cji ) = (T ? 1)/T , and for ?, P (W = wi|C =
?) = ", P (W = ?|C = ?) = 1 ? ". Here T is the
number of possible words for cji . " is a predefined
small number such as 10?8. We use this assignment
in the evaluation.
2.3.2 P (Ci|X,D)
P (Ci = c|X = x,D = d) is the probability that
concept c is chosen from D(Ci) to indicate x in d.
The developers of DSs cannot provide
P (Ci|X,D) in advance because D(Ci) is context-
dependent. Therefore, we take an approach of
composing P (Ci|X = x,D = d) from R(cji , x, d)
(cji ? D(Ci)\{?}). Here R(cji , x, d) is the rele-
vancy of concept cji to referent x with regard to d,
and 0 ? R(cji , x, d) ? 1. 1 means full relevancy
and 0 means no relevancy. 0.5 means neutral. For
example, a concept BOX will have a high relevancy
to a suitcase such as 0.8 but a concept BALL will
have a low relevancy to the suitcase such as 0.1.
If x is not in d, R(cji , x, d) is 0. Algorithm 1
in appendix A shows an algorithm to compose
P (Ci|X = x,D = d) from R(cji , x, d). Concept
? will be assigned a high probability if none of
cji ? D(Ci)\{?} has a high relevancy to x.
If cji is static,1 R(cji , x, d) is numerically given in
advance in the form of a table. If not static, it is im-
plemented as a function by the DS developer, that is,
R(cji , x, d) = fcji (x, d, I). Here I is all the informa-tion available from the DS.
1Whether a concept is static or not depends on each DS.
For example, given a situation such as shown in
figure 1, the relevancy function of a positional con-
cept LEFT (suppose a RE such as ?the left piece?)
can be implemented as below:
fLEFT(x, d, I) = (ux ? ur)/(ul ? ur). (2)
Here, ux, ul and ur are respectively the horizontal
coordinates of x, the leftmost piece in d, and the
rightmost piece in d, which are obtained from I . If
x is a RD, the relevancy is given as the average of
entities included in the RD.
2.3.3 P (X|D)
P (X = x|D = d) is the probability that entity x
in RD d is referred to, which is estimated according
to the contextual information at the time the corre-
sponding RE is uttered but irrespective of attributive
information in the RE. The contextual information
includes the history of referring so far (discourse)
and physical statuses such as the gaze of the referrer
(situation). We call P (X = x|D = d) the predic-
tion model.
The prediction model can be constructed by us-
ing a machine learning-based method. We use a
ranking-based method (Iida et al, 2010). The score
output by the method is input into the standard sig-
moid function and normalized to be a probability. If
x is not in d, P (X = x|D = d) is 0.
2.3.4 P (D)
P (D = d) is the probability that RD d is presup-
posed at the time the RE is uttered. We cannot col-
lect data to estimate this probabilistic model because
RDs are implicit. Therefore, we examine three a pri-
ori approximation functions based on the saliency of
d. Saliency is proportional to recency.2
Uniformmodel This model ignores saliency. This
is introduced to see the importance of saliency.
P (D = d) = 1/|D(D)| (3)
Linear model This model distributes probabilities
in proportion to saliency. This is an analogy of the
method used in (Denis, 2010).
P (D = d) = sal(d)?
d??D(D) sal(d?)
(4)
2Assignment of saliency is described in section 3.2.3.
240
Exponential model This model puts emphasis on
recent RDs. This function is so called soft-max.
P (D = d) = exp(sal(d))?
d??D(D) exp(sal(d?))
(5)
3 Experimental Evaluation
We evaluated the potential of the proposed frame-
work by using a situated human-human (HH) dia-
logue corpus.
3.1 Corpus
We used the REX-J Japanese referring expression
corpus (Spanger et al, 2010). The REX-J corpus
consists of 24 HH dialogues in each of which two
participants solve a Tangram puzzle of seven pieces
(see figure 1). The goal of the puzzle is combining
seven pieces to form a designated shape (such as a
swan). One of two subjects takes the role of opera-
tor (OP) and the other takes the role of solver (SV).
The OP can manipulate the virtual puzzle pieces dis-
played on a PC monitor by using a computer mouse
but does not know the goal shape. The SV knows
the goal shape but cannot manipulate the pieces. The
states of the pieces and the mouse cursor operated by
the OP are shared by the two subjects in real time.
Thus, the two participants weave a collaborative dia-
logue including many REs to the pieces. In addition
to REs, the positions and directions of the pieces, the
position of the mouse cursor, and the manipulation
by the OP were recorded with timestamps and the
IDs of relevant pieces.
3.1.1 Annotation
Each RE is annotated with its referent(s) as shown
in table 1. The 1st RE okkiisankaku3 big triangle ?a
big triangle? in the table is ambiguous and refers to
either piece 1 or 2. The 7th and 8th REs refer to
the set of pieces 1 and 2. The other REs refer to an
individual piece.
To skip the structural analysis of REs to avoid
problems due to errors in such analysis, we have
additionally annotated the corpus with intermediate
structures, from which REBNs are constructed. Be-
cause we focus on s-REX only in this paper, the
3Words are not separated by white spaces in Japanese.
intermediate structures are straightforward:4 paren-
thesized lists of separated words as shown in ta-
ble 1. The procedure to generate a REBN of s-REX
from such an intermediate structure is also straight-
forward and thus it is not explained due to the page
limitation.
3.2 Implementations
We use BNJ5 for probabilistic computation. Here
we describe the implementations of resources and
procedures that are more or less specific to the task
domain of REX-J.
3.2.1 Concept dictionary
Table 2 shows an excerpt of the concept dictio-
nary defined for REX-J. We manually defined 40
concepts by observing the dialogues.
3.2.2 Static relevancy table and relevancy
functions
For 13 concepts out of 40, their relevancy values
were manually determined by the authors. Table 3
shows an excerpt of the static relevancy table defined
for the seven pieces shown in figure 1. TRI is rele-
vant only to pieces 1 to 5, and SQR is relevant only
to pieces 6 and 7 but is not totally relevant to piece 7
because it is not a square in a precise sense. FIG is
equally but not very relevant to all the pieces,6
For the remaining 27 concepts, we implemented
relevancy functions (see appendix B).
3.2.3 Updating the list of RDs
In our experiment, REs are sequentially resolved
from the beginning of each dialogue in the corpus.
In the course of resolution, RDs are added into a list
and updated by the following procedure. RDs are
sorted in descending order according to saliency.
At each time of resolution, we assume that all the
previous REs are correctly resolved. Therefore, af-
ter each time of resolution, if the correct referent of
the last RE is a set, we add a new RD equivalent
to the set into the list of RDs, unless the list con-
tains another equivalent RD already. In either case,
the saliency of the RD equivalent to the set is set to
?+1 unless the RD is at the head of the list already.
4In the case of c-REX, graph-like structures are required.
5http://bnj.sourceforge.net/
6This is because concept FIG in REX-J is usually used to
refer to not a single piece but a shaped form (combined pieces).
241
D-ID Role Start End Referring expression Referents Intermediate structure
0801 SV 17.345 18.390 okkiisankaku big triangle 1 or 2 (okkii sankaku)
0801 SV 20.758 21.368 sore it 1 (sore)
0801 SV 23.394 24.720 migigawanookkiisankaku right big triangle 1 (migigawano okkii sankaku)
0801 SV 25.084 25.277 kore this 1 (kore)
0801 SV 26.512 26.671 sono that 1 (sono)
0801 SV 28.871 29.747 konookkiisankaku this big triangle 2 (kono okkii sankaku)
0801 OP 46.497 48.204 okkinasankakkei big triangle 1, 2 (okkina sankakkei)
0801 OP 51.958 52.228 ryo?ho? both 1, 2 (ryo?ho?)
?D-ID? means dialogue ID. ?Start? and ?End? mean the end points of a RE.
Table 1: Excerpt of the corpus annotation (w/ English literal translations).
Concept Words
TRI triangle, right triangle
SQR quadrate, square, regular tetragon
FIG figure, shape
Table 2: Dictionary (excerpted and translated in English).
Concept Relevancy values by piece(1) (2) (3) (4) (5) (6) (7)
TRI 1 1 1 1 1 0 0
SQR 0 0 0 0 0 1 0.8
FIG 0.3 0.3 0.3 0.3 0.3 0.3 0.3
Table 3: Static relevancy table.
Here, ? is the largest saliency value in the list at the
moment (the saliency value of the head RD).
Before each time of resolution, we check whether
the piece that is most recently manipulated after the
previous RE constitutes a perceptual group by using
the method explained in section 3.2.4 at the onset
time of the target RE. If such a group is recognized,
we add a new RD equivalent to the recognized group
unless the list contains another equivalent RD. In ei-
ther case, the saliency of the RD equivalent is set to
?+1 unless the RD is at the head of the list already,
and the focus of the equivalent RD is set to the most
recently manipulated piece.
When a new RD@m is added to the list, a comple-
mentary RD @n and a subsuming RD @l are also in-
serted just after @m in the list. Here, @n = @0\@m
and @l = [@m?,@n]. This operation is required to
handle a concept REST, e.g., ?the remaining pieces.?
3.2.4 Perceptual grouping
There is a generally available method of simulated
perceptual grouping (Tho?risson, 1994). It works
well in a spread situation such as shown in figure 1
but tends to produce results that do not match our
intuition when pieces are tightly packed at the end
of a dialogue. Therefore, we adopt a simple method
that recognizes a group when a piece is attached to
another. This method is less general but works sat-
isfactorily in the REX-J domain due to the nature of
the Tangram puzzle.
3.2.5 Ranking-based prediction model
As mentioned in section 2.3.3, a ranking-based
method (Iida et al, 2010) using SVMrank (Joachims,
2006) was adopted for constructing the prediction
model P (X|D). This model ranks entities accord-
ing to 16 binary features such as whether the tar-
get entity is previously referred to (a discourse fea-
ture), whether the target is under the mouse cursor
(a mouse cursor feature), etc.7
When a target is a set (i.e., a RD), discourse fea-
tures for it are computed as in the case of a piece;
meanwhile, mouse cursor features are handled in a
different manner. That is, if one of the group mem-
bers meets the criterion of a mouse cursor feature,
the group is judged as meeting the criterion.
In (Iida et al, 2010), preparing different models
for pronouns and non-pronouns achieved better per-
formance. Therefore we trained two linear kernel
SVM models for pronouns and non-pronouns with
the 24 dialogues.
3.3 Experiment
We used the 24 dialogues for evaluation.8 As men-
tioned in section 2.1, we focused on s-REX. These
24 dialogues contain 1,474 s-REX instances and 28
c-REX instances. In addition to c-REX, we ex-
cluded REs mentioning complicated concepts, for
which it is difficult to implement relevancy func-
tions in a short time.9 After excluding those REs,
7Following the results shown in (Iida et al, 2010), we did
not use the 6 manipulation-related features (CO1 . . . CO6).
8We used the same data to train the SVM-rank models. This
is equivalent to assuming that we have data large enough to sat-
urate the performance of the prediction model.
9Mostly, those are metaphors such as ?neck? and concepts
related to operations such as ?put.? For example, although
242
P (D) model Most-recent Mono-domain Uniform Linear Exponential
Category Single Plural Total Single Plural Total Single Plural Total Single Plural Total Single Plural Total
w/o S/P info. 42.4 28.8 40.0 77.5 47.3 73.3 77.1 40.6 72.0 78.3 45.1 73.7 76.2 48.4 72.3
w/ S/P info. 44.3 35.4 42.7 84.8 58.8 81.2 84.4 55.0 80.3 85.6 61.0 82.1 83.4 68.1 81.3
Table 4: Results of reference resolution (Accuracy in %).
1,310 REs were available. Out of the 1,310 REs, 182
REs (13.9%) refers to sets, and 612 REs (46.7%) are
demonstrative pronouns such as sore ?it.?
3.3.1 Settings
We presupposed the following conditions.
Speaker role independence: We assumed REs
are independent of speaker roles, i.e., SV and OP.
All REs were mixed and processed serially.
Perfect preprocessing and past information:
As mentioned in sections 3.1.1 and 3.2.3, we as-
sumed that no error comes from preprocessing in-
cluding speech recognition, morphological analysis,
and syntactic analysis;10 and all the correct referents
of past REs are known.11
No future information: In HH dialogue, some-
times information helpful for resolving a RE is pro-
vided after the RE is uttered. We, however, do not
consider such future information.
Numeral information: Many languages includ-
ing English grammatically require indication of nu-
meral distinctions by using such as articles, singu-
lar/plural forms of nouns and copulas, etc. Although
Japanese does not have such grammatical devices,12
it would be possible to predict such distinctions by
using a machine learning technique with linguistic
?putting a piece? and ?getting a piece out? are distinguished
due to speakers? intentions, they are (at least superficially) ho-
mogeneous in the physical data available from the corpus and
difficult for machines to distinguish each other.
10In general, the speech and expressions in human-machine
(HM) dialogue are less complex and less difficult to process
than those in HH dialogue data. This is typcially observed as
fewer disfluencies (Shriberg, 2001) and simpler sentences with
fewer omissions (Itoh et al, 2002). Therefore, when we apply
our framework to real DSs, we can expect clearer and simpler
input and thus better performance. We supposed that the condi-
tion of perfect preprocessing in HH dialogue approximates the
results to those obtained when HM dialogue data is used.
11If a reference is misinterpreted (i.e., wrongly resolved) in a
dialogue, usually that misinterpretation will be repaired by the
interlocutors in the succeeding interaction once the misinterpre-
tation becomes apparent. Therefore, accumulating all past er-
rors in resolution is rather irrational as an experimental setting.
12Japanese has a plurality marker -ra (e.g., sore-ra), but use
of it is not mandatory (except for personal pronouns).
and gestural information. Therefore we observed the
effect of providing such information. In the follow-
ing experiment we provide the singular/plural dis-
tinction information to REBNs by looking at the an-
notations of the correct referents in advance. This
is achieved by adding a special evidence node C0,
where D(C0) = {S,P}. P (C0 = S|X = x) = 1
and P (P|x) = 0 if x is a piece. On the contrary,
P (S|x) = 0 and P (P|x) = 1 if x is a set.
3.3.2 Baselines
To our best knowledge, there is no directly com-
parable method. We set up two baselines. The first
baseline uses the most recent as the resolved refer-
ent for each RE (Initial resolution of each dialogue
always fails). This baseline is called Most-recent.
As the second baseline, we prepared another
P (D) model in addition to those explained in sec-
tion 2.3.4, which is called Mono-domain. In Mono-
domain, D(D) consists of only a single RD @?0,
which contains individual pieces and the RDs recog-
nized up to that point in time. That is, @?0 = D(X).
Resolution using this model can be considered as
a straightforward extension of (Iida et al, 2010),
which enables handling of richer concepts in REs13
and handling of REs to sets14.
3.3.3 Results
The performance of reference resolution is pre-
sented by category and by condition in terms of ac-
curacy (# of correctly resolved REs/# of REs).
We set up the three categories in evaluating res-
olution, that is, Single, Plural, and Total. Category
Single is the collection of REs referring to a single
piece. Plural is the collection of REs referring to a
set of pieces. Total is the sum of them. Ambigu-
ous REs such as the first one in table 1 are counted
as ?Single? and the resolution of such a RE is con-
sidered correct if the resolved result is one of the
possible referents.
13(Iida et al, 2010) used only object types and sizes. Other
concepts such as LEFT were simply ignored.
14(Iida et al, 2010) did not deal with REs to sets.
243
?w/o S/P info.? indicates experimental results
without singular/plural distinction information. ?w/
S/P info.? indicates experimental results with it.
Table 4 shows the results of reference resolution
per P (D) modeling method.15 Obviously S/P infor-
mation has a significant impact.
While the best performance for category Single
was achieved with the Linear model, the best perfor-
mance for Plural was achieved with the Exponen-
tial model. If it is possible to know whether a RE
is of Single or Plural, that is, if S/P information is
available, we can choose a suitable P (D) model.
Therefore, by switching models, the best perfor-
mance of Total with S/P information reached 83.4%,
and a gain of 2.0 points against Mono-domain was
achieved (sign test, p < 0.0001).
Because the corpus did not include many in-
stances to which the notion of reference domains is
effective, the impact of RDs may appear small on the
whole. In fact, the impact was not small. By intro-
ducing RDs, resolution in category Plural achieved
a significant advancement. The highest gain from
Mono-domain was 9.3 points (sign test, p < 0.005).
Moreover, more REs containing positional concepts
such as LEFT and RIGHT were correctly resolved
in the cases of Uniform, Linear, and Exponential.
Table 5 summarizes the resolution results of four
positional concepts (with S/P information). While
Mono-domain resolved 65% of them, Linear cor-
rectly resolved 75% (sign test, p < 0.05).
As shown in table 4, the performance of the Uni-
form model was worse than that of Mono-domain.
This indicates that RDs introduced without an ap-
propriate management of them would be harmful
noise. Conversely, it also suggests that there might
be a room for improvement by looking deeply into
the management of RDs (e.g., forgetting old RDs).
4 Conclusion
This paper proposed a probabilistic approach to ref-
erence resolution, REBNs, which stands for Refer-
ring Expression Bayesian Networks. At each time
of resolution, a dedicated BN is constructed for the
15According to the results of preliminary experiments, even
in the case of the Uniform/Linear/Exponential models, we re-
solved the REs having demonstratives with the Mono-domain
model. This is in line with the finding of separating models
between pronouns and non-pronouns in (Iida et al, 2010).
Concept Count Mono Uni. Lin. Exp.
LEFT 21 11 12 16 13
RIGHT 33 23 23 25 27
UPPER 9 6 6 6 4
LOWER 6 5 4 5 4
Total 69 45 45 52 48
(Count means the numbers of occurrence of each concept. Mono, Uni.,
Lin., and Exp. correspond to Mono-domain, Uniform, Linear and Ex-
ponential.)
Table 5: Numbers of correctly resolved REs containing
positional concepts.
RE in question. The constructed BN deals with ei-
ther descriptive, deictic or anaphoric REs in a uni-
fied manner. REBNs incorporate the notion of ref-
erence domains (RDs), which enables the resolution
of REs with context-dependent attributes and han-
dling of REs to sets. REBNs are for task-oriented
dialogue systems and presuppose a certain amount
of domain-dependent manual implementation by de-
velopers. Therefore, REBNs would not be suited
to general text processing or non-task-oriented sys-
tems. However, REBNs have the potential to be a
standard approach that can be used for any and all
task-oriented applications such as personal agents in
smart phones, in-car systems, service robots, etc. ?
The proposed approach was evaluated with the
REX-J human-human dialogue corpus and promis-
ing results were obtained. The impact of incorpo-
rating RDs in the domain of the REX-J corpus was
recognizable but not so large on the whole. How-
ever, in other types of task domains where grouping
and comparisons of objects occur frequently, the im-
pact would be larger. Note that REBNs are not lim-
ited to Japanese, even though the evaluation used a
Japanese corpus. Evaluations with human-machine
dialogue are important future work.
Although this paper focused on the simple type of
REs without relations, REBNs are potentially able
to deal with complex REs with relations. The eval-
uation for complex REs is necessary to validate this
potential of REBN. Currently REBN assumes REs
whose referents are concrete entities. An extension
for handling abstract entities (Byron, 2002; Mu?ller,
2007) is important future work. Another direction
would be generating REs with REBNs. A generate-
and-test approach is a naive application of REBN
for generation. More efficient method is, however,
necessary.
244
References
John D. Burger and Dennis Connoly. 1992. Probabilistic
resolution of anaphoric reference. In Proceedings of
the AAAI Fall Symposium on Intelligent Probabilistic
Approaches to Natural Language, pages 17?24.
Donna Byron. 2002. Resolving pronominal reference
to abstract entities. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 80?87.
Eugene Charniak and Robert Goldman. 1989. A se-
mantics for probabilistic quantifier-free first-order lan-
guages with particular application to story understand-
ing. In Proceedings of the Eleventh International Joint
Conference on Artificial Intelligence (IJCAI), pages
1074?1079, Menlo Park, CA, USA.
Sehyeong Cho and Anthony Maida. 1992. Using a
Bayesian framework to identify the referent of definite
descriptions. In Proceedings of the AAAI Fall Sympo-
sium on Intelligent Probabilistic Approaches to Natu-
ral Language, pages 39?46.
Robert Dale and Ehud Reiter. 1995. Computational in-
terpretations of the Gricean maxims in the generation
of referring expressions. Cognitive Science, 18:233?
263.
Robert Dale and Jette Viethen. 2009. Referring expres-
sion generation through attribute-based heuristics. In
Proceedings of the the 12th European Workshop on
Natural Language Generation (ENLG), pages 59?65,
Athens, Greece, March.
Alexandre Denis. 2010. Generating referring expres-
sions with reference domain theory. In Proceedings
of the 6th International Natural Language Generation
Conference (INLG), pages 27?35.
Petra Gieselmann. 2004. Reference resolution mech-
anisms in dialogue management. In Proceedings of
the 8th workshop on the semantics and pragmatics of
dialogue (CATALOG), pages 28?34, Barcelona, Italy,
July.
Ryu Iida, Shumpei Kobayashi, and Takenobu Tokunaga.
2010. Incorporating extra-linguistic information into
reference resolution in collaborative task dialogue. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, pages 1259?
1267, Uppsala, Sweden, July.
Toshihiko Itoh, Atsuhiko Kai, Tatsuhiro Konishi, and
Yukihiro Itoh. 2002. Linguistic and acoustic changes
of user?s utterances caused by different dialogue situa-
tions. In Proceedings of the 7th International Confer-
ence on Spoken Language Processing (ICSLP), pages
545?548.
Finn V. Jensen and Thomas D. Nielsen. 2007. Bayesian
Networks and Decision Graphs. Springer, second edi-
tion.
Thorsten Joachims. 2006. Training linear SVMs in lin-
ear time. In Proceedings of the ACM Conference on
Knowledge Discovery and Data Mining (KDD), pages
217?226, Philadelphia, PA, USA, August.
Hans Kamp and Uwe Reyle. 1993. From Discourse to
Logic. Kluwer Academic Publishers.
Emiel Krahmer, Sebastiaan van Erk, and Andre? Verleg.
2003. Graph-based generation of referring expres-
sions. Computational Linguistics, 29:53?72.
Pierre Lison, Carsten Ehrler, and Geert-Jan M. Kruijff.
2010. Belief modelling for situation awareness in
human-robot interaction. In Proceedings of the 19th
International Symposium on Robot and Human In-
teractive Communication (RO-MAN), pages 138?143,
Viareggio, Italy, September.
Ruslan Mitkov. 2002. Anaphora Resolution. Studies in
Language and Linguistics. Pearson Education.
Christoph Mu?ller. 2007. Resolving it, this, and that in
unrestricted multi-party dialog. In Proceedings of the
45th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 816?823.
Vincent Ng. 2010. Supervised noun phrase coreference
research: The first fifteen years. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1396?1411, Uppsala, Swe-
den, July.
Judea Pearl. 1988. Probabilistic Reasoning in Intelli-
gent Systems: Networks of Plausible Inference. Mor-
gan Kaufmann, San Mateo, CA, USA.
Matthew Richardson and Pedor Domingos. 2006.
Markov logic networks. Machine Learning, 62(1?
2):107?136.
Deb K. Roy. 2002. Learning visually-grounded words
and syntax for a scene description task. Computer
Speech and Language, 16(3):353?385.
Susanne Salmon-Alt and Laurent Romary. 2000. Gen-
erating referring expressions in multimodal contexts.
In Proceedings of the INLG 2000 workshop on Coher-
ence in Generated Multimedia, Mitzpe Ramon, Israel,
June.
Susanne Salmon-Alt and Laurent Romary. 2001. Ref-
erence resolution within the framework of cognitive
grammar. In Proceedings of the International Col-
loqium on Cognitive Science, San Sebastian, Spain,
May.
Elizabeth Shriberg. 2001. To ?errrr? is human: ecology
and acoustics of speech disfluencies. Journal of the
International Phonetic Association, 31(1):153?169.
Philipp Spanger, Masaaki Yasuhara, Ryu Iida, Takenobu
Tokunaga, Asuka Terai, and Naoko Kuriyama. 2010.
REX-J: Japanese referring expression corpus of sit-
uated dialogs. Language Resources and Evaluation.
Online First, DOI: 10.1007/s10579-010-9134-8.
245
Kristinn R. Tho?risson. 1994. Simulated perceptual
grouping: An application to human-computer interac-
tion. In Proceedings of the 16th Annual Conference
of the Cognitive Science Society, pages 876?881, At-
lanta, GA, USA.
Davy Weissenbacher and Adeline Nazarenko. 2007. A
Bayesian approach combining surface clues and lin-
guistic knowledge: Application to the anaphora reso-
lution problem. In Proceedings of the International
Conference Recent Advances in Natural Language
Processing (RANLP), Borovets, Bulgaria.
Davy Weissenbacher. 2005. A Bayesian network for the
resolution of non-anaphoric pronoun it. In Proceed-
ings of the NIPS 2005 Workshop on Bayesian Meth-
ods for Natural Language Processing, Whistler, BC,
Canada.
A Algorithm to compose P (C|X,D)
Algorithm 1 Composing P (C|X = x,D = d).
Input: D(C); R(c, x, d) for all c ? D(C)\{?}
Output: P (C|X = x,D = d)
1: n ? 0, s ? 0, S = D(C)\{?}
2: for all c ? S do
3: r[c] ? R(c, x, d) #{Relevancy of concept c}
4: s ? s+ r[c] #{Sum of relevancy r[c]}
5: n ? n+ (1? r[c]) #{Sum of residual (1? r[c])}
6: end for
7: r[?] ? n/|S|
8: s ? s+ r[?]
9: for all c ? D(C) do
10: P (C = c|X = x,D = d) ? r[c]/s
11: end for
(#{. . . } is a comment.)
B Relevancy functions
As explained in section 2.3.2, the relevancy func-
tions for positional concepts such as LEFT and
RIGHT were implemented as geometric calcula-
tions. Here several other relevancy functions are
shown with corresponding example REs.
?this figure?:
R(FIG, x, d)
=
?
?
?
0.3 : if single(x)
1 : if not single(x) and shape(x)
0 : otherwise
(single(x) means x is a single piece. shape(x)
means x is a set of pieces that are concatenated and
form a shape. 0.3 comes from the static relevancy
table.)
?both the triangles?:
R(BOTH, x, d) =
{
1 : if |x| = 2
0 : otherwise
?another one?:
R(ANOTHER, x, d) =
{
1 : if foc(d) 6= x
0 : otherwise
?the remaining ones?:
R(REST, x, d) =
{
1 : if d = [x, y?]
0 : otherwise
(REST requires |d| = 2, and both x and y are sets.
ANOTHER does not.)
?all?:
R(ALL, x, d) =
{
1 : if x = d
0 : otherwise
(ALL does not always refer to @0.)
246
Proceedings of the 14th European Workshop on Natural Language Generation, pages 147?151,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Automatic Voice Selection in Japanese based on Various Linguistic
Information
Ryu Iida and Takenobu Tokunaga
Department of Computer Science, Tokyo Institute of Technology
W8-73, 2-12-1 Ookayama Meguro Tokyo, 152-8552 Japan
{ryu-i,take}@cl.cs.titech.ac.jp
Abstract
This paper focuses on a subtask of natu-
ral language generation (NLG), voice se-
lection, which decides whether a clause is
realised in the active or passive voice ac-
cording to its contextual information. Au-
tomatic voice selection is essential for re-
alising more sophisticated MT and sum-
marisation systems, because it impacts
the readability of generated texts. How-
ever, to the best of our knowledge, the
NLG community has been less concerned
with explicit voice selection. In this pa-
per, we propose an automatic voice se-
lection model based on various linguistic
information, ranging from lexical to dis-
course information. Our empirical evalua-
tion using a manually annotated corpus in
Japanese demonstrates that the proposed
model achieved 0.758 in F-score, outper-
forming the two baseline models.
1 Introduction
Generating a readable text is the primary goal
in natural language generation (NLG). To realise
such text, we need to arrange discourse entities
(e.g. NPs) in appropriate positions in a sentence
according to their discourse salience. Consider the
two following Japanese texts, each of which con-
sists of two sentences.
(1) Tomi-wa kouenj-ni it-ta .
Tomi-TOP parkj-IOBJ go-PAST
(Tomi went to a parkj .)
Karei-wa sokoj-de ookina inu-ni oikake-rareta .
hei-TOP therej-LOC big dog-IOBJ chase-PASSIVE/PAST
(Hei was chased by a big dog therej .)
(2) Tomi-wa kouenj-ni it-ta .
Tomi-TOP parkj-IOBJ go-PAST
(Tomi went to a parkj .)
Ookina inu-ga sokoj-de karei-o oikake-ta .
big dog-SUBJ therej-LOC hei-OBJ chase-PAST
(A big dog chased himi therej .)
In (1), ?Tomi? is topicalised in the first sentence,
and then it appears at the subject position in the
second sentence. In contrast, the same argu-
ment, i.e. ?hei? is realised at the object position
in the second sentence of text (2). Intuitively,
text (1) is relatively more natural than text (2).
Thus, given the two predicate argument relations,
go(SUBJ:Tomi, IOBJ:parkj) and chase(SUBJ:big
dog, OBJ:Tomi, IOBJ:parkj), a generation system
should choose text (1).
The realisation from a semantic representation
(e.g. predicate argument structures) to an actual
text has been mainly developed in the area of nat-
ural language generation (Reiter and Dale, 2000),
and has been applied to various NLP applications
such as multi-document summarisation (Radev
and McKeown, 1998) and tutoring systems (Di
Eugenio et al, 2005). During the course of a
text generation process, various kinds of decisions
should be made, including decisions on textual
content, clustering the content of each clause, dis-
course structure of the clauses, lexical choices,
types of referring expressions and syntactic struc-
tures. Since these different kinds of decisions are
interrelated to each other, it is not a trivial prob-
lem to find an optimal order among these deci-
sions. This issue has been much discussed in terms
of architecture of generation systems. Although a
variety of architectures has been proposed in the
past, e.g. an integrated architecture (Appelt, 1985)
and a revision-based architecture (Inui et al, 1994;
Robin, 1994), a pipeline architecture is considered
as a consensus architecture in which decisions
are made in a predetermined order (Reiter, 1994).
Voice selection is a syntactic decision that tends to
be made in a later stage of the pipeline architec-
ture, even though it influences various decisions,
such as discourse structure and lexical choice. Un-
like referring expression generation, voice selec-
tion has received less attention and been less dis-
cussed in the past. Against this background, this
147
research tackles the problem of voice selection
considering a wide range of linguistic information
that is assumed to be already decided in the pre-
ceding stages of a generation process.
The paper is organised as follows. We first
overview the related work in Section 2, and then
propose a voice selection model based on the four
kinds of information that impact voice selection
in Section 3. Section 4 then demonstrates the re-
sults of empirical evaluation using the NAIST Text
Corpus (Iida et al, 2007) as training and evalu-
ation data sets. Finally, Section 5 concludes and
discusses our future directions.
2 Related work
The task of automatic voice selection has been
mainly developed in the NLG community. How-
ever, it has attracted less attention compared with
other major NLG problems, such as generating re-
ferring expressions. There is less work focusing
singly on voice selection, but not entirely with-
out exception, such as Abb et al (1993). In their
work, passivisation is performed by taking into ac-
count both linguistic and extra-linguistic informa-
tion. The linguistic information explains passivi-
sation in an incremental generation process; realis-
ing the most salient discourse entity in short term
memory as a subject eventually leads to passivi-
sation. In contrast, extra-linguistic information is
used to move a less salient entity to a subject posi-
tion when an explicit agent is missing in the text.
Although these two kinds of information seem ad-
equate for explaining passivisation, their applica-
bility was not examined in empirical evaluations.
Sheikha and Inkpen (2011) focused attention on
voice selection in the generation task distinguish-
ing formal and informal sentences. In their work,
passivisation is considered as a rhetorical tech-
nique for conveying formal intentions. However,
they did not discuss passivisation in terms of dis-
course coherence.
3 Voice selection model
We recast the voice selection task into a binary
classification problem, i.e. given a predicate with
its arguments and its preceding context, we clas-
sify the predicate into either an active or passive
class, taking into account predicate argument rela-
tions and the preceding context of the predicate.
As shown in examples (1) and (2) in Section 1,
several factors have an impact on voice selection
in a text. In this work, we take into account the
following four information as features The details
of the feature set are shown in Table 1.
Passivisation preference of each verb An im-
portant factor of voice selection is the preference
for how frequently a verb is used in passive sen-
tences. This means each verb has a potential ten-
dency of being used in passive sentences in a do-
main. For example, the verb ?yosou-suru (to ex-
pect)? tends to be realised in the passive in the
newspaper domain because Japanese journalists
tend to write their opinions objectively by omitting
the agent role. To take into account this preference
of verb passivisation, we define a preference score
by the following formula:
scorepas(vi) =
freqpas(vi)
freqall(vi)
? log freqall(vi) (1)
where vi is a verb in question1, freqall(vi) is
the frequency of vi appearing in corpora, and
freqpas(vi) is the frequency of vi with the passive
marker, (ra)reru. The logarithm of freqall(vi) is
multiplied due to avoiding the overestimation of
the score for less frequent instances. In the evalua-
tion, the preference score was calculated based on
the frequency of each verb in the 12 years worth
of newspaper articles, which had been morpho-
syntactically analysed by a Japanese morpholog-
ical analyser Mecab3 and a dependency parser
CaboCha4.
Syntactic decisions As described in Section 1,
various kinds of decisions are interrelated to voice
selection. Particularly, syntactic decisions includ-
ing voice selection directly impact sentence struc-
ture. Therefore, we introduce syntactic informa-
tion except for voice selection which prescribes
how an input predicate-argument structure will be
realised in an actual text.
Semantic category of arguments Animacy of
the arguments of a predicate has an impact on their
syntactic positions. Unlike in English, inanimate
subjects tend to be avoided in Japanese. In order
to capture this tendency, we use the semantic cate-
gory of the arguments of the verb in question (e.g.
1Note that the preference needs to be defined for each
word sense. However, we here ignore the difference of senses
because selecting a correct verb sense for a given context is
still difficult.
1Bunsetsu is a basic unit in Japanese, consisting of at least
one content word and more than zero functional words.
2http://nlp.cs.nyu.edu/irex/index-e.html
3https://code.google.com/p/mecab/
4https://code.google.com/p/cabocha/
148
type feature definition
PRED scorepas passivisation preference score defined in equation (1).
lexical lemma of P .
func lemma of functional words following P , excluding passive markers.
SYN sent end 1 if P appears in the last bunsetsu1-unit in a sentence; otherwise 0.
adnom 1 if P appears in an adnominal clause; otherwise 0.
first sent (last sent) 1 if P appears in the first (last) sentence of a text; otherwise 0.
subj(obj,iobj) embedded 1 if the head of the adnominal clause including P is semantic subject (object, indi-
rect object) of P ; otherwise 0.
ARG subj(obj,iobj) ne named entity class (based on IREX2) of the subject (object, indirect object) of P .
subj(obj,iobj) sem semantic class of the subject (object, indirect object) of P in terms of Japanese
ontology, nihongo goi taikei (Ikehara et al, 1997).
COREF subj(obj,iobj) exo 1 if the subject (object, indirect object) of P is unrealised and it is annotated as
exophoric; otherwise 0.
subj(obj,iobj) srl order order of the subject (object, indirect object) of P in the SRL.
subj(obj,iobj) srl rank rank of the subject (object, indirect object) of P in the SRL.
subj(obj,iobj) coref num number of discourse entities in the coreference chain including P?s subject (object,
indirect object) in the preceding context.
P stands for the predicate in question. The four feature types (PRED, SYN, ARG and COREF) correspond to each information
described in Section 3.
Table 1: Feature set for voice selection
named entity labels provided by CaboCha, such as
Person and Organisation, and the ontological in-
formation defined in a Japanese ontology, nihongo
goi taikei (Ikehara et al, 1997)) as features.
Coreference and anaphora of arguments As
discussed in discourse theories such as Centering
Theory (Grosz et al, 1995), arguments which have
been already most salient in the preceding context
tend to be placed at the beginning of a sentence for
reducing the cognitive cost of reading, as argued in
Functional Grammar (Halliday and Matthiessen,
2004). In order to consider the characteristic, we
employ an extension of Centering Theory (Grosz
et al, 1995), proposed by Nariyama (2002) for
implementing the COREF type features in Table 1.
She proposed a generalised version of the forward
looking-center list, called the Salient Reference
List (SRL), which stores all salient discourse en-
tities (e.g. NP) in the preceding contexts in the or-
der of their saliency. A highly ranked argument?s
entity in the SRL tends to be placed in the sub-
ject position, resulting in a passive sentence if that
salient entity has a THEME role in the predicate-
argument structure. To capture this characteristic,
the order and rank of discourse entities in the SRL
are used as features5.
In addition, as described in Abb et al (1993),
if the agent filler of a predicate is underspecified,
the passive voice is preferred so as to unfocus the
underspecified agent. Likewise, if the argument
5In Table 1 ?* srl rank? stands for how highly the argu-
ment?s referent ranked out of the discourse entities in the
SRL, while ?* srl order? stands for which slot (e.g. TOP slot
or SUBJ slot, etc.) stores the argument?s referent.
(in this case, the agent filler) of a predicate is ex-
ophoric, the passive voice is selected.
4 Experiments
We conducted an empirical evaluation using man-
ually annotated newspaper articles in Japanese. To
estimate the feature weights of each classifier, we
used MEGAM6, an implementation of the Maxi-
mum Entropy model, with default parameter set-
tings. We also used SVM7 with a polynomial ker-
nel for explicitly handling the dependency of the
proposed features.
4.1 Data and baseline models
For training and evaluation, we used the NAIST
Text Corpus (Iida et al, 2007). Because the cor-
pus contains manually annotated predicate argu-
ment relations and coreference relations, we used
those for the inputs of voice selection. In our prob-
lem setting, we conducted an intrinsic evaluation;
given manually annotated predicate argument re-
lations and coreference relations of arguments, a
model determines whether a predicate in question
is actually realised in the passive or active voice
in the original text. The performance is measured
based on recall, precision and F-score of correctly
detecting passive voice. For evaluation, we di-
vided the texts in the corpus into two sets; one is
used for training and the other for evaluation. The
details of this division are shown in Table 2.
We employed two baseline models for compar-
6http://www.cs.utah.edu/?hal/megam/
7http://svmlight.joachims.org/
149
#articles #predicates #passive predicates
training 1,753 65,592 4,974 (7.6%)
test 696 24,884 1,891 (7.6%)
Table 2: Data set division for evaluation
R P F
? = 0.1 0.768 0.269 0.399
? = 0.2 0.573 0.357 0.440
? = 0.3 0.403 0.450 0.425
? = 0.4 0.293 0.512 0.373
? = 0.5 0.161 0.591 0.253
? = 0.6 0.091 0.692 0.162
? = 0.7 0.060 0.717 0.111
? = 0.8 0.030 0.851 0.058
? = 0.9 0.014 1.000 0.027
Table 3: Effect of threshold ? for scorepas
ison. One is based on the passivisation preference
of each verb. The model uses only scorepas(vi)
defined in equation (1), that is, it selects the pas-
sive voice if the score is more than the threshold
parameter ?; otherwise, it selects the active voice.
The other baseline model is based on the infor-
mation that the existence of an exophoric subject
results in selecting the passive voice. To capture
this characteristic, the model classifies a verb in
question as passive if the annotated subject is ex-
ophoric; otherwise, it selects the active voice.
4.2 Results
We first evaluated performance of the first baseline
model with various ?. The results are shown in
Table 3, demonstrating that the baseline achieved
its best F-score when ? is 0.2. Therefore, we set
the ? to 0.2 in the following comparison.
Table 4 shows the results of the baselines and
proposed models. To investigate the impact of
each feature type, we conducted feature ablation
when using the maximum entropy model (ME:* in
Table 4). Table 4 shows that the model using the
feature type PRED achieves the best performance
among the four models when using a single feature
type. In addition, by adding feature type(s), the F-
score monotonically improves. Finally, the results
of the model using the PRED, ARG and COREF fea-
tures achieved the best F-score, 0.605, out of the
two baselines and models based on the maximum
entropy model. It indicates that each of the fea-
tures except SYN feature contributes to improving
performance in a complementary manner.
Furthermore, the results of the model using
SVM with the second degree polynomial kernel
show better performance than any model based on
model R P F
baseline1: scorepas ? 0.2 0.573 0.357 0.440
baseline2: exophora 0.493 0.329 0.395
ME: PRED 0.270 9.612 0.374
ME: SYN 0.000 N/A N/A
ME: ARG 0.095 0.516 0.161
ME: COREF 0.092 0.574 0.159
ME: PRED+SYN 0.282 0.618 0.387
ME: PRED+ARG 0.380 0.647 0.479
ME: PRED+COREF 0.480 0.762 0.589
ME: SYN+ARG 0.133 0.558 0.215
ME: SYN+COREF 0.147 9.618 9.238
ME: ARG+COREF 0.267 0.661 0.380
ME: PRED+SYN+ARG 0.397 0.656 0.494
ME: PRED+SYN+COREF 0.485 0.760 0.592
ME: PRED+ARG+COREF 0.506 0.752 0.605
ME: SYN+ARG+COREF 0.281 0.673 0.397
ME: ALL 0.507 0.747 0.604
SVM(linear): ALL 0.456 0.792 0.579
SVM(poly-2d): ALL 0.679 0.858 0.758
Table 4: Results of automatic voice selection
the maximum entropy model. This means that the
combination of features is important in this task
because of the dependency among the four kinds
of information introduced in Section 3.
5 Conclusion
This paper focused on the task of automatic voice
selection in text generation, taking into account
four kinds of linguistic information: passivisa-
tion preference of verbs, syntactic decisions, se-
mantic category of the arguments of a predicate,
and coreference or anaphoric relations of the argu-
ments. For empirical evaluation of voice selection
in Japanese, we used the predicate argument re-
lations and coreference relations annotated in the
NAIST Text Corpus (Iida et al, 2007). Integrat-
ing the four kinds of linguistic information into
a machine learning-based approach contributed to
improving F-score by about 0.3, compared to the
best baseline model, which utilises only the pas-
sivisation preference. Finally, we achieved 0.758
in F-score by combining features using SVM.
As future work, we are planning to incorpo-
rate the proposed voice selection model into natu-
ral language generation models for more sophisti-
cated text generation. In particular, generating re-
ferring expressions and voice selection are closely
related because both tasks utilise similar linguistic
information (e.g. salience and semantic informa-
tion of arguments) for generation. Therefore, our
next challenge is to solve problems about gener-
ating referring expressions and voice selection si-
multaneously by using optimisation techniques.
150
References
B. Abb, M. Herweg, and K. Lebeth. 1993. The incre-
mental generation of passive sentences. In Proceed-
ings of the 6th EACL, pages 3?11.
Douglas E. Appelt. 1985. Planning English referring
expressions. Artificial Intelligence, 26(1):1?33.
Barbara Di Eugenio, Davide Fossati, Dan Yu, Susan
Haller, and Michael Glass. 2005. Natural language
generation for intelligent tutoring systems: A case
study. In Proceedings of the 2005 conference on Ar-
tificial Intelligence in Education: Supporting Learn-
ing through Intelligent and Socially Informed Tech-
nology, pages 217?224.
B. J. Grosz, A. K. Joshi, and S. Weinstein. 1995.
Centering: A framework for modeling the local co-
herence of discourse. Computational Linguistics,
21(2):203?226.
M. A. K. Halliday and C. Matthiessen. 2004. An Intro-
duction to Functional Grammar. Routledge.
R. Iida, M. Komachi, K. Inui, and Y. Matsumoto. 2007.
Annotating a Japanese text corpus with predicate-
argument and coreference relations. In Proceeding
of the ACL Workshop ?Linguistic Annotation Work-
shop?, pages 132?139.
S. Ikehara, M. Miyazaki, S. Shirai A. Yokoo,
H. Nakaiwa, K. Ogura, Y. Ooyama, and Y. Hayashi.
1997. Nihongo Goi Taikei (in Japanese). Iwanami
Shoten.
Kentaro Inui, Takenobu Tokunaga, and Hozumi
Tanaka. 1994. Text revision: A model and its
implementation. In Aspects of Automated Natural
Language Generation: Proceedings of the 6th Inter-
national Natural Language Generation Workshop,
pages 215?230.
S. Nariyama. 2002. Grammar for ellipsis resolution
in Japanese. In Proceedings of the 9th International
Conference on Theoretical and Methodological Is-
sues in Machine Translation, pages 135?145.
D. R. Radev and K. R. McKeown. 1998. Generat-
ing natural language summaries from multiple on-
line sources. Computational Linguistics, 24(3):469?
500.
E. Reiter and R. Dale. 2000. Building Natural Lan-
guage Generation Systems. Cambridge University
Press.
Ehud Reiter. 1994. Has a consensus NL generation
architecture appeared, and is it psycholinguistically
plausible? In Proceedings of the Seventh Interna-
tional Workshop on Natural Language Generation,
pages 163?170.
Jacques Robin. 1994. Revision-based Generation
of Natural Language Summaries Providing His-
torical Background ? Corpus-based Analysis, De-
sign, Implementation and Evaluation. Ph.D. thesis,
Columbia University.
F. Abu Sheikha and D. Inkpen. 2011. Generation of
formal and informal sentences. In Proceedings of
the 13th European Workshop on Natural Language
Generation, pages 187?193.
151
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 214?222,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Investigation of annotator?s behaviour using eye-tracking data
Ryu Iida Koh Mitsuda Takenobu Tokunaga
Department of Computer Science, Tokyo Institute of Technology
{ryu-i,mitsudak,take}@cl.cs.titech.ac.jp
Abstract
This paper presents an analysis of an anno-
tator?s behaviour during her/his annotation
process for eliciting useful information for
natural language processing (NLP) tasks.
Text annotation is essential for machine
learning-based NLP where annotated texts
are used for both training and evaluat-
ing supervised systems. Since an annota-
tor?s behaviour during annotation can be
seen as reflecting her/his cognitive process
during her/his attempt to understand the
text for annotation, analysing the process
of text annotation has potential to reveal
useful information for NLP tasks, in par-
ticular semantic and discourse processing
that require deeper language understand-
ing. We conducted an experiment for col-
lecting annotator actions and eye gaze dur-
ing the annotation of predicate-argument
relations in Japanese texts. Our analysis
of the collected data suggests that obtained
insight into human annotation behaviour
is useful for exploring effective linguis-
tic features in machine learning-based ap-
proaches.
1 Introduction
Text annotation is essential for machine learn-
ing (ML)-based natural language processing
(NLP) where annotated texts are used for
both training and evaluating supervised systems.
This annotation-then-learning approach has been
broadly applied to various NLP tasks, ranging
from shallow processing tasks, such as POS tag-
ging and NP chunking, to tasks requiring deeper
linguistic information, such as coreference resolu-
tion and discourse relation classification, and has
been largely successful for shallow NLP tasks in
particular. The key to this success is how use-
ful information can be effectively introduced into
ML algorithms as features. With shallow NLP
tasks, surface information like words and their
POS within a window of a certain size can be eas-
ily employed as useful features. In contrast, in
semantic and discourse processing, such as coref-
erence resolution and discourse structure analy-
sis, it is not trivial to employ as features deeper
linguistic knowledge and human linguistic intu-
ition that are indispensable for these tasks. In
order to improve system performance, past at-
tempts have integrated deeper linguistic knowl-
edge through manually constructed linguistic re-
sources such as WordNet (Miller, 1995) and lin-
guistic theories such as Centering Theory (Grosz
et al, 1995). They partially succeed in improv-
ing performance, but there is still room for further
improvement (duVerle and Prendinger, 2009; Ng,
2010; Lin et al, 2010; Pradhan et al, 2012).
Unlike past attempts relying on heuristic fea-
ture engineering, we take a cognitive science ap-
proach to improving system performance. In stead
of employing existing resources and theories, we
look into human behaviour during annotation and
elicit useful information for NLP tasks requir-
ing deeper linguistic knowledge. Particularly we
focus on annotator eye gaze during annotation.
Because of recent developments in eye-tracking
technology, eye gaze data has been widely used
in various research fields, including psycholin-
guistics and problem solving (Duchowski, 2002).
There have been a number of studies on the rela-
tions between eye gaze and language comprehen-
sion/production (Griffin and Bock, 2000; Richard-
son et al, 2007). Compared to the studies on
language and eye gaze, the role of gaze in gen-
eral problem solving settings has been less stud-
ied (Bednarik and Tukiainen, 2008; Rosengrant,
2010; Tomanek et al, 2010). Since our current in-
terest, text annotation, can be considered a prob-
lem solving as well as language comprehension
task, we refer to them when defining our prob-
214
lem setting. Through analysis of annotators? eye-
tracking data, we aim at finding useful information
which can be employed as features in ML algo-
rithms.
This paper is organised as follows. Section 2
presents the details of the experiment for collect-
ing annotator behavioural data during annotation
as well as details on the collected data. Section 3
explains the structure of the annotation process
for a single annotation instance. Section 4 pro-
vides a detailed analysis of human annotation pro-
cesses, suggesting usages of those results in NLP.
Section 5 reviews the related work and Section 6
concludes and discusses future research direc-
tions.
2 Data collection
2.1 Materials and procedure
We conducted an experiment for collecting anno-
tator actions and eye gaze during the annotation
of predicate-argument relations in Japanese texts.
Given a text in which candidates of predicates
and arguments were marked as segments (i.e. text
spans) in an annotation tool, the annotators were
instructed to add links between correct predicate-
argument pairs by using the keyboard and mouse.
We distinguished three types of links based on the
case marker of arguments, i.e. ga (nominative),
o (accusative) and ni (dative). For elliptical argu-
ments of a predicate, which are quite common in
Japanese texts, their antecedents were linked to the
predicate. Since the candidate predicates and ar-
guments were marked based on the automatic out-
put of a parser, some candidates might not have
their counterparts.
We employed a multi-purpose annotation tool
Slate (Kaplan et al, 2012), which enables anno-
tators to establish a link between a predicate seg-
ment and its argument segment with simple mouse
and keyboard operations. Figure 1 shows a screen-
shot of the interface provided by Slate. Segments
for candidate predicates are denoted by light blue
rectangles, and segments for candidate arguments
are enclosed with red lines. The colour of links
corresponds to the type of relations; red, blue and
green denote nominative, accusative and dative re-
spectively.
In order to collect every annotator operation, we
modified Slate so that it could record several im-
portant annotation events with their time stamp.
The recorded events are summarised in Table 1.
Event label Description
create link start creating a link starts
create link end creating a link ends
select link a link is selected
delete link a link is deleted
select segment a segment is selected
select tag a relation type is selected
annotation start annotating a text starts
annotation end annotating a text ends
Table 1: Recorded annotation events
Figure 2: Snapshot of annotation using Tobii T60
Annotator gaze was captured by the Tobii T60
eye tracker at intervals of 1/60 second. The Tobii?s
display size was 1, 280?1, 024 pixels and the dis-
tance between the display and the annotator?s eye
was maintained at about 50 cm. The five-point cal-
ibration was run before starting annotation. In or-
der to minimise the head movement, we used a
chin rest as shown in Figure 2.
We recruited three annotators who had experi-
ences in annotating predicate-argument relations.
Each annotator was assigned 43 texts for annota-
tion, which were the same across all annotators.
These 43 texts were selected from a Japanese bal-
anced corpus, BCCWJ (Maekawa et al, 2010). To
eliminate unneeded complexities for capturing eye
gaze, texts were truncated to about 1,000 charac-
ters so that they fit into the text area of the annota-
tion tool and did not require any scrolling. It took
about 20?30 minutes for annotating each text. The
annotators were allowed to take a break whenever
she/he finished annotating a text. Before restart-
ing annotation, the five-point calibration was run
every time. The annotators accomplished all as-
signed texts after several sessions for three or more
days in total.
215
SL
A
P
Figure 1: Screenshot of the annotation tool Slate
2.2 Results
The number of annotated links between predicates
and arguments by three annotators A0, A1 and A2
were 3,353 (A0), 3,764 (A1) and 3,462 (A2) re-
spectively. There were several cases where the
annotator added multiple links with the same link
type to a predicate, e.g. in case of conjunctive ar-
guments; we exclude these instances for simplicity
in the analysis below. The number of the remain-
ing links were 3,054 (A0), 3,251 (A1) and 2,996
(A2) respectively. In addition, because our anal-
yses explained in Section 4 require an annotator?s
fixation on both a predicate and its argument, the
number of these instances were reduced to 1,776
(A0), 1,430 (A1) and 1,795 (A2) respectively. The
details of the instances for our analysis are sum-
marised in Table 2. These annotation instances
were used for the analysis in the rest of this paper.
3 Anatomy of human annotation
From a qualitative analysis of the annotator?s be-
haviour in the collected data, we found the an-
case A0 A1 A2 total
ga (nominative) 1,170 904 1,105 3,179
o (accusative) 383 298 421 1,102
ni (dative) 223 228 269 720
total 1,776 1,430 1,795 5,001
Table 2: Results of annotation by each annotator
notation process for predicate-argument relations
could be decomposed into the following three
stages.
1. An annotator reads a given text and under-
stands its contents.
2. Having fixed a target predicate, she/he
searches for its argument in the set of preced-
ing candidate arguments considering a type
of relations with the predicate.
3. Once she/he finds a probable argument in a
text, she/he looks around its context in order
to confirm the relation. The confirmation is
finalised by creating a link between the pred-
icate and its argument.
216
The strategy of searching for arguments after fix-
ing a predicate would reflect the linguistic knowl-
edge that a predicate subcategorises its arguments.
In addition, since Japanese is a head-final lan-
guage, a predicate basically follows its arguments.
Therefore searching for each argument within a
sentence can begin at the same position, i.e. the
predicate, toward the beginning of the sentence,
when the predicate-first search strategy is adopted.
The idea of dividing a cognitive process into
different functional stages is common in cogni-
tive science. For instance, Just and Carpenter
(1985) divided a problem solving process into
three stages: searching, comparison and confirma-
tion. In their task, given a picture of two cubes
with a letter on each surface, a participant is in-
structed to judge whether they can be the same or
not. Since one of the cubes is relatively rotated
in a certain direction and amount, the participant
needs to mentally rotate the cubes for matching.
Russo and Leclerc (1994) divided a visual deci-
sion making process into three stages: orienta-
tion, evaluation and verification. In their exper-
iment, participants were asked to choose one of
several daily food products that were visually pre-
sented. The boundaries of the above three stages
were identified based on the participants? eye gaze
and their verbal protocols. Malcolm and Hender-
son (2009) applied the idea to a visual search pro-
cess, dividing it into initiation, scanning and ver-
ification. Gidlo?f et al (2013) discussed the dif-
ference between a decision making process and a
visual search process in terms of the process divi-
sion. Although the above studies deal with the dif-
ferent cognitive processes, it is common that the
first stage is for capturing an overview of a prob-
lem, the second is for searching for a tentative so-
lution, and the third is for verifying their solution.
Our division of the annotation process conforms
with this idea. Particularly, our task is similar to
the decision making process as defined by Russo
and Leclerc (1994). Unlike these past studies,
however, the beginning of an orientation stage1 is
not clear in our case, since we collected the data
in a natural annotation setting, i.e. a single anno-
tation session for a text includes creation of mul-
tiple links. In other words, the first stage might
correspond to multiple second and third stages. In
addition, in past research on decision making, a
single object is chosen, but our annotation task in-
1We follow the wording by Russo and Leclerc (1994).
???
link creation
first dwell on the linked argument
first dwell on the target predicate
? ?? ?
orientation
? ?? ?
evaluation
? ?? ?
verification
-
time
Figure 3: Division of an annotation process
volves two objects to consider, i.e. a predicate and
an argument.
Considering these differences and the propos-
als of previous studies (Russo and Leclerc, 1994;
Gidlo?f et al, 2013)?we define the three stages as
follows. As explained above, we can not identify
the beginning of an orientation stage based on any
decisive clue. We define the end of an orientation
stage as the onset of the first dwell2 on a predi-
cate being considered. The succeeding evaluation
stage starts at the onset of the first dwell on the
predicate and ends at the onset of the first dwell on
the argument that is eventually linked to the pred-
icate. The third stage, a verification stage, starts
at the onset of the first dwell on the linked argu-
ment and ends at the creation of the link between
the predicate and argument. These definitions and
the relations between the stages are illustrated in
Figure 3.
The time points indicating the stage boundaries
can be identified from the recorded eye gaze and
tool operation data. First, gaze fixations were ex-
tracted by using the Dispersion-Threshold Identi-
fication (I-DT) algorithm (Salvucci and Goldberg,
2000). Based on a rationale that the eye movement
velocity slows near fixations, the I-DT algorithm
identifies fixations as clusters of consecutive gaze
points within a particular dispersion. It has two pa-
rameters, the dispersion threshold that defines the
maximum distance between gaze points belonging
to the same cluster, and the duration threshold that
constrains the minimum fixation duration. Con-
sidering the experimental configurations, i.e. (i)
the display size and its resolution, (ii) the distance
between the display and the annotator?s eyes, and
(iii) the eye-tracker resolution, we set the disper-
sion threshold to 16 pixels. Following Richard-
son et al (2007), we set the duration threshold
to 100 msec. Based on fixations, a dwell on a
segment was defined as a series of fixations that
consecutively stayed on the same segment where
2A dwell is a collection of one or several fixations within
a certain area of interest, a segment in our case.
217
two consecutive fixations were not separated by
more than 100 msec. We allowed a horizontal er-
ror margin of 16 pixels (one-character width) for
both sides of a segment when identifying a dwell.
Time points of link creation were determined by
the ?create link start? event in Table 1.
Among these three stages, the evaluation stage
would be most informative for extracting useful
features for ML algorithms, because an annotator
identifies a probable argument for a predicate un-
der consideration during this stage. Analysing an-
notator eye gaze during this stage could reveal use-
ful information for predicate-argument analysis. It
is, however, insufficient to regard only fixated ar-
guments as being under the annotator?s consider-
ation during the evaluation stage. The annotator
captures an overview of the current problem dur-
ing the previous orientation stage, in which she/he
could remember several candidate arguments in
her/his short-term memory, then moves on to the
evaluation stage. Therefore, all attended argu-
ments are not necessarily observed through gaze
dwells. As we explained earlier, we have no means
to identify a rigid duration of an orientation stage,
thus it is difficult to identify a precise set of can-
didate arguments under the annotator?s considera-
tion in the evaluation stage. For this purpose, we
need a different experimental design so that every
predicate-argument relation is annotated at a time
in the same manner as the above decision making
studies conducted. Another possibility is using an
annotator?s verbal protocols together with her/his
eye gaze as done in Russo and Leclerc (1994).
On the other hand, in the verification stage a
probable argument has been already determined
and its validity confirmed by investigating its com-
petitors. We would expect considered competi-
tors are explicitly fixated during this stage. Since
we have a rigid definition of the verification stage
duration, it is possible to analyse the annotator?s
behaviour during this stage based on her/his eye
gaze. For this reason, we concentrate on the anal-
ysis of the verification stage of annotation hence-
forth.
4 Analysis of the verification stage
Given the set of annotation instances, i.e. pred-
icate, argument and case triplets, we categorise
these instances based on the annotator?s behaviour
during the verification stage. We focus on two fac-
tors for categorising annotation instances: (i) the
1100
10,000
0 10 20 30 40 50 60 70 80 90 100# I
nstan
ces
Distance between predicate and argument
0%50%
100%
0 10 20 30 40 50 60 70 80 90 100Dis
tribu
tion
Distance between predicate and argument
? Distracted      ? Concentrated
Figure 4: Distance of predicate and argument
distance of a predicate and if its argument is ei-
ther near or far, and (ii) whether annotator gaze
dwelled on other arguments than the eventually
linked argument before creating the link. We call
the former factor Near/Far distinction, and the lat-
ter Concentrated/Distracted distinction.
To decide the Near/Far distinction, we inves-
tigated the distribution of distances of predicates
and their argument. The result is shown in the
upper graph of Figure 4, where the x-axis is the
character-based distance and the y-axis shows the
number of instances in each distance bin. Figure 4
demonstrates that the instances concentrate at the
bin of distance 1. This reflects the frequently
occurring instances where a one-character case
maker follows an argument, and immediately pre-
cedes its predicate. The lower graph in Figure 4
shows the ratio of Distracted instances to Con-
centrated at each bin. The distribution indi-
cates that there is no remarkable relation between
the distance and Concentrated/Distracted distinc-
tion. The correlation coefficient between the dis-
tance and the number of Concentrated instances
is ?0.26. We can conclude that the distance of
a predicate and its argument does not impact the
Concentrated/Distracted distinction. Considering
the above tendency, we set the distance threshold
to 22, the average distance of all annotation in-
stances; instances with a distance of less than 22
are considered Near.
These two factors make four combinations
in total, i.e. Near-Concentrated (NC), Near-
Distracted (ND), Far-Concentrated (FC) and Far-
Distracted (FD). We analysed 5,001 instances
shown in Table 2 to find three kinds of tendencies,
which are described in the following sections.
218
case Near Far total
ga (nominative) 2,201 (0.44) 978 (0.90) 3,179 (0.64)
o (accusative) 1,042 (0.34) 60 (0.05) 1,102 (0.22)
ni (dative) 662 (0.22) 58 (0.05) 720 (0.14)
Table 3: Distribution of cases over Near/Far
NC ND FC FD
ga 0.40 0.47 0.92 0.90
o, ni 0.60 0.53 0.08 0.10
Table 4: Distribution of arguments across four cat-
egories
4.1 Predicate-argument distance and
argument case
We hypothesise that an annotator changes her/his
behaviour with regard to the case of the argu-
ment. The argument case in Japanese is marked
by a case marker which roughly corresponds to
the argument?s semantic role, such as Agent and
Theme. We therefore analysed the relationship
between the Near/Far distinction and argument
case. The results are shown in Table 3. The ta-
ble shows the distribution of argument cases, il-
lustrating that Near instances are dispersed over
three cases, while Far instances are concentrated
in the ga (nominative) case. In other words, ga-
arguments tend to appear far from their predi-
cate. This tendency reflects the characteristic of
Japanese where a nominative argument tends to be
placed in the beginning of a sentence; furthermore,
ga-arguments are often omitted to make ellipses.
In our annotation guideline, a predicate with an el-
liptical argument should be linked to the referent
of the ellipsis, which would be realised at a fur-
ther distant position in the preceding context. In
contrast, o (accusative) and ni (dative) arguments
less frequently appeared as Far instances because
they are rarely omitted due to their tighter rela-
tion with arguments. This observation suggests
that each case requires an individual specific treat-
ment in the model of predicate argument analysis;
the model searches for o and ni arguments close to
its predicate, while it considers all preceding can-
didates for a ga argument.
Table 4 shows the break down of the
Near/Far columns with regards to the Con-
centrated/Distracted distinction, demonstrating
that the Concentrated/Distracted distinction does
not impact the distribution of the argument types.
05
1015
20
110
1001000
0
5
10
15
20
# instances
# existing links # dw
ells o
n co
mpe
titor
s
Figure 5: Relationship between the number of
dwells on competitors and already-existing links
4.2 Effect of already-existing links
In the Concentrated instances, an annotator can
verify if an argument is correct without inspect-
ing its competitors. As illustrated in Figure 1, al-
ready annotated arguments are marked by explicit
links to their predicate. These links make the ar-
guments visually as well as cognitively salient in
an annotator?s short-term memory because they
have been frequently annotated in the preceding
annotation process. Thus, we expected that both
types of saliency help to confirm the predicate-
argument relation under consideration. For in-
stance, when searching for an argument of pred-
icate P in Figure 1, argument A that already has
six links (SL) is more salient than other competi-
tors.
To verify this hypothesis, we examined the re-
lation of the number of already-existing links and
the number of dwells on competitors, which is
shown in Figure 5. In this analysis, we used only
Far instances because the Near arguments tended
to have less already-existing links as they were
under current interest. Figure 5 shows a three-
dimensional declining slope that peaks around the
intersection for instances with the fewest number
of links and dwells on competitors. It reveals
a mostly symmetrical relation between existing
links and dwells on competitors for instances with
a lower number of existing links, but that this sym-
metry brakes for instances with a higher number
of existing links, visible by the conspicuous hole
219
toward the left of the figure. This suggests that
visual and cognitive saliency reduces annotators?
cognitive load, and thus contributes to efficiently
confirming the correct argument.
This result implies that the number of already-
existing links of a candidate argument would re-
flect its saliency, thus more linked candidates
should be preferred in the analysis of predicate-
argument relations. Although we analysed the ver-
ification stage, the same effect could be expected
in the evaluation stage as well. Introducing such
information into ML algorithms may contribute to
improving system performance.
4.3 Specificity of arguments and dispersal of
eye gaze
Existing Japanese corpora annotated with
predicate-argument relations (Iida et al, 2007;
Kawahara et al, 2002) have had syntactic heads
(nouns) of their projected NPs related their pred-
icates. Since Japanese is a head-final language,
a head noun is always placed in the last position
of an NP. This scheme has the advantage that
predicate-argument relations can be annotated
without identifying the starting boundary of the
argument NP under consideration. The scheme
is also reflected in the structure of automatically
constructed Japanese case frames, e.g. Sasano et
al. (2009), which consist of triplets in the form
of ?Noun, Case, Verb?. Noun is a head noun
extracted from its projected NP in the original
text. We followed this scheme in our annotation
experiments.
However, a head noun of an argument does not
always have enough information. A nominaliser
which often appears in the head position in an
NP does not have any semantic meaning by it-
self. For instance, in the NP ?benkyo? suru koto
(to study/studying)?, the head noun ?koto? has no
specific semantic meaning, corresponding to an
English morpheme ?to? or ?-ing?. In such cases,
inspecting a whole NP including its modifiers is
necessary to verify the validity of the NP for an
argument in question. We looked at our data to
see if annotators actually behaved like this.
For analysis, the annotation instances were dis-
tinguished if an argument had any modifier or not
(column ?w/o mod? and ?w/ mod? in Table 5).
The ?w/ mod? instances are further divided into
two classes: ?within NP? and ?out of NP?, the for-
mer if all dwells remain ?within? the region of the
w/o mod w/ mod total
within NP out of NP
Concentrated 1,562 1190 ? 2,752
Distracted 1,168 242 839 2,249
Table 5: Relation of argument modifiers and gaze
dispersal
argument NP or the later if they go ?out of? the
region. Note that our annotation scheme creates
a link between a predicate and the head of its ar-
gument as described earlier. Thus, a Distracted
instance does not always mean an ?out of NP? in-
stance, since a distracted dwell might still remains
on a segment within the NP region despite not be-
ing its head. Table 5 shows the distribution of the
instances over this categorisation.
We found that the number of instances is almost
the same between Concentrated and Distracted,
i.e. (2752 : 2249 = 0.55 : 0.45). In this re-
spect, both Concentrated and Distracted instances
can be treated in the same way in the analysis of
predicate-argument relations. A closer look at the
break down of the ?w/ mod? category, however, re-
veals that almost 22% of the Distracted arguments
with any modifier attracted gaze dwells within the
NP region. This fact suggests that we need to treat
candidate arguments differently depending on if
they have modifiers or not. In addition to argument
head information, we could introduce information
of modifiers into ML algorithms as features that
characterise a candidate argument more precisely.
5 Related work
Recent developments in the eye-tracking technol-
ogy enables various research fields to employ eye-
gaze data (Duchowski, 2002).
Bednarik and Tukiainen (2008) analysed eye-
tracking data collected while programmers debug
a program. They defined areas of interest (AOI)
based on the sections of the integrated develop-
ment environment (IDE): the source code area,
the visualised class relation area and the program
output area. They compared the gaze transitions
among these AOIs between expert and novice pro-
grammers to find different transition patterns be-
tween them. Since the granularity of their AOIs
is coarse, it could be used for evaluating a pro-
grammer?s expertise, but hardly explains why the
expert transition pattern realises a good program-
ming skill. In order to find useful information for
language processing, we employed smaller AOIs
220
at the character level.
Rosengrant (2010) proposed an analysis method
named gaze scribing where eye-tracking data is
combined with a subject?s thought process derived
by the think-aloud protocol (TAP) (Ericsson and
Simon, 1984). As a case study, he analysed a pro-
cess of solving electrical circuit problems on the
computer display to find differences of problem
solving strategy between novice and expert sub-
jects. The AOIs are defined both at a macro level,
i.e. the circuit, the work space for calculation,
and a micro level, i.e. electrical components of
the circuit. Rosengrant underlined the importance
of applying gaze scribing to the solving process
of other problems. Although information obtained
from TAP is useful, it increases her/his cognitive
load, and thus might interfere with her/his achiev-
ing the original goal.
Tomanek et al (2010) utilised eye-tracking data
to evaluate the degree of difficulty in annotating
named entities. They are motivated by selecting
appropriate training instances for active learning
techniques. They conducted experiments in vari-
ous settings by controlling characteristics of target
named entities. Compared to their named entity
annotation task, our annotation task, annotating
predicate-argument relations, is more complex. In
addition, our experimental setting is more natural,
meaning that all possible relations in a text were
annotated in a single session, while each session
targeted a single named entity (NE) in a limited
context in the setting of Tomanek et al (2010).
Finally, our fixation target is more precise, i.e.
words, rather than a coarse area around the target
NE.
We have also discussed evaluating annotation
difficulty for predicate-argument relations by us-
ing the same data introduced in this paper (Toku-
naga et al, 2013). Through manual analysis of
the collected data, we suggested that an annotation
time necessary for annotating a single predicate-
argument relation was correlated with the agree-
ment ratio among multiple human annotators.
6 Conclusion
This paper presented an analysis of an annota-
tor?s behaviour during her/his annotation process
for eliciting useful information for NLP tasks.
We first conducted an experiment for collect-
ing three annotators? actions and eye gaze dur-
ing their annotation of predicate-argument rela-
tions in Japanese texts. The collected data were
analysed from three aspects: (i) the relationship
of predicate-argument distances and argument?s
cases, (ii) the effect of already-existing links and
(iii) specificity of arguments and dispersal of eye
gaze. The analysis on these aspects suggested that
obtained insight into human annotation behaviour
could be useful for exploring effective linguistic
features in ML-based approaches.
As future work, we need to further investigate
the data from other aspects. There are advantages
to manual analysis, such as done in this paper.
Mining techniques for finding unknown but useful
information may also be advantageous. Therefore,
we are planning to employ mining techniques for
finding useful gaze patterns for various NLP tasks.
In this paper, we suggested useful information
that could be incorporated into ML algorithms as
features. It is necessary to implement these fea-
tures in a specific ML algorithm and evaluate their
effectiveness empirically.
Our analysis was limited to the verification
stage of annotation, in which a probable argument
of a predicate was confirmed by comparing it with
other competitors. The preceding evaluation stage
should be also analysed, since it is the stage where
annotators search for a correct argument of a pred-
icate in question, thus probably includes useful in-
formation for computational models in identifying
predicate-argument relations. For the analysis of
the evaluation stage, a different design of experi-
ments would be necessary, as already mentioned,
employing single annotation at a time scheme as
Tomanek et al (2010) did, or using an annota-
tor?s verbal protocol together as Russo and Leclerc
(1994), and Rosengrant (2010) did.
Last but not least, data collection and analy-
sis in different annotation tasks are indispensable.
It is our ultimate goal to establish a methodol-
ogy for collecting an analysing annotators? be-
havioural data during annotation in order to elicit
effective features for ML-based NLP.
References
Roman Bednarik and Markku Tukiainen. 2008. Tem-
poral eye-tracking data: Evolution of debugging
strategies with multiple representations. In Proceed-
ings of the 2008 symposium on Eye tracking re-
search & applications (ETRA ?08), pages 99?102.
Andrew T. Duchowski. 2002. A breadth-first survey of
eye-tracking applications. Behavior Research Meth-
221
ods, Instruments, and Computers, 34(4):455?470.
David duVerle and Helmut Prendinger. 2009. A novel
discourse parser based on support vector machine
classification. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 665?673.
K. Anders Ericsson and Herbert A. Simon. 1984. Pro-
tocol Analysis ? Verbal Reports as Data ?. The MIT
Press.
Kerstin Gidlo?f, Annika Wallin, Richard Dewhurst, and
Kenneth Holmqvist. 2013. Using eye tracking to
trace a cognitive process: Gaze behaviour during de-
cision making in a natural environment. Journal of
Eye Movement Research, 6(1):1?14.
Zenzi M. Griffin and Kathryn Bock. 2000. What the
eyes say about speaking. Psychological Science,
11(4):274?279.
Barbara J. Grosz, Aravind K. Joshi, and Scott Wein-
stein. 1995. Centering: A framework for model-
ing the local coherence of discourse. Computational
Linguistics, 21(2):203?225.
Ryu Iida, Mamoru Komachi, Kentaro Inui, and Yuji
Matsumoto. 2007. Annotating a Japanese text cor-
pus with predicate-argument and coreference rela-
tions. In Proceeding of the ACL Workshop ?Linguis-
tic Annotation Workshop?, pages 132?139.
Marcel Adam Just and Patricia A. Carpenter. 1985.
Cognitive coordinate systems: Accounts of mental
rotation and individual differences in spatial ability.
Psychological Review, 92(2):137?172.
Dain Kaplan, Ryu Iida, Kikuko Nishina, and Takenobu
Tokunaga. 2012. Slate ? a tool for creating and
maintaining annotated corpora. Journal for Lan-
guage Technology and Computational Linguistics,
26(2):89?101.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti
Hasida. 2002. Construction of a Japanese
relevance-tagged corpus (in Japanese). In Proceed-
ings of the 8th Annual Meeting of the Association for
Natural Language Processing, pages 495?498.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010.
A PDTB-styled end-to-end discourse parser. Tech-
nical Report TRB8/10, School of Computing, Na-
tional University of Singapore.
Kikuo Maekawa, Makoto Yamazaki, Takehiko
Maruyama, Masaya Yamaguchi, Hideki Ogura,
Wakako Kashino, Toshinobu Ogiso, Hanae Koiso,
and Yasuharu Den. 2010. Design, compilation,
and preliminary analyses of balanced corpus of
contemporary written Japanese. In Proceedings of
the Eigth International Conference on Language
Resources and Evaluation (LREC 2010), pages
1483?1486.
George L. Malcolm and John M. Henderson. 2009.
The effects of target template specificity on visual
search in real-world scenes: Evidence from eye
movements. Journal of Vision, 9(11):8:1?13.
George A. Miller. 1995. WordNet: A lexical database
for English. Communications of the ACM, 38:39?
41.
Vincent Ng. 2010. Supervised noun phrase corefer-
ence research: The first fifteen years. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics (ACL 2010), pages
1396?1411.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 shared task: Modeling multilingual unre-
stricted coreference in OntoNotes. In Joint Confer-
ence on EMNLP and CoNLL ? Shared Task, pages
1?40.
Daniel C. Richardson, Rick Dale, and Michael J.
Spivey. 2007. Eye movements in language and cog-
nition: A brief introduction. In Monica Gonzalez-
Marquez, Irene Mittelberg, Seana Coulson, and
Michael J. Spivey, editors, Methods in Cognitive
Linguistics, pages 323?344. John Benjamins.
David Rosengrant. 2010. Gaze scribing in physics
problem solving. In Proceedings of the 2010 sym-
posium on Eye tracking research & applications
(ETRA ?10), pages 45?48.
J. Edward Russo and France Leclerc. 1994. An
eye-fixation analysis of choice processes for con-
sumer nondurables. Journal of Consumer Research,
21(2):274?290.
Dario D. Salvucci and Joseph H. Goldberg. 2000.
Identifying fixations and saccades in eye-tracking
protocols. In Proceedings of the 2000 symposium on
Eye tracking research & applications (ETRA ?00),
pages 71?78.
Ryohei Sasano, Daisuke Kawahara, and Sadao Kuro-
hashi. 2009. The effect of corpus size on case frame
acquisition for discourse analysis. In Proceedings of
Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics (NAACL-
HLT 2009), pages 521?529.
Takenobu Tokunaga, Ryu Iida, and Koh Mitsuda.
2013. Annotation for annotation - toward elicit-
ing implicit linguistic knowledge through annota-
tion -. In Proceedings of the 9th Joint ISO - ACL
SIGSEM Workshop on Interoperable Semantic An-
notation (ISA-9), pages 79?83.
Katrin Tomanek, Udo Hahn, Steffen Lohmann, and
Ju?rgen Ziegler. 2010. A cognitive cost model of
annotations based on eye-tracking data. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics (ACL 2010), pages
1158?1167.
222
