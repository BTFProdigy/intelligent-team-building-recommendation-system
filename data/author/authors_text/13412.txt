	


	


Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 139?144,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Graphical Interface for MT Evaluation and Error Analysis
Meritxell Gonza`lez and Jesu?s Gime?nez and Llu??s Ma`rquez
TALP Research Center
Universitat Polite`cnica de Catalunya
{mgonzalez,jgimenez,lluism}@lsi.upc.edu
Abstract
Error analysis in machine translation is a nec-
essary step in order to investigate the strengths
and weaknesses of the MT systems under de-
velopment and allow fair comparisons among
them. This work presents an application that
shows how a set of heterogeneous automatic
metrics can be used to evaluate a test bed of
automatic translations. To do so, we have
set up an online graphical interface for the
ASIYA toolkit, a rich repository of evaluation
measures working at different linguistic lev-
els. The current implementation of the inter-
face shows constituency and dependency trees
as well as shallow syntactic and semantic an-
notations, and word alignments. The intelli-
gent visualization of the linguistic structures
used by the metrics, as well as a set of navi-
gational functionalities, may lead towards ad-
vanced methods for automatic error analysis.
1 Introduction
Evaluation methods are a key ingredient in the de-
velopment cycle of machine translation (MT) sys-
tems. As illustrated in Figure 1, they are used to
identify and analyze the system weak points (error
analysis), to introduce new improvements and adjust
the internal system parameters (system refinement),
and to measure the system performance in compari-
son to other systems or previous versions of the same
system (evaluation).
We focus here on the processes involved in the
error analysis stage in which MT developers need to
understand the output of their systems and to assess
the improvements introduced.
Automatic detection and classification of the er-
rors produced by MT systems is a challenging prob-
lem. The cause of such errors may depend not only
on the translation paradigm adopted, but also on the
language pairs, the availability of enough linguistic
resources and the performance of the linguistic pro-
cessors, among others. Several past research works
studied and defined fine-grained typologies of trans-
lation errors according to various criteria (Vilar et
al., 2006; Popovic? et al, 2006; Kirchhoff et al,
2007), which helped manual annotation and human
analysis of the systems during the MT development
cycle. Recently, the task has received increasing at-
tention towards the automatic detection, classifica-
tion and analysis of these errors, and new tools have
been made available to the community. Examples
of such tools are AMEANA (Kholy and Habash,
2011), which focuses on morphologically rich lan-
guages, and Hjerson (Popovic?, 2011), which ad-
dresses automatic error classification at lexical level.
In this work we present an online graphical inter-
face to access ASIYA, an existing software designed
to evaluate automatic translations using an heteroge-
neous set of metrics and meta-metrics. The primary
goal of the online interface is to allow MT develop-
ers to upload their test beds, obtain a large set of met-
ric scores and then, detect and analyze the errors of
their systems using just their Internet browsers. Ad-
ditionally, the graphical interface of the toolkit may
help developers to better understand the strengths
and weaknesses of the existing evaluation measures
and to support the development of further improve-
ments or even totally new evaluation metrics. This
information can be gathered both from the experi-
139
Figure 1: MT systems development cycle
ence of ASIYA?s developers and also from the statis-
tics given through the interface to the ASIYA?s users.
In the following, Section 2 gives a general
overview of the ASIYA toolkit. Section 3 describes
the variety of information gathered during the eval-
uation process, and Section 4 provides details on the
graphical interface developed to display this infor-
mation. Finally, Section 5 overviews recent work re-
lated to MT error analysis, and Section 6 concludes
and reports some ongoing and future work.
2 The ASIYA Toolkit
ASIYA is an open toolkit designed to assist devel-
opers of both MT systems and evaluation measures
by offering a rich set of metrics and meta-metrics
for assessing MT quality (Gime?nez and Ma`rquez,
2010a). Although automatic MT evaluation is still
far from manual evaluation, it is indeed necessary
to avoid the bottleneck introduced by a fully man-
ual evaluation in the system development cycle. Re-
cently, there has been empirical and theoretical justi-
fication that a combination of several metrics scoring
different aspects of translation quality should corre-
late better with humans than just a single automatic
metric (Amigo? et al, 2011; Gime?nez and Ma`rquez,
2010b).
ASIYA offers more than 500 metric variants for
MT evaluation, including the latest versions of the
most popular measures. These metrics rely on dif-
ferent similarity principles (such as precision, recall
and overlap) and operate at different linguistic layers
(from lexical to syntactic and semantic). A general
classification based on the similarity type is given
below along with a brief summary of the informa-
tion they use and the names of a few examples1.
Lexical similarity: n-gram similarity and edit dis-
tance based on word forms (e.g., PER, TER,
WER, BLEU, NIST, GTM, METEOR).
Syntactic similarity: based on part-of-speech tags,
base phrase chunks, and dependency and con-
stituency trees (e.g., SP-Overlap-POS, SP-
Overlap-Chunk, DP-HWCM, CP-STM).
Semantic similarity: based on named entities, se-
mantic roles and discourse representation (e.g.,
NE-Overlap, SR-Overlap, DRS-Overlap).
Such heterogeneous set of metrics allow the user
to analyze diverse aspects of translation quality at
system, document and sentence levels. As discussed
in (Gime?nez and Ma`rquez, 2008), the widely used
lexical-based measures should be considered care-
fully at sentence level, as they tend to penalize trans-
lations using different lexical selection. The combi-
nation with complex metrics, more focused on ad-
equacy aspects of the translation (e.g., taking into
account also semantic information), should help re-
ducing this problem.
3 The Metric-dependent Information
ASIYA operates over a fixed set of translation test
cases, i.e., a source text, a set of candidate trans-
lations and a set of manually produced reference
translations. To run ASIYA the user must provide
a test case and select the preferred set of metrics
(it may depend on the evaluation purpose). Then,
ASIYA outputs complete tables of score values for
all the possible combination of metrics, systems,
documents and segments. This kind of results is
valuable for rapid evaluation and ranking of trans-
lations and systems. However, it is unfriendly for
MT developers that need to manually analyze and
compare specific aspects of their systems.
During the evaluation process, ASIYA generates
a number of intermediate analysis containing par-
tial work outs of the evaluation measures. These
data constitute a priceless source for analysis pur-
poses since a close examination of their content al-
lows for analyzing the particular characteristics that
1A more detailed description of the metric set and its imple-
mentation can be found in (Gime?nez and Ma`rquez, 2010b).
140
Reference The remote control of the Wii
helps to diagnose an infantile
ocular disease .
Ol score
Candidate 1 The Wii Remote to help diag-
nose childhood eye disease .
7
17 = 0.41
Candidate 2 The control of the Wii helps
to diagnose an ocular infantile
disease .
13
14 = 0.93
Table 1: The reference sentence, two candidate
translation examples and the Ol scores calculation
differentiate the score values obtained by each can-
didate translation.
Next, we review the type of information used by
each family of measures according to their classifi-
cation, and how this information can be used for MT
error analysis purposes.
Lexical information. There are several variants un-
der this family. For instance, lexical overlap (Ol)
is an F-measure based metric, which computes sim-
ilarity roughly using the Jaccard coefficient. First,
the sets of all lexical items that are found in the ref-
erence and the candidate sentences are considered.
Then, Ol is computed as the cardinality of their in-
tersection divided by the cardinality of their union.
The example in Table 1 shows the counts used to cal-
culate Ol between the reference and two candidate
translations (boldface and underline indicate non-
matched items in candidate 1 and 2, respectively).
Similarly, metrics in another category measure the
edit distance of a translation, i.e., the number of
word insertions, deletions and substitutions that are
needed to convert a candidate translation into a ref-
erence. From the algorithms used to calculate these
metrics, these words can be identified in the set of
sentences and marked for further processing. On
another front, metrics as BLEU or NIST compute
a weighted average of matching n-grams. An inter-
esting information that can be obtained from these
metrics are the weights assigned to each individual
matching n-gram. Variations of all of these mea-
sures include looking at stems, synonyms and para-
phrases, instead of the actual words in the sentences.
This information can be obtained from the imple-
mentation of the metrics and presented to the user
through the graphical interface.
Syntactic information. ASIYA considers three lev-
els of syntactic information: shallow, constituent
and dependency parsing. The shallow parsing an-
notations, that are obtained from the linguistic pro-
cessors, consist of word level part-of-speech, lem-
mas and chunk Begin-Inside-Outside labels. Use-
ful figures such as the matching rate of a given
(sub)category of items are the base of a group of
metrics (i.e., the ratio of prepositions between a
reference and a candidate). In addition, depen-
dency and constituency parse trees allow for captur-
ing other aspects of the translations. For instance,
DP-HCWM is a specific subset of the dependency
measures that consists of retrieving and matching all
the head-word chains (or the ones of a given length)
from the dependency trees. Similarly, CP-STM, a
subset of the constituency parsing family of mea-
sures, consists of computing the lexical overlap ac-
cording to the phrase constituent of a given type.
Then, for error analysis purposes, parse trees com-
bine the grammatical relations and the grammati-
cal categories of the words in the sentence and dis-
play the information they contain. Figure 2 and 3
show, respectively, several annotation levels of the
sentences in the example and the constituency trees.
Semantic information. ASIYA distinguishes also
three levels of semantic information: named enti-
ties, semantic roles and discourse representations.
The former are post-processed similarly to the lex-
ical annotations discussed above; and the semantic
predicate-argument trees are post-processed and dis-
played in a similar manner to the syntactic trees.
Instead, the purpose of the discourse representation
analysis is to evaluate candidate translations at doc-
ument level. In the nested discourse structures we
could identify the lexical choices for each discourse
sub-type. Presenting this information to the user re-
mains as an important part of the future work.
4 The Graphical Interface
This section presents the web application that makes
possible a graphical visualization and interactive ac-
cess to ASIYA. The purpose of the interface is
twofold. First, it has been designed to facilitate the
use of the ASIYA toolkit for rapid evaluation of test
beds. And second, we aim at aiding the analysis of
the errors produced by the MT systems by creating
141
Figure 2: PoS, chunk and named entity annota-
tions on the source, reference and two translation
hypotheses
Figure 3: Constituency trees for the reference and
second translation candidate
a significant visualization of the information related
to the evaluation metrics.
The online interface consists of a simple web form
to supply the data required to run ASIYA, and then,
it offers several views that display the results in
friendly and flexible ways such as interactive score
tables, graphical parsing trees in SVG format and
interactive sentences holding the linguistic annota-
tions captured during the computation of the met-
rics, as described in Section 3.
4.1 Online MT evaluation
ASIYA allows to compute scores at three granular-
ity levels: system (entire test corpus), document and
sentence (or segment). The online application ob-
tains the measures for all the metrics and levels and
generates an interactive table of scores displaying
the values for all the measures. Table organiza-
Figure 4: The bar charts plot to compare the metric
scores for several systems
tion can swap among the three levels of granularity,
and it can also be transposed with respect to sys-
tem and metric information (transposing rows and
columns). When the metric basis table is shown, the
user can select one or more metric columns in or-
der to re-rank the rows accordingly. Moreover, the
source, reference and candidate translation are dis-
played along with metric scores. The combination of
all these functionalities makes it easy to know which
are the highest/lowest-scored sentences in a test set.
We have also integrated a graphical library2 to
generate real-time interactive plots to show the met-
ric scores graphically. The current version of the in-
terface shows interactive bar charts, where different
metrics and systems can be combined in the same
plot. An example is shown in Figure 4.
4.2 Graphically-aided Error Analysis and
Diagnosis
Human analysis is crucial in the development cy-
cle because humans have the capability to spot er-
rors and analyze them subjectively, in relation to the
underlying system that is being examined and the
scores obtained. Our purpose, as mentioned previ-
ously, is to generate a graphical representation of
the information related to the source and the trans-
lations, enabling a visual analysis of the errors. We
have focused on the linguistic measures at the syn-
tactic and semantic level, since they are more robust
than lexical metrics when comparing systems based
on different paradigms. On the one hand, one of
the views of the interface allows a user to navigate
and inspect the segments of the test set. This view
highlights the elements in the sentences that match a
2http://www.highcharts.com/
142
given criteria based on the various linguistic annota-
tions aforementioned (e.g., PoS prepositions). The
interface integrates also the mechanisms to upload
word-by-word alignments between the source and
any of the candidates. The alignments are also vi-
sualized along with the rest of the annotations, and
they can be also used to calculate artificial annota-
tions projected from the source in such test beds for
which there is no linguistic processors available. On
the other hand, the web application includes a library
for SVG graph generation in order to create the de-
pendency and the constituent trees dynamically (as
shown in Figure 3).
4.3 Accessing the Demo
The online interface is fully functional and accessi-
ble at http://nlp.lsi.upc.edu/asiya/. Al-
though the ASIYA toolkit is not difficult to install,
some specific technical skills are still needed in or-
der to set up all its capabilities (i.e., external com-
ponents and resources such as linguistic processors
and dictionaries). Instead, the online application re-
quires only an up to date browser. The website in-
cludes a tarball with sample input data and a video
recording, which demonstrates the main functional-
ities of the interface and how to use it.
The current web-based interface allows the user
to upload up to five candidate translation files, five
reference files and one source file (maximum size of
200K each, which is enough for test bed of about
1K sentences). Alternatively, the command based
version of ASIYA can be used to intensively evaluate
a large set of data.
5 Related Work
In the literature, we can find detailed typologies of
the errors produced by MT systems (Vilar et al,
2006; Farru?s et al, 2011; Kirchhoff et al, 2007) and
graphical interfaces for human classification and an-
notation of these errors, such as BLAST (Stymne,
2011). They represent a framework to study the
performance of MT systems and develop further re-
finements. However, they are defined for a specific
pair of languages or domain and they are difficult
to generalize. For instance, the study described in
(Kirchhoff et al, 2007) focus on measures relying on
the characterization of the input documents (source,
genre, style, dialect). In contrast, Farru?s et al (2011)
classify the errors that arise during Spanish-Catalan
translation at several levels: orthographic, morpho-
logical, lexical, semantic and syntactic errors.
Works towards the automatic identification and
classification of errors have been conducted very re-
cently. Examples of these are (Fishel et al, 2011),
which focus on the detection and classification of
common lexical errors and misplaced words using
a specialized alignment algorithm; and (Popovic?
and Ney, 2011), which addresses the classifica-
tion of inflectional errors, word reordering, missing
words, extra words and incorrect lexical choices us-
ing a combination of WER, PER, RPER and HPER
scores. The AMEANA tool (Kholy and Habash,
2011) uses alignments to produce detailed morpho-
logical error diagnosis and generates statistics at dif-
ferent linguistic levels. To the best of our knowl-
edge, the existing approaches to automatic error
classification are centered on the lexical, morpho-
logical and shallow syntactic aspects of the transla-
tion, i.e., word deletion, insertion and substitution,
wrong inflections, wrong lexical choice and part-
of-speech. In contrast, we introduce additional lin-
guistic information, such as dependency and con-
stituent parsing trees, discourse structures and se-
mantic roles. Also, there exist very few tools de-
voted to visualize the errors produced by the MT
systems. Here, instead of dealing with the automatic
classification of errors, we deal with the automatic
selection and visualization of the information used
by the evaluation measures.
6 Conclusions and Future Work
The main goal of the ASIYA toolkit is to cover the
evaluation needs of researchers during the develop-
ment cycle of their systems. ASIYA generates a
number of linguistic analyses over both the candi-
date and the reference translations. However, the
current command-line interface returns the results
only in text mode and does not allow for fully ex-
ploiting this linguistic information. We present a
graphical interface showing a visual representation
of such data for monitoring the MT development cy-
cle. We believe that it would be very helpful for car-
rying out tasks such as error analysis, system com-
parison and graphical representations.
143
The application described here is the first release
of a web interface to access ASIYA online. So
far, it includes the mechanisms to analyze 4 out of
10 categories of metrics: shallow parsing, depen-
dency parsing, constituent parsing and named en-
tities. Nonetheless, we aim at developing the sys-
tem until we cover all the metric categories currently
available in ASIYA.
Regarding the analysis of the sentences, we have
conducted a small experiment to show the ability of
the interface to use word level alignments between
the source and the target sentences. In the near fu-
ture, we will include the mechanisms to upload also
phrase level alignments. This functionality will also
give the chance to develop a new family of evalua-
tion metrics based on these alignments.
Regarding the interactive aspects of the interface,
the grammatical graphs are dynamically generated
in SVG format, which proffers a wide range of inter-
active functionalities. However their interactivity is
still limited. Further development towards improved
interaction would provide a more advanced manip-
ulation of the content, e.g., selection, expansion and
collapse of branches.
Concerning the usability of the interface, we will
add an alternative form for text input, which will re-
quire users to input the source, reference and candi-
date translation directly without formatting them in
files, saving a lot of effort when users need to ana-
lyze the translation results of one single sentence.
Finally, in order to improve error analysis capa-
bilities, we will endow the application with a search
engine able to filter the results according to varied
user defined criteria. The main goal is to provide
the mechanisms to select a case set where, for in-
stance, all the sentences are scored above (or below)
a threshold for a given metric (or a subset of them).
Acknowledgments
This research has been partially funded by the Span-
ish Ministry of Education and Science (OpenMT-
2, TIN2009-14675-C03) and the European Commu-
nity?s Seventh Framework Programme under grant
agreement numbers 247762 (FAUST project, FP7-
ICT-2009- 4-247762) and 247914 (MOLTO project,
FP7-ICT-2009-4- 247914).
References
Enrique Amigo?, Julio Gonzalo, Jesu?s Gime?nez, and Fe-
lisa Verdejo. 2011. Corroborating text evaluation re-
sults with heterogeneous measures. In Proc. of the
EMNLP, Edinburgh, UK, pages 455?466.
Mireia Farru?s, Marta R. Costa-Jussa`, Jose? B. Marin?o,
Marc Poch, Adolfo Herna?ndez, Carlos Henr??quez, and
Jose? A. Fonollosa. 2011. Overcoming Statistical Ma-
chine Translation Limitations: Error Analysis and Pro-
posed Solutions for the Catalan?Spanish Language
Pair. LREC, 45(2):181?208.
Mark Fishel, Ondr?ej Bojar, Daniel Zeman, and Jan Berka.
2011. Automatic Translation Error Analysis. In Proc.
of the 14th TSD, volume LNAI 3658. Springer Verlag.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2008. Towards Het-
erogeneous Automatic MT Error Analysis. In Proc. of
LREC, Marrakech, Morocco.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2010a. Asiya:
An Open Toolkit for Automatic Machine Translation
(Meta-)Evaluation. The Prague Bulletin of Mathemat-
ical Linguistics, (94):77?86.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2010b. Linguistic
Measures for Automatic Machine Translation Evalua-
tion. Machine Translation, 24(3?4):77?86.
Ahmed El Kholy and Nizar Habash. 2011. Automatic
Error Analysis for Morphologically Rich Languages.
In Proc. of the MT Summit XIII, Xiamen, China, pages
225?232.
Katrin Kirchhoff, Owen Rambow, Nizar Habash, and
Mona Diab. 2007. Semi-Automatic Error Analysis for
Large-Scale Statistical Machine Translation Systems.
In Proc. of the MT Summit XI, Copenhagen, Denmark.
Maja Popovic? and Hermann Ney. 2011. Towards Auto-
matic Error Analysis of Machine Translation Output.
Computational Linguistics, 37(4):657?688.
Maja Popovic?, Hermann Ney, Adria` de Gispert, Jose? B.
Marin?o, Deepa Gupta, Marcello Federico, Patrik Lam-
bert, and Rafael Banchs. 2006. Morpho-Syntactic
Information for Automatic Error Analysis of Statisti-
cal Machine Translation Output. In Proc. of the SMT
Workshop, pages 1?6, New York City, USA. ACL.
Maja Popovic?. 2011. Hjerson: An Open Source Tool
for Automatic Error Classification of Machine Trans-
lation Output. The Prague Bulletin of Mathematical
Linguistics, 96:59?68.
Sara Stymne. 2011. Blast: a Tool for Error Analysis of
Machine Translation Output. In Proc. of the 49th ACL,
HLT, Systems Demonstrations, pages 56?61.
David Vilar, Jia Xu, Luis Fernando D?Haro, and Her-
mann Ney. 2006. Error Analysis of Machine Trans-
lation Output. In Proc. of the LREC, pages 697?702,
Genoa, Italy.
144
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 181?186,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
tSEARCH: Flexible and Fast Search over Automatic Translations for
Improved Quality/Error Analysis
Meritxell Gonza`lez and Laura Mascarell and Llu??s Ma`rquez
TALP Research Center
Universitat Polite`cnica de Catalunya
{mgonzalez,lmascarell,lluism}@lsi.upc.edu
Abstract
This work presents tSEARCH, a web-based
application that provides mechanisms for
doing complex searches over a collection
of translation cases evaluated with a large
set of diverse measures. tSEARCH uses the
evaluation results obtained with the ASIYA
toolkit for MT evaluation and it is connected
to its on-line GUI, which makes possible
a graphical visualization and interactive ac-
cess to the evaluation results. The search
engine offers a flexible query language al-
lowing to find translation examples match-
ing a combination of numerical and struc-
tural features associated to the calculation of
the quality metrics. Its database design per-
mits a fast response time for all queries sup-
ported on realistic-size test beds. In sum-
mary, tSEARCH, used with ASIYA, offers
developers of MT systems and evaluation
metrics a powerful tool for helping transla-
tion and error analysis.
1 Introduction
In Machine Translation (MT) system develop-
ment, a qualitative analysis of the translations is a
fundamental step in order to spot the limitations of
a system, compare the linguistic abilities of differ-
ent systems or tune the parameters during system
refinement. This is especially true in statistical
MT systems, where usually no special structured
knowledge is used other than parallel data and lan-
guage models, but also on systems that need to
reason over linguistic structures. The need for an-
alyzing and comparing automatic translations with
respect to evaluation metrics is also paramount
for developers of translation quality metrics, who
need elements of analysis to better understand the
behavior of their evaluation measures.
This paper presents tSEARCH, a web applica-
tion that aims to alleviate the burden of manual
analysis that developers have to conduct to as-
sess the translation quality aspects involved in the
above mentioned situations. As a toy example,
consider for instance an evaluation setting with
two systems, s1 and s2, and two evaluation met-
rics m1 and m2. Assume also that m1 scores s1 to
be better than s2 in a particular test set, while m2
predicts just the contrary. In order to analyze this
contradictory evaluation one might be interested in
inspecting from the test set the particular transla-
tion examples that contribute to these results, i.e.,
text segments t for which the translation provided
by s1 is scored better by m1 than the translation
provided by s2 and the opposite behavior regard-
ing metric m2. tSEARCH allows to retrieve (vi-
sualize and export) these sentences with a simple
query in a fast time response. The search can be
further constrained, by requiring certain margins
on the differences, by including other systems or
metrics, or by requiring some specific syntactic or
semantic constructs to appear in the examples.
tSEARCH is build on top of ASIYA (Gime?nez
and Ma`rquez, 2010), an open-source toolkit for
MT evaluation; and it can be used along with
the ASIYA ON-LINE INTERFACE (Gonza`lez et al,
2012), which provides an interactive environment
to examine the sentences. ASIYA allows to ana-
lyze a wide range of linguistic aspects of candi-
date and reference translations using a large set
of automatic and heterogeneous evaluation met-
rics. In particular, it offers a especially rich set
of measures that use syntactic and semantic infor-
mation. The intermediate structures generated by
the parsers, and used to compute the scoring mea-
sures, could be priceless for MT developers, who
can use them to compare the structures of several
translations and see how they affect the perfor-
mance of the metrics, providing more understand-
ing in order to interpret the actual performance of
the automatic translation systems.
tSEARCH consists of: 1) a database that stores
181
the resources generated by ASIYA, 2) a query lan-
guage and a search engine able to look through
the information gathered in the database, and 3) a
graphical user interface that assists the user to
write a query, returns the set of sentences that ful-
fill the conditions, and allows to export these re-
sults in XML format. The application is publicly
accessible on-line1, and a brief explanation of its
most important features is given in the demonstra-
tive video.
In the following, Section 2 gives an overview
of the ASIYA toolkit and the information gathered
from the evaluation output. Section 3 and Sec-
tion 4 describe in depth the tSEARCH application
and the on-line interface, respectively. Finally,
Section 5 reviews similar applications in compari-
son to the functionalities addressed by tSEARCH.
2 MT Evaluation with the ASIYA Toolkit
Currently, ASIYA contains more than 800 variants
of MT metrics to measure the similarity between
two translations at several linguistic dimensions.
Moreover, the scores can be calculated at three
granularity levels: system (entire test-set), docu-
ment and sentence (or segment).
As shown in Figure 1, ASIYA requires the user
to provide a test suite. Then, the input files are
processed in order to calculate the annotations, the
parsing trees and the final metric scores. Sev-
eral external components are used for both, met-
ric computation and automatic linguistic analysis2.
The use of these tools depends on the languages
supported and the type of measures that one needs
to obtain. Hence, for instance, lexical-based
measures are computed using the last version
of most popular metrics, such as BLEU, NIST,
METEOR or ROUGE. The syntax-wise measures
need the output of taggers, lemmatizers, parsers
1http://asiya.lsi.upc.edu/demo
2A complete list of external components can be found in
the Technical Manual at the ASIYA web-site
Figure 1: ASIYA processes and data files
and other analyzers. In those cases, ASIYA uses
the SVMTool (Gime?nez and Ma`rquez, 2004),
BIOS (Surdeanu et al, 2005), the Charniak-
Johnson and Berkeley constituent parsers (Char-
niak and Johnson, 2005; Petrov and Klein, 2007),
and the MALT dependency parser (Nivre et al,
2007), among others.
In the tSEARCH platform, the system manages
the communication with an instance of the ASIYA
toolkit running on the server. For every test suite,
the system maintains a synchronized representa-
tion of the input data, the evaluation results and the
linguistic information generated. Then, the system
updates a database where the test suites are stored
for further analysis using the tSEARCH tool, as de-
scribed next.
3 The tSEARCH Tool
tSEARCH offers a graphical search engine to ana-
lyze a given test suite. The system core retrieves
all translation examples that satisfy certain prop-
erties related to either the evaluation scores or the
linguistic structures. The query language designed
is simple and flexible, and it allows to combine
many properties to build sophisticated searches.
The tSEARCH architecture consists of the three
components illustrated in Figure 2: the web-based
interface, the storage system based on NoSQL
technology and the tSEARCH core, composed of
a query parser and a search engine.
The databases (Section 3.1) are fed through the
tSearch Data Loader API used by ASIYA. At
run-time, during the calculation of the measures,
ASIYA inserts all the information being calcu-
lated (metrics and parses) and a number of pre-
calculated variables (e.g., average, mean and per-
centiles). These operations are made in parallel,
which makes the overhead of filling the database
marginal.
The query parser (Section 3.2) receives the
query from the on-line interface and converts it
Figure 2: tSEARCH architecture
182
(a) Scores Column Family (b) Statistics Column Family
(c) Linguistic Elements Column Family
Figure 3: tSEARCH data model
into a binary tree structure where each leaf is a sin-
gle part of an operation and each node combines
the partial results of the children. The search en-
gine obtains the final results by processing the tree
bottom-up until the root is reached.
3.1 Data Representation, Storage and Access
The amount of data generated by ASIYA can be
very large for test sets with thousands of sen-
tences. In order to handle the high volume of
information, we decided to use the Apache Cas-
sandra database3, a NoSQL (also known as not
only SQL) solution that deals successfully with
this problem.
It is important to remark that there is no similar-
ity between NoSQL and the traditional relational
database management system model (RDBMS).
Actually, RDBMS uses SQL as its query language
and requires a relational model, whereas NoSQL
databases do not. Besides, the tSEARCH query
language can be complex, with several conditions,
which makes RDBMS perform poorly due the
complexity of the tables. In contrast, NoSQL-
databases use big-tables having many querying
information precalculated as key values, which
yields for direct access to the results.
The Cassandra data model is based on column
families (CF). A CF consists of a set of rows that
are uniquely identified by its key and have a set
of columns as values. So far, the tSEARCH data
model has the three CFs shown in Figure 3. The
scores CF in Figure 3(a) stores information related
to metrics and score values. Each row slot contains
the list of segments that matches the column key.
3http://cassandra.apache.org/
The statistics CF in Figure 3(b) stores basic statis-
tics, such as the minimum, maximum, average,
median and percentiles values for every evaluation
metric. The CF having the linguistic elements in
Figure 3(c) stores the results of the parsers, such
as part-of-speech, grammatical categories and de-
pendency relationships.
One of the goals of NoSQL databases is to ob-
tain the information required in the minimum ac-
cess time. Therefore, the data is stored in the
way required by the tSEARCH application. For
instance, the query BLEU > 0.4 looks for all
segments in the test suite having a BLEU score
greater than 0.4. Thus, in order to get the query
result in constant time, we use the metric identi-
fier as a part of the key for the scores CF, and the
score 0.4 as the column key.
3.2 The Query Language and Parser
The Query Parser module is one of the key ingre-
dients in the tSEARCH application because it de-
termines the query grammar and the allowed op-
erations, and it provides a parsing method to an-
alyze any query and produce a machine-readable
version of its semantics. It is also necessary in or-
der to validate the query.
There are several types of queries, depending on
the operations used: arithmetic comparisons, sta-
tistical functions (e.g., average, quartiles), range
of values, linguistic elements and logical opera-
tors. Furthermore, the queries can be applied at
segment-, document- and/or system-level, and it
is even possible to create any group of systems
or metrics. This is useful, for instance, in or-
der to limit the search to certain type of systems
(e.g., rule-based vs. statistical) and specific met-
183
Figure 4: (top) Query operations and functions, (bottom) Queries for group of systems and metrics
rics (e.g., lexical vs. syntactic). All possible query
types are described in the following subsections
(3.2.1 to 3.2.3) and listed in Figure 4.
3.2.1 Segment-level and Metric-based
Queries
The most basic queries are those related to
segment level scores, i.e., obtain all segments
scored above/below a value for a concrete met-
ric. The common comparison operators are sup-
ported, such as for instance, BLEU > 0.4 and
BLEU gt 0.4, that are both correct and equiva-
lent queries.
Basic statistics are also calculated at run-time,
which allows to use statistic variables as values,
e.g., obtain the segments scored in the fourth quar-
tile of BLEU. The maximum, minimum, average,
median and percentile values of each metric are
precalculated and saved into the MAX, MIN, AVG,
MEDIAN and PERC variables, respectively. The
thresholds and quartiles (TH,Q) are calculated at
run-time based on percentiles. MIN and MAX can
also be used and allow to get al segments in the
test set (i.e.,BLEU ge MIN).
The threshold function implies a percentage.
The query BLEU > TH(20) gets all segments
that have a BLEU score greater than the score
value of the bottom 20% of the sentences.
It is also possible to specify an interval of values
using the operator IN[x,y]. The use of paren-
thesis is allowed in order to exclude the bound-
aries. The arguments for this operator can be
either numerical values or the predefined func-
tions for quartiles and percentiles. Therefore,
the following example BLEU IN [TH(20),
TH(30)] returns all segments with a BLEU score
in the range between the threshold of the 20% (in-
cluded) and the 30% (excluded).
The quartile function Q(X) takes a value be-
tween 1 and 4 and returns all segments that
have their score in that quartile. In contrast,
the percentile function generalizes the previous:
PERC(n,M), where 1 < M <= 100; 1 <= n <=
M , returns all the segments with a score in the nth
part, when the range of scores is divided in M parts
of equal size.
Finally, a query can be composed of more than
one criterion. To do so, the logical operators AND
and OR are used to specify intersection and union,
respectively.
3.2.2 System- and Document-level Queries
The queries described next implement the search
procedures for more sophisticated queries involv-
ing system and document level properties, and
also the linguistic information used in the calcu-
184
lation of the evaluation measures. The purpose of
this functionality is to answer questions related to
groups of systems and/or metrics.
As explained in the introduction, one may
want to find the segments with good scores
for lexical metrics and, simultaneously, bad
scores for syntactic-based ones, or viceversa.
The following query illustrates how to do
it: ((srb[LEX] > AVG) OR (s3[LEX]
< AVG)) AND ((srb[SYN] < AVG) OR
(s3[SYN] > AVG) ), where srb = {s1, s2}
is the definition of a group of the rule-based
systems s1 and s2, s3 is another transla-
tion system, and LEX={BLEU,NIST} and
SYN={CP-Op(*),SP-Oc(*)} are two groups
of lexical- and syntactic-based measures, respec-
tively. The output of this kind of queries can help
developers to inspect the differences between the
systems that meet these criteria.
Concerning queries at document level, its struc-
ture is the same but applied at document scope.
They may help to find divergences when translat-
ing documents from different domains.
3.2.3 Linguistic Element-based Queries
The last functionality in tSEARCH allows search-
ing the segments that contain specific linguistic
elements (LE), estimated with any of the ana-
lyzers used to calculate the linguistic structures.
Linguistic-wise queries will allow the user to
find segments which match the criteria for any
linguistic feature calculated by ASIYA: part-of-
speech, lemmas, named entities, grammatical cat-
egories, dependency relations, semantic roles and
discourse structures.
We have implemented queries that match
n-grams of lemmas (lemma), parts-of-speech
(pos) and items of shallow (SP) or constituent
parsing (CP), dependency relations (DP) and se-
mantic roles SR, such as LE[lemma(be),
pos(NN,adj), SP(NP,ADJP,VP),
CP(VP,PP)]. The DP function allows also
specifying a compositional criterion (i.e., the
categories of two words and their dependency
relationship) and even a chain of relations, e.g.,
LE[DP(N,nsubj,V,dep,V)]. In turn, the
SR function obtains the segments that match a
verb and its list of arguments, e.g., LE[SR(ask,
A0, A1)].
The asterisk symbol can be used to substi-
tute any LE-item, e.g., LE[SP(NP,*,PP),
DP(*,*,V)]. When combined with semantic
roles, one asterisk substitutes any verb that has all
the arguments specified, e.g., LE[SR(*, A0,
A1)], whereas two asterisks in a row allow
arguments to belong to different verbs in the
same sentence. For instance, LE[SR(**, A1,
AM-TMP)] matches the sentence Those who pre-
fer to save money, may try to wait a few more days,
where the verb wait has the argument AM-TMP
and the verb prefer has the argument A1.
4 On-line Interface and Export of the
Results
tSEARCH is fully accessible on-line through the
ASIYA ON-LINE INTERFACE. The web applica-
tion runs ASIYA remotely, calculates the scores
and fills the tSEARCH database. It also offers the
chance to upload the results of a test suite previ-
ously processed. This way it feeds the database
directly, without the need to run ASIYA.
Anyhow, once the tSEARCH interface is already
accessible, one can see a tools icon on the right
of the search box. It shows the toolbar with all
available metrics, functions and operations. The
search box allows to query the database using the
query language described in Section 3.2.
After typing a query, the user can navigate the
results using three different views that organize
them according to the user preferences: 1) All
segments shows all segments and metrics men-
tioned in the query, the segments can be sorted
by the score, in ascendent or descendent order,
just tapping on the metric name; 2) Grouped by
system groups the segments by system and, for
Figure 5: The tSEARCH Interface
185
each system, by document; 3) Grouped by segment
displays the segment organization, which allows
an easy comparison between several translations.
Each group contains all the information related to
a segment number, such as the source and the ref-
erence sentences along with the candidate transla-
tions that matched the query.
Additionally, moving the mouse over the seg-
ments displays a floating box as illustrated in Fig-
ure 5. It shows some relevant information, such
as the source and references segments, the system
that generated the translation, the document which
the segment belongs to, and the scores.
Finally, all output data obtained during the
search can be exported as an XML file. It is possi-
ble to export all segments, or the results structured
by system, by segment, or more specific informa-
tion from the views.
5 Related Work and Conclusions
The ultimate goal of tSEARCH is to provide the
community with a user-friendly tool that facilitates
the qualitative analysis of automatic translations.
Currently, there are no freely available automatic
tools for aiding MT evaluation tasks. For this rea-
son, we believe that tSEARCH can be a useful tool
for MT system and evaluation metric developers.
So far, related works in the field address (semi)-
automatic error analysis from different perspec-
tives. A framework for error analysis and classifi-
cation was proposed in (Vilar et al, 2006), which
has inspired more recent works in the area, such
as (Fishel et al, 2011). They propose a method
for automatic identification of various error types.
The methodology proposed is language indepen-
dent and tackles lexical information. Nonetheless,
it can also take into account language-dependent
information if linguistic analyzers are available.
The user interface presented in (Berka et al, 2012)
provides also automatic error detection and clas-
sification. It is the result of merging the Hjer-
son tool (Popovic?, 2011) and Addicter (Zeman et
al., 2011). This web application shows alignments
and different types of errors colored.
In contrast, the ASIYA interface and the
tSEARCH tool together facilitate the qualitative
analysis of the evaluation results yet providing
a framework to obtain multiple evaluation met-
rics and linguistic analysis of the translations.
They also provide the mechanisms to search and
find relevant translation examples using a flexible
query language, and to export the results.
Acknowledgments
This research has been partially funded by
the Spanish Ministry of Education and Science
(OpenMT-2, TIN2009-14675-C03), the European
Community?s Seventh Framework Programme un-
der grant agreement number 247762 (FAUST,
FP7-ICT-2009-4-247762) and the EAMT Spon-
sorhip of Activities: Small research and develop-
ment project, 2012.
References
Jan Berka, Ondrej Bojar, Mark Fishel, Maja Popovic,
and Daniel Zeman. 2012. Automatic MT error anal-
ysis: Hjerson helping Addicter. In Proc. 8th LREC.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-Fine N-best Parsing and MaxEnt Discriminative
Reranking. In Proc. 43rd Meeting of the ACL.
Mark Fishel, Ondr?ej Bojar, Daniel Zeman, and Jan
Berka. 2011. Automatic Translation Error Analy-
sis. In Proc. 14th Text, Speech and Dialogue (TSD).
Jesu?s Gime?nez and Llu??s Ma`rquez. 2004. SVMTool:
A general POS tagger generator based on Support
Vector Machines. In Proc. 4th Intl. Conf. LREC.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2010. Asiya: An
Open Toolkit for Automatic Machine Translation
(Meta-)Evaluation. The Prague Bulletin of Mathe-
matical Linguistics, 94.
Meritxell Gonza`lez, Jesu?s Gime?nez, and Llu??s
Ma`rquez. 2012. A Graphical Interface for MT Eval-
uation and Error Analysis. In Proc. 50th Meeting of
the ACL. System Demonstration.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav
Marinov, and Erwin Marsi. 2007. MaltParser: A
language-independent system for data-driven depen-
dency parsing. Natural Language Engineering, 13.
Slav Petrov and Dan Klein. 2007. Improved Inference
for Unlexicalized Parsing. In Proc. HLT.
Maja Popovic?. 2011. Hjerson: An Open Source
Tool for Automatic Error Classification of Machine
Translation Output. The Prague Bulletin of Mathe-
matical Linguistics, 96.
Mihai Surdeanu, Jordi Turmo, and Eli Comelles. 2005.
Named Entity Recognition from Spontaneous Open-
Domain Speech. In Proc. 9th INTERSPEECH.
David Vilar, Jia Xu, Luis Fernando D?Haro, and Her-
mann Ney. 2006. Error Analysis of Machine Trans-
lation Output. In Proc. 5th LREC.
Daniel Zeman, Mark Fishel, Jan Berka, and Ondrej Bo-
jar. 2011. Addicter: What Is Wrong with My Trans-
lations? The Prague Bulletin of Mathematical Lin-
guistics, 96.
186
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 217?220,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Cooperative User Models in Statistical Dialog Simulators
Meritxell Gonza?lez1,2, Silvia Quarteroni1, Giuseppe Riccardi1, Sebastian Varges1
1 DISI - University of Trento, Povo (Trento), Italy
2 TALP Center - Technical University of Catalonia, Barcelona, Spain
mgonzalez@lsi.upc.edu, name.lastname@disi.unitn.it
Abstract
Statistical user simulation is a promis-
ing methodology to train and evaluate the
performance of (spoken) dialog systems.
We work with a modular architecture for
data-driven simulation where the ?inten-
tional? component of user simulation in-
cludes a User Model representing user-
specific features. We train a dialog sim-
ulator that combines traits of human be-
havior such as cooperativeness and con-
text with domain-related aspects via the
Expectation-Maximization algorithm. We
show that cooperativeness provides a finer
representation of the dialog context which
directly affects task completion rate.
1 Introduction
Data-driven techniques are a promising approach
to the development of robust (spoken) dialog sys-
tems, particularly when training statistical dialog
managers (Varges et al, 2009). User simulators
have been introduced to cope with the scarcity of
real user conversations and optimize a number of
SDS components (Schatzmann et al, 2006).
In this work, we investigate the combination of
aspects of human behavior with contextual aspects
of conversation in a joint yet modular data-driven
simulation model. For this, we integrate conversa-
tional context representation, centered on a Dialog
Act and a Concept Model, with a User Model rep-
resenting persistent individual features. Our aim
is to evaluate different simulation regimes against
real dialogs to identify any impact of user-specific
features on dialog performance.
In this paper, Section 2 presents our simulator
architecture and Section 3 focuses on our model of
cooperativeness. Our experiments are illustrated
Work partly funded by EU project ADAMACH (022593)
and Spanish project OPENMT-2 (TIN2009-14675-C03).
in Section 4 and conclusions are summarized in
Section 5.
2 Simulator Architecture
Data-driven simulation takes place within the rule-
based version of the ADASearch system (Varges
et al, 2009), which uses a taxonomy of 16 dialog
acts and a dozen concepts to deal with three tasks
related to tourism in Trentino (Italy): Lodging En-
quiry, Lodging Reservation and Event Enquiry.
Simulation in our framework occurs at the in-
tention level, where the simulator and the Dia-
log Manager (DM) exchange actions, i.e. or-
dered sequences of dialog acts and a number of
concept-value pairs. In other words, we repre-
sent the DM action as as = {da0, .., dan}, (s
is for ?System?) where daj is short for a dialog
act defined over zero or more concept-value pairs,
daj(c0(v0), .., cm(vm)).
In response to the DM action as, the different
modules that compose the User Simulator gener-
ate an N -best list of simulated actions Au(as) =
{a0u, .., a
N
u }. The probability of each possible ac-
tion being generated after the DM action as is es-
timated based on the conversation context. Such a
context is represented by a User Model, a Dialog
Act Model, a Concept Model and an Error Model
(Quarteroni et al, 2010). The User Model simu-
lates the behavior of an individual user in terms of
goals and other behavioral features such as coop-
erativeness and tendency to hang up. The Dialog
Act Model generates a distribution of M actions
Au = {a0u, .., a
M
u }. Then, one action a?u is chosen
out of Au. In order to vary the simulation behav-
ior, the choice of the user action a?u is a random
sampling according to the distribution of proba-
bilities therein; making the simulation more realis-
tic. Finally, the Concept Model generates concept
values for a?u; and the Error Model simulates the
noisy ASR-SLU channel by ?distorting? a?u.
These models are derived from the ADASearch
217
dataset, containing 74 spoken human-computer
conversations.
2.1 User Model
The User Model represents user-specific fea-
tures, both transient and persistent. The
transient feature we focus on in this work is
the user?s goal in the dialog (UG), represented
as a task name and the list of concepts and
values required to fulfill it: an example of
UG is {Activity(EventEnquiry), Time day(2),
Time month(may), Event type(fair), Loca-
tion name(Povo)}.
Persistent features included in our model so far
are: patience, silence (no input) and cooperative-
ness. Patience pat is defined as the tendency
to abandon the conversation (hang up event), i.e.
pat = P (HangUp|as). Similarly, NoInput prob-
ability noi is used to account for user behavior in
noisy environments: noi = P (NoInput|as). Fi-
nally, cooperativeness coop is a real value repre-
senting the ratio of concepts mentioned in as that
also appear in a?u (see Section 3).
2.2 Dialog Act Model
We define three Dialog Act (DA) Models: Obedi-
ent (OB), Bigram (BI) and Task-based (TB).
In the Obedient model, total patience and coop-
erativeness are assumed of the user, who will al-
ways respond to each query requiring values for a
set of concepts with an answer concerning exactly
such concepts. Formally, the model responds to a
DM action as with a single user action a?u obtained
by consulting a rule table, having probability 1. In
case a request for clarification is issued by the DM,
this model returns a clarifying answer. Any offer
from the DM to continue the conversation will be
either readily met with a new task request or de-
nied at a fixed probability: Au(as) = {(a?u, 1)}.
In the Bigram model, first defined in (Eckert et
al., 1997), a transition matrix records the frequen-
cies of transition from DM actions to user actions,
including hang up and no input/no match. Given
a DM action as, the model responds with a list of
M user actions and their probabilities estimated
according to action distribution in the real data:
Au(as) = {(a0u, P (a
0
u|as)), .., (a
M
u , P (a
M
u |as))}.
The Task-based model, similarly to the ?goal?
model in (Pietquin, 2004), produces an action dis-
tribution containing only the actions observed in
the dataset of dialogs in the context of a spe-
cific task Tk. The TB model divides the dataset
into one partition for each Tk, then creates a
task-specific bigram model, by computing ? k:
Au(as) = {(a0u, P (a
0
u|as, Tk)), .., (a
M
u , P (a
M
u |as, Tk))}.
As the partition of the dataset reduces the number
of observations, the TB model includes a mech-
anism to back off to the simpler bigram and uni-
gram models.
2.3 Concept & Error Model
The Concept Model takes the action a?u selected
by the DA Model and attaches values and sam-
pled interpretation confidences to its concepts. In
this work, we adopt a Concept Model which as-
signs the corresponding User Goal values for the
required concepts, which makes the user simulated
responses consistent with the user goal.
The Error Model is responsible of simulating
the noisy communication channel between user
and system; as we simulate the error at SLU level,
errors consist of incorrect concept values. We ex-
periment with a data-driven model where the pre-
cision Prc obtained by a concept c in the refer-
ence dataset is used to estimate the frequency with
which an error in the true value v? of c will be in-
troduced: P (c(v)|c(v?)) = 1? Prc (Quarteroni et
al., 2010).
3 Modelling Cooperativeness
As in e.g. (Jung et al, 2009), we define coop-
erativeness at the turn level (coopt) as a function
of the number of dialog acts in the DM action as
sharing concepts with the dialog acts in the user
action au; at the dialog level, coop is the average
of turn-level cooperativeness.
We discretize coop into a binary variable reflect-
ing high vs low cooperativeness based on whether
or not the dialog cooperativeness exceeds the me-
dian value of coop found in a reference corpus; in
our ADASearch dataset, the median value found
for coop is 0.28; hence, we annotate dialogs as co-
operative if they exceed such a threshold, and as
uncooperative otherwise. Using a corpus thresh-
old allows domain- and population-driven tuning
of cooperativeness rather than a ?hard? definition
(as in (Jung et al, 2009)).
We then model cooperativeness as two bigram
models, reflecting the high vs low value of coop.
In practice, given a DM action as and the coop
value (? = high/low) we obtain a list of user ac-
tions and their probabilities:
Au(as, ?) = {(a0u, P (a
0
u|as, ?)), .., (a
M
u , P (a
M
u |as, ?))}.
218
3.1 Combining cooperativeness and context
At this point, the distribution Au(as, ?) is lin-
early interpolated with the distribution of actions
Au(as, ?) obtained using the DA model ? (in the
Task-based DA model; ? can have three values,
one for each task as explained in Section 2.2):
Au(as) = ?? ?Au(as, ?) + ?? ?Au(as, ?),
where ?? and ?? are the weights of each fea-
ture/model and ?? + ?? = 1.
For each user action aiu, ?? and ?? are
estimated using the Baum-Welch Expectation-
Maximization algorithm as proposed by (Jelinek
and Mercer, 1980). We use the distributions of ac-
tions obtained from our dataset and we align the
set of actions of the two models. Since we only
have two models, we only need to calculate ex-
pectation for one of the distributions:
P (?|as, a
i
u) =
P (aiu|as, ?)
P (aiu|as, ?) + P (aiu|as, ?)
?Mi=0a
i
u
where M is the number of actions. Then, the
weights ?? and ?? that maximize the data like-
lihood are calculated as follows:
?? =
?M
j=0 P (?|as, a
j
u)
M
;?? = 1? ??.
The resulting combined distribution Au(as) is
obtained by factoring the probabilities of each ac-
tion with the weight estimated for the particular
distribution:
Au(as) = {(a
0
u, ???P (a
0
u|as, ?)), .., (a
M
u , ???P (a
M
u |as, ?)),
(a0u, ?? ? P (a
0
u|as, ?)), .., (a
M
u , ?? ? P (a
M
u |as, ?))}
3.2 Effects of cooperativeness
To assess the effect of the cooperativeness feature
in the final distribution of actions, we set a 5-fold
cross-validation experiment with the ADASearch
dataset where we average the ?? estimated at each
turn of the dialog. We investigated in which con-
text cooperativeness provides more contribution
by comparing the ?? weights attributed by high
vs. low coop models to user action distributions in
response to Dialog Manager actions.
Figure 1 shows the values achieved by ?? for
several DM actions for high vs low coop regimes.
We can see that ?? achieves high values in case
of uncooperative users in response to DM dialog
acts as [ClarificationRequest] and [Info-request].
In contrast, forward-looking actions, such as the
ones including [Offer], seem to discard the con-
tribution of the low coop model, but to favor the
contribution provided by high coop.
!"
!#$"
!#%"
!#&"
'()*+,-*./012345" '-*./012345" '67189/34:3;<5" '=/33<,>?3/5" '>?3/5" '6718/@,>?3/5"
A0+A" *8B"
Figure 1: Estimated ?? weights in response to se-
lected DM actions in case of high/low coop
4 Experiments
We evaluate our simulator models using two meth-
ods: first, ?offline? statistics are used to assess
how realistic the action estimations by DA Models
are with respect to a dataset of real conversations
(Sec. 4.1); then, ?online? statistics (Sec. 4.2) eval-
uate end-to-end simulator performance in terms of
dialog act distributions, error robustness and task
duration and completion rates by comparing real
dialogs with fresh simulated dialogs using action
sampling in the different simulation models.
4.1 ?Offline? statistics
In order to compare simulated and real user ac-
tions, we evaluate dialog act Precision (PDA)
and Recall (RDA) following the methodology in
(Schatzmann et al, 2005).
For each DM action as the simulator picks a
user action a?u from Au(as) and we compare it
with the real user choice a?u. A simulated dialog
act is correct when it appears in the real action
a?u. The measurements were obtained using 5-fold
cross-validation on the ADASearch dataset.
Table 1: Dialog Act Precision and Recall
Simulation (a?u) Most frequent (a
?
u)
DA Model PDA RDA PDA RDA
OB 33.8 33.4 33.9 33.5
BI (+coop) 35.6 (35.7) 35.5 (35.8) 49.3 (47.9) 48.8 (47.4)
TB (+coop) 38.2 (39.7) 38.1 (39.4) 51.1 (50.6) 50.6 (50.2)
Table 1 shows PDA/RDA obtained for the OB,
BI and TB models alone and with cooperative-
ness models (+coop). First, we see that TB is
much better than BI and OB at reproducing real
action selection. This is also visible in both PDA
and RDA obtained by selecting a?u, the most fre-
quent user action from the As generated by each
model. By definition, a?u maximizes the expected
PDA and RDA, providing an upper bound for our
models; however, to reproduce any possible user
behavior, we need to sample au rather than choos-
ing it by frequency. By now inspecting (+coop)
219
values in Table 1, we see that explicit cooperative-
ness models match real dialogs more closely. It
points out that partitioning the reference dataset in
high vs low coop sets allows better data represen-
tation. There is however no improvement in the
a?u case: we explain this by the fact that by ?slic-
ing? the reference dataset, the cooperative model
augments data sparsity, affecting robustness.
4.2 ?Online? statistics
We now discuss online deployment of our sim-
ulation models with different user behaviors and
?fresh? user goals and data. To align with the
ADASearch dataset, we ran 60 simulated dialogs
between the ADASearch DM and each combina-
tion of the Task-based and Bigram models and
high and low values of coop. For each set of simu-
lated dialogs, we measured task duration, defined
as the average number of turns needed to complete
each task, and task completion rate, defined as:
TCR = number of times a task has been completedtotal number of task requests .
Table 2 reports such figures in comparison
to the ones obtained for real dialogs from the
ADASearch dataset. In general, we see that task
duration is closer to real dialogs in the Bigram and
Task-based models when compared to the Obedi-
ent model. Moreover, it can easily be observed
in both BI and TB models that under high-coop
regime (in boldface), the number of turns taken
to complete tasks is lower than under low-coop.
Furthermore, in both TB and BI models, TCR
is higher when cooperativeness is higher, indicat-
ing that cooperative users make dialogs not only
shorter but also more efficient.
Table 2: Task duration and TCR in simulated di-
alogs with different regimes vs real dialogs.
Lodging Enquiry Lodging Reserv Event Enquiry All
Model #turns TCR #turns TCR #turns TCR TCR
OB 9.2?0.0 78.1 9.7?1.4 82.4 8.1?2.9 66.7 76.6
BI+low 15.1?4.1 71.4 14.2?3.9 69.4 9.3?1.8 52.2 66.7
BI+high 12.1?2.5 74.6 12.9?3.1 82.9 7.8?1.8 75.0 77.4
TB+low 13.6?4.1 75.8 13.4?3.7 83.3 8.4?3.3 64.7 77.2
TB+high 11.6?2.8 80.0 12.6?3.6 83.7 6.5?1.9 57.1 78.4
Real dialogs 11.1?3.0 71.4 12.7?4.7 69.6 9.3?4.0 85.0 73.4
5 Conclusion
In this work, we address data-driven dialog sim-
ulation for the training of statistical dialog man-
agers. Our simulator supports a modular combina-
tion of user-specific features with different models
of dialog act and concept-value estimation, in ad-
dition to ASR/SLU error simulation.
We investigate the effect of joining a model of
user intentions (Dialog Act Model) with a model
of individual user traits (User Model). In partic-
ular, we represent the user?s cooperativeness as
a real-valued feature of the User Model and cre-
ate two separate simulator behaviors, reproducing
high and low cooperativeness. We explore the im-
pact of combining our cooperativeness model with
the Dialog Act model in terms of dialog act accu-
racy and task success.
We find that 1) an explicit modelling of user
cooperativeness contributes to an improved accu-
racy of dialog act estimation when compared to
real conversations; 2) simulated dialogs with high
cooperativeness result in higher task completion
rates than low-cooperativeness dialogs. In future
work, we will study yet more fine-grained and re-
alistic User Model features.
References
W. Eckert, E. Levin, and R. Pieraccini. 1997. User
modeling for spoken dialogue system evaluation. In
Proc. IEEE ASRU.
F. Jelinek and R. L. Mercer. 1980. Interpolated estima-
tion of Markov source parameters from sparse data.
In Workshop on Pattern Recognition in Practice.
S. Jung, C. Lee, K. Kim, and G. G. Lee. 2009. Hy-
brid approach to user intention modeling for dialog
simulation. In Proc. ACL-IJCNLP.
O. Pietquin. 2004. A Framework for Unsupervised
Learning of Dialogue Strategies. Ph.D. thesis, Fac-
ulte? Polytechnique de Mons, TCTS Lab (Belgique).
S. Quarteroni, M. Gonza?lez, G. Riccardi, and
S. Varges. 2010. Combining user intention and error
modeling for statistical dialog simulators. In Proc.
INTERSPEECH.
J. Schatzmann, K. Georgila, and S. Young. 2005.
Quantitative evaluation of user simulation tech-
niques for spoken dialogue systems. In Proc. SIG-
DIAL.
J. Schatzmann, K. Weilhammer, M. Stuttle, and
S. Young. 2006. A survey of statistical user sim-
ulation techniques for reinforcement-learning of di-
alogue management strategies. Knowl. Eng. Rev.,
21(2):97?126.
S. Varges, S. Quarteroni, G. Riccardi, A. V. Ivanov, and
P. Roberti. 2009. Leveraging POMDPs trained with
user simulations and rule-based dialogue manage-
ment in a spoken dialogue system. In Proc. SIG-
DIAL.
220
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 127?132,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
The UPC Submission to the WMT 2012 Shared Task on Quality Estimation
Daniele Pighin Meritxell Gonza?lez Llu??s Ma`rquez
Universitat Polite`cnica de Catalunya, Barcelona
{pighin,mgonzalez,lluism}@lsi.upc.edu
Abstract
In this paper, we describe the UPC system that
participated in the WMT 2012 shared task on
Quality Estimation for Machine Translation.
Based on the empirical evidence that fluency-
related features have a very high correlation
with post-editing effort, we present a set of
features for the assessment of quality estima-
tion for machine translation designed around
different kinds of n-gram language models,
plus another set of features that model the
quality of dependency parses automatically
projected from source sentences to transla-
tions. We document the results obtained on
the shared task dataset, obtained by combining
the features that we designed with the baseline
features provided by the task organizers.
1 Introduction
Quality Estimation (QE) for Machine Translations
(MT) is the task concerned with the prediction of the
quality of automatic translations in the absence of
reference translations. The WMT 2012 shared task
on QE for MT (Callison-Burch et al, 2012) required
participants to score and rank a set of automatic
English to Spanish translations output by a state-
of-the-art phrase based machine translation system.
Task organizers provided a training dataset of 1, 832
source sentences, together with reference, automatic
and post-edited translations, as well as human qual-
ity assessments for the automatic translations. Post-
editing effort, i.e., the amount of editing required to
produce an accurate translation, was selected as the
quality criterion, with assessments ranging from 1
(extremely bad) to 5 (good as it is). The organizers
also provided a set of linguistic resources and pro-
cessors to extract 17 global indicators of translation
quality (baseline features) that participants could de-
cide to employ for their models. For the evaluation,
these features are used to learn a baseline predictors
for participants to compare against. Systems partic-
ipating in the evaluation are scored based on their
ability to correctly rank the 422 test translations (us-
ing DeltaAvg and Spearman correlation) and/or to
predict the human quality assessment for each trans-
lation (using Mean Average Error - MAE and Root
Mean Squared Error - RMSE).
Our initial approach to the task consisted of sev-
eral experiments in which we tried to identify com-
mon translation errors and correlate them with qual-
ity assessments. However, we soon realized that
simple regression models estimated on the baseline
features resulted in more consistent predictors of
translation quality. For this reason, we eventually
decided to focus on the design of a set of global in-
dicators of translation quality to be combined with
the strong features already computed by the baseline
system.
An analysis of the Pearson correlation of the
baseline features (Callison-Burch et al, 2012)1
with human quality assessments shows that the two
strongest individual predictors of post-editing ef-
fort are the n-gram language model perplexities es-
timated on source and target sentences. This ev-
idence suggests that a reasonable approach to im-
1Baseline features are also described in http://www.
statmt.org/wmt12/quality-estimation-task.
html.
127
Feature Pearson |r| Feature Pearson |r|
BL/4 0.3618 DEP/C+/Q4/R 0.0749
BL/5 0.3544 BL/13 0.0741
BL/12 0.2823 DEP/C?/Q1/W 0.0726
BL/14 0.2675 DEP/C+/Q4/W 0.0718
BL/2 0.2667 DEP/C+/Q34/R 0.0687
BL/1 0.2620 BL/3 0.0623
BL/8 0.2575 DEP/C+/Q34/W 0.0573
BL/6 0.2143 SEQ/sys-ref/W 0.0495
DEP/C?/S 0.2072 SEQ/sys/W 0.0492
BL/10 0.2033 SEQ/ref-sys/W 0.0390
DEP/C?/Q12/S 0.1858 BL/7 0.0351
BL/17 0.1824 SEQ/sys/SStop 0.0312
BL/16 0.1725 SEQ/sys/RStop 0.0301
DEP/C?/W 0.1584 SEQ/sys-ref/SStop 0.0291
DEP/C?/R 0.1559 SEQ/sys-ref/RStop 0.0289
DEP/C?/Q12/R 0.1447 DEP/Coverage/S 0.0286
DEP/Coverage/W 0.1419 SEQ/ref-sys/S 0.0232
DEP/C?/Q1/S 0.1413 SEQ/ref-sys/R 0.0205
BL/15 0.1368 SEQ/ref-sys/RStop 0.0187
DEP/C+/Q4/S 0.1257 SEQ/sys-ref/R 0.0184
DEP/Coverage/R 0.1239 SEQ/sys/R 0.0177
SEQ/ref-sys/PStop 0.1181 SEQ/ref-sys/Chains 0.0125
SEQ/sys/PStop 0.1173 SEQ/ref-sys/SStop 0.0104
SEQ/sys-ref/PStop 0.1170 SEQ/sys/S 0.0053
DEP/C?/Q12/W 0.1159 SEQ/sys-ref/S 0.0051
DEP/C?/Q1/R 0.1113 SEQ/sys/Chains 0.0032
DEP/C+/Q34/S 0.0933 SEQ/sys-ref/Chains 0.0014
BL/9 0.0889 BL/11 0.0001
Table 1: Pearson correlation (in absolute value) of the
baseline (BL) features and the extended feature set (SEQ
and DEP) with the quality assessments.
prove the accuracy of the baseline would be to con-
centrate on the estimation of other n-gram language
models, possibly working at different levels of lin-
guistic analysis and combining information coming
from the source and the target sentence. On top of
that, we add another class of features that capture
the quality of grammatical dependencies projected
from source to target via automatic alignments, as
they could provide clues about translation quality
that may not be captured by sequential models.
The novel features that we incorporate are de-
scribed in full detail in the next section; in Sec-
tion 3 we describe the experimental setup and the
resources that we employ, while in Section 4 we
present the results of the evaluation; finally, in Sec-
tion 5 we draw our conclusions.
2 Extended features set
We extend the set of 17 baseline features with 35
new features:
SEQ: 21 features based on n-gram language mod-
els estimated on reference and automatic trans-
lations, combining lexical elements of the tar-
get sentence and linguistic annotations (POS)
automatically projected from the source;
DEP: 18 features that estimate a language model
on dependency parse trees automatically pro-
jected from source to target via unsupervised
alignments.
All the related models are estimated on a cor-
pus of 150K newswire sentences collected from the
training/development corpora of previous WMT edi-
tions (Callison-Burch et al, 2007; Callison-Burch et
al., 2011). We selected this resource because we pre-
fer to estimate the models only on in-domain data.
The models for SEQ features are computed based
on reference translations (ref ) and automatic trans-
lations generated by the same Moses (Koehn et al,
2007) configuration used by the organizers of this
QE task. As features, we encode the perplexity of
observed sequences with respect to the two models,
or the ratio of these values. For DEP features, we es-
timate a model that explicitly captures the difference
between reference and automatic translations for the
same sentence.
2.1 Sequential features (SEQ)
The simplest sequential models that we estimate
are 3-gram language models2 on the following se-
quences:
W: (Word), the sequence of words as they appear
in the target sentence;
R: (Root), the sequence of the roots of the words in
the target;
S: (Suffix) the sequence of the suffixes of the words
in the target;
As features, for each automatic translation we en-
code:
? The perplexity of the corresponding sequence
according to automatic (sys) translations: for
2We also considered using longer histories, i.e., 5-grams, but
since we could not observe any noticeable difference we finally
selected the least over-fitting alternative.
128
example, SEQ/sys/R and SEQ/sys/W are the
root-sequence and word-sequence perplexities
estimated on the corpus of automatic transla-
tions;
? The ratio between the perplexities according
the two sets of translations: for example,
SEQ/ref-sys/S is the ratio between the perplex-
ity of suffix-sequences on reference and auto-
matic translations, and SEQ/sys-ref/S is its in-
verse.3
We also estimate 3-gram language models on
three variants of a sequence in which non-stop words
(i.e., all words belonging to an open class) are re-
placed with either:
RStop: the root of the word;
SStop: the suffix of the word;
PStop: the POS of the aligned source word(s).
This last model (PStop) is the only one that requires
source/target pairs in order to be estimated. If the
target word is aligned to more than one word, we
use the ordered concatenation of the source words
POS tags; if the word cannot be aligned, we replace
it with the placeholder ?*?, e.g.: ?el NN de * VBZ
JJ en muchos NNS .?. Also in this case, different
features encode the perplexity with respect to au-
tomatic translations (e.g., SEQ/sys/PStop) or to the
ratio between automatic and reference translations
(e.g., SEQ/ref-sys/RStop).
Finally, a last class of sequences (Chains) col-
lapses adjacent stop words into a single token.
Content-words or isolated stop-words are not in-
cluded in the sequence, e.g: ?mediante la de los
de la y de las y la a los?. Again, we consider
the same set of variants, e.g. SEQ/sys/Chains or
SEQ/sys-ref/Chains.
Since there are 7 sequence types and 3 combinations
(sys, sys-ref, ref-sys) we end up with 21 new fea-
tures.
3Features extracted solely from reference translations have
been considered, but they were dropped during development
since we could not observe a noticeable effect on prediction
quality.
2.2 Dependency features (DEP)
These features are based on the assumption that
by observing how dependency parses are projected
from source to target we can gather clues concern-
ing translation quality that cannot be captured by se-
quential models. The features encode the extent to
which the edges of the projected dependency tree are
observed in reference-quality translations.
The model for DEP features is estimated on
the same set of 150K English sentences and the
corresponding reference and automatic translations,
based on the following algorithm:
1. Initialize two maps M+ and M? to store edge
counts;
2. Then, for each source sentence s: parse s with
a dependency parser;
3. Align the words of s with the reference and the
automatic translations r and a;
4. For each dependency relation ?d, sh, sm? ob-
served in the source, where d is the relation
type and sh and sm are the head and modifier
words, respectively:
(a) Identify the aligned head/modifier words
in r and a, i.e., ?rh, rm? and ?ah, am?;
(b) If rh = ah and rm = am, then incre-
ment M+?d,ah,am? by one, otherwise incre-
ment M??d,ah,am?.
In other terms, M+ keeps track of how many times
a projected dependency is the same in the automatic
and in the reference translation, while M? accounts
for the cases in which the two projections differ.
Let T be the set of dependency relations projected
on an automatic translation. In the feature space we
represent:
Coverage: The ratio of dependency edges found in
M? or M+ over the total number of projected
edges, i.e.
Coverage(T ) =
?
D?T M
+
D +M
?
D
|T |
;
C+: The quantity C+ = 1|T |
?
D?T
M+D
M+D?M
?
D
;
129
C?: The quantity C? = 1|T |
?
D?T
M?D
M+D?M
?
D
.
Intuitively, high values of C+ mean that most pro-
jected dependencies have been observed in reference
translations; conversely, high values of C? suggest
that most of the projected dependencies were only
observed in automatic translations.
Similarly to SEQ features, also in this case we ac-
tually employ three variants of these features: one in
which we use word forms (i.e., DEP/Coverage/W,
DEP/C+/W and DEP/C?/W), one in which we
look at roots (i.e., DEP/Coverage/R, DEP/C+/R
and DEP/C?/R) and one in which we only con-
sider suffixes (i.e., DEP/Coverage/S, DEP/C+/S and
DEP/C?/S).
Moreover, we also estimate C+ in the top (Q4)
and top two (Q34) fourths of edge scores, and C? in
the bottom (Q1) and bottom two (Q12) fourths. As
an example, the feature DEP/C+/Q4/R encodes the
value of C+ within the top fourth of the ranked list of
projected dependencies when only considering word
roots, while DEP/C?/W is the value of C? on the
whole edge set estimated using word forms.
3 Experiment setup
To extract the extended feature set we use an align-
ment model, a POS tagger and a dependency parser.
Concerning the former, we trained an unsupervised
model with the Berkeley aligner4, an implementa-
tion of the symmetric word-alignment model de-
scribed by Liang et al (2006). The model is trained
on Europarl and newswire data released as part of
WMT 2011 (Callison-Burch et al, 2011) training
data. For POS tagging and semantic role annota-
tion we use SVMTool5 (Jesu?s Gime?nez and Llu??s
Ma`rquez, 2004) and Swirl6 (Surdeanu and Turmo,
2005), respectively, with default configurations. To
estimate the SEQ and DEP features we use refer-
ence and automatic translations of the newswire sec-
tion of WMT 2011 training data. The automatic
translations are generated by the same configura-
tion generating the data for the quality estimation
task. The n-gram models are estimated with the
4http://code.google.com/p/berkeleyaligner
5http://www.lsi.upc.edu/?nlp/SVMTool/
6http://www.surdeanu.name/mihai/swirl/
Feature set DeltaAvg MAE
Baseline 0.4664 0.6346
Extended 0.4694 0.6248
Table 2: Comparison of the baseline and extended feature
set on development data.
SRILM toolkit 7, with order equal to 3 and Kneser-
Ney (Kneser and Ney, 1995) smoothing.
As a learning framework we resort to Support
Vector Regression (SVR) (Smola and Scho?lkopf,
2004) and learn a linear separator using the SVM-
Light optimizer by Joachims (1999)8. We represent
feature values by means of their z-scores, i.e., the
number of standard deviations that separate a value
from the average of the feature distribution. We
carry out the system development via 5-fold cross
evaluation on the 1,832 development sentences for
which we have quality assessments.
4 Evaluation
In Table 1 we show the absolute value of the Pear-
son correlation of the features used in our model,
i.e., the 17 baseline features (BL/*), the 21 sequence
(SEQ/*) and the 18 dependency (DEP/*) features,
with the human quality assessments. The more cor-
related features are in the top (left) part of the ta-
ble. At a first glance, we can see that 9 of the 10
features having highest correlation are already en-
coded by the baseline. We can also observe that
DEP features show a higher correlation than SEQ
features. This evidence seems to contradict our ini-
tial expectations, but it can be easily ascribed to the
limited size of the corpus used to estimate the n-
gram models (150K sentences). This point is also
confirmed by the fact that the three variants of the
*PStop model (based on sequences of target stop-
words interleaved by POS tags projected from the
source sentence and, hence, on a very small vocab-
ulary) are the three sequential models sporting the
highest correlation. Alas, the lack of lexical anchors
makes them less useful as predictors of translation
quality than BL/4 and BL/5. Another interesting as-
7http://www-speech.sri.com/projects/
srilm
8http://svmlight.joachims.org/
130
System DeltaAvg MAE
Baseline 0.55 0.69
Official Evaluation 0.22 0.84
Amended Evaluation 0.51 0.71
Table 3: Official and amended evaluation on test data of
the extended feature sets.
pect is that DEP/C? features show higher correlation
than DEP/C+. This is an expected behaviour, as be-
ing indicators of possible errors they are intended to
have discriminative power with respect to the human
assessments. Finally, we can see that more than 50%
of the included features, including five baseline fea-
tures, have negligible (less than 0.1) correlation with
the assessments. Even though these features may not
have predictive power per se, their combination may
be useful to learn more accurate models of quality.9
Table 2 shows a comparison of the baseline fea-
tures against the extended feature set as the average
DeltaAvg score and Mean Absolute Error (MAE) on
the 10 most accurate development configurations. In
both cases, the extended feature set results in slightly
more accurate models, even though the improve-
ment is hardly significant.
Table 3 shows the results of the official evaluation.
Our submission to the final evaluation (Official) was
plagued by a bug that affected the values of all the
baseline features on the test set. As a consequence,
the official performance of the model is extremely
poor. The row labeled Amended shows the results
that we obtained after correcting the problem. As we
can see, on both tasks the baseline outperforms our
model, even though the difference between the two
is only marginal. Ranking-wise, our official submis-
sion is last on the ranking task and last-but-one on
the quality prediction task. In contrast, the amended
model shows very similar accuracy to the baseline,
as the majority of the systems that took part in the
evaluation.
9Our experiments on development data were not signifi-
cantly affected by the presence or removal of low-correlation
features. Given the relatively small feature space, we adopted
a conservative strategy and included all the features in the final
models.
5 Discussion and conclusions
We have described the system with which we par-
ticipated in the WMT 2012 shared task on quality
estimation. The model incorporates all the base-
line features, plus two sets of novel features based
on: 1) n-gram language models estimated on mixed
sequences of target sentence words and linguistic
annotations projected from the source sentence by
means of automatic alignments; and 2) the likeli-
hood of the projection of dependency relations from
source to target.
On development data we found out that the ex-
tended feature set granted only a very marginal im-
provement with respect to the strong feature set of
the baseline. In the official evaluation, our submis-
sion was plagued by a bug affecting the generation
of baseline features for the test set, and as a result
we had an incredibly low performance. After fix-
ing the bug, re-evaluating on the test set confirmed
that the extended set of features, at least in the cur-
rent implementation, does not have the potential to
significantly improve over the baseline features. On
the contrary, the accuracy of the corrected model is
slightly lower than the baseline on both the ranking
and the quality estimation task.
During system development it was clear that im-
proving significantly over the results of the base-
line features would be very difficult. In our expe-
rience, this is especially due to the presence among
the baseline features of extremely strong predictors
of translation quality such as the perplexity of the
automatic translation. We could also observe that
the parametrization of the learning algorithm had
a much stronger impact on the final accuracy than
the inclusion/exclusion of specific features from the
model.
We believe that the information that we encode,
and in particular dependency parses and stop-word
sequences, has the potential to be quite relevant for
this task. On the other hand, it may be necessary to
estimate the models on much larger datasets in order
to compensate for their inherent sparsity. Further-
more, more refined methods may be required in or-
der to incorporate the relevant information in a more
determinant way.
131
Acknowledgments
This research has been partially funded by
the Spanish Ministry of Education and Science
(OpenMT-2, TIN2009-14675-C03) and the Euro-
pean Community?s Seventh Framework Programme
(FP7/2007-2013) under grant agreement numbers
247762 (FAUST project, FP7-ICT-2009-4-247762)
and 247914 (MOLTO project, FP7-ICT-2009-4-
247914).
References
[Callison-Burch et al2007] Chris Callison-Burch,
Philipp Koehn, Cameron Shaw Fordyce, and Christof
Monz, editors. 2007. Proceedings of the Second
Workshop on Statistical Machine Translation. ACL,
Prague, Czech Republic.
[Callison-Burch et al2011] Chris Callison-Burch,
Philipp Koehn, Christof Monz, and Omar F. Zaidan,
editors. 2011. Proceedings of the Sixth Workshop
on Statistical Machine Translation. Association for
Computational Linguistics, Edinburgh, Scotland, July.
[Callison-Burch et al2012] Chris Callison-Burch,
Philipp Koehn, Christof Monz, Matt Post, Radu
Soricut, and Lucia Specia. 2012. Findings of the
2012 workshop on statistical machine translation.
In Proceedings of the Seventh Workshop on Statis-
tical Machine Translation, Montreal, Canada, June.
Association for Computational Linguistics.
[Jesu?s Gime?nez and Llu??s Ma`rquez2004] Jesu?s Gime?nez
and Llu??s Ma`rquez. 2004. SVMTool: A general POS
tagger generator based on Support Vector Machines.
In Proceedings of the 4th LREC.
[Joachims1999] Thorsten Joachims. 1999. Making large-
Scale SVM Learning Practical. In B. Scho?lkopf,
C. Burges, and A. Smola, editors, Advances in Kernel
Methods - Support Vector Learning.
[Kneser and Ney1995] Reinhard Kneser and Hermann
Ney. 1995. Improved backing-off for m-gram lan-
guage modeling. In In Proceedings of the IEEE Inter-
national Conference on Acoustics, Speech and Signal
Processing, volume I, pages 181?184, Detroit, Michi-
gan, May.
[Koehn et al2007] Philipp Koehn, Hieu Hoang, Alexan-
dra Birch, Chris Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bo-
jar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
Companion Volume Proceedings of the Demo and
Poster Sessions, pages 177?180, Prague, Czech Re-
public, June. Association for Computational Linguis-
tics.
[Liang et al2006] Percy Liang, Benjamin Taskar, and
Dan Klein. 2006. Alignment by agreement. In HLT-
NAACL.
[Smola and Scho?lkopf2004] Alex J. Smola and Bernhard
Scho?lkopf. 2004. A tutorial on support vector regres-
sion. Statistics and Computing, 14(3):199?222, Au-
gust.
[Surdeanu and Turmo2005] Mihai Surdeanu and Jordi
Turmo. 2005. Semantic Role Labeling Using Com-
plete Syntactic Analysis. In Proceedings of the
Ninth Conference on Computational Natural Lan-
guage Learning (CoNLL-2005), pages 221?224, Ann
Arbor, Michigan, June.
132
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 359?364,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
The TALP-UPC Approach to System Selection: ASIYA Features
and Pairwise Classification using Random Forests
Llu??s Formiga1, Meritxell Gonza`lez1, Alberto Barro?n-Ceden?o1,2
Jose? A. R. Fonollosa1 and Llu??s Ma`rquez1
1 TALP Research Center, Universitat Polite`cnica de Catalunya, Spain
2 Facultad de Informa?tica, Universidad Polite?cnica de Madrid, Spain
{lluis.formiga,jose.fonollosa}@upc.edu, {mgonzalez,albarron,lluism}@lsi.upc.edu
Abstract
This paper describes the TALP-UPC par-
ticipation in the WMT?13 Shared Task
on Quality Estimation (QE). Our partic-
ipation is reduced to task 1.2 on System
Selection. We used a broad set of fea-
tures (86 for German-to-English and 97
for English-to-Spanish) ranging from stan-
dard QE features to features based on
pseudo-references and semantic similarity.
We approached system selection by means
of pairwise ranking decisions. For that,
we learned Random Forest classifiers es-
pecially tailored for the problem. Evalua-
tion at development time showed consider-
ably good results in a cross-validation ex-
periment, with Kendall?s ? values around
0.30. The results on the test set dropped
significantly, raising different discussions
to be taken into account.
1 Introduction
In this paper we discuss the TALP-UPC1 partici-
pation in the WMT?13 Shared Task on Quality Es-
timation (QE). Our participation is circumscribed
to task 1.2, which deals with System Selection.
Concretely, we were required to rank up to five al-
ternative translations for the same source sentence
produced by multiple MT systems, in the absence
of any reference translation.
We used a broad set of features; mainly avail-
able through the last version of the ASIYA toolkit
for MT evaluation2 (Gime?nez and Ma`rquez,
2010). Concretely, we derived 86 features for
the German-to-English subtask and 97 features for
English-to-Spanish. These features cover different
approaches and include standard Quality Estima-
tion features, as provided by the above mentioned
1Center for Language and Speech Technologies and Ap-
plications (TALP), Technical University of Catalonia (UPC).
2http://asiya.lsi.upc.edu
ASIYA toolkit and Quest (Specia et al, 2010),
but also a variety of features based on pseudo-
references (Soricut and Echihabi, 2010), explicit
semantic analysis (Gabrilovich and Markovitch,
2007) and specialized language models. See sec-
tion 3 for details.
In order to model the ranking problem associ-
ated to the system selection task, we adapted it
to a classification task of pairwise decisions. We
trained Random Forest classifiers (and compared
them to SVM classifiers), expanding the work of
Formiga et al (2013), from which a full ranking
can be derived and the best system per sentence
identified.
Evaluation at development time, using cross-
validation, showed considerably good and stable
results for both language pairs, with correlation
values around 0.30 (Kendall ? coefficient) classi-
fication accuracies around 52% (pairwise classifi-
cation) and 41% (best translation identification).
Unfortunately, the results on the test set were sig-
nificantly lower. Current research is devoted to ex-
plain the behavior of the system at testing time. On
the one hand, it seems clear that more research re-
garding the assignment of ties is needed in order
to have a robust model. On the other hand, the re-
lease of the gold standard annotations for the test
set will facilitate a deeper analysis and understand-
ing of the current results.
The rest of the paper is organized as follows.
Section 2 describes the ranking models studied for
the system selection problem. Section 3 describes
the features used for learning. Section 4 presents
the setting for parameter optimization and feature
selection and the results obtained. Finally, Sec-
tion 5 summarizes the lessons learned so far and
outlines some lines for further research.
2 Ranking Model
We considered two learning strategies to obtain the
best translation ranking model: SVM and Random
359
Forests. Both strategies were based on predicting
pairwise quality ranking decisions by means of su-
pervised learning. These decision was motivated
from our previous work (Formiga et al, 2013)
were we learned that they were more consistent to
select the best system (according to human and au-
tomatic metrics) compared to absolute regression
approaches. In that work we used only the subset
of features 1, 2, 3 and 8 described in Section 3.
For this shared task we have introduced additional
similarity measures (subsets 4 to 7) that feature se-
mantic analysis and automatic alignments between
the source and the translations.
The rationale for transforming a ranking prob-
lem to a pairwise classification problem has been
described previously in several work (Joachims,
2002; Burges et al, 2005). The main idea is to en-
semble the features of both individuals and assign
a class {-1,1} which tries to predict the pairwise
relation among them. For linear based approach
this adaptation is as simple to compute the differ-
ence between features between all the pairs of the
training data.
We used two different learners to perform that
task. First, we trained a Support Vector Machine
ranker by means of pairwise comparison using
the SVMlight toolkit (Joachims, 1999), but with
the ?-z p? parameter, which can provide system
rankings for all the members of different groups.
The learner algorithm was run according to the
following parameters: RBF-kernel, expanding the
working set by 9 variables at each iteration, for a
maximum of 50,000 iterations and with a cache
size of 100 for kernel evaluations. The trade-off
parameter was empirically set to 0.001. This im-
plementation ignores the ties for the training step
as it only focuses in better than/ worse than rela-
tions.
Secondly, we used Random Forests (Breiman,
2001), the rationale was the same as ranking-to-
pairwise implementation from SVMlight. How-
ever, SVMlight considers two different data pre-
processing methods depending on the kernel of
the classifier: LINEAR and RBF-Kernel. We
used the same data-preprocessing algorithm from
SVMlight in order to train a Random Forest clas-
sifier with ties (three classes: {0,-1,1}) based
upon the pairwise relations. We used the Random
Forests implementation of scikit-learn toolkit (Pe-
dregosa et al, 2011) with 50 estimators.
Once the classes are given by the Random For-
est, we build a graph by means of the adjacency
matrix of the pairwise decision. Once the adja-
cency matrix has been built, we assign the final
ranking through a dominance scheme similar to
Pighin et al (2012). In that case, however, there
are not topological problems as the pairwise rela-
tions are complete across all the edges.
3 Features Sets
We considered a broad set of features: 97 and
86 features for English-to-Spanish (en-es) and
German-to-English (de-en), respectively. We
grouped them into the following categories: base-
line QE metrics, comparison against pseudo-
references, source-translation, and adapted lan-
guage models. We describe them below. Unless
noted otherwise, the features apply to both lan-
guage pairs.
3.1 Baseline Features
The baseline features are composed of well-known
quality estimation metrics:
1. Quest Baseline (QQE)
Seventeen baseline features from Specia et
al. (2010). This set includes token counts
(and their ratio), LM probabilities for source
and target sentences, percentage of n-grams
in different quartiles of a reference corpus,
number of punctuation marks, and fertility
ratios. We used these features in the en-es
partition only.
2. ASIYA?s QE-based features (AQE)
Twenty-six QE features provided by
ASIYA (Gonza`lez et al, 2012), comprising
bilingual dictionary ambiguity and overlap;
ratios concerning chunks, named-entities and
PoS; source and candidate LM perplexities
and inverse perplexities over lexical forms,
chunks and PoS; and out-of-vocabulary word
indicators.
3.2 Pseudo-Reference-based Features
Soricut and Echihabi (2010) introduced the con-
cept of pseudo-reference-based features (PR) for
translation ranking estimation. The principle is
that, in the lack of human-produced references,
automatic ones are still good for differentiating
good from bad translations. One or more sec-
ondary MT systems are required to generate trans-
lations starting from the same input, which are
360
taken as pseudo-references. The similarity to-
wards the pseudo-references can be calculated
with any evaluation measure or text similarity
function, which gives us all feature variants in this
group. We consider the following PR-based fea-
tures:
3. Derived from ASIYA?s metrics (APR)
Twenty-three PR features, including GTM-l
(l?{1,2,3}) to reward different length match-
ing (Melamed et al, 2003), four variants of
ROUGE (-L, -S*, -SU* and -W) (Lin and
Och, 2004), WER (Nie?en et al, 2000),
PER (Tillmann et al, 1997), TER, and
TERbase (i.e., without stemming, synonymy
look-up, nor paraphrase support) (Snover et
al., 2009), and all the shallow and full pars-
ing measures (i.e., constituency and depen-
dency parsing, PoS, chunking and lemmas)
that ASIYA provides either for Spanish or En-
glish as target languages.
4. Lexical similarity (NGM)
Cosine and Jaccard coefficient similarity
measures for both token and character
n-grams considering n ? [2, 5] (i.e., sixteen
features). Additionally, one Jaccard-based
similarity measure for ?pseudo-prefixes?
(considering only up to four initial characters
for every token).
5. Based on semantic information (SEM)
Twelve features calculated with named
entity- and semantic role-based evaluation
measures (again, provided by ASIYA). Sen-
tences are automatically annotated using
SwiRL (Surdeanu and Turmo, 2005) and
BIOS (Surdeanu et al, 2005). We used these
features in the de-en subtask only.
6. Explicit semantic analysis (ESA)
Two versions of explicit semantic analy-
sis (Gabrilovich and Markovitch, 2007), a
semantic similarity measure, built on top of
Wikipedia (we used the opening paragraphs
of 100k Wikipedia articles as in 2010).
3.3 Source-Translation Extra Features
Source-translation features include explicit com-
parisons between the source sentence and its trans-
lation. They are meant to measure how adequate
the translation is, that is, to what extent the trans-
lation expresses the same meaning as the source.
Note that a considerable amount of the features
described in the baseline group (QQE and AQE)
fall in this category. In this subsection we include
some extra features we devised to capture source?
translation dependencies.
7. Alignment-based features (ALG / ALGPR)
One measure calculated over the aligned
words between a candidate translation and
the source (ALG); and two measures based on
the comparison between these alignments for
two different translations (e.g., candidate and
pseudo-reference) and the source (ALGPR).3
8. Length model (LeM)
A measure to estimate the quality likeli-
hood of a candidate sentence by considering
the ?expected length? of a proper translation
from the source. The measure was introduced
by (Pouliquen et al, 2003) to identify docu-
ment translations. We estimated its param-
eters over standard MT corpora, including
Europarl, Newswire, Newscommentary and
UN.
3.4 Adapted Language-Model Features
We interpolated different language models com-
prising the WMT?12 Monolingual corpora (EPPS,
News, UN and Gigafrench for English). The in-
terpolation weights were computed as to minimize
the perplexity according to the WMT Translation
Task test data (2008-2010)4. The features are as
follow:
9. Language Model Features (LM)
Two log-probabilities of the translation can-
didate with respect to the above described in-
terpolated language models over word forms
and PoS labels.
4 Experiments and Results
In this section we describe the experiments car-
ried out to select the best feature set, learner, and
learner configuration. Additionally, we present
the final performance within the task. The set-
up experiments were addressed doing two separate
10-fold cross validations on the training data and
averaging the final results. We evaluated the re-
sults through three indicators: Kendall?s ? with no
3Alignments were computed with the Berkeley aligner
https://code.google.com/p/berkeleyaligner/
4http://www.statmt.org/wmt13/translation-task.html
361
penalization for the ties, accuracy in determining
the pairwise relationship between candidate trans-
lations, and global accuracy in selecting the best
candidate for each source sentence.
First, we compared our SVM learner against
Random Forests with the two variants of data
preprocessing (LINEAR and RBF). In terms of
Kendall?s ? , we found that the Random Forests
(RF) were clearly better compared to SVM imple-
mentation. Concretely, depending on the final fea-
ture set, we found that RF achieved a ? between
0.23 and 0.29 while SVM achieved a ? between
0.23 and 0.25. With respect to the accuracy mea-
sures we did not find noticeable differences be-
tween methods as their results moved from 49% to
52%. However, considering the accuracy in terms
of selecting only the best system there was a dif-
ference of two points (42.2% vs. 40.0%) between
methods, being RF again the best system. Regard-
ing the pairwise preprocessing the results between
RBF and LINEAR based preprocessing were com-
parable, being RBF slightly better than LINEAR.
Hence, we selected Random Forests with RBF
pairwise preprocessing as our final learner.
de-en ? with ties AccuracyIgnored Penalized All Best
AQE+LeM+ALGPR+LM 33.70 15.72 52.56 41.57
AQE+SEM+LM 32.49 14.61 52.72 40.92
AQE+LeM+ALGPR+ESA+LM 32.08 13.81 52.71 41.37
AQE+ALG+ESA+SEM+LM 32.06 13.96 52.20 40.64
AQE+ALG+LM 31.97 14.29 52.00 40.83
AQE+LeM+ALGPR+SEM+LM 31.93 13.57 52.52 40.98
AQE+ESA+SEM+LM 31.79 13.68 52.50 40.76
AQE+LeM+ALGPR+ESA+SEM+LM 31.72 14.01 52.65 40.83
AQE+ALG+SEM+LM 31.17 12.86 52.18 40.51
AQE+ALG+SEM 30.72 12.58 51.75 39.66
AQE+LeM+ALGPR+ESA+SEM 30.47 11.79 51.85 39.58
AQE+ESA+LM 30.31 12.23 52.60 40.69
AQE+ALG+ESA+LM 30.26 12.40 52.03 40.99
AQE+LeM+ALGPR 30.24 11.83 51.96 40.42
AQE+LeM+ALGPR+SEM 30.23 11.84 52.10 40.32
AQE+LeM+ALGPR+ESA 29.89 11.87 51.83 40.07
AQE+ALG+ESA 29.81 11.30 51.37 39.47
AQE+SEM 29.80 12.06 51.75 39.52
AQE+NGM+APR+ESA+SEM+LM 29.34 10.58 51.33 38.55
AQE+ESA+SEM 29.31 11.46 51.66 39.24
AQE+ESA 29.13 11.12 51.82 39.90
AQE+ALG+ESA+SEM 28.35 10.32 51.37 38.98
AQE+NGM+APR+ESA+SEM 27.55 9.22 51.01 38.12
Table 1: Set-up results for de-en
For the feature selection process, we considered
the most relevant combinations of feature groups.
Table 1 shows the set-up results for the de-en sub-
task and Table 2 shows the results for the en-es
subtask.
In terms of ? we observed similar results be-
tween the two language pairs. However accura-
cies for the de-en subtask were one point above
the ones for en-es. Regarding the features used, we
found that the best feature combination to use was
composed of: i) a baseline QE feature set (Asiya
or Quest) but not both of them, ii) Length Model,
iii) Pseudo-reference aligned based features and
the use of iv) adapted language models. However,
within the de-en subtask, we found that substitut-
ing Length Model and Aligned Pseudo-references
by the features based on Semantic Roles (SEM)
could bring marginally better accuracy. We also
noticed that the learner was sensitive to the fea-
tures used so selecting the appropriate set of fea-
tures was crucial to achieve a good performance.
en-es ? with ties AccuracyIgnored Penalized All Best
QQE+LeM+ALGPR+LM 33.81 15.87 51.66 41.01
AQE+LeM+ALGPR+LM 33.75 16.44 51.56 41.52
QQE+AQE+LM 32.71 14.59 51.18 41.02
QQE+AQE+LM+ESA 32.69 15.30 51.48 41.30
QQE+AQE+LeM+ALGPR+LM+ESA 32.63 13.64 51.39 40.48
QQE+AQE+LeM+ALGPR+LM 32.41 14.06 51.43 40.49
QQE+LeM+ALGPR+LM+ESA 31.66 13.39 51.37 41.05
QQE+AQE+ALG+LM 31.46 13.62 51.28 41.29
AQE+LeM+ALGPR+LM+ESA 31.29 14.10 51.55 41.43
QQE+AQE+ALG+LM+ESA 31.25 13.58 51.64 41.66
QQE+AQE+NGM+APR+LM+ESA 30.58 12.48 50.93 40.66
QQE+AQE+NGM+APR+LM 29.94 12.54 50.95 40.25
QQE+AQE 28.98 10.92 49.97 39.65
QQE+AQE+LeM+ALGPR 28.94 10.48 49.99 39.71
QQE+AQE+NGM+ESA+LM 28.85 11.88 50.90 40.22
AQE+LeM+ALGPR 28.81 10.11 50.06 40.01
QQE+AQE+ESA 28.68 10.31 49.96 39.27
AQE+ESA 28.67 10.81 50.35 39.18
AQE 28.65 10.68 49.76 38.90
QQE+AQE+ALG 28.47 9.63 49.67 39.66
QQE+AQE+NGM+APR+ESA 28.43 9.75 49.67 38.74
QQE+AQE+NGM 27.23 9.10 49.44 38.98
QQE+AQE+ALG+ESA 27.08 7.93 50.26 39.71
QQE+AQE+LeM+ALGPR+ESA 27.03 8.65 50.35 40.49
AQE+LeM+ALGPR+ESA 26.96 8.26 50.30 39.47
QQE+AQE+NGM+ESA 26.59 7.56 49.52 38.62
QQE+AQE+NGM+APR 25.39 6.97 49.90 39.53
Table 2: Setup results for en-es
de-en ? (ties penalized,
ID non-symmetric between [-1,1])
Best 0.31
UPC AQE+SEM+LM 0.11
UPC AQE+LeM+ALGPR+LM 0.10
Baseline Random-ranks-with-ties -0.12
Worst -0.49
Table 3: Official results for the de-en subtask (ties
penalized)
en-es ? (ties penalized,
ID non-symmetric between [-1,1])
Best 0.15
UPC QQE+LeM+ALGPR+LM -0.03
UPC AQE+LeM+ALGPR+LM -0.06
Baseline Random-ranks-with-ties -0.23
Worst -0.63
Table 4: Official results for the en-es subtask (ties
penalized)
In Tables 3, 4, 5 and 6 we present the official re-
sults for the WMT?13 Quality Estimation Task, in
all evaluation variants. In each table we compare
to the best/worst performing systems and also to
the official baseline.
We can observe that in general the results on
the test sets drop significantly, compared to our
362
de-en ? (ties ignored, Non-ties
ID symmetric /between [-1,1]) (882 dec.)
Best 0.31 882
UPC AQE+SEM+LM 0.27 768
UPC AQE+LeM+ALGPR+LM 0.24 788
Baseline Random-ranks-with-ties 0.08 718
Worst -0.03 558
Table 5: Official results for the de-en subtask (ties
ignored)
en-es ? (ties ignored, Non-ties
ID symmetric /between [-1,1]) (882 dec.)
Best 0.23 192
UPC QQE+LeM+ALGPR+LM 0.11 554
UPC AQE+LeM+ALGPR+LM 0.08 554
Baseline Random-ranks-with-ties 0.03 507
Worst -0.11 633
Table 6: Official results for the en-es subtask (ties
ignored)
set-up experiments. Restricting to the evaluation
setting in which ties are not penalized (i.e., cor-
responding to our setting during system and pa-
rameter tuning), we can see that the results corre-
sponding to de-en (Table 5) are comparable to our
set-up results and close to the best performing sys-
tem. However, in the en-es language pair the final
results are comparatively much lower (Table 6).
We find this behavior strange. In this respect, we
analyzed the inter-annotator agreement within the
gold standard. Concretely we computed the Co-
hen?s ? for all overlapping annotations concerning
at least 4 systems for both language pairs. The re-
sults of our analysis are presented in Table 7 and
therefore it confirms our hypothesis that en-es an-
notations had more noise providing an explanation
for the accuracy decrease of our QE models and
setting the subtask into a more challenging sce-
nario. However, further research will be needed to
analyze other factors such as oracles and improve-
ment on automatic metrics prediction and reliabil-
ity compared to linguistic expert annotators.
Another remaining issue for our research con-
cerns investigating better ways to deal with ties,
as their penalization lowered our results dramati-
cally. In this direction we plan to work further on
# of Lang Cohen?s # ofsystems ? elements
4 en-es 0.210 560de-en 0.369 640
5 en-es 0.211 130de-en 0.375 145
Table 7: Golden standard test set agreement coef-
ficients measured by Cohen?s ?
the adjacency matrix reconstruction heuristics and
presenting the features to the learner in a struc-
tured form.
5 Conclusions
This paper described the TALP-UPC participation
in the WMT?13 Shared Task. We approached the
Quality Estimation task based on system selection,
where different systems have to be ranked accord-
ing to their quality. We derive a full ranking and
identify the best system per sentence on the basis
of Random Forest classifiers.
After the model set-up, we observed consid-
erably good and robust results for both transla-
tion directions, German-to-English and English-
to-Spanish: Kendall?s ? around 0.30 as well as
accuracies around 52% on pairwise classification
and 41% on best translation identification. How-
ever, the results over the official test set were
significantly lower. We have found that the low
inter-annotator agreement between users on that
set might provide an explanation to the poor per-
formance of our QE models.
Our current efforts are centered on explaining
the behavior of our QE models when facing the of-
ficial test sets. We are following two directions: i)
studying the ties? impact to come out with a more
robust model and ii) revise the English-to-Spanish
gold standard annotations in terms of correlation
with automatic metrics to facilitate a deeper un-
derstanding of the results.
Acknowledgments
Acknowledgements
This work has been partially funded by the
Spanish Ministerio de Econom??a y Competitivi-
dad, under contracts TEC2012-38939-C03-02
and TIN2009-14675-C03, as well as from
the European Regional Development Fund
(ERDF/FEDER) and the European Commu-
nity?s FP7 (2007-2013) program under the
following grants: 247762 (FAUST, FP7-ICT-
2009-4-247762) and 246016 (ERCIM ?Alain
Bensoussan? Fellowship).
References
Leo Breiman. 2001. Random forests. Machine Learn-
ing, 45(1):5?32.
Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier,
Matt Deeds, Nicole Hamilton, and Greg Hullender.
363
2005. Learning to rank using gradient descent. In
Proceedings of the 22nd international conference on
Machine learning, pages 89?96. ACM.
Llu??s Formiga, Llu??s Ma`rquez, and Jaume Pujantell.
2013. Real-life translation quality estimation for mt
system selection. In Proceedings of 14th Machine
Translation Summit (MT Summit), Nice, France,
September. EAMT.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing Semantic Relatedness Using Wikipedia-
based Explicit Semantic Analysis. In Proceedings
of the 20th International Joint Conference on Artifi-
cial Intelligence, pages 1606?1611, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2010. Asiya: An
Open Toolkit for Automatic Machine Translation
(Meta-)Evaluation. The Prague Bulletin of Mathe-
matical Linguistics, (94):77?86.
Meritxell Gonza`lez, Jesu?s Gime?nez, and Llu??s
Ma`rquez. 2012. A graphical interface for mt evalu-
ation and error analysis. In Proceedings of the ACL
2012 System Demonstrations, pages 139?144, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Thorsten Joachims, 1999. Advances in Kernel Methods
? Support Vector Learning, chapter Making large-
Scale SVM Learning Practical. MIT Press.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In ACM, editor, Proceed-
ings of the ACM Conference on Knowledge Discov-
ery and Data Mining (KDD).
Chin-Yew Lin and Franz Josef Och. 2004. Auto-
matic evaluation of machine translation quality us-
ing longest common subsequence and skip-bigram
statistics. In Proceedings of the 42nd Meeting
of the Association for Computational Linguistics
(ACL?04), Main Volume, pages 605?612, Barcelona,
Spain, July.
I. Dan Melamed, Ryan Green, and Joseph P. Turian.
2003. Precision and recall of machine translation.
In HLT-NAACL.
Sonja Nie?en, Franz Josef Och, Gregor Leusch, and
Hermann Ney. 2000. An evaluation tool for ma-
chine translation: Fast evaluation for mt research.
In Proceedings of the 2nd Language Resources and
Evaluation Conference (LREC 2000).
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine Learn-
ing in Python . Journal of Machine Learning Re-
search, 12:2825?2830.
Daniele Pighin, Llu??s Formiga, and Llu??s Ma`rquez.
2012. A graph-based strategy to streamline trans-
lation quality assessments. In Proceedings of the
Tenth Conference of the Association for Machine
Translation in the Americas (AMTA?2012), San
Diego, USA, October. AMTA.
Bruno Pouliquen, Ralf Steinberger, and Camelia Ignat.
2003. Automatic Identification of Document Trans-
lations in Large Multilingual Document Collections.
In Proceedings of the International Conference on
Recent Advances in Natural Language Processing
(RANLP-2003), pages 401?408, Borovets, Bulgaria.
Matthew G. Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. TER-Plus: Paraphrase,
Semantic, and Alignment Enhancements to Trans-
lation Edit Rate. Machine Translation, 23(2):117?
127.
Radu Soricut and Abdessamad Echihabi. 2010.
Trustrank: Inducing trust in automatic translations
via ranking. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 612?621, Uppsala, Sweden, July. As-
sociation for Computational Linguistics.
Lucia Specia, Dhwaj Raj, and Marco Turchi. 2010.
Machine Translation Evaluation Versus Quality Es-
timation. Machine Translation, 24:39?50, March.
Mihai Surdeanu and Jordi Turmo. 2005. Semantic
Role Labeling Using Complete Syntactic Analysis.
In Proceedings of CoNLL Shared Task.
Mihai Surdeanu, Jordi Turmo, and Eli Comelles. 2005.
Named Entity Recognition from Spontaneous Open-
Domain Speech. In Proceedings of the 9th Inter-
national Conference on Speech Communication and
Technology (Interspeech).
C. Tillmann, S. Vogel, H. Ney, A. Zubiaga, and
H Sawaf. 1997. Accelerated dp based search for
statistical translation. In Proceedings of European
Conference on Speech Communication and Technol-
ogy.
364
