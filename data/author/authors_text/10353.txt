Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 497?504
Manchester, August 2008
 
Understanding and Summarizing Answers in Community-Based 
Question Answering Services 
Yuanjie Liu1, Shasha Li2, Yunbo Cao1,3, Chin-Yew Lin3, Dingyi Han1, Yong Yu1 
1Shanghai Jiao Tong University, 
Shanghai, China, 200240 
{lyjgeorge,handy,yyu} 
@apex.sjtu.edu.cn 
2National University of 
Defense Technology, 
Changsha, China, 410074
Shashali 
@nudt.edu.cn 
3Microsoft Research Asia,
Beijing, China, 100080 
{yunbo.cao,cyl} 
@microsoft.com 
 
Abstract 
Community-based question answering 
(cQA) services have accumulated millions 
of questions and their answers over time. 
In the process of accumulation, cQA ser-
vices assume that questions always have 
unique best answers. However, with an in-
depth analysis of questions and answers 
on cQA services, we find that the assump-
tion cannot be true. According to the anal-
ysis, at least 78% of the cQA best answers 
are reusable when similar questions are 
asked again, but no more than 48% of 
them are indeed the unique best answers. 
We conduct the analysis by proposing 
taxonomies for cQA questions and an-
swers. To better reuse the cQA content, 
we also propose applying automatic sum-
marization techniques to summarize an-
swers. Our results show that question-type 
oriented summarization techniques can 
improve cQA answer quality significantly. 
1 Introduction 
Community-based question and answering (cQA) 
service is becoming a popular type of search re-
lated activity. Major search engines around the 
world have rolled out their own versions of cQA 
service. Yahoo! Answers, Baidu Zhidao, and 
Naver Ji-Sik-In1 are some examples.  
In general, a cQA service has the following 
workflow. First, a question is posted by the asker 
in a cQA service and then people in the commu-
nity can answer the question. After enough num-
ber of answers are collected, a best answer can 
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution- Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
be chosen by the asker or voted by the communi-
ty. The resulting question and answer archives 
are large knowledge repositories and can be used 
to complement online search. For example, Nav-
er?s Ji-Sik-In (Knowledge iN) has accumulated 
about 70 million entries2.  
In an ideal scenario, a search engine can serve 
similar questions or use best answers as search 
result snippets when similar queries are submit-
ted. To support such applications, we have to 
assume the best answers from cQA services are 
good and relevant answers for their pairing ques-
tions. However, the assumption might not be true 
as exemplified by the following examples. 
Question Title 
Which actress has the most seductive 
voice?..could range from a giggly goldie 
hawn..to a sultry anne bancroft? 
Question  
Description 
or any other type of voice that you find allur-
ing. .. 
Best Answer 
(Polls & Surveys) Fenella Fielding, wow!!!! 
Best Answer 
(Movies) i think joanna lumlley has a really sexy voice 
Table 1. Same Question / Different Best Answers
 
Question Title Does anyone know of any birthdays coming up soon? 
Question  
Description 
Celerities, people you know, you? Anyway I 
need the name and the date. If you want to 
know it is for my 
site,  http://www.jessicaparke2.piczo.com... 
and that is not site advertising.  
Answer 
Novembers Are: 
Paul Dickov nov 1st 
Nelly (not furtado) nov 2nd ? 
Best Answer Check imdb.com, they have this celebrity birthdays listed. 
Table 2. Question with Alternative Answers 
Table 1 presents a question asking communi-
ty opinions about ?who is the actress has the 
most seductive voice?. The asker posted the same 
question twice at different Yahoo! Answers cate-
gories: one in Polls & Surveys and one in Movies. 
                                                 
1 Yahoo! Answers: answers.yahoo.com; Baidu Zhidao: zhi-
dao.baidu.com; Naver Ji-Sik-In: kin.naver.com 
2www.iht.com/articles/2007/07/04/technology/naver.php 
497
 Two different best answers were chosen by the 
same asker due to non-overlapping of answers. 
Table 2 shows another example, it asks about 
?the coming birthdays of stars?. The best answer 
chosen by the asker is very good because it pro-
vides useful URL information where the asker 
can find her answers. However, other answers 
listed a variety of birthdays of stars that also 
answered the question. These two examples indi-
cate that the conventional cQA policy of allow-
ing askers or voters to choose best answers might 
be working fine with the purpose of cQA but it 
might not be a good one if we want to reuse these 
best answers without any post-processing. 
To find out what might be the alternatives to 
the best answers, we first carried out an in-depth 
analysis of cQA data by developing taxonomies 
for questions and answers. Then we propose 
summarizing answers in a consideration of ques-
tion type, as the alternative to the best answers. 
For example, for the ?actress voice? question, a 
summary of different people?s opinions ranked 
by popularity might be a better way for express-
ing the question?s answers. Similar to the ?ac-
tress voice? question, the ?celebrity birthday? 
question does not have a fix set of answers but is 
different from the ?actress voice? question that its 
answers are facts not opinions. For fact-based 
open ended questions, combining different an-
swers will be useful for reuse of those answers.  
The rest of this paper is arranged as follows. 
We review related work in Section 2. We devel-
op a framework for answer type taxonomy in 
Section 3 and a cQA question taxonomy in Sec-
tion 4. Section 5 presents methods to summarize 
cQA answers. Finally, we conclude this paper 
and discuss future work in Section 6. 
2 Related Work 
Previous research on cQA (community-based 
Question and Answering) domain focused on 
three major areas: (1) how to find similar ques-
tions given a new question (Jeon et al 2005a; 
Jeon et al, 2005b), (2) how to find experts given 
a community network(Liu et al, 2005; Jurczyk & 
Agichtein, 2007), and (3) how to measure answer 
quality and its effect on question retrieval. The 
third area of focus is the most relevant to our re-
search. Jeon et al (2006)?s work on assessing 
cQA answer quality is one typical example. They 
found that about 1/3 of the answers among the 
1,700 Q&A pairs from Naver.com cQA data 
have quality problems and approximately 1/10 of 
them have bad answers 3 . They used 13 non-
textual features and trained a maximum entropy 
model to predict answer quality. They showed 
that retrieval relevance was significantly im-
proved when answer quality measure was inte-
grated in a log likelihood retrieval model.  
As mentioned in Section 1, cQA services 
provide an alternative way for users to find in-
formation online. Questions posted on cQA sites 
should reflect users? needs as queries submitted 
to search engines do. Broder (2002) proposed 
that search queries can be classified into three 
categories, i.e. navigational, informational, and 
transactional. Ross and Levinson (2004) sug-
gested a more elaborated taxonomy with five 
more subcategories for informational queries and 
four more subcategories for resource (transac-
tional) queries. In open-domain question answer-
ing research that automatic systems are required 
to extract exact answers from a text database 
given a set of factoid questions (Voorhees and M. 
Ellen, 2003), all top performing systems had in-
corporated question taxonomies (Hovy et al, 
2001; Moldovan et al, 2000; Lytinen et al, 2002; 
Jijkoun et al, 2005). Based on the past expe-
riences from the annual NIST TREC Question 
and Answering Track 4  (TREC QA Track), an 
international forum dedicating to evaluate and 
compare different open-domain question answer-
ing systems, we conjecture that a cQA question 
taxonomy would help us determine what type of 
best answer is expected given a question type.  
Automatic summarization of cQA answers is 
one of the main focuses of this paper. We pro-
pose that summarization techniques (Hovy and 
Lin, 1999; Lin and Hovy, 2002) can be used to 
create cQA answer summaries for different ques-
tion types. Creating an answer summary given a 
question and its answers can be seen as a multi-
document summarization task. We simply re-
place documents with answers and apply these 
techniques to generate the answer summary. The 
task has been one of the main tasks the Docu-
ment Understanding Conference5 since 2004. 
3 A Framework  for Answer Type 
To study how to exploit the best answers of cQA, 
we need to first analyze cQA answers. We would 
like to know whether the existing best answer of 
a specific question is good for reuse. If not, we 
                                                 
3 Answers in Jeon el al.?s work were rated in three levels: 
good, medium, and bad. 
4 http://trec.nist.gov/data/qamain.html 
5 http://duc.nist.gov 
498
want to 
are. We 
by cQA 
ferentiat
tomatica
We m
swers fo
for answ
al categ
examini
the 4 mo
ries (100
tainmen
(S&C), H
we dev
based on
termines
can be r
ilar to th
One o
ercise an
The tax
cussions
notators 
category
annotato
on a sin
made t
taxonom
discuss 
answer t
Figur
Figur
onomy. 
Reusabl
means th
similar 
while a 
reused. 
Factual 
that can 
jective B
as the be
F
Unique
understand w
will refer to
askers or vo
e it with be
lly generate
ade use of
r developin
er type. The
ories in Yah
ng 400 rando
st popular 
 questions f
t & Music 
ealth, and 
eloped a cQ
 the princip
 a BA?s ans
eused or not
e BA?s ques
f the author
d developed
onomy was 
 among the 
to do the a
 label that 
rs. If none o
gle categor
he final d
y is describ
the question
axonomy in 
e 1. cQA Ser
e 1 shows th
It first divid
e and Not 
at it can be 
question to 
Not Reusab
The Reusabl
and Subject
be used as t
A is one of 
st answer. 
Reusable
actual
Not?
Unique
Direct Indire
Su
hy and wha
 the ?best a
ters as BA h
st answers 
d in our expe
 questions fr
g and testing
re are over 
oo! Answe
mly selecte
top Yahoo! 
rom each ca
(E&M), So
Computers &
A answer 
le of BA reu
wer type bas
 when a que
tion is asked
s carried ou
 the initial a
then modif
authors. We
nnotation. W
was agreed 
f the three a
y label, one
ecision. Th
ed in this 
 type and t
next section
vices BA Ty
e resulting 
es BA into
Reusable. A
reused as the
its question
le BA mea
e BA is fur
ive. A Fact
he best answ
the opinions
Best?
Answer
ct
bjective Re
t the alterna
nswers? sele
enceforth to
annotated or
riments. 
om Yahoo!
 our framew
1,000 hierar
rs. By manu
d questions 
Answers cat
tegory) ? E
ciety & Cu
 Internet (C
type taxon
sability tha
ed on ?if th
stion that is 
 again?. 
t this manua
nswer taxon
ied through
 asked three
e assigned
by at least
nnotators ag
 of the aut
e answer 
section and
he relation 
. 
pe Taxonom
answer type
 two catego
 Reusable
 best answe
 is asked a
ns it canno
ther divided
ual BA is a
er; while a 
 that can be 
Not??
Reusable
levant Irre
 
tives 
cted 
 dif-
 au-
 An-
ork 
chic-
ally 
from 
ego-
nter-
lture 
&I), 
omy 
t de-
e BA 
sim-
l ex-
omy. 
 dis-
 an-
 the 
 two 
reed 
hors 
type 
 we 
with 
 
y. 
 tax-
ries: 
BA 
r if a 
gain; 
t be 
 into 
 fact 
Sub-
used 
T
Uniq
a un
answ
Uniq
The 
type
its q
swer
ple, 
Indi
whil
birth
A
for o
ques
Each
T
vant
used
leva
aske
Nick
?I'm
ly So
answ
er?s 
ques
best
its q
tion 
give
answ
ema
To b
Answ
Uniqu
Direc
Indire
Factu
Subje
Reusa
Relev
Irrelev
Not R
T
type
mor
ries 
two 
 
A
mos
(50%
the o
or v
answ
levant
he Factual
ue and Not
ique best an
er add m
ue BA has
Not Unique
s: Direct an
uestion dire
s its questio
the question
rect BA wh
e there is al
day lists. 
 Subjective
pinions or r
tion asked ?
 answerer w
he Not Reus
 and Irrelev
 as a best an
nt to its qu
d ?Why was
 Lachey so 
 not sure wh
uth Jersey, 
er is releva
location wh
tion; an Irre
answer to i
uestion. Th
period has 
n that meets
er?.? of th
il without sh
ox? is in thi
er Type 
e 
t 
ct 
al Total 
ctive 
ble Total 
ant
ant
eusable Total
Table 3. D
able 3 show
s on four ca
e than 48%. 
tend to hav
categories.
mong the fo
tly not uniq
) of subjec
ne BA per c
oters is not g
er. Howeve
BA type 
 Unique. A 
swer to its q
ore informa
 other alter
BA type is d
d Indirect. A
ctly; while 
n through i
 mentioned
ich gives a
so a Direct a
BA answers
ecommenda
Which is th
ould have h
able BA has
ant. A Relev
swer to its 
estion, for 
 "I Can't Ha
shortlived??
ere you live
that song wa
nt but witho
ich does n
levant BA c
ts question a
e BA ?It ap
expired. If 
 your needs
e question ?
owing the em
s case. 
C&I
47%
28%
9%
84%
4%
88%
3%
9%
12%
istribution o
s the dist
tegories. Un
The C&I an
e more fac
ur categorie
ue and hav
tive answers
QA questio
ood enough
r, we might
has two s
Unique BA 
uestion and 
tion; while
native best 
ivided into 
 Direct BA
an Indirect
nference. Fo
 in section 1
 website re
nswer just g
 questions t
tions. For ex
e best sci-fi 
is own idea.
 two subtyp
ant BA cou
question bu
example, a 
te You Anym
 A Relevant
, but in NJ, 
s played ou
ut knowing
ot really an
ould not be u
nd it is irre
pears that t
an answer h
, please pic
how to for
ail address
E&M Heal
28% 48
7% 30
3% 5
38% 83%
40% 7
78% 90%
1% 1
21% 9
22% 10%
f Answer Ty
ribution of 
ique answer
d the Health
tual BAs th
s, S&C ans
e a high pe
. This indic
n chosen by
 for reuse as
 be able to a
ubtypes: 
has only 
no other 
 a Not 
answers. 
two sub-
 answers 
 BA an-
r exam-
 has the 
ference, 
ives the 
hat look 
ample, a 
movie?? 
 
es: Rele-
ld not be 
t it is re-
question 
ore" by 
BA said 
especial-
t??, this 
 the ask-
swer the 
sed as a 
levant to 
he ques-
as been 
k a ?best 
ward an 
es in the 
th S&C
% 13%
% 18%
% 2%
 33%
% 50%
 83%
% 0%
% 17%
 17%
pe 
Answer 
s are no 
 catego-
an other 
wers are 
rcentage 
ates that 
 its asker 
 the best 
pply au-
499
tomatic 
summar
(but not 
ible solu
E
T
Table
over wh
on a sin
the ques
stable (o
4 A C
As we w
my, we 
themselv
well. As
question
best answ
Rose an
engine q
their tax
engine q
we follo
onomy a
modate t
Fi
Figur
my. We
and prop
Informa
ilar as in
ry consi
an answ
with peo
Navig
seeking 
would li
know the
Trans
tend to g
compute
Navigat
summariza
ized answers
unique) answ
tions in Sect
Category
Computer & In
ntertainment &
Health 
Society & Cul
able 4. Disag
 4 shows t
ich none of
gle category
tion taxonom
ver at least 7
QA Quest
ere develop
often could
es and had 
 we discuss
 would help
er types.  
d Levinson?
ueries has 
onomy was 
ueries. Inste
wed the ba
nd made so
he particula
gure 2.  Que
e 2 shows th
 retain Brod
ose a new S
tional and Tr
 Broder?s ta
sts of questi
er but just w
ple in cQA 
ational ca
URLs of sp
ke to visit, 
 fan sites of
actional ca
et resources
r program th
ional Inform
Constant
Opinion
tion techni
 for at least
ers. We pro
ion 5. 
 
ternet 
 Music 
ture 
reement on 
he percenta
 the three a
 label. The r
y develope
9% question
ion Taxon
ing our answ
 not solely 
to consider t
ed in Sectio
 us determ
s (2004) tax
similar goal
developed t
ad of starti
sic hierarchy
me modific
r of cQA ser
stion Type T
e resulting 
er?s taxono
ocial catego
ansactional
xonomy whi
ons that do 
ere used to 
services. 
tegory con
ecific websit
for example
 Hannah Mo
tegory con
. A typical o
at lets you c
cQA
Question
ational
Dynamic
Context?
Dependent
Trans
ques to c
 half of reus
vide some p
Percenta
18%
17%
21%
20%
Answer Typ
ge of ques
nnotators ag
esults show
d above is p
s). 
omy 
er type tax
rely on ans
heir question
n 2, the typ
ine the expe
onomy of se
 to ours th
o classify se
ng from scr
 of R&L?s 
ations to acc
vices. 
axonomy
question tax
my at top le
ry. Navigati
 are defined 
le Social cat
not intend to
elicit intera
tains ques
es that the a
, ?Does any
ntana?? 
tains ques
ne is ?Is the
reate a plan
Open
actional
 
reate 
able 
oss-
ge 
e 
tions 
reed 
 that 
retty 
ono-
wers 
s as 
e of 
cted 
arch 
ough 
arch 
atch, 
tax-
om-
 
ono-
vels 
onal, 
sim-
ego-
 get 
ction 
tions 
sker 
body 
tions 
re a 
et?? 
F
to t
Con
answ
dich
port
betw
taxo
R&L
ques
latio
wou
F
tego
Opin
Que
peop
think
jects
ple.
tion
diffe
?Wh
diffe
Ope
som
have
selv
tion 
com
follo
clud
cont
T
serv
to g
joke
tially
or o
lazy
toge
beco
goog
will 
they
a ne
who
T
ques
cate
only
ques
occu
sinc
sear
Social
or Informati
wo subcateg
stant questio
ers while d
otomy of in
 our intentio
een the que
nomy. Cons
?s closed q
tion is ?Whi
n?? but ?W
ld be a dyna
or Dynamic
ries: Opinio
ion questio
stions in th
le in cQA 
 of some p
. ?Is Micros
Context-dep
s having dif
rent contex
at is the pop
rent answer
n category
e facts or m
 a variety o
es may have
?Can you li
ing week??
ws R&L?s 
es what is n
ext-depende
he new Soc
ices. Questio
et an answer
s and expre
, askers trea
nline forums
people com
ther with th
me a hacker
le search?
continue to 
 can give up
gative sentim
 asked how t
able 5 show
tion types 
gories. We 
 occupy 1
tions are ev
r in the sam
e people ve
ch engines 
on category,
ories: Con
ns have a f
ynamic que
formational
n to establi
stion taxon
tant questio
uery type. A
ch country h
hat is the po
mic question
category, w
n, Context-D
ns are those 
is category 
communiti
eople, some
oft Vista wo
endent ques
ferent answ
t. For exa
ulation of C
s according 
contains q
ethods. Th
f answers o
 unconstrain
st some birt
is an exam
open query 
ot covered 
nt categories
ial category
ns in this ca
. These que
ssing askers
t cQA servi
. The questi
e on here si
e question 
? It really is
hopefully so
ask, will cli
 faster? ? a
ent toward
o become a 
s the distr
on 4 differe
observe tha
1% ~ 20%
en fewer su
ple question
ry likely w
to discover 
 we first div
stant and D
ixed or stab
stions do n
 category is
sh intuitive 
omy and the
n type is s
n example 
as the large
pulation of 
. 
e define thre
ependent an
asking for o
seek opinio
es about w
 events, or s
rth it?? is a
tions are tho
ers accordin
mple, the 
hina?? sho
to the differ
uestions ask
e questions
r their answ
ed depth. T
hdays of sta
ple. This es
category. It
by the opin
. 
 is specific
tegory do n
stions includ
? own ideas
ce as chattin
on ?Why do 
mply just to
description 
n't that har
me of the pe
ck the link b
ctually is ex
s a number o
hacker. 
ibution of 
nt Yahoo! 
t constant q
 while nav
ch that they
s. This is re
ould be abl
answers of
ide it in-
ynamic. 
le set of 
ot. This 
 to sup-
mapping 
 answer 
imilar to 
constant 
st popu-
China?? 
e subca-
d Open. 
pinions. 
ns from 
hat they 
ome ob-
n exam-
se ques-
g to the 
question 
uld have 
ent date. 
ing for 
 usually 
er them-
he ques-
rs in the 
sentially 
 also in-
ion and 
 to cQA 
ot intend 
e telling 
. Essen-
g rooms 
so many 
 ask...?? 
?how to 
d to do a 
ople that 
elow so 
pressing 
f people 
different 
Answers 
uestions 
igational 
 do not 
asonable 
e to use 
 naviga-
500
 tional and constant questions. They do not have 
to ask these types of question on community-
based question answering services. On the con-
trary, open and opinion questions are frequently 
asked, it ranges from 56%~83%.  
Question Type C&I E&M Health S&C
Navigational Total 0% 0% 0% 0%
Constant 15% 20% 15% 11%
Opinion 8% 37% 16% 60%
Context     Dependent 0% 1% 1% 0%
Open 59% 19% 67% 18%
Dynamic Total 67% 57% 84% 78%
Informational Total 82% 77% 99% 89%
Transactional Total 14% 8% 0% 1%
Social Total 4% 15% 1% 10%
Table 5 Distribution of Question Type 
Intersection Number UNI DIR IND SUB REL IRR
Navigational 0 0 0 0 0 0
Constant 48 9 3 0 1 0
Open 51 62 13 15 5 17
Context-dep 0 0 1 0 0 1
Opinion 15 13 1 84 0 8
Transactional 10 7 4 1 0 1
Social 0 0 0 1 0 29
Table 6. Question Answer Correlation 
Table 6 (UNI: unique, DIR: direct, IND: indi-
rect, SUB: subjective, REL: relevant, IRR: irre-
levant) gives the correlation statistics of question 
type vs. answer type. There exists a strong corre-
lation between question type and answer type. 
Every question type tends to be associated with 
only one or two answer types (bold numbers in 
Table 6).  
5 Question-Type Oriented Answer 
Summarization 
Since the BAs for at least half of questions do 
not cover all useful information of other answers, 
it is better to adopt post-processing techniques 
such as answer summarization for better reuse of 
the BAs. As observed in the previous sections, 
answer types can be basically predicted by ques-
tion type. Thus, in this section, we propose to use 
multi-document summarization (MDS) tech-
niques for summarizing answers according to 
question type. Here we assume that question type 
can be determined automatically. In the follow-
ing sub-sections, we will focus on the summari-
zation of answers to open or opinion questions as 
they occupy more than half of the cQA questions. 
5.1 Open Questions 
Algorithm: For open questions, we follow typi-
cal MDS procedure: topic identification, inter-
pretation & fusion, and then summary generation 
(Hovy and Lin, 1999; Lin and Hovy, 2002). Ta-
ble 7 describes the algorithm.  
1. Employ the clustering algorithm on answers 
2. Extract the noun phrases in each cluster, using a shallow parser.6 
3. For each cluster and each label (or noun phrase), calculate the 
score by using the Relevance Scoring Function:  
?p?w|??PMI?w, l|C? ?  D??|C?
?
 
Where ? is the cluster, w is the word, l is the label or noun phrase, C 
is the background context which is composed of 5,000 questions 
in the same category, p(?) is conditional probability, PMI(?) is 
pointwise mutual information, and D(?) is KL-divergence 
4. Extract the key answer which contains the noun phrase that has 
the highest score in each cluster 
5. Rank these key answers by cluster size and present the results. 
Table 7. Summarization Algorithm(Open-Type) 
In the first step, we use a bottom-up approach 
for clustering answers to do topic identification. 
Initially, each answer forms a cluster. Then we 
combine the most similar two clusters as a new 
cluster if their similarity is higher than a thre-
shold. This process is repeated until no new clus-
ters can be formed. For computing similarities, 
we regard the highest cosine similarity of two 
sentences from two different clusters as the simi-
larity of the two clusters. Then we extract salient 
noun phrases, i.e. cluster labels, from each clus-
ter using the first-order relevance scoring func-
tion proposed by Mei et al (2007), (step 2,3 in 
Table 7).  In the fusion phase (step 4), these 
phrases are then used to rank answers within 
their cluster. Finally in the generation phase (step 
5), we present the summarized answer by ex-
tracting the most important answer in every clus-
ter and sort them according to the cluster size 
where they come from.  
Case Example: Table 8 presents an example 
of summarization results of open-type questions. 
The question asks how to change Windows XP 
desktop to Mac style. There are many softwares 
providing such functionalities. The BA only lists 
one choice ? the StarDock products, while other 
answers suggest Flyakite and LiteStep. The au-
tomatic summarized answer (ASA) contains a 
variety of for turning Windows XP desktop into 
Mac style with their names highlighted as cluster 
labels. Compared with manually-summarized 
answer (MSA), ASA contains most information 
of MSA while retains similar length with BA and 
MSA. 
5.2 Opinion Questions 
Algorithm: For opinion questions, a comprehen-
sive investigation of this topic would be beyond 
the scope of this paper since this is still a field 
                                                 
6 http://opennlp.sourceforge.net 
501
 under active development (Wiebe et al, 2003; 
Kim and Hovy, 2004). We build a simple yet 
novel opinion-focused answer summarizer which 
provides a global view of all answers. We divide 
opinion questions into two subcategories. One is 
sentiment-oriented question that asks the senti-
ment about something, for example, ?what do 
you think of ??. The other is list-oriented ques-
tion that intends to get a list of answers and see 
what item is the most popular.  
For sentiment-oriented questions, askers care 
about how many people support or against some-
thing. We use an opinion word dictionary7, a cue 
phrase list, a simple voting strategy, and some 
heuristic rules to classify the sentences into Sup-
port, Neutral, or Against category and use the 
overall attitude with key sentences to build sum-
marization. For list-oriented questions, a simple 
counting algorithm that tallies different answers 
of questions together with their supporting votes 
would be good answer summaries. Details of the 
algorithm are shown in Table 9, 10. 
Case Example: Table 11 presents the summa-
rization result of an sentiment-oriented question, 
it asks ?whether it is strange for a 16-year child 
to talk to a teddy bear??, the BA is a negative 
response. However, if we consider all answers, 
                                                 
7 Inquirer dictionary  http://www.wjh.harvard.edu/~inquirer. 
we find that half of the answers agree but another 
half of them disagree. The distribution of differ-
ent sentiments is similar as MSA. Table 12 
shows the summarization result of a list-oriented 
question, the question asks ?what is the best sci-fi 
movie?? The BA just gives one choice ?Indepen-
dence day? while the summarized answer gives a 
list of best sci-fi movies with the number of sup-
porting vote. Though it is not complete compared 
with MSA, it contains most of the options which 
has highest votes among all answers. 
1. Employ the same cluster procedure of Open-Type question. 
2. If an answer begins with negative cue phrase (e.g. ?No, it isn?t? 
etc.), it is annotated as Against. If a response begins with positive 
cue phrase (e.g. ?Yes, it is? etc.), it is annotated as Support. 
3. For a clause, if number of positive sentiment word is larger than 
negative sentiment word, the sentiment of the clause is Positive. 
Otherwise, the sentiment of the clause is Negative. 
4. If there are negative indicators such as ?don?t/never/?? in front 
of the clause, the sentiment should be reversed. 
5. If number of negative clauses is larger than number of positive 
clauses, the sentiment of the answer is Negative. Otherwise, the 
sentiment of the answer is Positive. 
6. Denote the sentiment value of question as s(q), the sentiment 
value of an answer as s(a), and then the final sentiment of the an-
swer is logical AND of s(q) and s(a) 
7. Present key sentiments with attitude label 
Table 9. Summarization Algorithm (Senti-
ment-Opinion) 
1. Segment the answers into sentences 
2. Cluster sentences  by using similar process in open-type 
3. For each cluster, choose the key sentence based on mutual infor-
mation between itself and other sentences within the cluster 
4. Rank the key sentences by the cluster size and present them 
ogether with votes 
Table 10. Summarization Algorithm (List-
Opinion) 
Question (http://answers.yahoo.com/question/?qid=1006050125145) 
I am 16 and i stil talk to my erm..teddy bear..am i wierd??? 
Best Answer Chosen 
not at all i'm 14 and i too do that 
Auto-summarized Answer 
Support 
A: It's might be a little uncommon for a 16 year old to talk to a 
teddy bear but there would be a serious problem if you told me that 
your teddy bear answered back as you talked to him!!:)  
A: I slept with my teddy bear until I graduated.  Can't say that I 
ever had a conversation with him, but if I had I'm sure he would've 
been a very good listener. 
Against 
A: i talk to a  seed im growing .. its not weird .... :)  
A: No, you're not weird.....you're Pratheek! :D  
A: no, i like to hold on to my old memories too. i do it sometimes 
too.  
A: It will get weird when he starts to answer back!  
A: not really. it depends how you talk i mean not if you talk to it 
like its a little kid like my brother does.  
Overall Attitude: Support 5 / Neutral 1 / Against 5  
Manually-summarized Answer 
support (vote 4) 
neutral (vote 2) 
against (vote 5) reasons: i like to hold on to my old memories too. 
(vote 1) I slept with my teddy bear until I graduated. (vote 1) i'm 14 
and i too do that (vote 1) 
Table 11. Summary of Sentiment-Opinion 
Question 
 
 
Question 
(http://answers.yahoo.com/question/?qid=1005120801427) 
What is the best way to make XP look like Mac osX? 
Best Answer Chosen 
I found the best way to do this is to use WindowsBlinds. A pro-
gram that, if you use the total StarDock, package will allow you to 
add the ObjectBar in addition to changed the toolbars to be OS X 
stylized. If you want added functionality you can download pro-
grams off the internet that will mimic the Expose feature which will 
show you a tiled set of all open windows. Programs that will do this 
include: WinPlosion, Windows Exposer, and Top Desk 
Auto-summarized Answer 
LiteStep:An additional option is LiteStep - a "Shell Replacement" 
for Windows that has a variety of themes you can install. Undoub-
tedly there are various Mac OSX themes avaialable for LiteStep. I 
have included a source to a max osx theme for Litestep at custom-
ize.org.  
Flyakite:Flyakite is a transformation pack and the most compre-
hensive in terms of converting an XP system's look to that of an OS 
X system, google it up and you should find it, v3 seems to be in 
development and should be out soon. 
Window Blinds:http://www.stardock.com/products/windowb... 
Manually-summarized Answer 
One way is to use WindowsBlinds. The package will allow you to 
add the ObjectBar for changing to the OSX theme. You can also 
make added functionality of Expose feature by downloading the 
programs like WinPlosion, Windows Exposer and Top Desk. The  
URL of it is http://www.stardock.com/products/windowblinds/. 
Another option is to use Flyakite which is a transformation pack. 
The third Option is the LiteStep, it is a "Shell Replacement" for 
windows that has a variety of Mac OSX tehmes you can install. 
The url is http://litestep.net and I have included a source of Mac OS 
theme for Litestep at http://www.customize.org/details/33409. 
Table 8. Summary of Open-Question 
502
  
Question (http://answers.yahoo.com/question/?qid= 
20060718083151AACYQJn) 
What is the best sci-fi movie u ever saw? 
Best Answer Chosen 
Independance Day 
Auto-summarized Answer 
star wars (5)  
Blade Runner (3) 
fi movie has to be Night of the Lepus (2)  
But the best "B" sci (2)  
I liked Stargate it didn't scare me and I thought they did a great job 
recreating Egypt (3)  
Independance Day (3) 
Manually-summarized Answer 
Star Wars (vote 6); The Matrix (vote 3); Independence Day (vote 
2); Blade Runner (vote 2); Starship Troopers (vote 2); Alien (vote 
2); Alien v.s Predator (vote 1); MST3K (vote 1);  
Table 12. Summary of List-Opinion Question
5.3 Experiments 
Information Content: To evaluate the effec-
tiveness of automatic summarization, we use the 
information content criterion for comparing ASA 
with BA. It focuses on whether ASA or BA con-
tains more useful information to the question. 
Information point is used in the evaluation. 
Usually, one kind of solution for open questions 
or one kind of reason for opinion questions can 
contribute one information point. By summing 
all information points in both ASA and BA, we 
then can compare which one contains more in-
formation. Intuitively, longer texts would contain 
more information. Thus, when comparing the 
information content, we limit the length of ASA 
with several levels to do the evaluation. Take 
question in Table 8 as an example, the BA just 
gives one software, which contributes one infor-
mation point while the ASA lists three kinds of 
software which contributes three information 
points. Thus, ASA is considered better than BA.  
For each question, we generate 100%, 150%, 
and 200% BA word-length ASAs. Three annota-
tors are asked to determine whether an ASA is 
better than, equal to, or worse than its corres-
ponding BA in terms of information content. 
Voting strategy is used to determine the final 
label. If three labels are all different, it is labeled 
as Unknown. We extract 163 open questions and 
121 opinion questions from all four categories by 
using final question category labels mentioned in 
Section 4. To make meaningful comparison, 
questions having unique answers or having only 
one answer are excluded. After the removal, 
there are 104 open questions and 99 opinion 
questions left for comparison. The results are 
shown in Table 13.  
We are encouraged by the evaluation results 
that our automatic summarization methods gen-
erate better coverage of contents in most of the 
cases at every answer summary length. We ob-
serve a big difference between 100% and 150% 
answer summaries. It should not be a surprise 
since a 150% answer summary contains 50% 
more content than its corresponding BA. While 
at the 100% length, we still have about 30% 
ASAs better than BA. Questions which have bet-
ter ASA than BA usually have a long BA but 
with little information. Table 14 provides the 
example. By using summarization, answers that 
are compact and direct to the question can be 
included. The results indicate that summary 
compression technique might be helpful to pack 
more information in short answers. 
Open ASA Better BA Better Equal Unknown 
100% 30% 12% 45% 13% 
150% 55% 7% 28% 10% 
200% 63% 4% 24% 9% 
Opinion ASA Better BA Better Equal Unknown
100% 37% 20% 32% 11% 
150% 44% 16% 30% 10% 
200% 54% 16% 23% 7% 
Table 13. Evaluation by Information Content 
Q Why wont japanese characters burn onto the DVD? 
BA man, the answers here are too stupid for hteir own.You are 
creating a DVD on Western Platform. I take it, you are 
using an OS that is in English?In order to "view" japanese 
as part of your filenames, you need your operating system 
to accept Japanese coding (characters).If you are using 
Windows, then you will need ot isntall the Japanese cha-
racter Set for your operating system 
If you are using MacOS . i have no idea. 
100% 
ASA
 
The dvd writer 
Probably because your burner, the DVD writer, doesn't 
support double bytes code, such as Japanese, Korean, and 
Chinese. Check the supporting language of your software. 
Or change all the file name in single byte code, like alpha-
bets. man, the answers here are too stupid for hteir own. 
You are creating a DVD on Western Platform. I take it, 
you are using an OS that is in English? 
Table 14. Examples of 100% ASA 
Readability: Besides the information content, 
we would also like to study the readability of 
automatic summarized answers. 10 questions 
(each from open and opinion category) are ex-
tracted and we make both manual summarized 
answer (MSA) and automatic summarized an-
swer (ASA) for comparison with BA. We used 
the information content (INFO) and readability 
(READ) criteria for evaluation. The readability is 
judged basically by the time for understanding. 
We make two kinds of comparison: ASA vs. BA 
and MSA vs. BA. The first one is used to judge 
whether the current summarization method is 
better than current cQA scenario. The second one 
is used as an expectation for how much the 
summarization methods can be better than BA. 
503
 For ASA vs. BA, the results in Table 15 show 
that all the annotators agree ASAs providing 
more information content but not being with sa-
tisfying readability. For MSA vs. BA, better re-
sults in readability can be achieved as Table 16. 
This suggests that the proposed approach can 
succeed as more sophisticated summarization 
techniques are developed. 
Open Annotator 1 Annotator 2 Annotator 3 
ASA INFO READ INFO READ INFO READ
Better 40% 10% 90% 10% 80% 0%
Equal 60% 60% 10% 80% 20% 60%
Worse 0% 30% 0% 10% 0% 40%
Opinion Annotator 1 Annotator 2 Annotator 3 
ASA INFO READ INFO READ INFO READ
Better 90% 10% 90% 10% 70% 40%
Equal 10% 60% 10% 60% 10% 20%
Worse 0% 30% 0% 30% 20% 40%
Table 15. ASA vs. BA Evaluation 
Open Annotator 1 Annotator 2 Annotator 3 
MSA INFO READ INFO READ INFO READ
Better 100% 30% 100% 90% 100% 90%
Equal 0% 50% 0% 0% 0% 0%
Worse 0% 20% 0% 10% 0% 10%
Opinion Annotator 1 Annotator 2 Annotator 3 
MSA INFO READ INFO READ INFO READ
Better 90% 20% 60% 70% 100% 100%
Equal 10% 80% 40% 30% 0% 0%
Worse 0% 0% 0% 0% 0% 0%
Table 16. MSA vs. BA Evaluation 
6 Conclusion and Future Work 
In this paper, we have carried out a comprehen-
sive analysis of the question types in community-
based question answering (cQA) services and 
have developed taxonomies for questions and 
answers. We find that questions do not always 
have unique best answers. Open and opinion 
questions usually have multiple good answers. 
They occupied about 56%~83% and most of 
their best answers can be improved. By using 
question type as a guide, we propose applying 
automatic summarization techniques to summa-
rization answers or improving cQA best answers 
through answer editing. Our results show that 
customized question-type focused summarization 
techniques can improve cQA answer quality sig-
nificantly.  
Looking into the future, we are to develop au-
tomatic question type identification methods to 
fully automate answer summarization. Further-
more, we would also like to utilize more sophis-
ticated summarization techniques to improve 
content compaction and readability. 
Acknowledgements 
We thank the anonymous reviewers for their val-
uable suggestions and comments to this paper. 
References 
Broder  A. A taxonomy of web search. 2002. SIGIR 
Forum Vol.36, No. 2, 3-10. 
Hovy Edward, Laurie Gerber,  Ulf Hermjakob, Chin-
Yew Lin,   Deepak Ravichandran. 2001. Toward 
Semantics-Based Answer Pinpointing. In Proc. of 
HLT?01. 
Hovy E., C. Lin. 1999. Automated Text Summariza-
tion and the SUMMARIST System. In Advances 
in Automated Text Summarization 
Jeon J., W. B. Croft, and J. Lee. 2005a. Finding se-
mantically similar questions based on their an-
swers. In Proc. of SIGIR?05. 
Jeon J., W. B. Croft, and J. Lee. 2005b. Finding simi-
lar questions in large question and answer arc-
hives. In Proc. of CIKM?05. 
Jurczyk P., E. Agichtein. 2007. Hits on question an-
swer portals: exploration of link analysis for au-
thor ranking. In Proc. of SIGIR '07. 
Jeon J. , W.B. Croft, J. Lee, S. Park. 2006. A Frame-
work to predict the quality of answers with non-
textual features. In Proc. of SIGIR ?06. 
Jijkoun V., M. R. 2005. Retrieving Answers from 
Frequently Asked Questions Pages on the Web. In 
Proc. of CIKM?05. 
Kleinberg J. 1999. Authoritative sources in a hyper-
linked environment. Journal of the ACM, vol. 46,  
Kim S., E.  Hovy. 2004. Determining the Sentiment of 
Opinions. In Proc. of COLING?04. 
Liu X., W.B. Croft, M. Koll. 2005. Finding experts in 
community-based question-answering services. 
In Proc. of CIKM '05. 
Lin C.Y., E. Hovy. 2002.  From single to multi-
document summarization: a prototype system and 
its evaluation.  In Proc. of ACL'02. 
Lytinen S., N. Tomuro. 2002. The Use of Question 
Types to Match Questions in FAQFinder. In Proc. 
of AAAI?02. 
Moldovan D., S. Harabagiu, et al 2000. The Structure 
and an Open-Domain Question Answering Sys-
tem. In Proc. of ACL?00. 
Mei Q., X. Shen, C. Zhai. 2007. Automatic labeling of 
multinomial topic models. In Proc. of   KDD'07. 
Rose  D. E., D. Levinson. 2004. Understanding user 
goals in web search. In Proc. of WWW '04. 
Voorhees, M. Ellen. 2003. Overview of the TREC 
2003 Question Answering Track. In Proc. of 
TREC?03. 
Wiebe J., E. Breck, et al 2003. Recognizing and Or-
ganizing Opinions Expressed in the World Press 
504
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 650?658,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Comparable Entity Mining from Comparative Questions 
 
 
Shasha Li1?Chin-Yew Lin2?Young-In Song2?Zhoujun Li3 
1National University of Defense Technology, Changsha, China 
2Microsoft Research Asia, Beijing, China 
3Beihang University, Beijing, China 
shashali@nudt.edu.cn1, {cyl,yosong}@microsoft.com2, 
lizj@buaa.edu.cn3 
  
 
Abstract 
Comparing one thing with another is a typical 
part of human decision making process. How-
ever, it is not always easy to know what to 
compare and what are the alternatives. To ad-
dress this difficulty, we present a novel way to 
automatically mine comparable entities from 
comparative questions that users posted on-
line. To ensure high precision and high recall, 
we develop a weakly-supervised bootstrapping 
method for comparative question identification 
and comparable entity extraction by leveraging 
a large online question archive. The experi-
mental results show our method achieves F1-
measure of 82.5% in comparative question 
identification and 83.3% in comparable entity 
extraction. Both significantly outperform an 
existing state-of-the-art method.  
1 Introduction 
Comparing alternative options is one essential 
step in decision-making that we carry out every 
day. For example, if someone is interested in cer-
tain products such as digital cameras, he or she 
would want to know what the alternatives are 
and compare different cameras before making a 
purchase. This type of comparison activity is 
very common in our daily life but requires high 
knowledge skill. Magazines such as Consumer 
Reports and PC Magazine and online media such 
as CNet.com strive in providing editorial com-
parison content and surveys to satisfy this need.  
In the World Wide Web era, a comparison ac-
tivity typically involves: search for relevant web 
pages containing information about the targeted 
products, find competing products, read reviews, 
and identify pros and cons. In this paper, we fo-
cus on finding a set of comparable entities given 
a user?s input entity. For example, given an enti-
ty, Nokia N95 (a cellphone), we want to find 
comparable entities such as Nokia N82, iPhone 
and so on.  
In general, it is difficult to decide if two enti-
ties are comparable or not since people do com-
pare apples and oranges for various reasons.  For 
example, ?Ford? and ?BMW? might be compa-
rable as ?car manufacturers? or as ?market seg-
ments that their products are targeting?, but we 
rarely see people comparing ?Ford Focus? (car 
model) and ?BMW 328i?.   Things also get more 
complicated when an entity has several functio-
nalities. For example, one might compare 
?iPhone? and ?PSP? as ?portable game player? 
while compare ?iPhone? and ?Nokia N95? as 
?mobile phone?. Fortunately, plenty of compara-
tive questions are posted online, which provide 
evidences for what people want to compare, e.g. 
?Which to buy, iPod or iPhone??. We call ?iPod? 
and ?iPhone? in this example as comparators.  In 
this paper, we define comparative questions and 
comparators as: 
 
? Comparative question: A question that in-
tends to compare two or more entities and it 
has to mention these entities explicitly in the 
question. 
? Comparator: An entity which is a target of 
comparison in a comparative question.  
 
According to these definitions, Q1 and Q2 be-
low are not comparative questions while Q3 is. 
?iPod Touch? and ?Zune HD? are comparators. 
 
Q1: ?Which one is better?? 
Q2: ?Is Lumix GH-1 the best camera?? 
Q3: ?What?s the difference between iPod 
Touch and Zune HD?? 
 
The goal of this work is mining comparators 
from comparative questions. The results would 
be very useful in helping users? exploration of 
650
alternative choices by suggesting comparable 
entities based on other users? prior requests.  
To mine comparators from comparative ques-
tions, we first have to detect whether a question 
is comparative or not. According to our defini-
tion, a comparative question has to be a question 
with intent to compare at least two entities. 
Please note that a question containing at least 
two entities is not a comparative question if it 
does not have comparison intent. However, we 
observe that a question is very likely to be a 
comparative question if it contains at least two 
entities. We leverage this insight and develop a 
weakly supervised bootstrapping method to iden-
tify comparative questions and extract compara-
tors simultaneously. 
To our best knowledge, this is the first attempt 
to specially address the problem on finding good 
comparators to support users? comparison activi-
ty. We are also the first to propose using com-
parative questions posted online that reflect what 
users truly care about as the medium from which 
we mine comparable entities. Our weakly super-
vised method achieves 82.5% F1-measure in 
comparative question identification, 83.3% in 
comparator extraction, and 76.8% in end-to-end 
comparative question identification and compa-
rator extraction which outperform the most rele-
vant state-of-the-art method by Jindal & Liu 
(2006b) significantly.   
The rest of this paper is organized as follows. 
The next section discusses previous works. Sec-
tion 3 presents our weakly-supervised method for 
comparator mining. Section 4 reports the evalua-
tions of our techniques, and we conclude the pa-
per and discuss future work in Section 5. 
 
2 Related Work 
2.1 Overview 
In terms of discovering related items for an enti-
ty, our work is similar to the research on recom-
mender systems, which recommend items to a 
user. Recommender systems mainly rely on simi-
larities between items and/or their statistical cor-
relations in user log data (Linden et al, 2003). 
For example, Amazon recommends products to 
its customers based on their own purchase histo-
ries, similar customers? purchase histories, and 
similarity between products. However, recom-
mending an item is not equivalent to finding a 
comparable item. In the case of Amazon, the 
purpose of recommendation is to entice their cus-
tomers to add more items to their shopping carts 
by suggesting similar or related items. While in 
the case of comparison, we would like to help 
users explore alternatives, i.e. helping them make 
a decision among comparable items. 
For example, it is reasonable to recommend 
?iPod speaker? or ?iPod batteries? if a user is 
interested in ?iPod?, but we would not compare 
them with ?iPod?. However, items that are com-
parable with ?iPod? such as ?iPhone? or ?PSP? 
which were found in comparative questions post-
ed by users are difficult to be predicted simply 
based on item similarity between them. Although 
they are all music players, ?iPhone? is mainly a 
mobile phone, and ?PSP? is mainly a portable 
game device. They are similar but also different 
therefore beg comparison with each other. It is 
clear that comparator mining and item recom-
mendation are related but not the same.  
Our work on comparator mining is related to 
the research on entity and relation extraction in 
information extraction (Cardie, 1997; Califf and 
Mooney, 1999; Soderland, 1999; Radev et al, 
2002; Carreras et al, 2003). Specifically, the 
most relevant work is by Jindal and Liu (2006a 
and 2006b) on mining comparative sentences and 
relations. Their methods applied class sequential 
rules (CSR) (Chapter 2, Liu 2006) and label se-
quential rules (LSR) (Chapter 2, Liu 2006) 
learned from annotated corpora to identify com-
parative sentences and extract comparative rela-
tions respectively in the news and review do-
mains. The same techniques can be applied to 
comparative question identification and compa-
rator mining from questions. However, their me-
thods typically can achieve high precision but 
suffer from low recall (Jindal and Liu, 2006b) 
(J&L). However, ensuring high recall is crucial 
in our intended application scenario where users 
can issue arbitrary queries. To address this prob-
lem, we develop a weakly-supervised bootstrap-
ping pattern learning method by effectively leve-
raging unlabeled questions.  
Bootstrapping methods have been shown to be 
very effective in previous information extraction 
research (Riloff, 1996; Riloff and Jones, 1999; 
Ravichandran and Hovy, 2002; Mooney and Bu-
nescu, 2005; Kozareva et al, 2008). Our work is 
similar to them in terms of methodology using 
bootstrapping technique to extract entities with a 
specific relation. However, our task is different 
from theirs in that it requires not only extracting 
entities (comparator extraction) but also ensuring 
that the entities are extracted from comparative 
questions (comparative question identification), 
which is generally not required in IE task. 
651
2.2 Jindal & Liu 2006 
In this subsection, we provide a brief summary 
of the comparative mining method proposed by 
Jindal and Liu (2006a and 2006b), which is used 
as baseline for comparison and represents the 
state-of-the-art in this area.  We first introduce 
the definition of CSR and LSR rule used in their 
approach, and then describe their comparative 
mining method. Readers should refer to J&L?s 
original papers for more details. 
CSR and LSR 
CSR is a classification rule. It maps a sequence 
pattern S(?1?2???) to a class C.  In our problem, 
C is either comparative or non-comparative. 
Given a collection of sequences with class in-
formation, every CSR is associated to two para-
meters: support and confidence. Support is the 
proportion of sequences in the collection contain-
ing S as a subsequence. Confidence is the propor-
tion of sequences labeled as C in the sequences 
containing the S. These parameters are important 
to evaluate whether a CSR is reliable or not. 
LSR is a labeling rule. It maps an input se-
quence pattern ?(?1?2??? ???)  to a labeled 
sequence ??(?1?2? ?? ???) by replacing one to-
ken (??) in the input sequence with a designated 
label (?? ). This token is referred as the anchor. 
The anchor in the input sequence could be ex-
tracted if its corresponding label in the labeled 
sequence is what we want (in our case, a compa-
rator). LSRs are also mined from an annotated 
corpus, therefore each LSR also have two para-
meters: support and confidence. They are simi-
larly defined as in CSR. 
Supervised Comparative Mining Method 
J&L treated comparative sentence identification 
as a classification problem and comparative rela-
tion extraction as an information extraction prob-
lem. They first manually created a set of 83 key-
words such as beat, exceed, and outperform that 
are likely indicators of comparative sentences. 
These keywords were then used as pivots to 
create part-of-speech (POS) sequence data. A 
manually annotated corpus with class informa-
tion, i.e. comparative or non-comparative, was 
used to create sequences and CSRs were mined. 
A Na?ve Bayes classifier was trained using the 
CSRs as features. The classifier was then used to 
identify comparative sentences. 
Given a set of comparative sentences, J&L 
manually annotated two comparators with labels 
$ES1 and $ES2 and the feature compared with 
label $FT for each sentence. J&L?s method was 
only applied to noun and pronoun. To differen-
tiate noun and pronoun that are not comparators 
or features, they added the fourth label $NEF, i.e. 
non-entity-feature. These labels were used as 
pivots together with special tokens li & rj
1 (token 
position), #start (beginning of a sentence), and 
#end (end of a sentence) to generate sequence 
data, sequences with single label only and mini-
mum support greater than 1% are retained, and 
then LSRs were created. When applying the 
learned LSRs for extraction, LSRs with higher 
confidence were applied first. 
J&L?s method have been proved effective in 
their experimental setups. However, it has the 
following weaknesses:  
 
? The performance of J&L?s method relies 
heavily on a set of comparative sentence in-
dicative keywords. These keywords were 
manually created and they offered no guide-
lines to select keywords for inclusion. It is 
also difficult to ensure the completeness of 
the keyword list.  
? Users can express comparative sentences or 
questions in many different ways. To have 
high recall, a large annotated training corpus 
is necessary. This is an expensive process.  
? Example CSRs and LSRs given in Jindal & 
Liu (2006b) are mostly a combination of 
POS tags and keywords. It is a surprise that 
their rules achieved high precision but low 
recall. They attributed most errors to POS 
tagging errors. However, we suspect that 
their rules might be too specific and overfit 
their small training set (about 2,600 sen-
tences). We would like to increase recall, 
avoid overfitting, and allow rules to include 
discriminative lexical tokens to retain preci-
sion. 
 
In the next section, we introduce our method to 
address these shortcomings. 
3 Weakly Supervised Method for Com-
parator Mining 
Our weakly supervised method is a pattern-based 
approach similar to J&L?s method, but it is dif-
ferent in many aspects: Instead of using separate 
CSRs and LSRs, our method aims to learn se-
                                                 
1 li marks a token is at the i
th 
position to the left of the pivot 
and rj marks a token is at j
th position to the right of the 
pivot where i and j are between 1 and 4 in J&L (2006b). 
652
quential patterns which can be used to identify 
comparative question and extract comparators 
simultaneously.  
In our approach, a sequential pattern is defined 
as a sequence S(s1s2? si ? sn) where si can be a 
word, a POS tag, or a symbol denoting either a 
comparator ($C), or the beginning (#start) or the 
end of a question (#end). A sequential pattern is 
called an indicative extraction pattern (IEP) if it 
can be used to identify comparative questions 
and extract comparators in them with high relia-
bility. We will formally define the reliability 
score of a pattern in the next section.  
Once a question matches an IEP, it is classified 
as a comparative question and the token se-
quences corresponding to the comparator slots in 
the IEP are extracted as comparators.  When a 
question can match multiple IEPs, the longest 
IEP is used 2 . Therefore, instead of manually 
creating a list of indicative keywords, we create a 
set of IEPs. We will show how to acquire IEPs 
automatically using a bootstrapping procedure 
with minimum supervision by taking advantage 
of a large unlabeled question collection in the 
following subsections. The evaluations shown in 
section 4 confirm that our weakly supervised 
method can achieve high recall while retain high 
precision. 
This pattern definition is inspired by the work 
of Ravichandran and Hovy (2002). Table 1 
shows some examples of such sequential pat-
terns. We also allow POS constraint on compara-
tors as shown in the pattern ?<, $C/NN or $C/NN 
? #end>?. It means that a valid comparator must 
have a NN POS tag. 
3.1 Mining Indicative Extraction Patterns 
Our weakly supervised IEP mining approach is 
based on two key assumptions:  
 
                                                 
2 It is because the longest IEP is likely to be the most specif-
ic and relevant pattern for the given question. 
 
Figure 1: Overview of the bootstrapping alogorithm  
 
? If a sequential pattern can be used to extract 
many reliable comparator pairs, it is very likely 
to be an IEP.  
? If a comparator pair can be extracted by an 
IEP, the pair is reliable. 
 
Based on these two assumptions, we design 
our bootstrapping algorithm as shown in Figure 1. 
The bootstrapping process starts with a single 
IEP. From it, we extract a set of initial seed com-
parator pairs. For each comparator pair, all ques-
tions containing the pair are retrieved from a 
question collection and regarded as comparative 
questions. From the comparative questions and 
comparator pairs, all possible sequential patterns 
are generated and evaluated by measuring their 
reliability score defined later in the Pattern Eval-
uation section. Patterns evaluated as reliable ones 
are IEPs and are added into an IEP repository.  
Then, new comparator pairs are extracted from 
the question collection using the latest IEPs. The 
new comparators are added to a reliable compa-
rator repository and used as new seeds for pattern 
learning in the next iteration. All questions from 
which reliable comparators are extracted are re-
moved from the collection to allow finding new 
patterns efficiently in later iterations. The 
process iterates until no more new patterns can 
be found from the question collection.  
There are two key steps in our method: (1) 
pattern generation and (2) pattern evaluation. In 
the following subsections, we will explain them 
in details.   
Pattern Generation 
To generate sequential patterns, we adapt the 
surface text pattern mining method introduced in 
(Ravichandran and Hovy, 2002). For any given 
comparative question and its comparator pairs, 
comparators in the question are replaced with 
symbol $Cs. Two symbols, #start and #end, are 
attached to the beginning and the end of a sen-
Sequential Patterns 
<#start which city is better, $C or $C ? #end> 
<, $C or $C ? #end> 
<#start $C/NN or $C/NN ? #end> 
<which NN is better, $C or $C ?> 
<which city is JJR, $C or $C ?>  
<which NN is JJR, $C or $C ?> 
... 
Table 1: Candidate indicative extraction pattern (IEP) 
examples of the question ?which city is better, NYC or 
Paris?? 
 
653
tence in the question. Then, the following three 
kinds of sequential patterns are generated from 
sequences of questions: 
 
 
? Lexical patterns: Lexical patterns indicate 
sequential patterns consisting of only words 
and symbols ($C, #start, and #end). They are 
generated by suffix tree algorithm (Gusfield, 
1997) with two constraints: A pattern should 
contain more than one $C, and its frequency 
in collection should be more than an empiri-
cally determined number ?. 
? Generalized patterns: A lexical pattern can 
be too specific. Thus, we generalize lexical 
patterns by replacing one or more words with 
their POS tags. 2? ? 1 generalized patterns 
can be produced from a lexical pattern con-
taining N words excluding $Cs.  
? Specialized patterns: In some cases, a pat-
tern can be too general. For example, al-
though a question ?ipod or zune?? is com-
parative, the pattern ?<$C or $C>? is too 
general, and there can be many non-
comparative questions matching the pattern, 
for instance, ?true or false??. For this reason, 
we perform pattern specialization by adding 
POS tags to all comparator slots. For exam-
ple, from the lexical pattern ?<$C or $C>? 
and the question ?ipod or zune??, ?<$C/NN 
or $C/NN?>? will be produced as a specia-
lized pattern.  
 
Note that generalized patterns are generated from 
lexical patterns and the specialized patterns are 
generated from the combined set of generalized 
patterns and lexical patterns. The final set of 
candidate patterns is a mixture of lexical patterns, 
generalized patterns and specialized patterns. 
Pattern Evaluation  
According to our first assumption, a reliability 
score ??(??) for a candidate pattern ??  at itera-
tion k can be defined as follows: 
 
?? ?? =
 ?? (????? ? )??? ????
??1
?? (????)
        (1) 
 
, where ??  can extract known reliable comparator 
pairs ??? . ??
??1 indicates the reliable compara-
tor pair repository accumulated until the 
(? ? 1)?? iteration. ??(?) means the number of 
questions satisfying a condition x. The condition 
?? ? ???  denotes that ???  can be extracted from 
a question by applying pattern ??  while the con-
dition ?? ??  denotes any question containing 
pattern ?? .  
However, Equation (1) can suffer from in-
complete knowledge about reliable comparator 
pairs. For example, very few reliable pairs are 
generally discovered in early stage of bootstrap-
ping. In this case, the value of Equation (1) 
might be underestimated which could affect the 
effectiveness of equation (1) on distinguishing 
IEPs from non-reliable patterns. We mitigate this 
problem by a lookahead procedure. Let us denote 
the set of candidate patterns at the iteration k by 
? ? . We define the support ? for comparator pair  
?? ?  which can be extracted by ? 
?   and does not 
exist in the current reliable set:  
 
? ?? ? = ??( ? 
?
? ?? ?)     (2) 
 
where ? ? ? ?? ?  means that one of the patterns in 
? ?  can extract ???  in certain questions. Intuitive-
ly, if  ?? ?  can be extracted by many candidate 
patterns in ? ? , it is likely to be extracted as a 
reliable one in the next iteration. Based on this 
intuition, a pair ???  whose support S is more than 
a threshold ? is regarded as a likely-reliable pair. 
Using likely-reliable pairs, lookahead reliability 
score ?  ??  is defined: 
 
? ? ?? =
 ?? (????? i )??? ???? ???
?
?? (????)
      (3) 
 
, where ?? ???
?  indicates a set of likely-reliable 
pairs based on ? ? .  
By interpolating Equation (1) and (3), the final 
reliability score ?(??)?????
?  for a pattern is de-
fined as follows: 
 
?(??)?????
? = ? ? ?? ?? + (1? ?) ? ? 
?(??)     (4) 
 
Using Equation (4), we evaluate all candidate 
patterns and select patterns whose score is more 
than threshold ? as IEPs. All necessary parame-
ter values are empirically determined. We will 
explain how to determine our parameters in sec-
tion 4. 
4 Experiments 
4.1 Experiment Setup 
Source Data 
All experiments were conducted on about 60M 
questions mined from Yahoo! Answers? question 
title field. The reason that we used only a title 
654
field is that they clearly express a main intention 
of an asker with a form of simple questions in 
general.  
Evaluation Data 
Two separate data sets were created for evalua-
tion. First, we collected 5,200 questions by sam-
pling 200 questions from each Yahoo! Answers 
category3. Two annotators were asked to label 
each question manually as comparative, non-
comparative, or unknown. Among them, 139 
(2.67%) questions were classified as comparative,  
4,934 (94.88%) as non-comparative, and 127 
(2.44%) as unknown questions which are diffi-
cult to assess. We call this set SET-A. 
Because there are only 139 comparative ques-
tions in SET-A, we created another set which 
contains more comparative questions. We ma-
nually constructed a keyword set consisting of 53 
words such as ?or? and ?prefer?, which are good 
indicators of comparative questions. In SET-A, 
97.4% of comparative questions contains one or 
more keywords from the keyword set. We then 
randomly selected another 100 questions from 
each Yahoo! Answers category with one extra 
condition that all questions have to contain at 
least one keyword. These questions were labeled 
in the same way as SET-A except that their com-
parators were also annotated. This second set of 
questions is referred as SET-B. It contains 853 
comparative questions and 1,747 non-
comparative questions. For comparative question 
identification experiments, we used all labeled 
questions in SET-A and SET-B. For comparator 
extraction experiments, we used only SET-B. All 
the remaining unlabeled questions (called as 
SET-R) were used for training our weakly super-
vised method. 
As a baseline method, we carefully imple-
mented J&L?s method. Specifically, CSRs for 
comparative question identification were learned 
from the labeled questions, and then a statistical 
classifier was built by using CSR rules as fea-
tures. We examined both SVM and Na?ve Bayes 
(NB) models as reported in their experiments.  
For the comparator extraction, LSRs were 
learned from SET-B and applied for comparator 
extraction.  
To start the bootstrapping procedure, we ap-
plied the IEP ?<#start nn/$c vs/cc nn/$c ?/. 
#end>? to all the questions in SET-R and ga-
thered 12,194 comparator pairs as the initial 
seeds.  For our weakly supervised method, there 
                                                 
3 There are 26 top level categories in Yahoo! Answers. 
are four parameters, i.e. ?, ?, ?, and ?, need to be 
determined empirically. We first mined all poss-
ible candidate patterns from the suffix tree using 
the initial seeds. From these candidate patterns, 
we applied them to SET-R and got a new set of 
59,410 candidate comparator pairs. Among these 
new candidate comparator pairs, we randomly 
selected 100 comparator pairs and manually clas-
sified them into reliable or non-reliable compara-
tors. Then we found ? that maximized precision 
without hurting recall by investigating frequen-
cies of pairs in the labeled set. By this method, ? 
was set to 3 in our experiments. Similarly, the 
threshold parameters ? and ? for pattern evalua-
tion were set to 10 and 0.8 respectively. For the 
interpolation parameter ?  in Equation (3), we 
simply set the value to 0.5 by assuming that two 
reliability scores are equally important.  
As evaluation measures for comparative ques-
tion identification and comparator extraction, we 
used precision, recall, and F1-measure. All re-
sults were obtained from 5-fold cross validation. 
Note that J&L?s method needs a training data but 
ours use the unlabeled data (SET-R) with weakly 
supervised method to find parameter setting. 
This 5-fold evaluation data is not in the unla-
beled data. Both methods were tested on the 
same test split in the 5-fold cross validation. All 
evaluation scores are averaged across all 5 folds. 
For question processing, we used our own sta-
tistical POS tagger developed in-house4.  
4.2 Experiment Results 
Comparative Question Identification and 
Comparator Extraction 
Table 2 shows our experimental results. In the 
table, ?Identification only? indicates the perfor-
mances in comparative question identification, 
?Extraction only? denotes the performances of 
comparator extraction when only comparative 
questions are used as input, and ?All? indicates 
the end-to-end performances when question 
identification results were used in comparator 
extraction. Note that the results of J&L?s method 
on our collections are very comparable to what is 
reported in their paper.  
In terms of precision, the J&L?s method is 
competitive to our method in comparative ques-
                                                 
4  We used NLC-PosTagger which is developed by NLC 
group of Microsoft Research Asia. It uses the modified 
Penn Treebank POS set for its output; for example, NNS 
(plural nouns), NN (nouns), NP (noun phrases), NPS (plural 
noun phrases), VBZ (verb, present tense, 3rd person singu-
lar), JJ (adjective), RB(adverb), and so on. 
655
tion identification. However, the recall is signifi-
cantly lower than ours. In terms of recall, our 
method outperforms J&L?s method by 35% and 
22% in comparative question identification and 
comparator extraction respectively. In our analy-
sis, the low recall of J&L?s method is mainly 
caused by low coverage of learned CSR patterns 
over the test set.  
In the end-to-end experiments, our weakly su-
pervised method performs significantly better 
than J&L?s method. Our method is about 55% 
better in F1-measure. This result also highlights 
another advantage of our method that identifies 
comparative questions and extracts comparators 
simultaneously using one single pattern. J&L?s 
method uses two kinds of pattern rules, i.e. CSRs 
and LSRs. Its performance drops significantly 
due to error propagations. F1-measure of J&L?s 
method in ?All? is about 30% and 32% worse 
than the scores of ?Identification only? and ?Ex-
traction? only respectively, our method only 
shows small amount of performance decrease 
(approximately 7-8%).  
We also analyzed the effect of pattern genera-
lization and specialization. Table 3 shows the 
results. Despite of the simplicity of our methods, 
they significantly contribute to performance im-
provements. This result shows the importance of 
learning patterns flexibly to capture various 
comparative question expressions. Among the 
6,127 learned IEPs in our database, 5,930 pat-
terns are generalized ones, 171 are specialized 
ones, and only 26 patterns are non-generalized 
and specialized ones.  
To investigate the robustness of our bootstrap-
ping algorithm for different seed configurations, 
we compare the performances between two dif-
ferent seed IEPs. The results are shown in Table 
4. As shown in the table, the performance of our 
bootstrapping algorithm is stable regardless of 
significantly different number of seed pairs gen-
erated by the two IEPs. This result implies that 
our bootstrapping algorithm is not sensitive to 
the choice of IEP.  
Table 5 also shows the robustness of our boot-
strapping algorithm. In Table 5, ?All? indicates 
the performances that all comparator pairs from a 
single seed IEP is used for the bootstrapping, and 
?Partial? indicate the performances using only 
1,000 randomly sampled pairs from ?All?. As 
shown in the table, there is no significant per-
formance difference.  
In addition, we conducted error analysis for 
the cases where our method fails to extract cor-
rect comparator pairs: 
 
? 23.75% of errors on comparator extraction 
are due to wrong pattern selection by our 
simple maximum IEP length strategy.  
? The remaining 67.63% of errors come from 
comparative questions which cannot be cov-
ered by the learned IEPs. 
 
 
 Recall Precision F-score 
Original Patterns 0.689  0. 449 0.544 
+ Specialized 0.731  0.602 0.665 
+ Generalized 0.760  0.776 0.768 
Table 3: Effect of pattern specialization and Generali-
zation in the end-to-end experiments.  
 
Seed patterns # of resulted 
seed pairs 
F-score 
<#start nn/$c vs/cc nn/$c 
?/. #end>  
12,194 0.768 
<#start which/wdt is/vb 
better/jjr , nn/$c or/cc 
nn/$c ?/. #end> 
1,478 0.760 
Table 4: Performance variation over different initial 
seed IEPs in the end-to-end experiments 
 
Set  (# of seed pairs) Recall Precision F-score 
All (12,194) 0.760 0.774 0.768 
Partial (1,000) 0.724 0.763 0.743 
Table 5: Performance variation over different sizes of 
seed pairs generated from a single initial seed IEP 
?<#start nn/$c vs/cc nn/$c ?/. #end>?. 
 
 
Identification only 
(SET-A+SET-B) 
Extraction only 
(SET-B) 
All 
(SET-B) 
J&L (CSR) Our  
Method 
J&L 
(LSR) 
Our  
Method 
J&L Our  
Method SVM NB SVM NB 
Recall 0.601 0.537 0.817* 0.621 0.760* 0.373 0.363 0.760* 
Precision 0.847 0.851 0.833 0.861 0.916* 0.729 0.703 0.776* 
F-score 0.704 0.659 0.825* 0.722 0.833* 0.493 0.479 0.768* 
Table 2: Performance comparison between our method and Jindal and Bing?s Method (denoted as J&L). 
The values with * indicate statistically significant improvements over J&L (CSR) SVM or J&L (LSR) 
according to t-test  at p < 0.01 level. 
 
656
Examples of Comparator Extraction  
By applying our bootstrapping method to the 
entire source data (60M questions), 328,364 
unique comparator pairs were extracted from 
679,909 automatically identified comparative 
questions.  
Table 6 lists top 10 frequently compared enti-
ties for a target item, such as Chanel, Gap, in our 
question archive. As shown in the table, our 
comparator mining method successfully discov-
ers realistic comparators. For example, for ?Cha-
nel?, most results are high-end fashion brands 
such as ?Dior? or ?Louis Vuitton?, while the rank-
ing results for ?Gap? usually contains similar ap-
parel brands for young people, such as ?Old Navy? 
or ?Banana Republic?. For the basketball player 
?Kobe?, most of the top ranked comparators are 
also famous basketball players. Some interesting 
comparators are shown for ?Canon? (the compa-
ny name). It is famous for different kinds of its 
products, for example, digital cameras and prin-
ters, so it can be compared to different kinds of 
companies. For example, it is compared to ?HP?, 
?Lexmark?, or ?Xerox?, the printer manufacturers, 
and also compared to ?Nikon?, ?Sony?, or ?Kodak?, 
the digital camera manufactures.  Besides gener-
al entities such as a brand or company name, our 
method also found an interesting comparable 
entity for a specific item in the experiments. For 
example, our method recommends ?Nikon d40i?, 
?Canon rebel xti?, ?Canon rebel xt?, ?Nikon 
d3000?, ?Pentax k100d?, ?Canon eos 1000d? as 
comparators for the specific camera product ?Ni-
kon 40d?. 
Table 7 can show the difference between our 
comparator mining and query/item recommenda-
tion. As shown in the table, ?Google related 
searches? generally suggests a mixed set of two 
kinds of related queries for a target entity: (1) 
queries specified with subtopics for an original 
query (e.g., ?Chanel handbag? for ?Chanel?) and 
(2) its comparable entities (e.g., ?Dior? for ?Cha-
nel?). It confirms one of our claims that compara-
tor mining and query/item recommendation are 
related but not the same. 
5 Conclusion 
In this paper, we present a novel weakly super-
vised method to identify comparative questions 
and extract comparator pairs simultaneously. We 
rely on the key insight that a good comparative 
question identification pattern should extract 
good comparators, and a good comparator pair 
should occur in good comparative questions to 
bootstrap the extraction and identification 
process. By leveraging large amount of unla-
beled data and the bootstrapping process with 
slight supervision to determine four parameters, 
we found 328,364 unique comparator pairs and 
6,869 extraction patterns without the need of 
creating a set of comparative question indicator 
keywords.  
The experimental results show that our me-
thod is effective in both comparative question 
identification and comparator extraction. It sig-
 Chanel Gap iPod Kobe Canon 
1 Dior Old Navy Zune Lebron Nikon 
2 Louis Vuitton American Eagle mp3 player Jordan Sony 
3 Coach Banana Republic PSP MJ Kodak 
4 Gucci Guess by Marciano cell phone Shaq Panasonic 
5 Prada ACP Ammunition iPhone Wade Casio 
6 Lancome Old Navy brand Creative Zen T-mac Olympus 
7 Versace Hollister Zen Lebron James Hp 
8 LV Aeropostal iPod nano Nash Lexmark 
9 Mac American Eagle outfitters iPod touch KG Pentax 
10 Dooney Guess iRiver Bonds Xerox 
Table 6: Examples of comparators for different entities  
Chanel Gap iPod Kobe Canon 
Chanel handbag Gap coupons iPod nano Kobe Bryant stats Canon t2i 
Chanel sunglass Gap outlet iPod touch Lakers Kobe Canon printers 
Chanel earrings Gap card iPod best buy Kobe espn Canon printer drivers 
Chanel watches Gap careers iTunes Kobe Dallas Mavericks Canon downloads 
Chanel shoes Gap casting call Apple Kobe NBA Canon copiers 
Chanel jewelry Gap adventures iPod shuffle Kobe 2009 Canon scanner 
Chanel clothing Old navy iPod support Kobe san Antonio Canon lenses 
Dior Banana republic iPod classic Kobe Bryant 24 Nikon 
Table 7: Related queries returned by Google related searches for the same target entities in Table 6. The bold 
ones indicate overlapped queries to the comparators in Table 6. 
 
657
nificantly improves recall in both tasks while 
maintains high precision. Our examples show 
that these comparator pairs reflect what users are 
really interested in comparing. 
Our comparator mining results can be used for 
a commerce search or product recommendation 
system. For example, automatic suggestion of 
comparable entities can assist users in their com-
parison activities before making their purchase 
decisions. Also, our results can provide useful 
information to companies which want to identify 
their competitors.  
In the future, we would like to improve extrac-
tion pattern application and mine rare extraction 
patterns. How to identify comparator aliases such 
as ?LV? and ?Louis Vuitton? and how to separate 
ambiguous entities such ?Paris vs. London? as 
location and ?Paris vs. Nicole? as celebrity are 
all interesting research topics. We also plan to 
develop methods to summarize answers pooled 
by a given comparator pair.  
6 Acknowledgement  
This work was done when the first author 
worked as an intern at Microsoft Research Asia. 
References  
Mary Elaine Califf and Raymond J. Mooney. 1999. 
Relational learning of pattern-match rules for in-
formation extraction. In Proceedings of AAAI?99 
/IAAI?99. 
Claire Cardie. 1997. Empirical methods in informa-
tion extraction. AI magazine, 18:65?79.  
Dan Gusfield. 1997. Algorithms on strings, trees, and 
sequences: computer science and computational 
biology. Cambridge University Press, New York, 
NY, USA 
Taher H. Haveliwala. 2002. Topic-sensitive pagerank. 
In Proceedings of WWW ?02, pages 517?526. 
Glen Jeh and Jennifer Widom. 2003. Scaling persona-
lized web search. In Proceedings of WWW ?03, 
pages 271?279. 
Nitin Jindal and Bing Liu. 2006a. Identifying compar-
ative sentences in text documents. In Proceedings 
of SIGIR ?06, pages 244?251. 
Nitin Jindal and Bing Liu. 2006b. Mining compara-
tive sentences and relations. In Proceedings of 
AAAI ?06. 
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 
2008. Semantic class learning from the web with 
hyponym pattern linkage graphs. In Proceedings of 
ACL-08: HLT, pages 1048?1056.  
Greg Linden, Brent Smith and Jeremy York. 2003. 
Amazon.com Recommendations: Item-to-Item 
Collaborative Filtering. IEEE Internet Computing, 
pages 76-80.  
Raymond J. Mooney and Razvan Bunescu. 2005. 
Mining knowledge from text using information ex-
traction. ACM SIGKDD Exploration Newsletter, 
7(1):3?10. 
Dragomir Radev, Weiguo Fan, Hong Qi, and Harris 
Wu and Amardeep Grewal. 2002. Probabilistic 
question answering on the web. Journal of the 
American Society for Information Science and 
Technology, pages 408?419. 
Deepak Ravichandran and Eduard Hovy. 2002. 
Learning surface text patterns for a question ans-
wering system. In Proceedings of ACL ?02, pages 
41?47. 
Ellen Riloff and Rosie Jones. 1999. Learning dictio-
naries for information extraction by multi-level 
bootstrapping. In Proceedings of AAAI ?99 
/IAAI ?99, pages 474?479. 
Ellen Riloff. 1996. Automatically generating extrac-
tion patterns from untagged text. In Proceedings of 
the 13th National Conference on Artificial Intelli-
gence, pages 1044?1049. 
Stephen Soderland. 1999. Learning information ex-
traction rules for semi-structured and free text. Ma-
chine Learning, 34(1-3):233?272.  
 
658
