Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 19?27,
ACL-08: HLT, Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Prior Derivation Models For Formally Syntax-based Translation Using
Linguistically Syntactic Parsing and Tree Kernels
Bowen Zhou
IBM T. J. Watson Research Center
Yorktown Heights, NY
zhou@us.ibm.com
Bing Xiang
IBM T. J. Watson Research Center
Yorktown Heights, NY
bxiang@us.ibm.com
Xiaodan Zhu
Dept. of Computer Science
University of Toronto
xzhu@cs.toronto.edu
Yuqing Gao
IBM T. J. Watson Research Center
Yorktown Heights, NY
yuqing@us.ibm.com
Abstract
This paper presents an improved formally
syntax-based SMT model, which is enriched
by linguistically syntactic knowledge obtained
from statistical constituent parsers. We pro-
pose a linguistically-motivated prior deriva-
tion model to score hypothesis derivations on
top of the baseline model during the trans-
lation decoding. Moreover, we devise a
fast training algorithm to achieve such im-
proved models based on tree kernel meth-
ods. Experiments on an English-to-Chinese
task demonstrate that our proposed models
outperformed the baseline formally syntax-
based models, while both of them achieved
significant improvements over a state-of-the-
art phrase-based SMT system.
1 Introduction
In recent years, syntax-based translation models
(Chiang, 2007; Galley et al, 2004; Liu et al, 2006)
have shown promising progress in improving trans-
lation quality. There are two major elements ac-
counting for such an improvement: namely the in-
corporation of phrasal translation structures adopted
from widely applied phrase-based models (Och
and Ney, 2004) to handle local fluency, and the
engagement of synchronous context-free grammars
(SCFG), which enhances the generative capacity of
the underlying model that is limited by finite-state
machinery.
Approaches to syntax-based translation models
using SCFG can be further categorized into two
classes, based on their dependency on annotated cor-
pus. Following Chiang (Chiang, 2007), we note the
following distinction between these two classes:
? Linguistically syntax-based: models that utilize
structures defined over linguistic theory and
annotations (e.g., Penn Treebank), and SCFG
rules are derived from parallel corpus that is
guided by explicitly parsing on at least one side
of the parallel corpus. Examples among others
are (Yamada and Knight, 2001) and (Galley et
al., 2004).
? Formally syntax-based: models are based on
hierarchical structures of natural language but
synchronous grammars are automatically ex-
tracted from parallel corpus without any usage
of linguistic knowledge or annotations. Exam-
ples include Wu?s (Wu, 1997) ITG and Chi-
ang?s hierarchical models (Chiang, 2007).
While these two often resemble in appearance,
from practical viewpoints, there are some distinc-
tions in training and decoding procedures differen-
tiating formally syntax-based models from linguis-
tically syntax-based models. First, the former has
no dependency on available linguistic theory and
annotations for targeting language pairs, and thus
the training and rule extraction are more efficient.
Secondly, the decoding complexity of the former is
lower 1, especially when integrating a n-gram based
1The complexity is dominated by synchronous parsing and
boundary words keeping. Thus binary SCFG employed in for-
mally syntax-based systems help to maintain efficient CKY de-
coding. Recent work by (Zhang et al, 2006) shows a practi-
cally efficient approach that binarizes linguistically SCFG rules
when possible.
19
language model, which is a key element to ensure
translation output quality.
On the other hand, available linguistic theory
and annotations could provide invaluable benefits in
grammar induction and scoring, as shown by recent
progress on such models (Galley et al, 2006). In
contrast, formally syntax-based grammars often lack
explicit linguistic constraints.
In this paper, we propose a scheme to enrich for-
mally syntax-based models with linguistically syn-
tactic knowledge. In other words, we maintain our
grammar to be based on formal syntax on surface,
but incorporate linguistic knowledge into our mod-
els to leverage syntax theory and annotations.
Our goal is two-fold. First, how to score SCFG
rules whose general abstraction forms are unseen in
the training data is an important question to answer.
In hierarchical models, Chiang (Chiang, 2007) uti-
lizes heuristics where certain assumptions are made
on rule distributions to obtain relative frequency
counts. We intend to explore if additional linguisti-
cally parsing information would be beneficial to im-
prove the scoring of formally syntactic SCFG gram-
mars. Secondly, we note that SCFG-based models
often come with an excessive memory consumption
as its rule size is an order of magnitude larger com-
pared to phrase-based models, which challenges its
practical deployment for online real-time translation
tasks. Furthermore, formal syntax rules are often re-
dundant as they are automatically extracted without
linguistic supervision. Therefore, we are motivated
to study approaches to further score and rank formal
syntax rules based on syntax-inspired methods, and
eventually to prune unnecessary rules without loss
of performance in general.
In our study, we propose a linguistically-
motivated method to train prior derivation models
for formally syntax-based translation. In this frame-
work, prior derivation models can be viewed as a
smoothing of rule translation models, addressing
the weakness of the baseline model estimation that
relies on relative counts obtained from heuristics.
First, we apply automatic parsers to obtain syntax
annotations on the English side of the parallel cor-
pus. Next, we extract tree fragments associated with
phrase pairs, and measure similarity between such
tree fragments using kernel methods (Collins and
Duffy, 2002; Moschitti, 2006). Finally, we score
and rank rules based on their minimal cluster sim-
ilarity of their nonterminals, which is used to com-
pute the prior distribution of hypothesis derivations
during decoding for improved translation.
The remainder of the paper is organized as fol-
lows. We start with a brief review of some related
work in Sec. 2. In Sec. 3, we describe our formally
syntax-based models and decoder implementation,
that is established as our baseline system. Sec. 4
presents the approach to score formal SCFG rules
using kernel methods. Experimental results are pro-
vided in Sec. 5. Finally, Sec. 6 summarized our con-
tributions with discussions and future work.
2 Related Work
Syntax-based translation models engaged with
SCFG have been actively investigated in the liter-
ature (Wu, 1997; Yamada and Knight, 2001; Gildea,
2003; Galley et al, 2004; Satta and Peserico, 2005).
Recent work by (Chiang, 2007; Galley et al,
2006) shows promising improvements compared to
phrase-based models for large-scale tasks. How-
ever, few previous work directly applied linguisti-
cally syntactic information into a formally syntax-
based models, which is explored in this paper.
Kernel methods leverage the fact that the only op-
eration in a procedure is the evaluation of inner dot
products between pairs of observations, where the
inner product is thus replaced with a Mercer kernel
that provides an efficient way to carry out computa-
tion when original feature dimension is large or even
infinite. Collins and Duffy (Collins and Duffy, 2002)
suggested to employ convolution kernels to measure
similarity between two trees in terms of their sub-
structures, and more recently, Moschitti (Moschitti,
2006) described in details a fast implementation of
tree kernels. To our knowledge, this paper is one of
the few efforts of applying kernel methods for im-
proved translation.
3 Formally Syntax-based Models
An SCFG is a synchronous rewriting system gen-
erating source and target side string pairs simulta-
neously based on context-free grammar. Each syn-
chronous production (i.e., rule) rewrites a nonter-
minal into a pair of strings, ? and ?, with both
terminals and nonterminals in both languages, sub-
20
ject to the constraint that there is a one-to-one cor-
respondence between nonterminal occurrences on
the source and target side. In particular, formally
syntax-based models explore hierarchical structures
of natural language and utilize only a unified nonter-
minal symbol X in the grammar,
X ? ??, ?,??, (1)
where ? is the one-to-one correspondence between
X?s in ? and ?, which is indicated by under-
scripted co-indices on both sides. For example,
some English-to-Chinese production rules can be
represented as follows:
X ? ?X1enjoy readingX2, (2)
X1xihuan(enjoy) yuedu(reading)X2?
X ? ?X1enjoy readingX2,
X1xihuan(enjoy)X2yuedu(reading)?
The set of rules, denoted as R, are automatically ex-
tracted from sentence-aligned parallel corpus (Chi-
ang, 2007). First, bidirectional word-level align-
ment is carried out on the parallel corpus running
GIZA++ (Och and Ney, 2000). Based on the result-
ing Viterbi alignments Ae2f and Af2e, the union,
AU = Ae2f ? Af2e, is taken as the symmetrizedword-level alignment. Next, bilingual phrase pairs
consistent with word alignments are extracted from
AU (Och and Ney, 2004). Specifically, any pairof consecutive sequences of words below a maxi-
mum length M is considered to be a phrase pair
if its component words are aligned only within the
phrase pair and not to any words outside. The re-
sulting bilingual phrase pair inventory is denoted as
BP . Each phrase pair PP ? BP is represented as
a production rule X ? ?f ji , elk?, which we refer toas phrasal rules. The SCFG rule set encloses all
phrase pairs, i.e., BP ? R. Next, we loop through
each phrase pair PP and generalize the sub-phrase
pair contained in PP, denoted as SPe and SPf sub-ject to SP = (SPf , SPe) ? BP , with co-indexednonterminal symbols. We thereby obtain a new rule.
We limit the number of nonterminals in each rule
no more than two, thus ensuring the rank of SCFG is
two. To reduce rule size and spurious ambiguity, we
apply constraints described in (Chiang, 2007). In
addition, we require that the sub-phrases being ab-
stracted by correspondent nonterminals have to be
aligned together in the original phrase pair, which
significantly reduces the number of rules. We will
hereafter refer to rules with nonterminal symbols as
abstract rules to distinguish them from phrasal rules.
Finally, an implicit glue rule is embedded with de-
coder to allow for translations that can be achieved
by sequentially linking sub-translations generated
chunk-by-chunk:
X ? ?X1X2, X1X2?. (3)
That is, X is also our sentence start symbol.
During such a rule extraction procedure, we note
that there is a many-to-many mapping between
phrase pairs (contiguous word sequences without
nonterminals) and derived rules (a mixed combina-
tion of word and nonterminal sequences). In other
words, one original phrase pair can induce a num-
ber of different rules, and the same rule can also be
derived from a number of different phrase pairs.
3.1 Models
All rules in R are paired with statistical parame-
ters (i.e., weighted SCFG), which combines with
other features to form our models using a log-linear
framework. Translation using SCFG for an input
sentence f is casted as to find the optimal derivation
on source and target side (as the grammar is syn-
chronous, the derivations on source and target sides
are identical). By ?optimal?, it indicates that the
derivation D maximizes following log-linear mod-
els over all possible derivations:
P (D) ? PLM (e)?LM?
?
i
?
X?<?,?>?D ?i(X ?< ?, ? >)?i , (4)
where the set of ?i(X ?< ?, ? >) are featuresdefined over given production rule, and PLM (e) isthe language model score on hypothesized output,
the ?i is the feature weight.Our baseline model follows Chiang?s hierarchical
model (Chiang, 2007) in conjunction with additional
features:
? conditional probabilities in both directions:
P (?|?) and P (?|?);
? lexical weights (Koehn et al, 2003) in both di-
rections: Pw(?|?) and Pw(?|?);
21
? word counts |e|;
? rule counts |D|;
? target n-gram language model PLM (e);
? glue rule penalty to learn preference of non-
terminal rewriting over serial combination
through Eq. 3;
Moreover, we propose an additional feature, namely
the abstraction penalty, to account for the accumu-
lated number of nonterminals applied in D:
? abstraction penalty exp(?Na), where Na =
?
X?<?,?>?D n(?)
where n(?) is the number of nonterminals in ?. This
feature aims to learn the preference among phrasal
rules, and abstract rules with one or two nontermi-
nals. This makes our syntax-based model includes a
total of nine features.
The training procedure described in (Chiang,
2007) employs heuristics to hypothesize a distri-
bution of possible rules. A count one is assigned
to every phrase pair occurrence, which is equally
distributed among rules that are derived from this
phrase pair. Hypothesizing this distribution as our
observations on rule occurrence, relative-frequency
estimation is used to obtain P (?|?) and P (?|?).
We note that, however, these parameters are of-
ten poorly estimated due to the usage of inaccurate
heuristics, which is the major problem that we alle-
viate in Sec. 4.
3.2 Decoder
The objective of our syntax-based decoder is to
search for the optimal derivation tree D from a for-
est of trees that can represent the input sentence. The
target side is mapped accordingly at each nontermi-
nal node in the tree, and a traverse of these nodes
obtains the target translation. Fig. 1 shows an ex-
ample for chart parsing that produces the translation
from the best parse.
Our decoder implements a modified CKY parser
in C++ with integrated n-gram language model scor-
ing. During search, chart cells are filled in a bottom-
up fashion until a tree rooted from nonterminal is
generated that covers the entire input sentence. The
dynamic programming item we bookkeep is denoted
Figure 1: A chart parsing-based decoding on SCFG pro-
duces translation from the best parse: f1f2f3f4f5 ?
e5e6e3e4e1e2.
as [X, i, j; eb], indicating a sub-tree rooted with Xthat has covered input from position i to j generat-
ing target translation with boundary words eb. Tospeed up the decoding, a pruning scheme similar to
the cube pruning (Chiang, 2007) is performed dur-
ing search.
4 Prior Derivation Models
As mentioned above, decoding searches for the op-
timal tree on source side to cover the input sentence
with respect to given models, as shown in Eq. 4.
Among these feature functions, ?i measures howlikely the source and hypothesized target sub-trees
rooted from same X are paired together through
symmetric conditional probabilities (e.g., P (?|?)
and P (?|?)), and the target language model mea-
sures the fluency on target string. It should be noted
that, however, the baseline models do not discrim-
inate between different parses on source side when
the target side is unknown.
Therefore, if we could obtain some prior distribu-
tions for the source side derivations, we can rewrite
Eq. 4 as:
P (D) ? PLM (e)?LM ?
?
X?<?,?>?D
(?i ?i(X ?< ?, ? >)?i)L(X ?< ?, ? >)?L ,(5)
where L(?) is a feature function defined over a pro-
duction but only depending on one side of the rules
22
and asterisk denotes arbitrary symbol sequences on
the other side consistent with our grammars 2. The
production of L(?) over all rules observed in a
derivation D measures the prior distribution of D.
In the baseline model, as a special case, we can see
that L(?) is a constant function.
The motivation is straightforward since some
derivations should be preferred over others. One
may make an analogy between our prior derivation
distributions to non-uniform source side segmenta-
tion models in phrase-based systems. However, it
should be noted that prior derivation models influ-
ence not only on phrase choices as what segmenta-
tion models do, but also on ordering options due to
the nonterminal usage in syntax-based models.
In principle, some quantitative schemes are
needed to evaluate the monolingual derivation prior
probability. Our scheme links the source side deriva-
tion prior probability with the expected ambiguity
on target side generation when mapping the source
side to target side given the derivation. That is, a
given source side derivation is favored if it intro-
duces less ambiguity on target generation compared
to others.
Let us revisit the rules in Eq. 2. We notice that
the same source side maps into different target or-
ders depending on the syntactic role (e.g., NP or PP)
of X2 in the rule. Furthermore, the following areexample rules trained from real data (see Sec. 5):
X ? ?X1passX2, X1gei (give)X2? (6)
X ? ?X1passX2, X1jingguo (traverse)X2? (7)
X ? ?X1passX2, X1piao (ticket)X2? (8)
Above three rules cover pretty well for different us-
ages of pass in English and its correspondence in
Chinese. Typically, applying the rule to inputs such
as ?my pass expired? obtains reasonable translations
with baseline models. However, it will fail on inputs
like ?my pass to the zoo? as none of th rules provides
a correct translation of X1X2piao (ticket) when X2is a prepositional phrase.
Such linguistic phenomena, among others, indi-
cates that the higher variation of syntax structures
2In general, we can plug in either L(X ?< ?, ? >) or
L(X ?< ?, ? >) here. For illustration purposes, we assume
that the model is on the source side.
the nonterminal embodies, the more translation op-
tions on target side needed to account for various
syntactic roles on source side. This suggests that
our prior derivation models should prefer nonter-
minals that cover more syntactically homogeneous
constituents. Such a model is thus proposed in
Sec. 4.1.
The prior derivation model can also be viewed
as a smoothing on rule translation probabilities esti-
mated using heuristics, as we mentioned in Sec. 3.1.
When there are more translation options, we deem
that there are more ambiguity for this rule. In cases
where some dominating translation option is overes-
timated from hypothesized distributions, all transla-
tion options of this rule are discounted as they are
less favored by prior derivation models.
4.1 Model Syntactic Variations
Each abstract rule is generalized from a set of origi-
nal relevant phrase pairs by grouping an appropriate
set of sub-phrases into a nonterminal symbol, with
each sub-phrase linked to a tree list. Therefore, the
joined tree lists form a forest for this nonterminal
symbol in the rule. For every abstract rule, we de-
fine the rule forest to be the set of tree fragments of
all sub-phrases abstracted within this rule.
We parse the English side of parallel corpus to ob-
tain a syntactic tree for each English sentence. For
each phrase extracted from this sentence, we de-
fine the tree fragment for this phrase as the mini-
mal set of internal tree whose leaves span exactly
over this phrase. As a common practice, we pre-
serve all phrase pairs in BP including those who
are not consistent with parser sub-trees. Therefore,
there will be many phrases that cross over syntactic
sub-trees, which subsequently produced tree frag-
ments lacking a root. We label those as ?incom-
plete? tree fragments, and introduce a parent node of
?INC? on top of them to form a single-rooted sub-
tree. For example, Fig. 2 shows the tree fragments
for phrases of ?reading books? and ?enjoy reading?,
where the latter is an ?incomplete? tree fragment.
Moreover, for sentences failed on parsing, we la-
bel all phrases extracted from those sentences with a
root of ?EMPTY?.
Subset trees of tree fragments are defined as any
sub-graph that contains more than one nodes, with
the restriction that entire rule productions must be
23
(a) S


HH
H
NP
PRP
I
VP


HH
H
VBP
enjoy
NP
 HHNN
reading
NNS
books
(b) NP
 HHNN
reading
NNS
books
(c) INC
 HHVBP
enjoy
NN
reading
Figure 2: Syntax parsing tree (a) and tree fragments for
phrases ?reading books? (b) and ?enjoy reading? (c).
NP
 HHNN
reading
NNS
books
NP
 HHNN
reading
NNS
NP
 HHNN NNS
books
NP
 HHNN NNS
NN
reading
NNS
books
Figure 3: Subset trees of the NP covering ?reading
books?.
included (Collins and Duffy, 2002). Fig. 3 enumer-
ates a list of subset trees for fragment (b) in Fig. 2.
To measure syntactic homogeneity, we define the
fragment similarity K(T1, T2) as the number ofcommon subset trees between two tree fragments T1and T2. Conceptually, if we enumerate all possiblesubset trees 1, . . . ,M , we can represent each tree
fragment T as a vector h(T ) = (c1, . . . , cM ) witheach element as the count of occurrences of each
subset tree in T . Thus, the similarity can be ex-
pressed by the inner products of these two vectors.
Note that M will be a huge number for our problem,
and thus we need kernel methods presented below to
make computation tractable.
4.2 Kernel Methods
Collins and Duffy (Collins and Duffy, 2002) intro-
duced a method employing convolution kernels to
measure similarity between two trees in terms of
their sub-structures. If we define an indicator func-
tion Ii(n) to be 1 if subset tree i is rooted at node nand 0 otherwise, we have:
K(T1, T2) =
?
n1?N1
?
n2?N2
C(n1, n2) (9)
where C(n1, n2) = ?i Ii(n1)Ii(n2) and N1, N2are the set of nodes in the tree fragment T1 and T2respectively. It is noted that C(n1, n2) can be com-puted recursively (Collins and Duffy, 2002):
1. C(n1, n2) = 0 if the productions at n1 and n2are different;
2. C(n1, n2) = 1 if the productions at n1 and n2are the same and both are pre-terminals;
3. Otherwise,
C(n1, n2) = ?
nc(n1)
?
j=1
(1 + C(chjn1 , ch
j
n2)) (10)
where chjn1 is the jth child of node n1, nc(n1) isthe number of children at n1 and 0 < ? ? 1 is adecay factor to discount the effects of deeper tree
structures.
In principle, the computational complexity of
Eq. 10 is O(|N1| ? |N2|). However, as noted by(Collins and Duffy, 2002), the worst case is quite un-
common to natural language syntactic trees. More
recently, Moschitti (Moschitti, 2006) introduced in
details a fast implementation of tree kernels, where a
node pair set is first constructed for those associated
with same production rules. Our work follows Mos-
chitti?s implementation, which runs in linear time on
average. We compute the normalized similarity as
K ?(T1, T2) = K(T1,T2)?K(T1,T1)?
?
K(T2,T2)
to ensure simi-
larity is normalized between 0 and 1.
4.3 Prior Derivation Cost
First we define the purity of a nonterminal forest
(with respect to a given rule) Pur(X) as the aver-
age similarity of all tree fragments in the cluster:
Pur(X) = 2N(N ? 1)
?
j
?
i<j
K ?(Ti, Tj), (11)
where N is number of tree fragments in the forest of
X . We now can define the derivation cost L(X ?<
24
?, ? >) for a rule production as:
L(X ?< ?, ? >) =
? log(( min
X1,X2??
(Pur(X1), Pur(X2)))k), (12)
where k ? 1 is the degree of smoothness. Note that
the prior derivation cost is set as L(?) = 0 by defini-
tion for phrasal rules.
Eq. 11 is quadratic complexity with N , however,
we note that rules with a large N will typically score
poorly on prior derivation models, and thus we can
avoid the computation for those by assigning them
a large cost. With the fast kernel computation, the
training procedure involved with the prior derivation
models for the task presented in Sec. 5 is about 5
times slower on a single machine, compared with
the training of the baseline system. However, we
note that our training procedure can be computed in
parallel, and therefore the training speed is not a bot-
tleneck when multiple CPUs are available.
5 Experiments
We perform our experiments on an English-to-
Chinese translation task in travel domain. Our train-
ing set contains 482017 parallel sentences (with
4.4M words on the English side), which are col-
lected from transcription and human translation of
conversations. The vocabulary size is 37K for En-
glish and 44K for Chinese after segmentation.
Our evaluation data is a held out data set of 2755
sentences pairs. We extracted every one out of two
sentence pairs into the dev-set, and left the remain-
der as the test-set. We thereby obtained a dev-set of
1378 sentence pairs, and a test-set with 1377 sen-
tence pairs. In both cases, there are about 15K run-
ning words on English side. All Chinese sentences
in training, dev and test sets are all automatically
segmented into words. Minimum-error-rate training
(Och, 2003) are conducted on dev-set to optimize
feature weights maximizing the BLEU score up to 4-
grams, and the obtained feature weights are blindly
applied on the test-set. To compare performances
excluding tokenization effects, all BLEU scores are
optimized (on dev-set) and reported (on test-set) at
Chinese character-level.
From training data, we extracted an initial phrase
pair set with 3.7M entries for phrases up to 8 words
on Chinese side. We trained a 4-gram language
model for Chinese at word level, which is shared by
all translation systems reported in this paper, using
the Chinese side of the parallel corpus that contains
around 2M segmented words.
We compare the proposed models with two base-
lines: a state-of-the-art phrase-based system and a
formal syntax-based system as described in Sec. 3.
The phrase-based system employs the 3.7M phrase
pairs to build the translation model, and it con-
tains a total set of 8 features, most of which are
identical to our baseline formal syntax-based model.
The difference only lies on that the glue and ab-
straction penalty are not applicable for phrase-based
system. Instead, a lexicalized reordering model is
trained from the word-aligned parallel corpus for
the phrase-based system. More details about our
multiple-graph based phrasal SMT can be found in
(Zhou et al, 2006; Zhou et al, 2008). For the base-
line syntax-based system, we generated a total of
15M rules and used 9 features.
We chose the Stanford parser (Klein and Man-
ning, 2002) as the English parser in our experiments
due to its high accuracy and relatively faster speed.
It was trained on the Wall Street Journal section of
the Penn Treebank. During the parsing, the input
English sentences were tokenized first, in a style
consistent with the data in the Penn Treebank.
We sent 482017 English sentences to the parser.
There were 1221 long sentences failed, less than
0.3% of the whole set. After the word alignment
and phrase extraction on the parallel corpus, we ob-
tained 2.2M unique English phrases. Among them
there are about 34K phrases having an empty tree
in their corresponding tree lists, due to the failure
in parsing. The number of unique tree fragments
for English phrases is 2.5M. Out of them there are
750K marked as incomplete. As mentioned previ-
ously, each rule covers a set of phrases, with each
phrase linked to a tree list. The total number of rules
with unique English side is around 8M.
The distribution of the number of rules over the
number of corresponding trees is shown in Table 1.
We observe that the majority of rules in our model
has less than 150 tree fragments. Therefore, consid-
ering the quadratic complexity in Eq. 11, we pun-
ish the rules with more than 150 unique tree frag-
ments with some floor cluster purity to speed up
25
Table 1: Distribution of rules over trees
Number of trees Number of rules
(0, 10] 3636766
(10, 20] 1556806
(20, 30] 989848
(30, 40] 916606
(40, 50] 488469
(50, 60] 270484
(60, 70] 198438
(70, 80] 86921
(80, 90] 58280
(90, 100] 29147
(100, 150] 437231
> 150 81060
the training. Not surprisingly, the rules with a large
number of tree fragments are typically those with
few stop words as terminals. For instance, the rule
X ?< X1aX2, ? > comes with more than 100Ktrees for the X1.
Table 2: English-to-Chinese BLEU score result on test-
set (character-based)
Models BLEU(4-gram)
Phrase-based 42.11
Formally Syntax-based 43.75
Formally Syntax-based
with prior derivation 44.51
Translation results are presented in Table 2
with character-based BLEU scores using 2 refer-
ences. Our baseline formally syntax-based mod-
els achieved the BLEU score of 43.75, an abso-
lute improvement of 1.6 point improvement over
phrase-based models. The improvement is statisti-
cally significant with p < 0.01 using the sign-test
described by (Collins et al, 2005). Applying the
prior derivation model into the syntax-based system,
BLEU score is further improved to 44.51, obtained
an another absolute improvement of 0.8 point, which
is also significantly better than our baseline syntax-
based models (p < 0.05).
6 Discussion and Summary
We introduced a prior derivation model to enhance
formally syntax-based SCFG for translation. Our
approach links a prior rule distribution with the syn-
tactic variations of abstracted sub-phrases, which
is modeled by distance measuring of linguistically
syntax parsing tree fragments using kernel meth-
ods. The proposed model has improved translation
performance over both phrase-based and formally
syntax-based models. Moreover, such a prior dis-
tribution can also be used to rank and prune SCFG
rules to reduce memory usage for online translation
systems based on syntax-based models.
Although the experiments in this paper are con-
ducted for prior derivation models on source side
in an English-to-Chinese task, we are interested in
applying this to foreign-to-English models as well.
As what we pointed out in Sec. 4, target side prior
derivation model fits with our framework as well.
7 Acknowledgement
The authors would like to thank Stanley F. Chen and
the anonymous reviewers for their helpful comments
on this paper.
References
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Comput. Linguist., 33(2):201?228.
Michael Collins and Nigel Duffy. 2002. Convolution
kernels for natural language. In Advances in Neural
Information Processing Systems 14.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proc. of ACL, pages 531?540.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proc.
of HLT/NAACL-04, Boston, USA, May.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc. of
ACL, pages 961?968.
Daniel Gildea. 2003. Loosely tree-based alignment for
machine translation. In Proc. of ACL, pages 80?87.
Dan Klein and Christopher D. Manning. 2002. Fast exact
inference with a factored model for natural language
parsing. In NIPS, pages 3?10.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc.
NAACL/HLT.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proc. of ACL, pages 609?616.
26
Alessandro Moschitti. 2006. Making tree kernels practi-
cal for natural language learning. In Proc. of EACL.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. In Proc. of ACL, pages 440?447, Hong
Kong, China, October.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Comput. Linguist., 30(4):417?449.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL, pages
160?167.
Giorgio Satta and Enoch Peserico. 2005. Some computa-
tional complexity results for synchronous context-free
grammars. In Proc. of HLT/EMNLP, pages 803?810.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Comput. Linguist., 23(3):377?403.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proc. of ACL, pages
523?530.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In Proc. of HLT/NAACL, pages 256?263.
Bowen Zhou, Stan F. Chen, and Yuqing Gao. 2006. Fol-
som: A fast and memory-efficient phrase-based ap-
proach to statistical machine translation. In IEEE/ACL
Workshop on Spoken Language Technology.
Bowen Zhou, Rong Zhang, and Yuqing Gao. 2008. Lex-
icalized reordering in multiple-graph based statistical
machine translation. In Proc. ICASSP.
27
Proceedings of NAACL HLT 2007, pages 228?235,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Combining Outputs from Multiple Machine Translation Systems
Antti-Veikko I. Rosti   and Necip Fazil Ayan

and Bing Xiang   and
Spyros Matsoukas   and Richard Schwartz   and Bonnie J. Dorr

  BBN Technologies, 10 Moulton Street, Cambridge, MA 02138

arosti,bxiang,smatsouk,schwartz  @bbn.com

Institute for Advanced Computer Studies, University of Maryland, College Park, MD 20742

nfa,bonnie  @umiacs.umd.edu
Abstract
Currently there are several approaches to
machine translation (MT) based on differ-
ent paradigms; e.g., phrasal, hierarchical
and syntax-based. These three approaches
yield similar translation accuracy despite
using fairly different levels of linguistic
knowledge. The availability of such a
variety of systems has led to a growing
interest toward finding better translations
by combining outputs from multiple sys-
tems. This paper describes three differ-
ent approaches to MT system combina-
tion. These combination methods oper-
ate on sentence, phrase and word level
exploiting information from  -best lists,
system scores and target-to-source phrase
alignments. The word-level combination
provides the most robust gains but the
best results on the development test sets
(NIST MT05 and the newsgroup portion
of GALE 2006 dry-run) were achieved by
combining all three methods.
1 Introduction
In recent years, machine translation systems based
on new paradigms have emerged. These systems
employ more than just the surface-level information
used by the state-of-the-art phrase-based translation
systems. For example, hierarchical (Chiang, 2005)
and syntax-based (Galley et al, 2006) systems have
recently improved in both accuracy and scalability.
Combined with the latest advances in phrase-based
translation systems, it has become more attractive
to take advantage of the various outputs in forming
consensus translations (Frederking and Nirenburg,
1994; Bangalore et al, 2001; Jayaraman and Lavie,
2005; Matusov et al, 2006).
System combination has been successfully ap-
plied in state-of-the-art speech recognition evalua-
tion systems for several years (Fiscus, 1997). Even
though the underlying modeling techniques are sim-
ilar, many systems produce very different outputs
with approximately the same accuracy. One of the
most successful approaches is consensus network
decoding (Mangu et al, 2000) which assumes that
the confidence of a word in a certain position is
based on the sum of confidences from each system
output having the word in that position. This re-
quires aligning the system outputs to form a con-
sensus network and ? during decoding ? simply
finding the highest scoring path through this net-
work. The alignment of speech recognition outputs
is fairly straightforward due to the strict constraint in
word order. However, machine translation outputs
do not have this constraint as the word order may be
different between the source and target languages.
MT systems employ various re-ordering (distortion)
models to take this into account.
Three MT system combination methods are pre-
sented in this paper. They operate on the sentence,
phrase and word level. The sentence-level combi-
nation is based on selecting the best hypothesis out
of the merged N-best lists. This method does not
generate new hypotheses ? unlike the phrase and
word-level methods. The phrase-level combination
228
is based on extracting sentence-specific phrase trans-
lation tables from system outputs with alignments
to source and running a phrasal decoder with this
new translation table. This approach is similar to
the multi-engine MT framework proposed in (Fred-
erking and Nirenburg, 1994) which is not capable of
re-ordering. The word-level combination is based
on consensus network decoding. Translation edit
rate (TER) (Snover et al, 2006) is used to align
the hypotheses and minimum Bayes risk decoding
under TER (Sim et al, 2007) is used to select the
alignment hypothesis. All combination methods use
weights which may be tuned using Powell?s method
(Brent, 1973) on  -best lists. Both sentence and
phrase-level combination methods can generate  -
best lists which may also be used as new system out-
puts in the word-level combination.
Experiments on combining six machine transla-
tion system outputs were performed. Three sys-
tems were phrasal, two hierarchical and one syntax-
based. The systems were evaluated on NIST MT05
and the newsgroup portion of the GALE 2006 dry-
run sets. The outputs were evaluated on both TER
and BLEU. As the target evaluation metric in the
GALE program was human-mediated TER (HTER)
(Snover et al, 2006), it was found important to im-
prove both of these automatic metrics.
This paper is organized as follows. Section 2
describes the evaluation metrics and a generic dis-
criminative optimization technique used in tuning of
the various system combination weights. Sentence,
phrase and word-level system combination methods
are presented in Sections 3, 4 and 5. Experimental
results on Arabic and Chinese to English newswire
and newsgroup test data are presented in Section 6.
2 Evaluation Metrics and Discriminative
Tuning
The official metric of the 2006 DARPA GALE
evaluation was human-mediated translation edit rate
(HTER). HTER is computed as the minimum trans-
lation edit rate (TER) between a system output and
a targeted reference which preserves the meaning
and fluency of the sentence (Snover et al, 2006).
The targeted reference is generated by human post-
editors who make edits to a reference translation so
as to minimize the TER between the reference and
the MT output without changing the meaning of the
reference. Computing the HTER is very time con-
suming due to the human post-editing. It is desir-
able to have an automatic evaluation metric that cor-
relates well with the HTER to allow fast evaluation
of the MT systems during development. Correla-
tions of different evaluation metrics have been stud-
ied (Snover et al, 2006) but according to various
internal HTER experiments it is not clear whether
TER or BLEU correlates better. Therefore it is prob-
ably safest to try and not degrade either.
The TER of a translation   is computed as

 	
 
ffProceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 492?500,
Beijing, August 2010
Feature-Rich Discriminative Phrase Rescoring for SMT 
Fei Huang   and    Bing Xiang 
 
IBM T. J. Watson Research Center 
{huangfe, bxiang}@us.ibm.com  
Abstract 
This paper proposes a new approach to 
phrase rescoring for statistical machine 
translation (SMT).  A set of novel fea-
tures capturing the translingual equiva-
lence between a source and a target 
phrase pair are introduced. These features 
are combined with linear regression 
model and neural network to predict the 
quality score of the phrase translation 
pair. These phrase scores are used to dis-
criminatively rescore the baseline MT 
system?s phrase library: boost good 
phrase translations while prune bad ones. 
This approach not only significantly im-
proves machine translation quality, but 
also reduces the model size by a consid-
erable margin. 
1 Introduction 
Statistical Machine Translation (SMT) systems, 
including phrase-based (Och and Ney 2002; 
Koehn et. al. 2003), syntax-based (Yamada and 
Knight 2001; Galley et. al. 2004) or hybrid sys-
tems (Chiang 2005; Zollmann and Venugopal 
2006), are typically built with bilingual phrase 
pairs, which are extracted from parallel sentences 
with word alignment. Due to the noises in the 
bilingual sentence pairs and errors from auto-
matic word alignment, the extracted phrase pairs 
may contain errors, such as  
? dropping content words  
(the $num countries ,||?:<null>),  
? length mismatch  
                 (along the lines of the || ?:of)  
? content irrelevance  
          (the next $num years, || 
??:level ??:aspect ?:<null>) 
   These incorrect phrase pairs compete with cor-
rect phrase pairs during the decoding process, 
and are often selected when their counts are high 
(if they contain systematic alignment errors) or 
certain model costs are low (for example, when 
some source content words are translated into 
target function words in an incorrect phrase pair, 
the language model cost of the incorrect pair may 
be small, making it more likely that the pair will 
be selected for the final translation). As a result, 
the translation quality is degraded when these 
incorrect phrase pairs are selected. 
Various approaches have been proposed over 
the past decade for the purpose of improving the 
phrase pair quality for SMT. For example, a term 
weight based model was presented in (Zhao, et 
al., 2004) to rescore phrase translation pairs. It 
models the translation probability with similari-
ties between the query (source phrase) and 
document (target phrase). Significant improve-
ment was obtained in the translation performance. 
In (Johnson, et al, 2007; Yang and Zheng, 2009), 
a statistical significance test was used to heavily 
prune the phrase table and thus achieved higher 
precision and better MT performance. 
In (Deng, et al, 2008), a generic phrase train-
ing algorithm was proposed with the focus on 
phrase extraction.  Multiple feature functions are 
utilized based on information metrics or word 
alignment. The feature parameters are optimized 
to directly maximize the end-to-end system per-
formance. Significant improvement was reported 
for a small MT task. But when the phrase table is 
large, such as in a large-scale SMT system, the 
computational cost of tuning with this approach 
will be high due to many iterations of phrase ex-
traction and re-decoding. 
In this paper we attempt to improve the quality 
of the phrase table using discriminative phrase 
rescoring method. We develop extensive set of 
features capturing the equivalence of bilingual 
492
phrase pairs. We combine these features using 
linear and nonlinear models in order to predict 
the quality of phrase pairs. Finally we boost the 
score of good phrases while pruning bad phrases. 
This approach not only significantly improves 
the translation quality, but also reduces the 
phrase table size by 16%. 
The paper is organized as follows: in section 2 
we discuss two regression models for phrase pair 
quality prediction: linear regression and neural 
network. In section 3 we introduce the rich set of 
features. We describe how to obtain the training 
data for supervised learning of the two models in 
section 4. Section 5 presents some approaches to 
discriminative phrase rescoring using these 
scores, followed by experiments on model re-
gression and machine translation in section 6. 
2 Problem Formulation 
Our goal is to predict the translation quality of a 
given bilingual phrase pair based on a set of 
features capturing their similarities. These 
features are combined with linear regression 
model and neural network. The training data for 
both models are derived from phrase pairs 
extracted from small amount of parallel 
sentences with hand alignment and machine 
alignment. Details are given in section 4. 
2.1 Linear regression model 
In the linear regression model, the predicted 
phrase pair quality score is defined as 
 
?=
i
ii feffeSco ),(),( ?  (1) 
where ),( fef i is the feature for the phrase pair 
(e,f), as to be defined in section 3. These feature 
values can be binary (0/1), integers or real val-
ues. ? s are the feature weights to be learned 
from training data. The phrase pair quality score 
in the training data is defined as the sum of the 
target phrase?s BLEU score (Papineni et. al. 
2002) and the source phrase?s BLEU score, 
where the reference translation is obtained from 
phrase pairs extracted from human alignment. 
Details about the training data are given in sec-
tion 4. The linear regression model is trained us-
ing a statistical package R1. After training, the 
                                               
1
 http://www.r-project.org/ 
learned feature weights are applied on a held-out 
set of phrase pairs with known quality scores to 
evaluate the model?s regression accuracy. 
2.2 Neural Network model 
A feed-forward back-propagation network (Bry-
son and Ho, 1969) is created with one hidden 
layer and 20 nodes. During training, the phrase 
pair features are fed into the network with their 
quality scores as expected outputs. After certain 
iterations of training, the neural net?s weights are 
stable and its mean square error on the training 
set has been significantly reduced.  Then the 
learned network weights are fixed, and are ap-
plied to the test phrase pairs for regression accu-
racy evaluation. We use MatLab??s neural net 
toolkit for training and test.   
      We will compare both models? prediction 
accuracy in section 6. We would like to know 
whether the non-linear regression model outper-
forms linear regression model in terms of score 
prediction error, and if fewer regression errors 
correspond to better translation quality. 
3 Feature Description 
In this section we will describe the features we 
use to model the equivalence of a bilingual 
phrase pair (e,f). These features are defined on 
the phrase pair, its compositional units (words 
and characters), attributes (POS tags, numbers), 
co-occurrence frequency, length ratio, coverage 
ratio and alignment pattern.  
? Phrase : )|( efPp , )|( fePp   
)(
),()|( fC
feCfePp =   (2) 
where ),( feC is the co-occurrence frequency of 
the phrase pair (e,f), and C(f) is the occurrence 
frequency of the source phrase f. )|( efPp is 
defined similarly. 
 
? Word : )|( efPw , )|( fePw    
?=
i
jijw fetfeP )|(max)|(   (3) 
where )|( ji fet  is the lexical translation prob-
ability. This is similar to the word-level phrase 
493
translation probability, as typically calculated in 
SMT systems (Brown et. al. 1993). Here we use 
max instead of sum. )|( efPw is calculated simi-
larly. 
? Character: )|( efPc , )|( fePc  
   When the source or target words are composed 
of smaller units, such as characters for Chinese 
words, or prefix/stem/suffix for Arabic words, 
we can calculate their translation probability on 
the sub-unit level. This is helpful for languages 
where the meaning of a word is closely related to 
its compositional units, such as Chinese and 
Arabic. 
?=
i
ninc cetfeP )|(max)|(  (4) 
where nc is the n-th character in the source 
phrase  f  (n=1,?,N). 
? POS tag: )|( efPt , )|( fePt  
   In addition to the probabilities estimated at the 
character, word and phrase levels based on the 
surface forms, we also compute the POS-based 
phrase translation probabilities.  For each source 
and target word in a phrase pair, we automati-
cally label their POS tags. Then POS-based 
probabilities are computed in a way similar to the 
calculation of the word-level phrase translation 
probability (formula 3). It is believed that such 
syntactic information can help to distinguish 
good phrase pairs from bad ones (for example, 
when a verb is aligned to a noun, its POS transla-
tion probability should be low). 
? Length ratio 
   This feature computes the ratio of the number 
of content words in the source and target phrases. 
It is designed to penalize phrases where content 
words in the source phrase are dropped in the 
target phrase (or vice versa). The ratio is defined 
to be 10 if the target phrase has zero content 
word while the source phrase has non-zero con-
tent words.  If neither phrase contains a content 
word, the ratio is defined to be 1.  
? Log frequency 
   This feature takes the logarithm of the co-
occurrence frequency of the phrase pair. High 
frequency phrase pairs are more likely to be cor-
rect translations if they are not due to systematic 
alignment errors. 
? Coverage ratio 
   We propose this novel feature based on the 
observation that if a phrase pair is a correct trans-
lation, it often includes correct sub-phrase pair 
translations (decomposition). Similarly a correct 
phrase pair will also appear in correct longer 
phrase pair translations (composition) unless it is 
a very long phrase pair itself. Formally we define 
the coverage ratio of a phrase pair (e,f) as: 
 
),(),(),( feCovfeCovfeCov cd += . (5) 
 
Here ),( feCovd is the decomposition coverage: 
? ?
?
?
?
?
?
=
ff
Pf
Pfe
i
d
i
Li
Lii
ee
feCov
)(*,
)(
1
),(
),( ,  (6) 
where if  is a sub-phrase of  f, and ( ie , if ) is a 
phrase pair in  the MT system?s bilingual phrase 
library LP . ),( 21 ee?  is defined to be 1 
if 21 ee ? , otherwise it is 0.  For each source 
sub-phrase if , this formula calculates the ratio 
that its target translation ie  is also a sub-phrase 
of the target phrase e, then the ratio is summed 
over all the source sub-phrases.  
Similarly the composition coverage is defined 
as  
? ?
?
?
?
?
?
=
j
L
j
L
jj
ff
Pf
Pfe
j
c
ee
feCov
)(*,
),(
1
),(
),(   (7) 
where jf is any source phrase containing f  and 
je  is one of jf ?s translations in LP . We call 
jf a super-phrase of f. For each source super-
phrase jf , this formula calculates the ratio that 
its target translation je  is also a super-phrase of 
the target phrase e, then the ratio is summed over 
all the source super-phrases.  
Short phrase pairs (such as a phrase pair with 
one source word translating into one target word) 
have less sub-phrases but more super-phrases 
(for long phrase pairs, it is the other way around).  
494
Combining the two coverage factors produces 
balanced coverage ratio, not penalizing too short 
or too long phrases.  
? Number match 
   During preprocessing of the training data, 
numbers are mapped into a special token ($num) 
for better generalization. Typically one number 
corresponds to one special token. During transla-
tion numbers should not be arbitrarily dropped or 
inserted. Therefore we can check whether the 
source and target phrases have the right number 
of $num to be matched. If they are the same the 
number match feature has value 1, otherwise it  
is 0. 
? Alignment pattern 
   This feature calculates the number of unaligned 
content words in a given phrase pair, where word 
alignment is obtained simply based on the maxi-
mum lexical translation probability of the source 
(target) word given all the target (source) words 
in the phrase pair.  
 
Among the above 13 features, the number 
match feature is a binary feature, the alignment 
pattern feature is an integer-value feature, and 
the rest are real-value features. Also note that 
most features are positively correlated with the 
phrase translation quality (the greater the feature 
value, the more likely it is a correct phrase trans-
lation) except the alignment pattern feature, 
where more unaligned content words corre-
sponds to bad phrase translations. 
4 Training Data  
The training data for both the linear regression 
and neural network models are bilingual phrase 
pairs with the above 13 feature values as well as 
their expected phrase quality scores. The feature 
values can be computed according to the 
description in section 3. The expected translation 
quality score for the phrase pair (e,f) is defined as 
)|,()|,(),( ** effBleufeeBleufeB +=
 (8) 
where *e is the human translation of the source 
phrase f, and *f is the human translation of the 
target phrase e. These human translations are 
obtained from hand alignment of some parallel 
sentences. 
1. Given hand alignment of some bilingual 
sentence pairs, extract gold phrase 
translation pairs. 
2. Apply automatic word alignment on the 
same bilingual sentences, and extract 
phrase pairs. Note that due to the word 
alignment errors, the extracted phrase 
pairs are noisy.  
3. For each phrase pair (e, f) in the noisy 
phrase table, find whether the source 
phrase f also appears in the gold phrase 
table as (e*, f). If so, use the correspond-
ing target phrase(s) e* as reference trans-
lation(s) to evaluate the BLEU score of 
the target phrase e in the noisy phrase ta-
ble. 
4. Similarly, for each e in (e, f), identify (e, 
f*) in the gold phrase table and compute 
the BLEU score of f using f* as the ref-
erence. 
5. The sum of the above two BLEU scores 
is the phrase pair?s translation quality 
score.   
5 Phrase Rescoring 
Given the bilingual phrase pairs? quality score, 
there are several ways to use them for statistical 
machine translation.  
5.1 Quality score as a decoder feature 
A straightforward way is to use the quality scores 
as an additional feature in the SMT system, com-
bined with other features (phrase scores, word 
scores, distortion scores, LM scores etc.) for MT 
hypotheses scoring. The feature weight can be 
empirically learned using manual tuning or 
automatic tuning such as MERT (Och 2003). In 
this situation, all the phrase pairs and their qual-
ity scores are stored in the MT system, which is 
different from the following approach where in-
correct phrase translations are pruned. 
5.2 Discriminative phrase rescoring 
Another approach is to select good and bad 
phrase pairs based on their predicted quality 
scores, then discriminatively rescore the phrase 
pairs in the baseline phrase library.  We sort the 
phrase pairs based on their quality scores in a 
decreasing order. The bottom N phrase pairs are 
495
considered as incorrect translations and pruned 
from the phrase library. The top M phrase pairs 
MP  are considered as good phrases with correct 
translations. As identifying correct sub-phrase 
translation requires accurate word alignment 
within phrase pairs, which is not easy to obtain 
due to the lack of rich context information within 
the phrase pair, we only boost the good phrase 
pairs? super-phrases in the phrase library. Given 
a phrase pair (e,f) with phrase co-occurrence 
count C(e,f), the weighted co-occurrence count is 
defined as: 
?
?
=
),(),(
),(),('
fefe
i
ii
bfeCfeC   (9) 
where ( ii fe , ) is a good sub-phrase pair of (e,f) 
belonging to MP , with quality score ib . Note 
that if (e,f) contains multiple good sub-phrase 
pairs, its co-occurrence count will be boosted 
multiple times. Here the boost factor is defined 
as the product of quality scores of good sub-
phrase pairs. Instead of product, one can also use 
sum, which did not perform as well in our ex-
periments. The weighted co-occurrence count is 
used to calculate the new phrase translation 
scores:  
?= )(*,'
),(')|(' fC
feCfeP  (10) 
?= ,*)('
),(')|('
eC
feC
efP  (11) 
which replace the original phrase translation 
scores in the SMT system. In addition to phrase 
co-occurrence count rescoring, the quality scores 
can also be used to rescore word translation lexi-
cons by updating word co-occurrence counts ac-
cordingly.  
6 Experiments 
We conducted several experiments to evaluate 
the proposed phrase rescoring approach. First we 
evaluate the two regression models? quality score 
prediction accuracy. Secondly, we apply the pre-
dicted phrase scores on machine translation tasks. 
We will measure the improvement on translation 
quality as well as the reduction of model size. 
Our experiments are on English-Chinese transla-
tion.  
  
6.1 Regression model evaluation 
We select 10K English-Chinese sentence pairs 
with both hand alignment and automatic HMM 
alignment, and extract 106K phrase pairs with 
true phrase translation quality scores as com-
puted according to formula 8. We choose 53K 
phrase pairs for regression model training and 
another 53K phrase pairs for model evaluation. 
There are 14 parameters to be learned (13 feature 
weights plus an intercept parameter) for the lin-
ear regression model, and 280 weights ( 2013?   
MSE of Phrase Pair Quality Scores
0.66
0.68
0.7
0.72
0.74
0.76
0.78
0.8
phr
t2s
cha
rt2
s cov alig
n
num log
fq
wo
rdt
2s
pos
t2s
pos
s2t
len
gth
wo
rds
2t
pha
rs2
t
cha
rs2
t
 
Figure 1. Linear regression model phrase pair pre-
diction MSE curve. Errors are significantly reduced 
when more features are introduced (phrs2t /phrt2s: 
phrase source-to-target/target-to-source features; 
words2t/wordt2s: word-level; chars2t/chart2s: 
character-level; poss2t/post2s: POS-level; cov: cov-
erage ratio; align: alignment pattern; logfq: log fre-
quency; num: number match; length: length ratio). 
 
 
Figure 2. Neural network model phrase pair predic-
tion MSE curve. Errors are significantly reduced 
with more training iterations.  
 
496
for the input weight matrix plus 120 ?  for the 
output weight vector) for the neural network 
model. In both cases, the training data size is 
much more than the parameters size, so there is 
no data sparseness problem.  
   After the model parameters are learned from 
the training data, we apply the regression model 
to the evaluation data set, then compute the 
phrase quality score prediction mean squared 
error (MSE, also known as the average residual 
sum of squares): 
[ ]2),(),(1 ? ?=
k
kktkkp feBfeBKMSE (12) 
where pB is the predicted quality score of the 
phrase pair ( kk fe , ), while tB is the true score 
calculated based on human translations. 
   Figure 1 shows the reduction of the regression 
error in the linear regression model trained with 
different features. One may find that the MSE is 
significantly reduced (from 0.78 to 0.70) when 
additional features are added into the regression 
model.  
Similarly, the neural network?s MSE curve is 
shown in Figure 2. It can be seen that the MSE is 
significantly reduced with more iterations of 
training (from the initial error of 1.33 to 0.42 
after 40 iterations). 
Table 2 shows some phrase pairs with 
high/low quality scores predicted by the linear 
regression model and the neural network. One 
can see that both models assign high scores to 
good phrase translations and low scores to noisy 
phrase pairs. Although the values of these scores 
are beyond the range of [0, 2] as defined in for-
mula 8, this is not a problem for our MT tasks, 
since they are only used as phrase boosting 
weights or pruning threshold. 
6.2 Machine translation evaluation 
We test the above phrase rescoring approach on 
English-Chinese machine translation. The SMT 
system is a phrase-based decoder similar to the 
description in (Tillman 2006), where various 
features are combined within the log-linear 
framework. These features include source-to-
target phrase translation score based on relative 
frequency, source-to-target and target-to-source 
word-to-word translation scores, language model 
score, distortion model scores and word count. 
The training data for these features are 10M Chi- 
 Linear Regression Neural Network  
Good  
phrase 
pairs 
 and|?|5.52327 
 amount|?? ??|4.03006 
 us|, ? -|3.91992 
 her husband|? ??|3.85536 
 the program|?? , ?|3.81078 
 the job|? ? ? ??|3.77406 
 shrine|; ????|3.74336 
 of course ,|, ?? , ? ?|3.7174 
 is only|? ? ? ?|3.69426 
 visit|?? ?|3.67256 
 facilities and|?? , ? ?|3.65402 
  rights|?? |6.96817 
  has become|? ?? |4.16468 
  why|??? |3.82629 
  by armed|? ?? |3.62988 
  o|O |3.47795 
  of drama|? ?? |3.36601 
  government and|?? ? |3.27347 
  introduction|?? |3.19113 
  heart disease|?? ?? |3.11829 
  heads|??? |3.05467 
  american consumers|?? ??? |2.99706 
Bad  
phrase 
pairs 
 as well|? ?|1.03234 
 closed|?? ??|1.01271 
 she was|???|0.99011 
 way|?? ??|0.955918 
 of a|? ? ?|0.914717 
 knowledge|??|0.875116 
 made|?? "|0.837358 
 the|?? ??|0.801142 
 end|??|0.769938 
 held|? ?? ?|0.742588 
  letter|?? ?? |0.39203 
  , though|?? ? |0.37020 
  levels of|? ? ?? |0.34892 
  - board|?? |0.32826 
  number of|? ?? |0.30499 
  indonesia|????? |0.27827 
  xinhua at|$num |0.24433 
  provinces|?? |0.20281 
  new .|?? ? ? ? , |0.15430 
  can|? ?? |0.09502 
Table 2. Examples of good and bad phrase pairs based on the linear regression model and neural network?s 
predicted quality scores. 
 
497
 BLEU NIST Phrase 
Table 
Size 
Baseline 38.67 9.3738 3.65M 
LR-mtfeat 39.31 9.5356 3.65M 
LR-boost (top30k) 39.36 9.5465 3.65M 
LR-prune (tail600k) 39.06 9.4890 3.05M 
LR-disc 
(top30K/tail600K) 
39.75 9.6388 3.05M 
NN-disc 
(top30K/tail600K) 
39.76 9.6547 3.05M 
LR-disc tuning 39.87 9.6594 3.05M 
Significance-prune 38.96 9.3953 3.01M 
Count-Prune 38.65 9.3549 3.05M 
 
Table 3. Translation quality improvements 
with rescored phrase tables. Best result (1.2 
BLEU gain) is obtained with discriminative res-
coring by boosting top 30K phrase pairs and 
pruning bottom 600K phrase pairs, with some 
weight tuning. 
 
nese-English sentence pairs, mostly newswire 
and UN corpora released by LDC. The parallel 
sentences have word alignment automatically 
generated with HMM and MaxEnt word aligner.  
Bilingual phrase translations are extracted from 
these word-aligned parallel corpora. Due to the 
noise in the bilingual sentence pairs and 
automatic word alignment errors, the phrase 
translation library contains many incorrect phrase 
translations, which lead to inaccurate translations, 
as seen in Figure 3.  
Our evaluation data is NIST MT08 English-
Chinese evaluation testset, which includes 1859 
sentences from 129 news documents. The auto-
matic metrics are BLEU and NIST scores, as 
used in the NIST 2008 English-Chinese MT 
evaluation. Note that as there is no whitespace as 
Chinese word boundary, the Chinese translations 
are segmented into characters before scoring in 
order to reduce the variance and errors caused by 
automatic word segmentation, which is also done 
in the NIST MT evaluation.  
Table 3 shows the automatic MT scores using 
the baseline phrase table and rescored phrase 
tables. When the phrase quality scores from the 
linear regression model are used as a separate 
feature in the SMT system (LR-mtfeat as de-
scribed in section 5.1), the improvement is 0.7 
BLEU points (0.16 in terms of NIST scores). By 
boosting the good phrase pairs (top 30K2 phrase 
pairs, LR-boost) from linear regression model, 
the MT quality is improved by 0.7 BLEU points 
over the baseline system. Pruning the bad phrase 
pairs (tail 600K phrase pairs) without using the 
quality scores as features (LR-prune) also im-
proves the MT by 0.4 BLEU points. Combining 
LR-boost and LR_prune, a discriminatively res-
cored phrase table (LR-disc) improved the BLEU 
score by 1.1 BLEU points, and reduce the phrase 
table size by 16% (from 3.6M to 3.0M phrase 
pairs). Manually tuning the boosting weights of 
good phrase pairs leads to additional improve-
ment. Discriminative rescoring using the neural 
net work scores (NN-disc) produced similar im-
provement. 
We also experiment with phrase table pruning 
using Fisher significant test, as proposed in 
(Johnson et. al. 2007). We tuned the pruning 
threshold for the best result. It shows that the 
significance pruning improves over the baseline 
by 0.3 BLEU pts with 17.5% reduction in phrase 
table, but is not as good as our proposed phrase 
rescoring method. In addition, we also show the 
MT result using a count pruning phrase table 
(Count-Prune) where 600K phrase translation 
pairs are pruned based on their co-occurrence 
counts. The MT performance of such phrase ta-
ble pruning is slightly worse than the baseline 
MT system, and significantly worse than the re-
sult using the proposed rescored phrase table. 
When comparing the linear regression and 
neural network models, we find rescoring with 
both models lead to similar MT improvements, 
even though the neural network model has much 
fewer regression errors (0.44 vs. 0.7 in terms of 
MSE). This is due to the rich parameter space of 
the neural network. 
Overall, the discriminative phrase rescoring 
improves the SMT quality by 1.2 BLEU points 
and reduces the phrase table size by 16%. With 
statistical significance test (Zhang and Vogel  
2004), all the improvements are statistically sig-
nificant with p-value < 0.0001.  
Figure 3 presents some English sentences, 
with phrase translation pairs selected in the final 
translations (the top one is from the baseline MT 
system and the bottom one is from the LR-disc 
system).  
                                               
2
 These thresholds are empirically chosen. 
498
 We find that incorrect phrase translations in the 
baseline system (as highlighted with blue bold 
font) are corrected and better translation results 
are obtained. 
7 Conclusion 
We introduced a discriminative phrase rescoring 
approach, which combined rich features with 
linear regression and neural network to predict 
phrase pair translation qualities. Based on these 
quality scores, we boost good phrase translations 
while pruning bad phrase translations. This led to 
statistically significant improvement (1.2 BLEU 
points) in MT and reduced phrase table size by 
16%. 
For the future work, we would like to explore 
other models for quality score prediction, such as 
SVM. We will want to try other approaches to 
utilize the phrase pair quality scores, in addition 
to rescoring the co-occurrence frequency. Finally, 
we will test this approach in other domain appli-
cations and language pairs. 
References 
Peter F. Brown, Vincent J. Della Pietra, Stephen A. 
Della Pietra, Robert L. Mercer. 1993.  The Mathe-
matics of Statistical Machine Translation: Parame-
ter Estimation, Computational Linguistics, v.19 n.2, 
June 1993. 
Arthur Earl Bryson, Yu-Chi Ho. 1969. Applied Opti-
mal Control: Optimization, Estimation, and Con-
trol. Blaisdell Publishing Company. p481. 
David Chiang. 2005. A Hierarchical Phrase-based 
Model for Statistical Machine Translation. 2005. 
In Proc. of ACL, pp. 263?270. 
Yonggang Deng, Jia Xu, and Yuqing Gao. 2008. 
Phrase Table Training for Precision and Recall: 
What Makes a Good Phrase and a Good Phrase 
Pair? In Proc. of ACL/HLT, pp. 81-88. 
Michel Galley, Mark Hopkins, Kevin Knight, Daniel 
Marcu. 2004. What's in a Translation Rule? In 
Proc. of NAACL 2004, pp. 273-280. 
Howard Johnson, Joel Martin, George Foster, and 
Roland Kuhn. 2007. Improving Translation Quality 
by Discarding Most of the Phrase Table. In Proc. 
of EMNLP-CoNLL, pp. 967-975. 
Src 
Baseline 
 
 
PhrResco 
 
Indonesian bird flu victim contracted virus indirectly: 
<indonesian bird flu|?? ???> <virus|??> <victim contracted|???> <indi-
rectly :|?? :> 
<indonesian bird flu|?? ???> <victim|???> <contracted|??> <virus|??
> <indirectly :|?? :> 
Src 
 
Baseline 
 
 
 
PhrResco 
The director of Palestinian human rights group Al-Dhamir, Khalil Abu Shammaleh, said 
he was also opposed to the move. 
<the director of|?? ?> <palestinian|????> <human rights group|?? ??> 
<al -|" ?? " ??> <,|,> <abu|Abu> <khalil|Khalil> <, said he was|? 
? , ?> <also opposed to|? ??> <the move .|? ? ?? ?> 
<the director of|?? ?> <palestinian|????> <human rights group|?? ??> 
<al -|al -> <, khalil|, khalil> <abu|??> <, said he was|? , ?> <also opposed to|? 
??> <the move .|? ? ?? ?> 
Src 
Baseline 
 
 
 
PhrResco 
A young female tourist and two of her Kashmiri friends were among the victims. 
<a young female|? ? ? ?? ??> <tourist and|?? ?> <$num of her|? ? 
$num ?> <kashmiri|????> <friends were|??> <among the|?? ?> <victims 
.|??? ?> 
<a young|? ? ?? ?> <female|??> <tourist and|?? ?> <$num of her|? ? 
$num ?> <kashmiri|????> <friends were|??> <among the|?? ?> <victims 
.|??? ?> 
Figure 3.  Examples of English sentences and their translation, with phrase pairs from baseline sys-
tem and phrase rescored system. Highlighted text are initial phrase translation errors which are cor-
rected in the PhrResco translations. 
 
499
Philipp Koehn, Franz Josef Och, Daniel Marcu. 2003. 
Statistical Phrase-based Translation, In Proc. of 
NAACL, pp. 48-54. 
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment 
Models, Computational Linguistics, v.29 n.1, 
pp.19-51, March 2003   
Franz Josef Och. 2003. Minimum Error Rate Training 
in Statistical Machine Translation, In Proc. of ACL, 
2003, pp. 160-167. 
Kishore Papineni, Salim Roukos, Todd Ward, Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic 
Evaluation of Machine Translation, In Proc.  of 
ACL, pp. 311-318. 
Christoph Tillmann. 2006. Efficient Dynamic Pro-
gramming Search Algorithms for Phrase-based 
SMT. In Proc. of the Workshop CHPSLP at 
HLT'06. 
Kenji Yamada and Kevin Knight. 2001. A Syntax-
based Statistical Translation Model, In Proc.  of 
ACL, pp.523-530. 
Mei Yang and Jing Zheng. 2009. Toward Smaller, 
Faster, and Better Hierarchical Phrase-based 
SMT. In Proc. of ACL-IJCNLP, pp. 237-240. 
Ying Zhang and Stephan Vogel. 2004. Measuring 
Confidence Intervals for the Machine Translation 
Evaluation Metrics, In Proc. TMI, pp. 4-6. 
Bing Zhao, Stephan Vogel, and Alex Waibel. 2004. 
Phrase Pair Rescoring with Term Weighting for 
Statistical Machine Translation. In Proc. of 
EMNLP, pp. 206-213. 
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax Augmented Machine Translation via Chart 
Parsing. In Proc. of NAACL 2006- Workshop on 
statistical machine translation. 
 
 
 
 
500
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 889?898,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
A Correction Model for Word Alignments
J. Scott McCarley, Abraham Ittycheriah, Salim Roukos, Bing Xiang, Jian-ming Xu
IBM T.J. Watson Research Center
1101 Kitchawan Road, Rt. 134
Yorktown Heights, NY 10598
{jsmc,abei,roukos,bxiang,jianxu}@us.ibm.com
Abstract
Models of word alignment built as sequences
of links have limited expressive power, but are
easy to decode. Word aligners that model the
alignment matrix can express arbitrary align-
ments, but are difficult to decode. We pro-
pose an alignment matrix model as a cor-
rection algorithm to an underlying sequence-
based aligner. Then a greedy decoding al-
gorithm enables the full expressive power of
the alignment matrix formulation. Improved
alignment performance is shown for all nine
language pairs tested. The improved align-
ments also improved translation quality from
Chinese to English and English to Italian.
1 Introduction
Word-level alignments of parallel text are crucial for
enabling machine learning algorithms to fully uti-
lize parallel corpora as training data. Word align-
ments appear as hidden variables in IBM Models 1-
5 (Brown et al, 1993) in order to bridge a gap be-
tween the sentence-level granularity that is explicit
in the training data, and the implicit word-level cor-
respondence that is needed to statistically model lex-
ical ambiguity and word order rearrangements that
are inherent in the translation process. Other no-
table applications of word alignments include cross-
language projection of linguistic analyzers (such as
POS taggers and named entity detectors,) a subject
which continues to be of interest. (Yarowsky et al,
2001), (Benajiba and Zitouni, 2010)
The structure of the alignment model is tightly
linked to the task of finding the optimal alignment.
Many alignment models are factorized in order to
use dynamic programming and beam search for ef-
ficient marginalization and search. Such a factoriza-
tion encourages - but does not require - a sequential
(often left-to-right) decoding order. If left-to-right
decoding is adopted (and exact dynamic program-
ming is intractable) important right context may ex-
ist beyond the search window. For example, the link-
age of an English determiner may be considered be-
fore the linkage of a distant head noun.
An alignment model that jointly models all of the
links in the entire sentence does not motivate a par-
ticular decoding order. It simply assigns comparable
scores to the alignment of the entire sentence, and
may be used to rescore the top-N hypotheses of an-
other aligner, or to decide whether heuristic pertur-
bations to the output of an existing aligner constitute
an improvement. Both the training and decoding of
full-sentence models have presented difficulties in
the past, and approximations are necessary.
In this paper, we will show that by using an ex-
isting alignment as a starting point, we can make a
significant improvement to the alignment by propos-
ing a series of heuristic perturbations. In effect, we
train a model to fix the errors of the existing aligner.
From any initial alignment configuration, these per-
turbations define a multitude of paths to the refer-
ence (gold) alignment. Our model learns alignment
moves that modify an initial alignment into the ref-
erence alignment. Furthermore, the resulting model
assigns a score to the alignment and thus could be
used in numerous rescoring algorithms, such as top-
N rescorers.
In particular, we use the maximum entropy frame-
889
work to choose alignment moves. The model is sym-
metric: source and target languages are interchange-
able. The alignment moves are sufficiently rich to
reach arbitrary phrase to phrase alignments. Since
most of the features in the model are not language-
specific, we are able to test the correction model
easily on nine language pairs; our corrections im-
proved the alignment quality compared to the input
alignments in all nine. We also tested the impact on
translation and found a 0.48 BLEU improvement on
Chinese to English and a 1.26 BLEU improvement
on English to Italian translation.
2 Alignment sequence models
Sequence models are the traditional workhorse for
word alignment, appearing, for instance, in IBM
Models 1-5. This type of alignment model is not
symmetric; interchanging source and target lan-
guages results in a different aligner. This parameter-
ization does not allow a target word to be linked to
more than one source word, so some phrasal align-
ments are simply not considered. Often the choice of
directionality is motivated by this restriction, and the
choice of tokenization style may be designed (Lee,
2004) to reduce this problem. Nevertheless, aligners
that use this parameterization internally often incor-
porate various heuristics in order to augment their
output with the disallowed alignments - for example,
swapping source and target languages to obtain a
second alignment (Koehn et al, 2007) with different
limitations. Training both directions jointly (Liang
et al, 2006) and using posterior probabilities dur-
ing alignment prediction even allows the model to
see limited right context. Another alignment combi-
nation strategy (Deng and Zhou, 2009) directly op-
timizes the size of the phrase table of a target MT
system.
Generative models (such as Models 1-5, and the
HMM model (Vogel et al, 1996)) motivate a narra-
tive where alignments are selected left-to-right and
target words are then generated conditioned upon
the alignment and the source words. Generative
models are typically trained unsupervised, from par-
allel corpora without manually annotated word-level
alignments.
Discriminative models of alignment incorporate
source and target words, as well as more linguisti-
cally motivated features into the prediction of align-
ment. These models are trained from annotated
word alignments. Examples include the maximum
entropy model of (Ittycheriah and Roukos, 2005) or
the conditional random field jointly normalized over
the entire sequence of alignments of (Blunsom and
Cohn, 2006).
3 Joint Models
An alternate parameterization of alignment is the
alignment matrix (Niehues and Vogel, 2008). For a
source sentence F consisting of words f1...fm, and
a target sentence E = e1...el, the alignment matrix
A = {?ij} is an l ? m matrix of binary variables.
If ?ij = 1, then ei is said to be linked to fj . If ei
is unlinked then ?ij = 0 for all j. There is no con-
straint limiting the number of source tokens to which
a target word is linked either; thus the binary ma-
trix allows some alignments that cannot be modeled
by the sequence parameterization. All 2lm binary
matrices are potentially allowed in alignment matrix
models. For typical l and m, 2lm  (m + 1)l, the
number of alignments described by a comparable se-
quence model. This parameterization is symmetric -
if source and target are interchanged, then the align-
ment matrix is transposed.
A straightforward approach to the alignment ma-
trix is to build a log linear model (Liu et al, 2005)
for the probability of the alignment A. (We continue
to refer to ?source? and ?target? words only for con-
sistency of notation - alignment models such as this
are indifferent to the actual direction of translation.)
The log linear model for the alignment (Liu et al,
2005) is
p(A|E,F ) = exp (
?
i ?i?i(A,E, F ))
Z(E,F ) (1)
where the partition function (normalization) is given
by
Z(E,F ) =
?
A
exp
(?
i
?i?i(A,E, F )
)
. (2)
Here the ?i(A,E, F ) are feature functions. The
model is parameterized by a set of weights ?i, one
for each feature function. Feature functions are often
binary, but are not required to be. Feature functions
890
may depend upon any number of components ?ij of
the alignment matrix A.
The sum over all alignments of a sentence pair
(2lm terms) in the partition function is computa-
tionally impractical except for very short sentences,
and is rarely amenable to dynamic programming.
Thus the partition function is replaced by an ap-
proximation. For example, the sum over all align-
ments may be restricted to a sum over the n-best
list from other aligners (Liu et al, 2005). This ap-
proximation was found to be inconsistent for small
n unless the merged results of several aligners were
used. Alternately, loopy belief propagation tech-
niques were used in (Niehues and Vogel, 2008).
Loopy belief propagation is not guaranteed to con-
verge, and feature design is influenced by consider-
ation of the loops created by the features. Outside
of the maximum entropy framework, similar models
have been trained using maximum weighted bipar-
tite graph matching (Taskar et al, 2005), averaged
perceptron (Moore, 2005), (Moore et al, 2006), and
transformation-based learning (Ayan et al, 2005).
4 Alignment Correction Model
In this section we describe a novel approach to word
alignment, in which we train a log linear (maximum
entropy) model of alignment by viewing it as correc-
tion model that fixes the errors of an existing aligner.
We assume a priori that the aligner will start from
an existing alignment of reasonable quality, and will
attempt to apply a series of small changes to that
alignment in order to correct it. The aligner naturally
consists of a move generator and a move selector.
The move generator perturbs an existing align-
ment A in order to create a set of candidate align-
mentsMt(A), all of which are nearby to A in the
space of alignments. We index the set of moves by
the decoding step t to indicate that we generate en-
tirely different (even non-overlapping) sets of moves
at different steps t of the alignment prediction. Typ-
ically the moves affect linkages local to a particular
word, e.g. the t?th source word.
The move selector then chooses one of the align-
ments At+1 ? Mt(At), and proceeds iteratively:
At+2 ? Mt+1(At+1), etc. until suitable termina-
tion criteria are reached. Pseudocode is depicted in
Fig. (1.) In practice, one move for each source and
Input: sentence pair E1 .. El, F1 .. Fm
Input: alignment A
Output: improved alignment Afinal
for t = 1? l do
generate moves:Mt(At)
select move:
At+1 ? argmaxA?Mt(At)p(A|At, E, F )
Afinal ? Al+1
{repeat for source words}
Figure 1: pseudocode for alignment correction
target word is sufficient.
4.1 Move generation
Many different types of alignment perturbations are
possible. Here we restrict ourselves to a very sim-
ple move generator that changes the linkage of ex-
actly one source word at a time, or exactly one target
word at a time. Many of our corrections are simi-
lar to those of (Setiawan et al, 2010), although our
motivation is perhaps closer to (Brown et al, 1993),
who used similar perturbations to approximate in-
tractable sums that arise when estimating the param-
eters of the generative models Models 3-5, and ap-
proach refined in (Och and Ney, 2003). We note that
our corrections are designed to improve even a high-
quality starting alignment; in contrast the model of
(Fossum et al, 2008) considers deletion of links
from an initial alignment (union of aligners) that is
likely to overproduce links.
From the point of view of the alignment ma-
trix, we consider changes to one row or one col-
umn (generically, one slice) of the alignment matrix.
At each step t, the move setMt(At) is formed by
choosing a slice of the current alignment matrix At,
and generating all possible alignments from a few
families of moves. Then the move generator picks
another slice and repeats. The m + l slices are cy-
cled in a fixed order: the first m slices correspond to
source words (ordered according to a heuristic top-
down traversal of the dependency parse tree if avail-
able), and the remaining l slices correspond to target
words, similarly parse-ordered. For each slice we
consider the following families of moves, illustrated
by rows.
? add link to row i - for one j such that ?ij = 0,
891
make ?ij = 1 (shown here for row i = 1.)
? ? ?
a ? ? ?
b ? ? ?
c ? ? ?
=?
? ? ?
a ? ? ?
b ? ? ?
c ? ? ?
? remove one or more links from row i - for some
j such that ?ij = 1, make ?ij = 0 (shown here
for i = 3.)
? ? ?
a ? ? ?
b ? ? ?
c ? ? ?
=?
? ? ?
a ? ? ?
b ? ? ?
c ? ? ?
? move a link in row i - for one j and one j? such
that ?ij = 1 and ?ij? = 0, make ?ij = 0 and
?ij? = 1 (shown here for i = 1.)
? ? ?
a ? ? ?
b ? ? ?
c ? ? ?
=?
? ? ?
a ? ? ?
b ? ? ?
c ? ? ?
? leave row i unchanged
Similar families of moves apply to column slices
(source words.) In practice, perturbations are re-
stricted by a window (typically ?5 from existing
links.) If the given source word is unlinked, we
consider adding a link to each target word in a win-
dow (?5 from nearby links.) The window size re-
strictions mean that some reference alignments are
not reachable from the starting point. However, this
is unlikely to limit performance - an oracle aligner
achieves 97.6%F -measure on the Arabic-English
training set.
4.2 Move selection
A log linear model for the selection of the candidate
alignment at t+1 from the set of alignmentsMt(At)
generated by the move generator at step t takes the
form:
p(At+1|E,F,Mt(At)) =
e
P
i ?i?i(At+1,E,F )
Z(E,F,Mt(At))
(3)
where the partition function is now given by
Z(E,F,M) =
?
A?M
e
P
i ?i?i(A,E,F ) (4)
and At+1 ? Mt(At) is required for correct normal-
ization. This equation is notationally very similar
to equation (1), except that the predictions of the
model are restricted to a small set of nearby align-
ments. For the move generator considered in this pa-
per, the summation in Eq.(4) is similarly restricted,
and hence training the model is tractable. The set
of candidate alignmentsMt(At) typically does not
contain the reference (gold) alignment; we model
the best alignment among a finite set of alternatives,
rather than the correct alignment from among all
possible alignments. This is a key difference be-
tween our model and (Liu et al, 2005).
Note that if we extended our definition of pertur-
bation to the limiting case that the alignment set in-
cluded all possible alignments then we would clearly
recover the standard log linear model of alignment.
4.3 Training
Since the model is designed to predict perturbation
to an alignment, it is trained from a collection of
errorful alignments and corresponding reference se-
quences of aligner moves that reach the reference
(gold) alignment. We construct a training set from
a collection of sentence pairs and reference align-
ments for training (A?n, En, Fn)Nn=1, as well as col-
lections of corresponding ?first pass? alignments An1
produced by another aligner. For each n, we form a
number of candidate alignment sets Mt(Ant ), one
for each source and target word. For training pur-
poses, the true alignment from the set is taken to be
the one identical withA?n in the slice targeted by the
move generator at the current step. (A small number
of move sets do not have an exact match and are dis-
carded.) Then we form an objective function from
the log likelihood of reference alignment, smoothed
with a gaussian prior
L =
?
n
Ln +
?
i
(?i/?)2 (5)
892
where the likelihood of each training sample is
Ln =
?
?
log p1(A0n|E,Fn;M(f?, A0n, E, Fn))
+
?
?
log p1(A0n|E,Fn;M(e?, A0n, E, Fn)) (6)
The likelihood has a term for each sentence pair
and for each decoder step. The model is trained
by gradient ascent using the l-BFGS method (Liu
and Nocedal, 1989), which has been successfully
used for training log linear models (Blunsom and
Cohn, 2006) in many natural language tasks, includ-
ing alignment.
5 Features
A wide variety of features were used in the model.
We group the features in three broad categories:
link-based, geometrical, and parse-based.
Link-based features are those which decompose
into a (linear) sum of alignment matrix elements ?ij .
An example link-based feature is one that fires if a
source language noun is linked to a target language
determiner. Note that this feature may fire more than
once in a given sentence pair: as with most fea-
tures in our model, it is an integer-valued feature
that counts the number of times a structure appears
in a sentence pair. These features do not capture any
correlation between different ?ij . Among the link-
based features are those based on Model 1 transla-
tion matrix parameters ?(ei|fj) and ?(fj |ei). We
bin the model 1 parameters, and form integer-valued
features for each bin that count the number of links
with ?0 < ?(ei|fj) < ?1.
Geometrical features are those which capture cor-
relation between different ?ij based on adjacency or
nearness. They capture the idea that nearby words
in one language link to nearby words in the other
language - the motivation of HMM-based models
of alignment. An example is a feature that counts
the number of times that the next word in the source
language is linked to the next word in the target lan-
guage:
?(A,E, F ) =
?
ij
?ij?i+1,j+1 (7)
Parse-based features are those which capture cor-
relation between different ?ij , but use parsing to de-
termine links which are correlated - for example, if a
determiner links to the same word as its head noun.
As an example, if ei is the headword of ei? , and fj is
the headword of fj? , then
?(A,E, F ) =
?
ij
?ij?i?j? (8)
counts the number of times that a dependency rela-
tion in one language is preserved by alignment in the
other language. This feature can also be decorated,
either lexically, or with part-of-speech tags (as many
features in all three categories are.)
5.1 Unsupervised Adaptation
We constructed a heuristic phrase dictionary for un-
supervised adapatation. After aligning a large unan-
notated parallel corpus with our aligner, we enumer-
ate fully lexicalized geometrical features that can be
extracted from the resulting alignments - these are
entries in a phrase dictionary. These features are
tied, and treated as a single real-valued feature that
fires during training and decoding phases if a set of
hypothesized links matches the geometrical feature
extracted from the unannotated data. The value of
this real-valued feature is the log of the number of
occurrences of the identical (lexicalized) geometri-
cal feature in the aligned unannotated corpus.
6 Results
We design our experiments to validate that a cor-
rection model using simple features, mostly non-
language-specific, can improve the alignment accu-
racy of a variety of existing aligners for a variety of
language pairs; we do not attempt to exactly match
features between comparison aligners - this is un-
likely to lead to a robust correction model.
6.1 Arabic-English alignment results
We trained the Arabic-English alignment system
on 5125 sentences from Arabic-English treebanks
(LDC2008E61, LDC2008E22) that had been an-
notated for word alignment. Reference parses
were used during the training. Results are mea-
sured on a 500 sentence test set, sampled from
a wide variety of parallel corpora, including vari-
ous genres. During alignment, only automatically-
generated parses (based on the parser of (Rat-
naparkhi, 1999)) were available. Alignments on
893
initial align correction model R (%) P (%) F (%) ?F
GIZA++ 76 76 76
corr(GIZA++) 86 94 90 14?
corr(ME-seq) 88 92 90 14?
HMM 73 73 73
corr(HMM) 87 92 89 16?
corr(ME-seq) 87 93 90 17?
ME-seq 82 84 83
corr(HMM) 88 92 90 7?
corr(GIZA++) 87 94 91 8?
corr(ME-seq) 89 94 91 8?
Table 1: Alignment accuracy for Arabic-English systems in percentage recall (R), precision(P), and F -measure. ?
denotes statistical significance (see text.)
lang method R (%) P(%) F (%) ?F
ZH?EN GIZA++ 55 67 61
ME-seq 66 72 69
corr(ME-seq) 74 76 75 6?
Table 2: Alignment accuracy for Chinese(ZH)-English(EN) systems. ? denotes statistical significance
lang aligner R(%) P(%) F (%) ?F
IT? EN ME-seq 74 87 80
corr(ME-seq) 84 92 88 8?
EN?IT ME-seq 75 86 80
corr(ME-seq) 84 92 88 8?
PT?EN ME-seq 77 83 80
corr(ME-seq) 87 91 89 9?
EN?PT ME-seq 79 87 83
corr(ME-seq) 88 90 89 6?
JA?EN ME-seq 72 78 75
corr(ME-seq) 77 83 80 5?
RU?EN ME-seq 81 85 83
corr(ME-seq) 82 92 87 4?
DE?EN ME-seq 77 82 79
corr(ME-seq) 78 87 82 3?
ES?EN ME-seq 93 86 90
corr(ME-seq) 92 88 90 0.6
FR?EN ME-seq 89 91 90
corr(ME-seq) 88 92 90 0.1
Table 3: Alignment accuracy for additional languages. ? denotes statistical significance; ? statistical significance not
available. IT=Italian, PT=Portuguese, JA=Japanese, RU=Russian, DE=German, ES=Spanish, FR=French
894
the training and test sets were decoded with three
other aligners, so that the robustness of the cor-
rection model to different input alignments could
be validated. The three aligners were GIZA++
(Och and Ney, 2003) (with the MOSES (Koehn
et al, 2007) postprocessing option -alignment
grow-diag-final-and) the posterior HMM
aligner of (Ge, 2004), a maximum entropy sequen-
tial model (ME-seq) (Ittycheriah and Roukos, 2005).
ME-seq is our primary point of comparison: it is
discriminatively trained (on the same training data,)
uses a rich set of features, and provides the best
alignments of the three. Three correction models
were trained: corr(GIZA++) is trained to correct
the alignments produced by GIZA++, corr(HMM)
is trained to correct the alignments produced by the
HMM aligner, and corr(ME-seq) is trained to correct
the alignments produced by the ME-seq model.
In Table (1) we show results for our system cor-
recting each of the aligners as measured in the usual
recall, precision, and F -measure.1 The resulting
improvements in F -measure of the alignments pro-
duced by our models over their corresponding base-
lines is statistically significant (p < 10?4, indicated
by a ?.) Statistical significance is tested by a Monte
Carlo bootstrap (Efron and Tibshirani, 1986) - sam-
pling with replacement the difference in F -measure
of the two system?s alignments of the same sentence
pair. Both recall and precision are improved, but the
improvement in precision is somewhat larger. We
also show cross-condition results in which a correc-
tion model trained to correct HMM alignments is ap-
plied to correct ME-seq alignments. These results
show that our correction model is robust to different
starting aligners.
6.2 Chinese-English alignment results
Table (2) presents results for Chinese-English word
alignments. The training set for the corr(ME-
seq) model consisted of approximately 8000 hand-
aligned sentences sampled from LDC2006E93 and
LDC2008E57. The model was trained to correct
the output of the ME-seq aligner, and tested on
the same condition. For this language pair, refer-
ence parses were not available in our training set, so
1We do not distinguish sure and possible links in our anno-
tations - under this circumstance, alignment error rate(Och and
Ney, 2003) is 1? F .
automatically-generated parses were used for both
training and test sets. Results are measured on a 512
sentence test set, sampled from a wide variety of par-
allel corpora of various genres. We compare perfor-
mance with GIZA++, and with the ME-seq aligner.
Again the resulting improvement over the ME-seq
aligner is statistically significant. However, here the
improvement in recall is somewhat larger than the
improvement in precision.
6.3 Additional language pairs
Table (3) presents alignment results for seven other
language pairs. Separate alignment corrector mod-
els were trained for both directions of Italian ?
English and Portuguese ? English. The training
and test data vary by language, and are sampled
uniformly from a diverse set of corpora of various
genres, including newswire, and technical manuals.
Manual alignments for training and test data were
annotated. We compare performance with the ME-
seq aligner trained on the same training data. As
with the Chinese results above, customization and
feature development for the language pairs was min-
imal. In general, machine parses were always avail-
able for the English half of the pair. Machine parses
were also available for French and Spanish. Ma-
chine part of speech tags were available for all lan-
guage (although character-based heuristic was sub-
stituted for Japanese.) Large amounts (up to 10 mil-
lion sentence pairs) of unaligned parallel text was
available for model 1 type features. Our model ob-
tained improved alignment F -measure in all lan-
guage pairs, although the improvements were small
for ES?EN and FR?EN, the language pairs for
which the baseline accuracy was the highest.
6.4 Analysis
Some of the improvement can be attributed to ?look-
ahead? during the decoding. For example, the
English word ?the?, which (during Arabic-English
alignment) should often be aligned to the same Ara-
bic words to which its headword is linked. The num-
ber of errors associated with ?the? dropped from 383
(186 false alarms, 197 misses) in the ME-seq model
to 137 (60 false alarms and 77 misses) in the current
model.
In table 5, we show contributions to performance
resulting from various classes of features. The
895
Zh-En Ar-En
method correct miss fa correct miss fa
hmm 147 256 300
GIZA++ 139 677 396 132 271 370
ME-seq 71 745 133 127 276 191
corr(ME-seq) 358 458 231 264 139 114
Table 4: Analysis of 2?1 alignments errors (misses and false alarms) for Zh-En and Ar-En aligners
largest contribution is noted by removing features
based on the Model 1 translation matrices. These
features contain a wealth of lexical information
learned from approximately 7 ? 106 parallel sen-
tences - information that cannot be learned from
a relatively small amount of word-aligned train-
ing data. Geometrical features contribute more
than parse-based features, but the contribution from
parse-based features is important, and these are
more difficult to incorporate into sequential mod-
els. We note that all of the comparison aligners had
equivalent lexical information.
We show a small improvement from the unsuper-
vised adaptation - learning phrases from the parallel
corpus that are not captured by the lexical features
based on model 1. The final row in the table shows
the result of running the correction model on its own
output. The improvement is not statistically signif-
icant, but it is important to note the performance is
stable - a further indication that the model is robust
to a wide variety of input alignments, and that our
decoding scheme is a reasonable approach to find-
ing the best alignment.
In table 4, we characterize the errors based on the
fertility of the source and target words. We focus
on the case that exactly one target word is linked to
exactly two source words. These are the links that
feature R(%) P(%) F (%) Nexact
base 89 94 91 136
base-M1 82 88 85 89
base-geometric 83 90 86 92
base-parse 87 93 90 116
base+un.adapt 89 94 92 141
+iter2 90 94 92 141
Table 5: Importance of feature classes - ablation experi-
ments
corpus-level p90
alignment TER BLEU TER BLEU
ME-seq 56.06 32.65 64.20 21.31
corr(Me-seq) 56.25 33.10 63.47 22.02
both 56.07 33.13 63.41 22.14
Table 6: Translation results, Zh to En. BLEU=BLEUr4n4
alignment TER BLEUr1n4
ME-seq 35.02 69.94
corr(Me-seq ) 33.10 71.20
Table 7: Translation results, En to It
are poorly suited for the HMM and ME-seq mod-
els used in this comparison because of the chosen
directionality: the source (Arabic, Chinese) words
are the states and the target (English) words are the
observation. The HMM is able to produce these
links only by the use of posterior probabilities, rather
than viterbi decoding. The ME-seq model only pro-
duces these links because of language-specific post-
processing. GIZA++ has an underlying sequential
model, but uses both directionalities. The correc-
tion model improved performance across all three of
these links structures. The single exception is that
the number of 2?1 false alarms increased (Zh-En
alignments) but in this case, the first pass ME-seq
alignment produced few false alarms because it sim-
ply proposed few links of this form. It is also notable
that 1?2 links are more numerous than 2?1 links,
in both language pairs. This is consequence of the
choice of directionality and tokenization style.
6.5 Translation Impact
We tested the impact of improved alignments on
the performance of a phrase-based translation sys-
tem (Ittycheriah and Roukos, 2007) for three lan-
896
guage pairs. Our alignment did not improve the
performance of a mature Arabic to English trans-
lation system, but two notable successes were ob-
tained: Chinese to English, and English to Italian.
It is well known that improved alignment perfor-
mance does not always improve translation perfor-
mance (Fraser and Marcu, 2007). A mature machine
translation system may incorporate alignments ob-
tained from multiple aligners, or from both direc-
tions of an asymmetric aligner. Furthermore, with
large amounts of training data (the Gale Phase 4
Arabic English corpus consisting of 8 ? 106 sen-
tences,) a machine translation system is subject to
a saturation effect: correcting an alignment may
not yield a significant improvement because the the
phrases learned from the correct alignment have al-
ready been acquired in other contexts.
For the Chinese to English translation system (ta-
ble 6) the training corpus consisted of 11? 106 sen-
tence pairs, subsampled to 106. The test set was
NIST MT08 Newswire, consisting of 691 sentences
and 4 reference translations. Corpus-level perfor-
mance (columns 2 and 3) improved when measured
by BLEU, but not by TER. Performance on the
most difficult sentences (near the 90th percentile,
columns 4 and 5) improved on both BLEU and TER
(Snover et al, 2006), and the improvement in BLEU
was larger for the more difficult sentences than it
was overall. Translation performance further im-
proved, by a smaller amount, using bothME-seq and
corr(ME-seq) alignments during the training.
The improved alignments impacted the transla-
tion performance of the English to Italian transla-
tion system (table 7) even more strongly. Here the
training corpus consisted of 9.4?106 sentence pairs,
subsampled to 387000 pairs. The test set consisted
of 7899 sentences. Overall performance improved
as measured by both TER and BLEU (1.26 points.)
7 Conclusions
A log linear model for the alignment matrix is used
to guide systematic improvements to an existing
aligner. Our system models arbitrary alignment ma-
trices and allows features that incorporate such in-
formation as correlations based on parse trees in
both languages. We train models to correct the er-
rors of several existing aligners; we find the resulting
models are robust to using different aligners as start-
ing points. Improvements in alignment F -measure,
often significant improvements, show that our model
successfully corrects input alignments from existing
models in all nine language pairs tested. The result-
ing Chinese-English and English-Italian word align-
ments also improved translation performance, espe-
cially on the English-Italian test, and notably on the
particularly difficult subset of the Chinese sentences.
Future work will assess its impact on translation for
the other language pairs, as well as its impact on
other tasks, such as named entity projection.
8 Acknowledgements
We would like to acknowledge the support of
DARPA under Grant HR0011-08-C-0110 for fund-
ing part of this work. The views, opinions, and/or
findings contained in this article/presentation are
those of the author/presenter and should not be in-
terpreted as representing the official views or poli-
cies, either expressed or implied, of the Defense Ad-
vanced Research Projects Agency or the Department
of Defense.
References
Necip Fazil Ayan, Bonnie J. Dorr, and Christof Monz.
2005. Alignment link projection using transformation-
based learning. In Proceedings of the conference on
Human Language Technology and Empirical Methods
in Natural Language Processing, HLT ?05, pages 185?
192, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Yassine Benajiba and Imed Zitouni. 2010. Enhanc-
ing mention detection using projection via aligned
corpora. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
EMNLP ?10, pages 993?1001. Association for Com-
putational Linguistics.
Phil Blunsom and Trevor Cohn. 2006. Discriminative
word alignment with conditional random fields. In In
Proc. of ACL-2006, pages 65?72.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematic
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Yonggang Deng and Bowen Zhou. 2009. Optimizing
word alignment combination for phrase table training.
In Proceedings of the ACL-IJCNLP 2009 Conference
897
Short Papers, ACLShort ?09, pages 229?232, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
B. Efron and R. Tibshirani. 1986. Bootstrap meth-
ods for standard errors, confidence intervals, and other
measures of statistical accuracy. Statistical Science,
1(1):pp. 54?75.
Victoria Fossum, Kevin Knight, and Steven Abney. 2008.
Using syntax to improve word alignment precision for
syntax-based machine translation. In Proceedings of
the Third Workshop on Statistical Machine Transla-
tion, StatMT ?08, pages 44?52. Association for Com-
putational Linguistics.
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine transla-
tion. Comput. Linguist., 33(3):293?303.
Niyu Ge. 2004. Improvement in word alignments. In
DARPA/TIDES MT workshop.
Abraham Ittycheriah and Salim Roukos. 2005. A maxi-
mum entropy word aligner for arabic-english machine
translation. In HLT-EMNLP, pages 89?96.
Abraham Ittycheriah and Salim Roukos. 2007. Direct
translation model 2. In Human Language Technolo-
gies 2007: The Conference of the NA-ACL, pages 57?
64, Rochester, New York, April. Association for Com-
putational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL.
Young-Suk Lee. 2004. Morphological analysis for sta-
tistical machine translation. In Proceedings of HLT-
NAACL 2004: Short Papers on XX, HLT-NAACL ?04,
pages 57?60. Association for Computational Linguis-
tics.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the main con-
ference on Human Language Technology Conference
of the North American Chapter of the Association of
Computational Linguistics, pages 104?111. Associa-
tion for Computational Linguistics.
Dong C. Liu and Jorge Nocedal. 1989. On the lim-
ited memory bfgs method for large scale optimization.
Mathematical Programming, 45:503?528.
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-linear
models for word alignment. In ACL ?05: Proceedings
of the 43rd Annual Meeting on Association for Com-
putational Linguistics, pages 459?466. Association for
Computational Linguistics.
Robert C. Moore, Wen-tau Yih, and Andreas Bode. 2006.
Improved discriminative bilingual word alignment. In
ACL-44: Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, pages 513?520. Association for Computa-
tional Linguistics.
Robert C. Moore. 2005. A discriminative framework for
bilingual word alignment. In In Proceedings of HLT-
EMNLP, pages 81?88.
Jan Niehues and Stephan Vogel. 2008. Discrimina-
tive word alignment via alignment matrix modeling.
In Proceedings of the Third Workshop on Statistical
Machine Translation, pages 18?25, Columbus, Ohio,
June. Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Adwait Ratnaparkhi. 1999. Learning to parse natu-
ral language with maximum entropy models. Mach.
Learn., 34:151?175, February.
Hendra Setiawan, Chris Dyer, and Philip Resnik. 2010.
Discriminative word alignment with a function word
reordering model. In Proceedings of the 2010 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?10, pages 534?544. Association for
Computational Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linea
Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In
Proceedings of Association for Machine Translation in
the Americas.
Ben Taskar, Simon Lacoste-julien, and Dan Klein. 2005.
A discriminative matching approach to word align-
ment. In In Proceedings of HLT-EMNLP, pages 73?
80.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of the 16th conference on Com-
putational linguistics, pages 836?841.
David Yarowsky, Grace Ngai, and Richard Wicentowski.
2001. Inducing multilingual text analysis tools via ro-
bust projection across aligned corpora. In Proceedings
of the first international conference on Human lan-
guage technology research, HLT ?01, pages 1?8. As-
sociation for Computational Linguistics.
898
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 501?512,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Anchor Graph:
Global Reordering Contexts for Statistical Machine Translation
Hendra Setiawan ?
IBM Research
1101 Kitchawan Road
NY 10598, USA
Bowen Zhou
IBM Research
1101 Kitchawan Road
NY 10598, USA
Bing Xiang ?
Thomson Reuters
3 Times Square
NY 10036, USA
Abstract
Reordering poses one of the greatest chal-
lenges in Statistical Machine Translation re-
search as the key contextual information may
well be beyond the confine of translation units.
We present the ?Anchor Graph? (AG) model
where we use a graph structure to model
global contextual information that is crucial
for reordering. The key ingredient of our AG
model is the edges that capture the relation-
ship between the reordering around a set of
selected translation units, which we refer to as
anchors. As the edges link anchors that may
span multiple translation units at decoding
time, our AG model effectively encodes global
contextual information that is previously ab-
sent. We integrate our proposed model into a
state-of-the-art translation system and demon-
strate the efficacy of our proposal in a large-
scale Chinese-to-English translation task.
1 Introduction
Reordering remains one of the greatest challenges
in Statistical Machine Translation (SMT) research as
the key contextual information may span across mul-
tiple translation units.1 Unfortunately, previous ap-
proaches fall short in capturing such cross-unit con-
textual information that could be critical in reorder-
ing. For example, state-of-the-art translation mod-
els, such as Hiero (Chiang, 2005) or Moses (Koehn
et al, 2007), are good at capturing local reordering
within the confine of a translation unit, but their for-
mulation is approximately a simple unigram model
? This work was done when the authors were with IBM.
1We define translation units as phrases in phrase-based SMT
or as translation rules in syntax-based SMT.
over derivation (a sequence of the application of
translation units) with some aid from target language
models. Moving to a higher order formulation (say
to a bigram model) is highly impractical for several
reasons: 1) it has to deal with a severe sparsity issue
as the size of the unigram model is already huge;
and 2) it has to deal with a spurious ambiguity issue
which allows multiple derivations of a sentence pair
to have radically different model scores.
In this paper, we develop ?Anchor Graph? (AG)
where we use a graph structure to capture global
contexts that are crucial for translation. To circum-
vent the sparsity issue, we design our model to rely
only on contexts from a set of selected translation
units, particularly those that appear frequently with
important reordering patterns. We refer to the units
in this special set as anchors where they act as ver-
tices in the graph. To address the spurious ambigu-
ity issue, we insist on computing the model score for
every anchors in the derivation, including those that
appear inside larger translation units, as such our AG
model gives the same score to the derivations that
share the same reordering pattern.
In AG model, the actual reordering is modeled
by the edges, or more specifically, by the edges? la-
bels where different reordering around the anchors
would correspond to a different label. As detailed
later, we consider two distinct set of labels, namely
dominance and precedence, reflecting the two domi-
nant views about reordering in literature, i.e. the first
one that views reordering as a linear operation over
a sequence and the second one that views reordering
as a recursive operation over nodes in a tree struc-
ture The former is prevalent in phrase-based con-
text, while the latter in hierarchical phrase-based and
501
syntax-based context. More concretely, the domi-
nance looks at the anchors? relative positions in the
translated sentence, while the precedence looks at
the anchors? relative positions in a latent structure,
induced via a novel synchronous grammar: Anchor-
centric, Lexicalized Synchronous Grammar.
From these two sets of labels, we develop two
probabilistic models, namely the dominance and the
orientation models. As the edges of AG link pairs
of anchors that may appear in multiple translation
units, our AG models are able to capture high or-
der contextual information that is previously absent.
Furthermore, the parameters of these models are es-
timated in an unsupervised manner without linguis-
tic supervision. More importantly, our experimental
results demonstrate the efficacy of our proposed AG-
based models, which we integrate into a state-of-the-
art syntax-based translation system, in a large scale
Chinese-to-English translation task. We would like
to emphasize that although we use a syntax-based
translation system in our experiments, in principle,
our approach is applicable to other translation mod-
els as it is agnostic to the translation units.
2 Anchor Graph Model
Formally, an AG consists of {A,L} where A is a
set of vertices that correspond to anchors, while L
is a set of labeled edges that link a pair of anchors.
In principle, our AG model is part of a transla-
tion model that focuses on the reordering within the
source sentence F and its translation E. Thus, we
start by first introducing A into a translation model
(either word-based, phrase-based or syntax-based
model) followed by L. Given an F , A is essentially
a subset of non-overlapping (word or phrase) units
that make up F . As the information related to A is
not observed, we introduce A as a latent variable.
Let P (E,? |F ) be a translation model where ?
corresponds to the alignments between units in F
and E. 2 We introduce A into a translation model,
2Alignment (?) represents an existing latent variable. De-
pending on the translation units, it can be defined at different
level, i.e. word, phrase or hierarchical phrase. As during trans-
lation, we are interested in the anchors that appear inside larger
translation units, we set ? at word level, which information can
be induced for (hierarchical) phrase units by either keeping the
word alignment from the training data inside the units or infer-
ring it via lexical translation probability. We use the former.
as follow:
P (E,? |F ) =
?
?A?
P (E,?,A?|F ) (1)
P (E,?,A?|F ) = P (E,? |A?, F )P (A?) (2)
As there can be many possible subsets of F and
summing over all possibleA is intractable, we make
the following approximation for P (A?) such that we
only need to consider one particular A?: P (A?) =
?(A? = A?) which returns 1 only for A?, otherwise
0. The exact definition of the heuristic will be de-
scribed in Section 7, but in short, we equateA? with
units that appear frequently with important reorder-
ing patterns in training data.
Given an A?, we then introduce the edges of AG
(L) into the equation as follow:
P (E,? |A?, F ) = P (E,?,L|A?, F ) (3)
Note that L is also a latent variable but its values are
derived deterministically from (F,E,?) and A?,
thus no extra summation is present in Eq. 3.
Then, we further simplify Eq. 3 by factorizing it
with respect to each individual edges, as follow:
P (E,?,L|A?, F ) ?
?
?am,an?A?
m<n
P (Lm,n|am, an) (4)
where Lm,n ? L corresponds to the label of an edge
that links am and an.
In principle, Lm,n can take any arbitrary value.
For addressing the reordering challenge, it should
ideally correspond to some aspect of the reordering
around am and an, for example, how the reorder-
ing around am affects the reordering around an. As
mentioned earlier, we choose to associate Lm,n with
the dominance and the precedence relations between
am and an, where the former looks at the relative po-
sitions of the two anchors when they are projected
into a latent tree structure, while the latter looks at
their relative positions when they are projected into
the target sentence. We illustrate the two in Fig. 1.
Furthermore, we assume that dominance and
precedence are independent and develop one model
for each, resulting in the dominance and the orien-
tation models, which we describe in Section 3 and 4
respectively. To make the model more compact, we
502
introduce an additional parameterO that restricts the
maximum order of AG as follows:
?
O?
o=1
|A?|+o?1?
i=0
Po(Li?o,i|ai?o, ai) (5)
Thus, we only consider edges that link two anchors
that are at most O? 1 anchors apart. For O = 1, the
AG model only considers relations between neigh-
boring anchors. Following the standard practice in
the n-gram language modeling, we append O num-
ber of pseudo anchors at the beginning and at the end
of F , which represent the sentence delimiter mark-
ers. We do so in a monotone order.
Figure 1: The illustration of the dominance and the prece-
dence relations. The former looks at the anchors? pro-
jection on a derivation structure. The latter looks at the
anchors? projection on the translated sentence.
3 Dominance Model
This section describes our dominance model where
we equate Lm,n in Eq. 4 with dom(am, an) that ex-
presses to the dominance relation between am and
an in a latent tree structure. Due to reordering, an-
chors can only appear in specific nodes. We first
describe a novel formalism of Anchor-centric, Lexi-
calized Synchronous Grammar (AL-SG), used to in-
duce the tree structure and then discuss the proba-
bilistic formulation of the model. Just to be clear,
we introduce AL-SG mainly to facilitate the compu-
tation of dom(am, an). The actual translation model
at decoding time remains either phrase-based, hier-
archical phrase-based or syntax-based model.
3.1 Anchor-centric, Lexicalized Synchronous
Grammar
Given (F,E,?) and A, Anchor-centric, Lexical-
ized Synchronous Grammar (AL-SG) produces a
tree structure where the nodes are decorated with
anchors-related information. As the name alludes,
the core of AL-SG is anchor-centric constituents
(ACC), which corresponds to nodes, composed from
merging anchors with by either their left, their right
neighboring constituents or both.
More concretely, first of all, we consider a span
on the source sentence F to be a constituent if it is
consistent with the alignment (?). Second of all, we
can construct a larger constituent by merging smaller
constituents given that the larger constituent is also
consistent with the alignment. These two constraints
are similar to the heuristic applied to extract hierar-
chical phrases (Chiang, 2005).
Then, specific to AL-SG, we consider an anchor a
to lexicalize a constituent c, if: a) we can compose c
from at most three smaller constituents: cL, a and cR
where a is the anchor while cL,cR are the (possibly
empty) constituents immediately to the left and to
the right of a; and b) we can create smaller anchors-
centric constituents from concatenating a with cL
and a with cR. If a can lexicalize c, then the node
associated with c would be marked with a. In com-
puting dom(am, an), we look at the constituents that
cover both anchors and check whether the anchors
can lexicalized any of such constituents.
Now, we will describe AL-SG in a formal way.
For simplicity, we use a simple grammar, called In-
version Transduction Grammar (ITG) (Wu, 1997),
although in practice, we handle a more powerful
synchronous grammar. Hence, we proceed to de-
scribe Anchor-centric, Lexicalized ITG (AL-ITG).
An AL-ITG is a quadruple {?,A,V,R} where:
? ? = {(f/e)} is a set of terminal symbols,
which represents all possible units defined over
(F,E,?) where each pair corresponds to a link
in ?. We define ? at the most fine-grained
level (i.e. word-level), as we insist on comput-
ing model score for each anchors even if they
appear inside larger units.
? A ? ? is a set of anchors, which is a subset of
the terminal symbols.
? V = {{P,X, Y } ? {A, ?}} is a set of (possi-
bly lexicalized) nonterminal symbols. P rep-
resents the terminal symbols (?); while X and
Y correspond to the spans that are created from
merging two adjacent constituents. On the tar-
503
Figure 2: An illustration of an aligned Chinese-English sentence pair with one possible AL-ITG derivation obtained
by applying the grammar in a left-to-right fashion. Circles represent alignment points. Black circle represents the
anchor; boxes represent the anchor?s neighbors. In the derivation tree, the anchors are represented by their position
and in bold. For succinctness, we omit the preterminal rules in the tree.
get side, for X , the order of the two children
follows the source order, while for Y , the or-
der follows the inverse. Nonterminal symbols
can be lexicalized with zero or more than one
anchor. We represent a lexicalized constituent
as a nonterminal symbol followed by a bracket
which contains the lexicalizing anchors, e.g.
P (H) where H is the anchors lexicalizing P .
? R is a set of production rules which can be clas-
sified into the following categories:
? Preterminal rules. We propagate the sym-
bol if it corresponds to an anchor.
P (H = f/e)? f/e, if f/e ? A?
P (H = ?)? f/e, otherwise
? Monotone production rules, which reorder
the children in monotone order, denoted
by square brackets (?[?,?]?).
X(H1 ?H2)? [P (H1)P (H2)]
X(H1 ?H2)? [X(H1)P (H2)]
X(H1 ?H2)? [X(H1)X(H2)]
X(H1)? [X(H1)Y (H2)]
X(H2)? [Y (H1)P (H2)]
X(H2)? [Y (H1)X(H2)]
X(?)? [Y (H1)Y (H2)]
? Inverse production rules, which reorder
the children in the inverse order, denoted
by angle brackets (???,???).
Y (H1 ?H2)? ?P (H1)P (H2)?
Y (H1 ?H2)? ?Y (H1)P (H2)?
Y (H1 ?H2)? ?Y (H1)Y (H2)?
Y (H1)? ?Y (H1)X(H2)?
Y (H2)? ?X(H1)P (H2)?
Y (H2)? ?X(H1)Y (H2)?
Y (?)? ?X(H1)X(H2)?
Like ITG, AL-ITG only permits two kind of re-
ordering operations, namely monotone and inverse.
To accommodate the lexicalization, we first assign
a unique nonterminal symbol for each, i.e. X for
monotone reordering and Y for inverse reordering.
Then, we lexicalize Xs and Y s with anchors as long
as they satisfy the constraint that the child shares the
same label as the parent. This constraint guarantees
that the constituents are valid ACCs. It also enables
the anchors to lexicalize long constituents, although
the terminal symbols are defined at word-level.
Fig. 2 illustrates an example Chinese-to-English
translation with a AL-ITG derivation when the
grammar is applied in a left-to-right fashion. Admit-
tedly, AL-ITG (or more generally AL-SG) is suscep-
tible to spurious ambiguity as it produces multiple
derivation trees for a given (F,E,?). Fortunately,
the value of dom(am, an) is identical for all deriva-
tions, since the computation of dom(am, an) relies
504
only on whether am and an can lexicalize at least
one constituent that covers both anchors. Hence,
we only need to look at one derivation to compute
dom(am, an). Generalizing AL-ITG to a more pow-
erful formalism is trivial; we just need to forbid the
propagation for non-binarizeable production rules.
3.2 Probabilistic Model
We read-off the dominance relations dom(am, an)
from D obtained from the application of AL-SG to
(F,E,?). As lexicalization is a bottom-up process,
for reading-off dom(am, an), it is sufficient to look
at the lowest common ancestor (LCA) of both an-
chors; if the anchors cannot lexicalize the LCA, they
won?t be able to lexicalize the constituents larger
than LCA. To be more concrete, let?s consider theD
in Fig. 2. In that D, the LCA of am = yu3/with10
and an = de7/that7 is Y5(7). Then, we check the
anchors that can lexicalize the LCA. Let V (H) be
the LCA, then dom(am, an) ?
(LH) , if am ? H ? an 6? H
(RH) , if am 6? H ? an ? H
(BL) , if am ? H ? an ? H
(BD) , if am 6? H ? an 6? H
The value refers to cases where am and an can
lexicalize V (H) and it is useful to model spans
that share a simple, uniform reordering, i.e. all-
monotone or all-inverse, while the value refers to
the cases where am and an cannot lexicalize V (H)
and it is useful to model spans that involve in a com-
plex reordering. Meanwhile, the and refer to cases
where only one anchor can lexicalize V (H), i.e. am
and an respectively. These values are useful for
modeling cases where the surroundings of the two
anchors exhibit different kind of reordering pattern.
With such definition, the edge labels L in Fig. 2
are indicated in Table 1. Note that in Table 1, we
don?t specify the relations involving pseudo anchors,
although they are crucial.
The final probabilistic formulation of the domi-
nance model is as follows:
?
O?
o=1
|A|+o?1?
i=0
Pdomo(dom(ai?o, ai)|ai?o, ai) (6)
As shown, we allocate a separate model Pdomo for
each separate order (o) where each Pdomo will con-
HHH
HHn
m
1 2 3 4 5
1 = (shi2/is2) - - - - -
2 = (yu3/with10) LH - - - -
3 = (you5/have8) LH BD - - -
4 = (de7/that7) LH RH RH - -
5 = (zhi10/of4) LH RH RH BL -
Table 1: The dominance relations between pairs of an-
chors according to the derivation in Fig. 2.
tribute as one additional feature in the log-linear
model of the translation model. In allocating a sep-
arate model for each o, we conjecture that different
pair of anchors contributes differently depending on
how far the two anchors are.
4 Orientation Model
In this section, we introduce the orientation model
(ori) where we equate Lm,n with the precedence re-
lations between a pair of anchors. Instead of directly
modeling the precedence between the two anchors,
we approximate it by modeling the precedence of
each anchor with its neighboring constituents. For-
mally, we approximate P (Lm,n|am, an) as
PoriR(ori(am,MR(am))|am)?
PoriL(ori(an,ML(an))|an) (7)
where MR(am) is the largest constituent to the right
of the first anchor am, ML(an) the largest con-
stituent to the left of the second anchor an, and ori()
a function that maps the anchor and the neighboring
constituent to a particular orientation.
Plugging Eq. 7 into Eq. 5 results in the following
approximation of P (?|A):
C.
|A|?1?
i=0
{PoriL(ori(ai,ML(ai))|ai)?
PoriR(ori(ai,MR(ai))|ai)}
O (8)
where C is a constant term related to the pseudo an-
chors and O is the maximum order of the AG. In
practice, we can safely ignore both C and O as they
are constant for a given AG. As shown, the orienta-
tion model is simplified into a model that looks at the
reordering of the anchors? neighboring constituents.
The exact definition of ML and MR will be
discussed in Section 5. Their orientation, i.e.
505
oriL(CL, a) and oriR(CR, a) respectively, may take
one of the following four values: (MA), (RA), (MG)
and (RG). The first clause (monotone, reverse) in-
dicates whether the target order follows the source
order; the second (adjacent, gap) indicates whether
the anchor and its neighboring constituent are adja-
cent or separated by an intervening when projected.
5 Parameter Estimation
For each (F,E,?), the training starts with the iden-
tification of the regions in the source sentences as
anchors (A). For our Chinese-English experiments,
we use a simple heuristic that equates anchors (A?)
with constituents whose corresponding word class
belongs to function words-related classes, bearing
a close resemblance to (Setiawan et al, 2007). In
total, we consider 21 part-of-speech tags; some of
which are as follows: VC (copula), DEG, DEG,
DER, DEV (de-related), PU (punctuation), AD (ad-
jectives) and P (prepositions).
5.1 Extracting Events from (F,E,?)
The parameter estimation first involves extracting
two statistics from (F,E,?), namely dom(am, an)
for the dominance model as well as ori(a,ML(a))
and ori(a,MR(a)) for the orientation model. In-
stead of developing a separate algorithm for each,
we describe a unified way to extract these statistics
via the largest neighboring constituents of the an-
chors, i.e. ML(a) and MR(a). This approach en-
ables the dominance model to share the same resid-
ual state information as the orientation model.3
Let am be an anchor and MR(am) be its largest
neighboring constituent to the right. Let an be
an anchor to the left of am and ML(an) be an?s
largest neighboring constituent to the left. Ac-
cording to AL-SG, we say that am dominates an
if ori(am,MR(am)) ? {MA,RA} and an ?
MR(am). By the same token, we say that an dom-
inates am if ori(an,ML(an)) ? {MA,RA} and
am ? ML(an). The constraints on the orientation
reflect the fact that in AL-SG, anchors can only be
propagated through monotone or inverse production
rules, which correspond to the MA and RA respec-
tively. The fact that we are looking at the largest
3The analogy in an n-gram language model is the first n?1
words of the hypothesis that have incomplete history.
neighboring constituents guarantees that if the other
anchor is outside that constituent, then that other an-
chor is never dominated.
More formally, given an aligned sentence pair
? = (F,E,?), let ?(?) be all possible con-
stituents that can be extracted from ?:4
{(f j2j1/e
i2
i1) :?(j, i) ??: ((j1? j? j2) ? (ii? i? i2))
?(?(j1? j? j2) ? ?(ii? i? i2))
Then, let the anchors A be a subset of ?(?).
Given A ? ?(?), let a = (f j2j1/e
i2
i1) ? A be a par-
ticular anchor. And, let CL(a) ? ?(?) be a?s left
neighbors and let CR(a) ? ?(?) be a?s right neigh-
bors, iff:
?CL = (f
j4
j3/e
i4
i3) ? CL(a) : j4 + 1 = j1
?CR = (f
j6
j5/e
i6
i5) ? CR(a) : j2 + 1 = j5
Then, let ML(a) ? CL(a) and MR(a) ? CR(a) be
the largest left and right neighbors according to:
ML(a) = arg max
(f
j4
j3
/e
i4
i3
)?CL(a)
(j4 ? j3)
MR(a) = arg max
(f
j6
j5
/e
i6
i5
)?CR(a)
(j6 ? j5)
Let ML = (f
j4
j3/e
i4
i3) and MR = (f
j6
j5/e
i6
i5).
We then proceed to extract oriL(a,ML(a)) and
oriR(a,MR(a)) respectively as follows:
? MA, if (i4 +1) = i1 for oriL or if (i2 +1) = i5
for oriR
? RA, if (i2 + 1) = i3 for oriL or if (i6 + 1) = i1
for oriR
? MG, if (i4 +1) < i1 for oriL or if (i2 +1) < i5
for oriR
? RG, if (i2 + 1) < i3 for oriL or if (i6 + 1) < i1
for oriR.
Then, we proceed to extract dom(am, an). Given
two anchors am, an where m < n, we define the
4We represent a constituent as a source and target phrase
pair (f j2j1/e
i2
i1
) where the subscript and the superscript indicate
the starting and the ending indices as such f j2j1 denotes a source
phrase that spans from j1 to j2.
506
dominance relation between am and an viaMR(am)
and ML(an). Let am = (f
j2
j1/e
i2
i1), MR(am) =
(f j4j3/e
i4
i3), an = (f
j6
j5/e
i6
i5) and ML(an) = (f
j8
j7/e
i8
i7).
Then, ldom(am, an) is true only if (j4 ? j6)
and oriR(am,MR(am)) ? {MA,RA}. Simi-
larly, rdom(am, an) is true only if (j7 ? j1) and
oriL(an,ML(an)) ? {MA,RA}.
Hence, dom(am, an) is as follows:
? LH, if ldom(am, an) ? ?rdom(am, an)
? RH, if ?ldom(am, an) ? rdom(am, an)
? BL, if ldom(am, an) ? rdom(am, an)
? BD, if ?ldom(am, an) ? ?rdom(am, an)
5.2 Parameterization and Training
After extracting events, we are now ready to train
the models. To estimate them, we train a discrimi-
native classifier for each model and use the normal-
ized posteriors at decoding time as additional feature
scores in SMT?s log-linear framework.
At a high level, we use a rich set of binary fea-
tures ranging from lexical to part-of-speech (POS)
and to syntactic features. Additionally, we augment
the feature set with compound features, e.g. a con-
junction of the source word of the left anchor and the
source word of the right anchor. Although they in-
crease the number of features significantly, we found
that they are empirically beneficial.
Suppose a = (f j2j1 /e
i2
i1), ML(a) = (f
j4
j3 /e
i4
i3) and
MR(a) = (f
j6
j5 /e
i6
i5), then based on the context?s
location, the elementary features employed in our
classifiers can be categorized into:
? anchor-related: (the actual word of f j2j1 ),
(part-of-speech (POS) tag of ), (?s parent in the
parse tree), (ei2i1?s actual target word).
? surrounding: (the previous word / f j1?1j1?1 ), (the
next word / f j2+1j2+1 ), (?s POS tag), (?s POS tag),
(?s parent), (?s parent).
? non-local: (the previous anchor?s source word)
, (the next anchor?s source word), (?s POS tag),
(?s POS tag).
There is a separate set of elementary features for am
and an and we come up with manual combination to
construct compound features.
In training the models, we manually come up with
around 30-50 types of features, which consists of a
combination of elementary and compound features.
Due to space constraints, we will describe the ac-
tual features that we use and the classification per-
formance of our models elsewhere. In total, we
generate around one hundred millions binary fea-
tures from our training data that contains six million
sentence pairs. To reduce the number of features,
we employ the L1-regularization in training to en-
force sparse solutions, using the off-the-shelf LIB-
LINEAR toolkit (Fan et al, 2008). After training,
the number of features in our classifiers decreases to
below 1 million features for each classifier.
6 Decoding
As mentioned earlier, we wish to avoid the spuri-
ous ambiguity issue where different derivations have
radically different scores although they lead to the
same reordering. This section describes our decod-
ing algorithm that avoids spurious ambiguity issue
by incrementally constructing MLs and MRs thus
allowing the computation of the models over partial
hypotheses.
In our experiments, we integrate our dominance
model as well as our orientation model into a syntax-
based SMT system that uses SCFG formalism. In-
tegrating the models into syntax-based SMT sys-
tems is non-trivial, especially since the anchors of-
ten reside within translation rules and the model
doesn?t always decompose naturally with the hy-
pothesis structure. To facilitate that, we need to
first induce the necessary alignment for all transla-
tion units in the hypothesis.
To describe the algorithm, let us consider a cheat-
ing exercise where we have to translate the Chinese
sentence in Fig. 2 with the following set of hierar-
chical phrases:
Xa??Aozhou
1shi2X1,Australia
1 is2X1?
Xb??yu
3 Beihan4X1, X1with
3 North4 Korea?
Xc??you
5bangjiao6, have5dipl.6 rels.?
Xd??X1 de
7shaoshu8 guojia9 zhi10 yi11,
one11of10the few8 countries9 that7X1?
As a case in point, let us consider D = Xa ? Xb
? Xd ? Xc, which will lead to the correct English
507
Target string (w/ source index) Symbol(s) read Op. Stack(s)
(1) Xc have
5 dipl.6 rels. [5][6] S,S,R Xc:[5-6]
(2) Xd one11 of
10 few8 countries9 [11][10] S,S,R [10-11]
that7 Xc
(3) [8][9] S,S,R,R [8-11]
(4) [7] S [8-11][7]
(5) Xc:[5,6] S Xd:[8-11][7][5,6]
(6) Xb Xd with
3 North4 Korea Xd:[8-11][7][5,6] S [8-11][7][5,6]
(7) [3][4] S,S,R,R Xb:[8-11][7][3-6]
(8) Xa Australia
1 is2 Xb [1][2] S,S,R [1-2]
(9) Xb:[8-11][7][3,6] S,A Xa:[1-2][8-11][7][3,6]
Table 2: The application of the shift-reduce parsing algorithm, which corresponds to the following derivation D =
Xa ? Xb ? Xd ? Xc. Anchor is in bold. In column Op., S, R and A refer to shift, reduce and accept operation
respectively.
translation as in Fig. 2. Note that the translation
rules contain internal word alignment, which we as-
sume to have been previously inferred.
The algorithm bears a close resemblance to the
shift-reduce algorithm found in phrase-based decod-
ing (Galley and Manning, 2008; Feng et al, 2010;
Cherry et al, 2012). A stack is used to accumulate
(partial) information about a, ML and MR for each
a ? A in the derivation. This algorithm takes an in-
put stream and applies either the shift or the reduce
operations starting from the beginning until the end
of the stream. The shift operation advances the input
stream by one symbol and push the symbol into the
stack; while the reduce operation applies some rule
to the top-most elements of the stack. The algorithm
terminates at the end of the input stream where the
resulting stack will be propagated to the parent for
the later stage of decoding. In our case, the input
stream is the target string of the rule and the symbol
is the corresponding source index of the elements of
the target string. The reduction rule looks at two in-
dices and merge them if they are adjacent (i.e. has
no intervening phrase). We forbid the application
of the reduction rule to anchors. Table 2 shows the
execution trace of the algorithm for the derivation
described earlier. For conciseness, we assume that
there is only one anchor and that is de7/that7.
As shown, the algorithm starts with an empty
stack. It then projects the source index to the corre-
sponding target word and then enumerates the target
string in a left to right fashion. If it finds a target
word with a source index, it applies the shift oper-
ation, pushing the index to the stack. Unless the
symbol corresponds to an anchor, it tries to apply
the reduce operation. Line (4) indicates the special
treatment to the anchor. If the symbol being read
is a nonterminal, then we push the entire stack that
corresponds to that nonterminal. For example, when
the algorithm reads Xd at line (6), it pushes the en-
tire stack from line (5).
As MLs and MRs are being incremen-
tally constructed, we can immediately com-
pute Pdomo(dom(am, an)|am, an) as soon
as a partial derivation covers both am
and an. For example, we can compute
Pdom1(dom(you5/have8, de7/that7) = ),
Pdom1(dom(de7/that7, zhi10/of4) = ) and
Pdom2(dom(you5/have8, zhi10/of4) = ) at
partial hypothesis Xd ? Xc which corresponds to a
constituent spanning from 5-11.
7 Experiments
Our baseline systems is a state-of-the-art string-to-
dependency system (Shen et al, 2008). The sys-
tem is trained on 10 million parallel sentences that
are available to the Phase 1 of the DARPA BOLT
Chinese-English MT task. The training corpora in-
clude a mixed genre of newswire, weblog, broad-
cast news, broadcast conversation, discussion fo-
rums and comes from various sources such as LDC,
HK Law, HK Hansard and UN data.
In total, our baseline model employs more than
50 features, including from our proposed dominance
and orientation models. In addition to the standard
508
Model
newswire weblog newswire+weblog
BLEU TER Comb BLEU TER Comb BLEU TER Comb
(a) (b) (c) (d) (e) (f) (g) (h) (i)
(1) S2D 37.63 53.17 7.77 27.60 57.19 14.77 33.39 54.97 10.79
(2) +dom1 38.12 52.31 7.10 27.56 56.58 14.51 33.64 54.24 10.30
(3) +dom2 38.31 52.28 6.99 27.66 56.57 14.45 33.78 54.20 10.21
(4) +dom3 38.31 52.52 7.10 28.24 56.56 14.16 34.02 54.33 10.15
(5) +dom4 38.54 52.22 6.84 28.38 56.55 14.08 34.20 54.16 9.98
(6) +dom5 38.17 52.57 7.20 28.67 56.27 13.80 34.16 54.27 10.05
(7) +dom6 38.17 52.52 7.18 28.64 56.22 13.79 34.10 54.18 10.04
(8) +ori 38.52 52.43 6.96 28.26 56.54 14.14 34.15 54.27 10.06
(9) +ori+dom1 38.87 52.05 6.59 28.01 56.48 14.23 34.26 54.03 9.89
(10) +ori+dom2 38.96 51.87 6.45 27.98 56.23 14.12 34.29 53.82 9.77
(11) +ori+dom3 39.19 51.77 6.29 28.19 56.15 13.98 34.52 53.73 9.61
(12) +ori+dom4 39.34 51.77 6.21 28.41 56.17 13.88 34.60 53.69 9.54
(13) +ori+dom5 39.31 51.67 6.18 28.62 56.09 13.74 34.76 53.65 9.45
Table 3: The NIST MT08 results on newswire (nw), weblog (wb) and combined genres. S2D is the baseline string-
to-dependency system (line 1). Lines 2-7 shows the results of the dominance model with O = 1 ? 6. Line 8 shows
result on adding ori to the baseline. Lines 9-13 shows the results of the orientation complemented with the dominance
model with varying O. The best BLEU, TER and Comb on each genre of the first set are in italic while those of the
second set are in bold. For BLEU, higher scores are better, while for TER and Comb, lower scores are better.
features such as translation probabilities, we incor-
porate features that are found useful for developing
a state-of-the-art baseline, such as the provenance
features (Chiang et al, 2011). We use a 6-gram
language model, which was trained on 10 billion
English words from multiple corpora, including the
English side of our parallel corpus plus other cor-
pora such as Gigaword (LDC2011T07) and Google
News. We also train a class-based language model
(Chen, 2009) on two million English sentences se-
lected from the parallel corpus. As for our string-to-
dependency system, we train 3-gram models for left
and right dependencies and unigram for head using
the target side of the parallel corpus. To train our
models, we select a set of 5 million sentence pairs.
For the tuning and development sets, we set
aside 1275 and 1239 sentences selected from
LDC2010E30 corpus. We tune the feature weights
with PRO (Hopkins and May, 2011) to minimize
(TER-BLEU)/2 metric. As for the blind test set,
we report the performance on the NIST MT08 eval-
uation set, which consists of 691 sentences from
newswire and 666 sentences from weblog. We pick
the weights that produce the highest development set
scores to decode the test set.
We perform two sets of experiments. The first set
looks at the contribution of the dominance model
with varying values of o. The second one looks at
the combination of the dominance model and the
orientation model. Table 3 summarizes the experi-
mental results on NIST MT08 sets, categorized by
genres. We report the results on newswire genre in
columns a-c, those on weblog genre in column d-f,
and those on mixed genre in column g-i. The perfor-
mance of our baseline string-to-dependency syntax-
based SMT is shown in the first line.
Lines 2-7 in Table 3 show the results of our first
set of experiments, starting from the result of dom1,
which looks at only at pairs of adjacent anchors, to
the result of dom6, which looks at pairs of anchors
that are at most 5 anchors away. As shown in line
2, our dominance model provides a nice improve-
ment of around 0.5 point over the baseline even if it
only looks at restricted context. Increasing the or-
der of our dominance model provides an additional
gain. However, the gain is more pronounced in the
weblog genre (up to around 1 BLEU point) than in
the newswire genre. We conjecture that this may be
the artifact of our tune set, which comes from the
weblog genre. We stop at dom6 because we observe
509
that the weight of the feature score that corresponds
to the maximum order (o = 6) has a negative sign,
which often indicates a high correlation between the
new features and existing ones.
Lines 8-13 in Table 3 shows the results of our sec-
ond set of experiments. Line 8 shows the result of
adding the orientation model (ori) to the baseline
system. As shown, integrating ori shows a signifi-
cant gain. On top of which, we then integrate dom1
to dom5. We see a very encouraging result as adding
the dominance model increases the performance fur-
ther, consistently over different value of o. This sug-
gests that the dominance model is complementary
to the orientation model. Our best result provides
more than 1 BP improvement and 1 TER reduction
consistently over different genres. We see this result
as confirming our intuition that the global contextual
information provided by our AG model can signifi-
cantly improve the performance of SMT even in a
state-of-the-art system.
8 Related Work
Our work intersects with existing work in many dif-
ferent respects. In this section, we mainly focus on
work related to introducing higher-order contextual
information to reordering model.
In providing global contextual information, our
work is related to a large amount of literature. To
name a few, Zens and Ney (2006) improves the lexi-
calized reordering model of Tillman (2004) by in-
corporating part-of-speech information. Chang et
al. (2009) incorporates contexts from syntactic parse
tree. Bach et al (2009) exploits the dependency in-
formation and Xiong et al (2012) uses the predicate-
argument structure.
Vaswani et al (2011) introduces rule markov
models for a forest-to-string model in which the
number of possible derivations is restricted. More
recently, Durrani et al (2013) and Zhang et al
(2013) cast reordering process as a Markov process.
Similar to these models, our proposed model also
provide context dependencies to the application of
translation rules, however, as they focus on mini-
mal translation units (MTU) where we focus on a
selected set of translation units. (Banchs et al, 2005)
introduces a bigram model for monotone phrase-
based system, but their definition of translation units
is suitable only for language pairs with limited re-
ordering, such as translating Spanish to English.
In equating anchors with the function word class,
our work is closely related to the function word-
centered model of Setiawan et al (2007), especially
the orientation model. Our dominance model is
closely related to the reordering model of Setiawan
et al (2009), except that they only look at pair of ad-
jacent anchors, forming a chain structure instead of
a graph like in our dominance model. Furthermore,
we provide a discriminative treatment to the model
to include a richer set of features including syntac-
tic features. This work can be seen as modeling the
identity of the neighboring of the anchors, similar to
(Setiawan et al, 2013). However, instead of looking
at the words at the borders, we look at whether the
neighboring constituents contain other anchors.
9 Conclusion
We propose the ?Anchor Graph? (AG) model to en-
code global contextual information. A selected set
of translation units, which we call anchors, serves
as the vertices of AG. And as the edges, we model
two types of relations, namely the dominance and
the precedence relations, where the former looks at
the positions of the anchors in the derivation struc-
ture, while the latter looks at the positions of the
anchors in the surface structure, resulting into two
probabilistic models over edge labels. As the mod-
els look at the pairs of anchors that go beyond multi-
ple translation units, our AG model provides global
contextual information.
Our AG model embodies (admittedly crudely)
some basic principles of sentence organization,
namely categorization (in categorizing units into an-
chors and non-anchors), linear order (in modeling
the precedence of anchors) and constituency struc-
ture (in modeling the dominance between anchors).
We are encouraged by the facts that we learn these
principles in an unsupervised way and that we can
achieve a significant improvement over a strong
baseline in a large-scale Chinese-to-English trans-
lation task. In the future, we hope to continue this
line of research, perhaps by learning to identify an-
chors automatically from training data or by using
our models to induce derivations directly from un-
aligned sentence pair.
510
Acknowledgements
We would like to acknowledge the support of
DARPA under Grant HR0011-12-C-0015 for fund-
ing part of this work. The views, opinions, and/or
findings contained in this article/presentation are
those of the author/presenter and should not be inter-
preted as representing the official views or policies,
either expressed or implied, of the DARPA.
References
Nguyen Bach, Qin Gao, and Stephan Vogel. 2009.
Source-side dependency tree reordering models with
subtree movements and constraints. In Proceedings of
the Twelfth Machine Translation Summit (MTSummit-
XII), Ottawa, Canada, August. International Associa-
tion for Machine Translation.
Rafael E. Banchs, Josep M. Crego, Adria` de Gispert, Pa-
trik Lambert, and Jose? B. Marin?o. 2005. Statisti-
cal machine translation of Euparl data by using bilin-
gual n-grams. In Proceedings of the ACL Workshop
on Building and Using Parallel Texts, pages 133?136,
Ann Arbor, Michigan, June. Association for Compu-
tational Linguistics.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and
Christopher D. Manning. 2009. Discriminative re-
ordering with Chinese grammatical relations features.
In Proceedings of the Third Workshop on Syntax and
Structure in Statistical Translation (SSST-3) at NAACL
HLT 2009, pages 51?59, Boulder, Colorado, June. As-
sociation for Computational Linguistics.
Stanley Chen. 2009. Shrinking exponential language
models. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 468?476, Boulder, Colorado,
June. Association for Computational Linguistics.
Colin Cherry, Robert C. Moore, and Chris Quirk. 2012.
On hierarchical re-ordering and permutation parsing
for phrase-based decoding. In Proceedings of the
Seventh Workshop on Statistical Machine Translation,
pages 200?209, Montre?al, Canada, June. Association
for Computational Linguistics.
David Chiang, Steve DeNeefe, and Michael Pust. 2011.
Two easy improvements to lexical weighting. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 455?460, Portland, Oregon, USA,
June. Association for Computational Linguistics.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL?05), pages 263?270, Ann
Arbor, Michigan, June. Association for Computational
Linguistics.
Nadir Durrani, Alexander Fraser, and Helmut Schmid.
2013. Model with minimal translation units, but de-
code with phrases. In Proceedings of the 2013 Con-
ference of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 1?11, Atlanta, Georgia, June. As-
sociation for Computational Linguistics.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Yang Feng, Haitao Mi, Yang Liu, and Qun Liu. 2010. An
efficient shift-reduce decoding algorithm for phrased-
based machine translation. In Coling 2010: Posters,
pages 285?293, Beijing, China, August. Coling 2010
Organizing Committee.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 848?856, Honolulu, Hawaii, October. Associa-
tion for Computational Linguistics.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In Proceedings of the 2011 Conference on Empir-
ical Methods in Natural Language Processing, pages
1352?1362, Edinburgh, Scotland, UK., July. Associa-
tion for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation, June.
Hendra Setiawan, Min-Yen Kan, and Haizhou Li. 2007.
Ordering phrases with function words. In Proceed-
ings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 712?719, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Hendra Setiawan, Min Yen Kan, Haizhou Li, and Philip
Resnik. 2009. Topological ordering of function words
in hierarchical phrase-based translation. In Proceed-
ings of the Joint Conference of the 47th Annual Meet-
ing of the ACL and the 4th International Joint Confer-
ence on Natural Language Processing of the AFNLP,
pages 324?332, Suntec, Singapore, August. Associa-
tion for Computational Linguistics.
Hendra Setiawan, Bowen Zhou, Bing Xiang, and Libin
Shen. 2013. Two-neighbor orientation model with
cross-boundary global contexts. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
511
1264?1274, Sofia, Bulgaria, August. Association for
Computational Linguistics.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-08: HLT, pages 577?585, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Christoph Tillman. 2004. A unigram orientation model
for statistical machine translation. In HLT-NAACL
2004: Short Papers, pages 101?104, Boston, Mas-
sachusetts, USA, May 2 - May 7. Association for
Computational Linguistics.
Ashish Vaswani, Haitao Mi, Liang Huang, and David
Chiang. 2011. Rule markov models for fast tree-to-
string translation. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 856?864,
Portland, Oregon, USA, June. Association for Compu-
tational Linguistics.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404, Sep.
Deyi Xiong, Min Zhang, and Haizhou Li. 2012. Model-
ing the translation of predicate-argument structure for
smt. In Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics (Volume 1:
Long Papers), pages 902?911, Jeju Island, Korea, July.
Association for Computational Linguistics.
Richard Zens and Hermann Ney. 2006. Discrimina-
tive reordering models for statistical machine trans-
lation. In Human Language Technology Conference
of the North American Chapter of the Association for
Computational Linguistics (HLT-NAACL): Proceed-
ings of the Workshop on Statistical Machine Transla-
tion, pages 55?63, New York City, NY, June. Associa-
tion for Computational Linguistics.
Hui Zhang, Kristina Toutanova, Chris Quirk, and Jian-
feng Gao. 2013. Beyond left-to-right: Multiple de-
composition structures for smt. In Proceedings of the
2013 Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 12?21, Atlanta, Geor-
gia, June. Association for Computational Linguistics.
512
Proceedings of the ACL 2010 Conference Short Papers, pages 22?26,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Diversify and Combine: Improving Word Alignment for Machine
Translation on Low-Resource Languages
Bing Xiang, Yonggang Deng, and Bowen Zhou
IBM T. J. Watson Research Center
Yorktown Heights, NY 10598
{bxiang,ydeng,zhou}@us.ibm.com
Abstract
We present a novel method to improve
word alignment quality and eventually the
translation performance by producing and
combining complementary word align-
ments for low-resource languages. Instead
of focusing on the improvement of a single
set of word alignments, we generate mul-
tiple sets of diversified alignments based
on different motivations, such as linguis-
tic knowledge, morphology and heuris-
tics. We demonstrate this approach on an
English-to-Pashto translation task by com-
bining the alignments obtained from syn-
tactic reordering, stemming, and partial
words. The combined alignment outper-
forms the baseline alignment, with signif-
icantly higher F-scores and better transla-
tion performance.
1 Introduction
Word alignment usually serves as the starting
point and foundation for a statistical machine
translation (SMT) system. It has received a signif-
icant amount of research over the years, notably in
(Brown et al, 1993; Ittycheriah and Roukos, 2005;
Fraser and Marcu, 2007; Hermjakob, 2009). They
all focused on the improvement of word alignment
models. In this work, we leverage existing align-
ers and generate multiple sets of word alignments
based on complementary information, then com-
bine them to get the final alignment for phrase
training. The resource required for this approach
is little, compared to what is needed to build a rea-
sonable discriminative alignment model, for ex-
ample. This makes the approach especially ap-
pealing for SMT on low-resource languages.
Most of the research on alignment combination
in the past has focused on how to combine the
alignments from two different directions, source-
to-target and target-to-source. Usually people start
from the intersection of two sets of alignments,
and gradually add links in the union based on
certain heuristics, as in (Koehn et al, 2003), to
achieve a better balance compared to using either
intersection (high precision) or union (high recall).
In (Ayan and Dorr, 2006) a maximum entropy ap-
proach was proposed to combine multiple align-
ments based on a set of linguistic and alignment
features. A different approach was presented in
(Deng and Zhou, 2009), which again concentrated
on the combination of two sets of alignments, but
with a different criterion. It tries to maximize the
number of phrases that can be extracted in the
combined alignments. A greedy search method
was utilized and it achieved higher translation per-
formance than the baseline.
More recently, an alignment selection approach
was proposed in (Huang, 2009), which com-
putes confidence scores for each link and prunes
the links from multiple sets of alignments using
a hand-picked threshold. The alignments used
in that work were generated from different align-
ers (HMM, block model, and maximum entropy
model). In this work, we use soft voting with
weighted confidence scores, where the weights
can be tuned with a specific objective function.
There is no need for a pre-determined threshold
as used in (Huang, 2009). Also, we utilize var-
ious knowledge sources to enrich the alignments
instead of using different aligners. Our strategy is
to diversify and then combine in order to catch any
complementary information captured in the word
alignments for low-resource languages.
The rest of the paper is organized as follows.
22
We present three different sets of alignments in
Section 2 for an English-to-Pashto MT task. In
Section 3, we propose the alignment combination
algorithm. The experimental results are reported
in Section 4. We conclude the paper in Section 5.
2 Diversified Word Alignments
We take an English-to-Pashto MT task as an exam-
ple and create three sets of additional alignments
on top of the baseline alignment.
2.1 Syntactic Reordering
Pashto is a subject-object-verb (SOV) language,
which puts verbs after objects. People have pro-
posed different syntactic rules to pre-reorder SOV
languages, either based on a constituent parse tree
(Dra?bek and Yarowsky, 2004; Wang et al, 2007)
or dependency parse tree (Xu et al, 2009). In
this work, we apply syntactic reordering for verb
phrases (VP) based on the English constituent
parse. The VP-based reordering rule we apply in
the work is:
? V P (V B?, ?) ? V P (?, V B?)
where V B? represents V B, V BD, V BG, V BN ,
V BP and V BZ .
In Figure 1, we show the reference alignment
between an English sentence and the correspond-
ing Pashto translation, where E is the original En-
glish sentence, P is the Pashto sentence (in ro-
manized text), and E? is the English sentence after
reordering. As we can see, after the VP-based re-
ordering, the alignment between the two sentences
becomes monotone, which makes it easier for the
aligner to get the alignment correct. During the
reordering of English sentences, we store the in-
dex changes for the English words. After getting
the alignment trained on the reordered English and
original Pashto sentence pairs, we map the English
words back to the original order, along with the
learned alignment links. In this way, the align-
ment is ready to be combined with the baseline
alignment and any other alternatives.
2.2 Stemming
Pashto is one of the morphologically rich lan-
guages. In addition to the linguistic knowledge ap-
plied in the syntactic reordering described above,
we also utilize morphological analysis by applying
stemming on both the English and Pashto sides.
For English, we use Porter stemming (Porter,
                          
                                        S                                                                                                 
 
 
                          S           CC            S 
  
            NP              VP            NP              VP 
 
            PRP  VBP        NP                  VBP        NP        ADVP 
 
                      PRP$         NNS                     PRP       RB 
 
      E:    they  are   your employees and you   know    them      well 
 
 
      P:  hQvy  stAsO   kArvAl   dy    Av  tAsO   hQvy    smh      pOZnB  
 
 
      E?: they  your  employees  are   and  you   them    well      know 
Figure 1: Alignment before/after VP-based re-
ordering.
1980), a widely applied algorithm to remove the
common morphological and inflexional endings
from words in English. For Pashto, we utilize
a morphological decompostion algorithm that has
been shown to be effective for Arabic speech
recognition (Xiang et al, 2006). We start from a
fixed set of affixes with 8 prefixes and 21 suffixes.
The prefixes and suffixes are stripped off from
the Pashto words under the two constraints:(1)
Longest matched affixes first; (2) Remaining stem
must be at least two characters long.
2.3 Partial Word
For low-resource languages, we usually suffer
from the data sparsity issue. Recently, a simple
method was presented in (Chiang et al, 2009),
which keeps partial English and Urdu words in the
training data for alignment training. This is similar
to the stemming method, but is more heuristics-
based, and does not rely on a set of available af-
fixes. With the same motivation, we keep the first
4 characters of each English and Pashto word to
generate one more alternative for the word align-
ment.
3 Confidence-Based Alignment
Combination
Now we describe the algorithm to combine mul-
tiple sets of word alignments based on weighted
confidence scores. Suppose aijk is an alignment
link in the i-th set of alignments between the j-th
source word and k-th target word in sentence pair
(S,T ). Similar to (Huang, 2009), we define the
confidence of aijk as
c(aijk|S, T ) =
?
qs2t(aijk|S, T )qt2s(aijk|T, S),
(1)
23
where the source-to-target link posterior probabil-
ity
qs2t(aijk|S, T ) =
pi(tk|sj)
?K
k?=1 pi(tk? |sj)
, (2)
and the target-to-source link posterior probability
qt2s(aijk|T, S) is defined similarly. pi(tk|sj) is
the lexical translation probability between source
word sj and target word tk in the i-th set of align-
ments.
Our alignment combination algorithm is as fol-
lows.
1. Each candidate link ajk gets soft votes from
N sets of alignments via weighted confidence
scores:
v(ajk|S, T ) =
N
?
i=1
wi ? c(aijk|S, T ), (3)
where the weight wi for each set of alignment
can be optimized under various criteria. In
this work, we tune it on a hand-aligned de-
velopment set to maximize the alignment F-
score.
2. All candidates are sorted by soft votes in de-
scending order and evaluated sequentially. A
candidate link ajk is included if one of the
following is true:
? Neither sj nor tk is aligned so far;
? sj is not aligned and its left or right
neighboring word is aligned to tk so far;
? tk is not aligned and its left or right
neighboring word is aligned to sj so far.
3. Repeat scanning all candidate links until no
more links can be added.
In this way, those alignment links with higher
confidence scores have higher priority to be in-
cluded in the combined alignment.
4 Experiments
4.1 Baseline
Our training data contains around 70K English-
Pashto sentence pairs released under the DARPA
TRANSTAC project, with about 900K words on
the English side. The baseline is a phrase-based
MT system similar to (Koehn et al, 2003). We
use GIZA++ (Och and Ney, 2000) to generate
the baseline alignment for each direction and then
apply grow-diagonal-final (gdf). The decoding
weights are optimized with minimum error rate
training (MERT) (Och, 2003) to maximize BLEU
scores (Papineni et al, 2002). There are 2028 sen-
tences in the tuning set and 1019 sentences in the
test set, both with one reference. We use another
150 sentence pairs as a heldout hand-aligned set
to measure the word alignment quality. The three
sets of alignments described in Section 2 are gen-
erated on the same training data separately with
GIZA++ and enhanced by gdf as for the baseline
alignment. The English parse tree used for the
syntactic reordering was produced by a maximum
entropy based parser (Ratnaparkhi, 1997).
4.2 Improvement in Word Alignment
In Table 1 we show the precision, recall and F-
score of each set of word alignments for the 150-
sentence set. Using partial word provides the high-
est F-score among all individual alignments. The
F-score is 5% higher than for the baseline align-
ment. The VP-based reordering itself does not im-
prove the F-score, which could be due to the parse
errors on the conversational training data. We ex-
periment with three options (c0, c1, c2) when com-
bining the baseline and reordering-based align-
ments. In c0, the weights wi and confidence scores
c(aijk|S, T ) in Eq. (3) are all set to 1. In c1,
we set confidence scores to 1, while tuning the
weights with hill climbing to maximize the F-
score on a hand-aligned tuning set. In c2, we com-
pute the confidence scores as in Eq. (1) and tune
the weights as in c1. The numbers in Table 1 show
the effectiveness of having both weights and con-
fidence scores during the combination.
Similarly, we combine the baseline with each
of the other sets of alignments using c2. They
all result in significantly higher F-scores. We
also generate alignments on VP-reordered partial
words (X in Table 1) and compared B + X and
B + V + P . The better results with B + V + P
show the benefit of keeping the alignments as di-
versified as possible before the combination. Fi-
nally, we compare the proposed alignment combi-
nation c2 with the heuristics-based method (gdf),
where the latter starts from the intersection of all 4
sets of alignments and then applies grow-diagonal-
final (Koehn et al, 2003) based on the links in
the union. The proposed combination approach on
B + V + S + P results in close to 7% higher F-
scores than the baseline and also 2% higher than
24
gdf. We also notice that its higher F-score is
mainly due to the higher precision, which should
result from the consideration of confidence scores.
Alignment Comb P R F
Baseline 0.6923 0.6414 0.6659
V 0.6934 0.6388 0.6650
S 0.7376 0.6495 0.6907
P 0.7665 0.6643 0.7118
X 0.7615 0.6641 0.7095
B+V c0 0.7639 0.6312 0.6913
B+V c1 0.7645 0.6373 0.6951
B+V c2 0.7895 0.6505 0.7133
B+S c2 0.7942 0.6553 0.7181
B+P c2 0.8006 0.6612 0.7242
B+X c2 0.7827 0.6670 0.7202
B+V+P c2 0.7912 0.6755 0.7288
B+V+S+P gdf 0.7238 0.7042 0.7138
B+V+S+P c2 0.7906 0.6852 0.7342
Table 1: Alignment precision, recall and F-score
(B: baseline; V: VP-based reordering; S: stem-
ming; P: partial word; X: VP-reordered partial
word).
4.3 Improvement in MT Performance
In Table 2, we show the corresponding BLEU
scores on the test set for the systems built on each
set of word alignment in Table 1. Similar to the
observation from Table 1, c2 outperforms c0 and
c1, and B + V + S + P with c2 outperforms
B + V + S + P with gdf. We also ran one ex-
periment in which we concatenated all 4 sets of
alignments into one big set (shown as cat). Over-
all, the BLEU score with confidence-based com-
bination was increased by 1 point compared to the
baseline, 0.6 compared to gdf, and 0.7 compared
to cat. All results are statistically significant with
p < 0.05 using the sign-test described in (Collins
et al, 2005).
5 Conclusions
In this work, we have presented a word alignment
combination method that improves both the align-
ment quality and the translation performance. We
generated multiple sets of diversified alignments
based on linguistics, morphology, and heuris-
tics, and demonstrated the effectiveness of com-
bination on the English-to-Pashto translation task.
We showed that the combined alignment signif-
icantly outperforms the baseline alignment with
Alignment Comb Links Phrase BLEU
Baseline 963K 565K 12.67
V 965K 624K 12.82
S 915K 692K 13.04
P 906K 716K 13.30
X 911K 689K 13.00
B+V c0 870K 890K 13.20
B+V c1 865K 899K 13.32
B+V c2 874K 879K 13.60
B+S c2 864K 948K 13.41
B+P c2 863K 942K 13.40
B+X c2 871K 905K 13.37
B+V+P c2 880K 914K 13.60
B+V+S+P cat 3749K 1258K 13.01
B+V+S+P gdf 1021K 653K 13.14
B+V+S+P c2 907K 771K 13.73
Table 2: Improvement in BLEU scores (B: base-
line; V: VP-based reordering; S: stemming; P: par-
tial word; X: VP-reordered partial word).
both higher F-score and higher BLEU score. The
combination approach itself is not limited to any
specific alignment. It provides a general frame-
work that can take advantage of as many align-
ments as possible, which could differ in prepro-
cessing, alignment modeling, or any other aspect.
Acknowledgments
This work was supported by the DARPA
TRANSTAC program. We would like to thank
Upendra Chaudhari, Sameer Maskey and Xiao-
qiang Luo for providing useful resources and the
anonymous reviewers for their constructive com-
ments.
References
Necip Fazil Ayan and Bonnie J. Dorr. 2006. A max-
imum entropy approach to combining word align-
ments. In Proc. HLT/NAACL, June.
Peter Brown, Vincent Della Pietra, Stephen Della
Pietra, and Robert Mercer. 1993. The mathematics
of statistical machine translation: parameter estima-
tion. Computational Linguistics, 19(2):263?311.
David Chiang, Kevin Knight, Samad Echihabi, et al
2009. Isi/language weaver nist 2009 systems. In
Presentation at NIST MT 2009 Workshop, August.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proc. of ACL, pages 531?540.
25
Yonggang Deng and Bowen Zhou. 2009. Optimizing
word alignment combination for phrase table train-
ing. In Proc. ACL, pages 229?232, August.
Elliott Franco Dra?bek and David Yarowsky. 2004. Im-
proving bitext word alignments via syntax-based re-
ordering of english. In Proc. ACL.
Alexander Fraser and Daniel Marcu. 2007. Getting the
structure right for word alignment: Leaf. In Proc. of
EMNLP, pages 51?60, June.
Ulf Hermjakob. 2009. Improved word alignment with
statistics and linguistic heuristics. In Proc. EMNLP,
pages 229?237, August.
Fei Huang. 2009. Confidence measure for word align-
ment. In Proc. ACL, pages 932?940, August.
Abraham Ittycheriah and Salim Roukos. 2005. A max-
imum entropy word aligner for arabic-english ma-
chine translation. In Proc. of HLT/EMNLP, pages
89?96, October.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc.
NAACL/HLT.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proc. of ACL, pages
440?447, Hong Kong, China, October.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proc. of ACL,
pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proc. of ACL, pages
311?318.
Martin Porter. 1980. An algorithm for suffix stripping.
In Program, volume 14, pages 130?137.
Adwait Ratnaparkhi. 1997. A linear observed time sta-
tistical parser based on maximum entropy models.
In Proc. of EMNLP, pages 1?10.
Chao Wang, Michael Collins, and Philipp Koehn.
2007. Chinese syntactic reordering for statistical
machine translation. In Proc. EMNLP, pages 737?
745.
Bing Xiang, Kham Nguyen, Long Nguyen, Richard
Schwartz, and John Makhoul. 2006. Morphological
decomposition for arabic broadcast news transcrip-
tion. In Proc. ICASSP.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
smt for subject-object-verb languages. In Proc.
NAACL/HLT, pages 245?253, June.
26
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 424?428,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Discriminative Feature-Tied Mixture Modeling for Statistical Machine
Translation
Bing Xiang and Abraham Ittycheriah
IBM T. J. Watson Research Center
Yorktown Heights, NY 10598
{bxiang,abei}@us.ibm.com
Abstract
In this paper we present a novel discrimi-
native mixture model for statistical machine
translation (SMT). We model the feature space
with a log-linear combination of multiple mix-
ture components. Each component contains a
large set of features trained in a maximum-
entropy framework. All features within the
same mixture component are tied and share
the same mixture weights, where the mixture
weights are trained discriminatively to max-
imize the translation performance. This ap-
proach aims at bridging the gap between the
maximum-likelihood training and the discrim-
inative training for SMT. It is shown that the
feature space can be partitioned in a vari-
ety of ways, such as based on feature types,
word alignments, or domains, for various ap-
plications. The proposed approach improves
the translation performance significantly on a
large-scale Arabic-to-English MT task.
1 Introduction
Significant progress has been made in statisti-
cal machine translation (SMT) in recent years.
Among all the proposed approaches, the phrase-
based method (Koehn et al, 2003) has become the
widely adopted one in SMT due to its capability
of capturing local context information from adja-
cent words. There exists significant amount of work
focused on the improvement of translation perfor-
mance with better features. The feature set could be
either small (at the order of 10), or large (up to mil-
lions). For example, the system described in (Koehn
et al, 2003) is a widely known one using small num-
ber of features in a maximum-entropy (log-linear)
model (Och and Ney, 2002). The features include
phrase translation probabilities, lexical probabilities,
number of phrases, and language model scores, etc.
The feature weights are usually optimized with min-
imum error rate training (MERT) as in (Och, 2003).
Besides the MERT-based feature weight opti-
mization, there exist other alternative discriminative
training methods for MT, such as in (Tillmann and
Zhang, 2006; Liang et al, 2006; Blunsom et al,
2008). However, scalability is a challenge for these
approaches, where all possible translations of each
training example need to be searched, which is com-
putationally expensive.
In (Chiang et al, 2009), there are 11K syntac-
tic features proposed for a hierarchical phrase-based
system. The feature weights are trained with the
Margin Infused Relaxed Algorithm (MIRA) effi-
ciently on a forest of translations from a develop-
ment set. Even though significant improvement has
been obtained compared to the baseline that has
small number of features, it is hard to apply the
same approach to millions of features due to the data
sparseness issue, since the development set is usu-
ally small.
In (Ittycheriah and Roukos, 2007), a maximum
entropy (ME) model is proposed, which utilizes mil-
lions of features. All the feature weights are trained
with a maximum-likelihood (ML) approach on the
full training corpus. It achieves significantly bet-
ter performance than a normal phrase-based system.
However, the estimation of feature weights has no
direct connection with the final translation perfor-
424
mance.
In this paper, we propose a hybrid framework, a
discriminative mixture model, to bridge the gap be-
tween the ML training and the discriminative train-
ing for SMT. In Section 2, we briefly review the ME
baseline of this work. In Section 3, we introduce the
discriminative mixture model that combines various
types of features. In Section 4, we present experi-
mental results on a large-scale Arabic-English MT
task with focuses on feature combination, alignment
combination, and domain adaptation, respectively.
Section 5 concludes the paper.
2 Maximum-Entropy Model for MT
In this section we give a brief review of a special
maximum-entropy (ME) model as introduced in (It-
tycheriah and Roukos, 2007). The model has the
following form,
p(t, j|s) = p0(t, j|s)
Z(s)
exp
?
i
?i?i(t, j, s), (1)
where s is a source phrase, and t is a target phrase.
j is the jump distance from the previously translated
source word to the current source word. During
training j can vary widely due to automatic word
alignment in the parallel corpus. To limit the sparse-
ness created by long jumps, j is capped to a win-
dow of source words (-5 to 5 words) around the last
translated source word. Jumps outside the window
are treated as being to the edge of the window. In
Eq. (1), p0 is a prior distribution, Z is a normal-
izing term, and ?i(t, j, s) are the features of the
model, each being a binary question asked about the
source, distortion, and target information. The fea-
ture weights ?i can be estimated with the Improved
Iterative Scaling (IIS) algorithm (Della Pietra et al,
1997), a maximum-likelihood-based approach.
3 Discriminative Mixture Model
3.1 Mixture Model
Now we introduce the discriminative mixture model.
Suppose we partition the feature space into multiple
clusters (details in Section 3.2). Let the probabil-
ity of target phrase and jump given certain source
phrase for cluster k be
pk(t, j|s) =
1
Zk(s)
exp
?
i
?ki?ki(t, j, s), (2)
where Zk is a normalizing factor for cluster k.
We propose a log-linear mixture model as shown
in Eq. (3).
p(t, j|s) = p0(t, j|s)
Z(s)
?
k
pk(t, j|s)wk . (3)
It can be rewritten in the log domain as
log p(t, j|s) = log p0(t, j|s)
Z(s)
+
?
k
wk log pk(t, j|s)
= log
p0(t, j|s)
Z(s)
?
?
k
wk log Zk(s)
+
?
k
wk
?
i
?ki?ki(t, j, s). (4)
The individual feature weights ?ki for the i-th
feature in cluster k are estimated in the maximum-
entropy framework as in the baseline model. How-
ever, the mixture weights wk can be optimized di-
rectly towards the translation evaluation metric, such
as BLEU (Papineni et al, 2002), along with other
usual costs (e.g. language model scores) on a devel-
opment set. Note that the number of mixture com-
ponents is relatively small (less than 10) compared
to millions of features in baseline. Hence the opti-
mization can be conducted easily to generate reliable
mixture weights for decoding with MERT (Och,
2003) or other optimization algorithms, such as
the Simplex Armijo Downhill algorithm proposed
in (Zhao and Chen, 2009).
3.2 Partition of Feature Space
Given the proposed mixture model, how to split the
feature space into multiple regions becomes crucial.
In order to surpass the baseline model, where all
features can be viewed as existing in a single mix-
ture component, the separated mixture components
should be complementary to each other. In this
work, we explore three different ways of partitions,
based on either feature types, word alignment types,
or the domain of training data.
In the feature-type-based partition, we split the
ME features into 8 categories:
? F1: Lexical features that examine source word,
target word and jump;
425
? F2: Lexical context features that examine
source word, target word, the previous source
word, the next source word and jump;
? F3: Lexical context features that examine
source word, target word, the previous source
word, the previous target word and jump;
? F4: Lexical context features that examine
source word, target word, the previous or next
source word and jump;
? F5: Segmentation features based on mor-
phological analysis that examine source mor-
phemes, target word and jump;
? F6: Part-of-speech (POS) features that examine
the source and target POS tags and their neigh-
bors, along with target word and jump;
? F7: Source parse tree features that collect the
information from the parse labels of the source
words and their siblings in the parse trees,
along with target word and jump;
? F8: Coverage features that examine the cover-
age status of the source words to the left and
to the right. They fire only if the left source
is open (untranslated) or the right source is
closed.
All the features falling in the same feature cate-
gory/cluster are tied to each other to share the same
mixture weights at the upper level as in Eq. (3).
Besides the feature-type-based clustering, we can
also divide the feature space based on word align-
ment types, such as supervised alignment versus un-
supervised alignment (to be described in the exper-
iment section). For each type of word alignment,
we build a mixture component with millions of ME
features. On the task of domain adaptation, we
can also split the training data based on their do-
main/resources, with each mixture component rep-
resenting a specific domain.
4 Experiments
4.1 Data and Baseline
We conduct a set of experiments on an Arabic-to-
English MT task. The training data includes the UN
parallel corpus and LDC-released parallel corpora,
with about 10M sentence pairs and 300M words in
total (counted at the English side). For each sentence
in the training, three types of word alignments are
created: maximum entropy alignment (Ittycheriah
and Roukos, 2005), GIZA++ alignment (Och and
Ney, 2000), and HMM alignment (Vogel et al,
1996). Our tuning and test sets are extracted from
the GALE DEV10 Newswire set, with no overlap
between tuning and test. There are 1063 sentences
(168 documents) in the tuning set, and 1089 sen-
tences (168 documents) in the test set. Both sets
have one reference translation for each sentence. In-
stead of using all the training data, we sample the
training corpus based on the tuning/test set to train
the systems more efficiently. In the end, about 1.5M
sentence pairs are selected for the sampled training.
A 5-gram language model is trained from the En-
glish Gigaword corpus and the English portion of the
parallel corpus used in the translation model train-
ing. In this work, the decoding weights for both
the baseline and the mixture model are tuned with
the Simplex Armijo Downhill algorithm (Zhao and
Chen, 2009) towards the maximum BLEU.
System Features BLEU
F1 685K 37.11
F2 5516K 38.43
F3 4457K 37.75
F4 3884K 37.56
F5 103K 36.03
F6 325K 37.89
F7 1584K 38.56
F8 1605K 37.49
Baseline 18159K 39.36
Mixture 18159K 39.97
Table 1: MT results with individual mixture component
(F1 to F8), baseline, or mixture model.
4.2 Feature Combination
We first experiment with the feature-type-based
clustering as described in Section 3.2. The trans-
lation results on the test set from the baseline and
the mixture model are listed in Table 1. The MT
performance is measured with the widely adopted
BLEU metric. We also evaluate the systems that uti-
lize only one of the mixture components (F1 to F8).
The number of features used in each system is also
426
listed in the table. As we can see, when using all
18M features in the baseline model, without mixture
weighting, the baseline achieved 3.3 points higher
BLEU score than F5 (the worst component), and 0.8
higher BLEU score than F7 (the best component).
With the log-linear mixture model, we obtained 0.6
gain compared to the baseline. Since there are ex-
actly the same number of features in the baseline
and mixture model, the better performance is due
to two facts: separate training of the feature weights
? within each mixture component; the discrimina-
tive training of mixture weights w. The first one al-
lows better parameter estimation given the number
of features in each mixture component is much less
than that in the baseline. The second factor connects
the mixture weighting to the final translation perfor-
mance directly. In the baseline, all feature weights
are trained together solely under the maximum like-
lihood criterion, with no differentiation of the vari-
ous types of features in terms of their contribution to
the translation performance.
System Features BLEU
ME 5687K 39.04
GIZA 5716K 38.75
HMM 5589K 38.65
Baseline 18159K 39.36
Mixture 16992K 39.86
Table 2: MT results with different alignments, baseline,
or mixture model.
4.3 Alignment Combination
In the baseline mentioned above, three types of word
alignments are used (via corpus concatenation) for
phrase extraction and feature training. Given the
mixture model structure, we can apply it to an align-
ment combination problem. With the phrase table
extracted from all the alignments, we train three
feature mixture components, each on one type of
alignments. Each mixture component contains mil-
lions of features from all feature types described in
Section 3.2. Again, the mixture weights are op-
timized towards the maximum BLEU. The results
are shown in Table 2. The baseline system only
achieved 0.3 minor gain compared to extracting fea-
tures from ME alignment only (note that phrases are
from all the alignments). With the mixture model,
we can achieve another 0.5 gain compared to the
baseline, especially with less number of features.
This presents a new way of doing alignment com-
bination in the feature space instead of in the usual
phrase space.
System Features BLEU
Newswire 8898K 38.82
Weblog 1990K 38.20
UN 4700K 38.21
Baseline 18159K 39.36
Mixture 15588K 39.81
Table 3: MT results with different training sub-corpora,
baseline, or mixture model.
4.4 Domain Adaptation
Another popular task in SMT is domain adapta-
tion (Foster et al, 2010). It tries to take advantage of
any out-of-domain training data by combining them
with the in-domain data in an appropriate way. In
our sub-sampled training corpus, there exist three
subsets: newswire (1M sentences), weblog (200K),
and UN data (300K). We train three mixture com-
ponents, each on one of the training subsets. All re-
sults are compared in Table 3. The baseline that was
trained on all the data achieved 0.5 gain compared to
using the newswire training data alone (understand-
ably it is the best component given the newswire test
data). Note that since the baseline is trained on sub-
sampled training data, there is already certain do-
main adaptation effect involved. On top of that, the
mixture model results in another 0.45 gain in BLEU.
All the improvements in the mixture models above
against the baseline are statistically significant with
p-value < 0.0001 by using the confidence tool de-
scribed in (Zhang and Vogel, 2004).
5 Conclusion
In this paper we presented a novel discriminative
mixture model for bridging the gap between the
maximum-likelihood training and the discriminative
training in SMT. We partition the feature space into
multiple regions. The features in each region are tied
together to share the same mixture weights that are
optimized towards the maximum BLEU scores. It
was shown that the same model structure can be ef-
427
fectively applied to feature combination, alignment
combination and domain adaptation. We also point
out that it is straightforward to combine any of these
three. For example, we can cluster the features based
on both feature types and alignments. Further im-
provement may be achieved with other feature space
partition approaches in the future.
Acknowledgments
We would like to acknowledge the support of
DARPA under Grant HR0011-08-C-0110 for fund-
ing part of this work. The views, opinions, and/or
findings contained in this article/presentation are
those of the author/presenter and should not be in-
terpreted as representing the official views or poli-
cies, either expressed or implied, of the Defense Ad-
vanced Research Projects Agency or the Department
of Defense.
References
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proceedings of ACL-08:HLT.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In Proceedings of NAACL-HLT.
Stephen Della Pietra, Vincent Della Pietra, and John Laf-
ferty. 1997. Inducing features of random fields. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adapta-
tion in satistical machine translation. In Proceedings
of EMNLP.
Abraham Ittycheriah and Salim Roukos. 2005. A maxi-
mum entropy word aligner for arabic-english machine
translation. In Proceedings of HLT/EMNLP, pages
89?96, October.
Abraham Ittycheriah and Salim Roukos. 2007. Di-
rect translation model 2. In Proceedings HLT/NAACL,
pages 57?64, April.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
NAACL/HLT.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative
approach to machine translation. In Proceedings of
ACL/COLING, pages 761?768, Sydney, Australia.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of ACL,
pages 440?447, Hong Kong, China, October.
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for sta-
tistical machine translations. In Proceedings of ACL,
pages 295?302, Philadelphia, PA, July.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL,
pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of ACL,
pages 311?318.
Christoph Tillmann and Tong Zhang. 2006. A discrim-
inative global training algorithm for statistical mt. In
Proceedings of ACL/COLING, pages 721?728, Syd-
ney, Australia.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based word alignment in statistical trans-
lation. In Proceedings of COLING, pages 836?841.
Ying Zhang and Stephan Vogel. 2004. Measuring con-
fidence intervals for the machine translation evalua-
tion metrics. In Proceedings of The 10th International
Conference on Theoretical and Methodological Issues
in Machine Translation.
Bing Zhao and Shengyuan Chen. 2009. A simplex
armijo downhill algorithm for optimizing statistical
machine translation decoding parameters. In Proceed-
ings of NAACL-HLT.
428
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 822?831,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Enlisting the Ghost: Modeling Empty Categories for Machine Translation
Bing Xiang
IBM T. J. Watson Research Center
1101 Kitchawan Rd
Yorktown Heights, NY 10598
bxiang@us.ibm.com
Xiaoqiang Luo *
Google Inc.
111 8th Ave
New York, NY 10011
xql@google.com
Bowen Zhou
IBM T. J. Watson Research Center
1101 Kitchawan Rd
Yorktown Heights, NY 10598
zhou@us.ibm.com
Abstract
Empty categories (EC) are artificial ele-
ments in Penn Treebanks motivated by the
government-binding (GB) theory to ex-
plain certain language phenomena such as
pro-drop. ECs are ubiquitous in languages
like Chinese, but they are tacitly ignored
in most machine translation (MT) work
because of their elusive nature. In this
paper we present a comprehensive treat-
ment of ECs by first recovering them with
a structured MaxEnt model with a rich
set of syntactic and lexical features, and
then incorporating the predicted ECs into
a Chinese-to-English machine translation
task through multiple approaches, includ-
ing the extraction of EC-specific sparse
features. We show that the recovered
empty categories not only improve the
word alignment quality, but also lead to
significant improvements in a large-scale
state-of-the-art syntactic MT system.
1 Introduction
One of the key challenges in statistical machine
translation (SMT) is to effectively model inher-
ent differences between the source and the target
language. Take the Chinese-English SMT as an
example: it is non-trivial to produce correct pro-
nouns on the target side when the source-side pro-
noun is missing. In addition, the pro-drop prob-
lem can also degrade the word alignment qual-
ity in the training data. A sentence pair observed
in the real data is shown in Figure 1 along with
the word alignment obtained from an automatic
word aligner, where the English subject pronoun
* This work was done when the author was with IBM.
?that? is missing on the Chinese side. Conse-
quently, ?that? is incorrectly aligned to the second
to the last Chinese word ?De?, due to their high
co-occurrence frequency in the training data. If
the dropped pronoun were recovered, ?that? would
have been aligned with the dropped-pro (cf. Fig-
ure 3), which is a much more sensible alignment.
Figure 1: Example of incorrect word alignment
due to missing pronouns on the Chinese side.
In order to account for certain language phe-
nomena such as pro-drop and wh-movement, a set
of special tokens, called empty categories (EC),
are used in Penn Treebanks (Marcus et al, 1993;
Bies and Maamouri, 2003; Xue et al, 2005). Since
empty categories do not exist in the surface form
of a language, they are often deemed elusive and
recovering ECs is even figuratively called ?chas-
ing the ghost? (Yang and Xue, 2010).
In this work we demonstrate that, with the avail-
ability of large-scale EC annotations, it is feasi-
ble to predict and recover ECs with high accu-
racy. More importantly, with various approaches
of modeling the recovered ECs in SMT, we are
able to achieve significant improvements1.
The contributions of this paper include the fol-
lowing:
? Propose a novel structured approach to EC
prediction, including the exact word-level lo-
1Hence ?Enlisting the ghost? in the title of this paper.
822
cation and EC labels. Our results are sig-
nificantly higher in accuracy than that of the
state-of-the-art;
? Measure the effect of ECs on automatic word
alignment for machine translation after inte-
grating recovered ECs into the MT data;
? Design EC-specific features for phrases and
syntactic tree-to-string rules in translation
grammar;
? Show significant improvement on top of the
state-of-the-art large-scale hierarchical and
syntactic machine translation systems.
The rest of the paper is organized as follows. In
Section 2, we present a structured approach to EC
prediction. In Section 3, we describe the integra-
tion of Chinese ECs in MT. The experimental re-
sults for both EC prediction and SMT are reported
in Section 4. A survey on the related work is con-
ducted in Section 5, and Section 6 summarizes the
work and introduces some future work.
2 Chinese Empty Category Prediction
The empty categories in the Chinese Treebank
(CTB) include trace markers for A?- and A-
movement, dropped pronoun, big PRO etc. A
complete list of categories used in CTB is shown
in Table 1 along with their intended usages. Read-
ers are referred to the documentation (Xue et al,
2005) of CTB for detailed discussions about the
characterization of empty categories.
EC Meaning
*T* trace of A?-movement
* trace of A-movement
*PRO* big PRO in control structures
*pro* pro-drop
*OP* operator in relative clauses
*RNR* for right node raising
Table 1: List of empty categories in the CTB.
In this section, we tackle the problem of recov-
ering Chinese ECs. The problem has been studied
before in the literature. For instance, Yang and
Xue (2010) attempted to predict the existence of
an EC before a word; Luo and Zhao (2011) pre-
dicted ECs on parse trees, but the position infor-
mation of some ECs is partially lost in their repre-
sentation. Furthermore, Luo and Zhao (2011) con-
ducted experiments on gold parse trees only. In
our opinion, recovering ECs from machine parse
trees is more meaningful since that is what one
would encounter when developing a downstream
application such as machine translation. In this
paper, we aim to have a more comprehensive treat-
ment of the problem: all EC types along with
their locations are predicted, and we will report the
results on both human parse trees and machine-
generated parse trees.
2.1 Representation of Empty Categories
Our effort of recovering ECs is a two-step process:
first, at training time, ECs in the Chinese Treebank
are moved and preserved in the portion of the tree
structures pertaining to surface words only. Origi-
nal ECs and their subtrees are then deleted without
loss of information; second, a model is trained on
transformed trees to predict and recover ECs.
Empty categories heavily depend on syntac-
tic tree structure. For this reason, we choose to
project them onto a parse tree node. To facili-
tate presentation, we first distinguish a solid vs.
an empty non-terminal node. A non-terminal node
is solid if and only if it contains at least one child
node that spans one or more surface words (as op-
posed to an EC); accordingly, an empty node is a
non-terminal node that spans only ECs. In the left
half of Figure 2, the NP node that is the immediate
child of IP has only one child node spanning an
EC ? (-NONE- *pro*), and is thus an empty
node; while all other non-terminal nodes have at
least one surface word as their child and are thus
all solid nodes.
We decide to attach an EC to its lowest solid
ancestor node. That is, the EC is moved up to the
first solid node in the syntactic tree. After ECs
are attached, all empty nodes and ECs are deleted
from the tree. In order to uniquely recover ECs,
we also need to encode the position information.
To this end, the relative child index of an EC is
affixed to the EC tag. Take the NP node spanning
the *pro* in Figure 2 as an example, the *pro*
is moved to the lowest solid ancestor, IP node,
and its position is encoded by @1 since the deleted
NP is the second child of the IP node (we use 0-
based indices). With this transformation, we are
able to recover not only the position of an EC, but
its type as well. A special tag NULL is attached
to non-terminal nodes without EC. Since an EC is
introduced to express the structure of a sentence,
it is a good practice to associate it with the syn-
823
Figure 2: Example of tree transformation on training data to encode an empty category and its position
information.
tactic tree, as opposed to simply attaching it to a
neighboring word, as was done in (Yang and Xue,
2010). We believe this is one of the reasons why
our model has better accuracy than that of (Yang
and Xue, 2010) (cf. Table 7).
In summary, a projected tag consists of an EC
type (such as *pro*) and the EC?s position in-
formation. The problem of predicting ECs is then
cast into predicting an EC tag at each non-terminal
node. Notice that the input to such a predictor is
a syntactic tree without ECs, e.g., the parse tree
on the right hand of Figure 2 without the EC tag
*pro*@1 is such an example.
2.2 A Structured Empty Category Model
We propose a structured MaxEnt model for pre-
dicting ECs. Specially, given a syntactic tree, T ,
whose ECs have been projected onto solid nodes
with the procedure described in Section 2.1, we
traverse it in post-order (i.e., child nodes are vis-
ited recursively first before the current node is vis-
ited). Let T = t1t2 ? ? ? tn be the sequence of
nodes produced by the post-order traversal, and
ei(i = 1, 2, ? ? ? , n) be the EC tag associated with
ti. The probabilistic model is then:
P (en1 |T ) =
n?
i=1
P (ei|T, ei?11 )
=
n?
i=1
exp
(?
k ?kfk(ei?11 , T, ei)
)
Z(ei?11 , T )
(1)
Eq. (1) is the familiar log linear (or MaxEnt)
model, where fk(ei?11 , T, ei) is the feature func-
tion and
Z(ei?11 , T ) =
?
e?E exp
(?
k ?kfk(ei?11 , T, e)
)
is the normalization factor. E is the set of ECs to be
predicted. In the CTB 7.0 processed by the proce-
dure in Section 2.1, the set consists of 32 EC tags
plus a special NULL symbol, obtained by modulat-
ing the list of ECs in Table 1 with their positions
(e.g., *pro*@1 in Figure 2).
Once the model is chosen, the next step is to de-
cide a set of features {fk(ei?11 , T, ei)} to be used
in the model. One advantage of having the rep-
resentation in Section 2.1 is that it is very easy to
compute features from tree structures. Indeed, all
features used in our system are computed from the
syntactic trees, including lexical features.
There are 3 categories of features used in the
model: (1) tree label features; (2) lexical features;
(3) EC features, and we list them in Table 2. In
the feature description column, all node positions
(e.g., ?left?, ?right?) are relative to the current
node being predicted.
Feature 1 to 10 are computed directly from
parse trees, and are straightforward. We include
up to 2 siblings when computing feature 9 and 10.
Feature 11 to 17 are lexical features. Note that we
use words at the edge of the current node: fea-
ture 11 and 12 are words at the internal boundary
of the current node, while feature 13 and 14 are
the immediately neighboring word external to the
current node. Feature 15 and 17 are from head
word information of the current node and the par-
ent node. Feature 18 and 19 are computed from
predicted ECs in the past ? that?s why the model
in Eq. (1) conditions on ei?11 .
Besides the features presented in Table 2, we
also use conjunction features between the current
node label with the parent node label; the cur-
rent node label with features computed from child
nodes; the current node label with features from
left and sibling nodes; the current node label with
lexical features.
824
No. Tree Label Features
1 current node label
2 parent node label
3 grand-parent node label
4 left-most child label or POS tag
5 right-most child label or POS tag
6 label or POS tag of the head child
7 the number of child nodes
8 one level CFG rule
9 left-sibling label or POS tag
10 right-sibling label or POS tag
Lexical Features
11 left-most word under the current node
12 right-most word under the current node
13 word immediately left to the span of the
current node
14 word immediately right to the span of the
current node
15 head word of the current node
16 head word of the parent node
17 is the current node head child of its parent?
EC Features
18 predicted EC of the left sibling
19 the set of predicted ECs of child nodes
Table 2: List of features.
3 Integrating Empty Categories in
Machine Translation
In this section, we explore multiple approaches of
utilizing recovered ECs in machine translation.
3.1 Explicit Recovery of ECs in MT
We conducted some initial error analysis on our
MT system output and found that most of the er-
rors that are related to ECs are due to the missing
*pro* and *PRO*. This is also consistent with
the findings in (Chung and Gildea, 2010). One of
the other frequent ECs, *OP*, appears in the Chi-
nese relative clauses, which usually have a Chi-
nese word ?De? aligned to the target side ?that?
or ?which?. And the trace, *T*, exists in both
Chinese and English sides. For MT we want to fo-
cus on the places where there exist mismatches be-
tween the source and target languages. A straight-
forward way of utilizing the recovered *pro* and
*PRO* is to pre-process the MT training and test
data by inserting ECs into the original source text
(i.e. Chinese in this case). As mentioned in the
previous section, the output of our EC predictor
is a new parse tree with the labels and positions
encoded in the tags. Based on the positional in-
formation in the tags, we can move the predicted
ECs down to the surface level and insert them be-
tween original source words. The same prediction
and ?pull-down? procedure can be conducted con-
sistently cross the MT training and test data.
3.2 Grammar Extraction on Augmented
Data
With the pre-processed MT training corpus, an un-
supervised word aligner, such as GIZA++, can be
used to generate automatic word alignment, as the
first step of a system training pipeline. The ef-
fect of inserting ECs is two-fold: first, it can im-
pact the automatic word alignment since now it al-
lows the target-side words, especially the function
words, to align to the inserted ECs and fix some
errors in the original word alignment; second, new
phrases and rules can be extracted from the pre-
processed training data. For example, for a hier-
archical MT system, some phrase pairs and Hiero
(Chiang, 2005) rules can be extracted with recov-
ered *pro* and *PRO* at the Chinese side.
In this work we also take advantages of the aug-
mented Chinese parse trees (with ECs projected
to the surface) and extract tree-to-string grammar
(Liu et al, 2006) for a tree-to-string MT system.
Due to the recovered ECs in the source parse
trees, the tree-to-string grammar extracted from
such trees can be more discriminative, with an in-
creased capability of distinguishing different con-
text. An example of an augmented Chinese parse
tree aligned to an English string is shown in Figure
3, in which the incorrect alignment in Figure 1 is
fixed. A few examples of the extracted Hiero rules
and tree-to-string rules are also listed, which we
would not have been able to extract from the orig-
inal incorrect word alignment when the *pro*
was missing.
3.3 Soft Recovery: EC-Specific Sparse
Features
Recovered ECs are often good indicators of what
hypothesis should be chosen during decoding. In
addition to the augmented syntax-based grammar,
we propose sparse features as a soft constraint to
boost the performance. For each phrase pair, Hi-
ero rule or tree-to-string rule in the MT system,
a binary feature fk fires if there exists a *pro*
on the source side and it aligns to one of its most
frequently aligned target words found in the train-
ing corpus. We also fire another feature if *pro*
825
Figure 3: Fixed word alignment and examples of
extracted Hiero rules and tree-to-string rules.
aligns to any other target words so the model can
choose to penalize them based on a tuning set.
Similar features can fire for *PRO*. The feature
weights can be tuned on a tuning set in a log-linear
model along with other usual features/costs, in-
cluding language model scores, bi-direction trans-
lation probabilities, etc. The motivation for such
sparse features is to reward those phrase pairs
and rules that have highly confident lexical pairs
specifically related to ECs, and penalize those who
don?t have such lexical pairs.
Table 3 listed some of the most frequent English
words aligned to *pro* or *PRO* in a Chinese-
English parallel corpus with 2M sentence pairs.
Their co-occurrence counts and the lexical trans-
lation probabilities are also shown in the table. In
total we use 15 sparse features for frequent lexical
pairs, including 13 for *pro* and 2 for *PRO*,
and two more features for any other target words
that align to *pro* or *PRO*.
Source Target Counts P (t|s)
*pro* the 93100 0.11
*pro* to 86965 0.10
*pro* it 45423 0.05
*pro* in 36129 0.04
*pro* we 24509 0.03
*pro* which 17259 0.02
*PRO* to 195464 0.32
*PRO* for 31200 0.05
Table 3: Example of frequent word pairs used for
sparse features.
4 Experimental Results
4.1 Empty Category Prediction
We use Chinese Treebank (CTB) v7.0 to train and
test the EC prediction model. We partition the
data into training, development and test sets. The
training set includes 32925 sentences from CTB
files 0001-0325, 0400-0454, 0500-0542, 0600-
0840, 0590-0596, 1001-1120, 2000-3000, cctv,
cnn, msnbc, and phoenix 00-06. The development
set has 3033 sentences, from files 0549-0554,
0900-0931, 1136-1151, 3076-3145, and phoenix
10-11. The test set contains 3297 sentences, from
files 0543-0548, 0841-0885, 1121-1135, 3001-
3075, and phoenix 07-09.
To measure the accuracy of EC prediction, we
project the predicted tags from the upper level
nodes in the parse trees down to the surface level
based on the position information encoded in the
tags. The position index for each inserted EC,
counted at the surface level, is attached for scor-
ing purpose. The same operation is applied on
both the reference and the system output trees.
Such projection is necessary, especially when the
two trees differ in structure (e.g. gold trees vs.
machine-generated trees). We compute the pre-
cision, recall and F1 scores for each EC on the
test set, and collect their counts in the reference
and system output. The results are shown in Ta-
ble 4, where the LDC gold parse trees are used to
extract syntactic features for the model. The first
row in the table shows the accuracy for the places
where no EC should be inserted. The predictor
achieves 99.5% F1 score for this category, with
limited number of missing or false positives. The
F1 scores for majority of the ECs are above 70%,
except for ?*?, which is relatively rare in the data.
For the two categories that are interesting to MT,
*pro* and *PRO*, the predictor achieves 74.3%
and 81.5% in F1 scores, respectively.
The results reported above are based on the
LDC gold parse trees. To apply the EC predic-
tion to NLP applications, such as MT, it is impos-
sible to always rely on the gold trees due to its
limited availability. We parse our test set with a
maximum entropy based statistical parser (Ratna-
parkhi, 1997) first. The parser accuracy is around
84% on the test set. Then we extract features based
on the system-generated parse trees, and decode
with the previously trained model. The results are
shown in Table 5. Compared to those in Table 4,
the F1 scores dropped by different degrees for dif-
826
Tag Ref Sys P R F1
NULL 75159 75508 99.3 99.7 99.5
*pro* 1692 1442 80.8 68.9 74.3
*PRO* 1410 1282 85.6 77.8 81.5
*T* 1851 1845 82.8 82.5 82.7
*OP* 1721 1853 90.9 97.9 94.2
*RNR* 51 39 87.2 66.7 75.6
* 156 96 63.5 39.1 48.4
Table 4: Prediction accuracy with gold parse trees,
where NULL represents the cases where no ECs
should be produced.
ferent types. Such performance drop is expected
since the system relies heavily on syntactic struc-
ture, and parsing errors create an inherent mis-
matching condition between the training and test-
ing time. The smallest drop among all types is on
NULL, at about 1.6%. The largest drop occurs for
*OP*, at 27.1%, largely due to the parsing errors
on the CP nodes. The F1 scores for *pro* and
*PRO* when using system-generated parse trees
are between 50% to 60%.
Tag Precision Recall F1
NULL 97.6 98.2 97.9
*pro* 51.1 50.1 50.6
*PRO* 66.4 50.5 57.3
*T* 68.2 59.9 63.8
*OP* 66.8 67.3 67.1
*RNR* 70.0 54.9 61.5
* 60.9 35.9 45.2
Table 5: Prediction accuracy with system-
generated parse trees.
To show the effect of ECs other than *pro*
and *PRO*, we remove all ECs in the training data
except *pro* and *PRO*. So the model only
predicts NULL, *pro* or *PRO*. The results on
the test set are listed in Table 6. There is 0.8% and
0.5% increase on NULL and *pro*, respectively.
The F1 score for *PRO* drops by 0.2% slightly.
As mentioned earlier, for MT we focus on re-
covering *pro* and *PRO* only. The model
generating the results in Table 6 is the one we ap-
plied in our MT experiments reported later.
In order to compare to the state-of-the-art mod-
els to see where our model stands, we switch our
training, development and test data to those used
in the work of (Yang and Xue, 2010) and (Cai et
Tag Precision Recall F1
NULL 98.5 98.9 98.7
*pro* 51.0 51.1 51.1
*PRO* 66.0 50.4 57.1
Table 6: Prediction accuracy with system-
generated parse trees, modeling *pro* and
*PRO* only.
al., 2011), for the purpose of a direct comparison.
The training set includes CTB files 0081 through
0900. The development set includes files 0041 to
0080, and the test set contains files 0001-0040 and
0901-0931. We merge all empty categories into
a single type in the training data before training
our EC prediction model. To compare the perfor-
mance on system-generated parse trees, we also
train a Berkeley parser on the same training data
and parse the test set. The prediction accuracy
for such single type on the test set with gold or
system-generated parse trees is shown in Table 7,
compared to the numbers reported in (Yang and
Xue, 2010) and (Cai et al, 2011). The model we
proposed achieves 6% higher F1 score than that in
(Yang and Xue, 2010) and 2.6% higher than that in
(Cai et al, 2011), which is significant. This shows
the effectiveness of our structured approach.
Model T P R F1
(Yang and Xue, 2010) G 95.9 83.0 89.0
Structured (this work) G 96.5 93.6 95.0
(Yang and Xue, 2010) S 80.3 52.1 63.2
(Cai et al, 2011) S 74.0 61.3 67.0
Structured (this work) S 74.9 65.1 69.6
Table 7: Comparison with the previous results, us-
ing the same training and test data. T: parse trees.
G: gold parse trees. S: system-generated parse
trees. P: precision. R: recall.
4.2 MT Results
In the Chinese-to-English MT experiments, we
test two state-of-the-art MT systems. One is an re-
implementation of Hiero (Chiang, 2005), and the
other is a hybrid syntax-based tree-to-string sys-
tem (Zhao and Al-onaizan, 2008), where normal
phrase pairs and Hiero rules are used as a backoff
for tree-to-string rules.
The MT training data includes 2 million sen-
tence pairs from the parallel corpora released by
827
LDC over the years, with the data from United
Nations and Hong Kong excluded 2. The Chi-
nese text is segmented with a segmenter trained
on the CTB data using conditional random field
(CRF), followed by the longest-substring match
segmentation in a second pass. Our language
model (LM) training data consists of about 10 bil-
lion English words, which includes Gigaword and
other newswire and web data released by LDC,
as well as the English side of the parallel train-
ing corpus. We train a 6-gram LM with modi-
fied Kneser-Ney smoothing (Chen and Goodman,
1998). Our tuning set for MT contains 1275 sen-
tences from LDC2010E30. We test our system
on the NIST MT08 Newswire (691 sentences)
and Weblog (666 sentences) sets. Both tuning
and test sets have 4 sets of references for each
sentence. The MT systems are optimized with
pairwise ranking optimization (Hopkins and May,
2011) to maximize BLEU (Papineni et al, 2002).
We first predict *pro* and *PRO* with our
annotation model for all Chinese sentences in the
parallel training data, with *pro* and *PRO* in-
serted between the original Chinese words. Then
we run GIZA++ (Och and Ney, 2000) to generate
the word alignment for each direction and apply
grow-diagonal-final (Koehn et al, 2003), same as
in the baseline. We want to measure the impact on
the word alignment, which is an important step for
the system building. We append a 300-sentence
set, which we have human hand alignment avail-
able as reference, to the 2M training sentence pairs
before running GIZA++. The alignment accuracy
measured on this alignment test set, with or with-
out *pro* and *PRO* inserted before running
GIZA++, is shown in Table 8. To make a fair
comparison with the baseline alignment, any tar-
get words aligned to ECs are deemed as unaligned
during scoring. We observe 1.2% improvement on
function word related links, and almost the same
accuracy on content words. This is understand-
able since *pro* and *PRO* are mostly aligned
to the function words at the target side. The pre-
cision and recall for function words are shown in
Table 9. We can see higher accuracy in both pre-
cision and recall when ECs (*pro* and *PRO*)
are recovered in the Chinese side. Especially, the
precision is improved by 2% absolute.
2The training corpora include LDC2003E07,
LDC2003E08, LDC2005T10, LDC2006E26, LDC2006G05,
LDC2007E103, LDC2008G05, LDC2009G01, and
LDC2009G02.
System Function Content All
Baseline 51.7 69.7 65.4
+EC 52.9 69.6 65.7
Table 8: Word alignment F1 scores with or without
*pro* and *PRO*.
System Precision Recall F1
Baseline 54.1 49.5 51.7
+EC 56.0 50.1 52.9
Table 9: Word alignment accuracy for function
words only.
Next we extract phrase pairs, Hiero rules and
tree-to-string rules from the original word align-
ment and the improved word alignment, and tune
all the feature weights on the tuning set. The
weights include those for usual costs and also the
sparse features proposed in this work specifically
for ECs. We test all the systems on the MT08
Newswire and Weblog sets.
The BLEU scores from different systems are
shown in Table 10 and Table 11, respectively. We
measure the incremental effect of prediction (in-
serting *pro* and *PRO*) and sparse features.
Pre-processing of the data with ECs inserted im-
proves the BLEU scores by about 0.6 for newswire
and 0.2 to 0.3 for the weblog data, compared to
each baseline separately. On top of that, adding
sparse features helps by another 0.3 on newswire
and 0.2 to 0.4 on weblog. Overall, the Hiero
and tree-to-string systems are improved by about 1
point for newswire and 0.4 to 0.7 for weblog. The
smaller gain on the weblog data could be due to
the more difficult data to parse, which affects the
accuracy of EC prediction. All the results in Table
10 and 11 marked with ?*? are statistically signif-
icant with p < 0.05 using the sign test described
in (Collins et al, 2005), compared to the baseline
results in each table. Two MT examples are given
in Table 12, which show the effectiveness of the
recovered ECs in MT.
System MT08-nw MT08-wb
Hiero 33.99 25.40
+prediction 34.62* 25.63
+prediction+sparse 34.95* 25.80*
Table 10: BLEU scores in the Hiero system.
828
System MT08-nw MT08-wb
T2S+Hiero 34.53 25.80
+prediction 35.17* 26.08
+prediction+sparse 35.51* 26.53*
Table 11: BLEU scores in the tree-to-string system
with Hiero rules as backoff.
5 Related Work
Empty categories have been studied in recent
years for several languages, mostly in the con-
text of reference resolution and syntactic process-
ing for English, such as in (Johnson, 2002; Di-
enes and Dubey, 2003; Gabbard et al, 2006).
More recently, EC recovery for Chinese started
emerging in literature. In (Guo et al, 2007),
non-local dependencies are migrated from En-
glish to Chinese for generating proper predicate-
argument-modifier structures from surface context
free phrase structure trees. In (Zhao and Ng,
2007), a decision tree learning algorithm is pre-
sented to identify and resolve Chinese anaphoric
zero pronouns. and achieves a performance com-
parable to a heuristic rule-based approach. Similar
to the work in (Dienes and Dubey, 2003), empty
detection is formulated as a tagging problem in
(Yang and Xue, 2010), where each word in the
sentence receives a tag indicating whether there is
an EC before it. A maximum entropy model is
utilized to predict the tags, but different types of
ECs are not distinguished. In (Cai et al, 2011),
a language-independent method was proposed to
integrate the recovery of empty elements into syn-
tactic parsing. As shown in the previous section,
our model outperforms the model in (Yang and
Xue, 2010) and (Cai et al, 2011) significantly us-
ing the same training and test data. (Luo and Zhao,
2011) also tries to predict the existence of an EC
in Chinese sentences, but the ECs in the middle of
a tree constituent are lumped into a single position
and are not uniquely recoverable.
There exists only a handful of previous work on
applying ECs explicitly to machine translation so
far. One of them is the work reported in (Chung
and Gildea, 2010), where three approaches are
compared, based on either pattern matching, CRF,
or parsing. However, there is no comparison be-
tween using gold trees and automatic trees. There
also exist a few major differences on the MT
part between our work and theirs. First, in ad-
dition to the pre-processing of training data and
inserting recovered empty categories, we imple-
ment sparse features to further boost the perfor-
mance, and tune the feature weights directly to-
wards maximizing the machine translation met-
ric. Second, there is no discussion on the quality
of word alignment in (Chung and Gildea, 2010),
while we show the alignment improvement on a
hand-aligned set. Last, they use a phase-based
system trained on only 60K sentences, while we
conduct experiments on more advanced Hiero and
tree-to-string systems, trained on 2M sentences in
a much larger corpus. We directly take advantage
of the augmented parse trees in the tree-to-string
grammar, which could have larger impact on the
MT system performance.
6 Conclusions and Future Work
In this paper, we presented a novel structured ap-
proach to EC prediction, which utilizes a max-
imum entropy model with various syntactic fea-
tures and shows significantly higher accuracy than
the state-of-the-art approaches. We also applied
the predicted ECs to a large-scale Chinese-to-
English machine translation task and achieved sig-
nificant improvement over two strong MT base-
829
lines, i.e. a hierarchical phase-based system and
a tree-to-string syntax-based system. More work
remain to be done next to further take advantages
of ECs. For example, the recovered ECs can be
encoded in a forest as the input to the MT decoder
and allow the decoder to pick the best MT output
based on various features in addition to the sparse
features we proposed in this work. Many promis-
ing approaches can be explored in the future.
Acknowledgments
We would like to acknowledge the support
of DARPA under Grant HR0011-12-C-0015 for
funding part of this work. The views, opin-
ions, and/or findings contained in this arti-
cle/presentation are those of the author/presenter
and should not be interpreted as representing the
official views or policies, either expressed or im-
plied, of the Defense Advanced Research Projects
Agency or the Department of Defense.
References
Ann Bies and Mohamed Maamouri. 2003.
Penn Arabic treebank guidelines. In
http://www.ircs.upenn.edu/arabic/Jan03release/
guidelines-TB-1-28-03.pdf.
Shu Cai, David Chiang, and Yoav Goldberg. 2011.
Language-independent parsing with empty ele-
ments. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics:short papers, pages 212?216.
Stanley F. Chen and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. In Technical Report TR-10-98, Computer
Science Group, Harvard University.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 263?270,
Ann Arbor, Michigan, June.
Tagyoung Chung and Daniel Gildea. 2010. Effects
of empty categories on machine translation. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 636?
645.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Lin-
guistics, pages 531?540.
Peter Dienes and Amit Dubey. 2003. Deep syntactic
processing by combining shallow methods. In Pro-
ceedings of the 41st Annual Meeting of the Associa-
tion for Computational Linguistics.
Ryan Gabbard, Seth Kulick, and Mitchell Marcus.
2006. Fully parsing the penn treebank. In Pro-
ceedings of the Human Language Technology Con-
ference of the North American Chapter of the ACL.
Yuqing Guo, Haifeng Wang, and Josef van Genabith.
2007. Recovering non-local dependencies for Chi-
nese. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL).
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352?1362.
Mark Johnson. 2002. A simple pattern-matching al-
gorithm for recovering empty nodes and their an-
tecedents. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguis-
tics.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings
of HLT-NAACL, pages 48?54.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 609?616.
Xiaoqiang Luo and Bing Zhao. 2011. A statistical
tree annotator and its applications. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 1230?1238.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. In Compu-
tational Linguistics, volume 19(2), pages 313?330.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting of the Association for Com-
putational Linguistics, pages 440?447, Hong Kong,
China, October.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
PA.
Adwait Ratnaparkhi. 1997. A linear observed time sta-
tistical parser based on maximum entropy models.
In Proceedings of Second Conference on Empirical
830
Methods in Natural Language Processing, pages 1?
10.
Nianwen Xue, Fei Xia, Fu dong Chiou, and Martha
Palmer. 2005. The Penn Chinese Treebank: Phrase
structure annotation of a large corpus. In Natural
Language Engineering, volume 11(2), pages 207?
238.
Yaqin Yang and Nianwen Xue. 2010. Chasing the
ghost: Recovering empty categories in the Chi-
nese Treebank. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics,
pages 1382?1390, Beijing, China, August.
Bing Zhao and Yaser Al-onaizan. 2008. Generaliz-
ing local and non-local word-reordering patterns for
syntax-based machine translation. In Proceedings of
the 2008 Conference on Empirical Methods in Nat-
ural Language Processing, pages 572?581.
Shanheng Zhao and Hwee Tou Ng. 2007. Identifica-
tion and resolution of Chinese zero pronouns: A ma-
chine learning approach. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL).
831
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1264?1274,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Two-Neighbor Orientation Model with Cross-Boundary Global Contexts
Hendra Setiawan, Bowen Zhou, Bing Xiang and Libin Shen
IBM T.J.Watson Research Center
1101 Kitchawan Road
Yorktown Heights, NY 10598, USA
{hendras,zhou,bxiang,lshen}@us.ibm.com
Abstract
Long distance reordering remains one of
the greatest challenges in statistical ma-
chine translation research as the key con-
textual information may well be beyond
the confine of translation units. In this
paper, we propose Two-Neighbor Orien-
tation (TNO) model that jointly models
the orientation decisions between anchors
and two neighboring multi-unit chunks
which may cross phrase or rule bound-
aries. We explicitly model the longest
span of such chunks, referred to as Max-
imal Orientation Span, to serve as a
global parameter that constrains under-
lying local decisions. We integrate our
proposed model into a state-of-the-art
string-to-dependency translation system
and demonstrate the efficacy of our pro-
posal in a large-scale Chinese-to-English
translation task. On NIST MT08 set, our
most advanced model brings around +2.0
BLEU and -1.0 TER improvement.
1 Introduction
Long distance reordering remains one of the great-
est challenges in Statistical Machine Translation
(SMT) research. The challenge stems from the
fact that an accurate reordering hinges upon the
model?s ability to make many local and global
reordering decisions accurately. Often, such
reordering decisions require contexts that span
across multiple translation units.1 Unfortunately,
previous approaches fall short in capturing such
cross-unit contextual information that could be
1We define translation units as phrases in phrase-based
SMT, and as translation rules in syntax-based SMT.
critical in reordering. Specifically, the popular dis-
tortion or lexicalized reordering models in phrase-
based SMT focus only on making good local pre-
diction (i.e. predicting the orientation of imme-
diate neighboring translation units), while transla-
tion rules in syntax-based SMT come with a strong
context-free assumption, which model only the re-
ordering within the confine of the rules. In this
paper, we argue that reordering modeling would
greatly benefit from richer cross-boundary contex-
tual information
We introduce a reordering model that incorpo-
rates such contextual information, named the Two-
Neighbor Orientation (TNO) model. We first iden-
tify anchors as regions in the source sentences
around which ambiguous reordering patterns fre-
quently occur and chunks as regions that are con-
sistent with word alignment which may span mul-
tiple translation units at decoding time. Most no-
tably, anchors and chunks in our model may not
necessarily respect the boundaries of translation
units. Then, we jointly model the orientations of
chunks that immediately precede and follow the
anchors (hence, the name ?two-neighbor?) along
with the maximal span of these chunks, to which
we refer as Maximal Orientation Span (MOS).
As we will elaborate further in next sections,
our models provide a stronger mechanism to make
more accurate global reordering decisions for the
following reasons. First of all, we consider the
orientation decisions on both sides of the anchors
simultaneously, in contrast to existing works that
only consider one-sided decisions. In this way, we
hope to upgrade the unigram formulation of exist-
ing reordering models to a higher order formula-
tion. Second of all, we capture the reordering of
chunks that may cross translation units and may
be composed of multiple units, in contrast to ex-
1264
isting works that focus on the reordering between
individual translation units. In effect, MOS acts as
a global reordering parameter that guides or con-
strains the underlying local reordering decisions.
To show the effectiveness of our model, we
integrate our TNO model into a state-of-the-
art syntax-based SMT system, which uses syn-
chronous context-free grammar (SCFG) rules to
jointly model reordering and lexical translation.
The introduction of nonterminals in the SCFG
rules provides some degree of generalization.
However as mentioned earlier, the context-free
assumption ingrained in the syntax-based for-
malism often limits the model?s ability to in-
fluence global reordering decision that involves
cross-boundary contexts. In integrating TNO, we
hope to strengthen syntax-based system?s ability
to make more accurate global reordering deci-
sions.
Our other contribution in this paper is a prac-
tical method for integrating the TNO model into
syntax-based translations. The integration is non-
trivial since the decoding of syntax-based SMT
proceeds in a bottom-up fashion, while our model
is more natural for top-down parsing, thus the
model?s full context sometimes is often available
only at the latest stage of decoding. We implement
an efficient shift-reduce algorithm that facilitates
the accumulation of partial context in a bottom-up
fashion, allowing our model to influence the trans-
lation process even in the absence of full context.
We show the efficacy of our proposal in a large-
scale Chinese-to-English translation task where
the introduction of our TNO model provides a
significant gain over a state-of-the-art string-to-
dependency SMT system (Shen et al, 2008) that
we enhance with additional state-of-the-art fea-
tures. Even though the experimental results car-
ried out in this paper employ SCFG-based SMT
systems, we would like to point out that our mod-
els is applicable to other systems including phrase-
based SMT systems.
The rest of the paper is organized as follows.
In Section 2, we introduce the formulation of our
TNO model. In Section 3, we introduce and moti-
vate the concept of Maximal Orientation Span. In
Section 4, we introduce four variants of the TNO
model with different model complexities. In Sec-
tion 5, we describe the training procedure to esti-
mate the parameters of our models. In Section 6,
we describe our shift-reduce algorithm which inte-
grates our proposed TNO model into syntax-based
SMT. In Section 7, we describe our experiments
and present our results. We wrap up with related
work in Section 8 and conclusion in Section 9.
2 Two-Neighbor Orientation Model
Given an aligned sentence pair ? = (F,E,?), let
?(?) be all possible chunks that can be extracted
from ? according to: 2
{(f j2j1/e
i2
i1) :?j1? j? j2,?i : (j, i)??, ii? i? i2 ?
?i1? i? i2,?j : (j, i)??, ji? j?j2}
Our Two-Neighbor Orientation model (TNO)
designatesA ? ?(?) as anchors and jointly mod-
els the orientation of chunks that appear immedi-
ately to the left and to the right of the anchors as
well as the identities of these chunks. We define
anchors as chunks, around which ambiguous re-
ordering patterns frequently occur. Anchors can
be learnt automatically from the training data or
identified from the linguistic analysis of the source
sentence. In our experiments, we use a simple
heuristics based on part-of-speech tags which will
be described in Section 7.
More concretely, given A ? ?(?), let a =
(f j2j1/e
i2
i1) ? A be a particular anchor. Then, let
CL(a) ? ?(?) be a?s left neighbors and let
CR(a) ? ?(?) be a?s right neighbors, iff:
?CL = (f j4j3/e
i4
i3) ? CL(a) : j4 + 1 = j1 (1)
?CR = (f j6j5/e
i6
i5) ? CR(a) : j2 + 1 = j5 (2)
Given CL(a) and CR(a), let CL = (f j4j3/ei4i3) and
CR = (f j6j5/e
i6
i5) be a particular pair of left and right
neighbors of a = (f j2j1/ei2i1). Then, the orientationof CL and CR are OL(CL, a) and OR(CR, a) re-
spectively and each may take one of the following
four orientation values (similar to (Nagata et al,
2006)):
? Monotone Adjacent (MA), if (i4 + 1) = i1
for OL and if (i2 + 1) = i5 for OR
? Reverse Adjacent (RA), if (i2 + 1) = i3 for
OL and if (i6 + 1) = i1 for OR
? Monotone Gap (MG), if (i4 + 1) < i1 for
OL and if (i2 + 1) < i5 for OR
2We represent a chunk as a source and target phrase pair
(f j2j1/ei2i1 ) where the subscript and the superscript indicate the
starting and the ending indices as such f j2j1 denotes a sourcephrase that spans from j1 to j2.
1265
Figure 1: An aligned Chinese-English sentence pair. Circles
represent alignment points. Black circle represents the an-
chor; boxes represent the anchor?s neighbors.
? Reverse Gap (RG), if (i2 + 1) < i3 for OL
and if (i6 + 1) < i1 for OR. (1)
The first clause (monotone, reverse) indicates
whether the target order of the chunks follows the
source order; the second (adjacent, gap) indicates
whether the chunks are adjacent or separated by an
intervening phrase when projected.
To be more concrete, let us consider an aligned
sentence pair in Fig. 1, which is adapted from
(Chiang, 2005). Suppose there is only one anchor,
i.e. a = (f77 /e77) which corresponds to the word
de(that). By applying Eqs. 1 and 2, we can infer
that a has three left neighbors and four right neigh-
bors, i.e. CL(a) = (f66 /e99), (f65 /e98), (f63 /e118 ) and
CR(a) = (f88 /e55), (f98 /e65), (f108 /e64), (f118 /e63)
respectively. Then, by applying Eq.
1, we can compute the orientation val-
ues of each of these neighbors, which
are OL(CL(a), a) = RG,RA,RA and
OR(CR(a), a) = RG,RA,RA,RA. As shown,
most of the neighbors have Reverse Adjacent
(RA) orientation except for the smallest left and
right neighbors (i.e. (f66 /e99) and (f88 /e55)) which
have Reverse Gap (RG) orientation.
Given the anchors together with its neighboring
chunks and their orientations, the Two-Neighbor
Orientation model takes the following form:
?
a?A
?
CL?CL(a),
CR?CR(a)
PTNO(CL, OL, CR, OR|a; ?) (2)
For conciseness, references that are clear from
context, such the reference to CL and a in
OL(CL, a), are dropped.
3 Maximal Orientation Span
As shown in Eq. 2, the TNO model has to enu-
merate all possible pairing of CL ? CL(a) and
CR ? CR(a). To make the TNO model more
tractable, we simplify the TNO model to consider
only the largest left and right neighbors, referred
to as the Maximal Orientation Span/MOS (M ).
More formally, given a = (f j2j1/ei2i1), the left andthe right MOS of a are:
ML(a) = arg max
(fj4j3 /e
i4
i3 )?CL(a)
(j4 ? j3)
MR(a) = arg max
(fj6j5 /e
i6
i5 )?CR(a)
(j6 ? j5)
Coming back to our example, the left and right
MOS of the anchor are ML(a) = (f63 /e118 ) and
MR(a) = (f118 /e63). In Fig. 1, they are denoted as
the largest boxes delineated by solid lines.
As such, we reformulate Eq. 2 into:
?
a?A
?
CL?CL(a),
CR?CR(a)
PTNO(ML, OL,MR, OR|a; ?).?CL==ML?
CR==MR
(3)
where ? returns 1 if (CL == ML ?CR == MR),
otherwise 0.
Beyond simplifying the computation, the key
benefit of modeling MOS is that it serves as a
global parameter that can guide or constrain un-
derlying local reorderings. As a case in point, let
us consider a cheating exercise where we have to
translate the Chinese sentence in Fig. 1 with the
following set of hierarchical phrases3:
Xa??Aozhou1shi2X1,Australia1 is2X1?
Xb??yu3 Beihan4X1, X1with3 North4 Korea?
Xc??you5bangjiao6, have5dipl.6 rels.?
Xd??X1de7shaoshu8 guojia9 zhi10 yi11,
one11of10the few8 countries9 that7X1?
This set of hierarchical phrases represents a trans-
lation model that has resolved all local ambiguities
(i.e. local reordering and lexical mappings) except
for the spans of the hierarchical phrases. With this
example, we want to show that accurate local de-
cisions (rather obviously) don?t always lead to ac-
curate global reordering and to demonstrate that
explicit MOS modeling can play a crucial role to
address this issue. To do so, we will again focus
on the same anchor de (that).
3We use hierarchical phrase-based translation system as a
case in point, but the merit is generalizable to other systems.
1266
d? ?X1de7shaoshu8 guojia9 zhi10 yi11?, ?one11of10the few8 countries9 that7X1?
a? ??Aozhou1shi2X1?de7shaoshu8 guojia9 zhi10 yi11?,
?one11of10the few8 countries9 that7?Australia1 is2X1??
b? ??Aozhou1shi2 ?yu3 Beihan4X1??de7shaoshu8 guojia9 zhi10 yi11?,
?one11of10the few8 countries9 that7?Australia1 is2?X1with3 North4 Korea???
c? ?d ?aAozhou1shi2 ?byu3 Beihan4 ?cyou5bangjiao6?c?b?ade7shaoshu8 guojia9 zhi10 yi11 ?d ,
?one11of10the few8 countries9 that7?Australia1 is2??have5dipl.6 rels.?with3 North4 Korea???
Table 1: Derivation of Xd ?Xa ?Xb ?Xc that leads to an incorrect translation.
a? ?Aozhou1shi2X1?, ?Australia1 is2X1?
b? ?Aozhou1shi2?yu3Beihan4X1??, ?Australia1 is2?X1with3 North4 Korea??
d? ?Aozhou1shi2?yu3Beihan4?X1de7shaoshu8 guojia9 zhi10 yi11???,
?Australia1 is2??one11of10the few8 countries9 that7X1?with3 North4 Korea??
c? ?aAozhou1shi2?byu3Beihan4 ?d ?cyou5bangjiao6?cde7shaoshu8 guojia9 zhi10 yi11 ?d ?b?a,
Australia1 is2??one11of10the few8 countries9 that7?have5dipl.6??with3 North4 Korea??
Table 2: Derivation of Xa ?Xb ?Xd ?Xc that leads to the correct translation.
As the rule?s identifier, we attach an alphabet
letter to each rule?s left hand side, as such the an-
chor de (that) appears in rule Xd. We also attach
the word indices as the superscript of the source
words and project the indices to the target words
aligned, as such ?have5? suggests that the word
?have? is aligned to the 5-th source word, i.e. you.
Note that to facilitate the projection, the rules must
come with internal word alignment in practice.
Now the indices on the target words in the rules
are different from those in Fig. 1. We will also
extensively use indices in this sense in the sub-
sequent section about decoding. In such a sense,
ML(a) = (f63 /e63) and MR(a) = (f118 /e118 ).
Given the rule set, there are three possible
derivations, i.e. Xd ?Xa ?Xb ?Xc,Xa ?Xb ?
Xd ?Xc, and Xa ?Xd ?Xb ?Xc, where ? in-
dicates that the first operand dominates the second
operand in the derivation tree. The application of
the rules would show that the first derivation will
produce an incorrect reordering while the last two
will produce the correct ones. Here, we would like
to point out that even in this simple example where
all local decisions are made accurate, this ambigu-
ity occurs and it would occur even more so in the
real translation task where local decisions may be
highly inaccurate.
Next, we will show that the MOS-related in-
formation can help to resolve this ambiguity, by
focusing more closely on the first and the second
derivations, which are detailed in Tables 1 and 2.
Particularly, we want to show that the MOS gen-
erated by the incorrect derivation does not match
the MOS learnt from Fig. 1. As shown, at the
end of the derivation, we have all the informa-
tion needed to compute the MOS (i.e. ?) which is
equivalent to that available at training time, i.e. the
source sentence, the complete translation and the
word alignment. Running the same MOS extrac-
tion procedure on both derivations would produce
the right MOS that agrees with the right MOS pre-
viously learnt from Fig. 1, i.e. (f118 /e118 ). How-
ever, that?s not the case for left MOS, which we
underline in Tables 1 and 2. As shown, the incor-
rect derivation produces a left MOS that spans six
words, i.e. (f61 /e61), while the correct derivation
produces a left MOS that spans four words, i.e.
(f63 /e63). Clearly, the MOS of the incorrect deriva-
tion doesn?t agree with the MOS we learnt from
Fig. 1, unlike the MOS of the correct translation.
This suggests that explicit MOS modeling would
provide a mechanism for resolving crucial global
reordering ambiguities that are beyond the ability
of local models.
Additionally, this illustration also shows a case
where MOS acts as a cross-boundary context
which effectively relaxes the context-free assump-
tion of hierarchical phrase-based formalism. In
Tables 1 and 2?s full derivations, we indicate rule
boundaries explicitly by indexing the angle brack-
ets, e.g. ?a indicates the beginning of rule Xa in
the derivation. As the anchor appears in Xd, we
1267
highlight its boundaries in box frames. de (that)?s
MOS respects rule boundaries if and only if all
the words come entirely from Xd?s antecedent or
?d and ?d appears outside of MOS; otherwise it
crosses the rule boundaries. As clearly shown in
Table 2, the left MOS of the correct derivation (un-
derlined) crosses the rule boundary (of Xd) since
?d appears within the MOS.
Going back to the formulation, focusing on
modeling MOS would simplify the formulation of
TNO model from Eq. 2 into:
?
a?A
PTNO(ML, OL,MR, OR|a; ?) (4)
which doesn?t require enumerating of all possible
pairs of CL and CR.
4 Model Decomposition and Variants
To make the model more tractable, we decompose
PTNO in Eq. 4 into the following four factors:
P (MR|a)? P (OR|MR, a)? P (ML|OR,MR, a)
? P (OL|ML, OR,MR, a). Subsequently, we will
refer to them as PMR , POR , PML and POL respec-
tively. Each of these factors will act as an addi-
tional feature in the log-linear framework of our
SMT system. The above decomposition follows
a generative story that starts from generating the
right neighbor first. There are other equally credi-
ble alternatives, but based on empirical results, we
settle with the above.
Next, we present four different variants of the
model (not to be confused with the four factors
above). Each variant has a different probabilistic
conditioning of the factors. We start by making
strong independence assumptions in Model 1 and
then relax them as we progress to Model 4. The
description of the models is as follow:
? Model 1. We assume PML and PMR to be
equal to 1 and POR ? P (OR|a; ?) to be in-
dependent of MR and POL ? P (OL|a; ?) to
be in independent of ML,MR and OR.
? Model 2. On top of Model 1, we
make POL dependent on POR , thus
POL?P (OL|OR, a; ?).
? Model 3. On top of Model 2, we make POR
dependent on MR and POL on MR and ML,
thus POR ? P (OR|MR, a; ?) and POL ?
P (OL|ML, OR,MR; a,?) .
? Model 4. On top of Model 3, we model PMR
and PML as multinomial distributions esti-
mated from training data.
Model 1 represents a model that focuses on
making accurate one-sided decisions, independent
of the decision on the other side. Model 2 is
designed to address the deficiency of Model 1
since Model 1 may assign non-zero probability to
improbable assignment of orientation values, e.g.
Monotone Adjacent for the left neighbor and Re-
verse Adjacent for the right neighbor. Model 2
does so by conditioning POL on OR. In Model 3,
we start incorporating MOS-related information in
predicting OL and OR. In Model 4, we explicitly
model the MOS of each anchor.
5 Training
The TNO model training consists of two differ-
ent training regimes: 1) discriminative for train-
ing POL ,POR ; and 2) generative for training PML ,
PMR . Before describing the specifics, we start by
describing the procedure to extract anchors and
their corresponding MOS from training data, from
which we collect statistics and extract features to
train the model.
For each aligned sentence pair (F,E,?) in the
training data, the training starts with the iden-
tification of the regions in the source sentences
as anchors (A). For our Chinese-English experi-
ments, we use a simple heuristic that equates as
anchors, single-word chunks whose corresponding
word class belongs to closed-word classes, bear-
ing a close resemblance to (Setiawan et al, 2007).
In total, we consider 21 part-of-speech tags; some
of which are as follow: VC (copula), DEG, DEG,
DER, DEV (de-related), PU (punctuation), AD
(adjectives) and P (prepositions).
Next we generate all possible chunks ?(?)
as previously described in Sec. 3. We then de-
fine a functionMinC(?, j1, j2) which returns the
shortest chunk that can span from j1 to j2. If
(f j2j1 /e
i2
i1) ? ?, then MinC returns (f j2j1 /ei2i1).The algorithm to extract MOS takes ? and an
anchor a = (f j2j1 /ei2i1) as input; and outputs thechunk that qualifies as MOS or none. Alg. 1
provides the algorithm to extract the right MOS;
the algorithm to extract the left MOS is identical
to Alg. 1, except that it scans for chunks to the
left of the anchor. In Alg. 1, there are two in-
termediate parameters si and ei which represent
the active search range and should initially be set
to j2 + 1 and |F | respectively. Once we obtain
a,ML(a) andMR(a), we computeOL(ML(a), a)
and OR(MR(a), a) and are ready for training.
1268
To estimate POL and POR , we train discrimi-
native classifiers that predict the orientation val-
ues and use the normalized posteriors at decoding
time as additional feature scores in SMT?s log lin-
ear framework. We train the classifiers on a rich
set of binary features ranging from lexical to part-
of-speech (POS) and to syntactic features.
Algorithm 1: Function MREx
input : a = (f j2j1 /ei2i1), si, ei: int; ?: chunks
output: (f j4j3 /ei4i3) : chunk or ?
(f j4j3 /e
i4
i3) = MinC(?, j2 + 1, ei)
if (j3 == j2 + 1 ? j4 == ei) then
? f j4j3 /e
i4
i3
else
if (j2 + 1 == ei) then
? ?
else
if (ei-2 ? si) then
?MREx(a, si, ei? 1,?)
else
m = d(si+ei)/2e
(f j4j3 /e
i4
i4) = MinC(?, j2 + 1,m)
if (j3 == j2 + 1) then
c = MREx(a,m, ei? 1,?)
if (c == ?) then
? f j4j3 /e
i4
i3
else
? c
end
else
?MREx(a, si,m? 1,?)
end
end
end
end
Suppose a = (f j2j1 /ei2i1), ML(a) = (f j4j3 /ei4i3)
and ML(a) = (f j6j5 /ei6i5), then based on the con-text?s location, the elementary features employed
in our classifiers can be categorized into:
1. anchor-related: slex (the actual word of
f j2j1 ), spos (part-of-speech (POS) tag of
slex), sparent (spos?s parent in the parse
tree), tlex (ei2i1?s actual target word)..
2. surrounding: lslex (the previous word /
f j1?1j1?1 ), rslex (the next word / f j2+1j2+1 ), lspos(lslex?s POS tag), rspos (rslex?s POS
tag), lsparent (lslex?s parent), rsparent
(rslex?s parent).
3. non-local: lanchorslex (the previous
anchor?s word) , ranchorslex (the next an-
chor?s word), lanchorspos (lanchorslex?s
POS tag), ranchorspos (ranchorslex?s
POS tag).
4. MOS-related: mosl int slex (the actual
word of f j3j3 ), mosl ext slex (the actual word
of f j3j3 ), mosl int spos (mosl int slex?sPOS tag), mosl ext spos (mosl ext spos?s
POS tag), mosr int slex (the actual word of
f j3j3 ), mosr ext slex (the actual word of f j3j3 ),
mosr int spos (mosr int slex?s POS tag),
mosr ext spos (mosr ext spos?s POS tag).
For Model 1, we train one classifier each for
POR and POL . For Model 2-4, we train four clas-
sifiers for POL for each value of OR. We use only
the MOS features for Model 3 and 4. Addition-
ally, we augment the feature set with compound
features, e.g. conjunction of the lexical of the an-
chor and the lexical of the left and the right an-
chors. Although they increase the number of fea-
tures significantly, we found that these compound
features are empirically beneficial.
We come up with > 50 types of features, which
consist of a combination of elementary and com-
pound features. In total, we generate hundreds of
millions of such features from the training data.
To keep the number features to a manageable size,
we employ the L1-regularization in training to en-
force sparse solutions, using the off-the-shelf LIB-
LINEAR toolkit (Fan et al, 2008). After training,
the number of features in our classifiers decreases
to below 5 million features for each classifier.
We train PML and PMR via the relative fre-
quency principle. To avoid the sparsity issue, we
represent ML as (mosl int spos,mosl ext spos)
and MR as (mosr int spos,mosr ext spos). We
condition PML and PMR only on spos and the ori-
entation, estimating them as follow:
P (ML|spos, OL) =
N(ML, spos, OL)
N(spos, OL)
P (MR|spos, OR) =
N(MR, spos, OR)
N(spos, OR)
where N returns the count of the events in the
training data.
1269
Target string (w/ source index) Symbol(s) read Op. Stack(s)
(1) Xc have5 dipl.6 rels. [5][6] S,S,R Xc:[5-6]
(2) Xd one11 of10 few8 countries9 [11][10] S,S,R [10-11]
that7 Xc
(3) [8][9] S,S,R,R [8-11]
(4) [7] S [8-11][7]
(5) Xc:[5,6] S Xd:[8-11][7][5,6]
(6) Xb Xd with3 North4 Korea Xd:[8-11][7][5,6] S [8-11][7][5,6]
(7) [3][4] S,S,R,R Xb:[8-11][7][3-6]
(8) Xa Australia1 is2 Xb [1][2] S,S,R [1-2]
(9) Xb:[8-11][7][3,6] S,A Xa:[1-2][8-11][7][3,6]
Table 3: The application of the shift-reduce parsing algorithm, which corresponds to Table 2?s derivation.
6 Decoding
Integrating the TNO Model into syntax-based
SMT systems is non-trivial, especially with the
MOS modeling. The method described in Sec. 3
assumes ? = (F,E,?), thus it is only applicable
at training or at the last stage of decoding. Since
many reordering decisions may have been made
at the earlier stages, the late application of TNO
model would limit the utility of the model. In this
section, we describe an algorithm that facilitates
the incremental construction of MOS and the com-
putation of TNO model on partial derivations.
The algorithm bears a close resemblance to the
shift-reduce algorithm where a stack is used to ac-
cumulate (partial) information about a, ML and
MR for each a ? A in the derivation. This al-
gorithm takes an input stream and applies either
the shift or the reduce operations starting from the
beginning until the end of the stream. The shift op-
eration advances the input stream by one symbol
and push the symbol into the stack; while the re-
duce operation applies some reduction rule to the
topmost elements of the stack. The algorithm ter-
minates at the end of the input stream where the
resulting stack will be propagated to the parent for
the later stage of decoding. In our case, the in-
put stream is the target string of the rule and the
symbol is the corresponding source index of the
elements of the target string. The reduction rule
looks at two indices and merge them if they are
adjacent (i.e. has no intervening phrase). We for-
bid the application of the reduction rule to anchors.
Table 3 shows the execution trace of the algorithm
for the derivation described in Table 2.
As shown, the algorithm starts with an empty
stack. It then projects the source index to the cor-
responding target word and then enumerates the
target string in a left to right fashion. If it finds
a target word with a source index, it applies the
shift operation, pushing the index to the stack. Un-
less the symbol corresponds to an anchor, it tries
to apply the reduce operation. Line (4) indicates
the special treatment to the anchor. If the symbol
read is a nonterminal, then we push the entire stack
that corresponds to that nonterminal. For example,
when the algorithm reads Xd at line (6), it pushes
the entire stack from line (5).
This algorithm facilitates the incremental con-
struction of MOS which may cross rule bound-
aries. For example, at the end of the application of
Xd at line (5), the current left MOS is [5-6]. How-
ever, the algorithm grows it to [3-6] after the appli-
cation of ruleXb at line (7). Furthermore, it allows
us to compute the models from partial hypothesis.
For example, at line (5), we can compute POL by
considering [5,6] as ML to be updated with [3,6]
in line (7). This way, we expect our TNO model
would play a bigger role at decoding time.
Specific to SCFG-based translation, the values
of OL and OR are identical in the partial or in
the full derivations. For example, the orientation
values of de (that)?s left neighbor is always RA.
This statement holds, even though at the end of
Section 2, we stated that de (that)?s left neigh-
bor may have other orientation values, i.e. RG
for CL(a) = (f66 /e99). The formal proof is omit-
ted, but the intuition comes from the fact that the
derivations for SCFG-based translation are sub-
set of ?(?) and that (f66 /e99) will never become
ML forMinC(CL(a), a) respectively (chunk that
spans a and CL). Consequently, for Model 1 and
Model 2, we can obtain the model score earlier in
the decoding process.
1270
7 Experiments
Our baseline systems is a state-of-the-art string-
to-dependency system (Shen et al, 2008). The
system is trained on 10 million parallel sentences
that are available to the Phase 1 of the DARPA
BOLT Chinese-English MT task. The training cor-
pora include a mixed genre of newswire, weblog,
broadcast news, broadcast conversation, discus-
sion forums and comes from various sources such
as LDC, HK Law, HK Hansard and UN data.
In total, our baseline model employs about
40 features, including four from our proposed
Two-Neighbor Orientation model. In addition to
the standard features including the rule transla-
tion probabilities, we incorporate features that are
found useful for developing a state-of-the-art base-
line, such as the provenance features (Chiang et
al., 2011). We use a large 6-gram language model,
which was trained on 10 billion English words
from multiple corpora, including the English side
of our parallel corpus plus other corpora such as
Gigaword (LDC2011T07) and Google News. We
also train a class-based language model (Chen,
2009) on two million English sentences selected
from the parallel corpus. As the backbone of
our string-to-dependency system, we train 3-gram
models for left and right dependencies and un-
igram for head using the target side of the bi-
lingual training data. To train our Two-Neighbor
Orientation model, we select a subset of 5 million
aligned sentence pairs.
For the tuning and development sets, we set
aside 1275 and 1239 sentences selected from
LDC2010E30 corpus. We tune the decoding
weights with PRO (Hopkins and May, 2011) to
maximize BLEU-TER. As for the blind test set,
we report the performance on the NIST MT08
evaluation set, which consists of 691 sentences
from newswire and 666 sentences from weblog.
We pick the weights that produce the highest de-
velopment set scores to decode the test set.
Table 4 summarizes the experimental results on
NIST MT08 newswire and weblog. In column 2,
we report the classification accuracy on a subset of
training data. Note that these numbers are for ref-
erence only and not directly comparable with each
other since the features used in these classifiers
include several gold standard information, such
as the anchors? target words, the anchors? MOS-
related features (Model 3 & 4) and the orientation
of the right MOS (Model 2-4); all of which have
Acc MT08 nw MT08 wbBLEU TER BLEU TER
S2D - 36.77 53.28 26.34 57.41
M1 72.5 37.60 52.70 27.59 56.33
M2 77.4 37.86 52.68 27.74 56.11
M3 84.5 38.02 52.42 28.22 55.82
M4 84.5 38.55 52.41 28.44 56.45
Table 4: The NIST MT08 results on newswire (nw) and we-
blog (wb) genres. S2D is the baseline string-to-dependency
system (line 1), on top of which Two-Neighbor Orientation
Model 1 to 4 are employed (line 2-5). The best TER and
BLEU results on each genre are in bold. For BLEU, higher
scores are better, while for TER, lower scores are better.
to be predicted at decoding time.
In columns 2 and 4, we report the BLEU scores,
while in columns 3 and 5, we report the TER
scores. The performance of our baseline string-
to-dependency syntax-based SMT is shown in the
first line, followed by the performance of our Two-
Neighbor Orientation model starting from Model
1 to Model 4. As shown, the empirical results
confirm our intuition that SMT can greatly benefit
from reordering model that incorporate cross-unit
contextual information.
Model 1 provides most of the gain across the
two genres of around +0.9 to +1.2 BLEU and -0.5
to -1.1 TER. Model 2 which conditions POL on
OR provides an additional +0.2 BLEU improve-
ment on BLEU score consistently across the two
genres. As shown in line 4, we see a stronger
improvement in the inclusion of MOS-related in-
formation as features in Model 3. In newswire,
Model 3 gives an additional +0.4 BLEU and -0.2
TER, while in weblog, it gives a stronger improve-
ment of an additional +0.5 BLEU and -0.3 TER.
The inclusion of explicit MOS modeling in Model
4 gives a significant BLEU score improvement of
+0.5 but no TER improvement in newswire. In
weblog, Model 4 gives a mixed results of +0.2
BLEU score improvement and a hit of +0.6 TER.
We conjecture that the weblog text has a more am-
biguous orientation span that are more challenging
to learn. In total, our TNO model gives an encour-
aging result. Our most advanced model gives sig-
nificant improvement of +1.8 BLEU/-0.8 TER in
newswire domain and +2.1 BLEU/-1.0 TER over
a strong string-to-dependency syntax-based SMT
enhanced with additional state-of-the-art features.
1271
8 Related Work
Our work intersects with existing work in many
different respects. In this section, we mainly focus
on work related to the probabilistic conditioning
of our TNO model and the MOS modeling.
Our TNO model is closely related to the Uni-
gram Orientation Model (UOM) (Tillman, 2004),
which is the de facto reordering model of phrase-
based SMT (Koehn et al, 2007). UOM views
reordering as a process of generating (b, o) in a
left-to-right fashion, where b is the current phrase
pair and o is the orientation of b with the pre-
viously generated phrase pair b?. UOM makes
strong independence assumptions and formulates
the model as P (o|b). Tillmann and Zhang (2007)
proposed a Bigram Orientation Model (BOM) to
include both phrase pairs (b and b?) into the model.
Their original intent is to model P (o, b|o?, b?), but
perhaps due to sparsity concerns, they settle with
P (o|b, b?), dropping the conditioning on the pre-
vious orientation o?. Subsequent improvements
use the P (o|b, b?) formula, for example, for in-
corporating various linguistics feature like part-of-
speech (Zens and Ney, 2006), syntactic (Chang et
al., 2009), dependency information (Bach et al,
2009) and predicate-argument structure (Xiong et
al., 2012). Our TNO model is more faithful to the
BOM?s original formulation.
Our MOS concept is also closely related to hi-
erarchical reordering model (Galley and Manning,
2008) in phrase-based decoding, which computes
o of b with respect to a multi-block unit that may
go beyond b?. They mainly use it to avoid overes-
timating ?discontiguous? orientation but fall short
in modeling the multi-block unit, perhaps due to
data sparsity issue. Our MOS is also closely re-
lated to the efforts of modeling the span of hi-
erarchical phrases in formally syntax-based SMT.
Early works reward/penalize spans that respect the
syntactic parse constituents of an input sentence
(Chiang, 2005), and (Marton and Resnik, 2008).
(Xiong et al, 2009) learn the boundaries from
parsed and aligned training data, while (Xiong et
al., 2010) learn the boundaries from aligned train-
ing data alone. Recent work couples span mod-
eling tightly with reordering decisions, either by
adding an additional feature for each hierarchical
phrase (Chiang et al, 2008; Shen et al, 2009) or
by refining the nonterminal label (Venugopal et
al., 2009; Huang et al, 2010; Zollmann and Vo-
gel, 2011). Common to this work is that the spans
modeled may not correspond to MOS, which may
be suboptimal as discussed in Sec. 3.
In equating anchors with the function word
class, our work, particularly Model 1, is closely
related to the function word-centered model of Se-
tiawan et al (2007) and Setiawan et al (2009).
However, we provide a discriminative treatment
to the model to include a richer set of features in-
cluding the MOS modeling. Our work in incorpo-
rating global context also intersects with existing
work in Preordering Model (PM), e.g. (Niehues
and Kolss, 2009; Costa-jussa` and Fonollosa, 2006;
Genzel, 2010; Visweswariah et al, 2011; Tromble
and Eisner, 2009). The goal of PM is to reorder the
input sentence F into F ? whose order is closer to
the target language order, whereas the goal of our
model is to directly reorder F into the target lan-
guage order. The crucial difference is that we have
to integrate our model into SMT decoder, which is
highly non-trivial.
9 Conclusion
We presented a novel approach to address a kind
of long-distance reordering that requires global
cross-boundary contextual information. Our ap-
proach, which we formulate as a Two-Neighbor
Orientation model, includes the joint modeling of
two orientation decisions and the modeling of the
maximal span of the reordered chunks through the
concept of Maximal Orientation Span. We de-
scribe four versions of the model and implement
an algorithm to integrate our proposed model into
a syntax-based SMT system. Empirical results
confirm our intuition that incorporating cross-
boundaries contextual information improves trans-
lation quality. In a large scale Chinese-to-English
translation task, we achieve a significant improve-
ment over a strong baseline. In the future, we hope
to continue this line of research, perhaps by learn-
ing to identify anchors automatically from training
data, incorporating a richer set of linguistics fea-
tures such as dependency structure and strength-
ening the modeling of Maximal Orientation Span.
Acknowledgements
We would like to acknowledge the support of DARPA un-
der Grant HR0011-12-C-0015 for funding part of this work.
The views, opinions, and/or findings contained in this arti-
cle/presentation are those of the author/ presenter and should
not be interpreted as representing the official views or poli-
cies, either expressed or implied, of the DARPA.
1272
References
Nguyen Bach, Qin Gao, and Stephan Vogel. 2009.
Source-side dependency tree reordering models with
subtree movements and constraints. In Proceed-
ings of the Twelfth Machine Translation Summit
(MTSummit-XII), Ottawa, Canada, August. Interna-
tional Association for Machine Translation.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and
Christopher D. Manning. 2009. Discriminative
reordering with Chinese grammatical relations fea-
tures. In Proceedings of the Third Workshop on Syn-
tax and Structure in Statistical Translation (SSST-3)
at NAACL HLT 2009, pages 51?59, Boulder, Col-
orado, June. Association for Computational Linguis-
tics.
Stanley Chen. 2009. Shrinking exponential language
models. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 468?476, Boulder, Col-
orado, June. Association for Computational Linguis-
tics.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the
2008 Conference on Empirical Methods in Natu-
ral Language Processing, pages 224?233, Honolulu,
Hawaii, October.
David Chiang, Steve DeNeefe, and Michael Pust.
2011. Two easy improvements to lexical weighting.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, pages 455?460, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL?05), pages
263?270, Ann Arbor, Michigan, June. Association
for Computational Linguistics.
Marta R. Costa-jussa` and Jose? A. R. Fonollosa. 2006.
Statistical machine reordering. In Proceedings of
the 2006 Conference on Empirical Methods in Nat-
ural Language Processing, pages 70?76, Sydney,
Australia, July. Association for Computational Lin-
guistics.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 848?856, Honolulu, Hawaii, October. As-
sociation for Computational Linguistics.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine trans-
lation. In Proceedings of the 23rd International
Conference on Computational Linguistics (Coling
2010), pages 376?384, Beijing, China, August. Col-
ing 2010 Organizing Committee.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352?1362, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Zhongqiang Huang, Martin Cmejrek, and Bowen
Zhou. 2010. Soft syntactic constraints for hierar-
chical phrase-based translation using latent syntac-
tic distributions. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 138?147, Cambridge, MA, Octo-
ber. Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion, June.
Yuval Marton and Philip Resnik. 2008. Soft syntac-
tic constraints for hierarchical phrased-based trans-
lation. In Proceedings of The 46th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1003?
1011, Columbus, Ohio, June.
Masaaki Nagata, Kuniko Saito, Kazuhide Yamamoto,
and Kazuteru Ohashi. 2006. A clustered global
phrase reordering model for statistical machine
translation. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 713?720, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
Jan Niehues and Muntsin Kolss. 2009. A POS-based
model for long-range reorderings in SMT. In Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation, pages 206?214, Athens, Greece,
March. Association for Computational Linguistics.
Hendra Setiawan, Min-Yen Kan, and Haizhou Li.
2007. Ordering phrases with function words. In
Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 712?
719, Prague, Czech Republic, June. Association for
Computational Linguistics.
Hendra Setiawan, Min Yen Kan, Haizhou Li, and Philip
Resnik. 2009. Topological ordering of function
words in hierarchical phrase-based translation. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
1273
of the AFNLP, pages 324?332, Suntec, Singapore,
August. Association for Computational Linguistics.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008.
A new string-to-dependency machine translation al-
gorithm with a target dependency language model.
In Proceedings of ACL-08: HLT, pages 577?585,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2009. Effective use of lin-
guistic and contextual information for statistical ma-
chine translation. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 72?80, Singapore, August. Asso-
ciation for Computational Linguistics.
Christoph Tillman. 2004. A unigram orienta-
tion model for statistical machine translation. In
HLT-NAACL 2004: Short Papers, pages 101?104,
Boston, Massachusetts, USA, May 2 - May 7. Asso-
ciation for Computational Linguistics.
Christoph Tillmann and Tong Zhang. 2007. A
block bigram prediction model for statistical ma-
chine translation. ACM Transactions on Speech and
Language Processing (TSLP), 4(3).
Roy Tromble and Jason Eisner. 2009. Learning linear
ordering problems for better translation. In Proceed-
ings of the 2009 Conference on Empirical Methods
in Natural Language Processing, pages 1007?1016,
Singapore, August. Association for Computational
Linguistics.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2009. Preference grammars:
Softening syntactic constraints to improve statisti-
cal machine translation. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 236?244,
Boulder, Colorado, June. Association for Computa-
tional Linguistics.
Karthik Visweswariah, Rajakrishnan Rajkumar, Ankur
Gandhe, Ananthakrishnan Ramanathan, and Jiri
Navratil. 2011. A word reordering model for im-
proved machine translation. In Proceedings of the
2011 Conference on Empirical Methods in Natural
Language Processing, pages 486?496, Edinburgh,
Scotland, UK., July. Association for Computational
Linguistics.
Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li.
2009. A syntax-driven bracketing model for phrase-
based translation. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural
Language Processing of the AFNLP, pages 315?
323, Suntec, Singapore, August. Association for
Computational Linguistics.
Deyi Xiong, Min Zhang, and Haizhou Li. 2010.
Learning translation boundaries for phrase-based
decoding. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 136?144, Los Angeles, California,
June. Association for Computational Linguistics.
Deyi Xiong, Min Zhang, and Haizhou Li. 2012. Mod-
eling the translation of predicate-argument structure
for smt. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 902?911, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Richard Zens and Hermann Ney. 2006. Discrimina-
tive reordering models for statistical machine trans-
lation. In Human Language Technology Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics (HLT-NAACL):
Proceedings of the Workshop on Statistical Machine
Translation, pages 55?63, New York City, NY, June.
Association for Computational Linguistics.
Andreas Zollmann and Stephan Vogel. 2011. A word-
class approach to labeling pscfg rules for machine
translation. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1?11,
Portland, Oregon, USA, June. Association for Com-
putational Linguistics.
1274
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 434?439,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Improving Twitter Sentiment Analysis with Topic-Based Mixture
Modeling and Semi-Supervised Training
Bing Xiang *
IBM Watson
1101 Kitchawan Rd
Yorktown Heights, NY 10598, USA
bingxia@us.ibm.com
Liang Zhou
Thomson Reuters
3 Times Square
New York, NY 10036, USA
l.zhou@thomsonreuters.com
Abstract
In this paper, we present multiple ap-
proaches to improve sentiment analysis
on Twitter data. We first establish a
state-of-the-art baseline with a rich fea-
ture set. Then we build a topic-based sen-
timent mixture model with topic-specific
data in a semi-supervised training frame-
work. The topic information is generated
through topic modeling based on an ef-
ficient implementation of Latent Dirich-
let Allocation (LDA). The proposed sen-
timent model outperforms the top system
in the task of Sentiment Analysis in Twit-
ter in SemEval-2013 in terms of averaged
F scores.
1 Introduction
Social media, such as Twitter and Facebook, has
attracted significant attention in recent years. The
vast amount of data available online provides a
unique opportunity to the people working on nat-
ural language processing (NLP) and related fields.
Sentiment analysis is one of the areas that has
large potential in real-world applications. For ex-
ample, monitoring the trend of sentiment for a spe-
cific company or product mentioned in social me-
dia can be useful in stock prediction and product
marketing.
In this paper, we focus on sentiment analysis of
Twitter data (tweets). It is one of the challenging
tasks in NLP given the length limit on each tweet
(up to 140 characters) and also the informal con-
versation. Many approaches have been proposed
previously to improve sentiment analysis on Twit-
ter data. For example, Nakov et al (2013) provide
an overview on the systems submitted to one of the
SemEval-2013 tasks, Sentiment Analysis in Twit-
ter. A variety of features have been utilized for
* This work was done when the author was with Thom-
son Reuters.
sentiment classification on tweets. They include
lexical features (e.g. word lexicon), syntactic fea-
tures (e.g. Part-of-Speech), Twitter-specific fea-
tures (e.g. emoticons), etc. However, all of these
features only capture local information in the data
and do not take into account of the global higher-
level information, such as topic information.
Two example tweets are given below, with the
word ?offensive? appearing in both of them.
? Im gonna post something that might be offen-
sive to people in Singapore.
? #FSU offensive coordinator Randy Sanders
coached for Tennessee in 1st #BCS title
game.
Generally ?offensive? is used as a negative word
(as in the first tweet), but it bears no sentiment in
the second tweet when people are talking about a
football game. Even though some local contextual
features could be helpful to distinguish the two
cases above, they still may not be enough to get the
sentiment on the whole message correct. Also, the
local features often suffer from the sparsity prob-
lem. This motivates us to explore topic informa-
tion explicitly in the task of sentiment analysis on
Twitter data.
There exists some work on applying topic in-
formation in sentiment analysis, such as (Mei et
al., 2007), (Branavan et al, 2008), (Jo and Oh,
2011) and (He et al, 2012). All these work are
significantly different from what we propose in
this work. Also they are conducted in a domain
other than Twitter. Most recently, Si et al (2013)
propose a continuous Dirichlet Process Mixture
model for Twitter sentiment, for the purpose of
stock prediction. Unfortunately there is no eval-
uation on the accuracy of sentiment classification
alone in that work. Furthermore, no standard train-
ing or test corpus is used, which makes compari-
son with other approaches difficult.
Our work is organized in the following way:
434
? We first propose a universal sentiment model
that utilizes various features and resources.
The universal model outperforms the top
system submitted to the SemEval-2013 task
(Mohammad et al, 2013), which was trained
and tested on the same data. The universal
model serves as a strong baseline and also
provides an option for smoothing later.
? We introduce a topic-based mixture model
for Twitter sentiment. The model is inte-
grated in the framework of semi-supervised
training that takes advantage of large amount
of un-annotated Twitter data. Such a mixture
model results in further improvement on the
sentiment classification accuracy.
? We propose a smoothing technique through
interpolation between universal model and
topic-based mixture model.
? We also compare different approaches for
topic modeling, such as cross-domain topic
identification by utilizing data from newswire
domain.
2 Universal Sentiment Classifier
In this section we present a universal topic-
independent sentiment classifier to establish a
state-of-the-art baseline. The sentiment labels are
either positive, neutral or negative.
2.1 SVM Classifier
Support Vector Machine (SVM) is an effec-
tive classifier that can achieve good performance
in high-dimensional feature space. An SVM
model represents the examples as points in space,
mapped so that the examples of the different cate-
gories are separated by a clear margin as wide as
possible. In this work an SVM classifier is trained
with LibSVM (Chang and Lin, 2011), a widely
used toolkit. The linear kernel is found to achieve
higher accuracy than other kernels in our initial ex-
periments. The option of probability estimation in
LibSVM is turned on so that it can produce the
probability of sentiment class c given tweet x at
the classification time, i.e. P (c|x).
2.2 Features
The training and testing data are run through
tweet-specific tokenization, similar to that used in
the CMU Twitter NLP tool (Gimpel et al, 2011).
It is shown in Section 5 that such customized tok-
enization is helpful. Here are the features that we
use for classification:
? Word N-grams: if certain N-gram (unigram,
bigram, trigram or 4-gram) appears in the
tweet, the corresponding feature is set to 1,
otherwise 0. These features are collected
from training data, with a count cutoff to
avoid overtraining.
? Manual lexicons: it has been shown in other
work (Nakov et al, 2013) that lexicons with
positive and negative words are important to
sentiment classification. In this work, we
adopt the lexicon from Bing Liu (Hu and
Liu, 2004) which includes about 2000 posi-
tive words and 4700 negative words. We also
experimented with the popular MPQA (Wil-
son et al, 2005) lexicon but found no extra
improvement on accuracies. A short list of
Twitter-specific positive/negative words are
also added to enhance the lexicons. We gen-
erate two features based on the lexicons: total
number of positive words or negative words
found in each tweet.
? Emoticons: it is known that people use emoti-
cons in social media data to express their
emotions. A set of popular emoticons are col-
lected from the Twitter data we have. Two
features are created to represent the presence
or absence of any positive/negative emoti-
cons.
? Last sentiment word: a ?sentiment word? is
any word in the positive/negative lexicons
mentioned above. If the last sentiment word
found in the tweet is positive (or negative),
this feature is set to 1 (or -1). If none of the
words in the tweet is sentiment word, it is set
to 0 by default.
? PMI unigram lexicons: in (Mohammad et
al., 2013) two lexicons were automatically
generated based on pointwise mutual infor-
mation (PMI). One is NRC Hashtag Senti-
ment Lexicon with 54K unigrams, and the
other is Sentiment140 Lexicon with 62K un-
igrams. Each word in the lexicon has an as-
sociated sentiment score. We compute 7 fea-
tures based on each of the two lexicons: (1)
sum of sentiment score; (2) total number of
435
positive words (with score s > 1); (3) to-
tal number of negative words (s < ?1); (4)
maximal positive score; (5) minimal negative
score; (6) score of the last positive words; (7)
score of the last negative words. Note that for
the second and third features, we ignore those
with sentiment scores between -1 and 1, since
we found that inclusion of those weak subjec-
tive words results in unstable performance.
? PMI bigram lexicon: there are also 316K bi-
grams in the NRC Hashtag Sentiment Lexi-
con. For bigrams, we did not find the sen-
timent scores useful. Instead, we only com-
pute two features based on counts only: total
number of positive bigrams; total number of
negative bigrams.
? Punctuations: if there exists exclamation
mark or question mark in the tweet, the fea-
ture is set to 1, otherwise set to 0.
? Hashtag count: the number of hashtags in
each tweet.
? Negation: we collect a list of negation words,
including some informal words frequently
observed in online conversations, such as
?dunno? (?don?t know?), ?nvr? (?never?),
etc. For any sentiment words within a win-
dow following a negation word and not af-
ter punctuations ?.?, ?,?, ?;?, ???, or ?!?, we re-
verse its sentiment from positive to negative,
or vice versa, before computing the lexicon-
based features mentioned earlier. The win-
dow size was set to 4 in this work.
? Elongated words: the number of words in the
tweet that have letters repeated by at least 3
times in a row, e.g. the word ?gooood?.
3 Topic-Based Sentiment Mixture
3.1 Topic Modeling
Latent Dirichlet Allocation (LDA) (Blei et al,
2003) is one of the widely adopted generative
models for topic modeling. The fundamental idea
is that a document is a mixture of topics. For each
document there is a multinomial distribution over
topics, and a Dirichlet prior Dir(?) is introduced
on such distribution. For each topic, there is an-
other multinomial distribution over words. One of
the popular algorithms for LDA model parameter
estimation and inference is Gibbs sampling (Grif-
fiths and Steyvers, 2004), a form of Markov Chain
Monte Carlo. We adopt the efficient implementa-
tion of Gibbs sampling as proposed in (Yao et al,
2009) in this work.
Each tweet is regarded as one document. We
conduct pre-processing by removing stop words
and some of the frequent words found in Twitter
data. Suppose that there are T topics in total in the
training data, i.e. t
1
, t
2
, ..., t
T
. The posterior prob-
ability of each topic given tweet x
i
is computed as
in Eq. 1:
P
t
(t
j
|x
i
) =
C
ij
+ ?
j
?
T
k=1
C
ik
+ T?
j
(1)
where C
ij
is the number of times that topic t
j
is
assigned to some word in tweet x
i
, usually aver-
aged over multiple iterations of Gibbs sampling.
?
j
is the j-th dimension of the hyperparameter of
Dirichlet distribution that can be optimized during
model estimation.
3.2 Sentiment Mixture Model
Once we identify the topics for tweets in the train-
ing data, we can split the data into multiple sub-
sets based on topic distributions. For each subset,
a separate sentiment model can be trained. There
are many ways of splitting the data. For example,
K-means clustering can be conducted based on
the similarity between the topic distribution vec-
tors or their transformed versions. In this work,
we assign tweet x
i
to cluster j if P
t
(t
j
|x
i
) > ?
or P
t
(t
j
|x
i
) = max
k
P
t
(t
k
|x
i
). Note that this is
a soft clustering, with some tweets possibily as-
signed to multiple topic-specific clusters. Similar
to the universal model, we train T topic-specific
sentiment models with LibSVM.
During classification on test tweets, we run
topic inference and sentiment classification with
multiple sentiment models. They jointly deter-
mine the final probability of sentiment class c
given tweet x
i
as the following in a sentiment mix-
ture model:
P (c|x
i
) =
T
?
j=1
P
m
(c|t
j
, x
i
)P
t
(t
j
|x
i
) (2)
where P
m
(c|t
j
, x
i
) is the probability of sentiment
c from topic-specific sentiment model trained on
topic t
j
.
436
3.3 Smoothing
Additionally, we also experiment with a smooth-
ing technique through linear interpolation between
the universal sentiment model and topic-based
sentiment mixture model.
P (c|x
i
) = ? ? P
U
(c|x
i
) + (1? ?)
?
T
?
j=1
P
m
(c|t
j
, x
i
)P
t
(t
j
|x
i
) (3)
where ? is the interpolation parameter and
P
U
(c|x
i
) is the probability of sentiment c given
tweet x
i
from the universal sentiment model.
4 Semi-supervised Training
In this section we propose an integrated frame-
work of semi-supervised training that contains
both topic modeling and sentiment classification.
The idea of semi-supervised training is to take
advantage of large amount low-cost un-annotated
data (tweets in this case) to further improve the ac-
curacy of sentiment classification. The algorithm
is as follows:
1. Set training corpus D for sentiment classifi-
cation to be the annotated training data D
a
;
2. Train a sentiment model with current training
corpus D;
3. Run sentiment classification on the un-
annotated data D
u
with the current sentiment
model and generate probabilities of sentiment
classes for each tweet, P (c|x
i
);
4. Perform data selection. For those tweets with
P (c|x
i
) > p, add them to current training
corpus D. The rest is used to replace the un-
annotated corpus D
u
;
5. Train a topic model on D, and store the topic
inference model and topic distributions of
each tweet;
6. Cluster data in D based on the topic distribu-
tions from Step 5 and train a separate senti-
ment model for each cluster. Replace current
sentiment model with the new sentiment mix-
ture model;
7. Repeat from Step 3 until finishing a pre-
determined number of iterations or no more
data is added to D in Step 4.
5 Experimental Results
5.1 Data and Evaluation
We conduct experiments on the data from the task
B of Sentiment Analysis in Twitter in SemEval-
2013. The distribution of positive, neutral and
negative data is shown in Table 1. The develop-
ment set is used to tune parameters and features.
The test set is for the blind evaluation.
Set Pos Neu Neg Total
Training 3640 4586 1458 9684
Dev 575 739 340 1654
Test 1572 1640 601 3813
Table 1: Data from SemEval-2013. Pos: positive;
Neu: neutral; Neg: negative.
For semi-supervised training experiments, we
explored two sets of additional data. The first
one contains 2M tweets randomly sampled from
the collection in January and February 2014. The
other contains 74K news documents with 50M
words collected during the first half year of 2013
from online newswire.
For evaluation, we use macro averaged F score
as in (Nakov et al, 2013), i.e. average of the F
scores computed on positive and negative classes
only. Note that this does not make the task a binary
classification problem. Any errors related to neu-
tral class (false positives or false negatives) will
negatively impact the F scores.
5.2 Universal Model
In Table 2, we show the incremental improvement
in adding various features described in Section 2,
measured on the test set. In addition to the fea-
tures, we also find SVM weighting on the training
samples is helpful. Due to the skewness in class
distribution in the training set, it is observed dur-
ing error analysis on the development set that sub-
jective (positive/negative) tweets are more likely
to be classified as neutral tweets. The weights for
positive, neutral and negative samples are set to
be (1, 0.4, 1) based on the results on the develop-
ment set. As shown in Table 2, weighting adds a
2% improvement. With all features combined, the
universal sentiment model achieves 69.7 on aver-
age F score. The F score from the best system in
SemEval-2013 (Mohammad et al, 2013) is also
listed in the last row of Table 2 for a comparison.
437
Model Avg. F score
Baseline with word N-grams 55.0
+ tweet tokenization 56.1
+ manual lexicon features 62.4
+ emoticons 62.8
+ last sentiment word 63.7
+ PMI unigram lexicons 64.5
+ hashtag counts 65.0
+ SVM weighting 67.0
+ PMI bigram lexicons 68.2
+ negations 69.0
+ elongated words 69.7
Mohammad et al, 2013 69.0
Table 2: Results on the test set with universal sen-
timent model.
5.3 Topic-Based Mixture Model
For the topic-based mixture model and semi-
supervised training, based on the experiments on
the development set, we set the parameter ? used
in soft clustering to 0.4, the data selection pa-
rameter p to 0.96, and the interpolation parame-
ter for smoothing ? to 0.3. We found no more
noticeable benefits after two iterations of semi-
supervised training. The number of topics is set
to 100.
The results on the test set are shown Table 3,
with the topic information inferred from either
Twitter data (second column) or newswire data
(third column). The first row shows the per-
formance of the universal sentiment model as
a baseline. The second row shows the results
from re-training the universal model by simply
adding tweets selected from two iterations of
semi-supervised training (about 100K). It serves
as another baseline with more training data, for
a fair comparison with the topic-based mixture
modeling that uses the same amount of training
data.
We also conduct an experiment by only consid-
ering the most likely topic for each tweet when
computing the sentiment probabilities. The results
show that the topic-based mixture model outper-
forms both the baseline and the one that considers
the top topics only. Smoothing with the universal
model adds further improvement in addition to the
un-smoothed mixture model. With the topic in-
formation inferred from Twitter data, the F score
is 2 points higher than the baseline without semi-
Model Tweet-topic News-topic
Baseline 69.7 69.7
+ semi-supervised 70.3 70.2
top topic only 70.6 70.4
mixture 71.2 70.8
+ smoothing 71.7 71.1
Table 3: Results of topic-based sentiment mixture
model on SemEval test set.
supervised training and 1.4 higher than the base-
line with semi-supervised data.
As shown in the third column in Table 3, sur-
prisingly, the model with topic information in-
ferred from the newswire data works well on the
Twitter domain. A 1.4 points of improvement can
be obtained compared to the baseline. This pro-
vides an opportunity for cross-domain topic iden-
tification when data from certain domain is more
difficult to obtain than others.
In Table 4, we provide some examples from the
topics identified in tweets as well as the newswire
data. The most frequent words in each topic are
listed in the table. We can clearly see that the top-
ics are about phones, sports, sales and politics, re-
spectively.
Tweet-1 Tweet-2 News-1 News-2
phone game sales party
call great stores government
answer play online election
question team retail minister
service win store political
text tonight retailer prime
texting super business state
Table 4: The most frequent words in example top-
ics from tweets and newswire data.
6 Conclusions
In this paper, we presented multiple approaches
for advanced Twitter sentiment analysis. We es-
tablished a state-of-the-art baseline that utilizes a
variety of features, and built a topic-based sen-
timent mixture model with topic-specific Twitter
data, all integrated in a semi-supervised training
framework. The proposed model outperforms the
top system in SemEval-2013. Further research is
needed to continue to improve the accuracy in this
difficult domain.
438
References
David Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. In Journal of Ma-
chine Learning Research. 3(2003), 993?1022.
S. R. K. Branavan, Harr Chen, Jacob Eisenstein, and
Regina Barzilay. 2008. Learning document-level
semantic properties from free-text annotations. In
Proceedings of the Annual Meeting of the Associa-
tion for Computational Linguistics (ACL-2008).
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. In
ACM Transactions on Intelligent Systems and Tech-
nology.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for Twitter: annotation, features, and experiments.
In Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. In Proceedings of the National
Academy of Science. 101, 5228?5235.
Yulan He, Chenghua Lin, Wei Gao, and Kam-Fai
Wong. 2012. Tracking sentiment and topic dynam-
ics from social media. In Proceedings of the 6th In-
ternational AAAI Conference on Weblogs and Social
Media (ICWSM-2012).
Mingqing Hu and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In Proceedings of the
Tenth ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining.
Yohan Jo and Alice Oh. 2011. Aspect and senti-
ment unification model for online review analysis.
In Proceedings of ACM Conference in Web Search
and Data Mining (WSDM-2011).
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su,
and ChengXiang Zhai. 2007. Topic sentiment mix-
ture: modeling facets and opinions in weblogs. In
Proceedings of International Conference on World
Wide Web (WWW-2007).
Saif M. Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. NRC-Canada: Building the state-
of-the-art in sentiment analysis of tweets. In Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation (SemEval 2013). 312-320, At-
lanta, Georgia, June 14-15, 2013.
Preslav Nakov, Zornitsa Kozareva, Alan Ritter, Sara
Rosenthal, Veselin Stoyanov, and Theresa Wilson.
2013. SemEval-2013 Task 2: Sentiment Analysis in
Twitter. In Proceedings of the Seventh International
Workshop on Semantic Evaluation (SemEval 2013).
312-320, Atlanta, Georgia, June 14-15, 2013.
Jianfeng Si, Arjun Mukherjee, Bing Liu, Qing Li,
Huayi Li, and Xiaotie Deng. 2013. Exploiting topic
based Twitter sentiment for stock prediction. In Pro-
ceedings of the 51st Annual Meeting of the Associ-
ation for Computational Linguistics. 24-29, Sofia,
Bulgaria, August 4-9,2013.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
HLT 05.
Limin Yao, David Mimno, and Andrew McCallum.
2009. Efficient methods for topic model inference
on streaming document collections. KDD?09.
439
Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 61?69,
ACL HLT 2011, Portland, Oregon, USA, June 2011. c?2011 Association for Computational Linguistics
Improving Reordering for Statistical Machine Translation with Smoothed
Priors and Syntactic Features
Bing Xiang, Niyu Ge, and Abraham Ittycheriah
IBM T. J. Watson Research Center
Yorktown Heights, NY 10598
{bxiang,niyuge,abei}@us.ibm.com
Abstract
In this paper we propose several novel ap-
proaches to improve phrase reordering for
statistical machine translation in the frame-
work of maximum-entropy-based modeling.
A smoothed prior probability is introduced to
take into account the distortion effect in the
priors. In addition to that we propose multi-
ple novel distortion features based on syntac-
tic parsing. A new metric is also introduced to
measure the effect of distortion in the transla-
tion hypotheses. We show that both smoothed
priors and syntax-based features help to sig-
nificantly improve the reordering and hence
the translation performance on a large-scale
Chinese-to-English machine translation task.
1 Introduction
Over the past decade, statistical machine translation
(SMT) has evolved into an attractive area in natural
language processing. SMT takes a source sequence,
S = [s1 s2 . . . sK ] from the source language, and
generates a target sequence, T ? = [t1 t2 . . . tL], by
finding the most likely translation given by:
T ? = arg max
T
p(T |S) (1)
In most of the existing approaches, following
(Brown et al, 1993), Eq. (1) is factored using the
source-channel model into
T ? = arg max
T
p(S|T )p?(T ), (2)
where the two models, the translation model,
p(S|T ), and the language model (LM), p(T ), are es-
timated separately: the former using a parallel cor-
pus and a hidden alignment model and the latter us-
ing a typically much larger monolingual corpus. The
weighting factor ? is typically tuned on a develop-
ment test set by optimizing a translation accuracy
criterion such as BLEU (Papineni et al, 2002).
In recent years, among all the proposed ap-
proaches, the phrase-based method has become
the widely adopted one in SMT due to its capa-
bility of capturing local context information from
adjacent words. Word order in the translation
output relies on how the phrases are reordered
based on both language model scores and distor-
tion cost/penalty (Koehn et al, 2003), among all
the features utilized in a maximum-entropy (log-
linear) model (Och and Ney, 2002). The distor-
tion cost utilized during the decoding is usually a
penalty linearly proportional to the number of words
in the source sentence that are skipped in a transla-
tion path.
In this paper, we propose several novel ap-
proaches to improve reordering in the phrase-based
translation with a maximum-entropy model. In Sec-
tion 2, we review the previous work that focused on
the distortion and phrase reordering in SMT. In Sec-
tion 3, we briefly review the baseline of this work.
In Section 4, we introduce a smoothed prior prob-
ability by taking into account the distortions in the
priors. In Section 5, we present multiple novel dis-
tortion features based on syntactic parsing. A new
distortion evaluation metric is proposed in Section
6 and experimental results on a large-scale Chinese-
English machine translation task are reported in Sec-
tion 7. Section 8 concludes the paper.
61
2 Previous Work
Significant amount of research has been conducted
in the past on the word reordering problem in SMT.
In (Brown et al, 1993) IBM Models 3 through 5
model reordering based on the surface word infor-
mation. For example, Model 4 attempts to assign
target-language positions to source-language words
by modeling d(j|i,K,L) where j is the target-
language position, i is the source-language position,
K and L are respectively source and target sentence
lengths. These models are not effective in modeling
reordering because they do not have enough context
and lack structural information.
Phrase-based SMT systems such as (Koehn et al,
2003) move from using words as translation units
to using phrases. One of the advantages of phrase-
based SMT systems is that the local reordering is in-
herent in the phrase translations. However, phrase-
based SMT systems capture reordering instances
and not reordering phenomena. It has trouble to pro-
duce the right translation order if the training data
does not contain the specific phrase pairs. For ex-
ample, phrases do not capture the phenomenon that
Arabic adjectives and nouns need to be reordered.
Instead of directly modeling the distance of word
movement, some phrase-level reordering models in-
dicate how to move phrases, also called orientations.
Orientations typically apply to the adjacent phrases.
Two adjacent phrases can be either placed mono-
tonically (sometimes called straight) or swapped
(non-monotonically or inverted). In (Och and Ney,
2004; Tillmann, 2004; Kumar and Byrne, 2005; Al-
Onaizan and Papineni, 2006; Xiong et al, 2006;
Zens and Ney, 2006; Ni et al, 2009), people pre-
sented models that use lexical features from the
phrases to predict their orientations. These models
are very powerful in predicting local phrase place-
ments. In (Galley and Manning, 2008) a hierar-
chical orientation model is introduced that captures
some non-local phrase reordering by a shift reduce
algorithm. Because of the heavy use of lexical fea-
tures, these models tend to suffer from data sparse-
ness problems.
Syntax information has been used for reordering,
such as in (Xia and McCord, 2004; Collins et al,
2005; Wang et al, 2007; Li et al, 2007; Chang et
al., 2009). More recently, in (Ge, 2010) a proba-
bilistic reordering model is presented to model di-
rectly the source translation sequence and explicitly
assign probabilities to the reordering of the source
input with no restrictions on gap, length or adja-
cency. The reordering model is used to generate a re-
ordering lattice which encodes many reordering and
their costs (negative log probability). Another recent
work is (Green et al, 2010), which estimates future
linear distortion cost and presents a discriminative
distortion model that predicts word movement dur-
ing translation based on multiple features.
This work differentiates itself from all the previ-
ous work on the phrase reordering as the following.
Firstly, we propose a smoothed distortion prior prob-
ability in the maximum-entropy-based MT frame-
work. It not only takes into account the distortion
in the prior, but also alleviates the data sparseness
problem. Secondly, we propose multiple syntactic
features based on the source-side parse tree to cap-
ture the reordering phenomena between two differ-
ent languages. The correct reordering patterns will
be automatically favored during the decoding, due to
the higher weights obtained through the maximum
entropy training on the parallel data. Finally, we
also introduce a new metric to quantify the effect on
the distortions in different systems. The experiments
on a Chinese-English MT task show that these pro-
posed approaches additively improve both the dis-
tortion and translation performance significantly.
3 Maximum-Entropy Model for MT
In this section we give a brief review of a special
maximum-entropy (ME) model as introduced in (It-
tycheriah and Roukos, 2007). The model has the
following form,
p(t, j|s) = p0(t, j|s)
Z
exp
?
i
?i?i(t, j, s), (3)
where s is a source phrase, and t is a target phrase.
j is the jump distance from the previously translated
source word to the current source word. During
training j can vary widely due to automatic word
alignment in the parallel corpus. To limit the sparse-
ness created by long jumps, j is capped to a win-
dow of source words (-5 to 5 words) around the last
translated source word. Jumps outside the window
are treated as being to the edge of the window. In
62
Eq. (3), p0 is a prior distribution, Z is a normalizing
term, and ?i(t, j, s) are the features of the model,
each being a binary question asked about the source
and target streams. The feature weights ?i can be
estimated with the Improved Iterative Scaling (IIS)
algorithm.
Several categories of features have been pro-
posed:
? Lexical features that examine source word, tar-
get word and jump;
? Lexical context features that examine the pre-
vious and next source words, and also the pre-
vious two target words;
? Segmentation features based on morphological
analysis;
? Part-of-speech (POS) features that collect the
syntactic information from the source and tar-
get words;
? Coverage features that examine the coverage
status of the source words to the left and to the
right. They fire only if the left source is open
(untranslated) or the right source is closed.
 
 
                <=-5          -4             -3             -2            -1            1              2             3              4           >=5 
                                                                                         jump 
Figure 1: Counts of jumps for words with POS NN.
4 Distortion Priors
Generally the prior distribution in Eq. (3) can con-
tain any information we know about the future.
 
 
                <=-5          -4             -3             -2            -1            1              2             3              4           >=5 
                                                                                        jump 
Figure 2: Counts of jumps for words with POS NT.
In (Ittycheriah and Roukos, 2007), the normalized
phrase count is utilized as the prior, i.e.
p0(t, j|s) ?
1
l
p0(t|s) =
C(s, t)
l ? C(s) (4)
where l is the jump window size (a constant), C(s, t)
is the co-ocurrence count of phrase pair (s, t), and
C(s) is the source phrase count of s. It can be seen
that distortion j is not taken into account in Eq. (4).
The contribution of distortion solely comes from the
features. In this work, we estimate the prior proba-
bility with distortion included,
p0(t, j|s) = p0(t|s)p(j|s, t) (5)
where p(j|s, t) is the distortion probability for a
given phrase pair (s, t).
Due to the sparseness issue in the estimation of
p(j|s, t), we choose to smooth it with the global dis-
tortion probability through
p(j|s, t) = ?pl(j|s, t) + (1 ? ?)pg(j), (6)
where pl is the local distortion probability estimated
based on the counts of jumps for each phrase pair
in the training, pg is the global distortion probability
estimated on all the training data, and ? is the inter-
polation weight. In this work, pg is estimated based
on either source POS (if it?s a single-word source
phrase) or source phrase size (if it?s more than one
word long), as shown below.
pg(j) =
{
Pg(j|POS), if |s| = 1
Pg(j||s|), if |s| > 1
(7)
63
In this way, the system can differentiate the distor-
tion distributions for single source words with differ-
ent POS tags, such as adjectives versus nouns. And
in the meantime, we also differentiate the distortion
distribution with different source phrase lengths. We
show several examples of the jump distributions in
Fig. 1 and 2 collected from 1M sentence pairs in
a Chinese-to-English parallel corpus with automatic
parsing and word alignment. Fig. 1 shows the count
histogram for single-word phrases with POS tag as
NN. The distortion with j = 1, i.e. monotone, domi-
nates the distribution with the highest count. The re-
ordering with j = ?1 has the second highest count.
Such pattern is shared by most of the other POS tags.
However, Fig. 2 shows that the distribution of jumps
for NT is quite different from NN. The jump with
j = ?1 is actually the most dominant, with higher
counts than monotone translation. This is due to the
different order in English when translating Chinese
temporal nouns.
5 Distortion Features
Although the maximum entropy translation model
has an explicit indicator of distortion, j, built into
the features, we discuss in this section some novel
features that try to capture the distortion phenomena
of translation. These features are questions about the
parse tree of the source language and in particular
about the local parse node neighborhood of the cur-
rent source word being translated. Figure 3 shows an
example sentence from the Chinese-English Parallel
Treebank (LDC2009E83) and the source language
parse is displayed on the left. The features below
can be viewed as either being within a parse node
or asking about the coverage status of neighborhood
nodes.
Since these features are asking about the current
coverage, they are specific to a path in the search lat-
tice during the decoding phase of translation. Train-
ing these features is done by evaluating on the path
defined by the automatic word alignment of the par-
allel corpus sentence.
5.1 Parse Tree Modifications
The ?de? construction in Chinese is by now famous.
In order to ask more coherent questions about the
parse neighborhood, we modify the parse structures
to ?raise? the ?de? structure. The parse trees anno-
tated by the LDC have a structure as shown in Fig.
4. After raising the ?de? structure we obtain the tree
in Fig. 5.
NP-OBJ
CP
IP
...
DEC
de
QP
...
NP
NN
Figure 4: Original parse tree from LDC.
DNP
CP
IP
...
DEC
de
QP
...
NP
NN
Figure 5: The parse tree after transformation.
The transformation has been applied to the exam-
ple shown in Figure 3. The resulting flat structure
facilitates the parse sibling feature discussed below.
5.2 Parse Coverage Feature
The first set of new features we will introduce is the
source parse coverage feature. This feature is in-
terior to a source parse node and asks if the leaves
under this parse node are covered (translated) or not
so far. The feature has the following components:
?i(SourceWord, TargetWord, SourceParseParent,
jump, Coverage).
Unary parents in the source parse tree are ex-
cluded since the feature has no ambiguity in cover-
age. In Figure 3, the ?PP? node above position 5 has
two children, P, NP. When translating source posi-
tion 6, this feature indicates that the PP node has a
leaf that is already covered.
5.3 Parse Sibling Feature
The second set of new features is the source parse
sibling feature. This feature asks whether the neigh-
64
 Figure 3: Chinese-English example.
boring parse node has been covered or not. The fea-
ture includes two types:
?i(SourceWord, TargetWord, SourceParseSibling,
jump, SiblingCoverage, SiblingOrientation)
and
?i(SourcePOS, TargetPOS, SourceParseSibling,
jump, SiblingCoverage, SiblingOrientation).
Some example features for the first type are
shown in Table 1, where ?i = e?i . The coverage
status (Cov) of the parse sibling node indicates if the
node is covered completely (1), partially (2) or not
covered (0). In order to capture the relationship of
the neighborhood node, we indicate the orientation
which can be either of {left (-1), right (1)}. Given
the example shown in Figure 3, at source position
10, the system can now ask about the ?CP? structure
to the left and the ?QP? and ?NP? structures to the
right. An ?i of greater than 1.0 (meaning ?i > 0)
indicates that the feature increases the probability of
the related target block. From these examples, it?s
clear that the system prefers to produce an empty
translation for the Chinese word ?de? when the ?QP?
and ?NP? nodes to the right of it are already covered
(the first two features in Table 1) and when the ?CP?
node to left is still uncovered (the third feature). The
last feature in the table shows ?i for the case when
?CP? has already been covered.
These features are able to capture neighborhoods
that are much larger than the original baseline model
which only asked questions about the immediate
lexical neighborhood of the current source word.
Cnt ?i Tgt Src Parse Cov Orien-
Node tation
18065 2.06 e0 de QP 1 1
366153 1.99 e0 de NP 1 1
143433 3.41 e0 de CP 0 -1
99297 1.05 e0 de CP 1 -1
Table 1: Parse Sibling Word Features (e0 represents
empty target).
6 A New Distortion Evaluation Metric
MT performance is usually measured by such met-
ric as BLEU which measures the MT output as a
whole including word choice and reordering. It is
useful to measure these components separately. Un-
igram BLEU (BLEUn1) measures the precision of
word choice. We need a metric for measuring re-
ordering accuracy. The naive way of counting accu-
racy at every source position does not account for the
case of the phrasal movement. If a phrase is moved
to the wrong place, every source word in the phrase
would be penalized whereas a more reasonable met-
ric would penalize the phrase movement only once
if the phrase boundary is correct.
We propose the following pair-wise distortion
metric. From an MT output, we first extract the
source visit sequence:
Hyp:{h1,h2, . . . hn}
where hi are the visit order of the source sentence.
From the reference, we extract the true visit se-
quence:
65
Ref:{r1,r2, . . . rn}
The Pair-wise Distortion metric PDscore can be
computed as follows:
PDscore(
??
H ) =
n
?
i=1
I(hi = rj ? hi?1 = rj?1)
n
(8)
It measures how often the translation output gets
the pair-wise source visit order correct. We notice
that an MT metric named LRscore was proposed in
(Birch and Osborne, 2010). It computes the distance
between two word order sequences, which is differ-
ent from the metric we proposed here.
7 Experiments
7.1 Data and Baseline
We conduct a set of experiments on a Chinese-to-
English MT task. The training data includes the UN
parallel corpus and LDC-released parallel corpora,
with about 11M sentence pairs, 320M words in to-
tal (counted at the English side). To evaluate the
smoothed distortion priors and different features, we
use an internal data set as the development set and
the NIST MT08 evaluation set as the test set, which
includes 76 documents (691 sentences) in newswire
and 33 documents (666 sentences) in weblog, both
with 4 sets of references for each sentence. Instead
of using all the training data, we sample the training
corpus based on the dev/test set to train the system
more efficiently. The most recent and good-quality
corpora are sampled first. For the given test set, we
obtain the first 20 instances of n-grams (length from
1 to 15) from the test that occur in the training uni-
verse and the resulting sentences then form the train-
ing sample. In the end, 1M sentence pairs are se-
lected for the sampled training for each genre of the
MT08 test set.
A 5-gram language model is trained from the En-
glish Gigaword corpus and the English portion of
the parallel corpus used in the translation model
training. The Chinese parse trees are produced
by a maximum entropy based parser (Ratnaparkhi,
1997). The baseline decoder is a phrase-based de-
coder that employs both normal phrases and also
non-contiguous phrases. The value of maximum
skip is set to 9 in all the experiments. The smoothing
parameter ? for distortion prior is set to 0.9 empiri-
cally based on the results on the development set.
7.2 Distortion Evaluation
We evaluate the MT distortion using the metric in
Eq. (8) on two hand-aligned test sets. Test-278 in-
cludes 278 held-out sentences. Test-52 contains the
first 52 sentences from the MT08 Newswire set, with
the Chinese input sentences manually aligned to the
first set of reference translations. From the hand
alignment, we extract the true source visit sequence
and this is the reference.
The evaluation results are in Table 2. It is shown
that the smoothed distortion prior, parse coverage
feature and parse sibling feature each provides im-
provement on the PDscore on Test-278 and Test-52.
The final system scores are 2 to 3 points absolute
higher than the baseline scores. The state visit se-
quence in the final system is closer to the true visit
sequence than that of the baseline. This indicates
the advantage of using both parse-based syntactic
features and also the smoothed prior that takes into
account of the distortion effect. We also provide
an upper-bound in the last row by computing the
PDscore between the first and second set of refer-
ences for Test-52. The number shows the agreement
between two human translators in terms of PDscore
is around 71%.
System Test-278 Test-52
ME Baseline 44.58 48.96
+Prior 45.12 49.22
+COV 45.00 49.03
+SIB 45.43 49.20
+COV+SIB 46.16 49.45
+Prior+COV+SIB 47.68 51.04
Ref1 vs. Ref2 - 70.99
Table 2: Distortion accuracy PDscore (Prior:smoothed
distortion prior; COV:parse coverage feature; SIB:parse
sibling feature).
7.3 Translation Results
Translation results on the MT08 Newswire set and
MT08 Weblog set are listed in Table 3 and Table 4
respectively. The MT performance is measured with
the widely adopted BLEU and TER (Snover et al,
2006) metrics. We also compare the results from
different configurations with a normal phrase-based
66
System Number of Features BLEU TER
PBT n/a 29.71 59.40
ME 9,008,382 32.12 56.78
+Prior 9,008,382 32.46 56.41
+COV 9,202,431 32.48 56.50
+SIB 10,088,487 32.73 56.26
+COV+SIB 10,282,536 32.94 55.97
+Prior+COV+SIB 10,282,536 33.15 55.62
Table 3: MT results on MT08 Newswire set (PBT:normal phrase-based MT; ME:Maximum-entropy baseline;
Prior:smoothed distortion prior; COV:parse coverage feature; SIB:parse sibling feature).
System Number of Features BLEU TER
PBT n/a 20.07 62.90
ME 9,192,617 22.42 60.36
+Prior 9,192,617 22.70 60.11
+COV 9,306,967 22.69 60.14
+SIB 9,847,445 22.91 59.92
+COV+SIB 9,961,795 23.04 59.78
+Prior+COV+SIB 9,961,795 23.25 59.56
Table 4: MT results on MT08 Weblog set (PBT:normal phrase-based MT; ME:Maximum-entropy baseline;
Prior:smoothed distortion prior; COV:parse coverage feature; SIB:parse sibling feature).
SMT system (Koehn et al, 2003) that is trained on
the same training data. The number of features used
in the systems are listed in the tables.
We start from the maximum-entropy baseline, a
system implemented similarly as in (Ittycheriah
and Roukos, 2007). It utilizes multiple features as
listed in Section 3, including lexical reordering fea-
tures, and produces an already significantly better
performance than the normal phrase-based MT sys-
tem (PBT). It is around 2.5 points better in both
BLEU and TER than the PBT baseline. By adding
smoothed priors, parse coverage features or parse
sibling features each separately, the MT perfor-
mance is improved by 0.3 to 0.6. The parse sibling
feature alone provides the largest individual contri-
bution. When adding both types of new features,
the improvement is around 0.6 to 0.8 on two gen-
res. Finally, applying all three results in the best
performance (the last row). On the Newswire set,
the final system is more than 3 points better than the
PBT baseline and 1 point better than the ME base-
line. On the Weblog set, it is more than 3 points
better than PBT and 0.8 better than the ME baseline.
All the MT results above are statistically significant
with p-value < 0.0001 by using the tool described in
(Zhang and Vogel, 2004).
7.4 Analysis
To better understand the distortion and translation
results, we take a closer look at the parse-based fea-
tures. In Table 5, we list the most frequent parse sib-
ling features that are related to the Chinese phrases
with ?PP VV? structures. It is known that in Chi-
nese usually the preposition phrases (?PP?) are writ-
ten/spoken before the verbs (?VV?), with a different
order from English. Table 5 shows how such re-
ordering phenomenon is captured by the parse sib-
ling features. Recall that when ?i is greater than 1,
the system prefers the reordering with that feature
fired. When ?i is smaller than 1, the system will
penalize the corresponding translation order during
the decoding search. When the coverage is equal to
1, it means ?PP? has been translated before translat-
ing current ?VV?. As shown in the table, those fea-
tures with coverage equal to 1 have ?i lower than 1,
which will result in penalties on incorrect translation
orders.
In Fig. 6, we show the comparison between the
67
Count ?i j TgtPOS SrcPOS ParseSib Cov Orien-
Node tation
3052 1.10 5 VBD VV PP 0 -1
2662 1.10 -1 VBD VV PP 0 -1
2134 1.25 4 VBD VV PP 0 -1
50 0.73 5 VBD VV PP 1 -1
39 0.84 -5 VBD VV PP 1 -1
18 0.95 -2 VBD VV PP 1 -1
Table 5: Parse Sibling Word Features related to Chinese ?PP VV?.
 
Src1 
 
 

  

 	


 

 

 

 , 1850  2005 

 , 
 
 

 1800    (were) (at) (annual) 3%  