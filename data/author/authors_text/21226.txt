NAACL-HLT 2012 Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 47?55,
Montre?al, Canada, June 7?8, 2012. c?2012 Association for Computational Linguistics
Communication strategies for a computerized caregiver for individuals with
Alzheimer?s disease
Frank Rudzicz1,2, ? and Rozanne Wilson1 and Alex Mihailidis2 and Elizabeth Rochon1
1 Department of Speech-Language Pathology,
2 Department of Occupational Science and Occupational Therapy
University of Toronto
Toronto Canada
Carol Leonard
School of Rehabilitation Sciences
University of Ottawa
Ottawa Canada
Abstract
Currently, health care costs associated with
aging at home can be prohibitive if individ-
uals require continual or periodic supervision
or assistance because of Alzheimer?s disease.
These costs, normally associated with human
caregivers, can be mitigated to some extent
given automated systems that mimic some of
their functions. In this paper, we present in-
augural work towards producing a generic au-
tomated system that assists individuals with
Alzheimer?s to complete daily tasks using ver-
bal communication. Here, we show how to
improve rates of correct speech recognition
by preprocessing acoustic noise and by mod-
ifying the vocabulary according to the task.
We conclude by outlining current directions of
research including specialized grammars and
automatic detection of confusion.
1 Introduction
In the United States, approximately $100 billion are
spent annually on the direct and indirect care of in-
dividuals with Alzheimer?s disease (AD), the major-
ity of which is attributed to long-term institutional
care (Ernst et al, 1997). As the population ages, the
incidence of AD will double or triple, with Medi-
care costs alone reaching $189 billion in the US by
2015 (Bharucha et al, 2009). Given the growing
need to support this population, there is an increas-
ing interest in the design and development of tech-
nologies that support this population at home and
extend ones quality of life and autonomy (Mihailidis
et al, 2008).
?Contact: frank@cs.toronto.edu
Alzheimer?s disease is a type of progres-
sive neuro-degenerative dementia characterized by
marked declines in mental acuity, specifically in
cognitive, social, and functional capacity. A decline
in memory (short- and long-term), executive capac-
ity, visual-spacial reasoning, and linguistic ability
are all typical effects of AD (Cummings, 2004).
These declines make the completion of activities of
daily living (e.g., finances, preparing a meal) diffi-
cult and more severe declines often necessitate care-
giver assistance. Caregivers who assist individuals
with AD at home are common, but are often the pre-
cursor to placement in a long-term care (LTC) facil-
ity (Gaugler et al, 2009).
We are building systems that automate, where
possible, some of the support activities that currently
require family or formal (i.e., employed) caregivers.
Specifically, we are designing an intelligent dialog
component that can engage in two-way speech com-
munication with an individual in order to help guide
that individual towards the completion of certain
daily household tasks, including washing ones hands
and brushing ones teeth. A typical installation setup
in a bathroom, shown in figure 1, consists of video
cameras that track a user?s hands and the area in and
around the sink, as well as microphones, speakers,
and a screen that can display prompting informa-
tion. Similar installations are being tested in other
household rooms as part of the COACH project (Mi-
hailidis et al, 2008), according to the task; this is
an example of ambient intelligence in which tech-
nology embedded in the environment is sensitive to
the activities of the user with it (Spanoudakis et al,
2010).
47
Our goal is to encode in software the kinds of
techniques used by caregivers to help their clients
achieve these activities; this includes automati-
cally identifying and recovering from breakdowns
in communication and flexibly adapting to the in-
dividual over time. Before such a system can be de-
ployed, the underlying models need to be adjusted
to the desired population and tasks. Similarly, the
speech output component would need to be pro-
grammed according to the vocabularies, grammars,
and dialog strategies used by caregivers. This paper
presents preliminary experiments towards dedicated
speech recognition for such a system. Evaluation
data were collected as part of a larger project exam-
ining the use of communication strategies by formal
caregivers while assisting residents with moderate to
severe AD during the completion of toothbrushing
(Wilson et al, 2012).
2 Background ? communication strategies
Automated communicative systems that are more
sensitive to the emotive and the mental states of their
users are often more successful than more neutral
conversational agents (Saini et al, 2005). In order to
be useful in practice, these communicative systems
need to mimic some of the techniques employed
by caregivers of individuals with AD. Often, these
caregivers are employed by local clinics or medical
institutions and are trained by those institutions in
ideal verbal communication strategies for use with
those having dementia (Hopper, 2001; Goldfarb and
Pietro, 2004). These include (Small et al, 2003) but
are not limited to:
1. Relatively slow rate of speech rate.
2. Verbatim repetition of misunderstood prompts.
3. Closed-ended questions (i.e., that elicit yes/no
responses).
4. Simple sentences with reduced syntactic com-
plexity.
5. Giving one question or one direction at a time.
6. Minimal use of pronouns.
These strategies, though often based on observa-
tional studies, are not necessarily based on quantita-
tive empirical research and may not be generalizable
across relevant populations. Indeed, Tomoeda et al
(1990) showed that rates of speech that are too slow
(a) Environmental setup
(b) On-screen prompting
Figure 1: Setup and on-screen prompting for COACH.
The environment includes numerous sensors including
microphones and video cameras as well as a screen upon
which prompts can be displayed. In this example, the
user is prompted to lather their hands after having applied
soap. Images are copyright Intelligent Assistive Technol-
ogy and Systems Lab).
may interfere with comprehension if they introduce
48
problems of short-term retention of working mem-
ory. Small, Andersen, and Kempler (1997) showed
that paraphrased repetition is just as effective as ver-
batim repetition (indeed, syntactic variation of com-
mon semantics may assist comprehension). Further-
more, Rochon, Waters, and Caplan (2000) suggested
that the syntactic complexity of utterances is not
necessarily the only predictor of comprehension in
individuals with AD; rather, correct comprehension
of the semantics of sentences is inversely related to
the increasing number of propositions used ? it is
preferable to have as few clauses or core ideas as
possible, i.e., one-at-a-time.
Although not the empirical subject of this pa-
per, we are studying methods of automating the
resolution of communication breakdown. Much of
this work is based on the Trouble Source-Repair
(TSR) model in which difficulties in speaking, hear-
ing, or understanding are identified and repairs are
initiated and carried out (Schegloff, Jefferson, and
Sacks, 1977). Difficulties can arise in a number
of dimensions including phonological (i.e., mispro-
nunciation), morphological/syntactic (e.g., incorrect
agreement among constituents), semantic (e.g., dis-
turbances related to lexical access, word retrieval,
or word use), and discourse (i.e., misunderstanding
of topic, shared knowledge, or cohesion) (Orange,
Lubinsky, and Higginbotham, 1996). The major-
ity of TSR sequences involve self-correction of a
speaker?s own error, e.g., by repetition, elaboration,
or reduction of a troublesome utterance (Schegloff,
Jefferson, and Sacks, 1977). Orange, Lubinsky,
and Higginbotham (1996) showed that while 18%
of non-AD dyad utterances involved TSR, whereas
23.6% of early-stage AD dyads and 33% of middle-
stage AD dyads involved TSR. Of these, individu-
als with middle-stage AD exhibited more discourse-
related difficulties including inattention, failure to
track propositions and thematic information, and
deficits in working memory. The most common
repair initiators and repairs given communication
breakdown involved frequent ?wh-questions and hy-
potheses (e.g., ?Do you mean??). Conversational
partners of individuals with middle-stage AD initi-
ated repair less frequently than conversational part-
ners of control subjects, possibly aware of their de-
teriorating ability, or to avoid possible further con-
fusion. An alternative although very closely related
paradigm for measuring communication breakdown
is Trouble Indicating Behavior (TIB) in which the
confused participant implicitly or explicitly requests
aid. In a study of 7 seniors with moderate/severe de-
mentia and 3 with mild/moderate dementia, Watson
(1999) showed that there was a significant difference
in TIB use (? < 0.005) between individuals with
AD and the general population. Individuals with
AD are most likely to exhibit dysfluency, lack of up-
take in the dialog, metalinguistic comments (e.g., ?I
can?t think of the word?), neutral requests for repeti-
tion, whereas the general population are most likely
to exhibit hypothesis formation to resolve ambiguity
(e.g., ?Oh, so you mean that you had a good time??)
or requests for more information.
2.1 The task of handwashing
Our current work is based on a study completed by
Wilson et al (2012) towards a systematic observa-
tional representation of communication behaviours
of formal caregivers assisting individuals with mod-
erate to severe AD during hand washing. In that
study, caregivers produced 1691 utterances, 78% of
which contained at least one communication strat-
egy. On average, 23.35 (? = 14.11) verbal strate-
gies and 7.81 (? = 5.13) non-verbal strategies were
used per session. The five most common communi-
cation strategies employed by caregivers are ranked
in table 1. The one proposition strategy refers to
using a single direction, request, or idea in the utter-
ance (e.g. ?turn the water on?). The closed-ended
question strategy refers to asking question with a
very limited, typically binary, response (e.g., ?can
you turn the taps on??) as opposed to questions elic-
iting a more elaborate response or the inclusion of
additional information. The encouraging comments
strategy refers to any verbal praise of the resident
(e.g., ?you are doing a good job?). The paraphrased
repetition strategy is the restatement of a misunder-
stood utterance using alternative syntactic or lexical
content (e.g., ?soap up your hands....please use soap
on your hands?). There was no significant difference
between the use of paraphrased and verbatim repe-
tition of misunderstood utterances. Caregivers also
reduced speech rate from an average baseline of 116
words per minute (s.d. 36.8) to an average of 36.5
words per minute (s.d. 19.8).
The least frequently used communication strate-
49
Number of occurrences % use of strategy Uses per session
Verbal strategy Overall Successful Overall Successful Mean SD
One proposition 619 441 35 36 8.6 6.7
Closed-ended question 215 148 12 12 3.0 3.0
Encouraging comments 180 148 10 12 2.9 2.5
Use of resident?s name 178 131 10 11 2.8 2.5
Paraphrased repetition 178 122 10 10 3.0 2.5
Table 1: Most frequent verbal communication strategies according to their number of occurrences in dyad communi-
cation. The % use of strategy is normalized across all strategies, most of which are not listed. These results are split
according to the total number of uses and the number of uses in successful resolution of a communication breakdown.
Mean (and standard deviation) of uses per session are given across caregivers. Adapted with permission from Wilson
et al (2012).
gies employed by experienced caregivers involved
asking questions that required verification of a res-
ident?s request or response (e.g., ?do you mean
that you are finished??), explanation of current ac-
tions (e.g., ?I am turning on the taps for you?), and
open-ended questions (e.g., ?how do you wash your
hands??).
The most common non-verbal strategies em-
ployed by experienced caregivers were guided touch
(193 times, 122 of which were successful) in which
the caregiver physically assists the resident in the
completion of a task, demonstrating action (113
times. 72 of which were successful) in which an
action is illustrated or mimicked by the caregiver,
handing an object to the resident (107 times, 85 of
which were successful), and pointing to an object
(105 times, 95 of which were successful) in which
the direction to an object is visually indicated by
the caregiver. Some of these strategies may be em-
ployed by the proposed system; for example, videos
demonstrating an action may be displayed on the
screen shown in figure 1(a), which may replace to
some extent the mimicry by the caregiver. A pos-
sible replication of the fourth most common non-
verbal strategy may be to highlight the required ob-
ject with a flashing light, a spotlight, or by display-
ing it on screen; these solutions require tangential
technologies that are beyond the scope of this cur-
rent study, however.
3 Data
Our experiments are based on data collected by Wil-
son et al (submitted) with individuals diagnosed
with moderate-to-severe AD who were recruited
from long-term care facilities (i.e., The Harold and
Grace Baker Centre and the Lakeside Long-Term
Care Centre) in Toronto. Participants had no pre-
vious history of stroke, depression, psychosis, alco-
holism, drug abuse, or physical aggression towards
caregivers. Updated measures of disease severity
were taken according to the Mini-Mental State Ex-
amination (Folstein, Folstein, and McHugh, 1975).
The average cognitive impairment among 7 individ-
uals classified as having severe AD (scores below
10/30) was 3.43 (? = 3.36) and among 6 individ-
uals classified as having moderate AD (scores be-
tween 10/30 and 19/30) was 15.8 (? = 4.07). The
average age of residents was 81.4 years with an aver-
age of 13.8 years of education and 3.1 years of resi-
dency at their respective LTC facility. Fifteen formal
caregivers participated in this study and were paired
with the residents (i.e., as dyads) during the comple-
tion of activities of daily living. All but one care-
giver were female and were comfortable with En-
glish. The average number of years of experience
working with AD patients was 12.87 (? = 9.61).
The toothbrushing task follows the protocol of the
handwashing task. In total, the data consists of 336
utterances by the residents and 2623 utterances by
their caregivers; this is manifested by residents utter-
ing 1012 words and caregivers uttering 12166 words
in total, using 747 unique terms. The toothbrushing
task consists of 9 subtasks, namely: 1) get brush and
paste, 2) put paste on brush, 3) turn on water, 4) wet
tooth brush, 5) brush teeth, 6) rinse mouth, 7) rinse
brush, 8) turn off water, 9) dry mouth.
These data were recorded as part of a large
project to study communication strategies of care-
givers rather than to study the acoustics of their
transactions with residents. As a result, the record-
50
ings were not of the highest acoustic quality; for
example, although the sampling rate and bit rate
were high (48 kHz and 384 kbps respectively), the
video camera used was placed relatively far from the
speakers, who generally faced away from the mi-
crophone towards the sink and running water. The
distribution of strategies employed by caregivers for
this task is the subject of ongoing work.
4 Experiments in speech recognition
Our first component of an automated caregiver
is the speech recognition subsystem. We test
two alternative systems, namely Carnegie Mellon?s
Sphinx framework and Microsoft?s Speech Plat-
form. Carnegie Mellon?s Sphinx framework (pock-
etsphinx, specifically) is an open-source speech
recognition system that uses traditional N -gram
language modeling, sub-phonetic acoustic hidden
Markov models (HMMs), Viterbi decoding and
lexical-tree structures (Lamere et al, 2003). Sphinx
includes tools to perform traditional Baum-Welch
estimation of acoustic models, but there were not
enough data for this purpose. The second ASR sys-
tem, Microsoft?s Speech Platform (version 11) is
less open but exposes the ability to vary the lexicon,
grammar, and semantics. Traditionally, Microsoft
has used continuous-density HMMs with 6000 tied
HMM states (senones), 20 Gaussians per state, and
Mel-cepstrum features (with delta and delta-delta).
Given the toothbrushing data described in section
3, two sets of experiments were devised to config-
ure these systems to the task. Specifically, we per-
form preprocessing of the acoustics to remove envi-
ronmental noise associated with toothbrushing and
adapt the lexica of the two systems, as described in
the following subsections.
4.1 Noise reduction
An emergent feature of the toothbrushing data is
very high levels of acoustic noise caused by the
running of water. In fact, the estimated signal-to-
noise ratio across utterances range from ?2.103 dB
to 7.63 dB, which is extremely low; for comparison
clean speech typically has an SNR of approximately
40 dB. Since the resident is likely to be situated close
to this source of the acoustic noise, it becomes im-
portant to isolate their speech in the incoming signal.
Speech enhancement involves the removal of
acoustic noise d(t) in a signal y(t), including am-
bient noise (e.g., running water, wind) and signal
degradation giving the clean ?source? signal x(t).
This involves an assumption that noise is strictly ad-
ditive, as in the formula:
y(t) = x(t) + d(t). (1)
Here, Yk, Xk, and Dk are the kth spectra of the
noisy observation y(t), source signal x(t), and un-
correlated noise signal d(t), respectively. Generally,
the spectral magnitude of a signal is more important
than its phase when assessing signal quality and per-
forming speech enhancement. Spectral subtraction
(SS), as the name suggests, subtracts an estimate of
the noisy spectrum from the measured signal (Boll,
1979; Martin, 2001), where the estimate of the noisy
signal is estimated from samples of the noise source
exclusively. That is, one has to learn estimates based
on pre-selected recordings of noise. We apply SS
speech enhancement given sample recordings of wa-
ter running. The second method of enhancement
we consider is the log-spectral amplitude estimator
(LSAE) which minimizes the mean squared error
(MMSE) of the log spectra given a model for the
source speech Xk = Ak exp(j?k), where Ak is the
spectral amplitude. The LSAE method is a modifi-
cation to the short-time spectral amplitude estima-
tor that attempts to find some estimate A?k that min-
imizes the distortion
E
[(
logAk ? log A?k
)2
]
, (2)
such that the log-spectral amplitude estimate is
A?k = exp (E [lnAk |Yk])
=
?k
1 + ?k
exp
(
1
2
? ?
vk
e?t
t
dt
)
Rk,
(3)
where ?k is the a priori SNR,Rk is the noisy spectral
amplitude, vk =
?k
1+?k
?k, and ?k is the a posteriori
SNR (Erkelens, Jensen, and Heusdens, 2007). Of-
ten this is based on a Gaussian model of noise, as
it is here (Ephraim and Malah, 1985). We enhance
our recordings by both the SS and LSAE methods.
Archetypal instances of typical, low, and (relatively)
high SNR waveform recordings and their enhanced
versions are shown in 4.1.
51
(a) Dyad1.1
(b) Dyad4.2
(c) Dyad11.1
Figure 2: Representative samples of toothbrushing data
audio. Figures show normalized amplitude over time for
signals cleaned by the LSAE method overlaid over the
larger-amplitude original signals.
We compare the effects of this enhanced audio
across two ASR systems. For the Sphinx system,
we use a continuous tristate HMM for each of the 40
phones from the CMU dictionary trained with audio
from the complete Wall Street Journal corpus and
the independent variable we changed was the num-
ber of Gaussians per state (n. ?). These parame-
ters are not exposed by the Microsoft speech system,
so we instead vary the minimum threshold of confi-
dence C ? [0..1] required to accept a word; in theory
lower values of C would result in more insertion er-
rors and higher values would result in more deletion
errors. For each system, we used a common dic-
tionary of 123, 611 unique words derived from the
Carnegie Mellon phonemic dictionary.
Table 2 shows the word error rate for each of
the two systems. Both the SS and LSAE methods
of speech enhancement result in significantly better
word error rates than with the original recordings at
the 99.9% level of confidence according to the one-
tailed paired t-test across both systems. The LSAE
method has significantly better word error rates than
the SS method at the 99% level of confidence with
this test. Although these high WERs are impractical
for a typical system, they are comparable to other re-
sults for speech recognition in very low-SNR envi-
ronments (Kim and Rose, 2003). Deng et al (2000),
for example, describe an ASR system trained with
clean speech that has a WER of 87.11% given addi-
tive white noise for a resulting 5 dB SNR signal for
a comparable vocabulary of 5000 words. An inter-
esting observation is that even at the low confidence
threshold of C = 0.2, the number of insertion er-
rors did not increase dramatically relative to for the
higher values in the Microsoft system; only 4.0% of
all word errors were insertion errors at C = 0.2, and
2.7% of all word errors at C = 0.8.
Given Levenshtein alignments between annotated
target (reference) and hypothesis word sequences,
we separate word errors across residents and across
caregivers. Specifically, table 3 shows the propor-
tion of deletion and substitution word errors (relative
to totals for each system separately) across residents
and caregivers. This analysis aims to uncover dif-
ferences in rates of recognition between those with
AD and the more general population. For exam-
ple, 12.6% of deletion errors made by Sphinx were
words spoken by residents. It is not possible to at-
52
Word error rate %
Parameters Original SS LSAE
Sphinx
n. ? = 4 98.13 75.31 70.61
n. ? = 8 98.13 74.95 69.66
n. ? = 16 97.82 75.09 69.78
n. ? = 32 97.13 74.88 67.22
Microsoft
C = 0.8 97.67 73.59 67.11
C = 0.6 97.44 72.57 67.08
C = 0.4 96.85 71.78 66.54
C = 0.2 94.30 71.36 64.32
Table 2: Word error rates for the Sphinx and Microsoft
ASR systems according to their respective adjusted pa-
rameters, i.e., number of Gaussians per HMM state (n. ?)
and minimum confidence threshold (C). Results are given
on original recordings and waveforms enhanced by spec-
tral subraction (SS) and MMSE with log-spectral ampli-
tude estimates (LSAE).
tribute word insertion errors to either the resident or
caregiver, in general. If we assume that errors should
be distributed across residents and caregivers in the
same proportion as their respective total number of
words uttered, then we can compute the Pearson ?2
statistic of significance. Given that 7.68% of all
words were uttered by residents, the observed num-
ber of substitutions was significantly different than
the expected value at the 99% level of confidence
for both the Sphinx and Microsoft systems, but the
number of deletions was not significantly different
even at the 95% level of confidence. In either case,
however, substantially more errors are made propor-
tionally by residents than we might expect; this may
in part be caused by their relatively soft speech.
Proportion of errors
Sphinx Microsoft
Res. Careg. Res. Careg.
deletion 13.9 86.1 12.6 87.4
substitution 23.2 76.8 18.4 81.6
Table 3: Proportion of deletion and substitution errors
made by both (Res)idents and (Careg)ivers. Proportions
are relative to totals within each system.
4.2 Task-specific vocabulary
We limit the common vocabulary used in each
speech recognizer in order to be more specific to the
task. Specifically, we begin with the 747 words ut-
tered in the data as our most restricted vocabulary.
Then, we expand this vocabulary according to two
methods. The first method adds words that are se-
mantically similar to those already present. This
is performed by taking the most common sense for
each noun, verb, adjective, and adverb, then adding
each entry in the respective synonym sets accord-
ing to WordNet 3.0 (Miller, 1995). This results in
a vocabulary of 2890 words. At this point, we it-
eratively add increments of words at intervals of
10, 000 (up to 120, 000) by selecting random words
in the vocabulary and adding synonym sets for all
senses as well as antonyms, hypernyms, hyponyms,
meronyms, and holonyms. The result is a vocabu-
lary whose semantic domain becomes increasingly
generic. The second approach to adjusting the vo-
cabulary size is to add phonemic foils to more re-
stricted vocabularies. Specifically, as before, we be-
gin with the restricted 747 words observed in the
data but then add increments of new words that
are phonemically similar to existing words. This
is done exhaustively by selecting a random word
and searching for minimal phonemic misalignments
(i.e., edit distance) among out-of-vocabulary words
in the Carnegie Mellon phonemic dictionary. This
approach of adding decoy words is an attempt to
model increasing generalization of the systems. Ev-
ery vocabulary is translated into the format expected
by each recognizer so that each test involves a com-
mon set of words.
Word error rates are measured for each vocabu-
lary size across each ASR system and the manner in
which those vocabularies were constructed (seman-
tic or phonemic expansion). The results are shown
in figure 4.2 and are based on acoustics enhanced
by the LSAE method. Somewhat surprisingly, the
method used to alter the vocabulary did appear to
have a very large effect. Indeed, the WER across
the semantic and phonemic methods were correlated
at ? >= 0.99 across both ASR systems; there was
no significant difference between traces (within sys-
tem) even at the 60% level of confidence using the
two-tailed heteroscedastic t-test.
5 Ongoing work
This work represents the first phase of development
towards a complete communicative artificial care-
giver for the home. Here, we are focusing on the
53
102 103 104 105 10635
40
45
50
55
60
65
70
Vocabulary size
Wo
rd E
rror
 Ra
te (%
)
 
 
Sphinx ? Phonemic
Microsoft ? PhonemicSphinx ? Semantic
Microsoft ? Semantic
Figure 3: Word error rate versus size of vocabulary (log
scale) for each of the Sphinx and Microsoft ASR systems
according to whether the vocabularies were expanded by
semantic or phonemic similarity.
speech recognition component and have shown re-
ductions in error of up to 72% (Sphinx ASR with
n.? = 4) and 63.1% (Sphinx ASR), relative to base-
line rates of error. While significant, baseline er-
rors were so severe that other techniques will need
to be explored. We are now collecting additional
data by fixing the Microsoft Kinect sensor in the
environment, facing the resident; this is the default
configuration and may overcome some of the ob-
stacles present in our data. Specifically, the beam-
forming capabilities in the Kinect (generalizable to
other multi-microphone arrays) can isolate speech
events from ambient environmental noise (Balan and
Rosca, 2002). We are also collecting speech data for
a separate study in which individuals with AD are
placed before directional microphones and complete
tasks related to the perception of emotion.
As tasks can be broken down into non-linear (par-
tially ordered) sets of subtasks (e.g., replacing the
toothbrush is a subtask of toothbrushing), we are
specifying grammars ?by hand? specific to those sub-
tasks. Only some subset of all subtasks are possible
at any given time; e.g., one can only place tooth-
paste on the brush once both items have been re-
trieved. The possibility of these subtasks depend on
the state of the world which can only be estimated
through imperfect techniques ? typically computer
vision. Given the uncertainty of the state of the
world, we are integrating subtask-specific grammars
into a partially-observable Markov decision process
(POMDP). These grammars include the semantic
state variables of the world and break each task
down into a graph-structure of interdependent ac-
tions. Each ?action? is associated with its own gram-
mar subset of words and phrases that are likely to
be uttered during its performance, as well as a set
of prompts to be spoken by the system to aid the
user. Along these lines, we we will attempt to gen-
eralize the approach taken in section 4.2 to gener-
ate specific sub-vocabularies automatically for each
subtask. The relative weighting of words will be
modeled based on ongoing data collection.
Acknowledgments
This research was partially funded by Mitacs and
an operating grant from the Canadian Institutes of
Health Research and the American Alzheimer As-
sociation (ETAC program). The authors acknowl-
edge and thank the administrative staff, caregivers,
and residents at the Harold and Grace Baker Centre
and the Lakeside Long-Term Care Centre.
References
Balan, Radu and Justinian Rosca. 2002. Microphone
Array Speech Enhancement by Bayesian Estimation
of Spectral Amplitude and Phase. In Proceedings of
IEEE Sensor Array and Multichannel Signal Process-
ing Workshop.
Bharucha, Ashok J., Vivek Anand, Jodi Forlizzi,
Mary Amanda Dew, Charles F. Reynolds III, Scott
Stevens, and Howard Wactlar. 2009. Intelligent assis-
tive technology applications to dementia care: Current
capabilities, limitations, and future challenges. Amer-
ican Journal of Geriatric Psychiatry, 17(2):88?104,
February.
Boll, S.F. 1979. Suppression of acoustic noise in speech
using spectral subtraction. IEEE Transactions on
Acoustics, Speech, and Signal Processing, 27(2):113?
120, April.
Cummings, Jeffrey L. 2004. Alzheimer?s disease. New
England Journal of Medicine, 351(1):56?67.
Deng, Li, Alex Acero, M. Plumpe, and Xuedong Huang.
2000. Large-vocabulary speech recognition under ad-
verse acoustic environments. In Proceedings of the In-
ternational Conference on Spoken Language Process-
ing, October.
54
Ephraim, Y. and D. Malah. 1985. Speech enhancement
using a minimum mean-square error log-spectral am-
plitude estimator. Acoustics, Speech and Signal Pro-
cessing, IEEE Transactions on, 33(2):443 ? 445, apr.
Erkelens, Jan, Jesper Jensen, and Richard Heusdens.
2007. A data-driven approach to optimizing spectral
speech enhancement methods for various error crite-
ria. Speech Communication, 49:530?541.
Ernst, Richard L., Joel W. Hay, Catharine Fenn, Jared
Tinklenberg, and Jerome A. Yesavage. 1997. Cog-
nitive function and the costs of alzheimer disease ?
an exploratory study. Archives of Neurology, 54:687?
693.
Folstein, Marshal F., Susan E. Folstein, and Paul R.
McHugh. 1975. Mini-mental state: A practical
method for grading the cognitive state of patients
for the clinician. Journal of Psychiatric Research,
12(3):189?198, November.
Gaugler, J. E., F. Yu, K. Krichbaum, and J.F. Wyman.
2009. Predictors of nursing home admission for per-
sons with dementia. Medical Care, 47(2):191?198.
Goldfarb, R. and M.J.S. Pietro. 2004. Support sys-
tems: Older adults with neurogenic communication
disorders. Journal of Ambulatory Care Management,
27(4):356?365.
Hopper, T. 2001. Indirect interventions to facilitate com-
munication in Alzheimers disease. Seminars in Speech
and Language, 22(4):305?315.
Kim, Hong Kook and Richard C. Rose. 2003. Cepstrum-
Domain Acoustic Feature Compensation Based on De-
composition of Speech and Noise for ASR in Noisy
Environments. IEEE Transactions on Speech and Au-
dio Processing, 11(5), September.
Lamere, Paul, Philip Kwok, Evandro Gouvea, Bhiksha
Raj, Rita Singh, William Walker, M. Warmuth, and
Peter Wolf. 2003. The CMU Sphinx-4 speech recog-
nition system. In IEEE International Conference on
Acoustics, Speech, and Signal Processing (ICASSP
2003), Hong Kong, April.
Martin, Rainer. 2001. Noise power spectral density es-
timation based on optimal smoothing and minimum
statistics. IEEE Transactions of Speech and Audio
Processing, 9(5):504?512, July.
Mihailidis, Alex, Jennifer N Boger, Tammy Craig, and
Jesse Hoey. 2008. The COACH prompting system to
assist older adults with dementia through handwash-
ing: An efficacy study. BMC Geriatrics, 8(28).
Miller, George A. 1995. WordNet: A Lexical Database
for English. Communications of the ACM, 38(11):39?
41.
Orange, J.B., Rosemary B. Lubinsky, and D. Jeffery Hig-
ginbotham. 1996. Conversational repair by individu-
als with dementia of the alzheimer?s type. Journal of
Speech and Hearing Research, 39:881?895, August.
Rochon, Elizabeth, Gloria S. Waters, and David Caplan.
2000. The Relationship Between Measures of Work-
ing Memory and Sentence Comprehension in Patients
With Alzheimer?s Disease. Journal of Speech, Lan-
guage, and Hearing Research, 43:395?413.
Saini, Privender, Boris de Ruyter, Panos Markopoulos,
and Albert van Breemen. 2005. Benefits of social in-
telligence in home dialogue systems. In Proceedings
of INTERACT 2005, pages 510?521.
Schegloff, Emanuel A., Gail Jefferson, and Harvey
Sacks. 1977. The preference for self-correction
in the organization of repair in conversation. 1977,
53(2):361?382.
Small, Jeff A., Elaine S. Andersen, and Daniel Kempler.
1997. Effects of working memory capacity on under-
standing rate-altered speech. Aging, Neuropsychology,
and Cognition, 4(2):126?139.
Small, Jeff A., Gloria Gutman, Saskia Makela, and
Beth Hillhouse. 2003. Effectiveness of communi-
cation strategies used by caregivers of persons with
alzheimer?s disease during activities of daily living.
Journal of Speech, Language, and Hearing Research,
46(2):353?367.
Spanoudakis, Nikolaos, Boris Grabner, Christina Kot-
siopoulou, Olga Lymperopoulou, Verena Moser-
Siegmeth, Stylianos Pantelopoulos, Paraskevi Sakka,
and Pavlos Moraitis. 2010. A novel architecture and
process for ambient assisted living - the hera approach.
In Proceedings of the 10th IEEE International Confer-
ence on Information Technology and Applications in
Biomedicine (ITAB), pages 1?4.
Tomoeda, Cheryl K., Kathryn A. Bayles, Daniel R.
Boone, Alfred W. Kaszniak, and Thomas J. Slauson.
1990. Speech rate and syntactic complexity effects
on the auditory comprehension of alzheimer patients.
Journal of Communication Disorders, 23(2):151 ?
161.
Watson, Caroline M. 1999. An analysis of trou-
ble and repair in the natural conversations of people
with dementia of the Alzheimer?s type. Aphasiology,
13(3):195 ? 218.
Wilson, Rozanne, Elizabeth Rochon, Alex Mihailidis,
and Carol Le?onard. 2012. Examining success of com-
munication strategies used by formal caregivers assist-
ing individuals with alzheimer?s disease during an ac-
tivity of daily living. Journal of Speech, Language,
and Hearing Research, 55:328?341.
Wilson, Rozanne, Elizabeth Rochon, Alex Mihailidis,
and Carol Le?onard. submitted. Quantitative analy-
sis of formal caregivers? use of communication strate-
gies while assisting individuals with moderate and se-
vere alzheimer?s disease during oral care. Journal of
Speech, Language, and Hearing Research.
55
Proceedings of the 5th Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 20?28,
Baltimore, Maryland USA, August 26 2014. c?2014 Association for Computational Linguistics
Speech recognition in Alzheimer?s disease with personal assistive robots
Frank Rudzicz1,2,? and Rosalie Wang1 and Momotaz Begum3 and Alex Mihailidis2,1
1 Toronto Rehabilitation Institute, Toronto ON; 2 University of Toronto, Toronto ON;
3 University of Massachussetts Lowell
?frank@cs.toronto.edu
Abstract
To help individuals with Alzheimer?s dis-
ease live at home for longer, we are de-
veloping a mobile robotic platform, called
ED, intended to be used as a personal care-
giver to help with the performance of ac-
tivities of daily living. In a series of ex-
periments, we study speech-based inter-
actions between each of 10 older adults
with Alzheimers disease and ED as the
former makes tea in a simulated home en-
vironment. Analysis reveals that speech
recognition remains a challenge for this
recording environment, with word-level
accuracies between 5.8% and 19.2% dur-
ing household tasks with individuals with
Alzheimer?s disease. This work provides a
baseline assessment for the types of tech-
nical and communicative challenges that
will need to be overcome in human-robot
interaction for this population.
1 Introduction
Alzheimer?s disease (AD) is a progressive neu-
rodegenerative disorder primarily impairing mem-
ory, followed by declines in language, ability to
carry out motor tasks, object recognition, and ex-
ecutive functioning (American Psychiatric Asso-
ciation, 2000; Gauthier et al., 1997). An accu-
rate measure of functional decline comes from
performance in activities of daily living (ADLs),
such as shopping, finances, housework, and self-
care tasks. The deterioration in language com-
prehension and/or production resulting from spe-
cific brain damage, also known as aphasia, is a
common feature of AD and other related con-
ditions. Language changes observed clinically
in older adults with dementia include increasing
word-finding difficulties, loss of ability to verbally
express information in detail, increasing use of
generic references (e.g., ?it?), and progressing dif-
ficulties understanding information presented ver-
bally (American Psychiatric Association, 2000).
Many nations are facing healthcare crises in the
lack of capacity to support rapidly aging popula-
tions nor the chronic conditions associated with
aging, including dementia. The current healthcare
model of removing older adults from their homes
and placing them into long-term care facilities
is neither financially sustainable in this scenario
(Bharucha et al., 2009), nor is it desirable. Our
team has been developing ?smart home? systems
at the Toronto Rehabilitation Institute (TRI, part
of the University Health Network) to help older
adults ?age-in-place? by providing different types
of support, such as step-by-step prompts for daily
tasks (Mihailidis et al., 2008), responses to emer-
gency situations (Lee and Mihaildis, 2005), and
means to communicate with family and friends.
These systems are being evaluated within a com-
pletely functional re-creation of a one-bedroom
apartment located within The TRI hospital, called
HomeLab. These smart home technologies use
advanced sensing techniques and machine learn-
ing to autonomously react to their users, but they
are fixed and embedded into the environment, e.g.,
as cameras in the ceiling. Fixing the location of
these technologies carries a tradeoff between util-
ity and feasibility ? installing multiple hardware
units at all locations where assistance could be re-
quired (e.g., bathroom, kitchen, and bedroom) can
be expensive and cumbersome, but installing too
few units will present gaps where a user?s activ-
ity will not be detected. Alternatively, integrat-
ing personal mobile robots with smart homes can
overcome some of these tradeoffs. Moreover, as-
sistance provided via a physically embodied robot
is often more acceptable than that provided by an
embedded system (Klemmer et al., 2006).
With these potential advantages in mind, we
conducted a ?Wizard-of-Oz? study to explore the20
feasibility and usability of a mobile assistive robot
that uses the step-by-step prompting approaches
for daily activities originally applied to our smart
home research (Mihailidis et al., 2008). We con-
ducted the study with older adults with mild or
moderate AD and the tasks of hand washing and
tea making. Our preliminary data analysis showed
that the participants reacted well to the robot itself
and the prompts that it provided, suggesting the
feasibility of using personal robots for this appli-
cation (Begum et al., 2013). One important iden-
tified issue is the need for an automatic speech
recognition system to detect and understand ut-
terances specifically from older adults with AD.
The development of such a system will enable
the assistive robot to better understand the be-
haviours and needs of these users for effective in-
teractions and will further enhance environmental-
based smart home systems.
This paper presents an analysis of the speech
data collected from our participants with AD when
interacting with the robot. In a series of exper-
iments, we measure the performance of modern
speech recognition with this population and with
their younger caregivers with and without signal
preprocessing. This work will serve as the basis
for further studies by identifying some of the de-
velopment needs of a speech-based interface for
robotic caregivers for older adults with AD.
2 Related Work
Research in smart home systems, assistive robots,
and integrated robot/smart home systems for older
adults with cognitive impairments has often fo-
cused on assistance with activities of daily living
(i.e., reminders to do specific activities according
to a schedule or prompts to perform activity steps),
cognitive and social stimulation and emergency
response systems. Archipel (Serna et al., 2007)
recognizes the user?s intended plan and provides
prompts, e.g. with cooking tasks. Autominder,
(Pollack, 2006), provides context-appropriate re-
minders for activity schedules, and the COACH
(Cognitive Orthosis for Assisting with aCtivities
in the Home) system prompts for the task of hand-
washing (Mihailidis et al., 2008) and tea-making
(Olivier et al., 2009). Mynatt et al. (2004) have
been developing technologies to support aging-in-
place such as the Cooks Collage, which uses a se-
ries of photos to remind the user what the last step
completed was if the user is interrupted during a
cooking task. These interventions tend to be em-
bedded in existing environments (e.g., around the
sink area).
More recent innovations have examined in-
tegrated robot-smart home systems where sys-
tems are embedded into existing environments that
communicate with mobile assistive robots (e.g.,
CompanionAble, (Mouad et al., 2010); Mobiserv
Kompai, (Lucet, 2012); and ROBADOM (Tapus
and Chetouani, 2010)). Many of these projects
are targeted towards older adults with cognitive
impairment, and not specifically those with sig-
nificant cognitive impairment. One of these sys-
tems, CompanionAble, with a fully autonomous
assistive robot, has recently been tested in a simu-
lated home environment for two days each with
four older adults with dementia (AD or Pick?s
disease/frontal lobe dementia) and two with mild
cognitive impairment. The system provides assis-
tance with various activities, including appoint-
ment reminders for activities input by users or
caregivers, video calls, and cognitive exercises.
Participants reported an overall acceptance of the
system and several upgrades were reported, in-
cluding a speech recognition system that had to be
deactivated by the second day due to poor perfor-
mance.
One critical component for the successful use of
these technological interventions is the usability of
the communication interface for the targeted users,
in this case older adults with Alzheimer?s disease.
As in communication between two people, com-
munication between the older adult and the robot
may include natural, freeform speech (as opposed
to simple spoken keyword interaction) and non-
verbal cues (e.g., hand gestures, head pose, eye
gaze, facial feature cues), although speech tends to
be far more effective (Green et al., 2008; Goodrich
and Schultz, 2007). Previous research indicates
that automated communication systems are more
effective if they take into account the affective
and mental states of the user (Saini et al., 2005).
Indeed, speech appears to be the most powerful
mode of communication for an assistive robot to
communicate with its users (Tapus and Chetouani,
2010; Lucet, 2012).
2.1 Language use in dementia and
Alzheimer?s disease
In order to design a speech interface for individ-
uals with dementia, and AD in particular, it is21
important to understand how their speech differs
from that of the general population. This then can
be integrated into future automatic speech recog-
nition systems. Guinn and Habash (2012) showed,
through an analysis of conversational dialogs, that
repetition, incomplete words, and paraphrasing
were significant indicators of Alzheimer?s dis-
ease relative but several expected measures such
as filler phrases, syllables per minute, and pro-
noun rate were not. Indeed, pauses, fillers, for-
mulaic speech, restarts, and speech disfluencies
are all hallmarks of speech in individuals with
Alzheimer?s (Davis and Maclagan, 2009; Snover
et al., 2004). Effects of Alzheimer?s disease on
syntax remains controversial, with some evidence
that deficits in syntax or of agrammatism could be
due to memory deficits in the disease (Reilly et al.,
2011).
Other studies has applied similar analyses to
related clinical groups. Pakhomov et al. (2010)
identified several different features from the au-
dio and corresponding transcripts of 38 patients
with frontotemporal lobar degeneration (FTLD).
They found that pause-to-word ratio and pronoun-
to-noun ratios were especially discriminative of
FTLD variants and that length, hesitancy, and
agramatism correspond to the phenomenology of
FTLD. Roark et al. (2011) tested the ability of an
automated classifier to distinguish patients with
mild cognitive impairment from healthy controls
that include acoustic features such as pause fre-
quency and duration.
2.2 Human-robot interaction
Receiving assistance from an entity with a physi-
cal body (such as a robot) is often psychologically
more acceptable than receiving assistance from an
entity without a physical body (such as an em-
bedded system) (Klemmer et al., 2006). Physical
embodiment also opens up the possibility of hav-
ing more meaningful interaction between the older
adult and the robot, as discussed in Section 5.
Social collaboration between humans and
robots often depends on communication in which
each participant?s intention and goals are clear
(Freedy et al., 2007; Bauer et al., 2008; Green
et al., 2008). It is important that the human
participant is able to construct a useable ?men-
tal model? of the robot through bidirectional com-
munication (Burke and Murphy, 1999) which can
include both natural speech and non-verbal cues
(e.g., hand gestures, gaze, facial cues), although
speech tends to be far more effective (Green et al.,
2008; Goodrich and Schultz, 2007).
Automated communicative systems that are
more sensitive to the emotive and the mental states
of their users are often more successful than more
neutral conversational agents (Saini et al., 2005).
In order to be useful in practice, these commu-
nicative systems need to mimic some of the tech-
niques employed by caregivers of individuals with
AD. Often, these caregivers are employed by lo-
cal clinics or medical institutions and are trained
by those institutions in ideal verbal communica-
tion strategies for use with those having demen-
tia (Hopper, 2001; Goldfarb and Pietro, 2004).
These include (Wilson et al., 2012) but are not
limited to relatively slow rate of speech, verba-
tim repetition of misunderstood prompts, closed-
ended (e.g., ?yes/no?) questions, and reduced syn-
tactic complexity. However, Tomoeda et al. (1990)
showed that rates of speech that are too slow
may interfere with comprehension if they intro-
duce problems of short-term retention of working
memory. Small et al. (1997) showed that para-
phrased repetition is just as effective as verbatim
repetition (indeed, syntactic variation of common
semantics may assist comprehension). Further-
more, Rochon et al. (2000) suggested that the syn-
tactic complexity of utterances is not necessarily
the only predictor of comprehension in individuals
with AD; rather, correct comprehension of the se-
mantics of sentences is inversely related to the in-
creasing number of propositions used ? it is prefer-
able to have as few clauses or core ideas as possi-
ble, i.e., one-at-a-time.
3 Data collection
The data in this paper come from a study to
examine the feasibility and usability of a per-
sonal assistive robot to assist older adults with
AD in the completion of daily activities (Begum
et al., 2013). Ten older adults diagnosed with
AD, aged ? 55, and their caregivers were re-
cruited from a local memory clinic in Toronto,
Canada. Ethics approval was received from the
Toronto Rehabilitation Institute and the Univer-
sity of Toronto. Inclusion criteria included fluency
in English, normal hearing, and difficulty com-
pleting common sequences of steps, according to
their caregivers. Caregivers had to be a family
or privately-hired caregiver who provides regular22
care (e.g., 7 hours/week) to the older adult partici-
pant. Following informed consent, the older adult
participants were screened using the Mini Mental
State Exam (MMSE) (Folstein et al., 2001) to as-
certain their general level of cognitive impairment.
Table 1 summarizes relevant demographics.
Sex Age (years) MMSE (/30)
OA1 F 76 9
OA2 M 86 24
OA3 M 88 25
OA4 F 77 25
OA5 F 59 18
OA6 M 63 23
OA7 F 77 25
OA8 F 83 19
OA9 F 84 25
OA10 M 85 15
Table 1: Demographics of older adults (OA).
(a)
(b)
Figure 1: ED and two participants with AD during
the tea-making task in the kitchen of HomeLab at
TRI.
3.1 ED, the personal caregiver robot
The robot was built on an iRobot base (operat-
ing speed: 28 cm/second) and both its internal
construction and external enclosure were designed
and built at TRI. It is 102 cm in height and has
separate body and head components; the latter is
primarily a LCD monitor that shows audiovisual
prompts or displays a simple ?smiley face? other-
wise, as shown in Figure 2. The robot has two
speakers embedded in its ?chest?, two video cam-
eras (one in the head and one near the floor, for
navigation), and a microphone. For this study,
the built-in microphones were not used in favor of
environmental Kinect microphones, discussed be-
low. This was done to account for situations when
the robot and human participant were not in the
same room simultaneously.
The robot was tele-operated throughout the
task. The tele-operator continuously monitored
the task progress and the overall affective state
of the participants in a video stream sent by the
robot and triggered social conversation, asked
task-related questions, and delivered prompts to
guide the participants towards successful comple-
tion of the tea-making task (Fig. 1).
Figure 2: The prototype robotic caregiver, ED.
The robot used the Cepstral commercial text-to-
speech (TTS) system using the U.S. English voice
?David? and its default parameters. This system
is based on the Festival text-to-speech platform in
many respects, including its use of linguistic pre-
processing (e.g., part-of-speech tagging) and cer-
tain heuristics (e.g., letter-to-sound rules). Spo-
ken prompts consisted of simple sentences, some-
times accompanied by short video demonstrations
designed to be easy to follow by people with a cog-
nitive impairment.
For efficient prompting, the tea-making task
was broken down into different steps or sub-task.
Audio or audio-video prompts corresponding to23
each of these sub-tasks were recorded prior to
data collection. The human-robot interaction pro-
ceeded according to the following script when col-
laborating with the participants:
1. Allow the participant to initiate steps in each
sub-task, if they wish.
2. If a participant asks for directions, deliver the
appropriate prompt.
3. If a participant requests to perform the sub-
task in their own manner, agree if this does
not involve skipping an essential step.
4. If a participant asks about the location of an
item specific to the task, provide a full-body
gesture by physically orienting the robot to-
wards the sought item.
5. During water boiling, ask the participant to
put sugar or milk or tea bag in the cup. Time
permitting, engage in a social conversation,
e.g., about the weather.
6. When no prerecorded prompt sufficiently an-
swers a participant question, respond with the
correct answer (or ?I don?t know?) through
the TTS engine.
3.2 Study set-up and procedures
Consent included recording video, audio, and
depth images with the Microsoft Kinect sensor in
HomeLab for all interviews and interactions with
ED. Following informed consent, older adults and
their caregivers were interviewed to acquire back-
ground information regarding their daily activi-
ties, the set-up of their home environment, and the
types of assistance that the caregiver typically pro-
vided for the older adult.
Participants were asked to observe ED mov-
ing in HomeLab and older adult participants were
asked to have a brief conversation with ED to
become oriented with the robot?s movement and
speech characteristics. The older adults were
then asked to complete the hand-washing and tea-
making tasks in the bathroom and kitchen, respec-
tively, with ED guiding them to the locations and
providing specific step-by-step prompts, as neces-
sary. The tele-operator observed the progress of
the task, and delivered the pre-recorded prompts
corresponding to the task step to guide the older
adult to complete each task. The TTS system
was used to respond to task-related questions and
to engage in social conversation. The caregivers
were asked to observe the two tasks and to in-
tervene only if necessary (e.g., if the older adult
showed signs of distress or discomfort). The
older adult and caregiver participants were then
interviewed separately to gain their feedback on
the feasibility of using such a robot for assis-
tance with daily activities and usability of the sys-
tem. Each study session lasted approximately 2.5
hours including consent, introduction to the robot,
tea-making interaction with the robot, and post-
interaction interviews. The average duration for
the tea-making task alone was 12 minutes.
4 Experiments and analysis
Automatic speech recognition given these data is
complicated by several factors, including a pre-
ponderance of utterances in which human care-
givers speak concurrently with the participants, as
well as inordinately challenging levels of noise.
The estimated signal-to-noise ratio (SNR) across
utterances range from?3.42 dB to 8.14 dB, which
is extremely low compared to typical SNR of 40
dB in clean speech. One cause of this low SNR
is that microphones are placed in the environment,
rather than on the robot (so the distance to the mi-
crophone is variable, but relatively large) and that
the participant often has their back turned to the
microphone, as shown in figure 1.
As in previous work (Rudzicz et al., 2012),
we enhance speech signals with the log-spectral
amplitude estimator (LSAE) which minimizes the
mean squared error of the log spectra given a
model for the source speech Xk = Ake(j?k),
where Ak is the spectral amplitude. The LSAE
method is a modification of the short-time spectral
amplitude estimator that finds an estimate of the
spectral amplitude, A?k, that minimizes the distor-
tion
E
[(
logAk ? log A?k
)2]
, (1)
such that the log-spectral amplitude estimate is
A?k = exp (E [lnAk |Yk])
= ?k1 + ?k
exp
(1
2
? ?
vk
e?t
t dt
)
Rk,
(2)
where ?k is the a priori SNR,Rk is the noisy spec-
tral amplitude, vk = ?k1+?k ?k, and ?k is the a pos-teriori SNR (Erkelens et al., 2007). Often this is
based on a Gaussian model of noise, as it is here
(Ephraim and Malah, 1985).24
As mentioned, there are many utterances in
which human caregivers speak concurrently with
the participants. This is partially confounded by
the fact that utterances by individuals with AD
tend to be shorter, so more of their utterance is lost,
proportionally. Examples of this type where the
caregiver?s voice is louder than the participant?s
voice are discarded, amounting to about 10% of
all utterances. In the following analyses, func-
tion words (i.e., prepositions, subordinating con-
junctions, and determiners) are removed from con-
sideration, although interjections are kept. Proper
names are also omitted.
We use the HTK (Young et al., 2006) toolchain,
which provides an implementation of a semi-
continuous hidden Markov model (HMM) that al-
lows state-tying and represents output densities by
mixtures of Gaussians. Features consisted of the
first 13 Mel-frequency cepstral coefficients, their
first (?) and second (??) derivatives, and the log
energy component, for 42 dimensions. Our own
data were z-scaled regardless of whether LSAE
noise reduction was applied.
Two language models (LMs) are used, both tri-
gram models derived from the English Gigaword
corpus, which contains 1200 word tokens (Graff
and Cieri, 2003). The first LM uses the first 5000
most frequent words and the second uses the first
64,000 most frequent words of that corpus. Five
acoustic models (AMs) are used with 1, 2, 4, 8,
and 16 Gaussians per output density respectively.
These are trained with approximately 211 hours
of spoken transcripts of the Wall Street Journal
(WSJ) from over one hundred non-pathological
speakers (Vertanen, 2006).
Table 2 shows, for the small- and large-
vocabulary LMs, the word-level accuracies of the
baseline HTK ASR system, as determined by
the inverse of the Levenshtein edit distance, for
two scenarios (sit-down interviews vs. during
the task), with and without LSAE noise reduc-
tion, for speech from individuals with AD and
for their caregivers. These values are computed
over all complexities of acoustic model and are
consistent with other tasks of this type (i.e., with
the challenges associated with the population and
recording set up), with this type of relatively un-
constrained ASR (Rudzicz et al., 2012). Apply-
ing LSAE results in a significant increase in ac-
curacy for both the small-vocabulary (right-tailed
homoscedastic t(58) = 3.9, p < 0.005, CI =
[6.19,?]) and large-vocabulary (right-tailed ho-
moscedastic t(58) = 2.4, p < 0.01, CI =
[2.58,?]) tasks. For the participants with AD,
ASR accuracy is significantly higher in inter-
views (paired t(39) = 8.7, p < 0.0001, CI =
[13.8,?]), which is expected due in large part
to the closer proximity of the microphone. Sur-
prisingly, ASR accuracy on participants with ASR
was not significantly different than on caregivers
(two-tailed heteroscedastic t(78) = ?0.32, p =
0.75, CI = [?5.54, 4.0]).
Figure 3 shows the mean ASR accuracy, with
standard error (?/?n), for each of the small-
vocabulary and large-vocabulary ASR systems.
The exponential function b0 + b1 exp(b2x) is fit
to these data for each set, where bi are coef-
ficients that are iteratively adjustable via mean
squared error. For the small-vocabulary data,
R2 = 0.277 and F8 = 3.06, p = 0.12 ver-
sus the constant model. For the large-vocabulary
data, R2 = 0.445 and F8 = 2.81, p = 0.13
versus the constant model. Clearly, there is an
increasing trend in ASR accuracy with MMSE
scores, however an n-way ANOVA on ASR ac-
curacy scores reveals that this increase is not sig-
nificant (F1 = 47.07, p = 0.164). Furthermore,
neither the age (F1 = 1.39, p = 0.247) nor the sex
(F1 = 0.98, p = 0.33) of the participant had a sig-
nificant effect on ASR accuracy. An additional n-
way ANOVA reveals no strong interaction effects
between age, sex, and MMSE.
8 10 12 14 16 18 20 22 24 2610
15
20
25
30
35
MMSE score
AS
R a
ccu
racy
 (%)
 
 Small vocab
Large vocab
Figure 3: MMSE score versus mean ASR accu-
racy (with std. error bars) and fits of exponential
regression for each of the small-vocabulary and
large-vocabulary ASR systems.
25
Scenario Noise reduction AD caregiver
Small vocabulary
Interview None 25.1 (? = 9.9) 28.8 (? = 6.0)LSAE 40.9 (? = 5.6) 40.2 (? = 5.3)
In task None 13.7 (? = 3.7) -LSAE 19.2 (? = 9.8) -
Large vocabulary
Interview None 23.7 (? = 12.9) 27.0 (? = 10.0)LSAE 38.2 (? = 6.3) 35.1 (? = 11.2)
In task None 5.8 (? = 3.7) -LSAE 14.3 (? = 12.8) -
Table 2: ASR accuracy (means, and std. dev.) across speakers, scenario (interviews vs. during the task),
and presence of noise reduction for the small and large language models.
5 Discussion
This study examined low-level aspects of speech
recognition among older adults with Alzheimer?s
disease interacting with a robot in a simulated
home environment. The best word-level accura-
cies of 40.9% (? = 5.6) and 39.2% (? = 6.3)
achievable with noise reduction and in a quiet in-
terview setting are comparable with the state-of-
the-art in unrestricted large-vocabulary text entry.
These results form the basis for ongoing work in
ASR and interaction design for this domain. The
trigram language model used in this work encap-
sulates the statistics of a large amount of speech
from the general population ? it is a speaker-
independent model derived from a combination
of English news agencies that is not necessarily
representative of the type of language used in the
home, or by our target population. The acoustic
models were also derived from newswire data read
by younger adults in quiet environments. We are
currently training and adapting language models
tuned specifically to older adults with Alzheimer?s
disease using data from the Carolina Conversa-
tions database (Pope and Davis, 2011) and the De-
mentiaBank database (Boller and Becker, 1983).
Additionally, to function realistically, a lot of
ambient and background noise will need to be
overcome. We are currently looking into deploy-
ing a sensor network in the HomeLab that will in-
clude microphone arrays. Another method of im-
proving rates of correct word recognition is to aug-
ment the process from redundant information from
a concurrent sensory stream, i.e., in multimodal
interaction (Rudzicz, 2006). Combining gesture
and eye gaze with speech, for example, can be
used to disambiguate speech-only signals.
Although a focus of this paper, verbal infor-
mation is not the only modality in which human-
robot interaction can take place. Indeed, Wil-
son et al. (2012) showed that experienced human
caregivers employed various non-verbal and semi-
verbal strategies to assist older adults with demen-
tia about 1/3 as often as verbal strategies (see sec-
tion 2.2). These non-verbal and semi-verbal strate-
gies included eye contact, sitting face-to-face, us-
ing hand gestures, a calm tone of voice, instru-
mental touch, exaggerated facial expressions, and
moving slowly. Multi-modal communication can
be extremely important for individuals with de-
mentia, who may require redundant channels for
disambiguating communication problems, espe-
cially if they have a language impairment or a sig-
nificant hearing impairment.
It is vital that our current technological ap-
proaches to caring for the elderly in their homes
progresses quickly, given the demographic shift
in many nations worldwide. This paper provides
a baseline assessment for the types of technical
and communicative challenges that will need to be
overcome in the near future to provide caregiving
assistance to a growing number of older adults.
6 Acknowledgements
The authors would like to thank Rajibul Huq and
Colin Harry, who designed and built the robot,
Jennifer Boger and Goldie Nejat for their assis-
tance in designing the study, and Sharon Cohen
for her consultations during the study.
References
American Psychiatric Association. 2000. Delirium,
dementia, and amnestic and other cognitive disor-
ders. In Diagnostic and Statistical Manual of Men-
tal Disorders, Text Revision (DSM-IV-TR), chap-
ter 2. American Psychiatric Association, Arlington,
VA, fourth edition.26
A. Bauer, D. Wollherr, and M. Buss. 2008. Human-
robot collaboration: A survey. International Journal
of Humanoid Robotics, 5:47?66.
Momotaz Begum, Rosalie Wang, Rajibul Huq, and
Alex Mihailidis. 2013. Performance of daily ac-
tivities by older adults with dementia: The role of
an assistive robot. In Proceedings of the IEEE In-
ternational Conference on Rehabilitation Robotics,
Washington USA, June.
Ashok J. Bharucha, Vivek Anand, Jodi Forlizzi,
Mary Amanda Dew, Charles F. Reynolds III, Scott
Stevens, and Howard Wactlar. 2009. Intelligent
assistive technology applications to dementia care:
Current capabilities, limitations, and future chal-
lenges. American Journal of Geriatric Psychiatry,
17(2):88?104, February.
Franc?ois Boller and James Becker. 1983. Dementia-
Bank database.
J.L. Burke and R.R. Murphy. 1999. Situation
awareness, team communication, and task perfor-
mance in robot-assisted technical search: Bujold
goes to bridgeport. CMPSCI Tech. Rep. CRASAR-
TR2004-23, University of South Florida.
B. Davis and M. Maclagan. 2009. Examining
pauses in Alzheimer?s discourse. American jour-
nal of Alzheimer?s Disease and other dementias,
24(2):141?154.
Y. Ephraim and D. Malah. 1985. Speech enhancement
using a minimum mean-square error log-spectral
amplitude estimator. Acoustics, Speech and Signal
Processing, IEEE Transactions on, 33(2):443 ? 445,
apr.
Jan Erkelens, Jesper Jensen, and Richard Heusdens.
2007. A data-driven approach to optimizing spec-
tral speech enhancement methods for various error
criteria. Speech Communication, 49:530?541.
M. F. Folstein, S. E. Folstein, T. White, and M. A.
Messer. 2001. Mini-Mental State Examination
user?s guide. Odessa (FL): Psychological Assess-
ment Resources.
A. Freedy, E. de Visser, G. Weltman, and N. Coeyman.
2007. Measurement of trust in human-robot collab-
oration. In Proceedings of International Conference
on Collaborative Technologies and Systems, pages
17 ?24.
Serge Gauthier, Michel Panisset, Josephine Nalban-
toglu, and Judes Poirier. 1997. Alzheimer?s dis-
ease: current knowledge, management and research.
Canadian Medical Association Journal, 157:1047?
1052.
R. Goldfarb and M.J.S. Pietro. 2004. Support systems:
Older adults with neurogenic communication dis-
orders. Journal of Ambulatory Care Management,
27(4):356?365.
M. A. Goodrich and A. C. Schultz. 2007. Human-
robot interaction: A survey. Foundations and Trends
in Human-Computer Interaction, 1:203?275.
David Graff and Christopher Cieri. 2003. English gi-
gaword. Linguistic Data Consortium.
S. A. Green, M. Billinghurst, X. Chen, and J. G. Chase.
2008. Human-robot collaboration: A literature re-
view and augmented reality approach in design. In-
ternational Journal Advanced Robotic Systems, 5:1?
18.
Curry Guinn and Anthony Habash. 2012. Technical
Report FS-12-01, Association for the Advancement
of Artificial Intelligence.
T Hopper. 2001. Indirect interventions to facilitate
communication in Alzheimers disease. Seminars in
Speech and Language, 22(4):305?315.
S. Klemmer, B. Hartmann, and L. Takayama. 2006.
How bodies matter: five themes for interaction de-
sign. In Proceedings of the conference on Designing
Interactive systems, pages 140?149.
Tracy Lee and Alex Mihaildis. 2005. An intelligent
emergency response system: Preliminary develop-
ment and testing of automated fall detection. Jour-
nal of Telemedicine and Telecare, 11:194?198.
Eric Lucet. 2012. Social Mobiserv Kompai Robot to
Assist People. In euRobotics workshop on Robots in
Healthcare and Welfare.
Alex Mihailidis, Jennifer N Boger, Tammy Craig, and
Jesse Hoey. 2008. The COACH prompting system
to assist older adults with dementia through hand-
washing: An efficacy study. BMC Geriatrics, 8(28).
Mehdi Mouad, Lounis Adouane, Pierre Schmitt,
Djamel Khadraoui, Benjamin Ga?teau, and Philippe
Martinet. 2010. Multi-agents based system to coor-
dinate mobile teamworking robots. In Proceedings
of the 4th Companion Robotics Institute, Brussels.
Elizabeth D. Mynatt, Anne-Sophie Melenhorst,
Arthur D. Fisk, and Wendy A. Rogers. 2004. Aware
technologies for aging in place: Understanding user
needs and attitudes. IEEE Pervasive Computing,
3:36?41.
Patrick Olivier, Andrew Monk, Guangyou Xu, and
Jesse Hoey. 2009. Ambient kitchen: Designing
situation services using a high fidelity prototyping
environment. In Proceedings of the ACM 2nd Inter-
national Conference on Pervasive Technologies Re-
lated to Assistive Environments, Corfu Greece.
S. V. Pakhomov, G. E. Smith, D. Chacon, Y. Feliciano,
N. Graff-Radford, R. Caselli, and D. S. Knopman.
2010. Computerized analysis of speech and lan-
guage to identify psycholinguistic correlates of fron-
totemporal lobar degeneration. Cognitive and Be-
havioral Neurology, 23:165?177.27
M. E. Pollack. 2006. Autominder: A case study of as-
sistive technology for elders with cognitive impair-
ment. Generations, 30:67?69.
Charlene Pope and Boyd H. Davis. 2011. Finding
a balance: The Carolinas Conversation Collection.
Corpus Linguistics and Linguistic Theory, 7(1).
J. Reilly, J. Troche, and M. Grossman. 2011. Lan-
guage processing in dementia. In A. E. Budson and
N. W. Kowall, editors, The Handbook of Alzheimer?s
Disease and Other Dementias. Wiley-Blackwell.
Brian Roark, Margaret Mitchell, John-Paul Hosom,
Kristy Hollingshead, and Jeffery Kaye. 2011. Spo-
ken language derived measures for detecting mild
cognitive impairment. IEEE Transactions on Au-
dio, Speech, and Language Processing, 19(7):2081?
2090.
Elizabeth Rochon, Gloria S. Waters, and David Ca-
plan. 2000. The Relationship Between Measures
of Working Memory and Sentence Comprehension
in Patients With Alzheimer?s Disease. Journal of
Speech, Language, and Hearing Research, 43:395?
413.
Frank Rudzicz, Rozanne Wilson, Alex Mihailidis, Eliz-
abeth Rochon, and Carol Leonard. 2012. Commu-
nication strategies for a computerized caregiver for
individuals with alzheimer?s disease. In Proceed-
ings of the Third Workshop on Speech and Language
Processing for Assistive Technologies (SLPAT2012)
at the 13th Annual Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics (NAACL 2012), Montreal Canada, June.
Frank Rudzicz. 2006. Clavius: Bi-directional parsing
for generic multimodal interaction. In Proceedings
of the joint meeting of the International Conference
on Computational Linguistics and the Association
for Computational Linguistics, Sydney Australia.
Privender Saini, Boris de Ruyter, Panos Markopoulos,
and Albert van Breemen. 2005. Benefits of social
intelligence in home dialogue systems. In Proceed-
ings of INTERACT 2005, pages 510?521.
A. Serna, H. Pigot, and V. Rialle. 2007. Modeling the
progression of alzheimer?s disease for cognitive as-
sistance in smart homes. User Modelling and User-
Adapted Interaction, 17:415?438.
Jeff A. Small, Elaine S. Andersen, and Daniel Kem-
pler. 1997. Effects of working memory capacity
on understanding rate-altered speech. Aging, Neu-
ropsychology, and Cognition, 4(2):126?139.
M. Snover, B. Dorr, and R. Schwartz. 2004. A
lexically-driven algorithm for disfluency detection.
In ?Proceedings of HLT-NAACL 2004: Short Papers,
pages 157?160.
Adriana Tapus and Mohamed Chetouani. 2010.
ROBADOM: the impact of a domestic robot on the
psychological and cognitive state of the elderly with
mild cognitive impairment. In Proceedings of the
International Symposium on Quality of Life Technol-
ogy Intelligent Systems for Better Living, Las Vegas
USA, June.
Cheryl K. Tomoeda, Kathryn A. Bayles, Daniel R.
Boone, Alfred W. Kaszniak, and Thomas J. Slau-
son. 1990. Speech rate and syntactic complexity
effects on the auditory comprehension of alzheimer
patients. Journal of Communication Disorders,
23(2):151 ? 161.
Keith Vertanen. 2006. Baseline WSJ acoustic models
for HTK and Sphinx: Training recipes and recogni-
tion experiments. Technical report, Cavendish Lab-
oratory, University of Cambridge.
Rozanne Wilson, Elizabeth Rochon, Alex Mihailidis,
and Carol Leonard. 2012. Examining success of
communication strategies used by formal caregivers
assisting individuals with alzheimer?s disease during
an activity of daily living. Journal of Speech, Lan-
guage, and Hearing Research, 55:328?341, April.
Steve Young, Gunnar Evermann, Mark Gales, Thomas
Hain, Dan Kershaw, Xunying (Andrew) Liu, Gareth
Moore, Julian Odell, Dave Ollason and Dan Povey,
Valtcho Valtchev, and Phil Woodland. 2006. The
HTK Book (version 3.4).
28
