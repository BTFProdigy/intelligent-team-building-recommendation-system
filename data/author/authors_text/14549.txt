Coling 2010: Poster Volume, pages 692?700,
Beijing, August 2010
Contextual Recommendation based on Text Mining
Yize Li, Jiazhong Nie, Yi Zhang
School of Engineering
University of California Santa Cruz
{yize,niejiazhong,yiz}@soe.ucsc.edu
Bingqing Wang
School of Computer Science Technology
Fudan University
wbq@fudan.edu.cn
Baoshi Yan, Fuliang Weng
Research and Technology Center
Robert Bosch LLC
Baoshi.Yan@us.bosch.com
Fuliang.Weng@us.bosch.com
Abstract
The potential benefit of integrating con-
textual information for recommendation
has received much research attention re-
cently, especially with the ever-increasing
interest in mobile-based recommendation
services. However, context based recom-
mendation research is limited due to the
lack of standard evaluation data with con-
textual information and reliable technol-
ogy for extracting such information. As
a result, there are no widely accepted con-
clusions on how, when and whether con-
text helps. Additionally, a system of-
ten suffers from the so called cold start
problem due to the lack of data for train-
ing the initial context based recommenda-
tion model. This paper proposes a novel
solution to address these problems with
automated information extraction tech-
niques. We also compare several ap-
proaches for utilizing context based on
a new data set collected using the pro-
posed solution. The experimental results
demonstrate that 1) IE-based techniques
can help create a large scale context data
with decent quality from online reviews,
at least for restaurant recommendations;
2) context helps recommender systems
rank items, however, does not help pre-
dict user ratings; 3) simply using context
to filter items hurts recommendation per-
formance, while a new probabilistic latent
relational model we proposed helps.
1 Introduction
In the information retrieval community, one ma-
jor research focus is developing proactive re-
trieval agent that acts in anticipation of informa-
tion needs of a user and recommends information
to the user without requiring him/her to issue an
explicit query. The most popular examples of such
kind of proactive retrieval agent are recommender
systems. Over the last several years, research
in standard recommender systems has been im-
proved significantly, largely due to the availability
of large scale evaluation data sets such as Netflix.
The current research focus goes beyond the stan-
dard user-item rating matrix. As researchers start
to realize that the quality of recommendations de-
pends on time, place and a range of other rele-
vant users? context, how to integrate contextual
information for recommendation is becoming an
ever increasingly important topic in the research
agenda (Adomavicius and Ricci, 2009).
One major challenge in context-aware recom-
mendation research is the lack of large scale an-
notated data set. Ideally, a good research data
set should contain contextual information besides
users? explicit ratings on items. However, such
kinds of data sets are not readily available for
researchers. Previous research work in context
based recommendation usually experiments on a
small data set collected through user studies. Al-
though undoubtedly useful, this approach is lim-
ited because 1) user studies are usually very ex-
pensive and their scales are small; 2) it is very hard
for the research community to repeat such study;
and 3) a personalized contextual system may not
692
1 I was very excited to try this place and my wife took me here on my birthday. . .
We ordered a side of the brussell sprouts and they were the highlight of the night.
2 A friend of mine suggested we meet up here for a night of drinks. . . This actually
a restaurant with a bar in it, but when we went it was 10pm and . . .
Table 1: Examples of the restaurant reviews
succeed until a user has interacted with it for a
long period of time to enable context based rec-
ommendation models well trained.
On the other hand, a large amount of re-
view documents from web sites such as tri-
padvisor.com, yelp.com, cnet.com, amazon.com,
are available with certain contextual information,
such as time and companion, implicitly in the re-
views (see Table 1 for examples). This offers us an
opportunity to apply information extraction tech-
niques for obtaining contextual information from
the review texts. Together with users? explicit rat-
ings on items, this might lead to a large research
data set for context based recommendation and
consequently address the cold start issue in the
recommender systems. This paper describes the
methods that extract the contextual information
from online reviews and their impact on the rec-
ommendation quality at different accuracy levels
of the extraction methods.
Another challenge is how to integrate contex-
tual information into existing recommendation al-
gorithms. Existing approaches can be classified
into three major categories: pre-filtering, post-
filtering and the modeling based approaches (Oku
et al, 2007; Adomavicius and Tuzhilin, 2008).
Pre-filtering approaches utilize contextual infor-
mation to select data for that context, and then pre-
dict ratings using a traditional recommendation
method on the selected data (Adomavicius et al,
2005). Post-filtering approaches first predict rat-
ings on the whole data using traditional methods,
then use the contextual information to adjust re-
sults. Both methods separate contextual informa-
tion from the rating estimation process and leads
to unsatisfying findings. For example, Adomavi-
cious et al (2005) found neither standard col-
laborative filtering nor contextual reduction-based
methods dominate each other in all the cases. In
the modeling based approaches, contextual infor-
mation is used directly in the rating prediction
process. For example, Oku et al (2007) propose
a context-aware SVM-based predictive model to
classify restaurants into ?positive? and ?negative?
classes, and contextual information is included as
additional input features for the SVM classifier.
However, treating recommendation as classifica-
tion is not a common approach, and does not take
advantage of the state of art collaborative filtering
techniques. In this paper, we propose a new prob-
abilistic model to integrate contextual information
into the state of art factorization based collabora-
tive filtering approach, and compare it with sev-
eral baselines.
2 Mining Contextual Information from
Textual Opinions
The context includes any information that can be
used to characterize the situation of entities. Ex-
amples of context are: location, identity and state
of people, companions, time, activities of the cur-
rent user, the devices being used etc. (Lee et
al., 2005). Without loss of generality, we looked
into widely available restaurant review data. More
specifically, we investigated four types of contex-
tual information for a dining event, as they might
affect users? dining decisions, and they have not
been studied carefully before. The four types of
contextual information are: Companion (whether
a dining event involves multiple people), Occa-
sion (for what occasions the event is), Time (what
time during the day) and Location (in which city
the event happens).
2.1 Text Mining Approaches
We developed a set of algorithms along with exist-
ing NLP tools (GATE (Cunningham et al, 2002)
etc.) for this task. More detailed description of
these algorithms is given below.
Time: we classified the meal time into the
following types: ?breakfast?, ?lunch?, ?dinner?,
?brunch?, ?morning tea?, ?afternoon tea?. We
693
compiled a list of lexicons for these different types
of meal times, and used a string matching method
to find the explicit meal times from reviews. Here,
the meal time with an expression, such as ?6pm?,
was extracted using ANNIE?s time named entity
recognition module from the GATE toolkit. For
example, if a user says, ?When we went there, it
was 10pm?, we infer that it was for dinner.
Occasion: The ANNIE?s time named en-
tity recognition module recognizes certain special
days from text. We augmented ANNIE?s lookup
function with a list of holidays in the United States
from Wikipedia1 as well as some other occasions,
such as birthdays and anniversaries.
Location: Ideally, a location context would be
a user?s departure location to the selected restau-
rant. However, such information rarely exists in
the review texts. Therefore, we used the location
information from a user?s profile to approximate.
Companion: Extracting a companion?s infor-
mation accurately from review data is more diffi-
cult. We utilized two methods to address the chal-
lenge:
Companion-Baseline: This is a string match-
ing based approach. First, we automatically gen-
erated a lexicon of different kinds of compan-
ion words/phrases by using prepositional patterns,
such as ?with my (our) NN (NNS)?. We extracted
the noun or noun phrases from the prepositional
phrases as the companion terms, which were then
sorted by frequency of occurrence and manually
verified. This led to a lexicon of 167 entries.
Next, we grouped these entries into 6 main cate-
gories of companions: ?family?, ?friend?, ?cou-
ple?, ?colleague?, ?food-buddy? and ?pet?. Fi-
nally, the review is tagged as one or more of the
companion categories if it contains a correspond-
ing word/phrase in that lexicon.
Companion-Classifier: In order to achieve bet-
ter precision, we sampled and annotated 1000
sentences with companion terms from the corpus
and built three classifiers: 1) a MaxEnt classi-
fier with bag-of-words features, 2) a rule-based
classifier, 3) a hybrid classifier. For the rule-
based classifier, we looked into the structural as-
pects of the window where companion terms oc-
1http://en.wikipedia.org/wiki/List of holidays by
country#United States of America
curred, specifically, the adjacent verbs and prepo-
sitions associated with those terms. We collected
high frequency structures including verbs, verb-
proposition combinations, and verb-genitive com-
binations from the whole corpus, and then con-
structed a list of rules to decide whether a compan-
ion context exists based on these structures. For
the hybrid classifier, we used the patterns identi-
fied by the rule-based classifier as features for the
MaxEnt model (Ratnaparkhi, 1998). To train the
classifier, we also included features such as POS
tags of the verb and of the candidate companion
term, the occurrence of a meal term (e.g. ?lunch?,
?dinner?), the occurrence of pronouns (e.g. ?we?
or ?us?) and the genitive of the companion term.
Based on the evaluation results (using 5-fold cross
validation) shown in Table 2, the hybrid classifier
is the best performing classifier and it is used for
the subsequent experiments in the paper.
Words Rule Hybrid
Precision 0.7181 0.7238 0.7379
Recall 0.8962 0.8947 0.9143
F-Score 0.7973 0.8003 0.8167
Table 2: Evaluation results for the bag-of-words-
based classifier (Words), the rule-based classifier
(Rule) and the hybrid classifier (Hybrid)
3 Recommendation based on Contextual
Information
Next we consider how to integrate various con-
textual information into recommender systems.
Assume there are N items and M users. Each
user reviews a set of items in the system. The
data set can be represented as a set of quadruplet
D = (y, i, j, c), where i is the index of user, j is
the index of item, c is a vector describing the con-
text of this rating data, and y is the rating value.
Let c = (c1, ..., ck), where each component ck
represents a type of context, such as ?dinner time?
or ?location=San Jose?. The observed features
(meta data) of user i and item j are represented
as vectors fi and fj respectively, where each com-
ponent in the vector represents a type of feature,
such as ?gender of the user? or ?price range of
the restaurant?. In the rest of this paper, we in-
694
tegrate context c into the user?s observed features
fi. This makes fi a dynamic feature vector, which
will change with different context. The goal is
to predict ratings for candidate items given user i
and context c, and recommend the top items. We
present two recommendation models for integrat-
ing contextual information in this section.
3.1 Boolean Model
The Boolean Model filters out items that do not
match the context. The Boolean model itself re-
turns an item set instead of a ranked list. We fur-
ther rank the items by predicted rating values. We
score items by the Boolean model as follows:
s(j) =
{
sm(j) if item j matches the context
?? otherwise
(1)
where sm(j) is the predicted rating computed us-
ing a rating prediction method m, such as a Col-
laborative Filtering model without using context.
3.2 Probabilistic Latent Relational Model
We propose a novel Probabilistic Latent Rela-
tional Model (PLRM) for integrating contextual
information. In a context-aware recommender
system, a user?s interest for item is influenced by
two factors: (1) the user?s long-term preference,
which can be learned from users? rating history;
(2) the current context (how the item matches the
current context). To capture the two factors si-
multaneously, we introduce a new probabilistic
model by assuming the rating value yi,j,c follows
a Gaussian distribution with mean ui,j,c and vari-
ance 1/?(y):
yi,j,c ? N (ui,j,c, 1/?(y)) (2)
ui,j,c = uTi Avj + (Wufi)T (Wvfj) (3)
where ui and vj are the hidden representations of
user i and item j to be learned from rating data,
and Wu and Wv are feature transformation matri-
ces for users and items respectively. In Equation
(3), the first term uTi Avj is the estimation based
on user? long term preferences, where A = {a} is
a matrix modeling the interaction between ui and
vj .2 The second term (Wufi)T (Wvfj) is the esti-
2We introduce A matrix so that the model can also
be used to model multiple different types of relation-
mation based on current context and the observed
features of users and items, since the context c is
integrated into user?s observed features fi.
{U, V,A,W} are the parameters of the model
to be estimated from the training data set D,
where W = {Wu,Wv} = {w} , U =
{u1,u2, ...uN} and V = {v1,v2, ...vM}. We as-
sume the prior distribution of the parameters fol-
low the Gaussian distributions centered on 0. We
use 1/?(u),1/?(v), 1/?(w) and 1/?(a) to represent
the variance of the corresponding Gaussian distri-
butions. The effect of the prior distribution is sim-
ilar to the ridge regression (norm-2 regularizer)
commonly used in machine learning algorithms to
control model complexity and avoid overfitting.
The proposed model is motivated by well per-
forming recommendation models in the literature.
It generalizes several existing models. If we set A
to the identity matrix and Wu,Wv to zero matri-
ces, the model presented in Equation (3) is equiv-
alent to the well known norm-2 regularized singu-
lar value decomposition, which performs well on
the Netflix competition(Salakhutdinov and Mnih,
2007). If we set A to zero matrix and Wu to iden-
tity matrix, the Model (3) becomes the bilinear
model that works well on Yahoo news recommen-
dation task (Chu and Park, 2009).
Based on the above model assumption, the joint
likelihood of all random variables (U , V , A, W
and D) in the system is:
P (U, V,A,W,D) =?
(i,j,c,y)?D
P (yi,j,c|ui,vj , fi, fj , A,Wu,Wv)
?
i
P (ui)
?
j
P (vj)P (A)P (Wu)P (Wv)(4)
3.3 Parameter Estimation
We use a modified EM algorithm for parame-
ter estimation to find the posterior distribution of
(U, V ) and max a posterior (MAP) of (A,W ).
The estimation can be used to make the final pre-
ships/interactions jointly, where each type of relationship
corresponds to a different A matrix. For the task in this pa-
per, A is not required and can be set to the identity matrix
for simplicity. However, we leave A as parameters to be es-
timated in the rest of this paper for generality.
695
dictions as follows:
y?i,j,c =
?
ui,vj
P (ui)P (vj)(uTi Avj
+(Wufi)TWvfj)duidvj
E Step: the Variational Bayesian approach is used
to estimate the posterior distributions of U and V .
Assuming (A,W ) are known, based on Equation
4, we have
P (U, V |A,W,D) ?
?
(y,i,j,c)?D
N (uTi Avj + (Wufi)TWvfj , 1/?(y))
?
M?
i=1
N (ui|0, 1/?(u)I)
N?
j=1
N (vj |0, 1/?(v)I)
Deriving the exact distribution and use it to predict
y will result in intractable integrals. Thus we ap-
proximate the posterior with a variational distribu-
tion Q(U, V ) = ?Mi=1Q(ui)
?N
j=1Q(vj). Q(ui)
and Q(vj) are restricted to Gaussian distributions
so that predicting y using Bayesian inference with
Q(U, V ) will be straightforward. Q(U, V ) can be
estimated by minimizing the KL-divergence be-
tween it and P (U, V |A,W,D). Since Q(U, V ) is
factorized into individual Q(ui) and Q(vj), we
can first focus on one Q(ui) (or Q(vj)) at a time
by fixing/ignoring other factors. For space consid-
erations, we omit the derivation in this paper. The
optimal Q(ui) is N (u?i,?i), where u?i = ?idi,
??1i =
?
(y,i,j,c)?D
?(y)A(v?jv?Tj + ?j)AT
+ ?(u)I
di =
?
(y,i,j,c)?D
?(y)y?Av?j
Similarly, the optimal Q(vj) isN (v?j,?j), where
v?j = ?jej ,
??1j =
?
(y,i,j,c)?D
?(y)AT (u?iu?Ti + ?i)A
+ ?(v)I
ej =
?
(y,i,j,c)?D
?(y)y?AT v?j
M Step: Based on the approximate pos-
terior estimation Q(U, V ) derived in the E
step, the maximum a posteriori estimation
of {A,W} can be found by maximizing
the expected posterior likelihood {A?, W?} =
argmaxA,W EQ(U,V )(logP (A,W,U, V |D)).
This can be done using the conjugate gradient
descent method, and the gradient of A,Wu,Wv
can be calculated as follows:
??
?A =
?
(y,i,j,c)?D
?(y)((y? ? y)u?iv?Tj
+ u?iu?Ti A?j + ?iAv?jv?Tj + ?iA?j)
+ ?(a)A
??
?Wu
=
?
(y,i,j,c)?D
?(y)(y? ? y)WvfjfTi
+ ?(w)Wu
??
?Wv
=
?
(y,i,j,c)?D
?(y)(y? ? y)WufifTj
+ ?(w)Wv
where ? = EQ(U,V )(logP (A,W,U, V |D)) and
y? = u?Ti Av?j + (Wufi)TWvfj .
4 Experimental Methodology
4.1 Data Collection
We collected an evaluation data set from a pop-
ular review web site where users review ser-
vices/products and provide integer ratings from 1
to 5. The user profile and the description of items,
such as user gender and the category of restau-
rants are also collected. The data set used in this
paper includes the restaurants in Silicon Valley
(Bay area) and the users who ever reviewed these
restaurants. We extract context from the review
texts. The four kinds of context considered in our
paper are described in Section 2.1. For each type
of context, we create a subset, in which all reviews
contain the corresponding contextual information.
Finally we construct four sub data sets and each
data set is described by the corresponding con-
text type: Time, Location, Occasion and Compan-
ion. We use ?All? to represent the whole data set.
Statistics about each data set are described in Ta-
ble 3.
696
(a) Time (b) Location (c) Occasion
(d) Companion (e) All
Figure 1: Performance on the top-K recommendation task. The plots focus on the top 20% ranking
region.
Dataset #Ratings #Users #Items
All 756,031 82,892 12,533
Location 583,051 56,026 12,155
Time 229,321 49,748 10,561
Occasion 22,732 12,689 4,135
Companion 196,000 47,545 10,246
Table 3: Statistics of data
4.2 Experimental Setup
We design the experiments to answer the follow-
ing questions: 1) Does including contextual in-
formation improve the recommendation perfor-
mance? 2) How does the probabilistic latent re-
lational modeling approach compare with pre-
filtering or post-filtering approaches? 3) How
does the extraction quality of the contextual infor-
mation affect the recommendation performance?
To answer the first question, we compare the
performance of the Probabilistic Latent Relational
Model on a standard collaborative filtering setting
where only rating information is considered, in-
dicated by Nocontext. We also evaluate the per-
formance of the Probabilistic Latent Relational
Model when integrating contextual information,
indicated by Context-X, where X represents the
type of contextual information considered. To
answer the second question, we compare the
performance of Context-X with the pre-filtering
Boolean Model, which first uses the context to se-
lect items and then ranks them using scores com-
puted by Nocontext. To answer the third question,
we compare the recommendation performance for
different extraction precision. The performance
on the following two recommendation tasks are
reported in this paper:
Top-K Recommendation: We rank the items
by the predicted rating values and retrieve the top
K items. This task simulates the scenario where
a real recommender system usually suggests a list
of ranked K items to a user. To simulate the sce-
nario that we only want to recommend the 5-star
items to users, we treat 5-star rating data in testing
data as relevant. Ideally, classic IR measures such
as Precision and Recall are used to evaluate the
recommendation algorithms. However, without
complete relevance judgements, standard IR eval-
uation is almost infeasible. Thus we use a varia-
tion of the evaluation method proposed by Koren
(Koren, 2008).
Rating Prediction: Given an active user i and a
target item j, the system predicts the rating of user
697
Training on Sub Data set Training on the Whole Data set
Testing Data ItemAvg Nocontext Context ItemAvg Nocontext Context
Time 1.1517 1.0067 1.0067 1.1052 0.9829 0.9822
Companion 1.2657 1.0891 1.0888 1.2012 1.0693 1.0695
Occasion 1.2803 1.1381 1.1355 1.2121 1.0586 1.0583
Location 1.1597 1.0209 1.0206 1.1597 1.0183 1.0183
All context - - - 1.1640 1.0222 1.0219
Table 4: RMSE on the rating prediction task
Time CompanionBaseline CompanionClassifier Occasion
#Reviews 300 300 300 200
#Contexts 115 148 114 207
Precision 84.4% 62.2% 77.1% -
Recall 80.2% 95.8% 91.7% -
F-Score 82.2% 75.4% 83.8% Accuracy 78.3%
Table 5: Performance of the context extraction module
i on item j. The prediction accuracy is measured
by Root Mean Square Error (RMSE), which is
commonly used in collaborative filtering research.
This task simulates the scenario that we need to
guess a user?s rating about an item, given that the
user has already purchased/selected the item.
For each data set (Time, Companion, Location,
Occasion and All), we randomly sample 10% for
testing, 80% for training and 10% for validation.
5 Experimental Results
5.1 Performance on Top-K Recommendation
Figure 1(a)-(e) shows the ranking performance on
each data set. The x-axis is the rank and the y-axis
is the portion of relevant products covered by this
level of rank. The results across all data sets are
consistent. With contextual information, PLRM
Context-X outperforms Nocontext, whereas using
context to pre-filter items (Boolean) does not help.
It means that contextual information can help if
used appropriately, however improperly utilizing
context, such as simply using it as a boolean filter,
may hurt the recommendation performance. Our
proposed PLRM is an effective way to integrate
contextual information.
5.2 Performance on Rating Prediction Task
Table 4 summaries the RMSE results of differ-
ent approaches on the rating prediction task. The
RMSE of simply using item?s average rating value
as the prediction is also reported as a reference
since it is a commonly used approach by non per-
sonalized recommender systems. For each con-
text, we can either train the model only on the sub-
set that consists of rating data with related context,
or train on a bigger data set by adding the rating
data without related context. The results on both
settings are reported here. Table 4 shows that uti-
lizing context does not affect the prediction accu-
racy. We may wonder why the effects of adding
context is so different on the rating task compared
with the ranking task. One possible explanation
is that the selection process of a user is influenced
by context, while how the user rates an item after
selecting it is less relevant to context. For exam-
ple, when a user wants to have a breakfast, he may
prefer a cafeteria rather than a formal restaurant.
However, how the user rates this cafeteria is more
based on user?s experiences in the cafeteria, such
as quality of services, food, price, environment,
etc.
5.3 How does Text Mining Accuracy Affect
Recommendation
To evaluate the extraction performance on ?Com-
panion?, ?Time? and ?Occasion?, we randomly
sample some reviews and evaluate the perfor-
698
mance on the samples3. The results are shown in
Table 5. Compared with other contexts, the ex-
traction of companion context is more challenging
and the string matching baseline algorithm pro-
duces significantly inferior results. However, by
using a MaxEnt classifier with features selection,
we can boost the precision of the companion con-
text extraction to a level comparable to other con-
texts.
To further investigate the relationship between
the quality of the extracted context and the perfor-
mance of the recommender system, we compare
the recommendation performance of Companion-
Baseline and Companion-Classifier in Figure
1(d). It shows that improving the quality of the
extraction task leads to a significant improvement
on the recommender systems? top-K ranking task.
6 Conclusions
Reviews widely available online contain a large
amount of contextual information. This paper
proposes to leverage information extraction tech-
niques to help recommender systems to train
better context-aware recommendation models by
mining reviews. We also introduce a probabilis-
tic latent relation model for integrating the cur-
rent context and the user?s long term preferences.
This model takes the advantages of traditional col-
laborative filtering approaches (CF). It also cap-
tures the interaction between contextual informa-
tion and item characteristics. The experimental
results demonstrate that context is an important
factor that affects user choices. If properly used,
contextual information helps ranking based rec-
ommendation systems, probably because context
influences users? purchasing decisions. Besides,
more accurate contextual information leads to bet-
ter recommendation models. However, contextual
information does not help the user rating predic-
tion task significantly, probably because context
doesn?t matter much given the user has already
chosen a restaurant.
As the first step towards using the information
3We sample 300 reviews for ?Time? and ?Companion?
evaluation. Due to the extremely low probability of occur-
rence of Occasion context, we futher sample 200 reviews
containing Occasion-related expressions and only evaluate
extraction accuracy on these samples
extraction techniques to help contextual recom-
mendation, the techniques used in this paper are
far from optimal. In the future, we will research
more effective text mining techniques for contex-
tual extraction(Mazur and Dale, 2008; McCallum
et al, 2000; Lafferty et al, 2001) at the same time
increasing the amount of annotated review data
for better classifier performance through actively
learning (Laws and Schu?tze, 2008). We also plan
to work towards a better understanding of con-
textual information in recommender systems, and
explore other types of contextual information in
different types of recommendation tasks besides
restaurant recommendations.
7 Acknowledgements
Part of this research is funded by National Sci-
ence Foundation IIS-0713111 and the Institute of
Education Science. Any opinions, findings, con-
clusions or recommendations expressed in this pa-
per are the authors?, and do not necessarily reflect
those of the sponsors. Bingqing Wang?s work is
done during his stay in the Research and Technol-
ogy Center, Robert Bosch LLC.
References
Adomavicius, Gediminas and Francesco Ricci. 2009.
Recsys?09 workshop 3: workshop on context-aware
recommender systems, cars-2009. In Proceedings
of the 3rd ACM Conference on Recommender Sys-
tems, RecSys 2009, pages 423?424.
Adomavicius, Gediminas and Alexander Tuzhilin.
2008. Context-aware recommender systems. In
Proceedings of the 2nd ACM Conference on Rec-
ommender Systems, RecSys 2008, pages 335?336.
Adomavicius, Gediminas, Ramesh Sankaranarayanan,
Shahana Sen, and Alexander Tuzhilin. 2005.
Incorporating contextual information in recom-
mender systems using a multidimensional approach.
ACM Transactions on Information Systems (TOIS),
23(1):103?145.
Chu, Wei and Seung-Taek Park. 2009. Personalized
recommendation on dynamic content using predic-
tive bilinear models. In Proceedings of the 18th In-
ternational Conference on World Wide Web, WWW
2009, pages 691?700.
Cunningham, Hamish, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. A frame-
work and graphical development environment for
699
robust nlp tools and applications. In Proceedings of
the 40th Anniversary Meeting of the Association for
Computational Linguistics, ACL 2002, pages 168?
175.
Koren, Yehuda. 2008. Factorization meets the
neighborhood: a multifaceted collaborative filtering
model. In Proceedings of the 14th ACM SIGKDD
International Conference on Knowledge Discovery
and Data Mining, SIGKDD 2008, pages 426?434.
Lafferty, John D., Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling
sequence data. In Proceedings of the 18th Inter-
national Conference on Machine Learning, ICML
2001, pages 282?289.
Laws, Florian and Hinrich Schu?tze. 2008. Stopping
criteria for active learning of named entity recogni-
tion. In Proceedings of the 22nd International Con-
ference on Computational Linguistics, Coling 2008,
pages 465?472, August.
Lee, Hong Joo, Joon Yeon Choi, and Sung Joo Park.
2005. Context-aware recommendations on the mo-
bile web. In On the Move to Meaningful Internet
Systems 2005: OTM 2005 Workshops, pages 142?
151.
Mazur, Pawel and Robert Dale. 2008. What?s the
date? high accuracy interpretation of weekday
names. In Proceedings of the 22nd International
Conference on Computational Linguistics, Coling
2008, pages 553?560.
McCallum, Andrew, Dayne Freitag, and Fernando
C. N. Pereira. 2000. Maximum entropy markov
models for information extraction and segmenta-
tion. In Proceedings of the 17th International Con-
ference on Machine Learning, ICML 2000, pages
591?598.
Oku, Kenta, Shinsuke Nakajima, Jun Miyazaki, and
Shunsuke Uemura. 2007. Investigation for design-
ing of context-aware recommendation system using
svm. In Proceedings of the International MultiCon-
ference of Engineers and Computer Scientists 2007,
IMECS 2007, pages 970?975.
Ratnaparkhi, A. 1998. MAXIMUM ENTROPY MOD-
ELS FOR NATURAL LANGUAGE AMBIGUITY
RESOLUTION. Ph.D. thesis, University of Penn-
sylvania.
Salakhutdinov, Ruslan and Andriy Mnih. 2007. Prob-
abilistic matrix factorization. In Advances in Neural
Information Processing Systems 20, Proceedings of
the 21st Annual Conference on Neural Information
Processing Systems, NIPS 2007, pages 1257?1264.
700
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 71?76,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Insertion, Deletion, or Substitution? Normalizing Text Messages without
Pre-categorization nor Supervision
Fei Liu1 Fuliang Weng2 Bingqing Wang3 Yang Liu1
1Computer Science Department, The University of Texas at Dallas
2Research and Technology Center, Robert Bosch LLC
3School of Computer Science, Fudan University
{feiliu, yangl}@hlt.utdallas.edu1
fuliang.weng@us.bosch.com2, wbq@fudan.edu.cn3
Abstract
Most text message normalization approaches
are based on supervised learning and rely on
human labeled training data. In addition, the
nonstandard words are often categorized into
different types and specific models are de-
signed to tackle each type. In this paper,
we propose a unified letter transformation ap-
proach that requires neither pre-categorization
nor human supervision. Our approach mod-
els the generation process from the dictionary
words to nonstandard tokens under a sequence
labeling framework, where each letter in the
dictionary word can be retained, removed, or
substituted by other letters/digits. To avoid
the expensive and time consuming hand label-
ing process, we automatically collected a large
set of noisy training pairs using a novel web-
based approach and performed character-level
alignment for model training. Experiments on
both Twitter and SMS messages show that our
system significantly outperformed the state-
of-the-art deletion-based abbreviation system
and the jazzy spell checker (absolute accuracy
gain of 21.69% and 18.16% over jazzy spell
checker on the two test sets respectively).
1 Introduction
Recent years have witnessed the explosive growth
of text message usage, including the mobile phone
text messages (SMS), chat logs, emails, and sta-
tus updates from the social network websites such
as Twitter and Facebook. These text message col-
lections serve as valuable information sources, yet
the nonstandard contents within them often degrade
2gether (6326) togetha (919) tgthr (250) togeda (20)
2getha (1266) togather (207) t0gether (57) toqethaa (10)
2gthr (178) togehter (94) togeter (49) 2getter (10)
2qetha (46) togethor (29) tagether (18) 2gtr (6)
Table 1: Nonstandard tokens originated from ?together?
and their frequencies in the Edinburgh Twitter corpus.
the existing language processing systems, calling
the need of text normalization before applying the
traditional information extraction, retrieval, senti-
ment analysis (Celikyilmaz et al, 2010), or sum-
marization techniques. Text message normalization
is also of crucial importance for building text-to-
speech (TTS) systems, which need to determine pro-
nunciation for nonstandard words.
Text message normalization aims to replace the
non-standard tokens that carry significant mean-
ings with the context-appropriate standard English
words. This is a very challenging task due to the
vast amount and wide variety of existing nonstan-
dard tokens. We found more than 4 million dis-
tinct out-of-vocabulary tokens in the English tweets
of the Edinburgh Twitter corpus (see Section 2.2).
Table 1 shows examples of nonstandard tokens orig-
inated from the word ?together?. We can see that
some variants can be generated by dropping let-
ters from the original word (?tgthr?) or substitut-
ing letters with digit (?2gether?); however, many
variants are generated by combining the letter in-
sertion, deletion, and substitution operations (?to-
qethaa?, ?2gthr?). This shows that it is difficult to
divide the nonstandard tokens into exclusive cate-
gories.
Among the literature of text normalization
71
(for text messages or other domains), Sproat et
al. (2001), Cook and Stevenson (2009) employed the
noisy channel model to find the most probable word
sequence given the observed noisy message. Their
approaches first classified the nonstandard tokens
into various categories (e.g., abbreviation, stylistic
variation, prefix-clipping), then calculated the pos-
terior probability of the nonstandard tokens based
on each category. Choudhury et al (2007) de-
veloped a hidden Markov model using hand anno-
tated training data. Yang et al (2009), Pennell and
Liu (2010) focused on modeling word abbreviations
formed by dropping characters from the original
word. Toutanova and Moore (2002) addressed the
phonetic substitution problem by extending the ini-
tial letter-to-phone model. Aw et al (2006), Kobus
et al (2008) viewed the text message normalization
as a statistical machine translation process from the
texting language to standard English. Beaufort et
al. (2010) experimented with the weighted finite-
state machines for normalizing French SMS mes-
sages. Most of the above approaches rely heavily
on the hand annotated data and involve categorizing
the nonstandard tokens in the first place, which gives
rise to three problems: (1) the labeled data is very
expensive and time consuming to obtain; (2) it is
hard to establish a standard taxonomy for categoriz-
ing the tokens found in text messages; (3) the lack of
optimized way to integrate various category-specific
models often compromises the system performance,
as confirmed by (Cook and Stevenson, 2009).
In this paper, we propose a general letter trans-
formation approach that normalizes nonstandard to-
kens without categorizing them. A large set of noisy
training word pairs were automatically collected via
a novel web-based approach and aligned at the char-
acter level for model training. The system was tested
on both Twitter and SMS messages. Results show
that our system significantly outperformed the jazzy
spell checker and the state-of-the-art deletion-based
abbreviation system, and also demonstrated good
cross-domain portability.
2 Letter Transformation Approach
2.1 General Framework
Given a noisy text message T , our goal is to nor-
malize it into a standard English word sequence S.
b - - - - d a y f - o t o z
h u b b i e
(1) birthday --> bday
(2) photos --> fotoz
(4) hubby --> hubbie
b i r t h d a y
p h o t o s
h u b b y
s o m e 1 - -
(6) someone --> some1
s o m e o n e
n u t h i n -
(3) nothing --> nuthin
n o t h i n g
4 - - e v a -
(5) forever --> 4eva
f o r e v e r
Figure 1: Examples of nonstandard tokens generated by
performing letter transformation on the dictionary words.
Under the noisy channel model, this is equivalent to
finding the sequence S? that maximizes p(S|T ):
S? = argmaxS p(S|T ) = argmaxS(
?
i
p(Ti|Si))p(S)
where we assume that each non-standard token Ti
is dependent on only one English word Si, that is,
we are not considering acronyms (e.g., ?bbl? for
?be back later?) in this study. p(S) can be cal-
culated using a language model (LM). We formu-
late the process of generating a nonstandard token
Ti from dictionary word Si using a letter transfor-
mation model, and use the model confidence as the
probability p(Ti|Si). Figure 1 shows several exam-
ple (word, token) pairs1. To form a nonstandard to-
ken, each letter in the dictionary word can be labeled
with: (a) one of the 0-9 digits; (b) one of the 26 char-
acters including itself; (c) the null character ?-?; (d)
a letter combination. This transformation process
from dictionary words to nonstandard tokens will be
learned automatically through a sequence labeling
framework that integrates character-, phonetic-, and
syllable-level information.
In general, the letter transformation approach will
handle the nonstandard tokens listed in Table 2 yet
without explicitly categorizing them. Note for the
tokens with letter repetition, we first generate a set
of variants by varying the repetitive letters (e.g. Ci =
{?pleas?, ?pleeas?, ?pleaas?, ?pleeaas?, ?pleeeaas?}
for Ti = {?pleeeaas?}), then select the maximum
posterior probability among all the variants:
p(Ti|Si) = max
T?i?Ci
p(T?i|Si)
1The ideal transform for example (5) would be ?for? to ?4?.
But in this study we are treating each letter in the English word
separately and not considering the phrase-level transformation.
72
(1) abbreviation tgthr, weeknd, shudnt
(2) phonetic sub w/- or w/o digit 4got, sumbody, kulture
(3) graphemic sub w/- or w/o digit t0gether, h3r3, 5top, doinq
(4) typographic error thimg, macam
(5) stylistic variation betta, hubbie, cutie
(6) letter repetition pleeeaas, togtherrr
(7) any combination of (1) to (6) luvvvin, 2moro, m0rnin
Table 2: Nonstandard tokens that can be processed by the
unified letter transformation approach.
2.2 Web based Data Collection w/o Supervision
We propose to automatically collect training data
(annotate nonstandard words with the corresponding
English forms) using a web-based approach, there-
fore avoiding the expensive human annotation. We
use the Edinburgh Twitter corpus (Petrovic et al,
2010) for data collection, which contains 97 mil-
lion Twitter messages. The English tweets were
extracted using the TextCat language identification
toolkit (Cavnar and Trenkle, 1994), and tokenized
into a sequence of clean tokens consisting of letters,
digits, and apostrophe.
For the out-of-vocabulary (OOV) tokens consist-
ing of letters and apostrophe, we form n Google
queries for each of them in the form of either
?w1 w2 w3? OOV or OOV ?w1 w2 w3?, where w1
to w3 are consecutive context words extracted from
the tweets that contain this OOV. n is set to 6 in this
study. The first 32 returned snippets for each query
are parsed and the words in boldface that are differ-
ent from both the OOV and the context words are
collected as candidate normalized words. Among
them, we further select the words that have longer
common character sequence with the OOV than with
the context words, and pair each of them with the
OOV to form the training pairs. For the OOV tokens
consisting of both letters and digits, we use simple
rules to recover possible original words. These rules
include: 1 ? ?one?, ?won?, ?i?; 2 ? ?to?, ?two?,
?too?; 3 ? ?e?; 4 ? ?for?, ?fore?, ?four?; 5 ? ?s?;
6 ? ?b?; 8 ? ?ate?, ?ait?, ?eat?, ?eate?, ?ight?,
?aight?. The OOV tokens and any resulting words
from the above process are included in the noisy
training pairs. In addition, we add 932 word pairs
of chat slangs and their normalized word forms col-
lected from InternetSlang.com that are not covered
by the above training set.
These noisy training pairs were further expanded
and purged. We apply the transitive rule on these
initially collected training pairs. For example, if the
two pairs ?(cause, cauz)? and ?(cauz, coz)? are in the
data set, we will add ?(cause, coz)? as another train-
ing pair. We remove the data pairs whose word can-
didate is not in the CMU dictionary. We also remove
the pairs whose word candidate and OOV are simply
inflections of each other, e.g., ?(headed, heading)?,
using a set of rules. In total, this procedure generated
62,907 training word pairs including 20,880 unique
candidate words and 46,356 unique OOVs.2
2.3 Automatic Letter-level Alignment
Given a training pair (Si, Ti) consisting of a word Si
and its nonstandard variant Ti, we propose a proce-
dure to align each letter in Si with zero, one, or more
letters/digits in Ti. First we align the letters of the
longest common sequence between the dictionary
word and the variant (which gives letter-to-letter cor-
respondence in those common subsequences). Then
for the letter chunks in between each of the obtained
alignments, we process them based on the following
three cases:
(a) (many-to-0): a chunk in the dictionary word
needs to be aligned to zero letters in the variant.
In this case, we map each letter in the chunk to
?-? (e.g., ?birthday? to ?bday?), obtaining letter-
level alignments.
(b) (0-to-many): zero letters in the dictionary word
need to be aligned to a letter/digit chunk in the
variant. In this case, if the first letter in the
chunk can be combined with the previous letter
to form a digraph (such as ?wh? when aligning
?sandwich? to ?sandwhich?), we combine these
two letters. The remaining letters, or the entire
chunk when the first letter does not form a di-
graph with the previous letter, are put together
with the following aligned letter in the variant.
(c) (many-to-many): non-zero letters in the dictio-
nary word need to be aligned to a chunk in the
variant. Similar to (b), the first letter in the vari-
ant chunk is merged with the previous alignment
if they form a digraph. Then we map the chunk
in the dictionary word to the chunk in the vari-
ant as one alignment, e.g., ?someone? aligned to
?some1?.
2Please contact the first author for the collected word pairs.
73
The (b) and (c) cases above generate chunk-level
(with more than one letter) alignments. To elimi-
nate possible noisy training pairs, such as (?you?,
?haveu?), we keep all data pairs containing digits,
but remove the data pairs with chunks involving
three letters or more in either the dictionary word or
the variant. For the chunk alignments in the remain-
ing pairs, we sequentially align the letters (e.g., ?ph?
aligned to ?f-?). Note that for those 1-to-2 align-
ments, we align the single letter in the dictionary
word to a two-letter combination in the variant. We
limit to the top 5 most frequent letter combinations,
which are ?ck?, ?ey?, ?ie?, ?ou?, ?wh?, and the pairs
involving other combinations are removed.
After applying the letter alignment to the col-
lected noisy training word pairs, we obtained
298,160 letter-level alignments. Some example
alignments and corresponding word pairs are:
e ? ? ? (have, hav) q ? k (iraq, irak)
e ? a (another, anotha) q ? g (iraq, irag)
e? 3 (online, 0nlin3) w?wh (watch, whatch)
2.4 Sequence Labeling Model for P (Ti|Si)
For a letter sequence Si, we use the conditional ran-
dom fields (CRF) model to perform sequence tag-
ging to generate its variant Ti. To train the model,
we first align the collected dictionary word and its
variant at the letter level, then construct a feature
vector for each letter in the dictionary word, using
its mapped character as the reference label. This la-
beled data set is used to train a CRF model with L-
BFGS (Lafferty et al, 2001; Kudo, 2005). We use
the following features:
? Character-level features
Character n-grams: c?1, c0, c1, (c?2 c?1),
(c?1 c0), (c0 c1), (c1 c2), (c?3 c?2 c?1),
(c?2 c?1 c0), (c?1 c0 c1), (c0 c1 c2), (c1 c2 c3).
The relative position of character in the word.
? Phonetic-level features
Phoneme n-grams: p?1, p0, p1, (p?1 p0),
(p0 p1). We use the many-to-many letter-
phoneme alignment algorithm (Jiampojamarn
et al, 2007) to map each letter to multiple
phonemes (1-to-2 alignment). We use three bi-
nary features to indicate whether the current,
previous, or next character is a vowel.
? Syllable-level features
Relative position of the current syllable in the
word; two binary features indicating whether
the character is at the beginning or the end of
the current syllable. The English hyphenation
dictionary (Hindson, 2006) is used to mark all
the syllable information.
The trained CRF model can be applied to any En-
glish word to generate its variants with probabilities.
3 Experiments
We evaluate the system performance on both Twitter
and SMS message test sets. The SMS data was used
in previous work (Choudhury et al, 2007; Cook and
Stevenson, 2009). It consists of 303 distinct non-
standard tokens and their corresponding dictionary
words. We developed our own Twitter message test
set consisting of 6,150 tweets manually annotated
via the Amazon Mechanical Turk. 3 to 6 turkers
were required to convert the nonstandard tokens in
the tweets to the standard English words. We extract
the nonstandard tokens whose most frequently nor-
malized word consists of letters/digits/apostrophe,
and is different from the token itself. This results
in 3,802 distinct nonstandard tokens that we use as
the test set. 147 (3.87%) of them have more than
one corresponding standard English words. Similar
to prior work, we use isolated nonstandard tokens
without any context, that is, the LM probabilities
P (S) are based on unigrams.
We compare our system against three approaches.
The first one is a comprehensive list of chat slangs,
abbreviations, and acronyms collected by Internet-
Slang.com; it contains normalized word forms for
6,105 commonly used slangs. The second is the
word-abbreviation lookup table generated by the su-
pervised deletion-based abbreviation approach pro-
posed in (Pennell and Liu, 2010). It contains
477,941 (word, abbreviation) pairs automatically
generated for 54,594 CMU dictionary words. The
third is the jazzy spell checker based on the Aspell
algorithm (Idzelis, 2005). It integrates the phonetic
matching algorithm (DoubleMetaphone) and Leven-
shtein distance that enables the interchanging of two
adjacent letters, and changing/deleting/adding of let-
ters. The system performance is measured using the
n-best accuracy (n=1,3). For each nonstandard to-
ken, the system is considered correct if any of the
corresponding standard words is among the n-best
output from the system.
74
System Accuracy
Twitter (3802 pairs) SMS (303 pairs)
1-best 3-best 1-best 3-best
InternetSlang 7.94 8.07 4.95 4.95
(Pennell et al 2010) 20.02 27.09 21.12 28.05
Jazzy Spell Checker 47.19 56.92 43.89 55.45
LetterTran (Trim) 57.44 64.89 58.09 70.63
LetterTran (All) 59.15 67.02 58.09 70.96
LetterTran (All) + Jazzy 68.88 78.27 62.05 75.91
(Choudhury et al 2007) n/a n/a 59.9 n/a
(Cook et al 2009) n/a n/a 59.4 n/a
Table 3: N-best performance on Twitter and SMS data
sets using different systems.
Results of system accuracies are shown in Ta-
ble 3. For the system ?LetterTran (All)?, we first
generate a lookup table by applying the trained CRF
model to the CMU dictionary to generate up to
30 variants for each dictionary word.3 To make
the comparison more meaningful, we also trim our
lookup table to the same size as the deletion ta-
ble, namely ?LetterTran (Trim)?. The trimming was
performed by selecting the most frequent dictionary
words and their generated variants until the length
limit is reached. Word frequency information was
obtained from the entire Edinburgh corpus. For both
the deletion and letter transformation lookup tables,
we generate a ranked list of candidate words for each
nonstandard token, by sorting the combined score
p(Ti|Si)?C(Si), where p(Ti|Si) is the model con-
fidence and C(Si) is the unigram count generated
from the Edinburgh corpus (we used counts instead
of unigram probability P (Si)). Since the string sim-
ilarity and letter switching algorithms implemented
in jazzy can compensate the letter transformation
model, we also investigate combining it with our ap-
proach, ?LetterTran(All) + Jazzy?. In this configura-
tion, we combine the candidate words from both sys-
tems and rerank them according to the unigram fre-
quency; since the ?LetterTran? itself is very effective
in ranking candidate words, we only use the jazzy
output for tokens where ?LetterTran? is not very
confident about its best candidate ((p(Ti|Si)?C(Si)
is less than a threshold ? = 100).
We notice the accuracy using the InternetSlang
list is very poor, indicating text message normal-
ization is a very challenging task that can hardly
3We heuristically choose this large number since the learned
letter/digit insertion, substitution, and deletion patterns tend to
generate many variants for each dictionary word.
be tackled by using a hand-crafted list. The dele-
tion table has modest performance given the fact
that it covers only deletion-based abbreviations and
letter repetitions (see Section 2.1). The ?Letter-
Tran? approach significantly outperforms all base-
lines even after trimming. This is because it han-
dles different ways of forming nonstandard tokens
in an unified framework. Taking the Twitter test
set for an example, the lookup table generated by
?LetterTran? covered 69.94% of the total test to-
kens, and among them, 96% were correctly normal-
ized in the 3-best output, resulting in 67.02% over-
all accuracy. The test tokens that were not covered
by the ?LetterTrans? model include those generated
by accidentally switching and inserting letters (e.g.,
?absolotuely? for ?absolutely?) and slangs (?addy?
or ?address?). Adding the output from jazzy com-
pensates these problems and boosts the 1-best ac-
curacy, achieving 21.69% and 18.16% absolute per-
formance gain respectively on the Twitter and SMS
test sets, as compared to using jazzy only. We also
observe that the ?LetterTran? model can be easily
ported to the SMS domain. When combined with
the jazzy module, it achieved 62.05% 1-best accu-
racy, outperforming the domain-specific supervised
system in (Choudhury et al, 2007) (59.9%) and
the pre-categorized approach by (Cook and Steven-
son, 2009) (59.4%). Regarding different feature cat-
egories, we found the character-level features are
strong indicators, and using phonetic- and syllabic-
level features also slightly benefits the performance.
4 Conclusion
In this paper, we proposed a generic letter trans-
formation approach for text message normaliza-
tion without pre-categorizing the nonstandard to-
kens into insertion, deletion, substitution, etc. We
also avoided the expensive and time consuming hand
labeling process by automatically collecting a large
set of noisy training pairs. Results in the Twitter
and SMS domains show that our system can signif-
icantly outperform the state-of-the-art systems and
have good domain portability. In the future, we
would like to compare our method with a statistical
machine translation approach performed at the let-
ter level, evaluate the system using sentences by in-
corporating context word information, and consider
many-to-one letter transformation in the model.
75
5 Acknowledgments
The authors thank Deana Pennell for sharing the
look-up table generated using the deletion-based ab-
breviation approach. Thank Sittichai Jiampojamarn
for providing the many-to-many letter-phoneme
alignment data sets and toolkit. Part of this work
was done while Fei Liu was working as a research
intern in Bosch Research and Technology Center.
References
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for sms text normaliza-
tion. In Proceedings of the COLING/ACL, pages 33?
40.
Richard Beaufort, Sophie Roekhaut, Louise-Ame?lie
Cougnon, and Ce?drick Fairon. 2010. A hybrid
rule/model-based finite-state framework for normaliz-
ing sms messages. In Proceedings of the ACL, pages
770?779.
William B. Cavnar and John M. Trenkle. 1994. N-gram-
based text categorization. In Proceedings of Third An-
nual Symposium on Document Analysis and Informa-
tion Retrieval, pages 161?175.
Asli Celikyilmaz, Dilek Hakkani-Tur, and Junlan Feng.
2010. Probabilistic model-based sentiment analysis of
twitter messages. In Proceedings of the IEEE Work-
shop on Spoken Language Technology, pages 79?84.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu.
2007. Investigation and modeling of the structure of
texting language. International Journal on Document
Analysis and Recognition, 10(3):157?174.
Paul Cook and Suzanne Stevenson. 2009. An unsuper-
vised model for text messages normalization. In Pro-
ceedings of the NAACL HLT Workshop on Computa-
tional Approaches to Linguistic Creativity, pages 71?
78.
Matthew Hindson. 2006. En-
glish language hyphenation dictionary.
http://www.hindson.com.au/wordpress/2006/11/11/english-
language-hyphenation-dictionary/.
Mindaugas Idzelis. 2005. Jazzy: The java open source
spell checker. http://jazzy.sourceforge.net/.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and hidden markov models to letter-to-phoneme con-
version. In Proceedings of the HLT/NAACL, pages
372?379.
Catherine Kobus, Franc?ois Yvon, and Ge?raldine
Damnati. 2008. Normalizing sms: Are two metaphors
better than one? In Proceedings of the COLING, pages
441?448.
Taku Kudo. 2005. CRF++: Yet another CRF took kit.
http://crfpp.sourceforge.net/.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of the ICML, pages 282?289.
Deana L. Pennell and Yang Liu. 2010. Normalization
of text messages for text-to-speech. In Proceedings of
the ICASSP, pages 4842?4845.
Sasa Petrovic, Miles Osborne, and Victor Lavrenko.
2010. The edinburgh twitter corpus. In Proceedings
of the NAACL HLT Workshop on Computational Lin-
guistics in a World of Social Media, pages 25?26.
Richard Sproat, Alan W. Black, Stanley Chen, Shankar
Kumar, Mari Ostendorf, and Christopher Richards.
2001. Normalization of non-standard words. Com-
puter Speech and Language, 15(3):287?333.
Kristina Toutanova and Robert C. Moore. 2002. Pronun-
ciation modeling for improved spelling correction. In
Proceedings of the ACL, pages 144?151.
Dong Yang, Yi cheng Pan, and Sadaoki Furui. 2009.
Automatic chinese abbreviation generation using con-
ditional random field. In Proceedings of the NAACL
HLT, pages 273?276.
76
