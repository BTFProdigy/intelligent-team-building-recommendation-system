A Flexible Example Annotation Schema: Translation Corresponding 
Tree Representation 
Fai WONG, Dong Cheng HU, Yu Hang MAO 
Speech and Language Processing Research Center,  
Tsinghua University, 100084 Beijing 
huangh01@mails.tsinghua.edu.cn 
{hudc, myh-dau}@mail.tsinghua.edu.cn
Ming Chui DONG 
Faculty of Science and Technology 
of University of Macao, 
PO Box 3001, Macao SAR 
dmc@inesc-macau.org.mo 
Abstract 
This paper presents work on the task of con-
structing an example base1 from a given bi-
lingual corpus based on the annotation 
schema of Translation Corresponding Tree 
(TCT). Each TCT describes a translation ex-
ample (a pair of bilingual sentences). It repre-
sents the syntactic structure of source 
language sentence, and more importantly is 
the facility to specify the correspondences be-
tween string (both the source and target sen-
tences) and the representation tree. 
Furthermore, syntax transformation clues are 
also encapsulated at each node in the TCT 
representation to capture the differentiation of 
grammatical structure between the source and 
target languages. With this annotation 
schema, translation examples are effectively 
represented and organized in the bilingual 
knowledge database that we need for the Por-
tuguese to Chinese machine translation sys-
tem. 
1 Introduction 
The construction of bilingual knowledge base, in 
the development of example-based machine 
translation systems (Sato and Nagao, 1990), is 
vitally critical. In the translation process, the ap-
plication of bilingual examples concerns with 
how examples are used to facilitate translation, 
which involves the factorization of an input sen-
tence into the format of stored examples and the 
conversion of source texts into target texts in 
terms of the existing translations by referencing 
to the bilingual knowledge base. Theoretically 
speaking, examples can be achieved from bilin-
                                                          
1 Or bilingual knowledge base, we use the two terms inter-
changeably. 
gual corpus where the texts are aligned in senten-
tial level, and technically, we need an example 
base for convenient storage and retrieval of ex-
amples. The way of how the translation examples 
themselves are actually stored is closely related 
to the problem of searching for matches. In struc-
tural example-based machine translation systems 
(Grishman, 1994; Meyers et al, 1998; Watanabe 
et al, 2000), examples in the knowledge base are 
normally annotated with their constituency (Kaji 
et al, 1992) or dependency structures (Matsu-
moto et al, 1993; Aramaki et al, 2001; Al-
Adhaileh et al, 2002), which allows the corre-
sponding relations between source and target sen-
tences to be established at the structural level. All 
of these approaches annotate examples by mean 
of a pair of analyzed structures, one for each lan-
guage sentence, where the correspondences be-
tween inter levels of source and target structures 
are explicitly linked. However, we found that 
these approaches require the bilingual examples 
that have ?parallel? translations or ?close? syntac-
tic structures (Grishman, 1994), where the source 
sentence and target sentences have explicit corre-
spondences in the sentences-pair. For example, in 
(Wu, 1995), the translation examples used for 
building the translation alignments are selected 
based on strict constraints. As a result, these ap-
proaches indirectly limit their application in us-
ing the translation examples that are ?free 
translation? to the development of example-
based machine translation system. In practice, 
most of the existing bilingual corpus, the mean-
ings of the source sentences are interpreted in 
target language in the nature of ?freer?, other than 
literally translated in a projective manner and 
stayed as close to the source text as possible, in 
particular for the languages-pair that are struc-
tural divergences, such as Portuguese and Chi-
nese. 
As illustrated in Figure 1, the translation of the 
Portuguese sentence ?Onde ficam as barracas de 
praia?? is interpreted into ???????? 
(Where are the bathhouses?)? other than 
straightly translated to ???????? ? 
(Where are the tents of beach?)?. The translations 
of the words, i.e. ?barracas? and ?praia?, of the 
source sentence do not explicitly appear in target 
sentence. As a result, in the conventional align-
ment process, to achieve a fully aligned structural 
representation for such sentences-pair may be 
problematic. However, we found that such type 
of examples is very common. We have investi-
gated around 2100 bilingual examples that are 
extracted from a grammar book ?Gram?tica da 
L?ngua Portuguesa? (Wang and Lu, 1999), and 
found that 63.4% of examples belong to the dis-
cussed case, where the number of unmatched 
words is more than half the number of words in 
source sentence. In this paper, we overcome the 
problem by designing a flexible representation 
schema, called Translation Corresponding Tree 
(TCT). We use the TCT as the basic structure to 
annotate the examples in our example bilingual 
knowledge base for the Portuguese to Chinese 
example-based machine translation system. 
? ?
Onde ficam as barracas de praia ?
?????
?
 
Figure 1. An example of ?free translation?, where 
the translations of some words in Portuguese sen-
tence do not appear in target Chinese sentence. 
2 Translation Corresponding Tree 
(TCT) Representation 
TCT structure, as an extension of structure string-
tree correspondence representation (Boitet and 
Zaharin, 1988), is a general structure that can 
flexibly associate not only the string of a sen-
tence to its syntactic structure in source language, 
but also allow the language annotator to explic-
itly associate the string from its translation in 
target language for the purpose to describe the 
correspondences between different languages.  
2.1 The TCT Structure 
The TCT representation uses a triple sequence 
intervals [SNODE(n)/STREE(n)/STC(n)] en-
coded for each node in the tree to represent the 
corresponding relations between the structure of 
source sentence and the substrings from both the 
source and target sentences. In TCT structure, the 
correspondence is made up of three interrelated 
correspondences: 1) one between the node and 
the substring of source sentence encoded by the 
interval SNODE(n), which denotes the interval 
containing the substring corresponding to the 
node; 2) one between the subtree and the sub-
string of source sentence represented by the in-
terval STREE(n), which indicates the interval of 
substring that is dominated by the subtree with 
the node as root; and 3) the other between the 
subtree of source sentence and the substring of 
target sentence expressed by the interval STC(n), 
which indicates the interval containing the sub-
string in target sentence corresponding to the 
subtree of source sentence. The associated sub-
strings may be discontinuous in all cases. This 
annotation schema is quite suitable for represent-
ing translation example, where it preserves the 
strength in describing non-standard and non-
projective linguistic phenomena for a language 
(Boitet and Zaharin, 1988; Al-Adhaileh et al, 
2002), on the other hand, it allows the annotator 
to flexibly define the corresponding translation 
substring from the target sentence to the repre-
sentation tree of source sentence when it is nec-
essary. This is actually the central idea behind the 
formalism of TCT. 
NP(4/3-6/1-3)
Onde1 ficam2 as3 de5 praia6
PP(5/5-6/?)
Adv(1/1/5-6) V(2/2/4)
S(2/1-6/1-6)
VP(2/2-6/1-4)
NP(4/3-4/?)
Syntactic Tree
Source
String{ Det(3/3/?) Prep(5/5/?) N(6/6/?)barracas4
N(4/4/?)
?4
?1
?2
?3
?5
?6
Target String{
 
Figure 2. An TCT representation for annotating 
the translation example "Onde ficam as barracas 
de praia? (Where are the bathhouses?) / 
???????" and its phrase structure together 
with the correspondences between the substrings 
(of both the source and target sentences) and the 
subtrees of sentence in source language. 
 
As illustrated in Figure 2, the translation ex-
ample ?Onde ficam as barracas de praia?/ 
???????? is annotated  in a TCT struc-
ture. Based on the interpretation structure of the 
source sentence ?Onde ficam as barracas de 
praia??, the correspondences between the sub-
strings (of source and target sentences) and the 
grammatical units at different inter levels of the 
syntactic tree of the source sentence are ex-
pressed in terms of sequence intervals. The words 
of the sentences pair are assigned with their posi-
tions respectively, i.e. ?Onde (1)?, ?ficam (2)?, 
?as (3)?, ?barracas (4)?, ?de (5)? and ?praia (6)? 
for the source sentence, as well as for the target 
sentence. But considering that Chinese uses 
ideograms in writing without any explicit word 
delimiters, the process to identify the boundaries 
of words is considered to be the task of word 
segmentation (Teahan et al, 2000), instead of 
assigning indices in word level with the help of 
word segmentation utility, a position interval is 
assigned to each character for the target (Chi-
nese) sentence, i.e. ?? (1)?, ?? (2)?, ?? (3)?, 
?? (4)?, ?? (5)? and ?? (6)?. Hence, a sub-
string in source sentence that corresponds to the 
node of its representation is denoted by the inter-
vals encoded in SNODE(n) for the node, e.g. the 
shaded node, NP, with interval, SNODE(NP)=4, 
corresponds to the substring ?barracas? in source 
sentence that has the same interval. A substring 
of source sentence that corresponds to a subtree 
of its syntactic tree is denoted by the interval re-
corded in STREE(n) attached to the root of the 
subtree, e.g. the subtree of the shaded node, NP, 
encoded with the interval, STREE(NP)=3-6, cor-
responds to the substring ?as barracas de praia? 
in source sentence. While the translation corre-
spondence between the subtree of source sen-
tence and substring in the target sentence is 
denoted by the interval assigned to the STC(n) of 
each node, e.g. the subtree rooted at shaded node, 
NP, with interval, STC(NP)=1-3, corresponds to 
the translation fragment (substring) ????? in 
target sentence. 
2.2 Expressiveness of Linguistic Infor-
mation 
Another inherited characteristic of TCT structure 
is that it can be flexibly extended to keep various 
kinds of linguistic information, if they are con-
sidered useful for specific purpose, in particularly 
the linguistic information that differentiating the 
characteristics of two languages which are struc-
tural divergences (Wong et al, 2001). Basically, 
each node representing a grammatical constituent 
in the TCT annotation is tagged with grammati-
cal category (part of speech). Such feature is 
quite suitable for the describing specific linguis-
tic phenomena due to the characteristic of a lan-
guage. For instance, in our case, the crossing 
dependencies (syntax transformation rules) for 
the sentence constituents between Portuguese and 
Chinese are captured and attached to each node 
in the TCT structure for a constituent that indi-
cates the order in forming the corresponding 
translation for the node from the subtrees it 
dominated. In many phrasal matching ap-
proaches, such as constituency-oriented (Kaji et 
al., 1992; Grishman, 1994) and dependency-
oriented (Matsumoto et al, 1993; Watanabe et 
al., 2000; Aramaki et al, 2001), crossing con-
straints are deployed implicitly in finding the 
structural correspondences between pair of repre-
sentation trees of a source sentence and its trans-
lation in target. Here, in our TCT representation, 
we adopted the use of constraint (Wu, 1995) for a 
constituent unit, where the immediate subtrees 
are only allowed to cross in the inverted order. 
Such constraints, during the phase of target lan-
guage generation, can help in determining the 
order in producing the translation for an interme-
diate constituent unit from its subtrees when the 
corresponding translation of the unit is not asso-
ciated in the TCT representation. 
Tree
Source
String { Onde1 ficam2
Adv(1/1/5-6) V(2/2/4)
NP(4/3-6/1-3)
S(2/1-6/1-6)
VP(2/2-6/1-4)
?4 ?5?6?1?2?3
Target
String {
as3 barracas4 de5 praia6
 
Figure 3. The transfer relationships between the 
sentence-constituents of source language and its 
translation in target language are recorded in 
TCT structure. 
Figure 3 demonstrates the crossing relations 
between the source and target constituents in an 
TCT representation structure. In graphical struc-
ture annotation, a horizontal line is used to repre-
sent the inversion of translation fragments of its 
immediate subtrees. For example, the translation 
substring ?????? of the shaded node, VP, 
can be obtained by inverting the order of the cor-
responding target translations ??? and ????? 
from the dominated nodes V and NP. Therefore, 
such schema can serve as a mean to represent 
translation examples, and find structural corre-
spondences for the purpose of transfer grammar 
learning (Watanabe et al, 2000; Matsumoto et 
al., 1993; Meyers et al, 1998). 
3 Construction of Example Base 
In the construction of bilingual knowledge base 
(example base) in example-based machine trans-
lation system (Sato and Nagao, 1990; Watanabe 
et al, 2000), translation examples are usually 
annotated by mean of a pair analyzed structures, 
where the corresponding relations between the 
source and target sentences are established at the 
structural level through the explicit links. Here, 
to facilitate such examples representation, we use 
the Translation Corresponding Tree as the basic 
annotation structure. The main different and ad-
vantage of our approach is that it uses a single 
language parser to process other than two differ-
ent parsers, one for each language (Tang and Al-
Adhaileh, 2001). 
In our example base, each translation pairs is 
stored in terms of an TCT structure. The con-
struction starts by analyzing the grammatical 
structure of Portuguese sentence with the aid of a 
Portuguese parser, and a shallow analysis to the 
Chinese sentence is carried out by using the Chi-
nese Lexical Analysis System (ICTCLAS) 
(Zhang, 2002) to segment and tag the words with 
a part of speech. The grammatical structure pro-
duced by the parser for Portuguese sentence is 
then used for establishing the correspondences 
between the surface substrings and the inter lev-
els of its structure, which includes the correspon-
dences between nodes and its substrings, as well 
as the correspondences between subtrees and 
substrings in the sentence. Next, in order to iden-
tify and establish the translation correspondences 
for structural constituents of Portuguese sentence, 
it relies on the grammatical information of the 
analyzed structure of Portuguese and a given bi-
lingual dictionary to search the corresponding 
translation substrings from the Chinese sentence. 
Finally, the consequent TCT structure will be 
verified and edited manually to obtain the final 
representation, which is the basic element of the 
knowledge base. 
3.1 The TCT Generation Algorithm 
In the overall construction processes, the task to 
compile the syntactic structure of source sentence 
into the TCT representation by linking the trans-
lation fragments from the target sentence is the 
vital part. The following steps present the com-
plete process to generate an TCT structure for a 
translation example ?Actos anteriores ? publici-
dade da ac??o (Publicity of action prior to acts) / 
????????????. 
Parsing Portuguese Sentence 
The process begins by parsing the Portuguese 
sentences with a Portuguese parser. The parsing 
result is a phrase structure in terms of bracketed 
annotation. Each bracketed constituent of the 
structure tree is attached with a grammatical 
category. Figure 4 shows the resultant parsed 
structure of the Portuguese sentence. 
(S (N Actos) (AdjP (Adj anteriores) (PP (Prep ?)
(NP (N publicidade) (PP (Prep da) (N ac??o))))))
ParserActos anteriores ? publicidade da ac??o
 
Figure 4. Portuguese sentence is analyzed by a 
linguistic parser, and its output is the phrase 
structure expressed in bracket notation. 
???????????
?/p ??/v ??/v ?/f ?/u ?/v ?/u ??/n
Lexical
Analyser
 
Figure 5. The analyzed lexical items for Chinese 
sentence. 
Analyzing Chinese Sentence 
The construction of TCT structure is fundamen-
tally based on the syntactic structure of Portu-
guese sentence. The finding of translation units 
between the sentences pair is relying on structure 
tree of Portuguese sentence and the sequences of 
lexical words from Chinese sentence. Thus, in-
stead of analyzing the Chinese sentence in deep, 
we analyze the Chinese sentence in the lexical 
level by using the Chinese Lexical Analysis Sys-
tem (ICTCLAS) (Zhang, 2002). Each Chinese 
word is delimited with spaces and assigned with 
a part of speech as illustrated in Figure 5. 
Constructing Correspondence Structure 
for Portuguese Sentence 
After parsing and obtaining the syntactic struc-
ture of Portuguese sentence, next step is to com-
pute the correspondences for the structure against 
the surface strings of the source sentence, which 
includes the corresponding phrase for a constitu-
ent unit in the tree and the corresponding content 
word that headed the constituent unit, both of 
these correspondences are denoted by the se-
quence intervals of the substrings spanning 
across the sentence fragments. In finding the cor-
responding phrasal substrings for subtrees, we 
start associating the lexical words to the corre-
sponding terminal nodes of the structure tree by 
assigning the related offsets to SNODE(n) and 
STREE(n) of the nodes. Then we proceed to next 
upper level constituent units in the tree where the 
corresponding substrings are derived by connect-
ing the lexical words from the nodes in the lower 
level it dominated. Theoretically, if node, N, has 
m daughters, N1?Nm, then the sequence interval 
for N will be STREE(N) = STREE(N1) ? 
STREE(N2) ??? STREE(Nm), the interval is 
bounded by spanning nodes of its immediate sub-
trees. To identify the lexical head for a constitu-
ent unit, we use simple rule to determine it by 
considering the grammatical category of the 
phrasal unit, and choose the word that owns the 
same category from the daughter nodes, then as-
sign the interval of chosen to SNODE(N). Figure 
6 shows the structure produced in this stage. 
Actos1
N(1/1)
anteriores2 ?3 publicidade4 da5 ac??o6
S(1/1-6)
AdjP(2/2-6)
PP(3/3-6)
NP(4/4-6)
PP(5/5-6)
Adj(2/2) Prep(3/3) N(4/4) Prep(5/5) N(6/6)
 
Figure 6. The Portuguese correspondence struc-
ture. 
Associating Translation Correspondences 
In this process, we adopt a search for alignments 
between constituent units of Portuguese sentence 
and the corresponding translation fragments from 
Chinese sentence, proceeding bottom-up through 
the tree. It makes use of the information about 
possible lexical correspondences from a bilingual 
dictionary and the grammatical categories of the 
lexical words, tagged in previous stage, to gener-
ate initial candidate alignments. Figure 7 presents 
the initial lexical alignments. 
ActosN  anterioresAdj  ?Prep  publicidadeN  daPrep  ac
? U  ? V  ? U  
??oN
? P  ?? V  ?? V  ? F  ?? N  
Figure 7. Initial candidate alignments of corre-
sponding words. 
Based on the possible word correspondences, 
the associated structure of the Portuguese sen-
tence, together with the grammatical categories 
information, the search proceeds to align phrases 
by gradually increasing length (phrasal corre-
spondences in different levels of constituent tree) 
based on the following criterions. 
First, for any un-aligned words sequence ?wua? 
being bounded by aligned words of daughter 
nodes ?wa-left? and ?wa-right?, we take the whole 
fragment ?wa-leftwuawa-right? (including the bound-
ing words or phrases) as the corresponding sub-
string for the parent node that immediately 
dominates the daughter nodes, such that STC(N) 
= STC(Nleft) ? STC(Nright).  
Second, for the case that the un-aligned frag-
ment is not bounded by any aligned units, our 
approach relies on the assumption that if two set 
of sentence constituents (source and target sen-
tences) are corresponding, their grammatical 
categories as well as the number of constituents 
should be consistent. The essential idea of the 
search is to look for inter levels where the con-
stituent units of the structure of Portuguese sen-
tence and the lexical words in Chinese sentence 
can be projected in one-to-one manner. We use 
the previous example ?Onde ficam as barracas 
de praia? (Where are the bathhouses?)/ 
???????? to illustrate the searching strat-
egy. Beside the corresponding lexical items, e.g. 
?Onde / ??? and ?Ficam / ??, that can be de-
termined with the aid of a given dictionary, the 
process proceeds bottom-up and searches through 
the tree by considering only the unmatched items 
that if the assumption hold or not. For example, 
at the leaf level, the different numbers of the 
lexical items (?asDet, barracasN, dePrep, praiaN? 
and ????N?) violates the assumption. The 
process repeats the investigation in next upper 
level in the representation structure of Portuguese 
sentence. As illustrated in Figure 8, the alignment 
can be identified only at the level where the 
number and the part of speech of constituent unit 
of Portuguese (?[as barracas de praia]NP?) are 
consistent to that of the lexical item in Chinese 
sentence (?[???]N?). Consequently, the corre-
spondences between the associated structure of 
Portuguese sentence and the translation frag-
ments of Chinese sentence can be determined and 
established. For any node in the structure which 
has no translation equivalent is assigned with 
?empty (?)? interval to STC(N). 
PP(5/5-6/?)
Adv(1/1/5-6) V(2/2/4)
NP(4/3-6/1-3)
S(2/1-6/1-6)
VP(2/2-6/1-4)
NP(4/3-4/?)
Det(3/3/?) N(4/4/?) Prep(5/5/?) N(6/6/?)
Onde1 ficam2 as3 barracas4 de5 praia6
?1?2?3[N] ?4[V] ?5?6[Adv]  
Figure 8. Finding the alignment for unbounded 
words. 
Third, for acquiring the crossing constraint for 
a constituent node in the representation tree, 
which is determined by examining the order of 
the translation correspondences of the spanning 
nodes against the sequence of those appeared in 
Chinese sentence. For any node that representing 
Portuguese phrase whose corresponding transla-
tion is derived from its daughters by inverting the 
corresponding translations is denoted by assign-
ing a Boolean value to INVERT(N) attached to 
the node. In graphical annotation, a horizontal 
line is used as a sign for indicating the inversion. 
As demonstrated in Figure 9, the corresponding 
translations of the daughters of node S are 
crossed between the sentences of Portuguese and 
its translation in Chinese. The corresponding 
translation ???????? of its second daugh-
ter appears prior to that ???? of the first daugh-
ter node in the target translation of Portuguese 
sentence. Hence the inversion property for the 
constituent node in the syntactic structure of 
source sentence is consequently determined.  
S(1/1-6/1-11)
?1?2?3?4?5?6             ?7?8?9             ?10?11
anteriores2 ?3 publicidade4 da5 ac??o6
AdjP(2/2-6/1-6)
N(1/1/10-11)
Actos1
 
Figure 9. Determination of crossing dependency 
between the translation correspondences 
Finally, in case the representation of TCT gen-
erated in previous process needs further editing, 
an TCT editor can be used to perform the neces-
sary amendment. Figure 10 presents the final 
TCT structure describing a translation example. 
S(1/1-6/1-11)
AdjP(2/2-6/1-6)
PP(3/3-6/1-5)
NP(4/4-6/2-5)
PP(5/5-6/2-3)
N(1/1/10-11) Adj(2/2/6) Prep(3/3/1) N(4/4/4-5) Prep(5/5/?) N(6/6/2-3)
Actos1 anteriores2 ?3 publicidade4 da5 ac??o6
? 1   ? 2? 3   ? 4? 5   ? 6   ? 7   ? 8   ? 9   ? 10? 11  
Figure 10. An TCT structure constructed for the 
translation example ?Actos anteriores ? publici-
dade da ac??o (Publicity of action prior to acts) / 
????????????. 
3.2 Translation Equivalents 
Through the notation of translation correspond-
ing structure for representing translation exam-
ples in the bilingual knowledge base, the 
translation units between the Portuguese sentence 
and its target translation in Chinese are explicitly 
expressed by the sequence intervals STREE(n) 
and STC(n) encoded in the intermediate nodes of 
an TCT structure, that may represent the phrasal 
and lexical correspondences. For instance, from 
the translation example being annotated under the 
TCT representation schema as shown in Figure 
10, the Chinese translation ??? ? of Portuguese 
word ?ac??o? is denoted by [STREE(n)=6/ 
STC(n)=2-3] in the terminal node. For phrasal 
translation, we may visit the higher level con-
stituents in the representing structure of TCT and 
apply the similar coding information to retrieve 
the corresponding translation for the unit that 
representing a phrasal constituent in a sentence. 
Each TCT structure is being indexed by its nodes 
in the bilingual knowledge base, in order that the 
representation examples can be effectively con-
sulted. 
4 Conclusion 
In this paper, a novel annotation schema for 
translation examples, called Translation Corre-
sponding Tree (TCT) structure, is proposed and 
has been applied to the construction of bilingual 
knowledge base (example base) to be used for the 
Portuguese to Chinese machine translation sys-
tem. The TCT representation provides a flexible 
nature to describe the corresponding relations 
between the inter levels of the structure against 
its substrings in a sentence, in particular the cor-
responding translation fragments (substrings) 
from the target translation sentence are explicitly 
expressed in the structure. We have proposed a 
strategy to semi-automate the example base con-
struction process. A preliminary TCT structure 
for a translation example is first produced by the 
system, then the representation structure can be 
further modified manually through an TCT editor 
to get the final structure.  
Acknowledgement 
The research work reported in this paper was 
supported by the Research Committee of Univer-
sity of Macao under grant CATIVO:3678. 
References 
Mosleh Hmoud Al-Adhaileh, Enya Kong Tang, and 
Yusoff Zaharin. 2002. A Synchronization Structure 
of SSTC and Its Applications in Machine Transla-
tion. The COLING 2002 Post-Conference Work-
shop on Machine Translation in Asia, Taipei, 
Taiwan. 
Eiji Aramaki, Sadao Kurohashi, Satoshi Sato, and 
Hideo Watanabe. 2001. Finding Translation Corre-
spondences from Parallel Parsed Corpus for Ex-
ample-based Translation. In Proceedings of MT 
Summit VIII, pp.27-32. 
Christian Boitet, and Yusoff Zaharin. 1988. Represen-
tation trees and string-tree correspondences. In 
Proceeding of COLING-88, Budapest, pp.59-64. 
Ralph Grishman. 1994. Iterative Alignment of Syntac-
tic Structures for a Bilingual Corpus. In Proceed-
ings of Second Annual Workshop on Very Large 
Corpora (WVLC2), Kyoto, Japan, pp.57-68. 
Hiroyuki Kaji, Yuuko Kida, and Yasutsugu Mori-
moto. 1992. Learning Translation Templates from 
Bilingual Text. In Proceeding of COLING-92, 
Nantes, pp.672-678. 
Yuji Matsumoto, Hiroyuki Isimoto, and Takehito 
Utsuro. 1993. Structural Matching of Parallel 
Texts. 31st Annual Meeting of the Association for 
Computational Linguistics, Columbus, Ohio, 
pp.23-30. 
Adam Meyers, Roman Yangarber, and Brown Ralf. 
1998. Deriving Transfer Rules from Dominance-
Preserving Alignments. In Proceedings of Coling-
ACL (1998), pp.843-847. 
Satoshi Sato, and Magnus Nagao. 1990. Toward 
Memory-Based Translation. In Proceeding of Col-
ing (1990): pp.247-252. 
Enya Kong Tang, and Mosleh Hmoud Al-Adhaileh. 
2001. Converting a Bilingual Dictionary into a Bi-
lingual Knowledge Bank based on the Synchronous 
SSTC. In Proceedings of Machine Translation 
Summit VIII, Spain, pp.351-356. 
Suo Ying Wang, and Yan Bin Lu. 1999. Gram?tica da 
L?ngua Portuguesa. Shanghai Foreign Language 
Education Press. 
Fai Wong, Yu Hang Mao, Qing Fu Dong, and Yi 
Hong Qi. 2001. Automatic Translation: Overcome 
the Barriers between European and Chinese Lan-
guages. In Proceedings (CD Version) of First In-
ternational UNL Open Conference 2001, SuZhou 
China. 
Dekai Wu. 1995. Grammarless extraction of phrasal 
translation examples from parallel texts. In Pro-
ceedings of TMI-95, Sixth International Confer-
ence on Theoretical and Methodological Issues in 
Machine Translation, v2, Leuven Belgium, pp.354-
372. 
Hua Ping Zhang. 2002. ICTCLAS. Institute of Com-
puting Technology,Chinese Academy of Sciences: 
http://www.ict.ac.cn/freeware/003_ictclas.asp. 
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 612 ? 623, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Machine Translation Based on Constraint-Based 
Synchronous Grammar 
Fai Wong1, Dong-Cheng Hu1, Yu-Hang Mao1, 
Ming-Chui Dong2, and Yi-Ping Li2 
1
 Speech and Language Processing Research Center, 
Department of Automation, Tsinghua University, 100084 Beijing 
huangh01@mails.tsinghua.edu.cn  
{hudc, myh-dau}@mail.tsinghua.edu.cn 
2
 Faculty of Science and Technology of University of Macao, 
Av. Padre Tom?s Pereira S.J., Taipa, Macao 
{dmc, ypli}@umac.mo 
Abstract. This paper proposes a variation of synchronous grammar based on 
the formalism of context-free grammar by generalizing the first component of 
productions that models the source text, named Constraint-based Synchronous 
Grammar (CSG). Unlike other synchronous grammars, CSG allows multiple 
target productions to be associated to a single source production rule, which can 
be used to guide a parser to infer different possible translational equivalences 
for a recognized input string according to the feature constraints of symbols in 
the pattern. Furthermore, CSG is augmented with independent rewriting that al-
lows expressing discontinuous constituents in the inference rules. It turns out 
that such grammar is more expressive to model the translational equivalences of 
parallel texts for machine translation, and in this paper, we propose the use of 
CSG as a basis for building a machine translation (MT) system for Portuguese 
to Chinese translation. 
1   Introduction 
In machine translation, to analyze the structure deviations of languages pair hence to 
carry out the transformation from one language into another as the target translation is 
the kernel part in a translation system, and this requires a large amount of structural 
transformations in both grammatical and concept level. The problems of syntactic com-
plexity and word sense ambiguity have been the major obstacles to produce promising 
quality of translation. In order to overcome the obstacles and hence to improve the qual-
ity of translation systems, several alternative approaches have been proposed.  
As stated in [1], much of the theoretical linguistics can be formulated in a very 
natural manner as stating correspondences between layers of representations. In simi-
lar, many problems in natural language processing, in particular language translation 
and grammar rewriting systems, can be expressed as transduction through the use of 
synchronous formalisms [2,3,4,5,6]. Recently, synchronous grammars are becoming 
more and more popular for the formal description of parallel texts representing trans-
lations for the same document. The underlying idea of such formalisms is to combine 
two generative devices through a pairing of their productions in such a way that right 
 Machine Translation Based on Constraint-Based Synchronous Grammar 613 
hand side non-terminal symbols in the paired productions are linked. However, such 
formalisms are less expressive and unable to express mutual translations that have 
different lengths and crossing dependencies. Moreover, synchronous formalisms do 
not deal with unification and feature structures, as in unification-based formalisms, 
that give patterns additional power for describing constraints on features. For exam-
ples, Multiple Context-Free Grammar [4], where functions are engaged to the non-
terminal symbols in the productions to further interpreting the symbols in target gen-
eration. In [7], Inversion Transduction Grammar (ITG) has been proposed for simul-
taneously bracketing parallel corpora as a variant of Syntax Directed translation 
schema [8]. But these formalisms are lacked of expressive to describe discontinuous 
constituents in linguistic expression. Generalized Multitext Grammar (GMTG) pro-
posed by [5,9] is constructed by maintaining two sets of productions as components, 
one for each language, for modeling parallel texts. Although GMTG is more expres-
sive and can be used to express as independent rewriting, the lack of flexibility in the 
way to describe constraints on the features associated with a non-terminal makes it 
difficult to the development of practical MT system. 
In this paper, a variation of synchronous grammar, Constraint-Based Synchronous 
Grammar (CSG), is proposed based on the formalism of context-free grammar. 
Through the use of feature structures as that in unification-based grammar, the first 
component of productions in CSG, that describes the sentential patterns for source 
text, is generalized while the corresponding target rewriting rules for each production 
are grouped in a vector representing the possible translation patterns for source pro-
duction. The choice of rule for target generation is based on the constraints on fea-
tures of non-terminal symbols in pattern. Our motivation is three-fold. First, synchro-
nous formalisms have been proposed for modeling of parallel text, and such algo-
rithms can infer the synchronous structures of texts for two different languages 
through the grammar representation of their syntactic deviations. That is quite suitable 
for use in the analysis of languages pair in the development of MT system. Secondly, 
by augmented the synchronous models with feature structures can enhance the pattern 
with additional power in describing gender, number, agreement, etc. Since the de-
scriptive power of unification-based grammars is considerably greater than that of 
classical CFG [10,11]. Finally, by retaining the notational and intuitive simplicity of 
CFG, we can enjoy both a grammar formalism with better descriptive power than 
CFG and more efficient parsing and generation algorithm controlled by the feature 
constraints of symbols hence to achieve the purposes of word sense and syntax dis-
ambiguation. 
2   Constraint-Based Synchronous Grammars 
Constraint-Based Synchronous Grammars (CSG) is defined by means of the syntax of 
context-free grammar (CFG) to the case of synchronous. The formalism consists of a 
set of generative productions and each production is constructed by a pair of CFG 
rules with zero and more syntactic head and link constraints for the non-terminal 
symbols in patterns. In a similar way, the first component (in right hand side of pro-
ductions) represents the sentential patterns of source language, while the second com-
ponent represents the translation patterns in target language, called source and target 
component respectively in CSG. Unlike other synchronous formalisms, the target 
614 F. Wong et al 
component of production consists of one or more generative rules associated with 
zero or more controlled conditions based on the features of non-terminal symbols of 
source rule for describing the possible generation correspondences in target transla-
tion. In such a way, the source components in CSG are generalized by leaving the task 
of handling constraints on features in target component, so this also helps to reduce 
the grammar size. For example, following is one of the productions used in the MT 
system for Portuguese to Chinese translation: 
S ? NP1 VP* NP2 PP NP3 {[NP1 VP1 NP3 VP2 NP2;VPcate=vb1, 
VPs:sem = NP1sem, VPio:sem=NP2sem,VPo:sem=NP3sem], 
[NP1 VP NP3 NP2 ;VP =vb0,VPs:sem =NP1sem, 
VPio:sem=NP2sem]} 
(1) 
The production has two components beside the reduced syntactic symbol on left 
hand side, the first modeling Portuguese and the second Chinese. The target compo-
nent in this production consists of two generative rules maintained in vector, and each 
of which is engaged with control conditions based on the features of symbols from the 
source component, and this is used as the selectional preferences in parsing. These 
constraints, in the parsing/generation algorithm, are used for inferring, not only, the 
structure of input to dedicate what structures are possible or probable, but also the 
structure of output text for target translation. For example, the condition expression: 
VPcate=vb1, VPs:sem=NP1sem, VPio:sem=NP2sem, VPo:sem=NP3sem, specifies if the senses of 
the first, second and the third nouns (NPs) in the input strings matched to that of the 
subject, direct and indirect objects governed by the verb, VP, with the category type 
of vb1. Once the condition gets satisfied, the source structure is successfully recog-
nized and the corresponding structure of target language, NP1 VP1 NP3 VP2 NP2, is 
determined also. 
Non-terminal symbols in source and target rules are linked if they are given the 
same index ?subscripts? for case of multiple occurrences, such as NPs in the produc-
tion: S ? NP1 VP NP2 PP NP3 [NP1 VP* NP3 NP2], otherwise symbols that appear 
only once in both the source and target rules, such as VPs, are implicitly linked to 
give the synchronous rewriting. Linked non-terminal must be derived from a se-
quence of synchronized pairs. Consider the production: S ? NP1 VP NP2 PP NP3 
[NP1 VP* NP3 NP2], the second NP (NP2) in the source rule corresponds to the third 
NP (NP2) in the target rule, the third NP (NP3) in source rule corresponds to the sec-
ond NP (NP3) in target pattern, while the first NP (NP1) and VP correspond to each 
other in both source and target rules. The symbol marked by an ?*? is designated as 
head element in pattern, this allows the features of designated head symbol propagate 
to the reduced non-terminal symbol in the left hand side of production rule, hence to 
achieve the property of features inheritance in CSG formalism. The use of features 
structures associated to non-terminal symbols will be discussed in the later section in 
this paper.  
In modeling of natural language, in particular for the process of languages-pair, the 
treatment for non-standard linguistic phenomena, i.e. crossing dependencies, discon-
tinuous constituents, etc., is very important due to the structure deviations of two 
different languages, in particular for languages from different families such as Portu-
guese and Chinese [12,13]. Linguistic expressions can vanish and appear in transla-
tion. For example, the preposition (PP) in the source rule does not show up in any of 
 Machine Translation Based on Constraint-Based Synchronous Grammar 615 
the target rules in Production (1). In contrast, Production (2) allows the Chinese char-
acters of ??? and ??? to appear in the target rules for purpose to modify the noun 
(NP) together with the quantifier (num) as the proper translation for the source text. 
This explicitly relaxes the synchronization constraint, so that the two components can 
be rewritten independently.  
NP ? num NP* {[num ? NP; NPsem=SEM_book],  
[num ? NP; NPsem=SEM_automobile]} (2) 
A remarkable strength of CSG is its expressive power to the description of discon-
tinuous constituents. In Chinese, the use of combination words that discontinuously 
distributed in a sentence is very common. For example, take the sentences pair [?Ele 
vendeu-me todas as uvas. (He sell me all the grapes.)?, ???????????? ?]. 
The Chinese preposition ??? and the verb ????? should be paired with the Portu-
guese verb ?vendeu?, and this causes a fan-out1 and discontinuous constituent in the 
Chinese component. The following fragment of CSG productions represents such 
relationships. 
S ? NP1 VP* NP2 NP3 {[NP1 VP1 NP3 VP2 NP2 ; VPcate=vb0,?],..} 
VP ? vendeu* {[?, ??? ;?]} 
(3) 
(4) 
In Production (3), the corresponding discontinuous constituents of VP (from source 
rule) are represented by VP1 and VP2 respectively in the target rule, where the ?super-
scripts? are added to indicate the pairing of the VP in target component. The corre-
sponding translation constituents in the lexicalized production are separated by com-
mas representing the discontinuity between constituents ??? and ????? in target 
translation. During the rewriting phase, the corresponding constituents will be used to 
replace the syntactic symbols in pattern rule. 
3   Definitions 
Let L be a context-free language defined over terminal symbol VT and generated by a 
context-free grammar G using non-terminal symbol VN disjointed with VT, starting 
symbol S, and productions of the form A ? w where A is in VN and w in (VN?VT)*. 
Let Z as a set of integers, each non-terminal symbol in VN is assigned with an integer, 
?(VN) = {W? | W ? VN, ? ? Z}. The elements of ?(VN) are indexed non-terminal sym-
bols. Now, we extend to include the set of terminal symbols VT? as the translation in 
target language, disjoint from VT, (VT VT?=?). Let R = {r1, ?, rn | ri? (?(VN)?VT?), 1 ? 
i ? n} be a finite set of rules, and C = {c1, ?, cm} be a finite set of constraints over the 
associated features of (?(VN)?VT), where the features of non-terminal ?(VN), the syn-
tactic symbols, are inherited from the designated head element during rule reduction. 
A target rule is defined as pair [r?R*, c?C*] in ?, where ? = R*?C* in form of [r, c]. 
Now, we define ?(?i) to denote the number of conjunct features being considered in 
                                                          
1 We use this term for describing a word where its translation is paired of discontinuous words 
in target language, e.g. ?vendeu[-pro] [NP]? in Portuguese gives similar English translation as 
?sell [NP] to [pro]?, so ?vendeu?, in this case, is corresponding to ?sell? and ?to?. 
616 F. Wong et al 
the associated constraint, hence to determine the degree of generalization for a con-
straint. Therefore, the rules, ?i and ?j, are orderable, ?i p  ?j, if ?(?i) ? ?(?j) (or ?i f  ?j, 
if ?(?i) < ?(?j)). For ?i p  ?j (?(?i) ? ?(?j)), we say, the constraint of the rule, ?i, is 
more specific, while the constraint of ?j is more general. In what follows, we consider 
a set of related target rules working over the symbols, w?, on the RHS of production A 
? w?, the source rule, where w? ? ?(VN)?VT. All of these non-terminals are co-
indexed as link. 
Definition 1: A target component is defined as a ordered vector of target rules in ?  
having the form ? = {?1, ?, ? q}, where 1 ? i ? q to denote the i-th tuple of ?. The 
rules are being arranged in the order of ?1 p ?2p  ?p ?q. 
In rule reduction, the association conditions of the target rules are used for investi-
gating the features of corresponding symbols in source rules, similar to that of feature 
unification, to determine if the active reduction successes or not. At the mean while, 
this helps in determining the proper structure as the target correspondence.  
Definition 2: A Constraint-Based Synchronous Grammar (CSG) is defined to be  
5-tuple G = (VN, VT, P, CT, S) which satisfies the following conditions: 
? VN is a finite set of non-terminal symbols; 
? VT is a finite set of terminal symbols which is disjoint with VN; 
? CT is a finite set of target components; 
? P is a finite set of productions of the form A ? ? ?, where ? ? (?(VN)?VT)* 
and, ? ? CT, the non-terminal symbols that occur from both the source and target 
rules are linked under the index given by ?(VN)2. 
? S ? VN is the initial symbol. 
For example, the following CSG productions can generate both of the parallel texts 
[?Ele deu um livro ao Jos?. (He gave a book to Jos?)?, ??????????] and [?Ele 
comprou um livro ao Jos?. (He bought a book from Jos?)?, ???????????]: 
S ? NP1 VP* NP2 PP NP3 {[NP1 VP1 NP3 VP2 NP2;VPcate=vb1, 
VPs:sem = NP1sem, Pio:sem=NP2sem,VPo:sem=NP3sem], 
[NP1 VP NP3 NP2 ;VP =vb0,VPs:sem =NP1sem, 
VPio:sem=NP2sem]} 
(5) 
 
VP ? v3 {[v ; ?]} (6) 
NP ? det NP* {[NP ; ?]} (7) 
NP ? num NP* {[num ?NP; NPsem=SEM_book]} (8) 
                                                          
2
  Link constraints are dedicated by the symbols indices, which is trivially for connecting the 
corresponding symbols between the source and target rules. Hence, we assume, without loss 
of generality, that index is only given to the non-terminal symbols that have multiple occur-
rences in the production rules. It is assumed that ?S ? NP1 VP2 PP3 NP4 {NP1 VP21 NP4 
VP22}? implies ?S ? NP1 VP PP NP2 {NP1 VP1 NP2 VP2}?. 
3
  Similar for the designation of head element in productions, the only symbol from the RHS of 
production will inherently be the head element. Thus, no head mark ?*? is given for such 
rules, and we assume that ?VP ? v*? implies ?VP ? v?. 
 Machine Translation Based on Constraint-Based Synchronous Grammar 617 
NP ? n {[n ; ?]} (9) 
NP ? pro {[pro ; ?]} (10) 
PP ? p {[p ; ?]} (11) 
n ? Jos? {[?? ; ?]}| livro {[? ; ?]} (12) 
pro ? ele {[? ; ?]} (13) 
v ? deu{[?? ; ?]} | comprou {[?, ?? ;?]} (14) 
num ? um {[? ; ?]} (15) 
p ? a {?} (16) 
det ? o {?} (17) 
A set P of productions is said to accept an input string s iff there is a derivation se-
quence Q for s using source rules of P, and any of the constraint associated with every 
target component in Q is satisfied4. Similarly, P is said to translate s iff there is a 
synchronized derivation sequence Q for s such that P accepts s, and the link con-
straints of associated target rules in Q is satisfied. The derivation Q then produces a 
translation t as the resulting sequence of terminal symbols included in the determined 
target rules in Q. The translation of an input string s essentially consists of three steps. 
First, the input string is parsed by using the source rules of productions. Secondly, the 
link constraints are propagated from source rule to target component to determine and 
build a target derivation sequence. Finally, translation of input string is generated 
from the target derivation sequence. 
3.1   Feature Representation 
In CSG, linguistic entities are modeled as feature structures which give patterns addi-
tional power for describing gender, number, semantic, attributes and number of the 
arguments required by a verb, and so on. These information are encoded in the com-
monly used attribute value matrices (AVMs), attached to each of the lexical and syn-
tactic symbols in CSG. This allows us to specify such as syntactic dependencies as 
agreement and sub-categorization in patterns. Unlike other unification-based gram-
mars [11,14], we do not carry out the unification in full, only interested conditions 
that are explicitly expressed in the rule constraints are tested and unified. Such unifi-
cation process can perform in constant time. The use of feature constraints has to be 
restricted to maintain the efficiency of parsing and generating algorithms, especially 
to the prevention from generating a large number of ambiguous structure candidates. 
The word selection in the target language can also be achieved by checking features. 
In the parsing and generating algorithm, the features information are propagated to the 
reduced symbol from the designated head element in pattern, hence to realize the 
mechanism of features inheritance. Features can either be put in lexical dictionary 
isolated from the formalism to make the work simpler to the construction of analytical 
grammar, or explicitly encoded in the pre-terminal rules as:  
                                                          
4
  If there is no any constraint associated to a target rule, during the parsing phase, the reduction 
of the source rule is assumed to be valid all the time.  
618 F. Wong et al 
Pro ? Jos?:[CAT:pro;NUM:sg;GEN:masc,SEM:hum] {[?? ; ?]} (18) 
n ? livro:[CAT:n;NUM:sg;GEN:masc;SEM:artifact+book] {[? ; ?]} (19) 
Where the features set is being bracketed, and separated by a semi-colon, the name 
and the value of a feature are delimited by a colon to represent the feature pair. An-
other way to enhance the CSG formalism is to apply the soft preferences other than 
hard constraints in the process of features unification. Our consideration is two-fold: 
first, we found that more than one combination of feature values engaged to a single 
lexical item is very common in the process of natural language, i.e. one word may 
have several translations according to the different senses and the pragmatic uses of 
the word, and this has been the problem of word senses disambiguation [15]. Sec-
ondly, the conventional feature unification method can only tell us if the process suc-
cesses or not. In case of a minor part of conditions get failed during the unification, all 
the related candidates are rejected without any flexibility to choosing the next prefer-
able or probable candidate. In order to resolve these problems, each feature structure 
is associates with a weight. It is then possible to rank the matching features according 
to the linear ordering of the weights rather than the order of lexical items expressed in 
grammars or dictionary. In our prototyping system, each symbol has its original 
weight, and according to preference measurement at the time in checking the feature 
constraints, a penalty is used to reduce from the weight to give the effective weight of 
associated features in a particular context. Features with the largest weight are to be 
chosen as the most preferable content. 
4   Application to Portuguese-Chinese MT 
CSG formalism can be parsed by any known CFG parsing algorithm including the 
Earley [16] and generalized LR algorithms [17] augmented by taking into account the 
features constraints and the inference of target structure. In the prototyping system, 
the parsing algorithm for our formalism is based on the generalized LR algorithm that 
we have development for MT system, since the method uses a parse table, it achieves 
a considerable efficiency over the Earley?s non-complied method which has to com-
pute a set of LR items at each stage of parsing [17]. Generalized LR algorithm was 
first introduced by Tomita for parsing the augmented Context-Free grammar that can 
ingeniously handle non-determinism and ambiguity through the use of graph-
structured stack while retaining much of the advantages of standard LR parsing5. It 
takes a shift-reduce approach using an extended LR parse table to guide its actions by 
allowing the multiple actions entries such as shift/reduce and reduce/reduce hence to 
handle the nondeterministic parse with pseudo-parallelism. In order to adapt to our 
formalism, we further extend the parse table by engaging with the features constraints 
and the target rules into the actions table. Our strategy is thus to parse the source rules 
of CSG productions through the normal shift actions proposed by the parsing table, 
while at the time reduce action to be fired, the associated conditions are checked to 
determine if the active reduction is a valid action or not depending on if the working 
symbols of patterns fulfill the constraints on features.  
                                                          
5
  Especially when the grammar is close to the LR grammars. 
 Machine Translation Based on Constraint-Based Synchronous Grammar 619 
4.1   The CSG Parse Table 
Fig. 1 shows an extended LR(1) parsing table for Productions (5)-(17)6 as constructed 
using the LR table construction method described in [18] extended to consider the 
rule components of productions by associating the corresponding target rules with 
constraints, which are explicitly expressed in table. The parsing table consists of two 
parts: a compact ACTION-GOTO table 7  and CSONTRAINT-RULE table. The 
ACTION-GOTO table s indexed by a state symbol s (row) and a symbols x ?VN?VT, 
including the end marker ???. The entry ACTION[s, x] can be one of the following: s 
n, r m, acc or blank. s n denotes a shift action representing GOTO[s, x]=n, defining 
the next state the parser should go to; r m means a reduction by the mth production 
located in the entry of CONSTRAINT-RULE in state s, and acc denotes the accept 
action and blank indicates a parsing error. The CONSTRAINT-RULE table is in-
dexed by state symbol s (row) and the number of productions m that may be applied 
for reduction in state s. The entry CONSTRAINT-RULE[s, m] consists of a set of 
involved productions together with the target rules and features constraints that are 
used for validating if the active parsing node can be reduced or not, then try to iden-
tify the corresponding target generative rule for reduced production. 
4.2   The CSG Parser  
In the parsing process, the algorithm operates by maintaining a number of parsing 
processes in parallel, each of which represents an individual parsed result, hence to 
handle the case of non-deterministic. In general, there are two major components in 
the process, shift(i) and reduce(i), which are called at each position i=0, 1, ?, n in 
an input string I = x1x2?xn. The shift(i) process with top of stack vertex v shifts on xi 
from its current state s to some successor state s? by creating a new leaf v?; estab-
lishing edge from v? to the top of stack v; and making v? as the new top of  
stack vertex.  
The reduce(i) executes a reduce action on a production p by following the chain 
ofparent links down from the top of stack vertex v to the ancestor vertex from which 
the process began scanning for p earlier, then popping intervening vertices off the 
stack. Now, for every reduction action in reduce(i), there exists a set C of ordered 
constraints, c1p ?p cm, with the production, each of which is associated with a target 
rule that may be the probable corresponding target structure for the production, de-
pending on whether the paired constraint gets satisfied or not according to the features 
of the parsed string p. Before reduction takes place, the constraints cj (1 ? j ? m) are 
tested in order started from the most specific one, the evaluation process stops once a 
positive result is obtained from evaluation. The corresponding target rule for the 
parsed string is determined and attached to the reduced syntactic symbol, which will 
be used for rewriting the target translation in phase of generation. At the mean while, 
the features information will be inherited from the designated head element of pro-
duction. The parsing algorithm for CSG formalism is given in Fig. 2. 
 
                                                          
6
  For simplicity, the productions used for building the parse table are deterministic, so no con-
flict actions such as shift/reduce and reduce/reduce appear in the parse table in Fig.1. 
7
  Original version introduced in [17] maintains two tables, ACTION and GOTO. 
620 F. Wong et al 
ACTIONs/GOTOs 
Ste 
p r
o  
n
u
m
 
n
 
v  d e
t 
p NP
 
VP
 
PP
 
S ? o a um
 
el
e 
Jo
s?
 
liv
ro
 
de
u 
co
m
pr
o
u
 Reduced Rules 
Constraints/Target 
Rules 
0 s8 s9 s10  s11  s7   s6  s5  s2 s1 s4 s3    
1               r1     (1) pro ? ele   {[? ; ?]} 
2              r1      (1) num ? um 
3                 r1   (1) n ? livro   {[? ; ?]} 
4                r1    (1) n ? Jos?   {[?? ; ?]} 
5            r1        (1) det ? o 
6           acc          
7    s14    s15          s12 s13  
8 r1                   (1) NP ? pro 
9 s8 s9 s10  s11  s16     s5  s2 s1 s4 s3    
10   r1                 (1) NP ? n 
11 s8 s9 s10  s11  s17     s5  s2 s1 s4 s3    
12                  r1  (1) v ? deu   {[?? ; ?]} 
13                   r1 (1) v ? comprou {[?, ?? ;?]} 
14    r1                (1) VP ? v 
15 s8 s9 s10  s11  s18     s5  s2 s1 s4 s3    
16       r1             (1) NP ? num NP* {[num ? NP; NPsem=SEM_book]} 
17       r1             (1) NP ? det NP*   {[NP ; ?]} 
18      s21   s20    s19        
19             r1       (1) p ? a 
20 s8 s9 s10  s11  s22     s5  s2 s1 s4 s3    
21      r1              (1) PP ? p 
22          r1          (1) S ? NP1 VP* NP2 PP NP3 {[...]} 
 
Fig. 1. Extended LR(1) parse table 
PARSE(grammar,x1 ? xn) 
x
n+1? ? 
Ui?? (0 ? i ? n) 
U0?v0 
for each terminal symbol xi (1 ? i ? n) 
P?? 
for each node v ? Ui-1 
P?P?v 
if ACTION[STATE(v),xi] = ?shift s??, SHIFT(v,s?) 
for each ?reduce p??ACTION[STATE(v),xi], REDUCE(v,p) 
if ?acc??ACTION[STATE(v),xi], accept 
if Ui=?, reject 
 
SHIFT(v,s) 
if v??Ui s.t. STATE(v?)=s and ANCESTOR(v?,1)=v and state 
transition ?(v,x)=v? 
do nothing 
 
 Machine Translation Based on Constraint-Based Synchronous Grammar 621 
else 
create a new node v? 
s.t. STATE(v?)=s and ANCESTOR(v?,1)=v and state tran-
sition ?(v,x)=v? 
Ui?Ui?v? 
 
REDUCE(v,p) 
for each possible reduced parent v1??ANCESTOR(v,RHS(p)) 
if UNIFY(v,p)=?success? 
s? ? GOTO(v1?,LHS(p)) 
if node v??Ui-1 s.t. STATE(v?)=s? 
if ?(v1?, LHS(p))=v? 
do nothing 
else 
if node v2??ANCESTOR(v?,1) 
let v
c
? s.t. ANCESTOR(v
c
?,1)=v1? and STATE(vc?)=s? 
for each ?reduce p? ? ACTION[STATE(v
c
?),xi] 
REDUCE(v
c
?,p) 
else 
if v??P 
let v
c
? st. ANCESTOR(v
c
?,1)=v1? and STATE(vc?)=s? 
for each ?reduce p? ? ACTION[STATE(v
c
?),xi] 
REDUCE(v
c
?,p) 
else 
create a new node v
n
 
s.t. STATE(v
n
)=s? and ANCESTOR(v
n
,1)=v1? and  
state transition ?(v
n
,x)=v1? 
Ui-1?Ui-1?vn 
else current reduction failed 
 
UNIFY(v,p) 
for ?constraint cj? ? CONSTRAINT(STATE(v)) (1 ? j ? m, 
c1p ?p cm) 
if ?(cj,p)=?true?  (?(?,p)=?true?) 
TARGET(v)?j 
return ?success? 
Fig. 2. Modified generalized LR Parsing algorithm 
The parser is a function of two arguments PARSE(grammar, x1 ? xn), where the 
grammar is provided in form of parsing table. It calls upon the functions SHIFT(v, s) 
and REDUCE(v, p) to process the shifting and rule reduction as described. The 
UNIFY(v, p) function is called for every possible reduction in REDUCE(v, p) to ver-
ify the legal reduction and select the target rule for the source structure for synchroni-
zation. The function TARGET(v) after unification passed is to dedicate the jth target 
rule as correspondence. 
622 F. Wong et al 
4.3   Translation as Parsing 
Our Portuguese-to-Chinese translation (PCT) system is a transfer-based translation 
system by using the formalism of Constraint-Based Synchronous Grammar (CSG) as its 
analytical grammar. Unlike other transfer-based MT systems that the major compo-
nents: analysis, transfer and generation are carried out individually in pipeline by using 
different sets of representation rules to achieve the tasks of structure analysis and trans-
formation [19], in PCT, only a single set of CS grammar is used to dominate the transla-
tion task. Since the structures of parallel languages are synchronized in formalism, as 
well as the deviations of their structures are also captured and described by the gram-
mar. Hence, to the translation of an input text, it essentially consists of three steps. First, 
for an input sentence s, the structure of string is analyzed by using the rules of source 
components from the CSG productions; by using the augmented generalized LR parsing 
algorithm as described. Secondly, the link constraints that are determined during the rule 
reduction process are propagated to the corresponding target rules R (as selection of 
target rules) to construct a target derivation sequence Q. And finally, based on the deri-
vation sequence Q, translation of the input sentence s is generated by referencing the set 
of generative rules R that attached to the corresponding constituent nodes in the parsed 
tree, hence to realize the translation in target language. 
5   Conclusion 
In this paper, we have proposed a variation of synchronous grammar based on the 
syntax of context-free grammar, called Constraint-based Synchronous Grammar 
(CSG). The source components of CSG are being generalized for representing the 
common structure of language. Different from other synchronous grammars, each 
source rule is associated with a set of target productions, where each of the target 
rules is connected with a constraint over the features of source patterns. The set of 
target rules are grouped and maintained in a vector ordered by the specificity of con-
straints. The objective of this formalism is to allow parsing and generating algorithms 
to inference different possible translation equivalences for an input sentence being 
analyzed according to the linguistic features. We have presented a modified general-
ized LR parsing algorithm that has been adapted to the parsing our formalism that we 
have developed for analyzing the syntactic structure of Portuguese in the machine 
translation system.  
References 
1. Rambow, O., Satta, G.: Synchronous Models of Language. In Proceedings of 34th Annual 
Meeting of the Association for Computational Linguistics, University of California, Santa 
Cruz, California, USA, Morgan Kaufmann (1996) 116-123. 
2. Lewis, P.M., Stearns, R.E.: Syntax-directed transduction. Journal of the Association for 
Computing Machinery, 15(3), (1968) 465-488. 
3. Shieber, S.M., Schabes, Y.: Synchronous Tree Adjoining Grammar. Proceedings of the 
13th International Conference on Computational Linguistic, Helsinki (1990) 
4. Seki, H., Matsumura, T., Fujii, M., Kasami, T.: On multiple context-free grammars. Theo-
retical Computer Science, 88(2) (1991) 191-229 
 Machine Translation Based on Constraint-Based Synchronous Grammar 623 
5. Melamed, I.D.: Multitext Grammars and Synchronous Parsers. In Proceedings of 
NAACL/HLT 2003, Edmonton, (2003) 79-86 
6. Wong, F., Hu, D.C., Mao, Y.H., Dong, M.C. A Flexible Example Annotation Schema: 
Translation Corresponding Tree Representation. In Proceedings of the 20th International 
Conference on Computational Linguistics, Switzerland, Geneva (2004) 1079-1085 
7. Wu, D.: Grammarless extraction of phrasal translation examples from parallel texts. In 
Proceedings of TMI-95, Sixth International Conference on Theoretical and Methodologi-
cal Issues in Machine Translation, v2, Leuven Belgium (1995) 354-372 
8. Aho, A.V., Ullman, J.D.: Syntax Directed Translations and the Pushdown Assembler. 
Journal of Computer and System Sciences, 3, (1969) 37-56 
9. Melamed, I.D., Satta. G., Wellington, B.: Generalized Multitext Grammars. In Proceed-
ings of 42th Annual Meeting of the Association for Computational Linguistics, Barcelona, 
Spain (2004) 661-668 
10. Kaplan, R.M., Bresnan, J.: Lexical-Functional Grammar: A Formal System for Grammati-
cal Representation. In Joan Bresnan, The Mental Representation of Grammatical Rela-
tions, Cambridge, Mass, MIT Press, (1982) 173-281 
11. Kaplan, R.M.: The Formal Architecture of Lexical-Functional Grammar. Information Sci-
ence and Engineering, 5, (1989) 30-322 
12. Wong, F., Mao, Y.H.: Framework of Electronic Dictionary System for Chinese and Ro-
mance Languages. Automatique des Langues (TAL), 44(2), (2003) 225-245 
13. Wong, F., Mao, Y.H., Dong, Q.F., Qi, Y.H.: Automatic Translation: Overcome the Barri-
ers between European and Chinese Languages. In Proceedings (CD Version) of First In-
ternational UNL Open Conference, SuZhou China (2001) 
14. Pollard, C., Sag, I.: Head-Driven Phrase Structure Grammar. University of Chicago Press, 
(1994) 
15. Ide, N., Veronis, J.: Word Sense Disambiguation: The State of the Art. Computational 
Linguistics, 24, (1), (1998) 1-41 
16. Earley, J.: An Efficient Context-Free Parsing Algorithm. CACM, 13(2), (1970) 94-102 
17. Tomita, M.: Computational Linguistics, 13(1-2), (1987) 31-46 
18. Aho, A.V., Sethi, R., Ullman, J.D.: Compiler: Principles, Techniques and Tools. Addison-
Wesley, (1986) 
19. Hutchins, W.J., Somers, H.L.: An Introduction to Machine Translation. Academic  
Press, (1992) 
Chinese Tagging Based on Maximum Entropy Model 
Ka Seng Leong 
Faculty of Science and Technology of 
University of Macau 
Av. Padre Tom?s Pereira, Taipa,  
Macau, China 
ma56538@umac.mo 
Fai Wong 
Faculty of Science and Technology of 
University of Macau, INESC Macau 
Av. Padre Tom?s Pereira, Taipa,  
Macau, China 
derekfw@umac.mo 
Yiping Li 
Faculty of Science and Technology of 
University of Macau 
Av. Padre Tom?s Pereira, Taipa,  
Macau, China 
ypli@umac.mo 
Ming Chui Dong 
Faculty of Science and Technology of 
University of Macau, INESC Macau 
Av. Padre Tom?s Pereira, Taipa,  
Macau, China 
dmc@inesc-macau.org.mo 
 
 
Abstract 
In the Fourth SIGHAN Bakeoff, we took 
part in the closed tracks of the word 
segmentation, part of speech (POS)  
tagging and named entity recognition (NER) 
tasks. Particularly, we evaluated our word 
segmentation model on all the corpora, 
namely Academia Sinica (CKIP), City 
University of Hong Kong (CITYU), 
University of Colorado (CTB), State 
Language Commission of P.R.C. (NCC) 
and Shanxi University (SXU). For POS 
tagging and NER tasks, our models were 
evaluated on CITYU corpus only. Our 
models for the evaulation are based on the 
maximum entropy approach, we 
concentrated on the word segmentation 
task for the bakeoff and our best official 
results on all the corpora for this task are 
0.9083 F-score on CITYU, 0.8985 on 
CKIP, 0.9077 on CTB, 0.8995 on NCC and 
0.9146 on SXU. 
1 Introduction 
In the Fourth SIGHAN Bakeoff, besides providing 
the evaluation tasks for the word segmentation and 
NER, it also introduced another important evalua-
tion task, POS tagging for Chinese language. In 
this bakeoff, our models built for the tasks are sim-
ilar to that in the work of Ng and Low (2004). The 
models are based on a maximum entropy frame-
work (Ratnaparkhi, 1996; Xue and Shen, 2003). 
They are trained on the corpora for the tasks from 
the bakeoff. To understand the model, the imple-
mentation of the models is wholly done ourselves. 
We used Visual Studio .NET 2003 and C++ as the 
implementation language. The Improved Iterative 
Scaling (IIS) (Pietra et al, 1997) is used as the pa-
rameter estimation algorithm for the models. We 
tried all the closed track tests of the word segmen-
tation, the CITYU closed track tests for POS tag-
ging and NER. 
2 Maximum Entropy 
In this bakeoff, our basic model is based on the 
framework described in the work of Ratnaparkhi 
(1996) which was applied for English POS tagging. 
The conditional probability model of the 
framework is called maximum entropy (Jaynes, 
1957). Maximum entropy model is a feature-based, 
probability model which can include arbitrary 
number of features that other generative models 
like N-gram model, hidden Markov model (HMM) 
(Rabiner, 1989) cannot do. The probability model 
can be defined over X ? Y, where X is the set of 
138
Sixth SIGHAN Workshop on Chinese Language Processing
possible histories and Y is the set of allowable 
futures or classes. The conditional probability of 
the model of a history x and a class y is defined as 
 
 ( , )
( | ) ( )
if x y
iip y x Z x? ?
?
??   (1) 
 
 
( , )( ) if x yi
y i
Z x? ????
  (2) 
 
where ? is a parameter which acts as a weight for 
the feature in the particular history. The equation 
(1) states that the conditional probability of the 
class given the history is the product of the weight-
ings of all features which are active under the con-
sideration of (x, y) pair, normalized over the sum 
of the products of all the classes. The normaliza-
tion constant is determined by the requirement that 
( | ) 1
y
p y x? ??
 for all x. 
To find the optimized parameters ? of the condi-
tional probability is one of the important processes 
in building the model. This can be done through a 
training process. The parameter estimation algo-
rithm used for training is Improved Iterative Scal-
ing (IIS) (Pietra et al, 1997) in our case. In train-
ing the models for this bakeoff, the training data is 
given in the form of a sequence of characters (for 
the tasks of word segmentation and NER) or words 
(POS tagging) and their classes (tags), the parame-
ters ? can be chosen to maximize the likelihood of 
the training data using p: 
 
( , )
1 1 1
1( ) ( , ) ( )
j i if x yn n m
i i j
i i j
L p p x y Z x? ?
?
? ? ?
? ?? ? ?
(3) 
 
But of course, the success of the model depends 
heavily on the selection of features for a particular 
task. This will be described in Section 5. 
3 Chinese Word Segmenter 
We concentrated on the word segmentation task in 
this bakeoff. For the Chinese word segmenter, it is 
based on the work that treats Chinese word seg-
mentation as tagging (Xue and Shen, 2003; Ng and 
Low, 2004). Given a Chinese sentence, it assigns a 
so-called boundary tag to each Chinese character 
in the sentence. There are four possible boundary 
tags: S for a character which is a single-character 
word, B for a character that is the first character of 
a multi-character word, E for a character that is the 
last character of a multi-character word and M for 
a character that is neither the first nor last of a mul-
ti-character word. With these boundary tags, the 
word segmentation becomes a tagging problem 
where each character in Chinese sentences is given 
one of the boundary tags which is the most proba-
ble one according to the conditional probability 
calculated by the model. And then sequences of 
characters are converted into sequences of words 
according to the tags. 
4 POS Tagger and Named Entity Recog-
nizer 
For the POS tagging task, the tagger is built based 
on the work of Ratnaparkhi (1996) which was ap-
plied for English POS tagging. Because of the time 
limitation, we could only try to port our imple-
mented maximum entropy model to this POS tag-
ging task by using the similar feature set (discussed 
in Section 5) for a word-based POS tagger as in the 
work of Ng and Low (2004). By the way, besides 
porting the model to the POS tagging task, it was 
even tried in the NER task by using the same fea-
ture set (discussed in Section 5) as used for the 
word segmentation in order to test the performance 
of the implemented model. 
The tagging algorithm for these two tasks is bas-
ically the same as used in word segmentation. Giv-
en a word or a character, the model will try to as-
sign the most probable POS or NE tag for the word 
or character respectively. 
5 Features 
To achieve a successful model for any task by us-
ing the maximum entropy model, an important step 
is to select a set of useful features for the task. In 
the following, the feature sets used in the tasks of 
the bakeoff are discussed. 
5.1 Word Segmentation Features 
The feature set used in this task is discussed in our 
previous work (Leong et al, 2007) which is cur-
rently the best in our implemented model. They are 
the unigram features: C-2, C-1, C0, C1 and C2, bi-
gram features: C-2C-1, C-1C0, C0C1, C1C2 and C-1C1 
where C0 is the current character, Cn (C-n) is the 
139
Sixth SIGHAN Workshop on Chinese Language Processing
character at the nth position to the right (left) of the 
current character. For example, given the character 
sequence ??????? (Victoria Harbour), while 
taking the character ??? as C0, then C-2 = ???, C-
1C1 = ????, etc. The boundary tag (S, B, M or E) 
feature T-1 is also applied, i.e., the boundary tag 
assigned to the previous character of C0. And the 
last feature WC0: This feature captures the word 
context in which the current character is found. It 
has the format ?W_C0?. For example, the character 
??? is a character of the word ???????. 
Then this will give the feature WC0 = ??????
_??. 
5.2 POS Tagging Features 
For this task, because of the time limitation as 
mentioned in the previous section, we could only 
port our implemented model by using a part of the 
feature set which was used in the word-based tag-
ger discussed in the work of Ng and Low (2004). 
The feature set includes: Wn (n = -2 to 2), WnWn+1 
(n = -2, -1, 0, 1), W-1W1, POS(W-2), POS(W-1), 
POS(W-2)POS(W-1) where W refers to a word, POS 
refers to the POS assigned to the word and n refers 
to the position of the current word being consi-
dered. For example, while considering this sen-
tence taken from the POS tagged corpus of CITYU: 
???/Ng  ??/Ac  ???/Nc  ??/Dc  ??
/Vt? (Hong Kong S.A.R. is established), taking ??
??? as W0, then W-2 = ????, W-1W1 = ??? ?
??, POS(W-2) = ?Ng?, POS(W-2)POS(W-1) = ?Ac 
Dc?, etc. 
5.3 Named Entity Recognition Features 
For the NER task, we directly used the same fea-
ture set as for the word segmentation basically. 
However, because the original NE tagged corpus is 
presented in two-column format, where the first 
column consists of the character and the second is 
a tag, a transformation which is to transform the 
original corpus to a sentence per line format before 
collecting the features or other training data is 
needed. This transformation actually continues to 
read the lines from the original corpus, whenever a 
blank line is found, a sentence of characters with 
NE tags can be formed. 
After that, the features collected are the unigram 
features: C-2, C-1, C0, C1 and C2, bigram features: 
C-2C-1, C-1C0, C0C1, C1C2 and C-1C1, NE tag fea-
tures: T-1, WC0 (this feature captures the NE con-
text in which the current character is found) where 
T-1 refers to the NE tag assigned to the previous 
character of C0, W refers to the named entity. So 
similar to the explanation of features of word seg-
mentation, for example, given the sequence from 
the NER tagged corpus of CITYU:  ??/N ?/N ?
/B-LOC ? /I-LOC ? /N? (One Chinese), while 
taking the character ??? as C0, then C-2 = ???, C-
1C1 = ????, WC0 = ??????, etc. 
For all the experiments conducted, training was 
done with a feature cutoff of 1. 
6 Testing 
For word segmentation task, during testing, given a 
character sequence C1 ? Cn, the trained model will 
try to assign a boundary tag to each character in the 
sequence based on the probability of the boundary 
tag calculated. Then the sequence of characters is 
converted into sequence of words according to the 
tag sequence t1 ? tn. But if each character was just 
assigned the boundary tag with the highest proba-
bility, invalid boundary tag sequences would be 
produced and wrong word segmentation results 
would be obtained. In particular, known words that 
are in the dictionary of the training corpus are 
segmented wrongly because of these invalid tag 
sequences. In order to correct these, the invalid 
boundary tag sequences are collected, such as for 
two-character words, they are ?B B?, ?B S?, ?M S?, 
?E E?, etc., for three-character words, they are ?B 
E S?, ?B M S?, etc., and for four-character words, 
they are ?B M M S?, ?S M M E?, etc. With these 
invalid boundary tag sequences, some post correc-
tion to the word segmentation result can be tried. 
That is after the model tagger has done the tagging 
for a Chinese sentence every time, the invalid 
boundary tag sequences will be searched within the 
preliminary result given by the tagger. When the 
invalid boundary tag sequence is found, the charac-
ters corresponding to that invalid boundary tag se-
quence will be obtained. After, the word formed by 
these characters is looked up to see if it is indeed a 
word in the dictionary, if it is, then the correction is 
carried out. 
Another kind of post correction to the word 
segmentation result is to make some guessed cor-
rection for some invalid boundary tag sequences 
such as ?B S?, ?S E?, ?B B?, ?E E?, ?B M S?, etc. 
That is, whenever those tag sequences are met 
140
Sixth SIGHAN Workshop on Chinese Language Processing
within the preliminary result given by the model 
tagger, they will be corrected no matter if there is 
word in the dictionary formed by the characters 
corresponding to the invalid boundary tag se-
quence. 
We believe that similar post correction can be 
applied to the NER task. For example, if such NE 
tag sequences ?B-PER N?, ?N I-PER N?, etc. oc-
cur in the result, then the characters corresponding 
to the invalid NE tag sequence can be obtained 
again and looked up in the named entity dictionary 
to see if they really form a named entity. However, 
we did not have enough time to adapt this for the 
NER task finally. Therefore, no such post correc-
tion was applied for the NER task in this bakeoff 
finally. 
7 Evaluation Results 
We evaluated our models in the closed tracks of 
the word segmentation, part of speech (POS)  
tagging and named entity recognition (NER) tasks. 
Particularly, our word segmentation model was 
evaluated on all the corpora, namely Academia 
Sinica (CKIP), City University of Hong Kong 
(CITYU), University of Colorado (CTB), State 
Language Commission of P.R.C. (NCC) and 
Shanxi University (SXU). For POS tagging and 
NER tasks, our models were evaluated on the 
CITYU corpus only. Table 1 shows our official 
results for the word segmentation task in the 
bakeoff. The columns R, P and F show the recall, 
precision and F-score respectively. 
 
Run_ID R P F 
cityu_a 0.9221 0.8947 0.9082 
cityu_b 0.9219 0.8951 0.9083 
ckip_a 0.9076 0.8896 0.8985 
ckip_b 0.9074 0.8897 0.8985 
ctb_a 0.9078 0.9073 0.9075 
ctb_b 0.9077 0.9078 0.9077 
ncc_a 0.8997 0.8992 0.8995 
ncc_b 0.8995 0.8992 0.8994 
sxu_a 0.9186 0.9106 0.9145 
sxu_b 0.9185 0.9107 0.9146 
Table 1. Official Results in the Closed Tracks of 
the Word Segmentation Task on all Corpora 
 
We submitted a few runs for each of the tests of 
the corpora. Table 1 shows the best two runs for 
each of the tests of the corpora for discussion here. 
The run (a) applied only the post correction to the 
known words that are in the dictionary of the train-
ing corpus but are segmented wrongly because of 
the invalid boundary tag sequences. The run (b) 
applied also the guessed post correction for some 
invalid boundary tag sequences in the results as 
mentioned in Section 6. From the results above, it 
can be seen that the runs with the guessed post cor-
rection generally gave a little bit better perfor-
mance than those that did not apply. This shows 
that the guess somehow made some good guesses 
for some unknown words that appear in the testing 
corpora. 
Table 2 shows our official results for the POS 
tagging task. The columns A shows the accuracy. 
The columns IV-R, OOV-R and MT-R show the 
recall on in-vocabulary words, out-of-vocabulary 
words and multi-POS words (multi-POS words are 
the words in the training corpus and have more 
than one POS-tag in either the training corpus or 
testing corpus) respectively. The run (a) used the 
paramters set which was observed to be the 
optimal ones for the model in the training phase. 
The run (b) used the parameters set of the model in 
the last iteration of the training phase. 
 
Run_ID A IV-R OOV-R MT-R 
cityu_a 0.1890 0.2031 0.0550 0.1704 
cityu_b 0.2793 0.2969 0.1051 0.2538 
Table 2. Official Results in the Closed Track of the 
POS Tagging Task on the CITYU Corpus 
 
It can be seen that our results were unexpectedly 
low in accuracy. After releasing the results, we 
found that the problem was due to the encoding 
problem of our submitted result files. The problem 
probably occurred after the conversion from our 
Big5 encoded results to the UTF-16 encoded 
results which are required by the bakeoff. 
Therefore, we did the evaluation ourselves by 
running our POS tagger again, using the official 
evaluation program and the truth test set. Finally, 
our best result was 0.7436 in terms of accuracy but 
this was still far lower than the baseline (0.8425) of 
the CITYU corpus. This shows that the direct 
porting of English word-based POS tagging to 
Chinese is not effective. 
Table 3 shows our official results for the NER 
task. The columns R, P and F show the recall, 
precision and F-score respectively. Again, similar 
to the POS tagging task, the run (a) used the 
141
Sixth SIGHAN Workshop on Chinese Language Processing
paramters set which was observed to be the 
optimal ones for the model in the training phase. 
The run (b) used the parameters set of the model in 
the last iteration of the training phase. 
 
Run_ID R P F 
cityu_a 0.0874 0.1058 0.0957 
cityu_b 0.0211 0.0326 0.0256 
Table 3. Official Results in the Closed Track of the 
NER Task on the CITYU Corpus 
 
It can be seen that our results were again 
unexpectedly low in accuracy. The cause of such 
low accuracy results was due to parts of the wrong 
format of the submitted result files compared with 
the correct format of the result file. So like the 
POS tagging task, we did the evaluation ourselves 
by running our NE recognizer again. Finally, our 
best result was 0.5198 in terms of F-score but this 
was again far lower than the baseline (0.5955) of 
the CITYU corpus. This shows that the similar 
feature set for the word segmentation task is not 
effective for the NER task. 
8 Conclusion 
This paper reports the use of maximum entropy 
approach for implementing models for the three 
tasks in the Fourth SIGHAN Bakeoff and our re-
sults in the bakeoff. From the results, we got good 
experience and knew the weaknesses of our mod-
els. These help to improve the performance of our 
models in the future. 
Acknowledgements 
The research work reported in this paper was par-
tially supported by ?Fundo para o 
Desenvolvimento das Ci?ncias e da Tecnologia? 
under grant 041/2005/A. 
References 
Adwait Ratnaparkhi. 1996. A maximum entropy model 
for part-of-speech tagging, in Proceedings of Confe-
rence on Empirical Methods in Natural Language 
Processing, Philadelphia, USA, pages 133-142. 
Edwin Thompson Jaynes. 1957. Information Theory and 
Statistical Mechanics, The Physical Review, 106(4): 
620-630. 
Hwee Tou Ng, and Jin Kiat Low. 2004. Chinese part-of-
speech tagging: One-at-a-time or all-at-once? Word-
based or character-based? In Proceedings of the 2004 
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2004), Barcelona, Spain, 
pages 277-284. 
Ka Seng Leong, Fai Wong, Yiping Li, and Ming Chui 
Dong. 2007. Chinese word boundaries detection 
based on maximum entropy model, in Proceedings of 
the 11th International Conference on Enhancement 
and Promotion of Computational Methods in Engi-
neering and Science (EPMESC-XI), Kyoto, Japan. 
Lawrence Rabiner. 1989. A tutorial on hidden Markov 
models and selected applications in speech recogni-
tion, Proceedings of the IEEE, 77(2): 257?286. 
Nianwen Xue, and Libin Shen. 2003. Chinese word 
segmentation as LMR tagging, in Proceedings of the 
2nd SIGHAN Workshop on Chinese Language 
Processing, Sapporo, Japan, pages 176-179.  
Steven Della Pietra, Vincent Della Pietra, and John Laf-
ferty. 1997. Inducing features of random fields. IEEE 
transactions on pattern analysis and machine intelli-
gence, 19(4): 380?393. 
 
142
Sixth SIGHAN Workshop on Chinese Language Processing
