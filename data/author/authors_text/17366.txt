Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 860?865,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
 
 
Detecting Turnarounds in Sentiment Analysis: Thwarting 
 
  
  
Abstract 
Thwarting and sarcasm are two uncharted 
territories in sentiment analysis, the for-
mer because of the lack of training corpo-
ra and the latter because of the enormous 
amount of world knowledge it demands. 
In this paper, we propose a working defi-
nition of thwarting amenable to machine 
learning and create a system that detects if 
the document is thwarted or not. We focus 
on identifying thwarting in product re-
views, especially in the camera domain. 
An ontology of the camera domain is cre-
ated. Thwarting is looked upon as the 
phenomenon of polarity reversal at a 
higher level of ontology compared to the 
polarity expressed at the lower level.   
This notion of thwarting defined with re-
spect to an ontology is novel, to the best 
of our knowledge. A rule based imple-
mentation building upon this idea forms 
our baseline. We show that machine learn-
ing with annotated corpora (thwarted/non-
thwarted) is more effective than the rule 
based system. Because of the skewed dis-
tribution of thwarting, we adopt the Area-
under-the-Curve measure of performance. 
To the best of our knowledge, this is the 
first attempt at the difficult problem of 
thwarting detection, which we hope will at 
least provide a baseline system to compare 
against. 
1 Credits 
The authors thank the lexicographers at Center 
for Indian Language Technology (CFILT) at IIT 
Bombay for their support for this work. 
2 Introduction 
Although much research has been done in the 
field of sentiment analysis (Liu et al, 2012), 
thwarting and sarcasm are not addressed, to the 
best of our knowledge. Thwarting has been iden-
tified as a common phenomenon in sentiment 
analysis (Pang et al, 2002, Ohana et al, 2009, 
Brooke, 2009) in various forms of texts but no 
previous work has proposed a solution to the 
problem of identifying thwarting. We focus on 
identifying thwarting in product reviews. 
The definition of an opinion as specified in 
Liu (2012) is  
?An opinion is a quintuple, (   ,     ,      , 
  ,   ), where    is the name of an entity,     is 
an aspect of   ,       is the sentiment on aspect 
    of entity   ,    is the opinion holder, and     
is the time when the opinion is expressed by   .? 
 
If the sentiment towards the entity or one of its 
important attribute contradicts the sentiment to-
wards all other attributes, we can say that the 
document is thwarted. 
Ankit Ramteke 
Dept. of Computer Science & Engg., 
Indian Institute of Technology  
Bombay, Mumbai, India. 
ankitr@cse.iitb.ac.in 
 
Pushpak Bhattacharyya 
Dept. of Computer Science & Engg., 
Indian Institute of Technology  
Bombay, Mumbai, India. 
pb@cse.iitb.ac.in 
 
Akshat Malu 
Dept. of Computer Science & Engg., 
Indian Institute of Technology  
Bombay, Mumbai, India. 
akshatmalu@cse.iitb.ac.in 
 
J. Saketha Nath 
Dept. of Computer Science & Engg., 
Indian Institute of Technology  
Bombay, Mumbai, India. 
saketh@cse.iitb.ac.in 
 
860
 
 
A domain ontology is an ontology of various 
features pertaining to a domain, arranged in a 
hierarchy. Subsumption in this hierarchy implies 
that the child is a part or feature of the parent. 
Domain ontology has been used by various 
works in NLP (Saggion et al, 2007 and Polpinij 
et al, 2008). In our work, we use domain ontol-
ogy of camera. We look upon thwarting as the 
phenomenon of reversal of polarity from the 
lower level of the ontology to the higher level. At 
the higher level of ontology the entities men-
tioned are the whole product or a large critical 
part of the product. So while statements about 
entities at the lower level of the ontology are on 
?details?, statements about entities at higher lev-
els are on the ?big picture?. Polarity reversal 
from details to the big picture is at the heart 
of thwarting. 
The motivation for our study on thwarting 
comes from the fact that: a) Thwarting is a chal-
lenging NLP problem and b) Special ML ma-
chinery is needed in view of the fact that the 
training data is so skewed. Additionally large 
amount of world and domain knowledge maybe 
called for to solve the problem. In spite of the 
relatively fewer occurrence of the thwarting phe-
nomenon the problem poses an intellectually 
stimulating exercise. We may also say that in the 
limit, thwarting approaches the very difficult 
problem of sarcasm detection (Tsur et al 2010). 
We start by defining and understanding the 
problem of thwarting in section 2. In section 3, 
we describe a method to create the domain on-
tology. In section 4, we propose a na?ve rule 
based approach to detect thwarting. In section 5 
we discuss a machine learning based approach 
which could be used to identify whether a docu-
ment is thwarted or not. This is followed by ex-
perimental results in section 6. Section 7 draws 
conclusions and points to future work. 
3 Definition 
Thwarting is defined by Pang et al, (2008) as 
follows:  
?Thwarted expectations basically refer to the 
phenomenon wherein the author of the text first 
builds up certain expectations for the topic, only 
to produce a deliberate contrast to the earlier 
discussion."       
 
For our computational purposes, we define 
thwarting as:  
?The phenomenon wherein the overall polarity of 
the document is in contrast with the polarity of 
majority of the document.? 
 
This definition emphasizes thwarting as piggy-
backing on sentiment analysis to improve the 
latter?s performance. The current work however 
only addresses the problem of whether a docu-
ment is thwarted or not and does not output the 
sentiment of the document. The basic block dia-
gram for our system is shown in figure 1. 
 
 
 
 
 
 
Figure 1: Basic Block Diagram 
 
An example of a thwarted document is: 
?I love the sleek design. The lens is impressive. 
The pictures look good but, somehow this cam-
era disappoints me. I do not recommend it.? 
 
While thwarting occurs in various forms of sen-
timent bearing texts, it is not a very frequent one. 
It accounts for hardly 1-2% of any given corpus. 
Thus, it becomes hard to find sufficient number 
of examples of thwarting to train a classifier.  
Since thwarting is a complex natural language 
phenomenon we require basic NLP tools and 
resources, whose accuracy in turn can affect the 
overall performance of a thwarting detection sys-
tem. 
4 Building domain ontology 
Domain ontology comprises of features and enti-
ties from the domain and the relationships be-
tween them. The process thus has two steps, viz. 
(a) identify the features and entities, and (b) con-
nect them in the form of a hierarchy. We decided 
to use a combination of review corpora mining 
and manual means for identifying key features. 
Our approach to building the domain ontology is 
as follows: 
Step 1: We use Latent Dirichlet Allocation 
(LDA) (Blei et al, 2003) on a corpus containing 
reviews of a particular product (camera, in our 
case) to identify key features from the domain. 
The output is then analyzed manually to finally 
select the key features. Some additional features 
get added by human annotator to increase the 
coverage of the ontology. For Example, in the 
camera domain, the corpus may include words 
Thwarting 
Detection 
System 
Input 
 Document 
Thwarted or 
 Not -Thwarted 
861
 
 
like memory, card, gb, etc. but, may not contain 
the word storage. The abstract concept of stor-
age is contributed by the human annotator 
through his/her world knowledge. 
Step 2: The features thus obtained are ar-
ranged in the form of a hierarchy by a human 
annotator. 
 
 
Figure 2: Ontology for the camera domain 
5 A rule based approach to thwarting 
recognition 
As per the definition of thwarting, most of the 
thwarted document carries a single sentiment; 
however, a small but critical portion of the text, 
carrying the contrary sentiment, actually decides 
the overall polarity. The critical statement, thus, 
should be strongly polar (either positive or nega-
tive), and it should be on some critical feature of 
the product. 
From the perspective of the domain ontology, the 
sentiment towards the overall product or towards 
some critical feature mentioned near the root of 
the ontology should be opposite to the sentiment 
towards features near the leaves. 
 
Based on these observations we propose the fol-
lowing na?ve approach to thwarting detection: 
 
For each sentence in a review to be tested 
   1. Get the dependency parse of the sentence. 
This step is essential. It makes explicit the adjec-
tive noun dependencies, which in turn uncovers 
the sentiment on a specific part or feature of the 
product. 
   2. Identify the polarities towards all nouns, us-
ing the dependency parse and sentiment lexicons.    
   3. If a domain feature, identified using the do-
main ontology, exists in the sentence, anno-
tate/update the ontology node, containing the 
feature, using the polarity obtained. 
Once the entire review is processed, we obtain 
the domain ontology, with polarity marking on 
nodes, for the corresponding review. 
The given review is thwarted if there is a con-
tradiction of sentiment among different levels of 
the domain ontology with polarity marking on 
nodes. 
The sentiment lexicons used are SentiWord-
Net (Esuli et al, 2006), Taboada (Taboada et al, 
2004), BL lexicon (Hu et al, 2004) and Inquirer 
(Stone et al, 1966). 
The procedure is illustrated by an example.  
?I love the sleek design. The lens is impressive. 
The pictures look good but, somehow this cam-
era disappoints me. I do not recommend it.? 
 
A part of the ontology, with polarity marking on 
nodes, for this example is shown in figure 3. 
 
Figure 3: ontology with polarity marking on nodes: 
example 
Based on this ontology we see that there is an 
opposition of sentiment between the root (?cam-
era?) and the lower nodes. We thus determine 
that this document is thwarted. 
However, since the nodes, within the same 
level, might have different weighting based upon 
the product under consideration, this method 
fails to perform well. For example, the body and 
video capability might be subjective whereas any 
fault in the lens or the battery will render the 
camera useless, hence they are more critical. We 
thus see a need for relative weighting among all 
features in the ontology. 
Camera - 
negative 
Lens  - 
positive 
Body 
Design - 
positive 
Display 
Picture - 
positive 
862
 
 
6 A Machine Learning based approach 
Manual fixing of relative weightages for the fea-
tures of the product is possible, but that would be 
ad hoc. We now propose a machine learning 
based approach to detect thwarting in documents. 
It uses the domain ontology to identify key fea-
tures related to the domain. The approach in-
volves two major steps namely learning the 
weights and building a model that classifies the 
reviews using the learnt weights. 
6.1  Learning Weights 
The weights are learnt using the loss-
regularization framework. The key idea is that 
the overall polarity of the document is deter-
mined by the polarities of individual words in the 
document. Since, we need to find the weights for 
the nodes in the domain ontology; we consider 
only the words belonging to the ontology for fur-
ther processing. Thus, if P is the polarity of the 
review and    is the polarity associated with 
word i then   ?        gives the linear model. 
The word i should belong to the ontology as well 
as the review. Similarly, the hinge loss is given 
by               where w is the weight 
vector and x is the feature vector consisting of   
    .  
Based on the intuition, that every word con-
tributes some polarity to its parent node in the 
domain ontology, we also learnt weights on the 
ontology by percolating polarities towards the 
root. We experimented with complete percola-
tion, wherein the polarity at a node is its polarity 
in the document summed with the polarities of 
all its descendants. We also define controlled 
percolation, wherein the value added for a par-
ticular descendant is a function of its distance 
from the node. We halved the polarity value per-
colated, for each edge between the two nodes. 
Thus, for the example in figure 2, the polarity 
value of camera would be 
                  
     
 
 
     
 
  
        
 
  
       
 
  
        
 
 
Where         is the final polarity for camera 
and       is the polarity of the word ? {camera, 
body, display, design, picture}.  
6.2 Classifier 
We use the SVM classifier with features generat-
ed using the following steps. We first create a 
vector of weighted polarity values for each re-
view. This is constructed by generating a value 
for each word in the domain ontology encoun-
tered while reading the review sequentially. The 
value is calculated by multiplying the weight, 
found in the previous step (5.1), with the polarity 
of the word as determined from the sentence. 
Since, these vectors will be of different dimen-
sionality for each review, we extract features 
from these reviews. These features are selected 
based on our understanding of the problem and 
the fact that thwarting is a function of the change 
of polarity values and also the position of 
change. 
The Features extracted are: 
Document polarity, number of flips of sign (i.e. 
change of polarity from positive to negative and 
vice versa), the maximum and minimum values 
in a sequence, the length of the longest contigu-
ous subsequence of positive values (LCSP), the 
length of the longest contiguous subsequence of 
negative values (LCSN), the mean of all values, 
total number of positive values in the sequence, 
total number of negative values in the sequence, 
the first and the last value in the sequence, the 
variance of the moving averages, the difference 
in the means of LCSP and LCSN. 
7 Results 
Experiments were performed on a dataset ob-
tained by crawling product reviews from Ama-
zon1 . We focused on the camera domain. We 
obtained 1196 reviews from this domain. The 
reviews were annotated for thwarting, i.e., 
thwarted or non-thwarted as well as polarity. The 
reviews crawled were given to three different 
annotators. The instructions given for annotation 
were as follows: 
1. Read the entire review and try to form a 
mental picture of how sentiment in the 
document is distributed. Ignore anything 
that is not the opinion of the writer. 
2. Try to determine the overall polarity of 
the document. The star rating of the doc-
ument can be used for this purpose. 
3. If the overall polarity of the document is 
negative but, most of the words in the 
document indicate positive sentiment, or 
vice versa, then consider the document 
as thwarted. 
Since, identifying thwarting is a difficult task 
even for humans, we calculated the Cohen?s 
kappa score (Cohen 1960) in order to determine 
the inter annotator agreement. It was found out to 
                                                 
1Reviews crawled from http://www.amazon.com/ 
863
 
 
be 0.7317. The annotators showed high agree-
ment (98%) in the non-thwarted class whereas 
they agreed on 70% of the thwarted documents. 
Out of the 1196 reviews, exactly 21 were 
thwarted documents, agreed upon by all annota-
tors. We used the Stanford Core NLP tools 2 
(Klein et al, 2003, Toutanova et al, 2003) for 
basic NL processing. The system was tested on 
the entire dataset.  
Since, the data is highly skewed; we used Area 
under the Curve (AUC) for the ROC curve as the 
measure of evaluation (Ling et al, 2003). The 
AUC for a random baseline is expected to be 
50%, and the rule based approach is close to the 
baseline (56.3%). 
Table 1 shows the results for the experiments 
with the machine learning model. We used the 
CVX3 library in Matlab to solve the optimization 
problem for learning weights and the LIBSVM4 
library to implement the svm classifier. In order 
to account for the data skew, we assign a class 
weight of 50 (determined empirically) to the 
thwarted instances and 1 for non-thwarted in-
stances in the classifier. All results were obtained 
using a 10 fold cross validation. The same da-
taset was used for this set of experiments. 
 
Loss type 
for 
weights 
Percolation 
type for 
weights 
AUC value for 
classification 
Linear Complete 73% 
 Controlled 81% 
Hinge Complete 70% 
 Controlled 76% 
 
Table 1: Results of the machine learning based  
approach to thwarting detection 
 
We see that the overall system for identification 
of thwarting performs well for the weights ob-
tained using the linear model with a controlled 
percolation of polarity values in the ontology. 
The system outperforms both the random base-
line as well as the rule based system. These re-
sults though great are to be taken with a pinch of 
salt. The basic objective for creating a thwarting 
detection system was to include such a module in 
the general sentiment analysis framework. Thus, 
using document polarity as a feature contradicts 
the objective of sentiment analysis, which is to 
find the document polarity. Without the docu-
                                                 
2http://nlp.stanford.edu/software/corenlp.shtml  
3http://cvxr.com/cvx 
4http://www.csie.ntu.edu.tw/~cjlin/libsvm/ 
ment polarity feature, the values drop by 10% 
which is not acceptable. 
8 Conclusions and Future Work 
We have described a system for detecting thwart-
ing, based on polarity reversal between opinion 
on most parts of the product and opinion on the 
overall product or a critical part of the product. 
The parts of the product are related to one anoth-
er through an ontology. This ontology guides a 
rule based approach to thwarting detection, and 
also provides features for an SVM based learning 
system.  The ML based system scores over the 
rule based system. Future work consists in trying 
out the approach across products and across do-
mains, doing better ontology harnessing from the 
reviews and investing and searching for distribu-
tions and learning algorithms more suitable for 
the problem. 
References  
Blei, D. M., Ng, A. Y., and Jordan, M. I. 2003. Latent   
Dirichlet alocation. In the Journal of machine 
Learning research, 3, pages 993-1022. 
Brooke, J. 2009. A Semantic Approach to Automated 
Text Sentiment Analysis. Ph.D. thesis, Simon Fra-
ser University. 
Chang, C. C., and Lin, C. J. 2011. LIBSVM: a library 
for support vector machines. ACM Transactions on 
Intelligent Systems and Technology (TIST),2(3), 
27. 
Cohen, J. 1960. A coefficient of agreement for nomi-
nal scales.  Educational and psychological meas-
urement 20, no. 1, pages 37-46. 
Esuli, A. and Sebastiani, F. 2006. Sentiwordnet: A 
publicly available lexical resource for opinion min-
ing. In Proceedings of LREC, Volume 6, pages 
417-422. 
Hu, M. and Liu, B. 2004. Mining and summarizing 
customer reviews. In Proceedings of the tenth 
ACM SIGKDD international conference on 
Knowledge discovery and data mining, pages 168-
177. ACM. 
Klein, D. and Manning, C. D. 2003. Accurate Unlexi-
calized Parsing. In Proceedings of the 41st Meeting 
of the Association for Computational Linguistics, 
pages 423-430. 
Ling, C. X., Huang, J. and Zhang, H.2003. AUC: A 
better measure than accuracy in comparing learn-
ing algorithms. In Advances in Artificial Intelli-
gence, pages 329-341, Springer Berlin Heidelberg. 
864
 
 
Liu, B., and Zhang, L. 2012. A survey of opinion 
mining and sentiment analysis. In Mining Text Da-
ta (pp. 415-463).Springer US. 
Liu B., 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1), 1-167. 
Ohana, B. and Tierney, B. 2009.Sentiment classifica-
tion of reviews using SentiWordNet. In 9th. IT & T 
Conference, page 13. 
Pang, B., and Lee, L. 2008. Opinion mining and sen-
timent analysis. Foundations and trends in infor-
mation retrieval, 2(1-2), 1-135. 
Pang, B., Lee, L. and Vaithyanathan S. 2002. Thumbs 
up? Sentiment Classification using Machine Learn-
ing Techniques. In Proceedings of EMNLP pages 
79-86). 
Polpinij, J. and Ghose, A. K. 2008.An ontology-based 
sentiment classification methodology for online 
consumer reviews. In Web Intelligence and Intelli-
gent Agent Technology. 
Taboada, M. and Grieve, J. 2004. Analyzing appraisal 
automatically. In Proceedings of AAAI Spring 
Symposium on Exploring Attitude and Affect in 
Text (AAAI Technical Report SS# 04# 07), Stanford 
University, CA, pages. 158-161. AAAI Press. 
Toutanova, K., Klein, D., Manning, C. D. and Singer 
Y. 2003. Feature-Rich Part-of-Speech Tagging 
with a Cyclic Dependency Network. 
In Proceedings of HLT-NAACL, pages 252-259. 
Tsur, O., Davidov, D., & Rappoport, A. 2010. IC-
WSM?A great catchy name: Semi-supervised 
recognition of sarcastic sentences in online product 
reviews. In Proceedings of the fourth international 
AAAI conference on weblogs and social me-
dia, pages. 162-169. 
Saggion, H., Funk, A., Maynard, D. and Bontcheva, 
K. 2007. Ontology-based information extraction 
for business intelligence. In The Semantic 
Web pages 843-856, Springer Berlin Heidelberg. 
Stone, P. J., Dunphy, D. C., Smith, M. S., Ogilvie, D. 
M. and Associates. 1966. The General Inquirer: A 
Computer Approach to Content Analysis. The MIT 
Press. 
865
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 495?500, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
 
IITB-Sentiment-Analysts: Participation in Sentiment Analysis 
in Twitter SemEval 2013 Task 
 
Karan Chawla, Ankit Ramteke, Pushpak Bhattacharyya 
Dept. of Computer Science and Engineering, IIT Bombay 
{chawlakaran,ankitr,pb}@cse.iitb.ac.in 
  
Abstract 
We propose a method for using discourse rela-
tions for polarity detection of tweets. We have 
focused on unstructured and noisy text like 
tweets on which linguistic tools like parsers and 
POS-taggers don?t work properly. We have 
showed how conjunctions, connectives, modals 
and conditionals affect the sentiments in tweets. 
We have also handled the commonly used ab-
breviations, slangs and collocations which are 
usually used in short text messages like tweets. 
This work focuses on a Web based application 
which produces results in real time. This ap-
proach is an extension of the previous work 
(Mukherjee et al 2012). 
1. Introduction 
Discourse relation is an important component of 
natural language processing which connects 
phrases and clauses together to establish a cohe-
rent relation. Linguistic constructs like conjunc-
tions, connectives, modals, conditionals and ne-
gation do alter the sentiments of a sentence. For 
example, the movie had quite a few memorable 
moments but I still did not like it. The overall 
polarity of the sentence is negative even though 
it has one positive and one negative clause. This 
is because of the presence of the conjunction but 
which gives more weightage to the clause fol-
lowing the conjunction.  
Traditional works in discourse analysis use a 
discourse parser (Marcu  et al, 2003; Polanyi et 
al., 2004; Wolf et al, 2005; Welner et al, 2006; 
Narayanan et al, 2009; Prasad et al, 2010). 
Many of these works and some other works in 
discourse (Taboada et al, 2008; Zhou et al, 
2011) build on the Rhetorical Structure Theory 
(RTS) proposed by Mann et al (1988) which 
tries to identify the relations between the nucleus 
 
 
and satellite in the sentence. 
 
Most of the work is based on well-structured text 
and the methods applied on that text is not suita-
ble for the discourse analysis on micro-blogs 
because of the following reasons: 
 
1. Micro-blogs like Twitter restricts a post 
(tweet) to be of only 140 characters. Thus, users 
do not use formal language to discuss their 
views. Thus, there are abundant spelling mis-
takes, abbreviations, slangs, collocations, discon-
tinuities and grammatical errors. 
These differences cause NLP tools like POS-
taggers and parsers to fail frequently, as these 
tools are built for well-structured text. Thus, 
most of the methods described in the previous 
works are not well suited for discourse analysis 
on Micro-blogs like text. 
2. The web-based applications require a 
fast response time. Using a heavy linguistic re-
source like parsing increases the processing time 
and slows down the application. 
  
Most of the previous work on discourse analysis 
does not take into consideration the conjunc-
tions, connectives, modals, conditionals etc and 
are based on bag-of-words model with features 
like part-of-speech information, unigrams, bi-
grams etc. along with other domain-specific fea-
tures like emoticons, hashtags etc. Our work 
harness the importance of discourse connectives 
like conjunctions, connectives, modals, condi-
tionals etc and show that along with bag-of-
words model, it gives better sentiment classifica-
tion accuracy. This work is the extension of 
(Mukherjee et al 2012). 
 
The roadmap for the rest of the paper is as fol-
lows: Section 2 studies the effect of discourse 
relations on sentiment analysis and identifies the  
 
495
critical ones. Section 3 talks about the semantic 
operators which influence the discourse rela-
tions. Section 4 discusses the lexicon based clas-
sification approach. Section 5 describes the fea-
ture engineering of the important features. Sec-
tion 6 gives the list of experiments conducted 
and analysis of the results. Conclusion and Fu-
ture Work is presented in Section 7. 
 
2. Discourse Relations Critical for Sen-
timent Analysis 
(Mukherjee et al 2012) showed that that the fol-
lowing discourse relations are critical for SA as 
all relations are not useful for SA. Table 1 pro-
vides examples of various discourse relations. 
 
Violated Expectations and Contrast: In Exam-
ple 2, a simple bag-of-words feature based clas-
sifier will classify it as positive. However, it ac-
tually represents a negative sentiment. Such cas-
es need to be handled separately. In Example 5, 
?memorable" has (+1) score and ?not like" has (-
1) score and overall polarity is 0 or objective 
whereas it should be negative as the final verdict 
following ?but" is the deciding factor. 
 
These kinds of sentences refute the neighboring 
clause. They can be classified as Conj_Prev in 
which the clause preceding the conjunction is 
preferred and Conj_Fol in which the clause fol-
lowing the conjunction is preferred. 
 
Conclusive or Inferential Conjunctions: These 
are the set of conjunctions, Conj_infer, that tend 
to draw a conclusion or inference. Hence, the 
discourse segment following them (subsequently 
in Example 11) should be given more weight. 
 
Conditionals: In Example 3, ?amazing" 
represent a positive sentiment. But the final po-
larity should be objective as we are talking of a 
hypothetical situation. 
 
Other Discourse Relations: Sentences under 
Cause-Effect, Similarity, Temporal Sequence, 
Attribution, Example, Generalization and Elabo-
ration, provide no contrasting, conflicting or hy-
pothetical information. They can be handled by 
taking a simple bag-of-words model.  
3. Semantic Operators Influencing Dis-
course Relations 
There are connectives or semantic operators 
present in the sentences which influence the dis-
course relation within a sentence. For example, 
in the sentence the cannon camera may bad de-
spite good battery life. The connective despite 
increases the weightage of the previous dis-
course element i.e. bad is weighted up but may 
introduces a certain kind of uncertainty which 
cannot be ignored.  
 
1.  (I did not study anything throughout the seme-
ster), so (I failed in the exams). 
2.  (Sourav failed to deliver in the penultimate test) 
despite (great expectations). 
3. If (I had bought the amazing Nokia phone), I 
would not be crying). 
4. (I love Cannon) and (I also love Sony). 
5. (The movie had quite a few memorable moments) 
but (I still did not like it). 
6. (The theater became interesting) after a while. 
7. According (to the reviews), (the movie must be 
bad). 
8. (Salman is a bad guy), for instance (he is always 
late). 
9. In addition (to the bad battery life), (the camera 
is also very costly). 
10. In general, (cameras from cannon (take great 
pictures). 
11. (They were not in favour of that camera) and 
subsequently (decided not to buy it). 
Table 1:  Examples of Discourse Coherent 
Relations 
Similarity, in the sentence He gave his best in 
the movie, but still it was not good enough to win 
an Oscar. The connective but increases the 
weight of the following discourse i.e. good and 
win are weighted up but presence of negation 
operator also cannot be ignored. 
 
496
1. Modals: Events that are happening or are 
bound to happen are called realis events. And 
those events that have possibly occurred or have 
some probability to occur in distant future are 
known as irrealis events. And it is important to 
distinguish between the two as it also alters the 
sentiments in a piece of text. Modals depict ir-
realis events and just cannot be handled by sim-
ple majority valence model. 
 
(Mukherjee et al 2012) divided modals into two 
categories: Strong_Mod and Weak_Mod. 
 
Strong_Mod is the set of modals that express a 
higher degree of uncertainty in any situation. 
Weak_Mod is the set of modals that express 
lesser degree of uncertainty and more emphasis 
on certain events or situations.  
 
Like conditionals, sentences with strong modals 
express higher degree of uncertainty, thus dis-
course elements near strong modals are weighted 
down. Thus, in the previous example the cannon 
camera may bad despite good battery life bad is 
toned down. 
 
Relations Attributes 
Conj_Fol but, however, never-
theless, otherwise, yet, 
still, nonetheless 
Conj_Prev till, until, despite, in 
spite, though, although 
Conj_Inf therefore, furthermore, 
consequently, thus, as 
a result, subsequently, 
eventually, hence 
Conditionals If 
Strong_Mod might, could, can, 
would, may 
Weak_Mod should, ought to, need 
not, shall, will, must 
Neg not, neither, never, no, 
nor 
Table 2: Discourse Relations and Semantic 
Operators Essential for Sentiment Analysis 
 
2. Negation: The negation operator inverts the 
polarity of the sentence following it. Usually, to 
handle negation a window (typically 3-5 words) 
is considered and the polarities of all the words 
are reversed. We have considered the window 
size to be 5 and reverse the polarities of all the 
words within the window, till either a conjunc-
tion comes or window size exceeds. For example 
In the sentence He gave his best in the movie, 
but still it was not good enough to win an Oscar 
polarities of good and win are reversed. 
  
4. Lexicon Based Classification 
We have used Senti-WordNet (Esuli et al 2006), 
Inquirer (Stone et. al 1996) and the Bing Liu 
sentiment lexicon (Hu et al 2004) to find out the 
word polarities. To compensate the bias effects 
introduced by the individual lexicons, we have 
used three different lexicons. The polarities of 
the reviews are given by (Mukherjee et al 2012) 
 
???? (  ??? ? ?????? ? ?(??? ))
??
?=1
?
?=1
 
 
????? ? ???  =  ??? ???   ?? ????? = 0 
                        
                             =  
??? ???  
2
 ?? ????? = 1  
   
Above equation finds the weighted, signed po-
larity of a review. The polarity of each word, 
pol(wij) being +1 or -1, is multiplied with its dis-
course weight fij and all the weighted polarities 
are added. Flipij indicates if the polarity of wij is 
to be negated. 
In case there is any conditional or strong modal 
in the sentence (indicated by ????? = 1 ), then 
the polarity of every word in the sentence is 
toned down, by considering half of its assigned 
polarity (
+1
2
 ,
?1
2
) 
Thus, if good occurs in the user post twice, it 
will contribute a polarity of +1 ? 2 = +2 to the 
overall review polarity, if ????? = 0. In the 
presence of a strong modal or conditional, it will 
contribute a polarity of 
+1
2
? 2 =  +1. 
497
All the stop words, discourse connectives and 
modals are ignored during the classification 
phase, as they have a zero polarity in the lexicon.  
We have handled commonly used slangs, ab-
breviations and collocations by manually tagging 
them as positive, negative or neutral.  
5. Feature Engineering 
The features specific for lexicon based classifi-
cation for the task sentiment Analysis, identified 
in Section 2.4, are handled as follows: 
 
a) The words following the Conj_Fol (Table 2) 
are given more weightage. Hence their frequency 
count is incremented by 1. 
We follow a naive weighting scheme whereby 
we give a (+1) weightage to every word we con-
sider important. In Example 5, ?memorable" gets 
(+1) score, while ?did not like" gets a (-2) score, 
making the overall score (-1) i.e. the example 
suggests a negative sentiment. 
 
b) The weightage of the words occurring before 
the Conj_Prev (Table 2) is increased by 1. In 
Example 2, ?failed" will have polarity (-2) in-
stead of (-1) and ?great expectations" will have 
polarity (+1), making the overall polarity (-1), 
which conforms to the overall sentiment. 
 
c) The weightage of the words in the sentences 
containing conditionals (if) and strong modals 
(might, could, can, would, may) are toned down. 
 
e) The polarity of all words appearing within a 
window of 5 from the occurrence of a negation 
operator (not, neither, nor, no, never) and before 
the occurrence of a violating expectation con-
junction is reversed. 
  
f) Exploiting sentence position information, the 
words appearing in the first k and last k sen-
tences, are given more weightage. The value of k 
is set empirically. 
 
g) The Negation Bias factor is treated as a para-
meter which is learnt from a small set of nega-
tive polarity tagged documents. The frequency 
count of all the negative words (in a rule based 
system) is multiplied with this factor to give 
negative words more weightage than positive 
words. 
6. Experiments and Evaluation 
For the lexicon-based approach, we performed 
two types of experiments- sentiment pertaining 
to a particular instance in a tweet (SemEval-
2013 Task A) and generic sentiment analysis of 
a tweet (SemEval-2013 Task B). We treat both 
the tasks similarly. 
 
6.1 Dataset 
 
We performed experiments on two Datasets: 
 
1) SemEval-2013-task 2 Twitter Dataset A con-
taining 4435 tweets without any external data. 
2) SemEval-2013-task 2 Twitter Dataset B con-
taining 3813 tweets without any external data. 
 
6.2 Results on the Twitter Dataset A and B 
 
The system performs best for the positive class 
tweets as shown in Table 3 and Table 4 and per-
forms badly for the negative class which is due 
to the fact that negative tweets can contain sar-
casm which is a difficult phenomenon to capture. 
Also the results of the neutral category are very 
less which suggests that our system is biased 
towards subjective tweets and we wish to give 
the majority sentiment in the tweets. 
  
Class Precision Recall F-score 
Positive 0.6706 0.5958 0.6310 
Negative 0.4124 0.5328 0.4649 
Neutral 0.0667 0.0063 0.0114 
Table 3: Results on Twitter Dataset A 
 
Class Precision Recall F-score 
Positive 0.4809  0.5941 0.5316 
Negative 0.1753   0.5374 0.2643 
Neutral 0.6071  0.0104 0.0204 
Table 4: Results on Twitter Dataset B 
498
6.3 Discussion 
 
The lexicon based classifier suffers from the 
problem of lexeme space where it is not able 
handle all the word senses. Also, short-noisy text 
like tweets often contain various spelling mis-
takes like great can be grt, g8t etc. or tomorrow 
can be tom, tomm, tommrrw etc. which will not 
be detected and handled properly.  
 
We suggest that a supervised approach compris-
ing of the discourse features along with the bag-
of-words model and the sense based features will 
improve the results. 
 
7. Conclusion and Future Work 
We have showed that discourse connectives, 
conjunctions, negations and conditionals do alter 
the sentiments of a piece of text. Most of the 
work on Micro-blogs like twitter is build on bag-
of-words model and does not incorporate dis-
course relations. We discussed an approach 
where we can incorporate discourse relations 
along-with bag-of-words model for a web-
application where parsers and taggers cannot be 
used as the results are required in real time. 
 
We need to take into consideration word senses 
and a supervised approach to use all the features 
collectively. Also, a spell checker would really 
help in the noisy text like in tweets.  
 
References  
A Agarwal and Pushpak Bhattacharyya. 2005. Senti-
ment Analysis: A New Approach for Effective Use of 
Linguistic Knowledge and Exploiting Similarities in a 
Set of Documents to be classified. International Con-
ference on Natural Language Processing (ICON 05), 
IIT Kanpur, India, December  
 
AR Balamurali, Aditya Joshi and Pushpak Bhattacha-
ryya. 2011. Harnessing WordNet Senses for Super-
vised Sentiment Classification. In Proceedings of 
Empirical Methods in Natural Language Processing 
(EMNLP). 
 
A Esuli and F Sebastiani, 2006. SentiWordNet: A 
Publicly Available Lexical Resource for Opinion 
Mining. In Proceedings from International Confe-
rence on Language Resources and Evaluation 
(LREC), Genoa.  
 
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proc. of ACM SIGKDD. 
 
Aditya Joshi, AR Balamurali, Pushpak Bhattacharyya 
and R Mohanty. 2010. C-Feel-It: A Sentiment Ana-
lyzer for Micro-blogs', Annual Meeting of the Associ-
ation of Computational Linguistics (ACL 2011), Ore-
gon, USA.  
 
William C. Mann and Sandra A. Thompson. Rhetori-
cal Structure Theory: Toward a functional theory of 
text organization. Text, 8 (3), 243-281. 1988 
 
R Narayanan, Bing Liu and A Choudhary. 2009. Sen-
timent Analysis of Conditional Sentences. In Pro-
ceedings of Conference on Empirical Methods in 
Natural Language Processing (EMNLP-09).  
 
L Polanyi and A Zaenen. 2004. Contextual Valence 
Shifters. In James G. Shanahan, Yan Qu, Janyce 
Wiebe (eds.), Computing Attitude and Affect in Text: 
Theory and Applications, pp. 1-10.  
 
BP Ramesh, R Prasad and H Yu. 2010. Identifying 
explicit discourse connective in biomedical text. In 
Annual Symposium proceedings, AMIA Symposium, 
Vol. 2010, pp. 657-661.  
 
R Soricut and D Marcu. 2003. Sentence level dis-
course parsing using syntactic and lexical informa-
tion. In Proc. of HLT-NAACL 
 
PJ Stone, DC Dunphy, MS Smith, DM Ogilvie and 
Associates. 1996. The General Inquirer: A Computer 
Approach to Content Analysis. The MIT Press  
 
Subhabrata Mukherjee and Pushpak Bhattacharyya. 
2012. Sentiment Analysis in Twitter with Lightweight 
Discourse Analysis. In Proceedings of  COLING 
2012 
Subhabrata Mukherjee and Pushpak Bhattacharyya. 
2012. Sentiment Analysis in Twitter with Lightweight 
Discourse Analysis. In Proceedings of the 21st ACM 
Conference on Information and Knowledge Manage-
ment (CIKM), short paper.   
 
Subhabrata Mukherjee, AR Balamurali, Akshat Malu 
and Pushpak Bhattacharyya. 2012. TwiSent: A Ro-
499
bust Multistage System for Analyzing Sentiment on 
Twitter. In Proceedings of the 21st ACM Conference 
on Information and Knowledge Management (CIKM), 
poster paper.  
 
Maite Taboada, Julian Brooke and Kimberly Voll. 
2008. Extracting Sentiment as a Function of Dis-
course Structure and Topicality. Simon Fraser Unive-
risty School of Computing Science Technical Report. 
 
B Wellner, J Pustejovski, A Havasi, A Rumshiskym 
and R Suair. 2006. Classification of discourse cohe-
rence relations: An exploratory study using multiple 
knowledge sources. In Proc. of SIGDIAL  
 
F Wolf and E Gibson. 2005. Representing Discourse 
Coherence: A Corpus-based Study. Computational 
Linguistics, 31(2), pp. 249-287.  
 
Lanjun Zhou, Binyang Li, Wei Gao, Zhongyu Wei 
and Kam-Fai Wong. 2011. Unsupervised discovery of 
discourse relations for eliminating intra-sentence po-
larity ambiguities. In Proceedings of EMNLP. 
500
