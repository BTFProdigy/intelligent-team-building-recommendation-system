Development of the HRL Route Navigation Dialogue System
Robert Belvin
HRL Laboatories, LLC
3011 Malibu Canyon Road
Malibu, CA 90265
310-317-5799
rsBelvin@hrl.com
Ron Burns
HRL Laboatories, LLC
3011 Malibu Canyon Road
Malibu, CA 90265
310-317-5445
rBurns@hrl.com
Cheryl Hein
HRL Laboatories, LLC
3011 Malibu Canyon Road
Malibu, CA 90265
310-317-5489
cheryl@hrl.com
ABSTRACT
In this paper we report on our work on a prototype route
navigation dialogue system for use in a vehicle.  The system
delivers spoken turn-by-turn directions, and has been developed
to accept naturally phrased navigation queries, as part of our
overall effort to create an in-vehicle information system which
delivers information as requested while placing minimal cognitive
load on the driver.
Keywords
Dialogue Systems, Discourse, Navigation, NLP, Pragmatics,
Dialogue Manager
1. INTRODUCTION
In this paper we report on our work on a spoken language
navigation system which runs in real-time on a high-end laptop or
PC, for use in a vehicle.   We focus on issues in developing a
system which can understand natural conversational queries and
respond in such a way as to maximize ease of use for the driver.
Because today?s technology has the potential to deliver massive
amounts of information to automobiles, it is crucial to deliver this
information in such way that the driver?s attention is not diverted
from the primary task of safe driving.  Our assumption has been
that a dialogue system with a near-human conversational ability
would place less of a cognitive load on the driver than one which
behaves very differently than a human.
We have implemented a testbed on which to develop and evaluate
driver interfaces to navigation systems. Our approach is multi-
modal and the interface will include a head-up display, steering
hub controls, and spoken language, though it is only the latter
modality that we report on here.  We first discuss our
development phases, and after this we provide an overview of our
implementation, emphasizing the natural language processing
aspects and application interface to the map databases.  Next we
provide results of our initial evaluation of the system, and finally
we draw conclusions and summarize plans for future work.
2. DEVELOPMENT PHASES
One can identify four distinct subproblems which must be solved
for a navigation system: 1) the natural language navigation
interface, 2) street name recognition, 3) the natural language
destination entry interface given street name recognition, and 4)
the map database interface. We have partitioned the problem and
have phased our development to progressively implement
solutions with increasing complexity.
Navigation system implementation is complicated by the potential
of having a very large street name vocabulary with many unusual
and uncommon pronunciations with significant variations across
speakers. The appropriate name space is dynamic since it depends
on the location of the vehicle.
Our initial system does not accept queries with proper street
names. In addition, we assume separate destination entry and
route planning systems, and that one or more routes have been
loaded into the navigation system. The system relies on open
dialogue to resolve the directions at any stage of the journey and
may or may not use the Global Positioning System (GPS) to
determine the progress along the route. By implementing this
system first we could concentrate on the dialogue aspects of the
navigation problem and also establish a baseline with which to
compare our other implementations.
In the second phase we include a limited set of street names as
part of the language model and lexicons. Initially we are using a
predefined set of names with hand tuning of the pronunciations.
Additional research is required to solve the street name
recognition problem generally and automatically. We assume in-
vehicle GPS and use a map matching system to determine the
vehicle?s position and if it is on-route. This phase includes
development of the natural language components for destination
entry and also broadens the scope of the navigation queries to
include questions with and about street names.  More distant plans
include on-road route replanning, providing information to
requests for specific street names or points of interest along the
route, and traffic information and workarounds.
3. IMPLEMENTATION
Our implementation is based on the Galaxy-II system [6] from the
Massachusetts Institute of Technology (MIT), which is the
baseline for the Communicator program of the Defense Advance
Research Projects Agency (DARPA). The architecture consists of
a hub client that communicates, using a standard protocol, with a
number of servers as shown in Figure 1. Each server generally
implements a key system function including Speech Recognition,
Frame Construction (language parsing), Context Tracking,
Dialogue Management, Application Interface, Language
Generation, and Text-to-speech.
?2001 HRL Laboratories, LLC, All rights reserved
Speech
Recognition
Language
Generation
Dialogue
Management
Application
Back-end
Context
Tracking
Frame
Construction
Audio
Server
Text-to-speech
Conversion
Hub
Figure 1. The client-server architecture of the MIT Galaxy-II
is used to implement our navigation system testbed.
3.1 Speech Recognition
We use the latest MIT SUMMIT recognizer [8] using weighted
finite-state transducers for the lexical access search. We have also
"plugged in" alternate recognizers such as the Microsoft Speech
SDK recognizer and the Sphinx [3] speech recognizer available as
open source code from Carnegie Mellon University.
We are in the process of developing a large database of in-vehicle
utterances collected in various car models under a wide range of
road and other background noise conditions. This data collection
will be carried out in two phases, the first of which is completed;
phase two is underway. Limited speech data will result from the
first phase and subtantial speech data (appropriate for training
acoustic models to represent in-vehicle noise conditions and
testing of recognition engines) will come out of the second phase,
and will become available through our partners in this collection
effort, CSLR at University of Colorado, Boulder [4]. In the
meantime we are using the MIT JUPITER acoustic models. The
performance is acceptable for our language and dialogue model
development, but we refrain from presenting any detailed
recognizer results here since they would not reflect fairly on
optimized recognizer performance.
Our vocabulary consists of about 400 words without street names.
We have an additional 600 street names gleaned from the Los
Angeles area where we do much of our system evaluation.
Baseforms for the vocabulary are derived from the PRONLEX
dictionary from the Linguistic Data Consortium at the University
of Pennsylvania. Extensive hand editing is needed especially for
the street names. The MIT rule set is used for production of word
graphs for the alternate pronunciation forms.  We have derived a
language model from a set of utterances that were initially
generated based our best guess of the query space. As evaluation
evolves, we modify the utterance list and retrain the language
model. The language model uses classes and includes both bigram
and trigram models.
3.2 Application Interface
We are building the application interface in several phases.
Initially we are only answering queries about turns and distances
during navigation. We obtain the database in two steps. First, we
access a commercial map database using standard text I/O for
destination entry and route planning. This produces a detailed set
of instruction that includes many short segments such as on- and
off-ramps. We filter this and rewrite the data to provide a set of
natural driving instructions suitable for verbal communication.
The result is a flat database, such as the one shown in Figure 2.
This is loaded into to the system and used to formulate answers to
the route queries. In the example in Figure 2 the estimated driving
time is 45 minutes. Each row is a segment of the trip. The first and
second columns code right, left, straight, and compass direction
information. The third column is the segment length in miles and
the last is the segment name.
HRL Airport 45
0 0  1.0 Malibu Canyon Road
0 2 13.3 PCH
1 1  3.4 East I-10 freeway
0 1  4.5 South I-405 freeway
3 0  0.3 Howard Hughes Parkway
0 1  1.7 Sepulveda Boulevard
0 4  0.4 Los Angeles International Airport
Figure 2: Example database used as source data to generate
guidance instructions for the route between HRL Laboratories
and the Los Angeles airport
A sample dialogue is shown in Figure 3 which illustrates the kind
of responses the system can generate from a database such as that
shown above, given navigation queries of the sort shown; this
sample was drawn from our phase I user-system data logs:
S Welcome to Navigator
U What routes are loaded?
S I have one set of directions loaded. It is
for the route to the airport from HRL
U What?s my first turn?
S From Malibu Canyon Road turn left onto
Pacific Coast Highway
U And after that?
S From Pacific Coast Highway go straight
onto East I-10 freeway
U How far is that turn?
S The distance is 14.3 miles
U My odometer reads fifteen point one miles.
S Your odometer reading is fifteen point one
U What is my next turn?
S From East I-10 freeway turn right onto
South I-405 freeway
U How far is it from here?
S The distance is 2.6 miles
U How far is the airport from there?
S The distance is 6.7 miles
U Bye Bye.
S Goodbye. Have a nice day.
Figure 3. A navigation system dialog illustrating some sample
queries and showing the responses derived by the dialogue
manager based on the database of Figure 2.
Off-line construction of the global navigation database is not
unrealistic since it could be done, at least in the near term, by a
service organization such as OnStar from General Motors (GM).
However as navigation systems become widely deployed, users
will expect destination entry including real time route re-planning
to be an integral part of system. We are developing a direct voice
interface to the commercial map database that includes destination
entry, route planning, and map matching using GPS data to
determine if the vehicle is on-route or not.
During the destination entry phase street names need to be
robustly recognized. We are currently working with a subset of
street names in the Los Angeles area preloaded in the recognizer
and language models. It is untenable to keep all of the street
names in Los Angeles loaded in the recognizer simultaneously
(there are around 16,000, including 8,000 base names), thus we
are developing a method for dynamic loading of map names local
to the vehicle position which we will report on in the near future.
We have experimented with using a subset of street names as a
filter list, and as a lookup list based on spelling the first few
letters, to try to resolve the destination requested.  If this fails, or
if the trip is outside of the area from which names are loaded, we
rely on more complete spelling to determine the destination. The
origin for the route plan is generally implied since it is determined
by the GPS position of the vehicle most of the time. Once the
destination is determined it is straightforward to continuously re-
plan the route based on the current vehicle position and thereby
be able to provide remedial instruction if the driver departs from
the route plan.
3.3 NL Analysis and Generation
The core NLP components in our system are a TINA [5] grammar,
context tracking mechanisms, and the GENESIS language
generation module.  The TINA grammar includes both syntactic
and semantic elements and we try to extract as much information
as possible from the parse. The information is coded in a
hierarchical frame (Figure 4a) as well as a flat key-value pair
(Figure 4b). In addition to handcrafting this grammar, a set of
rules was also developed for the TINA inheritance mechanism.
These rules are applied during context tracking, after the parse, to
incorporate information from the dialog history into phrases such
as "and after that" and "how about my second turn," and are also
used to incorporate modifications that are a result of dialogue
management.
a) Parse frame
{c locate
 :domain "Nav"
 :pred {p locate_object
      :topic {q turn
          :quantifier "poss_pro"
          :pred {p ord
              :topic 2 }}}}
b) Key-values Pairs
:clause "locate" :locate_object "turn" :ORD 2
c) Reply Frame
{c speak_turn
  :topic {q turn
      :turn_direction "straight"
      :current_roadway "PCH"
  :new_roadway "East I-10 freeway"
  :domain :Nav" }
Figure 4. Example frames produced for the simple query
"What?s my second turn?"
As noted, we use the MIT GENESIS server for language
generation. Again this module is rule driven and we developed the
lexicon, templates and rewrite rules needed for the three ways we
use GENESIS. We extract the key-value pairs (e.g. Figure 4b)
from the TINA parse frame. The key values are used to help
control the dialogue management as well as provide easy access to
the variable values. We use GENESIS to produce the English
reply string that is spoken by the synthesizer. The example frame
in Figure 4c in conjunction with our rules generates the sentence
"From Pacific Coast Highway turn straight onto East I-10
freeway" Lastly GENESIS is used to produce an SQL query
string for database access. Templates and rewrite rules determine
which form the output from GENESIS will take. Technically these
three uses (key-value, reply string, and SQL) are just generation
of different languages.
3.4 Dialogue Management
We have developed servers for dialog management and to control
the application interface for database query. The hub architecture
supports use of a control table to direct which server function is
called. This is especially useful for dialogue management. The
control table is specified by a set of rules using logic and
arithmetic operations on the key-value pairs. A well-designed set
of rules makes it far easier to visualize the flow and debug the
dialogue logic. For example, when a control rule such as:
 Clause "locate" !:from --> turn_from_here
fires on the key-value pairs (Figure 4b), the hub calls the turn
manager function "turn_from_here". In this simplified case, we
are assuming if there is no ":from" key, the request is to locate an
object (i.e. "turn") relative to the vehicle?s current position. In this
case the function needs only to extract the value of the key
":ORD" and look up the data for the second turn in the database
of Figure 2. This data is then written into the response frame, here
called "speak_turn" and shown in Figure 4c. GENESIS uses this
frame to generate the English language reply that is spoken by the
synthesizer as described above.
In the examples shown here we communicate with the database
from the dialogue manager by downloading a flat database such as
that of Figure 2, perhaps via a data link to an off-board service
organization such as OnStar. In cases where we access databases
directly, we use a separate server for this function. Generally,
communications between the dialogue manager and database
servers are routed via the Hub.
Our dialogue manager has been designed to use GPS data when
available (in which case GPS coordinates would also be a part of
the database) or to use location information based on current
odometer readings provided as input by drivers when GPS is not
available.  We use this latter method for demonstrating the system
in a desktop setting, though we have also recently completed a
utility for employing maps generated by our commercial
navigation database, graphically displaying a driver?s progress
along an imaginary route.  We are now employing this tool as part
of our current iteration of system testing and revision.
3.4.1 Referential amiguities in driver queries
The driver can query to determine turn or distance information
relative to current vehicle position, relative to another turn or
reference point in the database, or as an absolute reference into
the route plan stored in the database.  We have devoted
considerable effort to dealing with ambiguities which may arise as
a result of different ways users may be conceptualizing the route
(that is, in absolute or relative terms), as well as the driver being
at different points in the route, and at different points in the
progression of a discourse segment.  Queries such as ?what?s
next?? can be ambiguous.  Determining the correct interpretation
requires consideration of the discourse history and the user?s
circumstances.  For example, in the following dialog sequence
(drawn from our data), there are at least two possible
interpretations for ?what is next?? in the third turn (U:user,
S:system):1
   ---------------
   U:  what?s my next turn
   S:  From Malibu Canyon Road turn left
       onto Pacific Coast Highway.
   ---------------
   U:  and after that
   S:  From Pacific Coast Highway go
       straight onto East I-10 freeway
   ---------------
? U:  what?s next
Figure 5. Sample dialog containing ambiguous ?What is next?.
Notice that this query could be requesting information about the
next turn from the driver?s current position (i.e. the immediately
approaching turn), or it could be requesting information about the
third turn from the driver?s current position, that is, the next turn
from the most recently referred to turn.  We will henceforth refer
to these two interpretations as next-from-here and next-after-that,
respectively.
The factor which appears to have the most influence on which
interpretation is given to this utterance originates neither in the
utterance itself nor in the preceding dialog, but is purely
circumstantial, namely, how much time has passed since the last
utterance.  Our assumption has been that there is a kind of time-
dependency factor in coherent discourses: while ?what is next? is
still within the scope of the preceding discourse context, it may
(most likely will) be given the next-after-that interpretation.  But
after a certain length of time has elapsed, ?what is next? cannot be
interpreted as referring to some previously uttered instruction, but
only as referring to the driver?s current position.  If we think of
this in terms of the user?s frame of reference for talking about
their real or imagined location (we?ll refer to this as the FROM
value), then we could characterize this phenomenon as the value
of FROM being reset to HERE in the absence of immediate
discourse context.
Interpretations of numbered turn references (e.g. "what's my
second turn") can also vary depending on another purely
circumstantial factor, namely whether the driver is querying the
system while preparing to begin the trip, or after she has begun
driving.  Some drivers will want to preview trip information
before beginning to drive, and in this situation, interpretation of
certain query types may differ from interpretation done during the
trip.  When the driver is querying the system before beginning to
drive, she is more likely to conceive of and speak of the route in
an absolute sense (cf. [7]).  That is, the driver may conceive of the
route as a fixed plan, wherein each turn and segment have a
unique and constant order in a sequence.  When conceiving of the
                                                                
1
 There is at least one further possible interpretation to ?what is
next?? here, at least if the proper prosodic features are present.
If heavy emphasis is placed on ?what,? the query has a quasi
echo-question interpretation, indicating either that the user did
not hear, or else is surprised at the prior instruction and is
asking for clarification or repetition.
route in this way, one may refer to turns by number in the route,
rather than by number relative to current position.  Although we
have yet to gather real user data bearing on this question, our
intuition is that once the trip is underway, especially once any
significant distance has been traveled, if users do use numbered
turn references at all, they will be much more likely to use them
relative to their current position.
Queries of this type are, for practical purposes, only ambiguous
once the user has begun the trip, but prior to the absolute
numbered turn.  Drivers are very unlikely to be asking about the
second turn in the route once they have passed the second turn.
Moreover, since people will generally only keep track of turn
numbers in the range of 1-3, (give or take 1), numbered turn
references will only be ambiguous prior to the third or fourth turn
in the route (nobody is likely to be asking ?what is the eighth turn
in the route?).  What is more, if the user asks a numbered turn
query before beginning the trip, the system response will be the
same, since the relative and absolute turn numbers will at that
point coincide.  Thus, the only time a true ambiguity must be
handled by the system is the time after the trip is underway, and
before the fourth turn.  It is perhaps worth noting that if one looks
at the overall query interpretation problem as entailing a
determination of whether the user is asking a question relative to
their current position, or some other position, then the
absolute/relative distinction is just a special case that.
We have gone on the assumption that there are a substantial
number of these ambiguous queries, not only for those of the
"what's next" type, but also for some numbered turn requests, and
for a class of distance query [1].  However, we have now carried
out an experiment in which subjects interpreted such queries in a
controlled setting, and the results indicate there is far less
ambiguity in truly felicitous driver utterances than we originally
hypothesized [2].  There probably will be some genuinely
ambiguous queries, especially for a system which is not capable of
detecting prosodic cues, however, we now are of the opinion that
they will not comprise a significant percentage of driver queries.
For the system which we describe herein, however, the control
logic for queries of the type under discussion includes
consideration of the temporal "reset" threshold discussed above,
as indicated in the following table:
Table 1. Decision matrix showing some determining factors for
interpreting ?next? and numbered turn queries.
n-route -- -- ? -- -- -- --
eset threshold
eached
-- -- -- -- -- ?
ext turn ? ? ? ?
umbered turn n n n
From here? ? ?
After that? ? ?
QL Turn number c+n c+1 n r+n  r+1 r+1 c+1
? = set c = current position
blank = not set r = most recently mentioned turn
n = number value  -- = irrelevant
The table is to be read column-by-column.  Thus, the first column
tells us that if we have a query with a numbered turn reference and
a phrase which is semantically equivalent to ?from here? (which is
also the default), then the instruction number which will be
requested (via SQL query) from the database is current+number.
4. EVALUATION
We are have implemented an initial system and are conducting
ongoing evaluations and iterative enhancements as part of a
second phase of effort. We are in advanced development on the
second phase. We report here on some results of the first phase of
our project.
Evaluation of a route guidance system is difficult because the
majority of time is spent driving with only a periodic need for
instructions. Therefore, for the purposes of developing the
language and dialogue models we tried to expedite data collection
by having dialogues in which the user simulated a trip by means
of a more or less continuous conversation with the system. The
position of the vehicle along the route was determined by the user
providing odometer readings relative to the start of the trip. At
each point the user would query the system and input a new
odometer location along the route and continue the dialogue.
While certainly not as meaningful as queries under normal driving
condition, we did obtain good data for our recognizer language
model and grammar coverage. In addition we could debug and
tune our turn manager functions to make sure we were properly
accessing the database and providing correct responses.
Each query essentially represents a single task and the most
meaningful metric for this type of system seems to be the number
of dialogue turns per correct response. By correct response we
mean that the system provides the final answer versus providing a
request to repeat or disambiguate the user query. We have
accumulated several thousand utterances during dialogues that run
around fifteen to twenty turns per session for a simple route like
the one in Figure 2. About a third of the utterances are used to set
the vehicle position via inputting odometer data.
We can also divide the dialogues into task oriented dialogues,
where the user is trying to get helpful answers, and dialogues
where the user is exploring the limits of the system. We find with
the task oriented dialogues that the number of dialogue turns are
about 15-20% greater that the number of correct responses and
that the inital implementations even without street name
recognition is a useful system.
5. SUMMARY AND FUTURE PLANS
We have reported on our initial implementation and results for an
in-vehicle navigation system through the first phase of our system
development effort and into the second phase. Full exploitation of
the natural language interface is not fully completed in the first
phase because we are still developing an operational in-vehicle
navigation system to integrate with our dialogue system. The full
interface, including destination entry, route planning, position
tracking, and map matching will be available later this year. We
have, however, developed most of the NL components needed for
accessing the database functionality as it comes on-line. We plan
to add other important functionality such as points-of-interest and
traffic conditions as the project progresses.
Two other major elements need to be further explored to gain full
system functionality. The first is recognizer robustness in the
presence of in-vehicle noise during normal everyday use; the
second is the street name recognition and pronunciation synthesis
problem.  Recognizer performance is being addressed by means of
a full-scale data collection and corpora development project, in
collaboration with GM and the Center for Spoken Language
Research at the University of Colorado at Boulder. This work will
provide the in-vehicle acoustic data needed to re-train the
recognizer models as well as provide a database for developing
noise-mitigation and speaker adaptation algorithms for improving
recognizer performance. We are developing a method for dynamic
loading of street names which we will report on in the near future.
Acknowledgments. This work was supported in part by a
research contract from General Motors.
6. REFERENCES
[1] R. Belvin, R. Burns and C Hein "What?s next: A Case Study
in the Multidimensionality of a Dialogue System,"
Proceedings of ICSLP 2000, Beijing, China, October 2000.
[2] A. Kessell and R. Belvin "Unambiguous Amiguous
Questions: Pronominal Resolution in Human-to-Human
Navigation," unpublished ms., HRL Laboratories, 2001.
[3] K-F. Lee, Automatic Speech Recognition: The Development
of the Sphinx System, Kluwer, Boston, 1989.
[4] B. Pellom, W. Ward, J. Hansen, K. Hacioglu, J. Zhang, X.
Yu, and S. Pradhan, "University of Colorado Dialog
Systems for Travel and Navigation," these proceedings,
2001.
[5] S. Seneff. "TINA: A Natural Language System for Spoken
Language Applications," Computational Linguistics, Vol.
18, No. 1, pp. 61-86, 1992.
[6] S. Seneff, E. Hurley, R. Lau, C. Pao, P. Schmidt and V.
Zue, "Galaxy-II: A Reference Architecture For
Conversational System Development," Proc. ICSLP '98, pp.
931-934, Sydney, Australia, November 1998.
[7] L. Suchman. Plans and situated actions. Cambridge
University Press, Cambridge, 1987.
[8] V. Zue, S. Seneff, J. Glass, J. Polifroni, C. Pao, T. Hazen &
L. Heatherington, ?Jupiter: A Telephone-Based
Conversational Interface for Weather Information,?
IEEE:Transactions on Speech and Audio Processing, Vol.
8, No. 1, pp. 85-96, 2000.
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 89?92, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Transonics: A Practical Speech-to-Speech Translator for English-Farsi
Medical Dialogues
Emil Ettelaie, Sudeep Gandhe, Panayiotis Georgiou,
Kevin Knight, Daniel Marcu, Shrikanth Narayanan ,
David Traum
University of Southern California
Los Angeles, CA 90089
ettelaie@isi.edu, gandhe@ict.usc.edu,
georgiou@sipi.usc.edu, knight@isi.edu,
marcu@isi.edu, shri@sipi.usc.edu,
traum@ict.use.edu
Robert Belvin
HRL Laboratories, LLC
3011 Malibu Canyon Rd.
Malibu, CA 90265
rsbelvin@hrl.com
Abstract
We briefly describe a two-way speech-to-
speech English-Farsi translation system
prototype developed for use in doctor-
patient interactions.  The overarching
philosophy of the developers has been to
create a system that enables effective
communication, rather than focusing on
maximizing component-level perform-
ance.  The discussion focuses on the gen-
eral approach and evaluation of the
system by an independent government
evaluation team.
1 Introduction
In this paper we give a brief description of a
two-way speech-to-speech translation system,
which was created under a collaborative effort
between three organizations within USC (the
Speech Analysis and Interpretation Lab of the
Electrical Engineering department, the Information
Sciences Institute, and the Institute for Creative
Technologies) and the Information Sciences Lab of
HRL Laboratories.  The system is intended to pro-
vide a means of enabling communication between
monolingual English speakers and monolingual
Farsi (Persian) speakers.  The system is targeted at
a domain which may be roughly characterized as
"urgent care" medical interactions, where the Eng-
lish speaker is a medical professional and the Farsi
speaker is the patient.  In addition to providing a
brief description of the system (and pointers to pa-
pers which contain more detailed information), we
give an overview of the major system evaluation
activities.
2 General Design of the system
Our system is comprised of seven speech and
language processing components, as shown in Fig.
1. Modules communicate using a centralized mes-
sage-passing system. The individual subsystems
are the Automatic Speech Recognition (ASR) sub-
system, which uses n-gram Language Models
(LM) and produces n-best lists/lattices along with
the decoding confidence scores. The output of the
ASR is sent to the Dialog Manager (DM), which
displays the n-best and passes one hypothesis on to
the translation modules, according to a user-
configurable state. The DM sends translation re-
quests to the Machine Translation (MT) unit. The
MT unit works in two modes: Classifier based MT
and a fully Stochastic MT. Depending on the dia-
logue manager mode, translations can be sent to
the unit selection based Text-To-Speech synthe-
sizer (TTS), to provide the spoken output. The
same basic pipeline works in both directions: Eng-
lish ASR, English-Persian MT, Persian TTS, or
Persian ASR, Persian-English MT, English TTS.
There is, however, an asymmetry in the dia-
logue management and control, given the desire for
the English-speaking doctor to be in control of the
device and the primary "director" of the dialog.
The English ASR used the University of Colo-
rado Sonic recognizer, augmented primarily with
LM data collected from multiple sources, including
89
our own large-scale simulated doctor-patient dia-
logue corpus based on recordings of medical stu-
dents examining standardized patients (details in
Belvin et al 2004).
1
 The Farsi acoustic models r e-
quired an eclectic approach due to the lack of ex-
isting labeled speech corpora.  The approach
included borrowing acoustic data from English by
means of developing a sub-phonetic mapping be-
tween the two languages, as detailed in (Srini-
vasamurthy & Narayanan 2003), as well as use of
a small existing Farsi speech corpus (FARSDAT),
and our own team-internally generated acoustic
data.  Language modeling data was also obtained
from multiple sources.  The Defense Language
Institute translated approximately 600,000 words
of English medical dialogue data (including our
standardized patient data mentioned above), and in
addition, we were able to obtain usable Farsi text
from mining the web for electronic news sources.
Other  smaller  amounts of  training  data  were ob
tained from various sources, as detailed in  (Nara-
yanan et al 2003, 2004).  Additional detail on de-
velopment methods for all of these components,
system integration and evaluation can also be
found in the papers just cited.
The MT components, as noted, consist of both a
Classifier and a stochastic translation engine,  both
                                                           
1
 Standardized Patients are typically actors who have been
trained by doctors or nurses to portray symptoms of particular
illnesses or injuries.  They are used extensively in medical
education so that doctors in training don't have to "practice"
on real patients.
developed by USC-ISI team members.  The Eng-
lish Classifier uses approximately 1400 classes
consisting mostly of standard questions used by
medical care providers in medical interviews.
Each class has a large number of paraphrases asso-
ciated with it, such that if the care provider speaks
one of those phrases, the system will identify it
with the class and translate it to Farsi via table-
lookup.  If the Classifier cannot succeed in finding
a match exceeding a confidence threshold, the sto-
chastic MT engine will be employed.  The sto-
chastic MT engine relies on n-gram
correspondences between the source and target
languages.  As with ASR, the performance of the
component is highly dependent on very large
amounts of training data.  Again, there were multi-
ple sources of training data used, the most signifi-
cant being the data generated by our own team's
English collection effort, supported by translation
into Farsi by DLI. Further details of the MT com-
ponents can be found in Narayanan et al, op.cit.
3 Enabling Effective Communication
The approach taken in the development of Tran-
sonics was what can be referred to as the total
communication pathway.  We are not so concerned
with trying to maximize the performance of a
given component of the system, but rather with the
effectiveness of the system as a whole in facilitat-
ing actual communication.  To this end, our design
and development included the following:
MT
English to Farsi
Farsi to English
ASR
English
Prompts or TTS
Farsi
Prompts or TTS
English
ASR
Farsi
GUI:
prompts,
 confirmations,
 ASR switch
Dialog
Manager
SMT
English to Farsi
Farsi to English
Figure 1: Architecture of the Transonics system.  The Dialogue Manager acts as the hub through which the
individual components interact.
90
i. an "educated guess" capability (system
guessing at the meaning of an utterance) from the
Classifier translation mechanism?this proved very
useful for noisy ASR output, especially for the re-
stricted domain of medical interviews.
ii. a flexible and robust SMT good for filling in
where the more accurate Classifier misses.
iii. exploitation of a partial n-best list as part of
the GUI used by the doctor/medic for the English
ASR component and the Farsi-to-English transla-
tion component.
iv. a dialog manager which in essence occa-
sionally makes  "suggestions" (for next questions
for the doctor to ask) based on query sets which are
topically related to the query the system believes it
recognized the doctor to have spoken.
Overall, the system achieves a respectable level of
performance in terms of allowing users to follow a
conversational thread in a fairly coherent way, de-
spite the presence of frequent ungrammatical or
awkward translations (i.e. despite what we might
call non-catastrophic errors).
4 Testing and Evaluation
In addition to our own laboratory tests, the sys-
tem was evaluated by MITRE as part of the
DARPA program.  There were two parts to the
MITRE evaluations, a "live" part, designed pri-
marily to evaluate the overall task-oriented effec-
tiveness of the systems, and a "canned" part,
designed primarily to evaluate individual compo-
nents of the systems.
The live evaluation consisted of six medical
professionals (doctors, corpsmen and physician?s
assistants from the Naval Medical Center at Quan-
tico, and a nurse from a civilian institution) con-
ducting unrehearsed "focused history and physical
exam" style interactions with Farsi speakers play-
ing the role of patients, where the English-speaking
doctor and the Farsi-speaking patient communi-
cated by means of the Transonics system.  Since
the cases were common enough to be within the
realm of general internal medicine, there was no
attempt to align ailments with medical specializa-
tions among the medical professionals.
MITRE endeavored to find primarily monolin-
gual Farsi speakers to play the role of patient, so as
to provide a true test of the system to enable com-
munication between people who would otherwise
have no way to communicate.  This goal was only
partially realized, since one of the two Farsi patient
role-players was partially competent in English.
2
The Farsi-speaking role-players were trained by a
medical education specialist in how to simulate
symptoms of someone with particular injuries or
illnesses.  Each Farsi-speaking patient role-player
received approximately 30 minutes of training for
any given illness or injury.  The approach was
similar to that used in training standardized pa-
tients, mentioned above (footnote 1) in connection
with generation of the dialogue corpus.
MITRE established a number of their own met-
rics for measuring the success of the systems, as
well as using previously established metrics.  A
full discussion of these metrics and the results ob-
tained for the Transonics system is beyond the
scope of this paper, though we will note that one of
the most important of these was task-completion.
There were 5 significant facts (5 distinct facts for
each of 12 different scenarios) that the medical
professional should have discovered in the process
of interviewing/examining each Farsi patient.  The
USC/HRL system averaged 3 out of the 5 facts,
which was a slightly above-average score among
the 4 systems evaluated.  A "significant fact" con-
sisted of determining a fact which was critical for
diagnosis, such as the fact that the patient had been
injured in a fall down a stairway, the fact that the
patient was experiencing blurred vision, and so on.
Significant facts did not include items such as a
patient's age or marital status.
3
  We report on this
measure in that it is perhaps the single most im-
portant component in the assessment, in our opin-
ion, in that it is an indication of many aspects of
the system, including both directions of the trans-
lation system.  That is, the doctor will very likely
conclude correct findings only if his/her question is
translated correctly to the patient, and also if the
patient's answer is translated correctly for the doc-
tor.  In a true medical exam, the doctor may have
                                                           
2
 There were additional difficulties encountered as well, hav-
ing to do with one of the role-players not adequately grasping
the goal of role-playing.  This experience highlighted the
many challenges inherent in simulating domain-specific
spontaneous dialogue.
3
 Unfortunately, there was no baseline evaluation this could be
compared to,  such as assessing whether any of the critical
facts could be determined without the use of the system at all.
91
other means of determining some critical facts
even in the absence of verbal communication, but
in the role-playing scenario described, this is very
unlikely.  Although this measure is admittedly
coarse-grained, it simultaneously shows, in a crude
sense, that the USC/HRL system compared fa-
vorably against the other 3 systems in the evalua-
tion, and also that there is still significant room for
improvement in the state of the art.
As noted, MITRE devised a component evalua-
tion process also consisting of running 5 scripted
dialogs through the systems and then measuring
ASR and MT performance.  The two primary
component measures were a version of BLEU for
the MT component (modified slightly to handle the
much shorter sentences typical of this kind of dia-
log) and a standard Word-Error Rate for the ASR
output.  These scores are shown below.
Table 1:  Farsi BLEU Scores
IBM BLEU
ASR
IBM BLEU
TEXT
English to Farsi
0.2664 0.3059
Farsi  to English 0.2402 0.2935
The reason for the two different BLEU scores is
that one was calculated based on the ASR compo-
nent output being translated to the other language,
while the other was calculated from human tran-
scribed text being translated to the other language.
Table 2:  HRL/USC WER for Farsi and English
English Farsi
WER 11.5% 13.4%
5 Conclusion
In this paper we have given an overview of the
design, implementation and evaluation of the Tran-
sonics speech-to-speech translation system for nar-
row domain two-way translation.  Although there
are still many significant hurdles to be overcome
before this kind of technology can be called truly
robust, with appropriate training and two coopera-
tive interlocutors, we can now see some degree of
genuine communication being enabled.  And this is
very encouraging indeed.
6 Acknowledgements
This work was supported primarily by the DARPA
CAST/Babylon program, contract N66001-02-C-
6023.
References
R. Belvin, W. May, S. Narayanan, P. Georgiou, S. Gan-
javi.  2004. Creation of a Doctor-Patient Dialogue
Corpus Using Standardized Patients. In Proceedings of
the Language Resources and Evaluation Conference
(LREC), Lisbon, Portugal.
S. Ganjavi, P. G. Georgiou, and S. Narayanan. 2003.
Ascii based transcription schemes for languages with
the Arabic script: The case of Persian. In Proc. IEEE
ASRU,  St. Thomas, U.S. Virgin Islands.
S. Narayanan, S. Ananthakrishnan, R. Belvin, E. Ette-
laie, S. Ganjavi, P. Georgiou, C. Hein, S. Kadambe,
K. Knight, D. Marcu, H. Neely, N. Srinivasamurthy,
D. Traum and D. Wang.  2003. Transonics: A speech
to speech system for English-Persian Interactions,
Proc. IEEE ASRU,  St. Thomas, U.S. Virgin Islands.
S. Narayanan, S. Ananthakrishnan, R. Belvin, E. Ette-
laie, S. Gandhe, S. Ganjavi, P. G. Georgiou, C. M.
Hein, S. Kadambe, K. Knight, D. Marcu, H. E.
Neely, N. Srinivasamurthy, D. Traum, and D. Wang.
2004. The Transonics Spoken Dialogue Translator:
An aid for English-Persian Doctor-Patient interviews,
in Working Notes of the AAAI Fall symposium on
Dialogue Systems for Health Communication, pp 97-
-103.
N. Srinivasamurthy, and S. Narayanan. 2003. Language
adaptive Persian speech recognition. In proceedings
of Eurospeech 2003.
92
