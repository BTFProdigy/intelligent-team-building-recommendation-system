Information Extraction for Question Answering:
Improving Recall Through Syntactic Patterns
Valentin Jijkoun and Maarten de Rijke
Informatics Institute
University of Amsterdam
{jijkoun,mdr}@science.uva.nl
Jori Mur
Information Science
University of Groningen
mur@let.rug.nl
Abstract
We investigate the impact of the precision/recall
trade-off of information extraction on the per-
formance of an offline corpus-based question
answering (QA) system. One of our findings is
that, because of the robust final answer selection
mechanism of the QA system, recall is more im-
portant. We show that the recall of the extrac-
tion component can be improved using syntac-
tic parsing instead of more common surface text
patterns, substantially increasing the number of
factoid questions answered by the QA system.
1 Introduction
Current retrieval systems allow us to locate docu-
ments that might contain the pertinent information,
but most of them leave it to the user to extract the
useful information from a ranked list of documents.
Hence, the (often unwilling) user is left with a rel-
atively large amount of text to consume. There is
a need for tools that reduce the amount of text one
might have to read to obtain the desired informa-
tion. Corpus-based question answering is designed
to take a step closer to information retrieval rather
than document retrieval. The question answering
(QA) task is to find, in a large collection of data,
an answer to a question posed in natural language.
One particular QA strategy that has proved suc-
cessful on large collections uses surface patterns de-
rived from the question to identify answers. For ex-
ample, for questions like When was Gandhi born?,
typical phrases containing the answer are Gandhi
was born in 1869 and Gandhi (1869?1948). These
examples suggest that text patterns such as ?name
was born in birth date? and ?name (birth
year?death year)? formulated as regular ex-
pressions, can be used to select the answer phrase.
Similarly, such lexical or lexico-syntactic pat-
terns can be used to extract specific information on
semantic relations from a corpus offline, before ac-
tual questions are known, and store it in a repository
for quick and easy access. This strategy allows one
to handle some frequent question types: Who is. . . ,
Where is. . . , What is the capital of. . . etc. (Fleis-
chman et al, 2003; Jijkoun et al, 2003).
A great deal of work has addressed the problem
of extracting semantic relations from unstructured
text. Building on this, much recent work in QA
has focused on systems that extract answers from
large bodies of text using simple lexico-syntactic
patterns. These studies indicate two distinct prob-
lems associated with using patterns to extract se-
mantic information from text. First, the patterns
yield only a small subset of the information that may
be present in a text (the recall problem). Second, a
fraction of the information that the patterns yield is
unreliable (the precision problem). The precision of
the extracted information can be improved signif-
icantly by using machine learning methods to filter
out noise (Fleischman et al, 2003). The recall prob-
lem is usually addressed by increasing the amount
of text data for extraction (taking larger collections
(Fleischman et al, 2003)) or by developing more
surface patterns (Soubbotin and Soubbotin, 2002).
Some previous studies indicate that in the setting
of an end-to-end state-of-the-art QA system, with
additional answer finding strategies, sanity check-
ing, and statistical candidate answer re-ranking, re-
call is more of a problem than precision (Bernardi et
al., 2003; Jijkoun et al, 2003): it often seems use-
ful to have more data rather than better data. The
aim of this paper is to address the recall problem
by using extraction methods that are linguistically
more sophisticated than surface pattern matching.
Specifically, we use dependency parsing to extract
syntactic relations between entities in a text, which
are not necessarily adjacent on the surface level. A
small set of hand-built syntactic patterns allows us
to detect relevant semantic information. A com-
parison of the parsing-based approach to a surface-
pattern-based method on a set of TREC questions
about persons shows a substantial improvement in
the amount of the extracted information and num-
ber of correctly answered questions.
In our experiments we tried to understand
whether linguistically involved methods such as
parsing can be beneficial for information extraction,
where rather shallow techniques are traditionally
employed, and whether the abstraction from surface
to syntactic structure of the text does indeed help to
find more information, at the same time avoiding the
time-consuming manual development of increasing
numbers of surface patterns.
The remainder of the paper is organized as fol-
lows. In Section 2 we discuss related work on
extracting semantic information. We describe our
main research questions and experimental setting in
Section 3. Then, in Section 4 we provide details
on the extraction methods used (surface and syntac-
tic). Sections 5 and 6 contain a description of our
experiments and results, and an error analysis, re-
spectively. We conclude in Section 7.
2 Related Work
There is a large body of work on extracting seman-
tic information using lexical patterns. Hearst (1992)
explored the use of lexical patterns for extracting
hyponym relations, with patterns such as ?such as.?
Berland and Charniak (1999) extract ?part-of? rela-
tions. Mann (2002) describes a method for extract-
ing instances from text by means of part-of-speech
patterns involving proper nouns.
The use of lexical patterns to identify answers in
corpus-based QA received lots of attention after a
team taking part in one of the earlier QA Tracks
at TREC showed that the approach was competi-
tive at that stage (Soubbotin and Soubbotin, 2002;
Ravichandran and Hovy, 2002). Different aspects of
pattern-based methods have been investigated since.
E.g., Ravichandran et al (2003) collect surface pat-
terns automatically in an unsupervised fashion us-
ing a collection of trivia question and answer pairs
as seeds. These patterns are then used to generate
and assess answer candidates for a statistical QA
system. Fleischman et al (2003) focus on the preci-
sion of the information extracted using simple part-
of-speech patterns. They describe a machine learn-
ing method for removing noise in the collected data
and showed that the QA system based on this ap-
proach outperforms an earlier state-of-the-art sys-
tem. Similarly, Bernardi et al (2003) combine the
extraction of surface text patterns with WordNet-
based filtering of name-apposition pairs to increase
precision, but found that it hurt recall more than it
helped precision, resulting in fewer questions an-
swered correctly when the extracted information is
deployed for QA.
The application of deeper NLP methods has also
received much attention in the QA community. The
open-domain QA system by LCC (Moldovan et al,
2002) uses predicate-argument relations and lexical
chaining to actually prove that a text snippet pro-
vides an answer to a question. Katz and Lin (2003)
use syntactic dependency parsing to extract rela-
tions between words, and use these relations rather
than individual words to retrieve sentences relevant
to a question. They report a substantial improve-
ment for certain types of questions for which the
usual term-based retrieval performs quite poorly,
but argue that deeper text analysis methods should
be applied with care.
3 Experimental Setting
We set up experiments to address two related issues.
First, we wanted to understand how the usual pre-
cision/recall trade-off shows up in off-line corpus-
based QA, and specifically, whether extracting more
data of lower quality (i.e., favoring recall) gives
a QA system a better performance than extracting
smaller amounts of more accurate data (i.e., favor-
ing precision). Second, we tried to verify the hy-
pothesis that syntactic parsing for information ex-
traction does increase the extraction recall by iden-
tifying relations between entities not adjacent on the
surface layer but connected syntactically.
There are different approaches to the evaluation
of information extraction modules. The usual recall
and precision metrics (e.g., how many of the inter-
esting bits of information were detected, and how
many of the found bits were actually correct) require
either a test corpus previously annotated with the
required information, or manual evaluation (Fleis-
chman et al, 2003). Although intrinsic evaluation
of an IE module is important, we were mainly inter-
ested in measuring the performance of this module
in context, that is, working as a sub-part of a QA
system. We used the number of questions answered
correctly as our main performance indicator.
3.1 QA System
For the experiments described below we used an
open-domain corpus-based QA system QUARTZ
(Jijkoun et al, 2004). The system implements
a multi-stream approach, where several different
strategies are used in parallel to find possible an-
swers to a question. We ran the system turning on
only one stream, Table Lookup, which implements
an off-line strategy for QA.
The Table Lookup stream uses a number of
knowledge bases created by pre-processing a doc-
ument collection. Currently, QUARTZ? knowledge
bases include 14 semi-structured tables containing
various kinds of information: birth dates of persons,
dates of events, geographical locations of different
objects, capitals and currencies of countries, etc. All
this information is extracted from the corpus off-
line, before actual questions are known.
An incoming question is analyzed and assigned
to one of 37 predefined question types. Based on
the question type, the Table Lookup stream identi-
fies knowledge bases where answers to the question
can potentially be found. The stream uses keywords
from the question to identify relevant entries in the
selected knowledge bases and extracts candidate an-
swers. Finally, the QA system reranks and sanity
checks the candidates and selects the final answer.
3.2 Questions and Corpus
To get a clear picture of the impact of using dif-
ferent information extraction methods for the off-
line construction of knowledge bases, similarly to
(Fleischman et al, 2003), we focused only on
questions about persons, taken from the TREC-
8 through TREC 2003 question sets. The ques-
tions we looked at were of two different types:
person identification (e.g., 2301. What composer
wrote ?Die Go?tterda?mmerung??) and person defi-
nition (e.g., 959. Who was Abraham Lincoln?). The
knowledge base relevant for answering questions of
these types is a table with several fields containing
a person name, an information bit about the per-
son (e.g., occupation, position, activities), the con-
fidence value assigned by the extraction modules
to this information bit (based on its frequency and
the reliability of the patterns used for extraction),
and the source document identification. The Table
Lookup finds the entries whose relevant fields best
match the keywords from the question.
We performed our experiments with the 336
TREC questions about persons that are known to
have at least one answer in the collection. The
collection used at TREC 8, 9 and 10 (referred to
as TREC-8 in the rest of the paper) consists of
1,727,783 documents, with 239 of the correspond-
ing questions identified by our system as asking
about persons. The collection used at TREC 2002
and 2003 (AQUAINT) contains 1,033,461 docu-
ments and 97 of the questions for these editions of
TREC are person questions.
4 Extraction of Role Information
In this section we describe the two extraction meth-
ods we used to create knowledge bases containing
information about persons: extraction using surface
text patterns and using syntactic patterns.
Clearly, the performance of an information ex-
traction module depends on the set of language phe-
nomena or patterns covered, but this relation is not
straightforward: having more patterns allows one to
find more information, and thus increases recall, but
it might introduce additional noise that hurts preci-
sion. Since in our experiments we aimed at com-
paring extraction modules based on surface text vs.
syntactic patterns, we tried to keep these two mod-
ules parallel in terms of the phenomena covered.
First, the collections were tagged with a Named
Entity tagger based on TnT (TnT, 2003) and trained
on CoNLL data (CoNLL, 2003). The Named Entity
tagger was used mainly to identify person names as
separate entities. Although the tagging itself was
not perfect, we found it useful for restricting our
surface text patterns.
Below we describe the two extraction methods.
4.1 Extraction with Surface Text Patterns
To extract information about roles, we used the set
of surface patterns originally developed for the QA
system we used at TREC 2003 (Jijkoun et al, 2004).
The patterns are listed in Table 1.
In these patterns, person is a phrase that is
tagged as person by the Named Entity tagger, role
is a word from a list of roles extracted from the
WordNet (all hyponyms of the word ?person,? 15703
entries),1 role-verb is from a manually con-
structed list of ?important? verbs (discovered, in-
vented, etc.; 48 entries), leader is a phrase identify-
ing leadership from a manually created list of lead-
ers (president, minister, etc.; 22 entries). Finally,
superlat is the superlative form of an adjective
and location is a phrase tagged as location by
the Named Entity tagger.
4.2 Extraction with Syntactic Patterns
To use the syntactic structure of sentences for role
information extraction, the collections were parsed
with Minipar (Lin, 1998), a broad coverage depen-
dency parser for English. Minipar is reported to
achieve 88% precision and 80% recall with respect
to dependency relations when evaluated on the SU-
SANNE corpus. We found that it performed well
on the newpaper and newswire texts of our collec-
tions and was fairly robust to fragmented and not
well-formed sentences frequent in this domain. Be-
fore extraction, Minipar?s output was cleaned and
made more compact. For example, we removed
some empty nodes in the dependency parse to re-
solve non-local dependencies. While not loosing
any important information, this made parses easier
to analyse when developing patterns for extraction.
Table 2 lists the patterns that were used to ex-
tract information about persons; we show syntactic
dependencies as arrows from dependents to heads,
with Minipar?s dependency labels above the arrows.
As with the earlier surface patterns, role is one
of the nouns in the list of roles (hyponyms of person
1The list of roles is used to increase precision by filtering
out snippets that may not be about roles; in some of the experi-
ments below, we turn this filtering mechanism off.
Pattern Example
... role, person The British actress, Emma Thompson
... (superlat|first|last)..., person The first man to set foot on the moon, Armstrong
person,... role... Audrey Hepburn, goodwill ambassador for UNICEF.
person,... (superlat|first|last)... Brown, Democrats? first black chairman.
person,... role-verb... Christopher Columbus, who discovered America,
... role person District Attoney Gil Garcetti
role... person The captain of the Titanic Edward John Smith
person,... leader... location Tony Blair, the prime minister of England
location... leader, person The British foreign secretary , Jack Straw
Table 1: Surface patterns.
Pattern Example
Apposition person appo????role a major developer, Joseph Beard
Apposition person appo????role Jerry Lewis, a Republican congressman
Clause person subj????role-verb Bell invented the telephone
Person person person????role Vice President Al Gore
Nominal modifier person nn????role businessman Bill Shockley
Subject person subj????role Alvarado was chancellor from 1983 to 1984
Conjunction person conj????role Fu Wanzhong, director of the Provincial Department of Foreign Trade
(this is a frequent parsing error)
Table 2: Syntactic patterns.
in WordNet), role-verb is one of the ?important
verbs.? The only restriction for person was that it
should contain a proper noun.
When an occurence of a pattern was found in
a parsed sentence, the relation (person; info-
bit) was extracted, where info-bit is a se-
quence of all words below role or role-verb
in the dependency graph (i.e., all dependents along
with their dependents etc.), excluding the per-
son. For example, for the sentence Jane Goodall,
an expert on chimps, says that evidence for so-
phisticated mental performances by apes has be-
come ever more convincing, that matches the pat-
tern person appo????role, the extracted informa-
tion was (Jane Goodall; an expert on chimps).
5 Experiments and Results
We ran both surface pattern and syntactic pattern
extraction modules on the two collections, with a
switch for role filtering. The performance of the Ta-
ble Lookup stream of our QA system was then eval-
uated on the 336 role questions using the answer
patterns provided by the TREC organizers. An early
error analysis showed that many of the incorrect
answers were due to the table lookup process (see
Section 3) rather than the information extraction
method itself: correct answers were in the tables,
but the lookup mechanism failed to find them or
picked up other, irrelevant bits of information. Since
we were interested in evaluating the two extraction
methods rather than the lookup mechanism, we per-
formed another experiment: we reduced the sizes
of the collections to simplify the automatic lookup.
For each TREC question with an answer in the col-
lection, NIST provides a list of documents that are
known to contain an answer to this question. We put
together the document lists for all questions, which
left us with much smaller sub-collections (16.4 MB
for the questions for the TREC-8 collection and 3.2
MB for the AQUAINT collection). Then, we ran the
two extraction modules on these small collections
and evaluated the performance of the QA system on
the resulting tables. All the results reported below
were obtained with these sub-collections. Compari-
son of the extraction modules on the full TREC col-
lections gave very similar relative results.
Table 3 gives the results of the different runs for
the syntactic pattern extraction and the surface pat-
tern extraction on the TREC-8 collection: the num-
ber of correct answers (in the top one and the top
three answer candidates) for the 239 person ques-
tions. The columns labeled Roles+ show the results
for the extraction modules using the list of possible
roles from WordNet (Section 4), and the columns la-
beled Roles ? show the results when the extraction
modules consider any word as possibly denoting a
role. The results of the runs on the AQUAINT col-
lection with 97 questions are shown in Table 4.
The syntactic pattern module without role filter-
ing scored best of all, with more than a third of the
Syntactic patterns Surface patterns
Rank Roles ? Roles + Roles ? Roles +
1 80 (34%) 73 (31%) 59 (25%) 54 (23%)
1?3 90 (38%) 79 (33%) 68 (29%) 59 (25%)
Table 3: Correct answers for the TREC-8 collection
(239 questions).
Syntactic patterns Surface patterns
Rank Roles ? Roles + Roles ? Roles +
1 16 (17%) 14 (14%) 9 (9%) 6 (6%)
1?3 20 (21%) 14 (14%) 11 (11%) 6 (6%)
Table 4: Correct answers for the AQUAINT collec-
tion (97 questions).
questions answered correctly for the TREC-8 col-
lection. Another interesting observation is that in all
experiments the modules based on syntactic patterns
outperformed the surface-text-based extraction.
Furthermore, there is a striking difference be-
tween the results in Table 3 (questions from
TREC 8, 9 and 10) and the results in Table 4
(questions from TREC 2002 and 2003). The ques-
tions from the more recent editions of TREC are
known to be much harder: indeed, the Table Lookup
stream answers only 21% of the questions from
TREC 2002 and 2003, vs. 38% for earlier TRECs.
In all experiments, both for syntactic and surface
patterns, using the list of roles as a filtering mecha-
nism decreases the number of correct answers. Us-
ing lexical information from WordNet improves the
precision of the extraction modules less than it hurts
the recall. Moreover, in the context of our knowl-
edge base lookup mechanism, low precision of the
extracted information does not seem to be an ob-
stacle: the irrelevant information that gets into the
tables is either never asked for or filtered out during
the final sanity check and answer selection stage.
This confirms the conclusions of (Bernardi et al,
2003): in this specific task having more data seems
to be more useful than having better data.
To illustrate the interplay between the precision
and recall of the extraction module and the perfor-
mance of the QA system, Table 5 gives the com-
parison of the different extraction mechanisms (syn-
tactic and surface patterns, using or not using the
list of roles for filtering). The row labelled # facts
shows the size of the created knowledge base, i.e.,
the number of entries of the form (person, info), ex-
tracted by each method. The row labelled Preci-
sion shows the precision of the extracted informa-
tion (i.e., how many entries are correct, according to
a human annotator) estimated by random sampling
and manual evaluation of 1% of the data for each ta-
ble, similar to (Fleischman et al, 2003). The row la-
belled Corr. answers gives the number of questions
correctly answered using the extracted information.
Syntactic patterns Surface patterns
Roles ? Roles + Roles ? Roles +
# facts 29890 9830 28803 6028
Precision 54% 61% 23% 68%
Corr. answers 34% 31% 25% 23%
Table 5: Comparison of the tables built with differ-
ent extraction methods on the TREC-8 collection.
The results in Table 5 indicate that role filtering af-
fects the syntactic and surfaces modules quite dif-
ferently. Filtering seems almost essential for the
surface-pattern-based extraction, as it increases the
precision from 23% to 68%. This confirms the re-
sults of Fleischman et al (2003): shallow methods
may benefit significantly from the post-processing.
On the other hand, the precision improvement for
the syntactic module is modest: from 54% to 61%.
The data from the syntactic module contains
much less noise, although the sizes of the extracted
tables before role filtering are almost the same. Af-
ter filtering, the number of valid entries from the
syntactic module (i.e., the table size multiplied by
the estimated precision) is about 6000. This is sub-
stantially better than the recall of the surface module
(about 4100 valid entries).
6 Error Analysis
In theory, all relatively simple facts extracted by the
surface pattern module should also be extracted by
the syntactic pattern module. Moreover, the syn-
tactic patterns should extract more facts, especially
ones whose structure deviates from the patterns pre-
defined in the surface pattern module, e.g., where
elements adjacent in the syntactic parse tree are far
apart on the surface level. To better understand the
differences between the two extraction approaches
and to verify the conjecture that syntactic parsing
does indeed increase the recall of the extracted in-
formation, we performed a further (manual) error
analysis, identifying questions that were answered
with one extraction method but not with the other.
Tables 6 and 7 gives the breakdown of the per-
formance of the two modules, again in terms of the
questions answered correctly. We show the results
for the 239 questions on the TREC-8 collection; for
the 97 questions on the AQUAINT corpus the rela-
tive scores are similar. As Tables 6 and 7 indicate,
not all questions answered by the surface pattern
module were also answered by the syntactic pattern
module, contrary to our expectations. We took a
closer look at the questions for which the two mod-
ules performed differently.
Syntactic patterns
Su
rfa
ce
pa
tte
rn
s correct incorrect
correct 47 12
incorrect 32 148
Table 6: Performance analysis for the TREC-8 col-
lection with role filtering.
Syntactic patterns
Su
rfa
ce
pa
tte
rn
s correct incorrect
correct 51 17
incorrect 39 132
Table 7: Performance analysis for the TREC-8 col-
lection without role filtering.
6.1 Syntactic Patterns vs. Surface Patterns
There were three types of errors responsible for pro-
ducing an incorrect answer by the syntactic pattern
module for questions correctly answered with sur-
face patterns. The most frequent errors were pars-
ing errors. For 6 out of 12 questions (see Table 6)
the answer was not extracted by the syntactic pat-
tern method, because the sentences containing the
answers were not parsed correctly. The next most
frequent error was caused by the table lookup pro-
cess. For 4 questions out of the 12, the required
information was extracted but simply not selected
from the table as the answer due to a failure of the
lookup algorithm. The remaining errors (2 out of
12) were of a different type: for these 2 cases the
surface pattern extraction did perform better than
the syntactic method. In both cases this was because
of wildcards allowed in the surface patterns. E.g.,
for the sentence . . . aviator Charles Lindbergh mar-
ried Anne Spencer Morrow. . . the syntactic pattern
method extracted only the relation
(Charles Lindbergh; aviator),
whereas the surface pattern method also extracted
(Anne Spencer Morrow; aviator Charles Lindbergh
married),
because of the pattern ?role. . . person? with
role instantiated with aviator and person with
Anne Spencer Morrow. In fact, the extracted in-
formation is not even correct, because Anne is not
an aviator but Lindbergh?s wife. However, due to
the fuzzy nature of the lookup mechanism, this new
entry in the knowledge base allows the QA sys-
tem to answer correctly the question 646. Who was
Charles Lindbergh?s wife?, which is not answered
with the syntactic pattern extraction module.
To summarize, of the 12 questions where the sur-
face patterns outperformed the syntactic patterns
? 6 questions were not answered by the syntactic
method due to parsing errors,
? 4 were not answered because of the table
lookup failure and
? for 2 the surface-based method was more ap-
propriate.
6.2 Surface Patterns vs. Syntactic Patterns
We also took a closer look at the 32 questions for
which the syntactic extraction performed better than
the surface patterns (see Table 6). For the sur-
face pattern extraction module there were also three
types of errors. First, some patterns were miss-
ing, e.g., person role-verb.... The only
difference from one of the actually used patterns
(person,... role-verb...) is that there
is no comma between person and role-verb.
This type of incompleteness of the set of the surface
patterns was the cause for 16 errors out of 32.
The second class of errors was caused by the
Named Entity tagger. E.g., Abraham Lincoln was
always tagged as location, so the name never
matched any of the surface patterns. Out of 32 ques-
tions, 10 were answered incorrectly for this reason.
Finally, for 6 questions out of 32, the syntactic
extraction performed better because the information
could not be captured on the surface level. For ex-
ample, the surface pattern module did not extract
the fact that Oswald killed Kennedy from the sen-
tence . . . when Lee Harvey Oswald allegedly shot
and killed President John F. Kennedy. . . , because
none of the patterns matched. Indeed, Lee Harvey
Oswald and the potentially interesting verb killed
are quite far apart in the text, but there is an imme-
diate relation (subject) on the syntactic level.
It is worth pointing out that there were no lookup
errors for the surface pattern method, even though
it used the exact same lookup mechanism as the
approach based on syntactic patterns (that did ex-
perience various lookup errors, as we have seen).
It seems that the increased recall of the syntactic
pattern approach caused problems by making the
lookup process harder.
To summarize, out of 32 questions answered us-
ing syntactic extraction method but not by the sur-
face pattern approach
? 16 questions would have required extending
the set of surface patterns,
? 10 questions were not answered because of NE
tagging error, and
? 6 questions required syntactic analysis for ex-
traction of the relevant information.
6.3 Adding Patterns?
We briefly return to a problem noted for extrac-
tion based on surface patterns: the absence of cer-
tain surface patterns. The surface pattern person
role-verb... was not added because, we felt,
it would introduce too much noise in the knowledge
base. With dependency parsing this is not an is-
sue as we can require that person is the subject
of role-verb. So in this case the syntactic pat-
tern module has a clear advantage. More generally,
while we believe that extraction methods based on
hand-crafted patterns are necessarily incomplete (in
that they will fail to extract certain relevant facts),
these observations suggest that coping with the in-
completeness is a more serious problem for the sur-
face patterns than for the syntactic ones.
7 Conclusions
We described a set of experiments aimed at com-
paring different information extraction methods in
the context of off-line corpus-based Question An-
swering. Our main finding is that a linguistically
deeper method, based on dependency parsing and a
small number of simple syntactic patterns, allows an
off-line QA system to correctly answer substantially
more questions than a traditional method based on
surface text patterns. Although the syntactic method
showed lower precision of the extracted facts (61%
vs. 68%), in spite of parsing errors the recall was
higher than that of the surface-based method, judg-
ing by the number of correctly answered questions
(31% vs. 23%). Thus, the syntactic analysis can in
fact be considered as another, intensive way of im-
proving the recall of information extraction, in ad-
dition to successfully used extensive ways, such as
developing larger numbers of surface patterns or in-
creasing the size of the collection.
Moreover, we confirmed the claim that for a com-
plex off-line QA system, with statistical as well as
knowledge-intensive sanity checking answer selec-
tion modules, recall of the information extraction
module is more important than precision, and a sim-
ple WordNet-based method for improving precision
does not help QA. In our future work we plan to in-
vestigate the effect of more sophisticated and, prob-
ably, more accurate filtering methods (Fleischman
et al, 2003) on the QA results.
8 Acknowledgements
Valentin Jijkoun and Maarten de Rijke were sup-
ported by a grant from the Netherlands Organiza-
tion for Scientific Research (NWO) under project
number 220-80-001. De Rijke was also sup-
ported by NWO under project numbers 365-20-
005, 612.069.006, 612.000.106, 612.000.207, and
612.066.302.
References
M. Berland and E. Charniak. 1999. Finding parts in
very large corpora. In Proceedings of the 37th Annual
Meeting of the ACL.
R. Bernardi, V. Jijkoun, G. Mishne, and M. de Rijke.
2003. Selectively using linguistic resources through-
out the question answering pipeline. In Proceedings
of the 2nd CoLogNET-ElsNET Symposium.
M. Fleischman, E. Hovy, and A. Echihabi. 2003. Offline
strategies for online question answering: answering
questions before they are asked. In Proceedings of the
41st Annual Meeting of the ACL.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the 14th
International Conference on Computational Linguis-
tics (COLING-92).
V. Jijkoun, G. Mishne, and M. de Rijke. 2003. Prepro-
cessing Documents to Answer Dutch Questions. In
Proceedings of the 15th Belgian-Dutch Conference on
Artificial Intelligence (BNAIC?03).
V. Jijkoun, G. Mishne, C. Monz, M. de Rijke,
S. Schlobach, and O. Tsur. 2004. The University of
Amsterdam at the TREC 2003 Question Answering
Track. In Proceedings of the TREC-2003 Conference.
B. Katz and J. Lin. 2003. Selectively using relations to
improve precision in question answering. In Proceed-
ings of the EACL-2003 Workshop on Natural Lan-
guage Processing for Question Answering.
D. Lin. 1998. Dependency-based evaluation of Minipar.
In Proceedings of the Workshop on the Evaluation of
Parsing Systems.
G. Mann. 2002. Fine-grained proper noun ontologies
for question answering. In SemaNet?02: Building and
Using Semantic Networks.
D. Moldovan, S. Harabagiu, R. Girju, P. Morarescu,
A. Novischi F. Lacatusu, A. Badulescu, and O. Bolo-
han. 2002. LCC tools for question answering. In Pro-
ceedings of the TREC-2002.
TnT Statistical Part of Speech Tagging. 2003.
URL: http://www.coli.uni-sb.de/
?thorsten/tnt/.
CoNLL: Conference on Natural Language Learn-
ing. 2003. URL: http://cnts.uia.ac.be/
signll/shared.html.
D. Ravichandran and E. Hovy. 2002. Learning surface
text patterns for a question answering system. In Pro-
ceedings of the 40th Annual Meeting of the ACL.
D. Ravichandran, A. Ittycheriah, and S. Roukos. 2003.
Automatic derivation of surface text patterns for a
maximum entropy based question answering system.
In Proceedings of the HLT-NAACL Conference.
M.M. Soubbotin and S.M. Soubbotin. 2002. Use of pat-
terns for detection of likely answer strings: A system-
atic approach. In Proceedings of the TREC-2002 Con-
ference.
Why Are They Excited?
Identifying and Explaining Spikes in Blog Mood Levels
Krisztian Balog Gilad Mishne Maarten de Rijke
ISLA, University of Amsterdam
Kruislaan 403, 1098 SJ Amsterdam
kbalog,gilad,mdr@science.uva.nl
Abstract
We describe a method for discovering ir-
regularities in temporal mood patterns ap-
pearing in a large corpus of blog posts,
and labeling them with a natural language
explanation. Simple techniques based
on comparing corpus frequencies, coupled
with large quantities of data, are shown to
be effective for identifying the events un-
derlying changes in global moods.
1 Introduction
Blogs, diary-like web pages containing highly
opinionated personal commentary, are becoming
increasingly popular. This new type of media of-
fers a unique look into people?s reactions and feel-
ings towards current events, for a number of rea-
sons. First, blogs are frequently updated, and like
other forms of diaries are typically closely linked
to ongoing events in the blogger?s life. Second, the
blog contents tend to be unmoderated and subjec-
tive, more so than mainstream media?expressing
opinions, thoughts, and feeling. Finally, the large
amount of blogs enables aggregation of thousands
of opinions expressed every minute; this aggrega-
tion allows abstractions of the data, cleaning out
noise and focusing on the main issues.
Many blog authoring environments allow blog-
gers to tag their entries with highly individual (and
personal) features. Users of LiveJournal, one of
the largest weblog communities, have the option
of reporting their mood at the time of the post;
users can either select a mood from a predefined
list of common moods such as ?amused? or ?an-
gry,? or enter free-text. A large percentage of Live-
Journal users tag their postings with a mood. This
results in a stream of hundreds of weblog posts
tagged with mood information per minute, from
hundreds of thousands of users across the globe.
The collection of such mood reports from many
bloggers gives an aggregate mood of the blogo-
sphere for each point in time: the popularity of
different moods among bloggers at that time.
In previous work, we introduced a tool for
tracking the aggregate mood of the blogosphere,
and showed how it reflects global events (Mishne
and de Rijke, 2006a). The tool?s output includes
graphs showing the popularity of moods in blog
posts during a given interval; e.g., Figure 1 plots
the mood level for ?scared? during a 10 day pe-
riod. While such graphs reflect some expected
patterns (e.g., an increase in ?scared? around Hal-
loween in Figure 1), we have also witnessed spikes
and drops for which no associated event was
Figure 1: Blog posts labeled ?scared? during the October 26?
November 5, 2005 interval. The dotted (black) curve indi-
cates the absolute number of posts labeled ?scared,? while
the solid (red) curve shows the rate of change.
known to us. In this paper, we address this is-
sue: we seek algorithms for identifying unusual
changes in mood levels and explaining the under-
lying reasons for these changes. By ?explanation?
we mean a short snippet of text that describes the
event that caused the unusual mood change.
To produce such explanations, we proceed as
follows. If unusual spikes occur in the level of
mood m, we examine the language used in blog
posts labeled with m around and during the pe-
riod in which the spike occurs. We interpret words
207
that are not expected given a long-term language
model for m as signals for the spike in m?s level.
To operationalize the idea of ?unexpected words?
for a given mood, we use standard methods for
corpus comparison; once identified, we use the
?unexpected words? to consult a news corpus from
which we retrieve a small text snippet that we then
return as the desired explanation.
In Section 2 we briefly discuss related work.
Then, we detail how we detect spikes in mood lev-
els (in Section 3) and how we generate natural lan-
guage explanations for such spikes (in Section 4).
Experimental results are presented in Section 5,
and in Section 6 we present our conclusions.
2 Related work
As to burstiness phenomena in web data, Klein-
berg (2002) targets email and research papers, try-
ing to identify sharp rises in word frequencies in
document streams. Bursts can be found by search-
ing periods when a given word tends to appear at
unusually short intervals. Kumar et al (2003) ex-
tend Kleinberg?s algorithm to discover dense pe-
riods of ?bursty? intra-community link creation in
the blogspace, while Nanno et al (2004) extend it
to work on blogs. We use a simple comparison be-
tween long-term and short-term language models
associated with a given mood to identify unusual
word usage patterns.
Recent years have witnessed an increase in re-
search on extracting subjective and other non-
factual aspects of textual content; see (Shanahan et
al., 2005) for an overview. Much work in this area
focuses on recognizing and/or annotating evalu-
ative textual expressions. In contrast, work that
explores mood annotations is relatively scarce.
Mishne (2005) reports on text mining experiments
aimed at automatically tagging blog posts with
moods. Mishne and de Rijke (2006a) lift this work
to the aggregate level, and use natural language
processing and machine learning to estimate ag-
gregate mood levels from the text of blog entries.
3 Detecting spikes
Our first task is to identify spikes in moods re-
ported in blog posts. Many of the moods reported
by LiveJournal users display a cyclic behavior.
There are some obvious moods with a daily cycle.
For instance, people feel awake in the mornings
and tired in the evening (Figure 2). Other moods
show a weekly cycle. For instance, people drink
more at the weekends (Figure 3).
Figure 2: Daily cycles for ?awake? and ?tired.?
Figure 3: Weekend cycles for ?drunk.?
Our idea of detecting spikes tries to deal with
these cyclic events and aims at finding global
changes. Let POSTS (mood, date, hour) be the
number of posts labelled with a given mood and
created within a one-hour interval at the speci-
fied date. Similarly, ALLPOSTS (date, hour) is
the number of all posts created within the interval
specified by the date and hour. The ratio of posts
labeled with a given mood to all posts could be
expressed for all days of a week (Sunday, . . . , Sat-
urday) and for all one-hour intervals (0, . . . , 23)
using the formula:
R(mood, day, hour) =
?
DW (date)=day POSTS (mood, date, hour)
?
DW (date)=day ALLPOSTS (date, hour)
,
where day = 0, . . . , 6 and DW (date) is a day-of-
the-week function that returns 0, . . . , 6 depending
on the date argument.
The level of a given mood is changed within
a one-hour interval of a day, if the ratio of posts
labelled with that mood to all posts, created within
the interval, is significantly different from the ratio
that has been observed on the same hour of the
similar day of the week. Formally:
D(mood, date, hour) =
POSTS(mood,date,hour)
ALLPOSTS(date,hour)
R(mood,DW (date), hour) .
If |D| (the absolute value of D) exceeds a thresh-
old we conclude that a spike has occurred, while
208
the sign of D makes it possible to distinguish be-
tween positive and negative spikes. The absolute
value of D expresses the degree of the peak.
This method of identifying spikes allows us to
look at a period of a few hours instead of only
one, which is an effective smoothing method, es-
pecially if a sufficient number of posts cannot be
observed for a given mood.
4 Explaining peaks
Our next task is to explain the peaks identified by
the methods listed previously. We proceed in two
steps. First, we discover features in the peaking
interval which display a significantly different lan-
guage usage from that found in the general lan-
guage associated with the mood. Then we form
queries using these ?overused? words as well as
the date(s) of the peaking interval and run these as
queries against a news corpus.
4.1 Overused words To discover the reasons
underlying mood changes we use corpus-based
techniques to identify changes in language usage.
We compare two corpora: (1) the full set of blog
posts, referred to as the standard corpus, and (2) a
corpus associated with the peaking interval, re-
ferred to as the sample corpus.
To compare word frequencies across the two
corpora we apply the log-likelihood statistical
test (Dunning, 1993). Let Oi be the observed
frequency of a term, Ni its total frequency, and
Ei = (Ni ?
?
i Oi)/
?
i Ni its expected frequency
in corpus i (where i takes values 1 and 2 for the
standard and sample corpus, respectively). Then,
the log-likelihood value is calculated according to
this formula: ?2 ln? = 2?i Oi ln
(
Oi
Ei
)
.
4.2 Finding explanations Given the start and
end dates of a peaking interval and a list of
overused words from this period, a query is
formed. This query is then submitted to (head-
lines of) a news corpus. A headline is retrieved if
it contains at least one of the overused words and
is dated within the peaking interval or the day be-
fore the beginning of the peak. The hits are ranked
based on the number of overused terms contained
in the headline.
5 Experiments
In this section we illustrate our methods with some
examples and provide a preliminary analysis of
their effectiveness.
5.1 The blog corpus Our corpus consists of
all public blogs published in LiveJournal during
a 90 day period from July 5 to October 2, 2005,
adding up to a total of 19 million blog posts. For
each entry, the text of the post along with the date
and time are indexed. Posts without an explicit
mood indication (10M) are discarded. We applied
standard preprocessing steps (stopword removal,
stemming) to the text of blog posts.
5.2 The news corpus The collection con-
tains around 1000 news headlines that have
been published in Wikinews (http://www.
wikinews.org) during the period of July-
September, 2005.
5.3 Case studies We present three particular
cases where an irregular behavior in a certain
mood could be observed. We examine how accu-
rately the overused terms describe the events that
caused the spikes.
5.3.1 Harry Potter In July, 2005, a peak in
?excited? was discovered; see Figure 4, where the
shaded (green) area indicates the ?peak area.?
Figure 4: Peak in ?excited? around July 16, 2005.
Step 1 of our peak explanation method (Sec-
tion 4) reveals the following overused terms dur-
ing the peak period: ?potter,? ?book,? ?excit,?
?hbp,? ?read,? ?princ,? ?midnight.? Step 2 of
our peak explanation method (Section 4) exploits
these words to retrieve the following headline
from the news collection: ?July 16. Harry Potter
and the Half-Blood Prince released.?
5.3.2 Hurricane Katrina Our next exam-
ple illustrates the need for careful thresholding
when defining peaks (see Section 3). We show
peaks in ?worried? discovered around late Au-
gust, with a 40% and 80% threshold. Clearly, far
more peaks are identified with the lower threshold,
while the peaks identified in the bottom plot (with
the higher threshold), all appear to be clear peaks.
The overused terms during the peak period include
?orlean,? ?worri,? ?hurrican,? ?gas,? ?katrina? In
209
Figure 5: Peaks in ?worried? around August 29, 2005. (Top:
threshold 40% change; bottom: threshold 80% change)
Step 2 of our explanation method we retrieve the
following news headlines (top 5 shown only):
(Sept 1) Hurricane Katrina: Resources regarding
missing/located people
(Sept 2) Crime in New Orleans sharply increases
after Hurricane Katrina
(Sept 1) Fats Dominomissing in the wake of Hur-
ricane Katrina
(Aug 30) At least 55 killed by Hurricane Katrina;
serious flooding across affected region
(Aug 26) Hurricane Katrina strikes Florida, kills
seven
5.3.3 London terror attacks On July 7 a
sharp spike could be observed in the ?sad? mood;
see Figure 6; the tone of the shaded area shows the
degree of the peak. Overused terms identified for
this period include ?london,? ?attack,? ?terrorist,?
?bomb,? ?peopl?, ?explos.? Consulting our news
Figure 6: Peak in ?sad? around July 7, 2005.
corpus produced the following top ranked results:
(July 7) Coordinated terrorist attack hits London
(July 7) British PrimeMinister Tony Blair speaks
about London bombings
(July 7) Bomb scare closes main Edinburgh thor-
oughfare
(July 7) France raises security level to red in re-
sponse to London bombings
(July 6) Tanzania accused of supporting terror-
ism to destabilise Burundi
5.4 Failure analysis Evaluation of the meth-
ods described here is non-trivial. We found that
our peak detection method is effective despite its
simplicity. Anecdotal evidence suggests that our
approach to finding explanations underlying un-
usual spikes and drops in mood levels is effective.
We expect that it will break down, however, in case
the underlying cause is not news related but, for in-
stance, related to celebrations or public holidays;
news sources are unlikely to cover these.
6 Conclusions
We described a method for discovering irregulari-
ties in temporal mood patterns appearing in a large
corpus of blog posts, and labeling them with a
natural language explanation. Our method shows
that simple techniques based on comparing corpus
frequencies, coupled with large quantities of data,
are effective for identifying the events underlying
changes in global moods.
Acknowledgments This research was supported
by the Netherlands Organization for Scientific
Research (NWO) under project numbers 016.-
054.616, 017.001.190, 220-80-001, 264-70-050,
365-20-005, 612.000.106, 612.000.207, 612.013.-
001, 612.066.302, 612.069.006, 640.001.501, and
640.002.501.
References
T. Dunning. 1993. Accurate methods for the statistics of
surprise and coincidence. Comput. Ling., 19(1):61?74.
J. Kleinberg. 2002. Bursty and hierarchical structure in
streams. In Proc. 8th ACM SIGKDD Intern. Conf. on
Knowledge Discovery and Data Mining, pages 1?25.
R. Kumar, J. Novak, P. Raghavan, and A. Tomkins. 2003. On
the bursty evolution of blogspace. In Proc. 12th Intern.
World Wide Web Conf., pages 568?576.
G. Mishne and M. de Rijke. 2006a. Capturing global mood
levels using blog posts. In AAAI 2006 Spring Symp. on
Computational Approaches to Analysing Weblogs (AAAI-
CAAW 2006). To appear.
G. Mishne and M. de Rijke. 2006b. MoodViews: Tools
for blog mood analysis. In AAAI 2006 Spring Symp. on
Computational Approaches to Analysing Weblogs (AAAI-
CAAW 2006).
G. Mishne. 2005. Experiments with mood classification in
blog posts. In Style2005 ? 1st Workshop on Stylistic Anal-
ysis of Text for Information Access, at SIGIR 2005.
T. Nanno, T. Fujiki, Y. Suzuki, and M. Okumura. 2004. Au-
tomatically collecting, monitoring, and mining Japanese
weblogs. In Proc. 13th International World Wide Web
Conf., pages 320?321.
J.G. Shanahan, Y. Qu, and J. Wiebe, editors. 2005. Comput-
ing Attitude and Affect in Text: Theory and Applications.
Springer.
210
Proceedings of NAACL HLT 2007, pages 420?427,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
A Cascaded Machine Learning Approach
to Interpreting Temporal Expressions
David Ahn Joris van Rantwijk Maarten de Rijke
ISLA, University of Amsterdam
Kruislaan 403, 1098 SJ Amsterdam, The Netherlands
{ahn, rantwijk, mdr}@science.uva.nl
Abstract
A new architecture for identifying and in-
terpreting temporal expressions is intro-
duced, in which the large set of com-
plex hand-crafted rules standard in sys-
tems for this task is replaced by a series
of machine learned classifiers and a much
smaller set of context-independent seman-
tic composition rules. Experiments with
the TERN 2004 data set demonstrate that
overall system performance is comparable
to the state-of-the-art, and that normaliza-
tion performance is particularly good.
1 Introduction
In order to fully understand a piece of text, we
must understand its temporal structure. The first
step toward such an understanding is identifying ex-
plicit references to time. We focus on the task of
automatically annotating temporal expressions (or
timexes)?both identifying them in text and inter-
preting them to determine what times they refer to.
Timex annotation is more than normalizing date ex-
pressions. First, time consists of more than calen-
dar dates and clock times?it also includes points of
finer and coarser granularity, durations, and sets of
times. Second, the expressions that refer to time are
not just full date and time expressions?they may be
underspecified, ambiguous, and anaphoric.
Building a system for the full timex identifica-
tion and interpretation task can be tedious, requiring
a great deal of manual effort. The 2004 Temporal
Expression Recognition and Normalization (TERN)
evaluation1 evaluated systems on two tasks: timex
1http://timex2.mitre.org/tern.html
recognition (identification) alone and recognition
and normalization (interpretation) together. All the
full-task systems were rule-based systems; the top
performing full-task system uses in excess of one
thousand hand-crafted rules, which probe words and
their contexts in order to both identify timexes and
to assemble information necessary to interpret them
(Negri and Marseglia, 2004). By contrast, machine
learned systems dominated the recognition-only task
and even achieved slightly better recognition scores
than their rule-based counterparts.
We seek to demonstrate that a timex annotation
system that performs both recognition and normal-
ization need not be a tangle of rules that serve dou-
ble duty for identification and interpretation and that
mix up context-dependent and context-independent
processing. We propose a novel architecture that
clearly separates syntactic, semantic, and prag-
matic processing and factors out context-dependent
from context-independent processing. Factoring
out context-dependent disambiguation into separate
classification tasks introduces the opportunity for
using machine learning, which supports our main
goal: building a portable, trainable timex annota-
tion system in which the role of hand-crafted rules
is minimized. The system we present here (avail-
able from http://ilps.science.uva.nl/
Resources/timextag/) achieves the goal of
making use of only a small set of hand-crafted,
context-independent rules to achieve state-of-the-art
normalization performance.
In the following section, we define what a timex
is. We give an overview of our system architecture
in ?3 and describe the components in ?4?7. ?8 pro-
vides an evaluation of our system on the full timex
annotation task, and we conclude in ?9.
420
2 What is a timex?
Temporal semantics receives a great deal of attention
in the semantics literature (cf. (Mani et al, 2005)),
but the focus is generally on verbal semantics (i.e.,
tense and aspect). In determining what a timex is
and how one should be normalized, we simply fol-
low the TIDES TIMEX2 standard for timex annota-
tion (Ferro et al, 2004). According to this standard,
timexes are phrases or words that refer to times,
where times may be points or durations, or sets of
points or durations. Points are more than just in-
stanteous moments in time?a point may also be a
time with some duration, as long as it spans a single
unit of some temporal granularity. Whether a timex
refers to a point or a duration is a question of per-
spective rather than of ontology. A point-referring
timex such as October 18, 2006 refers to an interval
of one day as an atom at the granularity of a day. A
duration-referring timex such as the whole day may
refer to the same temporal interval, but it focuses on
the durative nature of this interval.
In addition to specifying which phrases are
timexes, the TIMEX2 standard also provides a set
of attributes for normalizing these timexes. We fo-
cus on the VAL attribute, which takes values that are
an extension of the ISO-8601 standard for represent-
ing time (ISO, 1997). TIMEX2 VAL attributes can
take one of three basic types of values:
Points are expressed as a string matching the pat-
tern dddd-dd-ddTdd:dd:dd.d+, where d in-
dicates a digit. Such a string is to be interpreted as
year-month-dateThour:minute:seconds, and may be
truncated from the right, indicating points of coarser
granularity. Any place may be filled with a place-
holder X, which indicates an unknown or vague
value, and there are also a handful of token values
(character strings) for seasons and parts of the day
which may substitute for months and times. There is
also an alternate week-based format dddd-Wdd-d,
interpreted as year-Wweek number-day of the week.
Durations are expressed as a string matching the
pattern Pd+u or PTd+u, where d+ indicates one or
more digits and u indicates a unit token (such as Y
for years). A placeholder X may be used instead of
a number to indicate vagueness.
Vague points: past ref, present ref,
future ref.
Parsed
document
Phrase 
classifier
Semantic 
classifier
phrases
timexes
Direction 
classifier
pre-norm
points
Temporal 
anchoring
Final 
normalization
pre-norm 
points
w/dir & 
anchor
pre-norm
points w/dir
Timex-
annotated
document
normalized
points
Semantic composition rules
(class-specific)
d
u
r
a
t
i
o
n
s
g
e
n
p
o
i
n
t
s
r
e
c
u
r
s
normalized
non-points
p
o
i
n
t
s
A B
C
D
E
F
g
e
n
d
u
r
s
Figure 1: Timex annotation architecture (letters for
ease of reference).
The other attribute which we address in this paper
is the boolean-valued SET attribute; a SET timex
is one that refers to a recurring time. The remain-
ing attributes are MOD, ANCHOR VAL, and AN-
CHOR DIR; our system produces values for these
attributes, but we do not address them in this paper.
The TIMEX2 annotation standard has been used
to create several manually annotated corpora. For
the experiments we present in this paper, we use
the corpora annotated for the TERN 2004 evalua-
tion (Ferro, 2004). These consist of a training set
of 511 documents of newswire and broadcast news
transcripts, with 5326 TIMEX2s, and a test set of
192 similar documents, with 1828 TIMEX2s.
3 Architecture
The architecture of our timex annotation system is
depicted in Fig. 1. Our system begins with parsed
documents as input. Our recognition module is a
machine learned classifier (A); it is described in ?4.
Phrases that have been classified as timexes are
then sent to the semantic class classifier (B). Seman-
tic class disambiguation is the first point at which
context dependence enters into timex interpretation.
While some timexes are unambiguous with respect
to whether they refer to a point, a duration, or a
set, many timexes are semantically ambiguous and
can only be disambiguated in context. The machine
learned classifier for this task is described in ?5.
Based on the class assigned by the semantic class
421
classifier, the semantic composition component (C)
generates (underspecified) semantic representations
using class-specific, context-independent rules. The
rules we use are simple pattern-matching rules that
map lexical items or sequences of lexical items
within a timex to semantic representations. We de-
scribe the semantic composition component in ?6.
For most classes of timexes, the semantic compo-
sition component generates a semantic representa-
tion that can be directly translated into a normalized
value. Timexes that refer to specific points are the
only exception. While some point timexes are fully
qualified, and thus also directly normalizable, many
need to be anchored to another time in context in
order to be fully normalized. Thus, context depen-
dence again enters the timex interpretation process,
and now in two ways. One is obvious: these refer-
ential timexes, which need a temporal anchor, have
to find it in context. This task requires a reference
resolution process (E), which is described in ?7.1.
The second ambiguity regards the relation be-
tween a referential timex and its anchor. Referen-
tial timexes, like anaphoric definites, relate to their
anchors through a bridging relation, which is deter-
mined primarily by the content of the timex?e.g.,
two years later refers to a point two years after its
anchor. For some referential timexes, though, the
direction of the relation (before or after the anchor)
is not specified. The machine learned classifier (D)
resolves this ambiguity; see ?7.2.
For referential timexes, final normalization (F) is
a straightforward combination of semantic represen-
tation, temporal anchor, and direction class.
Not pictured in Fig. 1 is a module that recognizes
and normalizes timexes in document metadata using
a set of simple regular expressions (REs; 14 in total).
This module also determines the document time-
stamp for referential timexes by using a few heuris-
tics to choose from among multiple timestamps or a
date from the document text, if necessary.
While our architecture is novel, we are not the first
to modularize timex annotation systems. Even thor-
oughly rule-based systems (Negri and Marseglia,
2004; Saquete et al, 2002), separate temporal an-
chor tracking from the rest of the normalization pro-
cess. The system of Mani and Wilson (2000) goes
further in using separate sets of hand-crafted rules
for recognition and normalization and in separating
out several disambiguation tasks. Ahn et al (2005b)
decouple recognition from normalization?even us-
ing machine learning for recognition?and handle
several disambiguation tasks separately. In none of
these systems, though, are context-independent and
context-dependent processing thoroughly separated,
as here, and in all these systems, it is the rules that
drive the processing?in both Mani et al and Ahn
et al?s systems, sets of rules are used to determine
which timexes need to be disambiguated.
4 Component A: Recognizing timexes
Systems that perform both recognition and nor-
malization tend to take a rule-based approach to
recognition (Mani and Wilson, 2000; Saquete et
al., 2002; Schilder, 2004; Negri and Marseglia,
2004). Recognition-only systems are often based on
machine learned classifiers (Hacioglu et al, 2005;
Bethard and Martin, 2006), although some do use
finite-state methods (Boguraev and Ando, 2005).
Ahn et al (2005a) find a benefit to decoupling recog-
nition from normalization, and since our goal is
to build a modular, trainable system, we take a
machine-learning approach to recognition that is in-
dependent of our normalization components.
Generally, machine learned timex recognition
systems reduce the task of identifying a timex
phrase to one of classifying individual words by us-
ing (some variant of) B-I-O tagging, in which each
word is tagged as (B)eginning, (I)nside, or (O)utside
a timex phrase. Such a tagging scheme is not in-
herently sensitive to syntactic constituency and not
well-suited to identifying nested timexes (but cf.
(Hacioglu et al, 2005)). Considering that syntactic
parsers are readily available, we have explored sev-
eral ways of leveraging parse information in recog-
nition, although we describe here only the method
we use for experiments later in this paper.
We treat timex recognition as a binary phrase
classification task: syntactic constituents are clas-
sified as timexes or non-timexes. We restrict clas-
sification to the following phrase types and lexical
categories (based on (Ferro et al, 2004, ?5)): NP,
ADVP, ADJP, NN, NNP, JJ, CD, RB, and PP.2 In
order to identify candidate phrases and to extract
2We include PPs despite the TIDES guidelines, which ex-
plicitly exclude temporal PPs such as before Thursday because
of prepositional modifiers such as around and about.
422
Identification Exact match
prec rec F prec rec F
TEXT 0.912 0.786 0.844 0.850 0.732 0.787
DOC 0.929 0.813 0.867 0.878 0.769 0.819
BRO 0.973 0.891 0.930 0.905 0.829 0.865
BFT 0.976 0.880 0.926 0.885 0.798 0.839
Table 1: Recognition results: Identification.
parse-based features, we parse the TEXT elements
of our documents with the Charniak parser (Char-
niak, 2000). Because of both parser and annotator
errors, only 90.2% of the timexes in the training data
align exactly with a parse, which gives an estimated
upper-bound on recall using this method.
We use support vector machines for classification,
in particular, the LIBSVM linear kernel implemen-
tation (Chang and Lin, 2001). The features we ex-
tract include character type patterns, lexical features
such as weekday name and numeric year, a context
window of two words to the left, and several parse-
based features: the phrase type, the phrase head and
initial word (and POS tag), and the dependency par-
ent (and corresponding relation) of the head.
As with all our experiments in this paper, we
train on the TERN training corpus and test on the
test corpus. Our scores (precision, recall and F-
measure for both identification (i.e., overlap) and
exact-match) are given in Table 1, along with the
scores of the best recognition-only (BRO) and full-
task (BFT) TERN 2004 systems. Since our phrase
classification method is only applied within docu-
ment TEXT elements, we also present results using
both our RE-based document metadata tagger and
our phrase classifier for full documents (DOC). Only
these scores can be compared with the TERN scores.
Our scores using this method approach those of
the best systems, but there is still a gap, which, as
we see in ?8, affects our overall task performance.
5 Component B: Semantic classification
Timexes may refer to points, durations, or recur-
rences. While some timexes refer unambiguously to
one of these, many timexes are ambiguous between
two or even three of these (see (Hitzeman, 1993) for
a theoretical semantic perspective on this ambigu-
ity). Timexes may also refer generically or vaguely,
which is another source of ambiguity.
While the TIMEX2 standard does not explicitly
specify semantic classes in its annotations, the se-
mantic classes we distinguish for our normalization
system can be easily inferred from the form of the
values of the attributes that are annotated, as follows:
Recurrence (recur): SET attribute set to true
Generic or vague duration (gendur): VAL begins
with PX or PTX
Duration: VAL begins with P[0-9] or PT[0-9]
Generic or vague point (genpoint): Three possi-
bilities: time-of-day w/o associated date expression
(VAL begins with T[0-9]); general reference to past,
present, or future (VAL is one of the vague tokens);
date expression with unspecified high-order position
(i.e., millennium position is X)
Point: Date expression with specified high-order
position (may be precise or not?i.e., may include X
at other positions?also may be of any granularity,
from millennium down to hundredths of a second).
Resolving semantic class ambiguities is a context-
dependent task that can be easily factored out of se-
mantic interpretation, reducing the burden on the se-
mantic interpretation rules. The classification task is
straightforward: each timex must be classified into
one of the five classes described above or into the
null class (for timexes that have no VAL). Since the
TERN data is not explicitly annotated for semantic
class, we use the class definitions above to derive the
semantic class of a timex from its VAL attribute.
We again use the LIBSVM linear kernel for clas-
sification, with the same features as for recogni-
tion. Even though some timexes are unambiguous
with respect to semantic class, we train the classi-
fier over all timexes, in the expectation that the con-
texts of unambiguous timexes will be similar enough
to those of ambiguous timexes of the same class to
help in classification. We compare the performance
of our machine learned classifier to a heuristic base-
line classifier that uses the head of the timex and the
presence of numbers, names, and certain modifiers
within the timex to decide how to classify it.
Table 2 gives the error rates, per class and overall,
for the baseline and learned classifiers over phrase-
aligned gold-standard timexes. The machine learned
classifier halves the error rate of the baseline, mostly
as a result of better performance on the duration and
point classes. In ?8, we see how this improvement
in classification affects end-to-end performance.
Mani and Wilson (2000) and Ahn et al (2005b)
423
classifier overall null duration . . .
BL 0.2085 1.0000 0.2534 . . .
SVM 0.1078 0.4143 0.1507 . . .
class dist 1290 70 146 . . .
. . . gendur genpoint point recur
. . . 0.0204 0.1462 0.1322 0.6087
. . . 0.1020 0.1462 0.0496 0.2174
. . . 49 253 726 46
Table 2: Error rates: semantic class.
also perform limited semantic class disambiguation.
Both use machine learned classifiers to distinguish
specific and generic uses of today, and Ahn et al
also use a machine learned classifier to disambiguate
timexes between a point and a duration reading.
Their error rate for this task is 27%, but since a set
of heuristics is first used to select just ambiguous
timexes, this score cannot be compared to ours.
6 Component C: Semantic composition
The semantic composition module uses context-
independent, class-specific rules to compute for each
timex an underspecified representation?a typed
feature structure that depends on the timex?s seman-
tic class (features include unit and value for dura-
tions, year, month, date, and referential class for
points; cf. (Dale and Mazur, 2006)). As the rules are
not responsible for identification or class or direc-
tion disambiguation, they are fewer in number and
simpler than in other systems (cf. 1000+ in (Negri
and Marseglia, 2004)). Each rule consists of an RE-
pattern, which may refer to a small lexicon of names,
units, and numeric words, and is applied using a cus-
tom transducer. In total, there are 89 rules; Table 3
gives the distribution of rules and an example rule
for each class. Tokens in ALLCAPS indicate lexical
classes; tokens in MixedCase indicate other rules;
and tokens in lowercase indicate lexical items.
7 Temporal anchors
Some point timexes are fully qualified, while others
require a reference time, or temporal anchor, to be
fully normalized.3 There are three ways in which
a temporal anchor is chosen for a timex. Some
timexes, such as today, three years ago, and next
week, are deictic and anchored to the time of speech
3Our use of the term temporal anchor is distinct from the
ANCHOR VAL and ANCHOR DIR attributes.
class rules example
dur 13 Numeric -? (UNIT | UNITS)
gendur 3 (UNIT | UNITS)
genpt 21 (NUM24 | NUMWORD) o ? clock
point 31 ? Approx? DAYNAME? MONTHNAME
.? Num31OrRank ,? YearNum
recur 11 (every | per) Numeric UNITS
misc 10 NUMWORD ((and | -)? NUMWORD)*
Table 3: Distribution of semantic composition rules.
(for us, the document timestamp). Others, such as
two months earlier and the next week, are anaphoric
and anchored to a salient time in discourse, just like
an anaphoric pronoun or definite. The distinction
between deictic and anaphoric timexes is not always
clear-cut, since many anaphoric timexes, in the ab-
sence of an appropriate antecedent, are anchored de-
ictically. A timex may also contain its own anchor:
e.g., two days after May 3, whose anchor is the em-
bedded anaphoric timex May 3.
Once a referential timex?s temporal anchor has
been determined, the value of the anchor must be
combined with the timex, which may be either an
offset or a name-like timex. Offsets, such as two
months earlier, provide a unit u, a magnitude m,
and optionally, a direction (before or after); the value
of an offset is the point (of granularity u) that is m
u units from its anchor in the indicated direction.
Name-like timexes provide a position in a cycle,
such as a day name within a week, and optionally,
a direction. The value of a name-like timex is the
time point bearing the name within the correspond-
ing cycle of its anchor (or the immediately preceding
or succeeding cycle, depending on the direction).
For both offsets and name-like timexes, the direc-
tion indication is optional. When no direction in-
dication is given, the appropriate direction must be
determined from context, as in this initial sentence
from an article from 1998-11-28:
(1) A fundamentalist Muslim lawmaker has vowed
to stop a shopping festival planned in February,
a newspaper reported Saturday.
The first timex, February, clearly refers to the Febru-
ary following its anchor (the timestamp), while the
second timex, Saturday, seems to refer to a point
preceding its anchor (also the timestamp).
The next two sections describe our methods for
temporal anchoring and direction classification.
424
7.1 Component E: Temporal anchor tracking
Since temporal anchors are not annotated in the
TIMEX2 standard, our system uses a simple heuris-
tic method for temporal anchoring (cf. (Wiebe et al,
1997), who use a more complex rule-based system
for timex anchoring in scheduling dialogues). Since
we distinguish deictic and anaphoric timexes during
semantic composition, we use a combination of two
methods: for deictic timexes, the document time-
stamp is used, and for (some) anaphoric timexes, the
most recent point timex, if it is fine-grained enough,
is used as the temporal anchor (otherwise, the docu-
ment timestamp is used). Because the documents in
our corpora are short news texts, we actually treat
anaphoric name-like points as deictic and use the
most recent timex only for anaphoric offsets.
7.2 Component D: Direction classification
The idea of separating direction classification from
the remainder of the normalization task is not new.
(Mani and Wilson, 2000) use a heuristic method
for this task, while (Ahn et al, 2005b) use a ma-
chine learned classifier. In contrast to Ahn et al,
who use a set of heuristics to identify ambiguous
timexes and train and test only on those, we train
our classifier on all point and genpoint timexes and
apply it to all point timexes. Genpoint timexes and
many point timexes are not ambiguous w.r.t. direc-
tion, but we expect that the contexts of unambiguous
timexes will be similar enough to those of ambigu-
ous timexes of the same class to help classification.
Direction class is not annotated as part of the
TIMEX2 standard. Given a temporal anchor track-
ing method, though, it is possible to derive imperfect
direction class information from the VAL attribute.
We use our anchor tracking method to associate each
point and genpoint timex with an anchor and then
compare the VAL of the timex with that of its an-
chor to decide what its direction class should be.
We again use the LIBSVM linear kernel for clas-
sification. We add two sets of features to those used
for recognition and semantic classification. The first
is inspired by Mani et al, who rely on the tense of
neighboring verbs to decide direction class. Since
verb tense alone is inherently deictic, it is not suffi-
cient to decide the direction, but we do add both the
closest verb (w.r.t. dependency paths) and its POS
classifier overall after before same
BL 0.1749 0.4587 0.0802 0.1934
SVM 0.2245 0.4404 0.1578 0.2305
SVM VERB 0.2094 0.3119 0.1631 0.2346
SVM ALL 0.1185 0.2110 0.0989 0.1070
class dist 726 109 374 243
Table 4: Error rates: direction class.
tag (as well as any verbs directly related to this verb)
as features. The second set of features compares day
names, month names, and years to the document
timestamp. The comparison determines whether,
within a single cycle of the appropriate granularity
(week for day-names and year for month-names),
the point named by the timex would be before, after,
or the same as the point referred to by the timestamp.
We compare our learned classifier with a heuristic
baseline classifier which first checks for the presence
of a year or certain modifiers such as ago or next in
the timex; if that fails, it computes the date features
described above for each word in the timex and re-
turns same if any word compares to the timestamp
as same; if that fails, it uses the tense of the nearest
verb; and finally, it defaults to same.
Table 4 shows the results of applying our clas-
sifiers to all phrase-aligned gold-standard point
timexes. BL is the baseline; SVM, SVM VERB, and
SVM ALL are the classifiers learned using our basic
feature set, the basic feature set plus the verb fea-
tures, and all the features, respectively. The learned
classifier using all the features reduces the error rate
of the baseline classifier by about a third. Note,
though, that the learned classifiers without the date
comparison features (SVM and SVM VERB) perform
substantially worse than even the baseline. One rea-
son for this becomes clear from Table 5, which gives
the error rates for the classifiers restricted to timexes
consisting solely of a month or a day name. Unlike
points in general, these timexes are all ambiguous
with respect to direction and are, in fact, the primary
motivation for both Mani et al and Ahn et al to con-
sider direction classification as a separate task.
These results demonstrate that the date compari-
son feature is responsible for a substantial reduction
in error rate (over 85% from SVM to SVM ALL) and
that for the same class, performance is perfect. This
is largely due to the writing style of the documents,
in which the current day is often referred to by name
425
classifier overall after before same
BL 0.1000 0.4348 0.1061 0.0000
SVM 0.3647 0.6087 0.3485 0.3086
SVM VERB 0.3176 0.3478 0.3485 0.2840
SVM ALL 0.0529 0.1304 0.0909 0.0000
class dist 170 23 66 81
Table 5: Error rates: direction month/day.
instead of as today, as in example (1).
Although both Mani et al and Ahn et al build
direction classifiers, neither provide comparable re-
sults. Mani et al do not evaluate their direction
heuristics at all, and Ahn et al train and test their
machine learned classifier only on timexes deter-
mined to be ambiguous by their heuristics. In any
case, their error rate is significantly higher, at 38%.
8 End-to-end performance
We now consider the performance of the entire sys-
tem and the contributions of the components. First,
though, we discuss our evaluation metrics.
8.1 Scoring
The official TERN scoring script computes precision
and recall for VAL only with respect to correctly rec-
ognized TIMEX2s with a non-null VAL. While this
may be useful in determining how far behind nor-
malization is from recognition for a given system, it
does not provide an accurate picture of end-to-end
system performance, since the recall base does not
include all possible timexes and the precision base
does not include incorrectly recognized timexes.
The scoring script provides several raw counts
that can be used to compute measures that are more
indicative of end-to-end performance: actTIMEX2
(# of actually recognized TIMEX2s); corrTIMEX2
(# of correctly recognized TIMEX2s); posVAL
(# of correctly recognized TIMEX2s with a non-
null gold VAL); corrVAL (# of correctly recog-
nized TIMEX2s with a non-null gold VAL for
which the system assigns the correct VAL); and
spurVAL (# of correctly recognized TIMEX2s with
null gold VAL for which the system assigns a
VAL). With these counts, we can define corrNOVAL
(# of correctly recognized TIMEX2s with a null
gold VAL for which the system assigns a null
VAL), as corrTIMEX2 ? posVAL ? spurVAL. We
then define end-to-end precision (absP) and recall
(absR) as (corrVAL + corrNOVAL)/actTIMEX2
and (corrVAL+corrNOVAL)/possTIMEX2, respec-
tively. Official precision and recall for VAL are com-
puted as corrVAL/actVAL and corrVAL/possVAL.
8.2 Results
Our first set of results (Table 6(Top)), which are
restricted to timexes in document TEXT elements,
compares our system (LLL) to a version of our sys-
tem (BL) that uses the baseline classifiers for seman-
tic and direction class. It also presents a series of or-
acle results that demonstrate the effect of swapping
in perfect classification for each of the learned clas-
sifiers. The oracle runs are labeled with a three-letter
code in which the first letter ((P)erfect or (L)earned)
refers to phrase classification; the second, to seman-
tic classification; and the third, to direction classi-
fication. Note: perfect phrase classification is not
the same as perfect recognition, since it excludes
timexes that fail to align with parsed phrases.
Using the learned classifiers (LLL), which reduce
error rates by about one-half for semantic class and
one-third for direction class over the baseline clas-
sifiers, results in a five-point improvement in abso-
lute F-measure over the baseline system (BL). We
also see from runs LLP, LPL, and LPP that further
improvement of these classifiers would substantially
improve end-to-end performance. Finally, we see
from runs PLL and PPP that recognition performance
is a major limiting factor in our end-to-end scores.
In Table 6(Bottom), we present results over full
documents, including metadata and text. LLL and
PLL are the same as before; ITC-IRST is the sys-
tem of (Negri and Marseglia, 2004), which achieved
the highest official F-measure in the TERN 2004
evaluation. The results of our system (LLL) are
comparable to those of ITC-irst: because we recog-
nize fewer timexes, our official F-measure is higher
(0.899 vs. 0.872) while our absolute F-measure is
lower (0.769 vs. 0.806). We see from run PLL that
our recognition module is largely to blame?with
perfect phrase classification for recognition, our nor-
malization modules produce substantially better re-
sults. With a system such as ITC-irst?s, it is not pos-
sible to separate recognition performance from nor-
malization performance, since there is a single rule
base that jointly performs the two tasks?all normal-
izable timexes are presumably already recognized.
426
System corrVAL corrNOVAL actTIMEX2 P R F absP absR absF
BL 859 32 1245 0.813 0.787 0.800 0.716 0.624 0.667
LLL 931 33 1245 0.882 0.853 0.867 0.774 0.676 0.722
LLP 938 33 1245 0.912 0.859 0.885 0.780 0.680 0.727
LPL 951 39 1245 0.916 0.871 0.893 0.795 0.694 0.741
LPP 987 39 1245 0.951 0.904 0.927 0.824 0.719 0.768
PLL 1008 63 1287 0.886 0.828 0.856 0.832 0.751 0.789
PPP 1097 70 1287 0.966 0.901 0.932 0.907 0.818 0.860
LLL 1285 33 1601 0.910 0.887 0.899 0.823 0.721 0.769
PLL 1362 63 1643 0.912 0.866 0.888 0.867 0.780 0.821
ITC-IRST 1365 35 1648 0.875 0.870 0.872 0.850 0.766 0.806
Table 6: Performance on VAL. (Top): TEXT-only. (Bottom): full document.
9 Conclusion
We have described a novel architecture for a timex
annotation system that eschews the complex set
of hand-crafted rules that is a hallmark of other
systems. Instead, we decouple recognition from
normalization and factor out context-dependent se-
mantic and pragmatic processing from context-
independent semantic composition. Our architec-
ture allows us to use machine learned classifiers to
make context-dependent disambiguation decisions,
which in turn allows us to use a small set of sim-
ple, context-independent rules for semantic compo-
sition. The normalization performance of this sys-
tem is competitive with the state of the art and our
overall performance is limited primarily by recog-
nition performance. Improvement in semantic and
direction classification will yield further improve-
ments in overall performance. Our other plans for
the future include experimenting with dependency
relations for semantic composition instead of lexi-
cal patterns, evaluating our temporal anchor tracking
method, and training the full system on other cor-
pora and adapting it for other languages.
Acknowledgement This research was supported
by the Netherlands Organization for Scientific Re-
search (NWO) under project numbers 017.001.190,
220-80-001, 264-70-050, 354-20-005, 600.065.120,
612-13- 001, 612.000.106, 612.066.302,
612.069.006, 640.001.501, 640.002.501, and
by the E.U. IST programme of the 6th FP for RTD
under project MultiMATCH contract IST-033104.
References
D. Ahn, S. Fissaha Adafre, and M. de Rijke. 2005a. Extracting
temporal information from open domain text: A comparative
exploration. In R. van Zwol, editor, Proc. DIR?05.
D. Ahn, S. Fissaha Adafre, and M. de Rijke. 2005b. Recog-
nizing and interpreting temporal expressions in open domain
texts. In S. Artemov et al, editors, We Will Show Them:
Essays in Honour of Dov Gabbay, Vol 1, pages 31?50.
S. Bethard and J.H. Martin. 2006. Identification of event men-
tions and their semantic class. In Proc. EMNLP 2006.
B. Boguraev and R. Kubota Ando. 2005. TimeML-compliant
text analysis for temporal reasoning. In Proc. IJCAI-05.
C.-C. Chang and C.-J. Lin, 2001. LIBSVM: a library for sup-
port vector machines. Software available at http://www.
csie.ntu.edu.tw/?cjlin/libsvm.
E. Charniak. 2000. A maximum-entropy-inspired parser. In
Proc. NAACL 2000, pages 132?139.
R. Dale and P. Mazur. 2006. Local semantics in the interpre-
tation of temporal expressions. In Proc. Workshop on Anno-
tating and Reasoning about Time and Events, pages 9?16.
L. Ferro, L. Gerber, I. Mani, and G. Wilson, 2004. TIDES 2003
Std. for the Annotation of Temporal Expressions. MITRE.
L. Ferro. 2004. Annotating the TERN corpus.
http://timex2.mitre.org/tern 2004/
ferro2 TERN2004 annotation sanitized.pdf.
K. Hacioglu, Y. Chen, and B. Douglas. 2005. Automatic time
expression labeling for English and Chinese text. In A.F.
Gelbukh, editor, CICLing, volume 3406 of Lecture Notes in
Computer Science, pages 548?559.
J. Hitzeman. 1993. Temporal Adverbials and the Syntax-
Semantics Interface. Ph.D. thesis, University of Rochester.
ISO. 1997. ISO 8601: Information interchange ? representa-
tion of dates and times.
I. Mani and G. Wilson. 2000. Robust temporal processing of
news. In Proc. ACL?2000, pages 69?76.
I. Mani, J. Pustejovsky, and R. Gaizauskas, editors. 2005. The
Language of Time: A Reader. Oxford University Press.
M. Negri and L. Marseglia. 2004. Recognition and normaliza-
tion of time expressions: ITC-irst at TERN 2004. Technical
report, ITC-irst, Trento.
E. Saquete, P. Mart??nez-Barco, and R. Mun?oz. 2002. Recogniz-
ing and tagging temporal expressions in Spanish. In Work-
shop on Annotation Standards for Temporal Information in
Natural Language, LREC 2002, pages 44?51.
F. Schilder. 2004. Extracting meaning from temporal nouns
and temporal prepositions. ACM TALIP.
J. Wiebe, T. O?Hara, K. McKeever, and T. O?hrstro?m Sandgren.
1997. An empirical approach to temporal reference resolu-
tion. In Proceedings of EMNLP-97, pages 174?186.
427
Enriching the Output of a Parser Using Memory-Based Learning
Valentin Jijkoun and Maarten de Rijke
Informatics Institute, University of Amsterdam
  jijkoun, mdr  @science.uva.nl
Abstract
We describe a method for enriching the output of a
parser with information available in a corpus. The
method is based on graph rewriting using memory-
based learning, applied to dependency structures.
This general framework allows us to accurately re-
cover both grammatical and semantic information
as well as non-local dependencies. It also facili-
tates dependency-based evaluation of phrase struc-
ture parsers. Our method is largely independent of
the choice of parser and corpus, and shows state of
the art performance.
1 Introduction
We describe a method to automatically enrich the
output of parsers with information that is present
in existing treebanks but usually not produced by
the parsers themselves. Our motivation is two-fold.
First and most important, for applications requiring
information extraction or semantic interpretation of
text, it is desirable to have parsers produce gram-
matically and semantically rich output. Second, to
facilitate dependency-based comparison and evalu-
ation of different parsers, their outputs may need to
be transformed into specific rich dependency for-
malisms.
The method allows us to automatically trans-
form the output of a parser into structures as they
are annotated in a dependency treebank. For a
phrase structure parser, we first convert the pro-
duced phrase structures into dependency graphs
in a straightforward way, and then apply a se-
quence of graph transformations: changing depen-
dency labels, adding new nodes, and adding new
dependencies. A memory-based learner trained
on a dependency corpus is used to detect which
modifications should be performed. For a depen-
dency corpus derived from the Penn Treebank and
the parsers we considered, these transformations
correspond to adding Penn functional tags (e.g.,
-SBJ, -TMP, -LOC), empty nodes (e.g., NP PRO)
and non-local dependencies (controlled traces, WH-
extraction, etc.). For these specific sub-tasks our
method achieves state of the art performance. The
evaluation of the transformed output of the parsers
of Charniak (2000) and Collins (1999) gives 90%
unlabelled and 84% labelled accuracy with respect
to dependencies, when measured against a depen-
dency corpus derived from the Penn Treebank.
The paper is organized as follows. After provid-
ing some background and motivation in Section 2,
we give the general overview of our method in Sec-
tion 3. In Sections 4 through 8, we describe all
stages of the transformation process, providing eval-
uation results and comparing our methods to earlier
work. We discuss the results in Section 9.
2 Background and Motivation
State of the art statistical parsers, e.g., parsers
trained on the Penn Treebank, produce syntactic
parse trees with bare phrase labels, such as NP, PP,
S, although the training corpora are usually much
richer and often contain additional grammatical and
semantic information (distinguishing various modi-
fiers, complements, subjects, objects, etc.), includ-
ing non-local dependencies, i.e., relations between
phrases not adjacent in the parse tree. While this in-
formation may be explicitly annotated in a treebank,
it is rarely used or delivered by parsers.1 The rea-
son is that bringing in more information of this type
usually makes the underlying parsing model more
complicated: more parameters need to be estimated
and independence assumptions may no longer hold.
Klein and Manning (2003), for example, mention
that using functional tags of the Penn Treebank
(temporal, location, subject, predicate, etc.) with a
simple unlexicalized PCFG generally had a negative
effect on the parser?s performance. Currently, there
are no parsers trained on the Penn Treebank that use
the structure of the treebank in full and that are thus
1Some notable exceptions are the CCG parser described in
(Hockenmaier, 2003), which incorporates non-local dependen-
cies into the parser?s statistical model, and the parser of Collins
(1999), which uses WH traces and argument/modifier distinc-
tions.
capable of producing syntactic structures containing
all or nearly all of the information annotated in the
corpus.
In recent years there has been a growing inter-
est in getting more information from parsers than
just bare phrase trees. Blaheta and Charniak (2000)
presented the first method for assigning Penn func-
tional tags to constituents identified by a parser.
Pattern-matching approaches were used in (John-
son, 2002) and (Jijkoun, 2003) to recover non-local
dependencies in phrase trees. Furthermore, experi-
ments described in (Dienes and Dubey, 2003) show
that the latter task can be successfully addressed by
shallow preprocessing methods.
3 An Overview of the Method
In this section we give a high-level overview of our
method for transforming a parser?s output and de-
scribe the different steps of the process. In the ex-
periments we used the parsers described in (Char-
niak, 2000) and (Collins, 1999). For Collins? parser
the text was first POS-tagged using Ratnaparkhi?s
maximum enthropy tagger.
The training phase of the method consists in
learning which transformations need to be applied
to the output of a parser to make it as similar to the
treebank data as possible.
As a preliminary step (Step 0), we convert the
WSJ2 to a dependency corpus without losing the an-
notated information (functional tags, empty nodes,
non-local dependencies). The same conversion is
applied to the output of the parsers we consider. The
details of the conversion process are described in
Section 4 below.
The training then proceeds by comparing graphs
derived from a parser?s output with the graphs
from the dependency corpus, detecting various mis-
matches, such as incorrect arc labels and missing
nodes or arcs. Then the following steps are taken to
fix the mismatches:
Step 1: changing arc labels
Step 2: adding new nodes
Step 3: adding new arcs
Obviously, other modifications are possible, such
as deleting arcs or moving arcs from one node to
another. We leave these for future work, though,
and focus on the three transformations mentioned
above.
The dependency corpus was split into training
(WSJ sections 02?21), development (sections 00?
2Thoughout the paper WSJ refers to the Penn Treebank II
Wall Street Journal corpus.
01) and test (section 23) corpora. For each of the
steps 1, 2 and 3 we proceed as follows:
1. compare the training corpus to the output of the
parser on the strings of the corpus, after apply-
ing the transformations of the previous steps
2. identify possible beneficial transformations
(which arc labels need to be changed or where
new nodes or arcs need to be added)
3. train a memory-based classifier to predict pos-
sible transformations given their context (i.e.,
information about the local structure of the
dependency graph around possible application
sites).
While the definitions of the context and application
site and the graph modifications are different for the
three steps, the general structure of the method re-
mains the same at each stage. Sections 6, 7 and 8
describe the steps in detail.
In the application phase of the method, we pro-
ceed similarly. First, the output of the parser is con-
verted to dependency graphs, and then the learners
trained during the steps 1, 2 and 3 are applied in
sequence to perform the graph transformations.
Apart from the conversion from phrase structures
to dependency graphs and the extraction of some
linguistic features for the learning, our method does
not use any information about the details of the tree-
bank annotation or the parser?s output: it works with
arbitrary labelled directed graphs.
4 Step 0: From Constituents to
Dependencies
To convert phrase trees to dependency structures,
we followed the commonly used scheme (Collins,
1999). The conversion routine,3 described below, is
applied both to the original WSJ structures and the
output of the parsers, though the former provides
more information (e.g., traces) which is used by the
conversion routine if available.
First, for the treebank data, all traces are resolved
and corresponding empty nodes are replaced with
links to target constituents, so that syntactic trees
become directed acyclic graphs. Second, for each
constituent we detect its head daughters (more than
one in the case of conjunction) and identify lexical
heads. Then, for each constituent we output new
dependencies between its lexical head and the lex-
ical heads of its non-head daughters. The label of
every new dependency is the constituent?s phrase
3Our converter is available at http://www.science.
uva.nl/?jijkoun/software.
(a)
S
NP?SBJ VP
to seek NP
seats
*?1
directors
NP?SBJ?1
this month
NP?TMP
VP
planned
S
(b)
VP
to seek NP
seats
VP
planned
S
directors
this month
      NP
     NP  S
(c)
planned
directors
VP|S
S|NP?SBJ
to
seek
seats
VP|NPmonth
 this
VP|TO
S|NP?TMP
NP|DT
S|NP?SBJ
(d)
planned
directors
VP|SS|NP
to
seek
seats
VP|NPmonth
 this
VP|TO
S|NP
NP|DT
Figure 1: Example of (a) the Penn Treebank WSJ annotation, (b) the output of Charniak?s parser, and the
results of the conversion to dependency structures of (c) the Penn tree and of (d) the parser?s output
label, stripped of all functional tags and coindex-
ing marks, conjoined with the label of the non-head
daughter, with its functional tags but without coin-
dexing marks. Figure 1 shows an example of the
original Penn annotation (a), the output of Char-
niak?s parser (b) and the results of our conversion of
these trees to dependency structures (c and d). The
interpretation of the dependency labels is straight-
forward: e.g., the label S   NP-TMP corresponds to
a sentence (S) being modified by a temporal noun
phrase (NP-TMP).
The core of the conversion routine is the selection
of head daughters of the constituents. Following
(Collins, 1999), we used a head table, but extended
it with a set of additional rules, based on constituent
labels, POS tags or, sometimes actual words, to ac-
count for situations where the head table alone gave
unsatisfactory results. The most notable extension
is our handling of conjunctions, which are often left
relatively flat in WSJ and, as a result, in a parser?s
output: we used simple pattern-based heuristics to
detect conjuncts and mark all conjuncts as heads of
a conjunction.
After the conversion, every resulting dependency
structure is modified deterministically:
 auxiliary verbs (be, do, have) become depen-
dents of corresponding main verbs (similar to
modal verbs, which are handled by the head ta-
ble);
 to fix a WSJ inconsistency, we move the -LGS
tag (indicating logical subject of passive in a
by-phrase) from the PP to its child NP.
5 Dependency-based Evaluation of
Parsers
After the original WSJ structures and the parsers?
outputs have been converted to dependency struc-
tures, we evaluate the performance of the parsers
against the dependency corpus. We use the standard
precision/recall measures over sets of dependencies
(excluding punctuation marks, as usual) and evalu-
ate Collins? and Charniak?s parsers on WSJ section
23 in three settings:
 on unlabelled dependencies;
 on labelled dependencies with only bare labels
(all functional tags discarded);
 on labelled dependencies with functional tags.
Notice that since neither Collins? nor Charniak?s
parser outputs WSJ functional labels, all dependen-
cies with functional labels in the gold parse will be
judged incorrect in the third setting. The evaluation
results are shown in Table 1, in the row ?step 0?.4
As explained above, the low numbers for the de-
pendency evaluation with functional tags are ex-
pected, because the two parsers were not intended
to produce functional labels.
Interestingly, the ranking of the two parsers is
different for the dependency-based evaluation than
for PARSEVAL: Charniak?s parser obtains a higher
PARSEVAL score than Collins? (89.0% vs. 88.2%),
4For meaningful comparison, the Collins? tags -A and -g
are removed in this evaluation.
Evaluation Parser unlabelled labelled with func. tagsP R f P R f P R f
after conversion Charniak 89.9 83.9 86.8 85.9 80.1 82.9 68.0 63.5 65.7
(step 0, Section 4) Collins 90.4 83.7 87.0 86.7 80.3 83.4 68.4 63.4 65.8
after relabelling Charniak 89.9 83.9 86.8 86.3 80.5 83.3 83.8 78.2 80.9
(step 1, Section 6) Collins 90.4 83.7 87.0 87.0 80.6 83.7 84.6 78.4 81.4
after adding nodes Charniak 90.1 85.4 87.7 86.5 82.0 84.2 84.1 79.8 81.9
(step 2, Section 7) Collins 90.6 85.3 87.9 87.2 82.1 84.6 84.9 79.9 82.3
after adding arcs Charniak 90.0 89.7 89.8 86.5 86.2 86.4 84.2 83.9 84.0
(step 3, Section 8) Collins 90.4 89.4 89.9 87.1 86.2 86.6 84.9 83.9 84.4
Table 1: Dependency-based evaluation of the parsers after different transformation steps
but slightly lower f-score on dependencies without
functional tags (82.9% vs. 83.4%).
To summarize the evaluation scores at this stage,
both parsers perform with f-score around 87%
on unlabelled dependencies. When evaluating on
bare dependency labels (i.e., disregarding func-
tional tags) the performance drops to 83%. The
new errors that appear when taking labels into ac-
count come from different sources: incorrect POS
tags (NN vs. VBG), different degrees of flatness of
analyses in gold and test parses (JJ vs. ADJP, or
CD vs. QP) and inconsistencies in the Penn anno-
tation (VP vs. RRC). Finally, the performance goes
down to around 66% when taking into account func-
tional tags, which are not produced by the parsers at
all.
6 Step 1: Changing Dependency Labels
Intuitively, it seems that the 66% performance on
labels with functional tags is an underestimation,
because much of the missing information is easily
recoverable. E.g., one can think of simple heuris-
tics to distinguish subject NPs, temporal PPs, etc.,
thus introducing functional labels and improving
the scores. Developing such heuristics would be
a very time consuming and ad hoc process: e.g.,
Collins? -A and -g tags may give useful clues for
this labelling, but they are not available in the out-
put of other parsers. As an alternative to hard-
coded heuristics, Blaheta and Charniak (2000) pro-
posed to recover the Penn functional tags automat-
ically. On the Penn Treebank, they trained a sta-
tistical model that, given a constituent in a parsed
sentence and its context (parent, grandparent, head
words thereof etc.), predicted the functional label,
possibly empty. The method gave impressive per-
formance, with 98.64% accuracy on all constituents
and 87.28% f-score for non-empty functional la-
bels, when applied to constituents correctly identi-
fied by Charniak?s parser. If we extrapolate these re-
sults to labelled PARSEVAL with functional labels,
the method would give around 87.8% performance
(98.64% of the ?usual? 89%) for Charniak?s parser.
Adding functional labels can be viewed as a
relabelling task: we need to change the labels
produced by a parser. We considered this more
general task, and used a different approach,
taking dependency graphs as input. We first
parsed the training part of our dependency tree-
bank (sections 02?21) and identified possible
relabellings by comparing dependencies output
by a parser to dependencies from the treebank.
E.g., for Collins? parser the most frequent rela-
bellings were S   NP   S   NP-SBJ, PP   NP-A   PP   NP,
VP   NP-A   VP   NP, S   NP-A   S   NP-SBJ and
VP   PP   VP   PP-CLR. In total, around 30% of
all the parser?s dependencies had different labels
in the treebank. We then learned a mapping from
the parser?s labels to those in the dependency
corpus, using TiMBL, a memory-based classifier
(Daelemans et al, 2003). The features used for
the relabelling were similar to those used by Bla-
heta and Charniak, but redefined for dependency
structures. For each dependency we included:
 the head (  ) and dependent (  ), their POS tags;
 the leftmost dependent of  and its POS;
 the head of
 (  ), its POS and the label of the
dependency

;
 the closest left and right siblings of  (depen-
dents of
 ) and their POS tags;
 the label of the dependency (   ) as derived
from the parser?s output.
When included in feature vectors, all dependency
labels were split at ? 	 ?, e.g., the label S   NP-A resulted
in two features: S and NP-A.
Testing was done as follows. The test corpus
(section 23) was also parsed, and for each depen-
dency a feature vector was formed and given to
TiMBL to correct the dependency label. After this
transformation the outputs of the parsers were eval-
uated, as before, on dependencies in the three set-
tings. The results of the evaluation are shown in
Table 1 (the row marked ?step 1?).
Let us take a closer look at the evaluation re-
sults. Obviously, relabelling does not change the
unlabelled scores. The 1% improvement for eval-
uation on bare labels suggests that our approach
is capable not only of adding functional tags, but
can also correct the parser?s phrase labels and part-
of-speech tags: for Collins? parser the most fre-
quent correct changes not involving functional la-
bels were NP   NN

NP   JJ and NP   JJ

NP   VBN, fix-
ing POS tagging errors. A very substantial increase
of the labelled score (from 66% to 81%), which is
only 6% lower than unlabelled score, clearly indi-
cates that, although the parsers do not produce func-
tional labels, this information is to a large extent im-
plicitly present in trees and can be recovered.
6.1 Comparison to Earlier Work
One effect of the relabelling procedure described
above is the recovery of Penn functional tags. Thus,
it is informative to compare our results with those
reported in (Blaheta and Charniak, 2000) for this
same task. Blaheta and Charniak measured tag-
ging accuracy and precision/recall for functional tag
identification only for constituents correctly identi-
fied by the parser (i.e., having the correct span and
nonterminal label). Since our method uses the de-
pendency formalism, to make a meaningful com-
parison we need to model the notion of a constituent
being correctly found by a parser. For a word   we
say that the constituent corresponding to its maxi-
mal projection is correctly identified if there exists

, the head of   , and for the dependency  

the
right part of its label (e.g., NP-SBJ for S   NP-SBJ) is
a nonterminal (i.e., not a POS tag) and matches the
right part of the label in the gold dependency struc-
ture, after stripping functional tags. Thus, the con-
stituent?s label and headword should be correct, but
not necessarily the span. Moreover, 2.5% of all con-
stituents with functional labels (246 out of 9928 in
section 23) are not maximal projections. Since our
method ignores functional tags of such constituents
(these tags disappear after the conversion of phrase
structures to dependency graphs), we consider them
as errors, i.e., reducing our recall value.
Below, the tagging accuracy, precision and recall
are evaluated on constituents correctly identified by
Charniak?s parser for section 23.
Method Accuracy P R f
Blaheta 98.6 87.2 87.4 87.3
This paper 94.7 90.2 86.9 88.5
The difference in the accuracy is due to two reasons.
First, because of the different definition of a cor-
rectly identified constituent in the parser?s output,
we apply our method to a greater portion of all la-
bels produced by the parser (95% vs. 89% reported
in (Blaheta and Charniak, 2000)). This might make
the task for out system more difficult. And second,
whereas 22% of all constituents in section 23 have a
functional tag, 36% of the maximal projections have
one. Since we apply our method only to labels of
maximal projections, this means that our accuracy
baseline (i.e., never assign any tag) is lower.
7 Step 2: Adding Missing Nodes
As the row labelled ?step 1? in Table 1 indicates,
for both parsers the recall is relatively low (6%
lower than the precision): while the WSJ trees,
and hence the derived dependency structures, con-
tain non-local dependencies and empty nodes, the
parsers simply do not provide this information. To
make up for this, we considered two further tran-
formations of the output of the parsers: adding new
nodes (corresponding to empty nodes in WSJ), and
adding new labelled arcs. This section describes the
former modification and Section 8 the latter.
As described in Section 4, when converting WSJ
trees to dependency structures, traces are resolved,
their empty nodes removed and new dependencies
introduced. Of the remaining empty nodes (i.e.,
non-traces), the most frequent in WSJ are: NP PRO,
empty units, empty complementizers, empty rela-
tive pronouns. To add missing empty nodes to de-
pendency graphs, we compared the output of the
parsers on the strings of the training corpus after
steps 0 and 1 (conversion to dependencies and re-
labelling) to the structures in the corpus itself. We
trained a classifier which, for every word in the
parser?s output, had to decide whether an empty
node should be added as a new dependent of the
word, and what its symbol (?*?, ?*U*? or ?0? in
WSJ), POS tag (always -NONE- in WSJ) and the
label of the new dependency (e.g., ?S   NP-SBJ? for
NP PRO and ?VP   SBAR? for empty complementiz-
ers) should be. This decision is conditioned on the
word itself and its context. The features used were:
 the word and its POS tag, whether the word
has any subject and object dependents, and
whether it is the head of a finite verb group;
 the same information for the word?s head (if
any) and also the label of the corresponding de-
pendency;
 the same information for the rightmost and
leftmost dependents of the word (if exist) along
with their dependency labels.
In total, we extracted 23 symbolic features for ev-
ery word in the corpus. TiMBL was trained on sec-
tions 02?21 and applied to the output of the parsers
(after steps 0 and 1) on the test corpus (section
23), producing a list of empty nodes to be inserted
in the dependency graphs. After insertion of the
empty nodes, the resulting structures were evaluated
against section 23 of the gold dependency treebank.
The results are shown in Table 1 (the row ?step 2?).
For both parsers the insertion of empty nodes im-
proves the recall by 1.5%, resulting in a 1% increase
of the f-score.
7.1 Comparison to Earlier Work
A procedure for empty node recovery was first de-
scribed in (Johnson, 2002), along with an evalua-
tion criterion: an empty node is correct if its cate-
gory and position in the sentence are correct. Since
our method works with dependency structures, not
phrase trees, we adopt a different but comparable
criterion: an empty node should be attached as a
dependent to the correct word, and with the correct
dependency label. Unlike the first metric, our cor-
rectness criterion also requires that possible attach-
ment ambiguities are resolved correctly (e.g., as in
the number of reports 0 they sent, where the empty
relative pronoun may be attached either to number
or to reports).
For this task, the best published results (using
Johnson?s metric) were reported by Dienes and
Dubey (2003), who used shallow tagging to insert
empty elements. Below we give the comparison to
our method. Notice that this evaluation does not in-
clude traces (i.e., empty elements with antecedents):
recovery of traces is described in Section 8.
Type
This paper Dienes&Dubey
P R f P R f
PRO-NP 73.1 63.89 68.1 68.7 70.4 69.5
COMP-SBAR 82.6 83.1 82.8 93.8 78.6 85.5
COMP-WHNP 65.3 40.0 49.6 67.2 38.3 48.8
UNIT 95.4 91.8 93.6 99.1 92.5 95.7
For comparison we use the notation of Dienes and
Dubey: PRO-NP for uncontrolled PROs (nodes ?*?
in the WSJ), COMP-SBAR for empty complemen-
tizers (nodes ?0? with dependency label VP   SBAR),
COMP-WHNP for empty relative pronouns (nodes
?0? with dependency label X   SBAR, where X
 
 VP)
and UNIT for empty units (nodes ?*U*?).
It is interesting to see that for empty nodes ex-
cept for UNIT both methods have their advantages,
showing better precision or better recall. Yet shal-
low tagging clearly performs better for UNIT.
8 Step 3: Adding Missing Dependencies
We now get to the third and final step of our trans-
formation method: adding missing arcs to depen-
dency graphs. The parsers we considered do not
explicitly provide information about non-local de-
pendencies (control, WH-extraction) present in the
treebank. Moreover, newly inserted empty nodes
(step 2, Section 7) might also need more links to the
rest of a sentence (e.g., the inserted empty comple-
mentizers). In this section we describe the insertion
of missing dependencies.
Johnson (2002) was the first to address recovery
of non-local dependencies in a parser?s output. He
proposed a pattern-matching algorithm: first, from
the training corpus the patterns that license non-
local dependencies are extracted, and then these pat-
terns are detected in unseen trees, dependencies be-
ing added when matches are found. Building on
these ideas, Jijkoun (2003) used a machine learning
classifier to detect matches. We extended Jijkoun?s
approach by providing the classifier with lexical in-
formation and using richer patterns with labels con-
taining the Penn functional tags and empty nodes,
detected at steps 1 and 2.
First, we compared the output of the parsers on
the strings of the training corpus after steps 0, 1 and
2 to the dependency structures in the training cor-
pus. For every dependency that is missing in the
parser?s output, we find the shortest undirected path
in the dependency graph connecting the head and
the dependent. These paths, connected sequences
of labelled dependencies, define the set of possible
patterns. For our experiments we only considered
patterns occuring more than 100 times in the train-
ing corpus. E.g., for Collins? parser, 67 different
patterns were found.
Next, from the parsers? output on the strings of
the training corpus, we extracted all occurrences of
the patterns, along with information about the nodes
involved. For every node in an occurrence of a pat-
tern we extracted the following features:
 the word and its POS tag;
 whether the word has subject and object depen-
dents;
 whether the word is the head of a finite verb
cluster.
We then trained TiMBL to predict the label of the
missing dependency (or ?none?), given an occur-
rence of a pattern and the features of all the nodes
involved. We trained a separate classifier for each
pattern.
For evaluation purposes we extracted all occur-
rences of the patterns and the features of their nodes
from the parsers? outputs for section 23 after steps
0, 1 and 2 and used TiMBL to predict and insert new
dependencies. Then we compared the resulting de-
pendency structures to the gold corpus. The results
are shown in Table 1 (the row ?step 3?). As ex-
pected, adding missing dependencies substantially
improves the recall (by 4% for both parsers) and
allows both parsers to achieve an 84% f-score on
dependencies with functional tags (90% on unla-
belled dependencies). The unlabelled f-score 89.9%
for Collins? parser is close to the 90.9% reported
in (Collins, 1999) for the evaluation on unlabelled
local dependencies only (without empty nodes and
traces). Since as many as 5% of all dependencies
in WSJ involve traces or empty nodes, the results in
Table 1 are encouraging.
8.1 Comparison to Earlier Work
Recently, several methods for the recovery of non-
local dependencies have been described in the lit-
erature. Johnson (2002) and Jijkoun (2003) used
pattern-matching on local phrase or dependency
structures. Dienes and Dubey (2003) used shallow
preprocessing to insert empty elements in raw sen-
tences, making the parser itself capable of finding
non-local dependencies. Their method achieves a
considerable improvement over the results reported
in (Johnson, 2002) and gives the best evaluation re-
sults published to date. To compare our results to
Dienes and Dubey?s, we carried out the transforma-
tion steps 0?3 described above, with a single mod-
ification: when adding missing dependencies (step
3), we only considered patterns that introduce non-
local dependencies (i.e., traces: we kept the infor-
mation whether a dependency is a trace when con-
verting WSJ to a dependency corpus).
As before, a dependency is correctly found if
its head, dependent, and label are correct. For
traces, this corresponds to the evaluation using the
head-based antecedent representation described in
(Johnson, 2002), and for empty nodes without an-
tecedents (e.g., NP PRO) this is the measure used
in Section 7.1. To make the results comparable to
other methods, we strip functional tags from the
dependency labels before label comparison. Be-
low are the overall precision, recall, and f-score for
our method and the scores reported in (Dienes and
Dubey, 2003) for antecedent recovery using Collins?
parser.
Method P R f
Dienes and Dubey 81.5 68.7 74.6
This paper 82.8 67.8 74.6
Interestingly, the overall performance of our post-
processing method is very similar to that of the
pre- and in-processing methods of Dienes and
Dubey (2003). Hence, for most cases, traces and
empty nodes can be reliably identified using only
local information provided by a parser, using the
parser itself as a black box. This is important, since
making parsers aware of non-local relations need
not improve the overall performance: Dienes and
Dubey (2003) report a decrease in PARSEVAL f-
score from 88.2% to 86.4% after modifying Collins?
parser to resolve traces internally, although this al-
lowed them to achieve high accuracy for traces.
9 Discussion
The experiments described in the previous sections
indicate that although statistical parsers do not ex-
plicitly output some information available in the
corpus they were trained on (grammatical and se-
mantic tags, empty nodes, non-local dependencies),
this information can be recovered with reasonably
high accuracy, using pattern matching and machine
learning methods.
For our task, using dependency structures rather
than phrase trees has several advantages. First, af-
ter converting both the treebank trees and parsers?
outputs to graphs with head?modifier relations, our
method needs very little information about the lin-
guistic nature of the data, and thus is largely corpus-
and parser-independent. Indeed, after the conver-
sion, the only linguistically informed operation is
the straightforward extraction of features indicating
the presence of subject and object dependents, and
finiteness of verb groups.
Second, using a dependency formalism facilitates
a very straightforward evaluation of the systems that
produce structures more complex than trees. It is
not clear whether the PARSEVAL evaluation can be
easily extended to take non-local relations into ac-
count (see (Johnson, 2002) for examples of such ex-
tension).
Finally, the independence from the details of the
parser and the corpus suggests that our method can
be applied to systems based on other formalisms,
e.g., (Hockenmaier, 2003), to allow a meaning-
ful dependency-based comparison of very different
parsers. Furthermore, with the fine-grained set of
dependency labels that our system provides, it is
possible to map the resulting structures to other de-
pendency formalisms, either automatically in case
annotated corpora exist, or with a manually devel-
oped set of rules. Our preliminary experiments with
Collins? parser and the corpus annotated with gram-
matical relations (Carroll et al, 2003) are promis-
ing: the system achieves 76% precision/recall f-
score, after the parser?s output is enriched with our
method and transformed to grammatical relations
using a set of 40 simple rules. This is very close
to the performance reported by Carroll et al (2003)
for the parser specifically designed for the extrac-
tion of grammatical relations.
Despite the high-dimensional feature spaces, the
large number of lexical features, and the lack of in-
dependence between features, we achieved high ac-
curacy using a memory-based learner. TiMBL per-
formed well on tasks where structured, more com-
plicated and task-specific statistical models have
been used previously (Blaheta and Charniak, 2000).
For all subtasks we used the same settings for
TiMBL: simple feature overlap measure, 5 nearest
neighbours with majority voting. During further ex-
periments with our method on different corpora, we
found that quite different settings led to a better per-
formance. It is clear that more careful and system-
atic parameter tuning and the analysis of the contri-
bution of different features have to be addressed.
Finally, our method is not restricted to syntac-
tic structures. It has been successfully applied
to the identification of semantic relations (Ahn et
al., 2004), using FrameNet as the training corpus.
For this task, we viewed semantic relations (e.g.,
Speaker, Topic, Addressee) as dependencies be-
tween a predicate and its arguments. Adding such
semantic relations to syntactic dependency graphs
was simply an additional graph transformation step.
10 Conclusions
We presented a method to automatically enrich the
output of a parser with information that is not pro-
vided by the parser itself, but is available in a tree-
bank. Using the method with two state of the art
statistical parsers and the Penn Treebank allowed
us to recover functional tags (grammatical and se-
mantic), empty nodes and traces. Thus, we are able
to provide virtually all information available in the
corpus, without modifying the parser, viewing it, in-
deed, as a black box.
Our method allows us to perform a meaningful
dependency-based comparison of phrase structure
parsers. The evaluation on a dependency corpus
derived from the Penn Treebank showed that, after
our post-processing, two state of the art statistical
parsers achieve 84% accuracy on a fine-grained set
of dependency labels.
Finally, our method for enriching the output of a
parser is, to a large extent, independent of a specific
parser and corpus, and can be used with other syn-
tactic and semantic resources.
11 Acknowledgements
We are grateful to David Ahn and Stefan Schlobach
and to the anonymous referees for their useful
suggestions. This research was supported by
grants from the Netherlands Organization for Scien-
tific Research (NWO) under project numbers 220-
80-001, 365-20-005, 612.069.006, 612.000.106,
612.000.207 and 612.066.302.
References
David Ahn, Sisay Fissaha, Valentin Jijkoun, and Maarten
de Rijke. 2004. The University of Amsterdam at
Senseval-3: semantic roles and logic forms. In Pro-
ceedings of the ACL-2004 Workshop on Evaluation of
Systems for the Semantic Analysis of Text.
Don Blaheta and Eugene Charniak. 2000. Assigning
function tags to parsed text. In Proceedings of the 1st
Meeting of NAACL, pages 234?240.
John Carroll, Guido Minnen, and Ted Briscoe. 2003.
Parser evaluation using a grammatical relation anno-
tation scheme. In Anne Abeille?, editor, Building and
Using Parsed Corpora, pages 299?316. Kluwer.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st Meeting of NAACL,
pages 132?139.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot,
and Antal van den Bosch, 2003. TiMBL: Tilburg
Memory Based Learner, version 5.0, Reference
Guide. ILK Technical Report 03-10. Available from
http://ilk.kub.nl/downloads/pub/papers/ilk0310.ps.gz.
Pe?ter Dienes and Amit Dubey. 2003. Antecedent recov-
ery: Experiments with a trace tagger. In Proceedings
of the 2003 Conference on Empirical Methods in Nat-
ural Language Processing, pages 33?40.
Julia Hockenmaier. 2003. Parsing with generative mod-
els of predicate-argument structure. In Proceedings of
the 41st Meeting of ACL, pages 359?366.
Valentin Jijkoun. 2003. Finding non-local dependen-
cies: Beyond pattern matching. In Proceedings of the
ACL-2003 Student Research Workshop, pages 37?43.
Mark Johnson. 2002. A simple pattern-matching al-
gorithm for recovering empty nodes and their an-
tecedents. In Proceedings of the 40th meeting of ACL,
pages 136?143.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Meeting of ACL, pages 423?430.
Alternative Approaches for Generating Bodies of Grammar Rules
Gabriel Infante-Lopez and Maarten de Rijke
Informatics Institute, University of Amsterdam
{infante,mdr}@science.uva.nl
Abstract
We compare two approaches for describing and gen-
erating bodies of rules used for natural language
parsing. In today?s parsers rule bodies do not ex-
ist a priori but are generated on the fly, usually with
methods based on n-grams, which are one particu-
lar way of inducing probabilistic regular languages.
We compare two approaches for inducing such lan-
guages. One is based on n-grams, the other on min-
imization of the Kullback-Leibler divergence. The
inferred regular languages are used for generating
bodies of rules inside a parsing procedure. We com-
pare the two approaches along two dimensions: the
quality of the probabilistic regular language they
produce, and the performance of the parser they
were used to build. The second approach outper-
forms the first one along both dimensions.
1 Introduction
N -grams have had a big impact on the state of the
art in natural language parsing. They are central
to many parsing models (Charniak, 1997; Collins,
1997, 2000; Eisner, 1996), and despite their sim-
plicity n-gram models have been very successful.
Modeling with n-grams is an induction task (Gold,
1967). Given a sample set of strings, the task is to
guess the grammar that produced that sample. Usu-
ally, the grammar is not be chosen from an arbitrary
set of possible grammars, but from a given class.
Hence, grammar induction consists of two parts:
choosing the class of languages amongst which to
search and designing the procedure for performing
the search. By using n-grams for grammar induc-
tion one addresses the two parts in one go. In par-
ticular, the use of n-grams implies that the solu-
tion will be searched for in the class of probabilis-
tic regular languages, since n-grams induce prob-
abilistic automata and, consequently, probabilistic
regular languages. However, the class of probabilis-
tic regular languages induced using n-grams is a
proper subclass of the class of all probabilistic reg-
ular languages; n-grams are incapable of capturing
long-distance relations between words. At the tech-
nical level the restricted nature of n-grams is wit-
nessed by the special structure of the automata in-
duced from them, as we will see in Section 4.2.
N -grams are not the only way to induce regular
languages, and not the most powerful way to do so.
There is a variety of general methods capable of in-
ducing all regular languages (Denis, 2001; Carrasco
and Oncina, 1994; Thollard et al, 2000). What is
their relevance for natural language parsing? Re-
call that regular languages are used for describing
the bodies of rules in a grammar. Consequently, the
quality and expressive power of the resulting gram-
mar is tied to the quality and expressive power of the
regular languages used to describe them. And the
quality and expressive power of the latter, in turn,
are influenced directly by the method used to induce
them. These observations give rise to a natural ques-
tion: can we gain anything in parsing from using
general methods for inducing regular languages in-
stead of methods based on n-grams? Specifically,
can we describe the bodies of grammatical rules
more accurately and more concisely by using gen-
eral methods for inducing regular languages?
In the context of natural language parsing we
present an empirical comparison between algo-
rithms for inducing regular languages using n-
grams on the one hand, and more general algorithms
for learning the general class of regular language on
the other hand. We proceed as follows. We gen-
erate our training data from the Wall Street Journal
Section of the Penn Tree Bank (PTB), by transform-
ing it to projective dependency structures, following
(Collins, 1996), and extracting rules from the result.
These rules are used as training material for the rule
induction algorithms we consider. The automata
produced this way are then used to build grammars
which, in turn, are used for parsing.
We are interested in two different aspects of the
use of probabilistic regular languages for natural
language parsing: the quality of the induced au-
tomata and the performance of the resulting parsers.
For evaluation purposes, we use two different met-
rics: perplexity for the first aspect and percentage
of correct attachments for the second. The main re-
sults of the paper are that, measured in terms of per-
plexity, the automata induced by algorithms other
than n-grams describe the rule bodies better than
automata induced using n-gram-based algorithms,
and that, moreover, the gain in automata quality
is reflected by an improvement in parsing perfor-
mance. We also find that the parsing performance
of both methods (n-grams vs. general automata) can
be substantially improved by splitting the training
material into POS categories. As a side product,
we find empirical evidence to suggest that the effec-
tiveness of rule lexicalization techniques (Collins,
1997; Sima?an, 2000) and parent annotation tech-
niques (Klein and Manning, 2003) is due to the fact
that both lead to a reduction in perplexity in the au-
tomata induced from training corpora.
Section 2 surveys our experiments, and later sec-
tions provide details of the various aspects. Sec-
tion 3 offers details on our grammatical frame-
work, PCW-grammars, on transforming automata
to PCW-grammars, and on parsing with PCW-
grammars. Section 4 explains the starting point of
this process: learning automata, and Section 5 re-
ports on parsing experiments. We discuss related
work in Section 6 and conclude in Section 7.
2 Overview
We want to build grammars using different algo-
rithms for inducing their rules. Our main question
is aimed at understanding how different algorithms
for inducing regular languages impact the parsing
performance with those grammars. A second issue
that we want to explore is how the grammars per-
form when the quality of the training material is im-
proved, that is, when the training material is sep-
arated into part of speech (POS) categories before
the regular language learning algorithms are run.
We first transform the PTB into projective depen-
dencies structures following (Collins, 1996). From
the resulting tree bank we delete all lexical informa-
tion except POS tags. Every POS in a tree belonging
to the tree-bank has associated to it two different,
possibly empty, sequences of right and left depen-
dents, respectively. We extract all these sequences
for all trees, producing two different sets containing
right and left sequences of dependents respectively.
These two sets form the training material used for
building four different grammars. The four gram-
mars differ along two dimensions: the number of
automata used for building them and the algorithm
used for inducing the automata. As to the latter di-
mension, in Section 4 we use two algorithms: the
Minimum Discriminative Information (MDI) algo-
rithm, and a bigram-based algorithm. As to the for-
mer dimension, two of the grammars are built us-
ing only two different automata, each of which is
built using the two sample set generated from the
PTB. The other two grammars were built using two
automata per POS, exploiting a split of the train-
ing samples into multiple samples, two samples per
POS, to be precise, each containing only those sam-
ples where the POS appeared as the head.
The grammars built from the induced automata
are so-called PCW-grammars (see Section 3), a for-
malism based on probabilistic context free gram-
mars (PCFGs); as we will see in Section 3, inferring
them from automata is almost immediate.
3 Grammatical Framework
We briefly detail the grammars we work with
(PCW-grammars), how automata give rise to these
grammars, and how we parse using them.
3.1 PCW-Grammars
We need a grammatical framework that models
rule bodies as instances of a regular language and
that allows us to transform automata to gram-
mars as directly as possible. We decided to em-
bed them in the general grammatical framework of
CW-grammars (Infante-Lopez and de Rijke, 2003):
based on PCFGs, they have a clear and well-
understood mathematical background and we do not
need to implement ad-hoc parsing algorithms.
A probabilistic constrained W-grammar (PCW-
grammar) consists of two different sets of PCF-like
rules called pseudo-rules and meta-rules respec-
tively and three pairwise disjoint sets of symbols:
variables, non-terminals and terminals. Pseudo-
rules and meta-rules provide mechanisms for build-
ing ?real? rewrite rules. We use ? w=? ? to indicate
that ? should be rewritten as ?. In the case of PCW-
grammars, rewrite rules are built by first selecting a
pseudo-rule, and then using meta-rules for instanti-
ating all the variables in the body of the pseudo-rule.
To illustrate these concepts, we provide an exam-
ple. Let W = (V,NT, T, S, m??, s??) be a CW-
grammar such that the set of variable, non-terminals
meta-rules pseudo-rules
Adj m??0.5 AdjAdj S s??1 AdjNoun
Adj m??0.5 Adj Adj s??0.1 big
Noun s??1 ball
.
.
.
and terminals are defined as follows: V = {Adj },
NT = {S, Adj , Noun}, T = {ball , big , fat ,
red , green , . . .}. As usual, the numbers attached
to the arrows indicate the probabilities of the rules.
The rules defined by W have the following shape:
S w=? Adj ? Noun . Suppose now that we want to
build the rule S w=? Adj Adj Noun . We take the
pseudo-rule S s??1 Adj Noun and instantiate the
variable Adj with Adj Adj to get the desired rule.
The probability for it is 1 ? 0.5 ? 0.5, that is, the
probability of the derivation for Adj Adj times the
probability of the pseudo-rule used. Trees for this
particular grammar are flat, with a main node S and
all the adjectives in it as daughters. An example
derivation is given in Figure 1(a).
3.2 From Automata to Grammars
Now that we have introduced PCW-grammars, we
describe how we build them from the automata
that we are going to induce in Section 4. Since
we will induce two families of automata (?Many-
Automata? where we use two automata per POS,
and ?One-Automaton? where we use only two au-
tomata to fit every POS), we need to describe two
automata-to-grammar transformations.
Let?s start with the case where we build two au-
tomata per POS. Let w be a POS in the PTB; let AwL
and AwR be the two automata associated to it. Let GwL
and GwR be the PCFGs equivalent to AwL and AwR, re-
spectively, following (Abney et al, 1999), and let
SwL and SwR be the starting symbols of GwL and GwR,
respectively. We build our final grammar G with
starting symbol S, by defining its meta-rules as the
disjoint union of all rules in GwL and GwR (for all POS
w), its set of pseudo-rules as the union of the sets
{W s??1 SwLwSwR and S
s??1 SwLwSwR}, where
W is a unique new variable symbol associated to w.
When we use two automata for all parts of
speech, the grammar is defined as follows. Let AL
and AR be the two automata learned. Let GL and
GR be the PCFGs equivalent to AL and AR, and let
SL and SR be the starting symbols of GL and GR,
respectively. Fix a POS w in the PTB. Since the au-
tomata are deterministic, there exist states SwL and
SwR that are reachable from SL and SR, respectively,
by following the arc labeled with w. Define a gram-
mar as in the previous case. Its starting symbol is S,
its set of meta-rules is the disjoint union of all rules
in GwL and GwR (for all POS w), its set of pseudo-
rules is {W s??1 SwLwSwR , S
s??1 SwLwSwR :
w is a POS in the PTB and W is a unique new vari-
able symbol associated to w}.
3.3 Parsing PCW-Grammars
Parsing PCW-grammars requires two steps: a
generation-rule step followed by a tree-building
step. We now explain how these two steps can be
carried out in one go. Parsing with PCW-grammars
can be viewed as parsing with PCF grammars. The
main difference is that in PCW-parsing derivations
for variables remain hidden in the final tree. To clar-
ify this, consider the trees depicted in Figure 1; the
tree in part (a) is the CW-tree corresponding to the
word red big green ball, and the tree in part (b) is
the same tree but now the instantiations of the meta-
rules that were used have been made visible.
S
Adj
red
Adj
big
Adj
green
Noun
ball
S
Adj 1
Adj 1
Adj 1
Adj
red
Adj
big
Adj
green
Noun
ball
(a) (b)
Figure 1: (a) A tree generated by W . (b) The same
tree with meta-rule derivations made visible.
To adapt a PCFG to parse CW-grammars, we
need to define a PCF grammar for a given PCW-
grammar by adding the two sets of rules while mak-
ing sure that all meta-rules have been marked some-
how. In Figure 1(b) the head symbols of meta-rules
have been marked with the superscript 1. After pars-
ing the sentence with the PCF parser, all marked
rules should be collapsed as shown in part (a).
4 Building Automata
The four grammars we intend to induce are com-
pletely defined once the underlying automata have
been built. We now explain how we build those au-
tomata from the training material. We start by de-
tailing how the material is generated.
4.1 Building the Sample Sets
We transform the PTB, sections 2?22, to depen-
dency structures, as suggested by (Collins, 1999).
All sentences containing CC tags are filtered out,
following (Eisner, 1996). We also eliminate all
word information, leaving only POS tags. For each
resulting dependency tree we extract a sample set of
right and left sequences of dependents as shown in
Figure 2. From the tree we generate a sample set
with all right sequences of dependents {, , }, and
another with all left sequences {, , red big green}.
The sample set used for automata induction is the
union of all individual tree sample sets.
4.2 Learning Probabilistic Automata
Probabilistic deterministic finite state automata
(PDFA) inference is the problem of inducing a
stochastic regular grammar from a sample set of
strings belonging to an unknown regular language.
The most direct approach for solving the task is by
SJJ
jj
red
JJ
jj
big
JJ
jj
green
nn
ball
ballgreenbigred
(a) (b)
jj jj nn
left right left right left right
    red big green 
(c)
Figure 2: (a), (b) Dependency representations of
Figure 1. (c) Sample instances extracted from this
tree.
using n-grams. The n-gram induction algorithm
adds a state to the resulting automaton for each se-
quence of symbols of length n it has seen in the
training material; it also adds an arc between states
a? and ?b labeled b, if the sequence a?b appears
in the training set. The probability assigned to the
arc (a?, ?b) is proportional to the number of times
the sequence a?b appears in the training set. For the
remainder, we take n-grams to be bigrams.
There are other approaches to inducing regular
grammars besides ones based on n-grams. The first
algorithm to learn PDFAs was ALERGIA (Carrasco
and Oncina, 1994); it learns cyclic automata with
the so-called state-merging method. The Minimum
Discrimination Information (MDI) algorithm (Thol-
lard et al, 2000) improves over ALERGIA and uses
Kullback-Leibler divergence for deciding when to
merge states. We opted for the MDI algorithm as
an alternative to n-gram based induction algorithms,
mainly because their working principles are rad-
ically different from the n-gram-based algorithm.
The MDI algorithm first builds an automaton that
only accepts the strings in the sample set by merg-
ing common prefixes, thus producing a tree-shaped
automaton in which each transition has a probability
proportional to the number of times it is used while
generating the positive sample.
The MDI algorithm traverses the lattice of all
possible partitions for this general automaton, at-
tempting to merge states that satisfy a trade-off that
can be specified by the user. Specifically, assume
that A1 is a temporary solution of the algorithm
and that A2 is a tentative new solution derived from
A1. ?(A1, A2) = D(A0||A2) ? D(A0||A1) de-
notes the divergence increment while going from
A1 to A2, where D(A0||Ai) is the Kullback-Leibler
divergence or relative entropy between the two
distributions generated by the corresponding au-
tomata (Cover and Thomas, 1991). The new solu-
tion A2 is compatible with the training data if the
divergence increment relative to the size reduction,
that is, the reduction of the number of states, is small
enough. Formally, let alha denote a compatibil-
ity threshold; then the compatibility is satisfied if
?(A1,A2)
|A1|?|A2| < alpha. For this learning algorithm,
alpha is the unique parameter; we tuned it to get
better quality automata.
4.3 Optimizing Automata
We use three measures to evaluate the quality of
a probabilistic automaton (and set the value of
alpha optimally). The first, called test sample
perplexity (PP), is based on the per symbol log-
likelihood of strings x belonging to a test sam-
ple according to the distribution defined by the au-
tomaton. Formally, LL = ? 1|S|
?
x?S log (P (x)),
where P (x) is the probability assigned to the string
x by the automata. The perplexity PP is defined as
PP = 2LL. The minimal perplexity PP = 1 is
reached when the next symbol is always predicted
with probability 1 from the current state, while
PP = |?| corresponds to uniformly guessing from
an alphabet of size |?|.
The second measure we used to evaluate the qual-
ity of an automaton is the number of missed samples
(MS). A missed sample is a string in the test sam-
ple that the automaton failed to accept. One such
instance suffices to have PP undefined (LL infinite).
Since an undefined value of PP only witnesses the
presence of at least one MS we decided to count the
number of MS separately, and compute PP without
taking MS into account. This choice leads to a more
accurate value of PP, while, moreover, the value of
MS provides us with information about the general-
ization capacity of automata: the lower the value of
MS, the larger the generalization capacities of the
automaton. The usual way to circumvent undefined
perplexity is to smooth the resulting automaton with
unigrams, thus increasing the generalization capac-
ity of the automaton, which is usually paid for with
an increase in perplexity. We decided not to use
any smoothing techniques as we want to compare
bigram-based automata with MDI-based automata
in the cleanest possible way. The PP and MS mea-
sures are relative to a test sample; we transformed
section 00 of the PTB to obtain one.1
1If smoothing techniques are used for optimizing automata
based on n-grams, they should also be used for optimizing
MDI-based automata. A fair experiment for comparing the
two automata-learning algorithms using smoothing techniques
would consist of first building two pairs of automata. The first
pair would consist of the unigram-based automaton together
The third measure we used to evaluate the quality
of automata concerns the size of the automata. We
compute NumEdges and NumStates (the number of
edges and the number of states of the automaton).
We used PP, US, NumEdges, and NumStates to
compare automata. We say that one automaton is of
a better quality than another if the values of the 4
indicators are lower for the first than for the sec-
ond. Our aim is to find a value of alpha that
produces an automaton of better quality than the
bigram-based counterpart. By exhaustive search,
using all training data, we determined the optimal
value of alpha. We selected the value of alpha
for which the MDI-based automaton outperforms
the bigram-based one.2
We exemplify our procedure by considering au-
tomata for the ?One-Automaton? setting (where we
used the same automata for all parts of speech). In
Figure 3 we plot all values of PP and MS computed
for different values of alpha, for each training set
(i.e., left and right). From the plots we can identify
values of alpha that produce automata having bet-
ter values of PP and MS than the bigram-based ones.
All such alphas are the ones inside the marked
areas; automata induced using those alphas pos-
sess a lower value of PP as well as a smaller num-
ber of MS, as required. Based on these explorations
MDI Bigrams
Right Left Right Left
NumEdges 268 328 20519 16473
NumStates 12 15 844 755
Table 1: Automata sizes for the ?One-Automaton?
case, with alpha = 0.0001.
we selected alpha = 0.0001 for building the au-
tomata used for grammar induction in the ?One-
Automaton? case. Besides having lower values of
PP and MS, the resulting automata are smaller than
the bigram based automata (Table 1). MDI com-
presses information better; the values in the tables
with an MDI-based automaton outperforming the unigram-
based one. The second one, a bigram-based automata together
with an MDI-based automata outperforming the bigram-based
one. Second, the two n-gram based automata smoothed into a
single automaton have to be compared against the two MDI-
based automata smoothed into a single automaton. It would
be hard to determine whether the differences between the final
automata are due to smoothing procedure or to the algorithms
used for creating the initial automata. By leaving smoothing
out of the picture, we obtain a clearer understanding of the dif-
ferences between the two automata induction algorithms.
2An equivalent value of alpha can be obtained indepen-
dently of the performance of the bigram-based automata by
defining a measure that combines PP and MS. This measure
should reach its maximum when PP and MS reach their mini-
mums.
suggest that MDI finds more regularities in the sam-
ple set than the bigram-based algorithm.
To determine optimal values for the ?Many-
Automata? case (where we learned two automata
for each POS) we used the same procedure as
for the ?One-Automaton? case, but now for ev-
ery individual POS. Because of space constraints
we are not able to reproduce analogues of Fig-
ure 3 and Table 1 for all parts of speech. Figure 4
contains representative plots; the remaining plots
are available online at http://www.science.
uva.nl/?infante/POS.
Besides allowing us to find the optimal alphas,
the plots provide us with a great deal of informa-
tion. For instance, there are two remarkable things
in the plots for VBP (Figure 4, second row). First,
it is one of the few examples where the bigram-
based algorithm performs better than the MDI al-
gorithm. Second, the values of PP in this plot are
relatively high and unstable compared to other POS
plots. Lower perplexity usually implies better qual-
ity automata, and as we will see in the next section,
better automata produce better parsers. How can we
obtain lower PP values for the VBP automata? The
class of words tagged with VBP harbors many dif-
ferent behaviors, which is not surprising, given that
verbs can differ widely in terms of, e.g., their sub-
categorization frames. One way to decrease the PP
values is to split the class of words tagged with VBP
into multiple, more homogeneous classes. Note
from Figures 3 and 4 that splitting the original sam-
ple sets into POS-dependent sets produces a huge
decrease on PP. One attempt to implement this idea
is lexicalization: increasing the information in the
POS tag by adding the lemma to it (Collins, 1997;
Sima?an, 2000). Lexicalization splits the class of
verbs into a family of singletons producing more ho-
mogeneous classes, as desired. A different approach
(Klein and Manning, 2003) consists in adding head
information to dependents; words tagged with VBP
are then split into classes according to the words that
dominate them in the training corpus.
Some POS present very high perplexities, but
tags such as DT present a PP close to 1 (and 0 MS)
for all values of alpha. Hence, there is no need
to introduce further distinctions in DT, doing so will
not increase the quality of the automata but will in-
crease their number; splitting techniques are bound
to add noise to the resulting grammars. The plots
also indicate that the bigram-based algorithm cap-
tures them as well as the MDI algorithm.
In Figure 4, third row, we see that the MDI-based
automata and the bigram-based automata achieve
the same value of PP (close to 5) for NN, but
 0
 5
 10
 15
 20
 25
 5e-05  0.0001  0.00015  0.0002  0.00025  0.0003  0.00035  0.0004
Alpha
Unique Automaton - Left Side
MDI Perplex. (PP)
Bigram Perplex. (PP)
MDI Missed Samples (MS)
Bigram Missed Samples (MS)
 0
 5
 10
 15
 20
 25
 30
 5e-05  0.0001  0.00015  0.0002  0.00025  0.0003  0.00035  0.0004
Alpha
Unique Automaton - Right Side
MDI Perplex. (PP)
Bigram Perplex. (PP)
MDI Missed Samples (MS)
Bigram Missed Samples (MS)
Figure 3: Values of PP and MS for automata used in building One-Automaton grammars. (X-axis): alpha.
(Y-axis): missed samples (MS) and perplexity (PP). The two constant lines represent the values of PP and
MS for the bigram-based automata.
 3
 4
 5
 6
 7
 8
 9
0.
0e
+0
0
2.
0e
-0
5
4.
0e
-0
5
6.
0e
-0
5
8.
0e
-0
5
1.
0e
-0
4
1.
2e
-0
4
1.
4e
-0
4
1.
6e
-0
4
1.
8e
-0
4
2.
0e
-0
4
Alpha
VBP - LeftSide
MDI Perplex. (PP)
Bigram Perplex. (PP)
MDI Missed Samples (MS)
Bigram Missed Samples (MS)
 3
 4
 5
 6
 7
 8
 9
0.
0e
+0
0
2.
0e
-0
5
4.
0e
-0
5
6.
0e
-0
5
8.
0e
-0
5
1.
0e
-0
4
1.
2e
-0
4
1.
4e
-0
4
1.
6e
-0
4
1.
8e
-0
4
2.
0e
-0
4
Alpha
VBP - LeftSide
MDI Perplex. (PP)
Bigram Perplex. (PP)
MDI Missed Samples (MS)
Bigram Missed Samples (MS)
 0
 5
 10
 15
 20
 25
 30
0.
0e
+0
0
2.
0e
-0
5
4.
0e
-0
5
6.
0e
-0
5
8.
0e
-0
5
1.
0e
-0
4
1.
2e
-0
4
1.
4e
-0
4
1.
6e
-0
4
1.
8e
-0
4
2.
0e
-0
4
Alpha
NN - LeftSide
MDI Perplex. (PP)
Bigram Perplex. (PP)
MDI Missed Samples (MS)
Bigram Missed Samples (MS)
 0
 5
 10
 15
 20
 25
 30
0.
0e
+0
0
2.
0e
-0
5
4.
0e
-0
5
6.
0e
-0
5
8.
0e
-0
5
1.
0e
-0
4
1.
2e
-0
4
1.
4e
-0
4
1.
6e
-0
4
1.
8e
-0
4
2.
0e
-0
4
Alpha
NN - RightSide
MDI Perplex. (PP)
Bigram Perplex. (PP)
MDI Missed Samples (MS)
Bigram Missed Samples (MS)
Figure 4: Values of PP and MS for automata for ad-hoc automata
the MDI misses fewer examples for alphas big-
ger than 1.4e ? 04. As pointed out, we built the
One-Automaton-MDI using alpha = 0.0001 and
even though the method allows us to fine-tune each
alpha in the Many-Automata-MDI grammar, we
used a fixed alpha = 0.0002 for all parts of speech,
which, for most parts of speech, produces better au-
tomata than bigrams. Table 2 lists the sizes of the
automata. The differences between MDI-based and
bigram-based automata are not as dramatic as in
the ?One-Automaton? case (Table 1), but the former
again have consistently lower NumEdges and Num-
States values, for all parts of speech, even where
bigram-based automata have a lower perplexity.
MDI Bigrams
POS Right Left Right Left
DT NumEdges 21 14 35 39
NumStates 4 3 25 17
VBP NumEdges 300 204 2596 1311
NumStates 50 45 250 149
NN NumEdges 104 111 3827 4709
NumStates 6 4 284 326
Table 2: Automata sizes for the three parts of speech
in the ?Many-Automata? case, with alpha =
0.0002 for parts of speech.
5 Parsing the PTB
We have observed remarkable differences in quality
between MDI-based and bigram-based automata.
Next, we present the parsing scores, and discuss the
meaning of the measures observed for automata in
the context of the grammars they produce. The mea-
sure that translates directly from automata to gram-
mars is automaton size. Since each automaton is
transformed into a PCFG, the number of rules in
the resulting grammar is proportional to the number
of arcs in the automaton, and the number of non-
terminals is proportional to the number of states.
From Table 3 we see that MDI compresses informa-
tion better: the sizes of the grammars produced by
the MDI-based automata are an order of magnitude
smaller that those produced using bigram-based au-
tomata. Moreover, the ?One-Automaton? versions
substantially reduce the size of the resulting gram-
mars; this is obviously due to the fact that all POS
share the same underlying automaton so that infor-
mation does not need to be duplicated across parts
of speech. To understand the meaning of PP and
One Automaton Many Automata
MDI Bigram MDI Bigram
702 38670 5316 68394
Table 3: Number of rules in the grammars built.
MS in the context of grammars it helps to think of
PCW-parsing as a two-phase procedure. The first
phase consists of creating the rules that will be used
in the second phase. And the second phase con-
sists in using the rules created in the first phase as a
PCFG and parsing the sentence using a PCF parser.
Since regular expressions are used to build rules, the
values of PP and MS quantify the quality of the set
of rules built for the second phase: MS gives us a
measure of the number rule bodies that should be
created but that will not be created, and, hence, it
gives us a measure of the number of ?correct? trees
that will not be produced. PP tells us how uncertain
the first phase is about producing rules.
Finally, we report on the parsing accuracy. We
use two measures, the first one (%Words) was pro-
posed by Lin (1995) and was the one reported in
(Eisner, 1996). Lin?s measure computes the frac-
tion of words that have been attached to the right
word. The second one (%POS) marks as correct a
word attachment if, and only if, the POS tag of the
head is the same as that of the right head, i.e., the
word was attached to the correct word-class, even
though the word is not the correct one in the sen-
tence. Clearly, the second measure is always higher
than the first one. The two measures try to cap-
ture the performance of the PCW-parser in the two
phases described above: (%POS) tries to capture
the performance in the first phase, and (%Words) in
the second phase. The measures reported in Table 4
are the mean values of (%POS) and (%Words) com-
puted over all sentences in section 23 having length
at most 20. We parsed only those sentences because
the resulting grammars for bigrams are too big:
parsing all sentences without any serious pruning
techniques was simply not feasible. From Table 4
MDI Bigrams
%Words %POS %Words %POS
One-Aut. 0.69 0.73 0.59 0.63
Many-Aut. 0.85 0.88 0.73 0.76
Table 4: Parsing results for the PTB
we see that the grammars induced with MDI out-
perform the grammars created with bigrams. More-
over, the grammar using different automata per POS
outperforms the ones built using only a single au-
tomaton per side (left or right). The results suggest
that an increase in quality of the automata has a di-
rect impact on the parsing performance.
6 Related Work and Discussion
Modeling rule bodies is a key component of parsers.
N -grams have been used extensively for this pur-
pose (Collins 1996, 1997; Eisner, 1996). In these
formalisms the generative process is not considered
in terms of probabilistic regular languages. Con-
sidering them as such (like we do) has two ad-
vantages. First, a vast area of research for induc-
ing regular languages (Carrasco and Oncina, 1994;
Thollard et al, 2000; Dupont and Chase, 1998)
comes in sight. Second, the parsing device itself can
be viewed under a unifying grammatical paradigm
like PCW-grammars (Chastellier and Colmerauer,
1969; Infante-Lopez and de Rijke, 2003). As PCW-
grammars are PCFGs plus post tree transformations,
properties of PCFGs hold for them too (Booth and
Thompson, 1973).
In our comparison we optimized the value of
alpha, but we did not optimize the n-grams, as
doing so would mean two different things. First,
smoothing techniques would have to be used to
combine different order n-grams. To be fair, we
would also have to smooth different MDI-based au-
tomata, which would leave us in the same point.
Second, the degree of the n-gram. We opted for
n = 2 as it seems the right balance of informative-
ness and generalization. N -grams are used to model
sequences of arguments, and these hardly ever have
length > 3, making higher degrees useless. To make
a fair comparison for the Many-Automata grammars
we did not tune the MDI-based automata individu-
ally, but we picked a unique alpha.
MDI presents a way to compact rule informa-
tion on the PTB; of course, other approaches exists.
In particular, Krotov et al (1998) try to induce a
CW-grammar from the PTB with the underlying as-
sumption that some derivations that were supposed
to be hidden were left visible. The attempt to use
algorithms other than n-grams-based for inducing
of regular languages in the context of grammar in-
duction is not new; for example, Kruijff (2003) uses
profile hidden models in an attempt to quantify free
order variations across languages; we are not aware
of evaluations of his grammars as parsing devices.
7 Conclusions and Future Work
Our experiments support two kinds of conclusions.
First, modeling rules with algorithms other than
n-grams not only produces smaller grammars but
also better performing ones. Second, the proce-
dure used for optimizing alpha reveals that some
POS behave almost deterministically for selecting
their arguments, while others do not. These find-
ings suggests that splitting classes that behave non-
deterministically into homogeneous ones could im-
prove the quality of the inferred automata. We saw
that lexicalization and head-annotation seem to at-
tack this problem. Obvious questions for future
work arise: Are these two techniques the best way to
split non-homogeneous classes into homogeneous
ones? Is there an optimal splitting?
Acknowledgments
We thank our referees for valuable comments. Both
authors were supported by the Netherlands Organi-
zation for Scientific Research (NWO) under project
number 220-80-001. De Rijke was also supported
by grants from NWO, under project numbers 365-
20-005, 612.069.006, 612.000.106, 612.000.207,
and 612.066.302.
References
S. Abney, D. McAllester, and F. Pereira. 1999. Relating
probabilistic grammars and automata. In Proc. 37th
Annual Meeting of the ACL, pages 542?549.
T. Booth and R. Thompson. 1973. Applying probability
measures to abstract languages. IEEE Transaction on
Computers, C-33(5):442?450.
R. Carrasco and J. Oncina. 1994. Learning stochastic
regular grammars by means of state merging method.
In Proc. ICGI-94, Springer, pages 139?150.
E. Charniak. 1997. Statistical parsing with a context-
free grammar and word statistics. In Proc. 14th Nat.
Conf. on Artificial Intelligence, pages 598?603.
G. Chastellier and A. Colmerauer. 1969. W-grammar.
In Proc. 1969 24th National Conf., pages 511?518.
M. Collins. 1996. A new statistical parser based on
bigram lexical dependencies. In Proc. 34th Annual
Meeting of the ACL, pages 184?191.
M. Collins. 1997. Three generative, lexicalized models
for statistical parsing. In Proc. 35th Annual Meeting
of the ACL and 8th Conf. of the EACL, pages 16?23.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania, PA.
M. Collins. 2000. Discriminative reranking for natural
language parsing. In Proc. ICML-2000, Stanford, Ca.
T. Cover and J. Thomas. 1991. Elements of Information
Theory. Jonh Wiley and Sons, New York.
F. Denis. 2001. Learning regular languages from simple
positive examples. Machine Learning, 44(1/2):37?66.
P. Dupont and L. Chase. 1998. Using symbol cluster-
ing to improve probabilistic automaton inference. In
Proc. ICGI-98, pages 232?243.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proc. COLING-
96, pages 340?245, Copenhagen, Denmark.
J. Eisner. 2000. Bilexical grammars and their cubic-time
parsing algorithms. In Advances in Probabilistic and
Other Parsing Technologies, pages 29?62. Kluwer.
E. M. Gold. 1967. Language identification in the limit.
Information and Control, 10:447?474.
G. Infante-Lopez and M. de Rijke. 2003. Natural lan-
guage parsing with W-grammars. In Proc. CLIN
2003.
D. Klein and C. Manning. 2003. Accurate unlexicalized
parsing. In Proc. 41st Annual Meeting of the ACL.
A. Krotov, M. Hepple, R.J. Gaizauskas, and Y. Wilks.
1998. Compacting the Penn Treebank grammar. In
Proc. COLING-ACL, pages 699?703.
G. Kruijff. 2003. 3-phase grammar learning. In Proc.
Workshop on Ideas and Strategies for Multilingual
Grammar Development.
D. Lin. 1995. A dependency-based method for evaluat-
ing broad-coverage parsers. In Proc. IJCAI-95.
K. Sima?an. 2000. Tree-gram Parsing: Lexical Depen-
dencies and Structual Relations. In Proc. 38th Annual
Meeting of the ACL, pages 53?60, Hong Kong, China.
F. Thollard, P. Dupont, and C. de la Higuera. 2000.
Probabilistic DFA inference using kullback-leibler di-
vergence and minimality. In Proc. ICML 2000.
Proceedings of ACL-08: HLT, pages 923?931,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Credibility Improves Topical Blog Post Retrieval
Wouter Weerkamp
ISLA, University of Amsterdam
weerkamp@science.uva.nl
Maarten de Rijke
ISLA, University of Amsterdam
mdr@science.uva.nl
Abstract
Topical blog post retrieval is the task of rank-
ing blog posts with respect to their relevance
for a given topic. To improve topical blog post
retrieval we incorporate textual credibility in-
dicators in the retrieval process. We consider
two groups of indicators: post level (deter-
mined using information about individual blog
posts only) and blog level (determined using
information from the underlying blogs). We
describe how to estimate these indicators and
how to integrate them into a retrieval approach
based on language models. Experiments on
the TREC Blog track test set show that both
groups of credibility indicators significantly
improve retrieval effectiveness; the best per-
formance is achieved when combining them.
1 Introduction
The growing amount of user generated content avail-
able online creates new challenges for the informa-
tion retrieval (IR) community, in terms of search and
analysis tasks for this type of content. The introduc-
tion of a blog retrieval track at TREC (Ounis et al,
2007) has created a platform where we can begin to
address these challenges. During the 2006 edition
of the track, two types of blog post retrieval were
considered: topical (retrieve posts about a topic)
and opinionated (retrieve opinionated posts about a
topic). Here, we consider the former task.
Blogs and blog posts offer unique features that
may be exploited for retrieval purposes. E.g.,
Mishne (2007b) incorporates time in a blog post
retrieval model to account for the fact that many
blog queries and posts are a response to a news
event (Mishne and de Rijke, 2006). Data quality
is an issue with blogs?the quality of posts ranges
from low to edited news article-like. Some ap-
proaches to post retrieval use indirect quality mea-
sures (e.g., elaborate spam filtering (Java et al,
2007) or counting inlinks (Mishne, 2007a)).
Few systems turn the credibility (Metzger, 2007)
of blog posts into an aspect that can benefit the re-
trieval process. Our hypothesis is that more credible
blog posts are preferred by searchers. The idea of us-
ing credibility in the blogosphere is not new: Rubin
and Liddy (2006) define a framework for assessing
blog credibility, consisting of four main categories:
blogger?s expertise and offline identity disclosure;
blogger?s trustworthiness and value system; infor-
mation quality; and appeals and triggers of a per-
sonal nature. Under these four categories the authors
list a large number of indicators, some of which can
be determined from textual sources (e.g., literary ap-
peal), and some of which typically need non-textual
evidence (e.g., curiosity trigger); see Section 2.
We give concrete form to Rubin and Liddy
(2006)?s indicators and test their impact on blog post
retrieval effectiveness. We do not consider all indi-
cators: we only consider indicators that are textual in
nature, and to ensure reproducibility of our results,
we only consider indicators that can be derived from
the TRECBlog06 corpus (and that do not need addi-
tional resources such as bloggers? profiles that may
be hard to obtain for technical or legal reasons).
We detail and implement two groups of credibility
indicators: post level (these use information about
individual posts) and blog level (these use informa-
tion from the underlying blogs). Within the post
level group, we distinguish between topic depen-
dent and independent indicators. To make matters
concrete, consider Figure 1: both posts are relevant
to the query ?tennis,? but based on obvious surface
level features of the posts we quickly determine Post
2 to be more credible than Post 1. The most obvious
features are spelling errors, the lack of leading capi-
tals, and the large number of exclamation marks and
923
Post 1
as for today (monday) we had no school! yaay
labor day. but we had tennis from 9-11 at the
highschool. after that me suzi melis & ashley
had a picnic at cecil park and then played ten-
nis. i just got home right now. it was a very
very very fun afternoon. (...) we will have a
short week. mine will be even shorter b/c i
wont be there all day on friday cuz we have
the Big 7 Tournament at like keystone oaks or
sumthin. so i will miss school the whole day.
Post 2
Wimbledon champion Venus Williams has
pulled out of next week?s Kremlin Cup with
a knee injury, tournament organisers said on
Friday. The American has not played since
pulling out injured of last month?s China
Open. The former world number one has been
troubled by various injuries (...) Williams?s
withdrawal is the latest blow for organisers af-
ter Australian Open champion and home fa-
vorite Marat Safin withdrew (...).
Figure 1: Two blog posts relevant to the query ?tennis.?
personal pronouns?i.e., topic independent ones?
and the fact that the language usage in the second
post is more easily associated with credible infor-
mation about tennis than the language usage in the
first post?i.e., a topic dependent feature.
Our main finding is that topical blog post retrieval
can benefit from using credibility indicators in the
retrieval process. Both post and blog level indi-
cator groups each show a significant improvement
over the baseline. When we combine all features
we obtain the best retrieval performance, and this
performance is comparable to the best performing
TREC 2006 and 2007 Blog track participants. The
improvement over the baseline is stable across most
topics, although topic shift occurs in a few cases.
The rest of the paper is organized as follows. In
Section 2 we provide information on determining
credibility; we also relate previous work to the cred-
ibility indicators that we consider. Section 3 speci-
fies our retrieval model, a method for incorporating
credibility indicators in our retrieval model, and es-
timations of credibility indicators. Section 4 gives
the results of our experiments aimed at assessing
the contribution of credibility towards blog post re-
trieval effectiveness. We conclude in Section 5.
2 Credibility Indicators
In our choice of credibility indicators we use (Ru-
bin and Liddy, 2006)?s work as a reference point.
We recall the main points of their framework and
relate our indicators to it. We briefly discuss other
credibility-related indicators found in the literature.
2.1 Rubin and Liddy (2006)?s work
Rubin and Liddy (2006) proposed a four factor an-
alytical framework for blog-readers? credibility as-
sessment of blog sites, based in part on evidential-
ity theory (Chafe, 1986), website credibility assess-
ment surveys (Stanford et al, 2002), and Van House
(2004)?s observations on blog credibility. The four
factors?plus indicators for each of them?are:
1. blogger?s expertise and offline identity disclo-
sure (a: name and geographic location; b: cre-
dentials; c: affiliations; d: hyperlinks to others;
e: stated competencies; f : mode of knowing);
2. blogger?s trustworthiness and value system (a:
biases; b: beliefs; c: opinions; d: honesty; e:
preferences; f : habits; g: slogans)
3. information quality (a: completeness; b: ac-
curacy; c: appropriateness; d: timeliness; e:
organization (by categories or chronology); f :
match to prior expectations; g: match to infor-
mation need); and
4. appeals and triggers of a personal nature (a:
aesthetic appeal; b: literary appeal (i.e., writing
style); c: curiosity trigger; d: memory trigger;
e: personal connection).
2.2 Our credibility indicators
We only consider credibility indicators that avoid
making use of the searcher?s or blogger?s identity
(i.e., excluding 1a, 1c, 1e, 1f, 2e from Rubin and
Liddy?s list), that can be estimated automatically
from available test collections only so as to facilitate
repeatability of our experiments (ruling out 3e, 4a,
4c, 4d, 4e), that are textual in nature (ruling out 2d),
and that can be reliably estimated with state-of-the-
art language technology (ruling out 2a, 2b, 2c, 2g).
For reasons that we explain below, we also ignore
the ?hyperlinks to others? indicator (1d).
The indicators that we do consider?1b, 2f, 3a,
3b, 3c, 3d, 3f, 3g, 4b?are organized in two groups,
924
depending on the information source that we use
to estimate them, post level and blog level, and the
former is further subdivided into topic independent
and topic dependent. Table 1 lists the indicators we
consider, together with the corresponding Rubin and
Liddy indicator(s).
Let us quickly explain our indicators. First, we
consider the use of capitalization to be an indicator
of good writing style, which in turn contributes to
a sense of credibility. Second, we identify West-
ern style emoticons (e.g., :-) and :-D) in blog
posts, and assume that excessive use indicates a less
credible blog post. Third, words written in all caps
are considered shouting in a web environment; we
consider shouting to be indicative for non-credible
posts. Fourth, a credible author should be able to
write without (a lot of) spelling errors; the more
spelling errors occur in a blog post, the less credi-
ble we consider it to be. Fifth, we assume that cred-
ible texts have a reasonable length; the text should
supply enough information to convince the reader of
the author?s credibility. Sixth, assuming that much
of what goes on in the blogosphere is inspired by
events in the news (Mishne and de Rijke, 2006), we
believe that, for news related topics, a blog post is
more credible if it is published around the time of
the triggering news event (timeliness). Seventh, our
semantic indicator also exploits the news-related na-
ture of many blog posts, and ?prefers? posts whose
language usage is similar to news stories on the
topic. Eighth, blogs are a popular place for spam-
mers; spam blogs are not considered credible and we
want to demote them in the search results. Ninth,
comments are a notable blog feature: readers of a
blog post often have the possibility of leaving a com-
ment for other readers or the author. When peo-
ple comment on a blog post they apparently find the
post worth putting effort in, which can be seen as an
indicator of credibility (Mishne and Glance, 2006).
Tenth, blogs consist of multiple posts in (reverse)
chronological order. The temporal aspect of blogs
may indicate credibility: we assume that bloggers
with an irregular posting behavior are less credible
than bloggers who post regularly. And, finally, we
consider the topical fluctuation of a blogger?s posts.
When looking for credible information we would
like to retrieve posts from bloggers that have a cer-
tain level of (topical) consistency: not the fluctuating
indicator topic de- post level/ related Rubin &
pendent? blog level Liddy indicator
capitalization no post 4b
emoticons no post 4b
shouting no post 4b
spelling no post 4b
post length no post 3a
timeliness yes post 3d
semantic yes post 3b, 3c
spam no blog 3b, 3c, 3f, 3g
comments no blog 1b
regularity no blog 2f
consistency no blog 2f
Table 1: Credibility indicators
behavior of a (personal) blogger, but a solid interest.
2.3 Other work
In a web setting, credibility is often couched in
terms of authoritativeness and estimated by exploit-
ing the hyperlink structure. Two well-known exam-
ples are the PageRank and HITS algorithms (Liu,
2007), that use the link structure in a topic indepen-
dent or topic dependent way, respectively. Zhou and
Croft (2005) propose collection-document distance
and signal-to-noise ratio as priors for the indication
of quality in web ad hoc retrieval. The idea of using
link structure for improving blog post retrieval has
been researched, but results do not show improve-
ments. E.g., Mishne (2007a) finds that retrieval per-
formance decreased. This confirms lessons from
the TREC web tracks, where participants found no
conclusive benefit from the use of link information
for ad hoc retrieval tasks (Hawking and Craswell,
2002). Hence, we restrict ourselves to the use of
content-based features for blog post retrieval, thus
ignoring indicator 1d (hyperlinks to others).
Related to credibility in blogs is the automatic as-
sessment of forum post quality discussed by Weimer
et al (2007). The authors use surface, lexical, syn-
tactic and forum-specific features to classify forum
posts as bad posts or good posts. The use of forum-
specific features (such as whether or not the post
contains HTML, and the fraction of characters that
are inside quotes of other posts), gives the highest
benefits to the classification. Working in the com-
munity question/answering domain, Agichtein et al
(2008) use a content features, as well non-content in-
formation available, such as links between items and
925
explicit quality ratings from members of the com-
munity to identify high-quality content.
As we argued above, spam identification may be
part of estimating a blog (or blog post?s) credibility.
Spam identification has been successfully applied in
the blogosphere to improve retrieval effectiveness;
see, e.g., (Mishne, 2007b; Java et al, 2007).
3 Modeling
In this section we detail the retrieval model that we
use, incorporating ranking by relevance and by cred-
ibility. We also describe how we estimate the credi-
bility indicators listed in Section 2.
3.1 Baseline retrieval model
We address the baseline retrieval task using a
language modeling approach (Croft and Lafferty,
2003), where we rank documents given a query:
p(d|q) = p(d)p(q|d)p(q)?1. Using Bayes? Theo-
rem we rewrite this, ignoring expressions that do not
influence the ranking, obtaining
p(d|q) ? p(d)p(q|d), (1)
and, assuming that query terms are independent,
p(d|q) ? p(d)
?
t?q p(t|?d)
n(t,q), (2)
where ?d is the blog post model, and n(t, q) denotes
the number of times term t occurs in query q. To
prevent numerical underflows, we perform this com-
putation in the log domain:
log p(d|q) ? log p(d) +
?
t?q
n(t, q) log p(t|?d) (3)
In our final formula for ranking posts based on rel-
evance only we substitute n(t, q) by the probability
of the term given the query. This allows us to assign
different weights to query terms and yields:
log p(d|q) ? log p(d) +
?
t?q
p(t|q) log p(t|?d). (4)
For our baseline experiments we assume that all
query terms are equally important and set p(t|q) set
to be n(t, q) ? |q|?1. The component p(d) is the topic
independent (?prior?) probability that the document
is relevant; in the baseline model, priors are ignored.
3.2 Incorporating credibility
Next, we extend Eq. 4 by incorporating estimations
of the credibility indicators listed in Table 1. Recall
that our credibility indicators come in two kinds?
post level and blog level?and that the post level
indicators can be topic indepedent or topic depen-
dent, while all blog level indicators are topic inde-
pendent. Now, modeling topic independent indi-
cators is easy?they can simply be incorporated in
Eq. 4 as a weighted sum of two priors:
p(d) = ? ? ppl(d) + (1? ?) ? pbl(d), (5)
where ppl(d) and pbl(d) are the post level and blog
level prior probability of d, respectively. The priors
ppl and pbl are defined as equally weighted sums:
ppl(d) =
?
i
1
5 ? pi(d)
pbl(d) =
?
j
1
4 ? pj(d),
where i ? {capitalization, emoticons, shouting,
spelling, post length} and j ? {spam, comments,
regularity, consistency}. Estimations of the priors
pi and pj are given below; the weighting parameter
? is determined experimentally.
Modeling topic dependent indicators is slighty
more involved. Given a query q, we create a query
model ?q that is a mixture of a temporal query model
?temporal and a semantic query model ?semantic:
p(t|?q) = (6)
? ? p(t|?temporal) + (1? ?) ? p(t|?semantic).
The component models ?temporal and ?semantic will
be estimated below; the parameter ? will be esti-
mated experimentally.
Our final ranking formula, then, is obtained by
plugging in Eq. 5 and 6 in Eq. 4:
log p(d|q) ? log p(d)
+ ? (
?
t p(t|q) ? log p(t|?d)) (7)
+ (1? ?) (
?
t p(t|?q) ? log p(t|?d)) .
3.3 Estimating credibility indicators
Next, we specify how each of the credibility indica-
tors is estimated; we do so in two groups: post level
and blog level.
926
3.3.1 Post level credibility indicators
Capitalization We estimate the capitalization
prior as follows:
pcapitalization(d) = n(c, s) ? |s|
?1, (8)
where n(c, s) is the number of sentences starting
with a capital and |s| is the number of sentences;
we only consider sentences with five or more words.
Emoticons The emoticons prior is estimated as
pemoticons(d) = 1? n(e, d) ? |d|
?1, (9)
where n(e, d) is the number of emoticons in the post
and |d| is the length of the post in words.
Shouting We use the following equation to esti-
mate the shouting prior:
pshouting(d) = 1? n(a, d) ? |d|
?1, (10)
where n(a, d) is the number of all caps words in blog
post d and |d| is the post length in words.
Spelling The spelling prior is estimated as
pspelling(d) = 1? n(m, d) ? |d|
?1, (11)
where n(m, d) is the number of misspelled (or un-
known) words and |d| is the post length in words.
Post length The post length prior is estimated us-
ing |d|, the post length in words:
plength(d) = log(|d|). (12)
Timeliness We estimate timeliness using the time-
based language models ?temporal proposed in (Li
and Croft, 2003; Mishne, 2007b). I.e., we use a news
corpus from the same period as the blog corpus that
we use for evaluation purposes (see Section 4.2). We
assign a timeliness score per post based on:
p(d|?temporal) = k
?1 ? (n(date(d), k) + 1) , (13)
where k is the number of top results from the initial
result list, date(d) is the date associated with doc-
ument d, and n(date(d), k) is the number of docu-
ments in k with the same date as d. For our initial
result list we perform retrieval on both the blog and
the news corpus and take k = 50 for both corpora.
Semantic A semantic query model ?semantic is
obtained using ideas due to Diaz and Metzler (2006).
Again, we use a news corpus from the same period
as the evaluation blog corpus and estimate ?semantic.
We issue the query to the external news corpus, re-
trieve the top 10 documents and extract the top 10
distinctive terms from these documents. These terms
are added to the original query terms to capture the
language usage around the topic.
3.3.2 Blog level credibility indicators
Spam filtering To estimate the spaminess of a
blog, we take a simple approach. We train an SVM
classifier on a labeled splog blog dataset (Kolari
et al, 2006) using the top 1500 words for both spam
and non-spam blogs as features. For each classified
blog d we have a confidence value s(d). If the clas-
sifier cannot make a decision (s(d) = 0) we set
pspam(d) to 0, otherwise we use the following to
transform s(d) into a spam prior pspam(d):
pspam(d) =
s(d)
2|s(d)|
+
?1 ? s(d)
2s(d)2 + 2|s(d)|
+
1
2
. (14)
Comments We estimate the comment prior as
pcomment(d) = log(n(r, d)), (15)
where n(r, d) is the number of comments on post d.
Regularity To estimate the regularity prior we use
pregularity(d) = log(?interval), (16)
where ?interval expresses the standard deviation of
the temporal intervals between two successive posts.
Topical consistency Here we use an approach
similar to query clarity (Cronen-Townsend and
Croft, 2002): based on the list of posts from the
same blog we compare the topic distribution of blog
B to the topic distribution in the collection C and
assign a ?clarity? value to B; a score further away
from zero indicates a higher topical consistency. We
estimate the topical consistency prior as
ptopic(d) = log(clarity(d)), (17)
where clarity(d) is estimated by
clarity(d) =
?
w p(w|B) ? log
(
p(w|B)
p(w)
)
?
w p(w|B)
(18)
with p(w) = count(w,C)|C| and p(w|B) =
count(w,B)
|B| .
927
3.3.3 Efficiency
All estimators discussed above can be imple-
mented efficiently: most are document priors and
can therefore be calculated offline. The only topic
dependent estimators are timeliness and language
usage; both can be implemented efficiently as spe-
cific forms of query expansion.
4 Evaluation
In this section we describe the experiments we con-
ducted to answer our research questions about the
impact of credibility on blog post retrieval.
4.1 Research questions
Our research revolves around the contribution of
credibility to the effectiveness of topical blog post
retrieval: what is the contribution of individual indi-
cators, of the post level indicators (topic dependent
or independent), of the blog level indicators, and of
all indicators combined? And do different topics
benefit from different indicators? To answer our re-
search question we compared the performance of the
baseline retrieval system (as detailed in Section 3.1)
with extensions of the baseline system with a single
indicator, a set of indicators, or all indicators.
4.2 Setup
We apply our models to the TREC Blog06 cor-
pus (Macdonald and Ounis, 2006). This corpus
has been constructed by monitoring around 100,000
blog feeds for a period of 11 weeks in early 2006,
downloading all posts created in this period. For
each permalink (HTML page containing one blog
post) the feed id is registered. We can use this id
to aggregate post level features to the blog level. In
our experiments we use only the HTML documents,
3.2M permalinks, which add up to around 88 GB.
The TREC 2006 and 2007 Blog tracks each offer
50 topics and assessments (Ounis et al, 2007; Mac-
donald et al, 2007). For topical relevancy, assess-
ment was done using a standard two-level scale: the
content of the post was judged to be topically rele-
vant or not. The evaluation metrics that we use are
standard ones: mean average precision (MAP) and
precision@10 (p@10) (Baeza-Yates and Ribeiro-
Neto, 1999). For all our retrieval tasks we use the
title field (T) of the topic statement as query.
To estimate the timeliness and semantic cred-
ibility indicators, we use AQUAINT-2, a set of
newswire articles (2.5 GB, about 907K documents)
that are roughly contemporaneous with the TREC
Blog06 collection (AQUAINT-2, 2007). Articles are
in English and come from a variety of sources.
Statistical significance is tested using a two-tailed
paired t-test. Significant improvements over the
baseline are marked with M (? = 0.05) or N (? =
0.01). We use O and H for a drop in performance
(for ? = 0.05 and ? = 0.01, respectively).
4.3 Parameter estimation
The models proposed in Section 3.2 contain param-
eters ?, ? and ?. These parameters need to be esti-
mated and, hence, require a training and test set. We
use a two-fold parameter estimation process: in the
first cycle we estimate the parameters on the TREC
2006 Blog topic set and test these settings on the top-
ics of the TREC 2007 Blog track. The second cycle
goes the other way around and trains on the 2007
set, while testing on the 2006 set.
Figure 2 shows the optimum values for ?, ?, and
? on the 2006 and the 2007 topic sets for both MAP
(bottom lines) and p@10 (top lines). When look-
ing at the MAP scores, the optimal setting for ? is
almost identical for the two topic sets: 0.4 for the
2006 set and 0.3 for the 2007 set, and also the op-
timal setting for ? is very similar for both sets: 0.4
for the 2006 set and 0.5 for the 2007 set. As to ?,
it is clear that timeliness does not improve the per-
formance over using the semantic feature alone and
the optimal setting for ? is therefore 0.0. Both ?
and ? show similar behavior on p@10 as on MAP,
but for ? we see a different trend. If early precision
is required, the value of ? should be increased, giv-
ing more weight to the topic-independent post level
features compared to the blog level features.
4.4 Retrieval performance
Table 2 lists the retrieval results for the baseline, for
each of the credibility indicators (on top of the base-
line), for four subsets of indicators, and for all in-
dicators combined. The baseline performs similar
to the median scores at the TREC 2006 Blog track
(MAP: 0.2203; p@10: 0.564) and somewhat below
the median MAP score at 2007 Blog track (MAP:
0.3340) but above the median p@10 score: 0.3805.
928
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
MAP
 / P1
0
lambda
20062007
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
MAP
 / P1
0
beta
20062007
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
MAP
 / P1
0
mu
20062007
Figure 2: Parameter estimation on the TREC 2006 and 2007 Blog topics. (Left): ?. (Center): ?. (Right): ?.
2006 2007
map p@10 map p@10
baseline 0.2156 0.4360 0.2820 0.5160
capitalization 0.2155 0.4500 0.2824 0.5160
emoticons 0.2156 0.4360 0.2820 0.5200
shouting 0.2159 0.4320 0.2833 0.5100
spelling 0.2179M 0.4480M 0.2839N 0.5220
post length 0.2502N 0.4960N 0.3112N 0.5700N
timeliness 0.1865H 0.4520 0.2660 0.4860
semantic 0.2840N 0.6240N 0.3379N 0.6640N
spam filtering 0.2093 0.4700 0.2814 0.5760N
comments 0.2497N 0.5000N 0.3099N 0.5600N
regularity 0.1658H 0.4940M 0.2353H 0.5640M
consistency 0.2141H 0.4220 0.2785O 0.5040
post level 0.2374N 0.4920N 0.2990N 0.5660N
(topic indep.)
post level 0.2840N 0.6240N 0.3379N 0.6640N
(topic dep.)
post level 0.2911N 0.6380N 0.3369N 0.6620N
(all)
blog level 0.2391N 0.4500 0.3023N 0.5580N
all 0.3051N 0.6880N 0.3530N 0.6900N
Table 2: Retrieval performance on 2006 and 2007 topics,
using ? = 0.3, ? = 0.4, and ? = 0.0.
Some (topic independent) post level indicators
hurt the MAP score, while others help (for both
years, and both measures). Combined, the topic
independent post level indicators perform less well
than the use of one of them (post length). As to
the topic dependent post level indicators, timeliness
hurts performance on MAP for both years, while
the semantic indicator provides significant improve-
ments across the board (resulting in a top 2 score in
terms of MAP and a top 5 score in terms of p@10,
when compared to the TREC 2006 Blog track par-
ticipants that only used the T field).
Some of the blog level features hurt more than
they help (regularity, consistency), while the com-
ments feature helps, on all measures, and for both
years. Combined, the blog level features help less
than the use of one of them (comments).
As a group, the combined post level features help
more than either of the two post level sub groups
alone. The blog level features show similar results to
the topic-independent post level features, obtaining
a significant increase on both MAP and p@10, but
lower than the topic-dependent post level features.
The grand combination of all credibility indica-
tors leads to a significant improvement over any of
the single indicators and over any of the four subsets
considered in Table 2. The MAP score of this run
is higher than the best performing run in the TREC
2006 Blog track and has a top 3 performance on
p@10; its 2007 performance is just within the top
half on both MAP and p@10.
4.5 Analysis
Next we examine the differences in average preci-
sion (per topic) between the baseline and subsets of
indicators (post and blog level) and the grand com-
bination. We limit ourselves to an analysis of the
MAP scores. Figure 3 displays the per topic average
precision scores, where topics are sorted by absolute
gain of the grand combination over the baseline.
In 2006, 7 (out of 50) topics were negatively af-
fected by the use of credibility indicators; in 2007,
15 (out of 50) were negatively affected. Table 3 lists
the topics that displayed extreme behavior (in terms
of relative performance gain or drop in AP score).
While the extreme drops for both years are in the
same range, the gains for 2006 are more extreme
than for 2007.
The topic that is hurt most (in absolute terms)
by the credibility indicators is the 2007 topic 910:
aperto network (AP -0.2781). The semantic indi-
cator is to blame for this decrease is: the terms in-
cluded in the expanded query shift the topic from a
wireless broadband provider to television networks.
929
-0.4
-0.3
-0.2
-0.1
 0
 0.1
 0.2
 0.3
 0.4
AP
 di
ffe
ren
ce
topics
all
post
blog
-0.4
-0.3
-0.2
-0.1
 0
 0.1
 0.2
 0.3
 0.4
AP
 di
ffe
ren
ce
topics
all
post
blog
Figure 3: Per-topic AP differences between baseline run and runs with blog level features (triangles), post level features
(circles) and all feature (squares) on the 2006 (left) en 2007 (right) topics.
Table 3: Extreme performance gains/drops of the grand
combination over the baseline (MAP).
2006
id topic % gain/loss
900 mcdonalds +525.9%
866 foods +446.2%
865 basque +308.6%
862 blackberry -21.5%
870 barry bonds -35.2%
898 business intelligence resources -78.8%
2007
id topic % gain/loss
923 challenger +162.1%
926 hawthorne heights +160.7%
945 bolivia +125.5%
943 censure -49.4%
928 big love -80.0%
904 alterman -84.2%
Topics that gain most (in absolute terms) are 947
(sasha cohen; AP +0.3809) and 923 (challenger; AP
+0.3622) from the 2007 topic set.
Finally, the combination of all credibility indica-
tors hurts 7 (2006) plus 15 (2007) equals 22 topics;
for the post level indicators get a performance drop
in AP for 28 topics (10 plus 18, respectively) and for
the blog level indicators we get a drop for 15 topics
(4 plus 11, respectively). Hence, the combination of
all indicators strikes a good balance between overall
performance gain and per topic risk.
5 Conclusions
We provided efficient estimations for 11 credibility
indicators and assessed their impact on topical blog
post retrieval, on top of a content-based retrieval
baseline. We compared the contribution of these in-
dicators, both individually and in groups, and found
that (combined) they have a significant positive im-
pact on topical blog post retrieval effectiveness. Cer-
tain single indicators, like post length and comments,
make good credibility indicators on their own; the
best performing credibility indicator group consists
of topic dependent post level ones. Other future
work concerns indicator selection: instead of taking
all indicators on board, consider selected indicators
only, in a topic dependent fashion.
Our choice of credibility indicators was based on
a framework proposed by Rubin and Liddy (2006):
the estimators we used are natural implementations
of the selected indicators, but by no means the only
possible ones. In future work we intend to extend
the set of indicators considered so as to include, e.g.,
stated competencies (1e), by harvesting and analyz-
ing bloggers? profiles, and to extend the set of esti-
mators for indicators that we already consider such
as reading level measures (e.g., Flesch-Kincaid) for
the literary appeal indicator (4b).
Acknowledgments
We would like to thank our reviewers for their feed-
back. Both authors were supported by the E.U. IST
programme of the 6th FP for RTD under project
MultiMATCH contract IST-033104. De Rijke was
also supported by NWO under project numbers
017.001.190, 220-80-001, 264-70-050, 354-20-005,
600.065.120, 612-13-001, 612.000.106, 612.066.-
302, 612.069.006, 640.001.501, and 640.002.501.
930
References
Agichtein, E., Castillo, C., Donato, D., Gionis, A., and
Mishne, G. (2008). Finding high-quality content in
social media. In WSDM ?08.
AQUAINT-2 (2007). URL: http://trec.
nist.gov/data/qa/2007_qadata/qa.
07.guidelines.html#documents.
Baeza-Yates, R. and Ribeiro-Neto, B. (1999). Modern
Information Retrieval. Addison Wesley.
Chafe, W. (1986). Evidentiality in English conversion
and academic writing. In Chaf, W. and Nichols, J., ed-
itors, Evidentiality: The Linguistic Coding of Episte-
mology, volume 20, pages 261?273. Ablex Publishing
Corporation.
Croft, W. B. and Lafferty, J., editors (2003). Language
Modeling for Information Retrieval. Kluwer.
Cronen-Townsend, S. and Croft, W. (2002). Quantifying
query ambiguity. In Proceedings of Human Language
Technology 2002, pages 94?98.
Diaz, F. and Metzler, D. (2006). Improving the estima-
tion of relevance models using large external corpora.
In SIGIR ?06: Proceedings of the 29th annual interna-
tional ACM SIGIR conference on Research and devel-
opment in information retrieval, pages 154?161, New
York. ACM Press.
Hawking, D. and Craswell, N. (2002). Overview of the
TREC-2001 web track. In The Tenth Text Retrieval
Conferences (TREC-2001), pages 25?31.
Java, A., Kolari, P., Finin, T., Joshi, A., and Martineau, J.
(2007). The blogvox opinion retrieval system. In The
Fifteenth Text REtrieval Conference (TREC 2006).
Kolari, P., Finin, T., Java, A., and Joshi, A.
(2006). Splog blog dataset. URL: http:
//ebiquity.umbc.edu/resource/html/
id/212/Splog-Blog-Dataset.
Li, X. and Croft, W. (2003). Time-based language mod-
els. In Proceedings of the 12th International Con-
ference on Information and Knowledge Managment
(CIKM), pages 469?475.
Liu, B. (2007). Web Data Mining. Springer-Verlag, Hei-
delberg.
Macdonald, C. and Ounis, I. (2006). The trec blogs06
collection: Creating and analyzing a blog test collec-
tion. Technical Report TR-2006-224, Department of
Computer Science, University of Glasgow.
Macdonald, C., Ounis, I., and Soboroff, I. (2007).
Overview of the trec 2007 blog track. In TREC 2007
Working Notes, pages 31?43.
Metzger, M. (2007). Making sense of credibility on the
web: Models for evaluating online information and
recommendations for future research. Journl of the
American Society for Information Science and Tech-
nology, 58(13):2078?2091.
Mishne, G. (2007a). Applied Text Analytics for Blogs.
PhD thesis, University of Amsterdam, Amsterdam.
Mishne, G. (2007b). Using blog properties to improve
retrieval. In Proceedings of ICWSM 2007.
Mishne, G. and de Rijke, M. (2006). A study of blog
search. In Lalmas, M., MacFarlane, A., Ru?ger, S.,
Tombros, A., Tsikrika, T., and Yavlinsky, A., editors,
Advances in Information Retrieval: Proceedings 28th
European Conference on IR Research (ECIR 2006),
volume 3936 of LNCS, pages 289?301. Springer.
Mishne, G. and Glance, N. (2006). Leave a reply: An
analysis of weblog comments. In Proceedings of
WWW 2006.
Ounis, I., de Rijke, M., Macdonald, C., Mishne, G.,
and Soboroff, I. (2007). Overview of the trec-2006
blog track. In The Fifteenth Text REtrieval Conference
(TREC 2006) Proceedings.
Rubin, V. and Liddy, E. (2006). Assessing credibility
of weblogs. In Proceedings of the AAAI Spring Sym-
posium: Computational Approaches to Analyzing We-
blogs (CAAW).
Stanford, J., Tauber, E., Fogg, B., and Marable, L. (2002).
Experts vs online consumers: A comparative cred-
ibility study of health and finance web sites. URL:
http://www.consumerwebwatch.org/
news/report3_credibilityresearch/
slicedbread.pdf.
Van House, N. (2004). Weblogs: Credibility and
collaboration in an online world. URL: people.
ischool.berkeley.edu/?vanhouse/Van\
%20House\%20trust\%20workshop.pdf.
Weimer, M., Gurevych, I., and Mehlhauser, M. (2007).
Automatically assessing the post quality in online dis-
cussions on software. In Proceedings of the ACL 2007
Demo and Poster Sessions, pages 125?128.
Zhou, Y. and Croft, W. B. (2005). Document quality
models for web ad hoc retrieval. In CIKM ?05: Pro-
ceedings of the 14th ACM international conference on
Information and knowledge management, pages 331?
332.
931
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 1057?1065,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
A Generative Blog Post Retrieval Model that Uses
Query Expansion based on External Collections
Wouter Weerkamp
w.weerkamp@uva.nl
Krisztian Balog
k.balog@uva.nl
ISLA, University of Amsterdam
Maarten de Rijke
mdr@science.uva.nl
Abstract
User generated content is characterized
by short, noisy documents, with many
spelling errors and unexpected language
usage. To bridge the vocabulary gap be-
tween the user?s information need and
documents in a specific user generated
content environment, the blogosphere, we
apply a form of query expansion, i.e.,
adding and reweighing query terms. Since
the blogosphere is noisy, query expansion
on the collection itself is rarely effective
but external, edited collections are more
suitable. We propose a generative model
for expanding queries using external col-
lections in which dependencies between
queries, documents, and expansion doc-
uments are explicitly modeled. Differ-
ent instantiations of our model are dis-
cussed and make different (in)dependence
assumptions. Results using two exter-
nal collections (news andWikipedia) show
that external expansion for retrieval of user
generated content is effective; besides,
conditioning the external collection on the
query is very beneficial, and making can-
didate expansion terms dependent on just
the document seems sufficient.
1 Introduction
One of the grand challenges in information re-
trieval is to bridge the vocabulary gap between a
user and her information need on the one hand and
the relevant documents on the other (Baeza-Yates
and Ribeiro-Neto, 1999). In the setting of blogs
or other types of user generated content, bridging
this gap becomes even more challenging. This has
several causes: (i) the spelling errors, unusual, cre-
ative or unfocused language usage resulting from
the lack of top-down rules and editors in the con-
tent creation process, and (ii) the (often) limited
length of user generated documents.
Query expansion, i.e., modifying the query by
adding and reweighing terms, is an often used
technique to bridge the vocabulary gap. In gen-
eral, query expansion helps more queries than
it hurts (Balog et al, 2008b; Manning et al,
2008). However, when working with user gener-
ated content, expanding a query with terms taken
from the very corpus in which one is searching
tends to be less effective (Arguello et al, 2008a;
Weerkamp and de Rijke, 2008b)?topic drift is
a frequent phenomenon here. To be able to ar-
rive at a richer representation of the user?s infor-
mation need, while avoiding topic drift resulting
from query expansion against user generated con-
tent, various authors have proposed to expand the
query against an external corpus, i.e., a corpus dif-
ferent from the target (user generated) corpus from
which documents need to be retrieved.
Our aim in this paper is to define and evaluate
generative models for expanding queries using ex-
ternal collections. We propose a retrieval frame-
work in which dependencies between queries,
documents, and expansion documents are explic-
itly modeled. We instantiate the framework in
multiple ways by making different (in)dependence
assumptions. As one of the instantiations we ob-
tain the mixture of relevance models originally
proposed by Diaz and Metzler (2006).
We address the following research questions:
(i) Can we effectively apply external expansion in
the retrieval of user generated content? (ii) Does
conditioning the external collection on the query
help improve retrieval performance? (iii) Can we
obtain a good estimate of this query-dependent
collection probability? (iv) Which of the collec-
tion, the query, or the document should the selec-
tion of an expansion term be dependent on? In
other words, what are the strongest simplifications
in terms of conditional independencies between
variables that can be assumed, without hurting per-
formance? (v) Do our models show similar behav-
ior across topics or do we observe strong per-topic
1057
differences between models?
The remainder of this paper is organized as fol-
lows. We discuss previous work related to query
expansion and external sources in ?2. Next, we
introduce our retrieval framework (?3) and con-
tinue with our main contribution, external expan-
sion models, in ?4. ?5 details how the components
of the model can be estimated. We put our models
to the test, using the experimental setup discussed
in ?6, and report on results in ?7. We discuss our
results (?8) and conclude in ?9.
2 Related Work
Related work comes in two main flavors: (i) query
modeling in general, and (ii) query expansion us-
ing external sources (external expansion). We
start by shortly introducing the general ideas be-
hind query modeling, and continue with a quick
overview of work related to external expansion.
2.1 Query Modeling
Query modeling, i.e., transformations of simple
keyword queries into more detailed representa-
tions of the user?s information need (e.g., by as-
signing (different) weights to terms, expanding the
query, or using phrases), is often used to bridge the
vocabulary gap between the query and the doc-
ument collection. Many query expansion tech-
niques have been proposed, and they mostly fall
into two categories, i.e., global analysis and local
analysis. The idea of global analysis is to expand
the query using global collection statistics based,
for instance, on a co-occurrence analysis of the en-
tire collection. Thesaurus- and dictionary-based
expansion as, e.g., in Qiu and Frei (1993), also
provide examples of the global approach.
Our focus in this paper is on local approaches
to query expansion, that use the top retrieved doc-
uments as examples from which to select terms
to improve the retrieval performance (Rocchio,
1971). In the setting of language modeling ap-
proaches to query expansion, the local analysis
idea has been instantiated by estimating addi-
tional query language models (Lafferty and Zhai,
2003; Tao and Zhai, 2006) or relevance mod-
els (Lavrenko and Croft, 2001) from a set of feed-
back documents. Yan and Hauptmann (2007) ex-
plore query expansion in a multimedia setting.
Balog et al (2008b) compare methods for sam-
pling expansion terms to support query-dependent
and query-independent query expansion; the lat-
ter is motivated by the wish to increase ?aspect
recall? and attempts to uncover aspects of the in-
formation need not captured by the query. Kur-
land et al (2005) also try to uncover multiple as-
pects of a query, and to that they provide an iter-
ative ?pseudo-query? generation technique, using
cluster-based language models. The notion of ?as-
pect recall? is mentioned in (Buckley, 2004; Har-
man and Buckley, 2004) and identified as one of
the main reasons of failure of the current informa-
tion retrieval systems. Even though we acknowl-
edge the possibilities of our approach in improving
aspect recall, by introducing aspects mainly cov-
ered by the external collection being used, we are
currently unable to test this assumption.
2.2 External Expansion
The use of external collections for query expan-
sion has a long history, see, e.g., (Kwok et al,
2001; Sakai, 2002). Diaz and Metzler (2006) were
the first to give a systematic account of query ex-
pansion using an external corpus in a language
modeling setting, to improve the estimation of rel-
evance models. As will become clear in ?4, Diaz
and Metzler?s approach is an instantiation of our
general model for external expansion.
Typical query expansion techniques, such as
pseudo-relevance feedback, using a blog or blog
post corpus do not provide significant perfor-
mance improvements and often dramatically hurt
performance. For this reason, query expansion
using external corpora has been a popular tech-
nique at the TREC Blog track (Ounis et al, 2007).
For blog post retrieval, several TREC participants
have experimented with expansion against exter-
nal corpora, usually a news corpus, Wikipedia, the
web, or a mixture of these (Zhang and Yu, 2007;
Java et al, 2007; Ernsting et al, 2008). For the
blog finding task introduced in 2007, TREC par-
ticipants again used expansion against an exter-
nal corpus, usually Wikipedia (Elsas et al, 2008a;
Ernsting et al, 2008; Balog et al, 2008a; Fautsch
and Savoy, 2008; Arguello et al, 2008b). The mo-
tivation underlying most of these approaches is to
improve the estimation of the query representa-
tion, often trying to make up for the unedited na-
ture of the corpus from which posts or blogs need
to be retrieved. Elsas et al (2008b) go a step fur-
ther and develop a query expansion technique us-
ing the links in Wikipedia.
Finally, Weerkamp and de Rijke (2008b) study
1058
external expansion in the setting of blog retrieval
to uncover additional perspectives of a given topic.
We are driven by the same motivation, but where
they considered rank-based result combinations
and simple mixtures of query models, we take
a more principled and structured approach, and
develop four versions of a generative model for
query expansion using external collections.
3 Retrieval Framework
We work in the setting of generative language
models. Here, one usually assumes that a doc-
ument?s relevance is correlated with query likeli-
hood (Ponte and Croft, 1998; Miller et al, 1999;
Hiemstra, 2001). Within the language model-
ing approach, one builds a language model from
each document, and ranks documents based on the
probability of the document model generating the
query. The particulars of the language modeling
approach have been discussed extensively in the
literature (see, e.g., Balog et al (2008b)) and will
not be repeated here. Our final formula for ranking
documents given a query is based on Eq. 1:
logP (D|Q) ?
logP (D) +
?
t?Q
P (t|?Q) logP (t|?D) (1)
Here, we see the prior probability of a document
being relevant, P (D) (which is independent of the
query Q), the probability of a term t for a given
query model, ?Q, and the probability of observ-
ing the term t given the document model, ?D.
Our main interest lies in in obtaining a better es-
timate of P (t|?Q). To this end, we take the query
model to be a linear combination of the maximum-
likelihood query estimate P (t|Q) and an expanded
query model P (t|Q?):
P (t|?Q) = ?Q ?P (t|Q)+ (1??Q) ?P (t|Q?) (2)
In the next section we introduce our models for es-
timating p(t|Q?), i.e., query expansion using (mul-
tiple) external collections.
4 Query Modeling Approach
Our goal is to build an expanded query model that
combines evidence from multiple external collec-
tions. We estimate the probability of a term t in the
expanded query Q? using a mixture of collection-
specific query expansion models.
P (t|Q?) =
?
c?C P (t|Q, c) ? P (c|Q), (3)
where C is the set of document collections.
To estimate the probability of a term given the
query and the collection, P (t|Q, c), we compute
the expectation over the documents in the collec-
tion c:
P (t|Q, c) =
?
D?c
P (t|Q, c,D) ? P (D|Q, c). (4)
Substituting Eq. 4 back into Eq. 3 we get
P (t|Q?) = (5)
?
c?C
P (c|Q) ?
?
D?c
P (t|Q, c,D) ? P (D|Q, c).
This, then, is our query model for combining evi-
dence from multiple sources.
The following subsections introduce four in-
stances of the general external expansion model
(EEM) we proposed in this section; each of the in-
stances differ in independence assumptions:
? EEM1 (?4.1) assumes collection c to be inde-
pendent of query Q and document D jointly,
and document D individually, but keeps the
dependence on Q and of t and Q on D.
? EEM2 (?4.2) assumes that term t and collec-
tion c are conditionally independent, given
document D and query Q; moreover, D and
Q are independent given c but the depen-
dence of t and Q on D is kept.
? EEM3 (?4.3) assumes that expansion term t
and original query Q are independent given
document D.
? On top of EEM3, EEM4 (?4.4) makes one
more assumption, viz. the dependence of col-
lection c on query Q.
4.1 External Expansion Model 1 (EEM1)
Under this model we assume collection c to be
independent of query Q and document D jointly,
and document D individually, but keep the depen-
dence on Q. We rewrite P (t|Q, c) as follows:
P (t|Q, c)
=
?
D?c
P (t|Q,D) ? P (t|c) ? P (D|Q)
=
?
D?c
P (t, Q|D)
P (Q|D)
? P (t|c) ?
P (Q|D)P (D)
P (Q)
?
?
D?c
P (t, Q|D) ? P (t|c) ? P (D) (6)
Note that we drop P (Q) from the equation as it
does not influence the ranking of terms for a given
1059
query Q. Further, P (D) is the prior probability
of a document, regardless of the collection it ap-
pears in (as we assumed D to be independent of
c). We assume P (D) to be uniform, leading to the
following equation for ranking expansion terms:
P (t|Q?) ?
?
c?C
P (t|c) ? P (c|Q) ?
?
D?c
P (t, Q|D). (7)
In this model we capture the probability of the ex-
pansion term given the collection (P (t|c)). This
allows us to assign less weight to terms that are
less meaningful in the external collection.
4.2 External Expansion Model 2 (EEM2)
Here, we assume that term t and collection c are
conditionally independent, given document D and
query Q: P (t|Q, c,D) = P (t|Q,D). This leaves
us with the following:
P (t|Q,D) =
P (t, Q,D)
P (Q,D)
=
P (t, Q|D) ? P (D)
P (Q|D) ? P (D)
=
P (t, Q|D)
P (Q|D)
(8)
Next, we assume document D and query Q to
be independent given collection c: P (D|Q, c) =
P (D|c). Substituting our choices into Eq. 4 gives
us our second way of estimating P (t|Q, c):
P (t|Q, c) =
?
D?c
P (t, Q|D)
P (Q|D)
? P (D|c) (9)
Finally, we put our choices so far together, and
implement Eq. 9 in Eq. 3, yielding our final term
ranking equation:
P (t|Q?) ? (10)
?
c?C
P (c|Q) ?
?
D?c
P (t, Q|D)
P (Q|D)
? P (D|c).
4.3 External Expansion Model 3 (EEM3)
Here we assume that expansion term t and both
collection c and original query Q are independent
given document D. Hence, we set P (t|Q, c,D) =
P (t|D). Then
P (t|Q, c)
=
?
D?c
P (t|D) ? P (D|Q, c)
=
?
D?c
P (t|D) ?
P (Q|D, c) ? P (D|c)
P (Q|c)
?
?
D?c
P (t|D) ? P (Q|D, c) ? P (D|c)
We dropped P (Q|c) as it does not influence the
ranking of terms for a given query Q. Assuming
independence of Q and c given D, we obtain
P (t|Q, c) ?
?
D?c
P (D|c) ? P (t|D) ? P (Q|D)
so
P (t|Q?) ?
?
c?C
P (c|Q) ?
?
D?c
P (D|c) ? P (t|D) ? P (Q|D).
We follow Lavrenko and Croft (2001) and assume
that P (D|c) = 1|Rc| , the size of the set of top
ranked documents in c (denoted by Rc), finally ar-
riving at
P (t|Q?) ?
?
c?C
P (c|Q)
|Rc|
?
?
D?Rc
P (t|D) ? P (Q|D). (11)
4.4 External Expansion Model 4 (EEM4)
In this fourth model we start from EEM3 and drop
the assumption that c depends on the query Q, i.e.,
P (c|Q) = P (c), obtaining
P (t|Q?) ?
?
c?C
P (c)
|Rc|
?
?
D?Rc
P (t|D) ? P (Q|D). (12)
Eq. 12 is in fact the ?mixture of relevance models?
external expansion model proposed by Diaz and
Metzler (2006). The fundamental difference be-
tween EEM1, EEM2, EEM3 on the one hand and
EEM4 on the other is that EEM4 assumes inde-
pendence between c and Q (thus P (c|Q) is set to
P (c)). That is, the importance of the external col-
lection is independent of the query. How reason-
able is this choice? Mishne and de Rijke (2006)
examined queries submitted to a blog search en-
gine and found many to be either news-related
context queries (that aim to track mentions of a
named entity) or concept queries (that seek posts
about a general topic). For context queries such as
cheney hunting (TREC topic 867) a news collec-
tion is likely to offer different (relevant) aspects
of the topic, whereas for a concept query such as
jihad (TREC topic 878) a knowledge source such
as Wikipedia seems an appropriate source of terms
that capture aspects of the topic. These observa-
tions suggest the collection should depend on the
query.
1060
EEM3 and EEM4 assume that expansion term t
and original query Q are independent given doc-
ument D. This may or may not be too strong an
assumption. Models EEM1 and EEM2 also make
independence assumptions, but weaker ones.
5 Estimating Components
The models introduced above offer us several
choices in estimating the main components. Be-
low we detail how we estimate (i) P (c|Q), the
importance of a collection for a given query,
(ii) P (t|c), the unimportance of a term for an ex-
ternal collection, (iii) P (Q|D), the relevance of
a document in the external collection for a given
query, and (iv) P (t, Q|D), the likelihood of a term
co-occurring with the query, given a document.
5.1 Importance of a Collection
Represented as P (c|Q) in our models, the im-
portance of an external collection depends on the
query; how we can estimate this term? We con-
sider three alternatives, in terms of (i) query clar-
ity, (ii) coherence and (iii) query-likelihood, using
documents in that collection.
First, query clarity measures the structure of a
set of documents based on the assumption that a
small number of topical terms will have unusu-
ally large probabilities (Cronen-Townsend et al,
2002). We compute the query clarity of the top
ranked documents in a given collection c:
clarity(Q, c) =
?
t
P (t|Q) ? log
P (t|Q)
P (t|Rc)
Finally, we normalize clarity(Q, c) over all col-
lections, and set P (c|Q) ? clarity(Q,c)P
c??C clarity(Q,c
?) .
Second, a measure called ?coherence score? is
defined by He et al (2008). It is the fraction of
?coherent? pairs of documents in a given set of
documents, where a coherent document pair is one
whose similarity exceeds a threshold. The coher-
ence of the top ranked documents Rc is:
Co(Rc) =
?
i6=j?{1,...,|Rc|} ?(di, dj)
|Rc|(|Rc| ? 1)
,
where ?(di, dj) is 1 in case of a similar pair (com-
puted using cosine similarity), and 0 otherwise.
Finally, we set P (c|Q) ? Co(Rc)P
c??C Co(Rc? )
.
Third, we compute the conditional probability
of the collection using Bayes? theorem. We ob-
serve that P (c|Q) ? P (Q|c) (omitting P (Q) as it
will not influence the ranking and P (c) which we
take to be uniform). Further, for the sake of sim-
plicity, we assume that all documents within c are
equally important. Then, P (Q|c) is estimated as
P (Q|c) =
1
|c|
?
?
D?c
P (Q|D) (13)
where P (Q|D) is estimated as described in ?5.3,
and |c| is the number of documents in c.
5.2 Unimportance of a Term
Rather than simply estimating the importance of
a term for a given query, we also estimate the
unimportance of a term for a collection; i.e., we
assign lower probability to terms that are com-
mon in that collection. Here, we take a straight-
forward approach in estimating this, and define
P (t|c) = 1 ? n(t,c)P
t? n(t
?,c) .
5.3 Likelihood of a Query
We need an estimate of the probability of a query
given a document, P (Q|D). We do so by using
Hauff et al (2008)?s refinement of term dependen-
cies in the query as proposed by Metzler and Croft
(2005).
5.4 Likelihood of a Term
Estimating the likelihood of observing both the
query and a term for a given document P (t, Q|D)
is done in a similar way to estimating P (Q|D), but
now for t, Q in stead of Q.
6 Experimental Setup
In his section we detail our experimental setup:
the (external) collections we use, the topic sets
and relevance judgements available, and the sig-
nificance testing we perform.
6.1 Collections and Topics
We make use of three collections: (i) a collec-
tion of user generated documents (blog posts),
(ii) a news collection, and (iii) an online knowl-
edge source. The blog post collection is the TREC
Blog06 collection (Ounis et al, 2007), which con-
tains 3.2 million blog posts from 100,000 blogs
monitored for a period of 11 weeks, from Decem-
ber 2005 to March 2006; all posts from this period
have been stored as HTML files. Our news col-
lection is the AQUAINT-2 collection (AQUAINT-
2, 2007), from which we selected news articles
that appeared in the period covered by the blog
1061
collection, leaving us with about 150,000 news
articles. Finally, we use a dump of the English
Wikipedia from August 2007 as our online knowl-
edge source; this dump contains just over 3.8 mil-
lion encyclopedia articles.
During 2006?2008, the TRECBlog06 collec-
tion has been used for the topical blog post re-
trieval task (Weerkamp and de Rijke, 2008a) at the
TREC Blog track (Ounis et al, 2007): to retrieve
posts about a given topic. For every year, 50 topics
were developed, consisting of a title field, descrip-
tion, and narrative; we use only the title field, and
ignore the other available information. For all 150
topics relevance judgements are available.
6.2 Metrics and Significance
We report on the standard IR metrics Mean Aver-
age Precision (MAP), precision at 5 and 10 doc-
uments (P5, P10), and the Mean Reciprocal Rank
(MRR). To determine whether or not differences
between runs are significant, we use a two-tailed
paired t-test, and report on significant differences
for ? = .05 (M and O) and ? = .01 (N and H).
7 Results
We first discuss the parameter tuning for our four
EEM models in Section 7.1. We then report on the
results of applying these settings to obtain our re-
trieval results on the blog post retrieval task. Sec-
tion 7.2 reports on these results. We follow with a
closer look in Section 8.
7.1 Parameters
Our model has one explicit parameter, and one
more or less implicit parameter. The obvious pa-
rameter is ?Q, used in Eq. 2, but also the num-
ber of terms to include in the final query model
makes a difference. For training of the param-
eters we use two TREC topic sets to train and
test on the held-out topic set. From the training
we conclude that the following parameter settings
work best across all topics: (EEM1) ?Q = 0.6,
30 terms; (EEM2) ?Q = 0.6, 40 terms; (EEM3
and EEM4) ?Q = 0.5, 30 terms. In the remainder
of this section, results for our models are reported
using these parameter settings.
7.2 Retrieval Results
As a baseline we use an approach without exter-
nal query expansion, viz. Eq. 1. In Table 1 we
list the results on the topical blog post finding task
model P (c|Q) MAP P5 P10 MRR
Baseline 0.3815 0.6813 0.6760 0.7643
EEM1
uniform 0.3976N 0.7213N 0.7080N 0.7998
0.8N/0.2W 0.3992 0.7227 0.7107 0.7988
coherence 0.3976 0.7187 0.7060 0.7976
query clarity 0.3970 0.7187 0.7093 0.7929
P (Q|c) 0.3983 0.7267 0.7093 0.7951
oracle 0.4126N 0.7387M 0.7320N 0.8252M
EEM2
uniform 0.3885N 0.7053M 0.6967M 0.7706
0.9N/0.1W 0.3895 0.7133 0.6953 0.7736
coherence 0.3890 0.7093 0.7020 0.7740
query clarity 0.3872 0.7067 0.6953 0.7745
P (Q|c) 0.3883 0.7107 0.6967 0.7717
oracle 0.3995N 0.7253N 0.7167N 0.7856
EEM3
uniform 0.4048N 0.7187M 0.7207N 0.8261N
coherence 0.4058 0.7253 0.7187 0.8306
query clarity 0.4033 0.7253 0.7173 0.8228
P (Q|c) 0.3998 0.7253 0.7100 0.8133
oracle 0.4194N 0.7493N 0.7353N 0.8413
EEM4 0.5N/0.5W 0.4048N 0.7187M 0.7207N 0.8261N
Table 1: Results for all model instances on all top-
ics (i.e., 2006, 2007, and 2008); aN/bW stands
for the weights assigned to the news (a) and
Wikipedia corpora (b). Significance is tested be-
tween (i) each uniform run and the baseline, and
(ii) each other setting and its uniform counterpart.
of (i) our baseline, and (ii) our model (instanti-
ated by EEM1, EEM2, EEM3, and EEM4). For
all models that contain the query-dependent col-
lection probability (P (c|Q)) we report on multi-
ple ways of estimating this: (i) uniform, (ii) best
global mixture (independent of the query, obtained
by a sweep over collection probabilities), (iii) co-
herence, (iv) query clarity, (v) P (Q|c), and (vi) us-
ing an oracle for which optimal settings were ob-
tained by the same sweep as (ii). Note that meth-
ods (i) and (ii) are not query dependent; for EEM3
we do not mention (ii) since it equals (i). Finally,
for EEM4 we only have a query-independent com-
ponent, P (c): the best performance here is ob-
tained using equal weights for both collections.
A few observations. First, our baseline per-
forms well above the median for all three years
(2006?2008). Second, in each of its four instances
our model for query expansion against external
corpora improves over the baseline. Third, we
see that it is safe to assume that a term is depen-
dent only on the document from which it is sam-
pled (EEM1 vs. EEM2 vs. EEM3). EEM3 makes
the strongest assumptions about terms in this re-
spect, yet it performs best. Fourth, capturing the
dependence of the collection on the query helps,
as we can see from the significant improvements
of the ?oracle? runs over their ?uniform? counter-
parts. However, we do not have a good method
yet for automatically estimating this dependence,
1062
as is clear from the insignificant differences be-
tween the runs labeled ?coherence,? ?query clar-
ity,? ?P (Q|c)? and the run labeled ?uniform.?
8 Discussion
Rather than providing a pairwise comparison of all
runs listed in the previous section, we consider two
pairwise comparisons?between (an instantion of)
our model and the baseline, and between two in-
stantiations of our model?and highlight phenom-
ena that we also observed in other pairwise com-
parisons. Based on this discussion, we also con-
sider a combination of approaches.
8.1 EEM1 vs. the Baseline
We zoom in on EEM1 and make a per-topic com-
parison against the baseline. First of all, we
observe behavior typical for all query expansion
methods: some topics are helped, some are not af-
fected, and some are hurt by the use of EEM1; see
Figure 1, top row. Specifically, 27 topics show a
slight drop in AP (maximum drop is 0.043 AP), 3
topics do not change (as no expansion terms are
identified) and the remainder of the topics (120)
improve in AP. The maximum increase in AP is
0.5231 (+304%) for topic 949 (ford bell); Top-
ics 887 (world trade organization, +87%), 1032
(I walk the line, +63%), 865 (basque, +53%), and
1014 (tax break for hybrid automobiles, +50%)
also show large improvements. The largest drop (-
20% AP) is for topic 1043 (a million little pieces,
a controversial memoir that was in the news dur-
ing the time coverd by the blog crawl); because we
do not do phrase or entity recognition in the query,
but apply stopword removal, it is reduced to mil-
lion pieces which introduced a lot of topic drift.
Let us examine the ?collection preference? of
topics: 35 had a clear preference for Wikipedia, 32
topics for news, and the remainder (83 topics) re-
quired a mixture of both collections. First, we look
at topics that require equal weights for both collec-
tions; topic 880 (natalie portman, +21% AP) con-
cerns a celebrity with a largeWikipedia biography,
as well as news coverage due to new movie re-
leases during the period covered by the blog crawl.
Topic 923 (challenger, +7% AP) asks for infor-
mation on the space shuttle that exploded dur-
ing its launch; the 20th anniversary of this event
was commemorated during the period covered by
the crawl and therefore it is newsworthy as well
as present in Wikipedia (due to its historic im-
pact). Finally, topic 869 (muhammad cartoon,
+20% AP) deals with the controversy surrounding
the publication of cartoons featuring Muhammad:
besides its obvious news impact, this event is ex-
tensively discussed in multiple Wikipedia articles.
As to topics that have a preference for
Wikipedia, we see some very general ones (as is to
be expected): Topic 942 (lawful access, +30%AP)
on the government accessing personal files; Topic
1011 (chipotle restaurant, +13% AP) on infor-
mation concerning the Chipotle restaurants; Topic
938 (plug awards, +21% AP) talks about an award
show. Although this last topic could be expected to
have a clear preference for expansion terms from
the news corpus, the awards were not handed out
during the period covered by the news collection
and, hence, full weight is given to Wikipedia.
At the other end of the scale, topics that show a
preference for the news collection are topic 1042
(david irving, +28% AP), who was on trial dur-
ing the period of the crawl for denying the Holo-
caust and received a lot of media attention. Further
examples include Topic 906 (davos, +20% AP),
which asks for information on the annual world
economic forum meeting in Davos in January,
something typically related to news, and topic 949
(ford bell, +304% AP), which seeks information
on Ford Bell, Senate candidate at the start of 2006.
8.2 EEM1 vs. EEM3
Next we turn to a comparison between EEM1
and EEM3. Theoretically, the main difference
between these two instantiations of our general
model is that EEM3 makes much stronger sim-
plifying indepence assumptions than EEM1. In
Figure 1 we compare the two, not only against
the baseline, but, more interestingly, also in terms
of the difference in performance brought about by
switching from uniform estimation of P (c|Q) to
oracle estimation. Most topics gain in AP when
going from the uniform distribution to the oracle
setting. This happens for both models, EEM1 and
EEM3, leading to less topics decreasing in AP
over the baseline (the right part of the plots) and
more topics increasing (the left part). A second
observation is that both gains and losses are higher
for EEM3 than for EEM1.
Zooming in on the differences between EEM1
and EEM3, we compare the two in the same way,
now using EEM3 as ?baseline? (Figure 2). We ob-
serve that EEM3 performs better than EEM1 in 87
1063
-0.4
-0.2
 0
 0.2
 0.4
AP dif
ferenc
e
topics
-0.4
-0.2
 0
 0.2
 0.4
AP dif
ferenc
e
topics
-0.4
-0.2
 0
 0.2
 0.4
AP dif
ferenc
e
topics
-0.4
-0.2
 0
 0.2
 0.4
AP dif
ferenc
e
topics
Figure 1: Per-topic AP differences between the
baseline and (Top): EEM1 and (Bottom): EEM3,
for (Left): uniform P (c|Q) and (Right): oracle.
-0.4
-0.2
 0
 0.2
 0.4
AP
 dif
fere
nce
topics
Figure 2: Per-topic AP differences between EEM3
and EEM1 in the oracle setting.
cases, while EEM1 performs better for 60 topics.
Topics 1041 (federal shield law, 47% AP), 1028
(oregon death with dignity act, 32%AP), and 1032
(I walk the line, 32% AP) have the highest differ-
ence in favor of EEM3; Topics 877 (sonic food in-
dustry, 139% AP), 1013 (iceland european union,
25% AP), and 1002 (wikipedia primary source,
23% AP) are helped most by EEM1. Overall,
EEM3 performs significantly better than EEM1 in
terms of MAP (for ? = .05), but not in terms of
the early precision metrics (P5, P10, and MRR).
8.3 Combining Our Approaches
One observation to come out of ?8.1 and 8.2 is that
different topics prefer not only different external
expansion corpora but also different external ex-
pansion methods. To examine this phenomemon,
we created an articificial run by taking, for ev-
ery topic, the best performing model (with settings
optimized for the topic). Twelve topics preferred
the baseline, 37 EEM1, 20 EEM2, and 81 EEM3.
The articifical run produced the following results:
MAP 0.4280, P5 0.7600, P10 0.7480, and MRR
0.8452; the differences in MAP and P10 between
this run and EEM3 are significant for ? = .01.
We leave it as future work to (learn to) predict for
a given topic, which approach to use, thus refining
ongoing work on query difficulty prediction.
9 Conclusions
We explored the use of external corpora for query
expansion in a user generated content setting. We
introduced a general external expansion model,
which offers various modeling choices, and in-
stantiated it based on different (in)dependence as-
sumptions, leaving us with four instances.
Query expansion using external collection is
effective for retrieval in a user generated con-
tent setting. Furthermore, conditioning the collec-
tion on the query is beneficial for retrieval perfor-
mance, but estimating this component remains dif-
ficult. Dropping the dependencies between terms
and collection and terms and query leads to bet-
ter performance. Finally, the best model is topic-
dependent: constructing an artificial run based on
the best model per topic achieves significant better
results than any of the individual models.
Future work focuses on two themes: (i) topic-
dependent model selection and (ii) improved es-
timates of components. As to (i), we first want
to determine whether a query should be expanded,
and next select the appropriate expansion model.
For (ii), we need better estimates of P (Q|c);
one aspect that could be included is taking P (c)
into account in the query-likelihood estimate of
P (Q|c). One can make this dependent on the task
at hand (blog post retrieval vs. blog feed search).
Another possibility is to look at solutions used in
distributed IR. Finally, we can also include the es-
timation of P (D|c), the importance of a document
in the collection.
Acknowledgements
We thank our reviewers for their valuable feed-
back. This research is supported by the DuOMAn
project carried out within the STEVIN programme
which is funded by the Dutch and Flemish Gov-
ernments (http://www.stevin-tst.org) under project
number STE-09-12, and by the Netherlands Or-
ganisation for Scientific Research (NWO) under
project numbers 017.001.190, 640.001.501, 640.-
002.501, 612.066.512, 612.061.814, 612.061.815,
640.004.802.
1064
References
AQUAINT-2 (2007). URL: http://trec.nist.gov/
data/qa/2007 qadata/qa.07.guidelines.
html#documents.
Arguello, J., Elsas, J., Callan, J., and Carbonell, J. (2008a).
Document representation and query expansion models for
blog recommendation. In Proceedings of ICWSM 2008.
Arguello, J., Elsas, J. L., Callan, J., and Carbonell, J. G.
(2008b). Document representation and query expansion
models for blog recommendation. In Proc. of the 2nd Intl.
Conf. on Weblogs and Social Media (ICWSM).
Baeza-Yates, R. and Ribeiro-Neto, B. (1999). Modern Infor-
mation Retrieval. ACM.
Balog, K., Meij, E., Weerkamp, W., He, J., and de Rijke, M.
(2008a). The University of Amsterdam at TREC 2008:
Blog, Enterprise, and Relevance Feedback. In TREC 2008
Working Notes.
Balog, K., Weerkamp, W., and de Rijke, M. (2008b). A few
examples go a long way: constructing query models from
elaborate query formulations. In SIGIR ?08: Proceedings
of the 31st annual international ACM SIGIR conference on
Research and development in information retrieval, pages
371?378, New York, NY, USA. ACM.
Buckley, C. (2004). Why current IR engines fail. In SIGIR
?04, pages 584?585.
Cronen-Townsend, S., Zhou, Y., and Croft, W. B. (2002). Pre-
dicting query performance. In SIGIR02, pages 299?306.
Diaz, F. and Metzler, D. (2006). Improving the estimation of
relevance models using large external corpora. In SIGIR
?06: Proceedings of the 29th annual international ACM
SIGIR conference on Research and development in infor-
mation retrieval, pages 154?161, New York, NY, USA.
ACM.
Elsas, J., Arguello, J., Callan, J., and Carbonell, J. (2008a).
Retrieval and feedback models for blog distillation. In The
Sixteenth Text REtrieval Conference (TREC 2007) Pro-
ceedings.
Elsas, J. L., Arguello, J., Callan, J., and Carbonell, J. G.
(2008b). Retrieval and feedback models for blog feed
search. In SIGIR ?08: Proceedings of the 31st annual in-
ternational ACM SIGIR conference on Research and de-
velopment in information retrieval, pages 347?354, New
York, NY, USA. ACM.
Ernsting, B., Weerkamp, W., and de Rijke, M. (2008). Lan-
guage modeling approaches to blog post and feed finding.
In The Sixteenth Text REtrieval Conference (TREC 2007)
Proceedings.
Fautsch, C. and Savoy, J. (2008). UniNE at TREC 2008: Fact
and Opinion Retrieval in the Blogsphere. In TREC 2008
Working Notes.
Harman, D. and Buckley, C. (2004). The NRRC reliable in-
formation access (RIA) workshop. In SIGIR ?04, pages
528?529.
Hauff, C., Murdock, V., and Baeza-Yates, R. (2008). Im-
proved query difficulty prediction for the web. In CIKM
?08: Proceedings of the seventeenth ACM conference on
Conference on information and knowledge management,
pages 439?448.
He, J., Larson, M., and de Rijke, M. (2008). Using
coherence-based measures to predict query difficulty.
In 30th European Conference on Information Retrieval
(ECIR 2008), page 689694. Springer, Springer.
Hiemstra, D. (2001). Using Language Models for Informa-
tion Retrieval. PhD thesis, University of Twente.
Java, A., Kolari, P., Finin, T., Joshi, A., and Martineau, J.
(2007). The blogvox opinion retrieval system. In The Fif-
teenth Text REtrieval Conference (TREC 2006) Proceed-
ings.
Kurland, O., Lee, L., and Domshlak, C. (2005). Better than
the real thing?: Iterative pseudo-query processing using
cluster-based language models. In SIGIR ?05, pages 19?
26.
Kwok, K. L., Grunfeld, L., Dinstl, N., and Chan, M. (2001).
TREC-9 cross language, web and question-answering
track experiments using PIRCS. In TREC-9 Proceedings.
Lafferty, J. and Zhai, C. (2003). Probabilistic relevance mod-
els based on document and query generation. In Language
Modeling for Information Retrieval, Kluwer International
Series on Information Retrieval. Springer.
Lavrenko, V. and Croft, W. B. (2001). Relevance based lan-
guage models. In SIGIR ?01, pages 120?127.
Manning, C. D., Raghavan, P., and Schu?tze, H. (2008). Intro-
duction to Information Retrieval. Cambridge University
Press.
Metzler, D. and Croft, W. B. (2005). A markov random field
model for term dependencies. In SIGIR ?05, pages 472?
479, New York, NY, USA. ACM.
Miller, D., Leek, T., and Schwartz, R. (1999). A hidden
Markov model information retrieval system. In SIGIR ?99,
pages 214?221.
Mishne, G. and de Rijke, M. (2006). A study of blog search.
In Lalmas, M., MacFarlane, A., Ru?ger, S., Tombros, A.,
Tsikrika, T., and Yavlinsky, A., editors, Advances in In-
formation Retrieval: Proceedings 28th European Confer-
ence on IR Research (ECIR 2006), volume 3936 of LNCS,
pages 289?301. Springer.
Ounis, I., Macdonald, C., de Rijke, M., Mishne, G., and
Soboroff, I. (2007). Overview of the TREC 2006 Blog
Track. In The Fifteenth Text Retrieval Conference (TREC
2006). NIST.
Ponte, J. M. and Croft, W. B. (1998). A language modeling
approach to information retrieval. In SIGIR ?98, pages
275?281.
Qiu, Y. and Frei, H.-P. (1993). Concept based query expan-
sion. In SIGIR ?93, pages 160?169.
Rocchio, J. (1971). Relevance feedback in information re-
trieval. In The SMART Retrieval System: Experiments in
Automatic Document Processing. Prentice Hall.
Sakai, T. (2002). The use of external text data in cross-
language information retrieval based on machine transla-
tion. In Proceedings IEEE SMC 2002.
Tao, T. and Zhai, C. (2006). Regularized estimation of mix-
ture models for robust pseudo-relevance feedback. In SI-
GIR ?06: Proceedings of the 29th annual international
ACM SIGIR conference on Research and development in
information retrieval, pages 162?169, New York, NY,
USA. ACM.
Weerkamp, W. and de Rijke, M. (2008a). Credibility im-
proves topical blog post retrieval. In ACL-08: HLT, pages
923?931.
Weerkamp, W. and de Rijke, M. (2008b). Looking at things
differently: Exploring perspective recall for informal text
retrieval. In 8th Dutch-Belgian Information Retrieval
Workshop (DIR 2008), pages 93?100.
Yan, R. and Hauptmann, A. (2007). Query expansion us-
ing probabilistic local feedback with application to mul-
timedia retrieval. In CIKM ?07: Proceedings of the six-
teenth ACM conference on Conference on information and
knowledge management, pages 361?370, New York, NY,
USA. ACM.
Zhang, W. and Yu, C. (2007). UIC at TREC 2006 Blog Track.
In The Fifteenth Text REtrieval Conference (TREC 2006)
Proceedings.
1065
BioGrapher: Biography Questions as a
Restricted Domain Question Answering Task
Oren Tsur
Text and Data Mining Group
Bar Ilan University
tsuror@cs.biu.ac.il
Maarten de Rijke
Informatics Institute
University of Amsterdam
mdr@science.uva.nl
Khalil Sima?an
Institute for Logic, Language
and Computation
University of Amsterdam
simaan@science.uva.nl
Abstract
We address Question Answering (QA) for biograph-
ical questions, i.e., questions asking for biographi-
cal facts about persons. The domain of biographical
documents differs from other restricted domains in
that the available collections of biographies are in-
herently incomplete: a major challenge is to answer
questions about persons for whom biographical in-
formation is not present in biography collections.
We present BioGrapher, a biographical QA system
that addresses this problem by machine learning al-
gorithms for biography classification. BioGrapher
first attempts to answer a question by searching in
a given collection of biographies, using techniques
tailored for the restricted nature of the domain. If
a biography is not found, BioGrapher attempts to
find an answer on the web: it retrieves documents
using a web search engine, filters these using the bi-
ography classifier, and then extracts answers from
documents classified as biographies. Our empirical
results show that biographical classification, prior to
answer extraction, improves the results.
1 Introduction
Although most current research in question answer-
ing (QA) is oriented towards open domains, as
witnessed by evaluation exercises such as TREC,
CLEF, and NTCIR, various significant applications
concern restricted domains, e.g., software manuals.
In restricted domains, a QA system faces questions
and documents that exhibit less variation in lan-
guage use (e.g., words and fixed phrases, more spe-
cific terminology) than in an open domain, and it
could access high-quality knowledge sources that
cover the entire domain. Open domain QA as it
is assessed at TREC, CLEF, and NTCIR concerns
a broad variety of fairly restricted question types,
such as location questions, monetary questions, bi-
ography questions, questions that ask for concept
definitions, etc. How useful or effective is it to
adopt a restricted domain approach to some of these
question types? In this paper we explore so-called
biographical questions, e.g., Who was Algar Hiss?
or Who is Sir John Hale?, i.e., questions that de-
mand answers consisting of biograpical key facts
that are typically found in biographies and that typ-
ically involve fixed phrases about, e.g., birthdates,
education, societal roles. This type of questions was
found to be quite frequent in search engine logs. We
believe that biographical questions can be usefully
viewed as defining a restricted domain for QA: the
domain of biographical information as represented
by biographies.
Ideally, biographical questions are answered by
retrieving a biography from some existing collec-
tion of biographies (such as biography.com)
and extracting snippets from it. Such resources,
however, have a limited coverage. There will al-
ways be people whose biographical information is
not contained in any of the existing collections.
This necessitates retrieval of ?biography-like? doc-
uments, i.e., documents with biographical informa-
tion. The problem of identifying biography-like
documents by machine learning algorithms turns
out to be a challenging but rewarding task as we will
see below.
In this paper we address the problem of question
answering within the biographical domain. We de-
scribe BioGrapher, a restricted domain QA system
for answering bibliographical questions in which a
baseline approach, that exploits biography collec-
tions, is extended with a trainable biography classi-
fier operating on the web in order to enhance cov-
erage. The baseline system helps us understand the
usefulness of existing high quality biography col-
lections within a QA system. The extension of our
baseline approach concerns the problem of identi-
fying biography-like documents, and the extraction
from such documents of answers for questions that
could not be answered using biography collections.
A main challenge lies in constructing an algorithm
for identifying documents containing usefull bio-
graphical information that may provide an answer
for a given question. To addresss this challenge, we
explore two machine learning algorithms: Ripper
(Cohen and Singer, 1996) and Support-Vector Ma-
chines (Joachims, 1998).
Section 2 provides some background, and in Sec-
tion 3 we briefly describe our baseline QA sys-
tem, based on external knowledge sources comple-
mented with a naive approach to retrieving biogra-
phy snippets using a web search engine. In Sec-
tion 4 we prepare the ground for our text classifica-
tion experiments. In Section 5 we present two clas-
sifiers: one loosely based on the Ripper algorithm
and the other based on an SVM classifier. In Sec-
tion 6 we compare the performanc of the baseline
against versions of the system integrated with the
two classifiers. Section 7 discusses the results and
considers the possibility of applying our approach
to other restricted domains. We conclude in Sec-
tion 8.
2 Related Work
Two kinds of related work are relevant to this pa-
per: question answering against external knowledge
sources, and genre detection (using classifiers). We
briefly discuss both.
Many QA research groups employed External
Knowledge Sources in order to improve perfor-
mance. For instance, (Chu-Carroll and Prager,
2002) used WordNet to answer what is questions,
using the isa hierarchy supported by WordNet.
(Hovy et al, 2002; Lin, 2002) used dictionaries
such as WordNet and web search results to re-rank
answers. (Yang et al, 2003) preformed structure
analysis of the knowledge obtained from WordNet
and the Web in order to further improve perfor-
mance.
We refer to (Sebastiani, 2002) for extensive re-
view about machine learning in automated text clas-
sification. (Lewis, 1992) were among the first to
use machine learning for genre detection trying to
categorize Reuters articles to predefined categories.
Probabilistic classifiers were used by many groups
(Lewis, 1998). Much current text classification re-
search is focused on Support Vector Machines, first
used for genre detection by (Joachims, 1998).
3 A Na??ve Baseline
In this section we describe our baseline QA system.
This system was used at TREC 2003, to produce an-
swers to so-called person definition questions (Jij-
koun et al, 2004; Voorhees, 2004). We present the
results and give a short analysis of the system?s per-
formance; as we will see, this provides further mo-
tivation for the use of text classification for identi-
fying biography-like documents.
Definition questions at TREC 2003
The QA track at TREC 2003 featured a subtask
devoted to definition questions. The latter came
in three flavors: person definitions (e.g., Who is
Colin Powell?), organization definitions (e.g., What
is the U.N.?), and concept definitions (e.g., What is
goth?). Here, we are only interested person defini-
tions.
In response to a definition question, systems had
to return an unordered set of snippets; each snippet
was supposed to be a facet in the definition of the
target. There were no limits placed on either the
length of an individual answer string or on the num-
ber of snippets in the list, although systems were
penalized for retrieving extraneous information.
As our primary strategy for handling person def-
inition questions, we consulted external resources.
The main resource used is biography.com.
While such resources contain biographies of many
historical and well-known people, they often lack
biographies of contemporary people that are not too
well-known. To be able to deal with such cases we
backed-off to using a web search engine (Google),
and applied a na??ve heuristic approach. We hand-
crafted a set of features (such as ?born?, ?gradu-
ated?, ?suffered?, etc.) that we felt would trigger
for biography-like snippets. Various subsets of the
large feature set, together with the target of the def-
inition question, were combined to form queries for
the web search engine.
Given a set of candidate answer snippets, we per-
formed two filtering steps before presenting the final
answer: we separated non-relevant snippets from
valuable snippets and we identified semantically-
close snippets. We addressed the first step by ana-
lyzing the distances between query terms submitted
to the search engine and the sets of features, and by
means of shallow syntactic aspects of the different
features such as sentence boundaries. To address
the second step we developed a snippet similarity
metric based on stemming, stopword removal and
keyword overlap by sorting and calculating the Lev-
enshtein distance measure of similarity.1. An ex-
ample of the snippets filtering can be found in Ta-
ble 1. The table presents 3 of the returned snippets
for the question Who is Sir John Hale?. The first
and third snippet are filtered out, the first one for
non-relevancy and the third for its semantic similar-
ity with the second, shorter, snippet.
1The Levenshtein measure is a measure of the similarity be-
tween two strings, which are refered to as the source string s
and the target string t. The distance is the number of deletions,
insertions, or substitutions required to transform s into t
1 Sir Malcolm Bradbury (writer/teacher) Dead.
Heart trouble. . . . Heywood Hale Broun (com-
mentator, writer ) ? Dead. John Brunner (au-
thor) Dead. Stroke. . . . Description: Debunks
the rumor of his death. . .
2 . . . Professor Sir John Hale woke up, had . . . For
her birthday in 1996, he wrote on the . . . John
Hale died in his sleep - possibly following an-
other stroke . . .
3 Observer On 29 July 1992, Professor Sir John
Hale woke up . . . her birthday in 1996, he wrote
on the . . . John Hale died in his sleep - possibly
following another stroke.
Table 1: Snippets filtering for Who is Sir John Hale?
Evaluation
Evaluation of individual person definition questions
was done using the F-measure: F = (?2 + 1)P ?
R)/(?2P + R), where P is precision (to be de-
fined shortly), R is recall (to be defined shortly),
and ? was set to 5, indicating that precision was
more important than recall. Length was used as a
crude approximation to precision; it gives a system
an allowance of 100 (non-white-space) characters
for each correct snippet it retrieved. The precision
score is set to one if the response is no longer than
this allowance, otherwise it is downgraded using the
function P = 1? ((length ? allowance)/length).
As to recall, for each question, the TREC asses-
sors marked some snippets as vital and the remain-
der as non-vital. The non-vital snippets act as ?don?t
care? condition. That is, systems should be penal-
ized for not retrieving vital nuggets, and for retriev-
ing snippets that are not in the assessors? snippet
lists at all, but should be neither penalized nor re-
warded for returning a non-vital snippet. To im-
plement the ?don?t care? condition, snippet recall is
computed only over vital snippets (Voorhees, 2004).
In total, 30 person definition questions were eval-
uated at the TREC 2003 QA track. The overall F
score of a run was obtained by averaging over all
the individual questions.
Results and Analysis
The F score obtained by the naive system described
in this section, on the TREC 2003 person definition
questions, was 0.392. An analysis of the results
shows that, for questions that could be answered
from external biography resources, the baseline sys-
tem obtains an F score of 0.586.
In post-submission experiments we changed the
subsets of features we use in the queries sent to
Google as well as the number of queries/subsets we
use. The snippet similarity threshold was also tuned
in order to filter out more snippets. This resulted in
fewer unanswered questions, while the average an-
swer length was decreased as well, by close to 50%.
All in all, an informal evaluation showed increase in
recall, precision and in the overall F score.
From our experience with our baseline system
we learned the following important lesson: having
a (relatively) small number of high quality biogra-
phy sources as a basis for each question?s answer
is far better than using a broad and large variety of
snippets returned by a web search engine. While
extending available biography resources so as to se-
riously boost their coverage is not a feasible option,
we want to do the next best thing: make sure we
identify good biography-like documents online, so
that we can use these to mine snippets from; to this
end we will use text classification.
4 Preparing for Text Classification
In the previous section we suggested that using a
text classifier might improve the performance of bi-
ography QA. Using text classifiers, we aim to iden-
tify biography-like documents from which we can
extract answers. In this section we detail the docu-
ment representations on which we will operate.
Document and Text Representation
Text classifiers represent a document as a set of fea-
tures d = {f1, f2,. . . , fn} where n is the number of
active features, that is, features that occur in the doc-
ument. A feature f can be a word, a set of words, a
stem or any phrasal structure, depending on its text
representation. Each feature has a weight, usually
representing the number of occurrences of this fea-
ture in the document.
What is a suitable abstract representation of doc-
uments for our biography domain? We have defined
7 clusters, groups of words (terms/tokens) with a
high degree of pair-wise semantic relatedness. Each
cluster has a meta-tag symbol (as can be seen in Ta-
ble 2) and all occurrences of members of a cluster
were substituted by the cluster?s meta-tag. An ex-
ample of a document abstraction can be found in Ta-
ble 3. This abstraction captures typical similarities
between biographical strings; e.g., for the two sen-
tences John Kennedy was born in 1917 and William
Shakespeare was born in 1564 we get the same ab-
straction <NAME> <NAME> was born in <YEAR>.
It is worth noting that some of the clusters,
such as <CAP> and <PLACE>, <CAP> and <PN>
and others may overlap. Looking at the exam-
ple in Table 3, we see that Abbey was born in
Chicago, Illinois, but the automatic abstractor mis-
interpreted the token ?Ill.,? marking it is <CAP> for
capitalized (possibly meaningful) word, but not as
<NAME> the name of the subject of the biography
<YEAR> four digits surrounded by white space,
probably a year
<D> sequence of number of digits other than
four digits, can be part of a date, age etc.
<CAP> a capitalized word in the middle of a
sentence that wasn?t substituted by any
other tag
<PN> a proper name that is not the subject of
the biography It substitutes any name
out of a list of thousand names
<PLACE> denotes a name of a place, city or coun-
try out of a list of more than thousand
places
<MONTH> denotes one of the twelve months
<NOM> denotes a nominative
Table 2: Seven meta-tags used for document ab-
straction
<PLACE>. A similar thing happens with the name
?Wooldridge? that is not very common; it should
have been <PN> instead of <CAP>.
All procedures described below are preformed on
abstract-meta-tagged documents.
5 Identifying Biography Documents
Given a document, the task of a biography classifier
is to decide whether a given document is a biogra-
phy or not. In this section we address the problem
of acquiring biography classifiers by training ma-
chine learning algorithms on data. We present two
biography classification algorithms: a naive classi-
fier based on Ripper (Cohen and Singer, 1996), and
another based on SVM (Joachims, 1998). The two
methods differ radically both in the way they rep-
resent the training data (i.e., document representa-
tion), and in their learning approaches. The naive
classifier is obtained by a repetitive rule learning al-
Original Lincoln, Abbey (b. Anna Marie
Wooldridge) 1930 ? Jazz singer, com-
poser/arranger, movie actress; born in
Chicago, Ill. While a teenager she
sang at school and church functions
and then toured locally with a dance
band.
Abstraction <NAME>, <NAME> ( b . <PN> <CAP>
<CAP> ) <YEAR> - <CAP> singer ,
composer/arranger , movie actress ;
born in <PLACE> <CAP> . While
a teenager <NOM> sang at school and
church functions and then toured lo-
cally with a dance band .
Table 3: Abstraction of jazz singer Abbey Wool-
ridge?s biography
gorithm. We modified this algorithm to specifically
fit the task of identifying biographies. The SVM
learns ?linear decision boundaries? between the dif-
ferent classes. We employ here the implementation
of SVMs by (Joachims, 1998). Next we discuss the
details of how each algorithm was used for learning
a biography classifier.
Naive Classifier
We employ this algorithm for its simplicity and scal-
ability. This algorithm learns user-friendly rules,
i.e., human-readable conjunctions of propositions,
which can be converted to queries for a Boolean
search engine. Furthermore, it is known to exhibit
relatively good results across a wide variety of class-
fication problems, including tasks that involve large
collections of noisy data, similar to the large doc-
ument collections that we face in definitional QA.
The naive classifier consists of two main stages:
(1) Rules building. This is similar to Ripper?s first
stage of building an initial rule set. Our algorithm
deviates from standard implementations of Ripper
in that the terms that serve as the literals in the
rules are n-grams of various lengths. We feel that
n-grams, as opposed to individual literals (as in (Co-
hen and Singer, 1996), better capture contextual ef-
fects, which could be crucial in text classification.
Our learner learns the rules as follows. The set of
k-most frequent n-grams representing the training
documents is split into two frequency-ordered lists:
TLP (term-list-positive) containing the positive ex-
ample set and TLN (term-list-negative) containing
the negative examples set. The vector ~w is initial-
ized to be TLP/(TLP ? TLN), i.e., the most fre-
quent n-grams extracted from the positive set that
are not top frequent in the negative set.
(2) Rule optimization. Instead of Ripper?s rule
pruning stage, our algorithm assigns a weight to
each rule/n-gram r in the rules vector according
to the formula g(n)?f(r)C , where g(n) is an increas-
ing function in the length of the n-gram (longer n-
grams receive higher weights), f(r) is the ratio of
the frequency of r in the positive examples to its
frequency in the negative examples, and C is the
size of the training set. The normalization by C
is merely for the purpose of tracking variations of
the weights in different sizes of training sets. The
preference for longer n-grams can be justified by
the intuition that longer n-grams are more informa-
tive as they stand for stricter contexts. For example,
the string ?(<NAME> , <NAME> born in <YEAR>?
seems more informative than the shorter string in
<YEAR>).
Training material. The corpus we used as our
training set is a collection of 350 biographies. Most
of the biographies were randomly sampled from
biography.com, while the rest were collected
from the web. About 130 documents from the New
York Times (NYT) 2000 collection were randomly
selected as negative example set. The volumes of
the positive and negative sets are equal.
Various considerations played a role in building
this corpus. The biographies from biography.
com are ?clean? in the sense that all of them were
written as biographies. To enable the learning of
features of informal biographies, some other ?noisy?
biographies such as biography-like newspaper re-
views were added. Furthermore, a small number of
different biographies of the same person were man-
ually added in order to enforce variation in style.
We also added a small number of biographies from
other different sources to avoid any bias towards the
biography.com domain.
Validation and tuning. We tuned the naive algo-
rithm on a separate validation set of documents. The
validation set was collected in the same way as the
training set. It contained 60 biographies, of which
40 were randomly sampled from biography.
com, 10 ?clean? biographies were collected from
various online sources, 10 other documents were
noisy biographies such as newspaper reviews. In
addition, another 40 non-biographical documents
were randomly retrieved from the web.
The vector ~w is now used to rank the documents
of the validation set V in order to set a threshold
? that minimizes the false-positive and the false-
negative errors. Each document dj ? V in the val-
idation set is represented by a vector ~x, where xi
counts the occurrences of wi in dj . The score of the
document is the normalized inner product of ~x and
~w given by the function score(dj) = ~x?~wlength(dj) .
In the validation stage some heuristic modifica-
tions were applied by the algorithm. For example,
when the person name tag is absent, the document
gets the score of zero even though other parameters
of the vector may be present. We also normalized
document scores by document length.
Support Vector Machines (SVMs)
Now we describe the learning of a biography clas-
sifier using SVMs. Unlike many other classifiers,
SVMs are capable of learning classification even of
non-linearly-separable classes. It is assumed that
classes that are non-linearly separable in one di-
mension may be linearly separable in higher di-
mension. SVMs offer two important advantages
for text classification (Joachims, 1998; Sebastiani,
2002): (1) Term selection is often not needed, as
SVMs tend to be fairly robust to overfitting and
can scale up to considerable dimensionalities, and
(2) No human and machine effort in parameter tun-
ing on a validation set is needed, as there is a the-
oretically motivated ?default? choice of parameter
settings which have been shown to provide best re-
sults.
The key idea behind SVMs is to boost the dimen-
sion of the representation vectors and then to find
the best line or hyper-plane from the widest set of
parallel hyper-planes. This hyper-plane maximizes
the distance between two elements in the set. The
elements in the set are the support vectors. Theo-
retically, the classifier is determined by a very small
number of examples defining the category frontier,
the support vectors. Practically, finding the support
vectors is not a trivial task.
Training SVMs. The implementation used is
SVM-light v.5 (Joachims, 1999). The classifier was
run with its default setting, with linear kernel func-
tion and no kernel optimization tricks. The SVM-
light was trained on the very same (meta-tagged)
training corpus the naive classifier was trained on.
Since SVM is supposed to be robust and to fit big
and noisy collections, no feature selection method
was applied. The special feature underlying SVMs
is the high dimensional representation of a docu-
ment, allowing categorization by a hyper-plane of
high dimension; therefore each document was rep-
resented by the vector of its stems. The dimen-
sion was boosted to include all the stems from the
positive set. The boosted vector dimension was
7935, the number of different stems in the collec-
tion. The number of support vectors discovered was
17, which turned out to be too small for this task.
Testing this model on the test set (the same test set
used to test the naive classifier from previous sec-
tion) yielded very poor results. It seemed that the
classification was totally random. Testing the classi-
fier on smaller subsets of the training set (200, 300,
400 documents) exposed signs of convergence, sug-
gesting the training set is too sparse for the SVM.
To overcome sparse data, more documents were
needed. The size of the training set was more than
doubled. A total of 9968 documents was used as
the training set. Just like the original training set,
most of the biographies were randomly extracted
from biography.com, while a few dozen bi-
ographies were manually extracted from various
online sources to correct for a possible bias in
biography.com. Training SVM-light on the
new training set yielded 232 support vectors, which
seems enough to perform this classification tasks.
6 Experimental Results
In order to test the effectiveness of the biography
classifiers in improving question answering, we in-
tegrated each one of them with the naive baseline
biographical QA system and tested the integrated
system, called BioGrapher (Figure 1). Before dis-
cussing the results of this experiment, we briefly
mention how the two classifiers performed on the
pure classification task. For this purpose, we cre-
ated a test set including 47 documents that were re-
trieved from the web. The evaluation measure was
the accuracy of the classifiers in recognizing biogra-
phies. The Ripper-based algorithm achieved 89%
success, outranking the SVM which achieved 83%.
A discussion of this difference is beyond the scope
of this paper (see (Tsur, 2003) for details).
We tested BioGrapher on 11 out of the 30 bio-
graphical questions in the TREC 2003 QA track.
Those 11 questions were chosen as a test set be-
cause the baseline system (Section 3) scored poorly
on them, suggesting that our baseline heuristics are
incapable of effectively dealing with this type of
questions.
Question
Question 
analyzer
Snippets filter
Answer 
snippets
Biography 
collection
Web
Retrieval 
engine
Document 
classifier
Figure 1: BioGrapher system overview
Two experiments were carried out, one for the
Ripper-based classifier and another for the SVM-
based one. For each definitional question Biogra-
pher submits two simple queries to a web search
engine (e.g., Sir John Hale and Sir John Hale biog-
raphy). It retrieves the top 20 documents returned
by the search engine, thus obtaining, for each ques-
tion, 40 documents amongst which it should find
a biography. BioGrapher then classifies the doc-
uments into biographies and non-biographic doc-
uments. The distribution of documents that were
classified as biographies can be found in Table 4.
To simplify the experiments, and especially the
Question Naive Classifier SVM
Who is Alberto Tomba? 2 4
Who is Albert Ghiorso? 8 2
Who is Alexander Pope? 13 11
Who is Alice Rivlin? 3 2
Who is Absalom? 2 3
Who is Nostradamus? 1 1
Who is Machiavelli? 3 6
Who is Andrea Bocceli? 1 1
Who is Al Sharpton? 2 6
Who is Aga Khan? 4 1
Who is Ben Hur? 2 1
Table 4: Distribution of documents retrieved (in-
cluding false-positive)
error analysis, we set up BioGrapher to return an-
swer snippets from a single biography or biography-
like document only. Recall, the test questions
were such that there were no biographies for the
question targets in the biography collection we
used (biography.com): the biographies used
were ones that BioGrapher identified on the web.
We evaluated BioGrapher in the following manner.
The assessor first determines whether the document
from which BioGrapher extracts answer snippets is
a proper biography or not. In case the document is
not a pure biography the F-score given to this ques-
tion is zero. Otherwise, the F-score was determined
in the manner described in Section 3.2
BioGrapher with the Ripper-based Classifier
The total number of documents that were classi-
fied as biographies is 41 (out of 440 retrieved docu-
ments). However, analysis of the results reveals that
the false positive ratio is high; only 20 of the 41 cho-
sen documents were proper biography-like pages,
the other ?biographies? were very noisy.
For 4 out of the 11 test questions, a proper
biography was returned as the top ranking docu-
ment. While all 4 questions scored 0 at the origi-
nal TREC evaluation, now their average F-score is
0.732, improving the average F-score over all biog-
raphy questions by 9.6% to 0.4659.
BioGrapher with the SVM Classifier
The total number of documents that were classi-
fied as biographies is 38 (out of 440 retrieved docu-
2Obviously, the F-score for snippets extracted from doc-
uments incorrectly classified as biographies could be higher
than zero because these documents could still contain valuable
pieces of biographical information that would contribute to the
answer?s F-score. However, we decided to compute precision
and recall only for snippets extracted from documents correctly
classified as biographies as we think of the biography classi-
fier as a means to identify (?on-the-fly?) quality documents that
could in principle be added to a biography collection.
ments). However, just like in the case of the Ripper-
based classifier, an analysis of the results reveals
that the false positive ratio is high; only 18 of the
38 chosen documents were biography-like.
The SVM classifier managed to return proper bi-
ographies (as top ranking documents) for 5 out of
11 questions. The average F-score for those ques-
tions is 0.674 instead of the original zero, improving
the average F-score over all biography questions by
9.7% to 0.4665.
No biographies at all were retrieved for 4 of the
11 definition targets in the test set, the same four
definition targets for which the Ripper-based classi-
fier did not find biographies. A closer look reveals
the same problems as with the Ripper-based classi-
fier: a relatively high false-positive error ratio and
weak ranking of the classified biographies.
7 Discussion
The results of the experiments using both classifiers
are quite similar. The system integrated with the
SVM-based classifier achieved a slightly higher F-
score but it still falls within the standard deviation.
Our experiment serves as a proof of concept for the
hypothesis that using text classification methods im-
proves the baseline QA system in a principled way.
In spite of the major improvement in the system?s
performance, we have found two main problems
with the classifiers. First, although the classifiers
managed to identify biography-like documents, they
have a high false-positive ratio and too many er-
rors in filtering out some of the non-pure-biography
documents. This happens when the documents re-
trieved by the web search engine simply cannot be
regarded as clean biographies by human assessors,
although they do contain many biographic details.
Second, most of the definition targets had biogra-
phies retrieved and even classified as biographies,
but the biographies were ranked below other noisy
biographical documents, therefore the best biogra-
phy was not presented as a source from which to
extract answer snippets. There are various obvi-
ous paths to improve over the current system: (1)
Improve the classifiers by better training and other
classification algorithms; (2) Enable the extraction
of answers from ?noisy? biography-like documents
in such a way that the gain in recall is not reversed
by a loss of precision; and (3) Allow for the extrac-
tion of answer snippets from multiple biography-
like documents, while avoiding to return overlap-
ping snippets.
8 Conclusion
In this paper we have addressed the problem of bi-
ographical question answering. The main challenge
in this restricted domain is the fact that the available
collections of biography documents are (unavoid-
ably) too small to admit answering all biographi-
cal questions. We use the web as a backoff source
for finding biography-like documents from which to
extract answers in case a given biography collec-
tion does not contain information about the ques-
tion target. We demonstrated the benefits of inte-
grating a text classifier into a restricted domain QA
system as a filter to web retrieval. Finally, we be-
lieve that the use of text classifiers can be benefi-
cial for definitional QA, especially for identifying
documents from which the final answer should be
extracted. Future work will address the weakness
of the current implementation: improved biography
classification and improved answer extraction from
biography-like documents.
Acknowledgments
Maarten de Rijke was supported by the Nether-
lands Organization for Scientific Research (NWO)
under project numbers 612-13-001, 365-20-
005, 612.069.006, 612.000.106, 220-80-001,
612.000.207, and 612.066.302.
References
J. Chu-Carroll and J. Prager. 2002. Use of Word-
Net hypernyms for answering what-is questions.
In Proceedings of the Tenth Text REtrieval Con-
ference (TREC 2001). NIST Special Publication
500-250.
W. Cohen and Y. Singer. 1996. Context sensitive
learning methods. In Proceedings of the 19th
ACM International Conference on Research and
Development in Information Retrieval (SIGIR-
96), pages 307?315. ACM Press.
E. Hovy, U. Hermjakob, and C.Y. Lin. 2002. The
use of external knowledge in factoid QA. In Pro-
ceedings of the Tenth Text REtrieval Conference
(TREC 2001). NIST Special Publication 500-
250.
V. Jijkoun, G. Mishne, C. Monz, M. de Rijke,
S. Schlobach, and O. Tsur. 2004. The Univer-
sity of Amsterdam at the TREC 2003 Question
Answering Track. In E.M. Voorhees, editor, Pro-
ceedings TREC 2003. NIST Special Publication
SP 500-255.
T. Joachims. 1998. Text categorization with sup-
port vector machines: Learning with many rele-
vant features. In Proceedings of ECML-98, 10th
European Conference on Machine Learning.
T. Joachims. 1999. Svm-light v.5, making large-
scale svm learning practical. advances in kernel
methods - support vector learning. B. Scholkopf
and C. Burges and A. Smola (ed.) MIT-Press.
D.D. Lewis. 1992. Representation and learning
in information retrieval. Ph.D. thesis, Graduate
School of the University of Maassachusetts.
D.D. Lewis. 1998. Naive (bayes) at forty: The in-
dependence assumption in information retrieval.
In Proceedings of the 10th European Conference
on Machine Learning, pages 137?142. Springer-
Verlag.
C.Y. Lin. 2002. The effectiveness of dictionary and
web based answer reranking. In The 19th Inter-
national Conference on Computational Linguis-
tics (COLING 2002).
F. Sebastiani. 2002. Machine learning in auto-
mated text categorization. ACM Computing Sur-
veys, 34(1):1?47.
O. Tsur. 2003. Definitional question answering
using trainable text classifiers. Master?s thesis,
ILLC, University of Amsterdam.
E.M. Voorhees. 2004. Overview of the TREC 2003
question answering track. In Text REtrieval Con-
ference (TREC 2004). NIST Special Publication:
SP 500-255.
H. Yang, T.-S. Chua, S. Wang, and C.-K. Koh.
2003. Structured use of external knowledge for
event-based open domain question answering.
In Proceedings of the 26th annual international
ACM SIGIR conference on Research and de-
velopment in informaion retrieval, pages 33?40.
ACM Press.
The University of Amsterdam at Senseval-3:
Semantic Roles and Logic Forms
David Ahn Sisay Fissaha Valentin Jijkoun Maarten de Rijke
Informatics Institute, University of Amsterdam
Kruislaan 403
1098 SJ Amsterdam
The Netherlands
{ahn,sfissaha,jijkoun,mdr}@science.uva.nl
Abstract
We describe our participation in two of the tasks or-
ganized within Senseval-3: Automatic Labeling of
Semantic Roles and Identification of Logic Forms
in English.
1 Introduction
This year (2004), Senseval, a well-established fo-
rum for the evaluation and comparison of word
sense disambiguation (WSD) systems, introduced
two tasks aimed at building semantic representa-
tions of natural language sentences. One task, Auto-
matic Labeling of Semantic Roles (SR), takes as its
theoretical foundation Frame Semantics (Fillmore,
1977) and uses FrameNet (Johnson et al, 2003) as
a data resource for evaluation and system develop-
ment. The definition of the task is simple: given
a natural language sentence and a target word in
the sentence, find other fragments (continuous word
sequences) of the sentence that correspond to ele-
ments of the semantic frame, that is, that serve as
arguments of the predicate introduced by the target
word.
For this task, the systems receive a sentence, a
target word, and a semantic frame (one target word
may belong to multiple frames; hence, for real-
world applications, a preliminary WSD step might
be needed to select an appropriate frame). The out-
put of a system is a list of frame elements, with their
names and character positions in the sentence. The
evaluation of the SR task is based on precision and
recall. For this year?s task, the organizers chose 40
frames from FrameNet 1.1, with 32,560 annnotated
sentences, 8,002 of which formed the test set.
The second task, Identification of Logic Forms
in English (LF), is based on the LF formalism de-
scribed in (Rus, 2002). The LF formalism is a sim-
ple logical form language for natural language se-
mantics with only predicates and variables; there
is no quantification or negation, and atomic predi-
cations are implicitly conjoined. Predicates corre-
spond directly to words and are composed of the
base form of the word, the part of speech tag, and a
sense number (corresponding to the WordNet sense
of the word as used). For the task, the system is
given sentences and must produce LFs. Word sense
disambiguation is not part of the task, so the pred-
icates need not specify WordNet senses. System
evaluation is based on precision and recall of pred-
icates and predicates together with all their argu-
ments as compared to a gold standard.
2 Syntactic Processing
For both tasks, SR and LF, the core of our systems
was the syntactic analysis module described in de-
tail in (Jijkoun and de Rijke, 2004). We only have
space here to give a short overview of the module.
Every sentence was part-of-speech tagged using
a maximum entropy tagger (Ratnaparkhi, 1996) and
parsed using a state-of-the-art wide coverage phrase
structure parser (Collins, 1999). Both the tagger and
the parser are trained on the Penn Treebank Wall
Street Journal Corpus (WSJ in the rest of this paper)
and thus produce structures similar to those in the
Penn Treebank. Unfortunately, the parser does not
deliver some of the information available in WSJ
that is potentially useful for our two applications:
Penn functional tags (e.g., subject, temporal, closely
related, logical subject in passive) and non-local de-
pendencies (e.g., subject and object control, argu-
ment extraction in relative clauses). Our syntactic
module tries to compensate for this and make this
information explicit in the resulting syntactic analy-
ses.
As a first step, we converted phrase trees pro-
duced by the parser to dependency structures, by
detecting heads of constituents and then propagat-
ing the lexical head information up the syntactic
tree, similarly to (Collins, 1999). The resulting de-
pendency structures were labeled with dependency
labels derived from corresponding Penn phrase la-
bels: e.g., a verb phrase (VP) modified by a prepo-
sitional phrase (PP) resulted in a dependency with
label ?VP|PP?.
Then, the information available in the WSJ (func-
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
VP
to seek NP
seats
VP
planned
S
directors
this month
      NP
     NP  S
planned
directors
VP|S
S|NP
S|NP
month
 this
NP|DT to
seek
seats
VP|NPVP|TO
planned
directors
VP|SS|NP?SBJ
S|NP?TMP
S|NP?SBJ
month
 this
NP|DT to
seek
seats
VP|NPVP|TO
(a) (b) (c)
Figure 1: Stages of the syntactic processing: (a) the parser?s output, (b) the result of conversion to a depen-
dency structure, (c) final output of our syntactic module
tional tags, non-local dependencies) was added to
dependency structures using Memory-Based Learn-
ing (Daelemans et al, 2003): we trained the learner
to change dependency labels, or add new nodes or
arcs to dependency structures. Trained and tested
on WSJ, our system achieves state-of-the-art perfor-
mance for recovery of Penn functional tags and non-
local dependencies (Jijkoun and de Rijke, 2004).
Figure 1 shows three stages of the syntactic anal-
ysis of the sentence Directors this month planned to
seek seats (a simplified actual sentence from WSJ):
(a) the phrase structure tree produced by Collins?
parser, (b) the phrase structure tree converted to a
dependency structure and (c) the transformed de-
pendency structure with added functional tags and a
non-local dependency?the final output of our syn-
tactic module. Dependencies are shown as arcs
from heads to dependents.
3 Automatic Labeling of Semantic Roles
For the SR task, we applied a method very similar to
the one used in (Jijkoun and de Rijke, 2004) for re-
covering syntactic structures and somewhat similar
to the first method for automatic semantic role iden-
tification described in (Gildea and Jurafsky, 2002).
Essentially, our method consists of extracting possi-
ble syntactic patterns (paths in syntactic dependency
structures), introducing semantic relations from a
training corpus, and then using a machine learn-
ing classifier to predict which syntactic paths cor-
respond to which frame elements.
Our main assumption was that frame elements,
as annotated in FrameNet, correspond directly to
constituents (constituents being complete subtrees
of dependency structures). Similarly to (Gildea and
Jurafsky, 2002), our own evaluation showed that
about 15% of frame elements in FrameNet 1.1 do
not correspond to constituents, even when applying
some straighforward heuristics (see below) to com-
pensate for this mismatch. This observation puts an
upper bound of around 85% on the accuracy of our
system (with strict evaluation, i.e., if frame element
boundaries must match the gold standard exactly).
Note, though, that these 15% of ?erroneous? con-
stituents also include parsing errors.
Since the core of our SR system operates on
words, constituents, and dependencies, two im-
portant steps are the conversion of FrameNet el-
ements (continuous sequences of characters) into
head words of constituents, and vice versa. The con-
version of FrameNet elements is straightforward:
we take the head of a frame element to be the word
that dominates the most words of this element in
the dependency graph of the sentence. In the other
direction, when converting a subgraph of a depen-
dency graph dominated by a word w into a contin-
uous sequence of words, we take all (i.e., not only
immediate) dependents of w, ignoring non-local de-
pendencies, unless w is the target word of the sen-
tence, in which case we take the word w alone. This
latter heuristic helps us to handle cases when a noun
target word is a semantic argument of itself. Sev-
eral other simple heristics were also found helpful:
e.g., if the result of the conversion of a constituent to
a word sequence contains the target word, we take
only the words to the right of the target word.
With this conversion between frame elements and
constituents, the rest of our system only needs to
operate on words and labeled dependencies.
3.1 Training: the major steps
First, we extract from the training corpus
(dependency-parsed FrameNet sentences, with
words marked as targets and frame elements) all
shortest undirected paths in dependency graphs that
connect target words with their semantic arguments.
In this way, we collect all ?interesting? syntactic
paths from the training corpus.
In the second step, for all extracted syntactic
paths and again for all training sentences, we extract
all occurences of the paths (i.e., paths, starting from
a target word, that actually exist in the dependency
graph), recording for each such occurrence whether
it connects a target word to one of its semantic ar-
guments. For performance reasons, we consider for
each target word only syntactic paths extracted from
sentences annotated with respect to the same frame,
and we ignore all paths of length more than 3.
For every extracted occurrence, we record the
features describing the occurrence of a path in more
detail: the frame name, the path itself, the words
along the path (including the target word and the
possible head of a frame element?first and last
node of the path, respectively), their POS tags and
semantic classes. For nouns, the semantic class
of a word is defined as the hypernym of the first
sense of the noun in WordNet, one of 19 manu-
ally selected terms (animal, person, social group,
clothes, feeling, property, phenomenon, etc.) For
lexical adverbs and prepositions, the semantic class
is one of the 6 clusters obtained automatically using
the K-mean clustering algorithm on data extracted
from FrameNet. Examples of the clusters are:
(abruptly, ironically, slowly, . . . ), (above, beneath,
inside, . . . ), (entirely, enough, better, . . . ). The list
of WordNet hypernyms and the number of clusters
were chosen experimentally. We also added features
describing the subcategorization frame of the tar-
get word; this information is straightforwardly ex-
tracted from the dependency graph. In total, the sys-
tem used 22 features.
The set of path occurrences obtained in the sec-
ond step, with all the extracted features, is a pool of
positive and negative examples of whether certain
syntactic patterns correspond to any semantic argu-
ments. The pool is used as an instance base to train
TiMBL, a memory-based learner (Daelemans et al,
2003), to predict whether the endpoint of a syntac-
tic path starting at a target word corresponds to a
semantic argument, and if so, what its name is.
We chose TiMBL for this task because we had
previously found that it deals successfully with
complex feature spaces and data sparseness (in our
case, in the presence of many lexical features) (Ji-
jkoun and de Rijke, 2004). Moreover, TiMBL is
very flexible and implements many variants of the
basic k-nearest neighbor algorithm. We found that
tuning various parameters (the number of neigh-
bors, weighting and voting schemes) made substan-
tial differences in the performance of our system.
3.2 Applying the system
Once the training is complete, the system can be
applied to new sentences (with the indicated target
word and its frame) as follows. A sentence is parsed
and its dependency structure is built, as described in
Section 2. All occurences of ?interesting? syntac-
tic paths are extracted, along with their features as
described above. The resulting feature vectors are
fed to TiMBL to determine whether the endpoints
of the syntactic paths correspond to semantic argu-
ments of the target word. For the path occurences
classified positively, the constituents of their end-
points are converted to continuous word sequences,
as described earlier; in this case the system has de-
tected a frame element.
3.3 Results
During the development of our system, we used
only the 24,558 sentences from FrameNet set aside
for training by the SR task organizers. To tune the
system, this corpus was randomly split into training
and development sets (70% and 30%, resp.), evenly
for all target words. The official test set (8002 sen-
tences) was used only once to produce the submitted
run, with the whole training set (24,558 sentences)
used for training.
We submitted one run of the system (with iden-
tification of both element boundaries and element
names). Our official scores are: precision 86.9%,
recall 75.2% and overlap 84.7%. Our own evalua-
tion of the submitted run with the strict measures,
i.e., an element is considered correct only if both its
name and boundaries match the gold standard, gave
precision 73.5% and recall 63.6%.
4 Logic Forms
4.1 Method
For the LF task, it was straightforward to turn de-
pendency structures into LFs. Since the LF for-
malism does not attempt to represent the more sub-
tle aspects of semantics, such as quantification, in-
tensionality, modality, or temporality (Rus, 2002),
the primary information encoded in a LF is based
on argument structure, which is already well cap-
tured by the dependency parses. Our LF genera-
tor traverses the dependency structure, turning POS-
tagged lexical items into LF predicates, creating ref-
erential variables for nouns and verbs, and using
dependency labels to order the arguments for each
predicate. We make one change to the dependency
graphs originally produced by the parser. Instead of
taking coordinators, such as and, to modify the con-
stituents they coordinate, we take the coordinated
constituents to be arguments of the coordinator.
Our LF generator builds a labeled directed graph
from a dependency structure and traverses this
graph depth-first. In general, a well-formed depen-
dency graph has exactly one root node, which cor-
responds to the main verb of the sentence. Sen-
tences with multiple independent clauses may have
one root per clause. The generator begins traversing
the graph at one of these root nodes; if there is more
than one, it completes traversal of the subgraph con-
nected to the first node before going on to the next
node.
The first step in processing a node?producing an
LF predicate from the node?s lexical item?is taken
care of in the graph-building stage. We use a base
form dictionary to get the base form of the lexical
item and a simple mapping of Penn Treebank tags
into ?n?, ?v?, ?a?, and ?r? to get the suffix. For words
that are not tagged as nouns, verbs, adjectives, or
adverbs, the LF predicate is simply the word itself.
As the graph is traversed, the processing of a node
depends on its type. The greatest amount of pro-
cessing is required for a node corresponding to a
verb. First, a fresh referential variable is generated
as the event argument of the verbal predication. The
out-edges are then searched for nodes to process.
Since the order of arguments in an LF predication
is important and some sentence constitutents are ig-
nored for the purposes of LF, the out-edges are cho-
sen in order by label: first particles (?VP|PRT?),
then arguments (?S|NP-SBJ?, ?VP|NP?, etc.), and
finally adjuncts. We attempt to follow the argu-
ment order implicit in the description of LF given
in (Rus, 2002), and as the formalism requires, we
ignore auxiliary verbs and negation. The processing
of each of these arguments or adjuncts is handled re-
cursively and returns a set of predications. For mod-
ifiers, the event variable also has to be passed down.
For referential arguments and adjuncts, a referen-
tial variable also is returned to serve as an argument
for the verb?s LF predicate. Once all the arguments
and adjuncts have been processed, a new predica-
tion is generated, in which the verb?s LF predicate
is applied to the event variable and the recursively
generated referential variables. This new predica-
tion, along with the recursively generated ones, is
returned.
The processing of a nominal node proceeds sim-
ilarly. A fresh referential variable is generated?
since determiners are ignored in the LF formalism,
it is simply assumed that all noun phrases corre-
spond to a (possibly composite) individual. Out-
edges are examined for modifiers and recursively
processed. Both the referential variable and the set
of new predications are returned. Noun compounds
introduce some additional complexity; each modi-
fying noun introduces two additional variables, one
for the modifying noun and one for composite indi-
vidual realizing the compound. This latter variable
then replaces the referential variable for the head
noun.
Processing of other types of nodes proceeds in a
similar fashion. For modifiers such as adjectives,
adverbs, and prepositional phrases, a variable (cor-
responding to the individual or event being modi-
fied) is passed in, and the LF predicate of the node
is applied to this variable, rather than to a fresh
variable. In the case of prepositional phrases, the
predicate is applied to this variable and to the vari-
able corresponding to the object of the preposition,
which must be processed, as well. The latter vari-
able is then returned along with the new predica-
tions. For other modifiers, just the predications are
returned.
4.2 Development and results
The rules for handling dependency labels were writ-
ten by hand. Of the roughly 1100 dependency la-
bels that the parser assigns (see Section 2), our sys-
tem handles 45 labels, all of which fall within the
most frequent 135 labels. About 50 of these 135
labels are dependencies that can be ignored in the
generation of LFs (labels involving punctuation, de-
terminers, auxiliary verbs, etc.); of the remaining
85 labels, the 45 labels handled were chosen to pro-
vide reasonable coverage over the sample corpus
provided by the task organizers. Extending the sys-
tem is straightforward; to handle a dependency label
linking two node types, a rule matching the label
and invoking the dependent node handler is added
to the head node handler.
On the sample corpus of 50 sentences to which
our system was tuned, predicate identification, com-
pared to the provided LFs, including POS-tags, was
performed with 89.1% precision and 87.1% recall.
Argument identification was performed with 78.9%
precision and 77.4% recall. On the test corpus of
300 sentences, our official results, which exclude
POS-tags, were 82.0% precision and 78.4% recall
for predicate identification and 73.0% precision and
69.1% recall for argument identification.
We did not get the gold standard for the test cor-
pus in time to perform error analysis for our official
submission, but we did examine the errors in the
LFs we generated for the trial corpus. Most could
be traced to errors in the dependency parses, which
is unsurprising, since the generation of LFs from de-
pendency parses is relatively straightforward. A few
errors resulted from the fact that our system does not
try to identify multi-word compounds.
Some discrepancies between our LFs and the LFs
provided for the trial corpus arose from apparent
inconsistencies in the provided LFs. Verbs with
particles were a particular problem. Sometimes,
as in sentences 12 and 13 of the trial corpus, a
verb-particle combination such as look forward to
is treated as a single predicate (look forward to); in
other cases, such as in sentence 35, the verb and its
particle (go out) are treated as separate predicates.
Other inconsistencies in the provided LFs include
missing arguments (direct object in sentence 24),
and verbs not reduced to base form (felt, saw, and
found in sentences 34, 48, 50).
5 Conclusions
Our main finding during the development of the sys-
tems for the two Senseval tasks was that semantic
relations are indeed very close to syntactic depen-
dencies. Using deep dependency structures helped
to keep the manual rules for the LF task simple
and made the learning for the SR task easier. Also
we found that memory-based learning can be effi-
ciently applied to complex, highly structured prob-
lems such as the identification of semantic roles.
Our future work includes more accurate fine-
tuning of the learner for the SR task, extending the
coverage of the LF generator, and experimenting
with the generated LFs for question answering.
6 Acknowledgments
Ahn and De Rijke were supported by a grant from
the Netherlands Organization for Scientific Re-
search (NWO) under project number 612.066.302.
Fissaha, Jijkoun, and De Rijke were supported by a
grant from NWO under project number 220-80-001.
De Rijke was also supported by grants from NWO,
under project numbers 365-20-005, 612.069.006,
612.000.106, and 612.000.207.
References
M. Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Uni-
versity of Pennsylvania.
W. Daelemans, J. Zavrel, K. van der Sloot, and
A. van den Bosch, 2003. TiMBL: Tilburg Mem-
ory Based Learner, version 5.0, Reference Guide.
ILK Technical Report 03-10. Available from
http://ilk.kub.nl/downloads/pub/papers/ilk0310.pdf.
C. J. Fillmore. 1977. The need for a frame semantics in
linguistics. Statistical Methods in Linguistics, 12:5?
29.
D. Gildea and D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
28(3):245?288.
V. Jijkoun and M. de Rijke. 2004. Enriching the output
of a parser using memory-based learning. In Proceed-
ings of ACL 2004.
C. Johnson, M. Petruck, C. Baker, M. Ellsworth, J. Rup-
penhofer, and C. Fillmore. 2003. Framenet: Theory
and practice. http://www.icsi.berkeley.edu/ framenet.
A. Ratnaparkhi. 1996. A maximum entropy part-of-
speech tagger. In Proceedings of the Empirical Meth-
ods in Natural Language Processing Conference.
V. Rus. 2002. Logic Form for WordNet Glosses. Ph.D.
thesis, Southern Methodist University.
Proceedings of the ACL Workshop on Feature Engineering for Machine Learning in NLP, pages 9?16,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Feature Engineering and Post-Processing for Temporal Expression
Recognition Using Conditional Random Fields
Sisay Fissaha Adafre Maarten de Rijke
Informatics Institute, University of Amsterdam
Kruislaan 403, 1098 SJ Amsterdam, The Netherlands
sfissaha,mdr@science.uva.nl
Abstract
We present the results of feature engineer-
ing and post-processing experiments con-
ducted on a temporal expression recogni-
tion task. The former explores the use of
different kinds of tagging schemes and of
exploiting a list of core temporal expres-
sions during training. The latter is con-
cerned with the use of this list for post-
processing the output of a system based on
conditional random fields.
We find that the incorporation of knowl-
edge sources both for training and post-
processing improves recall, while the use
of extended tagging schemes may help
to offset the (mildly) negative impact on
precision. Each of these approaches ad-
dresses a different aspect of the over-
all recognition performance. Taken sep-
arately, the impact on the overall perfor-
mance is low, but by combining the ap-
proaches we achieve both high precision
and high recall scores.
1 Introduction
Temporal expressions (timexes) are natural language
phrases that refer directly to time points or intervals.
They not only convey temporal information on their
own but also serve as anchors for locating events re-
ferred to in a text. Timex recognition is a named
entity recognition (NER) task to which a variety of
natural language processing and machine learning
techniques have been applied. As with other NER
tasks, timex recognition is naturally viewed as a se-
quence labeling task, easily lending itself to ma-
chine learning techniques such as conditional ran-
dom fields (CRFs) (Lafferty et al, 2001).
A preliminary experiment showed that, using
CRFs, a respectable recognition performance can
easily be achieved with a straightforward baseline
system that is based on a simple tagging scheme and
requires very little tuning, yielding F-scores around
0.78 (exact match) or even 0.90 (partial match).
Interestingly, these high scores are mainly due to
high or even very high precision scores, while recall
leaves much to be improved.
The main focus of this paper is on boosting re-
call while maintaining precision at an acceptable
(i.e., high) level. We report on two types of ex-
periments aimed at achieving this goal. One type
concerns feature engineering and the other concerns
post-processing the output of a machine learner.
While we do exploit the special nature of timexes,
for portability reasons we avoid using task-specific
and richer linguistic features (POS, chunks, etc.). In-
stead, we focus on features and techniques that can
readily be applied to other NER tasks.
Specifically, our feature engineering experiments
have two facets. The first concerns identification of
a set of simple features that results in high general-
ization ability (accuracy). Here, particular emphasis
will be placed on the use of a list of core timexes as
a feature. The assumption is that the performance of
data-driven approaches for timex recognition can be
improved by taking into account the peculiar prop-
erties of timexes. Timexes exhibit various patterns,
ranging from regular patterns that can easily be cap-
tured using simple regular expressions to complex
linguistic forms (phrases). While timexes are real-
9
ized in different phrase types, the core lexical items
of timexes are restricted. This suggests that a list
of core timexes can easily be compiled and used in
machine learning-based timex recognition. One ap-
proach of integrating such a list is using them to gen-
erate features, but the availability of such a list also
opens up other possibilities in feature design that we
present in later sections.
The second aspect concerns the tagging scheme.
As in most NER experiments, the task of recogniz-
ing timexes is reduced to tagging. Commonly used
tagging schemes are Inside-Outside (IO) and Begin-
Continue-End-Unique-Negative (BCEUN) (Borth-
wick et al, 1998). The IO tagging scheme, which we
use as a baseline, assigns the tag I to a token if it is
part of a timex and O otherwise. The richer BCEUN
scheme assigns the five tags B, C, E, U, and N to to-
kens depending on whether the token is single-token
timex (U), a non-timex (N), appears at the beginning
(B), at the end (E) or inside a timex boundary (C). In
this paper, we compare the IO, BCEUN and an ex-
tended form of the BCEUN tagging scheme. The
extended scheme adds two tags, PRE and POST, to
the BCEUN scheme, which correspond to tokens ap-
pearing to the left and to the right of a timex.
In contrast, our post-processing experiments in-
vestigate the application of the list of core timexes
for filtering the output of a machine learner. The in-
corporation into the recognition process of explicit
knowledge in the form of a list for post-processing
requires a carefully designed strategy to ensure that
the important properties of the trained model are
kept intact as much as possible while at the same-
time improving overall results. We present an ap-
proach for using a list for post-processing that ex-
ploits the knowledge embodied in the trained model.
The paper is organized as follows. In Section 2
we provide background material, both on the timex
extraction task (?2.1) and on the machine learning
techniques on which we build in this paper, condi-
tional random fields (?2.2). Our ideas on engineer-
ing feature sets and tagging schemes are presented
in Section 3, while we describe our method for ex-
ploiting the explicit knowledge contained in a list in
Section 4. In Section 5, we describe the experimen-
tal setup and present the results of our experiments.
Related work is briefly reviewed in Section 6, and
we conclude in Section 7.
2 Background
2.1 Task Description
In recent years, temporal aspects of information ac-
cess have received increasing amounts of attention,
especially as it relates to news documents. In addi-
tion to factual content, news documents have a tem-
poral context, reporting events that happened, are
happening, or will happen in relation to the publi-
cation date. Temporal document retrieval concerns
the inclusion of both the document publication date
and the in-text temporal expressions in the retrieval
model (Kalczynski and Chou, 2005). The task in
which we are interested in this paper is identifying
the latter type of expressions, i.e., extraction of tem-
poral expressions. TERN, the Temporal Expression
Recognition and Normalization Evaluation, is orga-
nized under the auspices of the Automatic Content
Extraction program (ACE, http://www.nist.
gov/speech/tests/ace/). The TERN evalu-
ation provides specific guidelines for the identifica-
tion and normalization of timexes, as well as tagged
corpora for training and testing and evaluation soft-
ware. These guidelines and resources were used for
the experiments described below.
The TERN evaluation consisted of two distinct
tasks: recognition and normalization. Timex recog-
nition involves correctly detecting and delimiting
timexes in text. Normalization involves assigning
recognized timexes a fully qualified temporal value.
Our focus in this paper is on the recognition task;
it is defined, for human annotators, in the TIDES
TIMEX2 annotation guidelines (Ferro et al, 2004).
The recognition task is performed with respect to
corpora of transcribed broadcast news speech and
news wire texts from ACE 2002, ACE 2003, and
ACE 2004, marked up in SGML format and, for
the training set, hand-annotated for TIMEX2s. An
official scorer that evaluates the recognition perfor-
mance is provided as part of the TERN evaluation. It
computes precision, recall, and F-measure both for
TIMEX2 tags (i.e., for overlap with a gold standard
TIMEX2 element) and for extent of TIMEX2 ele-
ments (i.e., exact match of entire timexes).
2.2 Conditional Random Fields
We view the recognition of timexes task as a se-
quence labeling task in which each token in the text
10
is classified as being either a timex or not. One ma-
chine learning technique that has recently been in-
troduced to tackle the problem of labeling and seg-
menting sequence data is conditional random fields
(CRFs, (Lafferty et al, 2001)). CRFs are conditional
probability distributions that take the form of ex-
ponential models. The special case of linear chain
CRFs, which takes the following form, has been
widely used for sequence labeling tasks:
P (y | x) =
1
Z (x)
exp
(
?
t=1
?
k
?kfk (t, yt?1, yt, x)
)
,
where Z (x) is the normalization factor, X =
{x1, . . . , xn} is the observation sequence, Y =
{y1, . . . , yT } is the label sequences, fk and ?k are
the feature functions and their weights respectively.
An important property of these models is that proba-
bilities are computed based on a set of feature func-
tions, i.e., fk (usually binary valued), which are de-
fined on both the observation X and label sequences
Y . These feature functions describe different aspect
of the data and may overlap, providing a flexible way
of describing the task.
CRFs have been shown to perform well in a
number of natural language processing applications,
such as POS tagging (Lafferty et al, 2001), shallow
parsing or NP chunking (Sha and Pereira, 2003), and
named entity recognition (McCallum and Li, 2003).
In this paper, CRFs are applied to the recognition of
timexes; in our experiments we used the minorThird
implementation of CRFs (Cohen, 2004).
3 Feature Engineering
The success of applying CRFs depends on the qual-
ity of the set of features used and the tagging scheme
chosen. Below, we discuss these two aspects in
greater detail.
3.1 Feature sets
Our baseline feature set consists of simple lexical
and character features. These features are derived
from a context window of two words (left and right).
Specifically, the features are the lowercase form of
all the tokens in the span, with each token contribut-
ing a separate feature, and the tokens in the left and
right context window constitute another set of fea-
tures. These feature sets capture the lexical con-
tent and context of timexes. Additionally, charac-
ter type pattern features (such as capitalization, digit
sequence) of tokens in the timexes are used to cap-
ture the character patterns exhibited by some of the
tokens in temporal expressions. These features con-
stitute the basic feature set.
Another important feature is the list of core
timexes. The list is obtained by first extracting the
phrases with -TMP function tags from the PennTree
bank, and taking the words in these phrases (Marcus
et al, 1993). The resulting list is filtered for stop-
words. Among others, the list of core timexes con-
sists of the names of days of the week and months,
temporal units ?day,? ?month,? ?year,? etc. This list
is used to generate binary features. In addition, the
list is used to guide the design of other complex fea-
tures that may involve one or more of token-tag pairs
in the context of the current token. One way of using
the list for this purpose is to generate a feature that
involves bi-grams tokens. In certain cases, informa-
tion extracted from bi-grams, e.g. +Xx 99 (May 20),
can be more informative than information generated
from individual tokens. We refer to these features as
the list feature set.
3.2 Tagging schemes
A second aspect of feature engineering that we
consider in this paper concerns different tagging
schemes. As mentioned previously, the task of rec-
ognizing timexes is reduced to a sequence-labeling
task. We compare three tagging schemes, IO
(our baseline), BCEUN, and BCEUN+PRE&POST.
While the first two are relatively standard, the last
one is an extension of the BCEUN scheme. The
intuition underlying this tagging scheme is that the
most relevant features for timex recognition are ex-
tracted from the immediate context of the timex,
e.g., the word ?During? in (1) below.
(1) During <TIMEX2>the past week</TIMEX2>,
the storm has pounded the city.
During-PRE the-B past-C week-E ,-POST the
storm has pounded the city.
Therefore, instead of treating these elements uni-
formly as outside (N), which ignores their relative
importance, we conjecture that it is worthwhile to
11
assign them a special category, like PRE and POST
corresponding to the tokens immediately preceding
and following a timex, and that this leads to im-
proved results.
4 Post-processing Using a List
In this section, we describe the proposed method
for incorporating a list of core lexical timexes for
post-processing the output of a machine learner. As
we will see below, although the baseline system
(with the IO tagging scheme and the basic feature
set) achieves a high accuracy, the recall scores leave
much to be desired. One important problem that we
have identified is that timexes headed by core lexical
items on the list may be missed. This is either due
to the fact that some of these lexical items are se-
mantically ambiguous and appear in a non-temporal
sense, or the training material does not cover the par-
ticular context. In such cases, a reliable list of core
timexes can be used to identify the missing timexes.
For the purposes of this paper, we have created a
list containing mainly headwords of timexes. These
words are called trigger words since they are good
indicators of the presence of temporal expressions.
How can we use trigger words? Before describ-
ing our method in some detail, we briefly describe
a more naive (and problematic) approach. Observe
that trigger words usually appear in a text along with
their complements or adjuncts. As a result, pick-
ing only these words will usually contribute to token
recall but span precision is likely to drop. Further-
more, there is no principled way of deciding which
one to pick (semantically ambiguous elements will
also be picked). Let?s make this more precise. The
aim is to take into account the knowledge acquired
by the trained model and to search for the next op-
timal sequence of tags, which assigns the missed
timex a non-negative tag. However, searching for
this sequence by taking the whole word sequence
is impractical since the number of possible tag se-
quences (number of all possible paths in a viterbi
search) is very large. But if one limits the search to
a window of size n (n < 6), sequential search will
be feasible. The method, then, works on the output
of the system. We illustrate the method by using the
example given in (2) below.
(2) The chairman arrived in the city yesterday, and
will leave next week. The press conference will
be held tomorrow afternoon.
Now, assume that (2) is a test instance (a two-
sentence document), and that the system returns the
following best sequence (3). For readability, the tag
N is not shown on the words that are assigned nega-
tive tags in all the examples below.
(3) The chairman arrived in the city yesterday-U ,
and will leave next week . The press conference
will be held tomorrow-B afternoon-E .
According to (3), the system recognizes only ?yes-
terday? and ?tomorrow afternoon? but misses ?next
week?. Assuming our list of timexes contains the
word ?week?, it tells us that there is a missing tem-
poral expression, headed by ?week.? The naive
method is to go through the above output sequence
and change the token-tag pair ?week-N? to ?week-
U?. This procedure recognizes the token ?week? as a
valid temporal expression, but this is not correct: the
valid temporal expression is ?next week?.
We now describe a second approach to incorpo-
rating the knowledge contained in a list of core lexi-
cal timexes as a post-processing device. To illustrate
our ideas, take the complete sequence in (3) and ex-
tract the following segment, which is a window of 7
tokens centered at ?week?.
(4) . . . [will leave next week . The press] . . .
We reclassify the tokens in (4) assuming the history
contains the token ?and? (the token which appears to
the left of this segment in the original sequence) and
the associated parameters. Of course, the best se-
quence will still assign both ?next? and ?week? the N
tag since the underlying parameters (feature sets and
the associated weights) are the same as the ones in
the system. However, since the word sequence in (4)
is now short (contains only 7 words) we can main-
tain a list of all possible tag sequences for it and per-
form a sequential search for the next best sequence,
which assigns the ?week? token a non-negative tag.
Assume the new tag sequence looks as follows:
(5) . . . [will leave next-B week-E . The press] . . .
This tag sequence will then be placed back into the
original sequence resulting in (6):
12
(6) The chairman arrived in the city yesterday-U ,
and will leave next-B week-E . The press con-
ference will be held tomorrow-B afternoon-E .
In this case, all the temporal expressions will be ex-
tracted since the token sequence ?next week? is prop-
erly tagged. Of course, the above procedure can also
return other, invalid sequences as in (7):
(7) a. . . . will leave next-B week-C . The press . . .
b. . . . will leave next week-C . The press . . .
c. . . . will leave next week-C .-E The press . . .
The final extraction step will not return any timex
since none of the candidate sequences in (7) contains
a valid tag sequence. The assumption here is that of
all the tag sequences, which assign the token ?week?
a non-negative tag, those tag sequences which con-
tain the segment ?next-B week-E? are likely to re-
ceive a higher weight since the underlying system
is trained to recognize temporal expressions and the
phrase ?next week? is a likely temporal expression.
This way, we hypothesize, it is possible to ex-
ploit the knowledge embodied in the trained model.
As pointed out previously, simply going through
the list and picking only head words like ?week?
will not guarantee that the extracted tokens form a
valid temporal expression. On the other hand, the
above heuristics, which relies on the trained model,
is likely to pick the adjunct ?next?.
The post-processing method we have just out-
lined boils down to reclassifying a small segment
of a complete sequence using the same parameters
(feature sets and associated weights) as the original
model, and keeping all possible candidate sequences
and searching through them to find a valid sequence.
5 Experimental Evaluation
In this section we provide an experimental assess-
ment of the feature engineering and post-processing
methods introduced in Sections 3 and 4. Specifi-
cally, we want to determine what their impact is on
the precision and recall scores of the baseline sys-
tem, and how they can be combined to boost recall
while keeping precision at an acceptable level.
5.1 Experimental data
The training data consists of 511 files, and the test
data consists of 192 files; these files were made
available in the 2004 Temporal Expression Recog-
nition and Normalization Evaluation. The tempo-
ral expressions in the training files are marked with
XML tags. The minorThird system takes care of
automatically converting from XML format to the
corresponding tagging schemes. A temporal expres-
sion enclosed by <TIMEX2> tags constitutes a span.
The features in the training instances are generated
by looking at the surface forms of the tokens in the
spans and their surrounding contexts.
5.2 Experimental results
Richer feature sets Table 1 lists the results of the
first part of our experiments. Specifically, for every
tagging scheme, there are two sets of features, basic
and list. The results are based on both exact-match
and partial match between the spans in the gold stan-
dard and the spans in the output of the systems, as
explained in Subsection 2.1. In both the exact and
partial match criteria, the addition of the list features
leads to an improvement in recall, and no change or
a decrease in precision.
In sum, the feature addition helps recall more than
it hurts precision, as the F score goes up nearly ev-
erywhere, except for the exact-match/baseline pair.
Tagging schemes In Table 1 we also list the ex-
traction scores for the tagging schemes we con-
sider, IO, BCEUN, and BCEUN+PRE&POST, as
described in Section 3.2.
Let us first look at the impact of the different tag-
ging schemes in combination with the basic feature
set (rows 3, 5, 7). As we go from the baseline
tagging scheme IO to the more complex BCEUN
and BCEUN+PRE&POS, precision increases on
the exact-match criterion but remains almost the
same on the partial match criterion. Recall, on
the other hand, does not show the same trend.
BCEUN has the highest recall values followed by
BCEUN+PRE&POST and finally IO. In general,
IO based tagging seems to perform worse whereas
BCEUN based tagging scores slightly above its ex-
tended tagging scheme BCEUN+PRE&POST.
Next, considering the combination of extend-
ing the feature set and moving to a richer tagging
scheme (rows 4, 6, 8), we have very much the same
pattern. In both the exact match and the partial
match setting, BCEUN tops (or almost tops) the two
13
Exact Match Partial Match
Tagging scheme Features Prec. Rec. F Prec. Rec. F
IO (baseline) basic 0.846 0.723 0.780 0.973 0.832 0.897
basic + list 0.822 0.736 0.776 0.963 0.862 0.910
BCEUN basic 0.874 0.768 0.817 0.974 0.856 0.911
basic + list 0.872 0.794 0.831 0.974 0.887 0.928
BCEUN+PRE&POS basic 0.882 0.749 0.810 0.979 0.831 0.899
basic + list 0.869 0.785 0.825 0.975 0.881 0.925
Table 1: Timex: Results of training on basic and list features, and different tagging schemes. Highest scores
(Precision, Recall, F-measure) are in bold face.
other schemes in both precision and recall.
In sum, the richer tagging schemes function as
precision enhancing devices. The effect is clearly
visible for the exact-match setting, but less so for
partial matching. It is not the case that the learner
trained on the richest tagging scheme outperforms
all learners trained with poorer schemes.
Post-processing Table 2 shows the results of ap-
plying the post-processing method described in
Section 4. One general pattern we observe in
Table 2 is that the addition of the list features
improves precision for IO and BCEUN tagging
scheme and shows a minor reduction in precision
for BCEUN+PRE&POS tagging scheme in both
matching criteria. Similarly, in the presence of
post-processing, the use of a more complex tagging
scheme results in a better precision. On the other
hand, recall shows a different pattern. The addi-
tion of list features improves recall both for BCEUN
and BCEUN+PRE&POS, but hurts recall for the IO
scheme for both matching criteria.
Comparing the results in Table 1 and Table 2,
we see that post-processing is a recall enhancing
device since all the recall values in Table 2 are
higher than the recall values in Table 1. Pre-
cision values in Table 2, on the other hand, are
lower than those of Table 1. Importantly, the
use of a more complex tagging scheme such as
BCEUN+PRE&POS, allows us to minimize the
drop in precision. In general, the best result (on
partial match) in Table 1 is achieved through the
combination of BCEUN and basic&list features
whereas the best result in Table 2 is achieved by
the combination of BCEUN+PRE&POS and basic
&list features. Although both have the same over-
all scores on the exact match criteria, the latter per-
forms better on partial match criteria. This, in turn,
shows that the combination of post-processing, and
BCEUN+PRE&POS achieves better results.
Stepping back We have seen that the extended
tagging scheme and the post-processing methods
improve on different aspects of the overall per-
formance. As mentioned previously, the ex-
tended tagging scheme is both recall and precision-
oriented, while the post-processing method is pri-
marily recall-oriented. Combining these two meth-
ods results in a system which maintains both these
properties and achieves a better overall result. In or-
der to see how these two methods complement each
other it is sufficient to look at the highest scores
for both precision and recall. The extended tagging
scheme with basic features achieves the highest pre-
cision but has relatively low recall. On the other
hand, the simplest form, the IO tagging scheme
and basic features with post-processing, achieves
the highest recall and the lowest precision in par-
tial match. This shows that the IO tagging scheme
with basic features imposes a minimal amount of
constraints, which allows for most of the timexes in
the list to be extracted. Put differently, it does not
discriminate well between the valid vs invalid oc-
currences of timexes from the list in the text. At the
other extreme, the extended tagging scheme with 7
tags imposes strict criteria on the type of words that
constitute a timex, thereby restricting which occur-
rences of the timex in the list count as valid timexes.
In general, although the overall gain in score is
limited, our feature engineering and post-processing
efforts reveal some interesting facts. First, they show
one possible way of using a list for post-processing.
14
Exact Match Partial Match
Tagging scheme Features Prec. Rec. F Prec. Rec. F
IO basic (baseline) 0.846 0.723 0.780 0.973 0.832 0.897
basic 0.756 0.780 0.768 0.902 0.931 0.916
basic + list 0.772 0.752 0.762 0.930 0.906 0.918
BCEUN basic 0.827 0.789 0.808 0.945 0.901 0.922
basic + list 0.847 0.801 0.823 0.958 0.906 0.931
BCEUN+PRE&POS basic 0.863 0.765 0.811 0.973 0.863 0.915
basic + list 0.861 0.804 0.831 0.970 0.906 0.937
Table 2: Timex: Results of applying post-processing on the systems in Table 1. The baseline (from Table 1)
is repeated for ease of reference; it does not use post-processing. Highest scores (Precision, Recall, F-
measure) are in bold face.
This method is especially appropriate for situations
where better recall is important. It offers a means of
controlling the loss in precision (gain in recall) by
allowing a systematic process of recovering missing
expressions that exploits the knowledge already em-
bodied in a probabilistically trained model, thereby
reducing the extent to which we have to make ran-
dom decisions. The method is particularly sensitive
to the criterion (the quality of the list in the current
experiment) used for post-processing.
6 Related Work
A large number of publications deals with extraction
of temporal expressions; the task is often treated as
part of a more involved task combining recognition
and normalization of timexes. As a result, many
timex interpretation systems are a mixture of both
rule-based and machine learning approaches (Mani
and Wilson, 2000). This is partly due to the fact that
timex recognition is more amenable to data-driven
methods whereas normalization is best handled us-
ing primarily rule-based methods. We focused on
machine learning methods for the timex recognition
task only. See (Katz et al, 2005) for an overview of
methods used for addressing the TERN 2004 task.
In many machine learning-based named-entity
recognition tasks dictionaries are used for improving
results. They are commonly used to generate binary
features. Sarawagi and Cohen (2004) showed that
semi-CRFs models for NE recognition perform bet-
ter than conventional CRFs. One advantage of semi-
CRFs models is that the units that will be tagged are
segments which may contain one or more tokens,
rather than single tokens as is done in conventional
CRFs. This in turn allows one to incorporate seg-
ment based-features, e.g., segment length, and also
facilitates integration of external dictionaries since
segments are more likely to match the entries of an
external dictionary than tokens. In this paper, we
stuck to conventional CRFs, which are computation-
ally less expensive, and introduced post-processing
techniques, which takes into account knowledge em-
bodied in the trained model.
Kristjannson et al (2004) introduced constrained
CRFs (CCRFs), a model which returns an optimal
label sequence that fulfills a set of constraints im-
posed by the user. The model is meant to be used in
an interactive information extraction environment,
in which the system extracts structured information
(fields) from a text and presents it to the user, and
the user makes the necessary correction and submits
it back to the system. These corrections constitute
an additional set of constraints for CCRFs. CCRFs
re-computes the optimal sequence by taking these
constraints into account. The method is shown to
reduce the number of user interactions required in
validating the extracted information. In a very lim-
ited sense our approach is similar to this work. The
list of core lexical timexes that we use represents
the set of constraints on the output of the underly-
ing system. However, our method differs in the way
in which the constraints are implemented. In our
case, we take a segment of the whole sequence that
contains a missing timex, and reclassify the words
in this segment while keeping all possible tag se-
quences sorted based on their weights. We then
15
search for the next optimal sequence that assigns the
missing timex a non-negative tag sequentially. On
the other hand, Kristjannson et al (2004) take the
whole sequence and recompute an optimal sequence
that satisfies the given constraints. The constraints
are a set of states which the resulting optimal se-
quence should include.
7 Conclusion
In this paper we presented different feature engi-
neering and post-processing approaches for improv-
ing the results of timex recognition task. The first
approach explores the different set of features that
can be used for training a CRF-based timex recog-
nition system. The second investigates the effect of
the different tagging scheme for timex recognition
task. The final approach we considered applies a list
of core timexes for post-processing the output of a
CRF system. Each of these approaches addresses
different aspects of the overall performance. The
use of a list of timexes both during training and for
post-processing resulted in improved recall whereas
the use of a more complex tagging scheme results
in better precision. Their individual overall contri-
bution to the recognition performances is limited or
even negative whereas their combination resulted in
substantial improvements over the baseline.
While we exploited the special nature of timexes,
we did avoid using linguistic features (POS, chunks,
etc.), and we did so for portability reasons. We fo-
cused exclusively on features and techniques that
can readily be applied to other named entity recog-
nition tasks. For instance, the basic and list features
can also be used in NER tasks such as PERSON,
LOCATION, etc. Moreover, the way that we have
used a list of core expressions for post-processing is
also task-independent, and it can easily be applied
for other NER tasks.
Acknowledgments
Sisay Fissaha Adafre was supported by the Nether-
lands Organization for Scientific Research (NWO)
under project number 220-80-001. Maarten de
Rijke was supported by grants from NWO, under
project numbers 365-20-005, 612.069.006, 220-80-
001, 612.000.106, 612.000.207, 612.066.302, 264-
70-050, and 017.001.190.
References
[Borthwick et al1998] A. Borthwick, J. Sterling,
E. Agichtein, and R. Grishman. 1998. Exploiting
diverse knowledge sources via maximum entropy in
named entity recognition. In Workshop on Very Large
Corpora, ACL.
[Cohen2004] W. Cohen. 2004. Methods for identifying
names and ontological relations in text using heuris-
tics for inducing regularities from data. http://
minorthird.sourceforge.net.
[Ferro et al2004] L. Ferro, L. Gerber, I. Mani, and
G. Wilson, 2004. TIDES 2003 Standard for the An-
notation of Temporal Expressions. MITRE, April.
[Kalczynski and Chou2005] P.J. Kalczynski and A. Chou.
2005. Temporal document retrieval model for business
news archives. Information Processing and Manage-
ment, 41:635?650.
[Katz et al2005] G. Katz, J. Pustejovsky, and F. Schilder,
editors. 2005. Proceedings Dagstuhl Workshop on
Annotating, Extracting, and Reasoning about Time
and Events.
[Kristjannson et al2004] T. Kristjannson, A. Culotta,
P. Viola, and A. McCallum. 2004. Interactive infor-
mation extraction with constrained conditional random
fields. In Nineteenth National Conference on Artificial
Intelligence, AAAI.
[Lafferty et al2001] J. Lafferty, F. Pereira, and A. McCal-
lum. 2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. In
Proceedings of the International Conference on Ma-
chine Learning.
[Mani and Wilson2000] I. Mani and G. Wilson. 2000.
Robust temporal processing of news. In Proceedings
of the 38th ACL.
[Marcus et al1993] M.P. Marcus, B. Santorini, and M.A.
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn treebank. Computational
Linguistics, 19:313?330.
[McCallum and Li2003] A. McCallum and W. Li. 2003.
Early results for Named Entity Recognition with con-
ditional random fields, feature induction and web-
enhanced lexicons. In Proceedings of the 7th CoNLL.
[Sarawagi and Cohen2004] S. Sarawagi and W.W. Cohen.
2004. Semi-markov conditional random fields for in-
formation extraction. In NIPs (to appear).
[Sha and Pereira2003] F. Sha and F. Pereira. 2003. Shal-
low parsing with conditional random fields. In Pro-
ceedings of Human Language Technology-NAACL.
16
Representing and Querying Multi-dimensional Markup
for Question Answering
Wouter Alink, Valentin Jijkoun, David Ahn, Maarten de Rijke
ISLA, University of Amsterdam
alink,jijkoun,ahn,mdr@science.uva.nl
Peter Boncz, Arjen de Vries
CWI, Amsterdam, The Netherlands
boncz,arjen@cwi.nl
Abstract
This paper describes our approach to rep-
resenting and querying multi-dimensional,
possibly overlapping text annotations, as
used in our question answering (QA) sys-
tem. We use a system extending XQuery,
the W3C-standard XML query language,
with new axes that allow one to jump eas-
ily between different annotations of the
same data. The new axes are formulated in
terms of (partial) overlap and containment.
All annotations are made using stand-off
XML in a single document, which can be
efficiently queried using the XQuery ex-
tension. The system is scalable to giga-
bytes of XML annotations. We show ex-
amples of the system in QA scenarios.
1 Introduction
Corpus-based question answering is a complex
task that draws from information retrieval, infor-
mation extraction and computational linguistics to
pinpoint information users are interested in. The
flexibility of natural language means that poten-
tial answers to questions may be phrased in differ-
ent ways?lexical and syntactic variation, ambi-
guity, polysemy, and anaphoricity all contribute to
a gap between questions and answers. Typically,
QA systems rely on a range of linguistic analyses,
provided by a variety of different tools, to bridge
this gap from questions to possible answers.
In our work, we focus on how we can integrate
the analyses provided by completely independent
linguistic processing components into a uniform
QA framework. On the one hand, we would like
to be able, as much as possible, to make use of
off-the-shelf NLP tools from various sources with-
out having to worry about whether the output of
the tools are compatible, either in a strong sense
of forming a single hierarchy or even in a weaker
sense of simply sharing common tokenization. On
the other hand, we would like to be able to issue
simple and clear queries that jointly draw upon an-
notations provided by different tools.
To this end, we store annotated data as stand-
off XML and query it using an extension of
XQuery with our new StandOff axes, inspired by
(Burkowski, 1992). Key to our approach is the use
of stand-off annotation at every stage of the anno-
tation process. The source text, or character data,
is stored in a Binary Large OBject (BLOB), and all
annotations, in a single XML document. To gen-
erate and manage the annotations we have adopted
XIRAF (Alink, 2005), a framework for integrating
annotation tools which has already been success-
fully used in digital forensic investigations.
Before performing any linguistic analysis, the
source documents, which may contain XMLmeta-
data, are split into a BLOB and an XML docu-
ment, and the XML document is used as the ini-
tial annotation. Various linguistic analysis tools
are run over the data, such as a named-entity tag-
ger, a temporal expression (timex) tagger, and syn-
tactic phrase structure and dependency parsers.
The XML document will grow during this analy-
sis phase as new annotations are added by the NLP
tools, while the BLOB remains intact. In the end,
the result is a fully annotated stand-off document,
and this annotated document is the basis for our
QA system, which uses XQuery extended with the
new axes to access the annotations.
The remainder of the paper is organized as fol-
lows. In Section 2 we briefly discuss related work.
Section 3 is devoted to the issue of querying multi-
dimensional markup. Then we describe how we
coordinate the process of text annotation, in Sec-
3
tion 4, before describing the application of our
multi-dimensional approach to linguistic annota-
tion to question answering in Section 5. We con-
clude in Section 6.
2 Related Work
XML is a tree structured language and provides
very limited capabilities for representing several
annotations of the same data simultaneously, even
when each of the annotations is tree-like. In par-
ticular, in the case of inline markup, multiple an-
notation trees can be put together in a single XML
document only if elements from different annota-
tions do not cross each other?s boundaries.
Several proposals have tried to circumvent this
problem in various ways. Some approaches are
based on splitting overlapping elements into frag-
ments. Some use SGML with the CONCUR fea-
ture or even entirely different markup schemes
(such as LMNL, the Layered Markup and An-
notation Language (Piez, 2004), or GODDAGs,
generalized ordered-descendant directed acyclic
graphs (Sperberg-McQueen and Huitfeldt, 2000))
that allow arbitrary intersections of elements from
different hierarchies. Some approaches use empty
XML elements (milestones) to mark beginnings
and ends of problematic elements. We refer to
(DeRose, 2004) for an in-depth overview.
Although many approaches solve the problem
of representing possibly overlapping annotations,
they often do not address the issue of accessing
or querying the resulting representations. This
is a serious disadvantage, since standard query
languages, such as XPath and XQuery, and stan-
dard query evaluation engines cannot be used with
these representations directly.
The approach of (Sperberg-McQueen and Huit-
feldt, 2000) uses GODDAGs as a conceptual
model of multiple tree-like annotations of the
same data. Operationalizing this approach,
(Dekhtyar and Iacob, 2005) describes a system
that uses multiple inline XML annotations of the
same text to build a GODDAG structure, which
can be queried using EXPath, an extension of
XPath with new axis steps.
Our approach differs from that of Dekhtyar and
Iacob in several ways. First of all, we do not use
multiple separate documents; instead, all annota-
tion layers are woven into a single XML docu-
ment. Secondly, we use stand-off rather than in-
line annotation; each character in the original doc-
ument is referred to by a unique offset, which
means that specific regions in a document can be
denoted unambiguously with only a start and an
end offset. On the query side, our extended XPath
axes are similar to the axes of Dekhtyar and Iacob,
but less specific: e.g., we do not distinguish be-
tween left-overlapping and right-overlapping char-
acter regions.
In the setting of question answering there
are a few examples of querying and retrieving
semistructured data. Litowski (Litkowksi, 2003;
Litkowksi, 2004) has been advocating the use of
an XML-based infrastructure for question answer-
ing, with XPath-based querying at the back-end,
for a number of years. Ogilvie (2004) outlines the
possibility of using multi-dimensional markup for
question answering, with no system or experimen-
tal results yet. Jijkoun et al (2005) describe initial
experiments with XQuesta, a question answering
system based on multi-dimensional markup.
3 Querying Multi-dimensional Markup
Our approach to markup is based on stand-off
XML. Stand-off XML is already widely used, al-
though it is often not recognized as such. It can
be found in many present-day applications, es-
pecially where annotations of audio or video are
concerned. Furthermore, many existing multi-
dimensional-markup languages, such as LMNL,
can be translated into stand-off XML.
We split annotated data into two parts: the
BLOB (Binary Large OBject) and the XML anno-
tations that refer to specific regions of the BLOB.
A BLOB may be an arbitrary byte string (e.g., the
contents of a hard drive (Alink, 2005)), and the
annotations may refer to regions using positions
such as byte offsets, word offsets, points in time
or frame numbers (e.g., for audio or video appli-
cations). In text-based applications, such as de-
scribed in this paper, we use character offsets. The
advantage of such character-based references over
word- or token-based ones is that it allows us to
reconcile possibly different tokenizations by dif-
ferent text analysis tools (cf. Section 4).
In short, a multi-dimensional document consists
of a BLOB and a set of stand-off XML annota-
tions of the BLOB. Our approach to querying such
documents extends the common XML query lan-
guages XPath and XQuery by defining 4 new axes
that allow one to move from one XML tree to an-
other. Until recently, there have been very few
4
AB
C
E
D
XML tree 1
XML tree 2
BLOB
(text characters)
Figure 1: Two annotations of the same data.
approaches to querying stand-off documents. We
take the approach of (Alink, 2005), which allows
the user to relate different annotations using con-
tainment and overlap conditions. This is done us-
ing the new StandOff XPath axis steps that we add
to the XQuery language. This approach seems to
be quite general: in (Alink, 2005) it is shown that
many of the query scenarios given in (Iacob et al,
2004) can be easily handled by using these Stand-
Off axis steps.
Let us explain the axis steps by means of an
example. Figure 1 shows two annotations of the
same character string (BLOB), where the first
XML annotation is
<A start="10" end="50">
<B start="30" end="50"/>
</A>
and the second is
<E start="20" end="60">
<C start="20" end="40"/>
<D start="55" end="60">
</E>
While each annotation forms a valid XML tree and
can be queried using standard XML query lan-
guages, together they make up a more complex
structure.
StandOff axis steps, inspired by (Burkowski,
1992), allow for querying overlap and contain-
ment of regions, but otherwise behave like reg-
ular XPath steps, such as child (the step be-
tween A and B in Figure 1) or sibling (the step
between C and D). The new StandOff axes, de-
noted with select-narrow, select-wide,
reject-narrow, and reject-wide select
contained, overlapping, non-contained and non-
overlapping region elements, respectively, from
possibly distinct layers of XML annotation of the
data. Table 1 lists some examples for the annota-
tions of our example document.
In XPath, the new axis steps are used in exactly
the same way as the standard ones. For example,
Context Axis Result nodes
A select-narrow B C
A select-wide B C E
A reject-narrow E D
A reject-wide D
Table 1: Example annotations.
the XPath query:
//B/select-wide::*
returns all nodes that overlap with the span of a
B node: in our case the nodes A, B, C and E. The
query:
//*[./select-narrow::B]
returns nodes that contain the span of B: in our
case, the nodes A and E.
In implementing the new steps, one of our de-
sign decisions was to put all stand-off annotations
in a single document. For this, an XML processor
is needed that is capable of handling large amounts
of XML. We have decided to use MonetDB/X-
Query, an XQuery implementation that consists of
the Pathfinder compiler, which translates XQuery
statements into a relational algebra, and the re-
lational database MonetDB (Grust, 2002; Boncz,
2002).
The implementation of the new axis steps in
MonetDB/XQuery is quite efficient. When the
XMark benchmark documents (XMark, 2006)
are represented using stand-off notation, query-
ing with the StandOff axis steps is interactive for
document size up to 1GB. Even millions of re-
gions are handled efficiently. The reason for the
speed of the StandOff axis steps is twofold. First,
they are accelerated by keeping a database in-
dex on the region attributes, which allows fast
merge-algorithms to be used in their evaluation.
Such merge-algorithms make a single linear scan
through the index to compute each StandOff
step. The second technical innovation is ?loop-
lifting.? This is a general principle inMonetDB/X-
Query(Boncz et al, 2005) for the efficient execu-
tion of XPath steps that occur nested in XQuery
iterations (i.e., inside for-loops). A naive strategy
would invoke the StandOff algorithm for each it-
eration, leading to repeated (potentially many) se-
quential scans. Loop-lifted versions of the Stand-
Off algorithms, in contrast, handle all iterations to-
gether in one sequential scan, keeping the average
complexity of the StandOff steps linear.
5
The StandOff axis steps are part of release
0.10 of the open-source MonetDB/XQuery prod-
uct, which can be downloaded from http://
www.monetdb.nl/XQuery.
In addition to the StandOff axis steps, a key-
word search function has been added to the
XQuery system to allow queries asking for re-
gions containing specific words. This function
is called so-contains($node, $needle)
which will return a boolean specifying whether
$needle occurs in the given region represented
by the element $node.
4 Combining Annotations
In our QA application of multi-dimensional
markup, we work with corpora of newspaper arti-
cles, each of which comes with some basic anno-
tation, such as title, body, keywords, timestamp,
topic, etc. We take this initial annotation structure
and split it into raw data, which comprises all tex-
tual content, and the XML markup. The raw data
is the BLOB, and the XML annotations are con-
verted to stand-off format. To each XML element
originally containing textual data (now stored in
the BLOB), we add a start and end attribute
denoting its position in the BLOB.
We use a separate system, XIRAF, to coordi-
nate the process of automatically annotating the
text. XIRAF (Figure 2) combines multiple text
processing tools, each having an input descriptor
and a tool-specific wrapper that converts the tool
output into stand-off XML annotation. Figure 3
shows the interaction of XIRAF with an automatic
annotation tool using a wrapper.
The input descriptor associated with a tool is
used to select regions in the data that are candi-
dates for processing by that tool. The descrip-
tor may select regions on the basis of the original
metadata or annotations added by other tools. For
example, both our sentence splitter and our tempo-
ral expression tagger use original document meta-
data to select their input: both select document
text, with //TEXT. Other tools, such as syntac-
tic parsers and named-entity taggers, require sep-
arated sentences as input and thus use the output
annotations of the sentence splitter, with the input
descriptor //sentence. In general, there may
be arbitrary dependencies between text-processing
tools, which XIRAF takes into account.
In order to add the new annotations generated
by a tool to the original document, the output of
the tool must be represented using stand-off XML
annotation of the input data. Many text process-
ing tools (e.g., parsers or part-of-speech taggers)
do not produce XML annotation per se, but their
output can be easily converted to stand-off XML
annotation. More problematically, text process-
ing tools may actually modify the input text in the
course of adding annotations, so that the offsets
referenced in the new annotations do not corre-
spond to the original BLOB. Tools make a vari-
ety of modifications to their input text: some per-
form their own tokenization (i.e., inserting whites-
paces or other word separators), silently skip parts
of the input (e.g., syntactic parsers, when the pars-
ing fails), or replace special symbols (e.g., paren-
theses with -LRB- and -RRB-). For many of the
available text processing tools, such possible mod-
ifications are not fully documented.
XIRAF, then, must map the output of a process-
ing tool back to the original BLOB before adding
the new annotations to the original document. This
re-alignment of the output of the processing tools
with the original BLOB is one of the major hur-
dles in the development of our system. We ap-
proach the problems systematically. We compare
the text data in the output of a given tool with the
data that was given to it as input and re-align in-
put and output offsets of markup elements using
an edit-distance algorithm with heuristically cho-
sen weights of character edits. After re-aligning
the output with the original BLOB and adjusting
the offsets accordingly, the actual data returned by
the tool is discarded and only the stand-off markup
is added to the existing document annotations.
5 Question Answering
XQuesta, our corpus-based question-answering
system for English and Dutch, makes use of the
multi-dimensional approach to linguistic annota-
tion embodied in XIRAF. The system analyzes an
incoming question to determine the required an-
swer type and keyword queries for retrieving rel-
evant snippets from the corpus. From these snip-
pets, candidate answers are extracted, ranked, and
returned.
The system consults Dutch and English news-
paper corpora. Using XIRAF, we annotate the
corpora with named entities (including type infor-
mation), temporal expressions (normalized to ISO
values), syntactic chunks, and syntactic parses
(dependency parses for Dutch and phrase structure
6
;4XHVWD ;,5$))HDWXUH([WUDFWLRQ)UDPHZRUN;4XHU\6\VWHP
    	

 
 Learning to Recognize Blogs: A Preliminary Exploration 
 
 
Erik Elgersma and Maarten de Rijke 
ISLA, University of Amsterdam 
Kruislaan 403, 1098SJ Amsterdam, The Netherlands 
erik@elgersma.net, mdr@science.uva.nl 
 
  
 
Abstract 
We present results of our experiments 
with the application of machine learning 
on binary blog classification, i.e. deter-
mining whether a given web page is a 
blog page. We have gathered a corpus in 
excess of half a million blog or blog-like 
pages and pre-classified them using a 
simple baseline. We investigate which 
algorithms attain the best results for our 
classification problem and experiment 
with resampling techniques, with the aim 
of utilising our large dataset to improve 
upon our baseline. We show that the ap-
plication of off-the-shelf machine learn-
ing technology to perform binary blog 
classification offers substantial improve-
ment over our baseline. Further gains can 
sometimes be achieved using resampling 
techniques, but these improvements are 
relatively small compared to the initial 
gain. 
1 Introduction 
In recent years, weblogs (online journals in 
which the owner posts entries on a regular basis) 
have not only rapidly become popular as a new 
and easily accessible publishing tool for the 
masses, but its content is becoming ever more 
valuable as a ?window to the world,? an exten-
sive medium brimming with subjective content 
that can be mined and analysed to discover what 
people are talking about and why. In recent years 
the volume of blogs is estimated to have doubled 
approximately every six months. Technorati1 
report that about 11% of internet users are blog 
readers and that about 70 thousand new blogs are 
created daily. Popular blogosphere (the complete 
collection of all blogs) analysis tools estimate the 
blogosphere to contain anywhere between 201 
and 24 million2 blogs at time of writing. Given 
this growing popularity and size, research on 
blogs and the blogosphere is also increasing. A 
large amount of this research is being done on 
the content provided by the blogosphere and the 
nature of this content, like for example (Mishne 
and de Rijke, 2005), or the structure of the blo-
gosphere (Adar et al, 2004).  
In this paper, however, we address the task of 
binary blog classification: given a (web) docu-
ment, is this a blog or not? Our aim is to base 
this classification mostly on blog characteristics 
rather than content. We will by no means ignore 
content but it should not become a crucial part of 
the classification process. 
Reliable blog classification is an important 
task in the blogosphere as it allows researchers, 
ping feeds (used to broadcast blog updates), 
trend analysis tools and many others to separate 
real blog content from blog-like content such as 
bulletin boards, newsgroups or trade markets. It 
is a task that so far has proved difficult as can be 
witnessed by checking any of the major blog up-
date feeds such as weblogs.com3 or blo.gs.4 Both 
will at any given time list content that clearly is 
not a blog. In this paper we will explore blog 
classification using machine learning to improve 
blog detection and experiment with several 
methods to try and further improve the percent-
age of instances classified correctly. 
The main research question we address in this 
paper is exploratory in nature: 
- How hard is binary blog classification? 
Put more specifically, 
                                                 
1 Intelliseek?s BlogPulse, http://www.blogpulse.com 
2 Technorati, http://www.technorati.com 
3 Weblogs.com, http://www.weblogs.com 
4 Blo.gs, http://blo.gs 
24
- What is the performance of basic off-the-
shelf machine learning algorithms on this 
task? 
and 
- Can the performance of these methods be 
improved using resampling methods such 
as bootstrapping and co-training? 
 
An important complicating factor is the lack of 
labeled data. It is widely accepted that given a 
sufficient amount of training data, most machine 
learning algorithms will achieve similar per-
formance levels. For our experiments, we will 
have a very limited amount of training material 
available. Therefore, we expect to see substantial 
differences between algorithms. 
In this paper we will first discuss related work 
in the following section, before describing the 
experiments in detail and reporting on the results. 
Finally, we will draw conclusions based on the 
experiments and the results. 
2 Related Work 
Blog classification is still very much in its in-
fancy and to date no directly related work has 
been published as far as we are aware. There is, 
however, work related to several aspects of our 
experiments.  
Nanno et al (2004) describe a system for 
gathering a large collection of weblogs, not only 
those published using one of the many well-
known authoring tools but also the hand-written 
variety. A very much comparable system was 
developed and used for these experiments. 
Members of the BlogPulse team also describe 
blog crawling and corpus creation in some detail 
(Glance et al, 2004), but their system is aimed 
more at gathering updates and following active 
blogs rather than gathering as many blogs in their 
entirety, as our system is set up to do. 
As to the resampling methods used in this pa-
per?bootstrapping and co-training?, Jones et 
al. (1999) describe the application of bootstrap-
ping to text learning tasks and report very good 
results applying this method to these tasks. Even 
though text learning is a very different genre, 
their results provide hope that the application of 
this method may also prove useful for our blog 
classification problem. 
Blum and Mitchell (1998) describe the use of 
separate weak indicators to label unlabeled in-
stances as ?probably positive? to further train a 
learning algorithm and gathered results that sug-
gested that their method has the potential for im-
proving results on many practical learning prob-
lems. Indeed their example of web-page classifi-
cation is in many ways very similar to our binary 
blog classification problem. In these experiments 
however we will use a different kind of indica-
tors on the unlabeled data, namely the predic-
tions of several different types of algorithms. 
3 Binary blog classification 
In our first experiment, we attempted binary 
blog classification (?is this a blog or not??) using 
a small manually annotated dataset and a large 
variety of algorithms. The aim of this experiment 
was to discover what the performance of readily 
available, off-the-shelf algorithms is given this 
task. 
We used a broad spectrum of learners imple-
mented in the well-known Weka machine learn-
ing toolkit (Witten and Frank, 2005). 
3.1 Dataset 
For our later resampling experiments, a large 
amount of data was gathered, as will be ex-
plained further on in this paper. To create a data-
set for this experiment, 201 blog / blog-like 
pages were randomly selected from the collec-
tion, processed into Weka?s arff format and 
manually annotated. These instances were then 
excluded from the rest of the collection. This 
yielded a small but reliable dataset, which we 
hoped would be sufficient for this task. 
3.2 Attribute selection 
All pages were processed into instances de-
scribed by a variety of attributes. For binary blog 
classification to succeed, we had to find a large 
number of characteristics with which to accu-
rately describe the data. This was done by manu-
ally browsing the HTML source code of several 
blogs as well as some simple intuition. These 
attributes range from ?number of posts? and 
?post length? to checking for characteristic 
phrases such as ?Comments? or ?Archives? or 
checking for the use of style sheets. Interesting 
attributes are the ?firstLine? / ?lastLine? attrib-
utes, which calculate a score depending on the 
number of tokens found in those lines, which 
frequently occur in those lines in verified blog 
posts. The ?contentType? attribute does some-
thing very similar, but based on the complete 
clean text of a page rather than particular lines in 
posts. It counts how many of the 100 most fre-
quent tokens in clean text versions of actual 
blogs, are found in a page and returns a true 
25
value if more than 60% of these are found, in 
which case the page is probably a blog. The ?fre-
quent terms?-lists for these attributes were gen-
erated using a manually verified list gathered 
from a general purpose dataset used for earlier 
experiments. A ?host?-attribute is also used, 
which we binarised into a large number of binary 
host name attributes as most machine learning 
algorithms cannot cope with string attributes. For 
this purpose we took the 30 most common hosts 
in our dataset, which included Livejournal,5 
Xanga,6 20six,7 etc., but also a number of hosts 
that are obviously not blog sites (but host many 
pages that resemble blogs). Negative indicators 
on common hosts that don?t serve blogs are just 
as valuable to the machine learner as the positive 
indicators of common blog hosts. Last but not 
least a binary attribute was added that acts as a 
class label for the instance. This process left us 
with the following 46 attributes: 
 
Attribute Type 
nrOfPosts numeric 
avgPostLength numeric 
minPostLength numeric 
maxPostLength numeric 
firstLine numeric 
lastLine numeric 
containsBlog numeric 
containsMetaTag binary 
contentType binary 
containsComment binary 
containsPostedBy binary 
containsRSS binary 
containsArchives binary 
containsPreviousPosts binary 
StyleSheetsUsed binary 
livejournal.com binary 
msn.com binary 
wretch.cc binary 
xanga.com binary 
diaryland.com binary 
abazy.com binary 
20six.fr binary 
research101-411.com binary 
search-now700.com binary 
search-now999.com binary 
search-now600.com binary 
20six.co.uk binary 
research-bot.com binary 
                                                 
5 http://www.livejournal.com 
6 http://www.xanga.com 
7 http://www.20six.com 
blogsearchonline.com binary 
googane.com binary 
typepad.com binary 
findbestnow.com binary 
myblog.de binary 
quick-blog.com binary 
findhererightnow.com binary 
findfreenow.com binary 
websearch010.com binary 
twoday.net binary 
websearch013.com binary 
tracetotal.info binary 
kotobabooks.com binary 
cocolog-nifty.com binary 
20six.de binary 
is-here-online.com binary 
4moreadvice.info binary 
blog binary 
 
Table 1: Attributes selected for our experiments. 
3.3 Experimental setup 
For this experiment, we trained a wide range of 
learners using the manually annotated data and 
tested using ten-fold cross-validation. We then 
compared the results to a baseline. 
This baseline is based mostly on simple heu-
ristics, and is an extended version of the WWW-
Blog-Identify8 perl module that is freely avail-
able online. First of all, a URL check is done 
which looks for a large number of the well-
known blog hosts as an indicator. Should this 
fail, a search is done for metatags which indicate 
the use of well-known blog creation tools such as 
Nucleus,9 Greymatter,10 Movable Type11 etc. 
Should this also fail, an actual content search is 
done for other indicators such as particular icons 
blog creation tools leave on pages (?created us-
ing? .gif? etc). Next, the module checks for an 
RSS feed, and as a very last resort checks the 
number of times the term ?blog? is used on the 
page as an indicator. 
In earlier research, our version of the module 
was manually tested by a small group of indi-
viduals and found to have an accuracy of roughly 
80% which means it is very useful as a target to 
aim for with our machine learning algorithms 
and a good baseline. 
                                                 
8 http://search.cpan.org/~mceglows/WWW-Blog-Identify-
0.06/Identify.pm 
9 http://nucleuscms.org 
10 http://www.noahgrey.com/greysoft/ 
11 http://www.movabletype.org/ 
26
3.4 Results: single classifiers 
 
Figure 1: Chart showing the percentage cor-
rect predictions for each algorithm tested. 
 
It is clear that all algorithms bar ZeroR perform 
well, most topping 90%. ZeroR achieves no 
more than 73%, and is the only algorithm that 
actually performs worse than our baseline. The 
best algorithm for this task, and on this dataset, is 
clearly the support vector-based algorithm SMO, 
which scores 94.75%. These scores can be con-
sidered excellent for a classification task, and the 
wide success across the range of algorithms 
shows that our attribute selection has been a suc-
cess. The attributes clearly describe the data 
well.  
Full results of this experiment can be found in 
Appendix A. 
4 Resampling 
Now we turn to the second of our research ques-
tions: to what extent can resampling methods 
help create better blog classifiers. 
As reported earlier, the blogosphere today 
contains millions of blogs and therefore poten-
tially plenty of data for our classifier. However, 
this data is all unlabeled. Furthermore, we have a 
distinct lack of reliably labeled data. Resampling 
may provide us with a solution to this problem 
and allow us to reliably label the data from our 
unlabeled data source and further improve upon 
the results gained using our very small manually 
annotated dataset. 
For these experiments we selected two resam-
pling methods. The first is ordinary bootstrap-
ping, which we chose because it is the simplest 
way of relabeling unlabeled data on the basis of a 
machine learning model. Additionally, we chose 
a modified form of co-training, as co-training is 
also a well-known resampling method, which 
was easily adaptable to our problem and seem-
ingly offered a good approach. 
4.1 Data set 
To gather a large data set containing both blogs 
and non-blogs, a crawler was developed that in-
cluded a blog detection module based on the heu-
ristics in our baseline module mentioned earlier. 
After downloading a page judged likely to be a 
blog by the module on the basis of its URL, sev-
eral additional checks were done by the blog de-
tection module based on several other character-
istics, most importantly the presence of date-
entry combinations. Pages judged to be a blog 
and those judged not to be even though the URL 
looked promising, were consequently stored 
separately. Blogs were stored in html, clean text 
and single entry (text) formats. For non-blogs 
only the html was stored to conserve space while 
still allowing the documents to be fully analysed 
post-crawling.  
Using this system, 227.380 blog- and 285.337 
non-blog pages (often several pages were gath-
ered from the same blog, so the actual number of 
blogs gathered is significantly lower) were gath-
ered in the period from July 7 until November 3,  
2005. This amounts to roughly 30Gb of HTML 
and text, and includes blogs from all the well-
known blog sites as well as personal hand-
written blogs and in many different languages. 
The blog detection module in the crawler was 
used purely for the purpose of filtering out URLs 
and webpages that bear no resemblence to a 
blog. By performing this pre-classification, we 
were able to gather a dataset containing only 
blogs and pages that in appearance closely re-
semble blogs so that our dataset contained both 
positive examples and useful negative examples. 
This approach should force the machine learner 
to make a clear distinction between blogs and 
non-blogs. However, even though this data was 
pre-classified by our baseline, we treat it as unla-
beled data in our experiments and make no fur-
ther use of this pre-classification whatsoever. 
For our resampling experiments, we randomly 
divided the large dataset into small subsets con-
taining 1000 instances, one for each iteration. 
This figure ensures that the training set grows at 
a reasonable rate at every iteration while prevent-
ing the training set from becoming too large too 
quickly which would mean a lot of unlabeled 
27
instances being labeled on the basis of very few 
labeled instances and the model building process 
would take too long after only a few iterations. 
For training and test data we turned back to 
our manually annotated dataset used previously. 
Of this set, 100 instances were used for the initial 
training and the remaining 101 for testing. 
4.2 Experimental setup: bootstrapping 
Generally, bootstrapping is an iterative process 
where at every iteration unlabeled data is labeled 
using predictions made by the learner model 
based on the previously available training set 
(Jones et al, 1999). These newly labeled in-
stances are then added to the training set and the 
whole process repeats. Our expectation was that 
the increase in available training instances should 
improve the algorithm?s accuracy, especially as 
it proved quite accurate to begin with so the al-
gorihm?s predictions should prove quite reliable. 
For this experiment we used the best performing 
algorithm from Section 3, the SMO support-
vector based algorithm. The bootstrapping 
method is applied to this problem as follows: 
 
- Initialisation: use the training set contain-
ing 100 manually annotated instances to 
predict the labels of the first subset of 
1000 unlabeled instances. 
- Iterations: Label the unlabeled instances 
according to the algorithm?s prediction 
and add these instances to the previous 
training set to form a new training set. 
Build a new model based on the new train-
ing set and use it to predict the labels of 
the next subset. 
4.3 Results: bootstrapping 
We now present the results of our experiment 
using normal bootstrapping. After every itera-
tion, the model built by the learner was tested on 
our manually annotated test set.  
 
Iteration Nr. of 
training 
instances 
Correctly / 
incorrectly 
classified 
(%) 
Precision 
(yes/no) 
Recall 
(yes/no)
init 100 95.05 / 4.95 0.957 / 
0.949 
0.846 / 
0.987 
1 1100 94.06 / 5.94 0.955 / 
0.937 
0.808 / 
0.987 
2 2100 94.06 / 5.94 0.955 / 
0.937 
0.808 / 
0.987 
3 3100 94.06 / 5.94 0.955 / 
0.937 
0.808 / 
0.987 
4 4100 94.06 / 5.94 0.955 / 
0 937
0.808 / 
0 987
0.937 0.987 
5 5100 94.06 / 5.94 0.955 / 
0.937 
0.808 / 
0.987 
6 6100 94.06 / 5.94 0.955 / 
0.937 
0.808 / 
0.987 
7 7100 94.06 / 5.94 0.955 / 
0.937 
0.808 / 
0.987 
8 8100 93.07 / 6.93 0.952 / 
0.925 
0.769 / 
0.987 
9 9100 93.07 / 6.93 0.952 / 
0.925 
0.769 / 
0.987 
10 10100 93.07 / 6.93 0.952 / 
0.925 
0.769 / 
0.987 
11 - 42 11100 ? 
42100 
92.08 / 7.92 0.95 / 
0.914 
0.731 / 
0.987 
 
Table 2: Overview of results using normal boot-
strapping. 
 
After 36 iterations, the experiment was halted as 
there was clearly no more gain to be expected 
from any further iterations. Clearly, ordinary 
bootstrapping does not offer any advantages for 
our binary blog classification problem. Also, the 
availability of larger amounts of training in-
stances does nothing to improve results as the 
results are best using only the very small training 
set.  
Generally, both precision and recall slowly 
decrease as the training set grows, showing that 
classifier accuracy as a whole declines. However, 
recall of instances with class label ?no? (non-
blogs) remains constant throughout. Clearly the 
classifier is able to easily detect non-blog pages 
on the basis of the attributes provided, and is 
thwarted only by a small number of outliers. This 
can be explained by the fact that the learner rec-
ognizes non-blogs mostly on the basis of the first 
few attributes having zero values (nrOfPosts, 
minPostLength, maxPostLength etc.). The out-
liers consistently missed by the classifier are 
probably blog-like pages in which date-entry 
combinations have been found but which never-
theless have been manually classified as non-
blogs. Examples of this are calendar pages com-
monly associated with blogs (but which do not 
contain blog content), or MSN Space pages on 
which the user is using the photo album but 
hasn?t started a blog yet. In this case the page is 
recognized as a blog, but contains no blog con-
tent and is therefore manually labeled a non-
blog. 
4.4 Experimental setup: co-training 
As mentioned in Section 2, we will use the pre-
dictions of several of the most successful learn-
ing algorithms from Section 3 as our indicators 
28
in this experiment. The goal of our co-training 
experiment is to take unanimous predictions 
from the three best performing algorithms from 
Section 3, and use those predictions, which we 
assume to have a very high degree of confidence, 
to bootstrap the training set. We will then test to 
see if it offers an improvement over the SMO 
algorithm by itself. By unanimous predictions we 
mean the predictions of those instances, on 
which all the algorithms agree unanimously after 
they have been allowed to predict labels using 
their respective models.  
As instances for which the predictions are 
unanimous can be reasoned to have a very high 
level of confidence, the predictions for those in-
stances are almost certainly correct. Therefore 
we expect this method to offer substantial im-
provements over any single algorithm as it po-
tentially yields a very large number of correctly 
labeled instances for the learner to train on. 
 
Figure 2: Visual representation of our implemen-
tation of the co-training method. 
 
We chose to adapt the co-training idea in this 
fashion as we believe it to be a good way of radi-
cally reducing the fuzziness of potential predic-
tions and a way to gain a very high degree of 
confidence in the labels attached to previously 
unlabeled data. Should the algorithms disagree 
on a large number of instances there would still 
not be a problem as we have a very large pool of 
unlabeled instances (133.000, we only used part 
of our corpus for our experiments as our dataset 
was so large that there was no need to use all the 
data available). The potential maximum of 133 
iterations should prove quite sufficient even if 
the growth of the training set per iteration proves 
to be very small. 
The algorithms we chose for this experiment 
were SMO (support vector), J48 (decision tree, a 
C4.5 implementation) and Jrip (rule based). We 
chose not to use nearest neighbour algorithms for 
this experiment even though they performed well 
individually as we feared it would prove a less 
successful approach given the large training set 
sizes. Indeed, an earlier experiment done during 
our blog classification research showed the per-
formance of near neighbour algorithms bottomed 
out very quickly so no real improvement can be 
expected from those algorithms given larger 
training sets and given the unanimous nature of 
this method of co-training it may spoil any gain 
that might otherwise be achieved. 
The process started with the manually anno-
tated training set and used the predictions from 
the three algorithms, for unlabeled instances they 
agree unanimously on, to label those instances. 
Those instances were subsequently added to the 
trainingset and using this new trainingset, a 
number of the instances in another unlabeled set 
(1000 instances per set) were to be labeled 
(again, only those instances on which the algo-
rithms agree unanimously). Once again, those 
instances are added to the training set and so on 
and so forth for as many iterations as possible. 
4.5 Results: co-training 
We now turn to the results of our experiment 
using our unanimous co-training method de-
scribed above. The experiment was halted after 
30 iterations, as Weka ran out of memory. The 
experiment was not re-run with altered memory 
settings as it was clear that no more gain was to 
be expected by doing so. Again, testing after 
each iteration was performed by building a 
model using the SMO support-vector learning 
algorithm and testing classifier accuracy on the 
manually annotated test set. 
 
Iteration Nr. of 
training 
in-
stances 
Correctly/ 
incorectly 
classified 
(%) 
Precision 
(yes/no) 
Recall 
(yes/no)
init  100 95.05 / 4.95 0.957 / 
0.949 
0.846 / 
0.987 
1 1000 94.06 / 5.94 0.955 / 
0.937 
0.808 / 
0.987 
2 1903 93.07 / 6.93 0.952 / 
0.925 
0.769 / 
0.987 
3 2798 95.05 / 4.95 0.957 / 
0.949 
0.846 / 
0.987 
4 3696 95.05 / 4.95 0.957 / 
0.949 
0.846 / 
0.987 
5 4566 95.05 / 4.95 0.957 / 
0.949 
0.846 / 
0.987 
6 5458 96.04 / 3.96 0.958 / 
0.961 
0.885 / 
0.987 
7 6351 96.04 / 3.96 0.958 / 
0.961 
0.885 / 
0.987 
8 7235 95.05 / 4.95 0.957 / 
0.949 
0.846 / 
0.987 
9 8149 95.05 / 4.95 0.957 / 
0.949 
0.846 / 
0.987 
29
10 9041 95.05 / 4.95 0.957 / 
0.949 
0.846 / 
0.987 
11 9929 95.05 / 4.95 0.957 / 
0.949 
0.846 / 
0.987 
12 10810 95.05 / 4.95 0.957 / 
0.949 
0.846 / 
0.987 
13 - 43 11684 - 
38510 
94.06 / 5.94 0.955 / 
0.937 
0.808 / 
0.987 
 
Table 3: Overview of results using our unani-
mous co-training method. 
 
Even though the ?steps? in test percentages 
shown represent only one more blog being clas-
sified correctly (or incorrectly), the classifier 
does perform better than it did using only the 
manually annotated training set at some stages of 
the experiment. This means that gains in classi-
fier accuracy can be achieved by using this 
method of co-training on this problem. Also the 
classifier generally performs better than in our 
bootstrapping experiment, which shows that the 
instances unanimously agreed on by all three 
algorithms are certainly more reliable than the 
predictions of even the best algorithm by itself, 
as predicted. 
Clearly this method offers potential for an im-
provement even though the SMO algorithm was 
already very accurate in our first binary blog 
classification experiment. 
5 Discussion 
As the title suggests, these experiments are of a 
preliminary and exploratory nature. The high 
accuracy achieved by almost all algorithms in 
our binary classification experiment show that 
our attribute set clearly defines the subject well. 
However, these results must be viewed with an 
air of caution as they were obtained using a small 
subset and as such the data may not represent the 
nature of the complete dataset well. Indeed, how 
stable are the results obtained? 
Later experiments using a (disjoin, but) larger 
manually annotated dataset containing 700 in-
stances show that the results obtained here are 
optimistic. The extremely diverse nature of the 
blogosphere means that describing an entire 
dataset using a relatively small subset is very 
difficult and as such both the performance and 
ranking of off-the-shelf machine learning algo-
rithms will vary among different datasets. Off-
the-shelf algorithms do however still perform far 
better than our baseline and the best performing 
algorithms still achieve accuracy rates in excess 
of 90%. 
Two aspects of our attribute set that need to be 
worked on in future are date detection and con-
tent checks. Outliers are almost always caused by 
the date detection algorithm not detecting certain 
date formats, and pages containing date-entry 
combinations but no real blog content. Therefore, 
although it is possible to perform binary blog 
classification based purely on the particular char-
acteristics of blog pages with high accuracy, con-
tent checks are invaluable. The rise of blogspam, 
which cannot be separated from real blogs on the 
basis of page characteristics at all, further em-
phasises this. We have already developed a 
document frequency profile and replaced the 
contentType attribute used in these experiments, 
to extend the content-based attributes in our 
dataset and hopefully improve blog recognition. 
6 Conclusion 
Our experiments have shown that binary blog 
classification can be performed successfully if 
the right attributes are chosen to describe the 
data, even if the classifier is forced to rely on a 
small number of training instances. Almost all 
basic off-the-shelf machine learning algorithms 
perform well given this task, but support vector 
based algorithms performed best in this experi-
ment. Notable was that the best algorithms of 
each type achieved almost the same accuracy, all 
over 90% and the difference is never larger than 
a few percent even though they approach the 
problem in completely different manners. 
The performance of these algorithms can be 
improved by using resampling methods, but not 
all resampling methods achieve gains and those 
that do gain very little. The extremely high suc-
cess rates of the plain algorithms means that 
there is very little room for improvement, espe-
cially as the classification errors are almost al-
ways caused by outliers that none of the algo-
rithms manage to classify correctly. 
The results of later experiments with larger 
numbers of manually annotated instances show 
that a lot of work remains to be done and that 
although this paper shows that the application of 
machine learning to this problem offers substan-
tial improvements over our baseline, this prob-
lem is still far from solved. 
Future work will include further analysis of 
the results obtained using larger manually anno-
tated subsets as well as a detailed analysis of the 
contributions of the different features in the fea-
ture set described in Section 3. 
30
Acknowledgements 
The authors wish to thank Gilad Mishne for his 
input and valuable comments during these ex-
periments and the writing of this piece. 
Maarten de Rijke was supported by grants 
from the Netherlands Organization for Scientific 
Research (NWO) under project numbers 
017.001.190, 220-80-001, 264-70-050,  354-20-
005, 612-13-001, 612.000.106, 612.000.207, 
612.066.302, 612.069.006, 640.001.501, and 
640.002.501. 
References 
G. Mishne, M. de Rijke. 2006. Capturing Global 
Mood Levels using Blog Posts, In: AAAI 2006 
Spring Symposium on Computational Approaches 
to Analysing Weblogs (AAAI-CAAW 2006) 
E. Adar, L. Zhang, L. Adamic, R. Lukose. 2004. Im-
plicit Structure and the Dynamics of Blogspace, In: 
Workshop on the Weblogging Ecosystem, WWW 
Conference, 2004 
T. Nanno, Y. Suzuki, T. Fujiki, M. Okumura. 2004. 
Automatic Collection and Monitoring of Japanese 
Weblogs, In: Proceeding of WWW2004: the 13th 
international World Wide Web conference, New 
York, NY, USA, 2004. ACM Press. 
N. Glance, M. Hurst, T. Tomokiyo. 2004. BlogPulse: 
Automated Trend Discovery for Weblogs, In: Pro-
ceeding of WWW2004: the 13th international World 
Wide Web conference, New York, NY, USA, 2004. 
ACM Press. 
R. Jones, A. McCallum, K. Nigam, and E. Riloff. 
1999. Bootstrapping for Text Learning Tasks, In: 
IJCAI-99 Workshop on Text Mining: Foundations, 
Techniques and Applications, p52-63. 
A. Blum, T. Mitchell. 1998. Combining Labeled and 
Unlabeled Data with Co-Training, In: Proceedings 
of the 1998 Conference on Computational Learn-
ing Theory. 
I. Witten, E. Frank. 2005. Data Mining: Practical 
machine learning tools and techniques, 2nd Edi-
tion, Morgan Kaufmann, San Francisco, 2005. 
 
 
 
 
 
Appendix A. Full results of our binary 
blog classification experiment 
Algorithm Type Percentage 
correct predic-
tions 
Na?ve Bayes Bayes 90.07 
Na?ve Bayes Simple Bayes 89.64 
SMO Support 
Vector 
94.75 
IB1 Instance 
based 
93.00 
KStar Instance 
based 
93.30 
LWL Instance 
based 
91.25 
BayesNet Bayes 90.08 
DecisionStump Tree 91.25 
J48 Tree 93.29 
ZeroR Rule-based 73.00 
DecisionTable Rule-based 92.55 
OneR Rule-based 87.60 
ConjunctiveRule Rule-based 88.75 
NNGe Rule-based 93.73 
PART Rule-based 91.67 
Ridor Rule-based 91.26 
JRip Rule-based 93.73 
 
31
Finding Similar Sentences across Multiple Languages in Wikipedia
Sisay Fissaha Adafre Maarten de Rijke
ISLA, University of Amsterdam
Kruislaan 403, 1098 SJ Amsterdam
sfissaha,mdr@science.uva.nl
Abstract
We investigate whether the Wikipedia cor-
pus is amenable to multilingual analysis
that aims at generating parallel corpora.
We present the results of the application of
two simple heuristics for the identification
of similar text across multiple languages
in Wikipedia. Despite the simplicity of the
methods, evaluation carried out on a sam-
ple of Wikipedia pages shows encouraging
results.
1 Introduction
Parallel corpora form the basis of much multilin-
gual research in natural language processing, rang-
ing from developing multilingual lexicons to sta-
tistical machine translation systems. As a conse-
quence, collecting and aligning text corpora writ-
ten in different languages constitutes an important
prerequisite for these research activities.
Wikipedia is a multilingual free online encyclo-
pedia. Currently, it has entries for more than 200
languages, the English Wikipedia being the largest
one with 895,674 articles, and no fewer than eight
language versions having upwards of 100,000 ar-
ticles as of January 2006. As can be seen in Fig-
ure 1, Wikipedia pages for major European lan-
guages have reached a level where they can sup-
port multilingual research. Despite these devel-
opments in its content, research on Wikipedia has
largely focused on monolingual aspects so far; see
e.g., (Voss, 2005) for an overview.
In this paper, we focus on multilingual aspects
of Wikipedia. Particularly, we investigate to what
extent we can use properties of Wikipedia itself
to generate similar sentences acrose different lan-
guages. As usual, we consider two sentences sim-
ilar if they contain (some or a large amount of)
overlapping information. This includes cases in
which sentences may be exact translations of each
other, one sentence may be contained within an-
other, or both share some bits of information.
en de fr ja pl it sv nl pt es zh ru no fi da
0
100000
200000
300000
400000
500000
600000
700000
800000
900000
1000000
Figure 1: Wikipedia pages for the top 15 lan-
guages
The conceptually simple but fundamental task
of identifying similar sentences across multiple
languages has a number of motivations. For a
start, and as mentioned earlier, sentence aligned
corpora play an important role in corpus based lan-
guage processing methods in general. Second, in
the context of Wikipedia, being able to align sim-
ilar sentences across multiple languages provides
insight into Wikipedia as a knowledge source: to
which extent does a given topic get different kinds
of attention in different languages? And thirdly,
the ability to find similar content in other lan-
guages while creating a page for a topic in one lan-
guage constitutes a useful type of editing support.
Furthermore, finding similar content acrose differ-
ent languages can form the basis for multilingual
summarization and question answering support for
62
Wikipedia; at present the latter task is being devel-
oped into a pilot for CLEF 2006 (WiQA, 2006).
There are different approaches for finding sim-
ilar sentences across multiple languages in non-
parallel but comparable corpora. Most methods
for finding similar sentences assume the availabil-
ity of a clean parallel corpus. In Wikipedia, two
versions of a Wikipedia topic in two different lan-
guages are a good starting point for searching sim-
ilar sentences. However, these pages may not al-
ways conform to the typical definitions of a bitext
which current techniques assume. Bitext gener-
ally refers to two versions of a text in two differ-
ent languages (Melamed, 1996). Though it is not
known how information is shared among the dif-
ferent languages in Wikipedia, some pages tend to
be translations of each other whereas the majority
of the pages tend to be written independently of
each other. Therefore, two versions of the same
topic in two different languages can not simply be
taken as parallel corpora. This in turn limits the
application of some of the currently available tech-
niques.
In this paper, we present two approaches for
finding similar sentences across multiple lan-
guages in Wikipedia. The first approach uses
freely available online machine translation re-
sources for translating pages and then carries out
monolingual sentence similarity. The approach
needs a translation system, and these are not avail-
able for every pair of languages in Wikipedia.
This motivates a second approach to finding
similar sentences across multiple languages, one
which uses a bilingual title translation lexicon in-
duced automatically using the link structure of
Wikipedia. Briefly, two sentences are similar if
they link to the same entities (or rather: to pages
about the same entities), and we use Wikipedia it-
self to relate pages about a given entity across mul-
tiple languages. In Wikipedia, pages on the same
topic in different languages are topically closely
related. This means that even if one page is not
a translation of another, they tend to share some
common information. Our underlying assumption
here is that there is a general agreement on the
kind of information that needs to be included in the
pages of different types of topics such as a biogra-
phy of a person, and the definition and description
of a concept etc., and that this agreement is to a
consderable extent ?materialized? in the hypertext
links (and their anchor texts) in Wikipedia.
Our main research question in this paper is this:
how do the two methods just outlined differ? A
priori it seems that the translation based approach
to finding similar sentences across multiple lan-
guages will have a higher recall than the link-
based method, while the latter outperforms the for-
mer in terms of precision. Is this correct?
The remainder of the paper is organized as fol-
lows. In Section 2, we briefly discuss related
work. Section 3 provides a detailed description
of Wikipedia as a corpus. The two approaches to
identifying similar sentences across multiple lan-
guages are presented in Section 4. An experimen-
tal evaluation is presented in Section 5. We con-
clude in Section 6.
2 Related Work
The main focus of this paper lies with multilin-
gual text similarity and its application to infor-
mation access in the context of Wikipedia. Cur-
rent research work related to Wikipedia mostly
describes its monolingual properties (Ciffolilli,
2003; Vie?gas et al, 2004; Lih, 2004; Miller,
2005; Bellomi and Bonato, 2005; Voss, 2005; Fis-
saha Adafre and de Rijke, 2005). This is proba-
bly due to the fact that different language versions
of Wikipedia have different growth rates. Others
describe its application in question answering and
other types of IR systems (Ahn et al, 2005). We
believe that currently, Wikipedia pages for major
European languages have reached a level where
they can support multilingual research.
On the other hand, there is a rich body of knowl-
edge relating to multilingual text similarity. These
include example-based machine translation, cross-
lingual information retrieval, statistical machine
translation, sentence alignment cost functions, and
bilingual phrase translation (Kirk Evans, 2005).
Each approach uses relatively different features
(content and structural features) in identifying
similar text from bilingual corpora. Furthermore,
most methods assume that the bilingual corpora
can be sentence aligned. This assumption does
not hold for our case since our corpus is not par-
allel. In this paper, we use content based fea-
tures for identifying similar text across multilin-
gual corpora. Particularly, we compare bilingual
lexicon and MT system based methods for identi-
fying similar text in Wikipedia.
63
3 Wikipedia as a Multilingual Corpus
Wikipedia is a free online encyclopedia which is
administered by the non-profit Wikimedia Foun-
dation. The aim of the project is to develop free
encyclopedias for different languages. It is a col-
laborative effort of a community of volunteers, and
its content can be edited by anyone. It is attracting
increasing attention amongst web users and has
joined the top 50 most popular sites.
As of January 1, 2006, there are versions of
Wikipedia in more than 200 languages, with sizes
ranging from 1 to over 800,000 articles. We used
the ascii text version of the English and Dutch
Wikipedia, which are available as database dumps.
Each entry of the encyclopedia (a page in the on-
line version) corresponds to a single line in the text
file. Each line consists of an ID (usually the name
of the entity) followed by its description. The de-
scription part contains the body of the text that de-
scribes the entity. It contains a mixture of plain
text and text with html tags. References to other
Wikipedia pages in the text are marked using ?[[?
?]]? which corresponds to a hyperlink on the on-
line version of Wikipedia. Most of the formatting
information which is not relevant for the current
task has been removed.
3.1 Links within a single language
Wikipedia is a hypertext document with a rich link
structure. A description of an entity usually con-
tains hypertext links to other pages within or out-
side Wikipedia. The majority of these links cor-
respond to entities, which are related to the en-
tity being described, and have a separate entry
in Wikipedia. These links are used to guide the
reader to a more detailed description of the con-
cept denoted by the anchor text. In other words,
the links in Wikipedia typically indicate a topical
association between the pages, or rather the enti-
ties being described by the pages. E.g., in describ-
ing a particular person, reference will be made to
such entities as country, organization and other im-
portant entities which are related to it and which
themselves have entries in Wikipedia. In general,
due to the peculiar characteristics of an encyclope-
dia corpus, the hyperlinks found in encyclopedia
text are used to exemplify those instances of hy-
perlinks that exist among topically related entities
(Ghani et al, 2001; Rao and Turoff, 1990).
Each Wikipedia page is identified with a unique
ID. These IDs are formed by concatenating the
words of the titles of the Wikipedia pages which
are unique for each page, e.g., the page on Vin-
cent van Gogh has ?Vincent van Gogh? as its ti-
tle and ?Vincent van Gogh? as its ID. Each page
may, however, be represented by different anchor
texts in a hyperlink. The anchor texts may be sim-
ple morphological variants of the title such as plu-
ral form or may represent closely related seman-
tic concept. For example, the anchor text ?Dutch?
may point to the page for the Netherlands. In a
sense, the IDs function as the canonical form for
several related concepts.
3.2 Links across different languages
Different versions of a page in different languages
are also hyperlinked. For a given page, transla-
tions of its title in other languages for which pages
exist are given as hyperlinks. This property is par-
ticularly useful for the current task as it helps us to
align the corpus at the page level. Furthermore, it
also allows us to induce bilingual lexicon consist-
ing of the Wikipedia titles. Conceptual mismatch
between the pages (e.g. Roof vs Dakconstructie)
is rare, and the lexicon is generally of high qual-
ity. Unlike the general lexicon, this lexicon con-
tains a relatively large number of names of indi-
viduals and other entities which are highly infor-
mative and hence are useful in identifying similar
text. This lexicon will form the backbone of one
of the methods for identifying similar text across
different languages, as will be shown in Section 4.
4 Approaches
We describe two approaches for identifying simi-
lar sentences across different languages. The first
uses an MT system to obtain a rough translation of
a given page in one language into another and then
uses word overlap between sentences as a similar-
ity measure. One advantage of this method is that
it relies on a large lexical resource which is bigger
than what can be extracted from Wikipedia. How-
ever, the translation can be less accurate especially
for the Wikipedia titles which form part of the con-
tent of a page and are very informative.
The second approach relies on a bilingual lexi-
con which is generated from Wikipedia using the
link structure: pages on the same topic in differ-
ent languages are hyperlinked; see Figure 2. We
use the titles of the pages that are linked in this
manner to create a bilingual lexicon. Thus, our
bilingual lexicon consists of terms that represent
64
concepts or entities that have entries in Wikipedia,
and we will represent sentences by entries from
this lexicon: an entry is used to represent the con-
tent of a sentence if the sentence contains a hy-
pertext link to the Wikipedia page for that entry.
Sentence similarity is then captured in terms of the
shared lexicon entries they share. In other words,
the similarity measure that we use in this approach
is based on ?concept? or ?page title? overlap. In-
tuitively, this approach has the advantage of pro-
ducing a brief but highly accurate representation
of sentences, more accurate, we assume than the
MT approach as the titles carry important seman-
tic information; it will also be more accurate than
the MT approach because the translations of the
titles are done manually.
Figure 2: Links to pages devoted to the same topic
in other languages.
Both approaches assume that the Wikipedia cor-
pus is aligned at the page level. This is eas-
ily achieved using the link structure since, again,
pages on the same topic in different languages are
hyperlinked. This, in turns, narrows down the
search for similar text to a page level. Hence, for
a given text of a page (sentence or chunk) in one
language, we search for its equivalent text (sen-
tence or chunk) only in the corresponding page in
the other language, not in the entire corpus.
We now describe the two approaches in more
detail. To remain focused and avoid getting lost
in technical details, we consider only two lan-
guages in our technical descriptions and evalua-
tions below: Dutch and English; it will be clear
from our presentation, however, that our second
approach can be used for any pair of languages in
Wikipedia.
4.1 An MT based approach
In this approach, we translate the Dutch Wikipedia
page into English using an online MT system. We
refer to the English page as source and the trans-
lated (Dutch page) version as target. We used the
Babelfish MT system of Altavista. It supports a
number of language pairs among which are Dutch-
English pairs. After both pages have been made
available in English, we split the pages into sen-
tences or text chucks. We then link each text chunk
or sentence in the source to each chuck or sentence
in the target. Following this we compute a simple
word overlap score for each pair. We used the Jac-
card similarity measure for this purpose. Content
words are our main features for the computation
of similarity, hence, we remove stopwords. Gram-
matically correct translations may not be neces-
sary since we are using simple word overlap as our
similarity measure.
The above procedure will generate a large set
of pairs, not all of which will actually be similar.
Therefore, we filter the list assuming a one-to-one
correspondence, where for each source sentence
we identify at most one target sentence. This is
a rather strict criterion (another possibility being
one-to-many), given the fact that the corpus is gen-
erally assumed to be not parallel. But it gives some
idea on how much of the text corpus can be aligned
at smaller units (i.e., sentence or text chunks).
Filtering works as follows. First we sort the
pairs in decreasing order of their similarity scores.
This results in a ranked list of text pairs in which
the most similar pairs are ranked top whereas the
least similar pairs are ranked bottom. Next we take
the top most ranking pair. Since we are assuming
a one-to-one correspondence, we remove all other
pairs ranked lower in the list containing either of
the the sentences or text chunks in the top ranking
pair. We then repeat this process taking the second
top ranking pair. Each step results in a smaller list.
The process continues until there is no more pair
to remove.
4.2 Using a link-based bilingual lexicon
As mentioned previously, this approach makes
use of a bilingual lexicon that is generated from
Wikipedia using the link structure. A high level
description of the algorithm is given in Figure 3.
Below, we first describe how the bilingual lexicon
is acquired and how it is used for enriching the link
structure of Wikipedia. Finally, we detail how the
65
? Generating bilingual lexicon
? Given a topic, get the corresponding pages
from English and Dutch Wikipedia
? Split pages into sentences and enrich the
hyperlinks in the sentence or identify
named-entities in the pages.
? Represent the sentences in these pages us-
ing the bilingual lexicon.
? Compute term overlap between the sen-
tences thus represented.
Figure 3: The Pseudo-algorithm for identifying
similar sentences using a link-based bilingual lex-
icon.
bilingual lexicon is used for the identification of
similar sentences.
Generating the bilingual lexicon
Unlike the MT based approach, which uses con-
tent words from the general vocabulary as fea-
tures, in this approach, we use page titles and their
translations (as obtained through hyperlinks as ex-
plained above) as our primitives for the compu-
tation of multilingual similarity. The first step of
this approach, then, is acquiring the bilingual lexi-
con, but this is relatively straightforward. For each
Wikipedia page in one language, translations of
the title in other languages, for which there are
separate entries, are given as hyperlinks. This in-
formation is used to generate a bilingual transla-
tion lexicon. Most of these titles are content bear-
ing noun phrases and are very useful in multilin-
gual similarity computation (Kirk Evans, 2005).
Most of these noun phrases are already disam-
buiguated, and may consist of either a single word
or multiword units.
Wikipedia uses a redirection facility to map
several titles into a canonical form. These titles
are mostly synonymous expressions. We used
Wikipedia?s redirect feature to identify synony-
mous expression.
Canonical representation of a sentence
Once we have the bilingual lexicon, the next step
is to represent the sentences in both language pairs
using this lexicon. Each sentence is represented by
the set of hyperlinks it contains. We search each
hyperlink in the bilingual lexicon. If it is found,
we replace the hyperlink with the corresponding
unique identification of the bilingual lexicon entry.
If it is not found, the hyperlink will be included as
is as part of the representation. This is done since
Dutch and English are closely related languages
and may share many cognate pairs.
Enriching the Wikipedia link structure
As described in the previous section, the method
uses hyperlinks in a sentence as a highly focused
entity-based representation of the aboutness of the
sentence. In Wikipedia, not all occurrences of
named-entities or concepts that have entries in
Wikipedia are actually used as anchor text of a
hypertext link; because of this, a number of sen-
tences may needlessly be left out from the simi-
larity computation process. In order to avoid this
problem, we automatically identify other relevant
hyperlinks using the bilingual lexicon generated in
the previous section.
Identification of additional hyperlinks in
Wikipedia sentences works as follows. First
we split the sentences into constituent words.
We then generate N gram words keeping the
relative order of words in the sentences. Since the
anchor texts of hypertext links may be multiword
expressions, we start with higher order N gram
words (N=4). We search these N grams in the
bilingual lexicon. If the N gram is found in the
lexicon, it is taken as a new hyperlink and will
form part of the representation of a sentence. The
process is repeated for lower order N grams.
Identifying similar sentences
Once we are done representing the sentences as
described previously, the final step involves com-
putation of the term overlap between the sentence
pairs and filtering the resulting list. The remain-
ing steps are similar to those described in the MT
based approach. For completeness, we briefly re-
peat the steps here. First, all sentences from a
Dutch Wikipedia page are linked to all sentences
of the corresponding English Wikipedia page. We
then compute the similarity between the sentence
representations, using the Jaccard similarity coef-
ficient.
A sentence in Dutch page may be similar to
several sentences in English page which may re-
sult in a large number of spurious pairs. There-
fore, we filter the list using the following recursive
procedure. First, the sentence pairs are sorted by
their similarity scores. We take the pairs with the
highest similarity scores. We then eliminate all
66
other sentence pairs from the list that contain ei-
ther of sentences in this pair. We continue this pro-
cess taking the second highest ranking pair. Note
that this procedure assumes a one-to-one matching
rule; a sentences in Dutch can be linked to at most
one sentence in English.
5 Experimental Evaluation
Now that we have described the two algorithms
for identifying similar sentences, we return to our
research questions. In order to answer them we
run the experiment described below.
5.1 Set-up
We took a random sample of 30 English-Dutch
Wikipedia page pairs. Each page is split into sen-
tences. We generated candidate Dutch-English
sentence pairs and passed them on to the two
methods. Both methods return a ranked list of sen-
tence pairs that are similar. As explained above,
we assumed a one-to-one correspondence, i.e., one
English sentence can be linked to at most to one
Dutch sentence.
The outputs of the systems are manually evalu-
ated. We apply a relatively lenient criteria in as-
sessing the results. If two sentences overlap in-
terms of their information content then we con-
sider them to be similar. This includes cases in
which sentences may be exact translation of each
other, one sentence may be contained within an-
other, or both share some bits of information.
5.2 Results
Table 1 shows the results of the two methods de-
scribed in Section 4. In the table, we give two
types of numbers for each of the two methods
MT and Bilingual lexicon: Total (the total number
of sentence pairs) and Match (the number of cor-
rectly identified sentence pairs) generated by the
two approaches.
Overall, the two approaches tend to produce
similar numbers of correctly identified similar sen-
tence pairs. The systems seem to perform well
on pages which tend to be alignable at sentence
level, i.e., parallel. This is clearly seen on the
following pages: Pierluigi Collina, Marcus Cor-
nelius Fronto, George F. Kennan, which show a
high similarity at sentence level. Some pages con-
tain very small description and hence the figures
for correct similar sentences are also small. Other
topics such as Classicism (Dutch: Classicisme),
Tennis, and Tank, though they are described in suf-
ficient details in both languages, there tends to be
less overlap among the text. The methods tend to
retrieve more accurate similar pairs from person
pages than other pages especially those pages de-
scribing a more abstract concepts. However, this
needs to be tested more thoroughly.
When we look at the total number of sentence
pairs returned, we notice that the bilingual lexi-
con based method consistently returns a smaller
amount of similar sentence pairs which makes
the method more accurate than the MT based ap-
proach. On average, the MT based approach re-
turns 4.5 (26%) correct sentences and the bilingual
lexicon based approach returns 2.9 correct sen-
tences (45%). But, on average, the MT approach
returns three times as many sentence pairs as bilin-
gual lexicon approach. This may be due to the fact
that the former makes use of restricted set of im-
portant terms or concepts whereas the later uses a
large general lexicon. Though we remove some
of the most frequently occuring stopwords in the
MT based approach, it still generates a large num-
ber of incorrect similar sentence pairs due to some
common words.
In general, the number of correctly identified
similar pages extracted seems small. However,
most of the Dutch pages are relatively small,
which sets the upper bound on the number of
correctly identified sentence pairs that can be ex-
tracted. On average, each Dutch Wikipedia page
in the sample contains 18 sentences whereas En-
glish Wikipedia pages contain 65 sentences. Ex-
cluding the pages for Tennis, Tank (Dutch: vo-
ertuig), and Tricolor, which are relatively large,
each Dutch page contains on average 8 sentences,
which is even smaller. Given the fact that the
pages are in general not parallel, the methods,
using simple heuristics, identified high quality
translation equivalent sentence pairs from most
Wikipedia pages. Furthermore, a close examina-
tion of the output of the two approaches show that
both tend to identify the same set of similar sen-
tence pairs.
We ran our bilingual lexicon based approach on
the whole Dutch-English Wikipedia corpus. The
method returned about 80M of candidate similar
sentences. Though we do not have the resources
to evaluate this output, the results we got from
sample data (cf. Table 1) suggest that it contains
a significant amount of correctly identified similar
67
Title MT Bilingual Lexicon
English Dutch Total Match Total Match
Hersfeld Rotenburg Hersfeld Rotenburg 2 3 2
Manganese nodule Mangaanknol 5 2 1 1
Kettle Ketel 1 1
Treason Landverraad 2 1
Pierluigi Collina Pierluigi Collina 14 13 13 11
Province of Ferrara Ferrara (provincie) 7 1 1 1
Classicism Classicisme 8 1
Tennis Tennis 93 4 15 3
Hysteria Hysterie 14 6 9 5
George F. Kennan George Kennan 27 12 29 11
Marcus Cornelius Fronto Marcus Cornelius Fronto 11 9 5 5
Delphi Delphi (Griekenland) 34 2 8 1
De Beers De Beers 11 5 10 5
Pavel Popovich Pavel Popovytsj 7 4 4 4
Rice pudding Rijstebrij 11 1 4
Manta ray Reuzenmanta 15 3 7 2
Michelstadt Michelstadt 1 1 1 1
Tank Tank (voertuig) 84 3 27 2
Cheyenne(Wyoming) Cheyenne(Wyoming) 5 2 2 2
Goa Goa(deelstaat) 13 4 6 1
Tricolour Driekleur 57 36 13 12
Oral cancer Mondkanker 25 2 7 2
Pallium Pallium 12 2 5 4
Ajanta Ajanta 3 3 2 2
Captain Jack (band) Captain Jack 16 3 2 2
Proboscis Monkey Neusaap 15 6 4 1
Patti Smith Patti Smith 6 2 4 2
Flores Island, Portugal Flores (Azoren) 3 2 1 1
Mercury 8 Mercury MA 8 11 3 4 1
Mutation Mutatie 16 4 6 3
Average 17.6 4.5 6.5 2.9
Table 1: Test topics (column 1 and 2). The total number of sentence pairs (column 3) and the number
of correctly identified similar sentence pairs (column 4) returned by the MT based approach. The to-
tal number of sentence pairs (column 5) and the number of correctly identified similar sentence pairs
(column 6) returned by the method using a bilingual lexicon.
sentences.
6 Conclusion
In this paper we focused on multilingual aspects of
Wikipedia. Particularly, we investigated the poten-
tial of Wikipedia for generating parallel corpora by
applying different methods for identifying similar
text across multiple languages. We presented two
methods and carried out an evaluation on a sam-
ple of Dutch-English Wikipedia pages. The results
show that both methods, using simple heuristics,
were able to identify similar text between the pair
of Wikipedia pages though they differ in accuracy.
The bilingual lexicon approach returns fewer in-
correct pairs than the MT based approach. We
interpret this as saying that our bilingual lexicon
based method provides a more accurate represen-
tation of the aboutness of sentences in Wikipedia
than the MT based approach. Furthermore, the re-
sult we obtained on a sample of Wikipedia pages
and the output of running the bilingual based ap-
proach on the whole Dutch-English gives some in-
dication of the potential of Wikipedia for generat-
ing parallel corpora.
68
As to future work, the sentence similarity de-
tection methods that we considered are not perfect.
E.g., the MT based approach relies on rough trans-
lations; it is important to investigate the contri-
bution of high quality translations. The bilingual
lexicon approach uses only lexical features; other
language specific sentence features might help im-
prove results.
Acknowledgments
This research was supported by the Nether-
lands Organization for Scientific Research (NWO)
under project numbers 017.001.190, 220-80-
001, 264-70-050, 612-13-001, 612.000.106,
612.000.207, 612.066.302, 612.069.006, 640.-
001.501, and 640.002.501.
References
D. Ahn, V. Jijkoun, G. Mishne, K. Mu?ller, M. de Rijke,
and S. Schlobach. 2005. Using Wikipedia at the
TREC QA Track. In E.M. Voorhees and L.P. Buck-
land, editors, The Thirteenth Text Retrieval Confer-
ence (TREC 2004).
F. Bellomi and R. Bonato. 2005. Lex-
ical authorities in an encyclopedic cor-
pus: a case study with wikipedia. URL:
http://www.fran.it/blog/2005/01/
lexical-authorities-in-encyclopedic.
htm%l. Site accessed on June 9, 2005.
A. Ciffolilli. 2003. Phantom authority, selfselective re-
cruitment and retention of members in virtual com-
munities: The case of Wikipedia. First Monday,
8(12).
S. Fissaha Adafre and M. de Rijke. 2005. Discovering
missing links in Wikipedia. In Proceedings of the
Workshop on Link Discovery: Issues, Approaches
and Applications (LinkKDD-2005).
R. Ghani, S. Slattery, and Y. Yang. 2001. Hypertext
categorization using hyperlink patterns and meta
data. In Carla Brodley and Andrea Danyluk, ed-
itors, Proceedings of ICML-01, 18th International
Conference on Machine Learning, pages 178?185.
D. Kirk Evans. 2005. Identifying similarity
in text: Multi-lingual analysis for summariza-
tion. URL: http://www1.cs.columbia.
edu/nlp/theses/dave_evans.pdf. Site
accessed on January 5, 2006.
A. Lih. 2004. Wikipedia as participatory journalism:
Reliable sources? Metrics for evaluating collabora-
tive media as a news resource. In Proceedings of the
5th International Symposium on Online Journalism.
D. Melamed. 1996. A geometric approach to mapping
bitext correspondence. In Eric Brill and Kenneth
Church, editors, Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, pages 1?12, Somerset, New Jersey. Association
for Computational Linguistics.
N. Miller. 2005. Wikipedia and the disappearing
?Author?. ETC: A Review of General Semantics,
62(1):37?40.
U. Rao and M. Turoff. 1990. Hypertext functionality:
A theoretical framework. International Journal of
Human-Computer Interaction.
F. Vie?gas, M. Wattenberg, and D. Kushal. 2004.
Studying cooperation and conflict between authors
with history flow visualization. In Proceedings of
the 2004 conference on Human factors in comput-
ing systems.
J. Voss. 2005. Measuring Wikipedia. In Proceedings
10th International Conference of the International
Society for Scientometrics and Informetrics.
WiQA. 2006. Question answering using Wikipedia.
URL: http://ilps.science.uva.nl/
WiQA/. Site accessed on January 5, 2006.
69
TextGraphs-2: Graph-Based Algorithms for Natural Language Processing, pages 53?60,
Rochester, April 2007 c?2007 Association for Computational Linguistics
Learning to Transform Linguistic Graphs
Valentin Jijkoun and Maarten de Rijke
ISLA, University of Amsterdam
Kruislaan 403, 1098 SJ Amsterdam, The Netherlands
jijkoun,mdr@science.uva.nl
Abstract
We argue in favor of the the use of la-
beled directed graph to represent various
types of linguistic structures, and illustrate
how this allows one to view NLP tasks as
graph transformations. We present a gen-
eral method for learning such transforma-
tions from an annotated corpus and de-
scribe experiments with two applications
of the method: identification of non-local
depenencies (using Penn Treebank data)
and semantic role labeling (using Propo-
sition Bank data).
1 Introduction
Availability of linguistically annotated corpora such
as the Penn Treebank (Bies et al, 1995), Proposition
Bank (Palmer et al, 2005), and FrameNet (John-
son et al, 2003) has stimulated much research on
methods for automatic syntactic and semantic anal-
ysis of text. Rich annotations of corpora has al-
lowed for the development of techniques for recov-
ering deep linguistic structures: syntactic non-local
dependencies (Johnson, 2002; Hockenmaier, 2003;
Dienes, 2004; Jijkoun and de Rijke, 2004) and se-
mantic arguments (Gildea, 2001; Pradhan et al,
2005; Toutanova et al, 2005; Giuglea andMoschitti,
2006). Most state-of-the-art methods for the latter
two tasks use a cascaded architecture: they employ
syntactic parsers and re-cast the corresponding tasks
as pattern matching (Johnson, 2002) or classifica-
tion (Pradhan et al, 2005) problems. Other meth-
ods (Jijkoun and de Rijke, 2004) use combinations
of pattern matching and classification.
The method presented in this paper belongs to
the latter category. Specifically, we propose (1) to
use a flexible and expressive graph-based represen-
tation of linguistic structures at different levels; and
(2) to view NLP tasks as graph transformation prob-
lems: namely, problems of transforming graphs of
one type into graphs of another type. An exam-
ple of such a transformation is adding a level of
the predicate argument structure or semantic argu-
ments to syntactically annotated sentences. Further-
more, we describe a general method to automati-
cally learn such transformations from annotated cor-
pora. Our method combines pattern matching on
graphs and machine learning (classification) and can
be viewed as an extension of the Transformation-
Based Learning paradigm (Brill, 1995). After de-
scribing the method for learning graph transforma-
tions we demonstrate its applicability on two tasks:
identification of non-local dependencies (using Penn
Treebank data) and semantic roles labeling (using
Proposition Bank data).
The paper is organized as follows. In Section 2
we give our motivations for using graphs to encode
linguistic data. In Section 3 we describe our method
for learning graph transformations and in Section 4
we report on experiments with applications of our
method. We conclude in Section 5.
2 Graphs for linguistic structures and
language processing tasks
Trees and graphs are natural and common ways of
encoding linguistic information, in particular, syn-
53
VP
to seek NP
seats
VP
planned
S
directors S
NP?SBJthis month
NP?TMP
*
NP?SBJ
Figure 1: Local and non-local syntantic relations.
VP
using NP
S
stoppedLorillard Inc
in
cigarette filters
NP in NP
1956
ARG0 head ARG1 ARGM
feature=TMP
pred
crocidolite
S
PP PP
NP VP
Figure 2: Syntactic structure and semantic roles.
tactic structures (phrase trees, dependency struc-
tures). In this paper we use node- and edge-labeled
directed graphs as our representational formalism.
Figures 1 and 2 give informal examples of such rep-
resentations.
Figure 1 shows a graph encoding of the Penn
Treebank annotation of the local (solid edges) and
non-local (dashed edges) syntantic structure of the
sentence directors this month planned to seek more
seats. In this example, the co-indexing-based im-
plicit annotation of the non-local dependency (sub-
ject control) in the Penn Treebank (Bies et al, 1995)
is made explicit in the graph-based encoding.
Figure 2 shows a graph encoding of linguistic
structures for the sentence Lorillard Inc stopped us-
ing crocodolite in sigarette filters in 1956. Here,
solid lines correspond to surface syntactic structure,
produced by Charniak?s parser (Charniak, 2000),
and dashed lines are an encoding of the Proposition
Bank annotation of the semantic roles with respect
to the verb stopped.
Graph-based representations allow for a uniform
view on the linguistic structures on different layers.
An advantage of such a uniform view is that ap-
parently different NLP tasks can be considered as
VP
to seek NP
seats
VP
planned
S
directors S
this month
NP
NP
Figure 3: Output of a syntactic parser.
manipulations with graphs, in other words, as graph
transformation problems.
Consider the task of recovering non-local depen-
dencies (such as control, WH-extraction, topicaliza-
tion) in the surface syntactic phrase trees produced
by the state-of-the-art parser of (Charniak, 2000).
Figure 3 shows a graph-based encoding of the output
of the parser, and the task in question would consist
in transforming the graph in Figure 3 into the graph
in Figure 1. We notice that this transformation can
be realised as a sequence of independent and rela-
tively simple graph transformations: adding nodes
and edges to the graph or changing their labels (e.g.,
from NP to NP-SBJ).
Similarly, for the example in Figure 2, adding a
semantic layer (dashed edges) to the syntactic struc-
ture can also be seen as transforming a graph.
In general, we can view NLP tasks as adding ad-
ditional linguistic information to text, based on the
information already present: e.g., syntactic pars-
ing taking part-of-speech tagged sentences as in-
put (Collins, 1999), or anaphora resolution tak-
ing sequences of syntactically analysed and named-
entity-tagged sentences. If both input and output lin-
guistic structures are encoded as graphs, such NLP
tasks become graph transformation problems.
In the next section we describe our general
method for learning graph transformations from an
annotated corpus.
3 Learning graph transformations
We start with a few basic definitions. Similar
to (Schu?rr, 1997), we define ?emphgraph as a rela-
tional structure, i.e., a set of objects and relations
between them; we represent such structures as sets
of first-order logic atomic predicates defining nodes,
54
directed edges and their attributes (labels). Con-
stants used in the predicates represent objects (nodes
and edges) of graphs, as well as attribute names and
values. Atomic predicates node(?), edge(?, ?, ?) and
attr(?, ?, ?) define nodes, edges and their attributes.
We refer to (Schu?rr, 1997; Jijkoun, 2006) for formal
definitions and only illustrate these concepts with an
example. The following set of predicates:
node(n1), node(n2), edge(e, n1, n2),
attr(n1, label, Src), attr(n2, label,Dst)
defines a graph with two nodes, n1 and n2, hav-
ing labels Src and Dst (encoded as attributes named
label), and an (unlabelled) edge e going from n1 to
n2.
A pattern is an arbitrary graph and an occurence
of a pattern P in graph G is a total injective homo-
morphism ? from P to G, i.e., a mapping that asso-
ciates each object of P with one object G and pre-
serves the graph structure (relations between nodes,
edges, attribute names and values). We will also use
the term occurence to refer to the graph?(P ), a sub-
graph of G, the image of the mapping ? on P .
A graph rewrite rule is a triple r =
?lhsr, Cr, rhsr?: the left-hand side, the constraint
and the right-hand side of r, respectively, where lhsr
and rhsr are graphs and Cr is a function that returns
0 or 1 given a graphG, pattern lhsr and its occurence
in G (i.e., Cr specifies a constraint on occurences of
a pattern in a graph).
To apply a rewrite rule r = ?lhsr, Cr, rhsr? to
a graph G means finding all occurences of lhsr in
G for which Cr evaluates to 1, and replacing such
occurences of lhsr with occurences of rhsr. Effec-
tively, objects and relations present in lhsr but not in
rhsr will be removed from G, objects and relations
in rhsr but not in lhsr will be added to G, and com-
mon objects and relations will remain intact. Again,
we refer to (Jijkoun, 2006) for formal definitions.
As will be discussed below, our method for learn-
ing graph transformations is based on the ability to
compare pairs of graphs, identifying where the two
graphs are similar and where they differ. An align-
ment of two graphs is a partial one-to-one homomor-
phism between their nodes and edges, such that if
two edges of the two graphs are aligned, their re-
spective endpoints are aligned as well. A maximal
alignment of two graphs is an alignment that maxi-
mizes the sum of (1) the number of aligned objects
(nodes and edges), and (2) the number of match-
ing attribute values of all aligned objects. In other
words, a maximal alignment identifies as many sim-
ilarities between two graphs as possible. Given an
alignment of two graphs, it is possible to extract a
list of rewrite rules that can transform one graph into
another. For a maximal alignment such a list will
consist of rules with the smallest possible left- and
right-hand sides. See (Jijkoun, 2006) for details.
As stated above, we view NLP applications as
graph transformation modules. Our supervised
method for learning graph transformation requires
two corpora: input graphs In = {Ink} and corre-
sponding output graphs Out = {Outk}, such that
Outk is the desired output of the NLP module on
the input Ink.
The result of the method is an ordered list of graph
rewrite rules R = ?r1, . . . rn?, that can be applied in
sequence to input graphs to produce the output of the
NLP module.
Our method for learning graph transforma-
tions follows the structure of Transformation-Based
Learning (Brill, 1995) and proceeds iteratively, as
shown in Figure 4. At each iteration, we compare
and align pairs of input and output graphs, identify
possible rewrite rules and select rules with the most
frequent left-hand sides. For each selected rewrite
rule r, we extract all occurences of its left-hand
side and use them to train a two-class classifier im-
plementing the constraint Cr: the classifier, given
an encoding of an occurence of the left-hand side
predicts whether this particular occurence should
be replaced with the corresponding right-hand side.
When encoding an occurence as a feature vector, we
add as features all paths and all attributes of nodes
and edges in the one-edge neighborhood from the
nodes of the occurence. For the experiments de-
scribed in this paper we used the SVM Light classi-
fier (Joachims, 1999) with a standard linear kernel.
See (Jijkoun, 2006) for details.
4 Applications
Having presented a general method for learning
graph transformations, we now illustrate the method
at work and describe two applications to concrete
55
Compare
Apply
Extract rules
Aligned graphs
Compare
Apply
Extract rules
Aligned graphs
rules rulesrules
Ideal output graphsInput graphs
...
Iteration 1 Iteration 2 Iteration N
Compare
Apply
Extract rules
Aligned graphs
...
Figure 4: Structure of our method for learning graph transformations.
NLP problems: identification of non-local depen-
dencies (with the Penn Treebank data) and semantic
role labeling (with the Proposition Bank data).
4.1 Non-local dependencies
State-of-the-art statistical phrase structure parsers,
e.g., Charniak?s and Collins? parsers trained on
the Penn Treebank, produce syntactic parse trees
with bare phrase labels, (NP, PP, S, see Figure 3),
i.e., providing surface grammatical analysis of sen-
tences, even though the training corpus, the Penn
Treebank, is richer and contains additional gram-
matical and semantic information: it distinguishes
various types of modifiers, complements, subjects,
objects and annotates non-local dependencies, i.e.,
relations between phrases not adjacent in the parse
tree (see Figure 1). The task of recovering this in-
formation in the parser?s output has received a good
deal of attention. (Campbell, 2004) presents a rule-
based algorithm for empty node identification in
syntactic trees, competitive with the machine learn-
ing methods we mention next. In (Johnson, 2002)
a simple pattern-matching algorithm was proposed
for inserting empty nodes into syntactic trees, with
patterns extracted from the Penn Treebank. (Dienes,
2004) used a preprocessor that identified surface lo-
cation of empty nodes and a syntactic parser incor-
porating non-local dependencies into its probabilis-
tic model. (Jijkoun and de Rijke, 2004) described
an extension of the pattern-matching method with a
classifier trained on the dependency graphs derived
from the Penn Treebank data.
In order to apply our graph transformation method
to the task of identifying non-local dependencies,
we need to encode the information provided in the
Penn Treebank annotations and in the output of a
syntactic parser using directed labeled graphs. We
used a straightforward encoding of syntactic trees,
with nodes representing terminals and non-terminals
and edges defining the parent-child relationship. For
each node, we used the attribute type to specify
whether it is a terminal or a non-terminal. Ter-
minals corresponding to Penn empty nodes were
marked with the attribute empty = 1. For each
terminal (i.e., each word), the values of attributes
pos, word and lemma provided the part-of-speech tag,
the actual form and the lemma of the word. For
non-terminals, the attribute label contained the la-
bel of the corresponding syntactic phrase. The co-
indexing of empty nodes and non-terminals used in
the Penn Treebank to annotate non-local dependen-
cies was encoded using explicit edges with a distinct
type attribute, connecting empty nodes with their an-
tecedents (e.g., the dashed edge in Figure 1). For
each non-terminal node, its head child was marked
by attaching attribute head with value 1 to the corre-
56
sponding parent-child edge, and the lexical head of
each non-terminal was explicitly indicated using ad-
ditional edges with the attribute type = lexhead. We
used a heuristic method of (Collins, 1999) for head
identification.
When Penn Treebank sentences and the output of
the parser are encoded as directed labeled graphs
as described above, the task of identifying non-
local dependencies can be formulated as transform-
ing phrase structure graphs produced by a parser into
graphs of the type used in Penn Treebank annota-
tions.
We parsed the strings of the Penn Treebank with
Charniak?s parser and then used the data from sec-
tions 02?21 of the Penn Treebank for training: en-
coding of the parser?s output was used as the cor-
pus of input graphs for our learning method, and
the encoding of the original Penn annotations was
used as the corpus of output graphs. Similarly, we
used the data of sections 00?01 for development and
section 23 for testing. Using the input and output
corpora, we ran the learning method as described
above, at each iteration considering 20 most frequent
left-hand sides of rewrite rules. At each iteration,
the learned rewrite rules were applied to the current
training and development corpora to create a cor-
pus of input graphs for the next iteration (see Fig-
ure 4) and to estimate the performance of the system
at the current iteration. The system was evaluated
on the development corpus with respect to non-local
dependencies using the ?strict? evaluation measure
of (Johnson, 2002): the F1 score of precision and
recall of correctly identified empty nodes and an-
tecedents. If the absolute improvement of the F1
score for the evaluation measure was smaller than
0.1, the learning cycle was terminated, otherwise a
new iteration was started.
The learning cycle terminated after 12 iterations.
The resulting sequence of 12 ? 20 = 240 graph
rewrite rules was applied to the test corpus of in-
put graphs: Charniak?s parser output on the strings
of section 23 of the Penn Treebank. The result
was evaluated against the original annotations of the
Penn Treebank.
The results of the evaluation of the system on
empty nodes and non-local dependencies and the
PARSEVAL F1 score on local syntactic phrase
structure against the test corpus at each iteration are
Stage P R F1 PARSEVAL F1
Initial 0.0 0.0 0.0 88.7
1 88.2 38.6 53.7 88.4
2 87.2 48.6 62.5 88.4
3 87.5 51.9 65.2 88.4
4 86.7 52.1 65.1 88.4
5 86.1 56.3 68.1 88.3
6 86.0 57.2 68.7 88.4
7 86.3 61.3 71.7 88.4
8 86.6 63.4 73.2 88.4
9 86.7 64.6 74.0 88.4
10 86.7 64.9 74.2 88.4
11 86.6 65.1 74.3 88.4
12 86.7 65.2 74.4 88.4
Table 1: Evaluation of our method for identification
of empty nodes and their antecedents (12 first itera-
tions).
shown in Table 1.
As one can expect, at each iteration the method
extracts graph rewrite rules that introduce empty
nodes and non-local relations into syntactic struc-
tures, increasing the recall. The performance of the
final system (P/R/F1 = 86.7/65.2/74.4) for the task
of identifying non-local dependencies is compara-
ble to the performance of the best model of (Di-
enes, 2004): P/R/F1=82.5/70.1/75.8. The PARSE-
VAL score for the present system (88.4) is, however,
higher than the 87.3 for the system of Dienes.
Another effect of the learned transformations is
changing node labels of non-terminals, specifically,
modifying labels to include Penn functional tags
(e.g., changing NP in the input graph in Figure 3 to
NP-SBJ in the output graph in Figure 1). In fact, 17%
of all learned rewrite rules involved only changing
labels of non-terminal nodes. Analysis of the results
showed that the system is capable of assigning Penn
function tags to constituents produced by Charniak?s
parser with F1 = 91.4 (we use here the evalua-
tion measure of (Blaheta, 2004): the F1 score of the
precision and recall for assigning function tags to
constituents with surface spans correctly identified
by Charniak?s parser). Comparison to the evalua-
tion results of the function tagging method presented
in (Blaheta, 2004) is shown in Table 2.
The present system outperforms the system of
Blaheta on semantic tags such as -TMP or -MNR
marking temporal and manner adjuncts, respec-
tively, but performs worse on syntactic tags such
as -SBJ or -PRD marking subjects and predicatives,
57
(Blaheta, 2004) Here
Type Count P / R / F1 P / R / F1
All tags 8480 - 93.3 / 89.6 / 91.4
Syntactic 4917 96.5 / 95.3 / 95.9 95.4 / 95.5 / 95.5
Semantic 3225 86.7 / 80.3 / 83.4 89.7 / 82.5 / 86.0
Table 2: Evaluation of adding Penn Treebank func-
tion tags.
respectively. Note that the present method was not
specifically designed to add functional tags to con-
stituent labels. The method is not even ?aware? that
functional tags exists: it simply treats NP and NP-SBJ
as different labels and tries to correct labels compar-
ing input and output graphs in the training corpora.
In general, of the 240 graph rewrite rules ex-
tracted during the 12 iterations of the method, 25%
involved only one graph node in the left-hand side,
16% two nodes, 12% three nodes, etc. The two
most complicated extracted rewrite rules involved
left-hand sides with ten nodes.
We now switch to the second application of our
graph transformation method.
4.2 Semantic role labeling
Put very broadly, the task of semantic role labeling
consists in detecting and labeling simple predicates:
Who did what to whom, where, when, how, why, etc.
There is no single definition of a universal set of
semantic roles and moreover, different NLP appli-
cations may require different specificity of role la-
bels. In this section we apply the graph transforma-
tion method to the task of identification of semantic
roles as annotated in the Proposition Bank (Palmer
et al, 2005), PropBank for short. In PropBank, for
all verbs (except copular) of the syntactically anno-
tated sentences of the Wall Street Journal section of
the Penn Treebank, semantic arguments are marked
using references to the syntactic constituents of the
Penn Treebank. For the 49,208 syntactically anno-
tated sentences of the Penn Treebank, the PropBank
annotated 112,917 verb predicates (2.3 predicates
per sentence on average), with a total of 292,815 se-
mantic arguments (2.6 arguments per predicate on
average).
PropBank does not aim at cross-verb semantically
consistent labeling of arguments, but rather at anno-
tating the different ways arguments of a verb can
be realized syntactically in the corpus, which re-
sulted in the choice of theory-neutral numbered la-
bels (e.g., Arg0, Arg1, etc.) for semantic arguments.
Figure 2 shows an example of a PropBank annota-
tion (dashed edges).
In this section we address a specific NLP task:
identifying and labeling semantic arguments in the
output of a syntactic parser. For the example in
Figure 2 this task corresponds to adding ?semantic?
nodes and edges to the syntactic tree.
As before, in order to apply our graph transfor-
mation method, we need to encode the available in-
formation using graphs. Our encoding of syntactic
phrase structure is the same as in Section 4.1 and the
encoding of the semantic annotations of PropBank
is straightforward. For each PropBank predicate, a
new node with attributes type = propbank and label =
pred is added. Another node with label = head and
nodes for all semantic arguments of the predicate
(with labels indicating PropBank argument names)
are added and connected to the predicate node. Ar-
gument nodes with label ARGM (adjunct) addition-
ally have a feature attribute with values TMP, LOC,
etc., as specified in PropBank. The head node and
all argument nodes are linked to their respective syn-
tactic constituents, as specified in the PropBank an-
notation. All introduced semantic edges are marked
with the attribute type = propbank.
As before, we used section 02?21 of the Prop-
Bank (which annotates the same text as the Penn
Treebank) to train our graph transformation system,
section 00-01 for development and section 23 for
testing. We ran three experiments, taking three dif-
ferent corpora of input graphs:
1. the original syntactic structures of the Penn
Treebank containing function tags, empty
nodes, non-local dependencies, etc.;
2. the output of Charniak?s parser (i.e., bare syn-
tactic trees) on the strings of sections 02?21;
and
3. the output of Charniak?s parser processed
with the graph transformation system described
in 4.1.
For all three experiments we used the gold stan-
dard syntactic and semantic annotations from the
58
Penn Treebank Charniak Charniak +
Iter. P R P R P R
1 90.0 70.7 79.5 58.6 79.9 59.1
2 90.7 76.5 81.2 63.9 81.0 64.2
3 90.7 78.1 81.3 65.6 81.1 65.8
4 90.6 78.9 81.4 66.5 81.2 66.7
5 90.5 80.4 81.4 67.0 81.2 68.3
6 90.4 81.2 81.4 68.3 81.1 68.8
7 90.3 81.9 81.3 68.9 81.0 69.3
8 90.3 82.2 81.3 69.3 81.0 69.8
9 90.3 82.5 81.3 69.6 81.0 70.1
10 90.3 82.8 81.4 69.8 81.0 70.3
11 90.3 83.0 81.3 69.9 81.0 70.4
12 90.3 83.2
Table 3: Evaluation of our method for semantic role
identification with Propbank: with Charniak parses
and with parses processed by the system of Sec-
tion 4.1.
Penn Treebank and PropBank as the corpora of out-
put graphs (for the experiment with bare Charniak
parses, we dropped function tags, empty nodes and
non-local dependencies from the syntactic annota-
tion of the output graphs: we did not want our sys-
tem to start recovering these annotations, but were
interested in the identification of PropBank informa-
tion alone).
For each of the experiments, we used the corpora
of input and output graphs as before, at each itera-
tion extracting 20 rewrite rules with most frequent
left-hand sides, applying the rules to the develop-
ment data to measure the current performance of the
system. We stopped the learning in case the perfor-
mance improvement was less than a threshold and,
otherwise, continued the learning loop. As our per-
formance measure we used the F1 score of precision
and recall of the correctly identified and labeled non-
empty constituents?semantic arguments.
In all experiments, the learning stopped after 11
or 12 iterations. The results of the evaluation of the
system at each iteration on the test section of Prop-
Bank are shown in Table 3.
As one may expect, the performance of our se-
mantic role labeler is substantially higher on the
gold Penn Treebank syntactic structures than on the
parser?s output. Surprisingly, however, adding extra
information to the parser?s output (i.e., processing it
with the system of Section 4.1) does not significantly
improve the performance of the resulting system.
In Table 4 we compare our system for semantic
System P R F1
(Pradhan et al, 2005) 80.9 76.8 78.8
Here 81.0 70.4 75.3
Table 4: Evaluation of our methods for semantic role
identification with Propbank (12 first iterations).
roles labeling with the output of Charniak?s parser to
the state-of-the-art system of (Pradhan et al, 2005).
While showing good precision, our system per-
forms worse than state-of-the-art with respect to re-
call. Taking into account the iterative nature of
the method and imperfect rule selection criteria (we
simply take the most frequent left-hand sides), we
believe that it is the rule selection and learning termi-
nation condition that account for the relatively low
recall values. Indeed, in all three experiments de-
scribed above the learning loop stops while the recall
is still on the rise, albeit very slowly. It seems that
a more careful rule selection mechanism and loop
termination criteria are needed to address the recall
problem.
5 Conclusions
In this paper we argued that encoding diverse and
complex linguistic structures as directed labeled
graphs allows one to view many NLP tasks as graph
transformation problems. We proposed a general
method for learning graph transformation from an-
notated corpora and described experiments with two
NLP applications.
For the task of identifying non-local dependen-
cies and for function tagging our general method
demonstrates performance similar to the state-of-
the-art systems, designed specifically for these tasks.
For the PropBank semantic role labeling the method
shows a relatively low recall, which can be explained
by our sub-optimal ?rule of thumb? heuristics (such
as selecting 20 most frequent rewrite rules at each
iteration of the learning method). We see two ways
of avoiding such heuristics. First, one can define
and fine-tune the heuristics for each specific appli-
cation. Second, one can use more informed rewrite
rule selection methods, based on graph-based rela-
tional learning and frequent subgraph detection al-
gorithms (Cook and Holder, 2000; Yan and Han,
2002). Furthermore, more experiments are required
59
to see how the details of encoding linguistic in-
formation in graphs affect the performance of the
method.
Acknowledgements
This research was supported by the Netherlands
Organization for Scientific Research (NWO) un-
der project numbers 017.001.190, 220-80-001,
264-70-050, 354-20-005, 600.065.120, 612-13-
001, 612.000.106, 612.066.302, 612.069.006,
640.001.501, 640.002.501, and by the E.U. IST
programme of the 6th FP for RTD under project
MultiMATCH contract IST-033104.
References
Ann Bies, Mark Ferguson, Karen Katz, and Robert Mac-
Intyre. 1995. Bracketing guidelines for Treebank II
style Penn Treebank project. Technical report, Uni-
versity of Pennsylvania.
Don Blaheta. 2004. Function Tagging. Ph.D. thesis,
Brown University.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part-of-speech tagging. Computational Lin-
guistics, 21(4):543?565.
Richard Campbell. 2004. Using linguistic principles
to recover empty categories. In Proceedings of the
42nd Annual Meeting on Association for Computa-
tional Linguistics, pages 645?653.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st Meeting of NAACL,
pages 132?139.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Diane J. Cook and Lawrence B. Holder. 2000.
Graph-based data mining. IEEE Intelligent Systems,
15(2):32?41.
Pe?ter Dienes. 2004. Statistical Parsing with Non-local
Dependencies. Ph.D. thesis, Universita?t des Saarlan-
des, Saarbru?cken, Germany.
Daniel Gildea. 2001. Statistical Language Understand-
ing Using Frame Semantics. Ph.D. thesis, University
of California, Berkeley.
Ana-Maria Giuglea and Alessandro Moschitti. 2006. Se-
mantic role labeling via framenet, verbnet and prop-
bank. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics, pages 929?936.
Julia Hockenmaier. 2003. Parsing with generative mod-
els of predicate-argument structure. In Proceedings of
the 41st Meeting of ACL, pages 359?366.
Valentin Jijkoun and Maarten de Rijke. 2004. Enrich-
ing the output of a parser using memory-based learn-
ing. In Proceedings of the 42nd Meeting of the Asso-
ciation for Computational Linguistics (ACL?04), Main
Volume, pages 311?318, Barcelona, Spain, July.
Valentin Jijkoun. 2006. Graph Transformations for Nat-
ural Language Processing. Ph.D. thesis, University of
Amsterdam.
Thorsten Joachims. 1999. Making large-scale svm
learning practical. In B. Scho?lkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods - Sup-
port Vector Learning. MIT-Press.
Christopher R. Johnson, Miriam R. L. Petruck, Collin F.
Baker, Michael Ellsworth, Josef Ruppenhofer, and
Charles J. Fillmore. 2003. FrameNet: Theory and
Practice. http://www.icsi.berkeley.edu/
?framenet.
Mark Johnson. 2002. A simple pattern-matching al-
gorithm for recovering empty nodes and their an-
tecedents. In Proceedings of the 40th meeting of ACL,
pages 136?143.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1).
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, JimMar-
tin, and Dan Jurafsky. 2005. Semantic role label-
ing using different syntactic views. In Proceedings of
ACL-2005.
A. Schu?rr. 1997. Programmed graph replacement sys-
tems. In Grzegorz Rozenberg, editor, Handbook of
Graph Grammars and Computing by Graph Transfor-
mation, chapter 7, pages 479?546.
Kristina Toutanova, Aria Haghighi, and Chris Manning.
2005. Joint learning improves semantic role labeling.
In Proceedings of the 43rd Meeting of the Association
for Computational Linguistics (ACL).
Xifeng Yan and Jiawei Han. 2002. gspan: Graph-based
substructure pattern mining. In Proceedings of the
2002 IEEE International Conference on Data Mining
(ICDM).
60
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 468?471,
Prague, June 2007. c?2007 Association for Computational Linguistics
UVA: Language Modeling Techniques for Web People Search
Krisztian Balog
ISLA, University of Amsterdam
kbalog@science.uva.nl
Leif Azzopardi
University of Glasgow
leif@dcs.gla.ac.uk
Maarten de Rijke
ISLA, University of Amsterdam
mdr@science.uva.nl
Abstract
In this paper we describe our participation in
the SemEval 2007 Web People Search task.
Our main aim in participating was to adapt
language modeling tools for the task, and to
experiment with various document represen-
tations. Our main finding is that single pass
clustering, using title, snippet and body to
represent documents, is the most effective
setting.
1 Introduction
The goal of the Web People Search task at SemEval
2007 was to disambiguate person names in a web
searching scenario (Artiles et al, 2007). Participants
were presented with the following setting: given a
list of documents retrieved from a web search engine
using a person?s name as a query, group documents
that refer to the same individual.
Our aim with the participation was to adapt lan-
guage modeling techniques to this task. To this
end, we employed two methods: single pass cluster-
ing (SPC) and probabilistic latent semantic analysis
(PLSA). Our main finding is that the former leads to
high purity, while the latter leads to high inverse pu-
rity scores. Furthermore, we experimented with var-
ious document representations, based on the snip-
pets and body text. Highest overall performance was
achieved with the combination of both.
The remainder of the paper is organized as fol-
lows. In Section 2 we present the two approaches we
employed for clustering documents. Next, in Sec-
tion 3 we discuss document representation and pre-
processing. Section 4 reports on our experiments.
We conclude in Section 5.
2 Modeling
2.1 Single Pass Clustering
We employed single pass clustering (Hill., 1968) to
automatically assign pages to clusters, where we as-
sume that each cluster is a set of pages related to one
particular sense of the person.
The process for assignment was performed as fol-
lows: The first document was taken and assigned
to the first cluster. Then each subsequent document
was compared against each cluster with a similarity
measure based on the log odds ratio (initially, there
was only the initial one created). A document was
assigned to the most likely cluster, as long as the
similarity score was higher than a threshold ?; oth-
erwise, the document was assigned to a new cluster,
unless the maximum number of desired clusters ?
had been reached; in that case the document was as-
signed to the last cluster (i.e., the left overs).
The similarity measure we employed was the log
odds ratio to decide whether the document was more
likely to be generated from that cluster or not. This
approach follows Kalt (1996)?s work on document
classification using the document likelihood by rep-
resenting the cluster as a multinomial term distribu-
tion (i.e., a cluster language model) and predicting
the probability of a document D, given the cluster
language model, i.e., p(D|?C). It is assumed that
the terms t in a document are sampled independently
and identically, so the log odds ratio is calculated as
468
follows:
logO(D,C) = log
p(D|?C)
p(D|?C?)
(1)
= log
?
t?D p(t|?C)
n(t,D)
?
t?D p(t|?C?)n(t,D)
,
where n(t,D) is the number of times a term ap-
pears in a document, and the ?C represents the lan-
guage model that represents not being in the cluster.
Note this is similar to a well-known relevance mod-
eling approach, where the clusters are relevance and
non-relevance, except, here, it is applied in the con-
text of classification as done by Kalt (1996).
The cluster language model was estimated by per-
forming a linear interpolation between the empirical
probability of a term occurring in the cluster p(t|C)
and the background model p(t), the probability of
a term occurring at random in the collection, i.e.,
p(t|?C) = ? ? p(t|C) + (1 ? ?) ? p(t), where ? was
set to 0.5.1 The ?not in the cluster? language model
was approximated by using the background model
p(t). The similarity threshold above (used for de-
ciding whether to assign a document to an existing
cluster) was set to ? = 1, and ? was set to 100.
2.2 Probabilistic Latent Semantic Analysis
The second method for disambiguation we em-
ployed was probabilistic latent semantic analysis
(PLSA) (Hofmann, 1999). PLSA clusters docu-
ments based on the term-document co-occurrence
which results in semantic decomposition of the term
document matrix into a lower dimensional latent
space. Formally, PLSA can be defined as:
p(t, d) = p(d)
?
z
p(t|z)p(z|d), (2)
where p(t, d) is the probability of term t and doc-
ument d co-occurring, p(t|z) is the probability of a
term given a latent topic z and p(z|d) is the probabil-
ity of a latent topic in a document. The prior prob-
ability of the document, p(d), was assumed to be
uniform. This decomposition can be obtained auto-
matically using the EM algorithm (Hofmann, 1999).
Once estimated, we assumed that each latent topic
represents one of the different senses of the person,
1This value was not tuned but selected based on best per-
forming range suggested by Lavrenko and Croft (2001).
so the document is assigned to one of the person-
topics. Here, we made the assignment based on the
maximum p(z|d), so if p(z|d) = max p(z|d), then d
was assigned to z.
In order to automatically select the number of
person-topics, we performed the following process
to decide when the appropriate number of person-
topics (defined by k) have been identified: (1) we
set k = 2 and computed the log-likelihood of the de-
composition on a held out sample of data; (2) we in-
cremented k and computed the log-likelihood; if the
log-likelihood had increased over a given threshold
(0.001) then we repeated step 2, else (3) we stopped
as we have maximized the log-likelihood of the de-
compositions, with respect to the number person-
topics. This point was assumed to be the optimal
with respect to the number of person senses. Since,
we are focusing on identifying the true number of
classes, this should result in higher inverse purity,
whereas with the single pass clustering the number
of clusters is not restricted, and so we would expect
single pass clustering to produce more clusters but
with a higher purity.
We used Lemur2 and the PennAspect implemen-
tation of PLSA (Schein et al, 2002) for our exper-
iments, where the parameters for PLSA where set
as follows. For each k we performed 10 initializa-
tions where the best initialization in terms of log-
likelihood was selected. The EM algorithm was
run using tempering with up to 100 EM Steps. For
tempering the setting suggested in (Hofmann, 1999)
were used. The models were estimated on 90% of
the data and 10% of the data was held out in order to
compute the log-likelihood of the decompositions.
3 Document Representation
This section describes the various document repre-
sentations we considered, and preprocessing steps
we applied.
For each document, we considered the title, snip-
pet, and body text. Title and snippet were pro-
vided by the output of the search engine results
(person name.xml files), while the body text
was extracted from the crawled index.html files.
2http://www.lemurproject.org
469
Method Title+Snippet Body Title+Snippet+Body
Pur InvP F0.5 F0.2 Pur InvP F0.5 F0.2 Pur InvP F0.5 F0.2
Train data
SPC 0.903 0.298 0.422 0.336 0.776 0.416 0.482 0.434 0.768 0.438 0.506 0.456
PLSA 0.589 0.833 0.636 0.716 0.591 0.656 0.563 0.592 0.579 0.724 0.588 0.641
Test data
SPC 0.867 0.541 0.640 0.575 0.818 0.570 0.647 0.596 0.810 0.607 0.669 0.628
PLSA 0.292 0.892 0.383 0.533 0.311 0.869 0.413 0.563 0.305 0.923 0.405 0.566
Table 1: Results of the clustering methods using various document representations.
3.1 Acquiring Plain-Text Content from HTML
Our aim is to extract the plain-text content from
HTML pages and to leave out blocks or segments
that contain little or no useful textual information
(headers, footers, navigation menus, adverts, etc.).
To this end, we exploit the fact that most web-
pages consist of blocks of text content with rel-
atively little markup, interspersed with navigation
links, images with captions, etc. These segments of
a page are usually separated by block-level HTML
tags. Our extractor first generates a syntax tree from
the HTML document. We then traverse this tree
while bookkeeping the stretch of uninterrupted non-
HTML text we have seen. Each time we encounter a
block-level HTML tag we examine the buffer of text
we have collected, and if it is longer than a threshold,
we output it. The threshold for the minimal length of
buffer text was empirically set to 10. In other words,
we only consider segments of the page, separated
by block-level HTML tags, that contain 10 or more
words.
3.2 Indexing
We used a standard (English) stopword list but we
did not apply stemming. A separate index was built
for each person, using the Lemur toolkit. We created
three index variations: title+snippet, body,
and title+snippet+body.
In our official run we used the
title+snippet+body index; however, in
the next section we report on all three variations.
4 Results
Table 1 reports on the results of our experiments us-
ing the Single Pass Clustering (SPC) and Probabilis-
tic Latent Semantic Analysis (PLSA) methods with
various document representations. The measures
(purity, inverse purity, and F-score with ? = 0.5
and ? = 0.2) are presented for both the train and
test data sets.
The results clearly demonstrate the difference in
the behaviors of the two clustering methods. SPC
assigns people to the same cluster with high preci-
sion, as is reflected by the high purity scores. How-
ever, it is overly restrictive, and documents that be-
long to the same person are distributed into a number
of clusters, which should be further merged. This
explains the low inverse purity scores. Further ex-
periments should be performed to evaluate to which
extent this restrictive behavior could be controlled
by the ? parameter of the method.
In contrast with SPC, the PLSA method produces
far fewer clusters per person. These clusters may
cover multiple referents of a name, as is witnessed
by the low purity scores. On the other hand, inverse
purity scores are very high, which means referents
are usually not dispersed among clusters.
As to the various document representations, we
found that highest overall performance was achieved
with the combination of title, snippet, and body text.
Since the data was not homogenous, it would be
interesting to see how performance varies on the dif-
ferent names. We leave this analysis to further work.
Our official run employed the SPC method, using
the title+snippet+body index. The results
of our official submission are presented in Table 2.
Our purity score was the highest of all submissions,
and our system was ranked overall 4th, based on the
F?=0.5 measure.
470
Pur InvP F0.5 F0.2
Lowest 0.30 0.60 0.40 0.55
Highest 0.81 0.95 0.78 0.83
Average 0.54 0.82 0.60 0.69
UVA 0.81 0.60 0.67 0.62
Table 2: Official submission results and statistics.
5 Conclusions
We have described our participation in the SemEval
2007 Web People Search task. Our main aim in par-
ticipating was to adapt language modeling tools for
the task, and to experiment with various document
representations. Our main finding is that single pass
clustering, using title, snippet and body to represent
documents, is the most effective setting.
We explored the two very different clustering
schemes with contrasting characteristics. Looking
forward, possible improvements might be pursued
by combining the two approaches into a more robust
system.
6 Acknowledgments
Krisztian Balog was supported by the Netherlands
Organization for Scientific Research (NWO) un-
der project number 220-80-001. Maarten de Rijke
was supported by NWO under project numbers
017.001.190, 220-80-001, 264-70-050, 354-20-005,
600.065.120, 612-13-001, 612.000.106, 612.066.-
302, 612.069.006, 640.001.501, 640.002.501, and
by the E.U. IST programme of the 6th FP for RTD
under project MultiMATCH contract IST-033104.
References
J. Artiles, J. Gonzalo, and S. Sekine. 2007. The
SemEval-2007 WePS Evaluation: Establishing a
benchmark for the Web People Search Task. In Pro-
ceedings of Semeval 2007, Association for Computa-
tional Linguistics.
D. R. Hill. 1968. A vector clustering technique. In
Samuelson, editor, Mechanised Information Storage,
Retrieval and Dissemination, North-Holland, Amster-
dam.
Thomas Hofmann. 1999. Probabilistic latent semantic
analysis. In Proc. of Uncertainty in Artificial Intelli-
gence, UAI?99, Stockholm.
T. Kalt. 1996. A new probabilistic model of text classifi-
cation and retrieval. Technical Report CIIR TR98-18,
University of Massachusetts, January 25, 1996.
V. Lavrenko and W. B. Croft. 2001. Relevance-based
language models. In Proceedings of the 24th annual
international ACM SIGIR conference, pages 120?127,
New Orleans, LA. ACM Press.
Andrew I. Schein, Alexandrin Popescul, Lyle H. Un-
gar, and David M. Pennock. 2002. Meth-
ods and metrics for cold-start recommendations.
In SIGIR ?02: Proceedings of the 25th an-
nual international ACM SIGIR conference on Re-
search and development in information retrieval,
pages 253?260, New York, NY, USA. ACM
Press. See http://www.cis.upenn.edu/
datamining/software dist/PennAspect/.
471
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 996?1006, Dublin, Ireland, August 23-29 2014.
Prior-informed Distant Supervision
for Temporal Evidence Classification
Ridho Reinanda
University of Amsterdam
Amsterdam, The Netherlands
r.reinanda@uva.nl
Maarten de Rijke
University of Amsterdam
Amsterdam, The Netherlands
derijke@uva.nl
Abstract
Temporal evidence classification, i.e., finding associations between temporal expressions and re-
lations expressed in text, is an important part of temporal relation extraction. To capture the
variations found in this setting, we employ a distant supervision approach, modeling the task as
multi-class text classification. There are two main challenges with distant supervision: (1) noise
generated by incorrect heuristic labeling, and (2) distribution mismatch between the target and
distant supervision examples. We are particularly interested in addressing the second problem
and propose a sampling approach to handle the distribution mismatch. Our prior-informed distant
supervision approach improves over basic distant supervision and outperforms a purely super-
vised approach when evaluated on TAC-KBP data, both on classification and end-to-end metrics.
1 Introduction
Temporal relation extraction is the problem of extracting the temporal extent of relations between entities.
A typical solution to the temporal relation extraction problem has three main components: (1) passage
retrieval, (2) temporal evidence classification, and (3) temporal evidence aggregation. A community-
based effort to evaluate temporal relation extraction was introduced in 2011 as a TAC Knowledge Base
Population task: Temporal Slot Filling, or TSF for short (Ji et al., 2011).
An illustration of temporal slot filling is as follows. Having identified a per:spouse relation be-
tween two entities (Freeman Dyson, Imme Dyson), a system must establish the temporal boundaries
from its supporting sentence. In the case of the sentence ?In 1958, he married Imme Dyson?, the goal
is to find that the relation lasts from 1958 until the present day. Within the TSF setting, the boundaries
are represented as beginning and ending intervals in a tuple (T
1
, T
2
, T
3
, T
4
) instead of an exact time
expression, so as to allow uncertainty in the system output. We investigate temporal relation extraction
following this setting. We focus on the temporal evidence classification part.
One of the challenges with relation extraction is the limited amount of training data available to capture
the variations in a target corpus: temporal relation extraction faces the same challenge. Employing distant
supervision (Mintz et al., 2009) is a way to address the challenge. But generating example training data
in the temporal setting is not straightforward: we have to find not only the query and related entity, but
also the time expression, in a single text segment.
Employing distant supervision for temporal evidence classification will introduce noise, in the form
of labels and additional contexts (e.g., lexical features). A lot of previous work in distant supervision has
been dedicated to reducing noise in distant supervision (Bunescu and Mooney, 2007; Riedel et al., 2010;
Wei et al., 2012). We are interested in another phenomenon: the class distributions found in training
data generated by a distant supervision approach. These distributions become an issue if the distant
supervision corpus has a different structure and different characteristics compared to the target corpus,
e.g., Wikipedia vs. news articles. We observe that in the case of temporal evidence, news articles and
Wikipedia do indeed contain different class distributions. Our working hypothesis is that incorporating
prior information about temporal class distribution helps improve our distant supervision approach. We
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
996
test this hypothesis by comparing a distant supervision strategy with class priors to a distant supervision
without class priors. We also demonstrate the effectiveness of our method by contrasting it with a purely
supervised approach. In addition, we investigate how the difference in performance in temporal evidence
classification affects the final score obtained in the overall end-to-end task.
We discuss related work in Section 2. In Section 3, we describe our distant supervision approach for
temporal evidence classification. Our experimental setup is detailed in Section 4. We follow with results
in Section 5 and a conclusion in Section 6.
2 Related Work
We discuss two groups of related work: on temporal slot filling and on distant supervision.
2.1 Temporal slot filling
Some previous work uses a pattern-based approach (Byrne and Dunnion, 2011); patterns are defined in
terms of query entity, temporal expression, and slot value. For example, the word divorce should trigger
that the relation per:spouse is ending. Other work uses temporal linking between time expressions and
events in an event-based approach (Burman et al., 2011), where the source documents are annotated with
TimeML event annotations (Pustejovsky et al., 2003); the authors use intra-sentence event-time links,
and inter-sentence event-event links, following a TempEval approach (UzZaman et al., 2012). Garrido
et al. (2012) use a graph-based document representation; they convert document context to a graph
representation and use TARSQI to determine links between time expressions and events in documents
and later map the resulting links into five temporal classes.
Li et al. (2012) combine flat and structured approaches to perform temporal classification. Their
approach relies on a custom SVM kernel designed around flat (window and shallow dependency) features
and structured (dependency path) features. The structured approach is designed to overcome the long
context problem. They use a distant supervision approach for the temporal classification part, obtained
on Freebase relations. They further extend their approach with self-training and relabeling (Ji et al.,
2013).
Finally, Surdeanu et al. (2011) use n-grams around temporal expressions to train a distant supervision
system. To be able to use Freebase facts, they find example sentences in Wikipedia, and use a window
of five words from the temporal expression, using Freebase facts as start and end trigger. They use
Jaccard correlation between n-grams to determine the association to start and end. Sil and Cucerzan
(2014) performed distant supervision using facts obtained from Wikipedia infoboxes. From Wikipedia
infoboxes, they retrieve the relevant sentences and build n-gram language models of the relations. In
a slightly different setting (exploratory search), Reinanda et al. (2013) establish the temporal extent of
entity associations simply by looking at their co-occurrence within documents in the corpus.
Our approach to temporal evidence classification differs from most existing approaches in its distant
supervision scheme. We use distant supervision to directly perform a multi-class classification of tem-
poral evidence against the five main temporal classes (including the before and after class), where most
of the previous systems train a model to detect the beginning and ending of relationships only.
2.2 Reducing noise in distant supervision
With distant supervision (Mintz et al., 2009), indirect examples in the form of relations from a knowledge
base such as Freebase and DBPedia are used. From these relation tuples, instances of relations in the
form of sentences in the corpus are searched. Text features are later extracted from these sentences that
are then used to train classifiers that can identify relations in the text corpus.
Reducing noise is an important ingredient when working with a distant supervision assumption. Re-
labeling is one such approach; Tamang and Ji (2012) perform relabeling based on semi-supervised lasso
regression to reduce incorrect labeling. Wei et al. (2012) show that instances may be labeled incorrectly
due to the knowledge base being incomplete. They propose to overcome the problem of incomplete
knowledge bases for distant supervision through passage retrieval model with relation extraction.
997
Ritter et al. (2003) focus on the issue of missing data for texts that contain rare entities that do not exist
in the original knowledge base. Riedel et al. (2010) work with a relaxed distant supervision assumption;
they design a factor graph to explicitly model whether two entities are related, and later train this model
with a semi-supervised constraint-driven algorithm; they achieve a 31 percent error reduction.
Bunescu and Mooney (2007) introduce multiple instance learning to handle the weak confidence in
the assigned label. They divide the instances into a positive bag (at least one positive example) and
a negative bag (all negative examples). They design a custom kernel to work with this weaker form of
supervision. Surdeanu et al. (2012) operate on the same principle, but model the relation between entities
and relation classes using graphical models. Hoffmann et al. (2011) also use multi-instance learning, but
focus on overlapping relations.
What we add on top of existing work is the use of sampling techniques to correct for skewed distri-
butions introduced through distant examples. We propose prior sampling, correcting the distributions of
the classes in the generated examples to fit the target corpora.
3 Method
The temporal slot filling task is defined as follows: given a relation R = (q, r, s), where q is a query
entity, r is a related entity, and s is a slot type, one must find T
R
, a tuple of four dates (T
1
, T
2
, T
3
, T
4
)
where R holds, where T
1
and T
2
form the beginning interval of the relation, and T
3
and T
4
is the ending
interval. A system first must retrieve all passages or sentences expressing the relation between q and r.
Each sentences and any time information within them will serve as intermediate evidence. This temporal
evidence will later be aggregated and converted to tuple representation T
R
.
In this paper, we focus on temporal evidence classification. That is, assuming the passage retrieval
component has retrieved the relevant passages as intermediate evidence of temporal relations, we must
classify whether the time expression t in the passage belongs to one these classes: BEGINNING, END-
ING, BEFORE, AFTER, andWITHIN. In the training and evaluation data available to us, only the offsets
of the time expression within the document are given for each intermediate evidence, therefore we first
extract the paragraph and find the context sentence mentioning t.
Distant supervision for temporal classification The temporal slot filling task, as specified by TAC-
KBP, defines 7 types of temporal-intensive relations. In our distant supervision approach, we use a
separate knowledge base to find instances of the equivalent relations. We use Freebase as our reference
knowledge base. That is, we use the temporal information found in Freebase to generate training exam-
ples. We manually map the TAC-KBP?s 8 temporal relations into 6 Freebase mediator relations. The
complete mapping of the relations can be found in Table 1.
In an article, entities and time expressions are not always referred to using their full mentions within a
single sentence. Sometimes information is scattered around several sentences: the query entity q in the
first sentence, later referred to using a pronoun in the second sentence that contains a time expression,
etc. One common way to deal with this problem is to run full co-reference resolution, therefore ensuring
all mentions are resolved. We handle this problem by relaxing the distant supervision rule. Rather than
retrieving sentences, we retrieve passages containing the query entity q, and related entity r instead. We
later replace every pronoun found within the passage with q. Based on our analysis of the Wikipedia
articles, this simple heuristic should work, because most Wikipedia articles are entity-centric, and a lot
of the pronouns mentioned in the articles will refer to the query entity q.
Each relation that we mapped from Freebase has temporal boundaries from and to. Following Li et al.
(2012), we use Algorithm 1 to generate the training examples, but adapt it suit to our assumption.
Sampling the DS examples We manually compared our main corpus (TAC document collection) and
our distant supervision corpus (Wikipedia) and noticed some discrepancies. The main corpus mainly
consists of newswire articles; one of the main difference between Wikipedia articles and newswire ar-
ticles is that Wikipedia articles mainly consist of milestone events. In terms of class distribution, this
means that most of the generated examples will be in the form of BEGINNING and ENDING class,
followed by the BEFORE and AFTER class, with the smallest number of examples belonging to the
998
TAC Relations Freebase Relations
per:spouse marriage
per:title employment-tenure,
goverment-position-
held
per:employee-of employment-tenure
per:member-of political-party-tenure
per:cities-of-
residence
places-lived
per:stateorprovinces-
of-residence
places-lived
per:countries-of-
residence
places-lived
org:top-
employees/members
organization-
leadership
Table 1: Relation mapping to Freebase.
Data: Freebase temporal relation (q, r, from, to)
Result: labeled training examples
Retrieve the Wikipedia article of the query entity q;
Split article into passages;
Retrieve the passages containing q, r;
Extract all time expressions from the passages;
for time-expression t do
Retrieve the context sentence s containing t;
If t is from : use s, t as BEGINNING example;
If t is to : use s, t as ENDING example;
If t before from : use s, t as BEFORE example;
If t after to : use s, t as AFTER example;
If t between from and to : use s, t as WITHIN example;
end
Algorithm 1: Training data generation.
WITHIN class. In newswire, however, we tend to see something different; most of the time expressions
will belong to the WITHIN class.
We argue that using the training data with a ?smarter? prior is important. More data not only means
more information, but may also mean more noise. This is particularly important with the relaxed dis-
tant supervision assumption that we have. Therefore, we choose to sample instead of using all of the
generated training examples.
We employ two sampling strategies: uniform, sampling from our generated training data and deliber-
ately fitting them to a uniform distribution; and prior-sampling, where we deliberately construct training
data to fit a prior distribution. One way to estimate such a prior is by looking at the distributions of classes
in the gold-standard training data that we have. In the case where gold-standard data is not available, we
can use a heuristic to estimate the distributions of temporal classes based on domain knowledge or on
observations of the target corpora.
In summary, we generate the final training data according to the following steps. First, generate train-
ing data with the DS approach described before. Next, estimate class distributions from the (supervised)
training data. Then, sample examples from the generated DS data with the probability estimated from
the supervised training data (i.e., the empirical prior). Keep sampling the training examples until we
999
reach the target percentage of the DS data. Finally, use the sampled training data to train the multi-class
classifier.
Feature representation Both for the training, evaluation, and DS data, we extract the context sentence,
i..e, the sentence containing the relation and time expression t.
We normalize the context sentence as follows. First, we detect named entities within the sentence and
replace the mentions with their entity types (PERSON, ORGANIZATION, or LOCATION). Second, we
detect other time expressions within the context and normalize them with regard to the main time expres-
sion t, i.e., by normalizing them into TIME-LT and TIME-GT. The idea is to capture the relationships
between time expressions as features.
We extract lexical features from the normalized sentence. This comprises tokens surrounding the
query entity, related entity (slot filler), and time expression. We consider the following four models as
our feature representations:
Model-1: bag-of-words All tokens within the normalized sentences are used as features.
Model-2: context window All tokens within the proximity of 3 tokens from the query entity, related
entity, and time expression are used as features.
Model-3: context window with trigger words lexicon All tokens within the proximity of 3 token from
the query entity, related entity, and time expression are used as features. In addition, a list of
keywords which might indicate the beginning and ending of relationships are used as gazetteer
features. These list of keywords are expanded by using WordNet to extract related terms.
Model-4: context window with position All tokens within the proximity of 3 tokens from the query
entity, related entity, and time expression are used as features. Rather than considered as bag-of-
words tokens, the positions of word occurrences are now taken into account as features.
4 Experimental Setup
We introduce the dataset and the setup of our experiments. Before that we formulate our research ques-
tions as these dictate our further choices.
Research questions We aim to answer the following research questions:
RQ1 How does a purely supervised approach with different features and learning algorithms perform
on the task of temporal evidence classification?
RQ2 How does the performance of a distant supervision approach compare to that of a supervised
learning approach on the task of temporal evidence classification?
RQ3 How does the performance of a prior-informed distant supervision approach compare to that of a
basic distant supervision approach on the task of temporal evidence classification?
RQ4 How do the approaches listed above compare in terms of their performance on the end-to-end
temporal relation extraction task?
Corpora and knowledge base We use the TAC 2011 document collection, which contains 1.7M docu-
ments, consisting of news wires, web texts, broadcast news, and broadcast conversation. We use a recent
version of Freebase (October 2013) as our knowledge base and retrieve the latest version of Wikipedia
as our distant supervision corpus.
Ground truth We use the TAC-KBP 2011 Temporal Slot Filling Task dataset (Ji et al., 2011) as the
ground truth in our experiments. The ground truth comes in two forms: intermediate evidence (with
classification labels) and tuples (boundaries of each relation). We use the intermediate evidence to eval-
uate our temporal evidence classification framework. We later use the provided tuples to evaluate the
end-to-end result.
The dataset contains 173 examples in the training set and 757 examples in the evaluation set. The
distribution of the classes is shown in Table 2.
1000
Class Training Evaluation DS Training
WITHIN 66 357 6,129
BEGINNING 59 217 22,508
ENDING 30 110 16,775
BEFORE 9 45 24,932
AFTER 9 28 12,499
Table 2: Class distribution statistics.
Evaluation metric We use F1 as the main evaluation metric for the temporal evidence classification
task. For the end-to-end temporal information extraction task, we use the evaluation metric proposed in
TAC-KBP 2011, i.e., the Q score. Given a relation r and the ground truth interval tuple G
r
, Q(T
r
), the
quality score of a tuple T
r
returned by system S is computed as follows:
Q(T
r
) =
1
4
4
P
i=1
1
1+d
i
,
where d
i
is the absolute difference between T
i
in system response and the ground truth tuple G
i
(mea-
sured in years). To obtain an overall systemQ score, we average theQ scores obtained from each relation
tuple returned.
Experiments We run four contrastive experiments. In Experiment 1, we contrast the performance on
the temporal evidence classification task of the different choices for our supervised methods (Model-1,
-2, -3, -4), using either Support Vector Machine, Naive Bayes, Random Forest, or Gradient Boosted
Regression Tree. In Experiment 2 we examine our distant supervision method and contrast its perfor-
mance with the supervised methods from Experiment 1. In Experiment 3, we contrast different sampling
methods for our distant supervision method.
In Experiment 4 we consider the overall performance on the temporal relation extraction task of our
methods; in this experiment we use three ?oracle runs? that we have not introduced yet: first, the Label-
Oracle run uses the actual temporal classification label from the ground truth, use these ground truth
label to aggregate the evidence and create the temporal tuples, and compute the end-to-end score; second,
Within-Oracle assigns all temporal evidence to the WITHIN class; third, Nil-Baseline is a lower-bound
run that assigns NIL to every element of the temporal tuples.
We use the implementations of the learning algorithms in the Scikit-learn machine learning package
(Pedregosa et al., 2011).
5 Results and Discussion
We present the outcomes of the four experiments specified in the previous section.
5.1 Preliminary experiment
To answer RQ1, How does the performance of the supervised learning approaches on the temporal
evidence classification task vary with different representations and learning algorithms?, we start with
a preliminary experiment. The aim of this experiment is to get an idea of the classification performance
with a purely supervised approach. The results are shown in Table 3.
As shown in Table 3, Model-4 with the SVM and NB classifiers achieves the best overall performance.
There seems to be a gradual increase in performance from the simpler to the more complex model with
SVM and NB classifiers, with the exception of RF. Interestingly, GBRT seems only slightly affected by
the different choice of model in this supervised setting.
5.2 Distant supervision experiments
Next, we evaluate the distant supervision approach. We aim to answer RQ2, How does the performance
of the distant supervision approach compare to that of the supervised learning approach? We generate
1001
Model SVM NB RF GBRT
Model-1 0.405 0.361 0.402 0.422
Model-2 0.409 0.417 0.354 0.420
Model-3 0.412 0.418 0.361 0.420
Model-4 0.426 0.424 0.241 0.422
Table 3: Experiment 1. Supervised approaches to temporal evidence classification.
training examples with the approach described in Section 3, and use the full generated training data to
train SVM and Naive Bayes classifiers with the same representation models that we use in the previous
experiments. The results are shown in Table 4.
Model Supervised DS DS-uniform DS-prior
Model-1 SVM 0.405 0.212 0.379 0.408
Model-2 SVM 0.409 0.185 0.389 0.450
Model-3 SVM 0.412 0.183 0.384 0.452
Model-4 SVM 0.426 0.200 0.400 0.463
Model-1 NB 0.361 0.413 0.379 0.431
Model-2 NB 0.417 0.299 0.372 0.451
Model-3 NB 0.418 0.300 0.368 0.446
Model-4 NB 0.424 0.270 0.400 0.486
Model-1 RF 0.402 0.162 0.406 0.397
Model-2 RF 0.354 0.177 0.399 0.418
Model-3 RF 0.361 0.176 0.391 0.403
Model-4 RF 0.241 0.171 0.399 0.446
Model-1 GBRT 0.422 0.142 0.316 0.344
Model-2 GBRT 0.420 0.137 0.343 0.418
Model-3 GBRT 0.420 0.138 0.343 0.403
Model-4 GBRT 0.422 0.140 0.399 0.433
Table 4: Experiment 2 and 3. Supervised, distant supervision, and distant supervision with sampling
approaches to temporal evidence classification.
We observe that the distant supervision approach trained on the full set of generated examples (the
column labeled ?DS?) performs poorly, well below the supervised approach. We hypothesize that the
accuracy drops due to the amount of noise generated with our distant supervision assumption trained
from full data, and different class distribution statistics.
In Section 3, we proposed our prior-sampling approach for distant supervision. The next experiment
is meant to answer RQ3, How does the performance of our prior-informed distant supervision approach
compare to that of the basic distant supervision approaches? We sample 20 percent of the generated
examples datasets with the following strategies: uniform and prior. The results are also shown in Table 4,
in the columns labeled ?DS-uniform? and ?DS-prior,? respectively.
By observing the results in Table 4, we notice that distant supervision with prior sampling performs
the best, for every combination of model and classification method. Uniform sampling already helps
in improving the performance, and prior sampling successfully boosts the performance of the basic
distant supervision (for all four models) further. Distant supervision with prior sampling also performs
consistently better than the supervised approaches (Table 3) in many cases?interestingly, for GBRT,
DS-prior only outperforms the supervised methods with sufficiently complex queries (Model-4 GBRT).
1002
5.3 End-to-end experiments
Next, we answer RQ4. That is, we consider how the classification performance on temporal evidence
classification affects the end-to-end result. We take the best performing models from the previous exper-
iments and evaluate their end-to-end scores. The results are shown in Table 5.
1
Model Avg-Q F1
Label-Oracle 0.925 1.000
Within-Oracle 0.676 0.302
Nil-Baseline 0.393 N/A
Supervised
Model-4 SVM 0.657 0.426
Model-4 NB 0.648 0.424
Model-4 RF 0.573 0.241
Model-4 GBRT 0.649 0.422
Distant supervision
Model-4 SVM 0.669 0.463
Model-4 NB 0.679 0.486
Model-4 RF 0.653 0.446
Model-4 GBRT 0.669 0.433
Table 5: Experiment 4. End-to-end scores (Avg-Q) next to F1 scores for temporal evidence classification.
From Table 5, we see that Model-4 RF (F1 on temporal evidence classification 0.446) and Model-4
GBRT (F1 on temporal evidence classification 0.433) translate into 0.653 and 0.669, respectively, in
terms of Q-score. This means that the misclassifications that Model-4 RF produces have a larger impact
than those of Model-4 GBRT. However, the difference in performance is not large.
The evaluation of this end-to-end task is important because not every misclassification has a similar
cost. Misclassification of class A into class B can result in a huge increase/decrease in performance. First,
the classification performance does not directly map to the end-to-end score. Second, several relations
have more pieces of evidence than others; performing misclassifications on relations that have a lot of
supporting evidence would probably have less effect on the final score.
The state of the art performance, using distant supervision (Li et al., 2012), achieves an end-to-end
Avg-Q score of 0.678 (on training data), where we achieve 0.679 (on evaluation data). However, our
scores are not directly comparable since we reduce the number of classes (and the amount of evidence)
in our evaluation. It is important to note that Li et al. (2012) use a complex combination of flat and
structured features as well as the web, where we use relatively simple features with Wikipedia and prior
sampling.
Furthermore, our approach manages to achieve the same level of end-to-end performance as the
Within-Oracle run, while achieving a significantly better F-score. More pieces of evidence were ac-
tually classified correctly, though this was not reflected directly in the end-to-end score due to issues
described above.
5.4 Error Analysis
We proceed to analyse parts of our end-to-end results to see what is causing errors in the temporal
evidence classification task. We found several common problems.
Semantic inference Some problems had to do with the fact that several snippets require semantic
inference. The fact that someone dies effectively ends any relationships that this person had. Another
example is when someone marries someone (AmarriesC), and this beginning of relationships effectively
1
As the Nil-Baseline is applied directly to the final tuples rather than the classification labels, there are is no F1 score for
this run.
1003
means the end of relationships for previous relations (A and B). A more complex method to deal with
this type of semantic inference is needed, simple classification does not work so well. Here is an example:
Angela Merkel is married to Joachim Sauer, a professor of chemistry at Berlin?s Humboldt
University, since 1998. Divorced from Ulrich Merkel. No children.
For this example the fact is that the time expression 1998 happens after with regard to the spouse relation
between Angela Merkel and Ulrich Merkel.
Concise temporal representations Newspaper articles contain lots of temporal information in a con-
cise way. For example in the form (X?Y). This implicit interval range is not expressed in a lexical context
but rather with symbolic conventions. In several articles, the information encoded is almost tabular rather
than expressed in explicitly. For example:
Elected as german chancellor Nov. 22, 2005. Chairwoman, christian democratic union,
2000-present. Chairwoman, christian democratic parliamentary group, 2002?2005.
Complex time-inference BEFORE and AFTER are especially tricky to deal with because they require
additional inference. Even if a passage contains the word after, the time expression linked to it would
probably contain the before relation.
He was called up by the Army in the spring of 1944, after marrying bea silverman in 1943,
and was sent to The Philippines.
For the above example, 1943 happens before the ?person joined the Army? event.
We observe quite a number of these cases on the evaluation data. Furthermore, the lack of context on
some examples and evidence that is scattered around multiple sentences complicates the problem even
more. Because of semantic and implicit evidence, temporal evidence classification remains a challenging
task. In order to achieve a better absolute performance, collective classification/inference of evidence
seems an interesting option.
6 Conclusion
We have presented a distant-supervision approach to temporal evidence classification. The main feature
of our distant supervision approach is that we consider the prior distribution of classes in the target do-
main in order to better model the task. We show that our prior-informed distant supervision approach
manages to outperform a purely supervised approach. Our method also achieves state-of-the-art perfor-
mance on end-to-end temporal relation extraction with fewer and simpler features than previous work.
Our error analysis on the temporal evidence classification task revealed several issues that inform our
future work aimed at further improving the performance on the subtask of temporal evidence classifica-
tion, and the overall temporal relation extraction task. In particular, we intend to deal with the challenging
aspect of semantic inference over relations found in the evidence passage. Another interesting direction
that we aim to tackle is dealing with evidence that is scattered across multiple sentences.
Acknowledgements
This research was supported by the European Communitys Seventh Framework Programme (FP7/2007-
2013) under grant agreements nrs 288024 and 312827, the Netherlands Organisation for Scientific Re-
search (NWO) under project nrs 727.011.005, 612.001.116, HOR-11-10, 640.006.013, the Center for
Creation, Content and Technology (CCCT), the QuaMerdes project funded by the CLARIN-nl pro-
gram, the TROVe project funded by the CLARIAH program, the Dutch national program COMMIT,
the ESF Research Network Program ELIAS, the Elite Network Shifts project funded by the Royal Dutch
Academy of Sciences (KNAW), the Netherlands eScience Center under project nr 027.012.105, the Ya-
hoo! Faculty Research and Engagement Program, the Microsoft Research PhD program, and the HPC
Fund.
1004
References
Razvan C. Bunescu and Raymond J. Mooney. 2007. Learning to extract relations from the web using minimal
supervision. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics
(ACL?07), Prague, Czech Republic, June.
Amev Burman, Arun Jayapal, Sathish Kannan, Ayman Kavilikatta, Madhu abd Alhelbawy, Leon Derczynski,
and Robert Gauzauskas. 2011. USFD at KBP 2011: Entity linking, slot filling and temporal bounding. In
Proceedings of the TAC-KBP 2011 Workshop. NIST.
Lorna Byrne and John Dunnion. 2011. UCD IIRG at tac 2011. In Proceedings of the TAC-KBP 2011 Workshop.
NIST.
Guillermo Garrido, Anselmo Pe?nas, Bernardo Cabaleiro, and
?
Alvaro Rodrigo. 2012. Temporally anchored relation
extraction. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long
Papers - Volume 1, ACL ?12, pages 107?116, Stroudsburg, PA, USA. Association for Computational Linguistics.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-based
weak supervision for information extraction of overlapping relations. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ?11,
pages 541?550, Stroudsburg, PA, USA. Association for Computational Linguistics.
Heng Ji, Ralph Grishman, and Hoa Trang Dang. 2011. Overview of the TAC 2011 knowledge base population
task. In Proceedings of the TAC-KBP 2011 Workshop. NIST.
Heng Ji, Taylor Cassidy, Qi Li, and Suzanne Tamang. 2013. Tackling representation, annotation and classification
challenges for temporal knowledge base population. Knowledge and Information Systems, pages 1?36.
Qi Li, Javier Artiles, Taylor Cassidy, and Heng Ji. 2012. Combining flat and structured approaches for temporal
slot filling or: how much to compress? In Proceedings of the 13th international conference on Computational
Linguistics and Intelligent Text Processing - Volume Part II, CICLing?12, pages 194?205, Berlin, Heidelberg.
Springer-Verlag.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without
labeled data. In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics:
Human Language Technologies (ACL ?09), pages 1003?1011, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Math-
ieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cour-
napeau, Mattheiu Brucher, Matthieu Perrot, and
?
Edouard Duchesnay. 2011. Scikit-learn: Machine learning in
Python. Journal of Machine Learning Research, 12:2825?2830.
James Pustejovsky, Jos?e Casta?no, Robert Ingria, Roser Saur??, Robert Gaizauskas, Andrea Setzer, and Graham
Katz. 2003. TimeML: Robust specification of event and temporal expressions in text. In Fifth International
Workshop on Computational Semantics (IWCS-5.
Ridho Reinanda, Daan Odijk, and Maarten de Rijke. 2013. Exploring entity associations over time. In SIGIR
2013 Workshop on Time-aware Information Access, August.
Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without
labeled text. In Proceedings of the 2010 European conference on Machine learning and knowledge discovery
in databases: Part III, ECML PKDD?10, pages 148?163, Berlin, Heidelberg. Springer-Verlag.
Alan Ritter, Luke Zettlemoyer, Mausam, and Oren Etzioni. 2003. Modeling missing data in distant supervision
for information extraction. In Transactions of the Association for Computational Linguistics, TACL?13, pages
367?378, Stroudsburg, PA, USA. Association for Computational Linguistics.
Avirup Sil and Silviu Cucerzan. 2014. Temporal scoping of relational facts based on Wikipedia data. In CoNLL:
Conference on Natural Language Learning.
Mihai Surdeanu, Sonal Gupta, John Bauer, David McClosky, Angel X. Chang, Valentin I. Spitkovsky, and Christo-
pher D. Manning. 2011. Stanford?s distantly-supervised slot-filling system. In Proceedings of the TAC-KBP
2011 Workshop. NIST.
1005
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, and Christopher D. Manning. 2012. Multi-instance multi-
label learning for relation extraction. In Proceedings of the 2012 Joint Conference on Empirical Methods in
Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ?12, pages
455?465, Stroudsburg, PA, USA. Association for Computational Linguistics.
Suzanne Tamang and Heng Ji. 2012. Relabeling distantly supervised training data for temporal knowledge base
population. In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale
Knowledge Extraction, AKBC-WEKEX ?12, pages 25?30, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Naushad UzZaman, Hector Llorens, James F. Allen, Leon Derczynski, Marc Verhagen, and James Pustejovsky.
2012. TempEval-3: Evaluating events, time expressions, and temporal relations. CoRR, abs/1206.5333.
Xu Wei, Raphael Hoffmann, Le Zhao, and Ralph Grishman. 2012. Filling knowledge base gaps for distant super-
vision of relation extraction. In Proceedings of the 50th Annual Meeting of the Association for Computational
Linguistics: Long Papers - Volume 1, ACL ?12, pages 825?834, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
1006
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 585?594,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Generating Focused Topic-specific Sentiment Lexicons
Valentin Jijkoun Maarten de Rijke Wouter Weerkamp
ISLA, University of Amsterdam, The Netherlands
jijkoun,derijke,w.weerkamp@uva.nl
Abstract
We present a method for automatically
generating focused and accurate topic-
specific subjectivity lexicons from a gen-
eral purpose polarity lexicon that allow
users to pin-point subjective on-topic in-
formation in a set of relevant documents.
We motivate the need for such lexicons
in the field of media analysis, describe
a bootstrapping method for generating a
topic-specific lexicon from a general pur-
pose polarity lexicon, and evaluate the
quality of the generated lexicons both
manually and using a TREC Blog track
test set for opinionated blog post retrieval.
Although the generated lexicons can be an
order of magnitude more selective than the
general purpose lexicon, they maintain, or
even improve, the performance of an opin-
ion retrieval system.
1 Introduction
In the area of media analysis, one of the key
tasks is collecting detailed information about opin-
ions and attitudes toward specific topics from var-
ious sources, both offline (traditional newspapers,
archives) and online (news sites, blogs, forums).
Specifically, media analysis concerns the follow-
ing system task: given a topic and list of docu-
ments (discussing the topic), find all instances of
attitudes toward the topic (e.g., positive/negative
sentiments, or, if the topic is an organization or
person, support/criticism of this entity). For every
such instance, one should identify the source of
the sentiment, the polarity and, possibly, subtopics
that this attitude relates to (e.g., specific targets
of criticism or support). Subsequently, a (hu-
man) media analyst must be able to aggregate
the extracted information by source, polarity or
subtopics, allowing him to build support/criticism
networks etc. (Altheide, 1996). Recent advances
in language technology, especially in sentiment
analysis, promise to (partially) automate this task.
Sentiment analysis is often considered in the
context of the following two tasks:
? sentiment extraction: given a set of textual
documents, identify phrases, clauses, sen-
tences or entire documents that express atti-
tudes, and determine the polarity of these at-
titudes (Kim and Hovy, 2004); and
? sentiment retrieval: given a topic (and possi-
bly, a list of documents relevant to the topic),
identify documents that express attitudes to-
ward this topic (Ounis et al, 2007).
How can technology developed for sentiment
analysis be applied to media analysis? In order
to use a sentiment extraction system for a media
analysis problem, a system would have to be able
to determine which of the extracted sentiments are
actually relevant, i.e., it would not only have to
identify specific targets of all extracted sentiments,
but also decide which of the targets are relevant
for the topic at hand. This is a difficult task, as
the relation between a topic (e.g., a movie) and
specific targets of sentiments (e.g., acting or spe-
cial effects in the movie) is not always straight-
forward, in the face of ubiquitous complex lin-
guistic phenomena such as referential expressions
(?. . . this beautifully shot documentary?) or bridg-
ing anaphora (?the director did an excellent jobs?).
In sentiment retrieval, on the other hand, the
topic is initially present in the task definition, but
it is left to the user to identify sources and targets
of sentiments, as systems typically return a list
of documents ranked by relevance and opinion-
atedness. To use a traditional sentiment retrieval
system in media analysis, one would still have to
manually go through ranked lists of documents re-
turned by the system.
585
To be able to support media analysis, we need to
combine the specificity of (phrase- or word-level)
sentiment analysis with the topicality provided by
sentiment retrieval. Moreover, we should be able
to identify sources and specific targets of opinions.
Another important issue in the media analysis
context is evidence for a system?s decision. If the
output of a system is to be used to inform actions,
the system should present evidence, e.g., high-
lighting words or phrases that indicate a specific
attitude. Most modern approaches to sentiment
analysis, however, use various flavors of classifi-
cation, where decisions (typically) come with con-
fidence scores, but without explicit support.
In order to move towards the requirements of
media analysis, in this paper we focus on two of
the problems identified above: (1) pinpointing ev-
idence for a system?s decisions about the presence
of sentiment in text, and (2) identifying specific
targets of sentiment.
We address these problems by introducing a
special type of lexical resource: a topic-specific
subjectivity lexicon that indicates specific relevant
targets for which sentiments may be expressed; for
a given topic, such a lexicon consists of pairs (syn-
tactic clue, target). We present a method for au-
tomatically generating a topic-specific lexicon for
a given topic and query-biased set of documents.
We evaluate the quality of the lexicon both manu-
ally and in the setting of an opinionated blog post
retrieval task. We demonstrate that such a lexi-
con is highly focused, allowing one to effectively
pinpoint evidence for sentiment, while being com-
petetive with traditional subjectivity lexicons con-
sisting of (a large number of) clue words.
Unlike other methods for topic-specific senti-
ment analysis, we do not expand a seed lexicon.
Instead, we make an existing lexicon more fo-
cused, so that it can be used to actually pin-point
subjectivity in documents relevant to a given topic.
2 Related Work
Much work has been done in sentiment analy-
sis. We discuss related work in four parts: sen-
timent analysis in general, domain- and target-
specific sentiment analysis, product review mining
and sentiment retrieval.
2.1 Sentiment analysis
Sentiment analysis is often seen as two separate
steps for determining subjectivity and polarity.
Most approaches first try to identify subjective
units (documents, sentences), and for each of these
determine whether it is positive or negative. Kim
and Hovy (2004) select candidate sentiment sen-
tences and use word-based sentiment classifiers
to classify unseen words into a negative or posi-
tive class. First, the lexicon is constructed from
WordNet: from several seed words, the structure
of WordNet is used to expand this seed to a full
lexicon. Next, this lexicon is used to measure the
distance between unseen words and words in the
positive and negative classes. Based on word sen-
timents, a decision is made at the sentence level.
A similar approach is taken by Wilson et al
(2005): a classifier is learnt that distinguishes be-
tween polar and neutral sentences, based on a prior
polarity lexicon and an annotated corpus. Among
the features used are syntactic features. After this
initial step, the sentiment sentences are classified
as negative or positive; again, a prior polarity lexi-
con and syntactic features are used. The authors
later explored the difference between prior and
contextual polarity (Wilson et al, 2009): words
that lose polarity in context, or whose polarity is
reversed because of context.
Riloff and Wiebe (2003) describe a bootstrap-
ping method to learn subjective extraction pat-
terns that match specific syntactic templates, using
a high-precision sentence-level subjectivity clas-
sifier and a large unannotated corpus. In our
method, we bootstrap from a subjectivity lexi-
cion rather than a classifier, and perform a topic-
specific analysis, learning indicators of subjectiv-
ity toward a specific topic.
2.2 Domain- and target-specific sentiment
The way authors express their attitudes varies
with the domain: An unpredictable movie can be
positive, but unpredictable politicians are usually
something negative. Since it is unrealistic to con-
struct sentiment lexicons, or manually annotate
text for learning, for every imaginable domain or
topic, automatic methods have been developed.
Godbole et al (2007) aim at measuring over-
all subjectivity or polarity towards a certain entity;
they identify sentiments using domain-specific
lexicons. The lexicons are generated from man-
ually selected seeds for a broad domain such as
Health or Business, following an approach simi-
lar to (Kim and Hovy, 2004). All named entites
in a sentence containing a clue from a lexicon are
586
considered targets of sentiment for counting. Be-
cause of the data volume, no expensive linguistic
processing is performed.
Choi et al (2009) advocate a joint topic-
sentiment analysis. They identify ?sentiment top-
ics,? noun phrases assumed to be linked to a sen-
timent clue in the same expression. They address
two tasks: identifying sentiment clues, and clas-
sifying sentences into positive, negative, or neu-
tral. They start by selecting initial clues from Sen-
tiWordNet, based on sentences with known polar-
ity. Next, the sentiment topics are identified, and
based on these sentiment topics and the current list
of clues, new potential clues are extracted. The
clues can be used to classifiy sentences.
Fahrni and Klenner (2008) identify potential
targets in a given domain, and create a target-
specific polarity adjective lexicon. To this end,
they find targets using Wikipedia, and associated
adjectives. Next, the target-specific polarity of ad-
jectives is detemined using Hearst-like patterns.
Kanayama and Nasukawa (2006) introduce po-
lar atoms: minimal human-understandable syn-
tactic structures that specify polarity of clauses.
The goal is to learn new domain-specific polar
atoms, but these are not target-specific. They
use manually-created syntactic patterns to identify
atoms and coherency to determine polarity.
In contrast to much of the work in the literature,
we need to specialize subjectivity lexicons not for
a domain and target, but for ?topics.?
2.3 Product features and opinions
Much work has been carried out for the task of
mining product reviews, where the goal is to iden-
tify features of specific products (such as picture,
zoom, size, weight for digital cameras) and opin-
ions about these specific features in user reviews.
Liu et al (2005) describe a system that identifies
such features via rules learned from a manually
annotated corpus of reviews; opinions on features
are extracted from the structure of reviews (which
explicitly separate positive and negative opinions).
Popescu and Etzioni (2005) present a method
that identifies product features for using corpus
statistics, WordNet relations and morphological
cues. Opinions about the features are extracted us-
ing a hand-crafted set of syntactic rules.
Targets extracted in our method for a topic are
similar to features extracted in review mining for
products. However, topics in our setting go be-
yond concrete products, and the diversity and gen-
erality of possible topics makes it difficult to ap-
ply such supervised or thesaurus-based methods to
identify opinion targets. Moreover, in our method
we directly use associations between targets and
opinions to extract both.
2.4 Sentiment retrieval
At TREC, the Text REtrieval Conference, there
has been interest in a specific type of sentiment
analysis: opinion retrieval. This interest materi-
alized in 2006 (Ounis et al, 2007), with the opin-
ionated blog post retrieval task. Finding blog posts
that are not just about a topic, but also contain an
opinion on the topic, proves to be a difficult task.
Performance on the opinion-finding task is domi-
nated by performance on the underlying document
retrieval task (the topical baseline).
Opinion finding is often approached as a two-
stage problem: (1) identify documents relevant to
the query, (2) identify opinions. In stage (2) one
commonly uses either a binary classifier to distin-
guish between opinionated and non-opinionated
documents or applies reranking of the initial result
list using some opinion score. Opinion add-ons
show only slight improvements over relevance-
only baselines.
The best performing opinion finding system at
TREC 2008 is a two-stage approach using rerank-
ing in stage (2) (Lee et al, 2008). The authors
use SentiWordNet and a corpus-derived lexicon
to construct an opinion score for each post in an
initial ranking of blog posts. This opinion score
is combined with the relevance score, and posts
are reranked according to this new score. We de-
tail this approach in Section 6. Later, the authors
use domain-specific opinion indicators (Na et al,
2009), like ?interesting story? (movie review), and
?light? (notebook review). This domain-specific
lexicon is constructed using feedback-style learn-
ing: retrieve an initial list of documents and use
the top documents as training data to learn an opin-
ion lexicon. Opinion scores per document are then
computed as an average of opinion scores over
all its words. Results show slight improvements
(+3%) on mean average precision.
3 Generating Topic-Specific Lexicons
In this section we describe how we generate a lex-
icon of subjectivity clues and targets for a given
topic and a list of relevant documents (e.g., re-
587
Extract all 
syntactic contexts 
of clue words 
Background 
corpus
Topic-independent 
subjectivity lexicon
Relevant docs
Topic
For each clue 
word, select D 
contexts with 
highest entropy
List of syntactic clues:
(clue word, syn. context)
Extract all 
occurrences 
endpoints of 
syntactic clues 
Extract all 
occurrences 
endpoints of 
syntactic clues 
Potential targets in 
background corpus
Potential targets in 
relevant doc. list
Compare frequencies 
using chi-square; 
select top T targets
List of T targets
For each target, 
find syn. clues it 
co-occurs with
Topic-specific lexicon of tuples:
(syntactic clue, target)
Step 1
Step 2
Step 3
Figure 1: Our method for learning a topic-
dependent subjectivity lexicon.
trieved by a search engine for the topic). As an ad-
ditional resource, we use a large background cor-
pus of text documents of a similar style but with
diverse subjects; we assume that the relevant doc-
uments are part of this corpus as well. As the back-
ground corpus, we used the set of documents from
the assessment pools of TREC 2006?2008 opin-
ion retrieval tasks (described in detail in section 4).
We use the Stanford lexicalized parser1 to extract
labeled dependency triples (head, label, modifier).
In the extracted triples, all words indicate their cat-
egory (noun, adjective, verb, adverb, etc.) and are
normalized to lemmas.
Figure 1 provides an overview of our method;
below we describe it in more detail.
3.1 Step 1: Extracting syntactic contexts
We start with a general domain-independent prior
polarity lexicon of 8,821 clue words (Wilson et al,
2005). First, we identify syntactic contexts in
which specific clue words can be used to express
1http://nlp.stanford.edu/software/
lex-parser.shtml
attitude: we try to find how a clue word can be syn-
tactically linked to targets of sentiments. We take a
simple definition of the syntactic context: a single
labeled directed dependency relation. For every
clue word, we extract all syntactic contexts, i.e.,
all dependencies, in which the word is involved
(as head or as modifier) in the background corpus,
along with their endpoints. Table 1 shows exam-
ples of clue words and contexts that indicate sen-
timents. For every clue, we only select those con-
texts that exhibit a high entropy among the lemmas
at the other endpoint of the dependencies. E.g.,
in our background corpus, the verb to like occurs
97,179 times with a nominal subject and 52,904
times with a direct object; however, the entropy of
lemmas of the subjects is 4.33, compared to 9.56
for the direct objects. In other words, subjects of
like are more ?predictable.? Indeed, the pronoun
I accounts for 50% of subjects, followed by you
(14%), they (4%), we (4%) and people (2%). The
most frequent objects of like are it (12%), what
(4%), idea (2%), they (2%). Thus, objects of to
like will be preferred by the method.
Our entropy-driven selection of syntactic con-
texts of a clue word is based on the following as-
sumption:
Assumption 1: In text, targets of sentiments
are more diverse than sources of sentiments
or other accompanying attributes such as lo-
cation, time, manner, etc. Therefore targets
exhibit higher entropy than other attributes.
For every clue word, we select the top D syntac-
tic contexts whose entropy is at least half of the
maximum entropy for this clue.
To summarize, at the end of Step 1 of our
method, we have extracted a list of pairs (clue
word, syntactic context) such that for occurrences
of the clue word, the words at the endpoint of the
syntactic dependency are likely to be targets of
sentiments. We call such a pair a syntactic clue.
3.2 Step 2: Selecting potential targets
Here, we use the extracted syntantic clues to iden-
tify words that are likely to serve as specific tar-
gets for opinions about the topic in the relevant
documents. In this work we only consider individ-
ual words as potential targets and leave exploring
other options (e.g., NPs and VPs as targets) for fu-
ture work. In extracting targets, we rely on the
following assumption:
588
Clue word Syntactic context Target Example
to like has direct object u2 I do still like U2 very much
to like has clausal complement criticize I don?t like to criticize our intelligence services
to like has about-modifier olympics That?s what I like about Winter Olympics
terrible is adjectival modifier of idea it?s a terrible idea to recall judges for...
terrible has nominal subject shirt And Neil, that shirt is terrible!
terrible has clausal complement can It is terrible that a small group of extremists can . . .
Table 1: Examples of subjective syntactic contexts of clue words (based on Stanford dependencies).
Assumption 2: The list of relevant documents
contains a substantial number of documents
on the topic which, moreover, contain senti-
ments about the topic.
We extract all endpoints of all occurrences of the
syntactic clues in the relevant documents, as well
as in the background corpus. To identify potential
attitude targets in the relevant documents, we com-
pare their frequency in the relevant documents to
the frequency in the background corpus using the
standard ?2 statistics. This technique is based on
the following assumption:
Assumption 3: Sentiment targets related to
the topic occur more often in subjective con-
text in the set of relevant documents, than
in the background corpus. In other words,
while the background corpus contains senti-
ments towards very diverse subjects, the rel-
evant documents tend to express attitudes re-
lated to the topic.
For every potential target, we compute the ?2-
score and select the top T highest scoring targets.
As the result of Steps 1 and 2, as candidate tar-
gets for a given topic, we only select words that oc-
cur in subjective contexts, and that do so more of-
ten than we would normally expect. Table 2 shows
examples of extracted targets for three TREC top-
ics (see below for a description of our experimen-
tal data).
3.3 Step 3: Generating topic-specific lexicons
In the last step of the method, we combine clues
and targets. For each target identified in Step 2,
we take all syntactic clues extracted in Step 1 that
co-occur with the target in the relevant documents.
The resulting list of triples (clue word, syntactic
context, target) constitute the lexicon. We conjec-
ture that an occurrence of a lexicon entry in a text
indicates, with reasonable confidence, a subjective
attitude towards the target.
Topic ?Relationship between Abramoff and Bush?
abramoff lobbyist scandal fundraiser bush fund-raiser re-
publican prosecutor tribe swirl corrupt corruption norquist
democrat lobbying investigation scanlon reid lawmaker
dealings president
Topic ?MacBook Pro?
macbook laptop powerbook connector mac processor note-
book fw800 spec firewire imac pro machine apple power-
books ibook ghz g4 ata binary keynote drive modem
Topic: ?Super Bowl ads?
ad bowl commercial fridge caveman xl endorsement adver-
tising spot advertiser game super essential celebrity payoff
marketing publicity brand advertise watch viewer tv football
venue
Table 2: Examples of targets extracted at Step 2.
4 Data and Experimental Setup
We consider two types of evaluation. In the next
section, we examine the quality of the lexicons
we generate. In the section after that we evaluate
lexicons quantitatively using the TREC Blog track
benchmark.
For extrinsic evaluation we apply our lexi-
con generation method to a collection of doc-
uments containing opinionated utterances: blog
posts. The Blogs06 collection (Macdonald and
Ounis, 2006) is a crawl of blog posts from 100,649
blogs over a period of 11 weeks (06/12/2005?
21/02/2006), with 3,215,171 posts in total. Be-
fore indexing the collection, we perform two pre-
processing steps: (i) when extracting plain text
from HTML, we only keep block-level elements
longer than 15 words (to remove boilerplate mate-
rial), and (ii) we remove non-English posts using
TextCat2 for language detection. This leaves us
with 2,574,356 posts with 506 words per post on
average. We index the collection using Indri,3 ver-
sion 2.10.
TREC 2006?2008 came with the task of opin-
ionated blog post retrieval (Ounis et al, 2007).
For each year a set of 50 topics was created, giv-
2http://odur.let.rug.nl/?vannoord/
TextCat/
3http://www.lemurproject.org/indri/
589
ing us 150 topics in total. Every topic comes with
a set of relevance judgments: Given a topic, a blog
post can be either (i) nonrelevant, (ii) relevant, but
not opinionated, or (iii) relevant and opinionated.
TREC topics consist of three fields (title, descrip-
tion, and narrative), of which we only use the title
field: a query of 1?3 keywords.
We use standard TREC evaluation measures for
opinion retrieval: MAP (mean average precision),
R-precision (precision within the top R retrieved
documents, where R is the number of known rel-
evant documents in the collection), MRR (mean
reciprocal rank), P@10 and P@100 (precision
within the top 10 and 100 retrieved documents).
In the context of media analysis, recall-oriented
measures such as MAP and R-precision are more
meaningful than the other, early precision-oriented
measures. Note that for the opinion retrieval task
a document is considered relevant if it is on topic
and contains opinions or sentiments towards the
topic.
Throughout Section 6 below, we test for signif-
icant differences using a two-tailed paired t-test,
and report on significant differences for ? = 0.01
(N and H), and ? = 0.05 (M and O).
For the quantative experiments in Section 6 we
need a topical baseline: a set of blog posts po-
tentially relevant to each topic. For this, we use
the Indri retrieval engine, and apply the Markov
Random Fields to model term dependencies in the
query (Metzler and Croft, 2005) to improve topi-
cal retrieval. We retrieve the top 1,000 posts for
each query.
5 Qualitative Analysis of Lexicons
Lexicon size (the number of entries) and selectiv-
ity (how often entries match in text) of the gen-
erated lexicons vary depending on the parame-
ters D and T introduced above. The two right-
most columns of Table 4 show the lexicon size
and the average number of matches per topic. Be-
cause our topic-specific lexicons consist of triples
(clue word, syntactic context, target), they actu-
ally contain more words than topic-independent
lexicons of the same size, but topic-specific en-
tries are more selective, which makes the lexicon
more focused. Table 3 compares the application
of topic-independent and topic-specific lexicons to
on-topic blog text.
We manually performed an explorative error
analysis on a small number of documents, anno-
There are some tragic mo-
ments like eggs freezing ,
and predators snatching the
females and little ones-you
know the whole NATURE
thing ... but this movie is
awesome
There are some tragic mo-
ments l ike eggs freezing ,
and predators snatching the
females and little ones-you
know the whole NATURE
thing ... but this movie is
awesome
Saturday was more errands,
then spent the evening with
Dad and Stepmum, and fi-
nallywas able to see March
of the Penguins, which
was wonderful. Christmas
Day was lovely, surrounded
by family, good food and
drink, and little L to play
with.
Saturday was more errands,
then spent the evening with
Dad and Stepmum, and fi-
nally was able to see March
of the Penguins, which
was wonderful. Christmas
Day was lovely, surrounded
by family, good food and
drink, and little L to play
with.
Table 3: Posts with highlighted targets (bold) and
subjectivity clues (blue) using topic-independent
(left) and topic-specific (right) lexicons.
tated using the smallest lexicon in Table 4 for the
topic ?March of the Pinguins.? We assigned 186
matches of lexicon entries in 30 documents into
four classes:
? REL: sentiment towards a relevant target;
? CONTEXT: sentiment towards a target that
is irrelevant to the topic due to context (e.g.,
opinion about a target ?film?, but refering to
a film different from the topic);
? IRREL: sentiment towards irrelevant target
(e.g., ?game? for a topic about a movie);
? NOSENT: no sentiment at all
In total only 8% of matches were manually clas-
sified as REL, with 62% classified as NOSENT,
23% as CONTEXT, and 6% as IRREL. On the
other hand, among documents assessed as opio-
nionated by TREC assessors, only 13% did not
contain matches of the lexicon entries, compared
to 27% of non-opinionated documents, which
does indicate that our lexicon does attempt to sep-
arate non-opinionated documents from opinion-
ated.
6 Quantitative Evaluation of Lexicons
In this section we assess the quality of the gen-
erated topic-specific lexicons numerically and ex-
trinsically. To this end we deploy our lexicons to
the task of opinionated blog post retrieval (Ounis
et al, 2007). A commonly used approach to this
task works in two stages: (1) identify topically rel-
evant blog posts, and (2) classify these posts as
being opinionated or not. In stage 2 the standard
590
approach is to rerank the results from stage 1, in-
stead of doing actual binary classification. We take
this approach, as it has shown good performance
in the past TREC editions (Ounis et al, 2007) and
is fairly straightforward to implement. We also ex-
plore another way of using the lexicon: as a source
for query expansion (i.e., adding new terms to the
original query) in Section 6.2. For all experiments
we use the collection described in Section 4.
Our experiments have two goals: to compare
the use of topic-independent and topic-specific
lexicons for the opinionated post retrieval task,
and to examine how different settings for the pa-
rameters of the lexicon generation affect the em-
pirical quality.
6.1 Reranking using a lexicon
To rerank a list of posts retrieved for a given topic,
we opt to use the method that showed best per-
formance at TREC 2008. The approach taken
by Lee et al (2008) linearly combines a (top-
ical) relevance score with an opinion score for
each post. For the opinion score, terms from a
(topic-independent) lexicon are matched against
the post content, and weighted with the probability
of term?s subjectivity. Finally, the sum is normal-
ized using the Okapi BM25 framework. The final
opinion score Sop is computed as in Eq. 1:
Sop(D) =
Opinion(D) ? (k1 + 1)
Opinion(D) + k1 ? (1 ? b +
b?|D|
avgdl )
, (1)
where k1, and b are Okapi parameters (set to their
default values k1 = 2.0, and b = 0.75), |D| is the
length of document D, and avgdl is the average
document length in the collection. The opinion
score Opinion(D) is calculated using Eq. 2:
Opinion(D) =
?
w?O
P (sub|w) ? n(w,D), (2)
where O is the set of terms in the sentiment lex-
icon, P (sub|w) indicates the probability of term
w being subjective, and n(w,D) is the number of
times term w occurs in document D. The opinion
scoring can weigh lexicon terms differently, using
P (sub|w); it normalizes scores to cancel out the
effect of varying document sizes.
In our experiments we use the method de-
scribed above, and plug in the MPQA polarity
lexicon.4 We compare the results of using this
4http://www.cs.pitt.edu/mpqa/
topic-independent lexicon to the topic-dependent
lexicons our method generates, which are also
plugged into the reranking of Lee et al (2008).
In addition to using Okapi BM25 for opinion
scoring, we also consider a simpler method. As
we observed in Section 5, our topic-specific lexi-
cons are more selective than the topic-independent
lexicon, and a simple number of lexicon matches
can give a good indication of opinionatedness of a
document:
Sop(D) = min(n(O,D), 10)/10, (3)
where n(O,D) is the number of matches of the
term of sentiment lexicon O in document D.
6.1.1 Results and observations
There are several parameters that we can vary
when generating a topic-specific lexicon and when
using it for reranking:
D: the number of syntactic contexts per clue
T : the number of extracted targets
Sop(D): the opinion scoring function.
?: the weight of the opinion score in the linear
combination with the relevance score.
Note that ? does not affect the lexicon creation,
but only how the lexicon is used in reranking.
Since we want to assess the quality of lexicons,
not in the opinionated retrieval performance as
such, we factor out ? by selecting the best setting
for each lexicon (including the topic-independent)
and each evaluation measure.
In Table 4 we present the results of evaluation
of several lexicons in the context of opinionated
blog post retrieval.
First, we note that reranking using all lexi-
cons in Table 4 significantly improves over the
relevance-only baseline for all evaluation mea-
sures. When comparing topic-specific lexicons to
the topic-independent one, most of the differences
are not statistically significant, which is surpris-
ing given the fact that most topic-specific lexicons
we evaluated are substantially smaller (see the two
rightmost columns in the table). The smallest lex-
icon in Table 4 is seven times more selective than
the general one, in terms of the number of lexicon
matches per document.
The only evaluation measure where the topic-
independent lexicon consistently outperforms
topic-specific ones, is Mean Reciprocal Rank that
depends on a single relevant opinionated docu-
ment high in a ranking. A possible explanation
591
Lexicon MAP R-prec MRR P@10 P@100 |lexicon| hits per doc
no reranking 0.2966 0.3556 0.6750 0.4820 0.3666 ? ?
topic-independent 0.3182 0.3776 0.7714 0.5607 0.3980 8,221 36.17
D T Sop
3 50 count 0.3191 0.3769 0.7276O 0.5547 0.3963 2,327 5.02
3 100 count 0.3191 0.3777 0.7416 0.5573 0.3971 3,977 8.58
5 50 count 0.3178 0.3775 0.7246O 0.5560 0.3931 2,784 5.73
5 100 count 0.3178 0.3784 0.7316O 0.5513 0.3961 4,910 10.06
all 50 count 0.3167 0.3753 0.7264O 0.5520 0.3957 4,505 9.34
all 100 count 0.3146 0.3761 0.7283O 0.5347O 0.3955 8,217 16.72
all 50 okapi 0.3129 0.3713 0.7247H 0.5333O 0.3833O 4,505 9.34
all 100 okapi 0.3189 0.3755 0.7162H 0.5473 0.3921 8,217 16.72
all 200 okapi 0.3229N 0.3803 0.7389 0.5547 0.3987 14,581 29.14
Table 4: Evaluation of topic-specific lexicons applied to the opinion retrieval task, compared to the topic-
independent lexicon. The two rightmost columns show the number of lexicon entries (average per topic)
and the number of matches of lexicon entries in blog posts (average for top 1,000 posts).
is that the large general lexicon easily finds a few
?obviously subjective? posts (those with heavily
used subjective words), but is not better at detect-
ing less obvious ones, as indicated by the recall-
oriented MAP and R-precision.
Interestingly, increasing the number of syntac-
tic contexts considered for a clue word (parame-
ter D) and the number of selected targets (param-
eter T ) leads to substantially larger lexicons, but
only gives marginal improvements when lexicons
are used for opinion retrieval. This shows that our
bootstrapping method is effective at filtering out
non-relevant sentiment targets and syntactic clues.
The evaluation results also show that the choice
of opinion scoring function (Okapi or raw counts)
depends on the lexicon size: for smaller, more fo-
cused lexicons unnormalized counts are more ef-
fective. This also confirms our intuition that for
small, focused lexicons simple presence of a sen-
timent clue in text is a good indication of subjec-
tivity, while for larger lexicons an overall subjec-
tivity scoring of texts has to be used, which can be
hard to interpret for (media analysis) users.
6.2 Query expansion with lexicons
In this section we evaluate the quality of targets
extracted as part of the lexicons by using them for
query expansion. Query expansion is a commonly
used technique in information retrieval, aimed at
getting a better representation of the user?s in-
formation need by adding terms to the original
retrieval query; for user-generated content, se-
lective query expansion has proved very benefi-
cial (Weerkamp et al, 2009). We hypothesize that
if our method manages to identify targets that cor-
respond to issues, subtopics or features associated
Run MAP P@10 MRR
Topical blog post retrieval
Baseline 0.4086 0.7053 0.7984
Rel. models 0.4017O 0.6867 0.7383H
Subj. targets 0.4190M 0.7373M 0.8470M
Opinion retrieval
Baseline 0.2966 0.4820 0.6750
Rel. models 0.2841H 0.4467H 0.5479H
Subj. targets 0.3075 0.5227N 0.7196
Table 5: Query expansion using relevance mod-
els and topic-specific subjectivity targets. Signifi-
cance tested against the baseline.
with the topic, the extracted targets should be good
candidates for query expansion. The experiments
described below test this hypothesis.
For every test topic, we select the 20 top-scoring
targets as expansion terms, and use Indri to re-
turn 1,000 most relevant documents for the ex-
panded query. We evaluate the resulting ranking
using both topical retrieval and opinionated re-
trieval measures. For the sake of comparison, we
also implemented a well-known query expansion
method based on Relevance Models (Lavrenko
and Croft, 2001): this method has been shown to
work well in many settings. Table 5 shows evalu-
ation results for these two query expansion meth-
ods, compared to the baseline retrieval run.
The results show that on topical retrieval query
expansion using targets significantly improves re-
trieval performance, while using relevance mod-
els actually hurts all evaluation measures. The
failure of the latter expansion method can be at-
tributed to the relatively large amount of noise
in user-generated content, such as boilerplate
592
material, timestamps of blog posts, comments
etc. (Weerkamp and de Rijke, 2008). Our method
uses full syntactic parsing of the retrieved doc-
uments, which might substantially reduce the
amount of noise since only (relatively) well-
formed English sentences are used in lexicon gen-
eration.
For opinionated retrieval, target-based expan-
sion also improves over the baseline, although the
differences are only significant for P@10. The
consistent improvement for topical retrieval sug-
gests that a topic-specific lexicon can be used both
for query expansion (as described in this section)
and for opinion reranking (as described in Sec-
tion 6.1). We leave this combination for future
work.
7 Conclusions and Future Work
We have described a bootstrapping method for de-
riving a topic-specific lexicon from a general pur-
pose polarity lexicon. We have evaluated the qual-
ity of generated lexicons both manually and using
a TREC Blog track test set for opinionated blog
post retrieval. Although the generated lexicons
can be an order of magnitude more selective, they
maintain, or even improve, the performance of an
opinion retrieval system.
As to future work, we intend to combine our
method with known methods for topic-specific
lexicon expansion (our method is rather concerned
with lexicon ?restriction?). Existing sentence-
or phrase-level (trained) sentiment classifiers can
also be used easily: when collecting/counting tar-
gets we can weigh them by ?prior? score provided
by such classifiers. We also want to look at more
complex syntactic patterns: Choi et al (2009) re-
port that many errors are due to exclusive use of
unigrams. We would also like to extend poten-
tial opinion targets to include multi-word phrases
(NPs and VPs), in addition to individual words.
Finally, we do not identify polarity yet: this can
be partially inherited from the initial lexicon and
refined automatically via bootstrapping.
Acknowledgements
This research was supported by the European
Union?s ICT Policy Support Programme as part
of the Competitiveness and Innovation Framework
Programme, CIP ICT-PSP under grant agreement
nr 250430, by the DuOMAn project carried out
within the STEVIN programme which is funded
by the Dutch and Flemish Governments under
project nr STE-09-12, and by the Netherlands Or-
ganisation for Scientific Research (NWO) under
project nrs 612.066.512, 612.061.814, 612.061.-
815, 640.004.802.
References
Altheide, D. (1996). Qualitative Media Analysis. Sage.
Choi, Y., Kim, Y., and Myaeng, S.-H. (2009). Domain-
specific sentiment analysis using contextual feature gen-
eration. In TSA ?09: Proceeding of the 1st international
CIKM workshop on Topic-sentiment analysis for mass
opinion, pages 37?44, New York, NY, USA. ACM.
Fahrni, A. and Klenner, M. (2008). Old Wine or Warm
Beer: Target-Specific Sentiment Analysis of Adjectives.
In Proc.of the Symposium on Affective Language in Hu-
man and Machine, AISB 2008 Convention, 1st-2nd April
2008. University of Aberdeen, Aberdeen, Scotland, pages
60 ? 63.
Godbole, N., Srinivasaiah, M., and Skiena, S. (2007). Large-
scale sentiment analysis for news and blogs. In Proceed-
ings of the International Conference on Weblogs and So-
cial Media (ICWSM).
Kanayama, H. and Nasukawa, T. (2006). Fully automatic lex-
icon expansion for domain-oriented sentiment analysis. In
EMNLP ?06: Proceedings of the 2006 Conference on Em-
pirical Methods in Natural Language Processing, pages
355?363, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Kim, S. and Hovy, E. (2004). Determining the sentiment of
opinions. In Proceedings of COLING 2004.
Lavrenko, V. and Croft, B. (2001). Relevance-based language
models. In SIGIR ?01: Proceedings of the 24th annual
international ACM SIGIR conference on research and de-
velopment in information retrieval.
Lee, Y., Na, S.-H., Kim, J., Nam, S.-H., Jung, H.-Y., and Lee,
J.-H. (2008). KLE at TREC 2008 Blog Track: Blog Post
and Feed Retrieval. In Proceedings of TREC 2008.
Liu, B., Hu, M., and Cheng, J. (2005). Opinion observer: an-
alyzing and comparing opinions on the web. In Proceed-
ings of the 14th international conference on World Wide
Web.
Macdonald, C. and Ounis, I. (2006). The TREC Blogs06
collection: Creating and analysing a blog test collection.
Technical Report TR-2006-224, Department of Computer
Science, University of Glasgow.
Metzler, D. and Croft, W. B. (2005). A markov random feld
model for term dependencies. In SIGIR ?05: Proceed-
ings of the 28th annual international ACM SIGIR con-
ference on research and development in information re-
trieval, pages 472?479, New York, NY, USA. ACM Press.
Na, S.-H., Lee, Y., Nam, S.-H., and Lee, J.-H. (2009). Im-
proving opinion retrieval based on query-specific senti-
ment lexicon. In ECIR ?09: Proceedings of the 31th Eu-
ropean Conference on IR Research on Advances in In-
formation Retrieval, pages 734?738, Berlin, Heidelberg.
Springer-Verlag.
Ounis, I., Macdonald, C., de Rijke, M., Mishne, G., and
Soboroff, I. (2007). Overview of the TREC 2006 blog
track. In The Fifteenth Text REtrieval Conference (TREC
2006). NIST.
Popescu, A.-M. and Etzioni, O. (2005). Extracting prod-
uct features and opinions from reviews. In Proceedings
of Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Process-
ing (HLT/EMNLP).
Riloff, E. and Wiebe, J. (2003). Learning extraction patterns
593
for subjective expressions. In Proceedings of the 2003
Conference on Empirical methods in Natural Language
Processing (EMNLP).
Weerkamp, W., Balog, K., and de Rijke, M. (2009). A gener-
ative blog post retrieval model that uses query expansion
based on external collections. In Joint conference of the
47th Annual Meeting of the Association for Computational
Linguistics and the 4th International Joint Conference on
Natural Language Processing of the Asian Federation of
Natural Language Processing (ACL-ICNLP 2009), Singa-
pore.
Weerkamp, W. and de Rijke, M. (2008). Credibility im-
proves topical blog post retrieval. In Proceedings of ACL-
08: HLT, page 923931, Columbus, Ohio. Association
for Computational Linguistics, Association for Computa-
tional Linguistics.
Wilson, T., Wiebe, J., and Hoffmann, P. (2005). Recognizing
contextual polarity in phrase-level sentiment analysis. In
HLT ?05: Proceedings of the conference on Human Lan-
guage Technology and Empirical Methods in Natural Lan-
guage Processing, pages 347?354, Morristown, NJ, USA.
Association for Computational Linguistics.
Wilson, T., Wiebe, J., and Hoffmann, P. (2009). Recog-
nizing contextual polarity: an exploration of features for
phrase-level sentiment analysis. Computational Linguis-
tics, 35(3):399?433.
594
Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics in a World of Social Media, pages 17?18,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Mining User Experiences from Online Forums: An Exploration?
Valentin Jijkoun Maarten de Rijke
Wouter Weerkamp
ISLA, University of Amsterdam
Science Park 107
1098 XG Amsterdam, The Netherlands
jijkoun,m.derijke,w.weerkamp@uva.nl
Paul Ackermans Gijs Geleijnse
Philips Research Europe
High Tech Campus 34
5656 AE Eindhoven, The Netherlands
paul.ackermans@philips.com
gijs.geleijnse@philips.com
1 Introduction
Recent years have shown a large increase in the
usage of content creation platforms?blogs, com-
munity QA sites, forums, etc.?aimed at the gen-
eral public.User generated data contains emotional,
opinionated, sentimental, and personal posts. This
characteristic makes it an interesting data source
for exploring new types of linguistic analysis, as is
demonstrated by research on, e.g., sentiment analy-
sis [4], opinion retrieval [3], and mood detection [1].
We introduce the task of experience mining. Here,
the goal is to gain insights into criteria that people
formulate to judge or rate a product or its usage.
These criteria can be formulated as the expectations
that people have of the product in advance (i.e., the
reasons to buy), but can also be expressed as reports
of experiences while using the product and compar-
isons with other products. We focus on the latter:
reports of experiences with products. In this paper,
we define the task, describe guidelines for manual
annotation and analyze linguistic features that can
be used in an automatic experience mining system.
2 Motivation
Our main use-case is user-centered design for prod-
uct development. User-centered design [2] is an in-
novation paradigm where users of a product are in-
volved in each step of the research and development
process. The first stage of the product design process
is to identify unmet needs and demands of users for
a specific product or a class of products. Forums,
?This research was supported by project STE-09-12 within
the STEVIN programme funded by the Dutch and Flemish gov-
ernments, and by the Netherlands Organisation for Scientific
Research (NWO) under projects 640.001.501, 640.002.501,
612.066.512, 612.061.814, 612.061.815, 640.004.802.
review sites, and mailing lists are platforms where
people share experiences about a subject they care
about. Although statements found in such platforms
may not always be representative for the general user
group, they can accelerate user-centered design.
Another use-case comes from online communi-
ties themselves. Users of online forums are often in-
terested in other people?s experiences with concrete
products and/or solutions for specific problems. To
quote one such user: [t]he polls are the only in-
formation we have, though, except for individual
[users] giving their own evaluations. With the vol-
ume of online data increasing rapidly, users need im-
proved access to previously reported experiences.
3 Experience mining
Experiences are particular instances of personally
encountering or undergoing something. We want
to identify experiences about a specific target prod-
uct, that are personal, involve an activity related to
the target and, moreover, are accompanied by judge-
ments or evaluative statements. Experience mining
is related to sentiment analysis and opinion retrieval,
in that it involves identifying attitudes; the key dif-
ference is, however, that we are looking for attitudes
towards specific experiences with products, not atti-
tudes towards the products themselves.
4 An explorative study
To assess the feasibility of automatic experience
mining, we carried out an explorative study: we
asked human assessors to find experiences in ac-
tual forum data and then examined linguistic fea-
tures likely to be useful for identifying experiences
automatically.
17
Mean and deviation in posts
Feature with exper. without exper.
subjectivity score2 0.07 ?0.23 0.17 ?0.35
polarity score2 0.87 ?0.30 0.77 ?0.38
#words per post 102.57 ?80.09 52.46 ?53.24
#sentences per post 6.00 ?4.16 3.34 ?2.33
# words per sentence 17.07 ?4.69 15.71 ?7.61
#questions per post 0.32 ?0.63 0.54 ?0.89
p(post contains question) 0.25 ?0.43 0.33 ?0.47
#I?s per post 5.76 ?4.75 2.09 ?2.88
#I?s per sentence 1.01 ?0.48 0.54 ?0.60
p(sentence in post contains I) 0.67 ?0.23 0.40 ?0.35
#non-modal verbs per post 19.62 ?15.08 9.82 ?9.57
#non-modal verbs per sent. 3.30 ?1.18 2.82 ?1.37
#modal verbs per sent. 0.22 ?0.22 0.26 ?0.36
fraction of past-tense verbs 0.26 ?0.17 0.17 ?0.19
fraction of present tense verbs 0.42 ?0.18 0.41 ?0.23
Table 1: Comparison of surface text features for posts
with and without experience; p(?) denotes probability.
We acquired data by crawling two forums on
shaving,1 with 111,268 posts written by 2,880 users.
Manual assessments Two assessors (both authors
of this paper) were asked to search for posts on five
specific target products using a standard keyword
search, and label each result post as:
? reporting no experience, or
? reporting an off-target experience, or
? reporting an on-target experience.
Moreover, posts should be marked as reporting an
experience only if (i) the author explicitly reports
his or someone else?s (a concrete person?s) use of
a product; and (ii) the author makes some conclu-
sions/judgements about the experience.
In total, 203 posts were labeled by the two asses-
sors, with 101 posts marked as reporting an experi-
ence by at least one assessor (71% of those an on-
target experience). The inter-annotator agreement
was 0.84, with Cohen?s ? = 0.71. If we merge
on- and off-target experience labels, the agreement
is 0.88, with ? = 0.76. The high level of agreement
demonstrates the validity of the task definition.
Features for experience mining We considered a
number of linguistic features and compared posts re-
porting experience (on- or off-target) to the posts
1www.shavemyface.com, www.menessentials.com/community
2Computed using LingPipe: http://alias-i.com/lingpipe
With experience Without experience
used 0.15, found 0.09,
bought 0.07, tried 0.07,
got 0.07, went 0.07, started
0.05, switched 0.04, liked
0.03, decided 0.03
got 0.09, thought 0.09,
switched 0.06, meant 0.06,
used 0.06, went 0.06, ig-
nored 0.03, quoted 0.03,
discovered 0.03, heard 0.03
Table 2: Most frequent past tense verbs following I in
posts with and without experience, with rel. frequencies.
with no experience. Table 1 lists the features and
the comparison results. Remarkably, the subjectiv-
ity score is lower for experience posts: this indicates
that our task is indeed different from sentiment re-
trieval. Experience posts are on average twice as
long as non-experience posts and contain more sen-
tences with pronoun I. They also contain more con-
tent (non-modal) verbs, especially past tense verbs.
Table 2 presents a more detailed analysis of the verb
use. Experience posts appear to contain more verbs
referring to concrete actions rather than to attitude
and perception. It is still to be seen, though, whether
this informal observation can be quantified using re-
sources such as standard semantic verb classification
(state, process, action), WordNet verb hierarchy or
FrameNet semantic frames.
5 Conclusions
We introduced the novel task of experience min-
ing. Users of products share their experiences, and
mining these could help define requirements for
next-generation products. We developed annotation
guidelines for labeling experiences, and used them
to annotate data from online forums. An initial ex-
ploration revealed multiple features that might prove
useful for automatic labeling via classification.
References
[1] K. Balog, G. Mishne, and M. de Rijke. Why are they
excited?: identifying and explaining spikes in blog
mood levels. In EACL ?06, pages 207?210, 2006.
[2] B. Buxton. Sketching User Experiences: Getting the
Design Right and the Right Design. Morgan Kauf-
mann Publishers Inc., 2007.
[3] I. Ounis, C. Macdonald, M. de Rijke, G. Mishne, and
I. Soboroff. Overview of the TREC 2006 Blog Track.
In TREC 2006, 2007.
[4] B. Pang and L. Lee. Opinion mining and senti-
ment analysis. Found. Trends Inf. Retr., 2(1-2):1?135,
2008.
18
