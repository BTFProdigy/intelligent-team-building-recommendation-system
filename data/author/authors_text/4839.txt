Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 136?144,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Ranking Reader Emotions Using Pairwise Loss Minimization and  
Emotional Distribution Regression 
 
 
Kevin Hsin-Yih Lin and Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
No. 1 Roosevelt Rd. Sec. 4, Taipei, Taiwan 
{f93141, hhchen}@csie.ntu.edu.tw 
 
 
 
 
 
 
Abstract 
This paper presents two approaches to ranking 
reader emotions of documents. Past studies 
assign a document to a single emotion cate-
gory, so their methods cannot be applied di-
rectly to the emotion ranking problem. 
Furthermore, whereas previous research ana-
lyzes emotions from the writer?s perspective, 
this work examines readers? emotional states. 
The first approach proposed in this paper 
minimizes pairwise ranking errors. In the sec-
ond approach, regression is used to model 
emotional distributions. Experiment results 
show that the regression method is more ef-
fective at identifying the most popular emo-
tion, but the pairwise loss minimization 
method produces ranked lists of emotions that 
have better correlations with the correct lists. 
1 Introduction 
Emotion analysis is an increasingly popular re-
search topic due to the emergence of large-scale 
emotion data on the web. Previous work primarily 
studies emotional contents of texts from the 
writer's perspective, where it is typically assumed 
that a writer expresses only a single emotion in a 
document. Unfortunately, this premise does not 
hold when analyzing a document from the reader's 
perspective, because readers rarely agree unani-
mously on the emotion that a document instills. 
Figure 1 illustrates this phenomenon. In the figure,  
 
0%
10%
20%
30%
40%
H
ea
rtw
ar
m
in
g
H
ap
py
S
ad
S
ur
pr
is
in
g
A
ng
ry
B
or
in
g
A
w
es
om
e
U
se
fu
l
Emotion
%
 o
f R
ea
de
rs
 
Figure 1. Emotional responses of 626 people after read-
ing a Yahoo! News article about an Iranian refugee 
mother and her two children who finally reunited with 
their family in the March of 2007 after been stranded in 
a Moscow airport for 10 months due to false passports. 
 
readers? responses are distributed among different 
emotion categories. In fact, none of the emotions in 
Figure 1 has a majority (i.e., more than 50%) of the 
votes. Intuitively, it is better to provide a ranking 
of emotions according to their popularity rather 
than associating a single reader emotion with a 
document. As a result, current writer-emotion 
analysis techniques for classifying a document into 
a single emotion category are not suitable for ana-
lyzing reader emotions. New methods capable of 
ranking emotions are required.  
Reader-emotion analysis has potential applica-
tions that differ from those of writer-emotion 
analysis. For example, by integrating emotion 
ranking into information retrieval, users will be 
able to retrieve documents that contain relevant 
contents and at the same time produce desired feel-
ings. In addition, reader-emotion analysis can as-
sist writers in foreseeing how their work will 
influence readers emotionally.  
136
In this paper, we present two approaches to 
ranking reader emotions. The first approach is in-
spired by the success of the pairwise loss minimi-
zation framework used in information retrieval to 
rank documents. Along a similar line, we devise a 
novel scheme to minimize the number of incor-
rectly-ordered emotion pairs in a document. In the 
second approach, regression is used to model 
reader-emotion distributions directly. Experiment 
results show that the regression method is more 
effective at identifying the most popular emotion, 
but the pairwise loss minimization method pro-
duces ordered lists of emotions that have better 
correlations with the correct lists. 
The rest of this paper is organized as follows. 
Section 2 describes related work. In Section 3, de-
tails about the two proposed approaches are pro-
vided. Section 4 introduces the corpus and Section 
5 presents how features are extracted from the cor-
pus. Section 6 shows the experiment procedures 
and results. Section 7 concludes the paper. 
2 Related Work 
Only a few studies in the past deal with the reader 
aspect of emotion analysis. For example, Lin et al 
(2007; 2008) classify documents into reader-
emotion categories. Most previous work focuses 
on the writer?s perspective. Pang et al (2002) de-
sign an algorithm to determine whether a docu-
ment?s author expresses a positive or negative 
sentiment. They discover that using Support Vec-
tor Machines (SVM) with word unigram features 
results in the best performance. Since then, more 
work has been done to find features better than 
unigrams. In (Hu et al, 2005), word sentiment in-
formation is exploited to achieve better classifica-
tion accuracy. 
Experiments have been done to extract emo-
tional information from texts at granularities finer 
than documents. Wiebe (2000) investigates the 
subjectivity of words, whereas Aman and Szpako-
wicz (2007) manually label phrases with emotional 
categories. In 2007, the SemEval-2007 workshop 
organized a task on the unsupervised annotation of 
news headlines with emotions (Strapparava and 
Mihalcea, 2007). 
As for the task of ranking, many machine-
learning algorithms have been proposed in infor-
mation retrieval. These techniques generate rank-
ing functions which predict the relevance of a 
document. One class of algorithms minimizes the 
errors resulting from ordering document pairs in-
correctly. Examples include (Joachims, 2002), 
(Freund et al, 2003) and (Qin et al, 2007). In par-
ticular, the training phase of the Joachims? Rank-
ing SVM (Joachims, 2002) is formulated as the 
following SVM optimization problem: 
 
min ?+ kjiCkji ,,T21,, ?? www,  
subject to: 
 
0  : 
1)),(),((  
:|),(),,(
,,
,,
T
,,
????
?????
>??
kji
kjijkik
jkikjkik
kji
dqdq
ssVdqdq
?
?w     (1) 
 
where V is the training corpus, ?(qk, di) is the fea-
ture vector of document di with respect to query qk, 
sk,i is the relevance score of di with respect to qk, w 
is a weight vector, C is the SVM cost parameter, 
and ?i,j,k are slack variables. The set of constraints 
at (1) means that document pairwise orders should 
be preserved. 
Unfortunately, the above scheme for exploiting 
pairwise order information cannot be applied di-
rectly to the emotion ranking task, because the task 
requires us to rank emotions within a document 
rather than provide a ranking of documents. In par-
ticular, the definitions of ?(qk,di), ?(qk,dj), sk,i and 
sk,j do not apply to emotion ranking. In the next 
section, we will show how the pairwise loss mini-
mization concept is adapted for emotion ranking. 
3 Ranking Reader Emotions 
In this section, we provide the formal description 
of the reader-emotion ranking problem. Then we 
describe the pairwise loss minimization (PLM) 
approach and the emotional distribution regression 
(EDR) approach to ranking emotions. 
3.1 Problem Specification 
The reader emotion ranking problem is defined as 
follows. Let D = {d1, d2, ?, dN} be the document 
space, and E = {e1, e2, ?, eM} be the emotion 
space. Let fi : E ? ? be the emotional probability 
function of di?D. That is, fi(ej) outputs the fraction 
of readers who experience emotion ej after reading 
document di. Our goal is to find a function r : D ? 
EM such that r(di) = (e?(1), e?(2), ?, e?(M)) where ? is 
137
Input: Set of emotion ordered pairs P 
1.  G ? a graph with emotions as vertices and no edge 
2.  while (P ? ?) 
3.    remove (ej,ek) with the highest confidence from P 
4.    if adding edge (ej,ek) to G produces a loop 
5.      then add (ek,ej) to G 
6.    else add (ej,ek) to G 
7.  return topological sort of G 
a permutation on {1, 2, ?, M}, and fi(e?(1)) ? fi(e?(2)) 
? ? ? fi(e?(M)). 
3.2 Pairwise Loss Minimization 
As explained in Section 2, the information retrieval 
framework for exploiting pairwise order informa-
tion cannot be applied directly to the emotion rank-
ing problem. Hence, we introduce a novel 
formulation of the emotion ranking problem into 
an SVM optimization problem with constraints 
based on pairwise loss minimization. 
Algorithm 1. Merge Pairwise Orders. 
 
We now describe how we rank the emotions of a 
previously unseen document using the M(M ? 1)/2 
pairwise ranking functions gjk created during the 
training phase. First, all of the pairwise ranking 
functions are applied to the unseen document, 
which generates the relative orders of every pair of 
emotions. These pairwise orders need to be com-
bined together to produce a ranked list of all the 
emotions. Algorithm 1 does exactly this.  
Whereas Ranking SVM generates only a single 
ranking function, our method creates a pairwise 
ranking function gjk : D ? ? for each pair of emo-
tions ej and ek, aiming at satisfying the maximum 
number of the inequalities: 
 In Algorithm 1, the confidence of an emotion 
ordered pair at Line 3 is the probability value re-
turned by a LIBSVM classifier for predicting the 
order. LIBSVM?s method for generating this prob-
ability is described in (Wu et al, 2003). Lines 4 
and 5 resolve the problem of conflicting emotion 
ordered pairs forming a loop in the ordering of 
emotions. The ordered list of emotions returned by 
Algorithm 1 at Line 7 is the final output of the 
PLM method. 
?di?D | fi(ej) > fi(ek) : gjk(di) > 0 
?di?D | fi(ej) < fi(ek) : gjk(di) < 0 
 
In other words, we want to minimize the number of 
incorrectly-ordered emotion pairs. We further re-
quire gjk(di) to have the linear form wT?(di) + b, 
where w is a weight vector, b is a constant, and 
?(di) is the feature vector of di. Details of feature 
extraction will be presented in Section 5. 
As Joachims (2002) points out, the above type 
of problem is NP-Hard. However, an approximate 
solution to finding gik can be obtained by solving 
the following SVM optimization problem: 
3.3 Emotional Distribution Regression 
In the second approach to ranking emotions, we 
use regression to model fi directly. A regression 
function hj : D ? ? is generated for each ej?E by 
learning from the examples (?(di), fi(ej)) for all 
documents di in the training corpus. 
 
min ?+ ib Ci ??, www T21,  
subject to: 
0  : 
1))((:)()(|
1)(:)()(|
T
T
??
??+??<??
??+?>??
i
iikijii
iikijii
i
bdefefQd
bdefefQd
?
?
?
w
w
 
The regression framework we adopt is Support 
Vector Regression (SVR), which is a regression 
analysis technique based on SVM (Sch?lkopf et al, 
2000). We require hj to have the form wT?(di) + b. 
Finding hj is equivalent to solving the following 
optimization problem: 
 
where C is the SVM cost parameter, ?i are slack 
variables, and Q is the training corpus. We assume 
each document di?Q is labeled with fi(ej) for every 
emotion ej?E. 
 
min )( 2,1,T2
1
,, 2,1, iib Cii ????, ++ ?www  
subject to: 
When formulated as an SVM optimization prob-
lem, finding gjk is equivalent to training an SVM 
classifier for classifying a document into the ej or 
ek category. Hence, we use LIBSVM, which is an 
SVM implementation, to obtain the solution.1 
0 , : 
)())((
))(()(
: 
2,1,
2,
T
1,
T
??
???+?
??+??
??
ii
ijii
iiji
i
i
efbd
bdef
Qd
??
??
??
w
w
 
                                                          
 1 http://www.csie.ntu.edu.tw/~cjlin/libsvm/ 
138
02000
4000
6000
8000
[2
0%
,3
0%
)
[3
0%
,4
0%
)
[4
0%
,5
0%
)
[5
0%
,6
0%
)
[6
0%
,7
0%
)
[7
0%
,8
0%
)
[8
0%
,9
0%
)
[9
0%
,1
00
%
]
Percentage of Votes Received by Most Popular Emotion
N
um
be
r o
f N
ew
s 
A
rt
ic
le
s
 
Figure 2. News articles in the entire corpus grouped by 
the percentage of votes received by the most popular 
emotion. 
 
where C is the cost parameter, ? is the maximum 
difference between the predicted and actual values 
we wish to maintain, ?i,1 and ?i,2 are slack variables, 
and Q is the training corpus. To solve the above 
optimization problem, we use SVMlight?s SVR im-
plementation.2 
When ranking the emotions of a previously un-
seen document dk, we sort the emotions ej?E in 
descending order of hj(dk). 
4 Constructing the Corpus 
The training and test corpora used in this study 
comprise Chinese news articles from Yahoo! Kimo 
News3, which allows a user to cast a vote for one 
of eight emotions to express how a news article 
makes her feel. Each Yahoo! news article contains 
a list of eight emotions at the bottom of the web-
page. A reader may select one of the emotions and 
click on a submit button to submit the emotion. As 
with many websites which collect user responses, 
such as the Internet Movie Database, users are not 
forced to submit their responses. After submitting a 
response, the user can view a distribution of emo-
tions indicating how other readers feel about the 
same article. Figure 1 shows the voting results of a 
Yahoo! news article. 
The eight available emotions are happy, sad, 
angry, surprising, boring, heartwarming, awesome, 
and useful. Useful is not a true emotion. Rather, it 
means that a news article contains practical infor-
mation. The value fi(ej) is derived by normalizing 
the number of votes for emotion ej in document di 
by the total number votes in di. 
The entire corpus consists of 37,416 news arti-
cles dating from January 24, 2007 to August 7, 
2007. News articles prior to June 1, 2007 form the 
training corpus (25,975 articles), and the remaining 
ones form the test corpus (11,441 articles). We 
collect articles a week after their publication dates 
to ensure that the vote counts have stabilized. 
                                                          
                                                          2 http://svmlight.joachims.org/ 
3 http://tw.news.yahoo.com 
As mentioned earlier, readers rarely agree 
unanimously on the emotion of a document. Figure 
2 illustrates this. In 41% of all the news articles in 
the entire corpus, the most popular emotion re-
ceives less than 60% of the votes. 
5 Extracting Features 
After obtaining news articles, the next step is to 
determine how to convert them into feature vectors 
for SVM and SVR. That is, we want to instantiate 
?. For this purpose, three types of features are ex-
tracted. 
The first feature type consists of Chinese charac-
ter bigrams, which are taken from the headline and 
content of each news article. The presence of a bi-
gram is indicated by a binary feature value. 
Chinese words form the second type of features. 
Unlike English words, consecutive Chinese words 
in a sentence are not separated by spaces. To deal 
with this problem, we utilize Stanford NLP 
Group?s Chinese word segmenter to split a sen-
tence into words.4 As in the case of bigrams, bi-
nary feature values are used. 
We use character bigram features in addition to 
word features to increase the coverage of Chinese 
words. A Chinese word is formed by one or more 
contiguous Chinese characters. As mentioned ear-
lier, Chinese words in a sentence are not separated 
by any boundary symbol (e.g., a space), so a Chi-
nese word segmentation tool is always required to 
extract words from a sentence. However, a word 
segmenter may identify word boundaries errone-
ously, resulting in the loss of correct Chinese 
words. This problem is particularly severe if there 
are a lot of out-of-vocabulary words in a dataset. In 
Chinese, around 70% of all Chinese words are 
Chinese character bigrams (Chen et al, 1997). 
Thus, using Chinese character bigrams as features 
will allow us to identify a lot of Chinese words, 
which when combined with the words extracted by 
the word segmenter, will give us a wider coverage 
of Chinese words. 
The third feature type is extracted from news 
metadata. A news article?s metadata are its news 
4 http://nlp.stanford.edu/software/segmenter.shtml 
139
NDCG@k is used because ACC@k has the dis-
advantage of not taking emotional distributions 
into account. Take Figure 1 as an example. In the 
figure, heartwarming and happy have 31.3% and 
30.7% of the votes, respectively. Since the two 
percentages are very close, it is reasonable to say 
that predicting happy as the first item in a ranked 
list may also be acceptable. However, doing so 
would be completely incorrect according to 
ACC@k. In contrast, NDCG@k would consider it 
to be partially correct, and the extent of correctness 
depends on how much heartwarming and happy?s 
percentages of votes differ. To be exact, if happy is 
predicted as the first item, then the corresponding 
NDCG@1 would be 30.7% / 31.3% = 0.98. 
category, agency, hour of publication, reporter, and 
event location. Examples of news categories in-
clude sports and political. Again, we use binary 
feature values. News metadata are used because 
they may contain implicit emotional information. 
6 Experiments 
The experiments are designed to achieve the fol-
lowing four goals: (i) to compare the ranking per-
formance of different methods, (ii) to analyze the 
pairwise ranking quality of PLM, (iii) to analyze 
the distribution estimation quality of EDR, and (iv) 
to compare the ranking performance of different 
feature sets. The Yahoo! News training and test 
corpora presented in Section 4 are used in all ex-
periments. 
The third metric is SACC@k, or set accuracy at 
k. It is a variant of ACC@k. According to 
SACC@k, a predicted ranked list is correct if the 
set of its first k items is the same as the true ranked 
list?s set of first k items. In effect, SACC@k evalu-
ates a ranking method?s ability to place the top k 
most important items in the first k positions. 
6.1 Evaluation Metrics for Ranking 
We employ three metrics as indicators of ranking 
quality: ACC@k, NDCG@k and SACC@k. 
ACC@k stands for accuracy at position k. Ac-
cording to ACC@k, a predicted ranked list is cor-
rect if the list?s first k items are identical (i.e., same 
items in the same order) to the true ranked list?s 
first k items. If two emotions in a list have the 
same number of votes, then their positions are in-
terchangeable. ACC@k is computed by dividing 
the number of correctly-predicted instances by the 
total number of instances. 
6.2 Tuning SVM and SVR Parameters 
SVM and SVR are employed in PLM and EDR, 
respectively. Both SVM and SVR have the adjust-
able C cost parameter, and SVR has an additional ? 
parameter. To estimate the optimal C value for a 
combination of SVM and features, we perform 4-
fold cross-validation on the Yahoo! News training 
corpus, and select the C value which results in the 
highest binary classification accuracy during cross-
validation. The same procedure is used to estimate 
the best C and ? values for a combination of SVR 
and features. The C-? pair which results in the 
lowest mean squared error during cross-validation 
is chosen. The candidate C values for both SVM 
and SVR are 2-10, 2-9, ?, 2-6. The candidate ? val-
ues for SVR are 10-2 and 10-1. All cross-validations 
are performed solely on the training data. The test 
data are not used to tune the parameters. Also, 
SVM and SVR allow users to specify the type of 
kernel to use. Linear kernel is selected for both 
SVM and SVR. 
NDCG@k, or normalized discounted cumulative 
gain at position k (J?rvelin and Kek?l?inen, 2002), 
is a metric frequently used in information retrieval 
to judge the quality of a ranked list when multiple 
levels of relevance are considered. This metric is 
defined as 
? = += ki ik irelzk 1 2 )1(log@NDCG  
 
where reli is the relevance score of the predicted 
item at position i, and zk is a normalizing factor 
which ensures that a correct ranked list has an 
NDCG@k value of 1. In the emotion ranking prob-
lem, reli is the percentage of reader votes received 
by the emotion at position i. Note that the log2(i+1) 
value in the denominator is a discount factor which 
decreases the weights of items ranked later in a list. 
NDCG@k has the range [0, 1], where 1 is the best. 
In the experiment results, NDCG@k values are 
averaged over all instances in the test corpus. 
6.3 Nearest Neighbor Baseline 
The nearest neighbor (NN) method is used as the 
baseline. The ranked emotion list of a news article 
in the test corpus is predicted as follows. First, the
140
0.0
0.2
0.4
0.6
0.8
1 2 3 4 5 6 7 8
ACC@
A
cc
ur
ac
y
NN
EDR
PLM
 
0.5
0.6
0.7
0.8
0.9
1.0
1 2 3 4 5 6 7 8
NDCG@
N
D
C
G
 V
al
ue
NN
EDR
PLM
 
0.0
0.2
0.4
0.6
0.8
1.0
1 2 3 4 5 6 7 8
SACC@
A
cc
ur
ac
y
NN
EDR
PLM
 
Figure 3. ACC@k Figure 4. NDCG@k Figure 5. SACC@k 
 
 
0%
20%
40%
60%
80%
100%
1 2 3 4 5 6 7 8
ACC@
%
 o
f T
es
t I
ns
ta
nc
es
Both
Incorrect
Only PLM
Correct
Only EDR
Correct
Both
Correct
 
Figure 6. Performance of PLM and EDR. 
 
test news article is compared to every training 
news article using cosine similarity, which is de-
fined as  
||||
||
),(cos
ii
ji
ji DD
DD
dd ?
?=  
 
where di and dj are two news articles, and Di and Dj 
are sets of Chinese character bigrams in di and dj, 
respectively. The ranked emotion list of the train-
ing article having the highest cosine similarity with 
the test article is used as the predicted ranked list. 
6.4 Comparison of Methods 
Figures 3 to 5 show the performance of different 
ranking methods on the test corpus. For both PLM 
and EDR, all of the bigram, word, and news meta-
data features are used. 
In Figure 3, EDR?s ACC@1 (0.751) is higher 
than those of PLM and NN, and the differences are 
statistically significant with p-value < 0.01. So, 
EDR is the best method at predicting the most 
popular emotion. However, PLM has the best 
ACC@k for k ? 2, and the differences from the 
other two methods are all significant with p-value 
< 0.01. This means that PLM?s predicted ranked 
lists better resemble the true ranked lists.  
Figure 3 displays a sharp decrease in ACC@k 
values as k increases. This trend indicates the hard-
ness of predicting a ranked list correctly. Looking 
from a different angle, the ranking task under the 
ACC@k metric is equivalent to the classification 
of news articles into one of 8!/(8 ? k)! classes, 
where we regard each unique emotion sequence of 
length k as a class. In fact, computing ACC@8 for 
a ranking method is the same as evaluating the 
method?s ability to classify a news article into one 
of 8! = 40,320 classes. So, producing a completely-
correct ranked list is a difficult task. 
In Figure 4, all of PLM and EDR?s NDCG@k 
improvements over NN are statistically significant 
with p-value < 0.01. For some values of k, the dif-
ference in NDCG@k between PLM and EDR is 
not significant. The high NDCG@k values (i.e., 
greater than 0.8) of PLM and EDR imply that al-
though it is difficult for PLM and EDR to generate 
completely-correct ranked lists, these two methods 
are effective at placing highly popular emotions to 
the beginning of ranked lists. 
In Figure 5, PLM outperforms the other two 
methods for 2 ? k ? 7, and the differences are all 
statistically significant with p-value < 0.01. For 
small values of k (e.g., 2 ? k ? 3), PLM?s higher 
SACC@k values mean that PLM is better at plac-
ing the highly popular emotions in the top posi-
tions of a ranked list. 
To further compare PLM and EDR, we examine 
their performance on individual test instances. Fig-
ure 6 shows the percentage of test instances where 
both PLM and EDR give incorrect lists, only PLM 
gives correct lists, only EDR gives ranked lists, 
and both methods give correct lists. The ?Only 
PLM Correct? and ?Only EDR Correct? categories 
are nonzero, so neither PLM nor EDR is always 
better than the other. 
In summary, EDR is the best at predicting the 
most popular emotion according to ACC@1, 
NDCG@1 and SACC@1. However, PLM gener-
ates ranked lists that better resemble the correct 
ranked lists according to ACC@k and SACC@k 
141
Method Average ?b Average p-value
PLM 0.584 0.068
EDR 0.474 0.114
NN 0.392 0.155
Table 1. Kendall?s ?b statistics. 
 
 He Su Sa Us Ha Bo An
Aw 0.80  0.75  0.78  0.77  0.82  0.76 0.79 
He  0.79  0.81  0.78  0.81  0.89 0.81 
Su   0.82  0.78  0.80  0.82 0.82 
Sa    0.78  0.80  0.84 0.82 
Us     0.82  0.91 0.82 
Ha      0.83 0.79 
Bo      0.80 
Table 2. Classification accuracies of SVM pairwise 
emotion classifiers on the test corpus. He = heartwarm-
ing, Su = surprising, Sa = sad, Us = useful, Ha = happy, 
Bo = boring, and An = angry. 
 
0.53
0.58
0.63
0.68
0.73
0.75 0.8 0.85 0.9
Accuracy of Pairwise Emotion Classification
A
ve
ra
ge
 D
is
cr
im
in
at
io
n
V
al
ue
 o
f E
m
ot
io
n 
P
ai
r
 
Figure 7. Accuracy of pairwise emotion classification 
and the corresponding average discrimination value. 
 
for k ? 2. Further analysis shows that neither 
method is always better than the other. 
6.5 Pairwise Ranking Quality of PLM 
In this subsection, we evaluate the performance of 
PLM in predicting pairwise orders. 
We first examine the quality of ranked lists gen-
erated by PLM in terms of pairwise orders. To do 
this, we use Kendall?s ?b correlation coefficient, 
which is a statistical measure for determining the 
correlation between two ranked lists when there 
may be ties between two items in a list (Liebetrau, 
1983). The value of ?b is determined based on the 
number of concordant pairwise orders and the 
number of discordant pairwise orders between two 
ranked lists. Therefore, this measure is appropriate 
for evaluating the effectiveness of PLM at predict-
ing pairwise orders correctly. ?b has the range [-1, 
1], where 1 means a perfect positive correlation, 
and -1 means two lists are the reverse of each other. 
When computing ?b of two ranked lists, we also 
calculate a p-value to indicate whether the correla-
tion is statistically significant. 
We compute ?b statistics between a predicted 
ranked list and the corresponding true ranked list. 
Table 1 shows the results. In Table 1, numbers in 
the ?Average ?b? and ?Average p-value? columns 
are averaged over all test instances. The statistics 
for EDR and NN are also included for comparison. 
From the table, we see that PLM has the highest 
average ?b value and the lowest average p-value, so 
PLM is better at preserving pairwise orders than 
EDR and NN methods. This observation verifies 
that PLM?s minimization of pairwise loss leads to 
better prediction of pairwise orders.  
We now look at the individual performance of 
the 28 pairwise emotion rankers gjk. As mentioned 
in Section 3.2, each pairwise emotion ranker gjk is 
equivalent to a binary classifier for classifying a 
document into the ej or ek category. So, we look at 
their classification accuracies in Table 2. In the 
table, accuracy ranges from 0.75 for the awesome-
surprising pair to 0.91 for the useful-boring pair. 
From the psychological perspective, the rela-
tively low accuracy of the awesome-surprising pair 
is expected, because awesome is surprising in a 
positive sense. So, readers should have a hard time 
distinguishing between these two emotions. And 
the SVM classifier, which models reader responses, 
should also find it difficult to discern these two 
emotions. Based on this observation, we suspect 
that the pairwise classification performance actu-
ally reflects the underlying emotional ambiguity 
experienced by readers. To verify this, we quantify 
the degree of ambiguity between two emotions, 
and compare the result to pairwise classification 
accuracy. 
To quantify emotional ambiguity, we introduce 
the concept of discrimination value between two 
emotions ej and ek in a document di, which is de-
fined as follows: 
 
)()(
)()(
kiji
kiji
efef
efef
+
?
 
 
where fi is the emotional probability function de-
fined in Section 3.1. Intuitively, the larger the dis-
crimination value is, the smaller the degree of 
ambiguity between two emotions is. 
Figure 7 shows the relationship between pair-
wise classification accuracy and the average dis-
crimination value of the corresponding emotion 
142
0.00
0.02
0.04
0.06
0.08
A
w
es
om
e
H
ea
rtw
ar
m
in
g
S
ur
pr
is
in
g
S
ad
U
se
fu
l
H
ap
py
B
or
in
g
A
ng
ry
Emotion
M
ea
n 
S
qu
ar
ed
 E
rr
or
NN
EDR
 
Figure 8. Mean squared error of NN and EDR for esti-
mating the emotional distributions of the test corpus. 
 
0.0
0.2
0.4
0.6
0.8
1 2 3 4 5 6 7 8
ACC@
A
cc
ur
ac
y
Metadata
Words
Bigrams
All
 
Figure 9. PLM performance using different features. 
 
pair. The general pattern is that as accuracy in-
creases, the discrimination value also increases. To 
provide concrete evidence, we use Pearson?s prod-
uct-moment correlation coefficient, which has the 
range of [-1, 1], where 1 means a perfect positive 
correlation (Moore, 2006). The coefficient for the 
data in Figure 7 is 0.726 with p-value < 0.01. Thus, 
pairwise emotion classification accuracy reflects 
the emotional ambiguity experienced by readers. 
In summary, PLM?s pairwise loss minimization 
leads to better pairwise order predictions than EDR 
and NN. Also, the pairwise classification results 
reveal the inherent ambiguity between emotions. 
6.6 Distribution Estimation Quality of EDR 
In this subsection, we evaluate EDR?s performance 
in estimating the emotional probability function fi. 
With the prior knowledge that a news article?s fi 
values sum to 1 over all emotions, and fi is between 
0 and 1, we adjust EDR?s fi predictions to produce 
proper distributions. It is done as follows. A pre-
dicted fi value greater than 1 or less than 0 is set to 
1 and 0, respectively. Then the predicted fi values 
are normalized to sum to 1 over all emotions.  
NN?s distribution estimation performance is in-
cluded for comparison. For NN, the predicted fi 
values of a test article are taken from the emotional 
distribution of the most similar training article.  
Figure 8 shows the mean squared error of EDR 
and NN for predicting fi. In the figure, the error 
generated by EDR is less than those by NN, and all 
the differences are statistically significant with p-
value < 0.01. Thus, EDR?s use of regression leads 
to better estimation of fi than the NN. 
6.7 Comparison of Features 
Figure 9 shows each of the three feature type?s 
ACC@k for predicting test instances? ranked lists 
when PLM is used. The feature comparison graph 
for EDR is not shown, because it exhibits a very 
similar trend as PLM. For both PLM and EDR, 
bigrams are better than words, which are in turn 
better than news metadata. In Figure 9, the combi-
nation of all three feature sets achieves the best 
performance. For both PLM and EDR, the im-
provements in ACC@k of using all features over 
words and metadata are all significant with p-value 
< 0.01, and the improvements over bigrams are 
significant for k ? 2. Hence, in general, it is better 
to use all three feature types together. 
7 Conclusions and Future Work 
This paper presents two methods to ranking reader 
emotions. The PLM method minimizes pairwise 
loss, and the EDR method estimates emotional dis-
tribution through regression. Experiments with 
significant tests show that EDR is better at predict-
ing the most popular emotion, but PLM produces 
ranked lists that have higher correlation with the 
correct lists. We further verify that PLM has better 
pairwise ranking performance than the other two 
methods, and EDR has better distribution estima-
tion performance than NN. 
As for future work, there are several directions 
we can pursue. An observation is that PLM ex-
ploits pairwise order information, whereas EDR 
exploits emotional distribution information. We 
plan to combine these two methods together. An-
other research direction is to improve EDR by 
finding better features. We would also like to inte-
grate emotion ranking into information retrieval. 
Acknowledgments 
We are grateful to the Computer and Information 
Networking Center, National Taiwan University, 
for the support of high-performance computing 
facilities. The research in this paper was partially 
supported by National Science Council, Taiwan, 
under the contract NSC 96-2628-E-002-240-MY3. 
143
References  
Saima Aman and Stan Szpakowicz. 2007. Identifying 
Expressions of Emotion in Text. In Proceedings of 
10th International Conference on Text, Speech and 
Dialogue, Lecture Notes in Computer Science 4629, 
196-205. Springer, Plze?, CZ. 
Aitao Chen, Jianzhang He, Liangjie Xu, Frederic Gey, 
and Jason Meggs. 1997. Chinese Text Retrieval 
wihtout using a Dictionary. In Proceedings of 20th 
Annual International ACM SIGIR Conference on 
Research and Development in Information Retrieval, 
42-49. Association for Computing Machinery, Phila-
delphia, US. 
Yoav Freund, Raj D. Iyer, Robert E. Schapire, and 
Yoram Singer. 2003. An Efficient Boosting Algorithm 
for Combining Preferences. Journal of Machine 
Learning Research, 4, 933-969. 
Yi Hu, Jianyong Duan, Xiaoming Chen, Bingzhen Pei, 
and Ruzhan Lu. 2005. A New Method for Sentiment 
Classification in Text Retrieval. In Proceedings of 
2nd International Joint Conference on Natural Lan-
guage Processing, 1-9. Jeju Island, KR. 
Kalervo J?rvelin and Jaana Kek?l?inen. Cumulative 
Gain-based Evaluation of IR Techniques. 2002. 
ACM Transactions on Information Systems, 20(4), 
422-446. 
Thorsten Joachims. 2002. Optimizing Search Engines 
using Clickthrough Data. In Proceedings of 8th 
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining. Association for 
Computing Machinery, Edmonton, CA. 
Albert M. Liebetrau. 1983. Measures of Association. 
Sage Publications, Newbury Park, US. 
Kevin H. Lin, Changhua Yang, and Hsin-Hsi Chen. 
2007. What Emotions do News Articles Trigger in 
their Readers? In Proceedings of 30th ACM SIGIR 
Conference, 733-734. Association for Computing 
Machinery, Amsterdam, NL. 
Kevin H. Lin, Changhua Yang, and Hsin-Hsi Chen. 
2008. Emotion Classification of Online News Articles 
from the Reader?s Perspective. In Proceedings of In-
ternational Conference on Web Intelligence. Institute 
of Electrical and Electronics Engineers, Sydney, AU. 
David Moore. 2006. The Basic Practice of Statistics. 
W.H. Freeman and Company, New York, US. 
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 
2002. Thumbs up? Sentiment Classification Using 
Machine Learning Techniques. In Proceedings of 
2002 Conference on Empirical Methods in Natural 
Language Processing, 79-86. Association for Com-
putational Linguistics, Philadelphia, US. 
Tao Qin, Tie-Yan Liu, Wei Lai, Xu-Dong Zhang, De-
Sheng Wang, and Hang Li. 2007. Ranking with Mul-
tiple Hyperplanes. In Proceedings of 30th ACM 
SIGIR Conference, 279-286. Association for Com-
puting Machinery, Amsterdam, NL. 
Bernhard Sch?lkopf, Alex J. Smola, Robert C. William-
son, and Peter L. Barlett. 2000. New Support Vector 
Algorithms. Neural Computation, 12(5), 1207-1245. 
Carlo Strapparava and Rada Mihalcea. 2007. SemEval-
2007 Task 14: Affective Text. In Proceedings of 4th 
International Workshop on Semantic Evaluations. 
Prague, CZ. 
Janyce M. Wiebe. 2000. Learning Subjective Adjectives 
from Corpora. In Proceedings of 17th Conference of 
the American Association for Artificial Intelligence, 
735-740. AAAI Press, Austin, US. 
Ting-Fan Wu, Chih-Jen Lin, and Ruby C. Weng. Prob-
ability Estimates for Multi-class Classification by 
Pairwise Coupling. 2004. Journal of Machine Learn-
ing Research, 5, 975-1005. 
144
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 133?136,
Prague, June 2007. c?2007 Association for Computational Linguistics
Building Emotion Lexicon from Weblog Corpora 
Changhua Yang        Kevin Hsin-Yih Lin        Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
#1 Roosevelt Rd. Sec. 4, Taipei, Taiwan 106 
{d91013, f93141, hhchen}@csie.ntu.edu.tw 
Abstract 
An emotion lexicon is an indispensable re-
source for emotion analysis.  This paper 
aims to mine the relationships between 
words and emotions using weblog corpora.  
A collocation model is proposed to learn 
emotion lexicons from weblog articles.  
Emotion classification at sentence level is 
experimented by using the mined lexicons 
to demonstrate their usefulness. 
1 Introduction 
Weblog (blog) is one of the most widely used cy-
bermedia in our internet lives that captures and 
shares moments of our day-to-day experiences, 
anytime and anywhere.  Blogs are web sites that 
timestamp posts from an individual or a group of 
people, called bloggers.  Bloggers may not follow 
formal writing styles to express emotional states.  
In some cases, they must post in pure text, so they 
add printable characters, such as ?:-)? (happy) and 
?:-(? (sad), to express their feelings.  In other cases, 
they type sentences with an internet messenger-
style interface, where they can attach a special set 
of graphic icons, or emoticons.  Different kinds of 
emoticons are introduced into text expressions to 
convey bloggers? emotions. 
Since thousands of blog articles are created eve-
ryday, emotional expressions can be collected to 
form a large-scale corpus which guides us to build 
vocabularies that are more emotionally expressive.  
Our approach can create an emotion lexicon free of 
laborious efforts of the experts who must be famil-
iar with both linguistic and psychological knowl-
edge. 
2 Related Works 
Some previous works considered emoticons from 
weblogs as categories for text classification.  
Mishne (2005), and Yang and Chen (2006) used 
emoticons as tags to train SVM (Cortes and Vap-
nik, 1995) classifiers at document or sentence level.  
In their studies, emoticons were taken as moods or 
emotion tags, and textual keywords were taken as 
features.  Wu et al (2006) proposed a sentence-
level emotion recognition method using dialogs as 
their corpus.  ?Happy, ?Unhappy?, or ?Neutral? 
was assigned to each sentence as its emotion cate-
gory.  Yang et al (2006) adopted Thayer?s model 
(1989) to classify music emotions.  Each music 
segment can be classified into four classes of 
moods.  In sentiment analysis research, Read (2005) 
used emoticons in newsgroup articles to extract 
instances relevant for training polarity classifiers. 
3 Training and Testing Blog Corpora 
We select Yahoo! Kimo Blog1 posts as our source 
of emotional expressions.  Yahoo! Kimo Blog 
service has 40 emoticons which are shown in Table 
1.  When an editing article, a blogger can insert an 
emoticon by either choosing it or typing in the 
corresponding codes.  However, not all articles 
contain emoticons.  That is, users can decide 
whether to insert emoticons into articles/sentences 
or not.  In this paper, we treat these icons as 
emotion categories and taggings on the 
corresponding text expressions. 
The dataset we adopt consists of 5,422,420 blog 
articles published at Yahoo! Kimo Blog from 
January to July, 2006, spanning a period of 212 
days.  In total, 336,161 bloggers? articles were col-
lected.  Each blogger posts 16 articles on average. 
We used the articles from January to June as the 
training set and the articles in July as the testing set.  
Table 2 shows the statistics of each set.  On aver-
age, 14.10% of the articles contain emotion-tagged 
expressions.  The average length of articles with 
tagged emotions, i.e., 272.58 characters, is shorter 
                                                 
1
 http://tw.blog.yahoo.com/ 
133
than that of articles without tagging, i.e., 465.37 
characters.  It seems that people tend to use emoti-
cons to replace certain amount of text expressions 
to make their articles more succinct. 
Figure 1 shows the three phases for the con-
struction and evaluation of emotion lexicons.  In 
phase 1, 1,185,131 sentences containing only one 
emoticon are extracted to form a training set to 
build emotion lexicons.  In phase 2, sentence-level 
emotion classifiers are constructed using the mined 
lexicons.  In phase 3, a testing set consisting of 
307,751 sentences is used to evaluate the classifi-
ers. 
4 Emotion Lexicon Construction 
The blog corpus contains a collection of bloggers? 
emotional expressions which can be analyzed to 
construct an emotion lexicon consisting of words 
that collocate with emoticons. We adopt a variation 
of pointwise mutual information (Manning and 
Sch?tze, 1999) to measure the collocation strength 
co(e,w) between an emotion e and a word w:  
)()(
),(log),(),o(
wPeP
weP
wecwec ?=  (1) 
where P(e,w)=c(e,w)/N, P(e)=c(e)/N, P(w)=c(w)/N, 
c(e)
 
and c(w) are the total occurrences of emoticon 
e and word w in a tagged corpus, respectively, 
c(e,w) is total co-occurrences of e and w, and N 
denotes the total word occurrences. 
A word entry of a lexicon may contain several 
emotion senses.  They are ordered by the colloca-
tion strength co.  Figure 2 shows two Chinese ex-
ample words, ??? ? (ha1ha1) and ??? ? 
(ke3wu4).  The former collocates with ?laughing? 
and ?big grin? emoticons with collocation strength 
25154.50 and 2667.11, respectively.  Similarly, the 
latter collocates with ?angry? and ?phbbbbt?.  
When all collocations (i.e., word-emotion pairs) 
are listed in a descending order of co, we can 
choose top n collocations to build an emotion lexi-
con.  In this paper, two lexicons (Lexicons A and B) 
are extracted by setting n to 25k and 50k.  Lexicon 
A contains 4,776 entries with 25,000 sense pairs 
and Lexicon B contains 11,243 entries and 50,000 
sense pairs. 
5 Emotion Classification 
Suppose a sentence S to be classified consists of n 
emotion words.  The emotion of S is derived by a 
mapping from a set of n emotion words to m emo-
tion categories as follows: 
 },...,{?},...,{ 11 m
tionclassifican
eeeewewS ???  
Table 1. Yahoo! Kimo Blog Emoticon Set. 
ID Emoticon Code Description ID Emoticon Code Description ID Emoticon Code Description ID Emoticon Code Description 
1 
 
:) happy 11 
 
:O surprise 21 
 
0:) angel 31 
 
(:| yawn 
2 
 
:( sad 12 
 
X-( angry 22 
 
:-B nerd 32 
 
=P~ drooling 
3 
 
;) winking 13 
 
:> smug 23 
 
=; 
talk to  
the hand 33  :-? thinking 
4 
 
:D big grin 14 
 
B-) cool 24 
 
I-) asleep 34 
 
;)) hee hee 
5 
 
;;) batting  eyelashes 15  :-S worried 25  8-) rolling eyes 35  =D> applause 
6 
 
:-/ confused 16 
 
>:) devil 26 
 
:-& sick 36 
 
[-o< praying 
7 
 
:x love struck 17 
 
:(( crying 27 
 
:-$ don't tell  anyone 37  :-< sigh 
8 
 
:?> blushing 18 
 
:)) laughing 28 
 
[-( not talking 38 
 
>:P phbbbbt 
9 
 
:p tongue 19 
 
:| straight face 29 
 
:o) clown 39 
 
@};- rose 
10 
 
:* kiss 20 
 
/:) raised  eyebrow 30  @-) hypnotized 40  :@) pig 
 
Table 2. Statistics of the Weblog Dataset. 
Dataset Article # Tagged # Percentage Tagged Len. Untagged L. 
Training 4,187,737 575,009 13.86% 269.77 chrs. 468.14 chrs. 
Testing 1,234,683 182,999 14.92% 281.42 chrs. 455.82 chrs. 
Total 5,422,420 764,788 14.10% 272.58 chrs. 465.37 chrs. 
 
Testing Set 
Figure 1. Emotion Lexicon Construction and Evaluation. 
Extraction 
Blog 
Articles 
Features 
Classifiers 
Evaluation 
Lexicon 
 Construction 
Training Set 
Phase 2
Phase 3 
Emotion 
 Lexicon 
Phase 1 
134
For each emotion word ewi, we may find several 
emotion senses with the corresponding collocation 
strength co by looking up the lexicon.  Three alter-
natives are proposed as follows to label a sentence 
S with an emotion: 
(a) Method 1 
(1)  Consider all senses of ewi as votes.  Label S 
with the emotion that receives the most votes. 
(2)  If more than two emotions get the same num-
ber of votes, then label S with the emotion that 
has the maximum co. 
(b) Method 2 
    Collect emotion senses from all ewi.  Label S 
with the emotion that has the maximum co. 
(c) Method 3 
The same as Method 1 except that each ewi v-
otes only one sense that has the maximum co. 
In past research, the approach used by Yang et 
al. (2006) was based on the Thayer?s model (1989), 
which divided emotions into 4 categories.  In sen-
timent analysis research, such as Read?s study 
(2006), a polarity classifier separated instances into 
positive and negative classes.  In our experiments, 
we not only adopt fine-grain classification, but also 
coarse-grain classification.  We first select 40 
emoticons as a category set, and also adopt the 
Thayer?s model to divide the emoticons into 4 
quadrants of the emotion space.  As shown in Fig-
ure 3, the top-right side collects the emotions that 
are more positive and energetic and the bottom-left 
side is more negative and silent.  A polarity classi-
fier uses the right side as positive and the left side 
as negative. 
6 Evaluation 
Table 3 shows the performance under various 
combinations of lexicons, emotion categories and 
classification methods.  ?Hit #? stands for the 
number of correctly-answered instances. The base-
line represents the precision of predicting the ma-
jority category, such as ?happy? or ?positive?, as 
the answer.  The baseline method?s precision in-
creases as the number of emotion classes decreases.  
The upper bound recall indicates the upper limit on 
the fraction of the 307,751 instances solvable by 
the corresponding method and thus reflects the 
limitation of the method.  The closer a method?s 
actual recall is to the upper bound recall, the better 
the method.  For example, at most 40,855 instances 
(14.90%) can be answered using Method 1 in 
combination with Lexicon A.  But the actual recall 
is 4.55% only, meaning that Method 1?s recall is 
more than 10% behind its upper bound.  Methods 
which have a larger set of candidate answers have 
higher upper bound recalls, because the probability 
that the correct answer is in their set of candidate 
answers is greater. 
Experiment results show that all methods utiliz-
ing Lexicon A have performance figures lower 
than the baseline, so Lexicon A is not useful.  In 
contrast, Lexicon B, which provides a larger col-
lection of vocabularies and emotion senses, outper-
forms Lexicon A and the baseline.  Although 
Method 3 has the smallest candidate answer set 
and thus has the smallest upper bound recall, it 
outperforms the other two methods in most cases.  
Method 2 achieves better precisions when using 
?? (ha1ha1) ?hah hah?  
Sense 1. (laughing) ? co: 25154.50 
e.g., ??...  ???????~ 
        ?hah hah?  I am getting lucky~? 
Sense 2. (big grin) ? co: 2667.11 
e.g., ??????????~??  
        ?I only memorized vowels today~ haha ? 
?? (ke3wu4) ?darn? 
Sense 1. (angry) ? co: 2797.82 
e.g., ??????...??  
        ?What's the hacker doing... darn it ? 
Sense 2. (phbbbbt) ? co: 619.24 
e.g., ???????  
        ?Damn those aliens ? 
Figure 2. Some Example Words in a Lexicon. 
 
Arousal (energetic) 
 
 
               
                       
                                                                  Valence 
(negative)                                            (positive) 
 
        
       
 
(silent) 
unassigned:  
Figure 3. Emoticons on Thayer?s model. 
135
Thayer?s emotion categories.  Method 1 treats the 
vote to every sense equally.  Hence, it loses some 
differentiation abilities.  Method 1 performs the 
best in the first case (Lexicon A, 40 classes). 
We can also apply machine learning to the data-
set to train a high-precision classification model.  
To experiment with this idea, we adopt LIBSVM 
(Fan et al, 2005) as the SVM kernel to deal with 
the binary polarity classification problem.  The 
SVM classifier chooses top k (k = 25, 50, 75, and 
100) emotion words as features.  Since the SVM 
classifier uses a small feature set, there are testing 
instances which do not contain any features seen 
previously by the SVM classifier.  To deal with 
this problem, we use the class prediction from 
Method 3 for any testing instances without any 
features that the SVM classifier can recognize.  In 
Table 4, the SVM classifier employing 25 features 
has the highest precision.  On the other hand, the 
SVM classifier employing 50 features has the 
highest F measure when used in conjunction with 
Method 3. 
7 Conclusion and Future Work 
Our methods for building an emotional lexicon 
utilize emoticons from blog articles collaboratively 
contributed by bloggers.  Since thousands of blog 
articles are created everyday, we expect the set of 
emotional expressions to keep expanding.  In the 
experiments, the method of employing each emo-
tion word to vote only one emotion category 
achieves the best performance in both fine-grain 
and coarse-grain classification. 
Acknowledgment 
Research of this paper was partially supported by 
Excellent Research Projects of National Taiwan 
University, under the contract of 95R0062-AE00-
02.  We thank Yahoo! Taiwan Inc. for providing 
the dataset for researches. 
References 
Corinna Cortes and V. Vapnik. 1995. Support-Vector 
Network. Machine Learning, 20:273?297. 
Rong-En Fan, Pai-Hsuen Chen and Chih-Jen Lin. 2005. 
Working Set Selection Using Second Order Informa-
tion for Training Support Vector Machines. Journal 
of Machine Learning Research, 6:1889?1918. 
Gilad Mishne. 2005. Experiments with Mood Classifi-
cation in Blog Posts. Proceedings of 1st Workshop on 
Stylistic Analysis of Text for Information Access. 
Jonathon Read. 2005. Using Emotions to Reduce De-
pendency in Machine Learning Techniques for Sen-
timent Classification. Proceedings of the ACL Stu-
dent Research Workshop, 43-48. 
Robert E. Thayer. 1989. The Biopsychology of Mood 
and Arousal, Oxford University Press. 
Changhua Yang and Hsin-Hsi Chen. 2006. A Study of 
Emotion Classification Using Blog Articles. Pro-
ceedings of Conference on Computational Linguistics 
and Speech Processing, 253-269. 
Yi-Hsuan Yang, Chia-Chu Liu, and Homer H. Chen. 
2006. Music Emotion Classification: A Fuzzy Ap-
proach. Proceedings of ACM Multimedia, 81-84. 
Chung-Hsien Wu, Ze-Jing Chuang, and Yu-Chung Lin. 
2006. Emotion Recognition from Text Using Seman-
tic Labels and Separable Mixture Models. ACM 
Transactions on Asian Language Information Proc-
essing, 5(2):165-182. 
Table 3. Evaluation Results. 
Method 1 (M1) Method 2 (M2) Method 3 (M3) 
 Baseline Upp. R. Hit # Prec. Reca. Upp. R. Hit # Prec. Reca. Upp. R. Hit # Prec. Reca. 
Lexicon A 
40 classes 8.04% 14.90% 14,009 4.86% 4.55% 14.90% 9,392 3.26% 3.05% 6.49% 13,929 4.83% 4.52% 
Lexicon A 
Thayer 38.38% 48.70% 90,332 32.46% 29.35% 48.70% 64,689 23.25% 21.02% 35.94% 93,285 33.53% 30.31% 
Lexicon A 
Polarity 63.49% 60.74% 150,946 54.25% 49.05% 60.74% 120,237 43.21% 39.07% 54.97% 153,292 55.09% 49.81% 
Lexicon B 
40 classes 8.04% 73.18% 45,075 15.65% 14.65% 73.18% 43,637 15.15% 14.18% 27.89% 45,604 15.83% 14.81% 
Lexicon B 
Thayer 38.38% 89.11% 104,094 37.40% 33.82% 89.11% 118,392 42.55% 38.47% 63.74% 110,904 39.86% 36.04% 
Lexicon B 
Polarity 63.49% 91.12% 192,653 69.24% 62.60% 91.12% 188,434 67.72% 61.23% 81.92% 195,190 70.15% 63.42% 
Upp. R. ? upper bound recall; Prec. ? precision; Reca. ? recall 
                          Table 4. SVM  Performance. 
Method Upp. R. Hit # Prec. Reca. F 
Lexicon B M3 81.92% 195,190 70.15% 63.42% 66.62% 
SVM 25 features 15.80% 38,651 79.49% 12.56% 21.69% 
SVM 50 features 26.27% 62,999 77.93% 20.47% 32.42% 
SVM 75 features 36.74% 84,638 74.86% 27.50% 40.23% 
SVM 100 features 45.49% 101,934 72.81% 33.12% 45.53% 
 (Svm-25 + M3) 90.41% 196,147 70.05% 63.73% 66.74% 
 (Svm-50 + M3) 90.41% 195,835 70.37% 63.64% 66.83% 
(Svm-75 + M3) 90.41% 195,229 70.16% 63.44% 66.63% 
 (Svm-100 + M3) 90.41% 195,054 70.01% 63.38% 66.53% 
                                F = 2?(Precision?Recall)/(Precision+Recall) 
136
