Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 72?80,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Effective Use of Linguistic and Contextual Information
for Statistical Machine Translation
Libin Shen and Jinxi Xu and Bing Zhang and
Spyros Matsoukas and Ralph Weischedel
BBN Technologies
Cambridge, MA 02138, USA
{lshen,jxu,bzhang,smatsouk,weisched}@bbn.com
Abstract
Current methods of using lexical features
in machine translation have difficulty in
scaling up to realistic MT tasks due to
a prohibitively large number of parame-
ters involved. In this paper, we propose
methods of using new linguistic and con-
textual features that do not suffer from
this problem and apply them in a state-of-
the-art hierarchical MT system. The fea-
tures used in this work are non-terminal
labels, non-terminal length distribution,
source string context and source depen-
dency LM scores. The effectiveness of
our techniques is demonstrated by signif-
icant improvements over a strong base-
line. On Arabic-to-English translation,
improvements in lower-cased BLEU are
2.0 on NIST MT06 and 1.7 on MT08
newswire data on decoding output. On
Chinese-to-English translation, the im-
provements are 1.0 on MT06 and 0.8 on
MT08 newswire data.
1 Introduction
Linguistic and context features, especially sparse
lexical features, have been widely used in re-
cent machine translation (MT) research. Unfor-
tunately, existing methods of using such features
are not ideal for large-scale, practical translation
tasks.
In this paper, we will propose several prob-
abilistic models to effectively exploit linguistic
and contextual information for MT decoding, and
these new features do not suffer from the scalabil-
ity problem. Our new models are tested on NIST
MT06 and MT08 data, and they provide signifi-
cant improvement over a strong baseline system.
1.1 Previous Work
The ideas of using labels, length preference and
source side context in MT decoding were explored
previously. Broadly speaking, two approaches
were commonly used in existing work.
One is to use a stochastic gradient descent
(SGD) or Perceptron like online learning algo-
rithm to optimize the weights of these features
directly for MT (Shen et al, 2004; Liang et al,
2006; Tillmann and Zhang, 2006). This method is
very attractive, since it opens the door to rich lex-
ical features. However, in order to robustly opti-
mize the feature weights, one has to use a substan-
tially large development set, which results in sig-
nificantly slower tuning. Alternatively, one needs
to carefully select a development set that simulates
the test set to reduce the risk of over-fitting, which
however is not always realistic for practical use.
A remedy is to aggressively limit the feature
space, e.g. to syntactic labels or a small fraction
of the bi-lingual features available, as in (Chiang
et al, 2008; Chiang et al, 2009), but that reduces
the benefit of lexical features. A possible generic
solution is to cluster the lexical features in some
way. However, how to make it work on such a
large space of bi-lingual features is still an open
question.
The other approach is to estimate a single score
or likelihood of a translation with rich features,
for example, with the maximum entropy (Max-
Ent) method as in (Carpuat and Wu, 2007; Itty-
cheriah and Roukos, 2007; He et al, 2008). This
method avoids the over-fitting problem, at the ex-
pense of losing the benefit of discriminative train-
ing of rich features directly for MT. However, the
feature space problem still exists in these pub-
lished models.
He et al (2008) extended the WSD-like ap-
proached proposed in (Carpuat and Wu, 2007) to
hierarchical decoders. In (He et al, 2008), lexical
72
features were limited on each single side due to the
feature space problem. In order to further reduce
the complexity of MaxEnt training, they ?trained
a MaxEnt model for each ambiguous hierarchical
LHS? (left-hand side or source side) of translation
rules. Different target sides were treated as possi-
ble labels. Therefore, the sample sets of each indi-
vidual MaxEnt model were very small, while the
number of features could easily exceed the number
of samples. Furthermore, optimizing individual
MaxEnt models in this way does not lead to global
maximum. In addition, MaxEnt models trained on
small sets are unstable.
The MaxEnt model in (Ittycheriah and Roukos,
2007) was optimized globally, so that it could bet-
ter employ the distribution of the training data.
However, one has to filter the training data ac-
cording to the test data to get competitive perfor-
mance with this model 1. In addition, the filtering
method causes some practical issues. First, such
methods are not suitable for real MT tasks, espe-
cially for applications with streamed input, since
the model has to be retrained with each new input
sentence or document and training is slow. Fur-
thermore, the model is ill-posed. The translation
of a source sentence depends on other source sen-
tences in the same batch with which the MaxEnt
model is trained. If we add one more sentence to
the batch, translations of other sentences may be-
come different due to the change of the MaxEnt
model.
To sum up, the existing models of employing
rich bi-lingual lexical information in MT are im-
perfect. Many of them are not ideal for practical
translation tasks.
1.2 Our Approach
As for our approach, we mainly use simple proba-
bilistic models, i.e. Gaussian and n-gram models,
which are more robust and suitable for large-scale
training of real data, as manifested in state-of-the-
art systems of speech recognition. The unique
contribution of our work is to design effective and
efficient statistical models to capture useful lin-
guistic and context information for MT decoding.
Feature functions defined in this way are robust
and ideal for practical translation tasks.
1According to footnote 2 of (Ittycheriah and Roukos,
2007), test set adaptation by test set sampling of the train-
ing corpus showed an advantage of more than 2 BLEU points
over a general system trained on all data.
1.2.1 Features
In this paper, we will introduce four new linguistic
and contextual feature functions. Here, we first
provide a high-level description of these features.
Details of the features are discussed in Section 2.
The first feature is based on non-terminal labels,
i.e. POS tags of the head words of target non-
terminals in transfer rules. This feature reduces
the ambiguity of translation rules. The other bene-
fit is that POS tags help to weed out bad target side
tree structures, as an enhancement to the target de-
pendency language model.
The second feature is based on the length dis-
tribution of non-terminals. In English as well as
in other languages, the same deep structure can
be represented in different syntactic structures de-
pending on the complexity of its constituents. We
model such preferences by associating each non-
terminal of a transfer rule with a probability distri-
bution over its length. Similar ideas were explored
in (He et al, 2008). However their length features
only provided insignificant improvement of 0.1
BLEU point. A crucial difference of our approach
is how the length preference is modeled. We ap-
proximate the length distribution of non-terminals
with a smoothed Gaussian, which is more robust
and gives rise to much larger improvement consis-
tently.
The third feature utilizes source side context in-
formation, i.e. the neighboring words of an input
span, to influence the selection of the target trans-
lation for a span. While the use of context infor-
mation has been explored in MT, e.g. (Carpuat
and Wu, 2007) and (He et al, 2008), the specific
technique we used by means of a context language
model is rather different. Our model is trained on
the whole training data, and it is not limited by the
constraint of MaxEnt training.
The fourth feature exploits structural informa-
tion on the source side. Specifically, the decoder
simultaneously generates both the source and tar-
get side dependency trees, and employs two de-
pendency LMs, one for the source and the other
for the target, for scoring translation hypotheses.
Our intuition is that the likelihood of source struc-
tures provides another piece of evidence about the
plausibility of a translation hypothesis and as such
would help weed out bad ones.
73
1.2.2 Baseline System and Experimental
Setup
We take BBN?s HierDec, a string-to-dependency
decoder as described in (Shen et al, 2008), as our
baseline for the following two reasons:
? It provides a strong baseline, which ensures
the validity of the improvement we would ob-
tain. The baseline model used in this paper
showed state-of-the-art performance at NIST
2008 MT evaluation.
? The baseline algorithm can be easily ex-
tended to incorporate the features proposed
in this paper. The use of source dependency
structures is a natural extension of the string-
to-tree model to a tree-to-tree model.
To ensure the generality of our results, we tested
the features on two rather different language pairs,
Arabic-to-English and Chinese-to-English, using
two metrics, IBM BLEU (Papineni et al, 2001)
and TER (Snover et al, 2006). Our experiments
show that each of the first three features: non-
terminal labels, length distribution and source side
context, improves MT performance. Surprisingly,
the source dependency feature does not produce
an improvement.
2 Linguistic and Context Features
2.1 Non-terminal Labels
In the original string-to-dependency model (Shen
et al, 2008), a translation rule is composed of a
string of words and non-terminals on the source
side and a well-formed dependency structure on
the target side. A well-formed dependency struc-
ture could be either a single-rooted dependency
tree or a set of sibling trees. As in the Hiero system
(Chiang, 2007), there is only one non-terminal X
in the string-to-dependency model. Any sub de-
pendency structure can be used to replace a non-
terminal in a rule.
For example, we have a source sentence in Chi-
nese as follows.
? jiantao zhuyao baohan liang fangmian
The literal translation for individual words is
? ?review? ?mainly? ?to consist of? ?two? ?part?
The reference translation is
? the review mainly consists of two parts
A single source word can be translated into
many English words. For example, jiantao can
be translated into a review, the review, reviews,
the reviews, reviewing, reviewed, etc. Suppose
we have source-string-to-target-dependency trans-
lation rules as shown in Figure 1. Since there is
no constraint on substitution, any translation for
jiantao could replace the X-1 slot.
One way to alleviate this problem is to limit the
search space by using a label system. We could
assign a label to each non-terminal on the target
side of the rules. Furthermore, we could assign a
label to the whole target dependency structure, as
shown in Figure 2. In decoding, each target de-
pendency sub-structure would be associated with
a label. Whenever substitution happens, we would
check whether the label of the sub-structure and
the label of the slot are the same. Substitutions
with unmatched labels would be prohibited.
In practice, we use a soft constraint by penaliz-
ing substitutions with unmatched labels. We intro-
duce a new feature: the number of times substitu-
tions with unmatched labels appear in the deriva-
tion of a translation hypothesis.
Obviously, to implement this feature we need to
associate a label with each non-terminal in the tar-
get side of a translation rule. The labels are gen-
erated during rule extraction. When we create a
rule from a training example, we replace a sub-
tree or dependency structure with a non-terminal
and associate it with the POS tag of the head word
if the non-terminal corresponds to a single-rooted
tree on the target side. Otherwise, it is assigned
the generic label X. (In decoding, all substitutions
of X are considered unmatched ones and incur a
penalty.)
2.2 Length Distribution
In English, the length of a phrase may determine
the syntactic structure of a sentence. For example,
possessive relations can be represented either as
?A?s B? or ?B of A?. The former is preferred if A
is a short phrase (e.g. ?the boy?s mother?) while
the latter is preferred if A is a complex structure
(e.g. ?the mother of the boy who is sick?).
Our solution is to build a model of length prefer-
ence for each non-terminal in each translation rule.
To address data sparseness, we assume the length
distribution of each non-terminal in a transfer rule
is a Gaussian, whose mean and variance can be
estimated from the training data. In rule extrac-
74
the
X
X
jiantao
reviews
the
X
X
jiantao
review
X X?1 baohan
X
X?1
consists
X?2
X?2
ofmainly
zhuyao
Figure 1: Translation rules with one label X
the
X jiantao
reviews
the
X jiantao
review
X X?1 baohan
consists
NNS VBZNN
NN?1 NNS?2
X?2
ofmainly
zhuyao
Figure 2: Translation rules with multiple labels
tion, each time a translation rule is generated from
a training example, we can record the length of the
source span corresponding to a non-terminal. In
the end, we have a frequency histogram for each
non-terminal in each translation rule. From the
histogram, a Gaussian distribution can be easily
computed.
In practice, we do not need to collect the fre-
quency histogram. Since all we need to know are
the mean and the variance, it is sufficient to col-
lect the sum of the length and the sum of squared
length.
Let r be a translation rule that occurs N
r
times
in training. Let x be a specific non-terminal in that
rule. Let l(r, x, i) denote the length of the source
span corresponding to non-terminal x in the i-th
occurrence of rule r in training. Then, we can
compute the following quantities.
m
r,x
=
1
N
r
N
r
?
i=1
l(r, x, i) (1)
s
r,x
=
1
N
r
N
r
?
i=1
l(r, x, i)
2
, (2)
which can be subsequently used to estimate the
mean ?
r,x
and variance ?2
r,x
of x?s length distri-
bution in rule r as follows.
?
r,x
= m
r,x
(3)
?
2
r,x
= s
r,x
?m
2
r,x
(4)
Since many of the translation rules have few oc-
currences in training, smoothing of the above esti-
mates is necessary. A common smoothing method
is based on maximum a posteriori (MAP) estima-
tion as in (Gauvain and Lee, 1994).
m?
r,x
=
N
r
N
r
+ ?
m
r,x
+
?
N
r
+ ?
m?
r,x
s?
r,x
=
N
r
N
r
+ ?
s
r,x
+
?
N
r
+ ?
s?
r,x
,
where ? stands for an MAP distribution and ? rep-
resents a prior distribution. m?
r,x
and s?
r,x
can
be obtained from a prior Gaussian distribution
N (??
r,x
, ??
r,x
) via equations (3) and (4), and ? is
a weight of smoothing.
There are many ways to approximate the prior
distribution. For example, we can have one prior
for all the non-terminals or one for individual non-
terminal type. In practice, we assume ??
r,x
= ?
r,x
,
and approximate ??
r,x
as (?2
r,x
+ s
r,x
)
1
2 .
In this way, we do not change the mean, but
relax the variance with s
r,x
. We tried differ-
ent smoothing methods, but the performance did
not change much, therefore we kept this simplest
setup. We also tried the Poisson distribution, and
the performance is similar to Gaussian distribu-
tion, which is about 0.1 point lower in BLEU.
When a rule r is applied during decoding, we
compute a penalty for each non-terminal x in r
according to
P (l | r, x) =
1
?
r,x
?
2pi
e
?
(l??
r,x
)
2
2?
2
r,x
,
where l is length of source span corresponding to
x.
Our method to address the problem of length
bias in rule selection is very different from the
maximum entropy method used in existing stud-
ies, e.g. (He et al, 2008).
75
2.3 Context Language Model
In the baseline string-to-dependency system, the
probability a translation rule is selected in decod-
ing does not depend on the sentence context. In
reality, translation is highly context dependent. To
address this defect, we introduce a new feature,
called context language model. The motivation of
this feature is to exploit surrounding words to in-
fluence the selection of the desired transfer rule for
a given input span.
To illustrate the problem, we use the same ex-
ample mentioned in Section 2.1. Suppose the
source span for rule selection is zhuyao baohan,
whose literal translation is mainly and to consist
of. There are many candidate translations for this
phrase, for example, mainly consist of, mainly
consists of, mainly including, mainly includes, etc.
The surrounding words can help to decide which
translation is more appropriate for zhuyao bao-
han. We compare the following two context-based
probabilities:
? P ( jiantao | mainly consist )
? P ( jiantao | mainly consists )
Here, jiantao is the source word preceding the
source span zhuyao baohan.
In the training data, jiantao is usually trans-
lated into the review, third-person singular, then
the probability P ( jiantao | mainly consists ) will
be higher than P ( jiantao | mainly consist ), since
we have seen more context events like the former
in the training data.
Now we introduce context LM formally. Let the
source words be f
1
f
2
..f
i
..f
j
..f
n
. Suppose source
sub-string f
i
..f
j
is translated into e
p
..e
q
. We can
define tri-gram probabilities on the left and right
sides of the source span:
? left : P
L
(f
i?1
|e
p
, e
p+1
)
? right : P
R
(f
j+1
|e
q
, e
q?1
)
In our implementation, the left and right context
LMs are estimated from the training data as part
of the rule extraction procedure. When we exact a
rule, we collect two 3-gram events, one for the left
side and the other for the right side.
In decoding, whenever a partial hypothesis is
generated, we calculate the context LM scores
based on the leftmost two words and the rightmost
two words of the hypothesis as well as the source
context. The product of the left and right context
LM scores is used as a new feature in the scoring
function.
Please note that our approach is very different
from other approaches to context dependent rule
selection such as (Ittycheriah and Roukos, 2007)
and (He et al, 2008). Instead of using a large num-
ber of fine grained features with weights optimized
using the maximum entropy method, we treat con-
text dependency as an ngram LM problem, and it
is smoothed with Witten-Bell discounting. The es-
timation of the context LMs is very efficient and
robust.
The benefit is two fold. The estimation of the
context LMs is very efficient. It adds only one new
weight to the scoring function.
2.4 Source Dependency Language Model
The context LM proposed in the previous sec-
tion only employs source words immediately be-
fore and after the current source span in decod-
ing. To exploit more source context, we use a
source side dependency language model as an-
other feature. The motivation is to take advantage
of the long distance dependency relations between
source words in scoring a translation theory.
We extended string-to-dependency rules in
the baseline system to dependency-to-dependency
rules. In each dependency-to-dependency rule, we
keep record of the source string as well as the
source dependency structure. Figure 3 shows ex-
amples of dependency-to-dependency rules.
We extended the string-to-dependency decod-
ing algorithm in the baseline to accommodate
dependency-to-dependency theories. In decoding,
we build both the source and the target depen-
dency structures simultaneously in chart parsing
over the source string. Thus, we can compute the
source dependency LM score in the same way we
compute the target side score, using a procedure
described in (Shen et al, 2008).
We introduce two new features for the source
side dependency LM as follows, in a way similar
to the target side.
? Source dependency LM score
? Discount on ill-formed source dependency
structures
The source dependency LM is trained on the
source side of the bi-lingual training data with
Witten-Bell smoothing. The source dependency
LM score represents the likelihood of the source
76
X?1 X?2zhuyao
baohan
the
X
X
jiantao
reviews
the
X
X
jiantao
review
X
X X?1
consists
X?2ofmainly
Figure 3: Dependency-to-dependency translation rules
dependency tree generated by the decoder. The
source dependency tree with the highest score is
the one that is most likely to be generated by the
dependency model that created the source side of
the training data.
Source dependency trees are composed of frag-
ments embedded in the translation rules. There-
fore, a source dependency LM score can be
viewed as a measure whether the translation rules
are put together in a way similar to the training
data. Therefore, a source dependency LM score
serves as a feature to represent structural con-
text information that is capable of modeling long-
distance relations.
However, unlike source context LMs, the struc-
tural context information is used only when two
partial dependency structures are combined, while
source context LMs work as a look-ahead feature.
3 Experiments
We designed our experiments to show the impact
of each feature separately as well as their cumula-
tive impact:
? BASE: baseline string-to-dependency system
? SLM: baseline + source dependency LM
? CLM: baseline + context LM
? LEN: baseline + length distribution
? LBL: baseline + syntactic labels
? LBL+LEN: baseline + syntactic labels +
length distribution
? LBL+LEN+CLM: baseline + syntactic labels
+ length distribution + context LM
All the models were optimized on lower-cased
IBM BLEU with Powell?s method (Powell, 1964;
Brent, 1973) on n-best translations (Ostendorf et
al., 1991), but evaluated on both IBM BLEU and
TER. The motivation is to detect if an improve-
ment is artificial, i.e., specific to the tuning met-
ric. For both Arabic-to-English and Chinese-to-
English MT, we tuned on NIST MT02-05 and
tested on MT06 and MT08 newswire sets.
The training data are different from what was
usd at MT06 or MT08. Our Arabic-to-English
data contain 29M Arabic words and 38M En-
glish words from 11 corpora: LDC2004T17,
LDC2004T18, LDC2005E46, LDC2006E25,
LDC2006G05, LDC2005E85, LDC2006E36,
LDC2006E82, LDC2006E95, Sakhr-A2E and
Sakhr-E2A. The Chinese-to-English data contain
107M Chinese words and 132M English words
from eight corpora: LDC2002E18, LDC2005T06,
LDC2005T10, LDC2006E26, LDC2006G05,
LDC2002L27, LDC2005T34 and LDC2003E07.
They are available under the DARPA GALE
program. Traditional 3-gram and 5-gram string
LMs were trained on the English side of the
parallel data plus the English Gigaword corpus
V3.0 in a way described in (Bulyko et al, 2007).
The target dependency LMs were trained on the
English side of the parallel training data. For that
purpose, we parsed the English side of the parallel
data. Two separate models were trained: one for
Arabic from the Arabic training data and the other
for Chinese from the Chinese training data.
To compute the source dependency LM for
Chinese-to-English MT, we parsed the Chinese
side of the Chinese-to-English parallel data. Due
to the lack of a good Arabic parser compatible
with the Sakhr tokenization that we used on the
source side, we did not test the source dependency
LM for Arabic-to-English MT.
When extracting rules with source dependency
structures, we applied the same well-formedness
constraint on the source side as we did on the tar-
get side, using a procedure described by (Shen
et al, 2008). Some candidate rules were thrown
away due to the source side constraint. On the
77
Model
MT06 MT08
BLEU TER BLEU TER
lower mixed lower mixed lower mixed lower mixed
Decoding (3-gram LM)
BASE 48.75 46.74 43.43 45.79 49.58 47.46 42.80 45.08
CLM 49.44 47.36 42.96 45.22 49.73 47.53 42.64 44.92
LEN 49.37 47.28 43.01 45.35 50.29 48.19 42.32 44.45
LBL 49.33 47.07 43.09 45.53 50.46 48.19 42.27 44.57
LBL+LEN 49.91 47.70 42.59 45.17 51.10 48.85 41.88 44.16
LBL+LEN+CLM 50.75 48.51 42.13 44.50 51.24 49.10 41.63 43.80
Rescoring (5-gram LM)
BASE 51.24 49.23 42.08 44.42 51.23 49.11 42.01 44.15
CLM 51.57 49.54 41.74 43.88 51.44 49.37 41.63 43.74
LEN 52.05 50.01 41.50 43.72 51.88 49.89 41.51 43.47
LBL 51.80 49.69 41.54 43.76 51.93 49.86 41.27 43.33
LBL+LEN 51.90 49.76 41.41 43.70 52.42 50.29 40.93 43.00
LBL+LEN+CLM 52.61 50.51 40.77 43.03 52.60 50.56 40.69 42.81
Table 1: BLEU and TER percentage scores on MT06 and MT08 Arabic-to-English newswire sets.
other hand, one string-to-dependency rule may
split into several dependency-to-dependency rules
due to different source dependency structures. The
size of the dependency-to-dependency rule set is
slightly smaller than the size of the string-to-
dependency rule set.
Tables 1 and 2 show the BLEU and TER per-
centage scores on MT06 and MT08 for Arabic-
to-English and Chinese-to-English translation re-
spectively. The context LM feature, the length
feature and the syntax label feature all produce
a small improvement for most of the conditions.
When we combined the three features, we ob-
served significant improvements over the baseline.
For Arabic-to-English MT, the LBL+LEN+CLM
system improved lower-cased BLEU by 2.0 on
MT06 and 1.7 on MT08 on decoding output.
For Chinese-to-English MT, the improvements in
lower-cased BLEU were 1.0 on MT06 and 0.8 on
MT08. After re-scoring, the improvements be-
came smaller, but still noticeable, ranging from 0.7
to 1.4. TER scores were also improved noticeably
for all conditions, suggesting there was no metric
specific over-tuning.
Surprisingly, source dependency LM did not
provide any improvement over the baseline. There
are two possible reasons for this. One is that
the source and target parse trees were generated
by two stand-alone parsers, which may cause in-
compatible structures on the source and target
sides. By applying the well-formed constraints
on both sides, a lot of useful transfer rules are
discarded. A bi-lingual parser, trained on paral-
lel treebanks recently made available to the NLP
community, may overcome this problem. The
other is that the search space of dependency-to-
dependency decoding is much larger, since we
need to add source dependency information into
the chart parsing states. We will explore tech-
niques to address these problems in the future.
4 Discussion
Linguistic information has been widely used in
SMT. For example, in (Wang et al, 2007), syntac-
tic structures were employed to reorder the source
language as a pre-processing step for phrase-based
decoding. In (Koehn and Hoang, 2007), shallow
syntactic analysis such as POS tagging and mor-
phological analysis were incorporated in a phrasal
decoder.
In ISI?s syntax-based system (Galley et al,
2006) and CMU?s Hiero extension (Venugopal et
al., 2007), non-terminals in translation rules have
labels, which must be respected by substitutions
during decoding. In (Post and Gildea, 2008; Shen
et al, 2008), target trees were employed to im-
prove the scoring of translation theories. Mar-
ton and Resnik (2008) introduced features defined
on constituent labels to improve the Hiero system
(Chiang, 2005). However, due to the limitation of
MER training, only part of the feature space could
used in the system. This problem was fixed by
78
Model
MT06 MT08
BLEU TER BLEU TER
lower mixed lower mixed lower mixed lower mixed
Decoding (3-gram LM)
BASE 37.44 35.62 54.64 56.47 33.05 31.26 56.79 58.69
SLM 37.30 35.48 54.24 55.90 33.03 31.00 56.59 58.46
CLM 37.66 35.81 53.45 55.19 32.97 31.01 55.99 57.77
LEN 38.09 36.26 53.98 55.81 33.23 31.34 56.51 58.41
LBL 38.37 36.53 54.14 55.99 33.25 31.34 56.60 58.49
LBL+LEN 38.36 36.59 53.95 55.60 33.72 31.83 56.79 58.65
LBL+LEN+CLM 38.41 36.57 53.83 55.70 33.83 31.79 56.55 58.51
Rescoring (5-gram LM)
BASE 38.91 37.04 53.65 55.45 34.34 32.32 55.60 57.60
SLM 38.27 36.38 53.64 55.29 34.25 32.28 55.35 57.21
CLM 38.79 36.88 53.09 54.80 35.01 32.98 55.39 57.28
LEN 39.22 37.30 53.34 55.06 34.65 32.70 55.61 57.51
LBL 39.11 37.30 53.61 55.29 35.02 33.00 55.39 57.48
LBL+LEN 38.91 37.17 53.56 55.27 35.03 33.08 55.47 57.46
LBL+LEN+CLM 39.58 37.62 53.21 54.94 35.72 33.63 54.88 56.98
Table 2: BLEU and TER percentage scores on MT06 and MT08 Chinese-to-English newswire sets.
Chiang et al (2008), which used an online learn-
ing method (Crammer and Singer, 2003) to handle
a large set of features.
Most SMT systems assume that translation
rules can be applied without paying attention to
the sentence context. A few studies (Carpuat and
Wu, 2007; Ittycheriah and Roukos, 2007; He et
al., 2008; Hasan et al, 2008) addressed this de-
fect by selecting the appropriate translation rules
for an input span based on its context in the in-
put sentence. The direct translation model in (It-
tycheriah and Roukos, 2007) employed syntactic
(POS tags) and context information (neighboring
words) within a maximum entropy model to pre-
dict the correct transfer rules. A similar technique
was applied by He et al (2008) to improve the Hi-
ero system.
Our model differs from previous work on the
way in which linguistic and contextual informa-
tion is used.
5 Conclusions and Future Work
In this paper, we proposed four new linguistic
and contextual features for hierarchical decoding.
The use of non-terminal labels, length distribution
and context LM features gave rise to significant
improvement on Arabic-to-English and Chinese-
to-English translation on NIST MT06 and MT08
newswire data over a state-of-the-art string-to-
dependency baseline. Unlike previous work, we
employed robust probabilistic models to capture
useful linguistic and contextual information. Our
methods are more suitable for practical translation
tasks.
In future, we will continue this work in two
directions. We will employ a Gaussian model
to unify various linguistic and contextual fea-
tures. We will also improve the dependency-to-
dependency method with a better bi-lingual parser.
Acknowledgments
This work was supported by DARPA/IPTO Con-
tract No. HR0011-06-C-0022 under the GALE
program.
References
R. P. Brent. 1973. Algorithms for Minimization With-
out Derivatives. Prentice-Hall.
I. Bulyko, S. Matsoukas, R. Schwartz, L. Nguyen, and
J. Makhoul. 2007. Language model adaptation in
machine translation from speech. In Proceedings of
the 32nd IEEE International Conference on Acous-
tics, Speech, and Signal Processing (ICASSP).
M. Carpuat and D. Wu. 2007. Context-dependent
phrasal translation lexicons for statistical machine
translation. In Proceedings of Machine Translation
Summit XI.
79
D. Chiang, Y. Marton, and P. Resnik. 2008. On-
line large-margin training of syntactic and structural
translation features. In Proceedings of the 2008
Conference of Empirical Methods in Natural Lan-
guage Processing.
D. Chiang, K. Knight, and W. Wang. 2009. 11,001
new features for statistical machine translation. In
Proceedings of the 2009 Human Language Technol-
ogy Conference of the North American Chapter of
the Association for Computational Linguistics.
D. Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings
of the 43th Annual Meeting of the Association for
Computational Linguistics (ACL).
D. Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2).
K. Crammer and Y. Singer. 2003. Ultraconservative
online algorithms for multiclass problems. Journal
of Machine Learning Research, 3:951?991.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,
W. Wang, and I. Thayer. 2006. Scalable infer-
ence and training of context-rich syntactic models.
In COLING-ACL ?06: Proceedings of 44th Annual
Meeting of the Association for Computational Lin-
guistics and 21st Int. Conf. on Computational Lin-
guistics.
J.-L. Gauvain and Chin-Hui Lee. 1994. Maximum a
posteriori estimation for multivariate gaussian mix-
tureobservations of markov chains. IEEE Transac-
tions on Speech and Audio Processing, 2(2).
S. Hasan, J. Ganitkevitch, H. Ney, and J. Andre?s-Ferrer.
2008. Triplet lexicon models for statistical machine
translation. In Proceedings of the 2008 Conference
of Empirical Methods in Natural Language Process-
ing.
Z. He, Q. Liu, and S. Lin. 2008. Improving statistical
machine translation using lexicalized rule selection.
In Proceedings of COLING ?08: The 22nd Int. Conf.
on Computational Linguistics.
A. Ittycheriah and S. Roukos. 2007. Direct translation
model 2. In Proceedings of the 2007 Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics.
P. Koehn and H. Hoang. 2007. Factored translation
models. In Proceedings of the 2007 Conference of
Empirical Methods in Natural Language Process-
ing.
P. Liang, A. Bouchard-Co?te?, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In COLING-ACL ?06: Proceed-
ings of 44th Annual Meeting of the Association for
Computational Linguistics and 21st Int. Conf. on
Computational Linguistics.
Y. Marton and P. Resnik. 2008. Soft syntactic con-
straints for hierarchical phrased-based translation.
In Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics (ACL).
M. Ostendorf, A. Kannan, S. Austin, O. Kimball,
R. Schwartz, and J. R. Rohlicek. 1991. Integra-
tion of diverse recognition methodologies through
reevaluation of nbest sentence hypotheses. In Pro-
ceedings of the DARPA Workshop on Speech and
Natural Language.
K. Papineni, S. Roukos, and T. Ward. 2001. Bleu: a
method for automatic evaluation of machine transla-
tion. IBM Research Report, RC22176.
M. Post and D. Gildea. 2008. Parsers as language
models for statistical machine translation. In The
Eighth Conference of the Association for Machine
Translation in the Americas.
M. J. D. Powell. 1964. An efficient method for finding
the minimum of a function of several variables with-
out calculating derivatives. The Computer Journal,
7(2).
L. Shen, A. Sarkar, and F. J. Och. 2004. Discriminative
reranking for machine translation. In Proceedings of
the 2004 Human Language Technology Conference
of the North American Chapter of the Association
for Computational Linguistics.
L. Shen, J. Xu, and R. Weischedel. 2008. A New
String-to-Dependency Machine Translation Algo-
rithm with a Target Dependency Language Model.
In Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics (ACL).
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In Proceedings of
Association for Machine Translation in the Ameri-
cas.
C. Tillmann and T. Zhang. 2006. A discrimina-
tive global training algorithm for statistical mt. In
COLING-ACL ?06: Proceedings of 44th Annual
Meeting of the Association for Computational Lin-
guistics and 21st Int. Conf. on Computational Lin-
guistics.
A. Venugopal, A. Zollmann, and S. Vogel. 2007.
An efficient two-pass approach to synchronous-cfg
driven statistical mt. In Proceedings of the 2007 Hu-
man Language Technology Conference of the North
American Chapter of the Association for Computa-
tional Linguistics.
C. Wang, M. Collins, and P. Koehn. 2007. Chinese
syntactic reordering for statistical machine transla-
tion. In Proceedings of the 2007 Conference of Em-
pirical Methods in Natural Language Processing.
80
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 307?314, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Combining Deep Linguistics Analysis and Surface Pattern Learning:
A Hybrid Approach to Chinese Definitional Question Answering
Fuchun Peng, Ralph Weischedel, Ana Licuanan, Jinxi Xu
BBN Technologies
50 Moulton Street, Cambridge, MA, 02138
 
fpeng, rweisched, alicuan, jxu  @bbn.com
Abstract
We explore a hybrid approach for Chinese
definitional question answering by com-
bining deep linguistic analysis with sur-
face pattern learning. We answer four
questions in this study: 1) How helpful are
linguistic analysis and pattern learning? 2)
What kind of questions can be answered
by pattern matching? 3) How much an-
notation is required for a pattern-based
system to achieve good performance? 4)
What linguistic features are most useful?
Extensive experiments are conducted on
biographical questions and other defini-
tional questions. Major findings include:
1) linguistic analysis and pattern learning
are complementary; both are required to
make a good definitional QA system; 2)
pattern matching is very effective in an-
swering biographical questions while less
effective for other definitional questions;
3) only a small amount of annotation is
required for a pattern learning system to
achieve good performance on biographi-
cal questions; 4) the most useful linguistic
features are copulas and appositives; re-
lations also play an important role; only
some propositions convey vital facts.
1 Introduction
Due to the ever increasing large amounts of online
textual data, learning from textual data is becom-
ing more and more important. Traditional document
retrieval systems return a set of relevant documents
and leave the users to locate the specific information
they are interested in. Question answering, which
combines traditional document retrieval and infor-
mation extraction, solves this problem directly by
returning users the specific answers. Research in
textual question answering has made substantial ad-
vances in the past few years (Voorhees, 2004).
Most question answering research has been focus-
ing on factoid questions where the goal is to return
a list of facts about a concept. Definitional ques-
tions, however, remain largely unexplored. Defini-
tional questions differ from factoid questions in that
the goal is to return the relevant ?answer nuggets?
of information about a query. Identifying such an-
swer nuggets requires more advanced language pro-
cessing techniques. Definitional QA systems are
not only interesting as a research challenge. They
also have the potential to be a valuable comple-
ment to static knowledge sources like encyclopedias.
This is because they create definitions dynamically,
and thus answer definitional questions about terms
which are new or emerging (Blair-Goldensoha et
al., 2004).
One success in factoid question answering
is pattern based systems, either manually con-
structed (Soubbotin and Soubbotin, 2002) or ma-
chine learned (Cui et al, 2004). However, it is
unknown whether such pure pattern based systems
work well on definitional questions where answers
are more diverse.
Deep linguistic analysis has been found useful in
factoid question answering (Moldovan et al, 2002)
and has been used for definitional questions (Xu et
al., 2004; Harabagiu et al, 2003). Linguistic analy-
307
sis is useful because full parsing captures long dis-
tance dependencies between the answers and the
query terms, and provides more information for in-
ference. However, merely linguistic analysis may
not be enough. First, current state of the art lin-
guistic analysis such as parsing, co-reference, and
relation extraction is still far below human perfor-
mance. Errors made in this stage will propagate and
lower system accuracy. Second, answers to some
types of definitional questions may have strong local
dependencies that can be better captured by surface
patterns. Thus we believe that combining linguistic
analysis and pattern learning would be complemen-
tary and be beneficial to the whole system.
Work in combining linguistic analysis with pat-
terns include Weischedel et al (2004) and Jijkoun et
al. (2004) where manually constructed patterns are
used to augment linguistic features. However, man-
ual pattern construction critically depends on the do-
main knowledge of the pattern designer and often
has low coverage (Jijkoun et al, 2004). Automatic
pattern derivation is more appealing (Ravichandran
and Hovy, 2002).
In this work, we explore a hybrid approach to
combining deep linguistic analysis with automatic
pattern learning. We are interested in answering
the following four questions for Chinese definitional
question answering:
  How helpful are linguistic analysis and pattern
learning in definitional question answering?
  If pattern learning is useful, what kind of ques-
tion can pattern matching answer?
  How much human annotation is required for a
pattern based system to achieve reasonable per-
formance?
  If linguistic analysis is helpful, what linguistic
features are most useful?
To our knowledge, this is the first formal study of
these questions in Chinese definitional QA. To an-
swer these questions, we perform extensive experi-
ments on Chinese TDT4 data (Linguistic Data Con-
sortium, 2002-2003). We separate definitional ques-
tions into biographical (Who-is) questions and other
definitional (What-is) questions. We annotate some
question-answer snippets for pattern learning and
we perform deep linguistic analysis including pars-
ing, tagging, name entity recognition, co-reference,
and relation detection.
2 A Hybrid Approach to Definitional Ques-
tion Answering
The architecture of our QA system is shown in Fig-
ure 1. Given a question, we first use simple rules to
classify it as a ?Who-is? or ?What-is? question and
detect key words. Then we use a HMM-based IR
system (Miller et al, 1999) for document retrieval
by treating the question keywords as a query. To
speed up processing, we only use the top 1000 rel-
evant documents. We then select relevant sentences
among the returned relevant documents. A sentence
is considered relevant if it contains the query key-
word or contains a word that is co-referent to the
query term. Coreference is determined using an in-
formation extraction engine, SERIF (Ramshaw et
al., 2001). We then conduct deep linguistic anal-
ysis and pattern matching to extract candidate an-
swers. We rank all candidate answers by predeter-
mined feature ordering. At the same time, we per-
form redundancy detection based on  -gram over-
lap.
2.1 Deep Linguistic Analysis
We use SERIF (Ramshaw et al, 2001), a linguistic
analysis engine, to perform full parsing, name entity
detection, relation detection, and co-reference reso-
lution. We extract the following linguistic features:
1. Copula: a copula is a linking verb such as ?is?
or ?become?. An example of a copula feature
is ?Bill Gates is the CEO of Microsoft?. In this
case, ?CEO of Microsoft? will be extracted as
an answer to ?Who is Bill Gates??. To extract
copulas, SERIF traverses the parse trees of the
sentences and extracts copulas based on rules.
In Chinese, the rule for identifying a copula is
the POS tag ?VC?, standing for ?Verb Copula?.
The only copula verb in Chinese is ?

?.
2. Apposition: appositions are a pair of noun
phrases in which one modifies the other. For
example, In ?Tony Blair, the British Prime Min-
ister, ...?, the phrase ?the British Prime Min-
ister? is in apposition to ?Blair?. Extraction
of appositive features is similar to that of cop-
ula. SERIF traverses the parse tree and iden-
tifies appositives based on rules. A detailed
description of the algorithm is documented
308
Question Classification
Document Retrieval
Linguistic Analysis
Semantic Processing
Phrase Ranking
Redundancy Remove
Lists of Response
Answer Annotation
Name Tagging
Parsing
Preposition finding
Co?reference
Relation Extraction Training data
TreeBank
Name Annotation
Linguistic motivated
Pattern motivated
Question
Pattern MatchingPattern Learning
Figure 1: Question answering system structure
in (Ramshaw et al, 2001).
3. Proposition: propositions represent predicate-
argument structures and take the form:
predicate(    : 	 
  , ...,    :  
  ). The
most common roles include logical subject,
logical object, and object of a prepositional
phrase that modifies the predicate. For ex-
ample, ?Smith went to Spain? is represented
as a proposition, went(logical subject: Smith,
PP-to: Spain).
4. Relations: The SERIF linguistic analysis en-
gine also extracts relations between two ob-
jects. SERIF can extract 24 binary relations
defined in the ACE guidelines (Linguistic Data
Consortium, 2002), such as spouse-of, staff-of,
parent-of, management-of and so forth. Based
on question types, we use different relations, as
listed in Table 1.
Relations used for Who-Is questions
ROLE/MANAGEMENT, ROLE/GENERAL-STAFF,
ROLE/CITIZEN-OF, ROLE/FOUNDER,
ROLE/OWNER, AT/RESIDENCE,
SOC/SPOUSE, SOC/PARENT,
ROLE/MEMBER, SOC/OTHER-PROFESSIONAL
Relation used for What-Is questions
AT/BASED-IN, AT/LOCATED, PART/PART-OF
Table 1: Relations used in our system
Many relevant sentences do not contain the query
key words. Instead, they contain words that are co-
referent to the query. For example, in ?Yesterday UN
Secretary General Anan Requested Every Side...,
He said ... ?. The pronoun ?He? in the second sen-
tence refers to ?Anan? in the first sentence. To select
such sentences, we conduct co-reference resolution
using SERIF.
In addition, SERIF also provides name tagging,
identifying 29 types of entity names or descriptions,
such as locations, persons, organizations, and dis-
eases.
We also select complete sentences mentioning the
term being defined as backup answers if no other
features are identified.
The component performance of our linguistic
analysis is shown in Table 2.
Pre. Recall F
Parsing 0.813 0.828 0.820
Co-reference 0.920 0.897 0.908
Name-entity detection 0.765 0.753 0.759
Table 2: Linguistic analysis component performance
for Chinese
2.2 Surface Pattern Learning
We use two kinds of patterns: manually constructed
patterns and automatically derived patterns. A man-
ual pattern is a commonly used linguistic expression
that specifies aliases, super/subclass and member-
ship relations of a term (Xu et al, 2004). For exam-
ple, the expression ?tsunamis, also known as tidal
waves? gives an alternative term for tsunamis. We
309
use 23 manual patterns for Who-is questions and 14
manual patterns for What-is questions.
We also classify some special propositions as
manual patterns since they are specified by compu-
tational linguists. After a proposition is extracted,
it is matched against a list of predefined predicates.
If it is on the list, it is considered special and will
be ranked higher. In total, we designed 22 spe-
cial propositions for Who-is questions, such as  
 (become),   (elected as), and  (resign),
14 for What-is questions, such as 
	 (located at),

	 (created at), and   (also known as).
However, it is hard to manually construct such
patterns since it largely depends on the knowledge
of the pattern designer. Thus, we prefer patterns
that can be automatically derived from training data.
Some annotators labeled question-answer snippets.
Given a query question, the annotators were asked
to highlight the strings that can answer the question.
Though such a process still requires annotators to
have knowledge of what can be answers, it does not
require a computational linguist. Our pattern learn-
ing procedure is illustrated in Figure 2.
Generate Answer Snippet
Pattern Generalization
Pattern Selection
POS Tagging
Merging POS Tagging
and Answer Tagging
Answer Annotation
Figure 2: Surface Pattern Learning
Here we give an example to illustrate how pat-
tern learning works. The first step is annotation. An
example of Chinese answer annotation with English
translation is shown in Figure 3. Question words are
assigned the tag QTERM, answer words are tagged
ANSWER, and all other words are assigned BKGD,
standing for background words (not shown in the ex-
ample to make the annotation more readable).
To obtain patterns, we conduct full parsing to ob-
tain the full parse tree for a sentence. In our current
Chinese annotation:  ? Proceedings of ACL-08: HLT, pages 577?585,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A New String-to-Dependency Machine Translation Algorithm
with a Target Dependency Language Model
Libin Shen
BBN Technologies
Cambridge, MA 02138, USA
lshen@bbn.com
Jinxi Xu
BBN Technologies
Cambridge, MA 02138, USA
jxu@bbn.com
Ralph Weischedel
BBN Technologies
Cambridge, MA 02138, USA
weisched@bbn.com
Abstract
In this paper, we propose a novel string-to-
dependency algorithm for statistical machine
translation. With this new framework, we em-
ploy a target dependency language model dur-
ing decoding to exploit long distance word
relations, which are unavailable with a tra-
ditional n-gram language model. Our ex-
periments show that the string-to-dependency
decoder achieves 1.48 point improvement in
BLEU and 2.53 point improvement in TER
compared to a standard hierarchical string-to-
string system on the NIST 04 Chinese-English
evaluation set.
1 Introduction
In recent years, hierarchical methods have been suc-
cessfully applied to Statistical Machine Translation
(Graehl and Knight, 2004; Chiang, 2005; Ding and
Palmer, 2005; Quirk et al, 2005). In some language
pairs, i.e. Chinese-to-English translation, state-of-
the-art hierarchical systems show significant advan-
tage over phrasal systems in MT accuracy. For ex-
ample, Chiang (2007) showed that the Hiero system
achieved about 1 to 3 point improvement in BLEU
on the NIST 03/04/05 Chinese-English evaluation
sets compared to a start-of-the-art phrasal system.
Our work extends the hierarchical MT approach.
We propose a string-to-dependency model for MT,
which employs rules that represent the source side
as strings and the target side as dependency struc-
tures. We restrict the target side to the so called well-
formed dependency structures, in order to cover a
large set of non-constituent transfer rules (Marcu et
al., 2006), and enable efficient decoding through dy-
namic programming. We incorporate a dependency
language model during decoding, in order to exploit
long-distance word relations which are unavailable
with a traditional n-gram language model on target
strings.
For comparison purposes, we replicated the Hiero
decoder (Chiang, 2005) as our baseline. Our string-
to-dependency decoder shows 1.48 point improve-
ment in BLEU and 2.53 point improvement in TER
on the NIST 04 Chinese-English MT evaluation set.
In the rest of this section, we will briefly dis-
cuss previous work on hierarchical MT and de-
pendency representations, which motivated our re-
search. In section 2, we introduce the model of
string-to-dependency decoding. Section 3 illustrates
of the use of dependency language models. In sec-
tion 4, we describe the implementation details of our
MT system. We discuss experimental results in sec-
tion 5, compare to related work in section 6, and
draw conclusions in section 7.
1.1 Hierarchical Machine Translation
Graehl and Knight (2004) proposed the use of target-
tree-to-source-string transducers (xRS) to model
translation. In xRS rules, the right-hand-side(rhs)
of the target side is a tree with non-terminals(NTs),
while the rhs of the source side is a string with
NTs. Galley et al (2006) extended this string-to-tree
model by using Context-Free parse trees to represent
the target side. A tree could represent multi-level
transfer rules.
The Hiero decoder (Chiang, 2007) does not re-
quire explicit syntactic representation on either side
of the rules. Both source and target are strings with
NTs. Decoding is solved as chart parsing. Hiero can
be viewed as a hierarchical string-to-string model.
Ding and Palmer (2005) and Quirk et al (2005)
577
itwill
find
boy
the
interesting
Figure 1: The dependency tree for sentence the boy will
find it interesting
followed the tree-to-tree approach (Shieber and Sch-
abes, 1990) for translation. In their models, depen-
dency treelets are used to represent both the source
and the target sides. Decoding is implemented as
tree transduction preceded by source side depen-
dency parsing. While tree-to-tree models can rep-
resent richer structural information, existing tree-to-
tree models did not show advantage over string-to-
tree models on translation accuracy due to a much
larger search space.
One of the motivations of our work is to achieve
desirable trade-off between model capability and
search space through the use of the so called well-
formed dependency structures in rule representation.
1.2 Dependency Trees
Dependency trees reveal long-distance relations be-
tween words. For a given sentence, each word has a
parent word which it depends on, except for the root
word.
Figure 1 shows an example of a dependency tree.
Arrows point from the child to the parent. In this
example, the word find is the root.
Dependency trees are simpler in form than CFG
trees since there are no constituent labels. However,
dependency relations directly model semantic struc-
ture of a sentence. As such, dependency trees are a
desirable prior model of the target sentence.
1.3 Motivations for Well-Formed Dependency
Structures
We restrict ourselves to the so-called well-formed
target dependency structures based on the following
considerations.
Dynamic Programming
In (Ding and Palmer, 2005; Quirk et al, 2005),
there is no restriction on dependency treelets used in
transfer rules except for the size limit. This may re-
sult in a high dimensionality in hypothesis represen-
tation and make it hard to employ shared structures
for efficient dynamic programming.
In (Galley et al, 2004), rules contain NT slots and
combination is only allowed at those slots. There-
fore, the search space becomes much smaller. Fur-
thermore, shared structures can be easily defined
based on the labels of the slots.
In order to take advantage of dynamic program-
ming, we fixed the positions onto which another an-
other tree could be attached by specifying NTs in
dependency trees.
Rule Coverage
Marcu et al (2006) showed that many useful
phrasal rules cannot be represented as hierarchical
rules with the existing representation methods, even
with composed transfer rules (Galley et al, 2006).
For example, the following rule
? <(hong)Chinese, (DT(the) JJ(red))English>
is not a valid string-to-tree transfer rule since the red
is a partial constituent.
A number of techniques have been proposed to
improve rule coverage. (Marcu et al, 2006) and
(Galley et al, 2006) introduced artificial constituent
nodes dominating the phrase of interest. The bi-
narization method used by Wang et al (2007) can
cover many non-constituent rules also, but not all of
them. For example, it cannot handle the above ex-
ample. DeNeefe et al (2007) showed that the best
results were obtained by combing these methods.
In this paper, we use well-formed dependency
structures to handle the coverage of non-constituent
rules. The use of dependency structures is due to the
flexibility of dependency trees as a representation
method which does not rely on constituents (Fox,
2002; Ding and Palmer, 2005; Quirk et al, 2005).
The well-formedness of the dependency structures
enables efficient decoding through dynamic pro-
gramming.
2 String-to-Dependency Translation
2.1 Transfer Rules with Well-Formed
Dependency Structures
A string-to-dependency grammar G is a 4-tuple
G =< R, X, Tf , Te >, where R is a set of transfer
rules. X is the only non-terminal, which is similar
to the Hiero system (Chiang, 2007). Tf is a set of
578
terminals in the source language, and Te is a set of
terminals in the target language1 .
A string-to-dependency transfer rule R ? R is a
4-tuple R =< Sf , Se, D,A >, where Sf ? (Tf ?
{X})+ is a source string, Se ? (Te ? {X})+ is a
target string, D represents the dependency structure
for Se, and A is the alignment between Sf and Se.
Non-terminal alignments in A must be one-to-one.
In order to exclude undesirable structures, we
only allow Se whose dependency structure D is
well-formed, which we will define below. In addi-
tion, the same well-formedness requirement will be
applied to partial decoding results. Thus, we will be
able to employ shared structures to merge multiple
partial results.
Based on the results in previous work (DeNeefe
et al, 2007), we want to keep two kinds of depen-
dency structures. In one kind, we keep dependency
trees with a sub-root, where all the children of the
sub-root are complete. We call them fixed depen-
dency structures because the head is known or fixed.
In the other, we keep dependency structures of sib-
ling nodes of a common head, but the head itself is
unspecified or floating. Each of the siblings must be
a complete constituent. We call them floating de-
pendency structures. Floating structures can repre-
sent many linguistically meaningful non-constituent
structures: for example, like the red, a modifier of
a noun. Only those two kinds of dependency struc-
tures are well-formed structures in our system.
Furthermore, we operate over well-formed struc-
tures in a bottom-up style in decoding. However,
the description given above does not provide a clear
definition on how to combine those two types of
structures. In the rest of this section, we will pro-
vide formal definitions of well-formed structures and
combinatory operations over them, so that we can
easily manipulate well-formed structures in decod-
ing. Formal definitions also allow us to easily ex-
tend the framework to incorporate a dependency lan-
guage model in decoding. Examples will be pro-
vided along with the formal definitions.
Consider a sentence S = w1w2...wn. Let
d1d2...dn represent the parent word IDs for each
word. For example, d4 = 2 means that w4 depends
1We ignore the left hand side here because there is only one
non-terminal X . Of course, this formalism can be extended to
have multiple NTs.
itwill
find
boy
the
find
boy
(a) (b) (c)
Figure 2: Fixed dependency structures
boy will
the
interestingit
(a) (b)
Figure 3: Floating dependency structures
on w2. If wi is a root, we define di = 0.
Definition 1 A dependency structure di..j is fixed
on head h, where h ? [i, j], or fixed for short, if
and only if it meets the following conditions
? dh /? [i, j]
? ?k ? [i, j] and k 6= h, dk ? [i, j]
? ?k /? [i, j], dk = h or dk /? [i, j]
In addition, we say the category of di..j is
(?, h,?), where ? means this field is undefined.
Definition 2 A dependency structure di...dj is float-
ing with children C , for a non-empty set C ?
{i, ..., j}, or floating for short, if and only if it meets
the following conditions
? ?h /? [i, j], s.t.?k ? C, dk = h
? ?k ? [i, j] and k /? C, dk ? [i, j]
? ?k /? [i, j], dk /? [i, j]
We say the category of di..j is (C,?,?) if j < h,
or (?,?, C) otherwise. A category is composed of
the three fields (A, h,B), where h is used to repre-
sent the head, and A and B are designed to model
left and right dependents of the head respectively.
A dependency structure is well-formed if and
only if it is either fixed or floating.
Examples
We can represent dependency structures with
graphs. Figure 2 shows examples of fixed structures,
Figure 3 shows examples of floating structures, and
Figure 4 shows ill-formed dependency structures.
It is easy to verify that the structures in Figures
2 and 3 are well-formed. 4(a) is ill-formed because
579
interestingwill
findfind
boy
(a) (b)
Figure 4: Ill-formed dependency structures
boy does not have its child word the in the tree. 4(b)
is ill-formed because it is not a continuous segment.
As for the example the red mentioned above, it is
a well-formed floating dependency structure.
2.2 Operations on Well-Formed Dependency
Structures and Categories
One of the purposes of introducing floating depen-
dency structures is that siblings having a common
parent will become a well-defined entity, although
they are not considered a constituent. We always
build well-formed partial structures on the target
side in decoding. Furthermore, we combine partial
dependency structures in a way such that we can ob-
tain all possible well-formed but no ill-formed de-
pendency structures during bottom-up decoding.
The solution is to employ categories introduced
above. Each well-formed dependency structure has
a category. We can apply four combinatory oper-
ations over the categories. If we can combine two
categories with a certain category operation, we can
use a corresponding tree operation to combine two
dependency structures. The category of the com-
bined dependency structure is the result of the com-
binatory category operations.
We first introduce three meta category operations.
Two of them are unary operations, left raising (LR)
and right raising (RR), and one is the binary opera-
tion unification (UF).
First, the raising operations are used to turn a
completed fixed structure into a floating structure.
It is easy to verify the following theorem according
to the definitions.
Theorem 1 A fixed structure with category
(?, h,?) for span [i, j] is also a floating structure
with children {h} if there are no outside words
depending on word h.
?k /? [i, j], dk 6= h. (1)
Therefore we can always raise a fixed structure if we
assume it is complete, i.e. (1) holds.
itwill
find
boy
the
interesting
LA
LA LA RA RA
LC RC
Figure 5: A dependency tree with flexible combination
Definition 3 Meta Category Operations
? LR((?, h,?)) = ({h},?,?)
? RR((?, h,?)) = (?,?, {h})
? UF((A1, h1, B1), (A2, h2, B2)) = NORM((A1 t
A2, h1 t h2, B1 t B2))
Unification is well-defined if and only if we can
unify all three elements and the result is a valid fixed
or floating category. For example, we can unify a
fixed structure with a floating structure or two float-
ing structures in the same direction, but we cannot
unify two fixed structures.
h1 t h2 =
?
?
?
h1 if h2 = ?
h2 if h1 = ?
undefined otherwise
A1 t A2 =
?
?
?
A1 if A2 = ?
A2 if A1 = ?
A1 ?A2 otherwise
NORM((A, h, B)) =
?
?
?
?
?
?
?
(?, h,?) if h 6= ?
(A,?,?) if h = ?, B = ?
(?,?, B) if h = ?, A = ?
undefined otherwise
Next we introduce the four tree operations on de-
pendency structures. Instead of providing the formal
definition, we use figures to illustrate these opera-
tions to make it easy to understand. Figure 1 shows
a traditional dependency tree. Figure 5 shows the
four operations to combine partial dependency struc-
tures, which are left adjoining (LA), right adjoining
(RA), left concatenation (LC) and right concatena-
tion (RC).
Child and parent subtrees can be combined with
adjoining which is similar to the traditional depen-
dency formalism. We can either adjoin a fixed struc-
ture or a floating structure to the head of a fixed
structure.
Complete siblings can be combined via concate-
nation. We can concatenate two fixed structures, one
fixed structure with one floating structure, or two
floating structures in the same direction. The flex-
ibility of the order of operation allows us to take ad-
580
will
find
boy
the
LA
LA
LA
will
find
boy
the
LA
LA
LC
2
3 2
1
1
3
(b)(a)
Figure 6: Operations over well-formed structures
vantage of various translation fragments encoded in
transfer rules.
Figure 6 shows alternative ways of applying op-
erations on well-formed structures to build larger
structures in a bottom-up style. Numbers represent
the order of operation.
We use the same names for the operations on cat-
egories for the sake of convenience. We can easily
use the meta category operations to define the four
combinatory operations. The definition of the oper-
ations in the left direction is as follows. Those in the
right direction are similar.
Definition 4 Combinatory category operations
LA((A1,?,?), (?, h2,?))
= UF((A1,?,?), (?, h2,?))
LA((?, h1,?), (?, h2,?))
= UF(LR((?, h1,?)), (?, h2,?))
LC((A1,?,?), (A2,?,?))
= UF((A1,?,?), (A2,?,?))
LC((A1,?,?), (?, h2,?))
= UF((A1,?,?), LR((?, h2,?)))
LC((?, h1,?), (A2,?,?))
= UF(LR((?, h1,?)), (A2,?,?))
LC((?, h1,?), (?, h2,?))
= UF(LR((?, h1,?)), LR((?, h2,?)))
It is easy to verify the soundness and complete-
ness of category operations based on one-to-one
mapping of the conditions in the definitions of cor-
responding operations on dependency structures and
on categories.
Theorem 2 (soundness and completeness)
Suppose X and Y are well-formed dependency
structures. OP(cat(X), cat(Y )) is well-defined for
a given operation OP if and only if OP(X,Y ) is
well-defined. Furthermore,
cat(OP(X, Y )) = OP(cat(X), cat(Y ))
Suppose we have a dependency tree for a red apple,
where both a and red depend on apple. There are
two ways to compute the category of this string from
the bottom up.
cat(Da red apple)
= LA(cat(Da), LA(cat(Dred), cat(Dapple)))
= LA(LC(cat(Da), cat(Dred)), cat(Dapple))
Based on Theorem 2, it follows that combinatory
operation of categories has the confluence property,
since the result dependency structure is determined.
Corollary 1 (confluence) The category of a well-
formed dependency tree does not depend on the or-
der of category calculation.
With categories, we can easily track the types of
dependency structures and constrain operations in
decoding. For example, we have a rule with depen-
dency structure find ? X , where X right adjoins
to find. Suppose we have two floating structures2 ,
cat(X1) = ({he, will},?,?)
cat(X2) = (?,?, {it, interesting})
We can replace X by X2, but not by X1 based on
the definition of category operations.
2.3 Rule Extraction
Now we explain how we get the string-to-
dependency rules from training data. The procedure
is similar to (Chiang, 2007) except that we maintain
tree structures on the target side, instead of strings.
Given sentence-aligned bi-lingual training data,
we first use GIZA++ (Och and Ney, 2003) to gen-
erate word level alignment. We use a statistical CFG
parser to parse the English side of the training data,
and extract dependency trees with Magerman?s rules
(1995). Then we use heuristic rules to extract trans-
fer rules recursively based on the GIZA alignment
and the target dependency trees. The rule extraction
procedure is as follows.
1. Initialization:
All the 4-tuples (P i,jf , P m,ne , D,A) are valid
phrase alignments, where source phrase P i,jf is
2Here we use words instead of word indexes in categories to
make the example easy to understand.
581
it
find
interesting(D1)
(D2)
it
X
find
interesting
(D?)
Figure 7: Replacing it with X in D1
aligned to target phrase P m,ne under alignment3
A, and D, the dependency structure for P m,ne ,
is well-formed. All valid phrase templates are
valid rules templates.
2. Inference:
Let (P i,jf , P m,ne , D1, A) be a valid rule tem-
plate, and (P p,qf , P s,te , D2, A) a valid phrase
alignment, where [p, q] ? [i, j], [s, t] ? [m,n],
D2 is a sub-structure of D1, and at least one
word in P i,jf but not in P
p,q
f is aligned.
We create a new valid rule template
(P ?f , P ?e, D?, A), where we obtain P ?f by
replacing P p,qf with label X in P
i,j
f , and obtain
P ?e by replacing P s,te with X in P m,ne . Further-
more, We obtain D? by replacing sub-structure
D2 with X in D14. An example is shown in
Figure 7.
Among all valid rule templates, we collect those
that contain at most two NTs and at most seven ele-
ments in the source as transfer rules in our system.
2.4 Decoding
Following previous work on hierarchical MT (Chi-
ang, 2005; Galley et al, 2006), we solve decoding
as chart parsing. We view target dependency as the
hidden structure of source fragments.
The parser scans all source cells in a bottom-up
style, and checks matched transfer rules according to
the source side. Once there is a completed rule, we
build a larger dependency structure by substituting
component dependency structures for corresponding
NTs in the target dependency structure of rules.
Hypothesis dependency structures are organized
in a shared forest, or AND-OR structures. An AND-
3By P i,jf aligned to P
m,n
e , we mean all words in P i,jf are
either aligned to words in P m,ne or unaligned, and vice versa.
Furthermore, at least one word in P i,jf is aligned to a word in
P m,ne .
4If D2 is a floating structure, we need to merge several
dependency links into one.
structure represents an application of a rule over
component OR-structures, and an OR-structure rep-
resents a set of alternative AND-structures with the
same state. A state means a n-tuple that character-
izes the information that will be inquired by up-level
AND-structures.
Supposing we use a traditional tri-gram language
model in decoding, we need to specify the leftmost
two words and the rightmost two words in a state.
Since we only have a single NT X in the formalism
described above, we do not need to add the NT la-
bel in states. However, we need to specify one of
the three types of the dependency structure: fixed,
floating on the left side, or floating on the right side.
This information is encoded in the category of the
dependency structure.
In the next section, we will explain how to ex-
tend categories and states to exploit a dependency
language model during decoding.
3 Dependency Language Model
For the dependency tree in Figure 1, we calculate the
probability of the tree as follows
Prob = PT (find)
?PL(will|find-as-head)
?PL(boy|will, find-as-head)
?PL(the|boy-as-head)
?PR(it|find-as-head)
?PR(interesting|it, find-as-head)
Here PT (x) is the probability that word x is the
root of a dependency tree. PL and PR are left and
right side generative probabilities respectively. Let
wh be the head, and wL1wL2 ...wLn be the children
on the left side from the nearest to the farthest. Sup-
pose we use a tri-gram dependency LM,
PL(wL1wL2 ...wLn |wh-as-head)
= PL(wL1 |wh-as-head)
?PL(wL2 |wL1 , wh-as-head)
?...? PL(wLn |wLn?1 , wLn?2) (2)
wh-as-head represents wh used as the head, and
it is different from wh in the dependency language
model. The right side probability is similar.
In order to calculate the dependency language
model score, or depLM score for short, on the fly for
582
partial hypotheses in a bottom-up decoding, we need
to save more information in categories and states.
We use a 5-tuple (LF,LN, h,RN,RF ) to repre-
sent the category of a dependency structure. h rep-
resents the head. LF and RF represent the farthest
two children on the left and right sides respectively.
Similarly, LN and RN represent the nearest two
children on the left and right sides respectively. The
three types of categories are as follows.
? fixed: (LF,?, h,?, RF )
? floating left: (LF,LN,?,?,?)
? floating right: (?,?,?, RN,RF )
Similar operations as described in Section 2.2 are
used to keep track of the head and boundary child
nodes which are then used to compute depLM scores
in decoding. Due to the limit of space, we skip the
details here.
4 Implementation Details
Features
1. Probability of the source side given the target
side of a rule
2. Probability of the target side given the source
side of a rule
3. Word alignment probability
4. Number of target words
5. Number of concatenation rules used
6. Language model score
7. Dependency language model score
8. Discount on ill-formed dependency structures
We have eight features in our system. The values of
the first four features are accumulated on the rules
used in a translation. Following (Chiang, 2005),
we also use concatenation rules like X ? XX for
backup. The 5th feature counts the number of con-
catenation rules used in a translation. In our sys-
tem, we allow substitutions of dependency struc-
tures with unmatched categories, but there is a dis-
count for such substitutions.
Weight Optimization
We tune the weights with several rounds of
decoding-optimization. Following (Och, 2003), the
k-best results are accumulated as the input of the op-
timizer. Powell?s method is used for optimization
with 20 random starting points around the weight
vector of the last iteration.
Rescoring
We rescore 1000-best translations (Huang and
Chiang, 2005) by replacing the 3-gram LM score
with the 5-gram LM score computed offline.
5 Experiments
We carried out experiments on three models.
? baseline: replication of the Hiero system.
? filtered: a string-to-string MT system as in
baseline. However, we only keep the transfer
rules whose target side can be generated by a
well-formed dependency structure.
? str-dep: a string-to-dependency system with a
dependency LM.
We take the replicated Hiero system as our
baseline because it is the closest to our string-to-
dependency model. They have similar rule extrac-
tion and decoding algorithms. Both systems use
only one non-terminal label in rules. The major dif-
ference is in the representation of target structures.
We use dependency structures instead of strings;
thus, the comparison will show the contribution of
using dependency information in decoding.
All models are tuned on BLEU (Papineni et al,
2001), and evaluated on both BLEU and Translation
Error Rate (TER) (Snover et al, 2006) so that we
could detect over-tuning on one metric.
We used part of the NIST 2006 Chinese-
English large track data as well as some LDC
corpora collected for the DARPA GALE program
(LDC2005E83, LDC2006E34 and LDC2006G05)
as our bilingual training data. It contains about
178M/191M words in source/target. Hierarchical
rules were extracted from a subset which has about
35M/41M words5, and the rest of the training data
were used to extract phrasal rules as in (Och, 2003;
Chiang, 2005). The English side of this subset was
also used to train a 3-gram dependency LM. Tra-
ditional 3-gram and 5-gram LMs were trained on a
corpus of 6G words composed of the LDC Gigaword
corpus and text downloaded from Web (Bulyko et
al., 2007). We tuned the weights on NIST MT05
and tested on MT04.
5It includes eight corpora: LDC2002E18, LDC2003E07,
LDC2004T08 HK News, LDC2005E83, LDC2005T06,
LDC2005T10, LDC2006E34, and LDC2006G05
583
Model #Rules
baseline 140M
filtered 26M
str-dep 27M
Table 1: Number of transfer rules
Model BLEU% TER%lower mixed lower mixed
Decoding (3-gram LM)
baseline 38.18 35.77 58.91 56.60
filtered 37.92 35.48 57.80 55.43
str-dep 39.52 37.25 56.27 54.07
Rescoring (5-gram LM)
baseline 40.53 38.26 56.35 54.15
filtered 40.49 38.26 55.57 53.47
str-dep 41.60 39.47 55.06 52.96
Table 2: BLEU and TER scores on the test set.
Table 1 shows the number of transfer rules ex-
tracted from the training data for the tuning and
test sets. The constraint of well-formed dependency
structures greatly reduced the size of the rule set. Al-
though the rule size increased a little bit after incor-
porating dependency structures in rules, the size of
string-to-dependency rule set is less than 20% of the
baseline rule set size.
Table 2 shows the BLEU and TER scores
on MT04. On decoding output, the string-to-
dependency system achieved 1.48 point improve-
ment in BLEU and 2.53 point improvement in
TER compared to the baseline hierarchical string-
to-string system. After 5-gram rescoring, it achieved
1.21 point improvement in BLEU and 1.19 improve-
ment in TER. The filtered model does not show im-
provement on BLEU. The filtered string-to-string
rules can be viewed the string projection of string-
to-dependency rules. It means that just using depen-
dency structure does not provide an improvement on
performance. However, dependency structures al-
low the use of a dependency LM which gives rise to
significant improvement.
6 Discussion
The well-formed dependency structures defined here
are similar to the data structures in previous work on
mono-lingual parsing (Eisner and Satta, 1999; Mc-
Donald et al, 2005). However, here we have fixed
structures growing on both sides to exploit various
translation fragments learned in the training data,
while the operations in mono-lingual parsing were
designed to avoid artificial ambiguity of derivation.
Charniak et al (2003) described a two-step string-
to-CFG-tree translation model which employed a
syntax-based language model to select the best
translation from a target parse forest built in the first
step. Only translation probability P (F |E) was em-
ployed in the construction of the target forest due to
the complexity of the syntax-based LM. Since our
dependency LM models structures over target words
directly based on dependency trees, we can build a
single-step system. This dependency LM can also
be used in hierarchical MT systems using lexical-
ized CFG trees.
The use of a dependency LM in MT is similar to
the use of a structured LM in ASR (Xu et al, 2002),
which was also designed to exploit long-distance re-
lations. The depLM is used in a bottom-up style,
while SLM is employed in a left-to-right style.
7 Conclusions and Future Work
In this paper, we propose a novel string-to-
dependency algorithm for statistical machine trans-
lation. For comparison purposes, we replicated
the Hiero system as described in (Chiang, 2005).
Our string-to-dependency system generates 80%
fewer rules, and achieves 1.48 point improvement in
BLEU and 2.53 point improvement in TER on the
decoding output on the NIST 04 Chinese-English
evaluation set.
Dependency structures provide a desirable plat-
form to employ linguistic knowledge in MT. In the
future, we will continue our research in this direction
to carry out translation with deeper features, for ex-
ample, propositional structures (Palmer et al, 2005).
We believe that the fixed and floating structures pro-
posed in this paper can be extended to model predi-
cates and arguments.
Acknowledgments
This work was supported by DARPA/IPTO Contract
No. HR0011-06-C-0022 under the GALE program.
We are grateful to Roger Bock, Ivan Bulyko, Mike
Kayser, John Makhoul, Spyros Matsoukas, Antti-
Veikko Rosti, Rich Schwartz and Bing Zhang for
their help in running the experiments and construc-
tive comments to improve this paper.
584
References
I. Bulyko, S. Matsoukas, R. Schwartz, L. Nguyen, and
J. Makhoul. 2007. Language model adaptation in
machine translation from speech. In Proceedings of
the 32nd IEEE International Conference on Acoustics,
Speech, and Signal Processing (ICASSP).
E. Charniak, K. Knight, and K. Yamada. 2003. Syntax-
based language models for statistical machine transla-
tion. In Proceedings of MT Summit IX.
D. Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proceedings of the
43th Annual Meeting of the Association for Computa-
tional Linguistics (ACL).
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2).
S. DeNeefe, K. Knight, W. Wang, and D. Marcu. 2007.
What can syntax-based mt learn from phrase-based
mt? In Proceedings of the 2007 Conference of Em-
pirical Methods in Natural Language Processing.
Y. Ding and M. Palmer. 2005. Machine translation using
probabilistic synchronous dependency insertion gram-
mars. In Proceedings of the 43th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 541?548, Ann Arbor, Michigan, June.
J. Eisner and G. Satta. 1999. Efficient parsing for bilex-
ical context-free grammars and head automaton gram-
mars. In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics (ACL).
H. Fox. 2002. Phrasal cohesion and statistical machine
translation. In Proceedings of the 2002 Conference of
Empirical Methods in Natural Language Processing.
M. Galley, M. Hopkins, K. Knight, and D. Marcu. 2004.
What?s in a translation rule? In Proceedings of the
2004 Human Language Technology Conference of the
North American Chapter of the Association for Com-
putational Linguistics.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefea,
W. Wang, and I. Thayer. 2006. Scalable inference and
training of context-rich syntactic models. In COLING-
ACL ?06: Proceedings of 44th Annual Meeting of the
Association for Computational Linguistics and 21st
Int. Conf. on Computational Linguistics.
J. Graehl and K. Knight. 2004. Training tree transducers.
In Proceedings of the 2004 Human Language Technol-
ogy Conference of the North American Chapter of the
Association for Computational Linguistics.
L. Huang and D. Chiang. 2005. Better k-best parsing.
In Proceedings of the 9th International Workshop on
Parsing Technologies.
D. Magerman. 1995. Statistical decision-tree models for
parsing. In Proceedings of the 33rd Annual Meeting of
the Association for Computational Linguistics.
D. Marcu, W. Wang, A. Echihabi, and K. Knight. 2006.
SPMT: Statistical machine translation with syntacti-
fied target language phraases. In Proceedings of the
2006 Conference of Empirical Methods in Natural
Language Processing.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Pro-
ceedings of the 43th Annual Meeting of the Association
for Computational Linguistics (ACL).
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1).
F. J. Och. 2003. Minimum error rate training for sta-
tistical machine translation. In Erhard W. Hinrichs
and Dan Roth, editors, Proceedings of the 41st Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 160?167, Sapporo, Japan, July.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1).
K. Papineni, S. Roukos, and T. Ward. 2001. Bleu: a
method for automatic evaluation of machine transla-
tion. IBM Research Report, RC22176.
C. Quirk, A. Menezes, and C. Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of the 43th Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 271?279, Ann Arbor, Michigan,
June.
S. Shieber and Y. Schabes. 1990. Synchronous tree ad-
joining grammars. In Proceedings of COLING ?90:
The 13th Int. Conf. on Computational Linguistics.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate with
targeted human annotation. In Proceedings of Associ-
ation for Machine Translation in the Americas.
W. Wang, K. Knight, and D. Marcu. 2007. Binarizing
syntax trees to improve syntax-based machine transla-
tion accuracy. In Proceedings of the 2007 Conference
of Empirical Methods in Natural Language Process-
ing.
P. Xu, C. Chelba, and F. Jelinek. 2002. A study on richer
syntactic dependencies for structured language model-
ing. In Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics (ACL).
585
Cross-lingual Information Retrieval using Hidden Markov Models 
Jinxi Xu 
BBN Technologies 
70 Fawcett St. 
Cambridge, MA, USA 02138 
jxu@bbn.com 
Ralph Weischedel 
BBN Technologies 
70 Fawcett St. 
Cambridge, MA, USA 02138 
weischedel @bbn.com 
Abstract 
This paper presents empirical results in 
cross-lingual information retrieval using 
English queries to access Chinese 
documents (TREC-5 and TREC-6) and 
Spanish documents (TREC-4). Since our 
interest is in languages where resources 
may be minimal, we use an integrated 
probabilistic model that requires only a 
bilingual dictionary as a resource. We 
explore how a combined probability 
model of term translation and retrieval can 
reduce the effect of translation ambiguity. 
In addition, we estimate an upper bound 
on performance, if translation ambiguity 
were a solved problem. We also measure 
performance as a function of bilingual 
dictionary size. 
1 Introduction 
Cross-language information retrieval (CLIR) can 
serve both those users with a smattering of 
knowledge of other languages and also those 
fluent in them. For those with limited 
knowledge of the other language(s), CLIR offers 
a wide pool of documents, even though the user 
does not have the skill to prepare ahigh quality 
query in the other language(s). Once documents 
are retrieved, machine translation or human 
translation, if desired, can make the documents 
usable. For the user who is fluent in two or 
more languages, even though e/she may be able 
to formulate good queries in each of the source 
languages, CLIR relieves the user from having 
to do so. 
Most CLIR studies have been based on a variant 
of tf-idf; our experiments instead use a hidden 
Markov model (HMM) to estimate the 
probability that a document is relevant given the 
query. We integrated two simple estimates of 
term translation probability into the mono- 
lingual HMM model, giving an estimate of the 
probability that a document is relevant given a 
query in another language. 
In this paper we address the following questions: 
? How can a combined probability model of 
term translation and retrieval minimize the 
effect of translation ambiguity? (Sections 3, 
5, 6, 7, and 10) 
? What is the upper bound performance using 
bilingual dictionary lookup for term 
translation? (Section 8) 
? How much does performance d grade due to 
omissions from the bilingual dictionary and 
how does performance vary with size of 
such a dictionary? (Sections 8-9) 
All experiments were performed using a 
common baseline, an HMM-based (mono- 
lingual) indexing and retrieval engine. In order 
to design controlled experiments for the 
questions above, the IR system was run without 
sophisticated query expansion techniques. 
Our experiments are based on the Chinese 
materials of TREC-5 and TREC-6 and the 
Spanish materials of TREC-4. 
2 HMM for Mono-Lingual Retrieval 
Following Miller et al, 1999, the IR system 
ranks documents according to the probability 
that a document D is relevant given the query Q, 
P(D is R IQ). Using Bayes Rule, and the fact 
that P(Q) is constant for a given query, and our 
initial assumption of a uniform a priori 
95 
probability that a document is relevant, ranking 
documents according to P(Q\[D is R) is the same 
as ranking them according to P(D is RIQ). The 
approach therefore estimates the probability that 
a query Q is generated, given the document D is 
relevant. (A glossary of symbols used appears 
below.) 
We use x to represent the language (e.g. 
English) for which retrieval is carried out. 
According to that model of monolingual 
retrieval, it can be shown that 
p(Q \[ D is R) = I I  (aP(W \[ Gx) + (1- a)e(w ID)), 
W inQ 
where W's are query words in Q. Miller et al 
estimated probabilities as follows: 
* The transition probability a is 0.7 using the 
EM algorithm (Rabiner, 1989) on the TREC4 
ad-hoc query set. 
number of occurrences of W in C x 
? e0e  IGx)= length of Cx 
which is the general language probability for 
word W in language x.
number of occurrences of W in D 
? e (WlD)  = 
length of D 
In principle, any large corpus Cx that is 
representative of language x can be used in 
computing the general language probabilities. 
In practice, the collection to be searched is
used for that purpose. The length of a 
Q a query 
English query 
a document 
a document in foreign language y 
document is relevant 
a word 
an English corpus 
a corpus in language x 
QX 
D 
Dr 
D isR  
W 
Gx 
Cx 
Wx 
BL 
an English word 
foreign language y 
Wy a word in 
a bilingual dictionary 
A Glossary of Notation used in Formulas 
collection is the sum of the document 
lengths. 
3 HMM for Cross-lingual IR 
For CLIR we extend the query generation 
process o that a document Dy written in 
language y can generate a query Qx in language 
x. We use Wx to denote aword in x and Wy to 
denote aword in y. As before, to model general 
query words from language x, we estimate P(Wx 
\]Gx) by using a large corpus Cx in language x.
Also as before, we estimate P(WyIDy) to be the 
sample distribution of Wy in Dy. 
We use P(Wx\[Wy) to denote the probability that 
Wy is translated as Wx. Though terms often 
should not be translated independent of their 
context, we make that simplifying assumption 
here. We assume that the possible translations 
are specified by a bilingual lexicon BL. Since 
the event spaces for Wy's in P(WyIDy) are 
mutually exclusive, we can compute the output 
probability P(WxIDy): 
P(WxIDy)= ~P(WylDy)P(WxIWy)  
W inBL y 
We compute P(Q~IDy is R) as below: 
P(Qx IDr /sR) = I~I(aetwx IG,)+O-a)P(W~ IDy)) 
w.~,o. 
The above model generates queries from 
documents, that is, it attempts o determine how 
likely a particular query is given a relevant 
document. The retrieval system, however, can 
use either query translation or document 
translation. We chose query translation over 
document translation for its flexibility, since it 
allowed us to experiment with a new method of 
estimating the translation probabilities without 
changing the index structure. 
4 Experimental Set-up 
For retrieval using English queries to search 
Chinese documents, we used the TREC5 and 
TREC6 Chinese data which consists of 164,789 
documents from the Xinhua News Agency and 
People's Daily, averaging 450 Chinese 
characters/document. Each of the TREC topics 
has three Chinese fields: title, description and 
96 
narrative, plus manually translated, English 
versions of each. We corrected some of the 
English queries that contained errors, such as 
"Dali Lama" instead of the correct "Dalai Lama" 
and "Medina" instead of "Medellin." Stop 
words and stop phrases were removed. We 
created three versions of Chinese queries and 
three versions of English queries: short (title 
only), medium (title and description), and long 
(all three fields). 
For retrieval using English queries to search 
Spanish documents, we used the TREC4 
Spanish data, which has 57,868 documents. It 
has 25 queries in Spanish with manual 
translations toEnglish. We will denote the 
Chinese data sets as Trec5C and Trec6C and the 
Spanish data set as Trec4S. 
We used a Chinese-English lexicon from the 
Linguistic Data Consortium (LDC). We pre- 
processed the dictionary as follows: 
1. Stem Chinese words via a simple algorithm 
to remove common suffixes and prefixes. 
2. Use the Porter stemmer on English words. 
3. Split English phrases into words. If an 
English phrase is a translation for a Chinese 
word, each word in the phrase is taken as a 
separate translation for the Chinese word. ~ 
4. Estimate the translation probabilities. (We 
first report results assuming a uniform 
distribution on a word's translations. If a 
Chinese word c has n translations el, e2, ...en. 
each of them will be assigned equal probability, 
i.e., P(ei lc)=l/n.  Section 10 supplements this 
with a corpus-based distribution.) 
5. Invert he lexicon to make it an English- 
Chinese lexicon. That is, for each English word 
e, we associate it with a list of Chinese words cl, 
c2, ... Cm together with non-zero translation 
probabilities P( elc~). 
The resulting English-Chinese l xicon has 
80,000 English words. On average, each 
English word has 2.3 Chinese translations. 
For Spanish, we downloaded a bilingual 
English-Spanish lexicon from the Internet 
(http://www.activa.arrakis.es) containing around 
22,000 English words (16,000 English stems) 
and processed it similarly. Each English word 
has around 1.5 translations on average. A co- 
occurrence based stemmer (Xu and Croft, 1998) 
was used to stem Spanish words. One 
difference from the treatment of Chinese is to 
include the English word as one of its own 
translations in addition to its Spanish 
translations in the lexicon. This is useful for 
translating proper nouns, which often have 
identical spellings in English and Spanish but 
are routinely excluded from a lexicon. 
One problem is the segmentation f Chinese 
text, since Chinese has no spaces between 
words. In these initial experiments, we relied on 
a simple sub-string matching algorithm to 
extract words from Chinese text. To extract 
words from a string of Chinese characters, the 
algorithm examines any sub-string of length 2 or 
greater and recognizes it as a Chinese word if it 
is in a predefined dictionary (the LDC lexicon in 
our case). In addition, any single character 
which is not part of any recognized Chinese 
words in the first step is taken as a Chinese 
word. Note that this algorithm can extract a
compound Chinese word as well as its 
components. For example, the Chinese word for 
"particle physics" as well as the Chinese words 
for "particle" and "physics" will be extracted. 
This seems desirable because it ensures the 
retrieval algorithm will match both the 
compound words as well as their components. 
The above algorithm was used in processing 
Chinese documents and Chinese queries. 
English data from the 2 GB of TREC disks l&2 
was used to estimate P(WlG,..ngti~h), the general 
language probabilities for English words. The 
evaluation metric used in this study is the 
average precision using the trec_eval program 
(Voorhees and Harman, 1997). Mono-lingual 
retrieval results (using the Chinese and Spanish 
queries) provided our baseline, with the HMM 
retrieval system (Miller et al 1999). 
1 Clearly, this is not correct; however, it 
simplified implementation. 
97 
5 Retrieval Results 
Table 2 reports average precision for mono- 
lingual retrieval, average precision for cross- 
lingual, and the relative performance ratio of 
cross-lingual retrieval to mono-lingual. 
Relative performance of cross-lingual IR varies 
between 67% and 84% of mono-lingual IR. 
Trec6 Chinese queries have a somewhat higher 
relative performance than Trec5 Chinese 
queries. Longer queries have higher elative 
performance than short queries in general. 
Overall, cross-lingual performance using our 
HMM retrieval model is around 76% of mono- 
lingual retrieval. A comparison of our mono- 
lingual results with Trec5 Chinese and Trec6 
Chinese results published in the TREC 
proceedings (Voorhees and Harman, 1997, 
1998) shows that our mono-lingual results are 
close to the top performers in the TREC 
conferences. Our Spanish mono-lingual 
performance is also comparable tothe top 
automatic runs of the TREC4 Spanish task 
(Harrnan, 1996). Since these mono-lingual 
results were obtained without using 
sophisticated query processing techniques such 
as query expansion, we believe the mono-lingual 
results form a valid baseline. 
Query sets Mono- Cross- % of 
lingual lingual Mono- 
lingual 
Trec5C-short 0.2830 0.1889 67% 
Trec5C-medium 0.3427 0.2449 72% 
Trec5C-long 0.3750 0.2735 73% 
Trec6C-short 0.3423 0.2617 77% 
Trec6C-medium 0.4606 0.3872 84% 
Trec6C-long 0.5104 0.4206 82% 
Trec4S 0.2252 0.1729 77% 
Table 2: Comparing mono-lingual and cross- 
lingual retrieval performance. The scores on 
the monolingual and cross-lingual columns are 
average precision. 
6 Comparison with other Methods 
In this section we compare our approach with 
two other approaches. One approach is "simple 
substitution", i.e., replacing a query term with 
all its translations and treating the translated 
query as a bag of words in mono-lingual 
retrieval. Suppose we have a simple query 
Q=(a, b), the translations for a are al, a2, a3, and 
the translations for b are bl, b2. The translated 
query would be (at, a2, a3, b~, b2). Since all terms 
are treated as equal in the translated query, this 
gives terms with more translations (potentially 
the more common terms) more credit in 
retrieval, even though such terms hould 
potentially be given less credit if they are more 
common. Also, a document matching different 
translations ofone term in the original query 
may be ranked higher than a document that 
matches translations ofdifferent terms in the 
original query. That is, a document that 
contains terms at, a2 and a3 may be ranked 
higher than a document which contains terms at 
and bl. However, the second ocument is more 
likely to be relevant since correct translations of
the query terms are more likely to co-occur 
(Ballesteros and Croft, 1998). 
A second method is to structure the translated 
query, separating the translations for one term 
from translations for other terms. This approach 
limits how much credit he retrieval algorithm 
can give to a single term in the original query 
and prevents the translations ofone or a few 
terms from swamping the whole query. There 
are several variations of such a method 
(Ballesteros and Croft, 1998; Pirkola, 1998; Hull 
1997). One such method is to treat different 
translations ofthe same term as synonyms. 
Ballesteros, for example, used the INQUERY 
(Callan et al 1995) synonym operator to group 
translations ofdifferent query terms. However, 
if a term has two translations inthe target 
language, it will treat hem as equal even though 
one of them is more likely to be the correct 
translation than the other. By contrast, our 
HMM approach supports translation 
probabilities. The synonym approach is
equivalent to changing all non-zero translation 
probabilities P(W~\[ Wy)'s to 1 in our retrieyal 
function. Even estimating uniform translation 
probabilities gives higher weights to 
unambiguous translations and lower weights to 
highly ambiguous translations. 
98 
These intuitions are supported empirically by the 
results in Table 3. We can see that the HMM 
performs best for every query set. Simple 
substitution performs worst. The synonym 
approach is significantly better than substitution, 
but is consistently worse than the HMM 
translations were kept in disambiguation, the 
improvement would be 4% for Trec6C-medium. 
The results of this manual disambiguation 
suggest that there are limits to automatic 
disambiguation. 
Substi- Synonym HMM 
tution 
Trec5C-long 0.0391 0.2306 0.2735 
Trec6C-long 0.0941 0.3842 0.4206 
Trec4S 0.0935 0.1594 0.1729 
Table 3: Comparing different methods of 
query translation. All numbers are average 
precision. 
7 Impact of Translation Ambiguity 
To get an upper bound on performance ofany 
disambiguation technique, we manually 
disambiguated the Trec5C-medium, Trec6C- 
medium and Trec4S queries. That is, for each 
English query term, a native Chinese or Spanish 
speaker scanned the list of translations in the 
bilingual exicon and kept one translation 
deemed to be the best for the English term and 
discarded the rest. If none of the translations 
was correct, the first one was chosen. 
The results in Table 4 show that manual 
disambiguation improves performance by 17% 
on Trec5C, 4% on Trec4S, but not at all on 
Trec6C. Furthermore, the improvement on 
Trec5C appears to be caused by big 
improvements for a small number of queries. 
The one-sided t-test (Hull, 1993) at significance 
level 0.05 indicated that the improvement on 
Trec5C is not statistically significant. 
It seems urprising that disambiguation does not 
help at all for Trec6C. We found that many 
terms have more than one valid translation. For 
example, the word "flood" (as in "flood 
control") has 4 valid Chinese translations. Using 
all of them achieves the desirable ffect of query 
expansion. It appears that for Trec6C, the benefit 
of disambiguation is cancelled by choosing only 
one of several alternatives, discarding those 
other good translations. If multiple correct 
Query sets 
Trec5C-medium 
Trec6C-medium 
Trec4S 
(+4%) 
Degree of Disambiguation 
None Manual % of 
Mono- 
lingual 
0.2449 0.2873 84% 
(+17%) 
0.3872 0.3830 83% 
(-1%) 
0.1729 0.1799 80% 
Table 4: The effect of disambiguation on 
retrieval performance. The scores reported 
are average precision. 
8 Impact of Missing Translations 
Results in the previous ection showed that 
manual disambiguation can bring performance 
of cross-lingual IR to around 82% of mono- 
lingual IR. The remaining performance gap 
between mono-lingual nd cross-lingual IR is 
likely to be caused by the incompleteness of the 
bilingual exicon used for query translation, i.e., 
missing translations for some query terms. This 
may be a more serious problem for cross-lingual 
IR than ambiguity. To test the conjecture, for 
each English query term, a native speaker in 
Chinese or Spanish manually checked whether 
the bilingual exicon contains acorrect 
translation for the term in the context of the 
query. If it does not, a correct ranslation for the 
term was added to the lexicon. For the query 
sets Trec5C-medium and Trec6C-medium, there 
are 100 query terms for which the lexicon does 
not have a correct ranslation. This represents 
19% of the 520 query terms (a term is counted 
only once in one query). For the query set 
Trec4S, the percentage is 12%. 
The results in Table 5 show that with augmented 
lexicons, performance of cross-lingual IR is 
91%, 99% and 95% of mono-lingual IR on 
Trec5C-mediurn, Trec6C-medium and Trec4S. 
99 
The improvement over using the original exicon 
is 28%, 18% and 23% respectively. The results 
demonstrate the importance cff a complete 
lexicon. Compared with the results in section 7, 
the results here suggest that missing translations 
have a much larger impact on cross-lingual IR 
than translation ambiguity does. 
Query sets Original Augmented % o f  
lexicon lexicon Mono- 
lingual 
Trec5C- 0.2449 0.3131 91% 
medium (+28%) 
Trec6C- 0.3872 0.4589 99% 
medium (+18%) 
Trec4S 0.1729 0.2128 95% 
(+23%) 
Table 5: The impact of missing the right 
translations on retrieval performance. All  
scores are average precision. 
9 Impact of  Lexicon Size 
In this section we measure CLIR performance as
a function of lexicon size. We sorted the 
English words from TREC disks l&2 in order of 
decreasing frequency. For a lexicon of size n, 
we keep only the n most frequent English words. 
The upper graph in Figure 1 shows the curve of 
cross-lingual IR performance asa function of the 
size of the lexicon based on the Chinese short 
and medium-length queries. Retrieval 
performance was averaged over Trec5C and 
Trec6C. Initially retrieval performance increases 
sharply with lexicon size. After the dictionary 
exceeds 20,000, performance l vels off. An 
examination of the translated queries hows that 
words not appearing in the 20,000-word lexicon 
usually do not appear in the larger lexicons 
either. Thus, increases in the general lexicon 
beyond 20,000 words did not result in a 
substantial increase in the coverage of the query 
terms. 
The lower graph in Figure 1 plots the retrieval 
performance asa function of the percent of the 
full lexicon. The figure shows that short queries 
are more susceptible toincompleteness of the 
lexicon than longer queries. Using a 7,000-word 
lexicon, the short queries only achieve 75% of 
their performance with the full lexicon. In 
comparison, the medium-length queries achieve 
87% of their performance. 
\[--*- Short Query 4-- Medium Query J 
0.35 
0.3 
o.25 
== o.2 
0.15 
~. 0.1 
O.O5 
0 
0 10000 20000 30000 40000 50000 60000 
Lexicon Size 
\[ -*-- Short + Medium \] 
_-- 120 
o lO0I 
~g 00 
0 o o_  60 
,f. o 
0 
O,, 
10000 20000 30000 40000 5(X)O0 60000 
Lexicon Size 
Figure 1 Impact of lexicon size on cross-lingual IR 
performance 
We categorized the missing terms and found that 
most of them are proper nouns (especially 
locations and person ames), highly technical 
terms, or numbers. Such words understandably 
do not normally appear in traditional lexicons. 
Translation of numbers can be solved using 
simple rules. Transliteration, a technique that 
guesses the likely translations of a word based 
on pronunciation, can be readily used in 
translating proper nouns. 
Another technique is automatic discovery of 
translations from parallel or non-parallel corpora 
(Fung and Mckeown, 1997). Since traditional 
lexicons are more or less static repositories of 
knowledge, techniques that discover translation 
from newly published materials can supplement 
them with corpus-specific vocabularies. 
100 
10 Using a Parallel Corpus 
In this section we estimate translation 
probabilities from a parallel corpus rather than 
assuming uniform likelihood as in section 4. A 
Hong Kong News corpus obtained from the 
Linguistic Data Consortium has 9,769 news 
stories in Chinese with English translations. It
has 3.4 million English words. Since the 
documents are not exact ranslations of each 
other, occasionally having extra or missing 
sentences, we used document-level co- 
occurrence toestimate translation probabilities. 
The Chinese documents were "segmented" using 
the technique discussed in section 4. Let co(e,c) 
be the number of parallel documents where an 
English word e and a Chinese word c co-occur, 
and df(c) be the document frequency of c. If a 
Chinese word c has n possible translations el to 
en in the bilingual exicon, we estimate the 
corpus translation probability as: 
co(e i , c) 
P_  corpus(ell c) = 
i=n 
MAX(df (c ) ,  ~ co(e i, c)) 
i=1 
Since several translations for c may co-occur in 
a document, ~co(e~ c) can be greater than df(c). 
Using the maximum of the two ensures that 
E P_corpus(eilc)_<l. 
Instead of relying solely on corpus-based 
estimates from a small parallel corpus, we 
employ a mixture model as follows: 
P( e I c) = ~ P _ corpus( eI c) + (1- #)P_ lexicon( e\[ c) 
The retrieval results in Table 6 show that 
combining the probability estimates from the 
lexicon and the parallel corpus does improve 
retrieval performance. The best results are 
obtained when 13=0.7; this is better than using 
uniform probabilities by 9% on Trec5C-medium 
and 4% on Trec6C-medium. Using the corpus 
probability estimates alone results in a 
significant drop in performance, the parallel 
corpus is not large enough nor diverse nough 
for reliable stimation of the translation 
probabilities. In fact, many words do not appear 
in the corpus at all. With a larger and better 
parallel corpus, more weight should be given to 
the probability estimates from the corpus. 
Trec5 - Trec6- 
medium medium 
P_lexicon 0.2449 0.3872 
13=0.3 0.2557 0.3980 
13=0.5 0.2605 0.4021 
13=0.7 0.2658 0.4035 
P_corpus 0.2293 0.2971 
Table 6: Performance with different values 
of 13. All scores are average precision. 
11 Related Work 
Other studies which view IR as a query 
generation process include Maron and Kuhns, 
1960; Hiemstra nd Kraaij, 1999; Ponte and 
Croft, 1998; Miller et al 1999. Our work has 
focused on cross-lingual retrieval. 
Many approaches tocross-lingual IR have been 
published. One common approach is using 
Machine Translation (MT) to translate the 
queries to the language of the documents or 
translate documents othe language of the 
queries (Gey et al 1999; Oard, 1998). For most 
languages, there are no MT systems at all. Our 
focus is on languages where no MT exists, but a 
bilingual dictionary may exist or may be 
derived. 
Another common approach is term translation, 
e.g., via a bilingual exicon. (Davis and Ogden, 
1997; Ballesteros and Croft, 1997; Hull and 
Grefenstette, 1996). While word sense 
disambiguation has been a central topic in 
previous tudies for cross-lingual IR, our study 
suggests that using multiple weighted 
translations and compensating for the 
incompleteness of the lexicon may be more 
valuable. Other studies on the value of 
disambiguation for cross-lingual IR include 
Hiernstra nd de Jong, 1999; Hull, 1997. 
Sanderson, 1994 studied the issue of 
disarnbiguation for mono-lingual IR. 
101 
The third approach to cross-lingual retrieval is to 
map queries and documents o some 
intermediate r presentation, e.g latent semantic 
indexing (LSI) (Littman et al 1998), or the 
General Vector space model (GVSM), 
(Carbonell et al 1997). We believe our 
approach is computationally ess costly than 
(LSI and GVSM) and assumes less resources 
(WordNet in Diekema et al, 1999). 
12 Conclusions and Future Work 
We proposed an approach to cross-lingual IR 
based on hidden Markov models, where the 
system estimates the probability that a query in 
one language could be generated from a 
document in another language. Experiments 
using the TREC5 and TREC6 Chinese test sets 
and the TREC4 Spanish test set show the 
following: 
? Our retrieval model can reduce the 
performance d gradation due to translation 
ambiguity This had been a major limiting 
factor for other query-translation 
approaches. 
? Some earlier studies uggested that query 
translation is not an effective approach to 
cross-lingual IR (Carbonell et al 1997). 
However, our results uggest that query 
translation can be effective particularly if a 
bilingual dictionary is the primary bilingual 
resource available. 
? Manual selection from the translations in the 
bilingual dictionary improves performance 
little over the HMM. 
? We believe an algorithm cannot rule out a 
possible translation with absolute 
confidence; it is more effective to rely on 
probability estimation/re-estimation to 
differentiate likely translations and unlikely 
translations. 
? Rather than translation ambiguity, a more 
serious limitation to effective cross-lingual 
IR is incompleteness of the bilingual exicon 
used for query translation. 
? Cross-lingual IR performance is typically 
75% that of mono-lingual for our HMM on 
the Chinese and Spanish collections. 
Future improvements in cross-lingual IR will 
come by attacking the incompleteness of 
bilingual dictionaries and by improved query 
expansion and context-dependent translation. 
Our current model assumes that query terms are 
generated one at time. We would like to extend 
the model to allow phrase generation i the 
query generation process. We also wish to 
explore techniques to extend bilingual exicons. 
References 
L. Ballesteros and W.B. Croft 1997. "Phrasal 
translation and query expansion techniques for 
cross-language information retrieval." Proceedings 
of the 20th ACM SIGIR International Conference 
on Research and Development in Information 
Retrieval 1997, pp. 84-91. 
L. Ballesteros and W.B. Croft, 1998. "Resolving 
ambiguity for cross-language retrieval." 
Proceedings of the 21st ACM SIGIR Conference on 
Research and Development in Information 
Retrieval, 1998, pp. 64-71. 
J.P. Callan, W.B. Croft and J. Broglio. 1995. "TREC 
and TIPSTER Experiments with INQUERY". 
Information Processing and Management, pages 
327-343, 1995. 
J. Carbonell, Y. Yang, R. Frederking, R. Brown, Y. 
Geng and D. Lee, 1997. "Translingual information 
retrieval: a comparative evaluation." In 
Proceedings of the 15th International Joint 
Conference on Artificial Intelligence, 1997. 
M. Davis and W. Ogden, 1997. "QUILT: 
Implementing a Large Scale Cross-language Text 
Retrieval System." Proceedings of ACM SIGIR 
Conference, 1997. 
A. Diekema, F. Oroumchain, P. Sheridan and E. 
Liddy, 1999. "TREC-7 Evaluation of Conceptual 
Interlingual Document Retrieval (CINDOR) in 
English and French." TREC7 Proceedings, NIST 
special publication. 
P. Fung and K. Mckeown. "Finding Terminology 
Translations from Non-parallel Corpora." The 5 'h 
Annual Workshop on Very Large Corpora, Hong 
Kong: August 1997, 192n202 
F. Gey, J. He and A. Chen, 1999. "Manual queries 
and Machine Translation in cross-language 
retrieval at TREC-7". In TREC7 Proceedings, 
NIST Special Publication, 1999. 
102 
Harman, 1996. The TREC-4 Proceedings. NIST 
Special publication, 1996. 
D. Hiemstra nd F. de Jong, 1999. "Disambiguafion 
strategies for Cross-language Information 
Retrieval." Proceedings of the third European 
Conference on Research and Advanced Technology 
for Digital Libraries, pp. 274-293, 1999. 
D. Hiemstra and W. Kraaij, 1999. "Twenty-One at 
TREC-7: ad-hoc and cross-language track." In 
TREC-7 Proceedings, NIST Special Publication, 
1999. 
D. Hull, 1993. "Using Statistical Testing in the 
Evaluation of Retrieval Experiments." Proceedings 
of the 16th Annual International ACM SIGIR 
Conference on Research and Development in 
Information Retrieval, pages 329-338, 1993. 
D. A. Hull and G. Grefenstette, 1996. "A dictionary- 
based approach to multilingual information 
retrieval". Proceedings of ACM SIGIR Conference, 
1996. 
D. A. Hull, 1997. "Using structured queries for 
disambiguation in cross-language information 
retrieval." In AAAI Symposium on Cross-Language 
Text and Speech Retrieval. AAAI, 1997. 
M. E. Maron and K. L. Kuhns, 1960. "On 
Relevance, Probabilistic Indexing and Information 
Retrieval." Journal of the Association for 
": Computing Machinery, 1960, pp 216-244. 
D. Miller, T. Leek and R. Schwartz, 1999. "A 
Hidden Markov Model Information Retrieval 
System." Proceedings of the 22nd Annual 
International ACM S1GIR Conference on Research 
and Development in Information Retrieval, pages 
214-221, 1999. 
D.W. Oard, 1998. "A comparative study of query and 
document translation for cross-language 
information retrieval." In Proceedings of the Third 
Conference of the Association for Machine 
Translation in America (AMTA ), 1998. 
Ari Pirkola, 1998. "The effects of query structure 
and dictionary setups in dictionary-based cross- 
language information retrieval." Proceedings of 
ACM SIGIR Conference, 1998, pp 55-63. 
J. Ponte and W.B. Croft, 1998. "A Language 
Modeling Approach to Information Retrieval." 
Proceedings of the 21st Annual International ACM 
S1GIR Conference on Research and Development 
in Information Retrieval, pages 275-281, 1998. 
L. Rabiner, 1989. "A tutorial on hidden Markov 
models and selected applications in speech 
recognition." Proc. IEEE 77, pp. 257-286, 1989. 
M. Sanderson. "Word sense disambiguation and 
information retrieval." Proceedings of ACM SIGIR 
Conference, 1994, pp 142-15 I. 
Voorhees and Harman, 1997. TREC-5 Proceedings. 
E. Voorhees and D. Harman, Editors. NIST 
special publication. 
Voorhees and Harman, 1998. TREC-6 Proceedings. 
E. Voorhees and D. Harrnan, Editors. NIST 
special publication. 
J. Xu and W.B. Croft, 1998. "Corpus-based 
stemming using co-occurrence of word variants". 
ACM Transactions on Information Systems, 
January 1998, vol 16, no. 1. 
103 
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 616?625,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Statistical Machine Translation with a Factorized Grammar
Libin Shen and Bing Zhang and Spyros Matsoukas and
Jinxi Xu and Ralph Weischedel
Raytheon BBN Technologies
Cambridge, MA 02138, USA
{lshen,bzhang,smatsouk,jxu,weisched}@bbn.com
Abstract
In modern machine translation practice, a sta-
tistical phrasal or hierarchical translation sys-
tem usually relies on a huge set of trans-
lation rules extracted from bi-lingual train-
ing data. This approach not only results in
space and efficiency issues, but also suffers
from the sparse data problem. In this paper,
we propose to use factorized grammars, an
idea widely accepted in the field of linguis-
tic grammar construction, to generalize trans-
lation rules, so as to solve these two prob-
lems. We designed a method to take advantage
of the XTAG English Grammar to facilitate
the extraction of factorized rules. We experi-
mented on various setups of low-resource lan-
guage translation, and showed consistent sig-
nificant improvement in BLEU over state-of-
the-art string-to-dependency baseline systems
with 200K words of bi-lingual training data.
1 Introduction
A statistical phrasal (Koehn et al, 2003; Och and
Ney, 2004) or hierarchical (Chiang, 2005; Marcu
et al, 2006) machine translation system usually re-
lies on a very large set of translation rules extracted
from bi-lingual training data with heuristic methods
on word alignment results. According to our own
experience, we obtain about 200GB of rules from
training data of about 50M words on each side. This
immediately becomes an engineering challenge on
space and search efficiency.
A common practice to circumvent this problem
is to filter the rules based on development sets in the
step of rule extraction or before the decoding phrase,
instead of building a real distributed system. How-
ever, this strategy only works for research systems,
for which the segments for translation are always
fixed.
However, do we really need such a large rule set
to represent information from the training data of
much smaller size? Linguists in the grammar con-
struction field already showed us a perfect solution
to a similar problem. The answer is to use a fac-
torized grammar. Linguists decompose lexicalized
linguistic structures into two parts, (unlexicalized)
templates and lexical items. Templates are further
organized into families. Each family is associated
with a set of lexical items which can be used to lex-
icalize all the templates in this family. For example,
the XTAG English Grammar (XTAG-Group, 2001),
a hand-crafted grammar based on the Tree Adjoin-
ing Grammar (TAG) (Joshi and Schabes, 1997) for-
malism, is a grammar of this kind, which employs
factorization with LTAG e-tree templates and lexical
items.
Factorized grammars not only relieve the burden
on space and search, but also alleviate the sparse
data problem, especially for low-resource language
translation with few training data. With a factored
model, we do not need to observe exact ?template
? lexical item? occurrences in training. New rules
can be generated from template families and lexical
items either offline or on the fly, explicitly or im-
plicitly. In fact, the factorization approach has been
successfully applied on the morphological level in
previous study on MT (Koehn and Hoang, 2007). In
this work, we will go further to investigate factoriza-
tion of rule structures by exploiting the rich XTAG
English Grammar.
We evaluate the effect of using factorized trans-
lation grammars on various setups of low-resource
language translation, since low-resource MT suffers
greatly on poor generalization capability of trans-
616
lation rules. With the help of high-level linguis-
tic knowledge for generalization, factorized gram-
mars provide consistent significant improvement
in BLEU (Papineni et al, 2001) over string-to-
dependency baseline systems with 200K words of
bi-lingual training data.
This work also closes the gap between compact
hand-crafted translation rules and large-scale unor-
ganized automatic rules. This may lead to a more ef-
fective and efficient statistical translation model that
could better leverage generic linguistic knowledge
in MT.
In the rest of this paper, we will first provide a
short description of our baseline system in Section 2.
Then, we will introduce factorized translation gram-
mars in Section 3. We will illustrate the use of the
XTAG English Grammar to facilitate the extraction
of factorized rules in Section 4. Implementation de-
tails are provided in Section 5. Experimental results
are reported in Section 6.
2 A Baseline String-to-Tree Model
As the baseline of our new algorithm, we use a
string-to-dependency system as described in (Shen
et al, 2008). There are several reasons why we take
this model as our baseline. First, it uses syntactic
tree structures on the target side, which makes it easy
to exploit linguistic information. Second, depen-
dency structures are relatively easier to implement,
as compared to phrase structure grammars. Third,
a string-to-dependency system provides state-of-the-
art performance on translation accuracy, so that im-
provement over such a system will be more convinc-
ing.
Here, we provide a brief description of the base-
line string-to-dependency system, for the sake of
completeness. Readers can refer to (Shen et al,
2008; Shen et al, 2009) for related information.
In the baseline string-to-dependency model, each
translation rule is composed of two parts, source and
target. The source sides is a string rewriting rule,
and the target side is a tree rewriting rule. Both
sides can contain non-terminals, and source and tar-
get non-terminals are one-to-one aligned. Thus, in
the decoding phase, non-terminal replacement for
both sides are synchronized.
Decoding is solved with a generic chart parsing
algorithm. The source side of a translation rule is
used to detect when this rule can be applied. The tar-
get side of the rule provides a hypothesis tree struc-
ture for the matched span. Mono-lingual parsing can
be viewed as a special case of this generic algorithm,
for which the source string is a projection of the tar-
get tree structure.
Figure 1 shows three examples of string-to-
dependency translation rules. For the sake of con-
venience, we use English for both source and target.
Upper-cased words represent source, while lower-
cased words represent target. X is used for non-
terminals for both sides, and non-terminal alignment
is represented with subscripts.
In Figure 1, the top boxes mean the source side,
and the bottom boxes mean the target side. As for
the third rule, FUN Q stands for a function word in
the source language that represents a question.
3 Translation with a Factorized Grammar
We continue with the example rules in Figure 1.
Suppose, we have ?... HATE ... FUN Q? in a given
test segment. There is no rule having both HATE
and FUN Q on its source side. Therefore, we have
to translate these two source words separately. For
example, we may use the second rule in Figure 1.
Thus, HATE will be translated into hates, which is
wrong.
Intuitively, we would like to have translation rule
that tell us how to translate X1 HATE X2 FUN Q
as in Figure 2. It is not available directly from the
training data. However, if we obtain the three rules
in Figure 1, we are able to predict this missing rule.
Furthermore, if we know like and hate are in the
same syntactic/semantic class in the source or target
language, we will be very confident on the validity
of this hypothesis rule.
Now, we propose a factorized grammar to solve
this generalization problem. In addition, translation
rules represented with the new formalism will be
more compact.
3.1 Factorized Rules
We decompose a translation rule into two parts,
a pair of lexical items and an unlexicalized tem-
plate. It is similar to the solution in the XTAG En-
glish Grammar (XTAG-Group, 2001), while here we
617
X1  LIKE  X2
likes
X1 X2
X1  HATE  X2
hates
X1 X2
X1  LIKE X2  FUN_Q
like
does X1 X2
Figure 1: Three examples of string-to-dependency translation rules.
X1  V  X2
VBZ
X1 X2
X1  V  X2
VBZ
X1 X2
X1  V  X2  FUN_Q
VB
does X1 X2
Figure 3: Templates for rules in Figure 1.
X1  HATE  X2  FUN_Q
hate
does X1 X2
Figure 2: An example of a missing rule.
work on two languages at the same time.
For each rule, we first detect a pair of aligned head
words. Then, we extract the stems of this word pair
as lexical items, and replace them with their POS
tags in the rule. Thus, the original rule becomes an
unlexicalized rule template.
As for the three example rules in Figure 1, we will
extract lexical items (LIKE, like), (HATE, hate) and
(LIKE, like) respectively. We obtain the same lexical
items from the first and the third rules.
The resultant templates are shown in Figure 3.
Here, V represents a verb on the source side, VB
stands for a verb in the base form, and VBZ means
a verb in the third person singular present form as
in the Penn Treebank representation (Marcus et al,
1994).
In the XTAG English Grammar, tree templates for
transitive verbs are grouped into a family. All transi-
tive verbs are associated with this family. Here, we
assume that the rule templates representing struc-
tural variations of the same word class can also be
organized into a template family. For example, as
shown in Figure 4, templates and lexical items are
associated with families. It should be noted that
a template or a lexical item can be associated with
more than one family.
Another level of indirection like this provides
more generalization capability. As for the missing
618
X1  V  X2
VBZ
X1 X2
Family Transitive_3
X1  V  X2  FUN_Q
VB
does X1 X2
X1  V  FUN_Past
VBD
X1
Family Intransitive_2
( LIKE, like ) ( HATE, hate ) ( OPEN, open ) ( HAPPEN, happen )
Figure 4: Templates and lexical items are associated with families.
rule in Figure 2, we can now generate it by replac-
ing the POS tags in the second template of Figure
4 with lexical items (HATE, hate) with their correct
inflections. Both the template and the lexical items
here are associated with the family Transitive 3..
3.2 Statistical Models
Another level of indirection also leads to a desirable
back-off model. We decompose a rule R into to two
parts, its template PR and its lexical items LR. As-
suming they are independent, then we can compute
Pr(R) as
Pr(R) = Pr(PR)Pr(LR), or
Pr(R) =
?
F Pr(PR|F )Pr(LR|F )Pr(F ), (1)
if they are conditionally independent for each fam-
ily F . In this way, we can have a good estimate for
rules that do not appear in the training data. The
second generative model will also be useful for un-
supervised learning of families and related probabil-
ities.
In this paper, we approximate families by using
target (English) side linguistic knowledge as what
we will explain in Section 4, so this changes the def-
inition of the task. In short, we will be given a list of
families. We will also be given an association table
B(L,F ) for lexical items L and families F , such
that B(L,F ) = true if and only L is associated
with F , but we do not know the distributions.
Let S be the source side of a rule or a rule tem-
plate, T the target side of a rule of a rule template.
We define Prb, the back-off conditional model of
templates, as follows.
Prb(PS |PT , L) =
?
F :B(L,F ) #(PS , PT , F )
?
F :B(L,F ) #(PT , F )
, (2)
where # stands for the count of events.
Let P and L be the template and lexical items of
R respectively. Let Prt be the MLE model obtained
from the training data. The smoothed probability is
then defined as follows.
Pr(RS |RT ) = (1 ? ?)Prt(RS |RT )
+?Prb(PS |PT , L), (3)
where ? is a parameter. We fix it to 0.1 in later ex-
periments. Conditional probability Pr(RT |RS) is
defined in a similar way.
3.3 Discussion
The factorized models discussed in the previous sec-
tion can greatly alleviate the sparse data problem,
especially for low-resource translation tasks. How-
ever, when the training data is small, it is not easy to
619
learn families. Therefore, to use unsupervised learn-
ing with a model like (1) somehow reduces a hard
translation problem to another one of the same diffi-
culty, when the training data is small.
However, in many cases, we do have extra infor-
mation that we can take advantage of. For example,
if the target language has rich resources, although
the source language is a low-density one, we can ex-
ploit the linguistic knowledge on the target side, and
carry it over to bi-lingual structures of the translation
model. The setup of X-to-English translation tasks
is just like this. This will be the topic of the next
section. We leave unsupervised learning of factor-
ized translation grammars for future research.
4 Using A Mono-Lingual Grammar
In this section, we will focus on X-to-English trans-
lation, and explain how to use English resources to
build a factorized translation grammar. Although we
use English as an example, this approach can be ap-
plied to any language pairs that have certain linguis-
tic resources on one side.
As shown in Figure 4, intuitively, the families
are intersection of the word families of the two lan-
guages involved, which means that they are refine-
ment of the English word families. For example,
a sub-set of the English transitive families may be
translated in the same way, so they share the same
set of templates. This is why we named the two fam-
ilies Transitive 3 and Intransitive 2 in Figure 4.
Therefore, we approximate bi-lingual families
with English families first. In future, we can use
them as the initial values for unsupervised learning.
In order to learn English families, we need to take
away the source side information in Figure 4, and
we end up with a template?family?word graph as
shown in Figure 5. We can learn this model on large
mono-lingual data if necessary.
What is very interesting is that there already exists
a hand-crafted solution for this model. This is the
XTAG English Grammar (XTAG-Group, 2001).
The XTAG English Grammar is a large-scale En-
glish grammar based on the TAG formalism ex-
tended with lexicalization and unification-based fea-
ture structures. It consists of morphological, syn-
tactic, and tree databases. The syntactic database
contains the information that we have represented
in Figure 5 and many other useful linguistic annota-
tions, e.g. features.
The XTAG English grammar contains 1,004 tem-
plates, organized in 53 families, and 221 individual
templates. About 30,000 lexical items are associ-
ated with these families and individual templates 1.
In addition, it also has the richest English morpho-
logical lexicon with 317,000 inflected items derived
from 90,000 stems. We use this resource to predict
POS tags and inflections of lexical items.
In our applications, we select all the verb fami-
lies plus one each for nouns, adjectives and adverbs.
We use the families of the English word as the fam-
ilies of bi-lingual lexical items. Therefore, we have
a list of about 20 families and an association table
as described in Section 3.2. Of course, one can use
other linguistic resources if similar family informa-
tion is provided, e.g. VerbNet (Kipper et al, 2006)
or WordNet (Fellbaum, 1998).
5 Implementation
Nowadays, machine translation systems become
more and more complicated. It takes time to write
a decoder from scratch and hook it with various
modules, so it is not the best solution for research
purpose. A common practice is to reduce a new
translation model to an old one, so that we can use
an existing system, and see the effect of the new
model quickly. For example, the tree-based model
proposed in (Carreras and Collins, 2009) used a
phrasal decoder for sub-clause translation, and re-
cently, DeNeefe and Knight (2009) reduced a TAG-
based translation model to a CFG-based model by
applying all possible adjunction operations offline
and stored the results as rules, which were then used
by an existing syntax-based decoder.
Here, we use a similar method. Instead of build-
ing a new decoder that uses factorized grammars,
we reduce factorized rules to baseline string-to-
dependency rules by performing combination of
templates and lexical items in an offline mode. This
is similar to the rule generation method in (DeNeefe
and Knight, 2009). The procedure is as follows.
In the rule extraction phase, we first extract all the
string-to-dependency rules with the baseline system.
1More information about XTAG is available online at
http://www.cis.upenn.edu/?xtag .
620
VBZ
X1 X2
Family Transitive
VB
does X1 X2
VBD
X1
Family Intransitive
like hate open happen
Figure 5: Templates, families, and words in the XTAG English Grammar.
For each extracted rule, we try to split it into various
?template?lexical item? pairs by choosing different
aligned words for delexicalization, which turns rules
in Figure 1 into lexical items and templates in Fig-
ure 3. Events of templates and lexical items are
counted according to the family of the target En-
glish word. If an English word is associated with
more than one family, the count is distributed uni-
formly among these families. In this way, we collect
sufficient statistics for the back-off model in (2).
For each family, we keep the top 200 most fre-
quent templates. Then, we apply them to all the
lexical items in this families, and save the gener-
ated rules. We merge the new rules with the original
one. The conditional probabilities for the rules in the
combined set is smoothed according to (2) and (3).
Obviously, using only the 200 most frequent tem-
plates for each family is just a rough approxima-
tion. An exact implementation of a new decoder for
factorized grammars can make better use of all the
templates. However, the experiments will show that
even an approximation like this can already provide
significant improvement on small training data sets,
i.e. with no more than 2M words.
Since we implement template application in an of-
fline mode, we can use exactly the same decoding
and optimization algorithms as the baseline. The de-
coder is a generic chart parsing algorithm that gen-
erates target dependency trees from source string in-
put. The optimizer is an L-BFGS algorithm that
maximizes expected BLEU scores on n-best hy-
potheses (Devlin, 2009).
6 Experiments on Low-Resource Setups
We tested the performance of using factorized gram-
mars on low-resource MT setups. As what we noted
above, the sparse data problem is a major issue when
there is not enough training data. This is one of the
cases that a factorized grammar would help.
We did not tested on real low-resource languages.
Instead, we mimic the low-resource setup with two
of the most frequently used language pairs, Arabic-
to-English and Chinese-to-English, on newswire
and web genres. Experiments on these setups will
be reported in Section 6.1. Working on a language
which actually has more resources allows us to study
the effect of training data size. This will be reported
in Section 6.2. In Section 6.3, we will show exam-
ples of templates learned from the Arabic-to-English
training data.
6.1 Languages and Genres
The Arabic-to-English training data contains about
200K (target) words randomly selected from an
LDC corpus, LDC2006G05 A2E set, plus an
Arabic-English dictionary with about 89K items.
We build our development sets from GALE P4 sets.
There are one tune set and two test sets for the MT
systems 2. TEST-1 has about 5000 segments and
TEST-2 has about 3000 segments.
2One of the two test sets will later be used to tune an MT
combination system.
621
MODEL
TUNE TEST-1 TEST-2
BLEU %BL MET BLEU %BL MET BLEU %BL MET
Arabic-to-English newswire
baseline 21.07 12.41 43.77 19.96 11.42 42.79 21.09 11.03 43.74
factorized 21.70 13.17 44.85 20.52 11.70 43.83 21.36 11.77 44.72
Arabic-to-English web
baseline 10.26 5.02 32.78 9.40 4.87 31.26 14.11 7.34 35.93
factorized 10.67 5.34 33.83 9.74 5.20 32.52 14.66 7.69 37.11
Chinese-to-English newswire
baseline 13.17 8.04 44.70 19.62 9.32 48.60 14.53 6.82 45.34
factorized 13.91 8.09 45.03 20.48 9.70 48.61 15.16 7.37 45.31
Chinese-to-English web
baseline 11.52 5.96 42.18 11.44 6.07 41.90 9.83 4.66 39.71
factorized 11.98 6.31 42.84 11.72 5.88 42.55 10.25 5.34 40.34
Table 1: Experimental results on Arabic-to-English / Chinese-to-English newswire and web data. %BL stands for
BLEU scores for documents whose BLEU scores are in the bottom 75% to 90% range of all documents. MET stands
for METEOR scores.
The Chinese-to-English training data contains
about 200K (target) words randomly selected from
LDC2006G05 C2E set, plus a Chinese-English dic-
tionary (LDC2002L27) with about 68K items. The
development data setup is similar to that of Arabic-
to-English experiments.
Chinese-to-English translation is from a morphol-
ogy poor language to a morphology rich language,
while Arabic-to-English translation is in the oppo-
site direction. It will be interesting to see if factor-
ized grammars help on both cases. Furthermore, we
also test on two genres, newswire and web, for both
languages.
Table 1 lists the experimental results of all the four
conditions. The tuning metric is expected BLEU.
We are also interested in the BLEU scores for doc-
uments whose BLEU scores are in the bottom 75%
to 90% range of all documents. We mark it as %BL
in the table. This metric represents how a system
performances on difficult documents. It is important
to certain percentile evaluations. We also measure
METEOR (Banerjee and Lavie, 2005) scores for all
systems.
The system using factorized grammars shows
BLEU improvement in all conditions. We measure
the significance of BLEU improvement with paired
bootstrap resampling as described by (Koehn, 2004).
All the BLEU improvements are over 95% confi-
dence level. The new system also improves %BL
and METEOR in most of the cases.
6.2 Training Data Size
The experiments to be presented in this section
are designed to measure the effect of training data
size. We select Arabic web for this set of experi-
ments. Since the original Arabic-to-English train-
ing data LDC2006G05 is a small one, we switch to
LDC2006E25, which has about 3.5M target words
in total. We randomly select 125K, 250K, 500K, 1M
and 2M sub-sets from the whole data set. A larger
one always includes a smaller one. We still tune on
expected BLEU, and test on BLEU, %BL and ME-
TEOR.
The average BLEU improvement on test sets is
about 0.6 on the 125K set, but it gradually dimin-
ishes. For better observation, we draw the curves of
BLEU improvement along with significance test re-
sults for each training set. As shown in Figure 6 and
7, more improvement is observed with fewer train-
ing data. This fits well with fact that the baseline MT
model suffers more on the sparse data problem with
smaller training data. The reason why the improve-
ment diminishes on the full data set could be that the
rough approximation with 200 most frequent tem-
plates cannot fully take advantage of this paradigm,
which will be discussed in the next section.
622
MODEL SIZE
TUNE TEST-1 TEST-2
BLEU %BL MET BLEU %BL MET BLEU %BL MET
Arabic-to-English web
baseline
125K
8.54 2.96 28.87 7.41 2.82 26.95 11.29 5.06 31.37
factorized 8.99 3.44 30.40 7.92 3.57 28.63 12.04 6.06 32.87
baseline
250K
10.18 4.70 32.21 8.94 4.35 30.31 13.71 6.93 35.14
factorized 10.57 4.96 33.22 9.34 4.78 31.51 14.02 7.28 36.25
baseline
500K
12.18 5.84 35.59 10.82 5.77 33.62 16.48 8.30 38.73
factorized 12.40 6.01 36.15 11.14 5.96 34.38 16.76 8.53 39.27
baseline
1M
13.95 7.17 38.49 12.48 7.12 36.56 18.86 10.00 42.18
factorized 14.14 7.41 38.99 12.66 7.34 37.14 19.11 10.29 42.56
baseline
2M
15.74 8.38 41.15 14.18 8.17 39.26 20.96 11.95 45.18
factorized 15.92 8.81 41.51 14.34 8.25 39.68 21.42 12.05 45.51
baseline
3.5M
16.95 9.76 43.03 15.47 9.08 41.28 22.83 13.24 47.05
factorized 17.07 9.99 43.18 15.49 8.77 41.41 22.72 13.10 47.23
Table 2: Experimental results on Arabic web. %BL stands for BLEU scores for documents whose BLEU scores are
in the bottom 75% to 90% range of all documents. MET stands for METEOR scores.
-0.4
-0.2
 0
 0.2
 0.4
 0.6
 0.8
 1
 100000  1e+06
BL
EU
 im
pro
vem
ent
data size in logscale
TEST-1
Figure 6: BLEU Improvement with 95% confidence
range by using factorized grammars on TEST-1.
6.3 Example Templates
Figure 8 lists seven Arabic-to-English templates
randomly selected from the transitive verb family.
TMPL 151 is an interesting one. It helps to alleviate
the pronoun dropping problem in Arabic. However,
we notice that most of the templates in the 200 lists
are rather simple. More sophisticated solutions are
needed to go deep into the list to find out better tem-
plates in future.
It will be interesting to find an automatic or
semi-automatic way to discover source counterparts
of target treelets in the XTAG English Grammar.
-0.4
-0.2
 0
 0.2
 0.4
 0.6
 0.8
 1
 100000  1e+06
BL
EU
 im
pro
vem
ent
data size in logscale
TEST-2
Figure 7: BLEU Improvement with 95% confidence
range by using factorized grammars on TEST-2.
Generic rules like this will be very close to hand-
craft translate rules that people have accumulated for
rule-based MT systems.
7 Conclusions and Future Work
In this paper, we proposed a novel statistical ma-
chine translation model using a factorized structure-
based translation grammar. This model not only al-
leviates the sparse data problem but only relieves the
burden on space and search, both of which are im-
minent issues for the popular phrasal and/or hierar-
chical MT systems.
623
VVB
TMPL_1
X1  V
VBD
X1
TMPL_121
TMPL_31
V  X1
for
VBG
X1
TMPL_151
TMPL_61
V  X1
VBN
by
X1
TMPL_181
TMPL_91
X1  V
VBD
X1
the
V  X1
VBD
he X1
X1  V  X2
VBZ
X1 X2
Figure 8: Randomly selected Arabic-to-English templates from the transitive verb family.
We took low-resource language translation, espe-
cially X-to-English translation tasks, for case study.
We designed a method to exploit family informa-
tion in the XTAG English Grammar to facilitate the
extraction of factorized rules. We tested the new
model on low-resource translation, and the use of
factorized models showed significant improvement
in BLEU on systems with 200K words of bi-lingual
training data of various language pairs and genres.
The factorized translation grammar proposed here
shows an interesting way of using richer syntactic
resources, with high potential for future research.
In future, we will explore various learning meth-
ods for better estimation of families, templates and
lexical items. The target linguistic knowledge that
we used in this paper will provide a nice starting
point for unsupervised learning algorithms.
We will also try to further exploit the factorized
representation with discriminative learning. Fea-
tures defined on templates and families will have
good generalization capability.
Acknowledgments
This work was supported by DARPA/IPTO Contract
HR0011-06-C-0022 under the GALE program3. We
thank Aravind Joshi, Scott Miller, Richard Schwartz
and anonymous reviewers for valuable comments.
3Distribution Statement ?A? (Approved for Public Release,
Distribution Unlimited). The views, opinions, and/or find-
ings contained in this article/presentation are those of the au-
thor/presenter and should not be interpreted as representing the
official views or policies, either expressed or implied, of the De-
fense Advanced Research Projects Agency or the Department of
Defense.
624
References
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved cor-
relation with human judgments. In Proceedings of the
43th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 101?104, Ann Arbor,
MI.
Xavier Carreras and Michael Collins. 2009. Non-
projective parsing for statistical machine translation.
In Proceedings of the 2009 Conference of Empirical
Methods in Natural Language Processing, pages 200?
209, Singapore.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 263?270, Ann Ar-
bor, MI.
Steve DeNeefe and Kevin Knight. 2009. Synchronous
tree adjoining machine translation. In Proceedings of
the 2009 Conference of Empirical Methods in Natural
Language Processing, pages 727?736, Singapore.
Jacob Devlin. 2009. Lexical features for statistical ma-
chine translation. Master?s thesis, Univ. of Maryland.
Christiane Fellbaum, editor. 1998. WordNet: an elec-
tronic lexical database. The MIT Press.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
adjoining grammars. In G. Rozenberg and A. Salo-
maa, editors, Handbook of Formal Languages, vol-
ume 3, pages 69?124. Springer-Verlag.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2006. Extensive classifications of en-
glish verbs. In Proceedings of the 12th EURALEX In-
ternational Congress.
P. Koehn and H. Hoang. 2007. Factored translation mod-
els. In Proceedings of the 2007 Conference of Empiri-
cal Methods in Natural Language Processing.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase based translation. In Proceedings
of the 2003 Human Language Technology Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 48?54, Edmonton,
Canada.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
the 2004 Conference of Empirical Methods in Natu-
ral Language Processing, pages 388?395, Barcelona,
Spain.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical machine
translation with syntactified target language phrases.
In Proceedings of the 2006 Conference of Empirical
Methods in Natural Language Processing, pages 44?
52, Sydney, Australia.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1994. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313?330.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4).
Kishore Papineni, Salim Roukos, and Todd Ward. 2001.
Bleu: a method for automatic evaluation of machine
translation. IBM Research Report, RC22176.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of the 46th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL).
Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2009. Effective Use of Lin-
guistic and Contextual Information for Statistical Ma-
chine Translation. In Proceedings of the 2009 Confer-
ence of Empirical Methods in Natural Language Pro-
cessing, pages 72?80, Singapore.
XTAG-Group. 2001. A lexicalized tree adjoining gram-
mar for english. Technical Report 01-03, IRCS, Univ.
of Pennsylvania.
625
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 667?673,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Combining Unsupervised and Supervised Alignments for MT:
An Empirical Study
Jinxi Xu and Antti-Veikko I. Rosti
Raytheon BBN Technologies, 10 Moulton Street, Cambridge, MA 02138, USA
{jxu,arosti}@bbn.com
Abstract
Word alignment plays a central role in statisti-
cal MT (SMT) since almost all SMT systems
extract translation rules from word aligned
parallel training data. While most SMT
systems use unsupervised algorithms (e.g.
GIZA++) for training word alignment, super-
vised methods, which exploit a small amount
of human-aligned data, have become increas-
ingly popular recently. This work empirically
studies the performance of these two classes
of alignment algorithms and explores strate-
gies to combine them to improve overall sys-
tem performance. We used two unsupervised
aligners, GIZA++ and HMM, and one super-
vised aligner, ITG, in this study. To avoid lan-
guage and genre specific conclusions, we ran
experiments on test sets consisting of two lan-
guage pairs (Chinese-to-English and Arabic-
to-English) and two genres (newswire and we-
blog). Results show that the two classes of al-
gorithms achieve the same level of MT perfor-
mance. Modest improvements were achieved
by taking the union of the translation gram-
mars extracted from different alignments. Sig-
nificant improvements (around 1.0 in BLEU)
were achieved by combining outputs of differ-
ent systems trained with different alignments.
The improvements are consistent across lan-
guages and genres.
1 Introduction
Word alignment plays a central role in training sta-
tistical machine translation (SMT) systems since al-
most all SMT systems extract translation rules from
word aligned parallel training data. Until recently,
most SMT systems used GIZA++ (Och and Ney,
2003), an unsupervised algorithm, for aligning par-
allel training data. In recent years, with the availabil-
ity of human aligned training data, supervised meth-
ods (e.g. the ITG aligner (Haghighi et al, 2009))
have become increasingly popular.
The main objective of this work is to show the
two classes (unsupervised and supervised) of al-
gorithms are complementary and combining them
will improve overall system performance. The use
of human aligned training data allows supervised
methods such as ITG to more accurately align fre-
quent words, such as the alignments of Chinese par-
ticles (e.g. ?bei?, ?de?, etc) to their English equiv-
alents (e.g. ?is/are/was/..?, ?of?, etc). On the other
hand, supervised methods can be affected by sub-
optimal alignments in hand-aligned data. For exam-
ple, the hand-aligned data used in our experiments
contain some coarse-grained alignments (e.g. ?lian-
he guo? to ?United Nations?) although fine-grained
alignments (?lian-he? to ?United? and ?guo? to ?Na-
tions?) are usually more appropriate for SMT. Un-
supervised methods are less likely to be affected
by this problem. We used two well studied unsu-
pervised aligners, GIZA++ (Och and Ney, 2003)
and HMM (Liang et al, 2006) and one supervised
aligner, ITG (Haghighi et al, 2009) as representa-
tives in this work.
We explored two techniques to combine different
alignment algorithms. One is to take the union of
the translation rules extracted from alignments pro-
duced by different aligners. This is motivated by
studies that showed that the coverage of translation
rules is critical to SMT (DeNeefe et al, 2007). The
667
other method is to combine the outputs of different
MT systems trained using different aligners. As-
suming different systems make independent errors,
system combination can generate a better transla-
tion than those of individual systems through voting
(Rosti et al, 2007).
Our work differs from previous work in two ways.
Past studies of combining alternative alignments fo-
cused on minimizing alignment errors, usually by
merging alternative alignments for a sentence pair
into a single alignment with the fewest number of
incorrect alignment links (Ayan and Dorr, 2006). In
contrast, our work is based on the assumption that
perfect word alignment is impossible due to the in-
trinsic difficulty of the problem, and it is more effec-
tive to resolve translation ambiguities at later stages
of the MT pipeline. A main focus of much previous
work on word alignments is on theoretical aspects
of the proposed algorithms. In contrast, the nature
of this work is purely empirical. Our system was
trained on a large amount of training data and evalu-
ated on multiple languages (Chinese-to-English and
Arabic-to-English) and multiple genres (newswire
and weblog). Furthermore, we used a state of the art
string-to-tree decoder (Shen et al, 2008) to estab-
lish the strongest possible baseline. In comparison,
experiments in previous studies typically used one
language pair and one genre (usually newswire), a
reduced amount of training data and a phrase based
decoder.
This paper is organized as follows. Section 2 de-
scribes the three alignment algorithms. Section 3
describes the two methods used to combine these
aligners to improve MT. The experimental setup
used to compare these methods is presented in Sec-
tion 4. Section 5 shows the results including a dis-
cussion. Section 6 discusses related work. Section 7
concludes the paper.
2 Alignment Algorithms
We used three aligners in this work: GIZA++ (Och
and Ney, 2003), jointly trained HMM (Liang et al,
2006), and ITG (Haghighi et al, 2009). GIZA++
is an unsupervised method based on models 1-5 of
Brown et al (1993). Given a sentence pair e ? f ,
it seeks the alignment a that maximizes the proba-
bility P (f, a|e). As in most previous studies using
GIZA++, we ran GIZA++ in both directions, from e
to f and from f to e, and symmetrized the bidirec-
tional alignments into one, using a method similar
to the grow-diagonal-final method described in Och
and Ney (2003). We ran GIZA++ up to model 4.
The jointly trained HMM aligner, or HMM for
short, is also unsupervised but it uses a small amount
of hand-aligned data to tweak a few high level pa-
rameters. Low level parameters are estimated in an
unsupervised manner like GIZA++.
The ITG aligner is a supervised method whose pa-
rameters are tuned to optimize alignment accuracy
on hand-aligned data. It uses the inversion transduc-
tion grammar (ITG) (Wu, 1997) to narrow the space
of possible alignments. Since the ITG aligner uses
features extracted from HMM alignments, HMM
was run as a prepossessing step in our experiments.
Both the HMM and ITG aligners are publicly avail-
able1.
3 Methods of Combining Alternative
Alignments for MT
We explored two methods of combining alternative
alignments for MT. One is to extract translation rules
from the three alternative alignments and take the
union of the three sets of rules as the single transla-
tion grammar. Procedurally, this is done by concate-
nating the alignment files before extracting transla-
tion rules. We call this method unioned grammar.
This method greatly increases the coverage of the
rules, as the unioned translation grammar has about
80% more rules than the ones extracted from the in-
dividual alignment in our experiments. As such, de-
coding is also slower.
The other is to use system combination to com-
bine outputs of systems trained using different align-
ers. Due to differences in the alignment algorithms,
these systems would produce different hypotheses
with independent errors. Combining a diverse set
of hypotheses could improve overall system perfor-
mance. While system combination is a well-known
technique, to our knowledge this work is the first to
apply it to explicitly exploit complementary align-
ment algorithms on a large scale.
Since system combination is an established tech-
nique, here we only briefly discuss our system com-
1http://code.google.com/p/berkeleyaligner/
668
bination setup. The basic algorithm was described in
Rosti et al (2007). In this work, we use incremental
hypothesis alignment with flexible matching (Rosti
et al, 2009) to produce the confusion networks. 10-
best lists from all systems are collected first. All
1-best hypotheses for each segment are used as con-
fusion network skeletons, the remaining hypotheses
are aligned to the confusion networks, and the result-
ing networks are connected in parallel into a joint
lattice with skeleton specific prior probabilities es-
timated from the alignment statistics on the initial
arcs. This lattice is expanded with an unpruned bi-
gram language model and the system combination
weights are tuned directly to maximize the BLEU
score of the 1-best decoding outputs. Given the
tuned system combination weights, a 300-best list
is extracted from the lattice, the hypotheses are re-
scored using an unpruned 5-gram language model,
and a second set of system combination weights is
tuned to maximize the BLEU score of the 1-best hy-
pothesis of the re-scored 300-best list. The same re-
scoring step is also applied to the outputs of individ-
ual systems.
4 Experiment Setup
To establish strong baselines, we used a string-to-
tree SMT system (Shen et al, 2008), one of the top
performing systems in the NIST 2009 MT evalua-
tion, and trained it with very large amounts of par-
allel and language model data. The system used
large sets of discriminatively tuned features (up to
55,000 on Arabic) inspired by the work of Chiang et
al. (2009). To avoid drawing language, genre, and
metric specific conclusions, we experimented with
two language pairs, Arabic-English and Chinese-
English, and two genres, newswire and weblog, and
report both BLEU (Papineni et al, 2002) and TER
(Snover et al, 2006) scores. Systems were tuned to
maximize BLEU on the tuning set using a procedure
described in Devlin (2009).
The sizes of the parallel training corpora are
238M words (target side) for Arabic-English MT
and 265M words for Chinese-English. While the
majority of the data is publicly available from the
Linguistic Data Consortium (LDC), some of the data
is available under the DARPA GALE program. Due
to the size of the parallel corpora, we divided them
into five chunks and aligned them in parallel to save
time. Due to its running complexity, we ran ITG
only on sentences with 60 or fewer words. For
longer sentences, we used HMM alignments instead,
which were conveniently generated in the prepro-
cessing step of ITG aligner. For language model
training, we used about 9 billion words of English
text, most of which are from English Gigaword cor-
pus and GoogleNews. Each system used a 3-gram
LM for decoding and a 5-gram LM for re-scoring.
The same 5-gram LM was also used for re-scoring
system combination results.
For each combination of language pair and genre,
we used three development sets:
? Tune, which was used to tune parameters of
individual MT systems. Each system was tuned
ten iterations based on BLEU.
? SysCombTune, which was used to tune pa-
rameters of system combination. A subset of it
was also used as validation for determining the
best iteration in tuning individual systems.
? Test, which was the blind test corpus for mea-
suring performances of both individual systems
and system combination.
Test materials were drawn from two sources:
NIST MT evaluations 2004 to 2008, and develop-
ment and evaluation data for the DARPA GALE pro-
gram. Due to the mixing of different data sources,
some test sentences have four reference translations
while the rest have only one. The average num-
ber of references per test sentence varies across test
sets. For this reason, MT scores are not comparable
across test sets. Table 1 shows the size and the av-
erage number of references per sentence of the test
sets.
Two hand-aligned corpora were used to train the
ITG aligner: LDC2009E82 (Arabic-English) and
LDC2009E83 (Chinese-English). We re-tokenized
the corpora using our tokenizers and projected the
LDC alignments to our tokenization heuristically.
The projection was not perfect and sometimes cre-
ated very coarse-grained alignments. We used a set
of filters to remove such problematic data. We ended
up with 3,667 Arabic-English and 879 Chinese-
English hand-aligned sentence pairs with sufficient
quality for training automatic aligners.
669
language and genre Tune SysCombTune Test
Arabic newswire 2963 (2.9) 3223 (2.7) 2242 (2.7)
Arabic web 4597 (1.5) 4526 (1.4) 2703 (2.7)
Chinese newswire 3085 (2.6) 3001 (2.7) 2055 (1.4)
Chinese web 4221 (1.3) 4285 (1.3) 3092 (1.2)
Table 1: Numbers of sentences and average number of references (in parentheses) of test sets
5 Results
Three baseline systems were trained using the three
different aligners. Case insensitive BLEU and TER
scores for Arabic newswire, Arabic weblog, Chi-
nese newswire, and Chinese weblog are shown in
Tables 2, 3, 4, and 5, respectively2. The BLEU
scores on the Test set are fairly similar but the
ordering between different alignment algorithms is
mixed between different languages and genres. To
compare the two alignment combination strategies,
we trained a system using the union of the rules ex-
tracted from the alternative alignments (union in
the tables) and a combination of the three baseline
system outputs (3 syscomb in the tables). The
system with the unioned grammar was also added
as an additional system in the combination marked
by 4 syscomb.
As seen in the tables, unioned grammar and sys-
tem combination improve MT on both languages
(Arabic and Chinese) and both genres (newswire
and weblog). While there are improvements on
both SysCombTune and Test, the results on
SysCombTune are not totally fair since it was used
for tuning system combination weights and as val-
idation for optimizing weights of the MT systems.
Therefore our discussion will focus on results on
Test. (We did not show scores on Tune because
systems were directly tuned on it.) Statistical sig-
nificance is determined at 95% confidence level us-
ing the bootstrap method described in Koehn (2004),
and is only applied on results obtained on the blind
Test set.
For unioned grammar, the overall improvement
in BLEU is modest, ranging from 0.1 to 0.6 point
2Dagger (?) indicates statistically better results than the best
individual alignment system. Double dagger (?) indicates sta-
tistically better results than both best individual alignment and
unioned grammar. Bold indicates best Test set performance
among individual alignment systems.
compared with the best baseline system, with little
change in TER score. The improvements in BLEU
score are statistically significant for Arabic (both
genres), but not for Chinese. The improvements in
TER are not significant for either language.
System combination produces bigger improve-
ments in performance. Compared with the best base-
line system, the improvement in BLEU ranges from
0.8 to 1.6 point. There are also noticeable improve-
ments in TER, around 1.0 point. The TER improve-
ments are mostly explained by the hypothesis align-
ment algorithm which is closely related to TER scor-
ing (Rosti et al, 2009). The results are interesting
because all three baseline systems (GIZA++, HMM
and ITG) are identical except for the word align-
ments used in rule extraction. The results confirm
that the aligners are indeed complementary, as we
conjectured earlier. Also, the four-system combi-
nation yields consistent gains over the three-system
combination, suggesting that the system using the
unioned grammar is somewhat complementary to
the three baseline systems. The statistical test in-
dicates that both the three and four system combi-
nations are significantly better than the single best
alignment system for all languages and genres in
BLEU and TER. In most cases, they are also sig-
nificantly better than unioned grammar.
Somewhat surprisingly, the GIZA++ trained sys-
tem is slightly better than the ITG trained system on
all genres but Chinese weblog. However, we should
point out that such a comparison is not entirely fair.
First, we only ran ITG on short sentences. (For long
sentences, we had to settle for HMM alignments for
computing reasons.) Second, the hand-aligned data
used for ITG training are not very clean, as we said
before. The ITG results could be improved if these
problems were not present.
670
SysCombTune Test
System BLEU TER BLEU TER
GIZA++ 51.31 38.01 50.96 38.38
HMM 50.87 38.49 50.84 38.87
ITG 51.04 38.44 50.69 38.94
union 51.55 37.93 51.53? 38.32
3 syscomb 52.66 37.20 52.43? 37.69?
4 syscomb 52.80 37.05 52.55? 37.46?
Table 2: MT results on Arabic newswire (see footnote 2).
SysCombTune Test
System BLEU TER BLEU TER
GIZA++ 27.49 55.00 38.00 49.55
HMM 27.42 55.53 37.81 50.12
ITG 27.19 55.32 37.77 49.94
union 27.66 54.82 38.43? 49.43
3 syscomb 27.65 53.89 38.70? 48.72?
4 syscomb 27.83 53.68 38.82? 48.53?
Table 3: MT results on Arabic weblog (see footnote 2).
SysCombTune Test
System BLEU TER BLEU TER
GIZA++ 36.42 54.21 26.77 57.67
HMM 36.12 54.50 26.17 58.22
ITG 36.23 54.11 26.53 57.40
union 36.57 54.07 26.83 57.37
3 syscomb 37.60 53.19 27.46? 56.88?
4 syscomb 37.77 53.11 27.57? 56.57?
Table 4: MT results on Chinese newswire (see footnote
2).
SysCombTune Test
System BLEU TER BLEU TER
GIZA++ 18.71 64.10 16.94 63.46
HMM 18.35 64.66 16.66 64.02
ITG 18.76 63.67 16.97 63.29
union 18.97 63.86 17.22 63.20
3 syscomb 19.66 63.40 17.98? 62.47?
4 syscomb 19.80 63.32 18.05? 62.36?
Table 5: MT results on Chinese weblog (see footnote 2).
5.1 Discussion
Inter-aligner agreements provide additional evi-
dence about the differences between the aligners.
Suppose on a common data set, the sets of align-
ment links produced by two aligners are A and
B, we compute their agreement as (|A
?
B|/|A| +
|A
?
B|/|B|)/2. (This is the average of recall and
precision of one set by treating the other set as refer-
ence.) The agreement between GIZA++ and ITG
is around 78% on a subset of the Arabic-English
parallel data. The agreements between GIZA++
and HMM, and between HMM and ITG are slightly
higher, around 83%. Since ITG could not align long
sentences, we only used short sentences (at most 60
words in length) in our calculation.
Due to the large differences between the align-
ers, significantly more rules were extracted with
the unioned grammar method in our experiments.
On average, the size of the grammar (number of
rules) was increased by about 80% compared with
the baseline systems. The larger grammar results
in more combinations of partial theories in decod-
ing. However, for computing reasons, we kept the
beam size of the decoder constant despite the in-
crease in grammar size, potentially pruning out good
theories. Performance could be improved further if
larger beam sizes were used. We will leave this to
future work.
6 Related Work
Ayan and Dorr (2006) described a method to min-
imize alignment errors by combining alternative
alignments into a single alignment for each sentence
pair. Deng and Zhou (2009) used the number of ex-
tractable translation pairs as the objective function
for alignment combination. Och and Ney (2003) and
Koehn et al (2003) used heuristics to merge the bidi-
rectional GIZA++ alignments into a single align-
ment. Despite differences in algorithms and objec-
tive functions in these studies, they all attempted to
produce a single final alignment for each sentence
pair. In comparison, all alternative alignments are
directly used by the translation system in this work.
The unioned grammar method in this work is
very similar to Gime?nez and Ma`rquez (2005), which
combined phrase pairs extracted from different
alignments into a single phrase table. The difference
671
from that work is that our focus is to leverage com-
plementary alignment algorithms, while theirs was
to leverage alignments of different lexical units pro-
duced by the same aligner.
Some studies leveraged other types of differences
between systems to improve MT. For example, de
Gispert et al (2009) combined systems trained with
different tokenizations.
The theory behind the GIZA++ aligner was due to
Brown et al (1993). The theory of Inversion Trans-
duction Grammars (ITG) was due to Wu (1997).
The ITG aligner (Haghighi et al, 2009) used in this
work extended the original ITG to handle blocks of
words in addition to single words. The use of HMM
for word alignment can be traced as far back as to
Vogel et al (1996). The HMM aligner used in this
work was due to Liang et al (2006). It refined the
original HMM alignment algorithm by jointly train-
ing two HMMs, one in each direction. Furthermore,
it used a small amount of supervised data to tweak
some high level parameters, although it did not di-
rectly use the supervised data in training.
7 Conclusions
We explored two methods to exploit complementary
alignment algorithms. One is to extract translation
rules from all alternative alignments. The other is to
combine outputs of different MT systems trained us-
ing different aligners. Experiments on two language
pairs and two genres show consistent improvements
over the baseline systems.
Acknowledgments
This work was supported by DARPA/IPTO Contract
No. HR0011-06-C-0022 under the GALE program3
(Approved for Public Release, Distribution Unlim-
ited). The authors are grateful to John DeNero and
John Blitzer for their help with the Berkeley HMM
and ITG aligners.
3The views, opinions, and/or findings contained in this ar-
ticle/presentation are those of the author/presenter and should
not be interpreted as representing the official views or policies,
either expressed or implied, of the Defense Advanced Research
Projects Agency or the Department of Defense.
References
Necip Fazil Ayan and Bonnie J. Dorr. 2006. A maximum
entropy approach to combining word alignments. In
Proceedings of the Human Language Technology Con-
ference of the North American Chapter of the ACL,
pages 96?103.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Amer-
ican Chapter of the ACL, pages 218?226.
Adria` de Gispert, Sami Virpioja, Mikko Kurimo, and
William Byrne. 2009. Minimum Bayes risk combi-
nation of translation hypotheses from alternative mor-
phological decompositions. In Proceedings of Human
Language Technologies: The 2009 Annual Conference
of the North American Chapter of the ACL, pages 73?
76.
Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What can syntax-based MT learn from
phrase-based MT? In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 755?763.
Yonggang Deng and Bowen Zhou. 2009. Optimizing
word alignment combination for phrase table training.
In Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th IJCNLP of the
AFNLP, pages 229?232.
Jacob Devlin. 2009. Lexical features for statistical ma-
chine translation. Master?s thesis, University of Mary-
land.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2005. Combining
linguistic data views for phrase-based SMT. In Pro-
ceedings of the ACL Workshop on Building and Using
Parallel Texts, pages 145?148.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with supervised
ITG models. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and the 4th
IJCNLP of the AFNLP, pages 923?931.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
the 2003 Human Language Technology Conference of
the North American Chapter of the ACL, pages 48?54.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing, pages 388?395.
672
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference of the North Ameri-
can Chapter of the ACL, pages 104?111.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics, pages 311?318.
Antti-Veikko I. Rosti, Bing Xiang, Spyros Matsoukas,
Richard Schwartz, Necip Fazil Ayan, and Bonnie J.
Dorr. 2007. Combining outputs from multiple ma-
chine translation systems. In Proceedings of Human
Language Technologies 2007: The Conference of the
North American Chapter of the ACL, pages 228?235.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2009. Incremental hypothe-
sis alignment with flexible matching for building con-
fusion networks: BBN system description for WMT09
system combination task. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, pages
61?65.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-08: HLT, pages 577?585.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciula, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the 7th Conference of the Associa-
tion for Machine Translation in the Americas, pages
223?231.
Stephan Vogel, Hermann Ney, and Christoph Tillman.
1996. HMM-based word alignment in statistical trans-
lation. In The 16th International Conference on Com-
putational Linguistics, pages 836?841.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3).
673
String-to-Dependency Statistical
Machine Translation
Libin Shen?
Raytheon BBN Technologies
Jinxi Xu??
Raytheon BBN Technologies
Ralph Weischedel?
Raytheon BBN Technologies
We propose a novel string-to-dependency algorithm for statistical machine translation. This
algorithm employs a target dependency language model during decoding to exploit long distance
word relations, which cannot be modeled with a traditional n-gram language model. Experiments
show that the algorithm achieves significant improvement in MT performance over a state-of-
the-art hierarchical string-to-string system on NIST MT06 and MT08 newswire evaluation sets.
1. Introduction
n-gram Language Models (LMs) have been widely used in current Statistical Machine
Translation (SMT) systems. Because they treat a sentence as a flat string of tokens, a
drawback of traditional n-gram LMs is that they cannot model long range word rela-
tions, such as predicate?argument attachments, that are critical to translation quality.
We propose a hierarchical string-to-dependency translation model that exploits
a dependency LM while decoding (as opposed to during reranking n-best output) to
score alternative translations based on their structural soundness. In order to generate
the structured output (dependency trees) required for dependency LM scoring, transla-
tion rules in our system represent the target side as dependency structures. We restrict
the target side of the rules to well-formed dependency structures to weed out bad
translation rules and enable efficient decoding through dynamic programming. Due to
the flexibility of well-formed dependency structures, such structures can cover a large
set of non-constituent transfer rules (Marcu et al 2006) that have been shown useful
for MT.
For comparison purposes, as our baseline, we replicated the Hiero decoder (Chiang
2005), a state-of-the-art hierarchical string-to-string model. Our experiments show that
the string-to-dependency decoder significantly improves MT performance. Overall, the
? 10 Moulton Street, Cambridge, MA 02138. E-mail: libinshen@gmail.com.
?? 10 Moulton Street, Cambridge, MA 02138. E-mail: jxu@bbn.com.
? 10 Moulton Street, Cambridge, MA 02138. E-mail: weisched@bbn.com.
Submission received: 6 March 2009; revised submission received: 1 December 2009; accepted for publication:
18 March 2010.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 4
improvement in BLEU score is around 2 BLEU points on NIST Arabic-to-English and
Chinese-to-English newswire test sets.
Section 2 briefly discusses previous approaches to SMT in order to motivate our
work. Section 3 provides an overview of our string-to-dependency translation system.
Section 4 provides a complete description of our system, including formal definitions
of well-formed dependency structures and their operations, as well as proofs about
their key properties. Section 5 describes the implementation details, which include
rule extraction, decoding, using dependency LM scores, and using labels in translation
rules. We discuss experimental results in Section 6, compare our work with related
work in Section 7, and draw conclusions in Section 8.
2. Previous Approaches to SMT
Phrase-based systems (Koehn, Och, and Marcu 2003; Och 2003) had dominated SMT
until recently. Such systems typically treat the input as a sequence of phrases (word
n-grams), reorder them, and produce a translation for the reordered sentence based on
translation options of each source phrase. A prominent feature of such systems is the
use of an n-gram LM to measure the quality of translation hypotheses. A drawback of
such systems is that the lack of structural information in the output makes it impossible
to score translation hypotheses based on their structural soundness.
The Hiero system (Chiang 2007) was a major breakthrough in SMT. Translation
rules in Hiero contain non-terminals (NTs), as well as words, which allow the input
to be translated in a hierarchical manner. Because both the source and target sides of
its translation rules are strings with NTs, Hiero can be viewed as a hierarchical string-
to-string model. Despite the hierarchical nature of its decoder, Hiero lacks the ability to
measure translation quality based on structural relations such as predicate?argument
agreement.
Yamada and Knight (2001) proposed a syntax-based translation model that transfers
a source parse tree into a target string. This method depends on the quality of source
side parsing, and ignores target information during source side analysis. Mi, Huang,
and Liu (2008) later proposed a translation model that takes the source parse forest as
MT input to reduce translation errors due to imperfect source side analysis.
Galley et al (2004) proposed an MT model which produces target parse trees for
string inputs in order to exploit the syntactic structure of the target language. Galley
et al (2006) formalized this approach with tree transducers (Graehl and Knight 2004)
by using context-free parse trees to represent the target side. However, it was later
shown by Marcu et al (2006) and Wang, Knight, and Marcu (2007) that coverage could
be a big issue for the constituent based rules, even though the translation rule set was
already very large.
Carreras and Collins (2009) introduced a string-to-tree MT model based on spinal
Tree Adjoining Grammar (TAG) (Joshi and Schabes 1997; Shen, Champollion, and Joshi
2008). In this model, a translation rule is composed of a source string and a target
elementary tree. Target hypothesis trees are combined with the adjoining operation,
and there are no NT slots for substitution as in LTAG-spinal parsing (Shen and Joshi
2005, 2008; Carreras, Collins, and Koo 2008). Without the constraint of NT slots, the
adjoining operation allows very flexible composition, so that the search space becomes
much larger. One has to carefully prune the search space.
DeNeefe and Knight (2009) proposed another TAG-based MT model. In their
implementation, a TAG grammar was transformed to an equivalent Tree Insertion
650
Shen, Xu, and Weischedel String-to-Dependency Statistical Machine Translation
Grammar (TIG). In this way, they do not have an explicit adjoining operation in their
system, and as such reduce the search space in decoding. Sub-trees are combined with
NT substitution.
Many researchers followed the tree-to-tree approach (Shieber and Schabes 1990) to
take advantage of structural knowledge on both sides?for example, as in the papers
by Hajic? et al (2002), Eisner (2003), Ding and Palmer (2005), and Quirk, Menezes, and
Cherry (2005). Although tree-to-tree models can represent rich structural information
of the input and the output, they have not significantly improved MT performance,
possibly due to a much larger grammar and search space. On the other hand, Smith and
Eisner (2006) showed the necessity of allowing loose transformations between the trees,
which made tree-to-tree models even more complicated.
3. Overview of String-to-Dependency Translation
Our system is designed to address problems with existing SMT approaches. It is novel
in two respects. First, it uses a dependency LM to model long-distance relations. Sec-
ond, it uses well-formed dependency structures to represent translation hypotheses to
achieve an effective trade-off between model coverage and decoding complexity.
3.1 Dependency-Based Translation and Language Models
Our system generates target dependency trees as output and exploits a dependency LM
in scoring translation hypotheses. As described before, the goal of using a dependency
LM is to exploit long-distance word dependencies and as such model the quality of the
output more accurately.
Figure 1 shows an example dependency tree. Each arrow points from a child to its
parent. In this example, the word find is the root.
For the purpose of comparison, we first show how a simplified SMT system uses an
n-gram LM to score translation hypotheses:
S?e = argmax
Se
P(Se|Sf )
w1P(Sf |Se)
w2P(Se)
w3 (1)
where w1,w2, and w3 are feature weights. Sf is the input and Se?s are outputs. P(Se|Sf )
is the probability of the target string given the source, and P(Sf |Ss) is the probability of
the source given the target. P(Se) is the prior probability of the target string Se using an
n-gram LM.
Figure 1
The dependency tree for sentence the boy will find it interesting.
651
Computational Linguistics Volume 36, Number 4
In comparison, the scoring function in our system is:
D? = argmax
D
P(D|Sf )
w1P(Sf |D)
w2P(D)w3 (2)
where P(D) is the dependency LM score of target dependency tree D. We will show how
to compute P(D) in Section 5.4.
We can rewrite Equation (2) with a linear model:
D? = argmax
D
n
?
i=1
wiFi(Sf ,D) (3)
where n = 3,F1 = log P(D|Sf ),F2 = log P(Sf |D), and F3 = log P(D). In practice, we use
both a dependency LM and a traditional n-gram LM (also known as a string LM), as
well as several other features, in our decoder. Section 5.6 lists all the features used in
our decoder.
3.2 Well-Formed Dependency Structures
A central question in our system design is: What kinds of dependency structures are
allowed in translation rules? One extreme would be to allow any arbitrary multiple
level treelets, as in Ding and Palmer (2005) and Quirk, Menezes, and Cherry (2005).
One can define translation rules on any fragment of a parse/dependency tree. It offers
maximum coverage of translation patterns, but suffers from data sparseness and a large
search space.
The other extreme would be to allow only complete (CFG) constituents. This offers
a more robust model and a small search space, but excludes many useful transfer rules.
In our system, the target side hypotheses are restricted to well-formed dependency
structures (see Section 4 for formal definitions) for a trade-off between rule coverage,
model robustness, and decoding complexity. In short, a well-formed dependency struc-
ture is either (1) a single rooted tree, with each child being a complete sub-tree, or (2) a
sequence of siblings, each being a complete sub-tree.
Well-formed dependency structures are very flexible and can represent a variety of
non-constituent rules in addition to rules that are complete constituents. For example,
the following translation
hong
Chinese-to-English
?????????????? the red
is obviously useful for Chinese-to-English MT, but cannot be represented in some tree-
based translation systems since the red is a partial constituent. However, it is a valid
dependency structure in our system.
4. Formalism
We first formally define the well-formed dependency structures, which are used to
represent target hypotheses. Then, we define the operations to build well-formed de-
pendency structures from the bottom up in decoding.
652
Shen, Xu, and Weischedel String-to-Dependency Statistical Machine Translation
4.1 Well-Formed Dependency Structures and Categories
In order to exclude undesirable structures and reduce the search space, we only allow
Se whose dependency structure D is well formed, which we will define subsequently.
The well-formedness requirement will be applied to partial decoding results.
Based on the results of previous work (DeNeefe et al 2007), we keep two kinds of
dependency structures, fixed and floating. Fixed structures consist of a sub-root with
children, each of which must be a complete constituent. We call them fixed dependency
structures because the head is known or fixed. Floating structures consist of a number
of consecutive sibling nodes of a common head, but the head itself is unspecified. Each
of the siblings must be a complete constituent. Floating structures can represent many
linguistically meaningful non-constituent structures: for example, like the red, a modifier
of a noun. Only those two kinds of dependency structures are well-formed structures in
our system.
In the rest of this section, we will provide formal definitions of well-formed
structures and combinatory operations over them, so that we can easily manipulate
them in decoding. Examples will be provided along with the formal definitions to aid
understanding.
Consider a sentence S = w1w2...wn. Let d1d2...dn represent the parent word IDs for
each word. For example, d4 = 2 means that w4 depends on w2. If wi is a root, we define
di = 0.
Definition 1
A dependency structure didi+1...dj, or di..j for short, is fixed on head h, where h ? [i, j],
or fixed for short, if and only if it meets the following conditions
1. dh /? [i, j]
2. ?k ? [i, j] and k = h, dk ? [i, j]
3. ?k /? [i, j], dk = h or dk /? [i, j]
We say the category of di..j is (?, h,?), where ? means this field is undefined.
Definition 2
A dependency structure di...dj is floating with children C, for a non-empty set C ?
{i, ..., j}, or floating for short, if and only if it meets the following conditions
1. ?h /? [i, j], s.t.?k ? C, dk = h
2. ?k ? [i, j] and k /? C, dk ? [i, j]
3. ?k /? [i, j], dk /? [i, j]
We say the category of di..j is (C,?,?) if j < h, which means that children are on the left
side of the head, or (?,?,C) otherwise.
A category is composed of the three fields (A, h,B), where h is used to represent the
head, and fields A and B represent left and right dependents of the head, respectively.
A dependency structure is well-formed if and only if it is either fixed or floating.
653
Computational Linguistics Volume 36, Number 4
Examples
We represent dependency structures with graphs. Figure 2 shows examples of fixed
structures, Figure 3 shows examples of floating structures, and Figure 4 shows ill-
formed dependency structures.
The structures in Figures 2 and 3 are well-formed. Figure 4(a) is ill-formed because
boy does not have its child word the in the tree. Figure 4(b) is ill-formed because it is not
a continuous segment due to the missing it.
As for the example the red mentioned earlier, it is a well-formed floating dependency
structure.
It is easy to see that a floating structure whose child set C has only one element is
also a fixed structure. Actually, this is a desirable property on which we will introduce
meta category operations later. However, for the sake of convenience, we would like to
assign a single category to each well-formed structure.
Figure 2
Fixed dependency structures.
Figure 3
Floating dependency structures.
Figure 4
Ill-formed dependency structures.
654
Shen, Xu, and Weischedel String-to-Dependency Statistical Machine Translation
Definition 3
Let structure T be well formed. Category cat of T is defined as follows
cat(T) =
?
?
?
(?, h,?) if T is fixed on h
(C,?,?) if T is floating with children C on the left side AND |C| > 1
(?,?,C) if T is floating with children C on the right side AND |C| > 1
cat is well-defined according to Definitions 1 and 2.
4.2 Operations
One of the purposes of introducing floating dependency structures is that siblings hav-
ing a common parent will become a well-defined entity, although they are not con-
sidered a constituent. We always build well-formed partial structures on the target side
in decoding. Furthermore, we combine partial dependency structures in a way such
that we can obtain all possible well-formed dependency structures (but no ill-formed
ones) during bottom?up decoding.
The solution is to employ categories (introduced earlier). Each well-formed depen-
dency structure has a category. We can apply four combinatory operations over the
categories. If we can combine two categories with a certain category operation, we can
use a corresponding tree operation to combine two dependency structures. The cate-
gory of the combined dependency structure is the result of the combinatory category
operations.
Operations on Well-Formed Dependency Structures
There are four types of operations on well-formed dependency structures. Instead of
providing formal definitions, we use figures to illustrate these operations to make them
easy to understand. Figure 1 shows a traditional dependency tree. Figure 5 shows the
four operations for combining partial dependency structures, which are left adjoining
(LA), right adjoining (RA), left concatenation (LC), and right concatenation (RC). We
always combine two well-formed structures in one of the four ways, and obtain a larger
well-formed structure.
Two structures can be combined by adjoining, which is similar to the traditional
dependency formalism. We can adjoin either a fixed structure or a floating structure to
the head of a fixed structure.
Completed siblings can be combined via concatenation. We can concatenate two
fixed structures, one fixed structure with one floating structure, or two floating struc-
tures in the same direction.
The flexibility of the order of operation allows us to take advantage of various
translation fragments encoded in transfer rules. Figure 6 shows alternative ways of
applying operations on well-formed structures to build larger structures in a bottom?
up style. Numbers represent the order of operation. The fact that the same dependency
structure can have multiple derivations means that we can utilize various rules learned
from different training samples. Such flexibility is important for MT.
655
Computational Linguistics Volume 36, Number 4
Figure 5
Operations over well-formed structures.
Figure 6
Two alternative derivations of an example dependency tree.
Meta Operations on Categories
We first introduce three meta category operations, which will later be used to define
category operations. Two of the meta operations are unary operations, left raising (LR)
and right raising (RR), and one is the binary operation unification (UF).
Definition 4
Meta Category Operations
 LR((?, h,?)) = ({h},?,?)
 RR((?, h,?)) = (?,?, {h})
 UF((A1, h1,B1), (A2, h2,B2)) = NORM((A1 unionsq A2, h1 unionsq h2,B1 unionsq B2))
First, the raising operations are used to turn a completed fixed structure into a
floating structure, according to Theorem 1.
Theorem 1
A fixed structure with category (?, h,?) for span [i, j] is also a floating structure with
children {h} if there are no outside words depending on word h, which means that
?k /? [i, j], dk = h (4)
Proof
It suffices to show that all the three conditions of floating structures hold. Conditions 1
and 2 immediately follow from conditions 1 and 2 of the fixed structure, respectively.
Condition 3 is met according to Equation (4) and condition 3 of the fixed structure. 
656
Shen, Xu, and Weischedel String-to-Dependency Statistical Machine Translation
Therefore, we can always raise a fixed structure if we assume it is complete, that is,
Equation (4) holds.
Unification is well-defined if and only if we can unify all three elements and the
result is a valid fixed or floating category. For example, we can unify a fixed structure
with a floating structure or two floating structures in the same direction, but we cannot
unify two fixed structures.
h1 unionsq h2 =
?
?
?
h1 if h2 = ?
h2 if h1 = ?
undefined otherwise
A1 unionsq A2 =
?
?
?
A1 if A2 = ?
A2 if A1 = ?
A1 ? A2 otherwise
NORM((A, h,B)) =
?
?
?
?
?
?
?
(?, h,?) if h = ?
(A,?,?) if h = ?,B = ?
(?,?,B) if h = ?,A = ?
undefined otherwise
Operations on Categories
Now we define category operations. For the sake of convenience, we use the same
names for category operations and dependency structure operations. We can easily use
the meta category operations to define the four combinatory category operations. The
definition of the operations is as follows.
Definition 5
Combinatory category operations
LA((A1,?,?), (?, h2,?)) = UF((A1,?,?), (?, h2,?))
LA((?, h1,?), (?, h2,?)) = UF(LR((?, h1,?)), (?, h2,?))
LC((A1,?,?), (A2,?,?)) = UF((A1,?,?), (A2,?,?))
LC((A1,?,?), (?, h2,?)) = UF((A1,?,?), LR((?, h2,?)))
LC((?, h1,?), (A2,?,?)) = UF(LR((?, h1,?)), (A2,?,?))
LC((?, h1,?), (?, h2,?)) = UF(LR((?, h1,?)), LR((?, h2,?)))
RA((?, h1,?), (?,?,B2)) = UF((?, h1,?), (?,?,B2))
RA((?, h1,?), (?, h2,?)) = UF((?, h1,?), RR((?, h2,?)))
RC((?,?,B1), (?,?,B2)) = UF((?,?,B1), (?,?,B2))
RC((?, h1,?), (?,?,B2)) = UF(RR((?, h1,?)), (?,?,B2))
RC((?,?,B1), (?, h2,?)) = UF((?,?,B1), RR((?, h2,?)))
RC((?, h1,?), (?, h2,?)) = UF(RR((?, h1,?)), RR((?, h2,?)))
657
Computational Linguistics Volume 36, Number 4
Based on the definitions of dependency structure operations and category op-
erations, one can verify the one-to-one correspondence. This correspondence can be
formally stated in the following theorem.
Theorem 2
Suppose X and Y are well-formed dependency structures and OP(cat(X), cat(Y)) is well-
defined. We have
cat(OP(X,Y)) = OP(cat(X), cat(Y)) (5)
Proof
The proof of the theorem is rather routine, so we just give a sketch here. One can show
it by induction on the number of nodes in dependency structures. It suffices to show
that Equation (5) holds for all the operations. Actually, the category operations are
designed to meet this requirement; the three fields of a category represent the head
and the children on both sides. 
With category operations, we can easily track the types of dependency structures
and constrain operations in decoding.
Soundness and Completeness
Now we show the soundness and completeness of the operations on dependency
structures. If we follow the operations defined herein, we will build all the well-formed
structures and only the well-formed structures.
Theorem 3 (Soundness)
Let X and Y be two well-defined dependency structures, and OP an operation over X
and Y. It can be shown that OP(X,Y) is also a well-defined dependency structure.
Proof
Theorem 3 immediately follows Theorem 2. 
Theorem 4 (Completeness)
Let Z be a well-defined dependency structure with at least two nodes. It can be
shown that there exist well-formed structures X,Y and an operation OP, such that
Z = OP(X,Y).
Proof
If Z is fixed on h, without losing generality, we assume g is the leftmost child (or
rightmost if there is no left child) of h. We detach g from h, and obtain two sub-trees
X and Y which are rooted on g and h respectively. It can be verified that X and Y are
well-formed, and Z = LA(X,Y).
If Z is floating with children {c1, c2, ..., cn}, where n > 1, we can split it into two
floating structures with children {c1} and {c2, ..., cn}, respectively. It is easy to verify
that they are the sub-structures X and Y that we are looking for. 
658
Shen, Xu, and Weischedel String-to-Dependency Statistical Machine Translation
5. Implementation
5.1 Translation Rules
Translation rules are central to an MT system. In our system, each rule translates a
source sub-string into a target dependency structure. The target side of the translation
rules constitutes a tree grammar.
One way to define a tree grammar is in the way we described earlier. Two well-
formed structures can be combined into a larger one with adjoining or concatenation,
and there is no non-terminal slot for substitution. This is similar to tree grammars
without substitution, such as the original TAG (Joshi, Levy, and Takahashi 1975) and
LTAG-spinal (Shen, Champollion, and Joshi 2008). A corresponding MT model was
proposed in Carreras and Collins (2009). Search space is a major problem for such an
approach, as we described earlier.
In our system, we introduced NT substitution to combat the search problem. The
NT slots for substitution come from what we have observed in training data. Combina-
tion of well-formed dependency structures can only happen on NT slots. By replacing
NT slots with well-formed structures, we implicitly adjoin or concatenate sub-structures
based on the dependency information stored in rules. We extract the translation rules
from the training data containing word-to-word alignment and target parse trees, which
we will explain in the next section. A similar strategy was employed by DeNeefe and
Knight (2009). They turned a TAG into an equivalent TIG.
In addition to these extracted rules, we also have special rules to adjoin or concate-
nate two neighboring hypotheses. Each of the special rules has two NT slots, but they
vary on target dependency structures. They are comparable to the glue rules in Chiang
(2005).
To formalize translation rules and grammars, a string-to-dependency grammar G is
a 4-tuple G = ?R,X,Tf ,Te? where R is a set of transfer rules. X is the only non-terminal
type.1 Tf is a set of terminals (words) in the source language, and Te is a set of terminals
in the target language.
A string-to-dependency transfer rule R ? R is a 4-tuple R = ?Sf ,Se,D,A? where
Sf ? (Tf ? {X})+ is a source string, Se ? (Te ? {X})+ is a target string, D represents the
dependency structure for Se, and A is the alignment between Sf and Se. Non-terminal
alignments in A must be one-to-one. We ignore the left hand side for both source and
target, since there is only one NT type.
5.2 Rule Extraction
Now we explain how we extract string-to-dependency rules from parallel training data.
The procedure is similar to Chiang (2007) except that we maintain tree structures on the
target side, instead of strings.
Given sentence-aligned bilingual training data, we first use GIZA++ (Och and
Ney 2003) to generate word level alignment. We use a statistical CFG parser to parse
1 Later in the article, we will introduce label information for NTs. However, labels are treated as soft
features, and there is still a single NT type. In fact, other useful information can also be treated as soft
features, for example, length distribution for each NT observed in the training data. Details are provided
in Shen et al (2009).
659
Computational Linguistics Volume 36, Number 4
Figure 7
An example to show the rule extraction procedure. In this example, the word it is replaced with
a non-terminal X, which generates a hierarchical translation rule.
the English side of the training data, and extract dependency trees with Magerman?s
rules (1995). Then we use heuristic rules to extract transfer rules recursively based on
word alignments and the target dependency trees. The rule extraction procedure is as
follows.
1. Initialization:
All the 4-tuples ?Pi,j
f
,Pm,ne ,D,A? are valid span templates, where source
phrase P
i,j
f
is aligned to target phrase Pm,ne under alignment
2 A. D is a
well-formed dependency structure for Pm,ne . All valid span templates are
valid rule templates.
2. Inference:
Let ?Pi,j
f
,Pm,ne ,D1,A? be a valid rule template, and ?P
p,q
f
,Ps,te ,D2,A? a
valid span template, where range [p, q] ? [i, j], [s, t] ? [m,n], D2 is a
sub-structure of D1, and at least one word in P
i,j
f
but not in P
p,q
f
is aligned.
We create a new valid rule template ?P?f ,P
?
e,D
?,A?, where we obtain P?f
by replacing P
p,q
f
with label X in P
i,j
f
, and obtain P?e by replacing P
s,t
e with
X in Pm,ne . Furthermore, we obtain D
? by replacing sub-structure D2
with X in D1.
3 An example is shown in Figure 7.
By applying the inference rule recursively, we can generate rules with arbitrary
aligned NT slots if there are enough words and alignments. In order to make the size
of the grammar manageable, we keep only rules with at most two NT slots and at most
seven source elements.
Following previous work (Och and Ney 2003; Chiang 2007), we have three features
for each rule, which are P(source|target), P(target|source), and the lexical translation
probability given by GIZA. The two conditional probabilities are simply estimated by
counting in all the extracted rules.
2 P
i,j
f
represents the ith to the jth words on the source side, and Pm,ne represents the m
th to the nth words
on the target side. By P
i,j
f
aligned to Pm,ne , we mean all words in P
i,j
f
are either aligned to words in
Pm,ne or unaligned, and vice versa. Furthermore, at least one word in P
i,j
f
is aligned to a word in Pm,ne .
3 If D2 is a floating structure, we need to merge several dependency links into one.
660
Shen, Xu, and Weischedel String-to-Dependency Statistical Machine Translation
5.3 Decoding
Following previous work on hierarchical MT (Chiang 2005; Galley et al 2006), we solve
the decoding problem with chart parsing. We view the target dependency trees as
hidden structures in the input. The task of decoding is then to find the best hidden
structure for the input given the transfer grammar and the language models (a string
n-gram LM and a dependency LM).
The parser scans all source cells in a bottom?up style, and checks matched transfer
rules according to the source side. Once there is a completed rule, we build a larger de-
pendency structure by substituting component dependency structures for correspond-
ing NTs in the target dependency structure of rules.
Hypotheses, that is, candidate dependency structures, are organized in a shared
forest, or AND?OR structures. An AND-structure represents an application of a rule
over component OR-structures, and an OR-structure represents a set of alternative
AND-structures with the same state. A state keeps the necessary information about
hypotheses under it, which is needed for computing scores for higher level hypotheses
for dynamic programming. For example, with an n-gram string LM in decoding, a state
keeps the leftmost n ? 1 words and the rightmost n ? 1 words shared by hypotheses in
that state. Because of the use of a dependency LM in decoding, the state information
also includes boundary information about dependency structures for the purpose of
computing dependency LM scores for larger structures.
In the next section, we will explain how to extend categories and states to exploit a
dependency language model during decoding.
5.4 Using Dependency LM Scores
For the dependency tree in Figure 1, we calculate the probability of the tree as follows
P = PT(find)
?PL(will | find-as-head)
?PL(boy | will, find-as-head)
?PL(the | boy-as-head)
?PR(it | find-as-head)
?PR(interesting | it, find-as-head)
Here PT(x) is the probability that word x is the root of a dependency tree. PL and PR
are left and right side generative probabilities respectively. Let wh be the head, and
wL1wL2 ...wLn be all the children on the left side, from the nearest to the farthest. We
use a tri-gram dependency LM,
PL(wL1wL2 ...wLn |wh-as-head) = PL(wL1 |wh-as-head) ? PL(wL2 |wL1 ,wh-as-head) ? ...
?PL(wLn |wLn?1 ,wLn?2 ) ? PL(STOP|wLn ,wLn?1 ) (6)
In this formula, wh-as-head represents the event that w is used as the head, and wLi
represents the event that wLi is a sibling word. The computation of STOP probabilities
greatly complicates the implementation of inside dependency LM probabilities, so we
ignored it in practice. Right side probability PR is defined in a similar way.
661
Computational Linguistics Volume 36, Number 4
We should note that other orders of dependency LMs (e.g., bi-gram or 4-gram) can
be used by changing the independence assumptions in the above formulas. The choice
of using a tri-gram model in our experiments is a trade-off between model robustness
and sharpness given the training data available.
In order to calculate the dependency language model score, or depLM score for
short, on the fly for partial hypotheses in a bottom?up decoding, we need to save more
information in categories and states.
We use a 5-tuple ?LF,LN, h,RN,RF? to represent the category of a dependency
structure. h represents the head. Relative to the head, LF is the farthest children on
the left side and RF the farthest children on the right side. Similarly, LN is the nearest
children on the left side and RN the nearest children on the right. The three types of
categories are as follows.
 fixed: ?LF,?, h,?,RF?
 floating left: ?LF,LN,?,?,??
 floating right: ??,?,?,RN,RF?
Furthermore, operations similar to those described in Section 4.2 are used to keep track
of the head and boundary child nodes, which are then used to compute depLM scores
in decoding.
5.5 Using Labels in Transfer Rules
In the formalism introduced in the previous section, there is only a single non-terminal
type X. This may result in loss of information in the training data. For example, there is
a rule whose target dependency structure is X1 ? says ? X2, where X1 and X2 depend
on says. In the training data, X1 comes from a tree rooted on a noun and X2 comes from a
tree rooted on a verb. Without this information in the rule, any structure could be placed
in either of these two slots in the decoding phase.
We alleviate this problem by associating a label with each non-terminal in the rules.
Specifically, each non-terminal has a label, and the whole target structure side also has
a label. When we replace an NT with a sub-structure, we check if the label of the sub-
structure is the same as the NT label. If they do not match, we assign a penalty to this
replacement.
An obvious choice of the label is the POS tag of the head word, if it is a fixed tree. In
the previous example, the target structure would generate X1(NN) ? says ? X2(VBP),
where NN means noun (singular or mass) and VBP means verb (non-3rd person sin-
gular present), and the whole target structure has a label of VBZ, which means verb
(3rd person singular present). If we replace NN with a sub-tree rooted at, for example,
a preposition, there will be a penalty.
In our system, we use the POS tag of the head word as the label of a fixed structure.
We always use the generic label for floating structures. Any NT substitution with this
label involved is regarded as a mismatch. In other words, there is a penalty for inserting
any floating structure during decoding.
This extension does not affect the basic formalism of dependency structures de-
scribed in the previous section. Instead, we modify the representation of translation
rules and states in the decoder. For each rule, if its dependency structure is of a fixed
type, the whole structure has a label which is the POS tag of the head word. Otherwise,
the label is X. Similarly, each NT slot has a label which is defined in the same way,
662
Shen, Xu, and Weischedel String-to-Dependency Statistical Machine Translation
based on the dependency structure from which the rule is extracted. In decoding,
each state has an extra field representing the label for the dependency structure of the
hypothesis.
5.6 Other Details
We have nine features in our system.
1. Log probability of the source side given the target side of a rule
2. Log probability of the target side given the source side of a rule
3. Log probability of word alignment
4. Number of target words
5. Number of special rules (see Section 5.1) used
6. Log probability of string LM
7. Log probability of dependency LM
8. Discount on ill-formed dependency structures
9. Discount on unmatched labels
The values of the first four features are accumulated on the rules used in a transla-
tion. The fifth feature counts the number of times the adjoining and concatenation rules
are used in a translation. The string LM score and dependency LM score are the next
two features.
In practice, we also allow hypotheses that do not have well-formed structures in
derivation, but they are penalized. For this purpose, we introduce the null dependency
structure e. For any operation OP and dependency structure X,Y, we have
OP(X,Y) = e if this operation is not defined in Definition 5
OP(X, e) = X
OP(e,X) = X
Because part of a hypothesis may have a null dependency structure, we cannot calculate
dependency LM scores on some of the related words. Therefore, we give a discount for
each of these words. This is the eighth feature.
There are two sources for null dependency structures. One is the use of an unde-
fined operation, for example, left-adjoining a right floating structure to a fixed structure.
The other source is a lack of target structure information in translation rules. The parser
that we used may fail to generate parse trees for short segments?for example, dictio-
nary items. In these cases, we extracted the so-called phrasal rules with null dependency
structures. We limited phrasal rules to at most three lexical items for each side.
The last feature counts the number of substitutions with unmatched labels.
In decoding, partial hypotheses are mapped into states. The states maintain suf-
ficient statistics for feature calculation. For example, each state should memorize the
leftmost two words and rightmost two words for LM score calculation. Similar exten-
sions are required for dependency LM score and NT labels. Therefore, we use beam
search with cube pruning as in Chiang (2005) for speedup. Like chart parsing, the
663
Computational Linguistics Volume 36, Number 4
computational complexity of decoding time is O(n3 ? B ? |G|), where n is the length
of the source sentence, B is beam width, and |G| is the maximal number of transfer rules
applicable to a span with translation grammar G. This number agrees with the empirical
results.
We tune the weights with several rounds of decoding and optimization. Following
Och (2003), the k-best results are accumulated as the input to the optimizer. Powell?s
method is used for optimization with 20 random starting points around the weight
vector of the last iteration. For improved results, we rescore 1,000-best translations,
generated using the technique described by Huang and Chiang (2005), by replacing
tri-gram string LM scores in the output with 5-gram string LM scores. The algorithm to
tune the rescoring weights is similar to the one to tune the decoder weights.
6. Experiments
We experimented with four models:
 baseline: hierarchical string to string translation, using our own replication
of the Hiero system (Chiang 2007)
 filtered: like the baseline, it uses string to string rules, except that rules
whose target side does not correspond to a well-formed structure in rule
extraction are excluded. No dependency LM is used in decoding
 str-dep: string-to-dependency system. It uses rules with target dependency
structures and a dependency LM in decoding
 labeled: an enhanced str-dep model with POS tags as labels
We use the Hiero model as our baseline because it is the closest to our string-to-
dependency model. They use similar rule extraction and decoding algorithms. The
major difference is in the representation of target structures. We use dependency
structures instead of strings; thus, the comparison will show the contribution of using
dependency information in decoding.
All models were tuned on BLEU (Papineni, Roukos, and Ward 2001), and evaluated
on BLEU, TER (Snover et al 2006), and METEOR (Banerjee and Lavie 2005). It is well
known that all automatic scores are crude approximations of translation quality. It is not
uncommon for a technique to improve the metric that is used for tuning but hurt other
metrics. The use of multiple metrics helps us avoid drawing false conclusions based on
metric-specific improvements. For both Arabic-to-English and Chinese-to-English MT,
we tuned on NIST MT02-05 and tested on MT06 and MT08 newswire sets.
The training data for Arabic-to-English MT contains around 1.9 million pairs of
bi-lingual sentences from ten corpora: LDC2004T17, LDC2004T18, LDC2005E46, LDC-
2006E25, LDC2006G05, LDC2005E85, LDC2006E36, LDC2006E82, LDC2006E95, and
SSUSAC27 (Sakhr Arabic-English Parallel Corpus). The training data for Chinese-to-
English MT contains around 1.0 million pairs of bi-lingual sentences from eight corpora:
LDC2002E18, LDC2005T06, LDC2005T10, LDC2006E26, LDC2006G05, LDC2002L27,
LDC2005T34, and LDC2003E07.
The dependency LMs were trained on the same parallel training data. For that pur-
pose, we parsed the English side of the parallel data. Two separate models were
trained: one for Arabic from the Arabic training data and the other for Chinese from
the Chinese training data. Traditional tri-gram and 5-gram string LMs were trained on
664
Shen, Xu, and Weischedel String-to-Dependency Statistical Machine Translation
Table 1
Number of transfer rules.
Model Arabic-to-English Chinese-to-English
baseline 337,542,137 193,922,173
filtered 32,057,337 39,005,696
str-dep 35,801,341 41,013,346
labeled 41,201,100 43,705,510
the English side of the parallel data as well as the English Gigaword corpus V3.0 in a
way described by Bulyko et al (2007).
Table 1 shows the number of transfer rules extracted from the training data for
the tuning and test sets. The constraint of well-formed dependency structures greatly
reduced the size of the rule set. Although the rule size increased a little bit after incorpo-
rating dependency structures and labels in rules, the size of string-to-dependency rule
set is about 10% to 20% of the baseline.
Tables 2 and 3 show the BLEU, TER, and METEOR scores on MT06 and MT08 for
Arabic-to-English MT. Tables 4 and 5 show the scores for Chinese-to-English MT.
For system comparison, we primarily rely on the lower-cased BLEU score of the
decoding output because it is the metric on which all systems were tuned. We measured
the significance of BLEU, TER, and METEOR with paired bootstrap resampling as
proposed by Koehn (2004). In Tables 2 through 5, (+/-) represent being better/worse
than the baseline at 95% confidence level, respectively, and (*) represents insignificant
difference from the baseline.
For Arabic-to-English MT, the str-dep model decoder improved BLEU by 1.3 on
MT06 and 1.2 on MT08 before 5-gram rescoring. For Chinese-to-English MT, the im-
provements in BLEU were 1.0 on MT06 and 1.4 on MT08. After rescoring, the improve-
ments became smaller, ranging from 0.8 to 1.3. All the BLEU improvements on 5-gram
scores are statistically significant.
The use of POS labels in transfer rules further improves the BLEU score by about
0.7 points on average. The overall BLEU improvement on lower-cased decoding output
Table 2
BLEU, TER, and METEOR percentage scores on MT06 Arabic-to-English newswire set.
Model BLEU TER METEOR
lower mixed lower mixed
Decoding (3-gram LM)
baseline 47.50 45.48 44.79 46.97 66.17
filtered 46.64 (-) 44.47 (-) 45.38 (*) 47.96 (-) 66.64 (*)
str-dep 48.75 (+) 46.74 (+) 43.43 (+) 45.79 (+) 67.18 (+)
labeled 49.33 (+) 47.07 (+) 43.09 (+) 45.53 (+) 67.04 (+)
Rescoring (5-gram LM)
baseline 50.38 48.33 42.64 44.87 67.25
filtered 49.60 (-) 47.51 (-) 43.50 (-) 45.81 (-) 67.44 (*)
str-dep 51.24 (+) 49.23 (+) 42.08 (*) 44.42 (*) 67.89 (+)
labeled 51.80 (+) 49.69 (+) 41.54 (+) 43.76 (+) 67.97 (+)
665
Computational Linguistics Volume 36, Number 4
Table 3
BLEU, TER, and METEOR percentage scores on MT08 Arabic-to-English newswire set.
Model BLEU TER METEOR
lower mixed lower mixed
Decoding (3-gram LM)
baseline 48.41 46.13 43.83 46.18 67.45
filtered 47.37 (-) 45.24 (-) 44.39 (-) 46.83 (-) 67.17 (*)
str-dep 49.58 (+) 47.46 (+) 42.80 (+) 45.08 (+) 68.08 (+)
labeled 50.46 (+) 48.19 (+) 42.27 (+) 44.57 (+) 67.78 (+)
Rescoring (5-gram LM)
baseline 50.50 48.35 42.78 44.92 67.98
filtered 49.56 (-) 47.49 (-) 43.20 (*) 45.44 (*) 67.79 (*)
str-dep 51.23 (+) 49.11 (+) 42.01 (+) 44.15 (+) 68.65 (+)
labeled 51.93 (+) 49.86 (+) 41.27 (+) 43.33 (+) 68.40 (+)
is 1.8 points on MT06 and 2.1 points on MT08 for Arabic-to-English translation, and
2.0 points on MT06 and 1.6 points on MT08 for Chinese-to-English translation.
METEOR scores became significantly better for all conditions. TER improved sig-
nificantly for Arabic-to-English but marginally on Chinese-to-English tasks. The results
on METEOR and TER suggested that the new model did improve translation accuracy.
The filtered string-to-string rules can be viewed as the string projection of string-
to-dependency rules. It shows the performance of using dependency structure for rule
filtering only. The results are very interesting. On Arabic-to-English, the filtered model
was significantly worse, which means that many useful rules were lost due to the
structural constraints. On Chinese-to-English, the tri-gram scores of the filtered model
were a little bit worse. However, after 5-gram rescoring, the BLEU scores became higher
than the baseline, and METEOR scores were even significantly better. We suspect that
the different performance that we observed is due to the difference in source languages
and their tokenization methods.
Table 4
BLEU, TER, and METEOR percentage scores on MT06 Chinese-to-English newswire set.
Model BLEU TER METEOR
lower mixed lower mixed
Decoding (3-gram LM)
baseline 36.40 34.79 54.98 56.53 57.25
filtered 36.02 (*) 34.23 (*) 55.29 (*) 57.03 (*) 57.60 (+)
str-dep 37.44 (+) 35.62 (+) 54.64 (*) 56.47 (*) 57.42 (+)
labeled 38.37 (+) 36.53 (+) 54.14 (+) 55.99 (*) 58.42 (+)
Rescoring (5-gram LM)
baseline 37.88 36.18 53.80 55.45 57.44
filtered 38.52 (*) 36.74 (*) 54.09 (*) 55.69 (*) 58.16 (+)
str-dep 38.91 (+) 37.04 (+) 53.65 (*) 55.45 (*) 57.99 (+)
labeled 39.11 (+) 37.30 (+) 53.61 (*) 55.29 (*) 58.69 (+)
666
Shen, Xu, and Weischedel String-to-Dependency Statistical Machine Translation
Table 5
BLEU, TER, and METEOR percentage scores on MT08 Chinese-to-English newswire set.
Model BLEU TER METEOR
lower mixed lower mixed
Decoding (3-gram LM)
baseline 31.64 29.56 57.35 59.37 54.93
filtered 31.26 (*) 29.42 (*) 57.46 (*) 59.28 (*) 55.16(+)
str-dep 33.05 (+) 31.26 (+) 56.79 (*) 58.69 (+) 55.18(+)
labeled 33.25 (+) 31.34 (+) 56.60 (+) 58.49 (+) 56.01(+)
Rescoring (5-gram LM)
baseline 33.06 31.21 55.84 57.71 55.18
filtered 33.25 (*) 31.22 (*) 56.53 (*) 58.39 (-) 56.08 (+)
str-dep 34.34 (+) 32.32 (+) 55.60 (*) 57.60 (*) 55.91 (+)
labeled 35.02 (+) 33.00 (+) 55.39 (*) 57.48 (*) 56.46 (+)
In any case, the purpose of the filtered model is not to propose the use of structural
constraints for rule filtering, although it greatly reduced the rule size and allowed
the use of more useful training data potentially. The use of structural constraints is
compulsory for the introduction of dependency LMs and non-terminal labels, which
compensated for the loss of rule filtering, and led to significant overall improvement.
7. Comparison to Related Work
Fox (2002), Ding and Palmer (2005), and Quirk, Menezes, and Cherry (2005) showed
that, for the purpose of representing word relations, dependency structures are ad-
vantageous over CFG structures because they do not require complete constituents. A
number of techniques have been proposed to improve rule coverage. Marcu et al (2006)
and Galley et al (2006) introduced artificial constituent nodes dominating the phrase of
interest. The binarization method used by Wang, Knight, and Marcu (2007) can cover
many non-constituent rules also, but not all of them. DeNeefe et al (2007) showed that
the best results were obtained by combining these methods.
Charniak, Knight, and Yamada (2003) described a two-step string-to-CFG-tree
translation model which employed a syntax-based language model to select the best
translation from a target parse forest built in the first step. A crucial difference from our
work is that they only used the tree-based LM in rescoring, possibly due to the com-
plexity of the syntax-based LM. In contrast, our system uses a dependency LM directly
in decoding and as such can prune out unpromising hypotheses as soon as possible.
The use of a dependency LM in MT is similar to the use of a structured LM in
ASR (Chelba and Jelinek 2000; Xu, Chelba, and Jelinek 2002), with the same motivation
of exploiting long-distance relations. A difference is that the dependency LM is used
bottom?up in our MT system, whereas the structured LM is used left-to-right in ASR.
Another difference is that long-distance relations are more important in MT due to
word re-orderings.
The well-formed dependency structures defined here are similar to the data struc-
tures in previous work on monolingual parsing (Eisner and Satta 1999; McDonald,
Crammer, and Pereira 2005), which allowed floating structures as well defined states
in derivation, too. However, as for monolingual parsing, one usually wants exactly
667
Computational Linguistics Volume 36, Number 4
one derivation for each parse tree, so as to avoid spurious ambiguity of derivations
for the same parse. The derivation model proposed by Eisner and Satta (1999) satisfied
this prerequisite, and had O(n3) complexity with a bi-lexical probability model, which
was O(n4) in many other derivation models. In our MT model, the motivation is to
exploit various translation fragments learned from the training data, and the opera-
tions in monolingual parsing were designed to avoid artificial ambiguity of derivation.
Another difference is that we have fixed structures growing on both sides, whereas
fixed structures in (Eisner and Satta 1999) can only grow in one direction.
The formalism for well-formed structures and the operations over them were
inspired by the well-known approach of Combinatory Categorial Grammar (CCG)
(Steedman 2000). In fact, the names of left raising and right raising stem from the
raising operation in CCG.
The string-to-dependency formalism can be viewed as a special case of Synchro-
nous Tree Adjoining Grammar (STAG) (Shieber and Schabes 1990). Trees on the source
side are weakened to strings, and multi-rooted structures are employed on the tar-
get side. The adjoining operation in our model is similar to attachment in LTAG-
spinal (Shen, Champollion, and Joshi 2008) and sister adjunction in variants (Rambow,
Shanker, and Weir 1995; Chiang 2000; Carreras, Collins, and Koo 2008) of TAG (Joshi and
Schabes 1997). Translation rules can be viewed as constraints on the tree operations.
8. Conclusions and Future Work
In this article, we propose a novel string-to-dependency algorithm for statistical ma-
chine translation. It employs a target dependency language model to exploit long dis-
tance word relations in decoding, which cannot be captured with a traditional n-gram
language model.
Compared with a state-of-the-art hierarchical string-to-string system, our string-to-
dependency system generates about 80% fewer rules. The overall gain in BLEU score
on lower-cased decoding output is about two points.
Dependency structures provide a desirable platform for employing linguistic
knowledge in MT. We will extend our approach with deeper linguistic features such as
propositional structures (Palmer, Gildea, and Kingsbury 2005). The fixed and floating
structures proposed in this article can be extended to model predicates and arguments.
Acknowledgments
This work4 was supported by DARPA/
IPTO Contract No. HR0011-06-C-0022
under the GALE program. The views,
opinions, and/or findings contained in
this article/presentation are those of the
author/presenter and should not be
interpreted as representing the official views
or policies, either expressed or implied, of
the Defense Advanced Research Projects
Agency or the Department of Defense.
We are grateful to our colleagues Roger Bock,
Ivan Bulyko, Mike Kayser, Jeff Ma, John
4 Distribution Statement ?A? (Approved for
Public Release, Distribution Unlimited).
Makhoul, Spyros Matsoukas, Antti-Veikko
Rosti, Rich Schwartz, Bing Zhang, and
Rabih Zbib for their help in running the
experiments and constructive comments
to improve this article. Mike Kayser helped
to proofread the manuscript. We also thank
Zhifei Li and the anonymous reviewers for
their suggestions to improve this article.
References
Banerjee, Satanjeev and Alon Lavie. 2005.
Meteor: An automatic metric for MT
evaluation with improved correlation
with human judgments. In Proceedings
of the 43th Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 101?104, Ann Arbor, MI.
668
Shen, Xu, and Weischedel String-to-Dependency Statistical Machine Translation
Bulyko, Ivan, Spyros Matsoukas, Richard
Schwartz, Long Nguyen, and John
Makhoul. 2007. Language model
adaptation in machine translation from
speech. In Proceedings of the 32nd IEEE
International Conference on Acoustics,
Speech, and Signal Processing (ICASSP),
pages 117?120, Honolulu, HI.
Carreras, Xavier and Michael Collins. 2009.
Non-projective parsing for statistical
machine translation. In Proceedings of
the 2009 Conference of Empirical Methods
in Natural Language Processing,
pages 200?209, Singapore.
Carreras, Xavier, Michael Collins, and
Terry Koo. 2008. TAG, dynamic
programming, and the perceptron for
efficient, feature-rich parsing. In
Proceedings of the 12th Conference on
Computational Natural Language Learning,
pages 9?16, Manchester.
Charniak, Eugene, Kevin Knight, and Knight
Yamada. 2003. Syntax-based language
models for statistical machine translation.
In Proceedings of MT Summit IX,
pages 40?46, New Orleans, LA.
Chelba, Ciprian and Frederick Jelinek. 2000.
Structured language modeling. Computer
Speech and Language, 14(4):283?332.
Chiang, David. 2000. Statistical parsing
with an automatically extracted tree
adjoining grammar. In Proceedings of
the 38th Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 456?463, Hong Kong.
Chiang, David. 2005. A hierarchical phrase-
based model for statistical machine
translation. In Proceedings of the 43th
Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 263?270, Ann Arbor, MI.
Chiang, David. 2007. Hierarchical
phrase-based translation. Computational
Linguistics, 33(2):201?228.
DeNeefe, Steve and Kevin Knight. 2009.
Synchronous tree adjoining machine
translation. In Proceedings of the 2009
Conference of Empirical Methods in Natural
Language Processing, pages 727?736,
Singapore.
DeNeefe, Steve, Kevin Knight, Wei Wang,
and Daniel Marcu. 2007. What can
syntax-based MT learn from phrase-based
MT? In Proceedings of the 2007 Conference of
Empirical Methods in Natural Language
Processing, pages 755?763, Prague.
Ding, Yuan and Martha Palmer. 2005.
Machine translation using probabilistic
synchronous dependency insertion
grammars. In Proceedings of the 43th Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 541?548, Ann
Arbor, MI.
Eisner, Jason. 2003. Learning non-isomorphic
tree mappings for machine translation.
In Proceedings of the 41st Annual Meeting of
the Association for Computational Linguistics
(ACL), pages 205?208, Sapporo.
Eisner, Jason and Giorgio Satta. 1999.
Efficient parsing for bilexical context-free
grammars and head automaton
grammars. In Proceedings of the 37th
Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 457?464, College Park, MD.
Fox, Heidi. 2002. Phrasal cohesion and
statistical machine translation.
In Proceedings of the 2002 Conference
of Empirical Methods in Natural
Language Processing, pages 304?311,
Philadelphia, PA.
Galley, Michel, Jonathan Graehl, Kevin
Knight, Daniel Marcu, Steve DeNeefe,
Wei Wang, and Ignacio Thayer. 2006.
Scalable inference and training of
context-rich syntactic models. In
COLING-ACL ?06: Proceedings of 44th
Annual Meeting of the Association for
Computational Linguistics and 21st
International Conference on Computational
Linguistics, pages 961?968, Sydney.
Galley, Michel, Mark Hopkins, Kevin Knight,
and Daniel Marcu. 2004. What?s in a
translation rule? In Proceedings of the 2004
Human Language Technology Conference
of the North American Chapter of the
Association for Computational Linguistics,
pages 273?280, Boston, MA.
Graehl, Jonathan and Kevin Knight. 2004.
Training tree transducers. In Proceedings of
the 2004 Human Language Technology
Conference of the North American Chapter of
the Association for Computational Linguistics,
pages 105?112, Boston, MA.
Hajic?, Jan, Martin C?mejrek, Jason Eisner,
Gerald Penn, Owen Rambow, Dragomir
Radev, Yuan Ding, Terry Koo, and Kristen
Parton. 2002. Natural language generation
in the context of machine translation.
Final report of JHU Summer Workshop
project, Johns Hopkins University,
Baltimore, MD.
Huang, Liang and David Chiang. 2005.
Better k-best parsing. In Proceedings of the
9th International Workshop on Parsing
Technologies, pages 53?64, Vancouver.
Joshi, Aravind K., Leon S. Levy, and Masako
Takahashi. 1975. Tree adjunct grammars.
669
Computational Linguistics Volume 36, Number 4
Journal of Computer and System Sciences,
10(1):136?163.
Joshi, Aravind K. and Yves Schabes. 1997.
Tree-adjoining grammars. In G. Rozenberg
and A. Salomaa, editors, Handbook of Formal
Languages, volume 3. Springer-Verlag,
Berlin, pages 69?124.
Koehn, Philipp. 2004. Statistical significance
tests for machine translation evaluation.
In Proceedings of the 2004 Conference
of Empirical Methods in Natural Language
Processing, pages 388?395, Barcelona.
Koehn, Philipp, Franz J. Och, and Daniel
Marcu. 2003. Statistical phrase based
translation. In Proceedings of the 2003 Human
Language Technology Conference of the North
American Chapter of the Association for
Computational Linguistics, pages 48?54,
Edmonton.
Magerman, David. 1995. Statistical decision-
tree models for parsing. In Proceedings of
the 33rd Annual Meeting of the Association
for Computational Linguistics,
pages 276?283, Cambridge, MA.
Marcu, Daniel, Wei Wang, Abdessamad
Echihabi, and Kevin Knight. 2006. SPMT:
Statistical machine translation with
syntactified target language phrases.
In Proceedings of the 2006 Conference of
Empirical Methods in Natural Language
Processing, pages 44?52, Sydney.
McDonald, Ryan, Koby Crammer, and
Fernando Pereira. 2005. Online large-
margin training of dependency parsers.
In Proceedings of the 43th Annual Meeting of
the Association for Computational Linguistics
(ACL), pages 91?98, Ann Arbor, MI.
Mi, Haitao, Liang Huang, and Qun Liu. 2008.
Forest-based translation. In Proceedings of
the 46th Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 192?199, Columbus, OH.
Och, Franz J. 2003. Minimum error rate
training for statistical machine translation.
In Proceedings of the 41st Annual Meeting of
the Association for Computational Linguistics
(ACL), pages 160?167, Sapporo.
Och, Franz J. and Hermann Ney. 2003.
A systematic comparison of various
statistical alignment models. Computational
Linguistics, 29(1):19?52.
Palmer, Martha, Daniel Gildea, and
Paul Kingsbury. 2005. The proposition
bank: An annotated corpus of semantic
roles. Computational Linguistics,
31(1):71?106.
Papineni, Kishore, Salim Roukos, and
Todd Ward. 2001. BLEU: A method for
automatic evaluation of machine
translation. IBM Research Report No.
RC22176, Armonk, NY.
Quirk, Chris, Arul Menezes, and Colin
Cherry. 2005. Dependency treelet
translation: Syntactically informed phrasal
SMT. In Proceedings of the 43th Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 271?279,
Ann Arbor, MI.
Rambow, Owen, Vijay K. Shanker, and
David Weir. 1995. D-tree grammars.
In Proceedings of the 33rd Annual Meeting
of the Association for Computational
Linguistics, pages 151?158,
Cambridge, MA.
Shen, Libin, Lucas Champollion, and
Aravind K. Joshi. 2008. LTAG-spinal
and the Treebank: A new resource for
incremental, dependency and semantic
parsing. Language Resources and Evaluation,
42(1):1?19.
Shen, Libin and Aravind K. Joshi. 2005.
Incremental LTAG Parsing. In Proceedings
of Human Language Technology Conference
and Conference on Empirical Methods in
Natural Language Processing, pages 811?818,
Vancouver.
Shen, Libin and Aravind K. Joshi. 2008.
LTAG dependency parsing with
bidirectional incremental construction.
In Proceedings of the 2008 Conference
of Empirical Methods in Natural Language
Processing, pages 495?504, Honolulu, HI.
Shen, Libin, Jinxi Xu, Bing Zhang, Spyros
Matsoukas, and Ralph Weischedel.
2009. Effective use of linguistic and
contextual information for statistical
machine translation. In Proceedings of the
2009 Conference of Empirical Methods in
Natural Language Processing, pages 72?80,
Singapore.
Shieber, Stuart and Yves Schabes. 1990.
Synchronous tree adjoining grammars.
In Proceedings of COLING ?90: The 13th
International Conference on Computational
Linguistics, pages 253?258, Helsinki.
Smith, David A. and Jason Eisner. 2006.
Quasi-synchronous grammars:
Alignment by soft projection of syntactic
dependencies. In Proceedings of the HLT-
NAACL Workshop on Statistical Machine
Translation, pages 23?30, New York, NY.
Snover, Matthew, Bonnie Dorr, Richard
Schwartz, Linnea Micciulla, and John
Makhoul. 2006. A study of translation
edit rate with targeted human annotation.
In Proceedings of Association for Machine
Translation in the Americas, pages 223?231,
Cambridge, MA.
670
Shen, Xu, and Weischedel String-to-Dependency Statistical Machine Translation
Steedman, Mark. 2000. The Syntactic Process.
The MIT Press, Cambridge, MA.
Wang, Wei, Kevin Knight, and Daniel Marcu.
2007. Binarizing syntax trees to improve
syntax-based machine translation accuracy.
In Proceedings of the 2007 Conference
of Empirical Methods in Natural Language
Processing, pages 746?754, Prague.
Xu, Peng, Ciprian Chelba, and Frederick
Jelinek. 2002. A study on richer syntactic
dependencies for structured language
modeling. In Proceedings of the 40th Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 191?198,
Philadelphia, PA.
Yamada, Kenji and Kevin Knight. 2001. A
syntax-based statistical translation model.
In Proceedings of the 39th Annual Meeting of
the Association for Computational Linguistics
(ACL), pages 523?530, Toulouse.
671

Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 165?169,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
How Much Can We Gain from Supervised Word Alignment? 
 
 
Jinxi Xu and Jinying Chen 
Raytheon BBN Technologies 
10 Moulton Street, Cambridge, MA 02138, USA 
{jxu,jchen}@bbn.com 
  
 
 
Abstract 
Word alignment is a central problem in sta-
tistical machine translation (SMT). In re-
cent years, supervised alignment algo-
rithms, which improve alignment accuracy 
by mimicking human alignment, have at-
tracted a great deal of attention. The objec-
tive of this work is to explore the perform-
ance limit of supervised alignment under 
the current SMT paradigm. Our experi-
ments used a manually aligned Chinese-
English corpus with 280K words recently 
released by the Linguistic Data Consortium 
(LDC). We treated the human alignment as 
the oracle of supervised alignment. The re-
sult is surprising:  the gain of human 
alignment over a state of the art unsuper-
vised method (GIZA++) is less than 1 point 
in BLEU. Furthermore, we showed the 
benefit of improved alignment becomes 
smaller with more training data, implying 
the above limit also holds for large training 
conditions. 
1 Introduction 
Word alignment is a central problem in statistical 
machine translation (SMT). A recent trend in this 
area of research is to exploit supervised learning to 
improve alignment accuracy by mimicking human 
alignment. Studies in this line of work include 
Haghighi et al, 2009; DeNero and Klein, 2010; 
Setiawan et al, 2010, just to name a few. 
The objective of this work is to explore the per-
formance limit of supervised word alignment. 
More specifically, we would like to know what 
magnitude of gain in MT performance we can ex-
pect from supervised alignment over the state of 
the art unsupervised alignment if we have access to 
a large amount of parallel data. Since alignment 
errors have been assumed to be a major hindrance 
to good MT, an answer to such a question might 
help us find new directions in MT research. 
Our method is to use human alignment as the 
oracle of supervised learning and compare its per-
formance against that of GIZA++ (Och and Ney 
2003), a state of the art unsupervised aligner. Our 
study was based on a manually aligned Chinese-
English corpus (Li, 2009) with 280K word tokens. 
Such a study has been previously impossible due to 
the lack of a hand-aligned corpus of sufficient size.   
To our surprise, the gain in MT performance us-
ing human alignment is very small, less than 1 
point in BLEU. Furthermore, our diagnostic ex-
periments indicate that the result is not an artifact 
of small training size since alignment errors are 
less harmful with more data. 
We would like to stress that our result does not 
mean we should discontinue research in improving 
word alignment. Rather it shows that current trans-
lation models, of which the string-to-tree model 
(Shen et al, 2008) used in this work is an example, 
cannot fully utilize super-accurate word alignment. 
In order to significantly improve MT quality we 
need to improve both word alignment and the 
translation model. In fact, we found that some of 
the information in the LDC hand-aligned corpus 
that might be useful for resolving certain transla-
tion ambiguities (e.g. verb tense, pronoun co-
references and modifier-head relations) is even 
harmful to the system used in this work. 
165
2 Experimental Setup 
2.1 Description of MT System 
We used a state of the art hierarchical decoder in 
our experiments. The system exploits a string to 
tree translation model, as described by Shen et al 
(2008). It uses a small set of linguistic and contex-
tual features, such as word translation probabilities, 
rule translation probabilities, language model 
scores, and target side dependency scores, to rank 
translation hypotheses. In addition, it uses a large 
number of discriminatively tuned features, which 
were inspired by Chiang et al (2009) and imple-
mented in a way described in (Devlin 2009). Some 
of the features, e.g. context dependent word trans-
lation probabilities and discriminative word pairs, 
are motivated in part to discount bad translation 
rules caused by noisy word alignment. The system 
used a 3-gram language model (LM) for decoding 
and a 5-gram LM for rescoring. Both LMs were 
trained on about 9 billion words of English text. 
We tuned the system on a set of 4,171 sentences 
and tested on a set of 4,060 sentences. Both sets 
were drawn from the Chinese newswire develop-
ment data for the DARPA GALE program. On av-
erage, each sentence has around 1.7 reference 
translations for both sets. The tuning metric was 
BLEU, but we reported results in BLEU (Papineni 
et al, 2002) and TER (Snover et al, 2006). 
2.2 Hand Aligned Corpus 
The hand aligned corpus we used is LDC2010E63, 
which has around 280K words (English side). This 
corpus was annotated with alignment links be-
tween Chinese characters and English words. Since 
the MT system used in this work is word-based, we 
converted the character-based alignment to word-
based alignment. We aligned Chinese word s to 
English word t if and only if s contains a character 
c that was aligned to t in the LDC annotation. 
 A unique feature of the LDC annotation is that 
it contains information beyond simple word corre-
spondences. Some links, called special links in this 
work, provide contextual information to resolve 
ambiguities in tense, pronoun co-reference, modi-
fier-head relation and so forth. The special links 
are similar to the so-called possible links described 
in other studies (Och and Ney, 2003; Fraser and 
Marcu, 2007), but are not identical. While such 
links are useful for making high level inferences, 
they cannot be effectively exploited by the transla-
tion model used in this work. Worse, they can hurt 
its performance by hampering rule extraction. 
Since the special links were marked with special 
tags to distinguish them from regular links, we can 
selectively remove them and check the impact on 
MT performance. 
Figure 1 shows an example sentence with hu-
man alignment. Solid lines indicate regular word 
correspondences while dashed lines indicate spe-
cial links. Tags inside [] indicate additional infor-
mation about the function of the words connected 
by special links. 
 
 
 
 
 
 
 
Figure 1: An example sentence pair with human 
alignment 
 
2.3 Parallel Corpora and Alignment Schemes 
Our experiments used two parallel training corpora, 
aligned by alternative schemes, from which trans-
lation rules were extracted. 
 
The corpora are: 
? Small: the 280K word hand-aligned cor-
pus, with human alignment removed 
? Large: a 31M word corpus of Chinese-
English text, comprising a number of 
component corpora, one of which is the 
small corpus1 
 
The alignment schemes are: 
? giza-weak: Subdivide the large corpus into 
110 chunks of equal size and run GIZA++ 
separately on each chunk. One of the 
chunks is the small corpus mentioned 
above. This produced low quality unsuper-
vised alignment. 
                                                          
1
 Other data items included are LDC{2002E18,2002L27, 
2005E83,2005T06,2005T10,2005T34,2006E24,2006E34, 
2006E85,2006E92,2006G05,2007E06,2007E101,2007E46, 
2007E87,2008E40,2009E16,2008E56} 
Chinese: gei[OMN]       ni    ti gong          jie shi 
 
English:  provide  you   with[OMN]  an[DET]   explanation  
166
? giza-strong: Run GIZA++ on the large 
corpus in one large chunk. Alignment for 
the small corpus was extracted for experi-
ments involving the small corpus. This 
produced high quality unsupervised align-
ment. 
? gold-original: human alignment, including 
special links 
? gold-clean: human alignment, excluding 
special links 
Needless to say, gold alignment schemes do not 
apply to the large corpus. 
3 Results  
3.1 Results on Small Corpus 
The results are shown in Table 2. The special links 
in the human alignment hurt MT (Table 2, gold-
original vs. gold-clean). In fact, with such links, 
human alignment is worse than unsupervised 
alignment (Table 2, gold-original vs. giza-strong). 
After removing such links, human alignment is 
better than unsupervised alignment, but the gain is 
small, 0.72 point in BLEU (Table 2, gold-clean vs. 
giza-strong). As expected, having access to more 
training data increases the quality of unsupervised 
alignment (Table 1) and as a result the MT per-
formance (Table 2, giza-strong vs. giza-weak).  
 
 
Alignment Precision Recall  F 
  gold-clean 1.00 1.00 1.00 
giza-strong 0.81 0.72 0.76 
giza-weak 0.65 0.58 0.61 
 
Table 1: Precision, recall and F score of different 
alignment schemes. F score is the harmonic mean 
of precision and recall. 
 
 
 
Alignment BLEU TER 
 giza-weak 18.73 70.50 
giza-strong 21.94 66.70 
gold-original 20.81 67.50 
gold-clean 22.66 65.92 
 
Table 2: MT results (lower case) on small corpus 
It is interesting to note that from giza-weak to giza-
strong, alignment accuracy improves by 15% and 
the BLEU score improves by 3.2 points. In com-
parison, from giza-strong to gold-clean, alignment 
accuracy improves by 24% but BLEU score only 
improves by 0.72 point. This anomaly can be 
partly explained by the inherent ambiguity of word 
alignment. For example, Melamed (1998) reported 
inter annotator agreement for human alignments in 
the 80% range. The LDC corpus used in this work 
has a higher agreement, about 90% (Li et al, 
2010). That means much of the disagreement be-
tween giza-strong and gold alignments is probably 
due to arbitrariness in the gold alignment. 
3.2 Results on Large Corpus 
As discussed before, the gain using human align-
ment over GIZA++ is small on the small corpus. 
One may wonder whether the small magnitude of 
the improvement is an artifact of the small size of 
the training corpus. 
To dispel the above concern, we ran diagnostic 
experiments on the large corpus to show that with 
more training data, the benefit from improved 
alignment is less critical. The results are shown in 
Table 3. On the large corpus, the difference be-
tween good and poor unsupervised alignments is 
2.37 points in BLEU (Table 3, giza-strong vs. giza-
weak). In contrast, the difference between the two 
schemes is larger on the small corpus, 3.21 points 
in BLEU (Table 2, giza-strong vs. giza-weak). 
Since the quality of alignment of each scheme does 
not change with corpus size, the results indicate 
that alignment errors are less harmful with more 
training data. We can therefore conclude the small 
magnitude of the gain using human alignment is 
not an artifact of small training. 
Comparing giza-strong of Table 3 with giza-
strong of Table 2, we can see the difference in MT 
performance is about 8 points in BLEU (20.94 vs. 
30.21). This result is reasonable since the small 
corpus is two orders of magnitude smaller than the 
large corpus. 
 
 
Alignment BLEU TER 
 giza-weak 27.84 59.38 
giza-strong 30.21 56.62 
 
Table 3: MT results (lower case) on large corpus 
167
3.3 Discussions  
Some studies on supervised alignment (e.g.  
Haghighi et al, 2009; DeNero and Klein, 2010) 
reported improvements greater than the limit we 
established using an oracle aligner. This seemingly 
inconsistency can be explained by a number of 
factors. First, we used more data (31M) to train 
GIZA++, which improved the quality of unsuper-
vised alignment. Second, some of the features in 
the MT system used in this work, such as context 
dependent word translation probabilities and dis-
criminatively trained penalties for certain word 
pairs, are designed to discount incorrect translation 
rules caused by alignment errors. Third, the large 
language model (trained with 9 billion words) in 
our experiments further alleviated the impact of 
incorrect translation rules. Fourth, the GALE test 
set has fewer reference translations than the NIST 
test sets typically used by other researchers (1.7 
references for GALE, 4 references for NIST).  It is 
well known that BLEU is very sensitive to the 
number of references used for scoring. Had we 
used a test set with more references, the improve-
ment in BLEU score would probably be higher. An 
area for future work is to examine the impact of 
each factor on BLEU score. While these factors 
can affect the numerical value of our result, they 
do not affect our main conclusion: Improving word 
alignment alone will not produce a breakthrough in 
MT quality.  
DeNero and Klein (2010) described a technique 
to exploit possible links, which are similar to spe-
cial links in the LDC hand aligned data, to improve 
rule coverage. They extracted rules with and with-
out possible links and used the union of the ex-
tracted rules in decoding. We applied the technique 
on the LDC hand aligned data but got no gain in 
MT performance. 
Our work assumes that unsupervised aligners 
have access to a large amount of training data. For 
language pairs with limited training, unsupervised 
methods do not work well. In such cases, super-
vised methods can make a bigger difference. 
4 Related Work 
The study of the relation between alignment qual-
ity and MT performance can be traced as far as to 
Och and Ney, 2003. A more recent study in this 
area is Fraser and Marcu, 2007. Unlike our work, 
both studies did not report MT results using oracle 
alignment. 
Recent work in supervised alignment include 
Haghighi et al, 2009; DeNero and Klein, 2010; 
Setiawan et al, 2010, just to name a few. Fossum 
et al (2008) used a heuristic based method to de-
lete problematic alignment links and improve MT.   
Li (2009) described the annotation guideline of 
the hand aligned corpus (LDC2010E63) used in 
this work. This corpus is at least an order of mag-
nitude larger than similar corpora. Without it this 
work would not be possible.  
5 Conclusions  
Our experiments showed that even with human 
alignment, further improvement in MT quality will 
be small with the current SMT paradigm. Our ex-
periments also showed that certain alignment in-
formation suitable for making complex inferences 
can even hamper current SMT models. A future 
direction for SMT is to develop translation models 
that can effectively employ such information. 
Acknowledgments 
This work was supported by DARPA/IPTO Con-
tract No. HR0011-06-C-0022 under the GALE 
program2 (Approved for Public Release, Distribu-
tion Unlimited). The authors are grateful to Mi-
chael Kayser for suggestions to improve the pres-
entation of this paper.  
References  
David Chiang, Kevin Knight, and Wei Wang. 2009. 
11,001 new features for statistical machine transla-
tion. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North 
American Chapter of the ACL, pages 218?226. 
John DeNero and Dan Klein. 2010. Discriminative 
Modeling of Extraction Sets for Machine Translation. 
In Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, pages 1453?
1463.  
                                                          
2
 The views, opinions, and/or findings contained in this arti-
cle/presentation are those of the author/presenter and should 
not be interpreted as representing the official views or policies, 
either expressed or implied, of the Defense Advanced Re-
search Projects Agency or the Department of Defense. 
168
Jacob Devlin. 2009. Lexical features for statistical ma-
chine translation. Master?s thesis, University of 
Maryland. 
Victoria Fossum, Kevin Knight and Steven Abney. 
2008. Using Syntax to Improve Word Alignment 
Precision for Syntax-Based Machine Translation, In 
Proceedings of the third Workshop on Statistical MT, 
ACL, pages 44-52.  
Alexander Fraser and Daniel Marcu. 2007. Measuring 
Word Alignment Quality for Statistical Machine 
Translation. Computational Linguistics. 33(3): 293-
303. 
Aria Haghighi, John Blitzer, John DeNero and Dan 
Klein. 2009. Better word alignments with supervised 
ITG models, In Proceedings of the Joint Conference 
of the 47th Annual Meeting of the ACL and the 4th 
International Joint Conference on Natural Language 
Processing of the AFNLP, pages 923-931.  
Xuansong Li. 2009. Guidelines for Chinese-English 
Word Alignment, Version 4.0, April 16, 2009, 
http://ww.ldc.upenn.edu/Project/GALE. 
Xuansong Li, Niyu Ge, Stephen Grimes, Stephanie M. 
Strassel and  Kazuaki Maeda. 2010. Enriching Word 
Alignment with Linguistic Tags. In Proceedings of 
the Seventh International Conference on Language 
Resources and Evaluation, Valletta, Malta. 
Dan Melamed. 1998. Manual annotation of translational 
equivalence: The blinker project. Technical Report 
98-07, Institute for Research in Cognitive Science, 
Philadelphia. 
Franz Josef Och and Hermann Ney. 2003. A systematic 
comparison of various statistical alignment models. 
Computational Linguistics, 29(1):19-51. 
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic 
evaluation of machine translation. In Proceedings of 
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318. 
Hendra Setiawan, Chris Dyer, and Philip Resnik. 2010. 
Discriminative Word Alignment with a Function 
Word Reordering Model. In Proceedings of 2010 
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 534?544. 
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A 
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In 
Proceedings of ACL-08: HLT, pages 577?585. 
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linea 
Micciulla, and John Makhoul. 2006. A Study of 
Translation Edit Rate with Targeted Human Annota-
tion. In Proceedings of Association for Machine 
Translation in the Americas, pages 223-231.  
 
 
169
