Collocation Extraction: Needs, Feeds and Results of an Extraction System
for German
Julia Ritz
Institut f?r Maschinelle Sprachverarbeitung (IMS)
Universit?t Stuttgart
Azenbergstr. 12
70174 Stuttgart
Germany
Julia.Ritz@ims.uni-stuttgart.de
Abstract
This paper provides a specification of re-
quirements for collocation extraction sys-
tems, taking as an example the extraction
of noun + verb collocations from German
texts. A hybrid approach to the extrac-
tion of habitual collocations and idioms
is presented, aiming at a detailed descrip-
tion of collocations and their morphosyn-
tax for natural language generation sys-
tems as well as to support learner lexicog-
raphy.
1 Introduction
Since Firth first described collocations as habit-
ual word combinations in the 1950ies (cf. Firth,
1968), a number of papers focusing on collocation
extraction have been published (see the overviews
in (Evert, 2004; Bartsch, 2004)). Most studies
concentrate on the extraction from English. How-
ever, the procedures proposed in these studies can-
not necessarily be applied to other languages as
English stands out, e.g. with respect to configu-
rationality. They rely on the fact that the syntax
of English (and of all configurational languages)
provides positional clues to the grammatical func-
tion of noun phrases, and they exploit this con-
cept by means of window-based, adjacency-based
or pattern-based extraction, combined with associ-
ation measures to identify co-occurrences that are
more frequent than statistically expectable. What
these procedures do not cover is semantic-oriented
definitions like (a) and (b).
a. A collocation is a combination of a free (?au-
tosematic?) element (the base) and a lexically
determined (?synsemantic?) element (the col-
locate, which may lose (some of) its meaning
in a collocation) (adapted from (Hausmann,
1979; Hausmann, 1989; Hausmann, 2003)).
b. A collocation is a word combination whose
semantic and/or syntactic properties cannot
be fully predicted from those of its compo-
nents, and which therefore has to be listed in
a lexicon (Evert, 2004).
We argue that linguistic knowledge could not
only improve results (Krenn, 2000b; Smadja,
1993) but is essential when extracting colloca-
tions from certain languages: this knowledge pro-
vides other applications (or a lexicon user, respec-
tively) with a fine-grained description of how the
extracted collocations are to be used in context.
Additional requirements resulting from the
needs of dictionary users are described in (Haus-
mann, 2003; Heid and Gouws, 2005) and are of
interest not only in lexicography but can also be
transferred to the field of natural language gener-
ation. These requirements influence the develop-
ment of collocation extraction systems, which mo-
tivates this paper.
The structure of the paper is as follows: In chap-
ter 2, the requirements, depending on factors like
the targeted language, are presented. We then dis-
cuss and suggest methods to meet the given needs.
A documentation of ongoing work on the extrac-
tion of noun + verb collocations from German
texts is given in chapter 3. Chapter 4 gives a con-
clusion and an outlook on work still to be done.
2 Collocation Extraction Tools:
Requirements
The development of a collocation extraction tool
depends on the following conditions:
1. properties of the targeted language
41
2. the targeted application
3. the kinds of collocations to be extracted
4. the degree of detail
Whereas issues 1 to 3 deal with the collocation
itself, issue 4 is focused at the collocation in con-
text, i.e. its behaviour (from a syntagmatic analy-
sis point of view) or, respectively, its use (from a
generation perspective).
2.1 Language factors
One of the most important factors is, of course, the
targeted language and its main characteristics with
respect to word formation and word order. De-
pending on word and constituent order, the pros
and cons of positional vs. relational extraction
patterns need to be considered. Positional pat-
terns (based on adjacency or a ?window?) are ad-
equate for configurational languages, but in lan-
guages with rather free word order, words belong-
ing to a phrase or collocation do not necessarily
occur within a predefined span1.
Extracting word combinations using relational
patterns (represented by part of speech (PoS) tags
or dependency rules) offers a higher level of ab-
straction and improves the results (cf. (Krenn,
2000b; Smadja, 1993)). However, this requires
part of speech tagging and possibly partial parsing.
A system extracting word combinations by apply-
ing relational patterns, obviously profits from lan-
guage specific knowledge about phrase and sen-
tence structure and word formation. One example
is the order of adjective + noun pairs: in English
and German, the adjective occurs left of the noun,
whereas in French, the adjective can occur left or
right of the noun. Another example is compound-
ing, handled differently in different languages:
noun + noun in English, typically separated by
a white space (e.g. death penalty) vs. noun +
prepositional phrase in French (e.g. peine de mort)
vs. compound noun in German (e.g. Todesstrafe).
Consequently, language specific word formation
rules need to be considered when designing ex-
traction patterns. For languages with a rich in-
flectional morphology where the individual word
forms are rather rare, frequency counts and results
1In German, e.g., in usual verb second constructions with
a full verb in the left sentence bracket (topological field the-
ory see (W?llstein-Leisten et al, 1997)), particles of particle
verbs appear in the right sentence bracket. The middle field
(containing arguments and possibly adjuncts of the verb) is
of undetermined length.
of statistical analyses are little reliable. To allow a
grouping of words sharing the same lemma, lem-
matisation is crucial.
2.2 Application factors
Other important factors are the targeted applica-
tion (i.e. analysis vs. generation) and, to some ex-
tent resulting from it, factors (3.) and (4.), above.
Depending on the purpose of the tool (or lexicon,
respectively), the collocation definition chosen as
an outline may vary, e.g. including transparent and
regular collocations (cf. (Tutin, 2004)) for genera-
tion purposes, but excluding them for analysis pur-
poses. In addition, a more detailed description of
the use of collocations in context (e.g. information
about preferences with respect to the determiner,
etc.) is needed for generation purposes than for
text analysis.
2.3 Factors of collocation definition
Collocations can be distinguished on two levels:
the formal level and the content level. On the for-
mal level, a collocation can be classified accord-
ing to the structural relation between its elements.
Typical patterns are shown in table 12 (taken from
(Heid and Gouws, 2005)).
On the content level, there are regular, transpar-
ent, and opaque collocations (according to (Tutin,
2004)) and, taking definition (b) into account, id-
ioms as well. However, as a classification at the
content level needs detailed semantic description,
we see no means of accomplishing this goal other
than manually at the moment.
2.4 Contextual factors
(Hausmann, 2003; Heid and Gouws, 2005; Evert
et al, 2004) argue that collocations have strong
preferences with respect to their morphosyntax
(see examples (1) and (2)) and may be combined
(see example (3)). The collocation in example (1)
(?to charge somebody?) is restricted with respect
to the determiner (null determiner) of the base,
whereas the same base shows a strong preference
for a (denite or indenite) determiner when used
2Abbreviations in table 1:
advl - adverbial
prd - predicative
subj - subject
obj - object
pobj - prepositional object
dat - dative case
gen - genitive case
quant - quantifying
42
No. Type Example
1 N + Adj tiefer Schlaf
2 Adj + Adv tief rot
3 V + Adv tief schlafen
4 V + NP   Baukl?tze staunen
5 V + N 	
 Frage + sich stellen
6 V + N   Anforderungen + gen?-
gen
7 V + N 
 Frage + aufwerfen
8 V + PP
 zu + Darstellung + gelan-
gen
9 V + Adj  verr?ckt spielen
10 N + N  Einreichung des Antrags
11 N       + N ein Schwarm Heringe
the category containing the base is underlined.
Table 1: Collocational patterns
with a different collocate (example (2), ?to drop
a lawsuit?). Example (3) shows two collocations
sharing the base can form a collocational sequence
(example taken from (Heid and Gouws, 2005)).
(1) ? Anklage erheben
(2) die/eine Anklage fallenlassen
(3) Kritik ?ben + scharfe Kritik
scharfe Kritik ?ben
For both natural language generation systems
and lexicography, such information is highly rel-
evant. Therefore, the extraction of contextual in-
formation (called ?context parameters? in the fol-
lowing) should be integrated into the collocation
extraction process.
3 Extracting noun + verb collocations
from German
The standard architecture for collocation extrac-
tion systems contains three stages (cf. (Krenn,
2000)): a more or less detailed linguistic analysis
of the corpus text (preprocessing), an extraction
step and a statistic filtering of the extracted word
combinations. We follow this architecture (see fig-
ure 1). However, our hypothesis differs from other
approaches. Collocations are often restricted with
respect to their morphosyntax. We test to what ex-
tent they can be identified via these restrictions.
3.1 Approach
In an experiment, we extracted relational word
combinations (verb + subject/object pairs) from
German newspaper texts.
The syntactic patterns for the extraction of these
combinations concentrate on verb-final construc-
tions as in example (4) and verb second con-
structions with a modal verb in the left sentence
bracket according to the topological field theory
(see (W?llstein-Leisten et al, 1997)) as in exam-
ple (5). The reason is that, in these constructions,
the particle forms one word with the verb (see ex-
ample (6)), as opposed to usual verb second con-
structions (see example (7)). Thus, we need not
recombine verb + particle groups that appear sep-
aratedly.
(4) ... wenn Wien einen Antrag auf Vollmitglied-
schaft stellt.
(?if Vienna an application for full member-
ship puts?)
(if Vienna applies for full membership)
(5) ... kann Wien einen Antrag auf Vollmitglied-
schaft stellen.
(?might Vienna an application for full mem-
bership put.?)
(Vienna might apply for full membership.)
(6) ..., da? er ein Schild auf stellt.
(?that he a sign upputs?)
(that he puts up a sign)
(7) Er stellt ein Schild auf.
(?He puts a sign up.?)
(He puts up a sign.)
Preprocessing
As data, we used a collection of 300 million
words from German newspaper texts dating from
1987 to 1993. The corpus is tokenized and PoS-
tagged by the Treetagger (Schmid, 1994), then
chunk annotated by YAC (Kermes, 2003). The
chunker YAC determines phrase boundaries and
heads, and disambiguates agreement information
as far as possible. It is based on the corpus query
language cqp (Christ et al, 1999)3 , which can in
turn be used to query the chunk annotations.
Data Extraction
The syntactic patterns used to extract verb +
subject/object combinations are based on PoS tags
and chunk information. These patterns are repre-
sented using cqp macros (see figure 2). The cqp
syntax largely overlaps with regular expressions.
3http://www.ims.uni-
stuttgart.de/projekte/CorpusWorkbench/
43
cqp
postprocessing
macros
preprocessing collocation identification
interpretationanalysis
(STTS tagset)
(newspaper texts) PoS tagging
tokenizing,
information
agreement
morphology
TreeTagger
phrase boundaries,
extraction
lexicons
stat. filtering
NLP resources
corpus
data
lemma and
IMSLex
wrt. morphosyntax
pairs and their features
extraction of noun + verb database
morphology
semantic classification
agreement
(annotation of
partial parsing
YAC
Figure 1: Tool architecture
( 1) MACRO n_vfin(0)
( 2) (
( 3) [pos = "(KOUS|VMFIN)"]
( 4) []*
( 5) Proceedings of the EACL 2009 Workshop on Language Technologies for African Languages ? AfLaT 2009, pages 17?24,
Athens, Greece, 31 March 2009. c?2009 Association for Computational Linguistics
Information Structure in African Languages: Corpora and Tools 
Christian Chiarcos*, Ines Fiedler**, Mira Grubic*, Andreas Haida**, Katharina 
Hartmann**, Julia Ritz*, Anne Schwarz**, Amir Zeldes**, Malte Zimmermann* 
 
* Universit?t Potsdam 
Potsdam, Germany 
{chiarcos|grubic| 
julia|malte}@ 
ling.uni-potsdam.de 
** Humboldt-Universit?t zu Berlin 
Berlin, Germany 
{ines.fiedler|andreas.haida| 
k.hartmann|anne.schwarz| 
amir.zeldes}@rz.hu-berlin.de 
 
Abstract 
In this paper, we describe tools and resources 
for the study of African languages developed 
at the Collaborative Research Centre ?Infor-
mation Structure?. These include deeply anno-
tated data collections of 25 subsaharan 
languages that are described together with 
their annotation scheme, and further, the cor-
pus tool ANNIS that provides a unified access 
to a broad variety of annotations created with a 
range of different tools. With the application 
of ANNIS to several African data collections, 
we illustrate its suitability for the purpose of 
language documentation, distributed access 
and the creation of data archives. 
1 Information Structure 
The Collaborative Research Centre (CRC) 
"Information structure: the linguistic means for 
structuring utterances, sentences and texts" 
brings together scientists from different fields of 
linguistics and neighbouring disciplines from the 
University of Potsdam and the Humboldt-
University Berlin. Our research comprises the 
use and advancement of corpus technologies for 
complex linguistic annotations, such as the 
annotation of information structure (IS). We 
define IS as the structuring of linguistic 
information in order to optimize information 
transfer within discourse: information needs to 
be prepared ("packaged") in different ways 
depending on the goals a speaker pursues within 
discourse.  
Fundamental concepts of IS include the 
concepts `topic?, `focus?, `background? and 
`information status?. Broadly speaking, the topic 
is the entity a specific sentence is construed 
about, focus represents the new or newsworthy 
information a sentence conveys, background is 
that part of the sentence that is familiar to the 
hearer, and information status refers to different 
degrees of familiarity of an entity.  
Languages differ wrt. the means of realization 
of IS, due to language-specific properties (e.g., 
lexical tone). This makes a typological 
comparison of traditionally less-studied 
languages to existing theories, mostly on 
European languages, very promising. Particular 
emphasis is laid on the study of focus, its 
functions and manifestations in different 
subsaharan languages, as well as the 
differentiation between different types of focus, 
i.e., term focus (focus on arguments/adjuncts), 
predicate focus (focus on verb/verb 
phrase/TAM/truth value), and sentence focus 
(focus on the whole utterance). 
We describe corpora of 25 subsaharan 
languages created for this purpose, together with 
ANNIS, the technical infrastructure developed to 
support linguists in their work with these data 
collections. ANNIS is specifically designed to 
support corpora with rich and deep annotation, as 
IS manifests itself on practically all levels of 
linguistic description. It provides user-friendly 
means of querying and visualizations for 
different kinds of linguistic annotations, 
including flat, layer-based annotations as used 
for linguistic glosses, but also hierarchical 
annotations as used for syntax annotation. 
2 Research Activities at the CRC 
Within the Collaborative Research Centre, there 
are several projects eliciting data in large 
amounts and great diversity. These data, 
originating from different languages, different 
modes (written and spoken language) and 
specific research questions characterize the 
specification of the linguistic database ANNIS. 
2.1 Linguistic Data Base 
The project ?Linguistic database for information 
structure: Annotation and Retrieval?, further 
17
database project, coordinates annotation 
activities in the CRC, provides service to projects 
in the creation and maintenance of data 
collections, and conducts theoretical research on 
multi-level annotations. Its primary goals, 
however, are the development and investigation 
of techniques to process, to integrate and to 
exploit deeply annotated corpora with multiple 
kinds of annotations. One concrete outcome of 
these efforts is the linguistic data base ANNIS 
described further below. For the specific 
facilities of ANNIS, its application to several 
corpora of African languages and its use as a 
general-purpose tool for the publication, 
visualization and querying of linguistic data, see 
Sect. 5. 
 
2.2 Gur and Kwa Languages 
 
Gur and Kwa languages, two genetically related 
West African language groups, are in the focus of 
the project ?Interaction of information structure 
and grammar in Gur and Kwa languages?, 
henceforth Gur-Kwa project. In a first research 
stage, the precise means of expression of the 
pragmatic category focus were explored as well 
as their functions in Gur and Kwa languages. For 
this purpose, a number of data collections for 
several languages were created (Sect. 3.1). 
Findings obtained with this data led to different 
subquestions which are of special interest from a 
cross-linguistic and a theoretical point of view.  
These concern (i) the analysis of syntactically 
marked focus constructions with features of 
narrative sentences (Schwarz & Fiedler 2007), 
(ii) the study of verb-centered focus (i.e., focus 
on verb/TAM/truth value), for which there are 
special means of realization in Gur and Kwa 
(Schwarz, forthcoming), (iii) the identification of 
systematic focus-topic-overlap, i.e., coincidence 
of focus and topic in sentence-initial nominal 
constituents (Fiedler, forthcoming). The project's 
findings on IS are evaluated typologically on 19 
selected languages. The questions raised by the 
project serve the superordinate goal to expand 
our knowledge of linguistically relevant 
information structural categories in the less-
studied Gur and Kwa languages as well as the 
interaction between IS, grammar and language 
type. 
2.3 Chadic Languages 
The project ?Information Structure in the Chadic 
Languages?, henceforth Chadic project, 
investigates focus phenomena in Chadic 
languages.  The Chadic languages are a branch of 
the Afro-Asiatic language family mainly spoken 
in northern Nigeria, Niger, and Chad. As tone 
languages, the Chadic languages represent an 
interesting subject for research into focus 
because here, intonational/tonal marking ? a 
commonly used means for marking focus in 
European languages ? is in potential conflict 
with lexical tone, and so, Chadic languages 
resort to alternative means for marking focus.  
The languages investigated in the Chadic 
project include the western Chadic languages 
Hausa, Tangale, and Guruntum and the central 
Chadic languages Bura, South Marghi, and Tera. 
The main research goals of the Chadic project 
are a deeper understanding of the following 
asymmetries: (i) subject focus is obligatorily 
marked, but marking of object focus is optional; 
(ii) in Tangale and Hausa there are sentences that 
are ambiguous between an object-focus 
interpretation and a predicate-focus 
interpretation, but in intonation languages like 
English and German, object focus and predicate 
focus are always marked differently from each 
other; (iii) in Hausa, Bole, and Guruntum there is 
only a tendency to distinguish different types of 
focus (new-information focus vs. contrastive 
focus), but in European languages like 
Hungarian and Finnish, this differentiation is 
obligatory. 
2.4 Focus from a Cross-linguistic 
Perspective 
The project "Focus realization, focus 
interpretation, and focus use from a cross-
linguistic perspective", further focus project, 
investigates the correspondence between the 
realization, interpretation and use of with an 
emphasis on focus in African and south-east 
Asian languages. It is structured into three fields 
of research: (i) the relation between differences 
in realization and differences in semantic 
meaning or pragmatic function, (ii) realization, 
interpretation and use of predicate focus, and (iii) 
association with focus.  
The relation between differences in realization 
and semantic/pragmatic differences (i) 
particularly pertains the semantic interpretation 
of focus: For Hungarian and Finnish, a 
differentiation between two semantic types of 
foci corresponding to two different types of 
focus realization was suggested, and we 
investigate whether the languages studied here 
have a similar distinction between two (or more) 
semantic focus types, whether this may differ 
18
from language to language, and whether 
differences in focus realization correspond to 
semantic or pragmatic differences.  
The investigation of realization, interpretation 
and use of predicate focus (ii) involves the 
questions why different forms of predicate focus 
are often realized in the same way, why they are 
often not obligatorily marked, and why they are 
often marked differently from term focus. 
Association with focus (iii) means that the 
interpretation of the sentence is influenced by the 
focusing of a particular constituent, marked by a 
focus-sensitive expression (e.g., particles like 
`only?, or quantificational adverbials like 
`always?), while usually, focus does not have an 
impact on the truth value of a sentence. The 
project investigates which focus-sensitive 
expressions there are in the languages studied, 
what kinds of constituents they associate with, 
how this association works, and whether it works 
differently for focus particles and 
quantificational adverbials. 
3 Collections of African Language Data 
at the CRC 
3.1 Gur and Kwa Corpora 
The Gur and Kwa corpora currently comprise  
data from 19 languages.  
Due to the scarceness of information available 
on IS in the Gur and Kwa languages, data had to 
be elicited, most of which was done during field 
research, mainly in West Africa, and some in 
Germany with the help of native speakers of the 
respective languages. The typologically diverse 
languages in which we elicited data by ourselves 
are: Baatonum, Buli, Byali, Dagbani, Ditammari, 
Gurene, Konkomba, Konni, Nateni, Waama, 
Yom (Gur languages) and Aja, Akan, Efutu, Ewe, 
Fon, Foodo, Lelemi, Anii (Kwa languages). 
The elicitation of the data based mainly on the 
questionnaire on information structure, 
developed by our research group (QUIS, see 
Section 4.2). This ensured that  comparable data 
for the typological comparison were obtained. 
Moreover, language-specific additional tasks and 
questionnaires tailored to a more detailed 
analysis or language-specific traits were 
developed. 
As the coding of IS varies across different 
types of texts, different text types were included 
in the corpus, such as (semi-)spontaneous 
speech, translations, mono- and dialogues. Most 
of the languages do not have a long literacy 
tradition, so that the corpus data mainly 
represents oral communication. 
In all, the carefully collected heterogeneous 
data provide a corpus that gives a comprehensive 
picture of IS, and in particular the focus systems, 
in these languages.  
3.2 Hausar Baka Corpus 
In the Chadic project, data from 6 Chadic 
languages are considered. 
One of the larger data sets annotated in the 
Chadic project is drawn from Hausar Baka 
(Randell, Bature & Schuh 1998), a collection of 
videotaped Hausa dialogues recording natural 
interaction in various cultural milieus, involving 
over fifty individuals of different age and gender. 
The annotated data set consists of approximately 
1500 sentences.  
The corpus was annotated according to the 
guidelines for Linguistic Information Structure 
Annotation (LISA, see Section 4.2). The Chadic 
languages show various forms of syntactic 
displacement, and in order to account for this, an 
additional annotation level was added: 
constituents are marked as ex-situ=?+?  if 
they occur displaced from their canonical, 
unmarked position. 
An evaluation of the focus type and the 
displacement status reveals tendencies in the 
morphosyntactic realization of different focus 
types, see Sect. 5.2. 
3.3 Hausa Internet Corpus 
Besides these data collections that are currently 
available in the CRC and in ANNIS, further 
resources are continuously created. As such, a 
corpus of written Hausa is created in cooperation 
with another NLP project of the CRC.   
The corpora previously mentioned mostly 
comprise elicited sentences from little-
documented languages with rather small 
language communities. Hausa, in contrast, is 
spoken by more than 24 million native speakers, 
with large amounts of Hausa material (some of it 
parallel to material in other, more-studied 
languages) available on the internet. This makes 
Hausa a promising language for the creation of 
resources that enable a quantitative study of 
information structure.  
The Hausa internet corpus is designed to cover 
different kinds of written language, including 
news articles from international radio stations 
(e.g., http://www.dw-world.de), religious texts, 
literary prose but also material similar to 
spontaneous spoken language (e.g., in chat logs). 
19
Parallel sections of the corpus comprise 
excerpts from the novel Ruwan Bagaja by 
Abubakar Imam, Bible and Qur?an sections, and 
the Declaration of Human Rights. As will be 
described in Section 4.3, these parallel sections 
open the possibility of semiautomatic 
morphosyntactic annotation, providing a unique 
source for the study of information structure in 
Hausa. Sect. 5.2 gives an example for 
bootstrapping  ex-situ constituents in 
ANNIS only on the basis of  morphosyntactic 
annotation. 
4 Data Elicitation and Annotation 
4.1 Elicitation with QUIS 
The questionnaire on information structure 
(Skopeteas et al, 2006) provides a tool for the 
collection of natural linguistic data, both spoken 
and written, and, secondly, for the elaboration of 
grammars of IS in genetically diverse languages. 
Focus information, for instance, is typically 
elicited by embedding an utterance in a question 
context. To avoid the influence of a mediator 
(working) language, the main body of QUIS is 
built on the basis of pictures and short movies 
representing a nearly culture- and language-
neutral context. Besides highly controlled 
experimental settings, less controlled settings 
serve the purpose of eliciting longer, cohesive, 
natural texts for studying categories such as 
focus and topic in a near-natural environment. 
4.2 Transcription and Manual Annotation 
In the CRC, the annotation scheme LISA has 
been developed with special respect to 
applicability across typologically different 
languages (Dipper et al, 2007). It comprises 
guidelines for the annotation of phonology, 
morphology, syntax, semantics and IS.  
The data mentioned above is, in the case of 
speech, transcribed according to IPA 
conventions, otherwise written according to 
orthographic conventions, and annotated with 
glosses and IS, a translation of each sentence into 
English or French, (optionally) additional notes, 
references to QUIS experiments, and references 
to audio files and metadata. 
4.3 (Semi-)automatic Annotation 
As to the automization of annotation, we pursue 
two strategies: (i) the training of classifiers on 
annotated data, and (ii) the projection of 
annotations on texts in a source language to 
parallel texts in a target language. 
Machine Learning. ANNIS allows to export 
query matches and all their annotated features to 
the table format ARFF which serves as input to 
the data mining tool WEKA (Witten & Frank, 
2005), where instances can be clustered, or used 
to train classifiers for any annotation level. 
Projection. Based on (paragraph-, sentence- 
or verse-) aligned sections in the Hausa internet 
corpus, we are about to project annotations from 
linguistically annotated English texts to Hausa, 
in a first step parts of speech and possibly  
nominal chunks. On the projected annotation, we 
will train a tagger/chunker to annotate the 
remaining, non-parallel sections of the Hausa 
internet corpus. Existing manual annotations 
(e.g. of the Hausar Baka corpus) will then serve 
as a gold standard for evaluation purposes. 
Concerning projection techniques, we expect 
to face a number of problems: (i) the question 
how to assign part of speech tags to categories 
existing only in the target language (e.g., the 
person-aspect complex in Hausa that binds 
together information about both the verb (aspect) 
and its (pronominal subject) argument),  (ii) 
issues of orthography: the official orthography 
Hausa (Boko) is systematically underspecified 
wrt. linguistically relevant distinctions. Neither 
vowel length nor different qualities of certain 
consonants (r) are represented, and also, there is 
no marking of tones (see Examples 1 and 2, fully 
specified word forms in brackets). To distinguish 
such homographs, however, is essential to the 
appropriate interpretation and linguistic analysis 
of  utterances.  
(1) ciki ? 1. [c?k?i, noun] 
stomach, 2. [c?k?, prep.] 
inside 
(2) dace ? 1. [d?ac?e, noun] 
coincidence, 2. [d?ac?e, verb] 
be appropriate 
 
We expect that in these cases, statistical 
techniques using context features may help to 
predict correct vowelization and tonal patterns. 
5 ANNIS ? the Linguistic Database of 
Information Structure Annotation 
5.1 Conception and Architecture 
ANNIS (ANNotation of Information Structure) 
is a web-based corpus interface built to query 
and visualize multilevel corpora. It allows the 
user to formulate queries on arbitrary, possibly 
nested annotation levels, which may be 
20
conflictingly overlapping or discontinuous. The 
types of annotations handled by ANNIS include, 
among others, flat, layer-based annotations (e.g., 
for glossing) and hierarchical trees (e.g., syntax). 
Source data. As an architecture designed to 
facilitate diverse and integrative research on IS, 
ANNIS can import formats from a broad variety 
of tools from NLP and manual annotation, the 
latter including EXMARaLDA (Schmidt, 2004), 
annotate (Brants and Plaehn, 2000), Synpathy 
(www.lat-mpi.eu/tools/synpathy/), MMAX2 
(M?ller and Strube, 2006), RSTTool (O'Donnell, 
2000), PALinkA (Orasan, 2003), Toolbox 
(Busemann & Busemann, 2008) etc. These tools 
allow researchers to annotate data for syntax, 
semantics, morphology, prosody, phonetics, 
referentiality, lexis and much more, as their 
research questions require. 
All annotated data are merged together via a 
general interchange format PAULA (Dipper 
2005, Dipper & G?tze 2005), a highly expressive 
standoff XML format that specifically allows 
further annotation levels to be added at a later 
time without disrupting the structure of existing 
annotations. PAULA, then, is the native format 
of ANNIS. 
Backend. The ANNIS server uses a relational 
database that offers many advantages including 
full Unicode support and regular expression 
searches. Extensive search functionalities are 
supported, allowing complex relations between 
individual word forms and annotations, such as 
all forms of overlapping, contained or adjacent 
annotation spans, dominance axes (children, 
ancestors etc., as well as common parent, left- or 
right-most child and more), etc. 
Search. In the user interface, queries can be 
formulated using the ANNIS Query Language 
(AQL). It is based on the definition of nodes to 
be searched for and the relationships between 
these nodes (see below for some examples). A 
graphical query builder is also included in the 
web interface to make access as easy as possible. 
Visualization. The web interface, realized as a 
window-based AJAX application written in Java, 
provides visualization facilities for search results. 
Available visualizations include token-based 
annotations, layered annotations, tree-like 
annotations (directed acyclic graphs), and a 
discourse view of entire texts for, e.g., 
coreference annotation. Multimodal data is 
represented using an embedded media player.  
Special features. By allowing queries on 
multiple, conflicting annotation levels 
simultaneously, the system supports the study of 
interdependencies between a potentially limitless 
variety of annotation levels.  
At the same time, ANNIS allows us to 
integrate and to search through heterogeneous 
resources by means of a unified interface, a 
powerful query language and a intuitive 
graphical query editor and is therefore 
particularly well-suited for the purpose of 
language documentation. In particular, ANNIS 
can serve as a tool for the publication of data 
collections via internet. A fine-grained user 
management allows granting privileged users 
access to specific data collections, to make a 
corpus available to the public, or to seal (but 
preserve) a resource, e.g., until legal issues 
(copyright) are settled. This also makes 
publishing linguistic data collections possible 
without giving them away. 
Moreover, ANNIS supports deep links to 
corpora and corpus queries. This means that 
queries and query results referred to in, e.g., a 
scientific paper, can be reproduced and quoted 
by means of (complex) links (see following 
example). 
5.2 Using ANNIS. An Example Query 
As an illustration for the application of ANNIS to 
the data collections presented above, consider a 
research question previously discussed in the 
study of object focus in Hausa. 
In Hausa, object focus can be realized in two 
ways: either ex-situ or in-situ (Section 3.2). It 
was found that these realizations do not differ in 
their semantic type (Green & Jaggar 2003, 
Hartmann & Zimmermann 2007): instead, the 
marked form signals that the focused constituent 
(or the whole speech act) is unexpected for the 
hearer (Zimmermann 2008). These assumptions 
are consistent with findings for other African 
languages (Fiedler et al 2006).  
In order to verify such claims on corpora with 
morphosyntactic and syntactic annotation for the 
example of Hausa, a corpus query can be 
designed on the basis of the Hausar Baka corpus 
that comprises not only annotations for 
grammatical functions and information-structural 
categories, but also an annotation of ex-situ 
elements. 
 
21
So, in (3), we look for ex-situ constituents 
(variable #1) in declarative sentences in the 
Hausa Bakar corpus, i.e., sentences that are not 
translated as questions (variable #2) such that 
#1 is included in #2 (#1 _i_ #2). 
(3) EX-SITU=?+? & 
TRANSLATION=?.*[^?]? &     #1 
_i_ #2 
Considering the first 25 matches for this query 
on Hausar Baka, 16 examples reveal to be 
relevant (excluding interrogative pronouns and 
elliptical utterances). All of these are directly 
preceded by a period (sentence-initial) or a 
comma (preceded by ee ?yes?, interjections or 
exclamations), with one exception, preceded by a 
sentence initial negation marker. 
Only seven examples are morphologically 
marked by focus particles (nee, cee), focus-
sensitive adverbs (kaw?i ?only?) or quantifiers 
(koomee ?every?). In nine cases, a personal 
pronoun follows the ex-situ constituent, followed 
by the verb. Together, these constraints describe 
all examples retrieved, and as a generalization, 
we can now postulate a number of patterns that 
only make use of morphosyntactic and syntactic 
annotation (token tok, morphological 
segmentation MORPH, parts of speech CLASS, 
nominal chunks CHUNK) with two examples 
given below: 
 
 (4) tok=/[,.!?]/ & 
CHUNK=?NC? & MORPH=/[cn]ee/ & 
#1 . #2 & #2 . #3 
 (5) tok=/[,.!?]/ & 
CHUNK=?NC? & CLASS=/PRON.*/ & 
CLASS=/V/ & #1 . #2 &   #2 . 
#3 & #3 . #4 
 
In (4), we search for a nominal chunk 
following a punctuation sign and preceding a 
focus particle (cee or nee), in (5), we search for a 
nominal chunk preceding a sequence of 
pronoun/aspect marker and  verb.  
One example matching template (5) from the 
Hausar Baka corpus is given in Fig. 1. 
While AQL can be used in this way to help 
linguists understand the grammatical realization 
of certain phenomena, and the grammatical 
context they occur in, patterns like (5) above are 
probably not too readable to an interested user. 
This deficit, however, is compensated by the 
graphical query builder that allows users to 
create AQL queries in a more intuitive way, cf. 
Fig. 2. 
Of course, these patterns are not exhaustive 
and overgenerate. However, they can be directly 
evaluated against the manual ex-situ annotation 
in the Hausar Baka corpus and further refined.  
So, the manual annotation of ex-situ 
constituents in the Hausar Baka corpus provides 
templates for the semi-automatic detection of ex-
situ constituents in a morphosyntactically 
annotated corpus of Hausa: The patterns generate 
a set of candidate examples from which a human 
annotator can then choose real ex-situ 
constituents. Indeed, for a better understanding 
of ex-situ object focus, a study with a larger 
database of more natural language would be of 
great advantage, and this pattern-based approach 
represents a way to create such a database of ex-
situ constructions in Hausa. 
Finally, it would also help find instances of 
predicate focus. When a V(P) constituent is 
focused in Hausa, it is nominalized, and fronted 
like a focused nominal constituent (Hartmann & 
Zimmermann 2007). 
5.3 Related Corpus Tools 
Some annotation tools come with search 
facilities, e.g. Toolbox (Busemann & Busemann, 
2008), a system for annotating, managing and 
 
Figure 2: ANNIS Query Builder, cf. example (5). 
 
Figure 1: ANNIS partitur view, Hausar Baka corpus. 
22
analyzing language data, mainly geared to 
lexicographic use, and ELAN (Hellwig et al, 
2008), an annotation tool for audio and video 
data.  
In contrast, ANNIS is not intended to provide 
annotation functionality. The main reason behind 
this is that both Toolbox and ELAN are problem-
specific annotation tools with limited capabilities 
for application to different phenomena than they 
were designed for. Toolbox provides an intuitive 
annotation environment and search facilities for 
flat, word-oriented annotations; ELAN, on the 
other hand, for annotations that stand in a 
temporal relation to each other. 
These tools ? as well as the other annotation 
tools used within the CRC ? are tailored to a 
particular type of annotation, neither of them 
being capable of sufficiently representing the 
data from all other tools. Annotation on different 
levels, however, is crucial for the investigation of 
information structural phenomena. In order to fill 
in this gap, ANNIS was designed primarily with 
the focus on visualization and querying of multi-
layer annotations. In particular, ANNIS allows to 
integrate annotations originating from different 
tools (e.g., syntax annotation created with 
Synpathy, coreference annotation created with 
MMAX2, and flat, time-aligned annotations 
created with ELAN) that nevertheless refer to the 
same primary data. In this respect, ANNIS, 
together with the data format PAULA and the 
libraries created for the work with both, is best 
compared to general annotation frameworks such 
as ATLAS, NITE and LAF.  
Taking the NITE XML Toolkit as a 
representative example for this kind of 
frameworks, it provides an abstract data model, 
XML-based formats for data storage and 
metadata, a query language, and a library with 
JAVA routines for data storage and manipulation, 
querying and visualization. Additionally, a set of 
command line tools and simple interfaces for 
corpus querying and browsing are provided, 
which illustrates how the libraries can be used to 
create one's own, project-specific corpus 
interfaces and tools.  
Similarly to ANNIS, NXT supports time-
aligned, hierarchical and pointer-based 
annotation, conflicting hierarchies and the 
embedding of multi-modal primary data. The 
data storage format is based on the bundling of 
multiple XML files similar to the standoff 
concept employed in LAF and PAULA.  
One fundamental difference between NXT and 
ANNIS, however, is to be seen in the primary 
clientele it targets: The NITE XML Toolkit is 
aimed at the developer and allows to build more 
specialized displays, interfaces, and analyses as 
required by their respective end users when 
working with highly structured data annotated on 
multiple levels.  
As compared to this, ANNIS is directly 
targeted at the end user, that is, a linguist trying 
to explore and to work with a particular set of 
corpora. Therefore, an important aspect of the 
ANNIS implementation is the integration with a 
data base and convenient means for visualization 
and querying.  
6 Conclusion 
In this paper, we described the Africanist projects 
of the CRC ?Information Structure? at the 
University of Potsdam and the Humboldt 
University of Berlin/Germany, together with 
their data collections from currently 25 
subsaharan languages. Also, we have presented 
the linguistic database ANNIS that can be used to 
publish, access, query and visualize these data 
collections. As one specific example of our work, 
we have described the design and ongoing 
construction of a corpus of written Hausa, the 
Hausa internet corpus, sketched the relevant NLP 
techniques for (semi)automatic morphosyntactic 
annotation, and the application of the ANNIS 
Query Language to filter out ex-situ constituents 
and their contexts, which are relevant with regard 
to our goal, a better understanding of focus and 
information structure in Hausa and other African 
languages. 
References  
T. Brants, O. Plaehn. 2000. Interactive Corpus 
Annotation. In: Proc. of LREC 2000, Athens, 
Greece. 
A. Busemann, K. Busemann. 2008. Toolbox 
Self-Training. Technical Report (Version 1.5.4 
Oct 2008)  http://www.sil.org/ 
J. Carletta, S. Evert, U. Heid, J. Kilgour, J. 
Robertson, H. Voormann. 2003. The NITE 
XML Toolkit: Flexible Annotation for Multi-
modal Language Data. Behavior Research 
Methods, Instruments, and Computers 35(3), 
353-363. 
J. Carletta, S. Evert, U. Heid, J. Kilgour. 2005. 
The NITE XML Toolkit: data model and 
query. Language Resources and Evaluation 
Journal, 313-334. 
23
S. Dipper. 2005. XML-based Stand-off 
Representation and Exploitation of Multi-
Level Linguistic Annotation. In: Proc. of 
Berliner XML Tage 2005 (BXML 2005), 
Berlin, Germany, 39-50. 
S. Dipper, M. G?tze. 2005. Accessing 
Heterogeneous Linguistic Data - Generic 
XML-based Representation and Flexible 
Visualization. In Proc. of the 2nd Language & 
Technology Conference: Human Language 
Technologies as a Challenge for Computer 
Science and Linguistics. Poznan, Poland, 206-
210. 
S. Dipper, M. G?tze, S. Skopeteas (eds.). 2007. 
Information structure in cross-linguistic 
corpora: Annotation guidelines for phonology, 
morphology, syntax, semantics and 
information structure. Interdisciplinary 
Studies on Information Structure 7,  147-187. 
Potsdam: University of Potsdam. 
I. Fiedler. forthcoming. Contrastive topic 
marking in Gbe. In Proc. of the 18th 
International Conference on Linguistics, 
Seoul, Korea, 21. - 26. July 2008. 
I. Fiedler, K. Hartmann, B. Reineke, A. Schwarz, 
M. Zimmermann.. forthcoming. Subject Focus 
in West African Languages. In M. 
Zimmermann, C. F?ry (eds.), Information 
Structure. Theoretical, Typological, and 
Experimental Perspectives. Oxford: Oxford 
University Press. . 
M. Green, P. Jaggar. 2003. Ex-situ and in-situ 
focus in Hausa: syntax, semantics and 
discourse. In J. Lecarme et al (eds.), Research 
in Afroasiatic Grammar 2 (Current Issues in 
Linguistic Theory). Amsterdam: John 
Benjamins. 187-213. 
K. Hartmann, M. Zimmermann. 2004. Nominal 
and Verbal Focus in the Chadic Languages. In 
F. Perrill et al (eds.), Proc. of the Chicago 
Linguistics Society. 87-101. 
K. Hartmann, M. Zimmermann. 2007. In Place - 
Out of Place? Focus in Hausa. In K. Schwabe, 
S. Winkler (eds.), On Information Structure, 
Meaning and Form: Generalizing Across 
Languages. Benjamins, Amsterdam: 365-403. 
B. Hellwig, D. Van Uytvanck, M. Hulsbosch. 
2008. ELAN ? Linguistic Annotator. Technical 
Report (as of 2008-07-31). http://www.lat-
mpi.eu/tools/elan/ 
?. Kiss.  1998. Identificational Focus Versus 
Information Focus. Language 74: 245-273. 
M. Krifka. 1992. A compositional semantics for 
multiple focus constructions, in Jacobs, J: 
Informationsstruktur und Grammatik, 
Opladen, Westdeutscher Verlag, 17-53. 
C. M?ller, M. Strube. 2006. Multi-Level 
Annotation of Linguistic Data with MMAX2. 
In: S. Braun et al (eds.), Corpus Technology 
and Language Pedagogy. New Resources, 
New Tools, New Methods. Frankfurt: Peter 
Lang, 197?214. 
M. O?Donnell. 2000. RSTTool 2.4 ? A Markup 
Tool for Rhetorical Structure Theory. In: Proc. 
of the International Natural Language 
Generation Conference (INLG'2000), 13-16 
June 2000, Mitzpe Ramon, Israel, 253?256. 
C. Orasan. 2003. Palinka: A Highly 
Customisable Tool for Discourse Annotation. 
In: Proc. of the 4th SIGdial Workshop on 
Discourse and Dialogue, Sapporo, Japan. 
R. Randell, A. Bature, R. Schuh. 1998. Hausar 
Baka.   http://www.humnet.ucla.edu/humnet/ 
aflang/hausarbaka/ 
T. Schmidt. 2004. Transcribing and Annotating 
Spoken Language with Exmaralda. In: Proc. 
of the LREC-Workshop on XML Based Richly 
Annotated Corpora, Lisbon 2004. Paris: 
ELRA. 
A. Schwarz. Verb and Predication Focus Markers 
in Gur. forthcoming. In I. Fiedler, A. Schwarz 
(eds.), Information structure in African 
languages (Typological Studies in Language),  
307-333. Amsterdam, Philadelphia: John 
Benjamins. 
A. Schwarz, I. Fiedler. 2007. Narrative focus 
strategies in Gur and Kwa. In E. Aboh et al 
(eds.): Focus strategies in Niger-Congo and 
Afroasiatic ? On the interaction of focus and 
grammar in some African languages, 267-286. 
Berlin: Mouton de Gruyter. 
S. Skopeteas, I. Fiedler, S. Hellmuth, A. 
Schwarz, R. Stoel, G. Fanselow, C. F?ry, M. 
Krifka. 2006. Questionnaire on information 
structure (QUIS). Interdisciplinary Studies on 
Information Structure 4.  Potsdam: University 
of Potsdam. 
I. H. Witten, E. Frank, Data mining: Practical 
machine learning tools and techniques, 2nd 
edn, Morgan Kaufman, San Francisco, 2005. 
M. Zimmermann. 2008. Contrastive Focus and 
Emphasis. In Acta Linguistica Hungarica 55: 
347-360.  
24
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 35?43,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
By all these lovely tokens...?
Merging Conflicting Tokenizations
Christian Chiarcos, Julia Ritz and Manfred Stede
Sonderforschungsbereich 632 ?Information Structure?
University of Potsdam
Karl-Liebknecht-Str. 24-25, 14476 Golm, Germany
{chiarcos|julia|stede}@ling.uni-potsdam.de
Abstract
Given the contemporary trend to modular
NLP architectures and multiple annotation
frameworks, the existence of concurrent
tokenizations of the same text represents
a pervasive problem in everyday?s NLP
practice and poses a non-trivial theoretical
problem to the integration of linguistic an-
notations and their interpretability in gen-
eral. This paper describes a solution for
integrating different tokenizations using a
standoff XML format, and discusses the
consequences for the handling of queries
on annotated corpora.
1 Motivation
1.1 Tokens: Functions and goals
For most NLP tasks and linguistic annotations,
especially those concerned with syntax (part-of-
speech tagging, chunking, parsing) and the inter-
pretation of syntactic structures (esp., the extrac-
tion of semantic information), tokens represent
the minimal unit of analysis: words (lexemes,
semantic units, partly morphemes) on the one
hand and certain punctuation symbols on the other
hand. From a corpus-linguistic perspective, tokens
also represent the minimal unit of investigation,
the minimal character sequence that can be ad-
dressed in a corpus query (e.g. using search tools
like TIGERSearch (Ko?nig and Lezius, 2000) or
CWB (Christ, 1994)). Tokens also constitute the
basis for ?word? distance measurements. In many
annotation tools and their corresponding formats,
the order of tokens provides a timeline for the
sequential order of structural elements (MMAX
(Mu?ller and Strube, 2006), GENAU (Rehm et al,
2009), GrAF (Ide and Suderman, 2007), TIGER
XML (Ko?nig and Lezius, 2000)). In several multi-
?Taken from the poem September by Helen Hunt Jackson.
layer formats, tokens also define the absolute po-
sition of annotation elements, and only by refer-
ence to a common token layer, annotations from
different layers can be related with each other
(NITE (Carletta et al, 2003), GENAU).
Thus, by their function, tokens have the fol-
lowing characteristics: (i) tokens are totally or-
dered, (ii) tokens cover the full (annotated portion
of the) primary data, (iii) tokens are the smallest
unit of annotation, and (iv) there is only one sin-
gle privileged token layer. The last aspect is es-
pecially relevant for the study of richly annotated
data, as an integration and serialization of anno-
tations produced by different tools can be estab-
lished only by reference to the token layer. From
a corpus-linguistic perspective, i.e., when focus-
ing on querying of annotated corpora, tokens need
to be well-defined and all information annotated
to a particular text is to be preserved without any
corruption. We argue that for this purpose, char-
acteristic (iii) is to be abandoned, and we will de-
scribe the data format and an algorithm for merg-
ing different tokenizations and their respective an-
notations.
Our goal is a fully automated merging of anno-
tations that refer to different tokenizations (hence-
forth T ? and T ?) of the same text. We regard the
following criteria as crucial for this task:
Information preservation. All annotations ap-
plied to the original tokenizations should be pre-
served.
Theoretically well-defined notion of token. It
should be possible to give a plausible list of posi-
tive criteria that define character sequences as to-
kens. Knowledge about the token definition is es-
sential for formulating queries for words, e.g. in a
corpus search interface.
Integrative representation. All annotations that
are consistent with the merged tokenization should
refer to the merged tokenization. This is necessary
in order to query across multiple annotations orig-
35
inating from different annotation layers or tools.
Unsupervised merging. The integration of con-
flicting tokenizations should not require manual
interference.
1.2 Tokenization
Tokenization is the process of mapping sequences
of characters to sequences of words (cf. Guo
1997). However, different research questions or
applications induce different conceptions of the
term ?word?. For a shallow morphosyntactic anal-
ysis (part of speech tagging), a ?simple? tokeniza-
tion using whitespaces and punctation symbols as
delimiters seems acceptable for the examples in
(1). A full syntactic analysis (parsing), however,
could profit from the aggregation of complex nom-
inals into one token each.
(1) a. department store
b. Herzog-von der Heide1
c. Red Cross/Red Crescent movement
Similarly, examples (2a) and (2b) can be ar-
gued to be treated as one token for (mor-
pho)syntactic analyses, respectively. Despite in-
tervening whitespaces and punctuation symbols,
they are complex instances of the ?classical? part-
of-speech adjective. For certain semantic analyses
such as in information extraction, however, it may
be useful to split these compounds in order to ac-
cess the inherent complements (E 605, No. 22).
(2) a. E 605-intoxicated
b. No. 22-rated
Finally, (3) illustrates a morphology-based tok-
enization strategy: the principle of splitting at
morpheme boundaries (Marcus et al, 1993, PTB)
(token boundaries represented by square brack-
ets). Morphological tokenization may help distri-
butional (co-occurrence-based) semantics and/or
parsing; however, the resulting tokens might be
argued as being less intuitive to users of a corpus
search tool.
(3) a. [Mitchell][?s], [they][?ve], [do][n?t]
b. [wo][n?t], [ca][n?t], [ai][n?t]
These examples show that different applications
(tagging, parsing, information extraction) and the
focus on different levels of description (morphol-
ogy, syntax, semantics) require specialized tok-
enization strategies. When working with multiple
1Double surname consisting of Herzog and von der Heide.
tools for standard NLP tasks, thus, it is the norm
rather than the exception that they disagree in their
tokenization, as shown in ex. (4).
(4) doesn?t
a. [does][n?t] (Marcus et al, 1993, PTB)
b. [doesn][?][t] (Brants, 2000, TnT)
When creating a corpus that is annotated at multi-
ple levels and/or using several tools, different tok-
enizations are not always avoidable, as some tools
(automatic NLP tools, but also tools for manual
annotation) have integrated tokenizers. Another
challenge is the representation of token bound-
aries. Commonly, token boundaries are repre-
sented by a line break (?\n?) or the whitespace
?character? (? ?) ? in which case token-internal
whitespaces are replaced, usually by an under-
score (? ?) ?, thereby corrupting the original data.
This practice makes reconciling/merging the data
a difficult enterprise.
Given this background, we suggest an XML-
based annotation of token boundaries, such that
token boundaries are marked without affecting the
original primary data. In a straightforward XML
model, tokens are represented by XML elements
enclosing primary text slices (c.f. the BNC encod-
ing scheme (Burnard, 2007)). However, treating
tokens as spans of text by means of the XML hier-
archy is impossible for tokenization conflicts as in
(4.a) and (4.b).
2 Conflicting tokenizations:
Straightforward strategies
By ?straightforward strategies?, we mean ap-
proaches that aim to preserve the definition of to-
kens as atomic, minimal, unambiguous units of
annotation when unifying different tokenizations
(henceforth T ? and T ?) of the same text. By ?un-
supervised straightforward strategies?, we mean
tokenization strategies that operate on the primary
data only, without consulting external resources
such as dictionaries or human expertise.
Unsupervised straightforward strategies to the
task include:
1. no merging In a conservative approach, we
could create independent annotation projects for
every tokenization produced, and thus represent
all tokenizations independently. This, however,
rules out any integration or combined evaluation
of annotations to T ? and annotations to T ?.
36
2. normalization Adopt one of the source tok-
enizations, say T ?, as the ?standard? tokenization.
Preserve only the information annotated to T ? that
is consistent with T ?. Where tokenization T ? de-
viates from T ?, all annotations to T ? are lost.2
3. maximal tokens For every token boundary
in T ? that is also found in T ?, establish a token
boundary in the merged tokenization (cf. Guo?s
1997 ?critical tokenization?). However, with to-
kens assumed to be the minimal elements of anno-
tation, we lose linguistic analyses of fine-grained
tokens. With respect to (4.a) and (4.b), the max-
imal token would be the whole phrase doesn?t.
Again, this results in a loss of information, as all
annotations applied to does, doesn, n?t, ? and t re-
fer to units that are smaller than the resulting to-
ken.
4. maximal common substrings For every
token boundary in T ? or T ?, establish a token
boundary, thereby producing minimal tokens:
one token for every maximal substring shared
between T ? and T ? (cf. Guo?s 1997 ?shortest
tokenization?). By defining the original tokens
(?supertokens?) as annotations spanning over
tokens, all annotations are preserved. However,
the concept of ?token? loses its theoretical motiva-
tion; there is no guarantee that maximal common
substrings are meaningful elements in any sense:
The maximum common substring tokenization
of 4.a and 4.b is [does][n][?][t], but [n] is not
a well-defined token. It is neither defined with
respect to morphology (like PTB tokens) nor is
it motivated from orthography (like TnT tokens),
but it is just the remainder of their intersection.
As shown in Table 1, none of the strategies
sketched above fulfills all criteria identified in Sec-
tion 1.1: Avoiding a merging process counteracts
data integration; token normalization and maximal
tokens violate information preservation, and maxi-
mal common substrings violate the requirement to
specify a theoretically well-defined notion of to-
ken.
As an alternative, we propose a formalism for
the lossless integration and representation of con-
2Alternatively, transformation rules to map annotations
from T ? to T ? would have to be developed. This does, how-
ever, not guarantee information preservation, and, addition-
ally, it requires manual work, as such transformations are
annotation-specific. Thus, it is not an option for the fully
automated merging of tokenizations.
Table 1: Deficits of ?straightforward? merging ap-
proaches
no normalize max. max. common
merge tokens substrings
information preservation
+ ? ? +
well-defined tokens
+ + (?) ?
integrative
? + + +
unsupervised
(+) + + +
flicting tokenizations by abandoning the assump-
tion that tokens are an atomic, primitive con-
cept that represents the minimal unit of annota-
tion. Rather, we introduce annotation elements
smaller than the actual token ? so-called termi-
nals or terms for short ? that are defined accord-
ing to the maximum common substrings strategy
described above.
Then, tokens are defined as nodes that span
over a certain range of terms similar to phrase
nodes that dominate other nodes in syntax annota-
tions. The representation of conflicting tokeniza-
tions, then, requires a format that is capable to
express conflicting hierarchies. For this purpose,
we describe an extension of the PAULA format, a
generic format for text-oriented linguistic annota-
tions based on standoff XML.
3 Conflicting tokenizations in the
PAULA format
3.1 Annotation structures in PAULA 1.0
The PAULA format (Dipper, 2005; Dipper and
Go?tze, 2005) is a generic XML format, used as a
pivot format in NLP pipelines (Stede et al, 2006)
and in the web-based corpus interface ANNIS
(Chiarcos et al, 2008). It uses standoff XML rep-
resentations, and is conceptually closely related to
the formats NITE XML (Carletta et al, 2003) and
GraF (Ide and Suderman, 2007).
PAULA was specifically designed to support the
lossless representation of different types of text-
oriented annotations (layer-based/timeline anno-
tations, hierarchical annotations, pointing rela-
tions), optimized for the annotation of multiple
layers, including conflicting hierarchies and sim-
ple addition/deletion routines for annotation lay-
ers. Therefore, primary data is stored in a separate
37
Table 2: PAULA 1.0 data types
nodes (structural units of annotation)
token character spans in the primary data that form the basis
for higher-level annotation
markable (spans of) token(s) that can be annotated with lin-
guistic information. Markables represent flat, layer-based
annotations defined with respect to the sequence of tokens
as a general timeline.
struct hierarchical structures (DAGs or trees) are formed by
establishing a dominance relation between a struct (e.g.,
a phrase) node as parent, and tokens, markables, or other
struct nodes as children.
edges (relational units of annotation, connecting tokens,
markables, structs)
dominance relation directed edge between a struct
and its children
pointing relations directed edge between nodes in
general (tokens, markables, structs)
labels (annotations: node or edge labels)
features represent annotations attached to a particular
(structural or relational) unit of annotation
file. Multiple annotations are also stored in sepa-
rate files to avoid interference between concurrent
annotations. Annotations refer to the primary data
or to other annotations by means of XLinks and
XPointers.
As types of linguistic annotation, we distinguish
nodes (token, markable, struct), edges (dominance
and pointing relations) and labels (annotations), as
summarized in Table 2. Each type of annotation
is stored in a separate file, so that competing or
ambiguous annotations can be represented in an
encapsulated way.
PAULA 1.0 is already sufficiently expressive for
capturing the data-heterogeneity sketched above,
including the representation of overlapping seg-
ments, intersecting hierarchies, and alternative an-
notations (e.g., for ambiguous annotations), but
only for annotations above the token level. Fur-
ther, PAULA 1.0 relies on the existence of a
unique layer of non-overlapping, atomic tokens as
minimal units of annotation: For all nodes, their
position and sequential order is defined with re-
spect to the absolute position of tokens that they
cover; and for the special case of markables, these
are defined solely in terms of their token range.
Finally, PAULA 1.0 tokens are totally ordered,
they cover the (annotated) primary data com-
pletely, and they are non-overlapping. Only on
this basis, the extension and (token-)distance of
annotated elements can be addressed; and only
by means of unambiguous reference, information
from different layers of annotation can be com-
bined and evaluated.
3.2 Introducing terminal nodes
In our extension of the PAULA format, we in-
troduce the new concept of term nodes: atomic
terminals that directly point to spans of primary
data. Terms are subject to the same constraints as
tokens in PAULA 1.0 (total order, full coverage,
non-overlapping). So, terms can be used in place
of PAULA 1.0 tokens to define the extension and
position of super-token level and sub-token level
annotation elements.
Markables are then defined with respect to
(spans of) terminal nodes rather than tokens, such
that alternative tokenizations can be expressed as
markables in different layers that differ in their ex-
tensions.
Although terms adopt several functions for-
merly associated with tokens, a privileged token
layer is still required: In many query languages,
including ANNIS-QL (Chiarcos et al, 2008), to-
kens define the application domain of regular ex-
pressions on the primary data. More impor-
tantly, tokens constitute the basis for conventional
(?word?) distance measurements and (?word?)
coverage queries. Consequently, the constraints
on tokens (total order, full coverage and absence
of overlap) remain.
The resulting specifications for structural units
of annotation are summarized in Table 3. Distin-
guishing terminal elements and re-defining the to-
ken layer as a privileged layer of markables al-
lows us to disentangle the technical concept of
?atomic element? and ?token? as the convention-
ally assumed minimal unit of linguistic analysis.
3.3 A merging algorithm
In order to integrate annotations on tokens, it is
not enough to represent two tokenizations side by
side with reference to the same layer of terminal
nodes. Instead, a privileged token layer is to be es-
tablished and it has to be ensured that annotations
can be queried with reference to the token layer.
38
Table 3: PAULA extensions: revised node types
terms specify character spans in the primary data
that form the basis for higher-level annota-
tion
markable defined as above, with terms taking the
place of tokens
structs defined as above, with terms taking the
place of tokens
tokens sub-class of structs that are non-
overlapping, arranged in a total order,
and cover the full primary data
Then, all annotations whose segmentation is con-
sistent with the privileged token layer are directly
linked with tokens.
Alg. 3.1 describes our merging algorithm, and
its application to the four main cases of conflict-
ing tokenization is illustrated in Figure 1.3 The
following section describes its main characteris-
tics and the consequences for querying.
4 Discussion
Alg. 3.1 produces a PAULA project with one sin-
gle tokenization. So, it is possible to define queries
spanning across annotations with originally differ-
ent tokenization:
Extension and precedence queries are
tokenization-independent: Markables refer to
the term layer, not the tok layer, structs also
(indirectly) dominate term nodes.
Dominance queries for struct nodes and tokens
yield results whenever the struct node dominates
only nodes with tok-compatible source tokeniza-
tion: Structs dominate tok nodes wherever the
original tokenization was consistent with the
privileged tokenization tok (case A and C in Fig.
1).
Distance queries are defined with respect to the
tok layer, and are applicable to all elements that
are are defined with reference to the tok layer (in
figure 1: tok?a, tok?a, tok?b, tok?b in case A; tokab
in case B; toka, tokb, tokab in case C; tokab, tokc
in case D). They are not applicable to elements
that do not refer to the tok layer (B: toka, tokb; D:
toka, tokbc).
3Notation: prim ? primary data / tok, term ? annota-
tion layers / t ? L ? t is a node on a layer L / a..b ? con-
tinuous span from tok/term a to tok/term b / a, b ? list of
tok/term/markable nodes a, b / t = [a] ? t is a node (struct,
markable, tok) that points to a node, span or list a
The algorithm is unsupervised, and the token
concept of the output tokenization is well-defined
and consistent (if one of the input tokenizations
is adopted as target tokenization). Also, as shown
below, it is integrative (enabling queries across dif-
ferent tokenizations) and information-preserving
(reversible).
4.1 Time complexity
After a PAULA project has been created, the time
complexity of the algorithm is quadratic with re-
spect to the number of characters in the primary
data n. This is due to the total order of tokens:
Step 2 and 3.a are applied once to all original to-
kens from left to right. Step 5 can be reformulated
such that for every terminal node, the relationship
between the directly dominating tok? and tok? is
checked. Then, Step 5 is also in O(n). In terms of
the number of markables m, the time complexity
in Step 3.b is in O(n m): for every markable, the
corresponding term element is to be found, tak-
ing at most n repositioning operations on the term
layer. Assuming that markables within one layer
are non-overlapping4 and that the number of lay-
ers is bound by some constant c5, then m ? n c,
so that 3.b is in O(n? c).
For realistic scenarios, the algorithm is thus
quadratic.
4.2 Reversibility
The merging algorithm is reversible ? and, thus,
lossless ? as shown by the splitting algorithm in
Alg. 3.2. For reasons of space, the correctness
of this algorithm cannot be demonstrated here, but
broadly speaking, it just removes every node that
corresponds to an original token of the ?other? tok-
enization, plus every node that points to it, so that
only annotations remain that are directly applied
to the target tokenization.
4.3 Querying merged tokenizations
We focus in this paper on the merging of analy-
ses with different tokenizations for the purpose of
users querying a corpus across multiple annota-
4Although PAULA supports overlapping markables
within one single layer, even with identical extension, this is
a reasonable assumption: In practice, overlapping markables
within one single layer are rare. More often, there is even a
longer sequence of primary data between one markable of a
particular layer and the next. In our experience, such ?gaps?
occur much more often than overlapping markables.
5Again, this is a practical simplication. Theoretically, the
number of layers is infinite.
39
Alg. 3.1 Merging different tokenizations
0. assume that we have two annotations analysis? and analysis? for the same primary data, but with different tokenizations
1. create PAULA 1.0 annotation projects for analysis? and analysis? with primary data files prim? and prim? and token
layers tok? and tok? respectively.
2. harmonize primary data
if prim? equals prim?, then
(i) rename prim? to prim
(ii) set al references in analysis? from prim? to prim
(iii) create a new annotation project analysis by copying prim and all annotation layers from analysis? and analysis?
otherwise terminate with error msg
3. harmonize terminal nodes
create a new annotation layer term, then
(a) for all overlapping tokens t? ? tok? and t? ? tok?: identify the maximal common substrings of t? and t?
for every substring s, create a new element terms pointing to the corresponding character span in the primary data
for every substring s, redefine t? and t? as markables referring to terms
(b) redefine markable spans as spans of terminal nodes
for every token t = [terms? ..terms? ] ? tok? ? tok? and every markable m = [w..xty..z]: set m =
[w..xterms? ..terms?y..z]
4. select token layer
rename tok? to tok, or rename tok? to tok, (cf. the normalization strategy in Sect. 2) or
rename term to tok (cf. the minimal tokens strategy in Sect. 2)
5. token integration
for every original token ot = [a..b] ? (tok? ? tok?) \ tok:
if there is a token t ? tok such that t = [a..b], then define ot as a struct with ot = [t], else
if there are tokens t?, .., tn ? tok such that t?..tn form a continuous sequence of tokens and t? = [a..x] and tn = [y..b],
then define ot as a struct such that ot = [t?, .., tn],
otherwise: change nothing
Figure 1: Merging divergent tokenizations
40
Alg. 3.2 Splitting a PAULA annotation project
with two different tokenizations
0. given a PAULA annotation project analysis with token
layer tok, terminal layer term, and two layers l? and l?
(that may be identical to term or tok) that convey the
information of the original token layers tok? and tok?
1. create analysis? and analysis? as copies of analysis
2. if l? represents a totally ordered, non-overlapping list of
nodes that cover the primary data completely, then modify
analysis?:
a. for every node in l?: substitute references to tok? by
references to term?
b. remove l? from analysis?
c. if l? 6= tok?, remove tok? from analysis?
d. for every annotation element (node/relation) e in
analysis? that directly or indirectly points to another
node in analysis? that is no longer present, remove e
from analysis?
e. remove every annotation layer from analysis? that
does not contain an annotation element
f. for every markable in l?: remove references to term?,
define the extension of l? nodes directly in terms of
spans of text in prim?
g. if l? 6= term?, remove term?
3. perform step 2. for l? and analysis?
tion layers. Although the merging algorithm pro-
duces annotation projects that allow for queries in-
tegrating annotations from analyses with different
tokenization, the structure of the annotations is al-
tered, such that the behaviour of merged and un-
merged PAULA projects may be different. Obvi-
ously, token-level queries must refer to the priv-
ileged tokenization T ?. Operators querying for
the relative precedence or extension of markables
are not affected: in the merged annotation project,
markables are defined with reference to the layer
term: originally co-extensional elements E? and
E? (i.e. elements covering the same tokens in the
source tokenization) will also cover the same ter-
minals in the merged project. Distance operators
(e.g. querying for two tokens with distance 2, i.e.
with two tokens in between), however, will oper-
ate on the new privileged tokenization, such that
results from queries on analysis may differ from
those on analysis?. Dominance operators are
also affected, as nodes that directly dominated a
token in analysis? or analysis? now indirectly
dominate it in analysis, with a supertoken as an
intermediate node.
Alg. 3.3 Iterative merging: modifications of Alg.
3.1, step.3
if analysis? has a layer of terminal nodes term?, then let
T ? = term?, otherwise T ? = tok?
if analysis? has a layer of terminal nodes term?, then let
T ? = term?, otherwise T ? = tok?
create a new annotation layer term, then
1. for all overlapping terminals/tokens t? ? T ? and t? ?
T ?: identify the maximal common substrings of t? and
t?
for every substring s, create a new element terms
pointing to the corresponding character span in the pri-
mary data
for every substring s, redefine t? and t? as markables
referring to terms
2. redefine markable spans as spans of terminal nodes
for every node t = [terms? ..terms? ] ? T ? ? T ?
and every markable m = [w..xty..z]: set
m = [w..xterms? ..terms?y..z]
3. for all original terminals t ? T ??T ?: if t is not directly
pointed at, remove t from analysis
Accordingly, queries applicable to PAULA
projects before the merging are not directly appli-
cable to merged PAULA projects. Users are to be
instructed to keep this in mind and to be aware of
the specifications for the merged tokenization and
its derivation.6
5 Extensions
5.1 Merging more than two tokenizations
In the current formulation, Alg. 3.1 is applied to
two PAULA 1.0 projects and generates extended
PAULA annotation projects with a term layer.
The algorithm, however, may be applied itera-
tively, if step 3 is slightly revised, such that ex-
tended PAULA annotation projects can also be
merged, see Alg. 3.3.
5.2 Annotation integration
The merging algorithm creates a struct node for
every original token. Although this guarantees re-
versibility, one may consider to remove such re-
dundant structs. Alg. 3.4 proposes an optional
postprocessing step for the merging algorithm.
This step is optional because these operations are
6The information, however, is preserved in the format and
may be addressed by means of queries that, for example, op-
erate on the extension of terminals.
41
Alg. 3.4 Annotation integration: Optional post-
processing for merging algorithm
6.a. remove single-token supertoken
for every original token ot = [t] ? tok? ? tok? with
t ? tok: replace all references in analysis to ot by
references to t, remove ot
6.b. merging original token layers tok? and tok? (if
tok? 6= tok and tok? 6= tok)
define new ?super token? layer stok.
for every ot ? tok? ? tok?:
if ot = [t] for some t ? tok, then see 6.a
if ot = [t?, .., tn] for some t?, .., tn ? tok, and
there is ot? = [t?, .., tn] ? tok? ? tok? ? stok,
then replace all references in analysis to ot? by
references to ot, move ot to layer stok, remove
ot? from analysis
move all remaining ot ? tok? ? tok? to stok, remove
layers tok? and tok?
6.c. unify higher-level annotations
for every markable mark? = [term?..termn] and
term?, .., termn ? term:
if there is a markable mark? in analysis such
that mark? = [term?..termn], then replace all
references in analysis to mark? by references to
mark?, remove mark?
for every struct struct? = [c?, .., cn] that covers ex-
actly the same children as another struct struct? =
[c?, .., cn], replace all references to struct? by refer-
ences to struct?, remove struct?
destructive: We lose the information about the ori-
gin (analysis? vs. analysis?) of stok elements
and their annotations.
6 Summary and Related Reasearch
In this paper, we describe a novel approach for the
integration of conflicting tokenizations, based on
the differentiation between a privileged layer of
tokens and a layer of atomic terminals in a stand-
off XML format: Tokens are defined as structured
units that dominate one or more terminal nodes.
Terminals are atomic units only within the re-
spective annotation project (there is no unit ad-
dressed that is smaller than a terminal). By iter-
ative applications of the merging algorithm, how-
ever, complex terms may be split up in smaller
units, so that they are not atomic in an absolute
sense.
Alternatively, terms could be identified a priori
with the minimal addressable unit available, i.e.,
characters (as in the formalization of tokens as
charspans and charseqs in the ACE information
extraction annotations, Henderson 2000). It is not
clear, however, how a character-based term defini-
tion would deal with sub-character and zero exten-
sion terms: A character-based definition of terms
that represent traces is possible only by corrupt-
ing the primary data.7 Consequently, a character-
based term definition is insufficient unless we re-
strict ourselves to a particular class of languages,
texts and phenomena.
The role of terminals can thus be compared to
timestamps: With reference to a numerical time-
line, it is always possible to define a new event
between two existing timestamps. Formats specif-
ically designed for time-aligned annotations, e.g.,
EXMARaLDA (Schmidt, 2004), however, typi-
cally lack a privileged token layer and a formal
concept of tokens. Instead, tokens, as well as
longer or shorter sequences, are represented as
markables, defined by their extension on the time-
line.
Similarly, GrAF (Ide and Suderman, 2007), al-
though being historically related to PAULA, does
not have a formal concept of a privileged token
layer in the sense of PAULA.8 We do, however,
assume that terminal nodes in GrAF can be com-
pared to PAULA 1.0 tokens.
For conflicting tokenizations, Ide and Suderman
(2007) suggest that ?dummy? elements are defined
covering all necessary tokenizations for controver-
sially tokenized stretches of primary data. Such
dummy elements combine the possible tokeniza-
tions for strategies 1 (no merging) and 3 (maxi-
mal tokens), so that the information preservation
deficit of strategy 3 is compensated by strategy 1,
and the integrativity deficit of strategy 1 is com-
pensated by strategy 3 (cf. Table 1). However, to-
kens, if defined in this way, are overlapping and
thus only partially ordered, so that distance opera-
tors are no longer applicable.9
7Similarly, phonological units that are not expressed in
the primary data can be subject to annotations, e.g., short e
and o in various Arabic-based orthographies, e.g., the Ajami
orthography of Hausa. A term with zero extension at the po-
sition of a short vowel can be annotated as having the phono-
logical value e or o without having character status.
8https://www.americannationalcorpus.
org/graf-wiki/wiki/WikiStart#GraphModel,
2009/05/08
9This can be compensated by marking the base segmen-
tation differently from alternative segmentations. In the ab-
stract GrAF model, however, this can be represented only by
means of labels, i.e., annotations. A more consistent con-
42
Another problem that arises from the introduc-
tion of dummy nodes is their theoretical status, as
it is not clear how dummy nodes can be distin-
guished from annotation structured on a concep-
tual level. In the PAULA formalization, dummy
nodes are not necessary, so that this ambiguity is
already resolved in the representation.
References
Thorsten Brants. 2000. TnT A Statistical Part-of-
Speech Tagger. In Proceedings of the Sixth Con-
ference on Applied Natural Language Processing
ANLP-2000. Seattle, WA.
Lou Burnard (ed.). 2007. Reference Guide
for the British National Corpus (XML Edi-
tion). http://www.natcorp.ox.ac.uk/
XMLedition/URG/bnctags.html.
Jean Carletta, Stefan Evert, Ulrich Heid, Jonathan
Kilgour, Judy Robertson, and Holger Voormann.
2003. The NITE XML Toolkit: Flexible Annotation
for Multi-modal Language Data. Behavior Research
Methods, Instruments, and Computers 35(3), 353-
363.
Christian Chiarcos, Stefanie Dipper, Michael Go?tze,
Ulf Leser, Anke Lu?deling, Julia Ritz, and Manfred
Stede. 2009. A Flexible Framework for Integrating
Annotations from Different Tools and Tagsets TAL
(Traitement automatique des langues) 49(2).
Oli Christ. 1994. A modular and flexible architec-
ture for an integrated corpus query system. COM-
PLEX?94, Budapest, Hungary.
Stefanie Dipper. 2005. XML-based Stand-off Repre-
sentation and Exploitation of Multi-Level Linguistic
Annotation. In Rainer Eckstein and Robert Tolks-
dorf (eds:): Proceedings of Berliner XML Tage,
pages 39-50.
Stefanie Dipper and Michael Go?tze. 2005. Accessing
Heterogeneous Linguistic Data ? Generic XML-
based Representation and Flexible Visualization. In
Proceedings of the 2nd Language & Technology
Conference 2005, Poznan, Poland, pages 23?30.
Stefanie Dipper, Michael Go?tze. 2006. ANNIS:
Complex Multilevel Annotations in a Linguistic
Database. Proceedings of the 5th Workshop on NLP
and XML (NLPXML-2006): Multi-Dimensional
Markup in Natural Language Processing. Trento,
Italy.
Jin Guo. 1997. Critical Tokenization and its Proper-
ties, Computational Linguistic, 23(4), pp.569-596.
ception would encode structural information on the structural
level, and only linguistic annotation and metadata on the con-
tents level.
John C. Henderson. 2000. A DTD for Reference Key
Annotation of EDT Entities and RDC Relations
in the ACE Evaluations (v. 5.2.0, 2000/01/05),
http://projects.ldc.upenn.edu/ace/
annotation/apf.v5.2.0.dtd (2009/06/04)
Nancy Ide and Keith Suderman. 2007. GrAF: A
Graph-based Format for Linguistic Annotations. In
Proceedings of the Linguistic Annotation Work-
shop,held in conjunction with ACL 2007, Prague,
June 28-29, 1-8.
Esther Ko?nig and Wolfgang Lezius. 2000. A descrip-
tion language for syntactically annotated corpora.
In: Proceedings of the COLING Conference, pp.
1056-1060, Saarbru?cken, Germany.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn treebank. Computa-
tional Linguistics 19, pp.313-330.
Christoph Mu?ller and Michael Strube. 2006. Multi-
Level Annotation of Linguistic Data with MMAX2.
In: S. Braun et al (eds.), Corpus Technology and
Language Pedagogy. New Resources, New Tools,
New Methods. Frankfurt: Peter Lang, 197?214.
Georg Rehm, Oliver Schonefeld, Andreas Witt, Chris-
tian Chiarcos, and Timm Lehmberg. 2009.
SPLICR: A Sustainability Platform for Linguistic
Corpora and Resources. In: Text Resources and
Lexical Knowledge. Selected Papers the 9th Confer-
ence on Natural Language Processing (KONVENS
2008), Berlin, Sept. 30 ? Oct. 2, 2008. Mouton de
Gruyter.
Helmut Schmid. 2002. Tokenizing & Tagging. In
Lu?deling, Anke and Kyto?, Merja (Hrsg.) Corpus
Linguistics. An International Handbook. (HSK Se-
ries). Mouton de Gryuter, Berlin
Thomas Schmidt. 2004. Transcribing and Annotat-
ing Spoken Language with Exmaralda. Proceedings
of the LREC-workshop on XML Based Richly Anno-
tated Corpora. Lisbon, Portugal. Paris: ELRA.
Manfred Stede, Heike Bieler, Stefanie Dipper, and
Arthit Suriyawongkul. 2006. SUMMaR: Combin-
ing Linguistics and Statistics for Text Summariza-
tion. Proceedings of the 17th European Conference
on Artificial Intelligence (ECAI-06). pp 827-828.
Riva del Garda, Italy.
Ralph Weischedel, Sameer Pradhan, Lance Ramshaw
and Linnea Micciulla. 2006. OntoNotes Release
1.0. Linguistic Data Consortium, Philadelphia.
43
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 166?171,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Creating and Exploiting a Resource of Parallel Parses
Christian Chiarcos? and Kerstin Eckart?? and Julia Ritz?
? Collaborative Research Centre 632 ?? Collaborative Research Centre 732
?Information Structure? ?Incremental Specification in Context?
Universita?t Potsdam Universita?t Stuttgart
{chiarcos|jritz}@uni-potsdam.de eckartkn@ims.uni-stuttgart.de
Abstract
This paper describes the creation of a re-
source of German sentences with multi-
ple automatically created alternative syn-
tactic analyses (parses) for the same text,
and how qualitative and quantitative inves-
tigations of this resource can be performed
using ANNIS, a tool for corpus querying
and visualization. Using the example of
PP attachment, we show how parsing can
benefit from the use of such a resource.
1 Introduction
In this paper, we describe the workflow and the
infrastructure to create and explore a corpus that
contains multiple parses of German sentences. A
corpus of alternative parses created by different
tools allows us to study structural differences be-
tween the parses in a systematic way.
The resource described in this paper is a collec-
tion of German sentences with -ung nominaliza-
tions extracted from the SDEWAC corpus (Faa?
et al, 2010), based on the DEWAC web corpus
(Baroni and Kilgarriff, 2006). These sentences
are employed for the study of lexical ambigui-
ties in German -ung nominalizations (Eberle et al,
2009); e.g., German Absperrung, derived from ab-
sperren ?to block?, can denote an event (?block-
ing?), a state (?blockade?) or an object (?barrier?).
Sortal disambiguation, however, is highly context-
dependent, and reliable and detailed analyses of
the linguistic context are crucial for a sortal dis-
ambiguation of these nominalizations.
More reliable and detailed linguistic analyses
can be achieved, for example, by combining the
information produced by different parsers: On the
basis of qualitative and quantitative analyses, gen-
eralized rules for the improvement of the respec-
tive parsers can be developed, as well as rules for
the mapping of their output to a tool-independent
representation, and weights for the parallel appli-
cation and combination of multiple parsers. This
approach has been previously applied to morpho-
logical and morphosyntactic annotations (Borin,
2000; Zavrel and Daelemans, 2000; Tufis?, 2000),
but only recently to syntax annotation (Francom
and Hulden, 2008; de la Clergerie et al, 2008).
Because of the complexity of syntax annotations
as compared to part of speech tags, however, novel
technologies have to be applied that allow us to
represent, to visualize and to query multiple syn-
tactic analyses of the same sentence.
This paper describes the workflow from raw text
to a searchable representation of the corpus. One
of the aims of this new resource is to assess po-
tential weaknesses in the parsers as well as their
characteristic strengths. For the example of am-
biguities in PP attachment, Sect. 4 shows how lin-
guistic analyses can be improved by combining in-
formation from different parsers.
2 Parsing
In order to maximize both coverage and gran-
ularity of linguistic analyses, we chose parsers
from different classes: A probabilistic constituent
parser and a rule-based parser that produces se-
mantically enriched dependency parses.
2.1 BitPar
BitPar (Schmid, 2006) is a probabilistic context
free parser using bit-vector operations (Schmid,
2004). Node categories are annotated along with
grammatical functions, part-of-speech tags and
morphological information in a parse tree. BitPar
analyses are conformant to the TIGER annotation
scheme (Brants et al, 2004), and the tool?s output
format is similar to the list-based bracketing for-
mat of the Penn Treebank (Bies et al, 1995). The
BitPar analysis of sentence (1) is visualized as the
right-most tree in Fig. 1.
166
(1) Der
the
Dax
Dax
reagiert
reacts
derzeit
presently
auf
on
die
the
Meldungen
messages
aus
from
London.
London
?Presently, the Dax [German stock index,
N.B.] is reacting to the news from London.?
2.2 B3 Tool
The second parser applied here is the B3 Tool
(Eberle et al, 2008), a rule-based parser that
provides syntactic-semantic analyses that com-
bine dependency parsing with FUDRT represen-
tations.1 The B3 Tool is developed on the basis
of a research prototype by Lingenio2 in the con-
text of a project on lexical ambiguities in German
nominalizations3.
For further processing, the output of the B3 Tool
is converted into a PTB-style bracketing format
similar to that used by BitPar. This transformation
involves the generation of a constituency graph
from the original dependency analysis: In the first
step, rules are used that insert nodes and projec-
tions as described by Eberle (2002). Then, another
transformation step is necessary: As the B3 Tool
aims for an abstract, flat semantics-oriented struc-
ture, certain aspects of the surface structure are not
represented in its output and need to be restored in
order to create analyses that can be aligned with
constituent-based representations. For example,
punctuation marks do not appear as leaves of the
syntactic tree, as their contribution is included in
the description of the head verb. Similarly, aux-
iliaries are not represented as individual words in
the B3 output, as their tense and aspect informa-
tion is integrated with the event description that
corresponds to the head verb.4 As we focus on the
integration of multiple syntactic analyses, leaves
from the B3 Tool output that represent semantic
information were not considered, e.g., information
on coreference.
The converted B3 analysis of sentence (1) is vi-
sualized as the left tree in Fig. 1.
1Flat Underspecified Discourse Representation Theory
(Eberle, 1997; Eberle, 2004)
2http://www.lingenio.de/English/
3Project B3 of the Collaborative Research Centre (Son-
derforschungsbereich) SFB 732, Stuttgart, Germany.
4For the study described here, punctuation marks were
added to the surface structure but auxiliaries not yet. There
are several possible approaches to dealing with these struc-
tural aspects (e.g. inserting empty elements, converting Bit-
Par into B3-like representations, etc.). The discussion of
these strategies is, however, beyond the scope of this tech-
nical paper.
3 Querying and Visualizing Alternative
Parses
In order to integrate multiple annotations created
by different tools, we employ a generic XML for-
mat, PAULA XML (Dipper and Go?tze, 2005).
PAULA XML is an XML linearization of the data
model underlying the ANNIS data base.5 It is
comparable to NITE XML (Carletta et al, 2005)
and GrAF (Ide, 2007). PAULA XML supports di-
verse data structures (trees, graphs, and flat spans
of tokens) and allows for conflicting hierarchies.
The integrated PAULA representation of the
multiple-parses corpus can be accessed using AN-
NIS, a web interface for querying and visualizing
richly annotated corpora. Fig. 1 shows the ANNIS
interface: top left is the query field; below that is
the ?match count? field (presenting the number of
instances matching the query). Below this field is
the list of corpora the user choses from. Matches
are visualized in the right window. Tokens and
token-level annotations are shown in a Key Word
In Context (KWIC) view (upper part of the search
result pane in Fig. 1), e.g., B3 morphology (2nd
row), BitPar parts of speech (3rd row), and BitPar
morphology (4th row). Trees are visualized with
the Tree view (below KWIC view).
4 Exploiting multiple parses
The goal of our research is to develop rules for
the combination of BitPar and B3 parses such that
the resulting merged parse provides more reliable
linguistic analyses than the ones provided by ei-
ther alone. The rule-based B3 Tool provides deep
semantic analyses. B3 parses are thus generally
richer in information than BitPar parses. Certain
ambiguities, however, are not resolved but rather
represented by underspecification. In this section,
we explore the possibility to employ BitPar parses
to resolve such underspecifications.
4.1 Studying PP attachment in ANNIS
The attachment of prepositional phrases is often
ambiguous between high attachment (e.g., PP as a
clausal adjunct) and low attachment (PP as a nom-
inal modifier). In such cases, the B3 Tool employs
underspecification, which is represented by a spe-
cial edge label xprep.6
5PAULA and ANNIS have been developed at the Col-
laborative Research Centre 632, http://www.sfb632.
uni-potsdam.de/?d1/annis/.
6The xprep label indicates underspecification as to
whether the PP has to be attached to its parent node or a node
167
Figure 1: ANNIS2 screenshot with query results
for QUERY 1
Using ANNIS, we retrieve all cases where a Bit-
Par PP corresponds to a B3 PP with the edge la-
beled xprep (the query used to accomplish this
will be referenced by QUERY 1 in the following).
Fig. 1 illustrates an example match: The B3 PP
(left tree) is attached to the root node with an edge
label xprep; in the BitPar analysis (right tree),
the prepositional phrase is correctly attached to the
other PP node.
Using an extended query, we conducted a quan-
titative analysis comparing the node labels as-
signed to the parent node of the respective PPs in
BitPar parses and B3 parses.
Considering only those matches where the B3
parent node was either VP or S (85%, 35 of 41),
high attachment is indicated by BitPar labels VP
or S for the BitPar parent node (34%, 12 of 35)
and low attachment by labels PP or NP (66%, 23
of 35). BitPar thus distinguishes low and high PP
attachment, with a preference for low attachment
in our data set.
Results of a subsequent qualitative analysis of
the first 20 matches retrieved by this query are
summarized in Tab. 1: Only 16% (3 of 19) Bit-
Par predictions are incorrect, 32% (6 of 19) are
possible (but different attachment would have pro-
duced a felicitous reading), and 53% (10 of 19) are
correct. BitPar analyses of PP attachment are thus
BitPar prediction correct possible incorrect total
low 57% 36% 7% 14
high 40% 20% 40% 5
low or high 53% 32% 16% 19?
? one match (non-sentence) excluded
Table 1: Qualitative analysis of the first 20
matches
relatively reliable, and where the B3 Tool indicates
underspecification with respect to PP attachment,
the point of attachment can be adopted from the
BitPar parse. With such a merging of BitPar parses
and B3 parses, a more detailed and more reliable
analysis is possible.
4.2 Merging B3 and BitPar parses
With the information from the comparison of Bit-
Par and B3 Tool attachments, a workflow is imag-
inable where both parsers are applied in paral-
lel, and then their output is merged into a com-
mon representation. As opposed to traditional ap-
proaches that reduce parse integration to a selec-
dominated by its parent.
168
tion between entire parses, cf. Crysmann et al
(2002), we employ a full merging between B3
parses and BitPar parses. This merging is based
on hand-crafted rules that express preferences be-
tween pieces of information from one parse or the
other in accordance with the results of quantitative
and qualitative analyses as described above.
B3 parses can be enriched with structural infor-
mation from BitPar, e.g., by the following exem-
plaric rule:7 if the B3 parse indicates underspec-
ification with respect to the PP attachment point
(QUERY 1), establish a dominance edge between
(i) the correspondent of the Bitpar PP (the PP
?from London? in the example) and (ii) the corre-
spondent of its parent node (the PP ?to the news?),
and delete the original, underspecified B3 edge.
The same procedure can also be applied to per-
form corrections of a parse, if further quantitative
and qualitative studies indicate that, for example,
the B3 parser systematically fails at a particular
phenomenon.
In some cases, we may also want to employ
context-dependent rules to exploit the advanta-
geous characteristics of a specific parser, e.g., to
preserve ambiguities. Example (2) illustrates that
PP attachment has an effect on the sortal interpre-
tation of Absperrung ?barrier/blocking/blockade?:
Different points of attachment can produce dif-
ferent possible readings. The PP by the police
specifies the subject of the nominalized verb ab-
sperren ?to block?. This indicates that here, the
event/state readings are preferred over the object
(=entity) reading.
(2) Die
the
Feuerwehr
fire brigade
unterstu?tzte
supported
die
the
Absperrung
blocking
durch
by
die
the
Polizei.
police
?The fire brigade supported the police?s
blockade/blocking.?
5 Conclusion
In this paper, we described the creation of a re-
source of German sentences with parallel parses
and the infrastructure employed to exploit this re-
source. We also identified possible fields of ap-
plication for this resource: By querying this re-
source one finds strong tendencies regarding the
relative reliability and level of detail of different
7Other formulations are possible, see Heid et al (2009)
for the enrichment of BitPar parses with lexical knowledge
from B3 parses.
parsers; on this basis, the strengths of several tools
can be weighted, as represented, e.g., by general-
ized, context-dependent rules to combine the out-
put of multiple parsers. Here, this approach was
illustrated for two parsers and their combination to
disambiguate PP attachment as part of a study of
German -ung nominalizations. A future perspec-
tive could be to add more tools to the comparison,
find out their characteristic strengths and perform
a sort of weighted voting to decide when an ana-
lysis should be enhanced by the information from
another one.
We have shown that the infrastructure provided
by the ANNIS data base and the underlying data
format PAULA can be employed to conduct this
kind of research. Although originally developed
for different purposes (representation and query-
ing of richly annotated corpora), its generic char-
acter allowed us to apply it with more than satis-
factory results to a new scenario.
Subsequent research may further exploit the po-
tential of the ANNIS/PAULA infrastructure and
the development of application-specific exten-
sions. In particular, it is possible to register in
ANNIS a problem-specific visualization for par-
allel parses that applies in place of the generic
tree/DAG view for the namespaces bitpar and
b3. Another extension pertains to the handling of
conflicting tokenizations: The algorithm described
by Chiarcos et al (2009) is sufficiently generic
to be applied to any PAULA project, but it may
be extended to account for B3-specific deletions
(Sect. 2.2). Further, ANNIS supports an annota-
tion enrichment cycle: Matches are exported as
WEKA tables, statistical, symbolic or neural clas-
sifiers can be trained on or applied to this data, and
the modified match table can be reintegrated with
the original corpus. This allows, for example, to
learn an automatic mapping between B3 and Bit-
Par annotations.
Acknowledgements
Collaborative Research Centre 732 (Universita?t
Stuttgart) and Collaborative Research Centre 632
(Humboldt Universita?t zu Berlin and Universita?t
Potsdam) are funded by Deutsche Forschungsge-
meinschaft (DFG).
169
References
Marco Baroni and Adam Kilgarriff. 2006. Large
linguistically-processed Web corpora for multiple
languages. In Proceedings of the 11th Conference of
the European Chapter of the Association for Com-
putational Linguistics, pages 87?90, Trento, Italy.
EACL.
Ann Bies, Mark Ferguson, Karen Katz, and
Robert MacIntyre. 1995. Bracketing guide-
lines for treebank ii style penn treebank
project. ftp://ftp.cis.upenn.edu/
pub/treebank/doc/manual/root.ps.gz
(May 31, 2010). version of January 1995.
Lars Borin. 2000. Something borrowed, something
blue: Rule-based combination of POS taggers. In
Proceedings of the 2nd International Conference on
Language Resources and Evaluation (LREC 2000),
Athens, Greece, May, 31st ? June, 2nd.
Sabine Brants, Stefanie Dipper, Peter Eisenberg, Sil-
via Hansen, Esther Ko?nig, Wolfgang Lezius, Chris-
tian Rohrer, George Smith, and Hans Uszkoreit.
2004. TIGER: Linguistic interpretation of a German
corpus. Research on Language and Computation,
2(4):597?620.
Jean Carletta, Stefan Evert, Ulrich Heid, and Jonathan
Kilgour. 2005. The NITE XML Toolkit: data
model and query. Language Resources and Eval-
uation Journal (LREJ), 39(4):313?334.
Christian Chiarcos, Julia Ritz, and Manfred Stede.
2009. By all these lovely tokens...: merging con-
flicting tokenizations. In Proceedings of the Third
Linguistic Annotation Workshop, pages 35?43. As-
sociation for Computational Linguistics.
Berthold Crysmann, Anette Frank, Kiefer Bernd, Ste-
fan Mueller, Guenter Neumann, Jakub Piskorski,
Ulrich Schaefer, Melanie Siegel, Hans Uszkoreit,
Feiyu Xu, Markus Becker, and Hans-Ulrich Krieger.
2002. An integrated architecture for shallow and
deep processing. In Proceedings of 40th Annual
Meeting of the Association for Computational Lin-
guistics, pages 441?448, Philadelphia, Pennsylva-
nia, USA, July.
Eric Villemonte de la Clergerie, Olivier Hamon,
Djamel Mostefa, Christelle Ayache, Patrick
Paroubek, and Anne Vilnat. 2008. PASSAGE:
from French Parser Evaluation to Large Sized
Treebank. In Proceedings of the 6th Conference on
Language Resources and Evaluation (LREC 2008),
Marrakech, Morocco, May.
Stefanie Dipper and Michael Go?tze. 2005. Accessing
Heterogeneous Linguistic Data ? Generic XML-
based Representation and Flexible Visualization. In
Proceedings of the 2nd Language & Technology
Conference 2005, pages 23?30, Poznan, Poland,
April.
Kurt Eberle, Ulrich Heid, Manuel Kountz, and Kerstin
Eckart. 2008. A tool for corpus analysis using par-
tial disambiguation and bootstrapping of the lexicon.
In Angelika Storrer, Alexander Geyken, Alexander
Siebert, and Kay-Michael Wu?rzner, editors, Text Re-
sources and Lexical Knowledge ? Selected Papers
from the 9th Conference on Natural Language Pro-
cessing (KONVENS 2008), pages 145?158, Berlin,
Germany. Mouton de Gruyter.
Kurt Eberle, Gertrud Faa?, and Ulrich Heid. 2009.
Proposition oder Temporalangabe? Disambigu-
ierung von -ung-Nominalisierungen von verba di-
cendi in nach-PPs. In Christian Chiarcos,
Richard Eckart de Castilho, and Manfred Stede, ed-
itors, Von der Form zur Bedeutung: Texte automa-
tisch verarbeiten / From Form to Meaning: Process-
ing Texts Automatically, Proceedings of the Biennial
GSCL Conference 2009, pages 81?91, Tu?bingen.
Gunter Narr Verlag.
Kurt Eberle. 1997. Flat underspecified representa-
tion and its meaning for a fragment of German. Ar-
beitspapiere des Sonderforschungsbereichs 340, Nr.
120, Universita?t Stuttgart, Stuttgart, Germany.
Kurt Eberle. 2002. Tense and Aspect Information
in a FUDR-based German French Machine Trans-
lation System. In Hans Kamp and Uwe Reyle, edi-
tors, How we say WHEN it happens. Contributions
to the theory of temporal reference in natural lan-
guage, pages 97?148. Niemeyer, Tu?bingen. Ling.
Arbeiten, Band 455.
Kurt Eberle. 2004. Flat underspecified representation
and its meaning for a fragment of German. Habil-
itationsschrift, Universita?t Stuttgart, Stuttgart, Ger-
many.
Gertrud Faa?, Ulrich Heid, and Helmut Schmid. 2010.
Design and application of a Gold Standard for mor-
phological analysis: SMOR as an example of mor-
phological evaluation. In Proceedings of the seventh
international conference on Language Resources
and Evaluation (LREC), Valetta, Malta.
Jerid Francom and Mans Hulden. 2008. Parallel Multi-
Theory Annotations of Syntactic Structure. In Pro-
ceedings of the Sixth International Language Re-
sources and Evaluation (LREC 2008), Marrakech,
Morocco, May.
Ulrich Heid, Kurt Eberle, and Kerstin Eckart. 2009.
Towards more reliable linguistic analyses: workflow
and infrastructure. Poster presentation at the GSCL
2009 workshop: Linguistic Processing Pipelines,
Potsdam.
Nancy Ide. 2007. GrAF: A Graph-based Format for
Linguistic Annotations. In Proceedings of the LAW
Workshop at ACL 2007, Prague.
Helmut Schmid. 2004. Efficient Parsing of Highly
Ambiguous Context-Free Grammars with Bit Vec-
tors. In Proceedings of the 20th International Con-
ference on Computational Linguistics, Coling?04,
volume 1, pages 162?168, Geneva, Switzerland.
170
Helmut Schmid. 2006. Trace Prediction and Recovery
With Unlexicalized PCFGs and Slash Features. In
Proceedings of COLING-ACL 2006, Sydney, Aus-
tralia.
Dan Tufis?. 2000. Using a large set of EAGLES-
compliant morpho-syntactic descriptors as a tagset
for probabilistic tagging. In Proceedings of the 2nd
International Conference on Language Resources
and Evaluation (LREC 2000), pages 1105?1112,
Athens, Greece, May, 31st ? June, 2nd.
Jakub Zavrel and Walter Daelemans. 2000. Boot-
strapping a Tagged Corpus through Combination of
Existing Heterogeneous Taggers. In Proceedings
of the 2nd International Conference on Language
Resources and Evaluation (LREC 2000), Athens,
Greece, May, 31st ? June, 2nd.
171
