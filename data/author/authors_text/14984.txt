Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 78?89,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Strongly Incremental Repair Detection
Julian Hough1,2
1Dialogue Systems Group
Faculty of Linguistics
and Literature
Bielefeld University
julian.hough@uni-bielefeld.de
Matthew Purver2
2Cognitive Science Research Group
School of Electronic Engineering
and Computer Science
Queen Mary University of London
m.purver@qmul.ac.uk
Abstract
We present STIR (STrongly Incremen-
tal Repair detection), a system that de-
tects speech repairs and edit terms on
transcripts incrementally with minimal la-
tency. STIR uses information-theoretic
measures from n-gram models as its prin-
cipal decision features in a pipeline of
classifiers detecting the different stages of
repairs. Results on the Switchboard dis-
fluency tagged corpus show utterance-final
accuracy on a par with state-of-the-art in-
cremental repair detection methods, but
with better incremental accuracy, faster
time-to-detection and less computational
overhead. We evaluate its performance us-
ing incremental metrics and propose new
repair processing evaluation standards.
1 Introduction
Self-repairs in spontaneous speech are annotated
according to a well established three-phase struc-
ture from (Shriberg, 1994) onwards, and as de-
scribed in Meteer et al. (1995)?s Switchboard cor-
pus annotation handbook:
John [ likes
? ?? ?
reparandum
+ {uh}
? ?? ?
interregnum
loves ]
? ?? ?
repair
Mary (1)
From a dialogue systems perspective, detecting re-
pairs and assigning them the appropriate structure
is vital for robust natural language understanding
(NLU) in interactive systems. Downgrading the
commitment of reparandum phases and assigning
appropriate interregnum and repair phases permits
computation of the user?s intended meaning.
Furthermore, the recent focus on incremental
dialogue systems (see e.g. (Rieser and Schlangen,
2011)) means that repair detection should oper-
ate without unnecessary processing overhead, and
function efficiently within an incremental frame-
work. However, such left-to-right operability on
its own is not sufficient: in line with the princi-
ple of strong incremental interpretation (Milward,
1991), a repair detector should give the best re-
sults possible as early as possible. With one ex-
ception (Zwarts et al., 2010), there has been no
focus on evaluating or improving the incremental
performance of repair detection.
In this paper we present STIR (Strongly In-
cremental Repair detection), a system which ad-
dresses the challenges of incremental accuracy,
computational complexity and latency in self-
repair detection, by making local decisions based
on relatively simple measures of fluency and sim-
ilarity. Section 2 reviews state-of-the-art methods;
Section 3 summarizes the challenges and explains
our general approach; Section 4 explains STIR in
detail; Section 5 explains our experimental set-up
and novel evaluation metrics; Section 6 presents
and discusses our results and Section 7 concludes.
2 Previous work
Qian and Liu (2013) achieve the state of the art in
Switchboard corpus self-repair detection, with an
F-score for detecting reparandum words of 0.841
using a three-step weighted Max-Margin Markov
network approach. Similarly, Georgila (2009)
uses Integer Linear Programming post-processing
of a CRF to achieve F-scores over 0.8 for reparan-
dum start and repair start detection. However nei-
ther approach can operate incrementally.
Recently, there has been increased interest
in left-to-right repair detection: Rasooli and
Tetreault (2014) and Honnibal and Johnson (2014)
present dependency parsing systems with reparan-
dum detection which perform similarly, the latter
equalling Qian and Liu (2013)?s F-score at 0.841.
However, while operating left-to-right, these sys-
tems are not designed or evaluated for their incre-
mental performance. The use of beam search over
78
different repair hypotheses in (Honnibal and John-
son, 2014) is likely to lead to unstable repair label
sequences, and they report repair hypothesis ?jit-
ter?. Both of these systems use a non-monotonic
dependency parsing approach that immediately re-
moves the reparandum from the linguistic anal-
ysis of the utterance in terms of its dependency
structure and repair-reparandum correspondence,
which from a downstream NLU module?s perspec-
tive is undesirable. Heeman and Allen (1999) and
Miller and Schuler (2008) present earlier left-to-
right operational detectors which are less accu-
rate and again give no indication of the incremen-
tal performance of their systems. While Heeman
and Allen (1999) rely on repair structure template
detection coupled with a multi-knowledge-source
language model, the rarity of the tail of repair
structures is likely to be the reason for lower per-
formance: Hough and Purver (2013) show that
only 39% of repair alignment structures appear
at least twice in Switchboard, supported by the
29% reported by Heeman and Allen (1999) on
the smaller TRAINS corpus. Miller and Schuler
(2008)?s encoding of repairs into a grammar also
causes sparsity in training: repair is a general pro-
cessing strategy not restricted to certain lexical
items or POS tag sequences.
The model we consider most suitable for in-
cremental dialogue systems so far is Zwarts et
al. (2010)?s incremental version of Johnson and
Charniak (2004)?s noisy channel repair detector,
as it incrementally applies structural repair anal-
yses (rather than just identifying reparanda) and
is evaluated for its incremental properties. Fol-
lowing (Johnson and Charniak, 2004), their sys-
tem uses an n-gram language model trained on
roughly 100K utterances of reparandum-excised
(?cleaned?) Switchboard data. Its channel model is
a statistically-trained S-TAG parser whose gram-
mar has simple reparandum-repair alignment rule
categories for its non-terminals (copy, delete, in-
sert, substitute) and words for its terminals. The
parser hypothesises all possible repair structures
for the string consumed so far in a chart, before
pruning the unlikely ones. It performs equally
well to the non-incremental model by the end of
each utterance (F-score = 0.778), and can make
detections early via the addition of a speculative
next-word repair completion category to their S-
TAG non-terminals. In terms of incremental per-
formance, they report the novel evaluation met-
ric of time-to-detection for correctly identified re-
pairs, achieving an average of 7.5 words from the
start of the reparandum and 4.6 from the start of
the repair phase. They also introduce delayed ac-
curacy, a word-by-word evaluation against gold-
standard disfluency tags up to the word before the
current word being consumed (in their terms, the
prefix boundary), giving a measure of the stability
of the repair hypotheses. They report an F-score
of 0.578 at one word back from the current prefix
boundary, increasing word-by-word until 6 words
back where it reaches 0.770. These results are the
point-of-departure for our work.
3 Challenges and Approach
In this section we summarize the challenges for
incremental repair detection: computational com-
plexity, repair hypothesis stability, latency of de-
tection and repair structure identification. In 3.1
we explain how we address these.
Computational complexity Approaches to de-
tecting repair structures often use chart storage
(Zwarts et al., 2010; Johnson and Charniak, 2004;
Heeman and Allen, 1999), which poses a com-
putational overhead: if considering all possible
boundary points for a repair structure?s 3 phases
beginning on any word, for prefixes of length n
the number of hypotheses can grow in the order
O(n
4
). Exploring a subset of this space is nec-
essary for assigning entire repair structures as in
(1) above, rather than just detecting reparanda:
the (Johnson and Charniak, 2004; Zwarts et al.,
2010) noisy-channel detector is the only system
that applies such structures but the potential run-
time complexity in decoding these with their S-
TAG repair parser is O(n5). In their approach,
complexity is mitigated by imposing a maximum
repair length (12 words), and also by using beam
search with re-ranking (Lease et al., 2006; Zwarts
and Johnson, 2011). If we wish to include full
decoding of the repair?s structure (as argued by
Hough and Purver (2013) as necessary for full in-
terpretation) whilst taking a strictly incremental
and time-critical perspective, reducing this com-
plexity by minimizing the size of this search space
is crucial.
Stability of repair hypotheses and latency Us-
ing a beam search of n-best hypotheses on a word-
by-word basis can cause ?jitter? in the detector?s
output. While utterance-final accuracy is desired,
79
for a truly incremental system good intermedi-
ate results are equally important. Zwarts et al.
(2010)?s time-to-detection results show their sys-
tem is only certain about a detection after process-
ing the entire repair. This may be due to the string
alignment-inspired S-TAG that matches repair and
reparanda: a ?rough copy? dependency only be-
comes likely once the entire repair has been con-
sumed. The latency of 4.6 words to detection and
a relatively slow rise to utterance-final accuracy up
to 6 words back is undesirable given repairs have
a mean reparandum length of ?1.5 words (Hough
and Purver, 2013; Shriberg and Stolcke, 1998).
Structural identification Classifying repairs
has been ignored in repair processing, despite the
presence of distinct categories (e.g. repeats, sub-
stitutions, deletes) with different pragmatic effects
(Hough and Purver, 2013).1 This is perhaps due to
lack of clarity in definition: even for human anno-
tators, verbatim repeats withstanding, agreement
is often poor (Hough and Purver, 2013; Shriberg,
1994). Assigning and evaluating repair (not just
reparandum) structures will allow repair interpre-
tation in future; however, work to date evaluates
only reparandum detection.
3.1 Our approach
To address the above, we propose an alternative
to (Johnson and Charniak, 2004; Zwarts et al.,
2010)?s noisy channel model. While the model
elegantly captures intuitions about parallelism in
repairs and modelling fluency, it relies on string-
matching, motivated in a similar way to automatic
spelling correction (Brill and Moore, 2000): it as-
sumes a speaker chooses to utter fluent utterance
X according to some prior distribution P (X), but
a noisy channel causes them instead to utter a
noisy Y according to channel model P (Y |X).
Estimating P (Y |X) directly from observed data
is difficult due to sparsity of repair instances, so a
transducer is trained on the rough copy alignments
between reparandum and repair. This approach
succeeds because repetition and simple substitu-
tion repairs are very common; but repair as a psy-
chological process is not driven by string align-
ment, and deletes, restarts and rarer substitution
forms are not captured. Furthermore, the noisy
channel model assumes an inherently utterance-
global process for generating (and therefore find-
1Though see (Germesin et al., 2008) for one approach,
albeit using idiosyncratic repair categories.
ing) an underlying ?clean? string ? much as sim-
ilar spelling correction models are word-global ?
we instead take a very local perspective here.
In accordance with psycholinguistic evidence
(Brennan and Schober, 2001), we assume charac-
teristics of the repair onset allow hearers to detect
it very quickly and solve the continuation prob-
lem (Levelt, 1983) of integrating the repair into
their linguistic context immediately, before pro-
cessing or even hearing the end of the repair phase.
While repair onsets may take the form of inter-
regna, this is not a reliable signal, occurring in
only ?15% of repairs (Hough and Purver, 2013;
Heeman and Allen, 1999). Our repair onset de-
tection is therefore driven by departures from flu-
ency, via information-theoretic features derived
incrementally from a language model in line with
recent psycholinguistic accounts of incremental
parsing ? see (Keller, 2004; Jaeger and Tily, 2011).
Considering the time-linear way a repair is pro-
cessed and the fact speakers are exponentially less
likely to trace one word further back in repair as
utterance length increases (Shriberg and Stolcke,
1998), backwards search seems to be the most ef-
ficient reparandum extent detection method.2 Fea-
tures determining the detection of the reparan-
dum extent in the backwards search can also be
information-theoretic: entropy measures of dis-
tributional parallelism can characterize not only
rough copy dependencies, but distributionally sim-
ilar or dissimilar correspondences between se-
quences. Finally, when detecting the repair end
and structure, distributional information allows
computation of the similarity between reparan-
dum and repair. We argue a local-detection-
with-backtracking approach is more cognitively
plausible than string-based left-to-right repair la-
belling, and using this insight should allow an im-
provement in incremental accuracy, stability and
time-to-detection over string-alignment driven ap-
proaches in repair detection.
4 STIR: Strongly Incremental Repair
detection
Our system, STIR (Strongly Incremental Repair
detection), therefore takes a local incremental ap-
2We acknowledge a purely position-based model for
reparandum extent detection under-estimates prepositions,
which speakers favour as the retrace start and over-estimates
verbs, which speakers tend to avoid retracing back to, prefer-
ring to begin the utterance again, as (Healey et al., 2011)?s
experiments also demonstrate.
80
?John? ?likes?
S
0
S
1
S
2
T0
?John? ?likes? ?uh?
ed
S
0
S
1
S
2
S
3
ed
T1
?John? ?likes? ?uh?
ed
?loves?
rp
start
S
0
S
1
S
2
S
3
ed
?
S
4
rp
start
T2
?John? ?likes?
rm
start
rm
end
?uh?
ed
?loves?
rp
start
S
0
S
1
S
2
rm
start
rm
end
S
3
ed
S
4
rp
start
T3
?John? ?likes?
rm
start
rm
end
?uh?
ed
?loves?
rp
start
rp
sub
end
S
0
S
1
S
2
rm
start
rm
end
S
3
ed
S
4
rp
start
rp
sub
end
T4
?John? ?likes?
rm
start
rm
end
?uh?
ed
?loves?
rp
start
rp
sub
end
?Mary?
S
0
S
1
S
2
rm
start
rm
end
S
3
ed
S
4
rp
start
rp
sub
end
S
5
T5
Figure 1: Strongly Incremental Repair Detection
proach to detecting repairs and isolated edit terms,
assigning words the structures in (2). We in-
clude interregnum recognition in the process, due
to the inclusion of interregnum vocabulary within
edit term vocabulary (Ginzburg, 2012; Hough and
Purver, 2013), a useful feature for repair detection
(Lease et al., 2006; Qian and Liu, 2013).
{
...[rm
start
...rm
end
+ {ed}rp
start
...rp
end
]...
...{ed}...
(2)
Rather than detecting the repair structure in its
left-to-right string order as above, STIR functions
as in Figure 1: first detecting edit terms (possibly
interregna) at step T1; then detecting repair onsets
rp
start
at T2; if one is found, backwards searching
to find rm
start
at T3; then finally finding the re-
pair end rp
end
at T4. Step T1 relies mainly on
lexical probabilities from an edit term language
model; T2 exploits features of divergence from a
fluent language model; T3 uses fluency of hypoth-
esised repairs; and T4 the similarity between dis-
tributions after reparandum and repair. However,
each stage integrates these basic insights via mul-
tiple related features in a statistical classifier.
4.1 Enriched incremental language models
We derive the basic information-theoretic features
required using n-gram language models, as they
have a long history of information theoretic anal-
ysis (Shannon, 1948) and provide reproducible re-
sults without forcing commitment to one partic-
ular grammar formalism. Following recent work
on modelling grammaticality judgements (Clark
et al., 2013), we implement several modifications
to standard language models to develop our basic
measures of fluency and uncertainty.
For our main fluent language models we train
a trigram model with Kneser-Ney smoothing
(Kneser and Ney, 1995) on the words and POS
tags of the standard Switchboard training data
(all files with conversation numbers beginning
sw2*,sw3* in the Penn Treebank III release), con-
sisting of ?100K utterances, ?600K words. We
follow (Johnson and Charniak, 2004) by clean-
ing the data of disfluencies (i.e. edit terms and
reparanda), to approximate a ?fluent? language
model. We call these probabilities plex
kn
, p
pos
kn
be-
low.3
3We suppress the pos and lex superscripts below where we
refer to measures from either model.
81
We then derive surprisal as our principal default
lexical uncertainty measurement s (equation 3) in
both models; and, following (Clark et al., 2013),
the (unigram) Weighted Mean Log trigram prob-
ability (WML, eq. 4)? the trigram logprob of the
sequence divided by the inverse summed logprob
of the component unigrams (apart from the first
two words in the sequence, which serve as the
first trigram history). As here we use a local ap-
proach we restrict the WML measures to single
trigrams (weighted by the inverse logprob of the
final word). While use of standard n-gram prob-
ability conflates syntactic with lexical probability,
WML gives us an approximation to incremental
syntactic probability by factoring out lexical fre-
quency.
s(w
i?2
. . . w
i
) = ? log
2
p
kn
(w
i
| w
i?2
, w
i?1
) (3)
WML(w
0
. . . w
n
) =
?
i=n
i=2
log
2
p
kn
(w
i
| w
i?2
, w
i?1
)
?
?
n
j=2
log
2
p
kn
(w
j
)
(4)
Distributional measures To approximate un-
certainty, we also derive the entropy H(w | c) of
the possible word continuations w given a context
c, from p(w
i
| c) for all words w
i
in the vocabu-
lary ? see (5). Calculating distributions over the
entire lexicon incrementally is costly, so we ap-
proximate this by constraining the calculation to
words which are observed at least once in context
c in training, w
c
= {w|count(c, w) ? 1} , assum-
ing a uniform distribution over the unseen suffixes
by using the appropriate smoothing constant, and
subtracting the latter from the former ? see eq. (6).
Manual inspection showed this approximation
to be very close, and the trie structure of our n-
gram models allows efficient calculation. We also
make use of the Zipfian distribution of n-grams
in corpora by storing entropy values for the 20%
most common trigram contexts observed in train-
ing, leaving entropy values of rare or unseen con-
texts to be computed at decoding time with little
search cost due to their small or empty w
c
sets.
H(w | c) = ?
?
w?V ocab
p
kn
(w | c) log
2
p
kn
(w | c) (5)
H(w | c) ?
[
?
?
w?w
c
p
kn
(w | c) log
2
p
kn
(w | c)
]
? [n? ? log
2
?]
where n = |V ocab| ? |w
c
|
and ? =
1?
?
w?w
c
p
kn
(w | c)
n
(6)
Given entropy estimates, we can also sim-
ilarly approximate the Kullback-Leibler (KL)
divergence (relative entropy) between distribu-
tions in two different contexts c
1
and c
2
, i.e.
?(w|c
1
) and ?(w|c
2
), by pair-wise computing
p(w|c
1
) log
2
(
p(w|c
1
)
p(w|c
2
)
) only for words w ? w
c
1
?
w
c
2
, then approximating unseen values by assum-
ing uniform distributions. Using p
kn
smoothed es-
timates rather than raw maximum likelihood es-
timations avoids infinite KL divergence values.
Again, we found this approximation sufficiently
close to the real values for our purposes. All such
probability and distribution values are stored in
incrementally constructed directed acyclic graph
(DAG) structures (see Figure 1), exploiting the
Markov assumption of n-gram models to allow ef-
ficient calculation by avoiding re-computation.
4.2 Individual classifiers
This section details the features used by the 4 indi-
vidual classifiers. To investigate the utility of the
features used in each classifier we obtain values
on the standard Switchboard heldout data (PTB III
files sw4[5-9]*: 6.4K utterances, 49K words).
4.2.1 Edit term detection
In the first component, we utilise the well-known
observation that edit terms have a distinctive
vocabulary (Ginzburg, 2012), training a bigram
model on a corpus of all edit words annotated in
Switchboard?s training data. The classifier simply
uses the surprisal slex from this edit word model,
and the trigram surprisal slex from the standard
fluent model of Section 4.1. At the current position
w
n
, one, both or none of words w
n
and w
n?1
are
classified as edits. We found this simple approach
effective and stable, although some delayed deci-
sions occur in cases where slex and WMLlex are
high in both models before the end of the edit, e.g.
?I like? ? ?I {like} want...?. Words classified as
ed are removed from the incremental processing
graph (indicated by the dotted line transition in
Figure 1) and the stack updated if repair hypothe-
ses are cancelled due to a delayed edit hypothesis
of w
n?1
.
4.2.2 Repair start detection
Repair onset detection is arguably the most crucial
component: the greater its accuracy, the better the
input for downstream components and the lesser
the overhead of filtering false positives required.
82
i havent had
any
good really
very
good experience with child
care
?1.4
?1.2
?1.0
?0.8
?0.6
?0.4
?0.2
0.0
W
M
L
Figure 2: WMLlex values for trigrams for a repaired utterance exhibiting the drop at the repair onset
We use Section 4.1?s information-theoretic fea-
tures s,WML,H for words and POS, and intro-
duce 5 additional information-theoretic features:
?WML is the difference between the WML val-
ues at w
n?1
and w
n
; ?H is the difference in en-
tropy between w
n?1
and w
n
; InformationGain
is the difference between expected entropy at
w
n?1
and observed s at w
n
, a measure that
factors out the effect of naturally high entropy
contexts; BestEntropyReduce is the best reduc-
tion in entropy possible by an early rough hy-
pothesis of reparandum onsets within 3 words;
and BestWMLBoost similarly speculates on the
best improvement of WML possible by positing
rm
start
positions up to 3 words back. We also in-
clude simple alignment features: binary features
which indicate if the word w
i?x
is identical to the
current word w
i
for x ? {1, 2, 3}. With 6 align-
ment features, 16 N-gram features and a single
logical feature edit which indicates the presence
of an edit word at position w
i?1
, rp
start
detection
uses 23 features? see Table 1.
We hypothesised repair onsets rpstart would
have significantly lower plex (lower lexical-
syntactic probability) and WMLlex (lower syntac-
tic probability) than other fluent trigrams. This
was the case in the Switchboard heldout data
for both measures, with the biggest difference
obtained for WMLlex (non-repair-onsets: -0.736
(sd=0.359); repair onsets: -1.457 (sd=0.359)). In
the POS model, entropy of continuation Hpos was
the strongest feature (non-repair-onsets: 3.141
(sd=0.769); repair onsets: 3.444 (sd=0.899)). The
trigram WMLlex measure for the repaired utter-
ance ?I haven?t had any [ good + really very good
] experience with child care? can be seen in Fig-
ure 2. The steep drop at the repair onset shows the
usefulness of WML features for fluency measures.
To compare n-gram measures against other lo-
cal features, we ranked the features by Informa-
tion Gain using 10-fold cross validation over the
Switchboard heldout data? see Table 1. The lan-
guage model features are far more discriminative
than the alignment features, showing the potential
of a general information-theoretic approach.
4.2.3 Reparandum start detection
In detecting rm
start
positions given a hypothe-
sised rp
start
(stage T3 in Figure 1), we use the
noisy channel intuition that removing the reparan-
dum (from rm
start
to rp
start
) increases fluency
of the utterance, expressed here as WMLboost as
described above. When using gold standard in-
put we found this was the case on the heldout
data, with a mean WMLboost of 0.223 (sd=0.267)
for reparandum onsets and -0.058 (sd=0.224) for
other words in the 6-word history- the negative
boost for non-reparandum words captures the in-
tuition that backtracking from those points would
make the utterance less grammatical, and con-
versely the boost afforded by the correct rm
start
detection helps solve the continuation problem for
the listener (and our detector).
Parallelism in the onsets of rp
start
and
rm
start
can also help solve the continuation
problem, and in fact the KL divergence be-
tween ?pos(w | rm
start
, rm
start?1
) and ?pos(w |
rp
start
, rp
start?1
) is the second most useful fea-
ture with average merit 0.429 (+- 0.010) in cross-
83
validation. The highest ranked feature is ?WML
(0.437 (+- 0.003)) which here encodes the drop in
the WMLboost from one backtracked position to
the next. In ranking the 32 features we use, again
information-theoretic ones are higher ranked than
the logical features.
average merit average rank attribute
0.139 (+- 0.002) 1 (+- 0.00) Hpos
0.131 (+- 0.001) 2 (+- 0.00) WMLpos
0.126 (+- 0.001) 3.4 (+- 0.66) WMLlex
0.125 (+- 0.003) 4 (+- 1.10) spos
0.122 (+- 0.001) 5.9 (+- 0.94) w
i?1
= w
i
0.122 (+- 0.001) 5.9 (+- 0.70) BestWMLBoostlex
0.122 (+- 0.002) 5.9 (+- 1.22) InformationGainpos
0.119 (+- 0.001) 7.9 (+- 0.30) BestWMLBoostpos
0.098 (+- 0.002) 9 (+- 0.00) H lex
0.08 (+- 0.001) 10.4 (+- 0.49) ?WMLpos
0.08 (+- 0.003) 10.6 (+- 0.49) ?Hpos
0.072 (+- 0.001) 12 (+- 0.00) POS
i?1
= POS
i
0.066 (+- 0.003) 13.1 (+- 0.30) slex
0.059 (+- 0.000) 14.2 (+- 0.40) ?WMLlex
0.058 (+- 0.005) 14.7 (+- 0.64) BestEntropyReducepos
0.049 (+- 0.001) 16.3 (+- 0.46) InformationGainlex
0.047 (+- 0.004) 16.7 (+- 0.46) BestEntropyReducelex
0.035 (+- 0.004) 18 (+- 0.00) ?H lex
0.024 (+- 0.000) 19 (+- 0.00) w
i?2
= w
i
0.013 (+- 0.000) 20 (+- 0.00) POS
i?2
= POS
i
0.01 (+- 0.000) 21 (+- 0.00) w
i?3
= w
i
0.009 (+- 0.000) 22 (+- 0.00) edit
0.006 (+- 0.000) 23 (+- 0.00) POS
i?3
= POS
i
Table 1: Feature ranker (Information Gain) for
rp
start
detection- 10-fold x-validation on Switch-
board heldout data.
4.2.4 Repair end detection and structure
classification
For rp
end
detection, using the notion of paral-
lelism, we hypothesise an effect of divergence be-
tween ?lex at the reparandum-final word rm
end
and the repair-final word rp
end
: for repetition re-
pairs, KL divergence will trivially be 0; for substi-
tutions, it will be higher; for deletes, even higher.
Upon inspection of our feature ranking this KL
measure ranked 5th out of 23 features (merit=
0.258 (+- 0.002)).
We introduce another feature encoding paral-
lelism ReparandumRepairDifference : the differ-
ence in probability between an utterance cleaned
of the reparandum and the utterance with its
repair phase substituting its reparandum. In
both the POS (merit=0.366 (+- 0.003)) and word
(merit=0.352 (+- 0.002)) LMs, this was the most
discriminative feature.
4.3 Classifier pipeline
STIR effects a pipeline of classifiers as in Fig-
ure 3, where the ed classifier only permits non
ed words to be passed on to rp
start
classification
and for rp
end
classification of the active repair
hypotheses, maintained in a stack. The rp
start
classifier passes positive repair hypotheses to the
rm
start
classifier, which backwards searches up
to 7 words back in the utterance. If a rm
start
is
classified, the output is passed on for rp
end
clas-
sification at the end of the pipeline, and if not re-
jected this is pushed onto the repair stack. Repair
hypotheses are are popped off when the string is
7 words beyond its rp
start
position. Putting limits
on the stack?s storage space is a way of controlling
for processing overhead and complexity. Embed-
ded repairs whose rm
start
coincide with another?s
rp
start
are easily dealt with as they are added to
the stack as separate hypotheses.4
Classifiers Classifiers are implemented using
Random Forests (Breiman, 2001) and we use dif-
ferent error functions for each stage using Meta-
Cost (Domingos, 1999). The flexibility afforded
by implementing adjustable error functions in a
pipelined incremental processor allows control of
the trade-off of immediate accuracy against run-
time and stability of the sequence classification.
Processing complexity This pipeline avoids an
exhaustive search all repair hypotheses. If we limit
the search to within the ?rm
start
, rp
start
? possibil-
ities, this number of repairs grows approximately
in the triangular number series? i.e. n(n+1)
2
, a
nested loop over previous words as n gets incre-
mented ? which in terms of a complexity class is
a quadratic O(n2). If we allow more than one
?rm
start
, rp
start
? hypothesis per word, the com-
plexity goes up to O(n3), however in the tests that
we describe below, we are able to achieve good de-
tection results without permitting this extra search
space. Under our assumption that reparandum on-
set detection is only triggered after repair onset de-
tection, and repair extent detection is dependent
on positive reparandum onset detection, a pipeline
with accurate components will allow us to limit
processing to a small subset of this search space.
4We constrain the problem not to include embedded
deletes which may share their rp
start
word with another re-
pair ? these are in practice very rare.
84
Figure 3: Classifier pipeline
5 Experimental set-up
We train STIR on the Switchboard data described
above, and test it on the standard Switchboard test
data (PTB III files 4[0-1]*). In order to avoid over-
fitting of classifiers to the basic language models,
we use a cross-fold training approach: we divide
the corpus into 10 folds and use language mod-
els trained on 9 folds to obtain feature values for
the 10th fold, repeating for all 10. Classifiers are
then trained as standard on the resulting feature-
annotated corpus. This resulted in better feature
utility for n-grams and better F-score results for
detection in all components in the order of 5-6%.5
Training the classifiers Each Random Forest
classifier was limited to 20 trees of maximum
depth 4 nodes, putting a ceiling on decoding time.
In making the classifiers cost-sensitive, MetaCost
resamples the data in accordance with the cost
functions: we found using 10 iterations over a re-
sample of 25% of the training data gave the most
effective trade-off between training time and accu-
racy.6 We use 8 different cost functions in rp
start
with differing costs for false negatives and posi-
tives of the form below, where R is a repair ele-
ment word and F is a fluent onset:
(
R
hyp
F
hyp
R
gold
0 2
F
gold
1 0
)
We adopt a similar technique in rm
start
using 5
different cost functions and in rp
end
using 8 dif-
ferent settings, which when combined gives a to-
tal of 320 different cost function configurations.
We hypothesise that higher recall permitted in the
pipeline?s first components would result in better
overall accuracy as these hypotheses become re-
fined, though at the cost of the stability of the hy-
5Zwarts and Johnson (2011) take a similar approach on
Switchboard data to train a re-ranker of repair analyses.
6As (Domingos, 1999) demonstrated, there are only rela-
tively small accuracy gains when using more than this, with
training time increasing in the order of the re-sample size.
potheses of the sequence and extra downstream
processing in pruning false positives.
We also experiment with the number of repair
hypotheses permitted per word, using limits of 1-
best and 2-best hypotheses. We expect that allow-
ing 2 hypotheses to be explored per rp
start
should
allow greater final accuracy, but with the trade-off
of greater decoding and training complexity, and
possible incremental instability.
As we wish to explore the incrementality versus
final accuracy trade-off that STIR can achieve we
now describe the evaluation metrics we employ.
5.1 Incremental evaluation metrics
Following (Baumann et al., 2011) we divide our
evaluation metrics into similarity metrics (mea-
sures of equality with or similarity to a gold stan-
dard), timing metrics (measures of the timing of
relevant phenomena detected from the gold stan-
dard) and diachronic metrics (evolution of incre-
mental hypotheses over time).
Similarity metrics For direct comparison to
previous approaches we use the standard measure
of overall accuracy, the F-score over reparandum
words, which we abbreviate F
rm
(see 7):
precision = rm
correct
rm
hyp
recall = rm
correct
rm
gold
F
rm
= 2 ?
precision ? recall
precision + recall
(7)
We are also interested in repair structural clas-
sification, we also measure F-score over all repair
components (rm words, ed words as interregna
and rp words), a metric we abbreviate F
s
. This
is not measured in standard repair detection on
Switchboard. To investigate incremental accuracy
we evaluate the delayed accuracy (DA) introduced
by (Zwarts et al., 2010), as described in section
2 against the utterance-final gold standard disflu-
ency annotations, and use the mean of the 6 word
F-scores.
85
Input and current repair labels edits
John
John likes
rm rp
(?rm) (?rp)
John likes uh
ed
(?rm) (?rp)?ed
John likes uh loves
rm ed rp
?rm?rp
John likes uh loves Mary
rm ed rp
Figure 4: Edit Overhead- 4 unnecessary edits
Timing and resource metrics Again for com-
parative purposes we use Zwarts et al?s time-to-
detection metrics, that is the two average distances
(in numbers of words) consumed before first de-
tection of gold standard repairs, one from rm
start
,
TD
rm
and one from rp
start
, TD
rp
. In our 1-best
detection system, before evaluation we know a pri-
ori TD
rp
will be 1 token, and TD
rm
will be 1 more
than the average length of rm
start
? rp
start
repair
spans correctly detected. However when we in-
troduce a beam where multiple rm
start
s are pos-
sible per rp
start
with the most likely hypothesis
committed as the current output, the latency may
begin to increase: the initially most probable hy-
pothesis may not be the correct one. In addition
to output timing metrics, we account for intrinsic
processing complexity with the metric processing
overhead (PO), which is the number of classifica-
tions made by all components per word of input.
Diachronic metrics To measure stability of re-
pair hypotheses over time we use (Baumann et al.,
2011)?s edit overhead (EO) metric. EO measures
the proportion of edits (add, revoke, substitute) ap-
plied to a processor?s output structure that are un-
necessary. STIR?s output is the repair label se-
quence shown in Figure 1, however rather than
evaluating its EO against the current gold stan-
dard labels, we use a new mark-up we term the in-
cremental repair gold standard: this does not pe-
nalise lack of detection of a reparandum word rm
as a bad edit until the corresponding rp
start
of that
rm has been consumed. While F
rm
, F
s
and DA
evaluate against what Baumann et al. (2011) call
the current gold standard, the incremental gold
standard reflects the repair processing approach
we set out in 3. An example of a repaired utterance
with an EO of 44% (4
9
) can be seen in Figure 4: of
the 9 edits (7 repair annotations and 2 correct flu-
ent words), 4 are unnecessary (bracketed). Note
Figure 6: Delayed Accuracy Curves
the final ?rm is not counted as a bad edit for the
reasons just given.
6 Results and Discussion
We evaluate on the Switchboard test data; Ta-
ble 2 shows results of the best performing settings
for each of the metrics described above, together
with the setting achieving the highest total score
(TS)? the average % achieved of the best per-
forming system?s result in each metric.7 The set-
tings found to achieve the highest F
rm
(the metric
standardly used in disfluency detection), and that
found to achieve the highest TS for each stage in
the pipeline are shown in Figure 5.
Our experiments showed that different system
settings perform better in different metrics, and
no individual setting achieved the best result in
all of them. Our best utterance-final F
rm
reaches
0.779, marginally though not significantly exceed-
ing (Zwarts et al., 2010)?s measure and STIR
achieves 0.736 on the previously unevaluated F
s
.
The setting with the best DA improves on (Zwarts
et al., 2010)?s result significantly in terms of mean
values (0.718 vs. 0.694), and also in terms of the
steepness of the curves (Figure 6). The fastest av-
erage time to detection is 1 word for TD
rp
and 2.6
words for TD
rm
(Table 3), improving dramatically
on the noisy channel model?s 4.6 and 7.5 words.
Incrementality versus accuracy trade-off We
aimed to investigate how well a system could do
in terms of achieving both good final accuracy and
incremental performance, and while the best F
rm
setting had a large PO and relatively slow DA in-
crease, we find STIR can find a good trade-off set-
7We do not include time-to-detection scores in TS as it
did not vary enough between settings to be significant, how-
ever there was a difference in this measure between the 1-best
stack condition and the 2-best stack condition ? see below.
86
??
?
rp
hyp
start
F
hyp
rp
gold
start
0 64
F
gold
1 0
?
?
?
?
?
?
rm
hyp
start
F
hyp
rm
gold
start
0 8
F
gold
1 0
?
?
?
?
?
?
rp
hyp
end
F
hyp
rp
gold
end
0 2
F
gold
1 0
?
?
?
Stack depth = 2
?
?
?
rp
hyp
start
F
hyp
rp
gold
start
0 2
F
gold
1 0
?
?
?
?
?
?
rm
hyp
start
F
hyp
rm
gold
start
0 16
F
gold
1 0
?
?
?
?
?
?
rp
hyp
end
F
hyp
rp
gold
end
0 8
F
gold
1 0
?
?
?
Stack depth = 1
Figure 5: The cost function settings for the MetaCost classifiers for each component, for the best F
rm
setting (top row) and best total score (TS) setting (bottom row)
F
rm
F
s
DA EO PO
Best Final rm F-score (F
rm
) 0.779 0.735 0.698 3.946 1.733
Best Final repair structure F-score (F
s
) 0.772 0.736 0.707 4.477 1.659
Best Delayed Accuracy of rm (DA) 0.767 0.721 0.718 1.483 1.689
Best (lowest) Edit Overhead (EO) 0.718 0.674 0.675 0.864 1.230
Best (lowest) Processing Overhead (PO) 0.716 0.671 0.673 0.875 1.229
Best Total Score (mean % of best scores) (TS) 0.754 0.708 0.711 0.931 1.255
Table 2: Comparison of the best performing system settings using different measures
F
rm
F
s
DA EO PO TD
rp
TD
rm
1-best rm
start
0.745 0.707 0.699 3.780 1.650 1.0 2.6
2-best rm
start
0.758 0.721 0.701 4.319 1.665 1.1 2.7
Table 3: Comparison of performance of systems with different stack capacities
ting: the highest TS scoring setting achieves an
F
rm
of 0.754 whilst also exhibiting a very good
DA (0.711) ? over 98% of the best recorded score
? and low PO and EO rates ? over 96% of the best
recorded scores. See the bottom row of Table 2.
As can be seen in Figure 5, the cost functions for
these winning settings are different in nature. The
best non-incremental F
rm
measure setting requires
high recall for the rest of the pipeline to work on,
using the highest cost, 64, for false negative rp
start
words and the highest stack depth of 2 (similar to a
wider beam); but the best overall TS scoring sys-
tem uses a less permissive setting to increase in-
cremental performance.
We make a preliminary investigation into the
effect of increasing the stack capacity by com-
paring stacks with 1-best rm
start
hypotheses per
rp
start
and 2-best stacks. The average differences
between the two conditions is shown in Table 3.
Moving to the 2-stack condition results in gain in
overall accuracy in F
rm
and F
s
, but at the cost of
EO and also time-to-detection scores TD
rm
and
TD
rp
. The extent to which the stack can be in-
creased without increasing jitter, latency and com-
plexity will be investigated in future work.
7 Conclusion
We have presented STIR, an incremental repair
detector that can be used to experiment with in-
cremental performance and accuracy trade-offs. In
future work we plan to include probabilistic and
distributional features from a top-down incremen-
tal parser e.g. Roark et al. (2009), and use STIR?s
distributional features to classify repair type.
Acknowledgements
We thank the three anonymous EMNLP review-
ers for their helpful comments. Hough is sup-
ported by the DUEL project, financially supported
by the Agence Nationale de la Research (grant
number ANR-13-FRAL-0001) and the Deutsche
Forschungsgemainschaft. Much of the work was
carried out with support from an EPSRC DTA
scholarship at Queen Mary University of Lon-
don. Purver is partly supported by ConCreTe:
the project ConCreTe acknowledges the financial
support of the Future and Emerging Technologies
(FET) programme within the Seventh Framework
Programme for Research of the European Com-
mission, under FET grant number 611733.
87
References
T. Baumann, O. Bu?, and D. Schlangen. 2011. Eval-
uation and optimisation of incremental processors.
Dialogue & Discourse, 2(1):113?141.
Leo Breiman. 2001. Random forests. Machine learn-
ing, 45(1):5?32.
S.E. Brennan and M.F. Schober. 2001. How listeners
compensate for disfluencies in spontaneous speech.
Journal of Memory and Language, 44(2):274?296.
Eric Brill and Robert C Moore. 2000. An improved er-
ror model for noisy channel spelling correction. In
Proceedings of the 38th Annual Meeting on Associa-
tion for Computational Linguistics, pages 286?293.
Association for Computational Linguistics.
Alexander Clark, Gianluca Giorgolo, and Shalom Lap-
pin. 2013. Statistical representation of grammat-
icality judgements: the limits of n-gram models.
In Proceedings of the Fourth Annual Workshop on
Cognitive Modeling and Computational Linguistics
(CMCL), pages 28?36, Sofia, Bulgaria, August. As-
sociation for Computational Linguistics.
Pedro Domingos. 1999. Metacost: A general method
for making classifiers cost-sensitive. In Proceed-
ings of the fifth ACM SIGKDD international con-
ference on Knowledge discovery and data mining,
pages 155?164. ACM.
Kallirroi Georgila. 2009. Using integer linear pro-
gramming for detecting speech disfluencies. In Pro-
ceedings of Human Language Technologies: The
2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, Companion Volume: Short Papers, pages
109?112. Association for Computational Linguis-
tics.
Sebastian Germesin, Tilman Becker, and Peter Poller.
2008. Hybrid multi-step disfluency detection.
In Machine Learning for Multimodal Interaction,
pages 185?195. Springer.
Jonathan Ginzburg. 2012. The Interactive Stance:
Meaning for Conversation. Oxford University
Press.
P. G. T. Healey, Arash Eshghi, Christine Howes, and
Matthew Purver. 2011. Making a contribution: Pro-
cessing clarification requests in dialogue. In Pro-
ceedings of the 21st Annual Meeting of the Society
for Text and Discourse, Poitiers, July.
Peter Heeman and James Allen. 1999. Speech repairs,
intonational phrases, and discourse markers: model-
ing speakers? utterances in spoken dialogue. Com-
putational Linguistics, 25(4):527?571.
Matthew Honnibal and Mark Johnson. 2014. Joint
incremental disfluency detection and dependency
parsing. Transactions of the Association of Com-
putational Linugistics (TACL), 2:131?142.
Julian Hough and Matthew Purver. 2013. Modelling
expectation in the self-repair processing of annotat-,
um, listeners. In Proceedings of the 17th SemDial
Workshop on the Semantics and Pragmatics of Di-
alogue (DialDam), pages 92?101, Amsterdam, De-
cember.
T Florian Jaeger and Harry Tily. 2011. On language
utility: Processing complexity and communicative
efficiency. Wiley Interdisciplinary Reviews: Cogni-
tive Science, 2(3):323?335.
Mark Johnson and Eugene Charniak. 2004. A TAG-
based noisy channel model of speech repairs. In
Proceedings of the 42nd Annual Meeting on Asso-
ciation for Computational Linguistics, pages 33?39,
Barcelona. Association for Computational Linguis-
tics.
Frank Keller. 2004. The entropy rate principle as a
predictor of processing effort: An evaluation against
eye-tracking data. In EMNLP, pages 317?324.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for m-gram language modeling.
In Acoustics, Speech, and Signal Processing, 1995.
ICASSP-95., 1995 International Conference on, vol-
ume 1, pages 181?184. IEEE.
Matthew Lease, Mark Johnson, and Eugene Charniak.
2006. Recognizing disfluencies in conversational
speech. Audio, Speech, and Language Processing,
IEEE Transactions on, 14(5):1566?1573.
W.J.M. Levelt. 1983. Monitoring and self-repair in
speech. Cognition, 14(1):41?104.
M. Meteer, A. Taylor, R. MacIntyre, and R. Iyer. 1995.
Disfluency annotation stylebook for the switchboard
corpus. ms. Technical report, Department of Com-
puter and Information Science, University of Penn-
sylvania.
Tim Miller and William Schuler. 2008. A syntactic
time-series model for parsing fluent and disfluent
speech. In Proceedings of the 22nd International
Conference on Computational Linguistics-Volume 1,
pages 569?576. Association for Computational Lin-
guistics.
David Milward. 1991. Axiomatic Grammar, Non-
Constituent Coordination and Incremental Interpre-
tation. Ph.D. thesis, University of Cambridge.
Xian Qian and Yang Liu. 2013. Disfluency detection
using multi-step stacked learning. In Proceedings of
NAACL-HLT, pages 820?825.
Mohammad Sadegh Rasooli and Joel Tetreault. 2014.
Non-monotonic parsing of fluent umm I mean dis-
fluent sentences. EACL 2014, pages 48?53.
Hannes Rieser and David Schlangen. 2011. Introduc-
tion to the special issue on incremental processing in
dialogue. Dialogue & Discourse, 2(1):1?10.
88
Brian Roark, Asaf Bachrach, Carlos Cardenas, and
Christophe Pallier. 2009. Deriving lexical and
syntactic expectation-based measures for psycholin-
guistic modeling via incremental top-down parsing.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
1-Volume 1, pages 324?333. Association for Com-
putational Linguistics.
Claude E. Shannon. 1948. A mathematical theory
of communication. technical journal. AT & T Bell
Labs.
Elizabeth Shriberg and Andreas Stolcke. 1998. How
far do speakers back up in repairs? A quantitative
model. In Proceedings of the International Confer-
ence on Spoken Language Processing, pages 2183?
2186.
Elizabeth Shriberg. 1994. Preliminaries to a Theory
of Speech Disfluencies. Ph.D. thesis, University of
California, Berkeley.
Simon Zwarts and Mark Johnson. 2011. The impact of
language models and loss functions on repair disflu-
ency detection. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies - Volume
1, HLT ?11, pages 703?711, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Simon Zwarts, Mark Johnson, and Robert Dale. 2010.
Detecting speech repairs incrementally using a noisy
channel approach. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics,
COLING ?10, pages 1371?1378, Stroudsburg, PA,
USA. Association for Computational Linguistics.
89
Incremental Semantic Construction in a Dialogue System?
Matthew Purver, Arash Eshghi, Julian Hough
Interaction, Media and Communication
School of Electronic Engineering and Computer Science, Queen Mary University of London
{mpurver, arash, jhough}@eecs.qmul.ac.uk
Abstract
This paper describes recent work on the DynDial project? towards incremental semantic inter-
pretation in dialogue. We outline our domain-general grammar-based approach, using a variant of
Dynamic Syntax integrated with Type Theory with Records and a Davidsonian event-based seman-
tics. We describe a Java-based implementation of the parser, used within the Jindigo framework to
produce an incremental dialogue system capable of handling inherently incremental phenomena such
as split utterances, adjuncts, and mid-sentence clarification requests or backchannels.
1 Introduction
Many dialogue phenomena seem to motivate an incremental view of language processing: for example,
a participant?s ability to change hearer/speaker role mid-sentence to produce or interpret backchannels,
or complete or continue an utterance (see e.g. Yngve, 1970; Lerner, 2004, amongst many others). Much
recent research in dialogue systems has pursued this line, resulting in frameworks for incremental di-
alogue processing (Schlangen and Skantze, 2009) and systems capable of mid-utterance backchannels
(Skantze and Schlangen, 2009) or utterance completions (DeVault et al, 2009; Bu? et al, 2010).
However, to date there has been little focus on semantics, with the systems produced either operating
in domains in which semantic representation is not required (Skantze and Schlangen, 2009), or using
variants of domain-specific canned lexical or phrasal matching (Bu? et al, 2010). Our intention is to
extend this work to finer-grained and more domain-general notions of grammar and semantics, by using
an incremental grammatical framework, Dynamic Syntax (DS, Kempson et al, 2001) together with the
structured semantic representation provided by Type Theory with Records (TTR, see e.g. Cooper, 2005).
(a)
A: I want to go to . . .
B: Uh-huh
A: . . . Paris by train.
(b)
A: I want to go to Paris . . .
B: Uh-huh
A: . . . by train.
(c)
A: I want to go to Paris.
B: OK. When do you . . .
A: By train.
Figure 1: Examples of motivating incremental dialogue phenomena
One aim is to deal with split utterances, both when the antecedent is inherently incomplete (see Fig-
ure 1(a)) and potentially complete (even if not intended as such ? Figure 1(b)). This involves producing
representations which are as complete as possible ? i.e contain all structural and semantic information
so far conveyed ? on a word-by-word basis, so that in the event of an interruption or a hesitation, the
system can act accordingly (by producing backchannels or contentful responses as above); but that can
be further incremented in the event of a continuation by the user.
Importantly, this ability should be available not only when an initial contribution is intended and/or
treated as incomplete (as in Figure 1(b)), but also when it is in fact complete, but is still subsequently
extended (Figure 1(c)). Treating A?s two utterances as distinct, with separate semantic representations,
must require high-level processes of ellipsis reconstruction to interpret the final fragment ? for example,
treating it as the answer to an implicit question raised by A?s initial sentence (Ferna?ndez et al, 2004). If,
?The authors were supported by the Dynamics of Conversational Dialogue project (DynDial ? ESRC-RES-062-23-0962).
We thank Shalom Lappin, Tim Fernando, Yo Sato, our project colleagues and the anonymous reviewers for helpful comments.
365
instead, we can treat such fragments as continuations which merely add directly to the existing represen-
tation, the task is made easier and the relevance of the two utterances to each other becomes explicit.
2 Dynamic Syntax (DS) and Type Theory with Records (TTR)
Our approach is a grammar-based one, as our interest is in using domain-general techniques that are
capable of fine-grained semantic representation. Dynamic Syntax (DS) provides an inherently incre-
mental grammatical framework which dispenses with an independent level of syntax, instead expressing
grammaticality via constraints on the word-by-word monotonic growth of semantic structures. In DS?s
original form, these structures are trees with nodes corresponding to terms in the lambda calculus; nodes
are decorated with labels expressing their semantic type and formula, and beta-reduction determines the
type and formula at a mother node from those at its daughters (Figure 2(a)). Trees can be partial, with
nodes decorated with requirements for future development; lexical actions (corresponding to words) and
computational actions (general capabilities) are defined as operations on trees which satisfy and/or add
requirements; and grammaticality of a word sequence is then defined as satisfaction of all requirements
(tree completeness) via the application of its associated actions ? see Kempson et al (2001) for details.
Previous work in DS has shown how this allows a treatment of split utterances and non-sentential
fragments (e.g. clarifications) as extensions of the semantic trees so far constructed, either directly or via
the addition of ?linked? trees (Purver and Kempson, 2004; Gargett et al, 2009).
(a) Ty(t),arrive(john)
Ty(e),
john
Ty(e ? t),
?x.arrive(x)
(b)
[
x=john : e
p=arrive(x) : t
]
[
x=john : e
]
?r :
[
x : e
]
[
x=r.x : e
p=arrive(x) : t
]
(c)
?
?
e=now : es
x=john : e
p=arrive(e,x) : t
?
?
[
x=john : e
]
?r :
[
x : e
]
?
?
e=now : es
x=r.x : e
p=arrive(e,x) : t
?
?
Figure 2: A simple DS tree for ?john arrives?: (a) original DS, (b) DS+TTR, (c) event-based
2.1 Extensions
More recent work in DS has started to explore the use of TTR to extend the formalism, replacing the
atomic semantic type and FOL formula node labels with more complex record types, and thus providing
a more structured semantic representation. Purver et al (2010) provide a sketch of one way to achieve
this and explain how it can be used to incorporate pragmatic information such as participant reference
and illocutionary force. As shown in Figure 2(b) above, we use a slightly different variant here: node
record types are sequences of typed labels (e.g. [x : e] for a label x of type e), with semantic content
expressed by use of manifest types (e.g. [x=john : e] where john is a singleton subtype of e).
We further adopt an event-based semantics along Davidsonian lines (Davidson, 1980). As shown
in Figure 2(c), we include an event term (of type es) in the representation: this allows tense and aspect
to be expressed (although Figure 2(c) shows only a simplified version using the current time now).
It also permits a straightforward analysis of optional adjuncts as extensions of an existing semantic
representation; extensions which predicate over the event term already in the representation. Adding
fields to a record type results in a more fully specified record type which is still a subtype of the original:
?
?
?
e=now : es
x=john : e
p=arrive(e,x) : t
?
?
?
7?
?
?
?
?
?
e=now : es
x=john : e
p=arrive(e,x) : t
p?=today(e) : t
?
?
?
?
?
?john arrives? 7? ?john arrives today?
Figure 3: Optional adjuncts as leading to TTR subtypes
366
3 Implementation
The resulting framework has been implemented in Java, following the formal details of DS as per (Kemp-
son et al, 2001; Cann et al, 2005, inter alia). This implementation, DyLan,1 includes a parser and gener-
ator for English, which take as input a set of computational actions, a lexicon and a set of lexical actions
(instructions for partial tree update); these are specified separately in text files in the IF-THEN-ELSE
procedural (meta-)language of DS, allowing any pre-written grammar to be loaded. Widening or chang-
ing its coverage, i.e. extending the system with new analyses of various linguistic phenomena, thus do
not involve modification or extension of the Java program, but only the lexicon and action specifications.
The current coverage includes a small lexicon, but a broad range of structures: complementation, relative
clauses, adjuncts, tense, pronominal and ellipsis construal, all in interaction with quantification.
3.1 The parsing process
Given a sequence of words (w1, w2, ..., wn), the parser starts from the axiom tree T0 (a requirement
to construct a complete tree of type t), and applies the corresponding lexical actions (a1, a2, . . . , an),
optionally interspersing general computational actions (which can apply whenever their preconditions
are met). More precisely: we define the parser state at step i as a set of partial trees Si. Beginning with
the singleton axiom state S0 = {T0}, for each word wi:
1. Apply all lexical actions ai corresponding to wi to each partial tree in Si?1. For each application
that succeeds (i.e. the tree satisfies the action preconditions), add resulting (partial) tree to Si.
2. For each tree in Si, apply all possible sequences of computational actions and add the result to Si.
If at any stage the state Si is empty, the parse has failed and the string is deemed ungrammatical. If the
final state Sn contains a complete tree (all requirements satisfied), the string is grammatical and its root
node will provide the full sentence semantics; partial trees provide only partial semantic specifications.2
3.2 Graph representations
Sato (2010) shows how this procedure can be modelled as a directed acyclic graph, rooted at T0, with
individual partial trees as nodes, connected by edges representing single actions. While Sato uses this to
model the search process, we exploit it (in a slightly modified form) to represent the linguistic context
available during the parse ? important in DS for ellipsis and pronominal construal. Details are given in
(Cann et al, 2007; Gargett et al, 2009), but three general mechanisms are available: 1) copying formulae
from some tree in context (used for e.g. anaphora and strict VP ellipsis); 2) rerunning actions in context
(for e.g. sloppy VP-ellipsis and fragment corrections); and 3) directly extending/augmenting the current
tree (used for most fragment types in (Ferna?ndez, 2006)). For any partial tree, then, the context available
to the parser must include not only the tree itself, but the sequence of actions and previous partial trees
which have gone into its construction. The parse graph (which we call the tree graph) provides exactly
this information, via the shortest path back to the root from the current node.
However, we can also take a coarser-grained view via a graph which we term the state graph; here,
nodes are states Si and edges the sets of action sequences connecting them. This subsumes the tree graph,
with state nodes containing possibly many tree-graph nodes; and here, nodes have multiple outgoing
edges only when multiple word hypotheses are present. This corresponds directly to the input word graph
(often called a word lattice) available from a speech recognizer, allowing close integration in a dialogue
system ? see below. We also see this as a suitable structure with which to begin to model phenomena
such as hesitation and self-repair: as edges are linear action sequences, intended to correspond to the
time-linear psycholinguistic processing steps involved, such phenomena may be analysed as building
further edges from suitable departure points earlier in the graph.3
1DyLan is short for Dynamics of Language. Available from http://dylan.sourceforge.net/.
2Note that only a subset of possible computational actions can apply to any given tree; together with a set of heuristics on
possible application order, and the merging of identical trees produced by different sequences, this helps reduce complexity.
3There are similarities to chart parsing here: the tree graph edges spanning a state graph edge could be seen as corresponding
to chart edges spanning a substring, with the tree nodes in the state Si as the agenda. However, the lack of a notion of syntactic
constituency means no direct equivalent for the active/passive edge distinction; a detailed comparison is still to be carried out.
367
4 Dialogue System
The DyLan parser has now been integrated into a working dialogue system by implementation as an
Interpreter module in the Java-based incremental dialogue framework Jindigo (Skantze and Hjal-
marsson, 2010). Jindigo follows Schlangen and Skantze (2009)?s abstract architecture specification and
is specifically designed to handle units smaller than fully sentential utterances; one of its specific imple-
mentations is a travel agent system, and our module integrates semantic interpretation into this.
As set out by Schlangen and Skantze (2009)?s specification, our Interpreter?s essential compo-
nents are a left buffer (LB), processor and right buffer (RB). Incremental units (IUs) of various types are
posted from the RB of one module to the LB of another; for our module, the LB-IUs are ASR word hy-
potheses, and after processing, domain-level concept frames are posted as RB-IUs for further processing
by a downstream dialogue manager. The input IUs are provided as updates to a word lattice, and new
edges are passed to the DyLan parser which produces a state graph as described above in 3.1 and 3.2:
new nodes are new possible parse states, with new edges the sets of DS actions which have created them.
These state nodes are then used to create Jindigo domain concept frames by matching against the TTR
record types available (see below), and these are posted to the RB as updates to the state graph (lattice
updates in Jindigo?s terminology).
Crucial in Schlangen and Skantze (2009)?s model is the notion of commitment: IUs are hypotheses
which can be revoked at any time until they are committed by the module which produces them. Our
module hypothesizes both parse states and associated domain concepts (although only the latter are
outputs); these are committed when their originating word hypotheses are committed (by ASR) and a
type-complete subtree is available; other strategies are possible and are being investigated.
4.1 Mapping TTR record types to domain concepts incrementally
Our Interpreter module matches TTR record types to domain concept frames via a simple XML
matching specification; TTR fields map to particular concepts in the domain depending on their se-
mantic type (e.g. go events map to Trip concepts, and the entity of manifest type paris maps to the
City[paris] concept). As the tree and parse state graphs are maintained, incremental sub-sentential
extensions can produce TTR subtypes and lead to enrichment of the associated domain concept.
User: I want to go to Paris . . .
?
?
?
?
?
?
?
?
?
?
?
?
e=,e17,P resentState : es
e1=,e19,FutureAccomp : es
x1=Paris : e
p2=to(e1,x1) : t
x=speaker : e
p1=go(e1,x) : t
p?=want(e,x,p1) : t
?
?
?
?
?
?
?
?
?
?
?
?
Trip(to : City[paris])
User: . . . from London
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
e=,e17,P resentState : es
e1=,e19,FutureAccomp : es
x1=Paris : e
p2=to(e1,x1) : t
x2=London : e
p3=from(e1,x2) : t
x=speaker : e
p1=go(e1,x) : t
p?=want(e,x,p1) : t
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Trip(from : City[london],
to : City[paris])
Figure 4: Incremental construction of a TTR record type over two turns
Figure 4 illustrates this process for a user continuation; the initial user utterance is parsed to produce
a TTR record type, with a corresponding domain concept ? a valid incremental unit to post in the RB.
The subsequent user continuation ?from London? extends the parser state graph, producing a new TTR
subtype (in this case via the DS apparatus of an adjoining linked tree (Cann et al, 2005)), and a more
368
fully specified concept (with a further argument slot filled) as output.
System behaviour between these two user contributions will depend on the committed status of the
input, and perhaps some independent prosody-based judgement of whether a turn is finished (Skantze
and Schlangen, 2009). An uncommitted input might be responded to with a backchannel (Yngve, 1970);
commitment might lead to the system beginning processing and starting to respond more substantively.
However, in either case, the maintenance of the parse state graph allows the user continuation to be
treated as extending a parse tree, subtyping the TTR record type, and finally mapping to a fully satisfied
domain concept frame that can be committed.
5 Conclusions
We have implemented an extension of the Dynamic Syntax framework, integrated with Type Theory with
Records, which provides structured semantic representations suitable for use in a dialogue system, and
which does so incrementally, producing well-defined partial representations on a word-by-word basis.
This has been integrated into a working Jindigo dialogue system, capable of incremental behaviour such
as mid-sentence backchannels and utterance continuations, which will be demonstrated at the conference.
The coverage of the parser is currently limited, but work is in progress to widen it; the possibility of using
grammar induction to learn lexical actions from real corpora is also being considered for future projects.
We are also actively pursuing possbilities for tighter integration of TTR and DS, with the aim of unifying
syntactic and semantic incremental construction.
References
Bu?, O., T. Baumann, and D. Schlangen (2010). Collaborating on utterances with a spoken dialogue system using
an ISU-based approach to incremental dialogue management. In Proceedings of the SIGDIAL 2010 Conference.
Cann, R., R. Kempson, and L. Marten (2005). The Dynamics of Language. Oxford: Elsevier.
Cann, R., R. Kempson, and M. Purver (2007). Context and well-formedness: the dynamics of ellipsis. Research
on Language and Computation 5(3), 333?358.
Cooper, R. (2005). Records and record types in semantic theory. Journal of Logic and Computation 15(2), 99?112.
Davidson, D. (1980). Essays on Actions and Events. Oxford, UK: Clarendon Press.
DeVault, D., K. Sagae, and D. Traum (2009). Can I finish? learning when to respond to incremental interpretation
results in interactive dialogue. In Proceedings of the SIGDIAL 2009 Conference.
Ferna?ndez, R. (2006). Non-Sentential Utterances in Dialogue: Classification, Resolution and Use. Ph. D. thesis,
King?s College London, University of London.
Ferna?ndez, R., J. Ginzburg, H. Gregory, and S. Lappin (2004). SHARDS: Fragment resolution in dialogue. In
H. Bunt and R. Muskens (Eds.), Computing Meaning, Volume 3. Kluwer Academic Publishers. To appear.
Gargett, A., E. Gregoromichelaki, R. Kempson, M. Purver, and Y. Sato (2009). Grammar resources for modelling
dialogue dynamically. Cognitive Neurodynamics 3(4), 347?363.
Kempson, R., W. Meyer-Viol, and D. Gabbay (2001). Dynamic Syntax: The Flow of Language Understanding.
Blackwell.
Lerner, G. H. (2004). Collaborative turn sequences. In Conversation analysis: Studies from the first generation,
pp. 225?256. John Benjamins.
Purver, M., E. Gregoromichelaki, W. Meyer-Viol, and R. Cann (2010). Splitting the ?I?s and crossing the ?You?s:
Context, speech acts and grammar. In Aspects of Semantics and Pragmatics of Dialogue. SemDial 2010, 14th
Workshop on the Semantics and Pragmatics of Dialogue.
Purver, M. and R. Kempson (2004). Incremental context-based generation for dialogue. In Proceedings of the 3rd
International Conference on Natural Language Generation (INLG04).
Sato, Y. (2010). Local ambiguity, search strategies and parsing in Dynamic Syntax. In E. Gregoromichelaki,
R. Kempson, and C. Howes (Eds.), The Dynamics of Lexical Interfaces. CSLI. to appear.
Schlangen, D. and G. Skantze (2009). A general, abstract model of incremental dialogue processing. In Proceed-
ings of the 12th Conference of the European Chapter of the ACL (EACL 2009).
Skantze, G. and A. Hjalmarsson (2010). Towards incremental speech generation in dialogue systems. In Proceed-
ings of the SIGDIAL 2010 Conference.
Skantze, G. and D. Schlangen (2009). Incremental dialogue processing in a micro-domain. In Proceedings of the
12th Conference of the European Chapter of the ACL (EACL 2009).
Yngve, V. H. (1970). On getting a word in edgewise. In Papers from the 6th regional meeting of the Chicago
Linguistic Society.
369
Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 94?103,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Incremental Grammar Induction from Child-Directed
Dialogue Utterances?
Arash Eshghi
Interaction Lab
Heriot-Watt University
Edinburgh, United Kingdom
eshghi.a@gmail.com
Julian Hough and Matthew Purver
Cognitive Science Research Group
Queen Mary University of London
London, United Kingdom
{julian.hough, mpurver}@eecs.qmul.ac.uk
Abstract
We describe a method for learning an in-
cremental semantic grammar from data in
which utterances are paired with logical
forms representing their meaning. Work-
ing in an inherently incremental frame-
work, Dynamic Syntax, we show how
words can be associated with probabilistic
procedures for the incremental projection
of meaning, providing a grammar which
can be used directly in incremental prob-
abilistic parsing and generation. We test
this on child-directed utterances from the
CHILDES corpus, and show that it results
in good coverage and semantic accuracy,
without requiring annotation at the word
level or any independent notion of syntax.
1 Introduction
Human language processing has long been
thought to function incrementally, both in pars-
ing and production (Crocker et al, 2000; Fer-
reira, 1996). This incrementality gives rise to
many characteristic phenomena in conversational
dialogue, including unfinished utterances, inter-
ruptions and compound contributions constructed
by more than one participant, which pose prob-
lems for standard grammar formalisms (Howes et
al., 2012). In particular, examples such as (1) sug-
gest that a suitable formalism would be one which
defines grammaticality not in terms of licensing
strings, but in terms of constraints on the semantic
construction process, and which ensures this pro-
cess is common between parsing and generation.
(1) A: I burnt the toast.
? We are grateful to Ruth Kempson for her support and
helpful discussions throughout this work. We also thank
the CMCL?2013 anonymous reviewers for their constructive
criticism. This work was supported by the EPSRC, RISER
project (Ref: EP/J010383/1), and in part by the EU, FP7
project, SpaceBook (Grant agreement no: 270019).
B: But did you burn . . .
A: Myself? Fortunately not.
[where ?did you burn myself?? if uttered by
the same speaker is ungrammatical]
One such formalism is Dynamic Syntax (DS)
(Kempson et al, 2001; Cann et al, 2005); it
recognises no intermediate layer of syntax, but
instead reflects grammatical constraints via con-
straints on the word-by-word incremental con-
struction of meaning, underpinned by attendant
concepts of underspecification and update.
Eshghi et al (2013) describe a method for in-
ducing a probabilistic DS lexicon from sentences
paired with DS semantic trees (see below) repre-
senting not only their meaning, but their function-
argument structure with fine-grained typing infor-
mation. They apply their method only to an ar-
tificial corpus generated using a known lexicon.
Here, we build on that work to induce a lexi-
con from real child-directed utterances paired with
less structured Logical Forms in the form of TTR
Record Types (Cooper, 2005), thus providing less
supervision. By assuming only the availability of a
small set of general compositional semantic opera-
tions, reflecting the properties of the lambda calcu-
lus and the logic of finite trees, we ensure that the
lexical entries learnt include the grammatical con-
straints and corresponding compositional seman-
tic structure of the language. Our method exhibits
incrementality in two senses: incremental learn-
ing, with the grammar being extended and refined
as each new sentence becomes available; resulting
in an inherently incremental, probabilistic gram-
mar for parsing and production, suitable for use
in state-of-the-art incremental dialogue systems
(Purver et al, 2011) and for modelling human-
human dialogue.
94
?Ty(t)
?Ty(e),
?
?Ty(e ? t)
??
?john?
?Ty(t)
Ty(e),
john
?Ty(e ? t),
?
??
?upset?
?Ty(t)
Ty(e),
john ?Ty(e ? t)
?Ty(e),
?
Ty(e ? (e ? t)),
?y?x.upset?(x)(y)
??
?mary?
Ty(t),?,
upset?(john?)(mary?)
Ty(e),
john
Ty(e ? t),
?x.upset?(x)(mary?)
Ty(e),
mary?
Ty(e ? (e ? t)),
?y?x.upset?(x)(y)
Figure 1: Incremental parsing in DS producing semantic trees: ?John upset Mary?
2 Background
2.1 Grammar Induction and Semantics
We can view existing grammar induction meth-
ods along a spectrum from supervised to unsu-
pervised. Fully supervised methods take a parsed
corpus as input, pairing sentences with syntactic
trees and words with their syntactic categories, and
generalise over the phrase structure rules to learn
a grammar which can be applied to a new set of
data. Probabilities for production rules sharing a
LHS category can be estimated, producing a gram-
mar suitable for probabilistic parsing and disam-
biguation e.g. a PCFG (Charniak, 1996). While
such methods have shown great success, they pre-
suppose detailed prior linguistic information and
are thus inadequate as human grammar learning
models. Fully unsupervised methods, on the other
hand, proceed from unannotated raw data; they
are thus closer to the human language acquisition
setting, but have seen less success. In its pure
form ?positive data only, without bias? unsu-
pervised learning is computationally too complex
(?unlearnable?) in the worst case (Gold, 1967).
Successful approaches involve some prior learning
or bias (see (Clark and Lappin, 2011)) e.g. a set
of known lexical categories, a probability distri-
bution bias (Klein and Manning, 2005) or a semi-
supervised method with shallower (e.g. POS-tag)
annotation (Pereira and Schabes, 1992).
Another point on the spectrum is lightly su-
pervised learning: providing information which
constrains learning but with little or no lexico-
syntactic detail. One possibility is the use of se-
mantic annotation, using sentence-level proposi-
tional Logical Forms (LF). It seems more cogni-
tively plausible, as the learner can be said to be
able to understand, at least in part, the meaning
of what she hears from evidence gathered from
(1) her perception of her local, immediate environ-
ment given appropriate biases on different patterns
of individuation of entities and relationships be-
tween them, and (2) helpful interaction, and joint
focus of attention with an adult (see e.g. (Saxton,
1997)). Given this, the problem she is faced with
is one of separating out the contribution of each
individual linguistic token to the overall meaning
of an uttered linguistic expression (i.e. decompo-
sition), while maintaining and generalising over
several such hypotheses acquired through time as
she is exposed to more utterances involving each
token.
This has been successfully applied in Combi-
natorial Categorial Grammar (CCG) (Steedman,
2000), as it tightly couples compositional seman-
tics with syntax (Zettlemoyer and Collins, 2007;
Kwiatkowski et al, 2010; Kwiatkowski et al,
2012); as CCG is a lexicalist framework, grammar
learning involves inducing a lexicon assigning to
each word its syntactic and semantic contribution.
Moreover, the grammar is learnt incrementally, in
the sense that the learner collects data over time
and does the learning sentence by sentence.
Following this approach, Eshghi et al (2013)
outline a method for inducing a DS grammar
from semantic LFs. This brings an added di-
mension of incrementality: not only is learning
sentence-by-sentence incremental, but the gram-
mar learned is inherently word-by-word incre-
mental (see section 2.2 below). However, their
method requires a higher degree of supervision
than (Kwiatkowski et al, 2012): the LFs assumed
are not simply flat semantic formulae, but full DS
semantic trees (see e.g. Fig. 1) containing infor-
mation about the function-argument structure re-
95
quired for their composition, in addition to fine
grained type and formula annotations. Further,
they test their method only on artificial data cre-
ated using a known, manually-specified DS gram-
mar. In contrast, in this paper we provide an
approach which can learn from LFs without any
compositional structure information, and test it on
real language data; thus providing the first prac-
tical learning system for an explicitly incremental
grammar that we are aware of.
2.2 Dynamic Syntax (DS)
Dynamic Syntax (Kempson et al, 2001; Cann et
al., 2005) is a parsing-directed grammar formal-
ism, which models the word-by-word incremental
processing of linguistic input. Unlike many other
formalisms, DS models the incremental building
up of interpretations without presupposing or in-
deed recognising an independent level of syntactic
processing. Thus, the output for any given string
of words is a purely semantic tree representing
its predicate-argument structure; tree nodes cor-
respond to terms in the lambda calculus, deco-
rated with labels expressing their semantic type
(e.g. Ty(e)) and formula, with beta-reduction de-
termining the type and formula at a mother node
from those at its daughters (Figure 1).
These trees can be partial, containing unsatis-
fied requirements for node labels (e.g. ?Ty(e) is a
requirement for future development to Ty(e)), and
contain a pointer ? labelling the node currently
under development. Grammaticality is defined as
parsability: the successful incremental construc-
tion of a tree with no outstanding requirements (a
complete tree) using all information given by the
words in a sentence. The complete sentential LF
is then the formula decorating the root node ? see
Figure 1. Note that in these trees, leaf nodes do
not necessarily correspond to words, and may not
be in linear sentence order; syntactic structure is
not explicitly represented, only the structure of se-
mantic predicate-argument combination.
2.2.1 Actions in DS
The parsing process is defined in terms of condi-
tional actions: procedural specifications for mono-
tonic tree growth. These include general structure-
building principles (computational actions), puta-
tively independent of any particular natural lan-
guage, and language-specific actions associated
with particular lexical items (lexical actions). The
latter are what we learn from data here.
Computational actions These form a small,
fixed set, which we assume as given here. Some
merely encode the properties of the lambda cal-
culus and the logical tree formalism itself, LoFT
(Blackburn and Meyer-Viol, 1994) ? these we
term inferential actions. Examples include THIN-
NING (removal of satisfied requirements) and
ELIMINATION (beta-reduction of daughter nodes
at the mother). These actions are language-
independent, cause no ambiguity, and add no new
information to the tree; as such, they apply non-
optionally whenever their preconditions are met.
Other computational actions reflect the fun-
damental predictivity and dynamics of the DS
framework. For example, *-ADJUNCTION in-
troduces a single unfixed node with underspec-
ified tree position (replacing feature-passing or
type-raising concepts for e.g. long-distance depen-
dency); and LINK-ADJUNCTION builds a paired
(?linked?) tree corresponding to semantic con-
junction (licensing relative clauses, apposition and
more). These actions represent possible parsing
strategies and can apply optionally whenever their
preconditions are met. While largely language-
independent, some are specific to language type
(e.g. INTRODUCTION-PREDICTION in the form
used here applies only to SVO languages).
Lexical actions The lexicon associates words
with lexical actions; like computational actions,
these are sequences of tree-update actions in an
IF..THEN..ELSE format, and composed of ex-
plicitly procedural atomic tree-building actions
such as make (creates a new daughter node),
go (moves the pointer), and put (decorates the
pointed node with a label). Figure 2 shows an ex-
ample for a proper noun, John. The action checks
whether the pointed node (marked as ?) has a re-
quirement for type e; if so, it decorates it with type
e (thus satisfying the requirement), formula John?
and the bottom restriction ???? (meaning that the
node cannot have any daughters). Otherwise the
action aborts, i.e. the word ?John? cannot be parsed
in the context of the current tree.
Graph-based Parsing & Generation These ac-
tions define the parsing process. Given a sequence
of words (w1, w2, ..., wn), the parser starts from
the axiom tree T0 (a requirement to construct a
complete propositional tree, ?Ty(t)), and applies
the corresponding lexical actions (a1, a2, . . . , an),
optionally interspersing computational actions.
96
Action Input tree Output tree
John
IF ?Ty(e)
THEN put(Ty(e))
put(Fo(John?)
put(????)
ELSE ABORT
?Ty(t)
?Ty(e),
?
?Ty(e ? t)
?John??? ?Ty(t)
Ty(e), ?Ty(e)
John?, ????,?
?Ty(e ? t)
Figure 2: Lexical action for the word ?John?
T0
T1intro
T2pred
T3
link-adj
T4*-adj
T5
john
abort
T6
john
?john?
T7
thin
T8
comp
T9
pred
T10
link-adj
T11
thin
T12
comp
T13
likes
abort
abort
?likes?
Figure 3: DS parsing as a graph: actions (edges) are transitions between partial trees (nodes).
This parsing process can be modelled as a di-
rected acyclic graph (DAG) rooted at T0, with par-
tial trees as nodes, and computational and lexi-
cal actions as edges (i.e. transitions between trees)
(Sato, 2011). Figure 3 shows an example: here,
intro, pred and *adj correspond to the computa-
tional actions INTRODUCTION, PREDICTION and
*-ADJUNCTION respectively; and ?john? is a lex-
ical action. Different DAG paths represent dif-
ferent parsing strategies, which may succeed or
fail depending on how the utterance is continued.
Here, the path T0?T3 will succeed if ?John? is the
subject of an upcoming verb (?John upset Mary?);
T0 ? T4 will succeed if ?John? turns out to be a
left-dislocated object (?John, Mary upset?).
This incrementally constructed DAG makes up
the entire parse state at any point. The right-
most nodes (i.e. partial trees) make up the current
maximal semantic information; these nodes with
their paths back to the root (tree-transition actions)
make up the linguistic context for ellipsis and
pronominal construal (Purver et al, 2011). Given
a conditional probability distribution P (a|w, T )
over possible actions a given a word w and (some
set of features of) the current partial tree T , we can
parse probabilistically, constructing the DAG in a
best-first, breadth-first or beam parsing manner.
Generation uses exactly the same actions and
structures, and can be modelled on the same DAG
with the addition only of a goal tree; partial
trees are checked for subsumption of the goal
at each stage. The framework therefore inher-
ently provides both parsing and generation that
are word-by-word incremental and interchange-
able, commensurate with psycholinguistic results
(Lombardo and Sturt, 1997; Ferreira and Swets,
2002) and suitable for modelling dialogue (Howes
et al, 2012). While standard grammar formalisms
can of course also be used with incremental pars-
ing or generation algorithms (Hale, 2001; Collins
and Roark, 2004; Clark and Curran, 2007), their
string-based grammaticality and lack of inherent
parsing-generation interoperability means exam-
ples such as (1) remain problematic.
3 Method
Our task here is to learn an incremental DS gram-
mar; following Kwiatkowski et al (2012), we
assume as input a set of sentences paired with
their semantic LFs. Eshghi et al (2013) outline a
method for inducing DS grammars from semantic
DS trees (e.g. Fig. 1), in which possible lexical en-
tries are incrementally hypothesized, constrained
by subsumption of the target tree for the sentence.
Here, however, this structured tree information is
not available to us; our method must therefore con-
strain hypotheses via compatibility with the sen-
tential LF, represented as Record Types of Type
Theory with Records (TTR).
3.1 Type Theory with Records (TTR)
Type Theory with Records (TTR) is an exten-
sion of standard type theory shown useful in se-
mantics and dialogue modelling (Cooper, 2005;
Ginzburg, 2012). It is also used for representing
97
non-linguistic context such as the visual percep-
tion of objects (Dobnik et al, 2012), suggesting
potential for embodied learning in future work.
Some DS variants have incorporated TTR as the
semantic LF representation (Purver et al, 2011;
Hough and Purver, 2012; Eshghi et al, 2012).
Here, it can provide us with the mechanism we
need to constrain hypotheses in induction by re-
stricting them to those which lead to subtypes of
the known sentential LF.
In TTR, logical forms are specified as record
types (RTs), sequences of fields of the form [ l : T ]
containing a label l and a type T . RTs can be wit-
nessed (i.e. judged true) by records of that type,
where a record is a sequence of label-value pairs
[ l = v ], and [ l = v ] is of type [ l : T ] just in case
v is of type T .
R1 :
?
?
l1 : T1
l2=a : T2
l3=p(l2) : T3
?
? R2 :
[
l1 : T1
l2 : T2?
]
R3 : []
Figure 4: Example TTR record types
Fields can be manifest, i.e. given a singleton
type e.g. [ l : Ta ] where Ta is the type of which
only a is a member; here, we write this using the
syntactic sugar [ l=a : T ]. Fields can also be de-
pendent on fields preceding them (i.e. higher) in
the record type ? see R1 in Figure 4. Importantly
for us here, the standard subtyping relation ? can
be defined for record types: R1 ? R2 if for all
fields [ l : T2 ] in R2, R1 contains [ l : T1 ] where
T1 ? T2. In Figure 4, R1 ? R2 if T2 ? T2? , and
both R1 and R2 are subtypes of R3.
Following Purver et al (2011), we assume
that DS tree nodes are decorated not with simple
atomic formulae but with RTs, and correspond-
ing lambda abstracts representing functions from
RT to RT (e.g. ?r : [ l1 : T1 ].[ l2=r.l1 : T1 ] where
r.l1 is a path expression referring to the label l1
in r) ? see Figure 5. The equivalent of conjunc-
tion for linked trees is now RT extension (concate-
nation modulo relabelling ? see (Cooper, 2005;
Ferna?ndez, 2006)). TTR?s subtyping relation now
allows a record type at the root node to be in-
ferred for any partial tree, and incrementally fur-
ther specified via subtyping as parsing proceeds
(Hough and Purver, 2012).
We assume a field head in all record types, with
this corresponding to the DS tree node type. We
also assume a neo-Davidsonian representation of
?, T y(t),
?
?
?
x=john : e
e=arrive : es
p=subj(e,x) : t
head=p : t
?
?
?
Ty(e),
[
x=john : e
head=x : e
]
Ty(e ? t),
?r :
[
head : e
]
.
?
?
?
x=r.head : e
e=arrive : es
p=subj(e,x) : t
head=p : t
?
?
?
Figure 5: DS-TTR tree
predicates, with fields corresponding to the event
and to each semantic role; this allows all available
semantic information to be specified incrementally
via strict subtyping (e.g. providing the subj() field
when subject but not object has been parsed) ? see
Figure 5 for an example.
3.2 Problem Statement
Our induction procedure now assumes as input:
? a known set of DS computational actions.
? a set of training examples of the form
?Si, RTi?, where Si = ?w1 . . . wn? is a sen-
tence of the language and RTi ? henceforth
referred to as the target RT ? is the record
type representing the meaning of Si.
The output is a grammar specifying the possi-
ble lexical actions for each word in the corpus.
Given our data-driven approach, we take a prob-
abilistic view: we take this grammar as associat-
ing each word w with a probability distribution ?w
over lexical actions. In principle, for use in pars-
ing, this distribution should specify the posterior
probability p(a|w, T ) of using a particular action
a to parse a word w in the context of a particular
partial tree T . However, here we make the sim-
plifying assumption that actions are conditioned
solely on one feature of a tree, the semantic type
Ty of the currently pointed node; and that actions
apply exclusively to one such type (i.e. ambiguity
of type implies multiple actions). This simplifies
our problem to specifying the probability p(a|w).
In traditional DS terms, this is equivalent to as-
suming that all lexical actions have a simple IF
clause of the form IF ?Ty(X); this is true of
most lexical actions in existing DS grammars (see
Fig. 2), but not all. Our assumption may there-
fore lead to over-generation ? inducing actions
which can parse some ungrammatical strings ? we
must rely on the probabilities learned to make such
98
parses unlikely, and evaluate this in Section 4.
Given this, our focus here is on learning the THEN
clauses of lexical actions: sequences of DS atomic
actions such as go, make, and put (Fig. 2), but now
with attendant posterior probabilities. We will
henceforth refer to these sequences as lexical hy-
potheses. We first describe how we construct lexi-
cal hypotheses from individual training examples;
we then show how to generalise over these, while
incrementally estimating corresponding probabil-
ity distributions.
3.3 Hypothesis construction
DS is strictly monotonic: actions can only extend
the current (partial) tree Tcur, deleting nothing ex-
cept satisfied requirements. Thus, we can hypoth-
esise lexical actions by incrementally exploring
the space of all monotonic, well-formed exten-
sions T of Tcur, whose maximal semantics R is
a supertype of (extendible to) the target RT (i.e.
R ? RT ). This gives a bounded space described
by a DAG equivalent to that of section 2.2.1: nodes
are trees; edges are possible extensions; paths start
from Tcur and end at any tree with LF RT . Edges
may be either known computational actions or
new lexical hypotheses. The space is further con-
strained by the properties of the lambda-calculus
and the modal tree logic LoFT (not all possible
trees and extensions are well-formed).1
Hypothesising increments In purely semantic
terms, the hypothesis space at any point is the pos-
sible set of TTR increments from the current LF
R to the target RT . We can efficiently compute
and represent these possible increments using a
type lattice (see Figure 6),2 which can be con-
structed for the whole sentence before processing
each training example. Each edge is a RTR repre-
senting an increment from one RT, Rj , to another,
Rj+1, such that Rj ? RI = Rj+1 (where ? rep-
resents record type intersection (Cooper, 2005));
possible parse DAG paths must correspond to
some path through this lattice.
Hypothesising tree structure These DAG paths
can now be hypothesised with the lattice as a con-
straint: hypothesising possible sequences of ac-
1We also prevent arbitrary type-raising by restricting the
types allowed, taking the standard DS assumption that noun
phrases have semantic type e (rather than a higher type as in
Generalized Quantifier theory) and common nouns their own
type cn, see Cann et al (2005), chapter 3 for details.
2Clark (2011) similarly use a concept lattice relating
strings to their contexts in syntactic grammar induction.
Ri : []
R11 :
[
a : b
]
R12 :
[
c : d
]
R12 :
[
e : f
]
R21 :
[
a : b
c : d
]
R22 :
[
a : b
e : f
]
R22 :
[
c : d
e : f
]
RT :
?
?
a : b
c : d
e : f
?
?
Figure 6: RT extension hypothesis lattice
tions which extend the tree to produce the required
semantic increment, while the increments them-
selves constitute a search space of their own which
we explore by traversing the lattice.
The lexical hypotheses comprising these DAG
paths are divide into two general classes: (1) tree-
building hypotheses, which hypothesise appropri-
ately typed daughters to compose a given node;
and (2) content hypotheses, which decorate leaf
nodes with appropriate formulae from Ri (non-
leaf nodes then receive their content via beta-
reduction/extension of daughters).
Tree-building can be divided into two general
options: functional decomposition (corresponding
to the addition of daughter nodes with appropri-
ate types and formulae which will form a suitable
mother node by beta-reduction); and type exten-
sion (corresponding to the adjunction of a linked
tree whose LF will extend that of the current tree,
see Sec. 3.1 above). The availability of the former
is constrained by the presence of suitable depen-
dent types in the LF (e.g. in Fig. 5, p = subj(e, x)
depends on the fields with labels x and e, and
could therefore be hypothesised as the body of a
function with x and/or e as argument). The latter is
more generally available, but constrained by shar-
ing of a label between the resulting linked trees.
Figure 7 shows an example: a template for
functional decomposition hypotheses, extending a
node with some type requirement ?Ty(X) with
daughter nodes which can combine to satisfy that
requirement ? here, of types Y and Y ? X.
Specific instantiations are limited to a finite set of
types: e.g. X = e ? t and Y = e is allowed,
but higher types for Y are not. We implement
these constraints by packaging together permitted
sequences of tree updates as macros, and using
these macros to hypothesise DAG paths commen-
surate with the lattice.
Finally, semantic content decorations (as se-
99
IF ?Ty(X)
THEN make(??0?); go(??0?)
put(?Ty(Y )); go(???)
make(??1?); go(??1?)
put(?Ty(Y ? X)); go(?)
ELSE ABORT
Figure 7: Tree-building hypothesis
quences of put operations) are hypothesised for
the leaf nodes of the tree thus constructed; these
are now determined entirely by the tree structure
so far hypothesised and the target LF RT .
3.4 Probabilistic Grammar Estimation
This procedure produces, for each training sen-
tence ?w1 . . . wn?, all possible sequences of ac-
tions that lead from the axiom tree T0 to a tree
with the target RT as its semantics. These must
now be split into n sub-sequences, hypothesising
a set of word boundaries to form discrete word hy-
potheses; and a probability distribution estimated
over this (large) word hypothesis space to provide
a grammar that can be useful in parsing. For this,
we apply the procedure of Eshghi et al (2013).
For each training sentence S = ?w1 . . . wn?,
we have a set HT of possible Hypothesis Tuples
(sequences of word hypotheses), each of the form
HTj = ?hj1 . . . h
j
n?, where hji is the word hypoth-
esis for wi in HTj . We must estimate a prob-
ability distribution ?w over hypotheses for each
word w, where ?w(h) is the posterior probability
p(h|w) of a given word hypothesis h being used to
parse w. Eshghi et al (2013) define an incremen-
tal version of Expectation-Maximisation (Demp-
ster et al, 1977) for use in this setting.
Re-estimation At any point, the Expectation
step assigns each hypothesis tuple HTj a proba-
bility based on the current estimate ??w:
p(HTj|S) =
n
?
i=1
p(hji |wi) =
n
?
i=1
??wi(h
j
i ) (2)
The Maximisation step then re-estimates
p(h|w) as the normalised sum of the probabilities
of all observed tuples HTj which contain h,w:
???w(h) =
1
Z
?
{j|h,w?HTj}
n
?
i=1
??wi(h
j
i ) (3)
where Z is the appropriate normalising constant
summed over all the HTj?s.
Incremental update The estimate of ?w is now
updated incrementally at each training example:
the new estimate ?Nw is a weighted average of the
previous estimate ?N?1w and the new value from
the current example ???w from equation (3):
?Nw (h) =
N ? 1
N ?
N?1
w (h) +
1
N ?
??
w(h) (4)
?e.not(aux|do(v|have(pro|he, det|a(x,n|hat(x)), e), e), e)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
e=have : es
p3=not(e) : t
p2=do-aux(e) : t
r :
?
?
x : e
p=hat(x) : t
head=x : e
?
?
x2=?(r.head,r) : e
x1=he : e
p1=object(e,x2) : t
p=subject(e,x1) : t
head=e : es
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 8: Conversion of LFs from FOL to TTR.
For the first training example, a uniform distribu-
tion is assumed; when subsequent examples pro-
duce new previously unseen hypotheses these are
assigned probabilities uniformly distributed over a
held-out probability mass.
4 Experimental Setup
Corpus We tested our approach on a section
of the Eve corpus within CHILDES (MacWhin-
ney, 2000), a series of English child-directed ut-
terances, annotated with LFs by Kwiatkowski et
al. (2012) following Sagae et al (2004)?s syntactic
annotation. We convert these LFs into semanti-
cally equivalent RTs; e.g. Fig 8 shows the conver-
sion to a record type for ?He doesn?t have a hat?.
Importantly, our representations remove all
part-of-speech or syntactic information; e.g. the
subject, object and indirect object predicates func-
tion as purely semantic role information express-
ing an event?s participants. This includes e.g.
do-aux(e) in (8), which is taken merely to rep-
resent temporal/aspectual information about the
event, and could be part of any word hypothesis.
From this corpus we selected 500 short
utterance-record type pairs. The minimum utter-
ance length in this set is 1 word, maximum 7,
mean 3.7; it contains 1481 word tokens of 246
types, giving a type:token ratio of 6.0). We use the
first 400 for training and 100 for testing; the test
set alo has a mean utterance length of 3.7 words,
and contains only words seen in training.
Evaluation We evaluate our learner by compar-
ing the record type semantic LFs produced using
the induced lexicon against the gold standard LFs,
calculating precision, recall and f-score using a
method similar to Allen et al (2008).
100
Coverage % Precision Recall F-Score
Top-1 59 0.548 0.549 0.548
Top-2 85 0.786 0.782 0.782
Top-3 92 0.854 0.851 0.851
Table 1: Results: parse coverage & accuracy using
the top N hypotheses induced in training.
Each field has a potential score in the range
[0,1]. A method maxMapping(R1, R2) con-
structs a mapping from fields in R1 to those in R2
to maximise alignment, with fields that map com-
pletely scoring a full 1, and partially mapped fields
receiving less, depending on the proportion of the
R1 field?s representation that subsumes its mapped
R2 field;e.g. a unary predicate field in RT2 such
as
[
p=there(e) : t
]
could score a maximum of
3 - 1 for correct type t, 1 for correct predicate
there and 1 for the subsumption of its argument
e; we use the total to normalise the final score.
The potential maximum for any pair is therefore
the number of fields in R1 (including those in em-
bedded record types). So, for hypothesis H and
goal record type G, with NH and NG fields re-
spectively:
(5) precision = maxMapping(H,G)/NH
recall = maxMapping(H,G)/NG
5 Results
Table 1 shows that the grammar learned achieves
both good parsing coverage and semantic accu-
racy. Using the top 3 lexical hypotheses induced
from training, 92% of test set utterances receive a
parse, and average LF f-score reaches 0.851.
We manually inspected the learned lexicon for
instances of ambiguous words to assess the sys-
tem?s ability to disambiguate (e.g. the word ??s?
(is) has three different senses in our corpus: (1)
auxiliary, e.g. ?the coffee?s coming?; (2) verb
predicating NP identity, e.g. ?that?s a girl?; and
(3) verb predicating location, e.g. ?where?s the
pencil?). From these the first two were in the top
3 hypotheses (probabilities p=0.227 and p=0.068).
For example, the lexical entry learned for (2) is
shown in Fig. 9.
However, less common words fared worse: e.g.
the double object verb ?put?, with only 3 tokens,
had no correct hypothesis in the top 5. Given suffi-
cient frequency and variation in the token distribu-
tions, our method appears successful in inducing
the correct incremental grammar. However, the
complexity of the search space also limits the pos-
sibility of learning from larger record types, as the
space of possible subtypes used for hypothesising
IF ?Ty(e ? t)
THEN make(??0?); go(??0?)
put(?Ty(e))
go(??0?)
make(??1?); go(??1?)
put(Ty(e ? (e ? t)))
put(Fo(
?r1 :
[
head : e
]
?r2 :
[
head : e
]
.
?
?
?
?
?
?
?
?
x1=r1.head : e
x2=r2.head : e
e=eq : es
p1=subj(e,x2) : t
p2=obj(e,x1) : t
head=e : t
?
?
?
?
?
?
?
?
))
put(????)
ELSE ABORT
Figure 9: Action learned for second sense of ?is?
tree structure grows exponentially with the num-
ber of fields in the type. Therefore, when learning
from longer, more complicated sentences, we may
need to bring in further sources of bias to constrain
our hypothesis process further (e.g. learning from
shorter sentences first).
6 Conclusions
We have outlined a novel method for the induc-
tion of a probabilistic grammar in an inherently in-
cremental and semantic formalism, Dynamic Syn-
tax, compatible with dialogue phenomena such
as compound contributions and with no indepen-
dent level of syntactic phrase structure. Assum-
ing only general compositional mechanisms, our
method learns from utterances paired with their
logical forms represented as TTR record types.
Evaluation on a portion of the CHILDES corpus
of child-directed dialogue utterances shows good
coverage and semantic accuracy, which lends sup-
port to viewing it as a plausible, yet idealised, lan-
guage acquisition model.
Future work planned includes refining the
method outlined above for learning from longer
utterances, and then from larger corpora e.g. the
Groningen Meaning Bank (Basile et al, 2012),
which includes more complex structures. This will
in turn enable progress towards large-scale incre-
mental semantic parsers and allow further investi-
gation into semantically driven language learning.
101
References
James F. Allen, Mary Swift, and Will de Beaumont.
2008. Deep Semantic Analysis of Text. In Johan
Bos and Rodolfo Delmonte, editors, Semantics in
Text Processing. STEP 2008 Conference Proceed-
ings, volume 1 of Research in Computational Se-
mantics, pages 343?354. College Publications.
Valerio Basile, Johan Bos, Kilian Evang, and Noortje
Venhuizen. 2012. Developing a large semantically
annotated corpus. In Proceedings of the Eight In-
ternational Conference on Language Resources and
Evaluation (LREC 2012), pages 3196?3200, Istan-
bul, Turkey.
Patrick Blackburn and Wilfried Meyer-Viol. 1994.
Linguistics, logic and finite trees. Logic Journal
of the Interest Group of Pure and Applied Logics,
2(1):3?29.
Ronnie Cann, Ruth Kempson, and Lutz Marten. 2005.
The Dynamics of Language. Elsevier, Oxford.
Eugene Charniak. 1996. Statistical Language Learn-
ing. MIT Press.
Stephen Clark and James Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493?552.
Alexander Clark and Shalom Lappin. 2011. Linguistic
Nativism and the Poverty of the Stimulus. Wiley-
Blackwell.
Alexander Clark. 2011. A learnable representation for
syntax using residuated lattices. In Philippe Groote,
Markus Egg, and Laura Kallmeyer, editors, Formal
Grammar, volume 5591 of Lecture Notes in Com-
puter Science, pages 183?198. Springer Berlin Hei-
delberg.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of the 42ndMeeting of the ACL, pages 111?118,
Barcelona.
Robin Cooper. 2005. Records and record types in se-
mantic theory. Journal of Logic and Computation,
15(2):99?112.
Matthew Crocker, Martin Pickering, and Charles
Clifton, editors. 2000. Architectures and Mecha-
nisms in Sentence Comprehension. Cambridge Uni-
versity Press.
A.P. Dempster, N.M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical So-
ciety. Series B (Methodological), 39(1):1?38.
Simon Dobnik, Robin Cooper, and Staffan Larsson.
2012. Modelling language, action, and perception in
type theory with records. In Proceedings of the 7th
International Workshop on Constraint Solving and
Language Processing (CSLP12), pages 51?63.
Arash Eshghi, Julian Hough, Matthew Purver, Ruth
Kempson, and Eleni Gregoromichelaki. 2012. Con-
versational interactions: Capturing dialogue dynam-
ics. In S. Larsson and L. Borin, editors, From Quan-
tification to Conversation: Festschrift for Robin
Cooper on the occasion of his 65th birthday, vol-
ume 19 of Tributes, pages 325?349. College Publi-
cations, London.
Arash Eshghi, Matthew Purver, and Julian Hough.
2013. Probabilistic induction for an incremental se-
mantic grammar. In Proceedings of the 10th In-
ternational Conference on Computational Seman-
tics (IWCS 2013) ? Long Papers, pages 107?118,
Potsdam, Germany, March. Association for Compu-
tational Linguistics.
Raquel Ferna?ndez. 2006. Non-Sentential Utterances
in Dialogue: Classification, Resolution and Use.
Ph.D. thesis, King?s College London, University of
London.
Fernanda Ferreira and Benjamin Swets. 2002. How
incremental is language production? evidence from
the production of utterances requiring the compu-
tation of arithmetic sums. Journal of Memory and
Language, 46:57?84.
Victor Ferreira. 1996. Is it better to give than to do-
nate? Syntactic flexibility in language production.
Journal of Memory and Language, 35:724?755.
Jonathan Ginzburg. 2012. The Interactive Stance:
Meaning for Conversation. Oxford University
Press.
E. Mark Gold. 1967. Language identification in the
limit. Information and Control, 10(5):447?474.
John Hale. 2001. A probabilistic Earley parser as
a psycholinguistic model. In Proceedings of the
2nd Conference of the North American Chapter of
the Association for Computational Linguistics, Pitts-
burgh, PA.
Julian Hough and Matthew Purver. 2012. Process-
ing self-repairs in an incremental type-theoretic di-
alogue system. In Proceedings of the 16th SemDial
Workshop on the Semantics and Pragmatics of Di-
alogue (SeineDial), pages 136?144, Paris, France,
September.
Christine Howes, Matthew Purver, Rose McCabe,
Patrick G. T. Healey, and Mary Lavelle. 2012.
Predicting adherence to treatment for schizophrenia
from dialogue transcripts. In Proceedings of the
13th Annual Meeting of the Special Interest Group
on Discourse and Dialogue (SIGDIAL 2012 Confer-
ence), pages 79?83, Seoul, South Korea, July. Asso-
ciation for Computational Linguistics.
Ruth Kempson,WilfriedMeyer-Viol, and Dov Gabbay.
2001. Dynamic Syntax: The Flow of Language Un-
derstanding. Blackwell.
102
Dan Klein and Christopher D. Manning. 2005. Nat-
ural language grammar induction with a genera-
tive constituent-context mode. Pattern Recognition,
38(9):1407?1419.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, andMark Steedman. 2010. Inducing probabilis-
tic CCG grammars from logical form with higher-
order unification. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1223?1233, Cambridge, MA, Oc-
tober. Association for Computational Linguistics.
Tom Kwiatkowski, Sharon Goldwater, Luke Zettle-
moyer, and Mark Steedman. 2012. A proba-
bilistic model of syntactic and semantic acquisition
from child-directed utterances and their meanings.
In Proceedings of the Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL).
Vincenzo Lombardo and Patrick Sturt. 1997. Incre-
mental processing and infinite local ambiguity. In
Proceedings of the 1997 Cognitive Science Confer-
ence.
Brian MacWhinney. 2000. The CHILDES Project:
Tools for Analyzing Talk. Lawrence Erlbaum As-
sociates, Mahwah, New Jersey, third edition.
Fernando Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed cor-
pora. In Proceedings of the 30th Annual Meeting
of the Association for Computational Linguistics,
pages 128?135, Newark, Delaware, USA, June. As-
sociation for Computational Linguistics.
Matthew Purver, Arash Eshghi, and Julian Hough.
2011. Incremental semantic construction in a di-
alogue system. In J. Bos and S. Pulman, editors,
Proceedings of the 9th International Conference on
Computational Semantics, pages 365?369, Oxford,
UK, January.
Kenji Sagae, Brian MacWhinney, and Alon Lavie.
2004. Adding syntactic annotations to transcripts of
parent-child dialogs. In Proceedings of the 4th In-
ternational Conference on Language Resources and
Evaluation (LREC), pages 1815?1818, Lisbon.
Yo Sato. 2011. Local ambiguity, search strate-
gies and parsing in Dynamic Syntax. In E. Gre-
goromichelaki, R. Kempson, and C. Howes, editors,
The Dynamics of Lexical Interfaces. CSLI Publica-
tions.
Matthew Saxton. 1997. The contrast theory of nega-
tive input. Journal of Child Language, 24(1):139?
161.
Mark Steedman. 2000. The Syntactic Process. MIT
Press, Cambridge, MA.
Luke Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed CCG grammars for parsing
to logical form. In Proceedings of the Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL).
103
Proceedings of the EACL 2014 Workshop on Type Theory and Natural Language Semantics (TTNLS), pages 80?88,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
Probabilistic Type Theory for Incremental Dialogue Processing
Julian Hough and Matthew Purver
Cognitive Science Research Group
School of Electronic Engineering and Computer Science
Queen Mary University of London
{j.hough,m.purver}@qmul.ac.uk
Abstract
We present an adaptation of recent work
on probabilistic Type Theory with Records
(Cooper et al., 2014) for the purposes of
modelling the incremental semantic pro-
cessing of dialogue participants. After
presenting the formalism and dialogue
framework, we show how probabilistic
TTR type judgements can be integrated
into the inference system of an incremen-
tal dialogue system, and discuss how this
could be used to guide parsing and dia-
logue management decisions.
1 Introduction
While classical type theory has been the predomi-
nant mathematical framework in natural language
semantics for many years (Montague, 1974, in-
ter alia), it is only recently that probabilistic type
theory has been discussed for this purpose. Sim-
ilarly, type-theoretic representations have been
used within dialogue models (Ginzburg, 2012);
and probabilistic modelling is common in dia-
logue systems (Williams and Young, 2007, inter
alia), but combinations of the two remain scarce.
Here, we attempt to make this connection, taking
(Cooper et al., 2014)?s probabilistic Type Theory
with Records (TTR) as our principal point of de-
parture, with the aim of modelling incremental in-
ference in dialogue.
To our knowledge there has been no practi-
cal integration of probabilistic type-theoretic in-
ference into a dialogue system so far; here we dis-
cuss computationally efficient methods for imple-
mentation in an extant incremental dialogue sys-
tem. This paper demonstrates their efficacy in sim-
ple referential communication domains, but we ar-
gue the methods could be extended to larger do-
mains and additionally used for on-line learning
in future work.
2 Previous Work
Type Theory with Records (TTR) (Betarte and
Tasistro, 1998; Cooper, 2005) is a rich type the-
ory which has become widely used in dialogue
models, including information state models for
a variety of phenomena such as clarification re-
quests (Ginzburg, 2012; Cooper, 2012) and non-
sentential fragments (Ferna?ndez, 2006). It has also
been shown to be useful for incremental semantic
parsing (Purver et al., 2011), incremental genera-
tion (Hough and Purver, 2012), and recently for
grammar induction (Eshghi et al., 2013).
While the technical details will be given in sec-
tion 3, the central judgement in type theory s ? T
(that a given object s is of type T ) is extended
in TTR so that s can be a (potentially complex)
record and T can be a record type ? e.g. s could
represent a dialogue gameboard state and T could
be a dialogue gameboard state type (Ginzburg,
2012; Cooper, 2012). As TTR is highly flexible
with a rich type system, variants have been con-
sidered with types corresponding to real-number-
valued perceptual judgements used in conjunction
with linguistic context, such as visual perceptual
information (Larsson, 2011; Dobnik et al., 2012),
demonstrating its potential for embodied learning
systems. The possibility of integration of per-
ceptron learning (Larsson, 2011) and naive Bayes
classifiers (Cooper et al., 2014) into TTR show
how linguistic processing and probabilistic con-
ceptual inference can be treated in a uniform way
within the same representation system.
Probabilistic TTR as described by Cooper et al.
(2014) replaces the categorical s ? T judgement
with the real number valued p(s ? T ) = v where
v ? [0,1]. The authors show how standard proba-
bility theoretic and Bayesian equations can be ap-
plied to TTR judgements and how an agent might
learn from experience in a simple classification
game. The agent is presented with instances of
80
a situation and it learns with each round by updat-
ing its set of probabilistic type judgements to best
predict the type of object in focus ? in this case
updating the probability judgement that something
is an apple given its observed colour and shape
p(s ? T
apple
? s ? T
Shp
, s ? T
Col
) where Shp ?{shp1, shp2} and Col ? {col1, col2}. From a
cognitive modelling perspective, these judgements
can be viewed as probabilistic perceptual informa-
tion derived from learning. We use similar meth-
ods in our toy domain, but show how prior judge-
ments could be constructed efficiently, and how
classifications can be made without exhaustive it-
eration through individual type classifiers.
There has also been significant experimental
work on simple referential communication games
in psycholinguistics, computational and formal
modelling. In terms of production and genera-
tion, Levelt (1989) discusses speaker strategies
for generating referring expressions in a simple
object naming game. He showed how speakers
use informationally redundant features of the ob-
jects, violating Grice?s Maxim of Quantity. In
natural language generation (NLG), referring ex-
pression generation (REG) has been widely stud-
ied (see (Krahmer and Van Deemter, 2012) for
a comprehensive survey). The incremental algo-
rithm (IA) (Dale and Reiter, 1995) is an iterative
feature selection procedure for descriptions of ob-
jects based on computing the distractor set of ref-
erents that each adjective in a referring expression
could cause to be inferred. More recently Frank
and Goodman (2012) present a Bayesian model
of optimising referring expressions based on sur-
prisal, the information-theoretic measure of how
much descriptions reduce uncertainty about their
intended referent, a measure which they claim cor-
relates strongly to human judgements.
The element of the referring expression do-
main we discuss here is incremental processing.
There is evidence from (Brennan and Schober,
2001)?s experiments that people reason at an in-
credibly time-critical level from linguistic infor-
mation. They demonstrated self-repair can speed
up semantic processing (or at least object refer-
ence) in such games, where an incorrect object
being partly vocalized and then repaired in the
instructions (e.g. ?the yell-, uh, purple square?)
yields quicker response times from the onset of
the target (?purple?) than in the case of the flu-
ent instructions (?the purple square?). This exam-
ple will be addressed in section 5. First we will
set out the framework in which we want to model
such processing.
3 Probabilistic TTR in an incremental
dialogue framework
In TTR (Cooper, 2005; Cooper, 2012), the princi-
pal logical form of interest is the record type (?RT?
from here), consisting of sequences of fields of the
form [ l ? T ] containing a label l and a type T .1
RTs can be witnessed (i.e. judged as inhabited)
by records of that type, where a record is a set of
label-value pairs [ l = v ]. The central type judge-
ment in TTR that a record s is of (record) type
R, i.e. s ? R, can be made from the component
type judgements of individual fields; e.g. the one-
field record [ l = v ] is of type [ l ? T ] just in case
v is of type T . This is generalisable to records and
RTs with multiple fields: a record s is of RT R if
s includes fields with labels matching those occur-
ring in the fields of R, such that all fields in R are
matched, and all matched fields in s must have a
value belonging to the type of the corresponding
field in R. Thus it is possible for s to have more
fields than R and for s ? R to still hold, but not
vice-versa: s ? R cannot hold if R has more fields
than s.
R
1
?
??????????
l
1
? T
1
l
2
? T
2
l
3
? T
3
(l
1
)
?????????? R2 ? [
l
1
? T
1
l
2
? T
2
?
] R
3
? []
Figure 1: Example TTR record types
Fields can have values representing predicate
types (ptypes), such as T
3
in Figure 1, and conse-
quently fields can be dependent on fields preced-
ing them (i.e. higher) in the RT, e.g. l
1
is bound in
the predicate type field l
3
, so l
3
depends on l
1
.
Subtypes, meets and joins A relation between
RTs we wish to explore is ? (?is a subtype of?),
which can be defined for RTs in terms of fields as
simply: R
1
? R
2
if for all fields [ l ? T
2
] in R
2
,
R
1
contains [ l ? T
1
] where T
1
? T
2
. In Figure 1,
both R
1
? R
3
and R
2
? R
3
; and R
1
? R
2
iff
T
2
? T
2
? . The transitive nature of this relation (if
R
1
? R
2
and R
2
? R
3
then R
1
? R
3
) can be used
effectively for type-theoretic inference.
1We only introduce the elements of TTR relevant to the
phenomena discussed below. See (Cooper, 2012) for a de-
tailed formal description.
81
We also assume the existence of manifest (sin-
gleton) types, e.g. T
a
, the type of which only a is
a member. Here, we write manifest RT fields such
as [ l ? T
a
] where T
a
? T using the syntactic sugar[ l=a ? T ]. The subtype relation effectively allows
progressive instantiation of fields (as addition of
fields to R leads to R? where R? ? R), which is
practically useful for an incremental dialogue sys-
tem as we will explain.
We can also define meet and join types of two
or more RTs. The representation of the meet type
of two RTs R
1
and R
2
is the result of a merge
operation (Larsson, 2010), which in simple terms
here can be seen as union of fields. A meet type
is also equivalent to the extraction of a maxi-
mal common subtype, an operation we will call
MaxSub(R
i
..R
n
):2
if R
1
= [ l1 ? T1
l
2
? T
2
] and R
2
= [ l2 ? T2
l
3
? T
3
]
R
1
?R
2
=
?????????
l
1
? T
1
l
2
? T
2
l
3
? T
3
?????????
= MaxSub(R
1
,R
2
)
R
1
and R
2
here are common supertypes of the
resulting R
1
? R
2
. On the other hand, the join of
two RTs R
1
and R
2
, the type R
1
? R
2
cannot be
represented by field intersection. It is defined in
terms of type checking, in that s ? R
1
? R
2
iff
s ? R
1
or s ? R
2
. It follows that if R
1
? R
2
then
s ? R
1
?R
2
iff s ? R
1
, and s ? R
1
? R
2
iff s ? R
2
.
While technically the maximally common su-
pertype of R
1
and R
2
is the join type R
1
? R
2
,
here we introduce the maximally common simple
(non disjunctive) supertype of two RTs R
1
and R
2
as field intersection:
if R
1
= [ l1 ? T1
l
2
? T
2
] and R
2
= [ l2 ? T2
l
3
? T
3
]
MaxSuper(R
1
, R
2
) = [ l
2
? T
2
]
We will explore the usefulness of this new op-
eration in terms of RT lattices in sec. 4.
3.1 Probabilistic TTR
We follow Cooper et al. (2014)?s recent extension
of TTR to include probabilistic type judgements of
the form p(s ? R) = v where v ? [0,1], i.e. the real
valued judgement that a record s is of RT R. Here
2Here we concern ourselves with simple examples that
avoid label-type clashes between two RTs (i.e. where R
1
con-
tains l
1
? T1 and R
2
contains l
1
? T2); in these cases the op-
erations are more complex than field concatenation/sharing.
we use probabilistic TTR to model a common psy-
cholinguistic experimental set up in section 5. We
repeat some of Cooper et al.?s calculations here
for exposition, but demonstrate efficient graphical
methods for generating and incrementally retriev-
ing probabilities in section 4.
Cooper et al. (2014) define the probability of the
meet and join types of two RTs as follows:
p(s ? R
1
?R
2
) = p(s ? R
1
)p(s ? R
2
? s ? R
1
)
p(s ? R
1
?R
2
) = p(s ? R
1
) + p(s ? R
2
) ? p(s ? R
1
?R
2
)
(1)
It is practically useful, as we will describe be-
low, that the join probability can be computed in
terms of the meet. Also, there are equivalences be-
tween meets, joins and subtypes in terms of type
judgements as described above, in that assuming
if R
1
? R
2
then p(s ? R
2
? s ? R
1
) = 1, we have:
if R
1
? R
2
p(s ? R
1
?R
2
) = p(s ? R
1
)
p(s ? R
1
?R
2
) = p(s ? R
2
)
p(s ? R
1
) ? p(s ? R
2
)
(2)
The conditional probability of a record being of
type R
2
given it is of type R
1
is:
p(s ? R
2
? s ? R
1
) = p(s ? R1 ? s ? R2)
p(s ? R
1
) (3)
We return to an explanation for these classical
probability equations holding within probabilistic
TTR in section 4.
Learning and storing probabilistic judgements
When dealing with referring expression games, or
indeed any language game, we need a way of stor-
ing perceptual experience. In probabilistic TTR
this can be achieved by positing a judgement set J
in which an agent stores probabilistic type judge-
ments.3 We refer to the sum of the value of proba-
bilistic judgements that a situation has been judged
to be of type R
i
within J as ?R
i
?
J
and the sum of
all probabilistic judgements in J simply as P (J );
thus the prior probability that anything is of type
R
i
under the set of judgements J is ?Ri?J
P (J ) . The
conditional probability p(s ? R
1
? s ? R
2
) un-
der J can be reformulated in terms of these sets
of judgements:
p
J
(s ? R
1
? s ? R
2
) = { ?R1?R2?J?R2?J iff ?R2?J ? 0
0 otherwise
(4)
3(Cooper et al., 2014) characterise a type judgement as an
Austinian proposition that a situation is of a given type with
a given probability, encoded in a TTR record.
82
where the sample spaces ?R
1
? R
2
?
J
and ?R
2
?
J
constitute the observations of the agent so far. J
can have new judgements added to it during learn-
ing. We return to this after introducing the incre-
mental semantics needed to interface therewith.
3.2 DS-TTR and the DyLan dialogue system
In order to permit type-theoretic inference in a
dialogue system, we need to provide suitable
TTR representations for utterances and the cur-
rent pragmatic situation from a parser, dialogue
manager and generator as instantaneously and ac-
curately as possible. For this purpose we use
an incremental framework DS-TTR (Eshghi et
al., 2013; Purver et al., 2011) which integrates
TTR representations with the inherently incre-
mental grammar formalism Dynamic Syntax (DS)
(Kempson et al., 2001).
?, T y(t),
????????????
x=john ? e
e=arrive ? es
p=subj(e,x) ? t
head=p ? t
????????????
Ty(e),
[ x=john ? e
head=x ? e ]
Ty(e ? t),
?r ? [ head ? e ] .????????????
x=r.head ? e
e=arrive ? es
p=subj(e,x) ? t
head=p ? t
????????????
Figure 2: DS-TTR tree
DS produces an incrementally specified, partial
logical tree as words are parsed/generated; follow-
ing Purver et al. (2011), DS tree nodes are dec-
orated not with simple atomic formulae but with
RTs, and corresponding lambda abstracts repre-
senting functions of type RT ? RT (e.g. ?r ?[ l
1
? T
1
].[ l
2=r.l
1
? T
1
] where r.l
1
is a path ex-
pression referring to the label l
1
in r) ? see Fig-
ure 2. Using the idea of manifestness of fields
as mentioned above, we have a natural represen-
tation for underspecification of leaf node content,
e.g. [x ? e ] is unmanifest whereas [x=john ? e ]4
is manifest and the latter is a subtype of the for-
mer. Functional application can apply incremen-
tally, allowing a RT at the root node to be com-
piled for any partial tree, which is incrementally
further specified as parsing proceeds (Hough and
Purver, 2012). Within a given parse path, due to
4This is syntactic sugar for [ x ? e
john
] and the = sign is
not the same semantically as that in a record.
DS-TTR?s monotonicity, each maximal RT of the
tree?s root node is a subtype of the parser?s previ-
ous maximal output.
Following (Eshghi et al., 2013), DS-TTR tree
nodes include a field head in all RTs which cor-
responds to the DS tree node type. We also as-
sume a neo-Davidsonian representation of predi-
cates, with fields corresponding to an event term
and to each semantic role; this allows all available
semantic information to be specified incrementally
in a strict subtyping relation (e.g. providing the
subj() field when subject but not object has been
parsed) ? see Figure 2.
We implement DS-TTR parsing and genera-
tion mechanisms in the DyLan dialogue system5
within Jindigo (Skantze and Hjalmarsson, 2010),
a Java-based implementation of the incremental
unit (IU) framework of (Schlangen and Skantze,
2009). In this framework, each module has input
and output IUs which can be added as edges be-
tween vertices in module buffer graphs, and be-
come committed should the appropriate condi-
tions be fulfilled, a notion which becomes im-
portant in light of hypothesis change and repair
situations. Dependency relations between differ-
ent graphs within and between modules can be
specified by groundedIn links (see (Schlangen and
Skantze, 2009) for details).
The DyLan interpreter module (Purver et al.,
2011) uses Sato (2011)?s insight that the context of
DS parsing can be characterized in terms of a Di-
rected Acyclic Graph (DAG) with trees for nodes
and DS actions for edges. The module?s state is
characterized by three linked graphs as shown in
Figure 3:
? input: a time-linear word graph posted by the
ASR module, consisting of word hypothesis
edge IUs between vertices W
n
? processing: the internal DS parsing DAG,
which adds parse state edge IUs between ver-
tices S
n
groundedIn the corresponding word
hypothesis edge IU
? output: a concept graph consisting of domain
concept IUs (RTs) as edges between vertices
C
n
, groundedIn the corresponding path in the
DS parsing DAG
Here, our interest is principally in the parser out-
put, to support incremental inference; a DS-TTR
generator is also included which uses RTs as goal
concepts (Hough and Purver, 2012) and uses the
5Available from http://dylan.sourceforge.net/
83
same parse graph as the interpreter to allow self-
monitoring and compound contributions, but we
omit the details here.
Figure 3: Normal incremental parsing in Dylan
4 Order theoretic and graphical methods
for probabilistic TTR
RT lattices to encode domain knowledge To
support efficient inference in DyLan, we represent
dialogue domain concepts via partially ordered
sets (posets) of RT judgements, following similar
insights used in inducing DS-TTR actions (Eshghi
et al., 2013). A poset has several advantages over
an unordered list of un-decomposed record types:
the possibility of incremental type-checking; in-
creased speed of type-checking, particularly for
pairs of/multiple type judgements; immediate use
of type judgements to guide system decisions; in-
ference from negation; and the inclusion of learn-
ing within a domain. We leave the final challenge
for future work, but discuss the others here.
We can construct a poset of type judgements
for any single RT by decomposing it into its con-
stituent supertype judgements in a record type lat-
tice. Representationally, as per set-theoretic lat-
tices, this can be visualised as a Hasse diagram
such as Figure 4, however here the ordering arrows
show ? (?subtype of?) relations from descendant to
ancestor nodes.
To characterize an RT lattice G ordered by ?,
we adapt Knuth (2005)?s description of lattices in
line with standard order theory: for a pair of RT
elements R
x
and R
y
, their lower bound is the set
of all R
z
? G such that R
z
? R
x
and R
z
? R
y
.
In the event that a unique greatest lower bound ex-
ists, this is their meet, which in G happily corre-
sponds to the TTR meet type R
x
? R
y
. Dually, if
their unique least upper bound exists, this is their
R
1200
= [] = ?
R
120
= [ a ? b ] R
121
= [ c ? d ] R
110
= [ e ? f ]
R
10
= [ a ? b
c ? d ] R11 = [ a ? be ? f ] R12 = [ c ? de ? f ]
R
1
=
?????????
a ? b
c ? d
e ? f
????????? = ?
Figure 4: Record Type lattice ordered by the sub-
type relation
join and in TTR terms is MaxSuper(R
x
, R
y
) but
not necessarily their join type R
x
? R
y
as here
we concern ourselves with simple RTs. One el-
ement covers another if it is a direct successor to
it in the subtype ordering relation hierarchy. G
has a greatest element (?) and least element (?),
with the atoms being the elements that cover ?;
in Figure 4 if R
1
is viewed as ? , the atoms are
R{10,11,12}. An RT element Rx has a comple-
ment if there is a unique element ?R
x
such that
MaxSuper(R
x
,?R
x
) = ? and R
x
? ?R
x
= ?
(the lattice in Figure 4 is complemented as this
holds for every element).
Graphically, the join of two elements can be
found by following the connecting edges upward
until they first converge on a single RT, giving us
MaxSuper(R
10
, R
12
) = R
121
in Figure 4, and the
meet can be found by following the lines down-
ward until they connect to give their meet type,
i.e. R
10
?R
12
= R
1
.
If we consider R
1
to be a domain concept in
a dialogue system, we can see how its RT lattice
G can be used for incremental inference. As in-
crementally specified RTs become available from
the interpreter they are matched to those in G to
determine how far down towards the final domain
concept R
1
our current state allows us to be. Dif-
ferent sequences of words/utterances lead to dif-
ferent paths. However, any practical dialogue sys-
tem must entertain more than one possible domain
concept as an outcome; G must therefore contain
multiple possible final concepts, constituting its
atoms, each with several possible dialogue move
sequences, which correspond to possible down-
ward paths ? e.g. see the structure of Figure 5.
Our aim here is to associate each RT in G with a
probabilistic judgement.
Initial lattice construction We define a simple
bottom-up procedure in Algorithm 1 to build a RT
84
lattice G of all possible simple domain RTs and
their prior probabilistic judgements, initialised by
the disjunction of possible final state judgements
(the priors),6 along with the absurdity ?, stipu-
lated a priori as the least element with probability
0 and the meet type of the atomic priors. The al-
gorithm recursively removes one field from the RT
being processed at a time (except fields referenced
in a remaining dependent ptype field), then orders
the new supertype RT in G appropriately.
Each node in G contains its RT R
i
and a sum
of probability judgements {?R
k
?
J
+ .. + ?R
n
?
J
}
corresponding to the probabilities of the priors it
stands in a supertype relation to. These sums are
propagated up from child to parent node as it is
constructed. It terminates when all simple maxi-
mal supertypes7 have been processed, leaving the
maximally common supertype as ? (possibly the
empty type [ ]), associated with the entire proba-
bility mass P (J ), which constitutes the denomina-
tor to all judgements- given this, only the numer-
ator of equation ?Ri?J
P (J ) needs to be stored at each
node.
Algorithm 1 Probabilistic TTR record type lattice
construction algorithm
INPUT: priors ? use the initial prior judgements for G?s atoms
OUTPUT: G
G = newGraph(priors) ? P(J) set to equal sum of prior probs
agenda = priors ? Initialise agenda
while not agenda is empty do
RT = agenda.pop()
for field ? RT do
if field ? RT.paths then ? Do not remove bound fields
continue
superRT = RT - field
if superRT ? G then ? not new? order w.r.t. RT and inherit RT?s priors
G.order(RT.address,G.getNode(superRT),?)
else ? new?
superNode = G.newNode(superRT) ? create new node w. empty priors
for node ? G do ? order superNode w.r.t. other nodes in G
if superRT.fields ? node.fields then
G.order(node,superNode,?) ? superNode inherits node?s priors
agenda.append(superRT) ? add to agenda for further supertyping
Direct inference from the lattice To explain
how our approach models incremental inference,
we assume Brennan and Schober (2001)?s experi-
mental referring game domain described in section
2: three distinct domain situation RTs R
1
, R
2
and
R
3
correspond to a purple square, a yellow square
and a yellow circle, respectively.
The RT lattice G constructed initially upon ob-
servation of the game (by instructor or instructee)
shown in Figure 5 uses a uniform distribution for
6Although the priors? disjunctive probability sums to 1 af-
ter G is constructed, i.e. in Figure 5 ?R1?J+?R2?J+?R3?J
P (J ) = 1,
the real values initially assigned to them need not sum to
unity, as they form the atoms of G (see (Knuth, 2005)).
7Note that it does not generate the join types but maximal
common supertypes defined by field intersection.
the three disjunctive final situations. Each node
shows an RT R
i
on the left and the derivation of
its prior probability p
J
(R
i
) that any game situa-
tion record will be of type R
i
on the right, purely
in terms of the relevant priors and the global de-
nominator P (J ).
G can be searched to make inferences in light
of partial information from an ongoing utterance.
We model inference as predicting the likelihood
of relevant type judgements R
y
? G of a situa-
tion s, given the judgement s ? R
x
we have so far.
To do this we use conditional probability judge-
ments following Knuth?s work on distributive lat-
tices, using the ? relation to give a choice function:
p
J
(s ? R
y
? s ? R
x
) =
???????????
1 if R
x
? R
y
0 if R
x
?R
y
= ?
p otherwise, where 0 ? p ? 1
(5)
The third case is the degree of inclusion of R
y
in R
x
, and can be calculated using the conditional
probability calculation (4) in sec. 3. For nega-
tive RTs, a lattice generated from Algorithm 1 will
be distributive but not guaranteed to be comple-
mented, however we can still derive p
J
(s ? R
y
?
s ? ?R
x
) by obtaining p
J
(s ? R
y
) in G modulo the
probability mass of R
x
and that of its subtypes:
p
J
(s ? R
y
? s ? ?R
x
) = {0 if Ry ? Rx
p
J
(s?R
y
)?p
J
(s?R
x
?R
y
)
p
J
(s??)?p
J
(s?R
x
) otherwise
(6)
The subtype relations and atomic, join and meet
types? probabilities required for (1) - (6) can be
calculated efficiently through graphical search al-
gorithms by characterising G as a DAG: the re-
verse direction of the subtype ordering edges can
be viewed as reachability edges, making ? the
source and ? the sink. With this characterisation,
if R
x
is reachable from R
y
then R
x
? R
y
.
In DAG terms, the probability of the meet of
two RTs R
x
and R
y
can be found at their highest
common descendant node ? e.g. p
J
(R
4
? R
5
) in
Figure 5 can be found as 1
3
directly at R
1
. Note if
R
x
is reachable from R
y
, i.e. R
x
? R
y
, then due
to the equivalences listed in (2), p
J
(R
x
? R
y
) can
be found directly at R
x
. If the meet of two nodes
is ? (e.g. R
4
and R
3
in Figure 5), then their meet
probability is 0 as p
J
(?)=0.
While the lattice does not have direct access to
the join types of its elements, a join type prob-
ability p
J
(R
x
? R
y
) can be calculated in terms
of p
J
(R
x
? R
y
) by the join equation in (1),
which holds for all probabilistic distributive lat-
85
PRIORS:?R
1
?
J
= 1
3?R
2
?
J
= 1
3?R
3
?
J
= 1
3
R
8
= [ x ?ind ] ?R1?J +?R2?J +?R3?J
P (J ) = ? = 1
R
4
= [ x ? ind
shp
sq
? square(x) ] ?R1?J +?R2?JP (J ) R5 = [ x ? indcol
p
? purple(x) ] ?R1?JP (J ) R6 = [ x ? indcol
y
? yellow(x) ] ?R2?J +?R3?JP (J ) R7 = [ x ? indshp
c
? circle(x) ] ?R3?JP (J )
R
1
=
?????????
x ? ind
col
p
? purple(x)
shp
sq
? square(x)
?????????
?R
1
?
J
P (J ) R2 =
?????????
x ? ind
col
y
? yellow(x)
shp
sq
? square(x)
?????????
?R
2
?
J
P (J ) R3 =
?????????
x ? ind
col
y
? yellow(x)
shp
c
? circle(x)
?????????
?R
3
?
J
P (J )
R
0
= ? = 0
Figure 5: Record type lattice with initial uniform prior probablities
tices (Knuth, 2005).8 As regards efficiency, worst
case complexity for finding the meet probability at
the common descendant of R
x
and R
y
is a linear
O(m+ n) where m and n are the number of edges
in the downward (possibly forked) paths R
x
? ?
and R
y
? ?.9
5 Simulating incremental inference and
self-repair processing
Interpretation in DyLan and its interface to the
RT lattice G follows evidence that dialogue agents
parse self-repairs efficiently and that repaired di-
alogue content (reparanda) is given special sta-
tus but not removed from the discourse context.
To model Brennan and Schober (2001)?s findings
of disfluent spoken instructions speeding up ob-
ject recognition (see section 2), we demonstrate
a self-repair parse in Figure 6 for ?The yell-, uh,
purple square? in the simple game of predicting
the final situation from {R
1
, R
2
, R
3
} continuously
given the type judgements made so far. We de-
scribe the stages T1-T4 in terms of the current
word being processed- see Figure 6:
At T1:?the? the interpreter will not yield a sub-
type checkable in G so we can only condition on
R
8
(?), giving us p
J
(s ? R
i
? s ? R
8
) = 1
3
for
i ? {1, 2, 3}, equivalent to the priors. At T2:
8The search for the meet probability is generalisable to
conjunctive types by searching for the conjuncts? highest
common descendant. The join probability is generalisable to
the disjunctive probability of multiple types, used, albeit pro-
gramatically, in Algorithm 1 for calculating a node?s proba-
bility from its child nodes.
9While we do not give details here, simple graphical
search algorithms for conjunctive and disjunctive multiple
types are linear in the number of conjuncts and disjuncts, sav-
ing considerable time in comparison to the algebraic calcula-
tions of the sum and product rules for distributive lattices.
?yell-?, the best partial word hypothesis is now
?yellow?;10 the interpreter therefore outputs an RT
which matches the type judgement s ? R
6
(i.e. that
the object is a yellow object). Taking this judge-
ment as the conditioning evidence using function
(5) we get p
J
(s ? R
1
? s ? R
6
) = 0 and us-
ing (4) we get p
J
(s ? R
2
? s ? R
6
) = 0.5 and
p
J
(s ? R
3
? s ? R
6
) = 0.5 (see the schematic
probability distribution at stage T2 in Figure 6 for
the three objects). The meet type probabilities
required for the conditional probabilities can be
found graphically as described above.
At T3:?uh purple?, low probability in the in-
terpreter output causes a self-repair to be recog-
nised, enforcing backtracking on the parse graph
which informally operates as follows (see Hough
and Purver (2012)) :
Self-repair:
IF from parsing word W the edge SE
n
is in-
sufficiently likely to be constructed from ver-
tex S
n
OR IF there is no sufficiently likely
judgement p(s ? R
x
) for R
x
? G
THEN parse word W from vertex S
n?1. IF
successful add a new edge to the top path,
without removing any committed edges be-
ginning at S
n?1; ELSE set n=n?1 and repeat.
This algorithm is consistent with a local model
for self-repair backtracking found in corpora
(Shriberg and Stolcke, 1998; Hough and Purver,
2013). As regards inference in G, upon detection
of a self-repair that revokes s ? R
6
, the type judge-
ment s ? ?R
6
, i.e. that this is not a yellow object,
10In practice, ASR modules yielding partial results are less
reliable than their non-incremental counterparts, but progress
is being made here (Schlangen and Skantze, 2009).
86
Figure 6: Incremental DS-TTR self-repair parsing. Inter-graph groundedIn links go top to bottom.
is immediately available as conditioning evidence.
Using (6) our distribution of RT judgements now
shifts: p
J
(s ? R
1
? s ? ?R
6
) = 1, p
J
(s ? R
2
?
s ? ?R
6
) = 0 and p
J
(s ? R
3
? s ? ?R
6
) = 0 be-
fore ?purple? has been parsed ? thus providing a
probabilistic explanation for increased subsequent
processing speed. Finally at T4: ?square? given
p
J
(s ? R
1
? s ? R
1
) = 1 and R
1
?R
2
= R
1
?R
3
= ?,
the distribution remains unchanged.
The system?s processing models how listen-
ers reason about the revocation itself rather than
predicting the outcome through positive evidence
alone, in line with (Brennan and Schober, 2001)?s
results.
6 Extensions
Dialogue and self-repair in the wild To move
towards domain-generality, generating the lattice
of all possible dialogue situations for interesting
domains is computationally intractable. We in-
tend instead to consider incrementally occurring
issues that can be modelled as questions (Lars-
son, 2002). Given one or more issues manifest in
the dialogue at any time, it is plausible to gener-
ate small lattices dynamically to estimate possible
answers, and also assign a real-valued relevance
measure to questions that can be asked to resolve
the issues. We are exploring how this could be
implemented using the inquiry calculus (Knuth,
2005), which defines information theoretic rele-
vance in terms of a probabilistic question lattice,
and furthermore how this could be used to model
the cause of self-repair as a time critical trade-off
between relevance and accuracy.
Learning in a dialogue While not our focus
here, lattice G?s probabilities can be updated
through observations after its initial construction.
If a reference game is played over several rounds,
the choice of referring expression can change
based on mutually salient functions from words
to situations- see e.g. (DeVault and Stone, 2009).
Our currently frequentist approach to learning is:
given an observation of an existing RT R
i
is made
with probability v, then ?R
i
?
J
, the overall denom-
inator P (J ) , and the nodes in the upward path
from R
i
to ? are incremented by v. The approach
could be converted to Bayesian update learning by
using the prior probabilities in G for calculating v
before it is added. Furthermore, observations can
be added to G that include novel RTs: due to the
DAG structure of G, their subtype ordering and
probability effects can be integrated efficiently.
7 Conclusion
We have discussed efficient methods for construct-
ing probabilistic TTR domain concept lattices or-
dered by the subtype relation and their use in
incremental dialogue frameworks, demonstrating
their efficacy for realistic self-repair processing.
We wish to explore inclusion of join types, the
scalability of RT lattices to other domains and
their learning capacity in future work.
Acknowledgements
We thank the two TTNLS reviewers for their com-
ments. Purver is supported in part by the European
Community?s Seventh Framework Programme un-
der grant agreement no 611733 (ConCreTe).
87
References
G. Betarte and A. Tasistro. 1998. Extension of Martin-
Lo?f type theory with record types and subtyping. In
G. Sambin and J. Smith, editors, 25 Years of Con-
structive Type Theory. Oxford University Press.
S. Brennan and M. Schober. 2001. How listeners
compensate for disfluencies in spontaneous speech.
Journal of Memory and Language, 44(2):274?296.
R. Cooper, S. Dobnik, S. Lappin, and S. Larsson. 2014.
A probabilistic rich type theory for semantic inter-
pretation. In Proceedings of the EACL Workshop
on Type Theory and Natural Language Semantics
(TTNLS).
R. Cooper. 2005. Records and record types in se-
mantic theory. Journal of Logic and Computation,
15(2):99?112.
R. Cooper. 2012. Type theory and semantics in flux.
In R. Kempson, N. Asher, and T. Fernando, edi-
tors, Handbook of the Philosophy of Science, vol-
ume 14: Philosophy of Linguistics, pages 271?323.
North Holland.
R. Dale and E. Reiter. 1995. Computational interpreta-
tions of the gricean maxims in the generation of re-
ferring expressions. Cognitive Science, 19(2):233?
263.
D. DeVault and M. Stone. 2009. Learning to interpret
utterances using dialogue history. In Proceedings of
the 12th Conference of the European Chapter of the
Association for Computational Linguistics (EACL).
S. Dobnik, R. Cooper, and S. Larsson. 2012. Mod-
elling language, action, and perception in type the-
ory with records. In Proceedings of the 7th Inter-
national Workshop on Constraint Solving and Lan-
guage Processing (CSLP12).
A. Eshghi, J. Hough, and M. Purver. 2013. Incre-
mental grammar induction from child-directed di-
alogue utterances. In Proceedings of the 4th An-
nual Workshop on Cognitive Modeling and Compu-
tational Linguistics (CMCL).
R. Ferna?ndez. 2006. Non-Sentential Utterances in Di-
alogue: Classification, Resolution and Use. Ph.D.
thesis, King?s College London, University of Lon-
don.
M. C. Frank and N. D. Goodman. 2012. Predicting
pragmatic reasoning in language games. Science,
336(6084):998?998.
J. Ginzburg. 2012. The Interactive Stance: Meaning
for Conversation. Oxford University Press.
J. Hough and M. Purver. 2012. Processing self-repairs
in an incremental type-theoretic dialogue system. In
Proceedings of the 16th SemDial Workshop on the
Semantics and Pragmatics of Dialogue (SeineDial).
J. Hough and M. Purver. 2013. Modelling expectation
in the self-repair processing of annotat-, um, listen-
ers. In Proceedings of the 17th SemDial Workshop
on the Semantics and Pragmatics of Dialogue (Di-
alDam).
R. Kempson, W. Meyer-Viol, and D. Gabbay. 2001.
Dynamic Syntax: The Flow of Language Under-
standing. Blackwell.
K. H. Knuth. 2005. Lattice duality: The origin of prob-
ability and entropy. Neurocomputing, 67:245?274.
E. Krahmer and K. Van Deemter. 2012. Computa-
tional generation of referring expressions: A survey.
Computational Linguistics, 38(1):173?218.
S. Larsson. 2002. Issue-based Dialogue Management.
Ph.D. thesis, Go?teborg University. Also published
as Gothenburg Monographs in Linguistics 21.
S. Larsson. 2010. Accommodating innovative mean-
ing in dialogue. Proc. of Londial, SemDial Work-
shop, pages 83?90.
S. Larsson. 2011. The TTR perceptron: Dynamic
perceptual meanings and semantic coordination. In
Proceedings of the 15th Workshop on the Semantics
and Pragmatics of Dialogue (SemDial 2011 - Los
Angelogue).
W. Levelt. 1989. Speaking: From Intention to Articu-
lation. MIT Press.
R. Montague. 1974. Formal Philosophy: Selected Pa-
pers of Richard Montague. Yale University Press.
M. Purver, A. Eshghi, and J. Hough. 2011. Incremen-
tal semantic construction in a dialogue system. In
J. Bos and S. Pulman, editors, Proceedings of the
9th International Conference on Computational Se-
mantics.
Y. Sato. 2011. Local ambiguity, search strate-
gies and parsing in Dynamic Syntax. In E. Gre-
goromichelaki, R. Kempson, and C. Howes, editors,
The Dynamics of Lexical Interfaces. CSLI Publica-
tions.
D. Schlangen and G. Skantze. 2009. A general, ab-
stract model of incremental dialogue processing. In
Proceedings of the 12th Conference of the European
Chapter of the ACL (EACL 2009).
E. Shriberg and A. Stolcke. 1998. How far do speakers
back up in repairs? A quantitative model. In Pro-
ceedings of the International Conference on Spoken
Language Processing.
G. Skantze and A. Hjalmarsson. 2010. Towards incre-
mental speech generation in dialogue systems. In
Proceedings of the SIGDIAL 2010 Conference.
J. Williams and S. Young. 2007. Scaling POMDPs
for spoken dialog management. IEEE Transac-
tions on Audio, Speech, and Language Processing,
15(7):2116?2129.
88
