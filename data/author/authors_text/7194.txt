Integrating Cross-Lingually Relevant News Articles and
Monolingual Web Documents in Bilingual Lexicon Acquisition
Takehito Utsuro? and Kohei Hino? and Mitsuhiro Kida?
Seiichi Nakagawa? and Satoshi Sato?
?Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto, 606-8501, Japan
?Department of Information and Computer Sciences, Toyohashi University of Technology
Tenpaku-cho, Toyohashi, 441?8580, Japan
Abstract
In the framework of bilingual lexicon acquisition
from cross-lingually relevant news articles on the
Web, it is relatively harder to reliably estimate bilin-
gual term correspondences for low frequency terms.
Considering such a situation, this paper proposes to
complementarily use much larger monolingual Web
documents collected by search engines, as a resource
for reliably re-estimating bilingual term correspon-
dences. We experimentally show that, using a suf-
ficient number of monolingual Web documents, it
is quite possible to have reliable estimate of bilin-
gual term correspondences for those low frequency
terms.
1 Introduction
Translation knowledge acquisition from paral-
lel/comparative corpora is one of the most im-
portant research topics of corpus-based MT.
This is because it is necessary for an MT sys-
tem to (semi-)automatically increase its trans-
lation knowledge in order for it to be used in
the real world situation. One limitation of
the corpus-based translation knowledge acquisi-
tion approach is that the techniques of transla-
tion knowledge acquisition heavily rely on avail-
ability of parallel/comparative corpora. How-
ever, the sizes as well as the domain of existing
parallel/comparative corpora are limited, while
it is very expensive to manually collect paral-
lel/comparative corpora. Therefore, it is quite
important to overcome this resource scarcity
bottleneck in corpus-based translation knowl-
edge acquisition research.
In order to solve this problem, we proposed
an approach of taking bilingual news articles
on Web news sites as a source for translation
knowledge acquisition (Utsuro et al, 2003). In
the case of Web news sites in Japan, Japanese
as well as English news articles are updated ev-
eryday. Although most of those bilingual news
articles are not parallel even if they are from
the same site, certain portion of those bilingual
news articles share their contents or at least re-
port quite relevant topics. This characteristic
is quite important for the purpose of transla-
tion knowledge acquisition. Utsuro et al (2003)
showed that it is possible to acquire translation
knowledge of domain specific named entities,
event expressions, and collocational expressions
from the collection of bilingual news articles on
Web news sites.
Based on the results of our previous study,
this paper further examines the correlation of
term frequency and the reliability of bilingual
term correspondences estimated from bilingual
news articles. We show that, for high frequency
terms, it is relatively easier to reliably estimate
bilingual term correspondences. However, for
low frequency terms, it is relatively harder to re-
liably estimate bilingual term correspondences.
Low frequency problem of this type often hap-
pens when a sufficient number of bilingual news
articles are not available at hand.
Considering such a situation, this paper then
proposes to complementarily use much larger
monolingual Web documents collected by search
engines, as a resource for reliably re-estimating
bilingual term correspondences. Those col-
lected monolingual Web documents are re-
garded as comparable corpora. Here, a stan-
dard technique of estimating bilingual term cor-
respondences from comparable corpora is em-
ployed. In the evaluation, we show that, using
a sufficient number of monolingual Web docu-
ments, it is relatively easier to have reliable esti-
mate of bilingual term correspondences. As one
of the most remarkable experimental evalua-
tion results, we further show that, for the terms
which appear infrequently in news articles, the
accuracy of re-estimating bilingual term corre-
spondences does actually improve.
Figure 1: Translation Knowledge Acquisition
from Web News Sites: Overview
2 Estimating Bilingual Term
Correspondences from
Cross-Lingually Relevant News
Articles
2.1 Overview
Figure 1 illustrates the overview of our frame-
work of translation knowledge acquisition from
Web news sites. First, pairs of Japanese and
English news articles which report identical con-
tents or at least closely related contents are re-
trieved. In this cross-lingual retrieval process,
translation knowledge such as a bilingual dic-
tionary and an MT software is used for mea-
suring similarity of Japanese and English arti-
cles across languages. Then, by applying pre-
viously studied techniques of translation knowl-
edge acquisition from parallel/comparative cor-
pora, translation knowledge such as bilingual
term correspondences are acquired.
2.2 Cross-Language Retrieval of Rel-
evant News Articles
This section gives the overview of our frame-
work of cross-language retrieval of relevant news
articles from Web news sites (Utsuro et al,
2003). First, from Web news sites, both
Japanese and English news articles within cer-
tain range of dates are retrieved. Let dJ and
dE denote one of the retrieved Japanese and
English articles, respectively. Then, each En-
glish article dE is translated into a Japanese
document dMTJ by some commercial MT soft-
ware1. Each Japanese article dJ as well as the
Japanese translation dMTJ of each English ar-
ticle are next segmented into word sequences,
and word frequency vectors v(dJ ) and v(dMTJ )
are generated. Then, cosine similarities between
v(dJ ) and v(dMTJ ) are calculated
2 and pairs of
articles dJ and dE which satisfy certain criterion
are considered as candidates for cross-lingually
relevant article pairs.
As we describe in section 4.1, on Web news
sites in Japan, the number of articles up-
dated per day is far greater (about 4 times)
in Japanese than in English. Thus, it is
much easier to find cross-lingually relevant ar-
ticles for each English query article than for
each Japanese query article. Considering this
fact, we estimate bilingual term correspon-
dences from the results of cross-lingually re-
trieving relevant Japanese articles with English
query articles. For each English query article
diE and its Japanese translation d
MTi
J , the set
DiJ of Japanese articles that are within certain
range of dates and are with cosine similarities
higher than or equal to a certain lower bound
Ld is constructed:
DiJ =
{
dJ | cos(v(dMTiJ ), v(dJ )) ? Ld
}
(1)
2.3 Estimating Bilingual Term Cor-
respondences with Pseudo-
Parallel Corpus
This section describes the technique we apply to
the task of estimating bilingual term correspon-
dences from cross-lingually relevant news texts.
Here, we regard cross-lingually relevant news
texts as a pseudo-parallel corpus, to which stan-
dard techniques of estimating bilingual term
correspondences from parallel corpora can be
applied3.
1In this query translation process, we compared an
MT software with a bilingual lexicon. CLIR with query
translation by an MT software performed much better
than that by a bilingual lexicon. In the case of news
articles on Web news sites, it is relatively easier to find
articles in the other language which report closely related
contents, with just a few days difference of report dates.
In such a case, exact query translation by an MT soft-
ware is suitable, because exact translation is expected to
easily match the closely related articles in the other lan-
guage. As we mention in section 3.3, this is opposite to
the situation of monolingual Web documents, where it is
much less expected to find closely related documents in
the other language.
2It is also quite possible to employ weights other than
word frequencies such as tf ?idf and similarity measures
other than cosine measure such as dice or Jaccard coef-
ficients.
3We also applied another technique based on con-
textual vector similarities (Utsuro et al, 2003), which
First, we concatenate constituent Japanese
articles of DiJ into one article D
?i
J , and regard
the article pair diE and D
?i
J as a pseudo-parallel
sentence pair. Next, we collect such pseudo-
parallel sentence pairs and construct a pseudo-
parallel corpus PPCEJ of English and Japanese
articles:
PPCEJ =
{
?diE , D
?i
J ? | D
i
J = ?
}
Then, we apply standard techniques of es-
timating bilingual term correspondences from
parallel corpora (Matsumoto and Utsuro, 2000)
to this pseudo-parallel corpus PPCEJ . First,
from a pseudo-parallel sentence pair diE and D
?i
J ,
we extract monolingual (possibly compound4)
term pair tE and tJ :
r?tE , tJ ? s.t. ?diE?dJ , tE in d
i
E , tJ in dJ , (2)
cos(v(dMTiJ ), v(dJ )) ? Ld
Then, based on the contingency table of co-
occurrence document frequencies of tE and tJ
below, we estimate bilingual term correspon-
dences according to the statistical measures
such as the mutual information, the ?2 statistic,
the dice coefficient, and the log-likelihood ratio.
tJ ?tJ
tE df(tE , tJ ) = a df(tE ,?tJ ) = b
?tE df(?tE , tJ) = c df(?tE ,?tJ) = d
We compare the performance of those four
measures, where the ?2 statistic and the log-
likelihood ratio perform best, the dice coefficient
the second best, and the mutual information the
worst. In section 4.3, we show results with the
?2 statistic as the bilingual term correspondence
corrEJ(tE , tJ):
?2(tE , tJ) =
(ad ? bc)2
(a + b)(a + c)(b + d)(c + d)
3 Re-estimating Bilingual Term
Correspondences using
Monolingual Web Documents
3.1 Overview
This section illustrates the overview of the pro-
cess of re-estimating bilingual term correspon-
dences using monolingual Web documents col-
lected by search engines. Figure 2 gives its
rough idea.
has been well studied in the context of bilingual lexicon
acquisition from comparable corpora. In this method,
we regard cross-lingually relevant texts as a compara-
ble corpus, where bilingual term correspondences are es-
timated in terms of contextual similarities across lan-
guages. This technique is less effective than the one we
describe here (Utsuro et al, 2003).
4In the evaluation of this paper, we restrict English
and Japanese terms t
E
and t
J
to be up to five words
long.
Figure 2: Re-estimating Bilingual Term Corre-
spondences using Monolingual Web Documents:
Overview
Suppose that we have an English term, and
that the problem to solve here is to find its
Japanese translation. As we described in the
previous section and in Figure 1, with a cross-
lingually relevant Japanese and English news
articles database, we can have a certain num-
ber of Japanese translation candidates for the
target English term. Here, for high frequency
terms, it is relatively easier to have reliable
ranking of those Japanese translation candi-
dates. However, for low frequency terms, hav-
ing reliable ranking of those Japanese transla-
tion candidates is difficult. Especially, low fre-
quency problem of this type often happens when
we do not have large enough language resources
(in this case, cross-lingually relevant news arti-
cles).
Considering such a situation, re-estimation of
bilingual term correspondences proceeds as fol-
lows, using much larger monolingual Web doc-
uments sets that are easily accessible through
search engines. First, English pages which
contain the target English term are collected
through an English search engine. In the simi-
lar way, for each Japanese term in the Japanese
translation candidates, Japanese pages which
contain the Japanese term are collected through
a Japanese search engine. Then, texts con-
tained in those English and Japanese pages are
extracted and are regarded as comparable cor-
pora. Here, a standard technique of estimat-
ing bilingual term correspondences from com-
parable corpora (e.g., Fung and Yee (1998) and
Rapp (1999)) is employed. Contextual sim-
ilarity between the target English term and
the Japanese translation candidate is measured
across languages, and all the Japanese transla-
tion candidates are re-ranked according to the
contextual similarities.
3.2 Filtering by Hits of Search En-
gines
Before re-estimating bilingual term correspon-
dences using monolingual Web documents, we
assume there exists certain correlation between
hits of the English term tE and the Japanese
term tJ returned by search engines. Depending
on the hits h(tE) of tE , we restrict the hits h(tJ )
of tJ to be within the range of a lower bound
hL and an upper bound hU :
hL < h(tJ ) ? hU
As search engines, we used AltaVista
(http://www. altavista.com/ for En-
glish, and goo (http://www.goo.ne.jp/) for
Japanese. With a development data set con-
sisting of translation pairs of an English term
and a Japanese term, we manually constructed
the following rules for determining the lower
bound hL and the upper bound hU :
1. 0 < h(tE) ? 100
hL = 0, hU = 10, 000 ? h(tE)
2. 100 < h(tE) ? 20, 000
hL = 0.05 ? h(tE), hU = 1, 000, 000
3. 20, 000 < h(tE)
hL = 1, 000, hU = 50 ? h(tE)
In the experimental evaluation of Section 4.4,
the initial set of Japanese translation candi-
dates consists of 50 terms for each English term,
which are then reduced to on the average 24.8
terms with this filtering.
3.3 Re-estimating Bilingual Term
Correspondences based on Con-
textual Similarity
This section describes how to re-estimate bilin-
gual term correspondences using monolingual
Web documents collected by search engines.
For an English term tE and a Japanese term
tJ , let D(tE) and D(tJ) be the sets of docu-
ments returned by search engines with queries
tE and tJ , respectively. Then, for the English
term tE, translated contextual vector cvtrJ (tE)
is constructed as below: each English sen-
tence sE which contains tE is translated into
Japanese sentence strJ , then the term frequency
vectors5 v(strJ ) of Japanese translation s
tr
J are
5In the term frequency vectores, compound terms are
restricted to be up to five words long both for English
and Japanese.
Table 1: Statistics of # of Days, Articles, and
Article Sizes
total total average # average
# of # of of articles article
days articles per day size (bytes)
Eng 935 23064 24.7 3228.9
Jap 941 96688 102.8 837.7
summed up into the translated contextual vec-
tor cvtrJ(tE):
cvtrJ (tE) =
?
?s
E
in D(t
E
) s.t. t
E
in s
E
v(strJ )
The contextual vector cv(tJ ) for the Japanese
term tJ is also constructed by summing up the
term frequency vectors v(sJ) of each Japanese
sentence sJ which contains tJ :
cv(tJ ) =
?
?s
J
in D(t
J
) s.t. t
J
in s
J
v(sJ)
In the translation of English sentences into
Japanese, we evaluated an MT software and a
bilingual lexicon in terms of the performance of
re-estimation of bilingual term correspondences.
Unlike the situation of cross-lingually relevant
news articles mentioned in Section 2.2, trans-
lation by a bilingual lexicon is more effective
for monolingual Web documents. In the case of
monolingual Web documents, it is much less ex-
pected to find closely related documents in the
other language. In such cases, multiple trans-
lation rather than exact translation by an MT
software is suitable. In Section 4.4, we show
evaluation results with translation by a bilin-
gual lexicon6.
Finally, bilingual term correspondence
corrEJ(tE , tJ) is estimated in terms of co-
sine measure cos(cvtrJ (tE), cv(tJ )) between
contextual vectors cvtrJ (tE) and cv(tJ ).
4 Experimental Evaluation
4.1 Japanese-English Relevant News
Articles on Web News Sites
We collected Japanese and English news articles
from a Web news site. Table 1 shows the total
number of collected articles and the range of
dates of those articles represented as the num-
ber of days. Table 1 also shows the number of
articles updated in one day, and the average ar-
ticle size. The number of Japanese articles up-
dated in one day are far greater (about 4 times)
than that of English articles.
6Eijiro Ver.37, 850,000 entries, http://homepage3.
nifty.com/edp/.
Table 2: # of Japanese/English Articles Pairs with Similarity Values above Lower Bounds
Lower Bound Ld of Articles? Sim w/o 0.3 0.4 0.5
Difference of Dates (days) CLIR ? 2
# of English Articles 23064 6073 2392 701
# of Japanese Articles 96688 12367 3444 882
# of English-Japanese Article Pairs ? 16507 3840 918
Next, for several lower bounds Ld of the
similarity between English and Japanese arti-
cles, Table 2 shows the numbers of English and
Japanese articles as well as article pairs which
satisfy the similarity lower bound. Here, the
difference of dates of English and Japanese arti-
cles is within two days, with which it is guaran-
teed that, if exist, closely related articles in the
other language can be discovered (see Utsuro et
al. (2003) for details). Note that it can happen
that one article has similarity values above the
lower bound against more than one articles in
the other language.
According to our previous study (Utsuro et
al., 2003), cross-lingually relevant news arti-
cles are available in the direction of English-
to-Japanese retrieval for more than half of the
retrieval query English articles. Furthermore,
with the similarity lower bound Ld = 0.3, pre-
cision and recall of cross-language retrieval are
around 30% and 60%, respectively. Therefore,
with the similarity lower bound Ld = 0.3, at
least 1,800 (? 6, 073?0.5?0.6) English articles
have relevant Japanese articles in the results of
cross-language retrieval. Based on this analysis,
the next section gives evaluation results with
the similarity lower bound Ld = 0.3.
4.2 English Term List for Evaluation
For the evaluation of this paper, we first man-
ually select target English terms and their
reference Japanese translation, and examine
whether reference bilingual term correspon-
dences can be estimated by the methods pre-
sented in Sections 2 and 3. Target English terms
are selected by the following procedure.
First, from the whole English articles of Ta-
ble 1, any sequence of more than one words
whose frequency is more than or equal to 10 is
enumerated. This enumeration is easily imple-
mented and efficiently computed by employing
the technique of PrefixSpan (Pei et al, 2001).
Here, certain portion of those word sequences
are appropriate as compound terms, while the
rest are some fragments of a compound term,
or concatenation of those fragments. In or-
der to automatically select candidates for cor-
rect compound terms, we parse those word se-
Figure 3: Accuracy of Estimating Bilingual
Term Correspondences with News Articles
quences by Charniak parser7, and collect noun
phrases which consist of adjectives, nouns, and
present/past participles. For each of those word
sequences, the ?2 statistic against Japanese
translation candidates is calculated, then those
word sequences are sorted in descending order of
their ?2 statistic. Finally, among top 3,000 can-
didates for compound terms, 100 English com-
pound terms are randomly selected for the eval-
uation of this paper. Selected 100 terms satisfy
the following condition: those English terms can
be correctly translated neither by the MT soft-
ware used in Section 2.2, nor by the bilingual
lexicon used in Section 3.3.
4.3 Estimating Bilingual Term Cor-
respondences with News Articles
For the 100 English terms selected in the pre-
vious section, Japanese translation candidates
which satisfy the condition of the formula (2) in
Section 2.3 are collected, and are ranked accord-
ing to the ?2 statistic. Figure 3 plots the rate
of reference Japanese translation being within
top n candidates. In the figure, the plot labeled
as ?full? is the result with the whole articles in
Table 1. In this case, the accuracy of the top
ranked Japanese translation candidate is about
40%, and the rate of reference Japanese trans-
lation within top five candidates is about 75%.
7http://www.cs.brown.edu/people/ec/
Table 3: Statistics of Average Document Frequencies and Number of Days
Document Frequencies of target English Term # of Days
Data Set df(tE) df(tE, tJ) Eng Jap
freq=10, 13.6 days 14.9 9.1 13.6 21.9
freq=10, 20 days 14.9 9.1 21.0 78.7
freq=10, 200 days 14.9 9.1 200 581
freq=70, 600 days 37.4 24.9 600 872
full 53.9 35.6 935 941
On the other hand, other plots labeled as
?Freq=x, y days? are the results when the num-
ber of the news articles is reduced, which are
simulations for estimating bilingual term cor-
respondences for low frequency terms. Here,
the label ?Freq=x, y days? indicates that news
articles used for ?2 statistic estimation is re-
stricted to certain portion of the whole news
articles so that the following condition be satis-
fied: i) co-occurrence document frequency of a
target English term and its reference Japanese
translation is fixed to be x,8 ii) the number of
days be greater than or equal to y. For each
news articles data set, Table 3 shows document
frequencies df(tE) of a target English term tE ,
co-occurrence document frequencies df(tE, tJ )
of tE and its reference Japanese translation tJ ,
and the numbers of days for English as well as
Japanese articles. Those numbers are all aver-
aged over the 100 English terms. The number of
days for Japanese articles could be at maximum
five times larger than that for English articles,
because relevant Japanese articles are retrieved
against a query English article from the dates of
differences within two days (details are in Sec-
tions 2.2 and 4.1).
As can be seen from the plots of Figure 3,
the smaller the news articles data set, the lower
the plot is. Especially, in the case of the small-
est news articles data set, it is clear that re-
liable ranking of Japanese translation candi-
dates is difficult. This is because it is not easy
to discriminate the reference Japanese transla-
tion and the other candidates with statistics ob-
tained from such a small news articles data set.
4.4 Re-estimating Bilingual Term
Correspondences with Monolin-
gual Web Documents
For the 100 target English terms evaluated in
the previous section, this section describes the
result of applying the technique presented in
Section 3.3, i.e., re-estimating bilingual term
8When the co-occurrence document frequency of t
E
and t
J
in the whole news articles is less than x, all the
co-occurring dates are included.
Figure 4: Accuracy of Re-estimating Bilingual
Term Correspondences with Monolingual Web
Documents
correspondences with monolingual Web docu-
ments. For each of the 100 target English
terms, bilingual term correspondences are re-
estimated against candidates of Japanese trans-
lation ranked within top 50 according to the
?2 statistic. Here, as a simulation for terms
that are infrequent in news articles, 50 can-
didate terms for Japanese translation are col-
lected from the smallest data set labeled as
?Freq=10, 13.6 days?. As mentioned in Sec-
tion 3.2, those 50 candidates are reduced to on
the average 24.8 terms with the filtering by hits
of search engines. For each of an English term
tE and a Japanese term tJ , 100 monolingual
documents are collected by search engines9 10.
Figure 4 compares the plots of re-estimation
with monolingual Web documents and estima-
tion by news articles (data set ?Freq=10, 13.6
9In the result of our preliminary evaluation, accuracy
of re-estimating bilingual term correspondences did not
improve even if more than 100 documents were used.
10Alternatively, as the monolingual documents from
which contextual vectors are constructed, we evaluated
each of the short passages listed in the summary pages
returned by search engines, instead of the whole docu-
ments of the URLs listed in the summary pages. The
difference of the performance of bilingual term corre-
spondence estimation is little, while the computational
cost can reduced to almost 5%.
days?). It is clear from this result that mono-
lingual Web documents contribute to improving
the accuracy of estimating bilingual term corre-
spondences for low frequency terms.
One of the major reasons for this improve-
ment is that topics of monolingual Web doc-
uments collected through search engines are
much more diverse than those of news articles.
Such diverse topics help discriminate correct
and incorrect Japanese translation candidates.
For example, suppose that the target English
term tE is ?special anti-terrorism law? and its
reference Japanese translation is ???????
????. In the news articles we used for evalua-
tion, most articles in which tE or tJ appear have
?dispatch of Self-Defense Force for reconstruc-
tion of Iraq? as their topics. Here, Japanese
translation candidates other than ??????
????? that are highly ranked according to
the ?2 statistic are: e.g., ????? (dissolution
of the House of Representatives)? and ?????
??? (assistance for reconstruction of Iraq)?,
which frequently appear in the topic of ?dis-
patch of Self-Defense Force for reconstruction
of Iraq?.
On the other hand, in the case of monolin-
gual Web documents collected through search
engines, it can be expected that topics of docu-
ments may vary according to the query terms.
In the case of the example above, the major
topic is ?dispatch of Self-Defense Force for re-
construction of Iraq? for both of reference terms
tE and tJ , while major topics for other Japanese
translation candidates are: ?issues on Japanese
Diet? for ????? (dissolution of the House
of Representatives)? and ?issues on reconstruc-
tion of Iraq, not only in Japan, but all over the
world? for ???????? (assistance for re-
construction of Iraq)?. Those topics of incor-
rect Japanese translation candidates are differ-
ent from that of the target English term tE, and
their contextual vector similarities against the
target English term tE are relatively low com-
pared with the reference Japanese translation
tJ . Consequently, the reference Japanese trans-
lation tJ is re-ranked higher compared with the
ranking based on news articles.
5 Related Works
In large scale experimental evaluation of bilin-
gual term correspondence estimation from com-
parable corpora, it is difficult to estimate bilin-
gual term correspondences against every possi-
ble pair of terms due to its computational com-
plexity. Previous works on bilingual term cor-
respondence estimation from comparable cor-
pora controlled experimental evaluation in var-
ious ways in order to reduce this computational
complexity. For example, Rapp (1999) filtered
out bilingual term pairs with low monolingual
frequencies (those below 100 times), while Fung
and Yee (1998) restricted candidate bilingual
term pairs to be pairs of the most frequent 118
unknown words. Cao and Li (2002) restricted
candidate bilingual compound term pairs by
consulting a seed bilingual lexicon and requir-
ing their constituent words to be translation
of each other across languages. On the other
hand, in the framework of bilingual term corre-
spondences estimation of this paper, the compu-
tational complexity of enumerating translation
candidates can be easily avoided with the help of
cross-language retrieval of relevant news texts.
Furthermore, unlike Cao and Li (2002), bilin-
gual term correspondences for compound terms
are not restricted to compositional translation.
6 Conclusion
In the framework of bilingual lexicon acquisition
from cross-lingually relevant news articles on
the Web, it has been relatively harder to reliably
estimate bilingual term correspondences for low
frequency terms. This paper proposed to com-
plementarily use much larger monolingual Web
documents collected by search engines, as a re-
source for reliably re-estimating bilingual term
correspondences. We showed that, for the terms
which appear infrequently in news articles, the
accuracy of re-estimating bilingual term corre-
spondences actually improved.
References
Y. Cao and H. Li. 2002. Base noun phrase translation
using Web data and the EM algorithm. In Proc. 19th
COLING, pages 127?133.
P. Fung and L. Y. Yee. 1998. An IR approach for trans-
lating new words from nonparallel, comparable texts.
In Proc. 17th COLING and 36th ACL, pages 414?420.
Y. Matsumoto and T. Utsuro. 2000. Lexical knowledge
acquisition. In R. Dale, H. Moisl, and H. Somers,
editors, Handbook of Natural Language Processing,
chapter 24, pages 563?610. Marcel Dekker Inc.
J. Pei, J. Han, B. Mortazavi-Asl, and H. Pinto. 2001.
Prefixspan: Mining sequential patterns efficiently by
prefix-projected pattern growth. In Proc. Inter. Conf.
Data Mining, pages 215?224.
R. Rapp. 1999. Automatic identification of word trans-
lations from unrelated English and German corpora.
In Proc. 37th ACL, pages 519?526.
T. Utsuro, T. Horiuchi, T. Hamamoto, K. Hino, and
T. Nakayama. 2003. Effect of cross-language IR in
bilingual lexicon acquisition from comparable cor-
pora. In Proc. 10th EACL, pages 355?362.
 
				Automatic Collection of Related Terms from the Web
Satoshi Sato and Yasuhiro Sasaki
Graduate School of Informatics
Kyoto University
Sakyo, Kyoto, 606-8501
Japan
sato@i.kyoto-u.ac.jp, sasaki@pine.kuee.kyoto-u.ac.jp
Abstract
This paper proposes a method of collect-
ing a dozen terms that are closely re-
lated to a given seed term. The proposed
method consists of three steps. The first
step, compiling corpus step, collects texts
that contain the given seed term by us-
ing search engines. The second step, au-
tomatic term recognition, extracts impor-
tant terms from the corpus by using Naka-
gawa?s method. These extracted terms be-
come the candidates for the final step. The
final step, filtering step, removes inappro-
priate terms from the candidates based on
search engine hits. An evaluation result
shows that the precision of the method is
85%.
1 Introduction
This study aims to realize an automatic method of
collecting technical terms that are related to a given
seed term. In case ?natural language processing? is
given as a seed term, the method is expected to col-
lect technical terms that are related to natural lan-
guage processing, such as morphological analysis,
parsing, information retrieval, and machine transla-
tion. The target application of the method is auto-
matic or semi-automatic compilation of a glossary or
technical-term dictionary for a certain domain. Re-
cursive application of the method enables to collect a
list of terms that are used in a certain domain: the list
becomes a glossary of the domain. A technical-term
dictionary can be compiled by adding an explanation
for every term in the glossary, which is performed by
term explainer (Sato, 2001).

?
?
?
a seed term
s
?
Compiling
corpus
??
?

?
?
?
corpus
C
s
?
?
?
?
?
the Web
??
ATR
?

?
?
?
related terms
T
? Filtering ?

?
?
?
candidates
X
Figure 1: System configuration
Automatic acquisition of technical terms in a cer-
tain domain has been studied as automatic term
recognition (Kageura and Umino, 1996; Kageura
and Koyama, 2000), and the methods require a large
corpus that are manually prepared for a target do-
main. In contrast, our system, which is proposed in
this paper, requires only a seed word; from this seed
word, the system compiles a corpus from the Web by
using search engines and produces a dozen technical
terms that are closely related to the seed word.
2 System
Figure 1 shows the configuration of the system. The
system consists of three steps: compiling corpus, au-
tomatic term recognition (ATR), and filtering. This
system is implemented for Japanese language.
2.1 Compiling corpus
The first step, compiling corpus, produces a corpus
C
s
for a seed term s. In general, compiling corpus is
to select the appropriate passages from a document
set. We use the Web for the document set and se-
lect the passages that describe s for the corpus. The
actual procedure of compiling corpus is:
1. Web page collection
For a given seed term s, the system first makes
four queries: ?s toha?, ?s toiu?, ?s ha?, and
?s?, where toha, ha, and toiu are Japanese
functional words that are often used for defin-
ing or explaining a term. Then, the system col-
lects the top K (= 100) pages at maximum for
each query by using a search engine. If a col-
lected page has a link whose anchor string is s,
the system collects the linked page too.
2. Sentence extraction
The system decomposes each page into sen-
tences, and extracts the sentences that contain
the seed term s.
The reason why we use the additional three queries
is that they work efficiently for collecting web pages
that contain a definition or an explanation of s. We
use two search engines, Goo1 and Infoseek2. We
send all four queries to Goo but only the query ?s? to
Infoseek, because Infoseek usually returns the same
result for the four queries. A typical corpus size is
about 500 sentences.
2.2 Automatic term recognition
The second step, automatic term recognition (ATR),
extracts important terms from the compiled cor-
pus. We use Nakagawa?s ATR method (Nakagawa,
2000), which works well for Japanese text, with
some modifications. The procedure is as follows.
1. Generation of term list
To make the term list L by extracting every
term that is a noun or a compound noun from
the compiled corpus.
2. Selection by scoring
To select the top N (= 30) terms from the list L
by using a scoring function.
For the scoring function of a term x, we use
the following function, which is multiplying Nak-
agawa?s Imp
1
by a frequency factor F (x, L)?.
score(x, L) = Imp
1
(x, L)? F (x, L)?
F (x, L) =
{
1 if x is a single noun
?frequency of x in L? otherwise
1www.goo.ne.jp
2www.infoseek.co.jp
While Nakagawa?s Imp
1
does not consider term fre-
quency, this function does: ? is a parameter that con-
trols how strongly the frequency is considered. We
use ? = 0.5 in experiments.
The result of automatic term recognition for ???
???? (natural language processing)? is shown in
the column candidate in Table 1.
2.3 Filtering
The filtering step is necessary because the obtained
candidates are noisy due to the small corpus size.
This step consists of two tests: technical-term test
and relation test.
2.3.1 Technical-term test
The technical-term test removes the terms that do
not satisfy conditions of technical terms. We employ
the following four conditions that a technical term
should satisfy.
1. The term is sometimes or frequently used in a
certain domain.
2. The term is not a general term.
3. There is a definition or explanation of the term.
4. There are several technical terms that are re-
lated to the term.
We have implemented the checking program of the
first two conditions in the system: the third condition
can be checked by integrating the system with term
explainer (Sato, 2001), which produces a definition
or explanation of a given term; the fourth condition
can be checked by using the system recursively.
There are several choices for implementing the
checking program. Our choice is to use the Web via
a search engine. A search engine returns a number,
hit, which is an estimated number of pages that sat-
isfy a given query. In case the query is a term, its hit
is the number of pages that contain the term on the
Web. We use the following notation.
H(x) = ?the number of pages that contain
the term x?
The number H(x) can be used as an estimated
frequency of the term x on the Web, i.e., on the
hugest set of documents. Based on this number, we
can infer whether a term is a technical term or not:
in case the number is very small, the term is not a
Table 1: Result for ?natural language processing?
candidate Tech. Rel.
?????? (natural langauge pro-
cessing; NLP)
- -
???????? (NLP technology) ? ?
?????????? (NLP system) ? ?
???????? (NLP research)
??????? (NLP study) ? ?
?? (processing)
?????? (text processing) ?
???? (research and development)
?????? (Information Processing
Society of Japan; IPSJ)
? ?
???? (semantic processing) ? ?
???? (speech processing) ?
?????? (speech information pro-
cessing)
? ?
???? (information processing)
???????? (NLP domain)
???? (research field) ? ?*
???? (parsing) ? ?
???? (information retrieval) ? ?
????????? (SIGNLP) ? ?
???? (speech recognition) ? ?
???? (machine translation) ? ?
????? (morphological analysis) ? ?
???????? (information pro-
cessing system)
?
?? (research)
???? (semantic analysis) ? ?
????????? (chair of NLP) ? ?*
???????????? (NLP sym-
posium)
?????? (application system) ?
?????? (knowledge information
processing)
? ?
?? (language)
?? (information)
technical term because it does not satisfy the first
condition; in case the number is large enough, the
term is probably a general term so that it is not a
technical term. Two parameters, Min and Max, are
necessary here. We have decided that we use search
engine Goo for H(x), and determined Min = 100
and Max = 100, 000, based on preliminary experi-
ments.
In summary, our technical-term test is:
If 100 ? H(x) ? 100, 000
then x is a technical term.
2.3.2 Relation test
The relation test removes the terms that are not
closely related to the seed term from the candidates.
Our conditions of ?x is closely related to s? is: (1)
x is a broader or narrower term of s; or (2) relation
degree between x and s is high enough, i.e., above a
given threshold.
The candidate terms can be classified from the
viewpoint of term composition. Under a given seed
term, we introduce the following five types for clas-
sification.
Type 0 the given seed term s: e.g., ?? ?? ??
(natural language processing)
Type 1 a term that contains s: e.g., ???????
??? (natural language processing system)
Type 2 a term that is a subsequence of s: e.g., ??
?? (natural language)
Type 3 a term that contains at least a component of
s: e.g., ???? (language analysis)
Type 4 others: e.g., ???? (parsing)
The reason why we introduce these types is that
the following rules are true with a few exception: (1)
A type-1 term is a narrower term of the seed term
s; (2) A type-2 term is a broader term of the seed
term s. We assume that these rules are always true:
they are used to determine whether x is a broader or
narrower term of s.
To measure the relation degree, we use con-
ditional probabilities, which are calculated from
search engine hits.
P (s|x) =
H(s? x)
H(x)
P (x|s) =
H(s? x)
H(s)
where
H(s? x) = ?the number of pages that contain
both s and x?
One of two probabilities is equal to or greater than
a given threshold Z, the system decides that x is
closely related to s. We use Z = 0.05 as the thresh-
old.
In summary, our relation test is:
If x is type-1 or type-2; or
P (s|x) ? 0.05 or P (x|s) ? 0.05
then x is closely related to s.
The result of the filtering step for ??????
? (natural language processing)? is in Table 1; a
Table 2: Experimental Result
Evaluation I Evaluation II
domain correct incorrect total S F A C R total
natural language processing 101 (93%) 8 ( 7%) 109 6 3 14 11 8 43
Japanese language 71 (81%) 17(19%) 88 7 0 19 5 1 32
information technology 113 (88%) 15 (12%) 128 10 5 27 13 0 55
current topics 106 (91%) 10 ( 9%) 116 2 0 13 19 5 39
persons in Japanese history 128 (76%) 41 (24%) 169 18 0 23 1 0 42
Total 519 (85%) 91(15%) 610 43 8 96 49 14 210
check mark ?
?
? indidates that the term passed the
test. Twenty terms out of the thrity candidate terms
passed the first techinical-term test (Tech.) and six-
teen terms out of the twenty terms passed the second
relation test (Rel.). The final result includes two in-
appropriate terms, which are indicated by ?*?.
3 Experiments and Disucssion
First, we examined the precision of the system. We
prepared fifty seed terms in total: ten terms for
each of five genres; natural language processing,
Japanese language, information technology, current
topics, and persons in Japanese history. From these
fifty terms, the system collected 610 terms in total;
the average number of output terms per input is 12.2
terms. We checked whether each of the 610 terms
is a correct related term of the original seed term by
hand. The result is shown in the left half (Evaluation
I) of Table 2. In this evaluation, 519 terms out of 610
terms were correct: the precision is 85%. From this
high value, we conclude that the system can be used
as a tool that helps us compile a glossary.
Second, we tried to examine the recall of the
system. It is impossible to calculate the actual re-
call value, because the ideal output is not clear and
cannot be defined. To estimate the recall, we first
prepared three to five target terms that should be
collected from each seed word, and then checked
whether each of the target terms was included in
the system output. We counted the number of tar-
get terms in the following five cases. The right half
(Evaluation II) in Table 2 shows the result.
S: the target term was collected by the system.
F: the target term was removed in the filtering step.
A: the target term existed in the compiled corpus,
but was not extracted by automatic term extrac-
tion.
C: the target term existed in the collected web
pages, but did not exist in the compiled corpus.
R: the target term did not exist on the collected web
pages.
Only 43 terms (20%) out of 210 terms were col-
lected by the system. This low recall primarily
comes from the failure of automatic term recogni-
tion (case A in the above classification). Improve-
ment of this step is necessary.
We also examined whether each of the 210 target
terms passes the filtering step. The result was that
133 (63%) terms passed; 44 terms did not satisfy
the condition H(x) ? 100; 15 terms did not satisfy
the condition H(x) ? 100, 000; and 18 terms did
not pass the relation test. These experimental results
suggest that the ATR step may be replaced with a
simple and exhaustive term collector from a corpus.
We have a plan to examine this possibility next.
References
Kyo Kageura and Teruo Koyama. 2000. Special issue:
Japanese term extraction. Terminolgy, 6(2).
Kyo Kageura and Bin Umino. 1996. Methods of au-
tomatic term recognition: A review. Terminology,
3(2):259?289.
Hiroshi Nakagawa. 2000. Automatic term recognition
based on statistics of compound nouns. Terminology,
6(2):195?210.
Satoshi Sato. 2001. Automated editing of hypertext
re?sume? from the world wide web. In Proceedings
of 2001 Symposium on Applications and the Internet
(SAINT 2001), pages 15?22.
Automatic Detection of Grammar Elements that Decrease Readability
Masatoshi Tsuchiya and Satoshi Sato
Department of Intelligence Science and Technology,
Graduate School of Informatics, Kyoto University
tsuchiya@pine.kuee.kyoto-u.ac.jp, sato@i.kyoto-u.ac.jp
Abstract
This paper proposes an automatic method
of detecting grammar elements that de-
crease readability in a Japanese sentence.
The method consists of two components:
(1) the check list of the grammar elements
that should be detected; and (2) the de-
tector, which is a search program of the
grammar elements from a sentence. By
defining a readability level for every gram-
mar element, we can find which part of the
sentence is difficult to read.
1 Introduction
We always prefer readable texts to unreadable texts.
The texts that transmit crucial information, such as
instructions of strong medicines, must be completely
readable. When texts are unreadable, we should
rewrite them to improve readability.
In English, measuring readability as reading age
is well studied (Johnson, 1978). The reading age
is the chronological age of a reader who could just
understand the text. The value is usually calculated
from the sentence length and the number of sylla-
bles. From this value, we find whether a text is read-
able or not for readers of a specific age; however, we
do not find which part we should rewrite to improve
readability when the text is unreadable.
The goal of our study is to present tools that help
rewriting work of improving readability in Japanese.
The first tool is to help detect the sentence frag-
ments (words and phrases) that should be rewrit-
ten; in other words, it is a checker of ?hard-to-read?
words and phrases in a sentence. Such a checker can
be realized with two components: the check list and
its detector. The check list provides check items and
their readability levels. The detector is a program
that searches the check items in a sentence. From
the detected items and their readability levels, we
can identify which part of the sentence is difficult to
read.
We are currently working on three aspects con-
cerned with readability of Japanese: kanji charac-
ters, vocabulary, and grammar. In this paper, we re-
ports the readability checker for the grammar aspect.
2 The check list of grammar elements
The first component of the readability checker is
the check list; in this list, we should define every
Japanese grammar element and its readability level.
A grammar element is a grammatical phenomenon
concerned with readability, and its readability level
indicates the familiarity of the grammar element.
In Japanese, grammar elements are classified into
four categories.
1. Conjugation: the form of a verb or an adjective
changes appropriately to the proceed word.
2. Functional word: postpositional particles work
as case makers; auxiliary verbs represent tense
and modality.
3. Sentential pattern: negation, passive form, and
question are represented as special sentence
patterns.
4. Functional phrase: there are idiomatic phrases
works functionally, like ?not only ... but also
...? in English.
A grammar section exists in a part of the Japanese
Language Proficiency Test, which is used to measure
and certify the Japanese language ability of a person
who is a non-Japanese. There are four levels in this
test; Level 4 is the elementary level, and Level 1 is
the advanced level.
Test Content Specifications (TCS) (Foundation
and Association of International Education, 1994) is
intended to serve as a reference guide in question
compilation of the Japanese Language Proficiency
Test. This book describes the list of grammar ele-
ments, which can be tested at each level. These lists
fit our purpose: they can be used as the check list for
the readability checker.
TCS describes grammar elements in two ways. In
the first way, a grammar element is described as a
3-tuple: its name, its patterns, and its example sen-
tences. The following 3-tuple is an example of the
grammar element that belongs to Level 4.
Name
daimeishi
??? (Pronoun)
Patterns
kore
?? (this), sore?? (that)
Examples
kore
??
ha
?
hon
?
desu.
???(This is a book.),
sore
??
ha
?
no?to
???
desu.
???(That is a note.)
Grammar elements of Level 3 and Level 4 are con-
jugations, functional words and sentential patterns
that are defined in this first way. In the second way,
a grammar element is described as a pair of its pat-
terns and its examples. The following pair is an ex-
ample of the grammar element that belongs to Level
2.
Patterns ?
ta
?
tokoro
??? (when ...)
Examples
sensei
??
no
?
otaku
??
he
?
ukagatta
???
tokoro
???
(When visiting the teacher?s home)
Grammar elements of Level 1 and Level 2 are func-
tional phrases that are defined in this second way.
We decided to use this example-based definition
for the check list, because the check list should be in-
dependent from the implementation of the detector.
If the check list depends on detector?s implementa-
tion, the change of implementation requires change
of the check list.
Each item of the check list is defined as a 3-tuple:
(1) readability level, (2) name, and (3) a list of exam-
ple pairs. There are four readability levels according
Table 1: The size of the check list
Level # of rules
1 134
2 322
3 97
4 95
Total 648
to the Japanese Language Proficiency Test. An ex-
ample pair consists of an example sentence and an
instance of the grammar element. It is an implicit
description of the pattern detecting the grammar el-
ement. For example, the check item for ?Adjective
(predicative, negative, polite)? is shown as follows,
Level 4
Name Adjective (predicative, negative, polite)
Test Pairs
Sentence1
kono
??
heya
??
ha
?
hiroku
??
nai
??
desu.
???
(This room is not large.)
Instance1
hiroku
??
nai
??
desu
??
(is not large)
The instance??????/hirokunaidesu/ consists
of three morphemes: (1)??/hiroku/, the adjective
means ?large? in renyo form, (2)??/nai/, the ad-
jective means ?not? in root form, and (3)??/desu/,
the auxiliary verb ends a sentence politely. So, this
test pair represents implicitly that the grammar el-
ement can be detected by a pattern ?Adjective(in
renyo form) + nai + desu?.
All example sentences are originated from TCS.
Some check items have several test pairs. Table 1
shows the size of the check list.
3 The grammar elements detector
The check list must be converted into an explicit
rule set, because each item of the check list shows
no explicit description of its grammar element, only
shows one or more pairs of an example sentence and
an instance.
3.1 The explicit rule set
Four categories of grammar elements leads that each
rule of the explicit rule set may take three different
types.
? Type M: A rule detecting a sequence of mor-
phemes
? Type B: A rule detecting a bunsetsu.
? Type R: A rule detecting a modifier-modifee re-
lationship.
Type M is the basic type of them, because almost of
grammar elements can be detected by morphologi-
cal sequential patterns.
Conversion from a check item to a Type M rule
is almost automatic. This conversion process con-
sists of three steps. First, an example sentence of
the check item is analyzed morphologically and syn-
tactically. Second, a sentence fragment covered by
the target grammar element is extracted based on
signs and fixed strings included in the name of the
check item. Third, a part of a generated rule is re-
laxed based on part-of-speech tags. For example,
the check item of the grammar element whose name
is ?Adjective (predicative, negative, polite)? is con-
verted to the following rule.
np( 4, ?Adjective
(predicative,negative,polite)?,
Dm({ H1=>?Adjective?,
K2=>?Basic Renyou Form? },
{ G=>???/nai/?,
H1=>?Postfix?, K2=>?Root Form? },
{ G=>???/desu/?,
H1=>?Auxiliary Verb? }) );
The function np() makes the declaration of the
rule, and the function Dm() describes a morphologi-
cal sequential pattern which matches the target. This
example means that this grammar element belongs
to Level 4, and can be detected by the pattern which
consists of three morphemes.
Type B rules are used to describe grammar ele-
ments such as conjugations including no functional
words. They are not generated automatically; they
are converted by hand from type M rules that are
generated automatically. For example, the rule de-
tecting the grammar element whose name is ?Adjec-
tive in Root Form? is defined as follows.
np( 4, ?Adjective in Root Form?,
Db( { H1=>?Adjective?,
K2=>?Root Form? } ) );
The function Db() describes a pattern which
matches a bunsetsu which consists of specified mor-
phemes. This example means that this grammar el-
ement belongs to Level 3, and shows the detection
pattern of this grammar element.
Converted Automatically
     + Modified by Hand
KNP
Juman
Detection
Converted
Automatically
Loaded
Sentence
Morphological
Analysis
Syntactic Analysis
+Detection against
  morphmes and 
  bunsetsues
Detection against
modifier-modifee
relationships
+ Lanking
KNP RuleRule Set
Check List
Sentence + Grammar Elements
Figure 1: System structure
Type R rules are used to describe grammar ele-
ments that include modifier-modifee relationships.
In the case of the grammar element whose name is
?Verb Modified by Adjective?, it includes a structure
that an adjective modifies a verb. It is impossible
to detect this grammar element by a morphological
continuous pattern, because any bunsetsus can be in-
serted between the adjective and the verb. For such a
grammar element, we introduce the function Dk()
that takes two arguments: the former is a modifier
and the latter is its modifee.
np( 4, ?Verb Modified by Adjective?,
Dk( Db({ H1=>?Adjective?,
K2=>?Basic Renyou Form? }),
Dm({ H1=>?Verb? }) ) );
3.2 The architecture of the detector
The architecture of the detector is shown in Figure 1.
The detector uses a morphological analyzer, Juman,
and a syntactic analyzer, KNP (Kurohashi and Na-
gao, 1994). The rule set is converted into the format
that KNP can read and it is added to the standard rule
set of KNP. This addition enables KNP to detect can-
didates of grammar elements. The ?Detection? part
selects final results from these candidates based on
preference information given by the rule set.
Figure 2 shows grammar elements detected by our
detector from the sentence ?
chizu
??
ha
?
oroka,
????
ryakuzu
??
sae
??
mo
?
kubarare
???
nakatta.
?????? which means ?Neither a
map nor a rough map was not distributed.?
4 Experiment
We conducted two experiments, in order to check
the performance of our detector.
Fragment Name Level
chizu
?? (a map) - -
ha
?
oroka
??? (neither) ?ha? oroka??? (neither ...) 1
? (,) ?? (comma) 4
ryakuzu
? ? (a rough map) - -
sae
?? (even) ? sae?? (even ...) 2
mo
? (nor) ?!? (huku postpositional particle means ?nor?) 4
kubarare
??? (distributed) ? reru?? (passive verb phrase) 3
nakatta
???? (was not) ? nai?? (predicative adjective means ?not?) 4
? (.) ?? (period) 4
Figure 2: Automatically detected grammar elements
The first test is a closed test, where we examine
whether grammar elements in example sentences of
TCS are detected correctly. TCS gives 840 example
sentences, and there are 802 sentences from which
their grammar elements are detected correctly. From
the rest 38 sentences, our detector failed to detect
the right grammar element. This result shows that
our program achieves the sufficient recall 95% in the
closed test. Almost of these errors are caused failure
of morphological analysis.
The second test is an open test, where we examine
whether grammar elements in example sentences of
the textbook, which is written for learners preparing
for the Japanese Language Proficiency Test (Tomo-
matsu et al, 1996), are detected correctly. The text-
book gives 1110 example sentences, and there are
680 sentences from which their grammar elements
are detected correctly. Wrong grammar elements
are detected from 71 sentences, and no grammar el-
ements are detected from the rest 359 sentences. So,
the recall of automatic detection of grammar ele-
ments is 61%, and the precision is 90%. The ma-
jor reason of these failures is strictness of several
rules; several rules that are generated from example
pairs automatically are overfitting to example pairs
so that they cannot detect variations in the textbook.
We think that relaxation of such rules will eliminate
these failures.
References
The Japan Foundation and Japan Association of Interna-
tional Education. 1994. Japanese Language Profi-
ciency Test: Test content Specifications (Revised Edi-
tion). Bonjin-sha Co.
Keith Johnson. 1978. Readability. http://www.
timetabler.com/readable.pdf.
Sadao Kurohashi and Makoto Nagao. 1994. A syntactic
analysis method of long Japanese sentences based on
the detection of conjunctive structures. Computational
Linguistics, 20(4).
Etsuko Tomomatsu, Jun Miyamoto, and Masako Waguri.
1996. Donna-toki Dou-tsukau Nihongo Hyougen
Bunkei 500. ALC Co.
Answer Validation by Keyword Association
Masatsugu Tonoike, Takehito Utsuro and Satoshi Sato
Graduate school of Informatics, Kyoto University
Yoshida-Honmachi, Sakyo-ku 606-8501 Kyoto, JAPAN
{tonoike,utsuro,sato}@pine.kuee.kyoto-u.ac.jp
Abstract
Answer validation is a component of question
answering system, which selects reliable answer
from answer candidates extracted by certain
methods. In this paper, we propose an approach
of answer validation based on the strengths of
lexical association between the keywords ex-
tracted from a question sentence and each an-
swer candidate. The proposed answer valida-
tion process is decomposed into two steps: the
first is to extract appropriate keywords from a
question sentence using word features and the
strength of lexical association, while the second
is to estimate the strength of the association
between the keywords and an answer candidate
based on the hits of search engines. In the re-
sult of experimental evaluation, we show that a
good proportion (79%) of a multiple-choice quiz
?Who wants to be a millionaire? can be solved
by the proposed method.
1 Introduction
The technology of searching for the answer of
a question written in natural language is called
?Question Answering?(QA), and has gotten a
lot of attention recently. Research activities of
QA have been promoted through competitions
such as TREC QA Track (Voorhees, 2004) and
NTCIR QAC (Fukumoto et al, 2004). Ques-
tion answering systems can be decomposed into
two steps: first step is to collect answer can-
didates, while the second is to validate each of
those candidates. The first step of collecting an-
swer candidates has been well studied so far. Its
standard technology is as follows: first, the an-
swer type of a question, such as LOCATION or
PERSON, is identified. Then, the documents
which may contain answer candidates are re-
trieved by querying available document set with
queries generated from the question sentence.
Finally, named entities which match the answer
type of the question sentence are collected from
the retrieved documents as answer candidates.
In this paper, we focus on the second step of
how to validate an answer candidate. Several
answer validation methods have been proposed.
One of the well-known approaches is that based
on deep understanding of text (e.g. Moldovan
et al (2003)). In the approach of answer valida-
tion based on deep understanding, first a ques-
tion and the paragraph including an answer can-
didate are parsed and transformed into logical
forms. Second, the validity of the answer candi-
date is examined through logical inference. One
drawback of this approach is that it requires a
rich set of lexical knowledge such as WordNet
and world knowledge such as the inference rule
set. Consequently, this approach is computa-
tionally expensive. In contrast, in this paper,
we propose another approach of answer vali-
dation, which is purely based on the estima-
tion of the strengths of lexical association be-
tween the keywords extracted from a question
sentence and each answer candidate. One un-
derlying motivation of this paper is to exam-
ine the effectiveness of quite low level semantic
operation such as measuring lexical association
against knowledge rich NLP tasks such as an-
swer validation of question answering. Surpris-
ingly, as we show later, given multiple-choices as
answer candidates of a question, a good propor-
tion of a certain set of questions can be solved
by our method based on lexical association.
In our framework of answer validation by key-
word association (in the remaining of this paper,
we call the notion of the lexical association in-
troduced above as ?keyword association?), the
answer validation process is decomposed into
two steps: the first step is to extract appro-
priate keywords from a question sentence, while
the second step is to estimate the strength of
the association between the keywords and an
answer candidate. We propose two methods for
the keyword selection step: one is by a small
number of hand-crafted rules for determining
word weights based on word features, while the
other is based on search engine hits. In the sec-
ond step of how to validate an answer candidate,
the web is used as a knowledge base for estimat-
ing the strength of the association between the
extracted keywords and an answer candidate.
Its basic idea is as follows: the stronger the as-
sociation between the keywords and an answer
candidate, the more frequently they co-occur on
the web. In this paper, we introduce several
measures for estimating the strength of the as-
sociation, and show their effectiveness through
experimental evaluation.
In this paper, in order to concentrate on the
issue of answer validation, but not the whole QA
processes, we use an existing multiple-choice
quiz as the material for our study. The multiple-
choice quiz we used is taken from ?Who wants to
be a millionaire?. ?Who wants to be a million-
aire? is a famous TV show, which originated in
the United Kingdom and has been localized in
more than fifty countries. We used the Japanese
version, which is produced by Fuji Television
Network, Inc.. In the experimental evaluation,
about 80% of the questions of this quiz can be
solved by the proposed method of answer vali-
dation by keyword association.
Section 2 introduces the idea of question an-
swering by keyword association. Section 3 de-
scribes how to select keywords from a question
sentence. Section 4 describes how to select the
answer of multiple-choice questions. Section 5
describes how to integrate the procedures of
keyword selection and answer selection. Sec-
tion 6 presents the results of experimental eval-
uations. Section 7 compares our work with sev-
eral related works Section 8 presents our con-
clusion and future works.
2 Answer Validation by Keyword
Association
2.1 Keyword Association
Here is an example of the multiple-choice quiz.
Q1: Who is the director of ?American Graffiti??
a: George Lucas
b: Steven Spielberg
c: Francis Ford Coppola
d: Akira Kurosawa
Suppose that you do not know the correct an-
swer and try to find it using a search engine
on the Web. The simplest way is to input the
query ?American Graffiti? to the search engine
and skim the retrieved pages. This strategy as-
sumes that the correct answer may appear on
the page that includes the keyword ?American
Graffiti?. A little cleverer way is to consider the
number of pages that contain both the keyword
and a choice. This number can be estimated
Table 1: Hits of Keywords and the Choices for
the Question Q1 (X:?American Graffiti?)
Y (choice) hits(X and Y )
?George Lucas? 15,500
?Steven Spielberg? 5,220
?Francis Ford Coppola? 4,800
?Akira Kurosawa? 836
from the hits of a search engine when you in-
put a conjunct query ?American Graffiti? and
?George Lucas?. Based on this assumption, it is
reasonable to hypothesize that the choice which
has the largest hits is the answer. For the above
question Q1, this strategy works. Table 1 shows
the hits of the conjunct queries for each of the
choices. We used ?google1? as a search engine.
Here, let X be the set of keywords, Y be the
choice. Function hits is defined as follows.
hits(X) ? hits(x
1
AND x
2
AND ? ? ?AND x
n
)
where
X = {x
1
, x
2
, . . . , x
n
}
The conjunct query with ?George Lucas?, which
is the correct answer, returns the largest hits.
Here, the question Q1 can be regarded as a
question on the strength of association between
keyword and an choice, and converted into the
following form.
Q1?: Select the one that has the strongest asso-
ciation with ?American Graffiti?.
a: George Lucas
b: Steven Spielberg
c: Francis Ford Coppola
d: Akira Kurosawa
We call this association between the keyword
and the choice as keyword association.
2.2 How to Select Keywords
It is important to select appropriate keywords
from a question sentence. Consider the follow-
ing question.
Q2: Who is the original author of the famous
movie ?Lord of the Rings??
a: Elijah Wood
b: JRR Tolkien
c: Peter Jackson
d: Liv Tyler
The numbers of hits are shown in Table 2. Here,
let X be ?Lord of the Rings?, X ? be ?Lord of the
1http://www.google.com
Table 2: Hits of Keywords and the Choices
for the Question Q2 (X:?Lord of the Rings?,
X ?:?Lord of the Rings? and ?original author?)
Y (choice) hits hits
(X and Y ) (X ? and Y )
?Elijah Wood? 682,000 213
?JRR Tolkien? 652,000 702
?Peter Jackson? 1,140,000 340
?Liv Tyler? 545,000 106
Rings? and ?original author?. When you select
the title of this movie ?Lord of the Rings? as a
keyword, the choice with the maximum hits is
?Peter Jackson?, which is not the correct an-
swer ?JRR Tolkien?. However, if you select
?Lord of the Rings? and ?original author? as
keywords, this question can be solved by select-
ing the choice with maximum hits. Therefore,
it is clear from this example that how to select
appropriate keywords is important.
2.3 Forward and Backward Association
For certain questions, it is not enough to gen-
erate a conjunct query consisting of some key-
words and a choice, and then to simply select
the choice with maximum hits. This section in-
troduces more sophisticated measures for select-
ing an appropriate answer. Consider the follow-
ing question.
Q3: Where is Pyramid?
a: Canada
b: Egypt
c: Japan
d: China
The numbers of hits are shown in Table 3. In
this case, given a conjunct query consisting of
a keyword ?Pyramid? and a choice, the choice
with the maximum hits, i.e., ?Canada? is not
the correct answer ?Egypt?. Why could not this
question be solved? Let us consider the hits of
the choices alone. The hits of the atomic query
?Canada? is about seven times larger than the
hits of the atomic query ?Egypt?. With this ob-
servation, we can hypothesize that the hits of a
conjunct query ?Pyramid? and a choice are af-
fected by the hits of the choice alone. Therefore
some normalization might be required.
Based on the analysis above, we employ the
metrics proposed by Sato and Sasaki (2003).
Sato and Sasaki (2003) has proposed two met-
rics for evaluating the strength of the relation
of two terms. Suppose that X be the set of
keywords and Y be the choice. In this paper,
we call the hits of a conjunct query consisting
of keywords X and a choice Y , which is nor-
malized by the hits of X, as forward association
FA(X, Y ). We also call the hits of a conjunct
query X and Y , which is normalized by the hits
of Y , as backward association BA(X, Y ).
FA(X, Y ) = hits(X ? {Y })/hits(X)
BA(X, Y ) = hits(X ? {Y })/hits({Y })
Note that when X is fixed, FA(X, Y ) is propor-
tional to hits(X ? {Y }).
Let?s go back to Q3. In this case, the choice
with the maximum BA is correct. Some ques-
tions may solved by referring to FA, while oth-
ers may be solved only by referring to BA.
Therefore, it is inevitable to invent a mecha-
nism which switches between FA and BA.
2.4 Summary
Based on the observation of Sections 2.1 ? 2.3,
the following three questions must be addressed
by answer validation based on keyword associ-
ation.
? How to select appropriate keywords from a
question sentence.
? How to identify the correct answer consid-
ering forward and/or backward association.
? How many questions can be solved by this
strategy based on keyword association.
3 Keyword Selection
This section describes two methods for selecting
appropriate keywords from a question sentence:
one is based on the features of each word, the
other based on hits of a search engine.
First, all the nouns are extracted from the
question sentence using a Japanese morpholog-
ical analyzer JUMAN(Kurohashi and Nagao,
1999) and a Japanese parser KNP(Kurohashi,
1998). Here, when the sequence of nouns con-
stitute a compound, only the longest compound
is extracted and their constituent nouns are not
extracted. Let N denote the set of those ex-
tracted nouns and compounds, from which key-
words are selected. In the following, the search
engine ?goo2? is used for obtaining the number
of hits.
2http://www.goo.ne.jp
Table 3: Hits of Keywords and the Choices for the Question Q3
X(keyword) hits(X)
Pyramid 3,170,000
Y(choice) hits(Y ) hits(Y and X) FA(X , Y ) BA(X , Y )
Canada 100,000,000 334,000 0.105 0.00334
Egypt 14,500,000 325,000 0.103 0.0224
Japan 63,100,000 246,000 0.0776 0.00390
China 53,600,000 225,000 0.0710 0.00420
3.1 Keyword Selection Based on Word
Features
In this method, keywords are selected by the
following procedure:
1. If the question sentence contains n quota-
tions with quotation marks ??? and ???,
those n quoted strings are selected as key-
words.
2. Otherwise:
2-1. According to the rules for word
weights in Table 4, weights are as-
signed to each element of the keyword
candidate set N .
2-2. Select the keyword candidate with the
maximum weight and that with the
second maximum weight.
2-3. i. If the hits of AND search of those
two keyword candidates are 15 or
more, both are selected as key-
words.
ii. Otherwise, select the one with the
maximum weight.
Let k denote the set of the selected keywords
(k ? N), we examine the correctness of k as
follows. Let c denote a choice, cF A
1
(k) the
choice with the maximum FA(k, c), and cBA
1
(k)
the choice with the maximum BA(k, c), respec-
tively.
cF A
1
(k) = argmax
c
FA(k, c)
c
BA
1
(k) = argmax
c
BA(k, c)
Here, we regard the selected keywords k to be
correct if either cF A
1
(k) or cBA
1
(k) is correct.
Against the development set which is to be in-
troduced in Section 6.1, the correct rate of the
keywords selected by the procedure above is
84.5%.
Table 4: Rules for Word Weights
rule weight
n-th segment (1 +
0.01 ? n)
stopword 0
quoted by quotation marks?? 3
person name 3
verbal nouns (?sahen?-verb stem) 0.5
word which expresses relation 2
Katakana 2
name of an award 2
name of an era 0.5
name of a country 0.5
number 3
hits > 1000000
and consists of one character 0.9
marked by a topic maker and
name of a job 0.1
hits > 100000 0.2
hits < 10000 1.1
number of characters = 1 0.2
number of characters = 2 0.25
number of characters = 3 0.5
number of characters = 4 1.1
number of characters ? 5 1.2
3.2 Keyword Selection Based on Hits
of Search Engine
3.2.1 Basic Methods
First, we introduce several basic methods for
selecting keywords based on hits of a search en-
gine. Let 2N denote the power set of N , where a
set of keywords k is an element of 2N (k ? 2N ).
Let k? denote the selected set of keywords and c?
the selected choice.
The first method is to simply select the pair
of ?k?, c?? which gives the maximum hits as below:
?k?, c?? = argmax
c, k?2
N
hits(k ? {c})
Against the development set, the correct rate of
the choice which is selected by this method is
35.7%.
In a similar way, another method which se-
lects the maximum FA or BA can be given as
below:
?k?, c?? = argmax
c, k?2
N
FA(k ? {c})
?k?, c?? = argmax
c, k?2
N
BA(k ? {c})
Their correct rates are 71.3% and 36.1%, respec-
tively.
3.2.2 Keyword Association Ratio
Next, we introduce more sophisticated meth-
ods which use the ratio of maximum and sec-
ond maximum associations such as FA or BA.
The underlying assumption of those methods
are that: the greater those ratios are, the more
reliable is the selected choice with the maximum
FA/BA. First, we introduce two methods: FA
ratio and BA ratio.
FA ratio This is the ratio of FA of the choice
with second maximum FA over one with maxi-
mum FA. FA ratio is calculated by the follow-
ing procedure.
1. Select the choices with maximum FA and
second maximum FA.
2. Estimate the correctness of the choice with
maximum FA by the ratio of their FAs.
The set k? of keywords and the choice c? to be
selected by FA ratio are expressed as below:
k? = argmin
k?2
N
FA(k, cF A
2
(k))
FA(k, cF A
1
(k))
c? = cF A
1
(k?)
c
F A
2
(k) = arg-secondmax
c
FA(k, c)
where arg-secondmax
c
is defined as a function
which selects c with second maximum value.
Similarly, the method based on BA ratio is
given as below:
BA ratio
k? = argmin
k?2
N
BA(k, cBA
2
(k))
BA(k, cBA
1
(k))
c? = cBA
1
(k?)
c
BA
2
(k) = arg-secondmax
c
BA(k, c)
Unlike the methods based on FA ratio and
BA ratio, the following two methods consider
both FA and BA. The motivation of those two
methods is to regard the decision by FA and
BA to be reliable if FA and BA agree on se-
lecting the choice.
Table 5: Evaluation of Keyword Association
Ratios (precision/coverage)(%)
max and second max
FA BA
ratio
FA 63.1/100 70.6/95.0
BA 75.8/93.2 67.6/100
BA ratio with maximum and second max-
imum FA
k? = argmin
k?2
N
BA(k, cF A
2
(k))
BA(k, cF A
1
(k))
c? = cF A
1
(k?)
FA ratio with maximum and second max-
imum BA
k? = argmin
k?2
N
FA(k, cBA
2
(k))
FA(k, cBA
1
(k))
c? = cBA
1
(k?)
Coverages and precisions of these four methods
against the development set are shown in Ta-
ble 5. Coverage is measured as the rate of ques-
tions for which the ratio is less than or equal to
13. Precisions are measured as the rate of ques-
tions for which the selected choice c? is the cor-
rect answer, over those covered questions. The
method having the greatest precision is BA ra-
tio with maximum and second maximum FA.
In the following sections, we use this ratio as
the keyword association ratio. Table 6 farther
examines the correlation of the range of the ra-
tio and the coverage/precision. When the ratio
is less than or equal to 0.25, about 60% of the
questions are solved with the precision close to
90%. This threshold of 0.25 is used in the Sec-
tion 5 when integrating the keyword association
ratio and word weights.
4 Answer Selection
In this section, we explain a method to identify
the correct answer considering forward and/or
backward association. After selecting keywords,
the following numbers are obtained by a search
engine.
? Hits of the keywords X: hits(X)
? Hits of the choice Y : hits({Y })
3For the ratios considering both FA and BA, the
ratio greater than 1 means that FA and BA disagree on
selecting the choice.
Table 6: Evaluation of Keyword Association
Ratio: BA ratio of FA max and second-max
ratio
# of questions
coverage precision
0 18.9% (163/888) 89.6% (146/163)
? 0.01 21.5% (191/888) 89.5% (171/191)
? 0.1 40.5% (360/888) 87.5% (315/360)
? 0.25 60.4% (536/888) 86.9% (466/536)
? 0.5 78.0% (693/888) 81.6% (566/693)
? 0.75 87.2% (774/888) 78.4% (607/774)
? 1 93.2% (828/888) 75.8% (628/828)
? Hits of the conjunct query:
hits(X ? {Y })
Then for each choice Y , FA and BA are cal-
culated. As introduced in section 3, cF A
1
(k) de-
notes the choice whose FA value is highest, and
cBA
1
(k) the choice whose BA value is highest.
What has to done here is to decide which of
cF A
1
(k) and cBA
1
(k) is correct.
After manually analyzing the search engine
hits against the development set, we hand-
crafted the following rules for switching between
cF A
1
(k) and cBA
1
(k).
1. if cF A
1
(k) = cBA
1
(k) then cF A
1
(k)
2. else if F A(k,c
BA
1
(k))
F A(k,c
FA
1
(k))
? 0.8 then cBA
1
(k)
3. else if F A(k,c
BA
1
(k))
F A(k,c
FA
1
(k))
? 0.2 then cF A
1
(k)
4. else if BA(k,c
FA
1
(k))
BA(k,c
BA
1
(k))
? 0.53 then cF A
1
(k)
5. else if hits(k) ? 1300 then cBA
1
(k)
6. else if F A(k,c
BA
1
(k))
F A(k,c
FA
1
(k))
? 0.6 then cBA
1
(k)
7. else cF A
1
(k)
Table 7 shows the results of evaluating preci-
sion of answer selection methods against the de-
velopment set, when the keywords are selected
based on word weights in Section 3.1. In the
table, in addition to the result of answer selec-
tion rules above, the results with baselines of
selecting the choice with maximum FA or BA
are also shown. It is clear from the table that
the answer selection rules described here signif-
icantly outperforms those baselines.
For each of the answer selection rules, Ta-
ble 8 shows its precision. In the development
set4, there are 541 questions (about 60%) where
4Four questions are excluded because hits of the con-
junct query hits(X ? {Y }) were 0
Table 7: Precision of Answer Selection (with
keyword selection by word weights)
method precision
max FA 70.8%
max BA 67.6%
selection rule 77.3%
Table 8: Evaluation of Each Answer Selection
Rule (with keyword selection by word weights)
rule answer precision
1 cFA
1
(k) = cBA
1
(k) 88.5% (479/541)
2 ? 6 - 60.3% (207/343)
total - 77.6% (686/884)
2 cBA
1
(k) 65.3% (32/49)
3 cFA
1
(k) 61.8% (68/110)
4 cFA
1
(k) 53.6% (37/69)
5 cBA
1
(k) 60.3% (35/58)
6 cBA
1
(k) 66.7% (12/18)
7 cFA
1
(k) 59.0% (23/39)
cF A
1
(k) and cBA
1
(k) are identical, and the 88.5%
of the selected choices are correct. This re-
sult shows that more than half of the questions
cF A
1
(k) is equal to cBA
1
(k) and about 90% of
these questions can be solved. This result shows
that whether FA and BA agree or not is very
important and is crucial for reliably selecting
the answer.
5 Total Procedure of Keyword
Selection and Answer Selection
Finally, the procedures of keyword selection and
answer selection presented in the previous sec-
tions are integrated as given below:
1. If ratio ? 0.25:
Use the set of keywords selected by BA ra-
tio with maximum and second maximum
FA. The choice to be selected is the one
with maximum BA.
2. Otherwise:
Use the set of keywords selected by word
weights. Answer selection is done by the
procedure of Section4.
6 Evaluation
6.1 Data Set
In this research, we used the card game ver-
sion of ?????????? (Who wants to be a
millionaire)?, which is sold by Tomy Company,
LTD. It has 1960 questions, which are classi-
fied into fifteen levels according to the amount
of prize money. Each question has four choices.
All questions are written in Japanese. The fol-
lowings give a few examples.
10,000 yen level
[A39]???????????????
??????
(Which continent are Egypt and
Kenya located in?)
A. ?????? (Africa)
B. ??????? (Eurasia)
C. ??????? (North America)
D. ??????? (South America)
[Correct Answer: ??????]
1,000,000 yen level
[J39] ???????????????
?????????????????
(What is the name of the ship in which
Columbus was sailing when he discov-
ered a new continent?)
A. ???????? (Atlantis)
B. ???? (Argo)
C. ??????? (Santa Maria)
D. ?????? (Nautilus)
[Correct Answer: ???????]
10,000,000 yen level
[O4] ???????????????
?????????????????
??
(In which summer Olympics did the
number of participating countries first
exceed 100?)
A. ????? (Rome Olympics)
B. ???? (Tokyo Olympics)
C. ?????? (Mexico Olympics)
D. ??????? (Munich Olympics)
[Correct Answer: ??????]
We divide questions of each level into two
halves: first of which is used as the develop-
ment set and the second as the test set. We
exclude questions with superlative expressions
(e.g., Out of the following four countries, select
the one with the maximum number of states.)
or negation (e.g., Out of the following four col-
ors, which is not used in the national flag of
France.) because they are not suitable for solv-
ing by keyword association. Consequently, the
development set comprises 888 questions, while
the test set comprises 906 questions. The num-
ber of questions per prize money amount is
shown in Table 9.
Table 9: The number of questions per prize
money amount
prize money amount # of questions
(yen) full dev test
10,000 160 71 74
20,000 160 71 77
30,000 160 67 70
50,000 160 75 71
100,000 160 73 73
150,000 160 76 72
250,000 160 71 77
500,000 160 74 77
750,000 160 78 71
1,000,000 160 73 76
1,500,000 120 53 58
2,500,000 90 38 42
5,000,000 70 30 32
7,500,000 50 24 21
10,000,000 30 14 15
total 1960 888 906
We compare the questions of ?Who wants to
be a millionaire? with those of TREC 2003 QA
track and those of NTCIR4 QAC2 task. The
questions of ?Who wants to be a millionaire?
are all classified as factoid question. They cor-
respond to TREC 2003 QA track factoid com-
ponent. The questions of NTCIR4 QAC2 are
also all classified as factoid question. We com-
pare bunsetsu 5 count of the questions of ?Who
wants to be a millionaire? with word count of
the questions of TREC 2003 QA track factoid
component and bunsetsu count of the questions
of NTCIR4 QAC2 Subtask1. The questions
of ?Who wants to be a millionaire? consist of
7.24 bunsetsu on average, while those of TREC
2003 QA track factoid component consist of 7.76
words on average, and those of NTCIR4 QAC2
Subtask1 consist of 6.19 bunsetsu on average.
Therefore, it can be concluded that the ques-
tions of ?Who wants to be a millionaire? are
not shorter than those of TREC 2003 QA track
and those of NTCIR4 QAC2 task.
6.2 Results
Against the development and the test sets,
Table 10 shows the results of evaluating the
total procedure of keyword selection and an-
swer selection presented in Section 5. The ta-
ble also shows the performance of baselines:
5A bunsetsu is one of the linguistic units in Japanese.
A bunsetsu consists of one content word possibly fol-
lowed by one or more function words.
Table 10: Total Evaluation Results (preci-
sion/coverage)(%)
method dev test
K.A.R. (r ? 1) 75.8/93.2 74.6/93.6
word weights
77.3/100 73.4/100
+ answer selection
Integration 78.6/100 75.9/100
K.A.R. (r ? 0.25) 86.9/60.4 86.0/61.5
word weights (r > 0.25)
65.9/39.6 59.9/38.5
+ answer selection
K.A.R.: keyword association ratio
?
??
??
??
??
??
??
??
??
??
???
??
???
?
??
???
?
??
???
?
??
???
?
??
???
??
??
???
??
??
???
??
??
???
??
??
???
??
???
??
???
?
???
??
???
?
???
??
???
?
???
??
???
?
???
??
???
?
??
???
???
??
????????????????????????
??
??
??
??
??
??
?
Figure 1: Precision classified by prize money
amount
i.e., keyword association ratio presented in Sec-
tion 3.2.2, and word weights of Section 3.1 +
answer selection of Section 4. Integration of
keyword association ratio and word weight out-
performs those baselines. In total, about 79%
(for the development set) and 76% (for the test
set) of the questions are solved by the proposed
answer validation method based on keyword as-
sociation.
Comparing the performance of the two data
sets, word weights + answer selection has 4%
lower precision in the test set. This result indi-
cates that rules for word weights as well as an-
swer selection rules overfit to the development
set. On the other hand, the difference of the pre-
cisions of the keyword association ratio is much
less between the two data sets, indicating that
keyword association ratio has less overfit to the
development set.
Finally, the result of the experiment where
the development set was solved by the inte-
gration method was classified by prize money
amount. The result is shown in Figure 1. The
more the prize money amount is, the lower the
precision seems to be, while their precisions are
all above 60%, and their differences are less than
20% in most cases. It can be concluded that our
system can solve questions of all the levels al-
most equally.
7 Related Work
Kwok et al (2001) proposed the first automated
question-answering system which uses the web.
First, it collects documents that are related to
the question sentence using google and picks an-
swer candidates up from them. Second, it se-
lects an answer based on the frequency of can-
didates which appear near the keywords.
In the method proposed by Brill et al (2002),
answer candidates are picked up from the sum-
mary pages returned by a search engine. Then,
each answer candidate is validated by searching
for relevant documents in the TREC QA docu-
ment collection. Both methods do not consider
the number of hits returned by the search en-
gine.
Magnini et al (2002) proposed an answer val-
idation method which uses the number of search
engine hits. They formulate search engine
queries using AltaVista?s OR and NEAR oper-
ators. Major difference between the method of
Magnini et al (2002) and ours is in keyword se-
lection. In the method of Magnini et al (2002),
the initial keywords are content words extracted
from a question sentence. If the hits of keywords
is less than a threshold, the least important key-
word is removed. This procedure is repeated un-
til the hits of the keywords is over the threshold.
On the other hand, in our method, keywords
are selected so that the strength of the associ-
ation between the keyword and an answer can-
didate is maximized. Intuitively, our method of
keyword selection is more natural than that of
Magnini et al (2002), since it considers both
the question sentence and an answer candidate.
As for measures for scoring answer candidates,
Magnini et al (2002) proposed three measures,
out of which ?Corrected Conditional Probabil-
ity? performs best. In our implementation, the
performance of ?Corrected Conditional Proba-
bility? is about 5% lower than our best result.
8 Conclusion and Future Work
In this paper, we proposed an approach of an-
swer validation based on the strengths of lexi-
cal association between the keywords extracted
from a question sentence and each answer can-
didate. The proposed answer validation process
is decomposed into two steps: the first is to
extract appropriate keywords from a question
sentence using word features and the strength
of lexical association, while the second is to es-
timate the strength of the association between
the keywords and an answer candidate based on
the hits of search engines. In the result of exper-
imental evaluation, we showed that a good pro-
portion (79%) of the multiple-choice quiz ?Who
wants to be a millionaire? can be solved by the
proposed method.
Future works include the followings: first, we
are planning to examine whether the syntactic
structures of the question sentence is useful for
selecting appropriate keywords from the ques-
tion sentence. Secondly, it is interesting to see
whether the keyword selection method proposed
in this paper is also effective for other applica-
tions such as answer candidate collection of the
whole question answering process.
References
E. Brill, J. Lin, M. Banko, S. Dumais, and A. Ng. 2002.
Data-intensive question answering. In Proc. TREC
2001.
J. Fukumoto, T. Kato, and F. Masui. 2004. Question
answering challenge for five ranked answers and list
answers -overview of ntcir4 qac2 subtask 1 and 2-. In
Proc. 4th NTCIR Workshop Meeting.
Sadao Kurohashi and Makoto Nagao, 1999. Japanese
Morphological Analysis System JUMAN version 3.62
Manual.
Sadao Kurohashi, 1998. Japanese Dependency/Case
Structure Analyzer KNP version 2.0b6 Manual.
C. C. T. Kwok, O. Etzioni, and D. S. Weld. 2001. Scal-
ing question answering to the web. In Proc. the 10th
WWW Conf., pages 150?161.
B. Magnini, M. Negri, R. Prevete, and H. Tanev. 2002.
Is it the right answer? exploiting web redundancy for
answer validation. In Proc. 40th ACL, pages 425?432.
D. Moldovan, S. Harabagiu, R. Girju, P. Morarescu, and
F. Lacatusu. 2003. Lcc tools for question answering.
In Proc. TREC 2002.
S. Sato and Y. Sasaki. 2003. Automatic collection of
related terms from the web. In Proc. 41st ACL, pages
121?124.
E. M. Voorhees. 2004. Overview of the trec 2003 ques-
tion answering track. In Proc. TREC 2003.
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 225?232
Manchester, August 2008
A Probabilistic Model for Measuring Grammaticality and Similarity
of Automatically Generated Paraphrases of Predicate Phrases
Atsushi Fujita Satoshi Sato
Graduate School of Engineering, Nagoya University
{fujita,ssato}@nuee.nagoya-u.ac.jp
Abstract
The most critical issue in generating and
recognizing paraphrases is development
of wide-coverage paraphrase knowledge.
Previous work on paraphrase acquisition
has collected lexicalized pairs of expres-
sions; however, the results do not ensure
full coverage of the various paraphrase
phenomena. This paper focuses on pro-
ductive paraphrases realized by general
transformation patterns, and addresses the
issues in generating instances of phrasal
paraphrases with those patterns. Our prob-
abilistic model computes how two phrases
are likely to be correct paraphrases. The
model consists of two components: (i) a
structured N -gram language model that
ensures grammaticality and (ii) a distribu-
tional similarity measure for estimating se-
mantic equivalence and substitutability.
1 Introduction
In many languages, a concept can be expressed
with several different linguistic expressions. Han-
dling such synonymous expressions in a given lan-
guage, i.e., paraphrases, is one of the key issues in
a broad range of natural language processing tasks.
For example, the technology for identifying para-
phrases would play an important role in aggregat-
ing the wealth of uninhibited opinions about prod-
ucts and services that are available on the Web,
from both the consumers and producers viewpoint.
On the other hand, whenever we draw up a docu-
ment, we always seek the most appropriate expres-
sion for conveying our ideas. In such a situation, a
system that generates and proposes alternative ex-
pressions would be extremely beneficial.
c
? Atsushi Fujita and Satoshi Sato, 2008. Licensed
under the Creative Commons Attribution-Noncommercial-
Share Alike 3.0 Unported license. Some rights reserved.
http://creativecommons.org/licenses/by-nc-sa/3.0/
Most of previous work on generating and recog-
nizing paraphrases has been dedicated to develop-
ing context-free paraphrase knowledge. It is typi-
cally represented with pairs of fragmentary expres-
sions that satisfy the following conditions:
Condition 1. Semantically equivalent
Condition 2. Substitutable in some context
The most critical issue in developing such
knowledge is ensuring the coverage of the para-
phrase phenomena. To attain this coverage, we
have proposed a strategy for dividing paraphrase
phenomena into the following two classes (Fujita
et al, 2007):
(1) Non-productive (idiosyncratic) paraphrases
a. burst into tears ? cried
b. comfort ? console
(Barzilay and McKeown, 2001)
(2) Productive paraphrases
a. be in our favor ? be favorable to us
b. show a sharp decrease ? decrease sharply
(Fujita et al, 2007)
Typical examples of non-productive paraphrases
are lexical paraphrases such as those shown in (1)
and idiomatic paraphrases of literal phrases (e.g.,
?kick the bucket? ? ?die?). Knowledge of this
class of paraphrases should be stored statically,
because they cannot be represented with abstract
patterns. On the other hand, a productive para-
phrase is one having a degree of regularity, as
exhibited by the examples in (2). It is therefore
reasonable to represent them with a set of general
patterns such as those shown in (3). This attains
a higher coverage, while keeping the knowledge
manageable.
(3) a. N
1
V N
2
? N
1
?s V -ing of N
2
b. N
1
V N
2
? N
2
be V -en by N
1
(Harris, 1957)
Various methods have been proposed to ac-
quire paraphrase knowledge (these are reviewed
in Section 2.1) where pairs of existing expres-
225
sions are collected from the given corpus, tak-
ing the above two conditions into account. On the
other hand, another issue arises when paraphrase
knowledge is generated from the patterns for pro-
ductive paraphrases such as shown in (3) by instan-
tiating variables with specific words, namely,
Condition 3. Both expressions are grammatical
This paper proposes a probabilistic model for
computing how likely a given pair of expressions
satisfy the aforementioned three conditions. In
particular, we focus on the post-generation assess-
ment of automatically generated productive para-
phrases of predicate phrases in Japanese.
In the next section, we review previous ap-
proaches and models. The proposed probabilis-
tic model is then presented in Section 3, where the
grammaticality factor and similarity factor are de-
rived from a conditional probability. In Section 4,
the settings for and results of an empirical exper-
iment are detailed. Finally, Section 5 summarizes
this paper.
2 Previous work
2.1 Acquiring paraphrase knowledge
The task of automatically acquiring paraphrase
knowledge is drawing the attention of an increas-
ing number of researchers. They are tackling the
problem of how precisely paraphrase knowledge
can be acquired, although they have tended to no-
tice that it is hard to acquire paraphrase knowl-
edge that ensures full coverage of the various
paraphrase phenomena from existing text corpora
alone. To date, two streams of research have
evolved: one acquires paraphrase knowledge from
parallel/comparable corpora, while the other uses
the regular corpus.
Several alignment techniques have been pro-
posed to acquire paraphrase knowledge from par-
allel/comparable corpora, imitating the techniques
devised for machine translation. Multiple trans-
lations of the same text (Barzilay and McKeown,
2001), corresponding articles from multiple news
sources (Barzilay and Lee, 2003; Quirk et al,
2004; Dolan et al, 2004), and bilingual corpus
(Bannard and Callison-Burch, 2005) have been
utilized. Unfortunately, this approach produces
only a low coverage because the size of the par-
allel/comparable corpora is limited.
In the second stream, i.e., paraphrase acquisition
from the regular corpus, the distributional hypothe-
sis (Harris, 1968) has been adopted. The similarity
of two expressions, computed from this hypothe-
sis, is called distributional similarity. The essence
of this measure is summarized as follows:
Feature representation: to compute the similar-
ity, given expressions are first mapped to
certain feature representations. Expressions
that co-occur with the given expression, such
as adjacent words (Barzilay and McKeown,
2001; Lin and Pantel, 2001), and modi-
fiers/modifiees (Yamamoto, 2002; Weeds et
al., 2005), have so far been examined.
Feature weighting: to precisely compute the sim-
ilarity, the weight for each feature is adjusted.
Point-wise mutual information (Lin, 1998)
and Relative Feature Focus (Geffet and Da-
gan, 2004) are well-known examples.
Feature comparison measures: to convert two
feature sets into a scalar value, several mea-
sures have been proposed, such as cosine,
Lin?s measure (Lin, 1998), Kullback-Leibler
(KL) divergence and its variants.
While most researchers extract fully-lexicalized
pairs of words or word sequences only, two algo-
rithms collect template-like knowledge using de-
pendency parsers. DIRT (Lin and Pantel, 2001)
collects pairs of paths in dependency parses that
connect two nominal entities. TEASE (Szpektor et
al., 2004) discovers dependency sub-parses from
theWeb, based on sets of representative entities for
a given lexical item. The output of these systems
contains the variable slots as shown in (4).
(4) a. X wrote Y ? X is the author of Y
b. X solves Y ? X deals with Y
(Lin and Pantel, 2001)
The knowledge in (4) falls between that in (1),
which is fully lexicalized, and that in (3), which
is almost fully abstracted. As a way of enrich-
ing such a template-like knowledge, Pantel et al
(2007) proposed the notion of inferential selec-
tional preference and collected expressions that
would fill those slots.
As mentioned in Section 1, the aim of the stud-
ies reviewed here is to collect paraphrase knowl-
edge. Thus, they need not to take the grammatical-
ity of expressions into account.
2.2 Generating paraphrase instances
Representing productive paraphrases with a set of
general patterns makes them maintainable and at-
tains a higher coverage of the paraphrase phe-
nomena. From the transformation grammar (Har-
226
ris, 1957), this approach has been adopted by
many researchers (Mel?c?uk and Polgue`re, 1987;
Jacquemin, 1999; Fujita et al, 2007). An impor-
tant issue arises when such a pattern is used to gen-
erate instances of paraphrases by replacing its vari-
ables with specific words. This involves assessing
the grammaticality of two expressions in addition
to their semantic equivalence and substitutability.
As a post-generation assessment of automati-
cally generated productive paraphrases, we have
applied distributional similarity measures (Fujita
and Sato, 2008). Our findings from a series of em-
pirical experiments are summarized as follows:
? Search engines are useful for retrieving the
contextual features of predicate phrases de-
spite some limitations (Kilgarriff, 2007).
? Distributional similarity measures produce a
tolerable level of performance.
The grammaticality of a phrase, however, is merely
assessed by issuing the phrase as a query to a com-
mercial search engine. Although a more frequent
expression is more grammatical, the length bias
should also be considered in the assessment.
Quirk et al (2004) built a paraphrase genera-
tion model from a monolingual comparable cor-
pus based on a statistical machine translation
framework, where the language model assesses
the grammaticality of the translations, i.e., gen-
erated expressions. The translation model, how-
ever, is not suitable for generating productive para-
phrases, because it learns word alignments at the
surface level. To cover all of the productive para-
phrases, we require an non-real comparable corpus
in which all instances of productive paraphrases
have a chance of being aligned. Furthermore, as
the translation model optimizes the word align-
ment at the sentence level, the substitutability of
the aligned word sequences cannot be explicitly
guaranteed.
2.3 Existing measures for paraphrases
To date, no model has been established that takes
into account all of the three aforementioned condi-
tions. With the ultimate aim of building an ideal
model, this section overviews the characteristics
and drawbacks of the four existing measures.
Lin?s measure
Lin (1998) proposed a symmetrical measure:
Par
Lin
(s ? t) =
?
f?F
s
?F
t
(w(s, f) + w(t, f))
?
f?F
s
w(s, f) +
?
f?F
t
w(t, f)
,
where F
s
and F
t
denote sets of features with posi-
tive weights for words s and t, respectively.
Although this measure has been widely cited
and has so far exhibited good performance, its
symmetry seems unnatural. Moreover, it may
not work well for dealing with general predicate
phrases because it is hard to enumerate all phrases
to determine the weights of features w(?, f). We
thus simply adopted the co-occurrence frequency
of the phrase and the feature as in (Fujita and Sato,
2008).
Skew divergence
The skew divergence, a variant of KL diver-
gence, was proposed in (Lee, 1999) based on an
insight: the substitutability of one word for another
need not be symmetrical. The divergence is given
by the following formula:
d
skew
(t, s) = D (P
s
??P
t
+ (1? ?)P
s
) ,
where P
s
and P
t
are the probability distributions
of features for the given original and substituted
words s and t, respectively. 0 ? ? ? 1 is a pa-
rameter for approximating KL divergence D. The
score can be recast into a similarity score via, for
example, the following function (Fujita and Sato,
2008):
Par
skew
(s?t) = exp (?d
skew
(t, s)) .
This measure offers an advantage: the weight
for each feature is determined theoretically. How-
ever, the optimization of ? is difficult because it
varies according to the task and even the data size
(confidence of probability distributions).
Translation-based conditional probability
Bannard and Callison-Burch (2005) proposed
a probabilistic model for acquiring phrasal para-
phrases1. The likelihood of t as a paraphrase of
the given phrase s is defined as follows:
P (t|s) =
?
f?tr(s)?tr(t)
P (t|f)P (f |s),
where tr (e) stands for a set of foreign language
phrases that are aligned with e in the given paral-
lel corpus. Parameters P (t|f) and P (f |s) are also
estimated using the given parallel corpus. A large-
scale parallel corpus may enable us to precisely ac-
quire a large amount of paraphrase knowledge. It
1In their definition, the term ?phrase? is a sequence of
words, while in this paper it designates the subtrees governed
by predicates (Fujita et al, 2007).
227
is not feasible, however, to build (or obtain) a par-
allel corpus in which all the instances of productive
paraphrases are translated to the same expression
in the other side of language.
3 Proposed probabilistic model
3.1 Formulation with conditional probability
Recall that our aim is to establish a measure that
computes the likelihood of a given pair of automat-
ically generated predicate phrases satisfying the
following three conditions:
Condition 1. Semantically equivalent
Condition 2. Substitutable in some context
Condition 3. Both expressions are grammatical
Based on the characteristics of the existing mea-
sures reviewed in Section 2.3, we propose a proba-
bilistic model. Let s and t be the source and target
predicate phrase, respectively. Assuming that s is
grammatical, the degree to which the above con-
ditions are satisfied is formalized as a conditional
probability P (t|s), as in (Bannard and Callison-
Burch, 2005). Then, assuming that s and t are
paradigmatic (i.e., paraphrases) and thus do not co-
occur, the proposed model is derived as follows:
P (t|s) =
?
f?F
P (t|f)P (f |s)
=
?
f?F
P (f |t)P (t)
P (f)
P (f |s)
= P (t)
?
f?F
P (f |t)P (f |s)
P (f)
,
where F denotes a set of features. The first
factor P (t) is called the grammaticality factor
because it quantifies the degree to which condi-
tion 3 is satisfied, except that we assume that
the given s is grammatical. The second factor
?
f?F
P (f |t)P (f |s)
P (f)
(Sim(s, t), hereafter), on the
other hand, is called the similarity factor because
it approximates the degree to which conditions 1
and 2 are satisfied by summing up the overlap of
the features of two expressions s and t.
The characteristics and advantages of the pro-
posed model are summarized as follows:
1) Asymmetric.
2) Grammaticality is assessed by P (t).
3) No heuristic is introduced. As the skew diver-
gence, the weight of the features can be simply
estimated as conditional probabilities P (f |t)
and P (f |s) and marginal probability P (f).
4) There is no need to enumerate all the phrases.
s and t are merely the given conditions.
The following subsections describe each factor.
3.2 Grammaticality factor
The factor P (t) quantifies how the phrase t is
grammatical using statistical language model.
Unlike English, in Japanese, predicates such as
verbs and adjectives do not necessarily determine
the order of their arguments, although they have
some preference. For example, both of the two
sentences in (5) are grammatical.
(5) a. kare-wa pasuta-o hashi-de taberu.
he-TOP pasta-ACC chopsticks-IMP to eat
He eats pasta with chopsticks.
b. kare-wa hashi-de pasuta-o taberu.
he-TOP chopsticks-IMP pasta-ACC to eat
He eats pasta with chopsticks.
This motivates us to use structured N -gram lan-
guage models (Habash, 2004). Given a phrase t,
its grammaticality P (t) is formulated as follows,
assuming a (N? 1)-th order Markov process for
generating its dependency structure T (t):
P (t) =
[
?
i=1...|T (t)|
P
d
(
c
i
|d
1
i
, d
2
i
, . . . , d
N?1
i
)
]
1/|T (t)|
,
where |T (t)| stands for the number of nodes in
T (t). To ignore the length bias of the target phrase,
a normalization factor 1/|T (t)| is introduced. dj
i
denotes the direct ancestor node of the i-th node
c
i
, where j is the distance from c
i
; for example, d1
i
and d2
i
are the parent and grandparent nodes of c
i
,
respectively.
Then, a concrete definition of the nodes in
the dependency structure is given. Widely-used
Japanese dependency parsers such as CaboCha2
and KNP3 consider a sequence of words as a node
called a ?bunsetsu? that consists of at least one
content word followed by a sequence of function
words if any. The hyphenated word sequences in
(6) exemplify those nodes.
(6) kitto kare-ha kyou-no
surely he-TOP today-GEN
kaigi-ni-ha ko-nai-daro-u.
meeting-DAT-TOP to come-NEG-must
He will surely not come to today?s meeting.
As bunsetsu can be quite long, involving more
than ten words, regarding it as a node makes
the model complex. Therefore, we compare the
2http://chasen.org/?taku/software/cabocha/
3http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp.html
228
<EOS>
. = punc
.  
u = aux
 
da = aux
 
nai = aux
i  
N: noun
V: verb
Adv: adverb
AdvN: adverbial noun
Pro: pronoun
cp: case particle
tp: topic-marking particle
ap: adnominal particle
aux: auxiliary verb
punc: punctuation
kuru = V
wa = tp
t
ni = cp
i
kaigi = N
i i
no = ap
kyou = AdvN
kitto = Adv
itt
kare = Pro
r
Japanese base-chunk
(bunsetsu)
wa = tp
t
Figure 1: MDS of sentence (6).
following two versions of dependency structures
whose nodes are smaller than bunsetsu.
MDS: Morpheme-based dependency structure
(Takahashi et al, 2001) regards a morpheme
as a node. MDS of sentence (6) is shown in
Figure 1.
CFDS: The node of a content-function-based de-
pendency structure is either a sequence of
content words or of function words. CFDS
of sentence (6) is shown Figure 2.
Structured N -gram language models were cre-
ated from 15 years of Mainichi newspaper articles4
using a dependency parser Cabocha, with N being
varied from 1 to 3. Then, the 3-gram conditional
probability P
d
(c
i
|d
1
i
, d
2
i
) is given by the linear in-
terpolation of those three models as follows:
P
d
(c
i
|d
1
i
, d
2
i
) = ?
3
P
ML
(c
i
|d
1
i
, d
2
i
)
+?
2
P
ML
(c
i
|d
1
i
)
+?
1
P
ML
(c
i
),
s.t.
?
j
?
j
= 1,
where mixture weights ?
j
are selected via an EM
algorithm using development data5 that has not
been used for estimating P
ML
.
3.3 Similarity factor
The similarity factor Sim(s, t) quantifies how two
phrases s and t are similar by comparing two sets
of contextual features f ? F for s and t.
4Mainichi 1991-2005 (1.5GB, 21M sentences).
5Yomiuri 2005 (350MB, 4.7M sentences) and Asahi 2005
(180MB, 2.7M sentences).
<EOS>
nai-daro-u-. = F
i .  
C: Content part
F: Function part
kuru = C
wa = F ni-wa = F
i
kaigi = C
i i
no = F
kyou = C
kitto = C
itt
kare = C
Japanese base-chunk
(bunsetsu)
Figure 2: CFDS of sentence (6).
We employ the following two types of feature
sets, which we have examined in our previous
work (Fujita and Sato, 2008), where a feature f
consists of an expression e and a relation r:
BOW: A pair of phrases is likely to be seman-
tically similar, if the distributions of the
words surrounding the phrases is similar.
The relation set R
BOW
contains only ?co-
occur in the same sentence?.
MOD: A pair of phrases is likely to be substi-
tutable with each other, provided they share
a number of instances of modifiers and mod-
ifiees: the set of the relation R
MOD
consists
of two relations ?modifier? and ?modifiee?.
Conditional probability distributions P (f |s)
and P (f |t) are estimated using a Web search en-
gine as in (Fujita and Sato, 2008). Given a phrase
p, snippets of Web pages are firstly obtained via
Yahoo API6 by issuing p as a query. The max-
imum number of snippets is set to 1,000. Then,
the features of the phrase are retrieved from those
snippets using a morphological analyzer ChaSen7
and CaboCha. Finally, the conditional probability
distribution P (f |p) is estimated as follows:
P (f |p) = P (?r, e?|p)
=
freq
sni
(p, r, e)
?
r
?
?R
?
e
?
freq
sni
(p, r
?
, e
?
)
,
where freq
sni
(p, r, e) stands for the frequency of
the expression e appealing with the phrase p in re-
lation r within the snippets for p.
The weight for features P (f) is estimated using
a static corpus based on the following equation:
P (f) = P (?r, e?)
=
freq
cp
(r, e)
?
r
?
?R
?
e
?
freq
cp
(r
?
, e
?
)
,
6http://developer.yahoo.co.jp/search/
7http://chasen.naist.jp/hiki/ChaSen/
229
where freq
cp
(r, e) indicates the frequency of the
expression e appearing with something in relation
r within the given corpus. Two different sorts of
corpora are separately used to build two variations
of P (f). The one is Mainichi, which is used for
building structured N -gram language models in
Section 3.2, while the other is a huge corpus con-
sisting of 470M sentences collected from the Web
(Kawahara and Kurohashi, 2006).
4 Experiments
4.1 Data
We conducted an empirical experiment to evalu-
ate the proposed model using the test suite devel-
oped in (Fujita and Sato, 2008). The test suite con-
sists of 176,541 pairs of paraphrase candidates that
are automatically generated using a pattern-based
paraphrase generation system (Fujita et al, 2007)
for 4,002 relatively high-frequency phrases sam-
pled from a newspaper corpus8.
To evaluate the system from a generation view-
point, i.e., how well a system can rank a correct
candidate first, we extracted paraphrase candidates
for 200 randomly sampled source phrases from
the test suite. Table 1 shows the statistics of the
test data. The ?All-Yield? column shows that the
number of candidates for a source phrase varies
considerably, which implies that the data contains
cases that have various difficulties. While the av-
erage number of candidates for each source phrase
was 48.3 (the maximum was 186), it was dramati-
cally reduced through extracting features for each
source and candidate paraphrase from Web snip-
pets: to 5.2 with BOW and to 4.8 with MOD. This
suggests that a large number of spurious phrases
were generated but discarded by going to the Web,
and the task was significantly simplified.
4.2 Questions
Through this experiment, we evaluated several ver-
sions of the proposed model to answer the follow-
ing questions:
Q1. Is the proposed model superior to existing
measures in practice? Par
Lin
and Par
skew
are regarded as being the baseline.
Q2. Which language model performs better at es-
timating P (t)? MDS and CFDS are com-
pared.
Q3. Which corpus performs better at estimating
P (f)? The advantage of Kawahara?s huge
8The grammaticality of the source phrases are guaranteed.
Table 1: Statistics of test data (?Ph.?: # of phrases).
Source All BOW MOD
Phrase type Ph. Ph. Yield Ph. Yield Ph. Yield
N :C:V 18 57 3.2 54 3.0 54 3.0
N
1
:N
2
:C:V 57 4,596 80.6 594 10.4 551 9.7
N :C:V
1
:V
2
54 4,767 88.3 255 4.7 232 4.3
N :C:Adv:V 16 51 3.2 39 2.4 38 2.4
Adj:N :C:V 2 8 4.0 5 2.5 5 2.5
N :C:Adj 53 173 3.3 86 1.6 83 1.6
Total 200 9,652 48.3 1,033 5.2 963 4.8
corpus (WebCP) over Mainichi is evaluated.
Q4. Which set of features performs better? In ad-
dition to BOW and MOD, the harmonic mean
of the scores derived from BOW and MOD is
examined (referred to as HAR).
Q5. Can the quality of P (f |s) and P (f |t) be im-
proved by using a larger number of snippets?
As the maximum number of snippets (N
S
),
we compared 500 and 1,000.
4.3 Results
Two assessors were asked to judge paraphrase can-
didates that are ranked first by either of the above
models if each candidate satisfies each of the three
conditions. The results for all the above options
are summarized in Table 2, where the strict preci-
sion is calculated based on those cases that gain
two positive judgements, while the lenient preci-
sion is for at least one positive judgement.
A1: Our greatest concern is the actual perfor-
mance of our probabilistic model. However, no
variation of the proposed model could outperform
the existing models (Par
Lin
and Par
skew
) that
only assess similarity. Furthermore, McNemer?s
test with p < 0.05 revealed that the precisions of
all the models, except the combination of CFDS
for P (t) and Mainichi for P (f), were significantly
worse than those of the best models.
To clarify the cause of these disappointing re-
sults, we investigated the performance of each fac-
tor. Table 3 shows how well the grammaticality
factors select a grammatical phrase, while Table 4
illustrates how well the similarity factors rank a
correct paraphrase first. As shown in these tables,
neither factor performed the task well, although
combinations produced a slight improvement in
performance. A detailed discussion is given below
in A2 for the grammaticality factors, and in A3-A5
for the similarity factors.
A2: Comparisons between MDS and CFDS
revealed that CFDS always produced better re-
sults than MDS not only when used for measuring
grammaticality (Table 3), but also when used as a
230
Table 2: Precision for 200 test cases.
N
S
= 500 Strict Lenient
Model BOW MOD HAR BOW MOD HAR
Par
Lin
78 (39%) 88 (44%) 87 (44%) 116 (58%) 128 (64%) 127 (64%)
Par
skew
81 (41%) 88 (44%) 88 (44%) 120 (60%) 127 (64%) 128 (64%)
MDS, Mainichi 72 (36%) 73 (37%) 76 (38%) 109 (55%) 112 (56%) 114 (57%)
MDS, WebCP 71 (36%) 73 (37%) 72 (36%) 108 (54%) 110 (55%) 113 (57%)
CFDS, Mainichi 79 (40%) 78 (39%) 83 (42%) 120 (60%) 119 (60%) 123 (62%)
CFDS, WebCP 79 (40%) 77 (39%) 80 (40%) 118 (59%) 116 (58%) 118 (59%)
N
S
= 1,000 Strict Lenient
Model BOW MOD HAR BOW MOD HAR
Par
Lin
79 (40%) 88 (44%) 88 (44%) 116 (58%) 128 (64%) 129 (65%)
Par
skew
84 (42%) 89 (45%) 89 (45%) 121 (61%) 128 (64%) 128 (64%)
MDS, Mainichi 72 (36%) 75 (38%) 76 (38%) 109 (55%) 114 (57%) 114 (57%)
MDS, WebCP 71 (36%) 74 (37%) 72 (36%) 109 (55%) 111 (56%) 113 (57%)
CFDS, Mainichi 79 (40%) 82 (41%) 83 (42%) 121 (61%) 121 (61%) 122 (61%)
CFDS, WebCP 79 (40%) 78 (39%) 79 (40%) 119 (60%) 116 (58%) 119 (60%)
Table 3: Precision of measuring grammaticality.
Model Strict Lenient
MDS 104 (52%) 141 (71%)
CFDS 108 (54%) 142 (71%)
Table 4: Precision of similarity factors.
Strict Lenient
N
S
Corpus BOW MOD HAR BOW MOD HAR
500 Mainichi 60 (30%) 68 (34%) 74 (37%) 98 (49%) 109 (55%) 114 (57%)
500 WebCP 57 (28%) 61 (31%) 74 (37%) 94 (47%) 99 (50%) 120 (60%)
1,000 Mainichi 57 (28%) 70 (35%) 74 (37%) 92 (46%) 113 (57%) 116 (58%)
1,000 WebCP 57 (28%) 60 (30%) 72 (36%) 93 (47%) 96 (48%) 116 (58%)
component of the entire model (Table 2). This re-
sult is quite natural because MDS cannot verify the
collocation between content words in those cases
where a number of function words appear between
them. On the other hand, CFDS with N = 3 could
verify this as a result of treating the sequence of
function words as a single node.
As mentioned in A1, however, a more sophisti-
cated language model must enhance the proposed
model. One way of obtaining a suitable granularity
of nodes is to introduce latent classes, such as the
Semi-Markov class model (Okanohara and Tsujii,
2007). The existence of many orthographic vari-
ants of both the content and function words may
prevent us from accurately estimating the gram-
maticality. We plan to normalize these variations
by using several existing resources such as the
Japanese functional expression dictionary (Mat-
suyoshi, 2008).
A3: Contrary to our expectations, the huge Web
corpus did not offer any advantage over the news-
paper corpus: Mainichi always produced better re-
sults than WebCP when it was combined with the
grammaticality factor or when MOD was used.
We can speculate that morphological and depen-
dency parsers produce errors when features are ex-
tracted, because they are tuned to newspaper arti-
cles. Likewise, P (f |s) and P (f |t) may involve
noise even though they are estimated using rela-
tively clean parts of Web text that are retrieved by
querying phrase candidates.
A4: For Par
Lin
and Par
skew
, different sets of
features led to consistent results with our previous
experiments in (Fujita and Sato, 2008), i.e., BOW
< MOD ' HAR. On the other hand, for the pro-
posed models, MOD and HAR led to only small
or sometimes negative effects. When the similar-
ity factor was used alone, however, these features
beat BOW. Furthermore, the impact of combining
BOW and MOD into HAR was significant.
Given this tendency, it is expected that the gram-
maticality factor might be excessively emphasized.
Our probability model was derived straightfor-
wardly from the conditional probability P (t|s);
however, the combination of the two factors should
be tuned according to their implementation.
A5: Finally, the influence of the number of Web
snippets was analyzed; no significant difference
was observed.
This is because we could retrieve more than 500
snippets for only 172 pairs of expressions among
our test samples. As it is time-consuming to ob-
tain a large number of Web snippets, the trade-off
between the number of Web snippets and the per-
formance should be investigated further, although
the quality of the Web snippets and what appears
at the top of the search results will vary according
to several factors other than linguistic ones.
231
5 Conclusion
A pair of expressions qualifies as paraphrases iff
they are semantically equivalent, substitutable in
some context, and grammatical. In cases where
paraphrase knowledge is represented with abstract
patterns to attain a high coverage of the paraphrase
phenomena, we should assess not only the first and
second conditions, but also the third condition.
In this paper, we proposed a probabilistic model
for computing how two phrases are likely to be
paraphrases. The proposed model consists of two
components: (i) a structured N -gram language
model that ensures grammaticality and (ii) a distri-
butional similarity measure for estimating seman-
tic equivalence and substitutability between two
phrases. Through an experiment, we empirically
evaluated the performance of the proposed model
and analyzed the characteristics.
Future work includes building a more sophis-
ticated structured language model to improve the
performance of the proposed model and conduct-
ing an experiment on template-like paraphrase
knowledge for other than productive paraphrases.
References
Bannard, Colin and Chris Callison-Burch. 2005. Paraphras-
ing with bilingual parallel corpora. In Proceedings of the
43rd Annual Meeting of the Association for Computational
Linguistics (ACL), pages 597?604.
Barzilay, Regina and Kathleen R. McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In Proceedings of
the 39th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 50?57.
Barzilay, Regina and Lillian Lee. 2003. Learning to
paraphrase: an unsupervised approach using multiple-
sequence alignment. In Proceedings of the 2003 Human
Language Technology Conference and the North American
Chapter of the Association for Computational Linguistics
(HLT-NAACL), pages 16?23.
Dolan, Bill, Chris Quirk, and Chris Brockett. 2004. Unsu-
pervised construction of large paraphrase corpora: exploit-
ing massively parallel news sources. In Proceedings of the
20th International Conference on Computational Linguis-
tics (COLING), pages 350?356.
Fujita, Atsushi, Shuhei Kato, Naoki Kato, and Satoshi Sato.
2007. A compositional approach toward dynamic phrasal
thesaurus. In Proceedings of the ACL-PASCAL Workshop
on Textual Entailment and Paraphrasing (WTEP), pages
151?158.
Fujita, Atsushi and Satoshi Sato. 2008. Computing para-
phrasability of syntactic variants using Web snippets. In
Proceedings of the 3rd International Joint Conference on
Natural Language Processing (IJCNLP), pages 537?544.
Geffet, Maayan and Ido Dagan. 2004. Feature vector qual-
ity and distributional similarity. In Proceedings of the
20th International Conference on Computational Linguis-
tics (COLING), pages 247?253.
Habash, Nizar. 2004. The use of a structural N-gram lan-
guage model in generation-heavy hybrid machine transla-
tion. In Proceedings of the 3rd International Natural Lan-
guage Generation Conference (INLG), pages 61?69.
Harris, Zellig. 1957. Co-occurrence and transformation in
linguistic structure. Language, 33(3):283?340.
Harris, Zellig. 1968. Mathematical structures of language.
John Wiley & Sons.
Jacquemin, Christian. 1999. Syntagmatic and paradigmatic
representations of term variation. In Proceedings of the
37th Annual Meeting of the Association for Computational
Linguistics (ACL), pages 341?348.
Kawahara, Daisuke and Sadao Kurohashi. 2006. Case frame
compilation from the Web using high-performance com-
puting. In Proceedings of the 5th International Conference
on Language Resources and Evaluation (LREC).
Kilgarriff, Adam. 2007. Googleology is bad science. Com-
putational Linguistics, 33(1):147?151.
Lee, Lillian. 1999. Measures of distributional similarity. In
Proceedings of the 37th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 25?32.
Lin, Dekang. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of the 36th Annual Meeting
of the Association for Computational Linguistics and the
17th International Conference on Computational Linguis-
tics (COLING-ACL), pages 768?774.
Lin, Dekang and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language En-
gineering, 7(4):343?360.
Matsuyoshi, Suguru. 2008. Hierarchically organized dictio-
nary of Japanese functional expressions: design, compi-
lation and application. Ph.D. thesis, Graduate School of
Informatics, Kyoto University.
Mel?c?uk, Igor and Alain Polgue`re. 1987. A formal lexicon
in meaning-text theory (or how to do lexica with words).
Computational Linguistics, 13(3-4):261?275.
Okanohara, Daisuke and Jun?ichi Tsujii. 2007. A discrimi-
native language model with pseudo-negative samples. In
Proceedings of the 45th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 73?80.
Pantel, Patrick, Rahul Bhagat, Bonaventura Coppola, Timo-
thy Chklovski, and Eduard Hovy. 2007. ISP: Learning
inferential selectional preferences. In Proceedings of Hu-
man Language Technologies 2007: The Conference of the
North American Chapter of the Association for Computa-
tional Linguistics (NAACL-HLT), pages 564?571.
Quirk, Chris, Chris Brockett, and William Dolan. 2004.
Monolingual machine translation for paraphrase genera-
tion. In Proceedings of the 2004 Conference on Empiri-
cal Methods in Natural Language Processing (EMNLP),
pages 142?149.
Szpektor, Idan, Hristo Tanev, Ido Dagan, and Bonaventura
Coppola. 2004. Scaling Web-based acquisition of entail-
ment relations. In Proceedings of the 2004 Conference
on Empirical Methods in Natural Language Processing
(EMNLP), pages 41?48.
Takahashi, Tetsuro, Tomoya Iwakura, Ryu Iida, Atsushi Fu-
jita, and Kentaro Inui. 2001. KURA: a transfer-based
lexico-structural paraphrasing engine. In Proceedings of
the 6th Natural Language Processing Pacific Rim Sym-
posium (NLPRS) Workshop on Automatic Paraphrasing:
Theories and Applications, pages 37?46.
Weeds, Julie, David Weir, and Bill Keller. 2005. The dis-
tributional similarity of sub-parses. In Proceedings of the
ACL Workshop on Empirical Modeling of Semantic Equiv-
alence and Entailment, pages 7?12.
Yamamoto, Kazuhide. 2002. Acquisition of lexical para-
phrases from texts. In Proceedings of the 2nd Interna-
tional Workshop on Computational Terminology (Com-
puTerm), pages 22?28.
232
Compiling French-Japanese Terminologies from the Web 
 
Xavier Robitaille?, Yasuhiro Sasaki?, Masatsugu Tonoike?,  
Satoshi Sato? and Takehito Utsuro? 
?Graduate School of Informatics, 
Kyoto University 
Yoshida-Honmachi, Sakyo-ku, 
Kyoto 606-8501 Japan 
?Graduate School of Engineering, 
Nagoya University 
Furo-cho, Chikusa-ku, 
Nagoya 464-8603 Japan 
{xavier, sasaki, tonoike, utsuro}@pine.kuee.kyoto-u.ac.jp, 
ssato@nuee.nagoya-u.ac.jp 
 
Abstract 
We propose a method for compiling bi-
lingual terminologies of multi-word 
terms (MWTs) for given translation pairs 
of seed terms. Traditional methods for bi-
lingual terminology compilation exploit 
parallel texts, while the more recent ones 
have focused on comparable corpora. We 
use bilingual corpora collected from the 
web and tailor made for the seed terms. 
For each language, we extract from the 
corpus a set of MWTs pertaining to the 
seed?s semantic domain, and use a com-
positional method to align MWTs from 
both sets. We increase the coverage of 
our system by using thesauri and by ap-
plying a bootstrap method. Experimental 
results show high precision and indicate 
promising prospects for future develop-
ments.  
1 Introduction 
Bilingual terminologies have been the center of 
much interest in computational linguistics. Their 
applications in machine translation have proven 
quite effective, and this has fuelled research aim-
ing at automating terminology compilation. Early 
developments focused on their extraction from 
parallel corpora (Daille et al (1994), Fung 
(1995)), which works well but is limited by the 
scarcity of such resources. Recently, the focus 
has changed to utilizing comparable corpora, 
which are easier to obtain in many domains. 
Most of the proposed methods use the fact that 
words have comparable contexts across lan-
guages. Fung (1998) and Rapp (1999) use so 
called context vector methods to extract transla-
tions of general words. Chiao and Zweigenbaum 
(2002) and D?jean and Gaussier (2002) apply 
similar methods to technical domains. Daille and 
Morin (2005) use specialized comparable cor-
pora to extract translations of multi-word terms 
(MWTs).  
These methods output a few thousand terms 
and yield a precision of more or less 80% on the 
first 10-20 candidates. We argue for the need for 
systems that output fewer terms, but with a 
higher precision. Moreover, all the above were 
conducted on language pairs including English. 
It would be possible, albeit more difficult, to ob-
tain comparable corpora for pairs such as 
French-Japanese. We will try to remove the need 
to gather corpora beforehand altogether. To 
achieve this, we use the web as our only source 
of data. This idea is not new, and has already 
been tried by Cao and Li (2002) for base noun 
phrase translation. They use a compositional 
method to generate a set of translation candidates 
from which they select the most likely translation 
by using empirical evidence from the web.  
The method we propose takes a translation 
pair of seed terms in input. First, we collect 
MWTs semantically similar to the seed in each 
language. Then, we work out the alignments be-
tween the MWTs in both sets. Our intuition is 
that both seeds have the same related terms 
across languages, and we believe that this will 
simplify the alignment process. The alignment is 
done by generating a set of translation candidates 
using a compositional method, and by selecting 
the most probable translation from that set. It is 
very similar to Cao and Li?s, except in two re-
spects. First, the generation makes use of 
thesauri to account for lexical divergence be-
tween MWTs in the source and target language. 
Second, we validate candidate translations using 
a set of terms collected from the web, rather than 
using empirical evidence from the web as a 
whole. Our research further differs from Cao and 
Li?s in that they focus only on finding valid 
translations for given base noun phrases. We at-
tempt to both collect appropriate sets of related 
MWTs and to find their respective translations. 
The initial output of the system contains 9.6 
pairs on average, and has a precision of 92%.  
We use this high precision as a bootstrap to 
augment the set of Japanese related terms, and 
obtain a final output of 19.6 pairs on average, 
with a precision of 81%. 
2 Related Term Collection 
Given a translation pair of seed terms (sf, sj), we 
use a search engine to gather a set F of French 
terms related to sf, and a set J of Japanese terms 
related to sj. The methods applied for both lan-
guages use the framework proposed by Sato and 
Sasaki (2003), outlined in Figure 1. We proceed 
in three steps: corpus collection, automatic term 
recognition (ATR), and filtering.   
2.1 Corpus Collection 
For each language, we collect a corpus C from 
web pages by selecting passages that contain the 
seed. 
Web page collection 
In French, we use Google to find relevant web 
pages by entering the following three queries: 
?sf?, ?sf est? (sf is), and ?sf sont? (sf are). In Japa-
nese, we do the same with queries ?sj?, ?sj???, 
?sj??, ?sj????, and ?sj??, where ?? toha, 
? ha, ??? toiu, and ? no are Japanese func-
tional words that are often used for defining or 
explaining a term. We retrieve the top pages for 
each query, and parse those pages looking for 
hyperlinks whose anchor text contain the seed. If 
such links exist, we retrieve the linked pages as 
well. 
Sentence extraction 
From the retrieved web pages, we remove html 
tags and other noise. Then, we keep only prop-
erly structured sentences containing the seed, as 
well as the preceding and following sentences ? 
that is, we use a window of three sentences 
around the seed. 
2.2 Automatic Term Recognition 
The next step is to extract candidate related terms 
from the corpus. Because the sentences compos-
ing the corpus are related to the seed, the same 
should be true for the terms they contain. The 
process of extracting terms is highly language 
dependent. 
French ATR 
We use the C-value method (Frantzi and 
Ananiadou (2003)), which extracts compound 
terms and ranks them according to their term-
hood. It consists of a linguistic part, followed by 
a statistical part. 
The linguistic part consists in applying a lin-
guistic filter to constrain the structure of terms 
extracted. We base our filter on a morphosyntac-
tic pattern for the French language proposed by 
Daille et al It defines the structure of multi-word 
units (MWUs) that are likely to be terms. Al-
though their work focused on MWUs limited to 
two content words (nouns, adjectives, verbs or 
adverbs), we extend our filter to MWUs of 
greater length. The pattern is defined as follows: 
( ) ( )( )+NumNounDetPrepAdjNumNoun ?  
The statistical part measures the termhood of 
each compound that matches the linguistic pat-
tern. It is given by the C-value:  
( )
( )
( )
( )
( )
??
??
?
??
??
?
?
??
?
?
?
??
?
?
?
?=?
?
?
otherwise
T
b
aaa
nestednotisaif
aa
a
a
Tb a
P
f
f)f(log
,
flog
valueC
2
2
 
where a is the candidate string, f(a) is its fre-
quency of occurrence in all the web pages re-
trieved, Ta is the set of extracted candidate terms 
that contain a, and P(Ta) is the number of these 
candidate terms. 
The nature of our variable length pattern is 
such that if a long compound matches the pat-
tern, all the shorter compounds it includes also 
match. For example, consider the N-Prep-N-
 
 
 
related term sets 
(F, J)
 the  Web ATR 
Filtering 
 
 Corpus collection 
corpora 
(Cf, Cj) 
 
term sets 
(Xf, Xj) 
seed terms
(sf, sj) 
Figure 1: Related term collection 
Prep-N structure in syst?me ? base de connais-
sances (knowledge based system). The shorter 
candidate syst?me ? base (based system) also 
matches, although we would prefer not to extract 
it. 
Fortunately, the strength of the C-value is the 
way it effectively handles nested MWTs. When 
we calculate the termhood of a string, we sub-
tract from its total frequency its frequency as a 
substring of longer candidate terms. In other 
words, a shorter compound that almost always 
appears nested in a longer compound will have a 
comparatively smaller C-value, even if its total 
frequency is higher than that of the longer com-
pound. Hence, we discard MWTs whose C-value 
is smaller than that of a longer candidate term in 
which it is nested. 
Japanese ATR 
Because compound nouns represent the bulk of 
Japanese technical MWTs, we extract them as 
candidate related terms. As opposed to Sato and 
Sasaki, we ignore single nouns. Also, we do not 
limit the number of candidates output by ATR as 
they did.  
2.3 Filtering 
Finally, from the output set of ATR, we select 
only the technical terms that are part of the 
seed?s semantic domain. Numerous measures 
have been proposed to gauge the semantic simi-
larity between two words (van Rijsbergen 
(1979)). We choose the Jaccard coefficient, 
which we calculate based on search engine hit 
counts. The similarity between a seed term s and 
a candidate term x is given by: ( )
( )xsH
xsHJac ?
?=  
where H(s ? x) is the hit count of pages contain-
ing both s and x, and H(s ? x) is the hit count of 
pages containing s or x. The latter can be calcu-
lated as follows: 
( ) ( ) ( )xsHxHsHxsH ??+=? )(  
Candidates that have a high enough coefficient 
are considered related terms of the seed.  
3 Term Alignment 
Once we have collected related terms in both 
French and Japanese, we must link the terms in 
the source language to the terms in the target 
language. Our alignment procedure is twofold. 
First, we first generate Japanese translation can-
didates for each collected French term. Second, 
we select the most likely translation(s) from the 
set of candidates. This is similar to the genera-
tion and selection procedures used in the litera-
ture (Baldwin and Tanaka (2004), Cao and Li, 
Langkilde and Knight (1998)). 
3.1 Translation Candidates Generation 
Translation candidates are generated using a 
compositional method, which can be divided in 
three steps. First, we decompose the French 
MWTs into combinations of shorter MWU ele-
ments. Second, we look up the elements in bilin-
gual dictionaries. Third, we recompose transla-
tion candidates by generating different combina-
tions of translated elements. 
Decomposition 
In accordance with Daille et al, we define the 
length of a MWU as the number of content 
words it contains. Let n be the length of the 
MWT to decompose. We produce all the combi-
nations of MWU elements of length less or equal 
to n. For example, consider the French transla-
tion of ?knowledge based system?: 
It has a length of three and yields the following 
four combinations1: 
Note the treatment given to the prepositions 
and determiners: we leave them in place when 
they are interposed between content words 
within elements, otherwise we remove them. 
Dictionary Lookup 
We look up each element in bilingual dictionar-
ies. Because some words appear in their inflected 
forms, we use their lemmata. In the example 
given above, we look up connaissance (lemma) 
rather than connaissances (inflected). Note that 
we do not lemmatize MWUs such as base de 
connaissances. This is due to the complexity of 
gender and number agreements of French com-
pounds. However, only a small part of the 
MWTs are collected in their inflected forms, and 
French-Japanese bilingual dictionaries do not 
contain that many MWTs to begin with. The per-
formance hit should therefore be minor.  
Already at this stage, we can anticipate prob-
lems arising from the insufficient coverage of 
                                                 
1 A MWT of length n produces 2n-1 combinations, 
including itself. 
syst?me ? base de connaissances
Noun Prep Noun Prep Noun 
[syst?me ? [base de [connaissances]
[syst?me]  [base de [connaissances]
[syst?me ? [base]  [connaissances]
[syst?me]  [base]  [connaissances]
French-Japanese lexicon resources. Bilingual 
dictionaries may not have enough entries, and  
existing entries may not include a great variety of 
translations for every sense. The former problem 
has no easy solution, and is one of the reasons 
we are conducting this research. The latter can be 
partially remedied by using thesauri ? we aug-
ment each element?s translation set by looking 
up in thesauri all the translations obtained with 
bilingual dictionaries. 
Recomposition 
To recompose the translation candidates, we 
simply generate all suitable combinations of 
translated elements for each decomposition. The 
word order is inverted to take into account the 
different constraints in French and Japanese. In 
the example above, if the lookup phase gave {?
? chishiki}, {?? dodai, ??? besu} and {?
? taikei, ???? shisutemu} as respective 
translation sets for syst?me, base and connais-
sance, the fourth decomposition given above 
would yield the following candidates: 
connaissance base syst?me 
?? ?? ?? 
?? ?? ????
?? ??? ?? 
?? ??? ????
If we do not find any translation for one of the 
elements, the generation fails. 
3.2 Translation Selection  
Selection consists of picking the most likely 
translation from the translation candidates we 
have generated. To discern the likely from the 
unlikely, we use the empirical evidence provided 
by the set of Japanese terms related to the seed. 
We believe that if a candidate is present in that 
set, it could well be a valid translation, as the 
French MWT in consideration is also related to 
the seed. Accordingly, our selection process con-
sists of picking those candidates for which we 
find a complete match among the related terms.  
3.3 Relevance of Compositional Methods 
The automatic translation of MWTs is no simple 
task, and it is worthwhile asking if it is best tack-
led with a compositional method. Intricate prob-
lems have been reported with the translations of 
compounds (Daille and Morin, Baldwin and Ta-
naka), notably:  
? fertility: source and target MWTs can be 
of different lengths. For example, table 
de v?rit? (truth table) contains two con-
tent words and translates into ??????
shinri ? chi ? hyo (lit. truth-value-table), 
which contains three. 
? variability of forms in the transla-
tions: MWTs can appear in many forms. 
For example, champ electromagn?tique 
(electromagnetic field) translates both 
into ???? denji? ba (lit. electromag-
netic field)???? denji?kai (lit. elec-
tromagnetic ?region?). 
? constructional variability in the trans-
lations: source and target MWTs have 
different morphological structures. For 
example, in the pair apprentissage auto-
matique??? ???  kikai ? gakushu 
(machine learning) we have (N-
Adj)?(N-N). In the pair programmation 
par contraintes???????? patan?
ninshiki (pattern recognition) we have 
(N-par-N)?(N-N). 
? non-compositional compounds: some 
compounds? meaning cannot be derived 
from the meaning of their components. 
For example, the Japanese term ???
aka?ten (failing grade, lit. ?red point?) 
translates into French as note d??chec (lit. 
failing grade) or simply ?chec (lit. fail-
ure).  
? lexical divergence: source and target 
MWTs can use different lexica to ex-
press a concept. For example, traduction 
automatique (machine translation, lit. 
?automatic translation?) translates as ?
???? kikai ? honyaku (lit. machine 
translation). 
It is hard to imagine any method that could ad-
dress all these problems accurately.  
Tanaka and Baldwin (2003) found that 48.7% 
of English-Japanese Noun-Noun compounds 
translate compositionality. In a preliminary ex-
periment, we found this to be the case for as 
much as 75.1% of the collected MWTs. If we are 
to maximize the coverage of our system, it is 
sensible to start with a compositional approach. 
We will not deal with the problem of fertility and 
non-compositional compounds in this paper. 
Nonetheless, lexical divergence and variability 
issues will be partly tackled by broader transla-
tions and related words given by thesauri. 
4 Evaluation 
4.1 Linguistic Resources 
The bilingual dictionaries used in the experi-
ments are the Crown French-Japanese Dictionary 
(Ohtsuki et al (1989)), and the French-Japanese 
Scientific Dictionary (French-Japanese Scientific 
Association (1989)). The former contains about 
50,000 entries of general usage single words. 
The latter contains about 50,000 entries of both 
single and multi-word scientific terms. These 
two complement each other, and by combining 
both entries we form our base dictionary to 
which we refer as DicFJ. 
The main thesaurus used is Bunrui Goi Hyo 
(National Institute for Japanese Language 
(2004)). It contains about 96,000 words, and 
each entry is organized in two levels: a list of 
synonyms and a list of more loosely related 
words. We augment the initial translation set by 
looking up the Japanese words given by DicFJ. 
The expanded bilingual dictionary comprised of 
the words from DicFJ combined with their syno-
nyms is denoted DicFJJ. The dictionary resulting 
of DicFJJ combined with the more loosely related 
words is denoted DicFJJ2. 
Finally, we build another thesaurus from a 
Japanese-English dictionary. We use Eijiro 
(Electronic Dictionary Project (2004)), which 
contains 1,290,000 entries. For a given Japanese 
entry, we look up its English translations. The 
Japanese translations of the English intermediar-
ies are used as synonyms/related words of the 
entry. The resulting thesaurus is expected to pro-
vide even more loosely related translations (and 
also many irrelevant ones). We denote it DicFJEJ. 
4.2 Notation 
Let F and J be the two sets of related terms col-
lected in French and Japanese. F? is the subset of 
F for which Jac?0.01: { }01.0)(' ??= fJacFfF  
F?* is the subset of valid related terms in F?, as 
determined by human evaluation. P is the set of 
all potential translation pairs among the collected 
terms (P=F?J). P? is the set of pairs containing 
either a French term or a Japanese term with 
Jac?0.01: 
( ){ }01.0)(01.0)(,' ?????= jJacfJacJjFfP  
P?* is the subset of valid translation pairs in P?, 
determined by human evaluation. These pairs 
need to respect three criteria: 1) contain valid 
terms, 2) be related to the seed, and 3) constitute 
a valid translation. M is the set of all translations 
selected by our system. M? is the subset of pairs 
in M with Jac?0.01 for either the French or the 
Japanese term. It is also the output of our system: { }01.0)(01.0)(),(' ????= jJacfJacMjfM  
M?* is the intersection of M? and P?*, or in other 
words, the subset of valid translation pairs output 
by our system. 
4.3 Baseline Method 
Our starting point is the simplest possible align-
ment, which we refer to as our baseline. It is 
worked out by using each of the aforementioned 
dictionaries independently. The output set ob-
tained using DicFJ is denoted FJ, the one using 
DicFJJ is denoted FJJ, and so on. The experiment 
is made using the eight seed pairs given in Table 
1. On average, we have |F'| =74.3, |F'*|=51.0 and 
|P'*|=24.0. Table 2 gives a summary of the key 
results. The precision and the recall are given by: 
'
'*
M
M
precision =  , 
'*
'*
P
M
recall =  
DicFJ contains only Japanese translations cor-
responding to the strict sense of French elements. 
Such a dictionary generates only a few transla-
tion candidates which tend to be correct when 
present in the target set. On the other hand, the 
lookup in DicFJJ2 and DicFJEJ interprets French 
Set |M'| |M'*| Prec. Recall 
FJ 10.5 9.6  92% 40% 
FJJ 15.3 12.6  83% 53% 
FJJ2 20.5 13.4  65% 56% 
FJEJ 30.9 14.1  46% 59% 
Table 2: Results for the baseline 
Id French Japanese (English)
1 analyse vectorielle ??????? bekutoru?kaiseki (vector analysis) 
2 circuit logique ????? ronri?kairo (logic circuit) 
3   intelligence artificielle          ????? jinko?chinou (artificial intelligence) 
4 linguistique informatique ?????? keisan?gengogaku (computational linguistics) 
5 reconnaissance des formes ??????? patan?ninshiki (pattern recognition) 
6 reconnaissance vocale ????? onsei?ninshiki (speech recognition) 
7 science cognitive ????? ninchi?kagaku (cognitive science) 
8 traduction automatique ????? kikai?honyaku (machine translation) 
Table 1: Seed pairs 
MWT elements with more laxity, generating 
more translations and thus more alignments, at 
the cost of some precision. 
4.4 Incremental Selection 
The progressive increase in recall given by the 
increasingly looser translations is in inverse pro-
portion to the decrease in precision, which hints 
that we should give precedence to the alignments 
obtained with the more accurate methods. Con-
sequently, we start by adding the alignments in 
FJ to the output set. Then, we augment it with 
the alignments from FJJ whose terms are not 
already in FJ. The resulting set is denoted FJJ'. 
We then augment FJJ' with the pairs from FJJ2 
whose terms are not in FJJ', and so on, until we 
exhaust the alignments in FJEJ.  
For instance, let FJ contain (synth?se de la 
parole? ? ? ? ? ? onsei ? gousei (speech 
synthesis)) and FJJ contain this pair plus 
(synth?se de la parole?????? onsei?kaiseki 
(speech analysis)). In the first iteration, the pair 
in FJ is added to the output set. In the second 
iteration, no pair is added because the output set 
already contains an alignment with synth?se de 
la parole. 
Table 3 gives the results for each incremental 
step. We can see an increase in precision for FJJ', 
FJJ2' and FJEJ' of respectively 5%, 9% and 8%, 
compared to FJJ, FJJ2 and FJEJ. We are effec-
tively filtering output pairs and, as expected, the 
increase in precision is accompanied by a slight 
decrease in recall.  Note that, because FJEJ is 
not a superset of FJJ2, we see an increase in both 
precision and recall in FJEJ' over FJEJ. None-
theless, the precision yielded by FJEJ' is not suf-
ficient, which is why DicFJEJ is left out in the 
next experiment. 
4.5 Bootstrapping 
The coverage of the system is still shy of the 20 
pairs/seed objective we gave ourselves. One 
cause for this is the small number of valid trans-
lation pairs available in the corpora. From an 
average of 51 valid related terms in the source 
set, only 24 have their translation in the target set. 
To counter that problem, we increase the cover-
age of Japanese related terms and hope that by 
doing so, we will also increase the coverage of 
the system as a whole.  
Once again, we utilize the high precision of 
the baseline method. The average 10.5 pairs in 
FJ include 92% of Japanese terms semantically 
similar to the seed. By inputting these terms in 
the term collection system, we collect many 
more terms, some of which are probably the 
translations of our French MWTs. 
The results for the baseline method with boot-
strapping are given in Table 4. The ones using 
incremental selection and bootstrapping are 
given in Table 5. FJ+ consists of the alignments 
given by a generation process using DicFJ and a 
selection performed on the augmented set of re-
lated terms. FJJ+ and FJJ2+ are obtained in the 
same way using DicFJJ and DicFJJ2. FJ+' contains 
the alignments from FJ, augmented with those 
from FJ+ whose terms are not in FJ. FJJ+' con-
tains FJ+', incremented with terms from FJJ. 
FJJ+'' contains FJJ+', incremented with terms 
from FJJ+, and so on.  
The bootstrap mechanism grows the target 
term set tenfold, making it very laborious to 
identify all the valid translation pairs manually. 
Consequently, we only evaluate the pairs output 
by the system, making it impossible to calculate 
recall. Instead, we use the number of valid trans-
lation pairs as a makeshift measure. 
Bootstrapping successfully allows for many 
more translation pairs to be found. FJ+, FJJ+, 
and FJJ2+ respectively contain 7.6, 8.7 and 8.5 
more valid alignments on average than FJ, FJJ 
and FJJ2. The augmented target term set is nois-
ier than the initial set, and it produces many more 
invalid alignments as well. Fortunately, the in-
cremental selection effectively filters out most of 
the unwanted, restoring the precision to accept-
able levels.  
Set |M'| |M'*| Prec. Recall 
FJJ' 14.0  12.3  88% 51% 
FJJ2' 16.1  12.8  79% 53% 
FJEJ' 29.1  15.5  53% 65% 
Table 3: Results for the incremental selection 
Set |M'| |M'*| Prec. 
FJ+' 19.5 16.1  83% 
FJJ+' 22.5 18.6  83% 
FJJ +'' 24.3 19.6  81% 
FJJ2+' 25.6 20.1  79% 
FJJ2+'' 28.6 20.6  72% 
Table 5: Results for the incremental 
selection with bootstrap expansion 
Set |M'| |M'*| Prec. 
FJ+ 20.9 16.8  80% 
FJJ+ 30.9 21.3  69% 
FJJ2+ 45.8 22.6  49% 
Table 4: Results for the baseline 
method with bootstrap expansion 
4.6 Analysis 
A comparison of all the methods is illustrated in 
the precision ? valid alignments curves of Figure 
2. The points on the four curves are taken from 
Tables 2 to 5. The gap between the dotted and 
filled curves clearly shows that bootstrapping 
increases coverage. The respective positions of 
the squares and crosses show that incremental 
selection effectively filters out erroneous align-
ments. FJJ+'', with 19.6 valid alignments and a 
precision of 81%, is at the rightmost and upper-
most position in the graph. The detailed results 
for each seed are presented in Table 6, and the 
complete output for the seed ?logic circuit? is 
given in Table 7.  
From the average 4.7 erroneous pairs/seed, 3.2 
(68%) were correct translations but were judged 
unrelated to the seed. This is not surprising, con-
sidering that our set of French related terms con-
tained only 69% (51/74.3) of valid related terms. 
Also note that, of the 24.3 pairs/seed output, 5.25 
are listed in the French-Japanese Scientific Dic-
tionary. However, only 3.9 of those pairs are in-
cluded in M'*. The others were deemed unrelated 
to the seed.  
In the output set of ?machine translation?, ?
??????? shizen ?gengo ?shori (natural lan-
guage processing) is aligned to both traitement 
du language naturel and traitement des langues 
naturelles. The system captures the term?s vari-
ability around langue/language. Lexical diver-
gence is also taken into account to some extent. 
The seed computational linguistics yields the 
alignment of langue maternelle (mother tongue) 
with ?? ?? bokoku ? go (literally [[mother-
country]-language]). The usage of thesauri en-
abled the system to include the concept of coun-
try in the translated MWT, even though it is not 
present in any of the French elements. 
5 Conclusion and future work 
We have proposed a method for compiling bilin-
gual terminologies of compositionally translated 
MWTs. As opposed to previous work, we use the 
web rather than comparable corpora as a source 
of bilingual data. Our main insight is to constrain 
source and target candidate MWTs to only those 
strongly related to the seed. This allows us to 
achieve term alignment with high precision. We 
showed that coverage reaches satisfactory levels 
by using thesauri and bootstrapping.  
Due to the difference in objectives and in cor-
pora, it is very hard to compare results: our 
method produces a rather small set of highly ac-
curate alignments, whereas extraction from com-
parable corpora generates much more candidates, 
but with an inferior precision. These two ap-
proaches have very different applications. Our 
method does however eliminate the requirement 
of comparable corpora, which means that we can 
use seeds from any domain, provided we have 
reasonably rich dictionaries and thesauri.  
Let us not forget that this article describes 
only a first attempt at compiling French-Japanese 
terminology, and that various sources of im-
provement have been left untapped. In particular, 
our alignment suffers from the fact that we do 
not discriminate between different candidate 
translations. This could be achieved by using any 
of the more sophisticated selection methods pro-
posed in the literature. Currently, corpus features 
are used solely for the collection of related terms. 
These could also be utilized in the translation 
selection, which Baldwin and Tanaka have 
shown to be quite effective. We could also make 
use of bilingual dictionary features as they did. 
Lexical context is another resource we have not 
exploited. Context vectors have successfully 
been applied in translation selection by Fung  as 
well as  Daille and Morin.  
On a different level, we could also apply the 
bootstrapping to expand the French set of related 
terms. Finally, we are investigating the possibil-
seed |F'| |F'*| |P'*| |M'| |M'*| Prec. 
1 89 40 14 26 13 50% 
2 64 55 24 14 14 100% 
3 72 59 38 40 33 83% 
4 67 49 22 23 18 78% 
5 85 70 22 21 17 81% 
6 67 50 27 22 21 95% 
7 36 27 16 20 17 85% 
8 114 58 29 28 24 86% 
avg 74.3 51.0 24.0  24.3  19.6  81% 
Table 6: Detailed results for  FJJ+'' 
70% 
80% 
90% 
100% 
25
Pr
ec
is
io
n 
0% 
10% 
20% 
30% 
40% 
50% 
60% 
0 5 10 15 20 
Baseline 
Baseline with bootstrap
Incremental 
Incremental with bootstrap
Number of Valid Alignments
Figure 2: Precision - Valid Alignments curves 
ity of resolving the alignments in the opposite 
direction: from Japanese to French. Surely the 
constructional variability of French MWTs 
would present some difficulties, but we are con-
fident that this could be tackled using translation 
templates, as proposed by Baldwin and Tanaka. 
References 
T. Baldwin and T. Tanaka. 2004. Translation by Ma-
chine of Complex Nominals: Getting it Right. In 
Proc. of the ACL 2004 Workshop on Multiword 
Expressions: Integrating Processing, pp. 24?31, 
Barcelona, Spain.  
Y. Cao and H. Li. 2002. Base Noun Phrase Transla-
tion Using Web Data and the EM Algorithm. In 
Proc. of COLING -02, Taipei, Taiwan. 
Y.C. Chiao and P. Zweigenbaum. 2002. Looking for 
Candidate Translational Equivalents in Specialized, 
Comparable Corpora. In Proc. of COLING-02, pp. 
1208?1212. Taipei, Taiwan. 
B. Daille, E. Gaussier, and J.M. Lange. 1994. To-
wards Automatic Extraction of Monolingual and 
Bilingual Terminology. In Proc. of COLING-94, 
pp. 515?521, Kyoto, Japan. 
B. Daille and E. Morin. 2005. French-English Termi-
nology Extraction from Comparable Corpora, In 
IJCNLP-05, pp. 707?718, Jeju Island, Korea. 
H. D?jean., E. Gaussier and F. Sadat. An Approach 
Based on Multilingual Thesauri and Model Com-
bination for Bilingual Lexicon Extraction. In Proc. 
of COLING-02, pp. 218?224. Taipei, Taiwan. 
Electronic Dictionary Project. 2004. Eijiro Japanese-
English Dictionary: version 79. EDP. 
K.T. Frantzi, and S. Ananiadou. 2003. The C-
Value/NC-Value Domain Independent Method for 
Multi-Word Term Extraction. Journal of Natural 
Language Processing, 6(3), pp. 145?179. 
French Japanese Scientific Association. 1989. French-
Japanese Scientific Dictionary: 4th edition. Haku-
suisha. 
P. Fung. 1995. A Pattern Matching Method for Find-
ing Noun and Proper Noun from Noisy Parallel 
Corpora. In Proc of the ACL-95, pp. 236?243, 
Cambridge, USA. 
P. Fung. 1998. A Statiscal View on Bilingual Lexicon 
Extraction: From Parallel Corpora to Non-parallel 
Corpora. In D. Farwell, L. Gerber and L. Hovy 
eds.: Proceedings of the AMTA-98, Springer, pp. 
1?16. 
I. Langkilde and K. Knight. 1998. Generation that 
exploits corpus-based statistical knowledge. In 
COLLING/ACL-98, pp. 704?710, Montreal, Can-
ada. 
National Institute for Japanese Language. 2004. Bun-
rui Goi Hyo: revised and enlarged edition Dainip-
pon Tosho. 
T. Ohtsuki et al 1989. Crown French-Japanese Dic-
tionary: 4th edition. Sanseido. 
R. Rapp. 1999. Automatic Identification of Word 
Translations from Unrelated English and German 
Corpora. In Proc. of the ACL-99. pp. 1?17. Col-
lege Park, USA. 
S. Sato and Y. Sasaki. 2003. Automatic Collection of 
Related Terms from the Web. In ACL-03 Compan-
ion Volume to the Proc. of the Conference, pp. 
121?124, Sapporo, Japan. 
T. Tanaka and T. Baldwin. 2003. Noun-Noun Com-
pound Machine Translation: A Feasibility Study on 
Shallow Processing. In Proc. of the ACL-2003 
Workshop on Multiword Expressions: Analysis, 
Acquisition and Treatment, pp. 17?24. Sapporo, 
Japan. 
van Rijsbergen, C.J. 1979. Information Retrieval. 
London: Butterworths. Second Edition. 
Jac (Fr.) French term Japanese term (English) eval? 
0.100  portes logiques ?????? ronri?geeto (logic gate) 2/2/2 
0.064  fonctions logiques ????? ronri?kansuu (logic function) 2/2/2 
0.064  fonctions logiques ????? ronri?kinou (logic function) 2/2/2 
0.048  registre ? d?calage ???????? shifuto?rejisuta (shift register) 2/2/2 
0.044  simulateur de circuit ????????? kairo?shimureeta (circuit simulator) 2/2/2 
0.040  circuit combinatoire ?????? kumiawase?kairo (combinatorial circuit) 2/2/2 
0.031  nombre binaire 2??? ni?shinsuu (binary number) 2/2/2 
0.024  niveaux logiques ?????? ronri?reberu (logical level) 2/2/2 
0.020  circuit logique combinatoire ????????? kumiawase?ronri?kairo (combinatorial logic circuit) 2/2/2 
0.017  valeur logique ???? ronri?chi (logical value) 2/2/2 
0.013  tension d' alimentation ????? dengen?denatsu (supply voltage) 2/2/2 
0.011  conception de circuits ????? kairo?sekkei (circuit design) 2/2/2 
0.007  conception d' un circuit logique ???????? ronri?kairo?sekkei (logic circuit design) 2/1/2 
0.005  nombre de portes ????? geeto?suu (number of gates) 2/1/2 
? relatedness / termhood / quality of the translation, on a scale of  0 to 2 
Table 7: System output for seed pair circuit logique ????? (logic circuit) 
Effect of Domain-Specific Corpus
in Compositional Translation Estimation for Technical Terms
Masatsugu Tonoike?, Mitsuhiro Kida?,
Takehito Utsuro?
?Graduate School of Informatics,
Kyoto University
Yoshida-Honmachi, Sakyo-ku,
Kyoto 606-8501 Japan
(tonoike,kida,takagi,sasaki,
utsuro)@pine.kuee.kyoto-u.ac.jp
Toshihiro Takagi?, Yasuhiro Sasaki?,
and Satoshi Sato?
?Graduate School of Engineering,
Nagoya University
Furo-cho, Chikusa-ku,
Nagoya 464-8603 JAPAN
ssato@nuee.nagoya-u.ac.jp
Abstract
This paper studies issues on compiling
a bilingual lexicon for technical terms.
In the task of estimating bilingual term
correspondences of technical terms, it
is usually quite difficult to find an exist-
ing corpus for the domain of such tech-
nical terms. In this paper, we take an
approach of collecting a corpus for the
domain of such technical terms from
the Web. As a method of translation
estimation for technical terms, we pro-
pose a compositional translation esti-
mation technique. Through experimen-
tal evaluation, we show that the do-
main/topic specific corpus contributes
to improving the performance of the
compositional translation estimation.
1 Introduction
This paper studies issues on compiling a bilingual
lexicon for technical terms. So far, several tech-
niques of estimating bilingual term correspon-
dences from a parallel/comparable corpus have
been studied (Matsumoto and Utsuro, 2000). For
example, in the case of estimation from compa-
rable corpora, (Fung and Yee, 1998; Rapp, 1999)
proposed standard techniques of estimating bilin-
gual term correspondences from comparable cor-
pora. In their techniques, contextual similarity
between a source language term and its transla-
tion candidate is measured across the languages,
and all the translation candidates are re-ranked ac-
cording to the contextual similarities. However,
collecting terms
of specific
domain/topic
(language S )
XSU (# of translations
is one)
compiled bilingual lexicon
process data
collecting
corpus
(language T )
domain/topic
specific
corpus
(language T )
sample terms
of specific 
domain/topic
(language S )
XSTU , XSTM ,YST
estimating bilingual term
correspondences
language pair (S,T )
term set
(language S )
XTU
(lang. T )
translation set
(language T )
web
(language S )
web
(language S )
existing
bilingual lexicon
XSM (# of translations
is more than one)
YS (# of translations
is zero)
web
(language T )
web
(language T )
looking up
bilingual lexicon
validating
translation
candidates
Figure 1: Compilation of a Domain/Topic Spe-
cific Bilingual Lexicon
there are limited number of parallel/comparable
corpora that are available for the purpose of es-
timating bilingual term correspondences. There-
fore, even if one wants to apply those existing
techniques to the task of estimating bilingual term
correspondences of technical terms, it is usually
quite difficult to find an existing corpus for the
domain of such technical terms.
Considering such a situation, we take an ap-
proach of collecting a corpus for the domain of
such technical terms from the Web. In this ap-
proach, in order to compile a bilingual lexicon
for technical terms, the following two issues have
to be addressed: collecting technical terms to be
listed as the headwords of a bilingual lexicon, and
estimating translation of those technical terms.
Among those two issues, this paper focuses on the
second issue of translation estimation of technical
terms, and proposes a method for translation es-
timation for technical terms using a domain/topic
specific corpus collected from the Web.
More specifically, the overall framework of
114
compiling a bilingual lexicon from the Web can
be illustrated as in Figure 1. Suppose that we have
sample terms of a specific domain/topic, techni-
cal terms to be listed as the headwords of a bilin-
gual lexicon are collected from the Web by the re-
lated term collection method of (Sato and Sasaki,
2003). Those collected technical terms can be di-
vided into three subsets according to the number
of translation candidates they have in an existing
bilingual lexicon, i.e., the subset XUS of terms for
which the number of translations in the existing
bilingual lexicon is one, the subset XMS of terms
for which the number of translations is more than
one, and the subset YS of terms which are not
found in the existing bilingual lexicon. (Hence-
forth, the union XUS ? XMS is denoted as XS .)
The translation estimation task here is to estimate
translations for the terms of XMS and YS . For the
terms of XMS , it is required to select an appro-
priate translation from the translation candidates
found in the existing bilingual lexicon. For ex-
ample, as a translation of the Japanese technical
term ??????, which belongs to the logic cir-
cuit field, the term ?register? should be selected
but not the term ?regista? of the football field. On
the other hand, for the terms of YS , it is required
to generate and validate translation candidates. In
this paper, for the above two tasks, we use a do-
main/topic specific corpus. Each term of XUS has
the only one translation in the existing bilingual
lexicon. The set of the translations of terms of
XUS is denoted as XUT . Then, the domain/topic
specific corpus is collected from the Web using
the terms in the set XUT . A new bilingual lexicon
is compiled from the result of translation estima-
tion for the terms of XMS and YS , as well as the
translation pairs which consist of the terms of XUS
and their translations found in the existing bilin-
gual lexicon.
For each term of XMS , from the translation can-
didates found in the existing bilingual lexicon, we
select the one which appears most frequently in
the domain/topic specific corpus. The experimen-
tal result of this translation selection process is de-
scribed in Section 5.2.
As a method of translation genera-
tion/validation for technical terms, we propose a
compositional translation estimation technique.
Compositional translation estimation of a term
can be done through the process of composi-
tionally generating translation candidates of the
term by concatenating the translation of the
constituents of the term. Here, those translation
candidates are validated using the domain/topic
specific corpus.
In order to assess the applicability of the com-
positional translation estimation technique, we
randomly pick up 667 Japanese and English tech-
nical term translation pairs of 10 domains from
existing technical term bilingual lexicons. We
then manually examine their compositionality,
and find out that 88% of them are actually com-
positional, which is a very encouraging result.
Based on this assessment, this paper proposes a
method of compositional translation estimation
for technical terms, and through experimental
evaluation, shows that the domain/topic specific
corpus contributes to improving the performance
of compositional translation estimation.
2 Collecting a Domain/Topic Specific
Corpus
When collecting a domain/topic specific corpus of
the language T , for each technical term xUT in the
set XUT , we collect the top 100 pages with search
engine queries including xUT . Our search engine
queries are designed so that documents which de-
scribe the technical term xUT is to be ranked high.
For example, an online glossary is one of such
documents. Note that queries in English and those
in Japanese do not correspond. When collect-
ing a Japanese corpus, the search engine ?goo?1
is used. Specific queries used here are phrases
with topic-marking postpositional particles such
as ?xUT ???, ?xUT ????, ?xUT ??, and an ad-
nominal phrase ?xUT ??, and ?xUT ?. When col-
lecting a English corpus, the search engine ?Ya-
hoo!?2 is used. Specific queries used here are ?xUT
AND what?s?, ?xUT AND glossary?, and ?xUT ?.
3 Compositional Translation Estimation
for Technical Terms
3.1 Overview
An example of compositional translation estima-
tion for the Japanese technical term ??????
1http://www.goo.ne.jp/
2http://www.yahoo.com/
115
? application(1)
? practical(0.3)
? applied(1.6)
? action(1)
? activity(1)
? behavior(1)
? analysis(1)
? diagnosis(1)
? assay(0.3)
? behavior analysis(10)
??Compositional generation 
of translation candidate
? applied behavior analysis(17.6)
? application behavior analysis(11)
? applied behavior diagnosis(1)
??Decompose source term into constituents  
??Translate constituents into target language      process
?? ?? ??a
?? ????b
Generated translation candidates
?(1.6?1?1)+(1.6?10)
? application(1)
? practical(0.3)
? applied(1.6)
Figure 2: Compositional Translation Estimation
for the Japanese Technical Term ????????
?? is shown in Figure 2. First, the Japanese tech-
nical term ???????? is decomposed into
its constituents by consulting an existing bilin-
gual lexicon and retrieving Japanese headwords.3
In this case, the result of this decomposition can
be given as in the cases ?a? and ?b? (in Fig-
ure 2). Then, each constituent is translated into
the target language. A confidence score is as-
signed to the translation of each constituent. Fi-
nally, translation candidates are generated by con-
catenating the translation of those constituents
without changing word order. The confidence
score of translation candidates are defined as the
product of the confidence scores of each con-
stituent. Here, when validating those translation
candidates using the domain/topic specific cor-
pus, those which are not observed in the corpus
are not regarded as candidates.
3.2 Compiling Bilingual Constituents
Lexicons
This section describes how to compile bilingual
constituents lexicons from the translation pairs of
the existing bilingual lexicon Eijiro. The under-
lying idea of augmenting the existing bilingual
lexicon with bilingual constituents lexicons is il-
lustrated with the example of Figure 3. Suppose
that the existing bilingual lexicon does not in-
clude the translation pair ?applied :???, while
it includes many compound translation pairs with
the first English word as ?applied? and the first
3Here, as an existing bilingual lexicon, we use Ei-
jiro(http://www.alc.co.jp/) and bilingual constituents lexi-
cons compiled from the translation pairs of Eijiro (details
to be described in the next section).
 
applied mathematics : ?? ??
applied science : ?? ??
applied robot : ?? ????
.
.
. frequency
? ??
applied : ?? : 40
 
Figure 3: Example of Estimating Bilingual Con-
stituents Translation Pair (Prefix)
Table 1: Numbers of Entries and Translation Pairs
in the Lexicons
lexicon # of entries # of translationEnglish Japanese pairs
Eijiro 1,292,117 1,228,750 1,671,230
P
2
232,716 200,633 258,211
B
P
38,353 38,546 112,586
B
S
22,281 20,627 71,429
Eijiro : existing bilingual lexicon
P
2
: entries of Eijiro with two constituents
in both languages
B
P
: bilingual constituents lexicon (prefix)
B
S
: bilingual constituents lexicon (suffix)
Japanese word ????.4 In such a case, we align
those translation pairs and estimate a bilingual
constituent translation pair, which is to be col-
lected into a bilingual constituents lexicon.
More specifically, from the existing bilingual
lexicon, we first collect translation pairs whose
English terms and Japanese terms consist of two
constituents into another lexicon P
2
. We compile
?bilingual constituents lexicon (prefix)? from the
first constituents of the translation pairs in P
2
and
compile ?bilingual constituents lexicon (suffix)?
from their second constituents. The numbers of
entries in each language and those of translation
pairs in those lexicons are shown in Table 1.
In the result of our assessment, only 27% of the
667 translation pairs mentioned in Section 1 can
be compositionally generated using Eijiro, while
the rate increases up to 49% using both Eijiro and
?bilingual constituents lexicons?.5
4Japanese entries are supposed to be segmented into a
sequence of words by the morphological analyzer JUMAN
(http://www.kc.t.u-tokyo.ac.jp/nl-resource/juman.html)
5In our rough estimation, the upper bound of this rate
is about 80%. Improvement from 49% to 80% could be
achieved by extending the bilingual constituents lexicons
and by introducing constituent reordering rules with preposi-
tions into the process of compositional translation candidate
generation.
116
3.3 Score of Translation Pairs in the
Lexicons
This section introduces a confidence score of
translation pairs in the various lexicons presented
in the previous section. Here, we suppose that
the translation pair ?s, t? of terms s and t is used
when estimating translation from the language of
the term s to that of the term t. First, in this pa-
per, we assume that translation pairs follow cer-
tain preference rules and can be ordered as below:
1. Translation pairs ?s, t? in the existing bilin-
gual lexicon Eijiro, where the term s consists
of two or more constituents.
2. Translation pairs in the bilingual constituents
lexicons whose frequencies in P
2
are high.
3. Translation pairs ?s, t? in the existing bilin-
gual lexicon Eijiro, where the term s consists
of exactly one constituent.
4. Translation pairs in the bilingual constituents
lexicons whose frequencies in P
2
are not
high.
As the definition of the confidence score
q(?s, t?) of a translation pair ?s, t?, in this paper,
we use the following:
q(?s, t?) =
?
?
?
?
?
10
(compo(s)?1) (?s, t? in Eijiro)
log
10
fp(?s, t?) (?s, t? in BP )
log
10
fs(?s, t?) (?s, t? in BS)
(1)
where compo(s) denotes the word (in English) or
morpheme (in Japanese) count of s, fp(?s, t?) the
frequency of ?s, t? as the first constituent in P
2
,
and fs(?s, t?) the frequency of ?s, t? as the second
constituent in P
2
.
6
3.4 Score of Translation Candidates
Suppose that a translation candidate yt is gener-
ated from translation pairs ?s
1
, t
1
?, ? ? ? , ?sn, tn?
by concatenating t
1
, ? ? ? , tn as yt = t1 ? ? ? tn.
Here, in this paper, we define the confidence score
of yt as the product of the confidence scores of the
6It is necessary to empirically examine whether this def-
inition of the confidence score is optimal or not. However,
according to our rough qualitative examination, the results
of the confidence scoring seem stable when without a do-
main/topic specific corpus, even with minor tuning by incor-
porating certain parameters into the score.
collecting terms
of specific
domain/topic
(language S )
XSU (# of translations
is one)
compiled bilingual lexicon
process data
collecting
corpus
(language T )
domain/topic
specific
corpus
(language T )
sample terms
of specific 
domain/topic
(language S )
XSTU , XSTM ,YST
estimating bilingual term
correspondences
language pair (S,T )
term set
(language S )
XTU
(lang. T )
translation set
(language T )
web
(language S )
web
(language S )
existing
bilingual lexicon
XSM (# of translations
is more than one)
YS (# of translations
is zero)
web
(language T )
web
(language T )
looking up
bilingual lexicon
validating
translation
candidates
Figure 4: Experimental Evaluation of Translation
Estimation for Technical Terms with/without the
Domain/Topic Specific Corpus (taken from Fig-
ure 1)
constituent translation pairs ?s
1
, t
1
?, ? ? ? , ?sn, tn?.
Q(yt) =
n
?
i=1
q(?si, ti?) (2)
If a translation candidate is generated from
more than one sequence of translation pairs, the
score of the translation candidate is defined as the
sum of the score of each sequence.
4 Translation Candidate Validation
using a Domain/Topic Specific Corpus
It is not clear whether translation candidates
which are generated by the method described in
Section 3 are valid as English or Japanese terms,
and it is not also clear whether they belong to the
domain/topic. So using a domain/topic specific
corpus collected by the method described in Sec-
tion 2, we examine whether the translation candi-
dates are valid as English or Japanese terms and
whether they belong to the domain/topic. In our
validation method, given a ranked list of trans-
lation candidates, each translation candidate is
checked whether it is observed in the corpus, and
one which is not observed in the corpus is re-
moved from the list.
5 Experiments and Evaluation
5.1 Translation Pairs for Evaluation
In our experimental evaluation, within the frame-
work of compiling a bilingual lexicon for tech-
nical terms, we evaluate the translation estima-
tion part which is indicated with bold line in Fig-
117
Table 2: Number of Translation Pairs for Evaluation
dictionaries categories |X
S
| |Y
S
|
S = English S = Japanese
|X
U
S
| |X
M
S
| C(S) |X
U
S
| |X
M
S
| C(S)
Electromagnetics 58 33 36 22 82% 32 26 76%
McGraw-Hill Electrical engineering 52 45 34 18 67% 25 27 64%
Optics 54 31 42 12 65% 22 32 65%
Iwanami Programming language 55 29 37 18 86% 38 17 100%Programming 53 29 29 24 86% 29 24 79%
Dictionary of (Computer) 100 100 91 9 46% 69 31 56%Computer
Anatomical Terms 100 100 91 9 86% 33 67 39%
Dictionary of Disease 100 100 91 9 74% 53 47 51%
250,000 Chemicals and Drugs 100 100 94 6 58% 74 26 51%
medical terms Physical Science and Statistics 100 100 88 12 64% 58 42 55%
Total 772 667 633 139 68% 433 339 57%
McGraw-Hill : Dictionary of Scientific and Technical Terms
Iwanami : Encyclopedic Dictionary of Computer Science
C(S) : for Y
S
, the rate of including correct translations within the collected domain/topic specific corpus
ure 4. In the evaluation of this paper, we sim-
ply skip the evaluation of the process of collecting
technical terms to be listed as the headwords of a
bilingual lexicon. In order to evaluate the transla-
tion estimation part, from ten categories of exist-
ing Japanese-English technical term dictionaries
listed in Table 2, terms are randomly picked up
for each of the set XUS , XMS , and YS . (Here, as
the terms of YS , these which consist of the only
one word or morpheme are excluded.) As de-
scribed in Section 1, the terms of XUT (the set
of the translations for the terms of XUS ) is used
for collecting a domain/topic specific corpus from
the Web. Translation estimation evaluation is to
be done against the set XMS and YS . For each of
the ten categories, Table 2 shows the sizes of XUS ,
XMS and YS , and for YS , the rate of including cor-
rect translation within the collected domain/topic
specific corpus, respectively.
5.2 Translation Selection from Existing
Bilingual Lexicon
For the terms of XMS , the selected translations are
judged by a human. The correct rates are 69%
from English to Japanese on the average and 75%
from Japanese to English on the average.
5.3 Compositional Translation Estimation
for Technical Terms without the
Domain/Topic Specific Corpus
Without the domain specific corpus, the cor-
rect rate of the first ranked translation candidate
is 19% on the average (both from English to
Japanese and from Japanese to English). The
rate of including correct candidate within top 10
is 40% from English to Japanese and 43% from
Japanese to English on the average. The rate of
compositionally generating correct translation us-
ing both Eijiro and the bilingual constituents lex-
icons (n = ?) is about 50% on the average (both
from English to Japanese and from Japanese to
English).
5.4 Compositional Translation Estimation
for Technical Terms with the
Domain/Topic Specific Corpus
With domain specific corpus, on the average, the
correct rate of the first ranked translation candi-
date improved by 8% from English to Japanese
and by 2% from Japanese to English. However,
the rate of including correct candidate within top
10 decreased by 7% from English to Japanese,
and by 14% from Japanese to English. This is be-
cause correct translation does not exist in the cor-
pus for 32% (from English to Japanese) or 43%
(from Japanese to English) of the 667 translation
pairs for evaluation.
For about 35% (from English to Japanese) or
30% (from Japanese to English) of the 667 trans-
lation pairs for evaluation, correct translation does
exist in the corpus and can be generated through
the compositional translation estimation process.
For those 35% or 30% translation pairs, Fig-
ure 5 compares the correct rate of the first ranked
translation pairs between with/without the do-
main/topic specific corpus. The correct rates in-
crease by 34?37% with the domain/topic specific
corpus. This result supports the claim that the do-
118
??
???
???
???
???
???
???
???
???
???
????
???
???
??
??
??
???
?
???
???
???
???
??
???
???
??
??
???
?
??
??
???
??
??
???
??
??
??
??
??
???
??
??
??
??
???
?
??
???
??
??
???
???
?
???
??
??
??
??
???
???
??
???
???
?
??
??
???
???
???
??
?
??
???
??
??
??
??
??
??
??
??
???
???
??
??
??
??
??
??
??
??
???
?
??
??
??
??
??
??
?
??????????????
???????????
(a) English to Japanese
??
???
???
???
???
???
???
???
???
???
????
???
???
??
??
??
???
?
???
???
???
???
??
???
???
??
??
???
?
??
??
???
??
??
???
??
??
??
??
??
???
??
??
??
??
???
?
??
???
??
??
???
???
?
???
??
??
??
??
???
???
??
???
???
?
??
??
???
???
???
??
?
??
???
??
??
??
??
??
??
??
??
???
???
??
??
??
??
??
??
??
??
???
?
??
??
??
??
??
??
?
??????????????
???????????
(b) Japanese to English
Figure 5: Evaluation against the Translation Pairs
whose Correct Translation Exist in the Corpus
and can be Generated Compositionally
main/topic specific corpus is effective in transla-
tion estimation of technical terms.
6 Related Works
As a related work, (Fujii and Ishikawa, 2001)
proposed a technique of compositional estima-
tion of bilingual term correspondences for the
purpose of cross-language information retrieval.
In (Fujii and Ishikawa, 2001), a bilingual con-
stituents lexicon is compiled from the translation
pairs included in an existing bilingual lexicon in
the same way as our proposed method. One of the
major differences of the technique of (Fujii and
Ishikawa, 2001) and the one proposed in this pa-
per is that in (Fujii and Ishikawa, 2001), instead of
the domain/topic specific corpus, they use a cor-
pus of the collection of the technical papers, each
of which is published by one of the 65 Japanese
associations for various technical domains. An-
other important difference is that in (Fujii and
Ishikawa, 2001), they evaluate only the perfor-
mance of cross-language information retrieval but
not that of translation estimation.
(Cao and Li, 2002) proposed a method of com-
positional translation estimation for compounds.
In the proposed method of (Cao and Li, 2002),
translation candidates of a term are composition-
ally generated by concatenating the translation
of the constituents of the term and are re-ranked
by measuring contextual similarity against the
source language term. One of the major differ-
ences of the technique of (Cao and Li, 2002) and
the one proposed in this paper is that in (Cao and
Li, 2002), they do not use the domain/topic spe-
cific corpus.
7 Conclusion
This paper proposed a method of compositional
translation estimation for technical terms using
the domain/topic specific corpus, and through
the experimental evaluation, showed that the do-
main/topic specific corpus contributes to improv-
ing the performance of compositional translation
estimation.
Future works include the followings: first, in
order to improve the proposed method with re-
spect to its coverage, for example, it is desir-
able to extend the bilingual constituents lexicons
and to introduce constituent reordering rules with
prepositions into the process of compositional
translation candidate generation. Second, we are
planning to introduce a mechanism of re-ranking
translation candidates based on the frequencies of
technical terms in the domain/topic specific cor-
pus.
References
Y. Cao and H. Li. 2002. Base noun phrase translation using
Web data and the EM algorithm. In Proc. 19th COLING,
pages 127?133.
Atsushi Fujii and Tetsuya Ishikawa. 2001. Japanese/english
cross-language information retrieval: Exploration of
query translation and transliteration. Computers and the
Humanities, 35(4):389?420.
P. Fung and L. Y. Yee. 1998. An IR approach for translating
new words from nonparallel, comparable texts. In Proc.
17th COLING and 36th ACL, pages 414?420.
Y. Matsumoto and T. Utsuro. 2000. Lexical knowledge ac-
quisition. In R. Dale, H. Moisl, and H. Somers, editors,
Handbook of Natural Language Processing, chapter 24,
pages 563?610. Marcel Dekker Inc.
R. Rapp. 1999. Automatic identification of word transla-
tions from unrelated English and German corpora. In
Proc. 37th ACL, pages 519?526.
S. Sato and Y. Sasaki. 2003. Automatic collection of related
terms from the web. In Proc. 41st ACL, pages 121?124.
119
Computing Paraphrasability of Syntactic Variants Using Web Snippets
Atsushi Fujita Satoshi Sato
Graduate School of Engineering, Nagoya University
{fujita,ssato}@nuee.nagoya-u.ac.jp
Abstract
In a broad range of natural language pro-
cessing tasks, large-scale knowledge-base of
paraphrases is anticipated to improve their
performance. The key issue in creating such
a resource is to establish a practical method
of computing semantic equivalence and syn-
tactic substitutability, i.e., paraphrasability,
between given pair of expressions. This
paper addresses the issues of computing
paraphrasability, focusing on syntactic vari-
ants of predicate phrases. Our model esti-
mates paraphrasability based on traditional
distributional similarity measures, where the
Web snippets are used to overcome the data
sparseness problem in handling predicate
phrases. Several feature sets are evaluated
through empirical experiments.
1 Introduction
One of the common characteristics of human lan-
guages is that the same concept can be expressed by
various linguistic expressions. Such linguistic vari-
ations are called paraphrases. Handling paraphrases
is one of the key issues in a broad range of natu-
ral language processing (NLP) tasks. In informa-
tion retrieval, information extraction, and question
answering, technology of recognizing if or not the
given pair of expressions are paraphrases is desired
to gain a higher coverage. On the other hand, a sys-
tem which generates paraphrases for given expres-
sions is useful for text-transcoding tasks, such as
machine translation and summarization, as well as
beneficial to human, for instance, in text-to-speech,
text simplification, and writing assistance.
Paraphrase phenomena can roughly be divided
into two groups according to their compositionality.
Examples in (1) exhibit a degree of compositional-
ity, while each example in (2) is composed of totally
different lexical items.
(1) a. be in our favor ? be favorable for us
b. show a sharp decrease ? decrease sharply
(Fujita et al, 2007)
(2) a. burst into tears ? cried
b. comfort ? console
(Barzilay and McKeown, 2001)
A number of studies have been carried out on
both compositional (morpho-syntactic) and non-
compositional (lexical and idiomatic) paraphrases
(see Section 2). In most research, paraphrases have
been represented with the similar templates, such as
shown in (3) and (4).
(3) a. N
1
V N
2
? N
1
?s V -ing of N
2
b. N
1
V N
2
? N
2
be V -en by N
1
(Harris, 1957)
(4) a. X wrote Y ? X is the author of Y
b. X solves Y ? X deals with Y
(Lin and Pantel, 2001)
The weakness of these templates is that they
should be applied only in some contexts. In other
words, the lack of applicability conditions for slot
fillers may lead incorrect paraphrases. One way
to specify the applicability condition is to enumer-
ate correct slot fillers. For example, Pantel et al
(2007) have harvested instances for the given para-
phrase templates based on the co-occurrence statis-
tics of slot fillers and lexicalized part of templates
(e.g. ?deal with? in (4b)). Yet, there is no method
which assesses semantic equivalence and syntactic
substitutability of resultant pairs of expressions.
537
In this paper, we propose a method of directly
computing semantic equivalence and syntactic sub-
stitutability, i.e., paraphrasability, particularly focus-
ing on automatically generated compositional para-
phrases (henceforth, syntactic variants) of predicate
phrases. While previous studies have mainly tar-
geted at words or canned phrases, we treat predicate
phrases having a bit more complex structures.
This paper addresses two issues in handling
phrases. The first is feature engineering. Gener-
ally speaking, phrases appear less frequently than
single words. This implies that we can obtain only a
small amount of information about phrases. To over-
come the data sparseness problem, we investigate if
the Web snippet can be used as a dense corpus for
given phrases. The second is the measurement of
paraphrasability. We assess how well the traditional
distributional similarity measures approximate the
paraphrasability of predicate phrases.
2 Related work
2.1 Representation of paraphrases
Several types of compositional paraphrases, such
as passivization and nominalization, have been rep-
resented with some grammar formalisms, such as
transformational generative grammar (Harris, 1957)
and synchronous tree adjoining grammar (Dras,
1999). These grammars, however, lack the informa-
tion of applicability conditions.
Word association within phrases has been an at-
tractive topic. Meaning-Text Theory (MTT) is a
framework which takes into account several types
of lexical dependencies in handling paraphrases
(Mel?c?uk and Polgue`re, 1987). A bottleneck of
MTT is that a huge amount of lexical knowledge is
required to represent various relationships between
lexical items. Jacquemin (1999) has represented the
syntagmatic and paradigmatic correspondences be-
tween paraphrases with context-free transformation
rules and morphological and/or semantic relations
between lexical items, targeting at syntactic variants
of technical terms that are typically noun phrases
consisting of more than one word. We have pro-
posed a framework of generating syntactic variants
of predicate phrases (Fujita et al, 2007). Following
the previous work, we have been developing three
sorts of resources for Japanese.
2.2 Acquiring paraphrase rules
Since the late 1990?s, the task of automatic acqui-
sition of paraphrase rules has drawn the attention of
an increasing number of researchers. Although most
of the proposed methods do not explicitly eliminate
compositional paraphrases, their output tends to be
non-compositional paraphrase.
Previous approaches to this task are two-fold. The
first group espouses the distributional hypothesis
(Harris, 1968). Among a number of models based
on this hypothesis, two algorithms are referred to
as the state-of-the-art. DIRT (Lin and Pantel, 2001)
collects paraphrase rules consisting of a pair of paths
between two nominal slots based on point-wise mu-
tual information. TEASE (Szpektor et al, 2004) dis-
covers binary relation templates from the Web based
on sets of representative entities for given binary re-
lation templates. These systems often output direc-
tional rules such as exemplified in (5).
(5) a. X is charged by Y
? Y announced the arrest of X
b. X prevent Y ? X lower the risk of Y
They are actually called inference/entailment rules,
and paraphrase is defined as bidirectional infer-
ence/entailment relation1. While the similarity score
in DIRT is symmetric for given pair of paths, the al-
gorithm of TEASE considers the direction.
The other utilizes a sort of parallel texts, such as
multiple translation of the same text (Barzilay and
McKeown, 2001; Pang et al, 2003), corresponding
articles from multiple news sources (Barzilay and
Lee, 2003; Dolan et al, 2004), and bilingual corpus
(Wu and Zhou, 2003; Bannard and Callison-Burch,
2005). This approach is, however, limited by the dif-
ficulty of obtaining parallel/comparable corpora.
2.3 Acquiring paraphrase instances
As reviewed in Section 1, paraphrase rules gener-
ate incorrect paraphrases, because their applicability
conditions are not specified. To avoid the drawback,
several linguistic clues, such as fine-grained classifi-
cation of named entities and coordinated sentences,
have been utilized (Sekine, 2005; Torisawa, 2006).
Although these clues restrict phenomena to those
appearing in particular domain or those describing
coordinated events, they have enabled us to collect
1See http://nlp.cs.nyu.edu/WTEP/
538
paraphrases accurately. The notion of Inferential Se-
lectional Preference (ISP) has been introduced by
Pantel et al (2007). ISP can capture more general
phenomena than above two; however, it lacks abili-
ties to distinguish antonym relations.
2.4 Computing semantic equivalence
Semantic equivalence between given pair of expres-
sions has so far been estimated under the distribu-
tional hypothesis (Harris, 1968). Geffet and Dagan
(2005) have extended it to the distributional inclu-
sion hypothesis for recognizing the direction of lex-
ical entailment. Weeds et al (2005), on the other
hand, have pointed out the limitations of lexical sim-
ilarity and syntactic transformation, and have pro-
posed to directly compute the distributional similar-
ity of pair of sub-parses based on the distributions
of their modifiers and parents. We think it is worth
examining if the Web can be used as the source for
extracting features of phrases.
3 Computing paraphrasability between
predicate phrases using Web snippets
We define the concept of paraphrasability as follows:
A grammatical phrase s is paraphrasable
with another phrase t, iff t satisfies the fol-
lowing three:
? t is grammatical
? t holds if s holds
? t is substitutable for s in some context
Most previous studies on acquiring paraphrase rules
have evaluated resultant pairs from only the second
viewpoint, i.e., semantic equivalence. Additionally,
we assume that one of a pair (t) of syntactic vari-
ants is automatically generated from the other (s).
Thus, grammaticality of t should also be assessed.
We also take into account the syntactic substitutabil-
ity, because head-words of syntactic variants some-
times have different syntactic categories.
Given a pair of predicate phrases, we compute
their paraphrasability in the following procedure:
Step 1. Retrieve Web snippets for each phrase.
Step 2. Extract features for each phrase.
Step 3. Compute their paraphrasability as distribu-
tional similarity between their features.
The rest of this section elaborates on each step in
turn, taking Japanese as the target language.
3.1 Retrieving Web snippets
In general, phrases appear less frequently than sin-
gle words. This raises a crucial problem in com-
puting paraphrasability of phrases, i.e., the sparse-
ness of features for given phrases. One possible way
to overcome the problem is to take back-off statis-
tics assuming the independence between constituent
words (Torisawa, 2006; Pantel et al, 2007). This ap-
proach, however, has a risk of involving noises due
to ambiguity of words.
We take another approach, which utilizes the Web
as a source of examples instead of a limited size of
corpus. For each of the source and target phrases, we
retrieve snippets via the Yahoo API2. The number of
snippets is set to 500.
3.2 Extracting features
The second step extracts the features for each phrase
from Web snippets. We have some options for fea-
ture set, feature weighting, and snippet collection.
Feature sets
To assess a given pair of phrases against the defi-
nition of paraphrasability, the following three sets of
features are examined.
HITS: A phrase must appear in the Web if it is
grammatical. The more frequently a phrase ap-
pears, the more likely it is grammatical.
BOW: A pair of phrases are likely to be semanti-
cally similar, if the distributions of words sur-
rounding the phrases are similar.
MOD: A pair of phrases are likely to be substi-
tutable with each other, if they share a number
of instances of modifiers and modifiees.
To extract BOW features from sentences includ-
ing the given phrase within Web snippets, a morpho-
logical analyzer MeCab3 was firstly used; however,
it resulted wrong POS tags for unknown words, and
hurt statistics. Thus, finally ChaSen4 is used.
To collect MOD features, a dependency parser
CaboCha5 is used. Figure 1 depicts an example
of extracting MOD features from a sentence within
Web snippet. A feature is generated from a bun-
setsu, the Japanese base-chunk, which is either mod-
2http://developer.yahoo.co.jp/search/
3http://mecab.sourceforge.net/
4http://chasen.naist.jp/hiki/ChaSen/
5http://chasen.org/?taku/software/cabocha/
539
kuwashiku
i
jikken-kekka-no
ji - -
saigen-sei-o
i - i-
kenshou-suru
- r
yotei-da
t i-
kare-no
r -
Features
Sentence within snippet
(dependency tree)
Modifiee/D: yotei
ifi / : t i
Modifier/D: kuwashii
ifi r/ : ii
Modifier/D: kare_no
ifi r/ : r
(plan)
(in detail)
(his)
Given phrase
(I am) planning to verify the reproducibility of his experimental result in detail.
Figure 1: An example of MOD feature extraction.
An oval in the dependency tree denotes a bunsetsu.
ifier or modifiee of the given phrase. Each feature
is composed of three or more elements: (i) modi-
fier or modifiee, (ii) dependency relation types (di-
rect dependency, appositive, or parallel, c.f., RASP
and MINIPAR), (iii) base form of the head-word,
and (iv) case marker following noun, auxiliary verb
and verbal suffixes if they appear. The last feature
is employed to distinguish the subtle difference of
meaning of predicate phrases, such as voice, tense,
aspect, and modality. While Lin and Pantel (2001)
have calculated similarities of paths based on slot
fillers of subject and object slots, MOD targets at
sub-trees and utilizes any modifiers and modifiees.
Feature weighting
Geffet and Dagan (2004) have reported on that the
better quality of feature vector (weighting function)
leads better results. So far, several weighting func-
tions have been proposed, such as point-wise mu-
tual information (Lin and Pantel, 2001) and Rela-
tive Feature Focus (Geffet and Dagan, 2004). While
these functions compute weights using a small cor-
pus for merely re-ranking samples, we are devel-
oping a measure that assesses the paraphrasability
of arbitrary pair of phrases, where a more robust
weighting function is necessary. Therefore we di-
rectly use frequencies of features within Web snip-
pets as weight. Normalization will be done when the
paraphrasability is computed (Section 3.3).
Source-focused feature extraction
Independent collection of Web snippets for each
phrase of a given pair might yield no intersection of
feature sets even if they have the same meaning. To
obtain more reliable feature sets, we retrieve Web
snippets by querying the phrase AND the anchor of
the source phrase. The ?anchored version? of Web
snippets is retrieved in the following steps:
Step 2-1. Determine the anchor using Web snip-
pets for the given source phrase. We regarded
a noun which most frequently modifies the
source phrase as its anchor. Examples of source
phrases and their anchors are shown in (6).
Step 2-2. Retrieve Web snippets by querying the
anchor for the source phrase AND each of
source and target phrases, respectively.
Step 2-3. Extract features for HITS, BOW, MOD.
Those sets are referred to as Anc.?, while the
normal versions are referred to as Nor.?.
(6) a. ?emi:o:ukaberu? ? ? ? ?manmen?
(be smiling ? ? ? from ear to ear)
b. ?doriburu:de:kake:agaru? ? ? ? ?saido?
(overlap by dribbling ? ? ? side)
c. ?yoi:sutaato:o:kiru? ? ? ? ?saisaki?
(make a good start ? ? ? good sign)
3.3 Computing paraphrasability
Paraphrasability is finally computed by two conven-
tional distributional similarity measures. The first is
the measure proposed in (Lin and Pantel, 2001):
Par
Lin
(s?t) =
?
f?F
s
?F
t
(w(s, f) + w(t, f))
?
f?F
s
w(s, f) +
?
f?F
t
w(t, f)
,
where Fs and Ft denote feature sets for s and t, re-
spectively. w(x, f) stands for the weight (frequency
in our experiment) of f in Fx.
While Par
Lin
is symmetric, it has been argued
that it is important to determine the direction of para-
phrase. As an asymmetric measure, we examine ?-
skew divergence defined by the following equation
(Lee, 1999):
dskew(t, s) = D (Ps??Pt + (1 ? ?)Ps) ,
where Px denotes a probability distribution esti-
mated6 from a feature set Fx. How well Pt approx-
imates Ps is calculated based on the KL divergence,
D. The parameter ? is set to 0.99, following tradi-
tion, because the optimization of ? is difficult. To
take consistent measurements, we define the para-
phrasability score Par
skew
as follows:
Par
skew
(s?t) = exp (?dskew(t, s)) .
6We estimate them simply using maximum likelihood esti-
mation, i.e., P
x
(f) = w(x, f)/
P
f
?
?Fx
w(x, f ?).
540
Table 1: # of sampled source phrases and automatically generated syntactic variants.
Phrase type # of tokens # of types th types Cov.(%) Output Ave.
N : C : V 20,200,041 4,323,756 1,000 1,014 10.7 1,536 (489) 3.1
N
1
: N
2
: C : V 3,796,351 2,013,682 107 1,005 6.3 88,040 (966) 91.1
N : C : V
1
: V
2
325,964 213,923 15 1,022 12.9 75,344 (982) 76.7
N : C : Adv : V 1,209,265 923,475 21 1,097 3.9 8,281 (523) 15.7
Adj : N : C : V 378,617 233,952 20 1,049 14.1 128 (50) 2.6
N : C : Adj 788,038 203,845 86 1,003 31.4 3,212 (992) 3.2
Total 26,698,276 7,912,633 6,190 176,541 (4,002) 44.1
Table 2: # of syntactic variants whose paraphrasability scores are computed.
Nor.HITS ? Nor.BOW.? ? Nor.MOD.?. Anc.HITS ? Anc.BOW.? ? Anc.MOD.?.
Nor.HITS ? Anc.HITS. Nor.BOW.? ? Anc.BOW.?. Nor.MOD.? ? Anc.MOD.?. X denotes the set of syntactic variants whose scores are computed based on X.
Nor.HITS Nor.BOW.? Nor.MOD.? Anc.HITS Anc.BOW.? Anc.MOD.? Mainichi
Phrase type Output Ave. Output Ave. Output Ave. Output Ave. Output Ave. Output Ave. Output Ave.
N : C : V 1,405 (489) 2.9 1,402 (488) 2.9 1,396 (488) 2.9 1,368 (488) 2.8 1,366 (487) 2.8 1,360 (487) 2.8 1,103 (457) 2.4
N
1
: N
2
: C : V 9,544 (964) 9.9 9,249 (922)10.0 8,652 (921) 9.4 7,437 (897) 8.3 7,424 (894) 8.3 6,795 (891) 7.6 3,041 (948) 3.2
N : C : V
1
: V
2
3,769 (876) 4.3 3,406 (774) 4.4 3,109 (762) 4.1 2,517 (697) 3.6 2,497 (690) 3.6 2,258 (679) 3.3 1,156 (548) 2.1
N : C : Adv : V 690 (359) 1.9 506 (247) 2.0 475 (233) 2.0 342 (174) 2.0 339 (173) 2.0 322 (168) 1.9 215 (167) 1.3
Adj : N : C : V 45 (20) 2.3 45 (20) 2.3 42 (17) 2.5 41 (18) 2.3 41 (18) 2.3 39 (16) 2.4 14 (7) 2.0
N : C : Adj 1,459 (885) 1.6 1,459 (885) 1.6 1,399 (864) 1.6 1,235 (809) 1.5 1,235 (809) 1.5 1,161 (779) 1.5 559 (459) 1.2
Total 16,912 (3,593) 4.7 16,067 (3,336) 4.8 15,073 (3,285) 4.6 12,940 (3,083) 4.2 12,902 (3,071) 4.2 11,935 (3,020) 4.0 6,088 (2,586) 2.4
Now Par
x
falls within [0, 1], and a larger Par
x
indi-
cates a more paraphrasable pair of phrases.
4 Experimental setting
We conduct empirical experiments to evaluate the
proposed methods. Settings are described below.
4.1 Test collection
First, source phrases were sampled from a 15 years
of newspaper articles (Mainichi 1991-2005, approx-
imately 1.5GB). Referring to the dependency struc-
ture given by CaboCha, we extracted most fre-
quent 1,000+ phrases for each of 6 phrase types.
These phrases were then fed to a system proposed
in (Fujita et al, 2007) to generate syntactic vari-
ants. The numbers of the source phrases and
their syntactic variants are summarized in Table 1,
where the numbers in the parentheses indicate that
of source phrases paraphrased. At least one can-
didate was generated for 4,002 (64.7%) phrases.
Although the system generates numerous syntactic
variants from a given phrase, most of them are er-
roneous. For example, among 159 syntactic vari-
ants that are automatically generated for the phrase
?songai:baishou:o:motomeru? (demand compensa-
tion for damages), only 8 phrases are grammatical,
and only 5 out of 8 are correct paraphrases.
Paraphrasability of each pair of source phrase and
candidate is then computed by the methods pro-
posed in Section 3. Table 2 summarizes the num-
bers of pairs whose features can be extracted from
the Web snippets. While more than 90% of candi-
dates were discarded due to ?No hits? in the Web,
at least one candidate survived for 3,020 (48.8%)
phrases. Mainichi is a baseline which counts HITS
in the corpus used for sampling source phrases.
4.2 Samples for evaluation
We sampled three sets of pairs for evaluation, where
Mainichi, ?.HITS, ?.BOW, ?.MOD, the harmonic
mean of the scores derived from ?.BOW and ?.MOD
(referred to as ?.HAR), and two distributional simi-
larity measures for ?.BOW, ?.MOD, and ?.HAR, in
total 15 models, are compared.
Ev.Gen: This investigates how well a correct can-
didate is ranked first among candidates for a
given phrase using the top-ranked pairs for ran-
domly sampled 200 source phrases for each of
15 models.
Ev.Rec: This assesses how well a method gives
higher scores to correct candidates using the
200-best pairs for each of 15 models.
Ev.Ling: This compares paraphrasability of each
phrase type using the 20-best pairs for each of
6 phrase type and 14 Web-based models.
4.3 Criteria of paraphrasability
To assess by human the paraphrasability discussed
in Section 3, we designed the following four ques-
tions based on (Szpektor et al, 2007):
Qsc: Is s a correct phrase in Japanese?
Qtc: Is t a correct phrase in Japanese?
Qs2t: Does t hold if s holds and can t substituted for
s in some context?
Qt2s: Does s hold if t holds and can s substituted
for t in some context?
541
5 Experimental results
5.1 Agreement of human judge
Two human assessors separately judged all of the
1,152 syntactic variant pairs (for 962 source phrases)
within the union of the three sample sets. They
agreed on all four questions for 795 (68.4%) pairs.
For the 963 (83.6%) pairs that passed Qsc and Qtc
in both two judges, we obtained reasonable agree-
ment ratios 86.9% and 85.0% and substantial Kappa
values 0.697 and 0.655 for assessing Qs2t and Qt2s.
5.2 Ev.Gen
Table 3 shows the results for Ev.Gen, where the
strict precision is calculated based on the number
of two positive judges for Qs2t, while the lenient
precision is for at least one positive judge for the
same question. ?.MOD and ?.HAR outperformed
the other models, although there was no statistically
significant difference7. Significant differences be-
tween Mainichi and the other models in lenient pre-
cisions indicate that the Web enables us to compute
paraphrasability more accurately than a limited size
of corpus.
From a closer look at the distributions of para-
phrasability scores of ?.BOW and ?.MOD shown in
Table 4, we find that if a top-ranked candidate for
a given phrase is assigned enough high score, it is
very likely to be correct. The scores of Anc.? are
distributed in a wider range than those of Nor.?, pre-
serving precision. This allows us to easily skim the
most reliable portion by setting a threshold.
5.3 Ev.Rec
The results for Ev.Rec, as summarized in Table 5,
show the significant differences of performances be-
tween Mainichi or ?.HITS and the other models.
The results of ?.HITS supported the importance of
comparing features of phrases. On the other hand,
?.BOW performed as well as ?.MOD and ?.HAR.
This sounds nice because BOW features can be ex-
tracted extremely quickly and accurately.
Unfortunately, Anc.? led only a small impact on
strict precisions. We speculate that the selection of
the anchor is inadequate. Another possible interpre-
tation is that source phrases are rarely ambiguous,
because they contain at least two content words. In
7p < 0.05 in 2-sample test for equality of proportions.
Table 3: Precision for 200 candidates (Ev.Gen).
Strict Lenient
Model Nor.? Anc.? Nor.? Anc.?
Mainichi 77 (39%) - - 101 (51%) - -
HITS 84 (42%) 83 (42%) 120 (60%) 119 (60%)
BOW.Lin 82 (41%) 85 (43%) 123 (62%) 124 (62%)
BOW.skew 86 (43%) 87 (44%) 125 (63%) 124 (62%)
MOD.Lin 91 (46%) 91 (46%) 130 (65%) 131 (66%)
MOD.skew 92 (46%) 90 (45%) 132 (66%) 130 (65%)
HAR.Lin 90 (45%) 90 (45%) 129 (65%) 130 (65%)
HAR.skew 93 (47%) 90 (45%) 134 (67%) 131 (66%)
Table 4: Distribution of paraphrasability scores and
lenient precision (Ev.Gen).
Nor.BOW Anc.BOW
Par(s?t) Lin skew Lin skew
0.9-1.0 11/ 12 (92%) 0/ 0 - 17/ 18 (94%) 2/ 2 (100%)
0.8-1.0 45/ 49 (92%) 1/ 1 (100%) 45/ 50 (90%) 6/ 6 (100%)
0.7-1.0 72/ 88 (82%) 7/ 7 (100%) 73/ 92 (79%) 10/ 11 (91%)
0.6-1.0 94/127 (74%) 11/ 11 (100%) 83/113 (74%) 12/ 13 (92%)
0.5-1.0 102/145 (70%) 13/ 13 (100%) 96/128 (75%) 14/ 15 (93%)
0.4-1.0 107/158 (68%) 13/ 14 (93%) 103/145 (71%) 21/ 22 (96%)
0.3-1.0 113/173 (65%) 25/ 26 (96%) 114/166 (69%) 31/ 32 (97%)
0.2-1.0 119/184 (65%) 40/ 41 (98%) 121/186 (65%) 49/ 50 (98%)
0.1-1.0 123/198 (62%) 74/ 86 (86%) 124/200 (62%) 82/ 99 (83%)
0.0-1.0 123/200 (62%) 125/200 (63%) 124/200 (62%) 124/200 (62%)
Variance 0.052 0.031 0.061 0.044
Nor.MOD Anc.MOD
Par(s?t) Lin skew Lin skew
0.9-1.0 2/ 2 (100%) 0/ 0 - 7/ 7 (100%) 1/ 1 (100%)
0.8-1.0 10/ 10 (100%) 0/ 0 - 12/ 13 (92%) 2/ 2 (100%)
0.7-1.0 13/ 14 (93%) 0/ 0 - 17/ 18 (94%) 6/ 6 (100%)
0.6-1.0 20/ 21 (95%) 1/ 1 (100%) 27/ 28 (96%) 9/ 9 (100%)
0.5-1.0 31/ 32 (97%) 6/ 6 (100%) 36/ 37 (97%) 10/ 10 (100%)
0.4-1.0 42/ 44 (96%) 11/ 11 (100%) 51/ 53 (96%) 12/ 12 (100%)
0.3-1.0 61/ 68 (90%) 12/ 12 (100%) 61/ 68 (90%) 13/ 14 (93%)
0.2-1.0 81/ 92 (88%) 13/ 13 (100%) 82/ 94 (87%) 18/ 19 (95%)
0.1-1.0 105/133 (79%) 17/ 18 (94%) 104/126 (83%) 24/ 25 (96%)
0.0-1.0 130/200 (65%) 132/200 (66%) 131/200 (66%) 130/200 (65%)
Variance 0.057 0.014 0.072 0.030
paraphrase generation, capturing the correct bound-
ary of phrases is rather vital, because the source
phrase is usually assumed to be grammatical. Qsc
for 55 syntactic variants (for 44 source phrases) were
actually judged incorrect.
The lenient precisions, which were reaching a
ceiling, implied the limitation of the proposed meth-
ods. Most common errors among the proposed
methods were generated by a transformation pattern
N
1
: N
2
: C : V ? N
2
: C : V . Typically,
dropping a nominal element N
1
of the given nominal
compound N
1
: N
2
generalizes the meaning that the
compound conveys, and thus results correct para-
phrases. However, it caused errors in some cases;
for example, since N
1
was the semantic head in (7),
dropping it was incorrect.
(7) s. ?shukketsu:taryou:de:shibou-suru?
(die due to heavy blood loss)
t.??taryou:de:shibou-suru? (die due to plenty)
542
Table 5: Precision for 200 candidates (Ev.Rec).
Strict Lenient
Model Nor.? Anc.? Nor.? Anc.?
Mainichi 78 (39%) - - 111 (56%) - -
HITS 71 (36%) 93 (47%) 113 (57%) 128 (64%)
BOW.Lin 159 (80%) 162 (81%) 193 (97%) 191 (96%)
BOW.skew 154 (77%) 158 (79%) 192 (96%) 191 (96%)
MOD.Lin 158 (79%) 164 (82%) 192 (96%) 193 (97%)
MOD.skew 156 (78%) 161 (81%) 191 (96%) 191 (96%)
HAR.Lin 157 (79%) 164 (82%) 192 (96%) 194 (97%)
HAR.skew 155 (78%) 160 (80%) 191 (96%) 191 (96%)
5.4 Ev.Ling
Finally the results for Ev.Ling is shown in Table 6.
Paraphrasability of syntactic variants for phrases
containing an adjective was poorly computed. The
primal source of errors for Adj : N : C : V type
phrases was the subtle change of nuance by switch-
ing syntactic heads as illustrated in (8), where un-
derlines indicate heads.
(8) s. ?yoi:shigoto:o:suru? (do a good job)
t
1
.=?yoku:shigoto-suru? (work hard)
t
2
.=?shigoto:o:yoku:suru? (improve the work)
Most errors in paraphrasing N : C : Adj type
phrases, on the other hand, were caused due to the
difference of aspectual property and agentivity be-
tween adjectives and verbs. For example, (9s) can
describe not only things those qualities have been
improved as inferred by (9t), but also those origi-
nally having a high quality. Qs2t for (9) was thus
judged incorrect.
(9) s. ?shitsu:ga:takai? (having high quality)
t.=?shitsu:ga:takamaru? (quality rises)
Precisions of syntactic variants for the other types
of phrases were higher, but they tended to include
trivial paraphrases such as shown in (10) and (11).
Yet, collecting paraphrase instances statically will
contribute to paraphrase recognition tasks.
(10) s. ?shounin:o:eru? (clear)
t. ?shounin-sa-re-ru? (be approved)
(11) s. ?eiga:o:mi:owaru? (finish seeing the movie)
t. ?eiga:ga:owaru? (the movie ends)
6 Discussion
As described in the previous sections, our quite
naive methods have shown fairly good performances
in this first trial. This section describes some re-
maining issues to be discussed further.
The aim of this study is to create a thesaurus
of phrases to recognize and generate phrases that
Table 6: Precision for each phrase type (Ev.Ling).
Phrase type Strict Lenient
N : C : V 52/ 98 (53%) 69/ 98 (70%)
N
1
: N
2
: C : V 51/ 72 (71%) 64/ 72 (89%)
N : C : V
1
: V
2
42/ 86 (49%) 60/ 86 (70%)
N : C : Adv : V 33/ 61 (54%) 44/ 61 (72%)
Adj : N : C : V 0/ 25 (0%) 4/ 25 (16%)
N : C : Adj 18/ 73 (25%) 38/ 73 (52%)
Total 196/415 (47%) 279/415 (67%)
Table 7: # of features.
Nor.BOW Nor.MOD Anc.BOW Anc.MOD
# of features (type) 73,848 471,720 72,109 409,379
average features (type) 1,322 211 1,277 202
average features (token) 4,883 391 4,728 383
are semantically equivalent and syntactically substi-
tutable, following the spirit described in (Fujita et
al., 2007). Through the comparisons of Nor.? and
Anc.?, we have shown a little evidence that the am-
biguity of phrases was not problematic at least for
handling syntactic variants, arguing the necessity of
detecting the appropriate phrase boundaries.
To overcome the data sparseness problem, Web
snippets are harnessed. Features extracted from the
snippets outperformed newspaper corpus; however,
the small numbers of features for phrases shown in
Table 7 and the lack of sophisticated weighting func-
tion suggest that the problem might persist. To ex-
amine the proposed features and measures further,
we plan to use TSUBAKI8, an indexed Web corpus
developed for NLP research, because it allow us to
obtain snippets as much as it archives.
The use of larger number of snippets increases
the computation time for assessing paraphrasability.
For reducing it as well as gaining a higher cover-
age, the enhancement of the paraphrase generation
system is necessary. A look at the syntactic variants
automatically generated by a system, which we pro-
posed, showed that the system could generate syn-
tactic variants for only a half portion of the input,
producing many erroneous ones (Section 4.1). To
prune a multitude of incorrect candidates, statisti-
cal language models such as proposed in (Habash,
2004) will be incorporated. In parallel, we plan to
develop a paraphrase generation system which lets
us to quit from the labor of maintaining patterns such
as shown in (4). We think a more unrestricted gener-
ation algorithm will gain a higher coverage, preserv-
ing the meaning as far as handling syntactic variants
of predicate phrases.
8http://tsubaki.ixnlp.nii.ac.jp/se/index.cgi
543
7 Conclusion
In this paper, we proposed a method of assessing
paraphrasability between automatically generated
syntactic variants of predicate phrases. Web snip-
pets were utilized to overcome the data sparseness
problem, and the conventional distributional similar-
ity measures were employed to quantify the similar-
ity of feature sets for the given pair of phrases. Em-
pirical experiments revealed that features extracted
from the Web snippets contribute to the task, show-
ing promising results, while no significant difference
was observed between two measures.
In future, we plan to address several issues such as
those described in Section 6. Particularly, at present,
the coverage and portability are of our interests.
Acknowledgments
We are deeply grateful to all anonymous reviewers
for their valuable comments. This work was sup-
ported in part by MEXT Grant-in-Aid for Young
Scientists (B) 18700143, and for Scientific Research
(A) 16200009, Japan.
References
Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing
with bilingual parallel corpora. In Proceedings of the 43rd
Annual Meeting of the Association for Computational Lin-
guistics (ACL), pages 597?604.
Regina Barzilay and Kathleen R. McKeown. 2001. Extracting
paraphrases from a parallel corpus. In Proceedings of the
39th Annual Meeting of the Association for Computational
Linguistics (ACL), pages 50?57.
Regina Barzilay and Lillian Lee. 2003. Learning to paraphrase:
an unsupervised approach using multiple-sequence align-
ment. In Proceedings of the 2003 Human Language Tech-
nology Conference and the North American Chapter of the
Association for Computational Linguistics (HLT-NAACL),
pages 16?23.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsu-
pervised construction of large paraphrase corpora: exploit-
ing massively parallel news sources. In Proceedings of the
20th International Conference on Computational Linguistics
(COLING), pages 350?356.
Mark Dras. 1999. Tree adjoining grammar and the reluctant
paraphrasing of text. Ph.D. thesis, Division of Information
and Communication Science, Macquarie University.
Atsushi Fujita, Shuhei Kato, Naoki Kato, and Satoshi Sato.
2007. A compositional approach toward dynamic phrasal
thesaurus. In Proceedings of the ACL-PASCAL Workshop on
Textual Entailment and Paraphrasing (WTEP), pages 151?
158.
Maayan Geffet and Ido Dagan. 2004. Feature vector quality
and distributional similarity. In Proceedings of the 20th In-
ternational Conference on Computational Linguistics (COL-
ING), pages 247?253.
Maayan Geffet and Ido Dagan. 2005. The distributional in-
clusion hypotheses and lexical entailment. In Proceedings
of the 43rd Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 107?116.
Nizar Habash. 2004. The use of a structural N-gram language
model in generation-heavy hybrid machine translation. In
Proceedings of the 3rd International Natural Language Gen-
eration Conference (INLG), pages 61?69.
Zellig Harris. 1957. Co-occurrence and transformation in lin-
guistic structure. Language, 33(3):283?340.
Zellig Harris. 1968. Mathematical structures of language.
John Wiley & Sons.
Christian Jacquemin. 1999. Syntagmatic and paradigmatic rep-
resentations of term variation. In Proceedings of the 37th
Annual Meeting of the Association for Computational Lin-
guistics (ACL), pages 341?348.
Lillian Lee. 1999. Measures of distributional similarity. In
Proceedings of the 37th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 25?32.
Dekang Lin and Patrick Pantel. 2001. Discovery of inference
rules for question answering. Natural Language Engineer-
ing, 7(4):343?360.
Igor Mel?c?uk and Alain Polgue`re. 1987. A formal lexicon in
meaning-text theory (or how to do lexica with words). Com-
putational Linguistics, 13(3-4):261?275.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003. Syntax-
based alignment of multiple translations: extracting para-
phrases and generating new sentences. In Proceedings of
the 2003 Human Language Technology Conference and the
North American Chapter of the Association for Computa-
tional Linguistics (HLT-NAACL), pages 102?109.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola, Timothy
Chklovski, and Eduard Hovy. 2007. ISP: Learning infer-
ential selectional preferences. In Proceedings of Human
Language Technologies 2007: The Conference of the North
American Chapter of the Association for Computational Lin-
guistics (NAACL-HLT), pages 564?571.
Satoshi Sekine. 2005. Automatic paraphrase discovery based
on context and keywords between NE pairs. In Proceedings
of the 3rd International Workshop on Paraphrasing (IWP),
pages 80?87.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaventura Cop-
pola. 2004. Scaling Web-based acquisition of entailment
relations. In Proceedings of the 2004 Conference on Em-
pirical Methods in Natural Language Processing (EMNLP),
pages 41?48.
Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007. Instance-
based evaluation of entailment rule acquisition. In Proceed-
ings of the 45th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 456?463.
Kentaro Torisawa. 2006. Acquiring inference rules with tem-
poral constraints by using Japanese coordinated sentences
and noun-verb co-occurrences. In Proceedings of the Hu-
man Language Technology Conference of the North Ameri-
can Chapter of the Association for Computational Linguis-
tics (HLT-NAACL), pages 57?64.
Julie Weeds, David Weir, and Bill Keller. 2005. The distribu-
tional similarity of sub-parses. In Proceedings of the ACL
Workshop on Empirical Modeling of Semantic Equivalence
and Entailment, pages 7?12.
Hua Wu and Ming Zhou. 2003. Synonymous collocation ex-
traction using translation information. In Proceedings of the
41st Annual Meeting of the Association for Computational
Linguistics (ACL), pages 120?127.
544
Automatic Paraphrasing of Japanese Functional Expressions
Using a Hierarchically Organized Dictionary
Suguru Matsuyoshi?,? Satoshi Sato?
? Graduate School of Informatics, Kyoto University, Japan
? Graduate School of Engineering, Nagoya University, Japan
{s matuyo,ssato}@nuee.nagoya-u.ac.jp
Abstract
Automatic paraphrasing is a transformation
of expressions into semantically equivalent
expressions within one language. For gener-
ating a wider variety of phrasal paraphrases
in Japanese, it is necessary to paraphrase
functional expressions as well as content ex-
pressions. We propose a method of para-
phrasing of Japanese functional expressions
using a dictionary with two hierarchies: a
morphological hierarchy and a semantic hi-
erarchy. Our system generates appropriate
alternative expressions for 79% of source
phrases in Japanese in an open test. It also
accepts style and readability specifications.
1 Introduction
Automatic paraphrasing is a transformation of ex-
pressions into semantically equivalent expressions
within one language. It is expected for various ap-
plications, such as information retrieval, machine
translation and a reading/writing aid.
Automatic paraphrasing of Japanese text has been
studied by many researchers after the first interna-
tional workshop on automatic paraphrasing (Sato
and Nakagawa, 2001). Most of them focus on para-
phrasing of content words, such as noun phrases and
verb phrases. In contrast, paraphrasing of functional
expressions has less attention. A functional expres-
sion is a function word or a multi-word expression
that works as a function word. For generating a
wider variety of phrasal paraphrases in Japanese, as
shown in Fig. 1, it is necessary to paraphrase func-
tional expressions as well as content expressions, be-
cause almost all phrases in Japanese include one or
more functional expressions. In this paper, we focus
on paraphrasing of Japanese functional expressions.
In several applications, such as a reading aid,
in paraphrasing of Japanese functional expressions,
control of readability of generated text is impor-
tant, because functional expressions are critical units
that determine sentence structures and meanings. In
case a reader does not know a functional expres-
sion, she fails to understand the sentence meaning.
If the functional expression can be paraphrased into
an easier one, she may know it and understand the
sentence meaning. It is desirable to generate expres-
sions with readability suitable for a reader because
easier functional expressions tend to have more than
one meaning.
A remarkable characteristic of Japanese func-
tional expressions is that each functional expression
has many different variants. Each variant has one of
four styles. In paraphrasing of Japanese functional
expressions, a paraphrasing system should accept
style specification, because consistent use in style is
required. For example, the paraphrase (b) in Fig. 1
is not appropriate for a document in normal style be-
cause the expression has polite style.
Paraphrasing a functional expression into a se-
mantically equivalent one that satisfies style and
readability specifications can be realized as a com-
bination of the following two processes:
1. Transforming a functional expression into an-
other one that is semantically equivalent to it,
often with changing readability.
691
A phrase keQtei-se / zaru-wo-e-nai
 
Paraphrasing of
content expressions
Paraphrasing of
functional expressions
kimeru
sadameru
keQtei-wo-suru
:
shika-nai
shika-ari-mase-N
yori-hoka-nai
:
?
?
Phrasal
paraphrases
(a) kimeru shika-nai
(b) kimeru shika-ari-mase-N
(c) sadameru shika-nai
(d) sadameru yori-hoka-nai
:
Figure 1: Generation of a wider variety of phrasal
paraphrases.
2. Rewriting a functional expression to a variant
of it, often with changing style.
We propose a method of paraphrasing of Japanese
functional expressions using a dictionary with two
hierarchies: a morphological hierarchy and a se-
mantic hierarchy. The former hierarchy provides
a list of all variants specified with style for each
functional expression, which is required for the
above process 2. The latter hierarchy provides se-
mantic equivalence classes of functional expressions
and readability level for each functional expression,
which are required for the above process 1.
2 Related Work
A few studies on paraphrasing of Japanese func-
tional expressions have been conducted. In order
to implement automatic paraphrasing, some stud-
ies (Iida et al, 2001; Tsuchiya et al, 2004) use a
set of paraphrasing rules, and others (Tanabe et al,
2001; Shudo et al, 2004) use semantic equivalence
classes.
All of these studies do not handle variants in a
systematic way. In case a system paraphrases a func-
tional expression f into f ?, it also should generate all
variants of f ? in potential. However, any proposed
system does not guarantee this requirement. Output
selection of variants should be determined accord-
ing to the given style specification. Any proposed
system does not have such selection mechanism.
Controlling readability of generated text is not a
central issue in previous studies. An exception is
a study by Tsuchiya et al (Tsuchiya et al, 2004).
Level Num
L
1 Headword 341
L
2 Headwords with unique meaning 435
L
3 Derivations 555
L
4 Alternations of function words 774
L
5 Phonetic variations 1,187
L
6 Insertion of particles 1,810
L
7 Conjugation forms 6,870
L
8 Normal or desu/masu forms 9,722
L
9 Spelling variations 16,801
Table 1: Nine levels of the morphological hierarchy.
Their system paraphrases a functional expression
into an easier one. However, it does not accept the
readability specification, e.g. for learners of begin-
ner course or intermediate course of Japanese.
3 A Hierarchically Organized Dictionary
of Japanese Functional Expressions
3.1 Morphological hierarchy
In order to organize many different variants of func-
tional expressions, we have designed a morpho-
logical hierarchy with nine abstraction levels (Mat-
suyoshi et al, 2006). Table 1 summarizes these nine
levels. The number of entries in L1 (headwords) is
341, and the number of leaf nodes in L9 (surface
forms) is 16,801. For each surface form in the hier-
archy, we specified one of four styles (normal, po-
lite, colloquial, and stiff) and connectability (what
word can be to the left and right of the expression).
3.2 Semantic hierarchy
There is no available set of semantic equivalence
classes of Japanese functional expressions for para-
phrasing. Some sets are described in books in lin-
guistics (Morita and Matsuki, 1989; Tomomatsu et
al., 1996; Endoh et al, 2003), but these are not for
paraphrasing. Others are proposed for paraphrasing
in natural language processing (Tanabe et al, 2001;
Shudo et al, 2004), but these are not available in
public.
For 435 entries in L2 (headwords with unique
meaning) of the morphological hierarchy, from the
viewpoint of paraphrasability, we have designed a
semantic hierarchy with three levels according to the
semantic hierarchy proposed by a book (Morita and
Matsuki, 1989). The numbers of classes in the top,
middle and bottom levels are 45, 128 and 199, re-
692
spectively. For each entry in L2, we specified one of
readability levels of A1, A2, B, C, and F according
to proficiency level in a book (Foundation and of In-
ternational Education, Japan, 2002), where A1 is the
most basic level and F is the most advanced level.
3.3 Producing all surface forms that satisfy
style and readability specifications
For a given surface form of a functional expression,
our dictionary can produce all variants of semanti-
cally equivalent functional expressions that satisfy
style and readability specifications. The procedure
is as follows:
1. Find the functional expression in L2 for a given
surface form according to the morphological
hierarchy.
2. Obtain functional expressions that are seman-
tically equivalent to the functional expression
according to the semantic hierarchy.
3. Exclude the functional expressions that do not
satisfy readability specification.
4. Enumerate all variants (surface forms) of the
remaining functional expressions according to
the morphological hierarchy.
5. Exclude the surface forms that do not satisfy
style specification.
4 Formulation of Paraphrasing of
Japanese Functional Expressions
As a source expression of paraphrasing, we select a
phrase (or Bunsetsu) in Japanese because it is a base
unit that includes functional expressions. In this pa-
per, we define a phrase as follows. Let c
i
be a con-
tent word, and f
j
a functional expression. Then, a
phrase is formulated as the following:
Phrase = c
1
c
2
? ? ? c
m
f
1
f
2
? ? ? f
n
, (1)
where c
1
c
2
? ? ? c
m
is the content part of the phrase
and f
1
f
2
? ? ? f
n
is the functional part of it.
Paraphrasing of a functional part of a phrase is
performed as a combination of the following five
types of paraphrasing:
1?1 Substituting a functional expression with an-
other functional expression (f ? f ?).
Paraphrasing type Num
1?1 only 214 (61%)
1?N (and 1?1) 69 (20%)
N?1 (and 1?1) 18 ( 5%)
M?N (and 1?1) 8 ( 2%)
Otherwise 44 (12%)
Sum 353 (100%)
Table 2: Number of paraphrases produced by a na-
tive speaker of Japanese.
1?N Substituting a functional expression with a
sequence of functional expressions (f ?
f
?
1
f
?
2
? ? ? f
?
N
).
N?1 Substituting a sequence of functional ex-
pressions with one functional expression
(f
1
f
2
? ? ? f
N
? f
?).
M?N Substituting a sequence of functional ex-
pressions with another sequence of functional
expressions (f
1
f
2
? ? ? f
M
? f
?
1
f
?
2
? ? ? f
?
N
).
f?c Substituting a functional expression with an
expression including one or more content
words.
In a preliminary experiment, we investigated
which type of the above a native speaker of Japanese
tended to use in paraphrasing a functional part. Ta-
ble 2 shows the classification result of 353 para-
phrases produced by the subject for 238 source
phrases.1 From this table, it was found out that para-
phrasing of ?1?1? type was major in that it was
used for producing 61% of paraphrases.
Because of dominance of paraphrasing of ?1?1?
type, we construct a system that paraphrases
Japanese functional expressions in a phrase by sub-
stituting a functional expression with a semantically
equivalent expression. This system paraphrases a
phrase defined as the form in Eq. (1) into the fol-
lowing form:
Alternative = c
1
c
2
? ? ? c
m?1
c
?
m
wf
?
1
f
?
2
? ? ? f
?
n
,
where c?
m
is c
m
or a conjugation form of c
m
, f ?
j
is a
functional expression that is semantically equivalent
to f
j
, and w is a null string or a function word that
is inserted for connecting f ?
1
to c?
m
properly.
1These source phrases are the same ones that we use in a
closed test in section 6.
693
INPUT
- kiku ya-ina-ya
(as soon as I hear)
Readability
specification:
A1, A2, B


Analysis

c
1
= kiku
f
1
= ya-ina-ya

Paraphrase
generation





Dictionary

- kiku to-sugu-ni
- kiku to-douzi-ni
- kii ta-totaN
:

Ranking

OUTPUT
1. kiku to-douzi-ni
2. kii ta-totaN
3. kiku to-sugu
:
Figure 2: Overview of our system.
The combination of simple substitution of a func-
tional expression and insertion of a function word
covers 22% (15/69) of the paraphrases by paraphras-
ing of ?1?N (and 1?1)? type in Table 2. There-
fore, our system theoretically covers 65% (229/353)
of the paraphrases in Table 2.
5 System
We have implemented a system that paraphrases
Japanese functional expressions using a hierarchi-
cally organized dictionary, by substituting a func-
tional expression with another functional expression
that is semantically equivalent to it. The system ac-
cepts a phrase in Japanese and generates a list of
ranked alternative expressions for it. The system
also accepts style and readability specifications.
Fig. 2 shows an overview of our system. This sys-
tem consists of three modules: analysis, paraphrase
generation, and ranking.
5.1 Analysis
Some methods have been proposed for detecting
Japanese functional expressions based on a set of
detection rules (Tsuchiya and Sato, 2003) and ma-
chine learning (Uchimoto et al, 2003; Tsuchiya et
al., 2006). However, because these methods detect
only a limited number of functional expressions (and
their variants), we cannot apply them to the analysis
of a phrase. Another method is to add a list of about
17,000 surface forms of functional expressions to a
dictionary of an existing morphological analyzer and
determine connecting costs based on machine learn-
ing. However, it is infeasible because there is no
large corpus in which all of these surface forms have
been tagged.
Instead of these methods, we use a different
method of decomposing a given phrase into a se-
quence of content words and functional expressions.
Our method uses two analyzers.
We constructed a functional-part analyzer (FPA).
This is implemented using a morphological analyzer
MeCab2 with a special dictionary containing only
functional expressions. FPA can decompose a func-
tional part (string) into a sequence of functional ex-
pressions, but fails to decompose a string when the
string includes one or more content words. In order
to extract a functional part from a given string, we
use original MeCab.
First, original MeCab decomposes a given string
into a sequence of morphemes m
1
m
2
? ? ?m
k
.
Next, we suppose that m
1
is a content part
and m
2
m
3
? ? ?m
k
is a functional part. If FPA
can decompose m
2
m
3
? ? ?m
k
into a sequence of
functional expressions f
1
f
2
? ? ? f
n
, then we obtain
c
1
f
1
f
2
? ? ? f
n
as shown in Eq. (1) as an analyzed
result, where c
1
= m
1
. Otherwise, we sup-
pose that m
1
m
2
is a content part and m
3
m
4
? ? ?m
k
is a functional part. If FPA can decompose
m
3
m
4
? ? ?m
k
into a sequence of functional expres-
sions f
1
f
2
? ? ? f
n
, then we obtain c
1
c
2
f
1
f
2
? ? ? f
n
as
an analyzed result, where c
1
= m
1
and c
2
= m
2
.
This procedure is continued until FPA succeeds in
decomposition.
5.2 Paraphrase generation
This module accepts an analyzed result
c
1
c
2
? ? ? c
m
f
1
f
2
? ? ? f
n
and generates a list of
alternative expressions for it.
First, the module obtains a surface form f ?
1
that
is semantically equivalent to f
1
from the dictionary
in section 3. Next, it constructs c
1
c
2
? ? ? c
m?1
c
?
m
wf
?
1
by connecting f ?
1
to c
1
c
2
? ? ? c
m
by the method de-
scribed in section 4. Then, it obtains a surface
form f ?
2
that is semantically equivalent to f
2
and
constructs c
1
c
2
? ? ? c
m?1
c
?
m
wf
?
1
f
?
2
in similar fashion.
This process proceeds analogously, and finally, the
module constructs c
1
c
2
? ? ? c
m?1
c
?
m
wf
?
1
f
?
2
? ? ? f
?
n
as
an alternative expression.
Because in practice the module obtains more than
one surface form that is semantically equivalent to
2http://mecab.sourceforge.net/
694
Top 1 Top 1 to 2 Top 1 to 3 Top 1 to 4 Top 1 to 5
Closed 177 (74%) 197 (83%) 210 (88%) 213 (90%) 213 (90%)
Closed (Perfect analysis) 196 (82%) 211 (89%) 219 (92%) 221 (93%) 221 (93%)
Open 393 (63%) 461 (73%) 496 (79%) 500 (80%) 501 (80%)
Open (Perfect analysis) 453 (72%) 508 (81%) 531 (85%) 534 (85%) 534 (85%)
Table 3: Evaluation of paraphrases generated by the paraphrasing system
f
j
by the method described in subsection 3.3, it gen-
erates more than one alternative expression by con-
sidering all possible combinations of these surface
forms and excluding candidates that include two ad-
jacent components that cannot be connected prop-
erly.
If the module generates no alternative expression,
it uses the semantic equivalence classes in the upper
level reluctantly.
5.3 Ranking
Because a functional expression seems to be more
standard and common as it appears more frequently
in newspaper corpus, we use frequencies of func-
tional expressions (strings) in newspaper corpus in
order to rank alternative expressions. We define a
scoring function as the product of frequencies of
functional expressions in a phrase.
6 Evaluation
We evaluate paraphrases generated by our para-
phrasing system for validating our semantic equiva-
lence classes, because the dictionary that the system
uses guarantees by the method described in subsec-
tion 3.3 that the system can generate all variants of a
functional expression and accept style and readabil-
ity specifications.
6.1 Methodology
We evaluated paraphrases generated by our para-
phrasing system from the viewpoint of an applica-
tion to a writing aid, where a paraphrasing system
is expected to output a few good alternative expres-
sions for a source phrase.
We evaluated the top 5 alternative expressions
generated by the system for a source phrase by clas-
sifying them into the following three classes:
Good Good alternative expression for the source
phrase.
Intermediate Expression that keeps the meaning
roughly that the source phrase has.
Bad Inappropriate expression.
Then, we counted source phrases for which at least
one of the alternative expressions of the top 1 to
n was judged as ?Good?. One of the authors per-
formed the judgment according to books (Morita
and Matsuki, 1989; Endoh et al, 2003).
As a closed test set, we used 238 example phrases
for 140 functional expressions extracted from a book
(Foundation and of International Education, Japan,
2002), which we had used for development of our
semantic equivalence classes. As an open test set,
we used 628 example phrases for 184 functional ex-
pressions extracted from a book (Tomomatsu et al,
1996). We used the Mainichi newspaper text corpus
(1991-2005, about 21 million sentences, about 1.5
gigabytes) for ranking alternative expressions.
6.2 Results
Table 3 shows the results. The rows with ?Perfect
analysis? in the table show the results in analyzing
source phrases by hand. Because the values in every
row of the table are nearly saturated in ?Top 1 to 3?,
we discuss the results of the top 1 to 3 hereafter.
Our system generated appropriate alternative ex-
pressions for 88% (210/238) and 79% (496/628) of
source phrases in the closed and the open test sets,
respectively. We think that this performance is high
enough.
We analyzed the errors made by the system. In the
closed and the open tests, it was found out that para-
phrasing of ?1?1? type could not generate alterna-
tive expressions for 7% (16/238) and 7% (41/628)
of source phrases, respectively. These values define
the upper limit of our system.
In the closed and the open tests, it was found out
that the system failed to analyze 3% (8/238) and 3%
(21/628) of source phrases, respectively, and that
695
ambiguity in meaning caused inappropriate candi-
dates to be ranked higher for 1% (2/238) and 4%
(23/628) of source phrases, respectively. The rows
with ?Perfect analysis? in Table 3 show that almost
all of these problems are solved in analyzing source
phrases by hand. Improvement of the analysis mod-
ule can solve these problems.
In the open test, insufficiency of semantic equiv-
alence classes and too rigid connectability caused
only 3% (19/628) and 3% (16/628) of source phrases
to have no good candidates, respectively. The small-
ness of the former value validates our semantic
equivalence classes.
The remaining errors were due to low frequencies
of good alternatives in newspaper corpus.
7 Conclusion and Future Work
We proposed a method of paraphrasing Japanese
functional expressions using a dictionary with two
hierarchies. Our system can generate all variants of a
functional expression and accept style and readabil-
ity specifications. The system generated appropriate
alternative expressions for 79% of source phrases in
an open test.
Tanabe et al have proposed paraphrasing rules
of ?1?N?, ?N?1?, and ?M?N? types (Tanabe
et al, 2001). For generating a wider variety of
phrasal paraphrases, future work is to incorporate
these rules into our system and to combine several
methods of paraphrasing of content expressions with
our method.
References
Orie Endoh, Kenji Kobayashi, Akiko Mitsui, Shinjiro
Muraki, and Yasushi Yoshizawa, editors. 2003. A
Dictionary of Synonyms in Japanese (New Edition).
Shogakukan. (in Japanese).
The Japan Foundation and Association of International
Education, Japan, editors. 2002. Japanese Language
Proficiency Test: Test Content Specifications (Revised
Edition). Bonjinsha. (in Japanese).
Ryu Iida, Yasuhiro Tokunaga, Kentaro Inui, and Junji
Etoh. 2001. Exploration of clause-structural and
function-expressional paraphrasing using KURA. In
Proceedings of the 63rd National Convention of Infor-
mation Processing Society of Japan, volume 2, pages
5?6. (in Japanese).
Suguru Matsuyoshi, Satoshi Sato, and Takehito Utsu-
ro. 2006. Compilation of a dictionary of Japanese
functional expressions with hierarchical organization.
In Proceedings of the 21st International Conference
on Computer Processing of Oriental Languages (IC-
CPOL), Lecture Notes in Computer Science, volume
4285, pages 395?402. Springer.
Yoshiyuki Morita and Masae Matsuki. 1989. Nihongo
Hyougen Bunkei, volume 5 of NAFL Sensho (Ex-
pression Patterns in Japanese). ALC Press Inc. (in
Japanese).
Satoshi Sato and Hiroshi Nakagawa, editors. 2001. Auto-
matic Paraphrasing: Theories and Applications, The
6th Natural Language Processing Pacific Rim Sympo-
sium (NLPRS) Post-Conference Workshop.
Kosho Shudo, Toshifumi Tanabe, Masahito Takahashi,
and Kenji Yoshimura. 2004. MWEs as non-
propositional content indicators. In Proceedings of the
2nd ACL Workshop on Multiword Expressions: Inte-
grating Processing (MWE-2004), pages 32?39.
Toshifumi Tanabe, Kenji Yoshimura, and Kosho Shudo.
2001. Modality expressions in Japanese and their au-
tomatic paraphrasing. In Proceedings of the 6th Natu-
ral Language Processing Pacific Rim Symposium (NL-
PRS), pages 507?512.
Etsuko Tomomatsu, Jun Miyamoto, and Masako Wakuri.
1996. 500 Essential Japanese Expressions: A Guide
to Correct Usage of Key Sentence Patterns. ALC Press
Inc. (in Japanese).
Masatoshi Tsuchiya and Satoshi Sato. 2003. Automatic
detection of grammar elements that decrease readabil-
ity. In Proceedings of 41st Annual Meeting of the As-
sociation for Computational Linguistics, pages 189?
192.
Masatoshi Tsuchiya, Satoshi Sato, and Takehito Utsuro.
2004. Automatic generation of paraphrasing rules
from a collection of pairs of equivalent sentences in-
cluding functional expressions. In Proceedings of the
10th Annual Meeting of the Association for Natural
Language Processing, pages 492?495. (in Japanese).
Masatoshi Tsuchiya, Takao Shime, Toshihiro Takagi,
Takehito Utsuro, Kiyotaka Uchimoto, Suguru Mat-
suyoshi, Satoshi Sato, and Seiichi Nakagawa. 2006.
Chunking Japanese compound functional expressions
by machine learning. In Proceedings of the workshop
on Multi-word-expressions in a multilingual context,
EACL 2006 Workshop, pages 25?32.
Kiyotaka Uchimoto, Chikashi Nobata, Atsushi Yamada,
Satoshi Sekine, and Hitoshi Isahara. 2003. Morpho-
logical analysis of a large spontaneous speech corpus
in Japanese. Proceedings of the 41st Annual Meet-
ing of the Association for Computational Linguistics,
pages 479?488.
696
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 353?360,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Japanese Idiom Recognition:
Drawing a Line between Literal and Idiomatic Meanings
Chikara Hashimoto? Satoshi Sato? Takehito Utsuro?
? Graduate School of
Informatics
Kyoto University
Kyoto, 606-8501, Japan
? Graduate School of
Engineering
Nagoya University
Nagoya, 464-8603, Japan
? Graduate School of Systems
and Information Engineering
University of Tsukuba
Tsukuba, 305-8573, Japan
Abstract
Recognizing idioms in a sentence is im-
portant to sentence understanding. This
paper discusses the lexical knowledge of
idioms for idiom recognition. The chal-
lenges are that idioms can be ambiguous
between literal and idiomatic meanings,
and that they can be ?transformed? when
expressed in a sentence. However, there
has been little research on Japanese idiom
recognition with its ambiguity and trans-
formations taken into account. We pro-
pose a set of lexical knowledge for idiom
recognition. We evaluated the knowledge
by measuring the performance of an idiom
recognizer that exploits the knowledge. As
a result, more than 90% of the idioms in a
corpus are recognized with 90% accuracy.
1 Introduction
Recognizing idioms in a sentence is important to
sentence understanding. Failure of recognizing id-
ioms leads to, for example, mistranslation.
In the case of the translation service of Excite1,
it sometimes mistranslates sentences that contain
idioms such as (1a), due to the recognition failure.
(1) a. Kare-wa
he-TOP
mondai-no
problem-GEN
kaiketu-ni
solving-DAT
hone-o
bone-ACC
o-tta.
break-PAST
?He made an effort to solve the problem.?
b. ?He broke his bone to the resolution of a
question.?
1http://www.excite.co.jp/world/
(1a) contains an idiom, hone-o oru (bone-ACC
break) ?make an effort.? (1b) is the mistranslation
of (1a), in which the idiom is interpreted literally.
In this paper, we discuss lexical knowledge for
idiom recognition. The lexical knowledge is im-
plemented in an idiom dictionary that is used by
an idiom recognizer we implemented. Note that
the idiom recognition we define includes distin-
guishing literal and idiomatic meanings.2 Though
there has been a growing interest in MWEs (Sag
et al, 2002), few proposals on idiom recognition
take into account ambiguity and transformations.
Note also that we tentatively define an idiom as a
phrase that is semantically non-compositional. A
precise characterization of the notion ?idiom? is
beyond the scope of the paper.3
Section 2 defines what makes idiom recognition
difficult. Section 3 discusses the classification of
Japanese idioms, the requisite lexical knowledge,
and implementation of an idiom recognizer. Sec-
tion 4 evaluates the recognizer that exploits the
knowledge. After the overview of related works
in Section 5, we conclude the paper in Section 6.
2 Two Challenges of Idiom Recognition
Two factors make idiom recognition difficult: am-
biguity between literal and idiomatic meanings
and ?transformations? that idioms could un-
dergo.4 In fact, the mistranslation in (1) is caused
by the inability of disambiguation between the two
meanings. ?Transformation? also causes mistrans-
2Some idioms represent two or three idiomatic meanings.
But those meanings in an idiom are not distinguished. We
concerned only whether a phrase is used as an idiom or not.
3For a detailed discussion of what constitutes the notion
of (Japanese) idiom, see Miyaji (1982), which details usages
of commonly used Japanese idioms.
4The term ?transformation? in the paper is not relevant to
the Chomskyan term in Generative Grammar.
353
lation. Sentences in (2) and (3a) contain an idiom,
yaku-ni tatu (part-DAT stand) ?serve the purpose.?
(2) Kare-wa
he-TOP
yaku-ni
part-DAT
tatu.
stand
?He serves the purpose.?
(3) a. Kare-wa
he-TOP
yaku-ni
part-DAT
sugoku
very
tatu.
stand
?He really serves the purpose.?
b. ?He stands enormously in part.?
Google?s translation system5 mistranslates (3a) as
in (3b), which does not make sense,6 though it suc-
cessfully translates (2). The only difference be-
tween (2) and (3a) is that bunsetu7 constituents of
the idiom are detached from each other.
3 Knowledge for Idiom Recognition
3.1 Classification of Japanese Idioms
Requisite lexical knowledge to recognize an idiom
depends on how difficult it is to recognize it. Thus,
we first classify idioms based on recognition diffi-
culty. The recognition difficulty is determined by
the two factors: ambiguity and transformability.
Consequently, we identify three classes (Figure
1).8 Class A is not transformable nor ambigu-
ous. Class B is transformable but not ambiguous.9
Class C is transformable and ambiguous. Class A
amounts to unambiguous single words, which are
easy to recognize, while Class C is the most diffi-
cult to recognize. Only Class C needs further clas-
sifications, since only Class C needs disambigua-
tion and lexical knowledge for disambiguation de-
pends on its part-of-speech (POS) and internal
structure. The POS of Class C is either verbal
or adjectival, as in Figure 1. Internal structure
represents constituent words? POS and a depen-
dency between bunsetus. The internal structure
5http://www.google.co.jp/language tools
6In fact, the idiom has no literal interpretation.
7A bunsetu is a syntactic unit in Japanese, consisting of
one independent word and more than zero ancillary words.
The sentence in (3a) consists of four bunsetu constituents.
8The blank space at the upper left in the figure implies that
there is no idiom that does not undergo any transformation
and yet is ambiguous. Actually, we have not come up with
such an example that should fill in the blank space.
9Anonymous reviewers pointed out that Class A and B
could also be ambiguous. In fact, one can devise a context
that makes the literal interpretation of those Classes possible.
However, virtually no phrase of Class A or B is interpreted
literally in real texts, and we think our generalization safely
captures the reality of idioms.
A
m
bi
gu
ou
s
U
na
m
bi
gu
o
u
s
TransformableUntransformable
Class B
yaku-ni
part-DAT
tatu
stand
?serve the purpose?
- Verbal
- Adjectival
Class C
hone-o
bone-ACC
oru
break
?make an effort?
- Verbal
- Adjectival
Class A
mizu-mo
water-TOO
sitataru
drip
?extremely handsome?
- Adnominal
- Nominal
- Adverbial
More Difficult
Figure 1: Idiom Classification based on the
Recognition Difficulty
of hone-o oru (bone-ACC bone), for instance, is
?(Noun/Particle Verb),? abbreviated as ?(N/P V).?
Then, let us give a full account of the further
classification of Class C. We exploit grammatical
differences between literal and idiomatic usages
for disambiguation. We will call the knowledge of
the differences the disambiguation knowledge.
For instance, a phrase, hone-o oru, does not al-
low passivization when used as an idiom, though
it does when used literally. Thus, (4), in which the
phrase is passivized, cannot be an idiom.
(4) hone-ga
bone-NOM
o-rareru
break-PASS
?A bone is broken.?
In this case, passivizability can be used as a dis-
ambiguation knowledge. Also, detachability of
the two bunsetu constituents can serve for disam-
biguating the idiom; they cannot be separated. In
general, usages applicable to idioms are also ap-
plicable to literal phrases, but the reverse is not
always true (Figure 2). Then, finding the disam-
Usages Applicable to Only Literal Phrases
Usages Applicable to Both
Idioms and Literal Phrases
Figure 2: Difference of Applicable Usages
biguation knowledge amounts to finding usages
applicable to only literal phrases.
Naturally, the disambiguation knowledge for an
idiom depends on its POS and internal structure.
354
As for POS, disambiguation of verbal idioms can
be performed by the knowledge of passivizability,
while that of adjectival idioms cannot. Regarding
internal structure, detachability should be anno-
tated on every boundary of bunsetus. Thus, the
number of annotations of detachability depends on
the number of bunsetus of an idiom.
There is no need for further classification of
Class A and B, since lexical knowledge for them is
invariable. The next section mentions their invari-
ableness. After all, Japanese idioms are classified
as in Figure 3. The whole picture of the subclasses
of Class C remains to be seen.
3.2 Knowledge for Each Class
What lexical knowledge is needed for each class?
Class A needs only a string information; idioms
of the class amount to unambiguous single words.
A string information is undoubtedly invariable
across all kinds of POS and internal structure.
Class B requires not only a string but also
knowledge that normalizes transformations id-
ioms could undergo, such as passivization and de-
tachment of bunsetus. We identify three types of
transformations that are relevant to idioms: 1) De-
tachment of Bunsetu Constituents, 2) Predicate?s
Change, and 3) Particle?s Change. Predicate?s
change includes inflection, attachment of a neg-
ative morpheme, a passive morpheme or modal
verbs, and so on. Particle?s change represents at-
tachment of topic or restrictive particles. (5b) is an
example of predicate?s change from (5a) by adding
a negative morpheme to a verb. (5c) is an example
of particle?s change from (5a) by adding a topic
particle to the preexsistent particle of an idiom.
(5) a. Kare-wa
he-TOP
yaku-ni
part-DAT
tatu.
stand
?He serves the purpose.?
b. Kare-wa
he-TOP
yaku-ni
part-DAT
tat-anai.
stand-NEG
?He does not serve the purpose.?
c. Kare-wa
he-TOP
yaku-ni-wa
part-DAT-TOP
tatu.
stand
?He serves the purpose.?
To normalize the transformations, we utilize a
dependency relation between constituent words,
and we call it the dependency knowledge. This
amounts to checking the presence of all the con-
stituent words of an idiom. Note that we ignore,
among constituent words, endings of a predicate
and case particles, ga (NOM) and o (ACC), since
they could change their forms or disappear.
The dependency knowledge is also invariable
across all kinds of POS and internal structure.
Class C requires the disambiguation knowl-
edge, as well as all the knowledge for Class B.
As a result, all the requisite knowledge for id-
iom recognition is summarized as in Table 1.
String Dependency Disambiguation
Class A ?
Class B ? ?
Class C ? ? ?
Table 1: Requisite Knowledge for each Class
As discussed in ?3.1, the disambiguation
knowledge for an idiom depends on which sub-
class it belongs to. A comprehensive idiom recog-
nizer calls for all the disambiguation knowledge
for all the subclasses, but we have not figured out
all of them. Then, we decided to blaze a trail to
discover the disambiguation knowledge by inves-
tigating the most commonly used idioms.
3.3 Disambiguation Knowledge for the
Verbal (N/P V) Idioms
What type of idiom is used most commonly? The
answer is the verbal (N/P V) type like hone-
o oru (bone-ACC break); it is the most abundant in
terms of both type and token. Actually, 1,834 out
of 4,581 idioms (40%) in Kindaichi and Ikeda
(1989), which is a Japanese dictionary with more
than 100,000 words, are this type.10 Also, 167,268
out of 220,684 idiom tokens in Mainichi newspa-
per of 10 years (?91??00) (76%) are this type.11
Then we discuss what can be used to disam-
biguate the verbal (N/P V) type. First, we exam-
ined literature of linguistics (Miyaji, 1982; Morita,
1985; Ishida, 2000) that observed characteristics
of Japanese idioms. Then, among the characteris-
tics, we picked those that could help with the dis-
ambiguation of the type. (6) summarizes them.
10Counting was performed automatically by means of the
morphological analyzer ChaSen (Matsumoto et al, 2000)
with no human intervention. Note that Kindaichi and Ikeda
(1989) consists of 4,802 idioms, but 221 of them were ig-
nored since they contained unknown words for ChaSen.
11We counted idiom tokens by string matching with inflec-
tion taken into account. And we referred to Kindaichi and
Ikeda (1989) for a comprehensive idiom list. Note that count-
ing was performed totally automatically.
355
Recognition
Difficulty
POS
Internal
Structure
Japanese Idioms
Class C
Verb
(N/P V)
hone-o
bone-ACC
oru
break
?make an effort?
(N/P N/P V)
mune-ni
chest-DAT
te-o
hand-ACC
ateru
put
?think over?
? ? ?
Adj
(N/P A)
atama-ga
head-NOM
itai
ache
?be in trouble?
? ? ?
Class B
yaku-ni
part-DAT
tatu
stand
?serve the purpose?
Class A
mizu-mo
water-TOO
sitataru
drip
?extremely handsome?
Figure 3: Classification of Japanese Idioms for the Recognition Task
(6) Disambiguation Knowledge for the
Verbal (N/P V) Idioms
a. Adnominal Modification Constraints
I. Relative Clause Prohibition
II. Genitive Phrase Prohibition
III. Adnominal Word Prohibition
b. Topic/Restrictive Particle Constraints
c. Voice Constraints
I. Passivization Prohibition
II. Causativization Prohibition
d. Modality Constraints
I. Negation Prohibition
II. Volitional Modality Prohibition12
e. Detachment Constraint
f. Selectional Restriction
For example, the idiom, hone-o oru, does not al-
low adnominal modification by a genitive phrase.
Thus, (7) can be interpreted only literally.
(7) kare-no
he-GEN
hone-o
bone-ACC
oru
break
?(Someone) breaks his bone.?
That is, the Genitive Phrase Prohibition, (6aII), is
in effect for the idiom. Likewise, the idiom does
not allow its case particle o (ACC) to be substi-
tuted with restrictive particles such as dake (only).
Thus, (8) represents only a literal meaning.
(8) hone-dake
bone-ONLY
oru
break
?(Someone) breaks only some bones.?
12
?Volitional Modality? represents those verbal expres-
sions of order, request, permission, prohibition, and volition.
This means the Restrictive Particle Constraint,
(6b), is also in effect. Also, (4) shows that the
Passivization Prohibition, (6cI), is in effect, too.
Note that the constraints in (6) are not always
in effect for an idiom. For instance, the Causativi-
zation Prohibition, (6cII), is invalid for the idiom,
hone-o oru. In fact, (9a) can be interpreted both
literally and idiomatically.
(9) a. kare-ni
he-DAT
hone-o
bone-ACC
or-aseru
break-CAUS
b. ?(Someone) makes him break a bone.?
c. ?(Someone) makes him make an effort.?
3.4 Implementation
We implemented an idiom dictionary based on the
outcome above and a recognizer that exploits the
dictionary. This section illustrates how they work,
and we focus on Class B and C hereafter.
The idiom recognizer looks up dependency
patterns in the dictionary that match a part of the
dependency structure of a sentence (Figure 4). A
dependency pattern is equipped with all the req-
uisite knowledge for idiom recognition. Rough
sketch of the recognition algorithm is as follows:
1. Analyze the morphology and dependency
structures of an input sentence.
2. Look up dependency patterns in the dictio-
nary that match a part of the dependency
structure of the input sentence.
3. Mark constituents of an idiom in the sentence
if any.13 Constituents that are marked are
constituent words and bunsetu constituents
that include one of those constituent words.
13As a constituent marker, we use an ID that is assigned to
each idiom in the dictionary.
356
Input
yaku-ni-wa
part-DAT-TOP
mattaku
totally
tat-anai
stand-NEG
Morphology &
Dependency
Analysis
Dependency
Matching
yaku
part
/ ni
DAT
/ wa
TOP
mattaku
totally
tatu
stand
/ nai
NEG
Output
yaku
part
/ ni
DAT
/ wa
TOP
mattaku
totally
tatu
stand
/ nai
NEG
Idiom
Recognizer
Idiom
Dictionary
? ? ?
yaku
part
/ ni
DAT
tatu
stand
? ? ?
Dependency Pattern
Figure 4: Internal Working of the Idiom Recognizer
Input Output
Idiom
Recognizer
ChaSen
Morphology
Analysis
CaboCha
Dependency
Analysis
TGrep2
Dependency
Matching
Dependency Pattern
Generator Pattern DB
Idiom
Dictionary
Figure 5: Organization of the System
As in Figure 5, we use ChaSen as a morphol-
ogy analyzer and CaboCha (Kudo and Matsumoto,
2002) as a dependency analyzer. Dependency
matching is performed by TGrep2 (Rohde, 2005),
which finds syntactic patterns in a sentence or tree-
bank. The dependency pattern is usually getting
complicated since it is tailored to the specifica-
tion of TGrep2. Thus, we developed the Depen-
dency Pattern Generator that compiles the pattern
database from a human-readable idiom dictionary.
Only the difference in treatments of Class B and
C lies in their dependency patterns. The depen-
dency pattern of Class B consists of only its depen-
dency knowledge, while that of Class C consists
of not only its dependency knowledge but also its
disambiguation knowledge (Figure 6).
The idiom dictionary consists of 100 idioms,
which are all verbal (N/P V) and belong to either
Class B or C. Among the knowledge in (6), the
Selectional Restriction has not been implemented
yet. The 100 idioms are those that are used most
frequently. To be precise, 50 idioms in Kindaichi
and Ikeda (1989) and 50 in Miyaji (1982) were
extracted by the following steps:14
1. From Miyaji (1982), 50 idioms that were
14We counted idiom tokens by string matching with inflec-
tion taken into account. Note that counting was performed
automatically without human intervention.
used most frequently in Mainichi newspaper
of 10 years (?91??00) were extracted.
2. From Kindaichi and Ikeda (1989), 50 idioms
that were used most frequently in the newspa-
per of 10 years but were not included in the
50 idioms from Miyaji (1982) were extracted.
As a result, 66 out of the 100 idioms were Class
B, and the other 34 idioms were Class C.15
4 Evaluation
4.1 Experiment Condition
We conducted an experiment to see the effective-
ness of the lexical knowledge we proposed.
As an evaluation corpus, we collected 300 ex-
ample sentences of the 100 idioms from Mainichi
newspaper of ?95: three sentences for each id-
iom. Then we added another nine sentences for
three idioms that are orthographic variants of one
of the 100 idioms. Among the three idioms, one
belonged to Class B and the other two belonged to
Class C. Thus, 67 out of the 103 idioms were Class
B and the other 36 were Class C. After all, 309
15We found that the most frequently used 100 idioms in
Kindaichi and Ikeda (1989) cover as many as 53.49% of all
tokens in Mainichi newspaper of 10 years. This implies that
our dictionary accounts for approximately half of all idiom
tokens in a corpus.
357
Dependency Pattern
Disambiguation
Knowledge
?Adnominal
Modification Cs
?Topic/Restrictive
Particle Cs
?Detachment C
?Voice Cs
?Modality Cs
Dependency
Knowledge
? Dependency of Constituents
hone
bone
/ o
ACC
oru
break
hone
bone
/ o
ACC
oru
break
Figure 6: Dependency Pattern of Class C
sentences were prepared. Table 2 shows the break-
down of them. ?Positive? indicates sentences in-
Class B Class C Total
Positive 200 66 266
Negative 1 42 43
Total 201 108 309
Table 2: Breakdown of the Evaluation Corpus
cluding a true idiom, while ?Negative? indicates
those including a literal-usage ?idiom.?
A baseline system was prepared to see the ef-
fect of the disambiguation knowledge. The base-
line system was the same as the recognizer except
that it exploited no disambiguation knowledge.
4.2 Result
The result is shown in Table 3. The left side shows
the performances of the recognizer, while the right
side shows that of the baseline. Differences of per-
formances between the two systems are marked
with bold. Recall, Precision, and F-Measure, are
calculated using the following equations.
Recall =
|Correct Outputs|
|Positive|
Precision =
|Correct Outputs|
|All Outputs|
F -Measure =
2? Precision ?Recall
Precision+Recall
As a result, more than 90% of the idioms can be
recognized with 90% accuracy. Note that the rec-
ognizer made fewer errors due to the employment
of the disambiguation knowledge.
The result shows the high performances. How-
ever, there turns out to be a long way to go to solve
the most difficult problem of idiom recognition:
drawing a line between literal and idiomatic mean-
ings. In fact, the precision of recognizing idioms
of Class C remains less than 70% as in Table 3.
Besides, the recognizer successfully rejected only
15 out of 42 negative sentences. That is, its suc-
cess rate of rejecting negative ones is only 35.71%
4.3 Discussion of the Disambiguation
Knowledge
First of all, positive sentences, i.e., sentences con-
taining true idioms, are in the blank region of Fig-
ure 2, while negative ones, i.e., those containing
literal phrases, are in both regions. Accordingly,
the disambiguation amounts to i) rejecting nega-
tive ones in the shaded region, ii) rejecting nega-
tive ones in the blank region, or iii) accepting pos-
itive ones in the blank region. i) is relatively easy
since there are visible evidences in a sentence that
tell us that it is NOT an idiom. However, ii) and
iii) are difficult due to the absence of visible evi-
dences. Our method is intended to perform i), and
thus has an obvious limitation.
Next, we look cloosely at cases of success or
failure of rejecting negative sentences. There were
15 cases where rejection succeeded, which corre-
spond to i). The disambiguation knowledge that
contributed to rejection and the number of sen-
tences it rejects are as follows.16
1. Genitive Phrase Prohibition (6aII) . . . . . . . 6
2. Relative Clause Prohibition (6aI) . . . . . . . . 5
3. Detachment Constraint (6e) . . . . . . . . . . . . . 2
4. Negation Prohibition (6dI) . . . . . . . . . . . . . . 1
This shows that the Adnominal Modification Con-
straints, 1. and 2. above, are the most effective.
There were 27 cases where rejection failed.
These are classified into two types:
16There was one case where rejection succeeded due to the
dependency analysis error.
358
Class B Class C All
Recall 0.975 (195
200
) 0.939 (62
66
) 0.966 (257
266
)
Precision 1.000 (195
195
) 0.697 (62
89
) 0.905 (257
284
)
F-Measure 0.987 0.800 0.935
Class B Class C All
0.975 (195
200
) 0.939 (62
66
) 0.966 (257
266
)
1.000 (195
195
) 0.602 ( 62
103
) 0.862 (257
298
)
0.987 0.734 0.911
Table 3: Performances of the Recognizer (left side) and the Baseline System (right side)
1. Those that could have been rejected by the
Selectional Restriction (6f) . . . . . . . . . . . . . .5
2. Those that might be beyond the current tech-
nology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
1. and 2. correspond to i) and ii), respectively.
We see that the Selectional Restriction would have
been as effective as the Adnominal Modification
Constraints. A part of a sentence that the knowl-
edge could have rejected is below.
(10) basu-ga
bus-NOM
tyuu-ni
midair-DAT
ui-ta
float-PAST
?The bus floated in midair.?
An idiom, tyuu-ni uku (midair-DAT float) ?remain
to be decided,? takes as its argument something
that can be decided, i.e., ?1000:abstract? rather
than ?2:concrete? in the sense of the Goi-Taikei
ontology (Ikehara et al, 1997). Thus, (10) has no
idiomatic sense.
A simplified example of 2. is illustrated in (11).
(11) ase-o
sweat-ACC
nagasi-te
shed-and
huku-o
clothes-ACC
kiru-yorimo,
wear-rather.than,
hadaka-ga
nudity-NOM
gouriteki-da
rational-DECL
?It makes more sense to be naked than
wearing clothes in a sweat.?
The phrase ase-o nagasu (sweat-ACC shed) could
have been an idiom meaning ?work hard.? It is
contextual knowledge that prevented it from being
the idiom. Clearly, our technique is unable to han-
dle such a case, which belongs to ii), since no vis-
ible evidence is available. Dealing with that might
require some sort of machine learning technique
that exploits contextual information. Exploring
that possibility is one of our future works.
Finally, the 42 negative sentences consist of 15
sentences, which we could disambiguate, 5 sen-
tences, which Selectional Restriction could have
disambiguated, and 22, which belong to ii) and are
beyond the current technique. Thus, the real chal-
lenge lies in 7% ( 22
309
) of all idiom occurrences.
4.4 Discussion of the Dependency Knowledge
The dependency knowledge failed in only five
cases. Three of them were due to the defect
of dealing with case particles? change like omis-
sion. The other two cases were due to the noun
constituent?s incorporation into a compound noun.
(12) is a part of such a case.
(12) kaihuku-kidou-ni
recovery-orbit-DAT
nori-hajimeru
ride-begin
?(Economics) get back on a recovery track.?
The idiom, kidou-ni noru (orbit-DAT ride) ?get on
track,? has a constituent, kidou, which is incorpo-
rated into a compound noun kaihuku-kidou ?re-
covery track.? This is unexpected and cannot be
handled by the current machinery.
5 Related Work
There has been a growing awareness of Japanese
MWE problems (Baldwin and Bond, 2002). How-
ever, few attempts have been made to recognize id-
ioms in a sentence with their ambiguity and trans-
formations taken into account. In fact, most of
them only create catalogs of Japanese idiom: col-
lecting idioms as many as possible and classifying
them based on some general linguistic properties
(Tanaka, 1997; Shudo et al, 2004).
A notable exception is Oku (1990); his id-
iom recognizer takes the ambiguity and transfor-
mations into account. However, he only uses
the Genitive Phrase Prohibition, the Detachment
Constraint, and the Selectional Restriction, which
would be too few to disambiguate idioms.17 As
well, his classification does not take the recogni-
tion difficulty into account. This makes his id-
iom dictionary get bloated, since disambiguation
knowledge is given to unambiguous idioms, too.
Uchiyama et al (2005) deals with disambiguat-
ing some Japanese verbal compounds. Though
verbal compounds are not counted as idioms, their
study is in line with this study.
17We cannot compare his recognizer with ours numerically
since no disambiguation success rate is presented in Oku
(1990); only the overall performance is presented.
359
Our classification of idioms correlates loosely
with that of MWEs by Sag et al (2002). Japanese
idioms that we define correspond to lexicalized
phrases. Among lexicalized phrases, fixed expres-
sions are equal to Class A. Class B and C roughly
correspond to semi-fixed or syntactically-flexible
expressions. Note that, though the three subtypes
of lexicalized phrases are distinguished based on
what we call transformability, no distinction is
made based on the ambiguity.18
6 Conclusion
Aiming at Japanese idiom recognition with am-
biguity and transformations taken into accout, we
proposed a set of lexical knowledge for idioms and
implemented a recognizer that exploits the knowl-
edge. We maintain that requisite knowledge de-
pends on its transformability and ambiguity; trans-
formable idioms require the dependency knowl-
edge, while ambiguous ones require the disam-
biguation knowledge as well as the dependency
knowledge. As the disambiguation knowledge,
we proposed a set of constraints applicable to a
phrase when it is used as an idiom. The experi-
ment showed that more than 90% idioms could be
recognized with 90% accuracy but the success rate
of rejecting negative sentences remained 35.71%.
The experiment also revealed that, among the dis-
ambiguation knowledge, the Adnominal Modifi-
cation Constraints and the Selectional Restriction
are the most effective.
What remains to be done is two things; one is
to reveal all the subclasses of Class C and all the
disambiguation knowledge, and the other is to ap-
ply a machine learning technique to disambiguat-
ing those cases that the current technique is unable
to handle, i.e., cases without visible evidence.
In conclusion, there is still a long way to go to
draw a perfect line between literal and idiomatic
meanings, but we believe we broke new ground in
Japanese idiom recognition.
Acknowledgment A special thank goes to
Gakushu Kenkyu-sha, who permitted us to use
Gakken?s Dictionary for our research.
18The notion of decomposability of Sag et al (2002)
and Nunberg et al (1994) is independent of ambigu-
ity. In fact, ambiguous idioms are either decomposable
(hara-ga kuroi (belly-NOM black) ?black-hearted?) or non-
decomposable (hiza-o utu (knee-ACC hit) ?have a brain-
wave?). Also, unambiguous idioms are either decomposable
(hara-o yomu (belly-ACC read) ?fathom someone?s think-
ing?) or non-decomposable (saba-o yomu (chub.mackerel-
ACC read) ?cheat in counting?).
References
Timothy Baldwin and Francis Bond. 2002. Multiword
Expressions: Some Problems for Japanese NLP. In
Proceedings of the 8th Annual Meeting of the As-
sociation of Natural Language Processing, Japan,
pages 379?382, Keihanna, Japan.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai,
Akio Yokoo, Hiromi Nakaiwa, Kentaro Ogura,
Yoshifumi Ooyama, and Yoshihiko Hayashi. 1997.
Goi-Taikei ? A Japanese Lexicon. Iwanami Shoten.
Priscilla Ishida. 2000. Doushi Kanyouku-ni taisuru
Tougoteki Sousa-no Kaisou Kankei (On the Hier-
archy of Syntactic Operations Applicable to Verb
Idioms). Nihongo Kagaku (Japanese Linguistics),
7:24?43, April.
Haruhiko Kindaichi and Yasaburo Ikeda, editors. 1989.
Gakken Kokugo Daijiten (Gakken?s Dictionary).
Gakushu Kenkyu-sha.
Taku Kudo and Yuji Matsumoto. 2002. Japanese De-
pendency Analyisis using Cascaded Chunking. In
Proceedings of the 6th Conference on Natural Lan-
guage Learning (CoNLL-2002), pages 63?69.
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita,
Yoshitaka Hirano, Hiroshi Matsuda, Kazuma
Takaoka, and Masayuki Asahara, 2000. Morpholog-
ical Analysis System ChaSen version 2.2.1 Manual.
Nara Institute of Science and Technology, Dec.
Yutaka Miyaji. 1982. Kanyouku-no Imi-to Youhou
(Usage and Semantics of Idioms). Meiji Shoin.
Yoshiyuki Morita. 1985. Doushikanyouku (Verb
Idioms). Nihongogaku (Japanese Linguistics),
4(1):37?44.
Geoffrey Nunberg, Ivan A. Sag, and Thomas Wasow.
1994. Idioms. Language, 70:491?538.
Masahiro Oku. 1990. Nihongo-bun Kaiseki-ni-okeru
Jutsugo Soutou-no Kanyouteki Hyougen-no Atsukai
(Treatments of Predicative Idiomatic Expressions in
Parsing Japanese). Journal of Information Process-
ing Society of Japan, 31(12):1727?1734.
Douglas L. T. Rohde, 2005. TGrep2 User Manual ver-
sion 1.15. Massachusetts Institute of Technology.
http://tedlab.mit.edu/?dr/Tgrep2/.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
expressions: A pain in the neck for nlp. In Compu-
tational Linguistics and Intelligent Text Processing:
Third International Conference, pages 1?15.
Kosho Shudo, Toshifumi Tanabe, Masahito Takahashi,
and Kenji Yoshimura. 2004. MWEs as Non-
propositional Content Indicators. In the 2nd ACL
Workshop on Multiword Expressions: Integrating
Processing, pages 32?39.
Yasuhito Tanaka. 1997. Collecting idioms and their
equivalents. In IPSJ SIGNL 1997-NL-121.
Kiyoko Uchiyama, Timothy Baldwin, and Shun
Ishizaki. 2005. Disambiguating Japanese Com-
pound Verbs. Computer Speech and Language,
Special Issue on Multiword Expressions, 19, Issue
4:497?512.
360
Proceedings of the Fourth International Natural Language Generation Conference, pages 41?43,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Adjective-to-Verb Paraphrasing in Japanese
Based on Lexical Constraints of Verbs
Atsushi Fujita? Naruaki Masuno?? Satoshi Sato? Takehito Utsuro?
?Graduate School of Engineering, Nagoya University
??IBM Engineering & Technology Services, IBM Japan, Ltd.
?Graduate School of Systems and Information Engineering, University of Tsukuba
Abstract
This paper describes adjective-to-verb
paraphrasing in Japanese. In this para-
phrasing, generated verbs require addi-
tional suffixes according to their difference
in meaning. To determine proper suffixes
for a given adjective-verb pair, we have ex-
amined the verbal features involved in the
theory of Lexical Conceptual Structure.
1 Introduction
Textual expressions that (roughly) convey the
same meaning are called paraphrases. Since gen-
erating and recognizing paraphrases has a poten-
tial to contribute to a broad range of natural lan-
guage applications, such as MT, IE, and QA, many
researchers have done a lot of practices on auto-
matic paraphrasing in the last decade.
Most previous studies have addressed para-
phrase phenomena where the syntactic category
is not changed: e.g., noun-to-noun (?document?
? ?article?), verb-to-verb (?raise?? ?bring up?).
In these inner-categorial paraphrasing, only lim-
ited types of problems arise when replacing words
or phrases with their synonymous expressions.
On the other hand, this paper focuses on inter-
categorial paraphrasing, such as adjective-to-verb
(?attractive? ? ?attract?) that leads to novel type
of problems due to the prominent differences in
meaning and usage. In other words, calculating
those differences is more crucial to determine how
they can or cannot be paraphrased.
The aim of this study is to clarify what lexical
knowledge is required for capturing those differ-
ences, and to explore where such a knowledge can
be obtained from. Recent work in lexical seman-
tics has shown that syntactic behaviors and seman-
tic properties of words provide useful informa-
tion to explain the mechanisms of several classes
of paraphrases. More specifically, lexical proper-
ties involved in the theory of Lexical Conceptual
Structure (LCS) (Jackendoff, 1990) have seemed
to be beneficial because each verb does not func-
tion idiosyncratically. However, in the literature,
there have been less studies for other syntactic
categories than verbs. To the best of our knowl-
edge, the Meaning-Text Theory (MTT) (Mel?c?uk
and Polgue`re, 1987) is one of the very few frame-
works. In MTT, lexical properties and inter-
categorial paraphrasing are realized with a unique
semantic representation irrespective of syntactic
categories and what are called lexical functions,
e.g., S
0
(receive) = reception.
To make out how the recent advances in lexi-
cal semantics for verbs can be extended to other
syntactic categories, we assess LCS for inter-
categorial paraphrasing. We choose adjectives as
a counterpart of paraphrasing because they behave
relatively similar to verbs compared with other
categories: both adjectives and verbs have inflec-
tion and function as predicates, adnominal ele-
ments, etc. Yet, we speculate that their difference
in meaning and usage reveal intriguing generation
problems. To put it briefly, adjective-to-verb para-
phrasing in Japanese requires verbal suffixes such
as ?ta (past / attributive)? in example (1)1:
(1) s. furui otera-no jushoku-o tazune-ta.
be old temple-GEN priest-ACC to visit-PAST
I visited a priest in the old temple.
t. furubi-ta otera-no jushoku-o tazune-ta.
to olden-ATTR temple-GEN priest-ACC to visit-PAST
I visited a priest in the olden(ed) temple.
2 Preliminary investigation
To make an investigation into the variation and
distribution of required verbal suffixes, we col-
lected a set of paraphrase examples through the
following semi-automatic procedure:
Step 1. We handcrafted adjective-verb pairs
based on JCore (Sato, 2004), which classifies
Japanese words into five-levels of readability.
Our 128 pairs (for 85 adjectives) contain only
those sharing first few phonemes (reading)
1For each example, ?s? and ?t? denote an original sen-
tence and its paraphrase, respectively.
41
Table 1: Distribution of verbal suffixes used.
Verbal suffix Cadc Cpr
1
Cpr
2
ru 9 16 0
tei-ru 5 42 0
re-ru 14 8 0
re-tei-ru 2 5 0
ta 57 0 7
tei-ta 2 0 2
re-ta 6 0 1
re-tei-ta 0 0 1
both ta and tei-ru 4 0 0
both ta and ru 1 0 0
tea-ru 0 2 0
Total 100 73 11
where
ru: base form
tei: progressive / perfective
re: passive / potential
ta: past / attributive
tea: perfective
and characters (kanji), and either of adjective
or verb falls into the easiest three levels.
Step 2. Candidate paraphrases for a given sen-
tence collection are automatically generated
by replacing adjectives with their corre-
sponding verbs. Multiple candidates are gen-
erated for adjectives that correspond to mul-
tiple verbs.
Step 3. The correctness of each candidate para-
phrase is judged by two human annotators.
The basic criterion for judgement is that two
sentences are regarded as paraphrases if and
only if they share at least one interpretation.
In this step, the annotators are allowed to re-
vise candidates: (i) append verbal suffixes,
(ii) change of case markers, and (iii) insert
adverbs. Finally, candidates that both anno-
tators judge correct qualify as paraphrases.
Assuming that the variation and distribution of
verbal suffixes vary according to the usage of ad-
jectives, we separately collected paraphrase exam-
ples for adnominal and predicative usages.
Adnominal usages: For 960 sentences randomly
extracted from a one-year newspaper corpus,
Mainichi 1995, we obtained 165 examples for 142
source sentences. We then divided them into two
portions: 12 adjectives that appeared only once
and at least one examples for the other adjectives
were kept unseen (Cad
o
), while the remaining ex-
amples (Cad
c
) were used for our investigation.
Predicative usages: For 157 example sentences
within IPAL adjective dictionary (IPA, 1990), we
generated candidate paraphrases. 84 candidates
for 70 sentences qualified as paraphrases. They
are then divided into two portions according to
the tense of adjectives: Cpr
1
consists of examples
where adjectives appear in base form and Cpr
2
is
for ?ta? form (past tense).
Table 1 shows the distribution of verbal suffixes
used for given adjective-verb pairs in each portion
of example collections. We confirmed that their
distribution was fairly different. In the remaining
sections, we focus on adnominal usages because
examples of predicative usages have displayed a
degree of compositionality. Which of ?ru? or ?ta?
must be used is given by the input: if a given ad-
jective accompanies past tense, the resultant ver-
bal suffix is necessarily that for present tense fol-
lowed by ?ta.?
3 Determining verbal suffixes
The task we address here is to determine verbal
suffixes for a given input, a pair of an adnominal
usage of adjective in a certain context and a candi-
date verb given by our adjective-verb list.
From the viewpoint of language generation,
this task can be thought of as generating verbal
expressions where options are already given in
Table 1. A straightforward way for determining
verbal suffixes is to make use of lexical properties
of verbs as constraints on generation. To manifest
them, in particular aspectual properties involved
in LCS, we first designed seven types of linguis-
tic tests shown in Table 2. They are derived from
a classical analysis of verb semantics in Japanese
(Kageyama, 1996) and some ongoing projects on
constructing LCS dictionaries (Kato et al, 2005;
Takeuchi et al, 2006). We then manually ex-
amined 128 verbs in Section 2 under those tests.
To determine the word sense in which the deriva-
tive relationship hold good, example sentences in
IPAL verb dictionary (IPA, 1987) for each verb
were used. For a verb which was out of the dic-
tionary, we manually gave a sample sentence.
Since our aim is to explain why a certain ver-
bal suffix is used for a given input, we have not
feverishly applied a machine learning algorithm to
the task. Instead, we have manually created a rule-
based model shown in Table 3 using Cad
c
, where
each if-then rule assigns either of verbal suffixes in
Table 1 to a given input based on verbal features in
Table 2 and some other features below:
? D: affix pair of the adjective and the candi-
date verb: e.g., ?A shii-V mu? for ?kuyashii
(be regretful)? ? ?kuyamu (to regret)?
? N : disjunction of semantic classes in a the-
saurus (The Natural Institute for Japanese
Language, 2004) for the modified noun
? C: whether the adjective is head of clause
4 Experiment and discussion
By conducting an empirical experiment with Cad
c
and Cad
o
, we evaluate how our model (RULE)
properly determines verbal suffixes. A compar-
ison with a simple baseline model (BL) is also
done. BL selects the most frequently used suffix
(in this experiment ?ta?) for any given input.
42
Table 2: Linguistic tests for verbs derived from Lexical Conceptual Structure (Kageyama, 1996).
Label Description
Va whether the verb allows accusative case
Vb whether the verb can co-occur with a temporal adverb ?ichi-jikan (for one hour)? or its variant
Vc whether the verb can co-occur with a temporal adverb ?ichi-jikan-de (in one hour)? or its variant
Vd whether the verb can be followed by ?tearu (perfective)? when its accusative case is moved to nominative
Ve interpretation of the verb followed by ?tei-ru (progressive / perfective)?
Vf when followed by ?ta,? whether the verb can have the perfective interpretation or just past tense
Vg whether the verb can co-occur with a sort of adverb which indicates intention of the action: e.g. ?wazato (purposely)? and ?iyaiya (reluctantly)?
Table 3: The rule-set for determining verbal suf-
fixes, where ?(non)? indicates non-paraphrasable.
Order Condition (conjunction of ?feature label =? value?) Verbal suffix
1 Va=?yes?? Vb=?yes? ? Vf=?no? ? re-ru
N=?except Human (1.10)? ?
D=?A ui?V bumu? ? ?A i?V mu? ? ?A asii?V u?
2 Va=?yes?? Vb=?yes? ? Vf=?no? ? ta
Vd=?no? ?N=?Mind: mind, attitude (1.303)?
3 Va=?no? ? Vg=?yes? ta
4 Va=?no? ? Vf=?yes??D=?A i?V migakaru? ta / tei-ru
5 C=?clause? ?D=?A i?V maru? ru
6 Va=?no? ? Vf=?yes? ta
7 Va=?no? ? Vb=?yes?? Vf=?no? ta
8 Vb=?yes? ? Vf=?no? ? Vc=?yes?? tei-ru
Vd=?yes?? Ve=?progressive??N=?Subject (1.2)?
9 ? (non)
Table 4 shows the experimental results, where
recall and precision are calculated with regard
to input adjective-verb pairs. Among rules in
Table 3, rules 1 (for ?re-ru?), 3, 6, and 7 (for ?ta?
where Va=?no?) performed much better than the
other rules. This indicates that these rules and fea-
tures in their conditions properly reflect our lin-
guistic intuition. For instance, rule 6 reflects that
a change-of-state intransitive verb expresses re-
sultative meaning as adjectives when it modifies
Theme of the event via ?ta? (Kageyama, 1996)
as shown in (1), and rule 2 does that a psycho-
logical verb modifies a nouns with ?re-ru? when
the noun arouses the specific emotion, such as re-
gretting mistakes (e.g., ?kuyashii (be regretful)?
? ?kuyama-re-ru (be regretted)?). The aspectual
property captured by the tests in Table 2 is used to
classify verbs into these semantic classes.
On the other hand, the rules for the other types
are immature due to lack of examples: we cannot
find out even necessary conditions to be ?ru,? ?tei-
ru,? etc. What is required to induce proper con-
ditions for these suffixes is a larger example col-
lection and discovering another semantic property
and a set of linguistic tests for capturing it.
5 Conclusion and future work
In this paper, we focused on inter-categorial para-
phrasing and reported on our study on an issue
in adjective-to-verb paraphrasing. Two general-
purpose resources and a task-specific rule-set have
been handcrafted to generate proper verbal suf-
fixes. Although the rule-based model has achieved
better performance than a simple baseline model,
there is a plenty of room for improvement.
Table 4: Recall and precision of determining ver-
bal suffix for given adjective-verb pairs.
Cadc Cado
Verbal suffix Recall Precision Recall Precision
ta (Va=?yes?) 3/13 3/3 1/6 1/1
ta (Va=?no?) 42/44 42/63 18/18 18/29
re-ru 12/14 12/19 7/13 7/11
ru 3/9 3/6 0/2 0/5
tei-ru 1/5 1/7 2/8 2/6
ta / tei-ru 2/4 2/2 1/2 1/1
No rule for 11 inputs for 7 inputs
Total (RULE) 63/100 63/100 29/56 29/53
(63%) (63%) (52%) (55%)
BL 57/100 57/148 24/56 24/83
(57%) (39%) (43%) (29%)
Future work includes (i) to enlarge our two
resources as in (Dorr, 1997; Habash and Dorr,
2003) evolving an effective construction method,
(ii) intrinsic evaluation of those resources, and, of
course, (iii) to enhance the paraphrasing models
through further experiments with a larger test-set.
References
B. J. Dorr. 1997. Large-scale dictionary construction for
foreign language tutoring and interlingual machine trans-
lation. Machine Translation, 12(4):271?322.
N. Habash and B. J. Dorr. 2003. A categorial variation
database for English. In Proceedings of the 2003 Human
Language Technology Conference and the North Ameri-
can Chapter of the Association for Computational Lin-
guistics (HLT-NAACL), pages 17?23.
IPA. 1987. IPA Lexicon of the Japanese language for com-
puters (Basic Verbs). Information-technology Promotion
Agency. (in Japanese).
IPA. 1990. IPA Lexicon of the Japanese language for com-
puters (Basic Adjectives). Information-technology Pro-
motion Agency. (in Japanese).
R. Jackendoff. 1990. Semantic structures. The MIT Press.
T. Kageyama. 1996. Verb semantics. Kurosio Publishers.
(in Japanese).
T. Kato, S. Hatakeyama, H. Sakamoto, and T. Ito. 2005.
Constructing Lexical Conceptual Structure dictionary for
verbs of Japanese origin. In Proceedings of the 11th An-
nual Meeting of the Association for Natural Language
Processing, pages 871?874. (in Japanese).
I. Mel?c?uk and A. Polgue`re. 1987. A formal lexicon in
meaning-text theory (or how to do lexica with words).
Computational Linguistics, 13(3-4):261?275.
S. Sato. 2004. Identifying spelling variations of Japanese
words. In Information Processing Society of Japan SIG
Notes, NL-161-14, pages 97?104. (in Japanese).
K. Takeuchi, K. Inui, and A. Fujita. 2006. Construction of
compositional lexical database based on Lexical Concep-
tual Structure for Japanese verbs. In T. Kageyama, editor,
Lexicon Forum No.2. Hitsuji Shobo. (in Japanese).
The Natural Institute for Japanese Language. 2004. Word
list by semantic principles, revised and enlarged edition.
Dainippon Tosho. (in Japanese).
43
A Comparative Study on Compositional Translation Estimation
using a Domain/Topic-Specific Corpus collected from the Web
Masatsugu Tonoike?, Mitsuhiro Kida?, Toshihiro Takagi?, Yasuhiro Sasaki?,
Takehito Utsuro??, Satoshi Sato? ? ?
?Graduate School of Informatics, Kyoto University
Yoshida-Honmachi, Sakyo-ku, Kyoto 606-8501, Japan
??Graduate School of Systems and Information Engineering, University of Tsukuba
1-1-1, Tennodai, Tsukuba, 305-8573, Japan
? ? ?Graduate School of Engineering, Nagoya University
Furo-cho, Chikusa-ku, Nagoya 464-8603, Japan
Abstract
This paper studies issues related to the
compilation of a bilingual lexicon for tech-
nical terms. In the task of estimating bilin-
gual term correspondences of technical
terms, it is usually rather difficult to find
an existing corpus for the domain of such
technical terms. In this paper, we adopt
an approach of collecting a corpus for the
domain of such technical terms from the
Web. As a method of translation esti-
mation for technical terms, we employ a
compositional translation estimation tech-
nique. This paper focuses on quantita-
tively comparing variations of the compo-
nents in the scoring functions of composi-
tional translation estimation. Through ex-
perimental evaluation, we show that the
domain/topic-specific corpus contributes
toward improving the performance of the
compositional translation estimation.
1 Introduction
This paper studies issues related to the compilation
of a bilingual lexicon for technical terms. Thus
far, several techniques of estimating bilingual term
correspondences from a parallel/comparable cor-
pus have been studied (Matsumoto and Utsuro,
2000). For example, in the case of estimation from
comparable corpora, (Fung and Yee, 1998; Rapp,
1999) proposed standard techniques of estimating
bilingual term correspondences from comparable
corpora. In their techniques, contextual similarity
between a source language term and its translation
candidate is measured across the languages, and
all the translation candidates are re-ranked accord-
ing to their contextual similarities. However, there
are limited number of parallel/comparable corpora
that are available for the purpose of estimating
bilingual term correspondences. Therefore, even
if one wants to apply those existing techniques to
the task of estimating bilingual term correspon-
dences of technical terms, it is usually rather dif-
ficult to find an existing corpus for the domain of
such technical terms.
On the other hand, compositional translation es-
timation techniques that use a monolingual corpus
(Fujii and Ishikawa, 2001; Tanaka and Baldwin,
2003) are more practical. It is because collecting a
monolingual corpus is less expensive than collect-
ing a parallel/comparable corpus. Translation can-
didates of a term can be compositionally generated
by concatenating the translation of the constituents
of the term. Here, the generated translation candi-
dates are validated using the domain/topic-specific
corpus.
In order to assess the applicability of the com-
positional translation estimation technique, we
randomly pick up 667 Japanese and English tech-
nical term translation pairs of 10 domains from ex-
isting technical term bilingual lexicons. We then
manually examine their compositionality, and find
out that 88% of them are actually compositional,
which is a very encouraging result.
But still, it is expensive to collect a
domain/topic-specific corpus. Here, we adopt
an approach of using the Web, since documents
of various domains/topics are available on the
Web. When validating translation candidates
using the Web, roughly speaking, there exist the
following two approaches. In the first approach,
translation candidates are validated through
the search engine (Cao and Li, 2002). In the
second approach, a domain/topic-specific corpus
is collected from the Web in advance and fixed
11
collecting terms
of specific
domain/topic
(language S )
XSU (# of translations
is one)
compiled bilingual lexicon
process data
collecting
corpus
(language T )
domain/topic
specific
corpus
(language T )
sample terms
of specific 
domain/topic
(language S )
XSTU , XSTM ,YST
estimating bilingual term
correspondences
language pair (S,T )
term set
(language S )
XTU
(lang. T )
translation set
(language T )
web
(language S )
web
(language S )
existing
bilingual lexicon
XSM (# of translations
is more than one)
YS (# of translations
is zero)
web
(language T )
web
(language T )
looking up
bilingual lexicon
validating
translation
candidates
web
(language T )
web
(language T )
Figure 1: Compilation of a Domain/Topic-
Specific Bilingual Lexicon using the Web
before translation estimation, then generated
translation candidates are validated against the
domain/topic-specific corpus (Tonoike et al,
2005). The first approach is preferable in terms of
coverage, while the second is preferable in terms
of computational efficiency. This paper mainly
focuses on quantitatively comparing the two
approaches in terms of coverage and precision of
compositional translation estimation.
More specifically, in compositional translation
estimation, we decompose the scoring function
of a translation candidate into two components:
bilingual lexicon score and corpus score. In this
paper, we examine variants for those components
and define 9 types of scoring functions in total.
Regarding the above mentioned two approaches
to validating translation candidates using the Web,
the experimental result shows that the second
approach outperforms the first when the correct
translation does exist in the corpus. Furthermore,
we examine the methods that combine two scor-
ing functions based on their agreement. The ex-
perimental result shows that it is quite possible to
achieve precision much higher than those of single
scoring functions.
2 Overall framework
The overall framework of compiling a bilingual
lexicon from the Web is illustrated as in Figure 1.
Suppose that we have sample terms of a specific
domain/topic, then the technical terms that are to
be listed as the headwords of a bilingual lexicon
are collected from the Web by the related term col-
lection method of (Sato and Sasaki, 2003). These
collected technical terms can be divided into three
subsets depending on the number of translation
candidates present in an existing bilingual lexicon,
i.e., the subset XUS of terms for which the number
of translations in the existing bilingual lexicon is
one, the subset XMS of terms for which the number
of translations is more than one, and the subset YS
of terms that are not found in the existing bilingual
lexicon (henceforth, the union XUS ? XMS will be
denoted as XS). Here, the translation estimation
task here is to estimate translations for the terms
of the subsets XMS and YS . A new bilingual lex-
icon is compiled from the result of the translation
estimation for the terms of the subsets XMS and
YS as well as the translation pairs that consist of
the terms of the subset XUS and their translations
found in the existing bilingual lexicon.
For the terms of the subset XMS , it is required
that an appropriate translation is selected from
among the translation candidates found in the ex-
isting bilingual lexicon. For example, as a trans-
lation of the Japanese technical term ?????,?
which belongs to the logic circuit domain, the term
?register? should be selected but not the term ?reg-
ista? of the football domain. On the other hand, for
the terms of YS , it is required that the translation
candidates are generated and validated. In this pa-
per, out of the above two tasks, we focus on the
latter of translation candidate generation and val-
idation using the Web. As we introduced in the
previous section, here we experimentally compare
the two approaches to validating translation candi-
dates. The first approach directly uses the search
engine, while the second uses the domain/topic-
specific corpus, which is collected in advance from
the Web. Here, in the second approach, we use the
term of XUS , which has only one translation in the
existing bilingual lexicon. The set of translations
of the terms of the subset XUS is denoted as XUT .
Then, in the second approach, the domain/topic-
specific corpus is collected from the Web using the
terms of the set XUT .
3 Compositional Translation Estimation
for Technical Terms
3.1 Overview
An example of compositional translation estima-
tion for the Japanese technical term ??????
?? is illustrated in Figure 2. First, the Japanese
technical term ???????? is decomposed
into its constituents by consulting an existing
bilingual lexicon and retrieving Japanese head-
12
? application(1)
? practical(0.3)
? applied(1.6)
? action(1)
? activity(1)
? behavior(1)
? analysis(1)
? diagnosis(1)
? assay(0.3)
? behavior analysis(10)
??Compositional generation 
of translation candidate
? applied behavior analysis(17.6)
? application behavior analysis(11)
? applied behavior diagnosis(1)
??Decompose source term into constituents  
??Translate constituents into target language      process
?? ?? ??a
?? ????b
Generated translation candidates
?(1.6?1?1)+(1.6?10)
? application(1)
? practical(0.3)
? applied(1.6)
Figure 2: Compositional Translation Estimation
for the Japanese Technical Term ????????
words.1 In this case, the result of this decompo-
sition can be given as in the cases ?a? and ?b?
(in Figure 2). Then, each constituent is translated
into the target language. A confidence score is as-
signed to the translation of each constituent. Fi-
nally, translation candidates are generated by con-
catenating the translation of those constituents ac-
cording to word ordering rules considering prepo-
sitional phrase construction.
3.2 Collecting a Domain/Topic-Specific
Corpus
When collecting a domain/topic-specific corpus of
the language T , for each technical term xUT in the
set XUT , we collect the top 100 pages obtained
from search engine queries that include the term
xUT . Our search engine queries are designed such
that documents that describe the technical term xUT
are ranked high. For example, an online glossary
is one such document. When collecting a Japanese
corpus, the search engine ?goo?2 is used. The spe-
cific queries that are used in this search engine
are phrases with topic-marking postpositional par-
ticles such as ?xUT ??,? ?xUT ???,? ?xUT ?,?
and an adnominal phrase ?xUT ?,? and ?xUT .?
3.3 Translation Estimation
3.3.1 Compiling Bilingual Constituents
Lexicons
This section describes how to compile bilingual
constituents lexicons from the translation pairs of
1Here, as an existing bilingual lexicon, we use Ei-
jiro(http://www.alc.co.jp/) and bilingual constituents lexicons
compiled from the translation pairs of Eijiro (details to be de-
scribed in the next section).
2http://www.goo.ne.jp/
 
applied mathematics : ?? ??
applied science : ?? ??
applied robot : ?? ????
.
.
. frequency
? ??
applied : ?? : 40
 
Figure 3: Example of Estimating Bilingual Con-
stituents Translation Pair (Prefix)
the existing bilingual lexicon Eijiro. The under-
lying idea of augmenting the existing bilingual
lexicon with bilingual constituents lexicons is il-
lustrated in Figure 3. Suppose that the existing
bilingual lexicon does not include the translation
pair ?applied : ??,? while it includes many
compound translation pairs with the first English
word ?applied? and the first Japanese word ??
?.?3 In such a case, we align those translation
pairs and estimate a bilingual constituent transla-
tion pair which is to be collected into a bilingual
constituents lexicon.
More specifically, from the existing bilingual
lexicon, we first collect translation pairs whose
English terms and Japanese terms consist of two
constituents into another lexicon P
2
. We com-
pile the ?bilingual constituents lexicon (prefix)?
from the first constituents of the translation pairs
in P
2
and compile the ?bilingual constituents lex-
icon (suffix)? from their second constituents. The
number of entries in each language and those of
the translation pairs in these lexicons are shown in
Table 1.
The result of our assessment reveals that only
48% of the 667 translation pairs mentioned in Sec-
tion 1 can be compositionally generated by using
Eijiro, while the rate increases up to 69% using
both Eijiro and ?bilingual constituents lexicons.?4
3.3.2 Score of Translation Candidates
This section gives the definition of the scores
of a translation candidate in compositional trans-
lation estimation.
First, let ys be a technical term whose transla-
tion is to be estimated. We assume that ys is de-
3Japanese entries are supposed to be segmented into a
sequence of words by the morphological analyzer JUMAN
(http://www.kc.t.u-tokyo.ac.jp/nl-resource/juman.html).
4In our rough estimation, the upper bound of this rate
is approximately 80%. An improvement from 69% to 80%
could be achieved by extending the bilingual constituents lex-
icons.
13
Table 1: Numbers of Entries and Translation Pairs
in the Lexicons
lexicon # of entries # of translationEnglish Japanese pairs
Eijiro 1,292,117 1,228,750 1,671,230
P
2
217,861 186,823 235,979
B
P
37,090 34,048 95,568
B
S
20,315 19,345 62,419
B 48,000 42,796 147,848
Eijiro : existing bilingual lexicon
P
2
: entries of Eijiro with two constituents
in both languages
B
P
: bilingual constituents lexicon (prefix)
B
S
: bilingual constituents lexicon (suffix)
B : bilingual constituents lexicon (merged)
composed into their constituents as below:
ys = s1, s2, ? ? ? , sn (1)
where each si is a single word or a sequence of
words.5 For ys, we denote a generated translation
candidate as yt.
yt = t1, t2, ? ? ? , tn (2)
where each ti is a translation of si. Then the trans-
lation pair ?ys, yt? is represented as follows.
?ys, yt? = ?s1, t1?, ?s2, t2?, ? ? ? , ?sn, tn? (3)
The score of a generated translation candidate is
defined as the product of a bilingual lexicon score
and a corpus score as follows.
Q(ys, yt) = Qdict(ys, yt) ? Qcoprus(yt) (4)
Bilingual lexicon score measures appropriateness
of correspondence of ys and yt. Corpus score
measures appropriateness of the translation candi-
date yt based on the target language corpus. If a
translation candidate is generated from more than
one sequence of translation pairs, the score of the
translation candidate is defined as the sum of the
score of each sequence.
Bilingual Lexicon Score
In this paper, we compare two types of bilin-
gual lexicon scores. Both scores are defined as the
product of scores of translation pairs included in
the lexicons presented in the previous section as
follows.
5Eijiro has both single word entries and compound word
entries.
? Frequency-Length
Qdict(ys, yt) =
n
?
i=1
q(?si, ti?) (5)
The first type of bilingual lexicon scores is re-
ferred to as ?Frequency-Length.? This score is
based on the length of translation pairs and the fre-
quencies of translation pairs in the bilingual con-
stituent lexicons (prefix,suffix) BP , BS in Table 1.
In this paper, we first assume that the translation
pairs follow certain preference rules and that they
can be ordered as below:
1. Translation pairs ?s, t? in the existing bilin-
gual lexicon Eijiro, where the term s consists
of two or more constituents.
2. Translation pairs in the bilingual constituents
lexicons whose frequencies in P
2
are high.
3. Translation pairs ?s, t? in the existing bilin-
gual lexicon Eijiro, where the term s consists
of exactly one constituent.
4. Translation pairs in the bilingual constituents
lexicons whose frequencies in P
2
are not
high.
As the definition of the confidence score
q(?s, t?) of a translation pair ?s, t?, we use the fol-
lowing:
q(?s, t?) =
?
?
?
10
(compo(s)?1) (?s, t? in Eijiro)
log
10
fp(?s, t?) (?s, t? in BP )
log
10
fs(?s, t?) (?s, t? in BS)
(6)
, where compo(s) denotes the word count of s,
fp(?s, t?) represents the frequency of ?s, t? as the
first constituent in P
2
, and fs(?s, t?) represents the
frequency of ?s, t? as the second constituent in P
2
.
? Probability
Qdict(ys, yt) =
n
?
i=1
P (si|ti) (7)
The second type of bilingual lexicon scores is re-
ferred to as ?Probability.? This score is calcu-
lated as the product of the conditional probabili-
ties P (si|ti). P (s|t) is calculated using bilingual
lexicons in Table 1.
P (s|t) =
fprob(?s, t?)
?
s
j
fprob(?sj , t?)
(8)
14
Table 2: 9 Scoring Functions of Translation Candidates and their Components
bilingual lexicon score corpus score corpus
score ID freq-length probability probability frequency occurrence off-line on-line
(search engine)
A prune/final prune/final o
B prune/final prune/final o
C prune/final prune/final o
D prune/final prune o
E prune/final
F prune/final final prune o
G prune/final prune/final o
H prune/final final o
I prune/final final o
fprob(?s, t?) denotes the frequency of the transla-
tion pair ?s, t? in the bilingual lexicons as follows:
fprob(?s, t?) =
{
10 (?s, t? in Eijiro)
fB(?s, t?) (?s, t? in B)
(9)
Note that the frequency of a translation pair in Ei-
jiro is regarded as 106 and fB(?s, t?) denotes the
frequency of the translation pair ?s, t? in the bilin-
gual constituent lexicon B.
Corpus Score
We evaluate three types of corpus scores as fol-
lows.
? Probability: the occurrence probability of yt
estimated by the following bi-gram model
Qcorpus(yt) = P (t1) ?
n
?
i=1
P (ti+1|ti) (10)
? Frequency: the frequency of a translation
candidate in a target language corpus
Qcorpus(yt) = freq(yt) (11)
? Occurrence: whether a translation candidate
occurs in a target language corpus or not
Qcorpus(yt) =
?
?
?
?
?
1 yt occurs in a corpus
0 yt does not occur
in a corpus
(12)
6It is necessary to empirically examine whether or not the
definition of the frequency of a translation pair in Eijiro is
appropriate.
Variation of the total scoring functions
As shown in Table 2, in this paper, we examine
the 9 combinations of the bilingual lexicon scores
and the corpus scores. In the table, ?prune? indi-
cates that the score is used for ranking and pruning
sub-sequences of generated translation candidates
in the course of generating translation candidates
using a dynamic programming algorithm. ?Final?
indicates that the score is used for ranking the fi-
nal outputs of generating translation candidates.
In the column ?corpus?, ?off-line? indicates that
a domain/topic-specific corpus is collected from
the Web in advance and then generated transla-
tion candidates are validated against this corpus.
?On-line? indicates that translation candidates are
directly validated through the search engine.
Roughly speaking, the scoring function ?A? cor-
responds to a variant of the model proposed by
(Fujii and Ishikawa, 2001). The scoring func-
tion ?D? is a variant of the model proposed by
(Tonoike et al, 2005) and ?E? corresponds to the
bilingual lexicon score of the scoring function ?D?.
The scoring function ?I? is intended to evaluate the
approach proposed in (Cao and Li, 2002).
3.3.3 Combining Two Scoring Functions
based on their Agreement
In this section, we examine the method that
combines two scoring functions based on their
agreement. The two scoring functions are selected
out of the 9 functions introduced in the previous
section. In this method, first, confidence of trans-
lation candidates of a technical term are measured
by the two scoring functions. Then, if the first
ranked translation candidates of both scoring func-
tions agree, this method outputs the agreed trans-
lation candidate. The purpose of introducing this
method is to prefer precision to recall.
15
collecting terms
of specific
domain/topic
(language S )
XSU (# of translations
is one)
compiled bilingual lexicon
process data
collecting
corpus
(language T )
sample terms
of specific 
domain/topic
(language S )
XSTU , XSTM ,YST
estimating bilingual term
correspondences
language pair (S,T )
term set
(language S )
XTU
(lang. T )
translation set
(language T )
web
(language S )
web
(language S )
existing
bilingual lexicon
XSM (# of translations
is more than one)
YS (# of translations
is zero)
web
(language T )
web
(language T )
looking up
bilingual lexicon
domain/topic
specific
corpus
(language T )
validating
translation
candidates
web
(language T )
web
(language T )
Figure 4: Experimental Evaluation of Translation
Estimation for Technical Terms with/without the
Domain/Topic-Specific Corpus (taken from Fig-
ure 1)
4 Experiments and Evaluation
4.1 Translation Pairs for Evaluation
In our experimental evaluation, within the frame-
work of compiling a bilingual lexicon for techni-
cal terms, we evaluate the translation estimation
portion that is indicated by the bold line in Fig-
ure 4. In this paper, we simply omit the evalua-
tion of the process of collecting technical terms to
be listed as the headwords of a bilingual lexicon.
In order to evaluate the translation estimation por-
tion, terms are randomly selected from the 10 cate-
gories of existing Japanese-English technical term
dictionaries listed in Table 3, for each of the sub-
sets XUS and YS (here, the terms of YS that consist
of only one word or morpheme are excluded). As
described in Section 1, the terms of the set XUT (the
set of translations for the terms of the subset XUS )
is used for collecting a domain/topic-specific cor-
pus from the Web. As shown in Table 3, size of the
collected corpora is 48MB on the average. Trans-
lation estimation evaluation is to be conducted for
the subset YS . For each of the 10 categories, Ta-
ble 3 shows the sizes of the subsets XUS and YS ,
and the rate of including correct translation within
the collected domain/topic-specific corpus for YS .
In the following, we show the evaluation results
with the source language S as English and the tar-
get language T as Japanese.
4.2 Evaluation of single scoring functions
This section gives the results of evaluating single
scoring functions A ? I listed in Table 2.
Table 4 shows three types of experimental re-
sults. The column ?the whole set YS? shows the
results against the whole set YS . The column
?generatable? shows the results against the trans-
lation pairs in YS that can be generated through
the compositional translation estimation process.
69% of the terms in ?the whole set YS? belongs
to the set ?generatable?. The column ?gene.-exist?
shows the result against the source terms whose
correct translations do exist in the corpus and that
can be generated through the compositional trans-
lation estimation process. 50% of the terms in ?the
whole set YS? belongs to the set ?gene.-exist?. The
column ?top 1? shows the correct rate of the first
ranked translation candidate. The column ?top 10?
shows the rate of including the correct candidate
within top 10.
First, in order to evaluate the effectiveness of
the approach of validating translation candidates
by using a target language corpus, we compare the
scoring functions ?D? and ?E?. The difference be-
tween them is whether or not they use a corpus
score. The results for the whole set YS show that
using a corpus score, the precision improves from
33.9% to 43.0%. This result supports the effec-
tiveness of the approach of validating translation
candidates using a target language corpus.
As can be seen from these results for the whole
set YS , the correct rate of the scoring function ?I?
that directly uses the web search engine in the cal-
culation of its corpus score is higher than those
of other scoring functions that use the collected
domain/topic-specific corpus. This is because,
for the whole set YS , the rate of including cor-
rect translation within the collected domain/topic-
specific corpus is 72% on the average, which is
not very high. On the other hand, the results of the
column ?gene.-exist? show that if the correct trans-
lation does exist in the corpus, most of the scor-
ing functions other than ?I? can achieve precisions
higher than that of the scoring function ?I?. This
result supports the effectiveness of the approach
of collecting a domain/topic-specific corpus from
the Web in advance and then validating generated
translation candidates against this corpus.
4.3 Evaluation of combining two scoring
functions based on their agreement
The result of evaluating the method that combines
two scoring functions based on their agreement is
shown in Table 5. This result indicates that com-
binations of scoring functions with ?off-line?/?on-
16
Table 3: Number of Translation Pairs for Evaluation (S=English)
dictionaries categories |Y
S
| |X
U
S
| corpus size C(S)
Electromagnetics 33 36 28MB 85%
McGraw-Hill Electrical engineering 45 34 21MB 71%
Optics 31 42 37MB 65%
Iwanami Programming language 29 37 34MB 93%Programming 29 29 33MB 97%
Dictionary of (Computer) 100 91 67MB 51%Computer
Anatomical Terms 100 91 73MB 86%
Dictionary of Disease 100 91 83MB 77%
250,000 Chemicals and Drugs 100 94 54MB 60%
medical terms Physical Science and Statistics 100 88 56MB 68%
Total 667 633 482MB 72%
McGraw-Hill : Dictionary of Scientific and Technical Terms
Iwanami : Encyclopedic Dictionary of Computer Science
C(S) : for Y
S
, the rate of including correct translations within the collected domain/topic-specific corpus
Table 4: Result of Evaluating single Scoring Functions
the whole set Y
S
(667 terms?100%) generatable (458 terms?69%) gene.-exist (333 terms?50%)
ID top 1 top 10 top 1 top 10 top 1 top 10
A 43.8% 52.9% 63.8% 77.1% 82.0% 98.5%
B 42.9% 50.7% 62.4% 73.8% 83.8% 99.4%
C 43.0% 58.0% 62.7% 84.5% 75.1% 94.6%
D 43.0% 47.4% 62.7% 69.0% 85.9% 94.6%
E 33.9% 57.3% 49.3% 83.4% 51.1% 84.1%
F 40.2% 47.4% 58.5% 69.0% 80.2% 94.6%
G 39.1% 46.8% 57.0% 68.1% 78.1% 93.4%
H 43.8% 57.3% 63.8% 83.4% 73.6% 84.1%
I 49.8% 57.3% 72.5% 83.4% 74.8% 84.1%
Table 5: Result of combining two scoring func-
tions based on their agreement
corpus combination precision recall F
?=1
A & I 88.0% 27.6% 0.420
off-line/ D & I 86.0% 29.5% 0.440
on-line F & I 85.1% 29.1% 0.434
H & I 58.7% 37.5% 0.457
A & H 86.0% 30.4% 0.450
F & H 80.6% 33.7% 0.476
off-line/ D & H 80.4% 32.7% 0.465
off-line A & D 79.0% 32.1% 0.456
A & F 74.6% 33.0% 0.457
D & F 68.2% 35.7% 0.469
line? corpus tend to achieve higher precisions than
those with ?off-line?/?off-line? corpus. This result
also shows that it is quite possible to achieve high
precisions even by combining scoring functions
with ?off-line?/?off-line? corpus (the pair ?A? and
?H?). Here, the two scoring functions ?A? and ?H?
are the one with frequency-based scoring func-
tions and that with probability-based scoring func-
tions, and hence, have quite different nature in the
design of their scoring functions.
5 Related Works
As a related work, (Fujii and Ishikawa, 2001) pro-
posed a technique for compositional estimation of
bilingual term correspondences for the purpose of
cross-language information retrieval. One of the
major differences between the technique of (Fu-
jii and Ishikawa, 2001) and the one proposed in
this paper is that in (Fujii and Ishikawa, 2001), in-
stead of a domain/topic-specific corpus, they use a
corpus containing the collection of technical pa-
pers, each of which is published by one of the
65 Japanese associations for various technical do-
mains. Another significant difference is that in
(Fujii and Ishikawa, 2001), they evaluate only the
performance of the cross-language information re-
trieval and not that of translation estimation.
(Cao and Li, 2002) also proposed a method
of compositional translation estimation for com-
pounds. In the method of (Cao and Li, 2002), the
translation candidates of a term are composition-
ally generated by concatenating the translation of
the constituents of the term and are validated di-
rectly through the search engine. In this paper,
we evaluate the approach proposed in (Cao and
Li, 2002) by introducing a total scoring function
17
that is based on validating translation candidates
directly through the search engine.
6 Conclusion
This paper studied issues related to the compila-
tion a bilingual lexicon for technical terms. In
the task of estimating bilingual term correspon-
dences of technical terms, it is usually rather dif-
ficult to find an existing corpus for the domain
of such technical terms. In this paper, we adopt
an approach of collecting a corpus for the do-
main of such technical terms from the Web. As
a method of translation estimation for technical
terms, we employed a compositional translation
estimation technique. This paper focused on quan-
titatively comparing variations of the components
in the scoring functions of compositional transla-
tion estimation. Through experimental evaluation,
we showed that the domain/topic specific corpus
contributes to improving the performance of the
compositional translation estimation.
Future work includes complementally integrat-
ing the proposed framework of compositional
translation estimation using the Web with other
translation estimation techniques. One of them is
that based on collecting partially bilingual texts
through the search engine (Nagata and others,
2001; Huang et al, 2005). Another technique
which seems to be useful is that of transliteration
of names (Knight and Graehl, 1998; Oh and Choi,
2005).
References
Y. Cao and H. Li. 2002. Base noun phrase translation using
Web data and the EM algorithm. In Proc. 19th COLING,
pages 127?133.
A. Fujii and T. Ishikawa. 2001. Japanese/english cross-
language information retrieval: Exploration of query
translation and transliteration. Computers and the Hu-
manities, 35(4):389?420.
P. Fung and L. Y. Yee. 1998. An IR approach for translating
new words from nonparallel, comparable texts. In Proc.
17th COLING and 36th ACL, pages 414?420.
F. Huang, Y. Zhang, and S. Vogel. 2005. Mining key phrase
translations from web corpora. In Proc. HLT/EMNLP,
pages 483?490.
K. Knight and J. Graehl. 1998. Machine transliteration.
Computational Linguistics, 24(4):599?612.
Y. Matsumoto and T. Utsuro. 2000. Lexical knowledge ac-
quisition. In R. Dale, H. Moisl, and H. Somers, editors,
Handbook of Natural Language Processing, chapter 24,
pages 563?610. Marcel Dekker Inc.
M. Nagata et al 2001. Using the Web as a bilingual dictio-
nary. In Proc. ACL-2001 Workshop on Data-driven Meth-
ods in Machine Translation, pages 95?102.
J. Oh and K. Choi. 2005. Automatic extraction of english-
korean translations for constituents of technical terms. In
Proc. 2nd IJCNLP, pages 450?461.
R. Rapp. 1999. Automatic identification of word translations
from unrelated English and German corpora. In Proc.
37th ACL, pages 519?526.
S. Sato and Y. Sasaki. 2003. Automatic collection of related
terms from the web. In Proc. 41st ACL, pages 121?124.
T. Tanaka and T. Baldwin. 2003. Translation selection for
japanese-english noun-noun compounds. In Proc. Ma-
chine Translation Summit IX, pages 378?85.
M. Tonoike, M. Kida, T. Takagi, Y. Sasaki, T. Utsuro, and
S. Sato. 2005. Effect of domain-specific corpus in com-
positional translation estimation for technical terms. In
Proc. 2nd IJCNLP, Companion Volume, pages 116?121.
18
Chunking Japanese Compound Functional Expressions
by Machine Learning
Masatoshi Tsuchiya? and Takao Shime? and Toshihiro Takagi?
Takehito Utsuro?? and Kiyotaka Uchimoto?? and Suguru Matsuyoshi?
Satoshi Sato?? and Seiichi Nakagawa??
?Computer Center / ??Department of Information and Computer Sciences,
Toyohashi University of Technology, Tenpaku-cho, Toyohashi, 441?8580, JAPAN
?Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto, 606?8501, JAPAN
??Graduate School of Systems and Information Engineering, University of Tsukuba,
1-1-1, Tennodai, Tsukuba, 305-8573, JAPAN
??National Institute of Information and Communications Technology,
3?5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619?0289 JAPAN
??Graduate School of Engineering, Nagoya University,
Furo-cho, Chikusa-ku, Nagoya, 464?8603, JAPAN
Abstract
The Japanese language has various types
of compound functional expressions,
which are very important for recogniz-
ing the syntactic structures of Japanese
sentences and for understanding their
semantic contents. In this paper, we
formalize the task of identifying Japanese
compound functional expressions in a
text as a chunking problem. We apply a
machine learning technique to this task,
where we employ that of Support Vector
Machines (SVMs). We show that the pro-
posed method significantly outperforms
existing Japanese text processing tools.
1 Introduction
As in the case of other languages, the Japanese
language has various types of functional words
such as post-positional particles and auxiliary
verbs. In addition to those functional words,
the Japanese language has much more compound
functional expressions which consist of more than
one words including both content words and func-
tional words. Those single functional words as
well as compound functional expressions are very
important for recognizing the syntactic structures
of Japanese sentences and for understanding their
semantic contents. Recognition and understanding
of them are also very important for various kinds
of NLP applications such as dialogue systems, ma-
chine translation, and question answering. How-
ever, recognition and semantic interpretation of
compound functional expressions are especially
difficult because it often happens that one com-
pound expression may have both a literal (in other
words, compositional) content word usage and
a non-literal (in other words, non-compositional)
functional usage.
For example, Table 1 shows two example sen-
tences of a compound expression ?? (ni)???
(tsuite)?, which consists of a post-positional par-
ticle ?? (ni)?, and a conjugated form ????
(tsuite)? of a verb ??? (tsuku)?. In the sentence
(A), the compound expression functions as a case-
marking particle and has a non-compositional
functional meaning ?about?. On the other hand,
in the sentence (B), the expression simply corre-
sponds to a literal concatenation of the usages of
the constituents: the post-positional particle ??
(ni)? and the verb ???? (tsuite)?, and has a
content word meaning ?follow?. Therefore, when
considering machine translation of those Japanese
sentences into English, it is necessary to precisely
judge the usage of the compound expression ??
(ni)??? (tsuite)?, as shown in the English trans-
lation of the two sentences in Table 1.
There exist widely-used Japanese text process-
ing tools, i.e., pairs of a morphological analysis
tool and a subsequent parsing tool, such as JU-
MAN1+ KNP2 and ChaSen3+ CaboCha4. How-
ever, they process those compound expressions
only partially, in that their morphological analy-
sis dictionaries list only limited number of com-
pound expressions. Furthermore, even if certain
expressions are listed in a morphological analysis
1http://www.kc.t.u-tokyo.ac.jp/
nl-resource/juman-e.html
2http://www.kc.t.u-tokyo.ac.jp/
nl-resource/knp-e.html
3http://chasen.naist.jp/hiki/ChaSen/
4http://chasen.org/?taku/software/
cabocha/
25
Table 1: Translation Selection of a Japanese Compound Expression ?? (ni)??? (tsuite)?
? (watashi) ? (ha) ? (kare) ? (ni)??? (tsuite) ??? (hanashita)
(A) (I) (TOP) (he) (about) (talked)
(I talked about him.)
? (watashi) ? (ha) ? (kare) ? (ni) ??? (tsuite) ??? (hashitta)
(B) (I) (TOP) (he) (ACC) (follow) (ran)
(I ran following him.)
Table 2: Classification of Functional Expressions based on Grammatical Function
# of major # of
Grammatical Function Type expressions variants Example
subsequent to predicate 36 67 ????
post-positional / modifying predicate (to-naru-to)
particle subsequent to nominal 45 121 ?????
type / modifying predicate (ni-kakete-ha)
subsequent to predicate, nominal 2 3 ???
/ modifying nominal (to-iu)
auxiliary verb type 42 146 ??? (te-ii)
total 125 337 ?
dictionary, those existing tools often fail in resolv-
ing the ambiguities of their usages, such as those
in Table 1. This is mainly because the frame-
work of those existing tools is not designed so as
to resolve such ambiguities of compound (possi-
bly functional) expressions by carefully consider-
ing the context of those expressions.
Considering such a situation, it is necessary
to develop a tool which properly recognizes and
semantically interprets Japanese compound func-
tional expressions. In this paper, we apply a ma-
chine learning technique to the task of identify-
ing Japanese compound functional expressions in
a text. We formalize this identification task as a
chunking problem. We employ the technique of
Support Vector Machines (SVMs) (Vapnik, 1998)
as the machine learning technique, which has been
successfully applied to various natural language
processing tasks including chunking tasks such
as phrase chunking (Kudo and Matsumoto, 2001)
and named entity chunking (Mayfield et al, 2003).
In the preliminary experimental evaluation, we fo-
cus on 52 expressions that have balanced distribu-
tion of their usages in the newspaper text corpus
and are among the most difficult ones in terms of
their identification in a text. We show that the pro-
posed method significantly outperforms existing
Japanese text processing tools as well as another
tool based on hand-crafted rules. We further show
that, in the proposed SVMs based framework, it is
sufficient to collect and manually annotate about
50 training examples per expression.
2 Japanese Compound Functional
Expressions and their Example
Database
2.1 Japanese Compound Functional
Expressions
There exist several collections which list Japanese
functional expressions and examine their usages.
For example, (Morita and Matsuki, 1989) examine
450 functional expressions and (Group Jamashii,
1998) also lists 965 expressions and their exam-
ple sentences. Compared with those two collec-
tions, Gendaigo Hukugouji Youreishu (National
Language Research Institute, 2001) (henceforth,
denoted as GHY) concentrates on 125 major func-
tional expressions which have non-compositional
usages, as well as their variants5 (337 expressions
in total), and collects example sentences of those
expressions. As a first step of developing a tool for
identifying Japanese compound functional expres-
sions, we start with those 125 major functional ex-
pressions and their variants. In this paper, we take
an approach of regarding each of those variants as
a fixed expression, rather than a semi-fixed expres-
sion or a syntactically-flexible expression (Sag et
al., 2002). Then, we focus on evaluating the ef-
fectiveness of straightforwardly applying a stan-
5For each of those 125 major expressions, the differences
between it and its variants are summarized as below: i) in-
sertion/deletion/alternation of certain particles, ii) alternation
of synonymous words, iii) normal/honorific/conversational
forms, iv) base/adnominal/negative forms.
26
Table 3: Examples of Classifying Functional/Content Usages
Expression Example sentence (English translation) Usage
(1) ???? ????????????? ????
??????
functional
(to-naru-to) (The situation is serious if it is not effec-
tive against this disease.)
(???? (to-naru-to) = if)
(2) ???? ???????????????
???? ????????
content
(to-naru-to) (They think that it will become a require-
ment for him to be the president.)
(????? (to-naru-to)
= that (something) becomes ?)
(3) ????? ???????? ????? ????
??????????
functional
(ni-kakete-ha) (He has a great talent for earning money.) (?????? (ni-kakete-ha)
= for ?)
(4) ????? ???? ????? ???? content
(ni-kakete-ha) (I do not worry about it.)
( (??)??????
((?)-wo-ki-ni-kakete-ha)
= worry about ?)
(5) ??? ??????? ??? ??????
??
functional
(to-iu) (I heard that he is alive.) (???? (to-iu) = that ?)
(6) ??? ????????????? ????? content
(to-iu) (Somebody says ?Please visit us.?.) (???? (to-iu)
= say (that) ?)
(7) ??? ???????????? ??? ? functional
(te-ii) (You may have a break after we finish this
discussion.)
(???? (te-ii) = may ?)
(8) ??? ????????? ??? ? content
(te-ii) (This bag is nice because it is big.) (???? (te-ii)
= nice because ?)
dard chunking technique to the task of identifying
Japanese compound functional expressions.
As in Table 2, according to their grammat-
ical functions, those 337 expressions in total
are roughly classified into post-positional particle
type, and auxiliary verb type. Functional expres-
sions of post-positional particle type are further
classified into three subtypes: i) those subsequent
to a predicate and modifying a predicate, which
mainly function as conjunctive particles and are
used for constructing subordinate clauses, ii) those
subsequent to a nominal, and modifying a predi-
cate, which mainly function as case-marking parti-
cles, iii) those subsequent to a nominal, and modi-
fying a nominal, which mainly function as adnom-
inal particles and are used for constructing adnom-
inal clauses. For each of those types, Table 2 also
shows the number of major expressions as well as
that of their variants listed in GHY, and an exam-
ple expression. Furthermore, Table 3 gives exam-
ple sentences of those example expressions as well
as the description of their usages.
2.2 Issues on Identifying Compound
Functional Expressions in a Text
The task of identifying Japanese compound func-
tional expressions roughly consists of detecting
candidates of compound functional expressions in
a text and of judging the usages of those can-
didate expressions. The class of Japanese com-
pound functional expressions can be regarded as
closed and their number is at most a few thousand.
27
Table 4: Examples of Detecting more than one Candidate Expression
Expression Example sentence (English translation) Usage
(9) ??? ????? ??? ???????? functional
(to-iu) (That?s why a match is not so easy.) (NP1??? (to-iu)NP2
= NP
2
called as NP
1
)
(10) ?????? ??? ?????? ???????? functional
(to-iu-mono-no) (Although he won, the score is bad.)
(???????
(to-iu-mono-no)
= although ?)
Therefore, it is easy to enumerate all the com-
pound functional expressions and their morpheme
sequences. Then, in the process of detecting can-
didates of compound functional expressions in a
text, the text are matched against the morpheme
sequences of the compound functional expressions
considered.
Here, most of the 125 major functional expres-
sions we consider in this paper are compound ex-
pressions which consist of one or more content
words as well as functional words. As we intro-
duced with the examples of Table 1, it is often
the case that they have both a compositional con-
tent word usage as well as a non-compositional
functional usage. For example, in Table 3, the
expression ????? (to-naru-to)? in the sen-
tence (2) has the meaning ? that (something) be-
comes ??, which corresponds to a literal concate-
nation of the usages of the constituents: the post-
positional particle ???, the verb ????, and the
post-positional particle ???, and can be regarded
as a content word usage. On the other hand, in
the case of the sentence (1), the expression ???
?? (to-naru-to)? has a non-compositional func-
tional meaning ?if?. Based on this discussion, we
classify the usages of those expressions into two
classes: functional and content. Here, functional
usages include both non-compositional and com-
positional functional usages, although most of the
functional usages of those 125 major expressions
can be regarded as non-compositional. On the
other hand, content usages include compositional
content word usages only.
More practically, in the process of detecting
candidates of compound functional expressions in
a text, it can happen that more than one can-
didate expression is detected. For example, in
Table 4, both of the candidate compound func-
tional expressions ???? (to-iu)? and ????
??? (to-iu-mono-no)? are detected in the sen-
tence (9). This is because the sequence of the two
morphemes ?? (to)? and ??? (iu)? constituting
the candidate expression ???? (to-iu)? is a sub-
sequence of the four morphemes constituting the
candidate expression ??????? (to-iu-mono-
no)? as below:
Morpheme sequence
? (to) ?? (iu) ?? (mono) ? (no)
Candidate expression??? (to-iu)
? (to) ?? (iu) ?? (mono) ? (no)
Candidate expression?????? (to-iu-mono-no)
? (to) ?? (iu) ?? (mono) ? (no)
This is also the case with the sentence (10).
Here, however, as indicated in Table 4, the sen-
tence (9) is an example of the functional usage of
the compound functional expression ???? (to-
iu)?, where the sequence of the two morphemes ?
? (to)? and ??? (iu)? should be identified and
chunked into a compound functional expression.
On the other hand, the sentence (10) is an ex-
ample of the functional usage of the compound
functional expression ??????? (to-iu-mono-
no)?, where the sequence of the four morphemes ?
? (to)?, ??? (iu)?, ??? (mono)?, and ?? (no)?
should be identified and chunked into a compound
functional expression. Actually, in the result of
our preliminary corpus study, at least in about 20%
of the occurrences of Japanese compound func-
tional expressions, more than one candidate ex-
pression can be detected. This result indicates that
it is necessary to consider more than one candidate
expression in the task of identifying a Japanese
compound functional expression, and also in the
task of classifying the functional/content usage of
a candidate expression. Thus, in this paper, based
on this observation, we formalize the task of iden-
tifying Japanese compound functional expressions
as a chunking problem, rather than a classification
problem.
28
Table 5: Number of Sentences collected from
1995 Mainichi Newspaper Texts (for 337 Expres-
sions)
# of expressions
50 ? # of sentences 187 (55%)
0 < # of sentences < 50 117 (35%)
# of sentences = 0 33 (10%)
2.3 Developing an Example Database
We developed an example database of Japanese
compound functional expressions, which is used
for training/testing a chunker of Japanese com-
pound functional expressions (Tsuchiya et al,
2005). The corpus from which we collect example
sentences is 1995 Mainichi newspaper text corpus
(1,294,794 sentences, 47,355,330 bytes). For each
of the 337 expressions, 50 sentences are collected
and chunk labels are annotated according to the
following procedure.
1. The expression is morphologically analyzed
by ChaSen, and its morpheme sequence6 is
obtained.
2. The corpus is morphologically analyzed by
ChaSen, and 50 sentences which include the
morpheme sequence of the expression are
collected.
3. For each sentence, every occurrence of the
337 expressions is annotated with one of the
usages functional/content by an annotator7.
Table 5 classifies the 337 expressions accord-
ing to the number of sentences collected from the
1995 Mainichi newspaper text corpus. For more
than half of the 337 expressions, more than 50 sen-
tences are collected, although about 10% of the
377 expressions do not appear in the whole cor-
pus. Out of those 187 expressions with more than
50 sentences, 52 are those with balanced distribu-
tion of the functional/content usages in the news-
paper text corpus. Those 52 expressions can be re-
garded as among the most difficult ones in the task
of identifying and classifying functional/content
6For those expressions whose constituent has conjugation
and the conjugated form also has the same usage as the ex-
pression with the original form, the morpheme sequence is
expanded so that the expanded morpheme sequences include
those with conjugated forms.
7For the most frequent 184 expressions, on the average,
the agreement rate between two human annotators is 0.93 and
the Kappa value is 0.73, which means allowing tentative con-
clusions to be drawn (Carletta, 1996; Ng et al, 1999). For
65% of the 184 expressions, the Kappa value is above 0.8,
which means good reliability.
usages. Thus, this paper focuses on those 52 ex-
pressions in the training/testing of chunking com-
pound functional expressions. We extract 2,600
sentences (= 52 expressions ? 50 sentences) from
the whole example database and use them for
training/testing the chunker. The number of the
morphemes for the 2,600 sentences is 92,899. We
ignore the chunk labels for the expressions other
than the 52 expressions, resulting in 2,482/701
chunk labels for the functional/content usages, re-
spectively.
3 Chunking Japanese Compound
Functional Expressions with SVMs
3.1 Support Vector Machines
The principle idea of SVMs is to find a separate
hyperplane that maximizes the margin between
two classes (Vapnik, 1998). If the classes are not
separated by a hyperplane in the original input
space, the samples are transformed in a higher di-
mensional features space.
Giving x is the context (a set of features) of
an input example; xi and yi(i = 1, ..., l, xi ?
Rn, yi?{1,?1}) indicate the context of the train-
ing data and its category, respectively; The deci-
sion function f in SVM framework is defined as:
f(x) = sgn
( l
?
i=1
?iyiK(xi,x) + b
)
(1)
where K is a kernel function, b ? R is a thresh-
old, and ?i are weights. Besides, the weights ?i
satisfy the following constraints:
0 ? ?i ? C (i = 1, ..., l) (2)
?l
i=1 ?iyi = 0 (3)
where C is a misclassification cost. The xi with
non-zero ?i are called support vectors. To train
an SVM is to find the ?i and the b by solving the
optimization problem; maximizing the following
under the constraints of (2) and (3):
L(?) =
l
?
i=1
?i?
1
2
l
?
i,j=1
?i?jyiyjK(x
i
,x
j
) (4)
The kernel function K is used to transform the
samples in a higher dimensional features space.
Among many kinds of kernel functions available,
we focus on the d-th polynomial kernel:
K(x,y) = (x ? y + 1)d (5)
29
Through experimental evaluation on chunking
Japanese compound functional expressions, we
compared polynomial kernels with d = 1, 2, and
3. Kernels with d = 2 and 3 perform best, while
the kernel with d = 3 requires much more compu-
tational cost than that with d = 2. Thus, through-
out the paper, we show results with the quadratic
kernel (d = 2).
3.2 Chunking with SVMs
This section describes details of formalizing the
chunking task using SVMs. In this paper, we use
an SVMs-based chunking tool YamCha8 (Kudo
and Matsumoto, 2001). In the SVMs-based
chunking framework, SVMs are used as classi-
fiers for assigning labels for representing chunks
to each token. In our task of chunking Japanese
compound functional expressions, each sentence
is represented as a sequence of morphemes, where
a morpheme is regarded as a token.
3.2.1 Chunk Representation
For representing proper chunks, we employ
IOB2 representation, one of those which have
been studied well in various chunking tasks of nat-
ural language processing (Tjong Kim Sang, 1999;
Kudo and Matsumoto, 2001). This method uses
the following set of three labels for representing
proper chunks.
I Current token is a middle or the end of a
chunk consisting of more than one token.
O Current token is outside of any chunk.
B Current token is the beginning of a chunk.
As we described in section 2.2, given a candi-
date expression, we classify the usages of the ex-
pression into two classes: functional and content.
Accordingly, we distinguish the chunks of the two
types: the functional type chunk and the content
type chunk. In total, we have the following five la-
bels for representing those chunks: B-functional,
I-functional, B-content, I-content, and O. Ta-
ble 6 gives examples of those chunk labels rep-
resenting chunks.
Finally, as for exending SVMs to multi-class
classifiers, we experimentally compare the pair-
wise method and the one vs. rest method, where
the pairwise method slightly outperformed the one
vs. rest method. Throughout the paper, we show
results with the pairwise method.
8http://chasen.org/?taku/software/
yamcha/
3.2.2 Features
For the feature sets for training/testing of
SVMs, we use the information available in the sur-
rounding context, such as the morphemes, their
parts-of-speech tags, as well as the chunk labels.
More precisely, suppose that we identify the chunk
label ci for the i-th morpheme:
?? Parsing Direction ??
Morpheme m
i?2
m
i?1
m
i
m
i+1
m
i+2
Feature set F
i?2
F
i?1
F
i
F
i+1
F
i+2
at a position
Chunk label c
i?2
c
i?1
c
i
Here, mi is the morpheme appearing at i-th po-
sition, Fi is the feature set at i-th position, and ci
is the chunk label for i-th morpheme. Roughly
speaking, when identifying the chunk label ci for
the i-th morpheme, we use the feature sets Fi?2,
Fi?1, Fi, Fi+1, Fi+2 at the positions i ? 2, i ? 1,
i, i + 1, i + 2, as well as the preceding two chunk
labels ci?2 and ci?1.
The detailed definition of the feature set Fi at i-
th position is given below. The feature set Fi is de-
fined as a tuple of the morpheme feature MF (mi)
of the i-th morpheme mi, the chunk candidate fea-
ture CF (i) at i-th position, and the chunk context
feature OF (i) at i-th position.
Fi = ? MF (mi), CF (i), OF (i) ?
The morpheme feature MF (mi) consists of the
lexical form, part-of-speech, conjugation type and
form, base form, and pronunciation of mi.
The chunk candidate feature CF (i) and the
chunk context feature OF (i) are defined consid-
ering the candidate compound functional expres-
sion, which is a sequence of morphemes includ-
ing the morpheme mi at the current position i. As
we described in section 2, the class of Japanese
compound functional expressions can be regarded
as closed and their number is at most a few thou-
sand. Therefore, it is easy to enumerate all the
compound functional expressions and their mor-
pheme sequences. Chunk labels other than O
should be assigned to a morpheme only when it
constitutes at least one of those enumerated com-
pound functional expressions. Suppose that a se-
quence of morphemes mj . . . mi . . . mk including
mi at the current position i constitutes a candidate
functional expression E as below:
m
j?2
m
j?1
m
j
. . . m
i
. . . m
k
m
k+1
m
k+2
candidate E of
a compound
functional expression
where the morphemes mj?2, mj?1, mk+1, and
mk+2 are at immediate left/right contexts of E.
Then, the chunk candidate feature CF (i) at i-th
position is defined as a tuple of the number of mor-
phemes constituting E and the position of mi in
E. The chunk context feature OF (i) at i-th posi-
tion is defined as a tuple of the morpheme features
30
Table 6: Examples of Chunk Representation and Chunk Candidate/Context Features
(a) Sentence (7) of Table 3
(English Chunk candidate Chunk context
Morpheme translation) Chunk label feature feature
?? (kono) (this) O ? ?
?? (giron) (discussion) O ? ?
? (ga) (NOM) O ? ?
???(owatt) (finish) O ? ?
?? (tara) (after) O ? ?
?? (kyuukei) (break) O ? ?
? (shi) (have) O ? ?
? (te) (may) B-functional ?2, 1? ? MF (?? (kyuukei)), ?, MF (? (shi)), ?,
?? (ii) I-functional ?2, 2? MF (?(period)), ?, ?, ? ?
?(period) (period) O ? ?
(b) Sentence (8) of Table 3
(English Chunk candidate Chunk context
Morpheme translation) Chunk label feature feature
?? (kono) (this) O ? ?
??? (bag) (discussion) O ? ?
? (ha) (TOP) O ? ?
??? (ookiku) (big) O ? ?
? (te) (because) B-content ?2, 1? ? MF (? (ha)), ?, MF (??? (ookiku)), ?,
?? (ii) (nice) I-content ?2, 2? MF (?(period)), ?, ?, ? ?
?(period) (period) O ? ?
as well as the chunk candidate features at immedi-
ate left/right contexts of E.
CF (i) = ? length of E, position of m
i
in E ?
OF (i) = ? MF (m
j?2
), CF (j ? 2),
MF (m
j?1
), CF (j ? 1),
MF (m
k+1
), CF (k + 1),
MF (m
k+2
), CF (k + 2) ?
Table 6 gives examples of chunk candidate fea-
tures and chunk context features
It can happen that the morpheme at the cur-
rent position i constitutes more than one candidate
compound functional expression. For example,
in the example below, the morpheme sequences
mi?1mimi+1, mi?1mi, and mimi+1mi+2 consti-
tute candidate expressions E
1
, E
2
, and E
3
, respec-
tively.
Morpheme sequence m
i?1
m
i
m
i+1
m
i+2
Candidate E
1
m
i?1
m
i
m
i+1
Candidate E
2
m
i?1
m
i
Candidate E
3
m
i
m
i+1
m
i+2
In such cases, we prefer the one starting with the
leftmost morpheme. If more than one candidate
expression starts with the leftmost morpheme, we
prefer the longest one. In the example above, we
prefer the candidate E
1
and construct the chunk
candidate features and chunk context features con-
sidering E
1
only.
4 Experimental Evaluation
The detail of the data set we use in the experimen-
tal evaluation was presented in section 2.3. As we
show in Table 7, performance of our SVMs-based
chunkers as well as several baselines including ex-
isting Japanese text processing tools is evaluated
in terms of precision/recall/F?=1 of identifying
functional chunks. Performance is evaluated also
in terms of accuracy of classifying detected can-
didate expressions into functional/content chunks.
Among those baselines, ?majority ( = functional)?
always assigns functional usage to the detected
candidate expressions. ?Hand-crafted rules? are
manually created 145 rules each of which has con-
ditions on morphemes constituting a compound
functional expression as well as those at immedi-
ate left/right contexts. Performance of our SVMs-
based chunkers is measured through 10-fold cross
validation.
As shown in Table 7, our SVMs-based chunkers
significantly outperform those baselines both in
F?=1 and classification accuracy9. We also evalu-
ate the effectiveness of each feature set, i.e., the
morpheme feature, the chunk candidate feature,
and the chunk context feature. The results in the
table show that the chunker with the chunk candi-
date feature performs almost best even without the
chunk context feature10.
9Recall of existing Japanese text processing tools is low,
because those tools can process only 50?60% of the whole
52 compound functional expressions, and for the remaining
40?50% expressions, they fail in identifying all of the occur-
rences of functional usages.
10It is also worthwhile to note that training the SVMs-
based chunker with the full set of features requires computa-
tional cost three times as much as training without the chunk
31
Table 7: Evaluation Results (%)
Identifying Acc. of classifying
functional chunks functional/content
Prec. Rec. F?=1 chunks
majority ( = functional) 78.0 100 87.6 78.0
Baselines Juman/KNP 89.2 49.3 63.5 55.8
ChaSen/CaboCha 89.0 45.6 60.3 53.2
hand-crafted rules 90.7 81.6 85.9 79.1
SVM morpheme 88.0 91.0 89.4 86.5
(feature morpheme + chunk-candidate 91.0 93.2 92.1 89.0
set) morpheme + chunk-candidate/context 91.1 93.6 92.3 89.2
Figure 1: Change of F?=1 with Different Number
of Training Instances
For the SVMs-based chunker with the chunk
candidate feature with/without the chunk context
feature, Figure 1 plots the change of F?=1 when
training with different number of labeled chunks
as training instances. With this result, the increase
in F?=1 seems to stop with the maximum num-
ber of training instances, which supports the claim
that it is sufficient to collect and manually annotate
about 50 training examples per expression.
5 Concluding Remarks
The Japanese language has various types of com-
pound functional expressions, which are very im-
portant for recognizing the syntactic structures of
Japanese sentences and for understanding their se-
mantic contents. In this paper, we formalized
the task of identifying Japanese compound func-
tional expressions in a text as a chunking prob-
lem. We applied a machine learning technique
to this task, where we employed that of Sup-
port Vector Machines (SVMs). We showed that
the proposed method significantly outperforms ex-
isting Japanese text processing tools. The pro-
context feature.
posed framework has advantages over an approach
based on manually created rules such as the one in
(Shudo et al, 2004), in that it requires human cost
to manually create and maintain those rules. On
the other hand, in our framework based on the ma-
chine learning technique, it is sufficient to collect
and manually annotate about 50 training examples
per expression.
References
J. Carletta. 1996. Assessing agreement on classification
tasks: the Kappa statistic. Computational Linguistics,
22(2):249?254.
Group Jamashii, editor. 1998. Nihongo Bunkei Jiten.
Kuroshio Publisher. (in Japanese).
T. Kudo and Y. Matsumoto. 2001. Chunking with support
vector machines. In Proc. 2nd NAACL, pages 192?199.
J. Mayfield, P. McNamee, and C. Piatko. 2003. Named entity
recognition using hundreds of thousands of features. In
Proc. 7th CoNLL, pages 184?187.
Y. Morita and M. Matsuki. 1989. Nihongo Hyougen Bunkei,
volume 5 of NAFL Sensho. ALC. (in Japanese).
National Language Research Institute. 2001. Gendaigo
Hukugouji Youreishu. (in Japanese).
H. T. Ng, C. Y. Lim, and S. K. Foo. 1999. A case study on
inter-annotator agreement for word sense disambiguation.
In Proc. ACL SIGLEXWorkshop on Standardizing Lexical
Resources, pages 9?13.
I. Sag, T. Baldwin, F. Bond, A. Copestake, and D. Flickinger.
2002. Multiword expressions: A pain in the neck for NLP.
In Proc. 3rd CICLING, pages 1?15.
K. Shudo, T. Tanabe, M. Takahashi, and K. Yoshimura. 2004.
MWEs as non-propositional content indicators. In Proc.
2nd ACL Workshop on Multiword Expressions: Integrat-
ing Processing, pages 32?39.
E. Tjong Kim Sang. 1999. Representing text chunks. In
Proc. 9th EACL, pages 173?179.
M. Tsuchiya, T. Utsuro, S. Matsuyoshi, S. Sato, and S. Nak-
agawa. 2005. A corpus for classifying usages of Japanese
compound functional expressions. In Proc. PACLING,
pages 345?350.
V. N. Vapnik. 1998. Statistical Learning Theory. Wiley-
Interscience.
32
Proceedings of the Workshop on A Broader Perspective on Multiword Expressions, pages 65?72,
Prague, June 2007. c?2007 Association for Computational Linguistics
Learning Dependency Relations of
Japanese Compound Functional Expressions
Takehito Utsuro? and Takao Shime? and Masatoshi Tsuchiya??
Suguru Matsuyoshi?? and Satoshi Sato??
?Graduate School of Systems and Information Engineering, University of Tsukuba,
1-1-1, Tennodai, Tsukuba, 305-8573, JAPAN
?NEC Corporation
??Computer Center, Toyohashi University of Technology,
Tenpaku-cho, Toyohashi, 441?8580, JAPAN
??Graduate School of Engineering, Nagoya University,
Furo-cho, Chikusa-ku, Nagoya, 464?8603, JAPAN
Abstract
This paper proposes an approach of process-
ing Japanese compound functional expressions
by identifying them and analyzing their depen-
dency relations through a machine learning tech-
nique. First, we formalize the task of identify-
ing Japanese compound functional expressions
in a text as a machine learning based chunking
problem. Next, against the results of identify-
ing compound functional expressions, we apply
the method of dependency analysis based on the
cascaded chunking model. The results of ex-
perimental evaluation show that, the dependency
analysis model achieves improvements when ap-
plied after identifying compound functional ex-
pressions, compared with the case where it is ap-
plied without identifying compound functional
expressions.
1 Introduction
In addition to single functional words, the Japanese
language has many more compound functional ex-
pressions which consist of more than one word in-
cluding both content words and functional words.
They are very important for recognizing syntactic
structures of Japanese sentences and for understand-
ing their semantic content. Recognition and under-
standing of them are also very important for vari-
ous kinds of NLP applications such as dialogue sys-
tems, machine translation, and question answering.
However, recognition and semantic interpretation of
compound functional expressions are especially dif-
ficult because it often happens that one compound
expression may have both a literal (i.e. compo-
sitional) content word usage and a non-literal (i.e.
non-compositional) functional usage.
For example, Table 1 shows two example sen-
tences of a compound expression ?? (ni) ???
(tsuite)?, which consists of a post-positional particle
?? (ni)?, and a conjugated form ???? (tsuite)? of
a verb ??? (tsuku)?. In the sentence (A), the com-
pound expression functions as a case-marking parti-
cle and has a non-compositional functional meaning
?about?. On the other hand, in the sentence (B), the
expression simply corresponds to a literal concate-
nation of the usages of the constituents: the post-
positional particle ?? (ni)? and the verb ????
(tsuite)?, and has a content word meaning ?follow?.
Therefore, when considering machine translation of
these Japanese sentences into English, it is neces-
sary to judge precisely the usage of the compound
expression ?? (ni)??? (tsuite)?, as shown in the
English translation of the two sentences in Table 1.
There exist widely-used Japanese text processing
tools, i.e. combinations of a morphological analy-
sis tool and a subsequent parsing tool, such as JU-
MAN1+ KNP2 and ChaSen3+ CaboCha4. However,
they process those compound expressions only par-
tially, in that their morphological analysis dictionar-
ies list only a limited number of compound expres-
sions. Furthermore, even if certain expressions are
listed in a morphological analysis dictionary, those
existing tools often fail in resolving the ambigui-
1http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/juman-e.html
2http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/knp-e.html
3http://chasen.naist.jp/hiki/ChaSen/
4http://chasen.org/?taku/software/
cabocha/
65
? (watashi) ? (ha) ? (kare) ? (ni)??? (tsuite) ??? (hanashita)
(A) (I) (TOP) (he) (about) (talked)
(I talked about him.)
? (watashi) ? (ha) ? (kare) ? (ni) ??? (tsuite) ??? (hashitta)
(B) (I) (TOP) (he) (ACC) (follow) (ran)
(I ran following him.)
Table 1: Translation Selection of a Japanese Compound Expression ?? (ni)??? (tsuite)?
Correct English Translation:
( As a means of solving the problem, USA recommended the activity of OSCE in which Russia participates.)
(1) Correct Dependency Relation by Identifying Compound Functional Expression: ??????
with a Case Marking Particle Usage.
(2)  Incorrect Dependency Relation without Identifying Compound Functional Expression: ??????,
which Literally Consists of a Post-positional Particle ??? (with) and a Conjugation Form ????
of a Verb ???? (do).
??? ???? ? ??? ??? ? ?? ?? ?? ????????? ??? ?????
USA-TOP as a means for solution       Russia-NOM also             participate in                                of  OSCE activity-ACC          recommended
??? ???? ? ??? ??? ? ?? ?? ?? ????????? ??? ?????
USA-TOP with a means for Russia-NOM also             participate in                            of  OSCE activity-ACC       recommended
solution
Figure 1: Example of Improving Dependency Analysis of Compound Functional Expressions by Identifying
them before Dependency Analysis
ties of their usages, such as those in Table 1. This
is mainly because the framework of these existing
tools is not designed so as to resolve such ambigu-
ities of compound (possibly functional) expressions
by carefully considering the context of those expres-
sions.
Actually, as we introduce in the next section, as a
first step towards studying computational processing
of compound functional expressions, we start with
125 major functional expressions which have non-
compositional usages, as well as their variants (337
expressions in total). Out of those 337 expressions,
111 have both a content word usage and a functional
usage. However, the combination of JUMAN+KNP
is capable of distinguishing the two usages only for
43 of the 111 expressions, and the combination of
ChaSen+CaboCha only for 40 of those 111 expres-
sions. Furthermore, the failure in distinguishing the
two usages may cause errors of syntactic analysis.
For example, (1) of Figure 1 gives an example of
identifying a correct modifiee of the second bunsetsu
segment 5 ???????? (as a means for solu-
tion)? including a Japanese compound functional ex-
pression ???? (as)?, by appropriately detecting
the compound functional expression before depen-
dency analysis. On the other hand, (2) of Figure 1
gives an example of incorrectly indicating an erro-
neous modifiee of the third bunsetsu ????, which
actually happens if we do not identify the compound
functional expression ???? (as)? before depen-
dency analysis of this sentence.
Considering such a situation, it is necessary to
develop a tool which properly recognizes and se-
mantically interprets Japanese compound functional
expressions. This paper proposes an approach of
processing Japanese compound functional expres-
sions by identifying them and analyzing their de-
pendency relations through a machine learning tech-
nique. The overall flow of processing compound
functional expressions in a Japanese sentence is il-
5A Japanese bunsetsu segment is a phrasal unit which con-
sits of at least one content word and zero or more functional
words.
66
( As a means of solving the 
problem, USA recommended the 
activity of OSCE in which Russia 
participates.)
???????????
????????????
??????????
?????
??
(solution)
??
(means)
?
(with)
?
(do)
?
(and)
? ? ?
? ? ?
??
(solution)
??
(means)
???
(as)
? ? ?
? ? ?
??
(solution)
??
(means)
???
(as)
? ? ?
? ? ?
morphological 
analysis
by ChaSen
??
(solution)
??
(means)
?
(with)
?
(do)
?
(and)
? ? ?
? ? ?
compound
functional 
expression
Identifying
compound
functional
expression
chunking
bunsetsu
segmentation
&
dependency
analysis
bunsetsu
segment
dependency
relation
Figure 2: Overall Flow of Processing Compound Functional Expressions in a Japanese Sentence
lustrated in Figure 2. First of all, we assume a
sequence of morphemes obtained by a variant of
ChaSen with all the compound functional expres-
sions removed from its outputs, as an input to our
procedure of identifying compound functional ex-
pressions and analyzing their dependency relations.
We formalize the task of identifying Japanese com-
pound functional expressions in a text as a machine
learning based chunking problem (Tsuchiya et al,
2006). We employ the technique of Support Vec-
tor Machines (SVMs) (Vapnik, 1998) as the ma-
chine learning technique, which has been success-
fully applied to various natural language process-
ing tasks including chunking tasks such as phrase
chunking and named entity chunking. Next, against
the results of identifying compound functional ex-
pressions, we apply the method of dependency anal-
ysis based on the cascaded chunking model (Kudo
and Matsumoto, 2002), which is simple and efficient
because it parses a sentence deterministically only
deciding whether the current bunsetsu segment mod-
ifies the one on its immediate right hand side. As
we showed in Figure 1, identifying compound func-
tional expressions before analyzing dependencies in
a sentence does actually help deciding dependency
relations of compound functional expressions.
In the experimental evaluation, we focus on 59
expressions having balanced distribution of their us-
ages in the newspaper text corpus and are among the
most difficult ones in terms of their identification in
a text. We first show that the proposed method of
chunking compound functional expressions signifi-
cantly outperforms existing Japanese text processing
tools. Next, we further show that the dependency
analysis model of (Kudo and Matsumoto, 2002) ap-
plied to the results of identifying compound func-
tional expressions significantly outperforms the one
applied to the results without identifying compound
functional expressions.
2 Japanese Compound Functional
Expressions
There exist several collections which list Japanese
functional expressions and examine their usages.
For example, (Morita and Matsuki, 1989) exam-
ine 450 functional expressions and (Group Jamashii,
1998) also lists 965 expressions and their example
sentences. Compared with those two collections,
Gendaigo Hukugouji Youreishu (National Language
Research Institute, 2001) (henceforth, denoted as
GHY) concentrates on 125 major functional expres-
sions which have non-compositional usages, as well
as their variants6, and collects example sentences of
those expressions. As we mentioned in the previous
section, as a first step towards developing a tool for
identifying Japanese compound functional expres-
sions, we start with those 125 major functional ex-
pressions and their variants (337 expressions in to-
6For each of those 125 major expressions, the differences
between it and its variants are summarized as below: i) inser-
tion/deletion/alternation of certain particles, ii) alternation of
synonymous words, iii) normal/honorific/conversational forms,
iv) base/adnominal/negative forms.
67
(a) Classification of Compound Functional Expressions based on Grammatical Function
Grammatical Function Type # of major expressions # of variants Example
post-positional conjunctive particle 36 67 ??? (kuse-ni)
particle type case-marking particle 45 121 ??? (to-shite)
adnominal particle 2 3 ??? (to-iu)
auxiliary verb type 42 146 ??? (te-ii)
total 125 337 ?
(b) Examples of Classifying Functional/Content Usages
Expression Example sentence (English translation) Usage
(1) ??? ??????? ??? ???????????????? functional
(kuse-ni) (To my brother, (someone) gave money, while (he/she) did noth-
ing to me but just sent a letter.)
(??? (kuse-ni) = while)
(2) ??? ???? ??? ??????? content
(kuse-ni) (They all were surprised by his habit.) (???? (kuse-ni)
= by one?s habit
(3) ??? ?????????? ??? ??????? functional
(to-shite) (He is known as an expert of the problem.) (???? (to-shite)
= as ?)
(4) ??? ?????????????? ??? ???? content
(to-shite) (Please make it clear whether this is true or not.) (?? ???? (to-shite)
= make ? ?
(5) ??? ??????? ??? ???????? functional
(to-iu) (I heard that he is alive.) (???? (to-iu) = that ?)
(6) ??? ?????????? ??? ????? content
(to-iu) (Somebody says ?Please visit us.?.) (???? (to-iu)
= say (that) ?)
(7) ??? ???????????? ??? ? functional
(te-ii) (You may have a break after we finish this discussion.) (???? (te-ii) = may ?)
(8) ??? ????????? ??? ? content
(te-ii) (This bag is nice because it is big.) (???? (te-ii)
= nice because ?)
Table 2: Classification and Example Usages of Compound Functional Expressions
tal). In this paper, following (Sag et al, 2002), we
regard each variant as a fixed expression, rather than
a semi-fixed expression or a syntactically-flexible
expression 7. Then, we focus on evaluating the
effectiveness of straightforwardly applying a stan-
dard chunking technique to the task of identifying
Japanese compound functional expressions.
As in Table 2 (a), according to their grammat-
ical functions, those 337 expressions in total are
roughly classified into post-positional particle type,
and auxiliary verb type. Functional expressions of
post-positional particle type are further classified
into three subtypes: i) conjunctive particle types,
which are used for constructing subordinate clauses,
ii) case-marking particle types, iii) adnominal parti-
cle types, which are used for constructing adnominal
7Compound functional expressions of auxiliary verb types
can be regarded as syntactically-flexible expressions.
clauses. Furthermore, for examples of compound
functional expressions listed in Table 2 (a), Table 2
(b) gives their example sentences as well as the de-
scription of their usages.
3 Identifying Compound Functional
Expressions by Chunking with SVMs
This section describes summaries of formalizing the
chunking task using SVMs (Tsuchiya et al, 2006).
In this paper, we use an SVMs-based chunking tool
YamCha8 (Kudo and Matsumoto, 2001). In the
SVMs-based chunking framework, SVMs are used
as classifiers for assigning labels for representing
chunks to each token. In our task of chunking
Japanese compound functional expressions, each
8http://chasen.org/?taku/software/
yamcha/
68
sentence is represented as a sequence of morphemes,
where a morpheme is regarded as a token.
3.1 Chunk Representation
For representing proper chunks, we employ IOB2
representation, which has been studied well in var-
ious chunking tasks of natural language processing.
This method uses the following set of three labels
for representing proper chunks.
I Current token is a middle or the end of a
chunk consisting of more than one token.
O Current token is outside of any chunk.
B Current token is the beginning of a chunk.
Given a candidate expression, we classify the us-
ages of the expression into two classes: functional
and content. Accordingly, we distinguish the chunks
of the two types: the functional type chunk and the
content type chunk. In total, we have the follow-
ing five labels for representing those chunks: B-
functional, I-functional, B-content, I-content, and
O. Finally, as for extending SVMs to multi-class
classifiers, we experimentally compare the pairwise
method and the one vs. rest method, where the pair-
wise method slightly outperformed the one vs. rest
method. Throughout the paper, we show results with
the pairwise method.
3.2 Features
For the feature sets for training/testing of SVMs, we
use the information available in the surrounding con-
text, such as the morphemes, their parts-of-speech
tags, as well as the chunk labels. More precisely,
suppose that we identify the chunk label c
i
for the
i-th morpheme:
?? Parsing Direction ??
Morpheme m
i?2
m
i?1
m
i
m
i+1
m
i+2
Feature set F
i?2
F
i?1
F
i
F
i+1
F
i+2
at a position
Chunk label c
i?2
c
i?1
c
i
Here, m
i
is the morpheme appearing at i-th posi-
tion, F
i
is the feature set at i-th position, and c
i
is
the chunk label for i-th morpheme. Roughly speak-
ing, when identifying the chunk label c
i
for the i-th
morpheme, we use the feature sets F
i?2
, F
i?1
, F
i
,
F
i+1
, F
i+2
at the positions i ? 2, i ? 1, i, i + 1,
i+2, as well as the preceding two chunk labels c
i?2
and c
i?1
. The detailed definition of the feature set
F
i
at i-th position is given in (Tsuchiya et al, 2006),
which mainly consists of morphemes as well as in-
formation on the candidate compound functional ex-
pression at i-th position.
4 Learning Dependency Relations of
Japanese Compound Functional
Expressions
4.1 Japanese Dependency Analysis using
Cascaded Chunking
4.1.1 Cascaded Chunking Model
First of all, we define a Japanese sen-
tence as a sequence of bunsetsu segments
B = ?b
1
, b
2
, . . . , b
m
? and its syntactic struc-
ture as a sequence of dependency patterns
D = ?Dep(1), Dep(2), . . . , Dep(m ? 1)?, where
Dep(i) = j means that the bunsetsu segment b
i
depends on (modifies) bunsetsu segment b
j
. In
this framework, we assume that the dependency
sequence D satisfies the following two constraints:
1. Japanese is a head-final language. Thus, except
for the rightmost one, each bunsetsu segment
modifies exactly one bunsetsu segment among
those appearing to its right.
2. Dependencies do not cross one another.
Unlike probabilistic dependency analysis models
of Japanese, the cascaded chunking model of Kudo
and Matsumoto (2002) does not require the proba-
bilities of dependencies and parses a sentence de-
terministically. Since Japanese is a head-final lan-
guage, and the chunking can be regarded as the cre-
ation of a dependency between two bunsetsu seg-
ments, this model simplifies the process of Japanese
dependency analysis as follows: 9
1. Put an O tag on all bunsetsu segments. The O
tag indicates that the dependency relation of the
current segment is undecided.
2. For each bunsetsu segment with an O tag, de-
cide whether it modifies the bunsetsu segment
on its immediate right hand side. If so, the O
tag is replaced with a D tag.
3. Delete all bunsetsu segments with a D tag that
immediately follows a bunsetsu segment with
an O tag.
9The O and D tags used in this section have no relation to
those chunk reppresentation tags introduced in section 3.1.
69
Initialization
?? ??? ??? ??? ?????
( He was moved by her warm heart. )
He her warm heart be moved
Input:
Tag:
?? ??? ??? ??? ?????
O O O O O
Input:
Tag:
?? ??? ??? ??? ?????
O O D D O
Deleted
Input:
Tag:
?? ??? ??? ?????
O D D O
Deleted
Input:
Tag:
?? ??? ?????
O D O
Input:
Tag:
?? ?????
O
Deleted
Input:
Tag:
?????
O
Finish
D
Deleted
Figure 3: Example of the Parsing Process with Cas-
caded Chunking Model
4. Terminate the algorithm if a single bunsetsu
segment remains, otherwise return to the step
2 and repeat.
Figure 3 shows an example of the parsing process
with the cascaded chunking model.
4.1.2 Features
As a Japanese dependency analyzer based on the
cascaded chunking model, we use the publicly avail-
able version of CaboCha (Kudo and Matsumoto,
2002), which is trained with the manually parsed
sentences of Kyoto text corpus (Kurohashi and Na-
gao, 1998), that are 38,400 sentences selected from
the 1995 Mainichi newspaper text.
The standard feature set used by CaboCha con-
sists of static features and dynamic features. Static
features are those solely defined once the pair
of modifier/modifiee bunsetsu segments is speci-
fied. For the pair of modifier/modifiee bunsetsu
segments, the following are used as static fea-
tures: head words and their parts-of-speech tags,
inflection-types/forms, functional words and their
parts-of-speech tags, inflection-types/forms, inflec-
tion forms of the words that appear at the end
of bunsetsu segments. As for features between
modifier/modifiee bunsetsu segments, the distance
of modifier/modifiee bunsetsu segments, existence
of case-particles, brackets, quotation-marks, and
punctuation-marks are used as static features. On the
other hand, dynamic features are created during the
parsing process, so that, when a certain dependency
relation is determined, it can have some influence
on other dependency relations. Dynamic features in-
clude bunsetsu segments modifying the current can-
didate modifiee (see Kudo and Matsumoto (2002)
for the details).
4.2 Coping with Compound Functional
Expressions
As we show in Figure 2, a compound functional ex-
pression is identified as a sequence of several mor-
phemes and then chunked into one morpheme. The
result of this identification process is then trans-
formed into the sequence of bunsetsu segments. Fi-
nally, to this modified sequence of bunsetsu seg-
ments, the method of dependency analysis based on
the cascaded chunking model is applied.
Here, when chunking a sequence of several mor-
phemes constituting a compound functional expres-
sion, the following two cases may exist:
(A) As in the case of the example (A) in Table 1, the
two morphemes constituting a compound func-
tional expression ?? (ni)??? (tsuite)? over-
laps the boundary of two bunsetsu segments.
In such a case, when chunking the two mor-
phemes into one morpheme corresponding to
a compound functional expression, those two
bunsetsu segments are concatenated into one
bunsetsu segment.
? ?
kare ni
(he)
???
tsuite
=?
? ????
kare ni-tsuite
(he) (about)
(B) As we show below, a compound functional ex-
pression ??? (koto)? (ga)?? (aru)? over-
laps the boundary of two bunsetsu segments,
though the two bunsetsu segments concatenat-
ing into one bunsetsu segment does include no
content words. In such a case, its immedi-
ate left bunsetsu segment (???(itt)? (ta)? in
the example below), which corresponds to the
content word part of ??? (koto)? (ga)??
(aru)?, has to be concatenated into the bunsetsu
segment ??? (koto)? (ga)?? (aru)?.
70
?? ?
itt ta
(went)
?? ?
koto ga
??
aru
=?
?? ? ?????
itt ta koto-ga-aru
(have been ?)
Next, to the compound functional expression, we
assign one of the four grammatical function types
listed in Table 2 as its POS tag. For example,
the compound functional expression ?? (ni)???
(tsuite)? in (A) above is assigned the grammatical
function type ?case-marking particle type?, while ?
?? (koto) ? (ga) ?? (aru)? in (B) is assigned
?auxiliary verb type?.
These modifications cause differences in the final
feature representations. For example, let us compare
the feature representations of the modifier bunsetsu
segments in (1) and (2) of Figure 1. In (1), the mod-
ifier bunsetsu segment is ????????? which
has the compound functional expression ?????
in its functional word part. On the other hand, in
(2), the modifier bunsetsu segment is ????, which
corresponds to the literal verb usage of a part of the
compound functional expression ?????. In the
final feature representations below, this causes the
following differences in head words and functional
words / POS of the modifier bunsetsu segments:
(1) of Figure 1 (2) of Figure 1
head word ?? (means) ?? (do)
functional word ??? (as) ? (and)
POS subsequent to nominal conjunctive
/ modifying predicate particle
5 Experimental Evaluation
5.1 Training/Test Data Sets
For the training of chunking compound functional
expressions, we collected 2,429 example sentences
from the 1995 Mainichi newspaper text corpus. For
each of the 59 compound functional expressions for
evaluation mentioned in section 1, at least 50 ex-
amples are included in this training set. For the
testing of chunking compound functional expres-
sions, as well as training/testing of learning depen-
dencies of compound functional expressions, we
used manually-parsed sentences of Kyoto text cor-
pus (Kurohashi and Nagao, 1998), that are 38,400
sentences selected from the 1995 Mainichi newspa-
per text (the 2,429 sentences above are selected so
that they are exclusive of the 37,400 sentences of
Kyoto text corpus.). To those data sets, we manually
annotate usage labels of the 59 compound functional
expressions (details in Table 3).
Usages # of
functional content total sentences
for chunker
training 1918 1165 3083 2429
Kyoto text corpus 5744 1959 7703 38400
Table 3: Statistics of Data Sets
Identifying
functional chunks
Acc. of
classifying
functional /
content
Prec. Rec. F
?=1
chunks
majority ( = functional) 74.6 100 85.5 74.6
Juman/KNP 85.8 40.5 55.0 58.4
ChaSen/CaboCha 85.2 26.7 40.6 51.1
SVM 91.4 94.6 92.9 89.3
Table 4: Evaluation Results of Chunking (%)
5.2 Chunking
As we show in Table 4, performance of our SVMs-
based chunkers as well as several baselines includ-
ing existing Japanese text processing tools is evalu-
ated in terms of precision/recall/F
?=1
of identifying
all the 5,744 functional chunks included in the test
data (Kyoto text corpus in Table 3). Performance is
evaluated also in terms of accuracy of classifying de-
tected candidate expressions into functional/content
chunks. Among those baselines, ?majority ( = func-
tional)? always assigns functional usage to the de-
tected candidate expressions. Performance of our
SVMs-based chunkers is measured through 10-fold
cross validation. Our SVMs-based chunker signif-
icantly outperforms those baselines both in F
?=1
and classification accuracy. As we mentioned in
section 1, existing Japanese text processing tools
process compound functional expressions only par-
tially, which causes damage in recall in Table 4.
5.3 Analyzing Dependency Relations
We evaluate the accuracies of judging dependency
relations of compound functional expressions by the
variant of CaboCha trained with Kyoto text cor-
pus annotated with usage labels of compound func-
tional expressions. This performance is measured
through 10-fold cross validation with the modified
version of the Kyoto text corpus. In the evaluation
phase, according to the flow of Figure 2, first we ap-
ply the chunker of compound functional expressions
trained with all the 2,429 sentences in Table 3 and
obtain the results of chunked compound functional
expressions with about 90% correct rate. Then, bun-
setsu segmentation and dependency analysis are per-
71
modifier modifiee
baselines CaboCha (w/o FE) 72.5 88.0
CaboCha (public) 73.9 87.6
chunker + CaboCha (proposed) 74.0 88.0
reference + CaboCha (proposed) 74.4 88.1
Table 5: Accuracies of Identifying Modi-
fier(s)/Modifiee (%)
formed by our variant of CaboCha, where accu-
racies of identifying modifier(s)/modifiee of com-
pound functional expressions are measured as in Ta-
ble 5 (?chunker + CaboCha (proposed)? denotes that
inputs to CaboCha (proposed) are with 90% correct
rate, while ?reference + CaboCha (proposed)? de-
notes that they are with 100% correct rate). Here,
?CaboCha (w/o FE)? denotes a baseline variant of
CaboCha, with all the compound functional expres-
sions removed from its inputs (which are outputs
from ChaSen), while ?CaoboCha (public)? denotes
the publicly available version of CaboCha, which
have some portion of the compound functional ex-
pressions included in its inputs.
For the modifier accuracy, the difference of
?chunker + CaboCha (proposed)? and ?CaboCha
(w/o FE)? is statistically significant at a level of
0.05. Identifying compound functional expressions
typically contributes to improvements when the lit-
eral constituents of a compound functional expres-
sion include a verb. In such a case, for bunsetsu
segments which usually modifies a verb, an incor-
rect modifee candidate is removed, which results in
improvements in the modifier accuracy. The dif-
ference between ?CaoboCha (public)? and ?chunker
+ CaboCha (proposed)? is slight because the pub-
licly available version of CaboCha seems to include
compound functional expressions which are dam-
aged in identifying their modifiers with ?CaboCha
(w/o FE)?. For the modifiee accuracy, the difference
of ?chunker + CaboCha (proposed)? and ?CaboCha
(w/o FE)? is zero. Here, more than 100 instances of
improvements like the one in Figure 1 are observed,
while almost the same number of additional fail-
ures are also observed mainly because of the sparse-
ness problem. Furthermore, in the case of the modi-
fiee accuracy, it is somehow difficult to expect im-
provement because identifying modifiees of func-
tional/content bunsetsu segments mostly depends on
features other than functional/content distinction.
6 Concluding Remarks
We proposed an approach of processing Japanese
compound functional expressions by identifying
them and analyzing their dependency relations
through a machine learning technique. This ap-
proach is novel in that it has never been applied
to any language so far. Experimental evaluation
showed that the dependency analysis model applied
to the results of identifying compound functional ex-
pressions significantly outperforms the one applied
to the results without identifying compound func-
tional expressions. The proposed framework has ad-
vantages over an approach based on manually cre-
ated rules such as the one in (Shudo et al, 2004), in
that it requires human cost to create manually and
maintain those rules. Related works include Nivre
and Nilsson (2004), which reports improvement of
Swedish parsing when multi word units are manu-
ally annotated.
References
Group Jamashii, editor. 1998. Nihongo Bunkei Jiten. Kuroshio
Publisher. (in Japanese).
T. Kudo and Y. Matsumoto. 2001. Chunking with support vec-
tor machines. In Proc. 2nd NAACL, pages 192?199.
T. Kudo and Y. Matsumoto. 2002. Japanese dependency ana-
lyisis using cascaded chunking. In Proc. 6th CoNLL, pages
63?69.
S. Kurohashi and M. Nagao. 1998. Building a Japanese parsed
corpus while improving the parsing system. In Proc. 1st
LREC, pages 719?724.
Y. Morita and M. Matsuki. 1989. Nihongo Hyougen Bunkei,
volume 5 of NAFL Sensho. ALC. (in Japanese).
National Language Research Institute. 2001. Gendaigo Huku-
gouji Youreishu. (in Japanese).
J. Nivre and J. Nilsson. 2004. Multiword units in syntactic
parsing. In Proc. LRECWorkshop, Methodologies and Eval-
uation of Multiword Units in Real-World Applications, pages
39?46.
I. Sag, T. Baldwin, F. Bond, A. Copestake, and D. Flickinger.
2002. Multiword expressions: A pain in the neck for NLP.
In Proc. 3rd CICLING, pages 1?15.
K. Shudo, T. Tanabe, M. Takahashi, and K. Yoshimura. 2004.
MWEs as non-propositional content indicators. In Proc. 2nd
ACL Workshop on Multiword Expressions: Integrating Pro-
cessing, pages 32?39.
M. Tsuchiya, T. Shime, T. Takagi, T. Utsuro, K. Uchimoto,
S. Matsuyoshi, S. Sato, and S. Nakagawa. 2006. Chunk-
ing Japanese compound functional expressions by machine
learning. In Proc. Workshop on Multi-Word-Expressions in
a Multilingual Context, pages 25?32.
V. N. Vapnik. 1998. Statistical Learning Theory. Wiley-
Interscience.
72
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 151?158,
Prague, June 2007. c?2007 Association for Computational Linguistics
A Compositional Approach toward Dynamic Phrasal Thesaurus
Atsushi Fujita Shuhei Kato Naoki Kato Satoshi Sato
Graduate School of Engineering, Nagoya University
{fujita,ssato}@nuee.nagoya-u.ac.jp
{shuhei,naoki}@sslab.nuee.nagoya-u.ac.jp
Abstract
To enhance the technology for computing
semantic equivalence, we introduce the no-
tion of phrasal thesaurus which is a natural
extension of conventional word-based the-
saurus. Among a variety of phrases that
conveys the same meaning, i.e., paraphrases,
we focus on syntactic variants that are com-
positionally explainable using a small num-
ber of atomic knowledge, and develop a sys-
tem which dynamically generates such vari-
ants. This paper describes the proposed sys-
tem and three sorts of knowledge developed
for dynamic phrasal thesaurus in Japanese:
(i) transformation pattern, (ii) generation
function, and (iii) lexical function.
1 Introduction
Linguistic expressions that convey the same mean-
ing are called paraphrases. Handling paraphrases
is one of the key issues in a broad range of nat-
ural language processing tasks, including machine
translation, information retrieval, information ex-
traction, question answering, summarization, text
mining, and natural language generation.
Conventional approaches to computing semantic
equivalence between two expressions are five-fold.
The first approximates it based on the similarities
between their constituent words. If two words be-
long to closer nodes in a thesaurus or semantic net-
work, they are considered more likely to be similar.
The second uses the family of tree kernels (Collins
and Duffy, 2001; Takahashi, 2005). The degree of
equivalence of two trees (sentences) is defined as
the number of common subtrees included in both
trees. The third estimates the equivalence based on
word alignment composed using templates or trans-
lation probabilities derived from a set of parallel
text (Barzilay and Lee, 2003; Brockett and Dolan,
2005). The fourth espouses the distributional hy-
pothesis (Harris, 1968): given two words are likely
to be equivalent if distributions of their surrounding
words are similar (Lin and Pantel, 2001; Weeds et
al., 2005). The final regards two expressions equiva-
lent if they can be associated by using a set of lexico-
syntactic paraphrase patterns (Mel?c?uk, 1996; Dras,
1999; Yoshikane et al, 1999; Takahashi, 2005).
Despite the results previous work has achieved,
no system that robustly recognizes and generates
paraphrases is established. We are not convinced of
a hypothesis underlying the word-based approaches
because the structure of words also conveys some
meaning. Even tree kernels, which take structures
into account, do not have a mechanism for iden-
tifying typical equivalents: e.g., dative alternation
and passivization, and abilities to generate para-
phrases. Contrary to the theoretical basis, the two
lines of corpus-based approaches have problems in
practice, i.e., data sparseness and computation cost.
The pattern-based approaches seem steadiest. Yet
no complete resource or methodology for handling
a wide variety of paraphrases has been developed.
On the basis of this recognition, we introduce the
notion of phrasal thesaurus to directly compute se-
mantic equivalence of phrases such as follows.
(1) a. be in our favor / be favorable for us
b. its reproducibility / if it is reproducible
c. decrease sharply / show a sharp decrease
d. investigate the cause of a fire /
investigate why there was a fire /
investigate what started a fire /
make an investigation into the cause of a fire
151
Phrasal thesaurus is a natural extension of conven-
tional word-based thesaurus. It is thus promised that
it will bring us the following benefits:
Enhancement of NLP applications: As conven-
tional thesauri, phrasal thesaurus must be
useful to handle paraphrases having different
structures in a wide range of NLP applications.
Reading and writing aids: Showing more appro-
priate alternative phrases must be a power-
ful aid at certain situations such as writing
text. Controlling readability of text by altering
phrases must also be beneficial to readers.
Our aim is to develop resources and mechanisms
for computing semantic equivalence on the working
hypothesis that phrase is the appropriate unit for that
purpose. This paper describes the first version of our
paraphrase generation system and reports on our on-
going work on constructing resources for realizing
phrasal thesaurus.
The following sections describe the range of phe-
nomena we treat (Section 2), the overall architec-
ture of our paraphrase generation system which
functions as phrasal thesaurus (Section 3), the im-
plementation of knowledge bases (Section 4) fol-
lowed by discussion (Section 5), and conclusion
(Section 6).
2 Dynamic phrasal thesaurus
2.1 Issue
Toward realizing phrasal thesaurus, the following
two issues should be discussed.
? What sorts of phrases should be treated
? How to cope with a variety of expressions
Although technologies of shallow parsing have
been dramatically improved in the last decade, it
is still difficult to represent arbitrary expression in
logical form. We therefore think it is reasonable to
define the range relying on lexico-syntactic struc-
ture instead of using particular semantic representa-
tion. According to the work of (Chklovski and Pan-
tel, 2004; Torisawa, 2006), predicate phrase (sim-
ple sentence) is a reasonable unit because it approx-
imately corresponds to the meaning of single event.
Combination of words and a variety of construc-
tion coerce us into handling an enormous number
of expressions than word-based approaches. One
may think taking phrase is like treading a thorny
path because one of the arguments in Section 1 is
about coverage. On this issue, we speculate that
one of the feasible approach to realize a robust sys-
tem is to divide phenomena into compositional and
non-compositional (idiosyncratic) ones1, and sepa-
rately develop resources to handle them as described
in (Fujita and Inui, 2005).
To compute semantic equivalence of idiosyncratic
paraphrases, pairs or groups of paraphrases have to
be statically compiled into a dictionary as word-
based thesaurus. The corpus-based approach is valu-
able for that purpose, although they are not guaran-
teed to collect all idiosyncratic paraphrases. On the
other hand, compositional paraphrases can be cap-
tured by a relatively small number of rules. Thus it
seems tolerable approach to generate them dynam-
ically by applying such rules. Our work is targeted
at compositional paraphrases and the system can be
called dynamic phrasal thesaurus. Hereafter, we
refer to paraphrases that are likely to be explained
compositionally as syntactic variants.
2.2 Target language: Japanese
While the discussion above does not depend on par-
ticular language, our implementation of dynamic
phrasal thesaurus is targeted at Japanese. Sev-
eral methods for paraphrasing Japanese predicate
phrases have been proposed (Kondo et al, 1999;
Kondo et al, 2001; Kaji et al, 2002; Fujita et al,
2005). The range they treat is, however, relatively
narrow because they tend to focus on particular para-
phrase phenomena or to rely on existing resources.
On the other hand, we define the range of phenom-
ena from a top-down viewpoint. As a concrete defi-
nition of predicate phrase in Japanese,
noun phrase + case marker + predicate
is employed which is hereafter referred to ?phrase.?
Noun phrase and predicate in Japanese them-
selves subcategorize various syntactic variants as
shown in Figure 1 and paraphrase phenomena for
above phrase also involve those focused on their in-
teraction. Thus the range of phenomena is not so
narrow, and intriguing ones, such as shown in exam-
ples2 (2) and (3), are included.
1We regard lexical paraphrases (e.g., ?scope? ? ?range?)
and idiomatic paraphrases (e.g., ?get the sack?? ?be dismissed
from employment?) as idiosyncratic.
2In each example, ?s? and ?t? denote an original sentence
and its paraphrase, respectively. SMALLCAPS strings indicate
the syntactic role of their corresponding Japanese expressions.
[N] indicates a nominalizer.
152
(2) Head switching
s. kakunin-o isogu.
checking-ACC to hurry-PRES
We hurry checking it.
t. isoide kakunin-suru.
in a hurry to check-PRES
We check it in a hurry.
(3) Noun phrase ? sub-clause
s. kekka-no saigensei-o kenshou-suru.
result-GEN reproducibility-ACC to validate-PRES
We validate its reproducibility.
t. [ kekka-o saigen-dekiru ]
result-ACC to reproduce-to be able
ka-douka-o kenshou-suru.
[N]-whether-ACC to validate-PRES
We validate whether it is reproducible.
We focus on syntactic variants at least one side of
which is subcategorized into the definition of phrase
above. For the sake of simplicity, we hereafter rep-
resent those expressions using part-of-speech (POS)
patterns. For instance, (2s) is called N : C : V type,
and (3s) is N
1
: no : N
2
: C : V type.
3 Paraphrase generation system
Given a phrase, the proposed system generates its
syntactic variants in the following four steps:
1. Morphological analysis
2. Syntactic transformation
3. Surface generation with lexical choice
4. SLM-based filtering
where no particular domain, occasion, and media is
assumed3. Candidates of syntactic variants are first
over-generated in step 2 and then anomalies among
them are filtered out in steps 3 and 4 using rule-based
lexical choice and statistical language model.
The rest of this section elaborates on each compo-
nent in turn.
3.1 Morphological analysis
Technologies of morphological analysis in Japanese
have matured by introducing machine learning tech-
niques and large-scale annotated corpus, and there
are freely available tools. Since the structure of input
phrase is assumed to be quite simple, employment of
dependency analyzer was put off. We simply use a
morphological analyzer MeCab4.
3This corresponds to the linguistic transformation layer of
KURA (Takahashi et al, 2001).
4http://mecab.sourceforge.net/
noun phrase
8
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
:
formal noun
8
<
:
?koto?
?mono?
?no?
content
8
>
>
>
>
<
>
>
>
>
:
single word
compound
j
N
1
N
2
N + suffixes
modified
8
>
<
>
:
N
1
+ ?no? +N
2
Adj+N
Adjectival verb+N
clause+N
predicate
8
>
>
>
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
>
>
>
:
verb phrase
8
>
>
>
>
>
<
>
>
>
>
>
:
single word
8
>
>
<
>
>
:
original verb
Sino-Japanese verb
lexical compound
light verb
Adv+ ?suru?
compound
8
>
<
>
:
original + original
Sino + original
Sino + Sino
N + Sino
Adj
j
single word
compound
Adjectival verb+ ?da?
Adv+ ?da?
Copula
Figure 1: Classification of syntactic variants of noun
phrase and predicate in Japanese.
Our system has a post-analysis processing. If ei-
ther of Sino-Japanese verbal nouns (e.g., ?kenshou
(validation)? and ?kandou (impression)?) or translit-
eration of verbs in foreign language (e.g., ?doraibu
(to drive)? and ?shifuto (to shift)?) is immediately
followed by ?suru (to do)? or ?dekiru (to be able),?
these adjacent two morphemes are joined into a sin-
gle morpheme to avoid incorrect transformation.
3.2 Syntactic transformation
The second step over-generates syntactic variants
using the following three sorts of knowledge:
(i) Transformation pattern: It gives skeletons of
syntactic variants. Each variant is represented
by POS symbols designating the input con-
stituents and triggers of the generation function
and lexical function below.
(ii) Generation function: It enumerates different
expressions that are constituted with the same
set of words and subcategorized into the re-
quired syntactic category. Some of generation
functions handle base phrases, while the rest
generates functional words. Base phrases the
former generates are smaller than that transfor-
mation patterns treat. Since some functional
words are disjunctive, the latter generates all
candidates with a separator ?/? and leaves the
selection to the following step.
153
Table 1: Grammar in Backus-Naur form, example, and instantiation for each knowledge.
Knowledge type Grammar / Example / Instantiation
(i) Transformation <transformation pattern> ::= <left pattern> ? <right pattern>
pattern <left pattern> ::= (<POS symbol>|<word form>)+
<POS symbol> ::= (N |C|V |Adj|Adv)
<word form> ::= (<hiragana>|<katakana>|<kanji>)+
<right pattern> ::=
(<POS symbol>|<word form>|<function definition>|<lexical function>)+
(a) N : C : V ? adv(V ) : vp(N)
(b) N
1
: no : N
2
: C : V ?N
1
: genCase() : vp(N
2
) : ka-douka : C : V
(a) kakunin : o : isogu ? adv (isogu) : vp(kakunin)
checking ACC to hurry adv(to hurry) vp(checking)
(b) kekka : no : saigensei : o : kenshou-suru
result GEN reproducibility ACC to validate-PRES
? kekka : genCase() : vp(saigensei) : ka-douka : o : kenshou-suru
result case marker vp(reproducibility) [N]-whether ACC to validate-PRES
(ii) Generation <generation function> ::= <function definition> ? ?{?<right pattern>+?}?
function <function definition> ::= <syntactic category>?(?<POS symbol>*?)?
<syntactic category> ::= (np | vp | lvc)
(a) vp(N) ? {v(N) : genVoice() : genTense()}
(b) np(N
1
, N
2
) ? {N
1
, N
2
, N
1
: N
2
, N
1
: no : N
2
, vp(N
1
) : N
2
,wh(N
2
) : vp(N
1
) : ka, . . .}
(a) vp(kakunin) ? { v(kakunin) : genVoice() : genTense() }
vp(verification) v(verification) verbal suffix for voice verbal suffix for tense
(b) np(shukka, gen-in)
np(starting fire, reason)
? { shukka , gen-in , shukka : gen-in , shukka : no : gen-in ,
starting fire reason starting fire reason starting fire GEN reason
vp(shukka) : gen-in , wh(gen-in) : vp(shukka) : ka , . . . }
vp(starting fire) reason wh(reason) vp(starting fire) [N]
(iii) Lexical <lexical function> ::= <relation>?(?<POS symbol>?)?
function <relation> ::= (n | v | adj | adjv | adv | wh)
(a) adv(V )
(b) wh(N)
(a) adv (isogu)
adv(to hurry)
?
? isoide
in a hurry
(given by a verb?adverb dictionary)
?
(b) wh(gen-in)
wh(reason)
?
? { naze , doushite }
why why
(given by a noun?interrogative dictionary)
?
(iii) Lexical function: It generates different lexi-
cal items in certain semantic relations, such
as derivative form, from a given lexical item.
The back-end of this knowledge is a set
of pre-compiled dictionaries as described in
Section 4.2.
Table 1 gives a summary of grammar in Backus-
Naur form, examples, and instantiations of each
knowledge. Figure 2 illustrates an example of
knowledge application flow for transforming (4s)
into (4t), where ?:? denotes delimiter of con-
stituents.
(4) s. ?kakunin:o:isogu?
t. ?isoide:{kakunin-suru:
{?, reru/rareru, seru/saseru}:{?, ta/da}}?
First, transformation patterns that match to the given
input are applied. Then, the skeletons of syntactic
variants given by the pattern are lexicalized by con-
secutively invoking generation functions and lexical
functions. Plural number of expressions that gen-
eration function and lexical function generate are
enumerated within curly brackets. Transformation
is ended when the skeletons are fully lexicalized.
In fact, knowledge design for realizing the trans-
formation is not really new, because we have been
inspired by the previous pattern-based approaches.
Transformation pattern is thus alike that in the
Meaning-Text Theory (MTT) (Mel?c?uk, 1996), Syn-
chronous Tree Adjoining Grammar (STAG) (Dras,
1999), meta-rule for Fastr (Yoshikane et al, 1999),
154
{v(kakunin) : genVoice() : genTense()}
okakunin
N
:
C
: isogu
V
Trans. Pat.
N:C:V? adv(V):vp(N)
adv(isogu) : vp(kakunin)
Gen. Func.
vp(N)
kakunin-suru
Lex. Func.
v(N)
Gen. Func.
genVoice()
Gen. Func.
genTense()
isoide
Lex. Func.
adv(V)
{?, reru/rareru, seru/saseru} {?, ta/da}
isoide : {kakunin-suru : {?, reru/rareru, seru/saseru} : {?, ta/da}}
Figure 2: Syntactic transformation (for (2)).
and transfer pattern for KURA (Takahashi et al,
2001). Lexical function is also alike that in MTT.
However, our aim in this research is beyond the
design. In other words, as described in Section 1,
we are aiming at the following two: to develop re-
sources for handling syntactic variants in Japanese,
and to confirm if phrasal thesaurus really contribute
to computing semantic equivalence.
3.3 Surface generation with lexical choice
The input of the third component is a bunch of candi-
date phrases such as shown in (4t). This component
does the following three processes in turn:
Step 1. Unfolding: All word sequences are gener-
ated by removing curly brackets one by one.
Step 2. Lexical choice: Disjunctive words are con-
catenated with ?/? (e.g., ?reru/rareru? in (4t)).
One of them is selected based on POS and con-
jugation types of the preceding word.
Step 3. Conjugation: In the transformation step,
conjugative words are moved to different po-
sitions and some of them are newly generated.
Inappropriate conjugation forms are corrected.
3.4 SLM-based filtering
In the final step, we assess the correctness of each
candidate of syntactic variants using a statistical lan-
guage model. Our model simply rejects candidate
phrases that never appear in a large size of raw text
corpus consisting of 15 years of newspaper articles
(Mainichi 1991?2005, approximately 1.8GB). Al-
though it is said that Japanese language has a degree
N:C:V
N1:N2:C:V+N
N:C:V1:V2
+V
N:C:Adv:V+Adv
Adj:N:C:V
+Adj
N:C:Adj
switch V with Adj
Figure 3: Derivations of phrase types.
of freedom in word ordering, current implementa-
tion does not yet employ structured language models
because phrases we handle are simple.
4 Knowledge implementation
4.1 Transformation patterns and generation
functions
An issue of developing resources is how to ensure
their coverage. Our approach to this issue is to de-
scribe transformation patterns by extending those for
simpler phrases. We first described following three
patterns for N : C : V type phrases which we con-
sider the simplest according to Figure 1.
(5) a. N : C : V ? vp(N)
b. N : C : V ? N : genCase() : lvc(V )
c. N : C : V ? adv(V ) : vp(N)
While the pattern (5c) is induced from example (2),
the patterns (5a-b) are derived from examples (6)
and (7), respectively.
(6) s. shigeki-o ukeru
inspiration-ACC to receive
to receive an inspiration
t. shigeki-sareru
to inspire-PASS
to be inspired
(7) s. hada-o shigeki-suru
skin-ACC to stimulate
to stimulate skin
t. hada-ni shigeki-o ataeru
skin-DAT stimulus-ACC to give
to give skin a stimulus
Regarding the patterns in (8) as the entire set of
compositional paraphrases for N : C : V type
phrases, we then extended them to a bit more com-
plex phrases as in Figure 3. For instance, 10 patterns
155
Table 2: Transformation patterns.
Target phrase # of patterns
N : C : V 3
N
1
: N
2
: C : V 10
N : C : V
1
: V
2
10
N : C : Adv : V 7
Adj : N : C : V 4
N : C : Adj 3
Total 37
Table 3: Generation functions.
Definition Syntactic category # of returned value
np(N
1
, N
2
) noun phrase 9
vp(N) verb phrase 1
vp(N
1
, N
2
) verb phrase 2
vp(V
1
, V
2
) verb phrase 3
lvc(V ) light verb construction 1
genCase() case marker 4
genVoice() verbal suffix for voice 3
genTense() verbal suffix for tense 2
genAspect () verbal suffix for aspect 2
for N
1
: N
2
: C : V type phrases shown in (8) have
been described based on patterns in (5), mainly fo-
cusing on interactions between newly introduced N
1
and other constituents.
(8) a. N
1
: N
2
: C : V ? vp(N
1
, N
2
) (5a)
b. N
1
: N
2
: C : V ?
N
1
: genCase() : vp(N
2
) (5a)
c. N
1
: N
2
: C : V ?
N
2
: genCase() : vp(N
1
) (5a)
d. N
1
: N
2
: C : V ?
np(N
1
, N
2
) : genCase() : lvc(V ) (5b)
e. N
1
: N
2
: C : V ? N
1
: genCase() :
N
2
: genCase() : lvc(V ) (5b)
f. N
1
: N
2
: C : V ? N
2
: genCase() :
N
1
: genCase() : lvc(V ) (5b)
g. N
1
: N
2
: C : V ?
adv (V ) : vp(N
1
, N
2
) (5c)
h. N
1
: N
2
: C : V ?
adv (V ) : N
1
: genCase() : vp(N
2
) (5c)
i. N
1
: N
2
: C : V ?
adv (V ) : N
2
: genCase() : vp(N
1
) (5c)
j. N
1
: N
2
: C : V ?
np(N
1
, N
2
) : C : V (new)
The number of transformation patterns we have so
far developed is shown in Table 2.
Generation functions shown in Table 3 are devel-
oped along with creating transformation patterns.
Although this is the heart of the proposed model,
two problems are remained: (i) the granularity of
each generation function is determined according to
Table 4: Dictionaries for lexical functions.
ID POS-pair |D| |C| |D ? C| |J |
(a) noun?verb 3,431 - 3,431 3,431
(b) noun?adjective 308 667 906 475 ?
(c) noun?adjectival verb 1,579 - 1,579 1,579
(d) noun?adverb 271 - 271 271
(e) verb?adjective 252 - 252 192 ?
(f) verb?adjectival verb 74 - 74 68 ?
(g) verb?adverb 74 - 74 64 ?
(h) adjective?adjectival verb 66 95 159 146 ?
(i) adjective?adverb 33 - 33 26 ?
(j) adjectival verb?adverb 70 - 70 70
Total 6,158 762 6,849 6,322
our linguistic intuition, and (ii) they do not ensure of
generating all possible phrases. Therefore, we have
to establish the methodology to create this knowl-
edge more precisely.
4.2 Lexical functions
Except wh(N), which generates interrogatives as
shown in the bottom line of Table 1, the relations
we have so far implemented are lexical derivations.
These roughly correspond to S, V, A, and Adv in
MTT. The back-end of these lexical functions is a
set of dictionaries built by the following two steps:
Step 1. Automatic candidate collection: Most
derivatives in Japanese share the beginning
of words and are characterized by the corre-
spondences of their suffixes. For example,
?amai (be sweet)? and ?amami (sweetness)?
has a typical suffix correspondence ??-i:?-mi?
of adjective?noun derivation. Using this clue,
candidates are collected by two methods.
? From dictionary: Retrieve all word pairs from
the given set of words those satisfying the
following four conditions: (i) beginning with
kanji character, (ii) having different POSs,
(iii) sharing at least the first character and the
first sound, and (iv) having a suffix pattern
which corresponds to at least two pairs.
? Using dictionary and corpus: Generate candi-
dates from a set of words by applying a set of
typical suffix patterns, and then check if each
candidate is an actual word using corpus. This
is based on (Langkilde and Knight, 1998).
Step 2. Manual selection: The set of word pairs
collected in the previous step includes those do
not have particular semantic relationship. This
step involves human to discard noises.
156
Table 4 shows the size of 10 dictionaries, where
each column denotes the number of word pairs re-
trieved from IPADIC5 (|D|), those using IPADIC,
seven patterns and the same corpus as in Section 3.4
(|C|), their union (|D ? C|), and those manu-
ally judged correct (|J |), respectively. The sets of
word pairs J are used as bi-directional lexical func-
tions, although manual screening for four dictionar-
ies without dagger (?) are still in process.
5 Discussion
5.1 Unit of processing
The working hypothesis underlying our work is that
phrase is the appropriate unit for computing seman-
tic equivalence. In addition to the arguments in
Section 1, the hypothesis is supported by what is
done in practice. Let us see two related fields.
The first is the task of word sense disambigua-
tion (WSD). State-of-the-art WSD techniques refer
to context as a clue. However, the range of context
is usually not so wide: words and their POSs within
small window centered the target word and content
words within the same sentence of the target word.
The task therefore can be viewed as determining the
meaning of phrase based on its constituent words
and surrounding content words.
Statistical language model (SLM) is another field.
SLMs usually deal with various things within word
sequence (or structure) at the same time. How-
ever, relations within a phrase should be differen-
tiated from that between phrases, because checking
the former is for grammaticality, while the latter for
cohesion. We think SLMs should take the phrase to
determine boundaries for assessing the correctness
of generated expressions more accurately.
5.2 Compositionality
We examined how large part of manually created
paraphrases could be generated in our compositional
approach. First, a set of paraphrase examples were
created in the following procedure:
Step 1. Most frequent 400 phrases typed N
1
: N
2
:
C : V were sampled from one year of newspa-
per articles (Mainichi 1991).
Step 2. An annotator produced paraphrases for each
phrase. We allowed to record more than one
5http://mecab.sourceforge.jp/
paraphrase for a given phrase and to give up
producing paraphrases. As a result, we ob-
tained 211 paraphrases for 170 input phrases.
Manual classification revealed that 42% (88 / 211)
of paraphrases could be compositionally explain-
able, and the (theoretical) coverage increases to 86%
(182 / 211) if we have a synonym dictionary. This
ratio is enough high to give these phenomena pref-
erence as the research target, although we cannot re-
ject a possibility that data has been biased.
5.3 Sufficient condition of equivalence
In our system, transformation patterns and genera-
tion functions offer necessary conditions for gener-
ating syntactic variants for given input. However,
we have no sufficient condition to control the appli-
cation of such a knowledge.
It has not been thoroughly clarified what clue can
be sufficient condition to ensure semantic equiva-
lence, even in a number of previous work. Though,
at least, roles of participants in the event have to be
preserved by some means, such as the way presented
in (Pantel et al, 2007). Kaji et al (2002) introduced
a method of case frame alignment in paraphrase gen-
eration. In the model, arguments of main verb in the
source are taken over by that of the target according
to the similarities between arguments of the source
and target. Fujita et al (2005) employed a semantic
representation of verb to realize the alignment of the
role of participants governed by the source and tar-
get verbs. According to an empirical experiment in
(Fujita et al, 2005), statistical language models do
not contribute to calculating semantic equivalence,
but to filtering out anomalies. We therefore plan to
incorporate above alignment-based models into our
system, for example, within or after the syntactic
transformation step (Figure 2).
5.4 Ideas for improvement
The knowledge and system presented in Section 3
are quite simple. Thus the following features should
be incorporated to improve the system in addition to
the one described in Section 5.3.
? Dependency structure: To enable flexible
matching, such as Adv : N : C : V type input
and transformation pattern for N : C : Adv :
V type phrases.
? Sophisticated SLM: The generation phase
should also take the structure into account to
157
evaluate generated expressions flexibly.
? Knowledge development: Although we have
not done intrinsic evaluation of knowledge, we
are aware of its incompleteness. We are con-
tinuing manual screening for the dictionaries
and planning to enhance the methodology of
knowledge development.
6 Conclusion
To enhance the technology for computing seman-
tic equivalence, we have introduced the notion of
phrasal thesaurus, which is a natural extension of
conventional word-based thesaurus. Plausibility of
taking phrase as the unit of processing has been dis-
cussed from several viewpoints. On the basis of
that, we have been developing a system to dynam-
ically generate syntactic variants in Japanese predi-
cate phrases which utilizes three sorts of knowledge
that are inspired by MTT, STAG, Fastr, and KURA.
Future work includes implementing more precise
features and larger resources to compute semantic
equivalence. We also plan to conduct an empirical
evaluation of the resources and the overall system.
Acknowledgments
This work was supported in part by MEXT Grants-
in-Aid for Young Scientists (B) 18700143, and for
Scientific Research (A) 16200009, Japan.
References
Regina Barzilay and Lillian Lee. 2003. Learning to paraphrase:
an unsupervised approach using multiple-sequence align-
ment. In Proceedings of the 2003 Human Language Tech-
nology Conference and the North American Chapter of the
Association for Computational Linguistics (HLT-NAACL),
pages 16?23.
Chris Brockett and William B. Dolan. 2005. Support Vector
Machines for paraphrase identification and corpus construc-
tion. In Proceedings of the 3rd International Workshop on
Paraphrasing (IWP), pages 1?8.
Timothy Chklovski and Patrick Pantel. 2004. VerbOcean: min-
ing the Web for fine-grained semantic verb relations. In Pro-
ceedings of the 2004 Conference on Empirical Methods in
Natural Language Processing (EMNLP), pages 33?40.
Michael Collins and Nigel Duffy. 2001. Convolution kernels
for natural language. In Advances in Neural Information
Processing Systems 14: Proceedings of the 2001 Confer-
ence, pages 625?632.
Mark Dras. 1999. Tree adjoining grammar and the reluctant
paraphrasing of text. Ph.D. thesis, Division of Information
and Communication Science, Macquarie University.
Atsushi Fujita, Kentaro Inui, and Yuji Matsumoto. 2005. Ex-
ploiting Lexical Conceptual Structure for paraphrase gener-
ation. In Proceedings of the 2nd International Joint Con-
ference on Natural Language Processing (IJCNLP), pages
908?919.
Atsushi Fujita and Kentaro Inui. 2005. A class-oriented ap-
proach to building a paraphrase corpus. In Proceedings
of the 3rd International Workshop on Paraphrasing (IWP),
pages 25?32.
Zellig Harris. 1968. Mathematical structures of language.
New York: Interscience Publishers.
Nobuhiro Kaji, Daisuke Kawahara, Sadao Kurohashi, and
Satoshi Sato. 2002. Verb paraphrase based on case frame
alignment. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics (ACL), pages
215?222.
Keiko Kondo, Satoshi Sato, and Manabu Okumura. 1999.
Paraphrasing of ?sahen-noun + suru?. IPSJ Journal,
40(11):4064?4074. (in Japanese).
Keiko Kondo, Satoshi Sato, and Manabu Okumura. 2001.
Paraphrasing by case alternation. IPSJ Journal, 42(3):465?
477. (in Japanese).
Irene Langkilde and Kevin Knight. 1998. Generation that ex-
ploits corpus-based statistical knowledge. In Proceedings of
the 36th Annual Meeting of the Association for Computa-
tional Linguistics and the 17th International Conference on
Computational Linguistics (COLING-ACL), pages 704?710.
Dekang Lin and Patrick Pantel. 2001. Discovery of inference
rules for question answering. Natural Language Engineer-
ing, 7(4):343?360.
Igor Mel?c?uk. 1996. Lexical functions: a tool for the descrip-
tion of lexical relations in a lexicon. In Leo Wanner, editor,
Lexical Functions in Lexicography and Natural Language
Processing, pages 37?102. John Benjamin Publishing Com-
pany.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola, Timothy
Chklovski, and Eduard Hovy. 2007. Isp: Learning infer-
ential selectional preferences. In Proceedings of Human
Language Technologies 2007: The Conference of the North
American Chapter of the Association for Computational Lin-
guistics (NAACL-HLT), pages 564?571.
Tetsuro Takahashi, Tomoya Iwakura, Ryu Iida, Atsushi Fujita,
and Kentaro Inui. 2001. KURA: a transfer-based lexico-
structural paraphrasing engine. In Proceedings of the 6th
Natural Language Processing Pacific Rim Symposium (NL-
PRS) Workshop on Automatic Paraphrasing: Theories and
Applications, pages 37?46.
Tetsuro Takahashi. 2005. Computation of semantic equiva-
lence for question answering. Ph.D. thesis, Graduate School
of Information Science, Nara Institute of Science and Tech-
nology.
Kentaro Torisawa. 2006. Acquiring inference rules with tem-
poral constraints by using Japanese coordinated sentences
and noun-verb co-occurrences. In Proceedings of the Hu-
man Language Technology Conference of the North Ameri-
can Chapter of the Association for Computational Linguis-
tics (HLT-NAACL), pages 57?64.
Julie Weeds, David Weir, and Bill Keller. 2005. The distribu-
tional similarity of sub-parses. In Proceedings of the ACL
Workshop on Empirical Modeling of Semantic Equivalence
and Entailment, pages 7?12.
Fuyuki Yoshikane, Keita Tsuji, Kyo Kageura, and Christian
Jacquemin. 1999. Detecting Japanese term variation in tex-
tual corpus. In Proceedings of the 4th International Work-
shop on Information Retrieval with Asian Languages, pages
97?108.
158
Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), pages 64?72,
Beijing, August 2010
Standardizing Complex Functional Expressions in Japanese  
Predicates: Applying Theoretically-Based Paraphrasing Rules  
 
Tomoko Izumi?    Kenji Imamura?    Genichiro Kikui?    Satoshi Sato? 
?NTT Cyber Space Laboratories, 
NTT Corporation 
{izumi.tomoko, imamura.kenji, 
kikui.genichiro}@lab.ntt.co.jp
?Graduate School of Engineering,  
Nagoya University 
ssato@nuee.nagoya-u.ac.jp 
 
 
 
Abstract 
In order to accomplish the deep semantic 
understanding of a language, it is essen-
tial to analyze the meaning of predicate 
phrases, a content word plus functional 
expressions. In agglutinating languages 
such as Japanese, however, sentential 
predicates are multi-morpheme expres-
sions and all the functional expressions 
including those unnecessary to the mean-
ing of the predicate are merged into one 
phrase. This triggers an increase in sur-
face forms, which is problematic for 
NLP systems. We solve this by introduc-
ing simplified surface forms of predi-
cates that retain only the crucial meaning 
of the functional expressions. We con-
struct paraphrasing rules based on syn-
tactic and semantic theories in linguistics. 
The results of experiments show that our 
system achieves the high accuracy of 
77% while reducing the differences in 
surface forms by 44%, which is quite 
close to the performance of manually 
simplified predicates. 
 
1 Introduction 
The growing need for text mining systems such 
as opinion mining and sentiment analysis re-
quires the deep semantic understanding of lan-
guages (Inui et al, 2008). In order to accomplish 
this, one needs to not only focus on the meaning 
of a single content word such as buy but also the 
meanings conveyed by function words or func-
tional expressions such as not and would like to. 
In other words, to extract and analyze a predi-
cate, it is critical to consider both the content 
word and the functional expressions (Nasukawa, 
2001). For example, the functional expressions 
would like to as in the predicate ?would like to 
buy? and can?t as in ?can?t install? are key ex-
pressions in detecting the customer?s needs and 
complaints, providing valuable information to 
marketing research applications, consumer opi-
nion analysis etc.  
Although these functional expressions are 
important, there have been very few studies that 
extensively deal with these functional expres-
sions for use in natural language processing 
(NLP) systems (e.g., Tanabe et al, 2001; Mat-
suyoshi and Sato, 2006, 2008). This is due to the 
fact that functional expressions are syntactically 
complicated and semantically abstract and so are 
poorly handled by NLP systems. 
In agglutinating languages such as Japanese, 
functional expressions appear in the form of 
suffixes or auxiliary verbs that follow the 
content word without any space. This sequence 
of a content word (c for short) plus several of 
functional expressions (f for short) forms a 
predicate in Japanese (COMP for completive 
aspect marker, NOM for nominalizer, COP for 
copular verb).   
(1) kat -chai -takat -ta -n -da 
buy -COMP -want -PAST -NOM -COP 
c -f1 -f2 -f3 -f4 -f5 
?(I) wanted to buy (it)? 
The meaning of ?want to? is expressed by -tai 
(f2) and the past tense is expressed by -ta (f3). 
64
The other functional expressions, -chai(f1), -n(f4), 
and -da(f5), only slightly alter the predicative 
meaning of ?wanted to buy,? as there is no direct 
English translation. Therefore, (1) expresses the 
same fact as (2). 
(2)  kai -takat -ta 
  buy -want -PAST 
?(I) wanted to buy (it).? 
As shown, in Japanese, once one extracts a 
predicate phrase, the number of differences in 
surface forms increases drastically regardless of 
their similarities in meaning. This is because 
sentential predicates are multi-word or multi-
morpheme expressions and there are two differ-
ent types of functional expressions, one which is 
crucial for the extraction of predicative meaning 
and the other, which is almost unnecessary for 
NLP applications. This increase in surface forms 
complicates NLP systems including text mining 
because they are unable to recognize that these 
seemingly different predicates actually express 
the same fact. 
In this study, we introduce paraphrasing rules 
to transform a predicate with complex functional 
expressions into a simple predicate. We use the 
term standardize to refer to this procedure. 
Based on syntactic and semantic theories in lin-
guistics, we construct a simple predicate struc-
ture and categorize functional expressions as 
either necessary or unnecessary. We then pa-
raphrase a predicate into one that only retains the 
crucial meaning of the functional expression by 
deleting unnecessary functional expressions 
while adding necessary ones. 
 The paper is organized as follows. In Section 
2, we provide related work on Japanese 
functional expressions in NLP systems as well as 
problems that need to be solved. Section 3 
introduces several linguistic theories and our 
standardizing rules that we constructed based on 
these theories. Section 4 describes the 
experiments conducted on our standardization 
system and the results. Section 5 discusses the 
results and concludes the paper. Throughout this 
paper, we use the term functional expressions to 
indicate not only a single function word but also 
compounds (e.g., would like to). 
 
2 Previous Studies and Problems  
Shudo et al (2004) construct abstract semantic 
rules for functional expressions and use them in 
order to find whether two different predicates 
mean the same. Matsuyoshi and Sato (2006, 
2008) construct an exhaustive dictionary of 
functional expressions, which are hierarchically 
organized, and use it to produce different func-
tional expressions that are semantically equiva-
lent to the original one.  
 Although these studies provide useful in-
sights and resources for NLP systems, if the in-
tention is to extract the meaning of a predicate, 
we find there are still problems that need to be 
solved. There are two problems that we focus on. 
 The first problem is that many functional ex-
pressions are unnecessary, i.e., they do not ac-
tually alter the meaning of a predicate.   
(3) yabure -teshimat -ta -no -dearu 
 rip -COMP -PAST -NOM -COP 
 c -f1 -f2 -f3 -f4  
 ?(something) ripped.? 
(3) can be simply paraphrased as (4) 
(4) yabure -ta 
 rip -PAST 
 c -f1  
In actual NLP applications such as text mining, 
it is essential that the system recognizes that (3) 
and (4) express the same event of something 
?ripped.? In order to achieve this, the system 
needs to recognize -teshimat, -no, and -dearu as 
unnecessary (f1, f3, f4 ??). Previous studies that 
focus on paraphrasing of one functional expres-
sion to another (f ? f?) cannot solve this prob-
lem. 
 The second problem is that we sometimes 
need to add certain functional expressions in 
order to retain the meaning of a predicate (? ?f). 
(5) (Hawai-ni) P1iki, P2nonbirishi -takat -ta 
 (Hawaii-to)  go relax -want -PAST 
   c1 c2 f1 f2 
?I wanted to go to Hawaii and relax.? 
(5) has a coordinate structure, and two verbal 
predicates, iki (P1) ?go? and nonbirishi-takat-ta 
(P2) ?wanted to relax?, are coordinated.  
 As the English translation indicates, the first 
predicate in fact means iki-takat-ta ?wanted to 
65
go,? which implies that the speaker was not able 
to go to Hawaii. If the first predicate was ex-
tracted and analyzed as iku, the base (present) 
form of ?go,? then this would result in a wrong 
extraction of predicate, indicating the erroneous 
fact of going to Hawaii in the future (Present 
tense in Japanese expresses a future event). In 
this case, we need to add the functional expres-
sions takat ?want? and ta, the past tense marker, 
to the first verbal predicate.  
As shown, there are two problems that need 
to be solved in order for a system to extract the 
actual meaning of a predicate. 
i. Several functional expressions are neces-
sary for sustaining the meaning of the event 
expressed by a predicate while others barely 
alter the meaning (f ??). 
ii. Several predicates in coordinate sentences 
lack necessary functional expressions at the 
surface level (? ?f) and this results in a 
wrong extraction of the predicate meaning. 
Based on syntactic and semantic theories in lin-
guistics, we construct paraphrasing rules and 
solve these problems by standardizing complex 
functional expressions. 
 
3 Construction of Paraphrasing Rules  
The overall flow of our standardizing system is 
depicted in Figure 1. The system works as fol-
lows. 
i. Given a parsed sentence as an input, it ex-
tracts a predicate(s) and assigns a semantic 
label to each functional expression based on 
Matsuyoshi and Sato (2006). 
ii. As for an intermediate predicate, necessary 
functional expressions are added if missing 
(? ?f). 
iii. From each predicate, delete unnecessary 
functional expressions that do not alter the 
meaning of the predicate (f ??). 
iv. Conjugate each element and generate a 
simplified predicate.  
There are two fundamental questions that we 
need to answer to accomplish this system. 
A) What are UNNECCESARY functional ex-
pressions (at least for NLP applications), 
i.e., those that do not alter the meaning of 
the event expressed by a predicate? 
B) How do we know which functional expres-
sions are missing and so should be added? 
We answer these questions by combining what is 
needed in NLP applications and what is dis-
cussed in linguistic theories. We first answer 
Question A. 
 
3.1 Categorization of Functional Expressions 
As discussed in Section 1 and in Inui et al 
(2008), what is crucial in the actual NLP appli-
cations is to be able to recognize whether two 
seemingly different predicates express the same 
fact.  
This perspective of factuality is similar to the 
truth-value approach of an event denoted by pre-
dicates as discussed in the field of formal seman-
tics (e.g., Chierchia and Mcconnel-Ginet, 2000; 
Portner, 2005). Although an extensive investiga-
tion of these theories is beyond the scope of this 
paper, one can see that expressions such as tense 
(aspect), negation as well as modality, are often 
discussed in relation to the meaning of an event 
(Partee et al, 1990; Portner, 2005). 
Tense (Aspect): Expresses the time in (at/for) 
which an event occurred. 
Negation: Reverses the truth-value of an event. 
Modality: Provides information such as possi-
bility, obligation, and the speaker?s eagerness 
with regard to an event and relate it to what is 
true in reality. 
The above three categories are indeed useful in 
explaining the examples discussed above. 
(6) kat -chai -takat -ta -n -da 
buy -COMP -want -PAST -NOM -COP 
 aspect modality tense(aspect) 
(7) kai -takat -ta 
 buy -want -PAST 
  modality tense(aspect) 
 ?wanted to buy? 
The predicate ?kat-chai-takat-ta-n-da? in (6) and 
?kai-takat-ta? in (7) express the same event be-
cause they share the same tense (past), negation 
(none), and modality (want). Although (6) has 
the completive aspect marker -chai while (7) 
does not, they still express the same fact. This is 
because the Japanese past tense marker -ta also 
has a function to express the completive aspect. 
The information expressed by -chai in (6) is re-
66
dundant and so unnecessary.  
On the other hand, the predicate ?iku? in (5) 
and ?iki-takat-ta,? which conveys the actual 
meaning of the predicate, express a different fact 
because they establish a different tense (present 
vs. past) and different modality (none vs. want).  
As shown, once we examine the abstract se-
mantic functions of functional expressions, we 
can see the factual information in a predicate is 
influenced by tense (aspect), negation, and mod-
ality. Therefore, the answer to Question A is that 
necessary functional expressions are those that 
belong to tense (aspect), negation, and modality. 
Furthermore, if there are several functional ex-
pressions that have the same semantic function, 
retaining one of them is sufficient. 
 
3.2 Adding Necessary Functional Expressions 
The next question that we need to answer is how 
we find which functional expressions are miss-
ing when standardizing an intermediate predicate 
in a coordinate structure (e.g., (5)). We solve this 
based on a detailed analysis of the syntactic 
structure of predicates. 
Coordinate structures are such that several 
equivalent phrases are coordinated by conjunc-
tions such as and, but, and or. If a predicate is 
coordinated with another predicate, these two 
predicates must share the same syntactic level. 
Therefore, the structure in (5) is indeed depicted 
as follows (What TP and ModP stand for will be 
discussed later). 
[TP[ModP[VP(Hawai-ni) iki][VPnonbirishi]takat]ta ] 
[TP[ModP[VP(Hawaii-to) go][VPrelax] want]PAST] 
This is the reason why the first predicate iki 
should be paraphrased as iki-takat-ta ?wanted to 
go.? It needs to be tagged with the modality ex-
pression tai and the past tense marker ta, which 
seems to attach only to the last predicate.  
 This procedure of adding necessary function-
al expressions to the intermediate predicate is 
not as simple as it seems, however.   
(8) nemutai-mitai-de kaeri -tagatte -tei -ta 
sleepy-seems-COP gohome-want-CONT-PAST 
?He seemed sleepy and wanted to go home.? 
In (8), the first predicate nemutai-mitai-de ?seem 
to be sleepy? should be paraphrased as nemutai-
mitai-dat-ta, ?seemed to be sleepy,? in which 
only the functional expression indicating past is 
required. The other functional expressions such 
as tagatte ?want,? and the aspect marker tei 
(CONTinuation) should not be added (nemutai-
mitai-de-tagat(want)-tei(CONT)-ta(PAST) is 
completely ungrammatical).  
Input  
A parsed Sentence 
Hontoo-wa Hawai-ni iki, nonbirishi takat ta n da kedo
Really-TOP Hawaii-to go relax want PAST NOM COP but 
?I wanted to go to Hawaii and relax if I could.? 
i. Predicate Extraction &
Labeling Semantic Classes 
to Functional Expressions 
ii. ADD necessary 
functional expressions 
(? ? f) 
iii. DELETE unnecessary 
functional expressions 
(f ? ?) 
iv. Conjugate and  
Generate simple predicates 
Output  
Simplified Predicates 
iki 
go 
c 
[[[VP] ?] ?] 
iki tai ta
go want PAST
c [??] [??]
iki takat ta
go want PAST 
nonbirishi takat ta  
relax want PAST  
iki-takat-ta
?wanted to go? 
nonbirishi-takat-ta 
?wanted to relax? 
Figure 1. The flow of Standardization. 
iki tai ta
go want PAST 
c [??] [??] 
nonbirishi takat ta n da kedo 
relax want PAST NOM COP but 
c [??] [??] [??] [??] [????]
nonbirishi takat ta n da kedo 
relax  want PAST NOM COP but 
c [??] [??] [??] [??] [????] 
[[[VP]             ModP]    TP] 
67
 Furthermore, the intermediate predicate in the 
following example does not allow any functional 
expressions to be added. 
(9) (imawa) yasui-ga (mukashiwa) takakat-ta 
(today) inexpensive-but (in old days) expensive-
PAST 
?(They) are inexpensive (today), (but) used to 
be very expensive (in the old days.)? 
In (9), the first predicate yasui ?inexpensive? 
should not be paraphrased as yasukat-ta ?was 
inexpensive? since this would result in the un-
grammatical predicate of ?*(they) were inexpen-
sive (today).?  
 In order to add necessary functional expres-
sions to an intermediate predicate, one needs to 
solve the following two problems. 
i. Find whether the target predicate indeed 
lacks necessary functional expressions.  
ii. If such a shortfall is detected, decide which 
functional expressions should be added to 
the predicate. 
We solve these problems by turning to the in-
completeness of the syntactic structure of a pre-
dicate. 
Studies such as Cinque (2006) and Rizzi 
(1999) propose detailed functional phrases such 
as TopP (Topic Phrase) in order to fully describe 
the syntactic structures of a language. We adopt 
this idea and construct a phrase structure of Jap-
anese predicates which borrows from the func-
tional phrases of TP, ModP, and FocP (Figure 2). 
 ModP stands for a modality phrase and this is 
where modality expressions can appear.1  FocP 
stands for a focus phrase. This is the phrase 
where the copula da appears. This phrase is 
needed because several modality expressions 
syntactically need the copula da in either the 
following or preceding position (Kato, 2007). 
The existence of FocP also indicates that the 
modality expressions within the phrase are com-
plete (no more modality phrase is attached). TP 
stands for a tense phrase and this is where the 
tense marker appears.  
 Note that this structure is constructed for the 
purpose of Standardization and other functional 
                                                 
1 The structure of Figure 2 is recursive. A modality expres-
sion can appear after a TP. Also, more than one ModP can 
appear although ModP and FocP are optional. 
projections such as NegP (negation phrase) will 
not be discussed although we assume there must 
be one. Based on the predicate structure in Fig-
ure 2, we solve the two problems as follows. 
 
The first problem: Detecting whether the target 
predicate lacks necessary functional expressions.  
? If the predicate has the past tense marker ta 
or if the coordinate conjunction following 
the predicate is for combining phrases with 
tense, then consider the predicate as com-
plete and do not add any functional expres-
sions. Otherwise, consider the predicate as 
incomplete and add the appropriate func-
tional expressions. 
The underlying principle of this rule is that if a 
predicate is tensed, then its syntactic structure is 
complete. As often described in syntactic theo-
ries (e.g., Adger, 2003), a sentence can be said to 
be a phrase with tense (i.e., TP). In other words, 
if a predicate is tensed, then it can stand alone as 
a sentence.  
 By adopting this idea, we judge the com-
pleteness of a predicate by the existence of tense. 
Because Japanese marks past tense by the past 
tense marker -ta, if a predicate has -ta, it is com-
plete and no functional expressions need be add-
ed.  
 However, Japanese does not hold an explicit 
present tense marker; the base form of a verb is 
also a present form. We solve this by looking at 
which conjunction follows the predicate. As dis-
cussed in Minami (1993), the finite state and the 
type of conjunction are related; some conjunc-
tions follow tensed phrases while others follow 
infinitival phrases. Following this, we categorize 
all the coordinate conjunctions based on whether 
they can combine with a tensed phrase. These 
conjunctions are listed as tensed in Table 1. If 
TP  
3 
(FocP) T:ta PAST [??] 
3 
(ModP)*   Foc:da COP [??] 
3 
VP   Mod: mitai ?seems? [??] 
4 
iku ?go? 
Figure 2. Structure of a predicate. 
68
the target phrase is followed by one of those 
conjunctions, then we do not add any functional 
expressions to them because they are complete. 
 
The second problem: Finding the appropriate 
functional expressions for incomplete interme-
diate predicates. 
As discussed, we assume that predicates are 
coordinated at one of the functional phrase levels 
in Figure 2. Functional expressions that need to 
be added are, therefore, those of the outer phras-
es of the target phrase.  
 For example, if the target phrase has da, the 
head of FocP, then it only needs the past tense 
marker to be added, which is located above the 
FocP (i.e., TP). This explains the paraphrasing 
pattern of (8). Therefore, by looking at which 
functional expressions held by the target predi-
cate, one can see that functional expressions to 
be added are those that belong to phrases above 
the target phrase. 
 As shown, the answer to Question B is that 
we only add functional expressions to incom-
plete predicates, which are judged based on the 
existence/absence of tense. The appropriate 
functional expressions to be added are those of 
outer phrases of the target phrase. 
 
3.3 Implementing the Standardization 
In this final subsection, we describe how we ac-
tually implement our theoretical observations in 
our standardization system.  
 
CATEGORIZE functional expressions 
First, we selected functional expressions that 
belong to our syntactic and semantic categories 
from those listed in Matsuyoshi and Sato (2006), 
a total of about 17,000 functional expressions 
with 95 different semantic labels. We use ab-
stract semantic labels, such as ?completion,? 
?guess,? and ?desire? for the categorization 
(Table 2).  
 We divided those that did not belong to our 
syntactic and semantic categories into Deletables 
and Undeletables. Deletables are those that do 
not alter the meaning of an event and are, there-
fore, unnecessary. Undeletables are those that 
are a part of content words, and so cannot be 
deleted (e.g., kurai [??] ?about? as in 1-man-
en-kurai-da ?is about one million yen?). Based 
on the categorization of semantic labels as well 
as surface forms of functional expressions, our 
system works as follows; 
 
ADD necessary functional expressions  
A-1: Examine whether the target predicate has 
the tense marker ta or it is followed by the 
conjunctions categorized as tensed. If not, 
then go to Step A-2. 
A-2: Based on the semantic label of the target 
predicate, decide which level of syntactic 
phrase the predicate projects. Add functional 
expressions from the last predicate that be-
longs to outer phrases. 
 
DELETE unnecessary functional expressions 
D-1: Delete all the functional expressions that 
are categorized as Deletables. 
D-2: Leave only one functional expression if 
there is more than one same semantic label. 
For those categorized as Negation, however, 
delete all if the number of negations is even. 
Otherwise, leave one. 
D-3: Delete those categorized as Focus if they 
do not follow or precede a functional expres-
sion categorized as Modality.  
 
GENERATE simple predicates 
Last, conjugate all the elements and generate 
simplified surface forms of predicates. 
 
4 Experiments and Evaluations 
4.1 Constructing Paraphrase Data 
We selected 2,000 sentences from newspaper 
and blog articles in which more than one predi-
cate were coordinated.2 We manually extracted 
predicates (c-f1-f2..fn). Half of them were those in 
which the last predicate had three or more func-
tional expressions (n ? 3). 
                                                 
2 We use Mainichi Newspapers from the year 2000. 
Table 1. Coordinate conjunctions. 
Not tensed Tensed 
gerundive 
form, te  
shi, dakedenaku, ueni, bakarika, 
hoka(ni)(wa), keredo, ga, nonitai-
shi(te),ippou(de),hanmen  
69
We then asked one annotator with a linguistic 
background to paraphrase each predicate into the 
simplest form possible while retaining the mean-
ing of the event.3 We asked another annotator, 
who also has a background in linguistics, to 
check whether the paraphrased predicates made 
by the first annotator followed our criterion, and 
if not, asked the first annotator to make at least 
one paraphrase. 424 out of 4,939 predicates 
(8.5%) were judged as not following the crite-
rion and were re-paraphrased. This means that 
the accuracy of 91.5% is the gold standard of our 
task. 
One of the authors manually assigned a cor-
rect semantic label to each functional expression. 
Procedure i in Figure 1 is, therefore, manually 
implemented in our current study. 
 
4.2 Experiments and Results 
Based on the standardization rules discussed in 
Section 3, our system automatically paraphrased 
functional expressions of test predicates into 
simple forms. We excluded instances that had 
segmentation errors and those that were judged 
as inappropriate as a predicate. 4  A total of 
1,501 intermediate predicates (287 for develop-
ment and 1,214 for test) and 1,958 last predi-
cates (391 for development and 1,567 for test) 
were transformed into simple predicates. 
 The accuracy was measured based on the ex-
act match in surface forms with the manually 
constructed paraphrases. For comparison, we 
                                                 
3 We asked to delete or add functional expressions from 
each predicate when paraphrasing. Only the surface forms 
(and not semantic labels) were used for annotation. 
4 In Japanese, a gerundive form of a verb is sometimes used 
as a postposition. The annotators excluded these examples 
as ?not-paraphrasable.? 
used the following baseline methods. 
? No Add/Delete: Do not add/delete any 
functional expression.  
? Simp Add: Simply add all functional ex-
pressions that the intermediate phrase does 
not have from the last predicate. 
Table 3 indicates the results. Our standardizing 
system achieved high accuracy of around 77% 
and 83 % in open (against the test set) and 
closed tests (against the development set) com-
pared to the baseline methods (No Add/Delete 
(open), 55%; Simp Add (open), 33%). 
We also measured the reduced rate of differ-
ences in surface forms. We counted the number 
of types of functional expressions in the last pre-
dicates (a sequence of f1-f2-f3 is counted as one) 
before and after the standardization. 
For comparison, we also counted the number 
of functional expressions of the manually pa-
raphrased predicates. Table 4 lists the results. As 
shown, our standardizing system succeeded in 
reducing surface differences in predicates from 
the original ones at the rate of 44.0%, which is 
quite close to the rate achieved by the human 
annotators (52.0%). 
 
5 Discussion and Conclusion 
Our standardization system succeeded in gene-
rating simple predicates in which only functional 
expressions crucial for the factual meaning of 
the predicate were retained.  
The predicates produced by our system 
showed fewer variations in their surface forms 
while around 77% of them exactly matched the 
simplified predicates produced by human anno-
tators, which is quite high compared to the base-
line systems.  
Table 2. Syntactic and semantic categorization of semantic labels. 
Syntactic  Semantic  Semantic Labels
T if the 
surface is ta 
Tense 
(Aspect) 
??(completion),??,??,??,??,??,??,??,???, ???, ??,?
?, ??, ?? 
 Negation ??(negation), ??,????,????,???,???,???, ???, ???
Mod Modality ??(guess),  ??(desire),??,??,??,??,??,??, ??, ??, ??, ??
??, ???, ???,???,??,???,???
Foc Focus ??(affirmation), ???,??
 Deletables ??(politeness),?-??,??,??,?-??,????,??, ??, ????, ?
?,??, ????,????,??,??,???
 Undele-
tables 
??(about), ??,??,???,???,??,??,??,??, ??, ????,?
?, ??, ??, ??, ??, ???, ???, ??, ???, ????, ????, ??, 
??, ???, ??,??,??,??,??,??,??,??,?? 
70
This was achieved because we constructed 
solid paraphrasing rules by applying linguistic 
theories in semantics and syntax. The quite low 
accuracy of the baseline method, especially 
SimpAdd, further supports our claim that im-
plementing linguistic theories in actual NLP ap-
plications can greatly improve system perfor-
mance. 
Unlike the study by Inui et al (2008), we did 
not include the meaning of a content word for 
deciding the factuality of the event nor did we 
include it in the paraphrasing rules. This lowers 
the accuracy. Several functional expressions, 
especially those expressing aspect, can be de-
leted or added depending on the meaning of the 
content word. This is because content words in-
herently hold aspectual information, and one 
needs to compare it to the aspectual information 
expressed by functional expressions. Because we 
need a really complicated system to compute the 
abstract semantic relations between a content 
word and functional expressions, we leave this 
problem for future research.  
Regardless of this, our standardizing system 
is useful for a lot of NLP applications let alne 
text mining. As mentioned in Inui et al (2008), 
bag-of-words-based feature extraction is insuffi-
cient for conducting statistically-based deep se-
mantic analysis, such as factual analysis. If stan-
dardized predicates were used instead of a single 
content word, we could expect an improvement 
in those statistically-based methods because each 
predicate holds important information about fact 
while differences in surface forms are quite li-
mited. 
In conclusion, we presented our system for 
standardizing complex functional expressions in 
Japanese predicates. Since our paraphrasing 
rules are based on linguistic theories, we suc-
ceeded in producing simple predicates that have 
only the functional expressions crucial to under-
standing of the meaning of an event. Our future 
research will investigate the relationship be-
tween the meaning of content words and those of 
functional expressions in order to achieve higher 
accuracy. We will also investigate the impact of 
our standardization system on NLP applications.  
 
References 
Adger, David (2003). Core Syntax: A minimalist ap-
proach. New York: Oxford University Press. 
Chierchia, Gennaro, & Sally McConnell-Ginet (2000). 
Meaning and grammar: An introduction to se-
mantics (2nd ed.). Cambridge, MA: The MIT 
press. 
Cinque, Guglielmo (2006). Restructuring and func-
tional heads: The cartography of syntactic struc-
tures, Vol. 4. New York: Oxford University Press. 
Haugh, Michael (2008). Utterance-final conjunctive 
particles and implicature in Japanese conversation. 
Pragmatics, 18 (3), 425-451. 
Inui, Kentaro, Shuya Abe, Kazuo Hara, Hiraku Mori-
ta, Chitose Sao, Megumi Eguchi, Asuka Sumida, 
Koji Murakami, & Suguru Matsuyoshi (2008). 
Experience mining: Building a large-scale data-
base of personal experiences and opinions from 
web documents. Proceedings of the 2008 
IEEE/WIC/ACM International Conference on 
Web Intelligence and Intelligent Agent Technolo-
gy, Vol. 1., 314-321. 
Kato, Shigehiro (2007). Nihongo-no jutsubu-kouzou 
to kyoukaisei [Predicate complex structure and 
morphological boundaries in Japanese]. The an-
nual report on cultural science, Vol. 122(6) (pp. 
97-155). Sapporo, Japan: Hokkaido University, 
Graduate School of Letters. 
Matsuyoshi, Suguru, & Satoshi Sato (2006). Compi-
lation of a dictionary of Japanese functional ex-
pressions with hierarchical organization. Proceed-
ings of the 21st International Conference on 
Computer Processing of Oriental Languages 
 Normalization No Add/Delete Simp Add 
Open (Intermediate) 77.7%(943/1214) 57.8%(702/1214) 32.8%(398/1214) 
Closed (Intermediate) 82.9%(238/287) 62.0%%(178/287) 35.2%(101/287) 
Open (Last) 76.2%(1194/1567) 51.9% (203/391) n.a 
Closed (Last) 83.4%(326/391) 48.1%(188/391) n.a. 
Table 3. Results of our normalization system. 
Original 943 types Reduced Rate
Normalization 530 types 44.0% 
Human Annotation 448 types 52.0% 
Table 4. Reduced rate of surface forms. 
71
(ICCPOL), Lecture Notes in Computer Science, 
Vol. 4285, 395-402.  
Matsuyoshi, Suguru, & Satoshi Sato (2008). Auto-
matic paraphrasing of Japanese functional expres-
sions using a hierarchically organized dictionary. 
Proceedings of the 3rd International Joint Confe-
rence on Natural Language Processing (IJCNLP), 
Vol. 1, 691-696. 
Minami, Fujio (1993). Gendai nihongobunpou-no 
rinkaku [Introduction to modern Japanese gram-
mar]. Tokyo: Taishuukan. 
Nasukawa, Tetsuya (2001). Kooru sentaa-niokeru 
tekisuto mainingu [Text mining application for 
call centers]. Journal of Japanese society for Ar-
tificial Intelligence, 16(2), 219-225. 
Partee, Barbara H., Alice ter Meulen, & Robert E. 
Wall (1990). Mathematical methods in Linguistics. 
Dordrecht, The Netherland: Kluwer. 
Portner, Paul H. (2005). What is meaning?: Funda-
mentals of formal semantics. Malden, MA: 
Blackwell. 
Rizzi, Luigi (1999). On the position ?Int(errogative)? 
in the left periphery of the clause. Ms., Universit? 
di Siena. 
Shudo, Kosho, Toshifumi Tanabe, Masahito Takaha-
shi, & Kenji Yoshimura (2004). MWEs as non-
propositional content indicators. Proceedings of 
second Association for Computational Linguistics 
(ACL) Workshops on Multiword Expressions: In-
tegrating Processing, 32-39. 
Tanabe, Toshifumi, Kenji Yoshimura & Kosho Shu-
do (2001). Modality expressions in Japanese and 
their automatic paraphrasing. Proceedings of the 
6th Natural Language Processing Pacific Rim 
Symposium (NLPRS), 507-512. 
Tsujimura, Natsuko. (2007). An Introduction to Jap-
anese Linguistics (2nd Ed.). Malden, MA: Black-
well. 
72
Proceedings of the SIGDIAL 2013 Conference, pages 70?77,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Generating More Specific Questions for Acquiring Attributes of
Unknown Concepts from Users
Tsugumi Otsuka?, Kazunori Komatani?, Satoshi Sato?, Mikio Nakano?
? Graduate School of Engineering, Nagoya University, Nagoya, Aichi 464?8603, Japan
? Honda Research Institute Japan Co., Ltd., Wako, Saitama 351?0114, Japan
{t ootuka,komatani,ssato}@nuee.nagoya-u.ac.jp, nakano@jp.honda-ri.com
Abstract
Our aim is to acquire the attributes of con-
cepts denoted by unknown words from
users during dialogues. A word unknown
to spoken dialogue systems can appear in
user utterances, and systems should be ca-
pable of acquiring information on it from
the conversation partner as a kind of self-
learning process. As a first step, we pro-
pose a method for generating more spe-
cific questions than simple wh-questions
to acquire the attributes, as such ques-
tions can narrow down the variation of
the following user response and accord-
ingly avoid possible speech recognition er-
rors. Specifically, we obtain an appropri-
ately distributed confidence measure (CM)
on the attributes to generate more specific
questions. Two basic CMs are defined us-
ing (1) character and word distributions in
the target database and (2) frequency of
occurrence of restaurant attributes on Web
pages. These are integrated to comple-
ment each other and used as the final CM.
We evaluated distributions of the CMs by
average errors from the reference. Re-
sults showed that the integrated CM out-
performed the two basic CMs.
1 Introduction
In most spoken dialogue systems, knowledge
bases for the systems are constructed off-line. In
other words, they are not updated during dia-
logues. On the other hand, humans update their
knowledge not only by reading books but also
through interaction with other people. When they
encounter an unknown word during conversations,
humans notice that it is new to them and acquire
knowledge about it by asking their conversational
partner. This self-learning process is one of the
Tell me about ?Osteria Liu?. I don?t know that restaurant.Is it ?Italian?? 
Tell me about?Toyo?. I don?t know that restaurant.What type of cuisine is it?
SystemUser
Figure 1: Example of simple and specific ques-
tions.
most intelligent features of humans. We think that
applying this intelligent feature to spoken dialogue
systems will make them more usable.
We present a method that generates appropri-
ate questions in order to acquire the attributes of
a concept that an unknown word denotes when it
appears in a user utterance. Here, we define un-
known words as those whose attributes necessary
for generating responses were not defined by the
system developer; that is, unknown to the response
generation module in the spoken dialogue system.
The system cannot reply to user utterances includ-
ing such words even if they are correctly recog-
nized by its automatic speech recognition (ASR)
module.
Questions to the user to acquire the attribute
should be specific. In spoken dialogue sys-
tems, specific questions are far preferable to wh-
questions because they can narrow down varia-
tions of the following user response. Such ques-
tions lead to a better ASR performance of the re-
sponse and reduce the risk that it includes new
other unknown words.
Two example dialogues are shown in Figure 1.
Since our target task is restaurant database re-
trieval, we set the unknown words as restaurant
names and the attribute as their cuisine in our
restaurant database. In the examples shown, the
system uses a simple wh-question (the upper part)
and a specific Yes-No question (the lower part) to
obtain cuisine types. Here, ?Toyo? and ?Osteria
Liu? are restaurant names. We assume that the
70
Table 1: Question types according to the number of cuisines (num).
num Question form Example
1 Yes-No question Is it cuisine c1?
2 Alternative question Which cuisine is it, c1 or c2?
3 3-choice question Which cuisine is it, c1, c2, or c3?
?4 Wh-question What cuisine is it?
system already knows these are restaurant names
but does not know its attributes such as its cuisine
type. The system uses a wh-question for ?Toyo?
since no clue is obtained for it. In contrast, since
?Osteria Liu? contains information on cuisines in
the name itself, a concrete Yes-No question is used
to ask whether the cuisine is ?Italian?.
We propose a method for providing a well-
distributed confidence measure (CM) to generate
more specific questions. For this purpose, we esti-
mate the cuisine type of a restaurant from its name,
which is assumed to be unknown to the system.
There have been many previous studies that esti-
mate word and character attributes using Web in-
formation (Pasca et al, 2006; Yoshinaga and Tori-
sawa, 2007). Our two estimation methods are rel-
atively simpler than these studies, since our main
focus is to generate more concrete questions on the
basis of appropriate CMs. That is, the CMs should
be high when the system seems to correctly esti-
mate a cuisine type and low when the estimation
seems difficult.
We assume a restaurant name as the input; that
is, we suppose that the system can recognize the
restaurant name in the user?s utterance correctly
by its ASR module and understand it is a restau-
rant name by its LU module. Nevertheless, it
still remains unknown to its response generation
module. This is a feasible problem when using
a large vocabulary continuous speech recognition
(LVCSR) engine containing over several million
words (Jyothi et al, 2012) and a statistical named
entity (NE) tagger (Tjong Kim Sang and Meul-
der, 2003; Zhou and Su, 2002; Ratinov and Roth,
2009).
The problem we tackle in this paper is differ-
ent from trying to estimate the NE class of an un-
known word (Takahashi et al, 2002; Meng et al,
2004). We assume the system already knows that
it is a restaurant name. Rather, we try to acquire
the attribute (e.g., cuisine type) of the concept of
the unknown word, which is required for generat-
ing responses about the restaurant in subsequent
dialogues.
2 Generating Questions Based on CM
The system determines a question type on the ba-
sis of CM. The CM is estimated for each cuisine
type cj in the target database. In this paper, the
number of cuisine types is 16, all of which are
in our restaurant database; that is, cj ? C and
|C| = 16.
Table 1 shows the four question types and their
examples. These are determined by parameter
num, which is the number of cuisine types that
should be included in the question. If the sys-
tem obtains one cuisine type that it is very con-
fident about and thus has a high CM, it should
generate the most specific question, i.e., a Yes-
No question; in this case, the number should be
1. In contrast, if unreliable cuisine types are ob-
tained, which means lower CMs, the system gen-
erates questions including several cuisine types.
The num can be determined by Equation (1):
num = min(n) s.t.
n?
j=1
CM(cj) > ?, (1)
where CM(cj) is a confidence measure for cui-
sine type cj in its descending order. ? is a constant
and can be manually decided considering the dis-
tribution of CM(cj). This equation means that if
only the CM(c1) is greater than ? (i.e., n = 1),
the system generates a specific question includ-
ing only cuisine type c1, while if the total from
CM(c1) toCM(c4) is smaller than ? (i.e., n = 4),
the system does not use estimated cuisine types
and instead generates a wh-question.
If the CM on the cuisine type is well-distributed,
the system can generate appropriate questions. In
the following section, methods to obtain such CMs
are explained.
3 Estimating Cuisine Types and
Calculating CM
The final CM is obtained by integrating two ba-
sic CMs. The system then uses this final CM to
71
Feature	 ?selec?n	 ?by	 ?mutual	 ?informa?on	 ?
Training	data	
?	 		
	 ?	 ?	 ?	 ?	 ?	 ?	
DB	
Japanese pub	
Sushi Goichi (?? ??)	
Koikoi (????)	
Japanese restaurant	Tanaka Sushi (????)?	
Maru Sushi (????) 	 Japanese restaurant	
Japanese restaurant	
Restaurant name	 Cuisine	
. . . .	 . . . .	
Quinci CENTRARE	 Italian	
Hyakuraku (??)	 Chinese restaurant?	
C?s ave cafe	 Cafe 	
Classifier:	 ?Maximum	 ?entropy	 ?(ME)	 ?model	
	 ?Azuma	 ?Sushi	 ?	 ?(???)	
Input:	 ?	 ?Restaurant	 ?name	
Output:	 ?CMD	
Japanese restaurant 	Japanese pub	Cafe	. . . . .  	
: 0.9	: 0.05	: 0.0006		
Figure 3: Overview of CMD calculation.
Web	
Es?ma?on	 ?from	 ?DB	 ?
Es?ma?on	 ?from	 ?Web	 ? CM	 ?Integra?on	 ?
DB	
CMW	 ?
Restaurant	 ?name	 ?
CMI	 ?
CMD	 ?
Question generation	based on CM	
Figure 2: Process overview.
generate questions. The two basic CM estimation
methods are:
1. Using word and character distribution in the
target database
2. Using frequency of the restaurant attributes
on the Web
A process overview of the proposed method is
shown in Figure 2. Its input to the system is an
unknown restaurant name and its output is the es-
timated CMs. The system generates questions on
the basis of the estimated CMs, which are calcu-
lated for each cuisine type.
3.1 Attribute Estimation Using Word and
Character Distribution in Database
We estimate the cuisine types of an unknown
restaurant by using the word and character distri-
bution in the target database. The target database
contains many pairs of restaurant names and cui-
sine types. The estimation is performed by us-
ing supervised machine learning trained with the
pairs. The overview of calculating CMD is shown
in Figure 3. This approach is based on our in-
tuition that some cuisine types can be estimated
from restaurant names on the basis of their char-
acter types or typical character sequences they
contain. For example, a restaurant name com-
posed of only katakana1 is probably a French or
Italian restaurant because words imported from
other countries to Japan are called ?katakana loan-
words? and are written in katakana characters
(Kay, 1995).
We use the maximum entropy (ME) model
(Berger et al, 1996) as a classifier. Its posterior
probability p(cj |si) is used as a CMD denoting
the CM estimated using a database. CMD is cal-
culated as
CMD(si, cj) = p(cj |si)
= 1Z exp
[
~?(cj) ? ~?(si)
]
, (2)
where si is a restaurant name, cj (? C) is a
cuisine type, ~?(si) is a feature vector obtained
from a restaurant name, ~?(cj) is a weight vector,
and Z is a normalization coefficient that ensures?
cj CMD(si, cj) = 1.We use three types of feature vectors obtained
from each restaurant name:
? Character n-grams (n = 1, 2, 3)
? Words
? Character types
The feature values of the character n-gram and the
word are scored as 1 if such features are contained
in the restaurant name. The Japanese morpholog-
ical analyzer Mecab (Kudo et al, 2004) with the
IPADIC dictionary is used to segment restaurant
names into word sequences. The character type
1Katakana is a Japanese syllabary. There are three kinds
of characters in Japanese. Kanji (Chinese character) are lo-
gograms and hiragana and katakana are syllabaries. Katakana
is mainly used for writing imported words and hiragana is
used for writing original Japanese words.
72
Web	 ?page	 Cuisine	 ?frequency	 ?Japanese restaurant	Italian restaurant	Western-style restaurant	
74  times	  8   times	  1  time	
Output:	 ?CMW	Japanese restaurant	Italian restaurant	Western-style restaurant	. . . .  	
??0.8	??0.11	??0.0009		
Azuma	 ?Sushi	 ?(???)	Input:	 ?	 ?Restaurant	 ?name	
	 ?Obtaining	 ?related	 ?pages	 ?	 ?about	 ?target	 ?restaurant	1. 
	 ?	 ?	 ?Calcula?ng	 ?Pfreq(cj)	2. 
	 ?	 ?Scaling	 ?Pfreq(cj)	3. 
Yahoo!	 ?Web	 ?search	 ?API 
?Azuma	 ?Sushi?	 ?	 ?	 ?Aichi	 ?	 ?	 ?restaurant	 ?(????????????)	 ?
Ranking:  2nd	
Search	 ?query	 ?	 ?
Number of 	cuisine types:  	3 
Figure 4: Overview of CMW calculation.
is represented by the four character types used in
the Japanese writing system: hiragana, katakana,
kanji (Chinese characters), and romaji (Roman let-
ters). For example, the restaurant name ?Maru
Sushi (????)? includes two character types:
?Maru (??)? is written in hiragana and ?Sushi
(??)? is written in kanji. Therefore, the fea-
ture values for hiragana and kanji are both 1, while
those for katakana and romaji are 0. Another ex-
ample is shown using the restaurant ?IB cafe (IB
???)?, in which the ?IB? part is romaji and the
?cafe (???)? part is katakana. Therefore, in this
case, the feature values of katakana and romaji are
1 and those of hiragana and kanji are 0.
We perform feature selection for the obtained
features set (Guyon and Elisseeff, 2003). The clas-
sifier needs to be built without overfitting because
we assume that a restaurant name as the input to
this module is unknown and does not exist in the
database. We use the mutual information (Peng
et al, 2005; Yang and Pedersen, 1997) between
each feature and the set of cuisine types as its cri-
terion. This represents how effective each feature
is for the classification. For example, in the fea-
tures obtained from the restaurant name ????
??, which is a Japanese restaurant, the 2-gram
feature ???? frequently co-occurs with the cui-
sine type ?Japanese restaurant?. This is an effec-
tive feature for the cuisine type estimation. In con-
trast, the 2-gram feature ???? is not effective be-
cause its co-occurrence with cuisine types is infre-
quent. Mutual information is calculated as
I(fk;C) =
?
cj?C
p(fk, cj) log
p(fk, cj)
p(fk)p(cj)
, (3)
where p(fk) is an occurrence probability of feature
fk in the database, p(cj) is an occurrence probabil-
ity of cuisine type cj (? C), and p(fk, cj) is a joint
probability of the feature and the cuisine type.
Features having lower mutual information val-
ues are removed until we deem that overfitting has
been avoided, specifically, when the estimation
accuracies become almost the same between the
closed and open tests. We confirm this by cross-
validations (CV) instead of open tests.
3.2 Estimation Using the Web
We estimate a restaurant?s cuisine type and calcu-
late CMs by using its frequency on the Web as
CMW . This is based on an assumption that a
restaurant?s name appears with its cuisine type on
Web pages. CMW is calculated in the following
steps, as shown in Figure 4.
1. Obtaining related Web pages:
Twenty pages per search query were ob-
tained, as this was the limit of the number of
pages when this experiment was performed.
We used the Yahoo! Web search API2. The
query is formed with the target restaurant
name and the following two words: ?Aichi
(??)? and ?restaurant (?????)?. The
two are added to narrow down the search re-
sult since our domain is a restaurant search
in Aichi prefecture. For example, the query
is ?<rest>???????? for the target
restaurant name <rest>.
2http://developer.yahoo.co.jp/webapi/search/websearch
/v2/web search.html
73
2. Calculating Pfreq(cj):
We count the frequency of each cuisine type
cj in the i-th Web pages, which are ranked
by the Web search API. We then sum up the
frequency through all the obtained pages and
calculate its posterior probability.
Pfreq(cj) =
?
i wi ? freqi(cj)?
cj
?
i wi ? freqi(cj)
(4)
Here, freqi(cj) is the frequency of cj in the
i-th page. Weight wi is calculated using two
factors, rank(i) and cuisine(i):
wi =
1
rank(i) ? cuisine(i) (5)
(a) rank(i): The ranking of pages in the
Web search API
We assume that a Web page is more re-
lated to the target restaurant if the Web
search API ranks it higher.
(b) cuisine(i): The number of cuisine
types in the i-th Web page
We assume that a Web page contain-
ing many different cuisine types does
not indicate one particular cuisine. For
example, a page on which only ?Chi-
nese restaurant? appears is more reliable
than that on which more cuisine types
(?Chinese restaurant?, ?Japanese restau-
rant?, ?Japanese pub?, and ?Western-
style restaurant?, for example) appear,
as a page indicating a ?Chinese restau-
rant?.
3. Scaling Pfreq(cj):
CMW is calculated by scaling each
Pfreq(cj) with the corresponding ?j . ?j
is a scaling coefficient that emphasizes the
differences among CMW : ?j is equal to
or smaller than 1 and becomes smaller as j
increases.
CMW (cj) =
?jPfreq(cj)?
cj ?jPfreq(cj)
(6)
?j = Pfreq(cj)/Pfreq(c1) (7)
3.3 Integration of CMs
We define CMI by integrating the two basic CMs:
CMD and CMW . Specifically, we integrate them
by the logistic regression (Hosmer Jr. et al, 2013)
shown in Equation (8). The optimal parameters,
i.e., weights for the CMs, are determined using a
data set with reference labels. The teacher signal
is 1 if the estimated cuisine type is correct and 0
otherwise.
CMI(cj) =
1
1 + exp(?f(cj))
(8)
f(cj) = wDCMD(cj) + wWCMW (cj) + w0
Here, wD and wW are the weights for CMD and
CMW , and w0 is a constant.
4 Experiment
We evaluate our method to obtain the CMs from
three aspects. First, we evaluate the effect of fea-
ture selection based on mutual information. Sec-
ond, we evaluate how the CMs were distributed
and whether they were appropriate measures for
question generation. Third, we determine the ef-
fectiveness of integrating the two basic CMs. In
this paper, we used a restaurant database in Aichi
prefecture containing 2,398 restaurants with 16
cuisine types.
4.1 Effect of Feature Selection Based on
Mutual Information
We determined whether overfitting could be
avoided by feature selection based on mutual in-
formation in the estimation using a database. We
regard overfitting to be avoided when estimation
accuracies become almost the same between the
closed and open tests. For the closed test, estima-
tion accuracy was calculated for all 2,398 restau-
rants in the database by using a classifier that was
trained with the same 2,398 restaurants. For the
open test, it was calculated by 10-fold CV for the
2,398 restaurants. This experiment is not for de-
termining a feature set but rather for determining
a feature selection ratio. That is, the feature se-
lection result is kept not as a feature set but as a
ratio. The resulting ratio is applied to the num-
ber of features appearing in another training data
(e.g., that in Section 4.2) and then the feature set
is determined.
Figure 5 shows the estimation accuracy of the
closed test and the 10-fold CV when the feature
selection was applied. The horizontal axis denotes
ratios of features used to train the classifier out of
20,679 features in total. They were selected in de-
scending order of mutual information. The ver-
tical axis denotes the estimation accuracy of the
74
 0.6
 0.7
 0.8
 0.9
 1
 1 10 100E
stim
ati
on
 ac
cu
rac
y (
%)
Feature selection ratio (%)
Closed10-fold CV
Figure 5: Estimation accuracies of closed test and
10-fold CV.
cuisine types. Figure 5 shows that, at first, over-
fitting occurs if all features were used for training;
that is, the feature selection ratio = 100%. This
can be seen by the difference in estimation accu-
racies, which was 28.1% between the closed test
and the 10-fold CV. The difference decreased as
the number of used features decreased, and almost
disappeared at feature selection ratio = 0.8%. In
these selected features, as an example, the 2-gram
?gyoza (??)?, which seems intuitively effective
for cuisine type estimation is, included3.
4.2 Evaluation for Distribution of CMs
We evaluate the distribution of CMs obtained with
the estimation results. Specifically, we evaluated
three types of distributions: CMD, CMW , and
CMI . We extracted 400 restaurants from the
database and used them as evaluation data. The
remaining 1,998 restaurants were used as training
data for the classifier to calculate CMD. In all
features obtained from these 1,998 restaurants, the
ME classifier uses 0.8% of them, which is the fea-
ture selection ratio based on the mutual informa-
tion determined in Section 4.1. That is, the feature
set itself obtained in the feature selection is not de-
livered into the evaluation in this section.
We used average distances between each CM
score and its reference as the criterion to evalu-
ate the distribution of the CMs. Generally, CMs
should be as highly scored as possible when the
estimation is correct and as lowly scored as possi-
ble otherwise. We calculate the distances over the
3?Gyoza (??)? is a kind of dumplings and one of the
most popular Chinese foods. It often appears in Chinese
restaurant names in Japan.
Table 3: Evaluation against each CM.
eval(CMx) MB(CMx)
CMD 0.31 0.37
CMW 0.28 0.32
CMI 0.25 0.28
400 estimation results.
eval(CMx) =
?N
i |CM ix ? ?ix|
N (9)
Here, N is the total number of the estimation re-
sult, so N = 400 in this paper. ?ix for CM ix is
defined as
?ix =
{
1, If estimation result i is correct
0, Otherwise (10)
Note that ?x depends on CMx because estimation
results differ depending on the CMx used.
We also set the majority baseline as Equation
(11). Here, all CMs are regarded as 0 or 1 in Equa-
tion (9). Because there were more correct estima-
tion results than incorrect ones, as shown in Table
2, we used 1 for the majority baseline, as
MB(CMx) =
?N
i |1 ? ?ix|
N . (11)
The results are shown in Table 3. A compar-
ison of the three eval(CMx) demonstrates that
the integrated CMI is the most appropriate in our
evaluation criterion because it is the lowest of the
three. The relative error reduction rates fromCMI
against CMD and CMW were 16% and 37%, re-
spectively. Each eval(CMx) outperformed the
corresponding majority baseline.
4.3 Effectiveness of Integrated CM
We verify the effectiveness of the CM integration
from another viewpoint. Specifically, we confirm
whether the number of correct estimation results
increases by integration.
First, we show the distribution of the three CMs
and whether they were correct or not in Table 2.
The bottom row of the table shows that CMI ob-
tained correct estimation results for 297 restau-
rants, which is the highest of the three CMs.
More specifically, we investigated how many
estimation results changed by using the three
CMs. Here, an estimation result means the cui-
sine type that is given the highest confidence. This
result is shown in Table 4, where C denotes a case
75
Table 2: Distribution of estimation results by CM values.
CMD CMW CMI
CM range Correct Incorrect Correct Incorrect Correct Incorrect
0.0 ? 0.1 0 0 0 32 2 10
0.1 ? 0.2 0 0 0 11 9 15
0.2 ? 0.3 1 16 14 22 15 18
0.3 ? 0.4 6 19 28 19 10 8
0.4 ? 0.5 11 25 29 21 13 12
0.5 ? 0.6 21 29 56 9 13 12
0.6 ? 0.7 22 28 85 7 15 7
0.7 ? 0.8 41 16 42 3 17 6
0.8 ? 0.9 21 9 19 1 19 9
0.9 ? 1.0 131 4 1 1 184 10
Total 254 146 274 124 297 103
Table 4: Estimation results by three CMs.
CMD / CMW
I / I I / C C / I C / C
C 0 51 33 213
CMI I 85 10 8 0
C: correct, I: incorrect
when a cuisine type was correctly estimated and I
denotes that it was not. The four columns with ?/?
denote the numbers of estimation results forCMD
and CMW . For example, the C/I column denotes
that estimation results based on the database were
correct and those using the Web were incorrect,
that is, the I/C and C/I columns mean that the
two estimation results differed. The table shows
that 102 of 400 restaurants corresponded to these
cases, that is, either of the two estimation results
was incorrect. It also shows that estimation results
for 84 of the 102 (82%) restaurants became correct
by the integration.
Two examples are shown for which the esti-
mation results became correct by the integration.
First, ?Kaya (??)? is a restaurant name whose
cuisine type is ?Japanese-style pancake?. Its cui-
sine type was correctly estimated by CMW while
it was incorrectly estimated as ?Japanese pub? by
CMD. This was because, in Japanese, ?Kaya (?
?)? has no special meaning associated with spe-
cific cuisine types. Thus, it is natural that its cui-
sine type was incorrectly estimated from the word
and character distribution of the name. On the
other hand, when Web pages about it were found,
?Japanese-style pancake? co-occurs frequently in
the obtained pages, and thus it was correctly es-
timated by CMW . Second, ?Tama-Sushi Imaike
(??? ??)? is a restaurant name whose cui-
sine type is ?Japanese restaurant?. Its cuisine type
was estimated correctly by CMD while it was in-
correctly estimated as ?Japanese pub? by CMW .
CMD was effective in this case because the part
of ?Sushi (??)? indicates a Japanese cuisine. No
Web pages for it were found indicating its cuisine
type correctly, and thus CMW failed to estimate
it.
5 Conclusion
Our aim is to acquire the attributes of an unknown
word?s concept from the user through dialogue.
Specifically, we set restaurant cuisine type as the
attribute to obtain and showed how to generate
specific questions based on the estimated CM. We
use two estimation methods: one based on the tar-
get database and the other on the Web. A more
appropriate CM was generated in terms of its dis-
tribution and estimation accuracy by integrating
these two CMs.
There is little prior research on obtaining and
updating system knowledge through dialogues,
with the notable exception of the knowledge au-
thoring system of (Knott and Wright, 2003). Their
system also uses the user?s text input for construct-
ing the system knowledge from scratch, which is
used to generate simple stories. Our study is dif-
ferent in two points: (1) we focus on generating
several kinds of questions because we use ASR,
and (2) we try to handle unknown words, which
will be stored in the target database to be used in
future dialogues.
We should point out that these kinds of ques-
tions can be generated only when the types of un-
known concepts are given. We assume the type
of unknown concepts is already known and thus
the attributes to be asked are also known. More
specifically, we assume that the concept denoted
by an unknown word is a restaurant name and its
attributes are also known. The cuisine type has
been estimated as one of the attributes. However,
76
when the type is unknown, the system first needs
to identify its attributes to ask. That is, the sys-
tem first needs to ask about its supertype and then
to ask about attributes that are typical for objects
of this type. This issue needs to be addressed in
order for the system to acquire arbitrary new con-
cepts. This paper has shown the first step for ob-
taining concepts through dialogues by generating
questions. Many issues remain in this field for fu-
ture work.
References
Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Computa-
tional Linguistics, 22(1):39?71.
Isabelle Guyon and Andre? Elisseeff. 2003. An intro-
duction to variable and feature selection. The Jour-
nal of Machine Learning Research, 3:1157?1182.
David W. Hosmer Jr., Stanley Lemeshow, and Rod-
ney X. Sturdivant. 2013. Applied logistic regres-
sion. Wiley. com.
Preethi Jyothi, Leif Johnson, Ciprian Chelba, and Brian
Strope. 2012. Large-scale discriminative language
model reranking for voice search. In Proceedings
of the NAACL-HLT 2012 Workshop: Will We Ever
Really Replace the N-gram Model? On the Future
of Language Modeling for HLT, pages 41?49.
Gillian Kay. 1995. English loanwords in Japanese.
World Englishes, 14(1):67?76.
Alistair Knott and Nick Wright. 2003. A dialogue-
based knowledge authoring system for text genera-
tion. In AAAI Spring Symposium on Natural Lan-
guage Generation in Spoken and Written Dialogue,
Stanford, CA, pages 71?78.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to
Japanese morphological analysis. In Proceedings of
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-2004), pages 230?237.
Helen Meng, P. C. Ching, Shuk Fong Chan, Yee Fong
Wong, and Cheong Chat Chan. 2004. ISIS:
An adaptive, trilingual conversational system with
interleaving interaction and delegation dialogs.
ACM Transactions on Computer-Human Interac-
tion, 11(3):268?299.
Marius Pasca, Dekang Lin, Jeffrey Bigham, Andrei
Lifchits, and Alpa Jain. 2006. Organizing and
searching the World Wide Web of facts - step one:
the one-million fact extraction challenge. In Pro-
ceedings of the 21st National Conference on Artifi-
cial intelligence - Volume 2, AAAI ?06, pages 1400?
1405. AAAI Press.
Hanchuan Peng, Fuhui Long, and Chris Ding. 2005.
Feature selection based on mutual information cri-
teria of max-dependency, max-relevance, and min-
redundancy. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 27(8):1226?1238.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of the Thirteenth Conference on Com-
putational Natural Language Learning, pages 147?
155.
Yasuhiro Takahashi, Kohji Dohsaka, and Kiyoaki
Aikawa. 2002. An efficient dialogue control
method using decision tree-based estimation of out-
of-vocabulary word attributes. In Proc. Int?l Conf.
Spoken Language Processing (ICSLP), pages 813?
816.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 shared task:
Language-independent named entity recognition. In
Proceedings of the seventh conference on Natural
language learning at HLT-NAACL 2003-Volume 4,
pages 142?147.
Yiming Yang and Jan O Pedersen. 1997. A compara-
tive study on feature selection in text categorization.
In ICML, volume 97, pages 412?420.
Naoki Yoshinaga and Kentaro Torisawa. 2007.
Open-domain attribute-value acquisition from semi-
structured texts. In Proceedings of the Workshop
of OntoLex07 - From Text to Knowledge: The Lexi-
con/Ontology Interface, pages 55?66.
Guo Dong Zhou and Jian Su. 2002. Named entity
recognition using an HMM-based chunk tagger. In
Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics, pages 473?
480.
77
