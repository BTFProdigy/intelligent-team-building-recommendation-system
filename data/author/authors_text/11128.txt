Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 25?32,
Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language Processing
Aggregating Machine Learning and Rule Based Heuristics for Named 
Entity Recognition 
 Karthik Gali, Harshit Surana, Ashwini Vaidya, Praneeth Shishtla and  
Dipti Misra Sharma 
Language Technologies Research Centre, 
International Institute of Information Technology, 
Hyderabad, India. 
karthikg@students.iiit.ac.in, surana.h@gmail.com,  
ashwini_vaidya@research.iiit.ac.in, praneethms@students.iiit.ac.in, 
dipti@iiit.ac.in 
 
 
 
Abstract 
This paper, submitted as an entry for the 
NERSSEAL-2008 shared task, describes a 
system build for Named Entity Recognition 
for South and South East Asian Languages.  
Our paper combines machine learning 
techniques with language specific heuris-
tics to model the problem of NER for In-
dian languages. The system has been tested 
on five languages: Telugu, Hindi, Bengali, 
Urdu and Oriya. It uses CRF (Conditional 
Random Fields) based machine learning, 
followed by post processing which in-
volves using some heuristics or rules. The 
system is specifically tuned for Hindi and 
Telugu, we also report the results for the 
other four languages. 
1 Introduction 
Named Entity Recognition (NER) is a task that 
seeks to locate and classify entities (?atomic ele-
ments?) in a text into predefined categories such as 
the names of persons, organizations, locations, ex-
pressions of times, quantities, etc. It can be viewed 
as a two stage process: 
  
1. Identification of entity boundaries 
2. Classification into the correct category 
 
For example, if ?Mahatma Gandhi? is a named 
entity in the corpus, it is necessary to identify the 
beginning and the end of this entity in the sentence. 
Following this step, the entity must be classified 
into the predefined category, which is NEP 
(Named Entity Person) in this case. 
This task is the precursor for many natural lan-
guage processing applications. It has been used in 
Question Answering (Toral et al 2005) as well as 
Machine Translation (Babych et al 2004). 
The NERSSEAL contest has used 12 categories 
of named entities to define a tagset. The data has 
been manually tagged for training and testing pur-
poses for the contestants. 
The task of building a named entity recognizer 
for South and South East Asian languages presents 
several problems related to their linguistic charac-
teristics. We will first discuss some of these lin-
guistic issues, followed by a description of the 
method used. Further, we show some of the heuris-
tics used for post-processing and finally an analy-
sis of the results obtained.  
2 Previous Work  
The linguistic methods generally use rules 
manually written by linguists. There are several 
rule based NER systems, containing mainly lexi-
calized grammar, gazetteer lists, and list of trigger 
words, which are capable of providing upto 92% f-
measure accuracy for English (McDonald, 1996; 
Wakao et al, 1996).  
Linguistic approach uses hand-crafted rules 
which need skilled linguistics. The chief disadvan-
tage of these rule-based techniques is that they re-
quire huge experience and grammatical knowledge 
of the particular language or domain and these sys-
tems are not transferable to other languages or do-
mains. However, given the closer nature of many 
Indian languages, the cost of adaptation of a re-
25
source from one language to another could be quite 
less (Singh and Surana, 2007). 
Various machine learning techniques have also 
been successfully used for the NER task. Generally 
hidden markov model (Bikel et al,1997), maxi-
mum entropy (Borthwick, 1999), conditional ran-
dom field (Li and Mccallum, 2004) are more popu-
lar machine learning techniques used for the pur-
pose of NER. 
Hybrid systems have been generally more effec-
tive at the task of NER. Given lesser data and more 
complex NE classes which were present in 
NERSSEAL shared task, hybrid systems make 
more sense. Srihari et al (2000) combines MaxEnt, 
hidden markov model (HMM) and handcrafted 
rules to build an NER system. 
Though not much work has been done for other 
South Asian languages, some previous work fo-
cuses on NER for Hindi. It has been previously 
attempted by Cucerzan and Yarowsky in their lan-
guage independent NER work which used morpho-
logical and contextual evidences (Cucerzan and 
Yarowsky, 1999). They ran their experiment with 
5 different languages. Among these the accuracy 
for Hindi was the worst. For Hindi the system 
achieved 42% f-value with a recall of 28% and 
about 85% precision. A result which highlights 
lack of good training data, and other various issues 
involved with linguistic handling of Indian lan-
guages. 
Later approaches have resulted in better results 
for Hindi. Hindi NER system developed by Wei Li 
and Andrew Mccallum (2004) using conditional 
random fields (CRFs) with feature induction have 
achieved f-value of 71%. (Kumar and Bhat-
tacharyya, 2006) used maximum entropy markov 
model to achieve f-value of upto 80%. 
3 Some Linguistic Issues 
3.1 Agglutinative Nature 
Some of the SSEA languages have agglutinative 
properties.  For example, a Dravidian language like 
Telugu has a number of postpositions attached to a 
stem to form a single word. An example is: 
 
guruvAraMwo = guruvAraM + wo  
up to Wednesday = Wednesday + up to 
 
Most of the NERs are suffixed with several dif-
ferent postpositions, which increase the number of 
distinct words in the corpus.  This in turn affects 
the machine learning process. 
3.2 No Capitalization 
All the five languages have scripts without graphi-
cal cues like capitalization, which could act as an 
important indicator for NER.  For a language like 
English, the NER system can exploit this feature to 
its advantage. 
3.3 Ambiguity 
One of the properties of the named entities in these 
languages is the high overlap between common 
names and proper names. For instance Kamal (in 
Hindi) can mean ?lotus?, which is not a named en-
tity, but it can also be a person?s name, in which 
case, it is a named entity. 
Among the named entities themselves, there is 
ambiguity between a location name Bangalore ek 
badzA shaher heI (Bangalore is a big city) or a per-
son?s surname ?M. Bangalore shikshak heI? (M. 
Bangalore is a teacher). 
3.4 Low POS Tagging Accuracy for Nouns 
For English, the available tools like POS (Part of 
Speech) tagger can be used to provide features for 
machine learning. This is not very helpful for 
SSEA languages because the accuracy for noun 
and proper noun tags is quite low (PVS and G., 
2006) Hence, features based on POS tags cannot 
be used for NER for these languages. 
To illustrate this difficulty, we conducted the 
following experiment. A POS tagger (described in 
PVS & G.,2006) was run on the Hindi test data.  
The data had 544 tokens with NEL, NEP, NEO 
tags.  The POS tagger should have given the NNP 
(proper noun) tag for all those named entities. 
However the tagger was able to tag only 80 tokens 
accurately. This meant that only 14.7% of the 
named entities were correctly recognized. 
3.5 Spelling Variation 
One other important language related issue is the 
variation in the spellings of proper names. For in-
stance the same name Shri Ram Dixit can be writ-
ten as Sri. Ram Dixit, Shree Ram Dixit, Sh. R. Dixit 
and so on. This increases the number of tokens to 
be learnt by the machine and would perhaps also 
require a higher level task like co-reference resolu-
tion. 
 
26
2.6 Pattern of suffixes We have converted this format into the BIO 
format as described in Ramshaw et. al. For exam-
ple, the above format will now be shown as: 
 
Named entities of Location (NEL) or Person 
(NEP) will share certain common suffixes, which 
can be exploited by the learning algorthm. For in-
stance, in Hindi, -pur (Rampur, Manipur) or -giri 
(Devgiri) are suffixes that will appear in the named 
entities for Location. Similarly, there are suffixes 
like -swamy (Ramaswamy, Krishnaswamy) or -
deva (Vasudeva, Mahadeva) which can be com-
monly found in named entities for person. These 
suffixes are cues for some of the named entities in 
the SSEA languages. 
 
Rabindranath  B-NEP 
Tagore   I-NEP 
ne   O 
kahaa   O 
 
The training data set contains (approximately) 
400,000 Hindi, 50,000 Telugu, 35,000 Urdu, 
93,000 Oriya and 120,000 Bengali words respec-
tively.  
A NER system can be rule-based, statistical or 
hybrid. A rule-based NER system uses hand-
written rules to tag a corpus with named entities. A 
statistical NER system learns the probabilities of 
named entities using training data, whereas hybrid 
systems use both. 
5 Conditional Random Fields 
Conditional Random Fields (CRFs) are undirected 
graphical models used to calculate the conditional 
probability of values on designated output nodes 
given values assigned to other designated input 
nodes. Developing rule-based taggers for NER can be cumbersome as it is a language specific process. 
Statistical taggers require large amount of anno-
tated data (the more the merrier) to train.  Our sys-
tem is a hybrid NER tagger which first uses Condi-
tional Random Fields (CRF) as a machine learning 
technique followed by some rule based post-
processing. 
In the special case in which the output nodes of 
the graphical model are linked by edges in a linear 
chain, CRFs make a first-order Markov independ-
ence assumption, and thus can be understood as 
conditionally-trained finite state machines (FSMs). 
Let o = (o,,o
We treat the named entity recognition problem 
as a sequential token-based tagging problem. 
According to Lafferty et. al. CRF outperforms 
other Machine Learning algorithms viz., Hidden 
Markov Models (HMM), Maximum Entropy 
Markov Model (MEMM) for  sequence labeling 
tasks.  
4 Training data 
The training data given by the organizers was in 
SSF format1. For example in SSF format, the 
named entity ?Rabindranath Tagore? will be shown 
in the following way: 
0 (( SSF 
1  ((  NP  <ne=NEP> 
1.1  Rabindranath 
1.2 Tagore 
)) 
2 ne 
3 kahaa 
 )) 
 
                                                          
1 http://shiva.iiit.ac.in/SPSAL2007/ssf-analysis-representation.pdf
2,o3 ,o4 ,... oT  ) be some observed in-
put data sequence, such as a sequence of words in 
text in a document,(the values on n input nodes of 
the graphical model). Let S be a set of FSM states, 
each of which is associated with a label, l ? ?. 
Let s = (s ,s ,s  ,s  ,... s1 2 3 4 T ) be some sequence of 
states, (the values on T output nodes). By the 
Hammersley-Clifford theorem, CRFs define the 
conditional probability of a state sequence given an 
input sequence to be: 
 
where Zo is a normalization factor over all state 
sequences is an arbitrary feature function over its 
arguments, and ?k is a learned weight for each fea-
ture function. A feature function may, for example, 
be defined to have value 0 or 1. Higher ? weights 
make their corresponding FSM transitions more 
likely. CRFs define the conditional probability of a 
label sequence based on the total probability over 
the state sequences, 
 
 
27
 
where l(s) is the sequence of labels correspond-
ing to the labels of the states in sequence s. 
Note that the normalization factor, Zo, (also 
known in statistical physics as the partition func-
tion) is the sum of the scores of all possible states. 
 
And that the number of state sequences is expo-
nential in the input sequence length, T. In arbitrar-
ily-structured CRFs, calculating the partition func-
tion in closed form is intractable, and approxima-
tion methods such as Gibbs sampling or loopy be-
lief propagation must be used. In linear-chain 
structured CRFs (in use here for sequence model-
ing), the partition function can be calculated effi-
ciently by dynamic programming. 
6 CRF Based Machine Learning 
We used the CRF model to perform the initial tag-
ging followed by post-processing. 
6.1 Statistical Tagging 
In the first phase, we have used language inde-
pendent features to build the model using CRF. 
Orthographic features (like capitalization, decimals), 
affixes (suffixes and prefixes), context (previous 
words and following words), gazetteer features, POS 
and morphological features etc. are generally used for 
NER. In English and some other languages, capitali-
zation features play an important role as NEs are 
 generally capitalized for these languages. Unfortu-
nately as explained above this feature is not applica-
ble for the Indian languages. 
Precision Recall F-Measure  
Pm Pn Pl Rm Rn Rl Fm Fn Fl  
Bengali 53.34 49.28 58.27 26.77 25.88 31.19 35.65 33.94 40.63 
Hindi 59.53 63.84 64.84 41.21 41.74 40.77 48.71 50.47 50.06 
Oriya 39.16 40.38 63.70 23.39 19.24 28.15 29.29 26.06 39.04 
Telugu 10.31 71.96 65.45 68.00 30.85 29.78 08.19 43.19 40.94 
Urdu 43.63 44.76 48.96 36.69 34.56 39.07 39.86 39.01 43.46 
Table 1: Evaluation of the NER System for Five Languages 
The exact set of features used are described be-
low. 
6.2 Window of the Words 
Words preceding or following the target word may 
be useful for determining its category. Following a 
few trials we found that a suitable window size is 
five. 
6.3 Suffixes 
Statistical suffixes of length 1 to 4 have been con-
sidered. These can capture information for named 
entities having the NEL tag like Hyderabad, 
Secunderabad, Ahmedabad etc., all of which end 
in -bad. We have collected lists of such suffixes for 
NEP (Named Entity Person) and NEL (Named En-
tity Location) for Hindi. In the machine learning 
model, this resource can be used as a binary fea-
ture. A sample of these lists is as follows: 
 
Type of NE Example suffixes 
(Hindi) 
NE- Location -desa, -vana, -nagara,  
-garh, -rashtra, -giri  
NE ? Person -raja, -natha, -lal, -bhai,-
pathi, -krishnan 
 Table 2: Suffixes for Hindi NER 
28
7 Heuristics Based Post Processing 6.4 Prefixes 
Statistical prefixes of length 1 to 4 have been con-
sidered. These can take care of the problems asso-
ciated with a large number of distinct tokens. As 
mentioned earlier, agglutinative languages can 
have a number of postpositions. The use of pre-
fixes will increase the probability of   Hyderabad 
and Hyderabadlo (Telugu for ?in Hyderabad?) be-
ing treated as the same token. 
Complex named entities like fifty five kilograms 
contain a Named Entity Number within a Named 
Entity Measure. We observed that these were not 
identified accurately enough in the machine learn-
ing based system. Hence, instead of applying ma-
chine learning to handle nested entities we make 
use of rule-based post processing.  
7.1 Second Best Tag 
Table 3: F-Measure (Lexical) for NE Tags 
 Bengali Hindi Oriya Telugu Urdu 
It was observed that the recall of the CRF model is 
low. In order to improve recall, we have used the 
following rule:  if the best tag given by the CRF 
model is O (not a named entity) and the confidence 
of the second best tag is greater than 0.15, then the 
second best tag is considered as the correct tag. 
NEP 35.22 54.05 52.22 01.93 31.22 
NED NA 42.47 01.97 NA 21.27 
NEO 11.59 45.63 14.50 NA 19.13 
NEA NA 61.53 NA NA NA 
We observed an increase of 7% in recall and 3% 
decrease in precision. This resulted in a 4% in-
crease in the F-measure, which is a significant in-
crease in performance. The decrease in precision is 
expected as we are taking the second tag. 
NEB NA NA NA NA NA 
NETP 42.30 NA NA NA NA 
NETO 33.33 13.77 NA 01.66 NA 
NEL 45.27 62.66 48.72 01.49 57.85 
7.2 Nested Entities NETI 55.85 79.09 40.91 71.35 63.47 
NEN 62.67 80.69 24.94 83.17 13.75 One of the important tasks in the contest was to 
identify nested named entities. For example if we 
consider eka kilo (Hindi: one kilo) as NEM 
(Named Entity Measure), it contains a NEN 
(Named Entity Number) within it. 
NEM 60.51 43.75 19.00 26.66 84.10 
NETE 19.17 31.52 NA 08.91 NA
The CRF model tags eka kilo as NEM and in or-
der to tag eka as NEN we have made use of other 
resources like a gazetteer for the list of numbers. 
We used such lists for four languages. 
6.5 Start of a sentence 
There is a possibility of confusing the NEN 
(Named Entity Number) in a sentence with the 
number that appears in a numbered list. The num-
bered list will always have numbers at the begin-
ning of a sentence and hence a feature that checks 
for this property will resolve the ambiguity with an 
actual NEN. 
7.3 Gazetteers 
For Hindi, we made use of three different kinds of 
gazetteers. These consisted of lists for measures 
(entities like kilogram, millimetre, lakh), numerals 
and quantifiers (one, first, second) and time ex-
pressions (January, minutes, hours) etc. Similar 
lists were used for all the other languages except 
Urdu. These gazetteers were effective in identify-
ing this relatively closed class of named entities 
and showed good results for these languages. 
6.6 Presence of digits 
Usually, the presence of digits indicates that the 
token is a named entity. For example, the tokens 
92, 10.1 will be identified as Named Entity Num-
ber based on the binary feature ?contains digits?. 
6.7 Presence of  four digits 8 Evaluation 
If the token is a four digit number, it is likelier to 
be a NETI (Named Entity Time). For example, 
1857, 2007 etc. are most probably years. 
The evaluation measures used for all the five lan-
guages are precision, recall and F-measure. These 
measures are calculated in three different ways: 
 
29
1. Maximal Matches: The largest possible 
named entities are matched with the refer-
ence data. 
The amount of annotated corpus available for 
Hindi was substantially more. This should have 
ideally resulted in better results for Hindi with the 
machine learning approach. But, the results were 
only marginally better than other languages. A ma-
jor reason for this was that a very high percentage 
(44%) of tags in Hindi were NETE. The tagset 
gives examples like ?Horticulture?, ?Conditional 
Random Fields? for the tag NETE. It has also been 
mentioned that even manual annotation is harder 
for NETE as it is domain specific. This affected the 
overall results for Hindi because the performance 
for NETE was low (Table 3). 
2. Nested Matches: The largest possible as 
well as nested named entities are matched. 
3. Lexical Item Matches: The lexical items 
inside largest possible named entities are 
matched. 
9 Results 
The results of evaluation as explained in the previ-
ous section are shown in the Table-1. The F-
measures for nested lexical match are also shown 
individually for each named entity tag separately in 
Table-3 
 Num of 
NE tokens
Num of 
known NE 
% of un-
known NE
Bengali 1185 277 23.37 
10 Unknown Words Hindi 1120 417 37.23 
Table 4 shows the number of unknown words pre-
sent in the test data when compared with the train-
ing data. 
Oriya 1310 563 42.97 
Telugu 1150 145 12.60 
First column shows the number of unique 
Named entity tags present in the test data for each 
language. Second column shows the number of 
unique known named entities present in the test 
data. Third column shows the percentage of unique 
unknown words present in the test data of different 
languages when compared to training data. 
Urdu 631 179 28.36 
Table 4: Unknown Word 
 
Also, the F-measures of NEN, NETI, and NEM 
could have been higher because they are relatively 
closed classes. However, certain NEN can be am-
biguous (Example: eka is a NEN for ?one? in 
Hindi, but in a different context it can be a non-
number. For instance eka-doosra is Hindi for ?each 
other?). 
11 Error Analysis 
We can observe from the results that the maximal 
F-measure for Telugu is very low when compared 
to lexical F-measure and nested F-measure. The 
reason is that the test data of Telugu contains a 
large number of long named entities (around 6 
words), which in turn contain around 4 - 5 nested 
named entities. Our system was able to tag nested 
named entities correctly unlike maximal named 
entity. 
In a language like Telugu, NENs will appear as 
inflected words. For example 2001lo, guru-
vaaramto. 
10     Conclusion and Further Work 
In this paper we have presented the results of using 
a two stage hybrid approach for the task of named 
entity recognition for South and South East Asian 
Languages. We have achieved decent Lexical F-
measures of 40.63, 50.06, 39.04, 40.94, and 43.46 
for Bengali, Hindi, Oriya, Telugu and Urdu respec-
tively without using many language specific re-
sources. 
We can also observe that the maximal F-
measure for Telugu is very low when compared to 
other languages. This is because Telugu test data 
has very few known words. 
Urdu results are comparatively low chiefly be-
cause gazetteers for numbers and measures were 
unavailable.  
We plan to extend our work by applying our 
method to other South Asian languages, and by 
using more language specific constraints and re-
sources. We also plan to incorporate semi-
supervised extraction of rules for NEs (Saha et. al, 
30
2008) and use transliteration techniques to produce 
Indian language gazetteers (Surana and Singh, 
2008). Use of character models for increasing the 
lower recalls (Shishtla et. al, 2008) is also under-
way. We also plan to enrich the Indian dependency 
tree bank (Begum et. al, 2008) by use of our NER 
system. 
 
11 Acknowledgments 
 
   We would like to thank the organizer Mr. Anil 
Kumar Singh deeply for his continuous support 
during the shared task.  
References 
B. Babych, and A. Hartley, Improving Machine transla-
tion Quality with Automatic Named Entity Recognition. 
www.mt-archive.info/EAMT-2003- Babych.pdf 
Rafiya Begum, Samar Husain, Arun Dhwaj, Dipti Misra 
Sharma, Lakshmi Bai, and Rajeev Sangal. 2008. De-
pendency annotation scheme for Indian languages. In 
Proceedings of IJCNLP-2008, Hyderabad, India. 
M. Bikel Daniel, Miller Scott, Schwartz Richard and 
Weischedel Ralph. 1997. Nymble: A High Perfor 
mance Learning Name-finder. In Proceedings of the 
Fifth Conference on Applied Natural Language 
Processing. 
S. Cucerzan, and D. Yarowsky, 1999. Language inde-
pendent named entity recognition combining mor-
phological and contextual evidence. Proceedings of 
the Joint SIGDAT Conference on EMNLP and VLC. 
N. Kumar and Pushpak Bhattacharyya. 2006. Named 
Entity Recognition in Hindi using MEMM. In Tech-
nical Report, IIT Bombay, India. 
John Lafferty, Andrew McCallum and Fernando          
Pereira. 2001. Conditional Random Fields: Probabil-
istic Models for Segmenting and Labeling Sequence 
Data. Proc.   18th International Conf. on Machine 
Learning. 
D. McDonald 1996. Internal and external evidence in 
the identification and semantic categorization of 
proper names. In B. Boguraev and J. Pustejovsky, 
editors, Corpus Processing for Lexical Acquisition. 
Avinesh PVS and Karthik G. Part-Of-Speech Tagging 
and Chunking Using Conditional Random Fields and 
Transformation Based Learning. Proceedings of the 
SPSAL workshop during IJCAI?07. 
Lance Ramshaw and Mitch Marcus. Text Chunking 
Using Transformation-Based Learning. Proceedings 
of the Third Workshop on Very Large Corpora. 
S.K. Saha , S. Chatterji , S. Dandapat , S. Sarkar  and P. 
Mitra 2008. A Hybrid Approach for Named Entity 
Recognition in Indian Languages. In Proceedings of 
IJCNLP Workshop on NER for South and South East 
Asian Languages. 
Fei Sha and Fernando Pereira. 2003. Shallow Parsing 
with Conditional Random Fields. In the Proceedings 
of HLT-NAACL. 
P. Shishtla, P. Pingali , V. Varma  2008. A Character n-
gram Based Approach for Improved Recall in Indian 
Language NER. In Proceedings of IJCNLP Work-
shop on NER for South and South East Asian Lan-
guages. 
Cucerzan Silviu and Yarowsky David. 1999. Language 
Independent Named Entity Recognition Combining 
Morphological and Contextual Evidence. In Proceed-
ings of the Joint SIGDAT Conference on EMNLP and 
VLC. 
A. K. Singh and H. Surana  Can Corpus Based Meas-
ures be Used for Comparative Study of Languages? 
In Proceedings of Ninth Meeting of the ACL Special 
Interest Group in Computational Morphology and 
Phonology. ACL. 2007. 
R. Srihari, C. Niu and W. Li  2000. A Hybrid Approach 
for Named Entity and Sub-Type Tagging. In Pro-
ceedings of the sixth conference on Applied natural 
language processing. 
H. Surana and A. K. Singh 2008. A More Discerning 
and Adaptable Multilingual Transliteration Mecha-
nism for Indian Languages. In Proceedings of the 
Third International Joint Conference on Natural 
Language Processing. 
Charles Sutton, An Introduction to Conditional Random 
Fields for Relational Learning. 
T. Wakao , R. Gaizauskas  and Y. Wilks 1996. Evalua-
tion of an algorithm for the recognition and classifi-
cation of proper names. In Proceedings of COLING. 
 
Li Wei and McCallum Andrew. 2004. Rapid Develop-
ment of Hindi Named Entity Recognition using Con-
ditional Random Fields and Feature Induction. In 
ACM Transactions on Computational Logic. 
CRF++:.Yet another Toolkit. 
http://crfpp.sourceforge.net/ 
 
 
31
 32
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 82?90,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
PropBank Annotation of Multilingual Light Verb Constructions 
 
 
Jena D. Hwang1, Archna Bhatia3, Clare Bonial1, Aous Mansouri1,  
Ashwini Vaidya1, Nianwen Xue2, and Martha Palmer1 
1Department of Linguistics, University of Colorado at Boulder, Boulder CO 80309 
2Department of Computer Science, Brandeis University, Waltham MA 02453 
3Department of Linguistics, University of Illinois at Urbana-Champaign, Urbana IL 61801 
{hwangd,claire.bonial,aous.mansouri,ashwini.vaidya,martha.palmer} 
@colorado.edu, bhatia@illinois.edu, xuen@brandeis.edu 
 
  
 
Abstract 
In this paper, we have addressed the task 
of PropBank annotation of light verb 
constructions, which like multi-word 
expressions pose special problems. To 
arrive at a solution, we have evaluated 3 
different possible methods of annotation. 
The final method involves three passes: 
(1) manual identification of a light verb 
construction, (2) annotation based on the 
light verb construction?s Frame File, and 
(3) a deterministic merging of the first 
two passes. We also discuss how in 
various languages the light verb 
constructions are identified and can be 
distinguished from the non-light verb 
word groupings.  
1 Introduction  
One of the aims in natural language processing, 
specifically the task of semantic role labeling 
(SRL), is to correctly identify and extract the 
different semantic relationships between words 
in a given text. In such tasks, verbs are 
considered important, as they are responsible for 
assigning and controlling the semantic roles of 
the arguments and adjuncts around it. Thus, the 
goal of the SRL task is to identify the arguments 
of the predicate and label them according to their 
semantic relationship to the predicate (Gildea 
and Jurafsky, 2002; Pradhan et al, 2003).  
To this end, PropBank (Palmer et. al., 2005) 
has developed semantic role labels and labeled 
large corpora for training and testing of 
supervised systems. PropBank identifies and 
labels the semantic arguments of the verb on a 
verb-by-verb basis, creating a separate Frame 
File that includes verb specific semantic roles to 
account for each subcategorization frame of the 
verb. It has been shown that training supervised 
systems with PropBank?s semantic roles for 
shallow semantic analysis yield good results (see 
CoNLL 2005 and 2008).  
However, semantic role labeling tasks are 
often complicated by multiword expressions 
(MWEs) such as idiomatic expressions (e.g., 
?Stop pulling my leg!?), verb particle 
constructions (e.g., ?You must get over your 
shyness.?), light verb constructions (e.g., ?take a 
walk?, ?give a lecture?), and other complex 
predicates (e.g., V+V predicates such as Hindi?s 
???? ??? nikal gayaa, lit. ?exit went?, means 
?left? or ?departed?). MWEs that involve verbs 
are especially challenging because the 
subcategorization frame of the predicate is no 
longer solely dependent on the verb alone. 
Rather, in many of these cases the argument 
structure is assigned by the union of two 
predicating elements. Thus, it is important that 
the manual annotation of semantic roles, which 
will be used by automatic SRL systems, define 
and label these MWEs in a consistent and 
effective manner. 
In this paper we focus on the PropBank 
annotation of light verb constructions (LVCs). 
We have developed a multilingual schema for 
annotating LVCs that takes into consideration the 
similarities and differences shared by the 
construction as it appears in English, Arabic, 
Chinese, and Hindi. We also discuss in some 
detail the practical challenges involved in the 
crosslinguistic analysis of LVCs, which we hope 
will bring us a step closer to a unified 
crosslinguistic analysis.    
Since NomBank, as a companion to 
PropBank, provides corresponding semantic role 
82
labels for noun predicates (Meyers et al, 2004), 
we would like to take advantage of NomBank?s 
existing nominalization Frame Files and 
annotations as much as possible.  A question that 
we must therefore address is, ?Are 
nominalization argument structures exactly the 
same whether or not they occur within an LVC?? 
as will be discussed in section 6.1. 
2 Identifying Light Verb Constructions 
Linguistically LVCs are considered a type of a 
complex predicate. Many studies from differing 
angles and frameworks have characterized 
complex predicates as a fusion of two or more 
predicative elements. For example, Rosen (1997) 
treats complex structures as complementation 
structures, where the argument structure of 
elements in a complex predicate are fused 
together.  Goldberg (1993) takes a constructional 
approach to complex predicates and arrives at an 
analysis that is comparable to viewing complex 
predicates as a single lexical item. Similarly, 
Mohanan (1997) assumes different levels of 
linguistic representation for complex predicates 
in which the elements, such as the noun and the 
light verb, functionally combine to give a single 
clausal nucleus. Alsina (1997) and Butt (1997) 
suggest that complex predicates may be formed 
by syntactically independent elements whose 
argument structures are brought together by a 
predicate composition mechanism.  
While there is no clear-cut definition of LVCs, 
let alne the whole range of complex predicates, 
for the purposes of this study, we have adapted 
our approach largely from Butt?s (2004) criteria 
for defining LVCs. LVCs are characterized by a 
light verb and a predicating complement 
(henceforth, true predicate) that ?combine to 
predicate as a single element.? (Ibid.) In LVC, 
the verb is considered semantically bleached in 
such a way that the verb does not hold its full 
predicating power. Thus, the light verb plus its 
true predicate can often be paraphrased by a 
verbal form of the true predicate without loss of 
the core meaning of the expression. For example, 
the light verb ?gave? and the predicate ?lecture? 
in ?gave a lecture?, together form a single 
predicating unit such that it can be paraphrased 
by ?lectured?. 
True predicates in LVCs can be a noun (the 
object of the verb or the object of the preposition 
in a prepositional phrase), an adjective, or a verb. 
One light verb plus true predicate combination 
found commonly across all our PropBank 
languages (i.e., English, Arabic, Chinese, and 
Hindi) is the noun as the object of the verb as in 
?Sara took [a stroll] along the beach?. In Hindi, 
true predicates can be adjectives or verbs, in 
addition to the nouns. 
??? ? ??? [?????]  ???         (Adjective) 
to-me  you [nice]  seem 
lit. ?You seem nice to me? 
'You (are) liked to me (=I like you).' 
?????  ?? ???  [??] ????   (Verb) 
I-ERG everything  [do] took 
lit. ?I took do everything? 
'I have done everything.' 
As for Arabic, the LVCs come in verb+noun 
pairings. However, they surface in two syntactic 
forms. It can either be the object of the verb just 
like in English: 
 
 ???? ????]?????? ] ????? ??  
gave.he Georges [lecture] PREP Lebanon 
lit.'Georges gave a lecture about Lebanon' 
?Georges lectured about Lebanon? 
or the complement can be the object of a 
preposition: 
 
 ??????]????? ]????? ????? 
conduct.I [PREP-visit] our.saint Ilias 
lit. ?I will conduct with visit Saint Ilias?s? 
?I will visit Saint Ilias?s? 
3 Standard PropBank  
Annotation Procedure 
The PropBank annotation process can be broken 
down into two major steps: creation of the Frame 
Files for verbs occurring in the data and 
annotation of the data using the Frame Files. 
During the creation of the Frame Files, the 
usages of the verbs in the data are examined by 
linguists (henceforth, ?framers?). Based on these 
observations, the framers create a Frame File for 
each verb containing one or more framesets, 
which correspond to coarse-grained senses of the 
predicate lemma. Each frameset specifies the 
PropBank labels (i.e., ARG0, ARG1,?ARG5) 
corresponding to the argument structure of the 
verb. Additionally, illustrative examples are 
included for each frameset, which will later be 
referenced by the annotators. These examples 
also include the use of the ARGM labels. 
Thus, the framesets are based on the 
examination of the data, the framers? linguistic 
knowledge and native-speaker intuition. At 
83
times, we also make use of the syntactic and 
semantic behavior of the verb as described by 
certain lexical resources. These resources include 
VerbNet (Kipper et. al., 2006) and FrameNet 
(Baker et. al., 1998) for English, a number of 
monolingual and bilingual dictionaries for 
Arabic, and Hindi WordNet and DS Parses 
(Palmer et. al., 2009) for Hindi. Additionally, if 
available, we consult existing framesets of words 
with similar meanings across different languages. 
The data awaiting annotation are passed onto 
the annotators for a double-blind annotation 
process using the previously created framesets. 
The double annotated data is then adjudicated by 
a third annotator, during which time the 
differences of the two annotations are resolved to 
produce the Gold Standard. 
Two major guiding considerations during the 
framing and annotating process are data 
consistency and annotator productivity. During 
the frameset creation process, verbs that share 
similar semantic and syntactic characteristics are 
framed similarly. During the annotation process, 
the data is organized by verbs so that each verb is 
tackled all at once. In doing so, we firstly ensure 
that the framesets of similar verbs, and in turn, 
the annotation of the verbs, will both be 
consistent across the data. Secondly, by tackling 
annotation on verb-by-verb basis, the annotators 
are able to concentrate on a single verb at a time, 
making the process easier and faster for the 
annotators. 
4 Annotating LVC 
A similar process must be followed when 
annotating light verb constructions The first step 
is to create consistent Frame Files for light verbs. 
Then in order to make the annotation process 
produce consistent data at a reasonable speed, we 
have decided to carry out the light verb 
annotation in three passes (Table 1):  (1) annotate 
the light verb, (2) annotate the true predicate, and 
(3) merge the two annotations into one. 
The first pass involves the identification of the 
light verb. The most important parts of this step 
are to identify a verb as having bleached 
meaning, thereafter assign a generic light verb 
frameset and identify the true predicating 
expression of the sentence, which would be 
marked with ARG-PRX (i.e., ARGument-
PRedicating eXpression). For English, for 
example, annotators were instructed to use Butt?s 
(2004) criteria as described in Section 2. These 
criteria required that annotators be able to 
recognize whether or not the complement of a 
potential light verb was itself a predicating 
element. To make this occasionally difficult 
judgment, annotators used a simple heuristic test 
of whether or not the complement was headed by 
an element that has a verbal counterpart.  If so, 
the light verb frameset was selected. 
The second pass involves the annotation of the 
sentence with the true predicate as the relation. 
During this pass, the true predicate is annotated 
with an appropriate frameset. In the third pass, 
the arguments and the modifiers of the two 
previous passes are reconciled and merged into a 
single annotation. In order to reduce the number 
of hand annotation, it is preferable for this last 
pass, the Pass 3, to be done automatically. 
Since the nature of the light verb is different 
from that of other verbs as described in Section 
2, the advantage of doing the annotation of the 
light verb and the true predicate on separate 
passes is that in the light verb pass the annotators 
will be able to quickly dispose of the verb as a 
light verb and in the second pass, they will be 
allowed to solely focus on the annotation of the 
light verb?s true predicate. 
The descriptions of how the arguments and 
modifiers of the light verbs and their true 
predicates are annotated are mentioned in Table 
1, but notably, none of the examples in it 
currently include the annotation of arguments 
 Pass 1: Pass 2: Pass 3: 
 Light Verb Annotation True Predicate Annotation Merge of Pass1&2 Annotation 
Relation Light verb True predicate Light verb + true predicate 
Arguments 
and 
Modifiers 
- Predicating expression is 
annotated with ARG-PRX 
- Arguments and modifiers of 
the light verb are annotated 
- Arguments and modifiers of 
the true predicate are annotated 
- Arguments and modifiers 
found in the two passes are 
merged, preferably 
automatically. 
Frameset Light verb frameset True predicate?s frameset LVC?s frameset 
 
Example 
?John took a brisk walk through the park.? 
REL: took 
ARG-PRX: a brisk walk 
ARG-MNR: brisk  
REL: walk 
REL: took walk 
ARG-MNR: brisk 
Table 1. Preliminary Annotation Scheme 
84
and modifiers.  This is intentional, as coming to 
an agreement concerning the details of what 
exactly each of the three passes looks like while 
meeting the needs of the four PropBank 
languages is quite challenging. Thus, for the rest 
of the paper we will discuss the strengths and 
weaknesses of the two trial methods of 
annotation we have considered and discarded in 
Section 5, as well as the final annotation scheme 
we chose in Section 6. 
5 Trials 
5.1 Method 1 
As our first attempt, the annotation of argument 
and adjuncts was articulated in the following 
manner (Table 2). 
Pass 1: Pass 2: 
Light verb True predicate 
- Predicating expression 
is labeled ARG-PRX 
- Annotate the Subject 
argument of the light 
verb as the Arg0. 
- Annotate the rest of the 
arguments and modifiers 
of the light verb with 
ARGM labels. 
- Annotate arguments 
and modifiers of the 
true predicate within 
its domain of locality. 
Generic light verb Frame 
File 
True predicate?s 
Frame File 
?-RKQ WRRN D EULVN ZDON WKURXJK WKH SDUN? 
ARG0: John 
REL: took 
ARG-PRX: a brisk walk 
ARG-DIR: through the park 
ARG-MNR: brisk  
REL: walk 
Table 2. Method 1 for annotation for Passes 1 and 2. 
Revised information is in italics. 
In Pass 1, in addition to annotating the 
predicating expression of the light verb with 
ARG-PRX, the subject argument was marked 
with an ARG0. The choice of ARG0, which 
corresponds to a proto-typical agent, was guided 
by the observation that English LVCs tend to 
lend a component of agentivity to the subject 
even in cases where the true predicate would not 
necessarily assign an agent as its subject. The 
rest of the arguments and modifiers were labeled 
with corresponding ARGM (i.e., modifier) 
labels. The assumption here is that the arguments 
of the light verb will also be the arguments of the 
true predicate.   
In Pass 2, then, the annotation of the 
arguments of the true predicate was restricted to 
its domain of locality (i.e., the span of the ARG-
PRX as marked in Pass1). That is, in the example 
?John took a brisk walk through the park?, the 
labeled spans for the true predicate would be 
limited to the NP ?a brisk walk? and neither 
?John? nor through the park? would be annotated 
as the arguments of the true predicate ?walk?. 
Frame Files: This method would require three 
Frame Files: a generic light verb Frame File, a 
true predicate Frame File, and an LVC Frame 
File. The Frame File for the light verb would not 
be specific to the form of the light verb (e.g., 
same frame for take and make). Rather, it would 
indicate a skeletal argument structure in order to 
reduce the amount of Frame Files made, 
including only Arg0 as its argument1.  
5.2 Weakness of Method 1 
This method has one glaring problem: the 
assumption that the semantic roles of the 
arguments as assigned by the light verb 
uniformly coincide with those assigned by the 
true predicate does not always hold. Consider the 
following English sentence2. 
whether Wu Shu-Chen would make another 
[appearance] in court was subject to observation 
In this example, ?Wu Shu-Chen? is the agent 
argument (Arg0) of the light verb ?make? and is 
the theme or patient argument (Arg1) of a typical  
?appearance? event. Also consider the following 
example from Hindi.  
It is possible that in a light verb construction, 
the light verb actually modifies the standard 
underlying semantics of a nominalization like 
appearance.  In any event, we cannot assume that 
the expected argument labels for the light verb 
and for the standard interpretation of the 
nominalization will always coincide. Thus, we 
could say that Pass 2?s true predicate annotation 
is only partial and is not representative of the 
complete argument structure. In particular, we 
are left with a very difficult merging problem, 
because the argument labels of the two separate 
passes conflict as seen in the above examples. 
5.3 Method 2 
In order to remedy the problem of conflicting 
argument labels, we revised Method 1?s Pass 2 
annotation scheme. This is shown in Table 3. 
Pass 1 remains unchanged from Method 1. 
In this method, both the light verb and the true 
predicate of the sentence receive complete sets of 
                                                          
1 This is why the rest of the argument/modifiers would be 
annotated using ARGM modifier labels. 
2  The light verb is in boldface, the true predicate is in bold 
and square brackets, and the argument/adjunct under 
consideration is underlined. 
85
argument and modifier labels. In Pass 2, the 
limitation of annotating within the domain of 
locality is removed. That is, the arguments and 
modifiers inside and outside the true predicate?s 
domain of control are annotated with respect to 
their semantic relationship to the true predicate 
(e.g., in the English example of Section 5.2, ?Wu 
Shu-Chen? would be considered ARG1 of 
?appearance?).  
Frame Files: This method would also require 
three Frame Files. The major difference is that 
with this method the Frame File for the true 
predicate includes arguments that are sisters to 
the light verb.  
5.4 Weaknesses of Method 2 
If in Method 1 we have committed the error of 
semantic unfaithfulness due to omission, in 
Method 2 we are faced with the problem of 
including too much. In the following sentence, 
consider the role of the underlined adjunct: 
A New York audience ? gave it a big round 
of applause when the music started to play. 
By the annotation in Method 2, the underlined 
temporal adjunct ?when the music started to 
play? is labeled as both the argument of ?give? 
and of ?applause?. The question here is does the 
argument apply to both the giving and the 
applauding event? In other words, does the 
adjunct play an equal role in both passes?  
 Since it could be easily said that the temporal 
phrase applies to both the applauding and the 
giving of the applause events, this example may 
not be particularly compelling. However, what if 
a syntactic complement of the light verb is a 
semantic argument of the true predicate and the 
true predicate only? This is seen more frequently 
in the cases where the light verb is less bleached 
than in the case of ?give? above. Consider the 
following Arabic example. 
 
 ????? ??]???????? ] ????? ????????? ??????? ??????  
took.we PREP DEF-consideration PREP 
prepertations.our possibility sustain.their losses 
?We took into [consideration] during our prepa-
rations the possibility of them sustaining losses? 
 
Here, even though the constituent ?of them 
sustaining losses? is the syntactic complement of 
the verb ?to take;? semantically, it modifies only 
the nominal object of the PP ?consideration.?  
There are similar phenomena in Chinese light 
verb constructions. Syntactic modifiers of the 
light verb are semantic arguments of the true 
predicate, which is usually a nominalization that 
serves as its complement.  
 
?? ?  ?    ? ? ??    [??]    ?? ? 
we now regarding this CL issue [conduct] discussion. 
lit.?We are conducting a discussion on this issue.? 
 ?We are discussing this issue.? 
 
The prepositional phrase ????? ?regarding 
this issue? is a sister to the light verb but 
semantically it is an argument of the nominalized 
predicate ?? ?discussion?. 
The logical next question would be: does the 
annotation of the arguments, adjuncts and 
modifiers have to be all or nothing? It could 
conceivably be possible to assign a selected set 
of arguments at the light verb or true predicate 
level. For example, in the Chinese sentence, the 
modifier ?regarding this CL issue?, though a 
syntactic adjunct to the light verb, could be left 
out from the semantic annotation in Pass 1 and 
included only in the Pass 2. 
However, the objection to this treatment 
comes from a more practical need. As mentioned 
above, in order to keep the manual annotation to 
a minimum, it would be necessary to keep Pass 3 
completely deterministic. As is, with the 
unmodified Method 2, there would be the need to 
choose between Pass 1 or Pass 2 annotation to 
when doing the automatic Pass 3. If we modify 
Method 2 by annotating only a selected set of 
syntactic arguments for the light verb or the true 
predicate, then this issue is exacerbated. In such 
a case there we would have to develop with strict 
rules for which arguments of which pass should 
be included in Pass 3. Pass 3 would no longer be 
automatic, and should be done manually.  
Pass 2: 
True predicate 
- Annotate the Subject argument of the light verb 
with the appropriate role of the true predicate 
- Annotate arguments and modifiers of the true 
predicate without limitation as to the domain of 
locality. 
True predicate?s Frame File 
?+H PDGH DQRWKHU DSSHDUDQFH DW WKH SDUW\? 
ARG1: He 
ARG-ADV: another 
REL: appearance 
ARG-DIR: at court 
Table 3. Method 2 for annotation for Pass 2. Pass 
1 as presented in Table 2 remains unchanged. 
Revised information for Pass 2 is in italics 
 
86
6 Final Annotation Scheme 
6.1 Semantic Fidelity 
Many of the objections so far to Methods 1 and 2 
have centered on the issue of semantic fidelity 
during the annotation of each of the two passes. 
The debate of whether both passes should be 
annotated and to what extent has practical 
implications for the third Pass, as described 
above. However, more importantly it comes 
down to whether or not the semantics of the final 
light verb plus true predicate combination is 
indeed distinct from the semantics of its parts 
(i.e. light verb and true predicate, separately). 
This may be a fascinating linguistic question, but 
it is not something our annotators can be 
debating for each and every instance.   
Instead, we argue that the semantic argument 
structure of the light verb plus true predicate 
combination can in practice be different from 
that of the expressions taken independently as 
has been proposed by various studies (Butt, 
2004; Rosen, 1997; Grimshaw & Mester, 1988). 
Thus, we resolve the cases in which the 
differences in argument roles as assigned by the 
light verb and the nominalization (Section 5.2) 
by handling the argument structure of the 
standard nominalization separately from that of 
the nominalization participating in the LVC. In 
the example ?Chen made another appearance in 
court?, we annotate ?Chen? as the Agent (ARG0) 
of the full predicate ?[make] [appearance]?, 
which is different from the argument structure of 
the standard nominalization which would label 
?Chen? to be the Patient argument (ARG1). 
6.2 Method 3: Final Method 
Our final method of light verb annotation reflects 
the notion that the noun, verb, or adjective as a 
true predicate within an LVC can have a 
different argument structure from that of the 
word alone. Table 4 shows the final annotation 
scheme for light verb construction.  
During Pass 1, the LVCs and their predicating 
expressions are identified in the data. Instances 
identified as LVCs in Pass 1 are then manually 
annotated during Pass 2, annotating the 
arguments and adjuncts of the light verb and the 
true predicate with roles that reflect their 
semantic relationships to the light verb plus true 
predicate. In practice, Pass 1 becomes a way of 
simply manually identifying the light verb 
usages. It is in Pass 2 that we make the final 
choice of argument labels for all of the 
arguments. Thus in Pass 3, the light verb and the 
true predicate lemmas from Pass 1 and 2 are 
joined into a single unit (e.g., in the example 
found in Table 4, the light verb ?took? would be 
joined with the true predicate ?walk? into 
?took+walk?) 3. In this final method, Pass 3 can 
be achieved completely deterministically. 
The major difference in this annotation 
scheme from that of Methods 1 and 2 is that 
instead of annotating in terms of the semantics of 
the bare noun, adjective or verb, the argument 
structure is determined for the entire predicate or 
the full event: semantics of the light verb plus the 
true predicate. This means that for the sentences 
where the argument roles of the verb and the 
nominalization disagree like ?Chen? in ?Chen 
                                                          
3 The order of Pass 2 and Pass 3 as presented in Table 4 is 
arguably a product of how the annotation tools for 
PropBank are set up for Arabic, Chinese, and English. That 
is, the order of the Pass 2 and Pass 3 could potentially be 
flipped provided that the tools and procedures of annotation 
support it, as is the case for Hindi PropBank. After the LVC 
and ARG-PRX are identified in Pass 1, the light verb and 
the true predicate can be deterministically joined into a 
single relation in Pass 2, leaving the manual annotation of 
LVC for Pass 3.  The advantage of this alternative ordering 
is that because the annotation of LVC is done around light 
verb plus the true predicate as a single relation, rather than 
the true predicate alone as in Table 4, the argument 
annotation may in actuality be more intuitive for annotators 
even with less training. 
 Pass 1: Pass 2:  Pass 3: 
 Light Verb Identification LVC Annotation Deterministic relation merge 
Relation Light verb True predicate Light verb + true predicate 
Arguments 
& Modifiers 
- Predicating expression is 
annotated with ARG-PRX 
- Arguments and modifiers of 
the LVCs are annotated 
- Arguments and modifiers 
are taken from Pass 2 
Frame File <no Frame File needed> LVC?s Frame File LVC?s Frame File 
 
Example 
?John took a brisk walk through the park.? 
REL: took 
ARG-PRX: a brisk walk 
ARG0: John 
ARG-MNR: brisk  
REL: walk 
ARGM-DIR: through the park 
ARG0: John 
ARG-MNR: brisk  
REL: [took][walk] 
ARGM-DIR: through the park 
Table 4. Final Annotation Scheme 
87
made another4 appearance in court?, we label the 
argument with the role that is consistent with the 
entire predicate (i.e. Agent, ARG0).  
Frame Files: The final advantage to this 
method is that only one Frame File is needed. 
Since Pass 1 is an identification round, no Frame 
File is required. A single Frame File for LVC 
that includes the argument structure with respect 
to the light verb plus true predicate combination 
will suffice for Pass 2 and Pass 3. 
7 Distinguishing LVCs from MWEs 
As we have discussed in Section 2, we adapted 
our approach from Butt?s (2004) definition of 
LVCs. That is, an LVC is characterized by a 
semantically bleached light verb and a true 
predicate. These elements combine as a single 
predicating unit, in such a way that the light verb 
plus its true predicate can be paraphrased by a 
verbal form of the true predicate without loss of 
the core meaning of the expression (e.g. 
?lectured? for ?gave a lecture?). Also, as 
discussed in Section 6.1, our approach advocates 
the notion that the semantic argument structure 
of the light verb plus true predicate is different 
from that of the expressions taken independently 
(as also proposed by Butt, 2004; Rosen, 1997; 
Grimshaw & Mester, 1988 among others). 
While these definitions are appropriate for the 
PropBank annotation task as we have presented 
it, there are still cases that merit closer attention. 
Even English with a rather limited set of verbs 
that are commonly cited as LVCs, includes a 
problematic mixture of what could arguably be 
termed either LVCs or idiomatic expressions: 
?make exception?, ?take charge?. This difficulty 
in part is the effect of frequency and 
entrenchment of particular constructions.  The 
light verbs themselves do not diminish in form 
over time in a manner similar to auxiliaries (Butt, 
2004), although the complements of common 
LVCs can change over time such that it is no 
longer clear that the complement is a predicating 
element.   
In the case of English, the expressions ?take 
charge? may be more commonly found today as a 
LVC than independently in its verbal form.  As 
we discovered with our annotators, native 
English speakers are uncomfortable using the 
verb ?charge? (i.e. to burden with a 
                                                          
4 The adjective ?another? is annotated as the modifier of the 
full predicate ?[make][appearance]? as it can be interpreted 
to mean that the make appearance event happened a 
previous appearance has been made. 
responsibility) as an independent matrix verb. A 
similar phenomenon can be seen in Arabic, 
where the predicate ??? ???? lit. ?release name? 
exemplifies a prototypical LVC that means ?to 
name?. However, in our data we see cases in 
which the complement is missing, while the 
semantics of the LVC remains intact: 
 ???? ???? ?? ??????? ??????  
CONJ REL be released.he PREP-him/it  
DEF-sector DEF-public 
lit ?Or what is released to it ?the public sector?? 
?Or what is called/named ?the public sector.?? 
This raises the question of: when does a 
construction that may have once been an LVC 
become more properly defined as an idiomatic 
expression due to such entrenchment?  Idiomatic 
expressions can potentially be distinguished from 
LVCs through judgments of how fixed or 
syntactically variable a construction is, and on 
the basis of how semantically transparent or 
decomposable the construction is (Nunberg et. 
al., 1994). However, sometimes the dividing line 
is hard to draw.  
A similar problem arises in determining 
whether a construction is a case of an LVC or 
simply a usage with a distinct sense of the verb. 
Take, for example, the following Arabic 
sentence. 
 ?????? ????? 
   take.he DEF-food 
lit. ?(he) took food? 
?he ate? 
Here, the Arabic word ???? ?food? is the noun 
derivation of the root shared by the verb ???? ?to 
eat?, in such a way that the sentence could be 
rephrased as ???? ?(he) ate?. This example falls 
neatly into the LVC category. However, further 
examples suggest that the example is a case of a 
distinct sense of ?to take orally? where the 
restrictions on the object are that the theme must 
be something that can be taken by mouth: 
?????? ????? 
take.he DEF-medicine 
?he took medicine? 
?????? ????? 
take.he DEF-soup 
?he took soup? 
Finally, determining the appropriate criteria to 
distinguish between a truly semantically 
bleached verb and verbs that seem to be 
participating in complex predication but 
contribute more to the semantics of the 
construction is a challenge for all languages. For 
example, in English data, there are potential 
LVCs with verbs that are not often thought of as 
light verbs, such as ?produce an alteration? and 
88
?issue a complaint?.  Although most English 
speakers would agree that the verbs in these 
constructions do not contribute to the semantics 
of the construction (e.g. ?issue a complaint? can 
be paraphrased to ?to complain?), there are 
similar constructions such as ?register a 
complaint,? wherein the verb cannot be 
considered light. For the purposes of annotation, 
where it is necessary for annotators to understand 
clear criteria for distinguishing light verbs, such 
cases are highly problematic because there is no 
deterministic way to measure the extent to which 
the verbal element contributes to the semantics 
of the construction.  In turn, there is not a good 
way to distinguish some of these borderline 
verbs from their normal, heavy usages.  
Such problems can be resolved by establishing 
language-specific semantic or syntactic tests that 
can be used for taking care of the borderline 
cases of LVCs. However, there is one other 
plausible manner we have identified that could 
help in detecting such atypical LVCs. This can 
be done by focusing on the argument structures 
of predicating complements rather than focusing 
on the verbs themselves.  Grimshaw & Mester 
(1988) suggest that the formation of LVCs 
involves argument transfer from the predicating 
complement to the verb, which is semantically 
bleached and thematically incomplete and 
assigns no thematic roles itself.  Similarly, 
Stevenson et al (2004) suggest that the 
acceptability of a potential LVC depends on the 
semantic properties of the complement.  Thus, 
atypical LVCs, such as the English construction 
?issue a complaint,? can potentially be detected 
during the annotation of eventive nouns, planned 
for all PropBank languages.  
This process will make our treatment of LVCs 
more comprehensive. Used with our language-
specific semantic and syntactic criteria relating to 
both the verb and the predicating complement, it 
will help us to more effectively capture as many 
types of LVCs as possible, including those of the 
V+ADJ and V+V varieties. 
8 Usefulness of our Approach 
Two basic approaches have previously been 
taken to handle all types of MWEs, including 
LVCs in natural language processing 
applications. The first is to treat MWEs quite 
simply as fixed expressions or long strings of 
words with spaces in between; the second is to 
treat MWEs as purely compositional (Sag et al, 
2002). The words-with-spaces approach is 
adequate for handling fixed idiomatic 
expressions, but issues of lexical proliferation 
and flexibility quickly arise when this approach 
is applied to light verbs, which are syntactically 
flexible and can number in the tens of thousands 
for a given language (Stevenson et al, 2004; Sag 
et al, 2002).  Nonetheless, large-scale lexical 
resources such as FrameNet (Baker et al, 1998) 
and WordNet (Fellbaum, 1999) continue to 
expand with entries that are MWEs.   
The purely compositional approach is also 
problematic for light verbs because it is 
notoriously difficult to predict which light verbs 
can grammatically combine with other 
predicating elements; thus, this approach leads to 
problems of overgeneration (Sag et al, 2002).  In 
order to overcome this problem, Stevenson et al 
(2004) attempted to determine which 
nominalizations could form a valid complement 
to the English light verbs take, give and make, 
using Levin?s (1993) verb classes to group 
similar nominalizations.  This approach was 
rather successful for take and give, but 
inconclusive for the verb make.  
Our approach can help to develop a resource 
that is useful whether one takes a words-with-
spaces approach or a compositional approach. 
Specifically, for those implementing a words-
with-spaces approach, the resulting PropBank 
annotation can serve as a lexical resource listing 
for LVCs. For those interested in implementing a 
compositional approach the PropBank annotation 
can serve to assist in predicting likely 
combinations. Moreover, information in the 
PropBank Frame Files can be used to generalize 
across classes of nouns that can occur with a 
given light verb with the help of lexical resources 
such as WordNet (Fellbaum, 1998), FrameNet 
(Baker et. al., 1998), and VerbNet (Kipper-
Schuler, 2005) (in a manner similar to the 
approach of Stevenson et al (2004)). 
Acknowledgements 
We also gratefully acknowledge the support of the 
National Science Foundation Grant CISE-CRI 
0709167, Collaborative: A Multi-Representational 
and Multi-Layered Treebank for Hindi/Urdu, and a 
grant from the Defense Advanced Research Projects 
Agency (DARPA/IPTO) under the GALE program, 
DARPA/CMO Contract No HR0011-06-C-0022, 
subcontract from BBN, Inc.  
Any opinions, findings, and conclusions or 
recommendations expressed in this material are those 
of the authors and do not necessarily reflect the views 
of the National Science Foundation. 
89
Reference 
Alsina, A. 1997. Causatives in Bantu and Romance. 
In A. Alsina, J. Bresnan, and P. Sells eds. Complex 
Predicates. Stanford, California: CSLI 
Publications, p. 203-246. 
Baker, Collin F., Charles J. Fillmore, and John B. 
Lowe. 1998. The Berkeley FrameNet Project. In 
Proceedings of the 17th International Conference 
on Computational Linguistics (COLING/ACL-98), 
pages 86?90, Montreal. ACL. 
Butt, M. 2004.  The Light Verb Jungle. In G. Aygen, 
C. Bowern & C. Quinn eds.  Papers from the 
GSAS/Dudley House Workshop on Light Verbs. 
Cambridge, Harvard Working Papers in 
Linguistics, p. 1-50.   
Butt, M. 1997. Complex Predicates in Urdu. In A. 
Alsina, J. Bresnan, and P. Sells eds. Complex 
Predicates. Stanford, California: CSLI 
Publications, p. 107-149. 
Fellbaum, Christine, ed.: 1998, WordNet: An 
Electronic Lexical Database, Cambridge, MA: 
MIT Press.  
Grimshaw, J., and A. Mester. 1988. Light verbs and 
?-marking. Linguistic Inquiry 19(2):205?232. 
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic 
Labeling of Semantic Roles. Computational 
Linguistics 28:3, 245-288. 
Goldberg, Adele E. 2003.  ?Words by Default: 
Inheritance and the Persian Complex Predicate 
Construction.? In E. Francis and L. Michaelis 
(eds). Mismatch: Form-Function Incongruity and 
the Architecture of Grammar. CSLI Publications.  
84-112. 
Kipper-Schuler, Karin. 2005. VerbNet: A broad 
coverage, comprehensive verb lexicon. Ph.D. 
thesis, University of Pennsylvania. 
Levin, B. 1993. English Verb Classes and 
Alternations: A Preliminary Investigation. 
Chicago: Chicago Univ. Press.  
Meyers, A., R. Reeves, C. Macleod, R. Szekely, V. 
Zielinska, B. Young, and R. Grishman. 2004. The 
NomBank Project: An interim report. In 
Proceedings of the HLT-NAACL 2004 Workshop: 
Frontiers in Corpus Annotation, pages 24- 31, 
Boston, MA. pages 430?437, Barcelona, Spain. 
Mohanan, T. 1997. Multidimensionality of 
Representation: NV Complex Predicates in Hindi. 
In A. Alsina, J. Bresnan, and P. Sells eds. Complex 
Predicates. Stanford, California: CSLI 
Publications, p. 431-471. 
Martha Palmer, Rajesh Bhatt, Bhuvana Narasimhan, 
Owen Rambow, Dipti Misra Sharma, Fei Xia, 
Hindi Syntax: Annotating Dependency, Lexical 
Predicate-Argument Structure, and Phrase 
Structure, In the Proceedings of the 7th 
International Conference on Natural Language 
Processing, ICON-2009, Hyderabad, India, Dec 
14-17, 2009 
Palmer, Martha, Daniel Gildea, and Paul Kingsbury. 
2005. The Proposition Bank: An Annotated Corpus 
of Semantic Roles. Computational Linguistics, 
31(1):71?106. 
Pradhan, Sameer, Kadri Hacioglu, Valerie Krugler, 
Wayne Ward, James H. Martin, Daniel Jurafsky. 
2004. Shallow Semantic Parsing using Support 
Vector Machines. University of Colorado 
Technical Report: TR-CSLR 2003-03. 
Rosen, C. 1997. Auxiliation and Serialization: On 
Discerning the Difference. In A. Alsina, J. 
Bresnan, and P. Sells eds. Complex Predicates. 
Stanford, California: CSLI Publications, p. 175-
202. 
Sag, I., Baldwin, T. Bond, F., Copestake, A., 
Flickinger, D. 2002.  Multiword expressions: A 
pain in the neck for NLP.  In Proceedings of teh 
Third International Conference on Intelligent Text 
processing and Computatinal Linguistics 
(CICLING 2002), p. 1-15, Mexico City, Mexico. 
ACL. 
Stevenson, S., Fazly, A., and North, R. (2004). 
Statistical measures of the semi-productivity of 
light verb constructions. In Proceedings of the 
ACL-04 Workshop on Multiword Expressions: 
Integrating Processing, p. 1?8. 
 
90
Proceedings of the Fifth Law Workshop (LAW V), pages 21?29,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Analysis of the Hindi Proposition Bank using Dependency Structure
Ashwini Vaidya Jinho D. Choi Martha Palmer Bhuvana Narasimhan
Institute of Cognitive Science
University of Colorado at Boulder
{vaidyaa,choijd,mpalmer,narasimb}@colorado.edu
Abstract
This paper makes two contributions. First, we
describe the Hindi Proposition Bank that con-
tains annotations of predicate argument struc-
tures of verb predicates. Unlike PropBanks
in most other languages, the Hind PropBank
is annotated on top of dependency structure,
the Hindi Dependency Treebank. We explore
the similarities between dependency and pred-
icate argument structures, so the PropBank an-
notation can be faster and more accurate. Sec-
ond, we present a probabilistic rule-based sys-
tem that maps syntactic dependents to seman-
tic arguments. With simple rules, we classify
about 47% of the entire PropBank arguments
with over 90% confidence. These preliminary
results are promising; they show how well
these two frameworks are correlated. This can
also be used to speed up our annotations.
1 Introduction
Proposition Bank (from now on, PropBank) is a cor-
pus in which the arguments of each verb predicate
are annotated with their semantic roles (Palmer et
al., 2005). PropBank annotation has been carried
out in several languages; most of them are annotated
on top of Penn Treebank style phrase structure (Xue
and Palmer, 2003; Palmer et al, 2008). However, a
different grammatical analysis has been used for the
Hindi PropBank annotation, dependency structure,
which may be particularly suited for the analysis of
flexible word order languages such as Hindi.
As a syntactic corpus, we use the Hindi Depen-
dency Treebank (Bhatt et al, 2009). Using de-
pendency structure has some advantages. First, se-
mantic arguments1 can be marked explicitly on the
syntactic trees, so annotations of the predicate ar-
gument structure can be more consistent with the
dependency structure. Second, the Hindi Depen-
dency Treebank provides a rich set of dependency
relations that capture the syntactic-semantic infor-
mation. This facilitates mappings between syntac-
tic dependents and semantic arguments. A success-
ful mapping would reduce the annotation effort, im-
prove the inter-annotator agreement, and guide a full
fledged semantic role labeling task.
In this paper, we briefly describe our annotation
work on the Hindi PropBank, and suggest mappings
between syntactic and semantic arguments based on
linguistic intuitions. We also present a probabilistic
rule-based system that uses three types of rules to
arrive at mappings between syntactic and semantic
arguments. Our experiments show some promising
results; these mappings illustrate how well those two
frameworks are correlated, and can also be used to
speed up the PropBank annotation.
2 Description of the Hindi PropBank
2.1 Background
The Hindi PropBank is part of a multi-dimensional
and multi-layered resource creation effort for the
Hindi-Urdu language (Bhatt et al, 2009). This
multi-layered corpus includes both dependency an-
notation as well as lexical semantic information in
the form of PropBank. The corpus also produces
phrase structure representations in addition to de-
1The term ?semantic argument? is used to indicate all num-
bered arguments as well as modifiers in PropBank.
21
pendency structure. The Hindi Dependency Tree-
bank has created an annotation scheme for Hindi
by adapting labels from Panini?s Sanskrit gram-
mar (also known as CPG: Computational Paninian
Grammar; see Begum et al (2008)). Previous work
has demonstrated that the English PropBank tagset
is quite similar to English dependency trees anno-
tated with the Paninian labels (Vaidya et al, 2009).
PropBank has also been mapped to other depen-
dency schemes such as Functional Generative De-
scription (Cinkova, 2006).
2.2 Hindi Dependency Treebank
The Hindi Dependency Treebank (HDT) includes
morphological, part-of-speech and chunking infor-
mation as well as dependency relations. These are
represented in the Shakti Standard Format (SSF; see
Bharati et al (2007)). The dependency labels de-
pict relations between chunks, which are ?minimal
phrases consisting of correlated, inseparable enti-
ties? (Bharati et al, 2006), so they are not neces-
sarily individual words. The annotation of chunks
also assumes that intra-chunk dependencies can be
extracted automatically (Husain et al, 2010).
The dependency tagset consists of about 43 labels,
which can be grouped into three categories: depen-
dency relation labels, modifier labels, and labels for
non-dependencies (Bharati et al, 2009). PropBank
is mainly concerned with those labels depicting de-
pendencies in the domain of locality of verb predi-
cates. The dependency relation labels are based on
the notion of ?karaka?, defined as ?the role played by
a participant in an action?. The karaka labels, k1-5,
are centered around the verb?s meaning. There are
other labels such as rt (purpose) or k7t (location)
that are independent of the verb?s meaning.
2.3 Annotating the Hindi PropBank
The Hindi PropBank (HPB) contains the labeling of
semantic roles, which are defined on a verb-by-verb
basis. The description at the verb-specific level is
fine-grained; e.g., ?hitter? and ?hittee?. These verb-
specific roles are then grouped into broader cate-
gories using numbered arguments (ARG#). Each
verb can also have modifiers not specific to the verb
(ARGM*). The annotation process takes place in two
stages: the creation of frameset files for individual
verb types, and the annotation of predicate argu-
ment structures for each verb instance. As annota-
tion tools, we use Cornerstone and Jubilee (Choi et
al., 2010a; Choi et al, 2010b). The annotation is
done on the HDT; following the dependency anno-
tation, PropBank annotates each verb?s syntactic de-
pendents as their semantic arguments at the chunk
level. Chunked trees are conveniently displayed for
annotators in Jubilee. PropBank annotations gener-
ated in Jubilee can also be easily projected onto the
SSF format of the original dependency trees.
The HPB currently consists of 24 labels including
both numbered arguments and modifiers (Table 1).
In certain respects, the HPB labels make some dis-
tinctions that are not made in some other language
such as English. For instance, ARG2 is subdivided
into labels with function tags, in order to avoid
ARG2 from being semantically overloaded (Yi,
2007). ARGC and ARGA mark the arguments of mor-
phological causatives in Hindi, which is different
from the ARG0 notion of ?causer?. We also intro-
duce two labels to represent the complex predicate
constructions: ARGM-VLV and ARGM-PRX.
Label Description
ARG0 agent, causer, experiencer
ARG1 patient, theme, undergoer
ARG2 beneficiary
ARG3 instrument
ARG2-ATR attribute ARG2-GOL goal
ARG2-LOC location ARG2-SOU source
ARGC causer
ARGA secondary causer
ARGM-VLV verb-verb construction
ARGM-PRX noun-verb construction2
ARGM-ADV adverb ARGM-CAU cause
ARGM-DIR direction ARGM-DIS discourse
ARGM-EXT extent ARGM-LOC location
ARGM-MNR manner ARGM-MNS means
ARGM-MOD modal ARGM-NEG negation
ARGM-PRP purpose ARGM-TMP temporal
Table 1: Hindi PropBank labels.
2.4 Empty arguments in the Hindi PropBank
The HDT and HPB layers have different ways of
handling empty categories (Bhatia et al, 2010).
HPB inserts empty arguments such as PRO (empty
subject of a non-finite clause), RELPRO (empty
22
relative pronoun), pro (pro-drop argument), and
gap-pro (gapped argument). HPB annotates syn-
tactic relations between its semantic roles, notably
co-indexation of the empty argument PRO as well as
gap-pro. The example in Figure 1 shows that Mo-
han and PRO are co-indexed; thus, Mohan becomes
ARG0 of read via the empty argument PRO. There is
no dependency link between PRO and read because
PRO is inserted only in the PropBank layer.
Mohan wanted to read a book
????_?
PRO
?? ???
????
Mohan_ERG
k1
vmod
ARG1
ARG0
ARG0
????
PRO book read want
k2
ARG1
Figure 1: Empty argument example. The upper and lower
edges indicate HDT and HPB labels, respectively.
3 Comparisons between syntactic and
semantic arguments
In this section, we describe the mappings between
HDT and HPB labels based on our linguistic intu-
itions. We show that there are several broad similar-
ities between two tagsets. These mappings form the
basis for our linguistically motivated rules in Sec-
tion 4.2.3. In section 5.5, we analyze whether the
intuitions discussed in this section are borne out by
the results of our probabilistic rule-based system.
3.1 Numbered arguments
The numbered arguments correspond to ARG0-3,
including function tags associated with ARG2. In
PropBank, ARG0 and ARG1 are conceived as
framework-independent labels, closely associated
with Dowty?s Proto-roles (Palmer et al, 2010). For
instance, ARG0 corresponds to the agent, causer, or
experiencer, whether it is realized as the subject of
an active construction or as the object of an adjunct
(by phrase) of the corresponding passive. In this re-
spect, ARG0 and ARG1 are very similar to k1 and
k2 in HDT, which are annotated based on their se-
mantic roles, not their grammatical relation. On the
other hand, HDT treats the following sentences sim-
ilarly, whereas PropBank does not:
? The boy broke the window.
? The window broke.
The boy and the window are both considered k1 for
HDT, whereas PropBank labels the boy as ARG0 and
The window as ARG1. The window is not consid-
ered a primary causer as the verb is unaccusative for
Propbank. For HDT, the notion of unaccusativity is
not taken into consideration. This is an important
distinction that needs to be considered while carry-
ing out the mapping. k1 is thus ambiguous between
ARG0 and ARG1. Also, HDT makes a distinction
between Experiencer subjects of certain verbs, label-
ing them as k4a. As PropBank does not make such
a distinction, k4a maps to ARG0. The Experiencer
subject information is included in the corresponding
frameset files of the verbs. The mappings to ARG0
and ARG1 would be accurate only if they make use
of specific verb information. The mappings for other
numbered arguments as well as ARGC and ARGA are
given in Table 2.
HDT label HPB label
k1 (karta); k4a (experiencer) Arg0
k2 (karma) Arg1
k4 (beneficiary) Arg2
k1s (attribute) Arg2-ATR
k5 (source) Arg2-SOU
k2p (goal) Arg2-GOL
k3 (instrument) Arg3
mk1 (causer) ArgC
pk1 (secondary causer) ArgA
Table 2: Mappings to the HPB numbered arguments.
Note that in HDT annotation practice, k3 and k5
tend to be interpreted in a broad fashion such that
they map not only to ARG3 and ARG2-SOU, but also
to ARGM-MNS and ARGM-LOC (Vaidya and Husain,
2011). Hence, a one-to-one mapping for these la-
bels is not possible. Furthermore, the occurrence of
morphological causatives (ARGC and ARGA) is fairly
low so that we may not be able to test the accuracy
of these mappings with the current data.
3.2 Modifiers
The modifiers in PropBank are quite similar in their
definitions to certain HDT labels. We expect a fairly
high mapping accuracy, especially as these are not
verb-specific. Table 3 shows mappings between
23
HDT labels and HPB modifiers. A problematic map-
ping could be ARGM-MNR, which is quite coarse-
grained in PropBank, applying not only to adverbs
of manner, but also to infinitival adjunct clauses.
HDT label HPB label
sent-adv (epistemic adv) ArgM-ADV
rh (cause/reason) ArgM-CAU
rd (direction) ArgM-DIR
rad (discourse) ArgM-DIS
k7p (location) ArgM-LOC
adv (manner adv) ArgM-MNR
rt (purpose) ArgM-PRP
k7t (time) ArgM-TMP
Table 3: Mappings to the HPB modifiers.
3.3 Simple and complex predicates
HPB distinguishes annotations between simple and
complex predicates. Simple predicates consist of
only a single verb whereas complex predicates con-
sist of a light verb and a pre-verbal element. The
complex predicates are identified with a special label
ARGM-PRX (ARGument-PRedicating eXpresstion),
which is being used for all light verb annotations
in PropBank (Hwang et al, 2010). Figure 2 shows
an example of the predicating noun mention anno-
tated as ARGM-PRX, used with come. The predicat-
ing noun also has its own argument, matter of, in-
dicated with the HDT label r6-k1. The HDT has
two labels, r6-k1 and r6-k2, for the arguments of
the predicating noun. Hence, the argument span for
complex predicates includes not only direct depen-
dents of the verb but also dependents of the noun.
??????_?_????? ??????_?? ????_?? ?? ?_??
hearing_of_during Wed._of matter_of mention_to
k7t
k7t
pof
ARGM-PRX
ARG1
ARGM-TMP
come
???
r6-k1
ARGM-TMP
During the hearing on Wednesday, the matter was mentioned
Figure 2: Complex predicate example.
The ARGM-PRX label usually overlaps with the
HDT label pof, indicating a ?part of units? as pre-
verbal elements in complex predicates. However, in
certain cases, HPB has its own analysis for noun-
verb complex predicates. Hence, not all the nom-
inals labeled pof are labeled as ARGM-PRX. In
the example in Figure 3, the noun chunk important
progress is not considered to be an ARGM-PRX by
HPB (in this example, we have pragati hona; (lit)
progess be; to progress). The nominal for PropBank
is in fact ARG1 of the verb be, rather than a com-
posite on the verb. Additional evidence for this is
that neither the nominal nor the light verb seem to
project arguments of their own.
Important progress has been made in this work
??_???_? ?_?
k7p
pof
ARG1
ARGM-LOC
?????? ??_?? ??
this_work_LOC important_progress be_PRES
Figure 3: HDT vs. HPB on complex predicates.
4 Automatic mapping of HDT to HPB
Mapping between syntactic and semantic structures
has been attempted in other languages. The Penn
English and Chinese Treebanks consist of several se-
mantic roles (e.g., locative, temporal) annotated on
top of Penn Treebank style phrase structure (Marcus
et al, 1994; Xue and Palmer, 2009). The Chinese
PropBank specifies mappings between syntactic and
semantic arguments in frameset files (e.g., SBJ ?
ARG0) that can be used for automatic mapping (Xue
and Palmer, 2003). However, these Chinese map-
pings are limited to certain types of syntactic argu-
ments (mostly subjects and objects). Moreover, se-
mantic annotations on the Treebanks are done inde-
pendently from PropBank annotations, which causes
disagreement between the two structures.
Dependency structure transparently encodes rela-
tions between predicates and their arguments, which
facilitates mappings between syntactic and seman-
tic arguments. Hajic?ova? and Kuc?erova? (2002) tried
to project PropBank semantic roles onto the Prague
Dependency Treebank, and showed that the projec-
tion is not trivial. The same may be true to our case;
however, our goal is not to achieve complete map-
pings between syntactic and semantic arguments,
24
but to find a useful set of mappings that can speed
up our annotation. These mappings will be applied
to our future data as a pre-annotation stage, so that
annotators do not need to annotate arguments that
have already been automatically labeled by our sys-
tem. Thus, it is important to find mappings with high
precision and reasonably good recall.
In this section, we present a probabilistic rule-
based system that identifies and classifies semantic
arguments in the HPB using syntactic dependents in
the HDT. This is still preliminary work; our system
is expected to improve as we annotate more data and
do more error analysis.
4.1 Argument identification
Identifying semantic arguments of each verb pred-
icate is relatively easy given the dependency Tree-
bank. For each verb predicate, we consider all syn-
tactic dependents of the predicate as its semantic
arguments (Figure 4). For complex predicates, we
consider the syntactic dependents of both the verb
and the predicating noun (cf. Section 3.3).
?? ???? ? ?? ??_ ? ?? ??? ??_ ??
Kishori Haridwar_from Delhi come_be
k1
k5
k2p
ARG2-GOL
ARG2-SOU
ARG0
Kishori came from Haridwar to Delhi
Figure 4: Simple predicate example.
With our heuristics, we get a precision of 99.11%,
a recall of 95.50%, and an F1-score of 97.27% for
argument identification. Such a high precision is
expected as the annotation guidelines for HDT and
HPB generally follow the same principles of iden-
tifying syntactic and semantic arguments of a verb.
About 4.5% of semantic arguments are not identi-
fied by our method. Table 4 shows distributions of
the most frequent non-identified arguments.
Label Dist. Label Dist. Label Dist.
ARG0 3.21 ARG1 0.90 ARG2? 0.09
Table 4: Distributions of non-identified arguments caused
by PropBank empty categories (in %).
Most of the non-identified argument are antecedents
of PropBank empty arguments. As shown in Fig-
ure 1, the PropBank empty argument has no depen-
dency link to the verb predicate. Identifying such
arguments requires a task of empty category reso-
lution, which will be explored as future work. Fur-
thermore, we do not try to identify PropBank empty
arguments for now, which will also be explored later.
4.2 Argument classification
Given the identified semantic arguments, we classify
their semantic roles. Argument classification is done
by using three types of rules. Deterministic rules are
heuristics that are straightforward given dependency
structure. Empirically-derived rules are generated
by measuring statistics of dependency features in as-
sociation with semantic roles. Finally, linguistically-
motivated rules are derived from our linguistic intu-
itions. Each type of rule has its own strength; how
to combine them is the art we need to explore.
4.2.1 Deterministic rule
Only one deterministic rule is used in our system.
When an identified argument has a pof dependency
relation with its predicate, we classify the argu-
ment as ARGM-PRX. This emphasizes the advan-
tage of using our dependency structure: classifying
ARGM-PRX cannot be done automatically in most
other languages where there is no information pro-
vided for light verb constructions. This determin-
istic rule is applied before any other type of rule.
Therefore, we do not generate further rules to clas-
sify the ARGM-PRX label.
4.2.2 Empirically-derived rules
Three kinds of features are used for the generation of
empirically-derived rules: predicate ID, predicate?s
voice type, and argument?s dependency label. The
predicate ID is either the lemma or the roleset ID
of the predicate. Predicate lemmas are already pro-
vided in HDT. When we use predicate lemmas, we
assume no manual annotation of PropBank. Thus,
rules generated from predicate lemmas can be ap-
plied to any future data without modification. When
we use roleset ID?s, we assume that sense annota-
tions are already done. PropBank includes anno-
tations of coarse verb senses, called roleset ID?s,
that differentiate each verb predicate with different
25
senses (Palmer et al, 2005). A verb predicate can
form several argument structures with respect to dif-
ferent senses. Using roleset ID?s, we generate more
fine-grained rules that are specific to those senses.
The predicate?s voice type is either ?active? or
?passive?, also provided in HDT. There are not many
instances of passive construction in our current data,
which makes it difficult to generate rules general
enough for future data. However, even with the lack
of training instances, we find some advantage of us-
ing the voice feature in our experiments. Finally, the
argument?s dependency label is the dependency la-
bel of an identified argument with respect to its pred-
icate. This feature is straightforward for the case of
simple predicates. For complex predicates, we use
the dependency labels of arguments with respect to
their syntactic heads, which can be pre-verbal ele-
ments. Note that rules generated with complex pred-
icates contain slightly different features for predicate
lemmas as well; instead of using predicate lemmas,
we use joined tags of the predicate lemmas and the
lemmas of pre-verbal elements.
ID V Drel PBrel #
come a k1 ARG0 1
come a k5 ARG2-SOU 1
come a k2p ARG2-GOL 1
come mention a k7t ARGM-TMP 2
come mention a r6-k1 ARG1 1
Table 5: Rules generated by the examples in Figures 4 and
2. The ID, V, and Drel columns show predicate ID, predicate?s
voice type, and argument?s dependency label. The PBrel col-
umn shows the PropBank label of each argument. The # column
shows the total count of each feature tuple being associated with
the PropBank label. ?a? stands for active voice.
Table 5 shows a set of rules generated by the exam-
ples in Figures 4 (come) and 2 (come mention). No
rule is generated for ARGM-PRX because the label
is already covered by our deterministic rule (Sec-
tion 4.2.1). When roleset ID?s are used in place of
the predicate ID, come and come mention are re-
placed with A.03 and A.01, respectively. These
rules can be formulated as a function rule such that:
rule(id, v, drel) = argmax i P (pbreli)
where P (pbreli) is a probability of the predicted
PropBank label pbreli, given a tuple of features
(id, v, drel). The probability is measured by es-
timating a maximum likelihood of each PropBank
label being associated with the feature tuple. For
example, a feature tuple (come, active, k1) can be
associated with two PropBank labels, ARG0 and
ARG1, with counts of 8 and 2, respectively. In this
case, the maximum likelihoods of ARG0 and ARG1
being associated with the feature tuple is 0.8 and 0.2;
thus rule(come, active, k1) = ARG0.
Since we do not want to apply rules with low con-
fidence, we set a threshold to P (pbrel), so predic-
tions with low probabilities can be filtered out. Find-
ing the right threshold is a task of handling the pre-
cision/recall trade-off. For our experiments, we ran
10-fold cross-validation to find the best threshold.
4.2.3 Linguistically-motivated rules
Linguistically-motivated rules are applied to argu-
ments that the deterministic rule and empirically-
derived rules cannot classify. These rules capture
general correlations between syntactic and seman-
tic arguments for each predicate, so they are not as
fine-grained as empirically-derived rules, but can be
helpful for predicates not seen in the training data.
The rules are manually generated by our annota-
tors and specified in frameset files. Table 6 shows
linguistically-motivated rules for the predicate ?A
(come)?, specified in the frameset file, ?A-v.xml?.3
Roleset Usage Rule
A.01 to come
k1 ? ARG1
k2p ? ARG2-GOL
A.03 to arrive
k1 ? ARG1
k2p ? ARG2-GOL
k5 ? ARG2-SOU
A.02 light verb No rule provided
Table 6: Rules for the predicate ?A (come)?.
The predicate ?A? has three verb senses and each
sense specifies a different set of rules. For instance,
the first rule of A.01 maps a syntactic dependent
with the dependency label k1 to a semantic ar-
gument with the semantic label ARG1. Note that
frameset files include rules only for numbered ar-
guments. Most of these rules should already be in-
cluded in the empirically-derived rules as we gain
3See Choi et al (2010a) for details about frameset files.
26
more training data; however, for an early stage of
annotation, these rules provide useful information.
5 Experiments
5.1 Corpus
All our experiments use a subset of the Hindi Depen-
dency Treebank, distributed by the ICON?10 con-
test (Husain et al, 2010). Our corpus contains about
32,300 word tokens and 2,005 verb predicates, in
which 546 of them are complex predicates. Each
verb predicate is annotated with a verse sense speci-
fied in its corresponding frameset file. There are 160
frameset files created for the verb predicates. The
number may seem small compared to the number
of verb predicates. This is because we do not cre-
ate separate frameset files for light verb construc-
tions, which comprise about 27% of the predicate
instances (see the example in Table 6).
All verb predicates are annotated with argument
structures using PropBank labels. A total of 5,375
arguments are annotated. Since there is a relatively
small set of data, we do not make a separate set for
evaluations. Instead, we run 10-fold cross-validation
to evaluate our rule-based system.
5.2 Evaluation of deterministic rule
First, we evaluate how well our deterministic rule
classifies the ARGM-PRX label. Using the determin-
istic rule, we get a 94.46% precision and a 100%
recall on ARGM-PRX. The 100% recall is expected;
the precision implies that about 5.5% of the time,
light verb annotations in the HPB do not agree with
the complex predicate annotations (pof relation) in
the HDT (cf. Section 3.3). More analysis needs to
be done to improve the precision of this rule.
5.3 Evaluation of empirically-derived rules
Next, we evaluate our empirically-derived rules with
respect to the different thresholds set for P (pbreli).
In general, the higher the threshold is, the higher
and lower the precision and recall become, respec-
tively. Figure 5 shows comparisons between preci-
sion and recall with respect to different thresholds.
Notice that a threshold of 1.0, meaning that using
only rules with 100% confidence, does not give the
highest precision. This is because the model with
this high of a threshold overfits to the training data.
Rules that work well in the training data do not nec-
essarily work as well on the test data.
   0 0.2 0.4 0.6 0.8 1
   
   30
40
50
60
70
80
Threshold
Acc
ura
cy (
in %
)
R
F1
P
0.93
Figure 5: Accuracies achieved by the empirically derived
rules using (lemma, voice, label) features. P, R, and F1
stand for precisions, recalls, and F1-scores, respectively.
We need to find a threshold that gives a high preci-
sion (so annotators do not get confused by the au-
tomatic output) while maintaining a good recall (so
annotations can go faster). With a threshold of 0.93
using features (lemma, voice, dependency label), we
get a precision of 90.37%, a recall of 44.52%, and
an F1-score of 59.65%. Table 7 shows accuracies
for all PropBank labels achieved by a threshold of
0.92 using roleset ID?s instead of predicate?s lem-
mas. Although the overall precision stays about the
same, we get a noticeable improvement in the over-
all recall using roleset ID?s. Note that some labels
are missing in Table 7. This is because either they
do not occur in our current data (ARGC and ARGA)
or we have not started annotating them properly yet
(ARGM-MOD and ARGM-NEG).
5.4 Evaluation of linguistically-motivated rules
Finally, we evaluate the impact of the linguistically-
motivated rules. Table 8 shows accuracies achieved
by the linguistically motivated rules applied after the
empirically derived rules. As expected, the linguis-
tically motivated rules improve the recall of ARGN
significantly, but bring a slight decrease in the pre-
cision. This shows that our linguistic intuitions are
generally on the right track. We may combine some
of the empirically derived rules with linguistically
motivated rules together in the frameset files so an-
notators can take advantage of both kinds of rules in
the future.
27
Dist. P R F1
ALL 100.00 90.59 47.92 62.69
ARG0 17.50 95.83 67.27 79.05
ARG1 27.28 94.47 61.62 74.59
ARG2 3.42 81.48 37.93 51.76
ARG2-ATR 2.54 94.55 40.31 56.52
ARG2-GOL 1.61 64.29 21.95 32.73
ARG2-LOC 0.87 90.91 22.73 36.36
ARG2-SOU 0.83 78.26 42.86 55.38
ARG3 0.08 0.00 0.00 0.00
ARGM-ADV 3.50 31.82 3.93 7.00
ARGM-CAU 1.44 50.00 5.48 9.88
ARGM-DIR 0.43 100.00 18.18 30.77
ARGM-DIS 1.63 26.67 4.82 8.16
ARGM-EXT 1.42 0.00 0.00 0.00
ARGM-LOC 10.77 83.80 27.42 41.32
ARGM-MNR 6.00 57.14 9.18 15.82
ARGM-MNS 0.79 77.78 17.50 28.57
ARGM-PRP 2.15 65.52 17.43 27.54
ARGM-PRX 10.75 94.46 100.00 97.15
ARGM-TMP 7.01 74.63 14.04 23.64
Table 7: Labeling accuracies achieved by the empirically de-
rived rules using (roleset ID, voice, label) features and a thresh-
old of 0.92. The accuracy for ARGM-PRX is achieved by the
deterministic rule. The Dist. column shows a distribution of
each label.
Dist. P R F1
ALL 100.00 89.80 55.28 68.44
ARGN 54.12 91.87 72.36 80.96
ARGM 45.88 85.31 35.14 49.77
ARGN w/o LM 93.63 58.76 72.21
Table 8: Labeling accuracies achieved by the linguistically
motivated rules. The ARGN and ARGM rows show statistics of
all numbered arguments and modifiers combined, respectively.
The ?ARGN w/o LM? row shows accuracies of ARGN achieved
only by the empirically derived rules.
5.5 Error anlaysis
The precision and recall results for ARG0 and ARG1,
are better than expected, despite the complexity of
the mapping (Section 3.1). This is because they oc-
cur most often in the corpus, so enough rules can
be extracted. The other numbered arguments are
closely related to particular types of verbs (e.g., mo-
tion verbs for ARG2-GOL|SOU). Our linguistically
motivated rules are more effective for these types
of HPB labels. We would expect the modifiers to
be mapped independently of the verb, but our ex-
periments show that the presence of the verb lemma
feature enhances the performance of modifiers. Al-
though section 3.2 expects one-to-one mappings for
modifiers, it is not the case in practice.
We observe that the interpretation of labels in an-
notation practice is important. For example, our sys-
tem performs poorly for ARGM-ADV because the la-
bel is used for various sentential modifiers and can
be mapped to as many as four HDT labels. On the
other hand, HPB makes some fine-grained distinc-
tions. For instance, means and causes are distin-
guished using ARGM-CAU and ARGM-MNS labels, a
distinction that HDT does not make. In the example
in Figure 6, we find that aptitude with is assigned to
ARGM-MNS, but gets the cause label rh in HDT.
Rajyapal can call upon any party with his aptitude
???????
Rajyapal
???
his
?? ??_ ?
aptitude_with
?? ??_ ??
any_EMPH
????_ ??
party_DAT
????_ ????_ ?
call_can_be
Figure 6: Means vs. cause example.
6 Conclusion and future work
We provide an analysis of the Hindi PropBank anno-
tated on the Hindi Dependency Treebank. There is
an interesting correlation between dependency and
predicate argument structures. By analyzing the
similarities between the two structures, we find rules
that can be used for automatic mapping of syntactic
and semantic arguments, and achieve over 90% con-
fidence for almost half of the data. These rules will
be applied to our future data, which will make the
annotation faster and possibly more accurate.
We plan to use different sets of rules generated by
different thresholds to see which rule set leads to the
most effective annotation. We also plan to develop
a statistical semantic role labeling system in Hindi,
once we have enough training data. In addition, we
will explore the possibility of using existing lexical
resource such as WordNet (Narayan et al, 2002) to
improve our system.
Acknowledgements
This work is supported by NSF grants CNS- 0751089, CNS-
0751171, CNS-0751202, and CNS-0751213. Any opinions,
findings, and conclusions or recommendations expressed in this
material are those of the authors and do not necessarily reflect
the views of the National Science Foundation.
28
References
Rafiya Begum, Samar Husain, Arun Dhwaj, Dipti Misra
Sharma, Lakshmi Bai, and Rajeev Sangal. 2008. De-
pendency annotation scheme for indian languages. In
In Proceedings of the 3rd International Joint Confer-
ence on Natural Language Processing, IJCNLP?08.
Akshar Bharati, Dipti Misra Sharma, Lakshmi Bai, and
Rajeev Sangal. 2006. AnnCorra: Guidelines for POS
and Chunk Annotation for Indian Languages. Techni-
cal report, IIIT Hyderabad.
Akshar Bharati, Rajeev Sangal, and Dipti Misra Sharma.
2007. Ssf: Shakti standard format guide. Technical
report, IIIT Hyderabad.
Akshara Bharati, Dipti Misra Sharma, Samar Husain,
Lakshmi Bai, Rafiya Begam, and Rajeev Sangal.
2009. Anncorra : Treebanks for indian languages,
guidelines for annotating hindi treebank. Technical re-
port, IIIT Hyderabad.
Archna Bhatia, Rajesh Bhatt, Bhuvana Narasimhan,
Martha Palmer, Owen Rambow, Dipti Misra Sharma,
Michael Tepper, Ashwini Vaidya, and Fei Xia. 2010.
Empty categories in a hindi treebank. In Proceedings
of the 7th International Conference on Language Re-
sources and Evaluation (LREC?10), pages 1863?1870.
Rajesh Bhatt, Bhuvana Narasimhan, Martha Palmer,
Owen Rambow, Dipti Sharma, and Fei Xia. 2009. A
Multi-Representational and Multi-Layered Treebank
for Hindi/Urdu. In In the Proceedings of the Third Lin-
guistic Annotation Workshop held in conjunction with
ACL-IJCNLP 2009.
Jinho D. Choi, Claire Bonial, and Martha Palmer. 2010a.
Propbank frameset annotation guidelines using a ded-
icated editor, cornerstone. In Proceedings of the 7th
International Conference on Language Resources and
Evaluation, LREC?10, pages 3650?3653.
Jinho D. Choi, Claire Bonial, and Martha Palmer. 2010b.
Propbank instance annotation guidelines using a ded-
icated editor, jubilee. In Proceedings of the 7th In-
ternational Conference on Language Resources and
Evaluation, LREC?10, pages 1871?1875.
Silvie Cinkova. 2006. From PropBank to EngVALLEX:
Adapting PropBank-Lexicon to the Valency Theory of
Functional Generative Description. In Proceedings
of the fifth International conference on Language Re-
sources and Evaluation (LREC 2006), Genova, Italy.
Eva Hajic?ova? and Ivona Kuc?erova?. 2002. Argu-
ment/valency structure in propbank, lcs database and
prague dependency treebank: A comparative pi-
lot study. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation,
LREC?02, pages 846?851.
Samar Husain, Prashanth Mannem, Bharat Ram Ambati,
and Phani Gadde. 2010. The ICON-2010 tools contest
on Indian language dependency parsing. In Proceed-
ings of ICON-2010 Tools Contest on Indian Language
Dependency Parsing, ICON?10, pages 1?8.
Jena D. Hwang, Archna Bhatia, Claire Bonial, Aous
Mansouri, Ashwini Vaidya, Nianwen Xue, and Martha
Palmer. 2010. PropBank Annotation of Multilingual
Light Verb Constructions. In Proceedings of the Lin-
guistic Annotation Workshop at ACL 2010.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert Macintyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The penn tree-
bank: Annotating predicate argument structure. In
ARPA Human Language Technology Workshop, pages
114?119.
Dipak Narayan, Debasri Chakrabarti, Prabhakar Pande,
and Pushpak Bhattacharyya. 2002. An experience
in building the indo wordnet - a wordnet for hindi.
In Proceedings of the 1st International Conference on
Global WordNet.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
106.
Martha Palmer, Olga Babko-Malaya, Ann Bies, Mona
Diab, Mohamed Maamouri, Aous Mansouri, and Wa-
jdi Zaghouani. 2008. A pilot arabic propbank. In Pro-
ceedings of the 6th International Language Resources
and Evaluation, LREC?08, pages 28?30.
Martha Palmer, Daniel Gildea, and Nianwen Xue. 2010.
Semantic role labeling. In Graeme Hirst, editor, Syn-
thesis Lectures on Human Language Technologies.
Morgan and Claypool.
Ashwini Vaidya and Samar Husain. 2011. A classifica-
tion of dependencies in the Hindi/Urdu Treebank. In
Presented at the Workshop on South Asian Syntax and
Semantics, Amherst, MA.
Ashwini Vaidya, Samar Husain, and Prashanth Mannem.
2009. A karaka based dependency scheme for En-
glish. In Proceedings of the CICLing-2009, Mexico
City, Mexico.
Nianwen Xue and Martha Palmer. 2003. Annotating the
propositions in the penn chinese treebank. In Proceed-
ings of the 2nd SIGHAN workshop on Chinese lan-
guage processing, SIGHAN?03, pages 47?54.
Nianwen Xue and Martha Palmer. 2009. Adding seman-
tic roles to the chinese treebank. Natural Language
Engineering, 15(1):143?172.
Szu-Ting Yi. 2007. Automatic Semantic Role Labeling.
Ph.D. thesis, University of Pennsylvania.
29
Proceedings of the 9th Workshop on Multiword Expressions (MWE 2013), pages 126?131,
Atlanta, Georgia, 13-14 June 2013. c?2013 Association for Computational Linguistics
Semantic Roles for Nominal Predicates: Building a Lexical Resource
Ashwini Vaidya and Martha Palmer and Bhuvana Narasimhan
Dept of Linguistics
Institute of Cognitive Science
University of Colorado, Boulder
Boulder, CO 80309
{vaidyaa, mpalmer, narasimb}@colorado.edu
Abstract
The linguistic annotation of noun-verb com-
plex predicates (also termed as light verb con-
structions) is challenging as these predicates
are highly productive in Hindi. For semantic
role labelling, each argument of the noun-verb
complex predicate must be given a role la-
bel. For complex predicates, frame files need
to be created specifying the role labels for
each noun-verb complex predicate. The cre-
ation of frame files is usually done manually,
but we propose an automatic method to expe-
dite this process. We use two resources for
this method: Hindi PropBank frame files for
simple verbs and the annotated Hindi Tree-
bank. Our method perfectly predicts 65% of
the roles in 3015 unique noun-verb combi-
nations, with an additional 22% partial pre-
dictions, giving us 87% useful predictions to
build our annotation resource.
1 Introduction
Ahmed et al (2012) describe several types of com-
plex predicates that are found in Hindi e.g. morpho-
logical causatives, verb-verb complex predicates and
noun-verb complex predicates. Of the three types,
we will focus on the noun-verb complex predicates
in this paper. Typically, a noun-verb complex pred-
icate chorii ?theft? karnaa ?to do? has two compo-
nents: a noun chorii and a light verb karnaa giving
us the meaning ?steal?. Complex predicates 1 may
be found in English e.g. take a walk and many other
languages such as Japanese, Persian, Arabic and
Chinese (Butt, 1993; Fazly and Stevenson, 2007).
1They are also otherwise known as light verb, support verb
or conjunct verb constructions.
The verbal component in noun-verb complex
predicates (NVC) has reduced predicating power
(although it is inflected for person, number, and gen-
der agreement as well as tense-aspect and mood) and
its nominal complement is considered the true pred-
icate, hence the term ?light verb?. The creation of
a lexical resource for the set of true predicates that
occur in an NVC is important from the point of view
of linguistic annotation. For semantic role labelling
in particular, similar lexical resources have been cre-
ated for complex predicates in English, Arabic and
Chinese (Hwang et al, 2010).
1.1 Background
The goal of this paper is to produce a lexical re-
source for Hindi NVCs. This resource is in the form
of ?frame files?, which are directly utilized for Prop-
Bank annotation. PropBank is an annotated cor-
pus of semantic roles that has been developed for
English, Arabic and Chinese (Palmer et al, 2005;
Palmer et al, 2008; Xue and Palmer, 2003). In
Hindi, the task of PropBank annotation is part of a
larger effort to create a multi-layered treebank for
Hindi as well as Urdu (Palmer et al, 2009).
PropBank annotation assumes that syntactic
parses are already available for a given corpus.
Therefore, Hindi PropBanking is carried out on top
of the syntactically annotated Hindi Dependency
Treebank. As the name suggests, the syntactic rep-
resentation is dependency based, which has several
advantages for the PropBank annotation process (see
Section 3).
The PropBank annotation process for Hindi fol-
lows the same two-step process used for other Prop-
Banks. First, the semantic roles that will occur with
each predicate are defined by a human expert. Then,
126
these definitions or ?frame files? are used to guide
the annotation of predicate-argument structure in a
given corpus.
Semantic roles are annotated in the form of num-
bered arguments. In Table 1 PropBank-style seman-
tic roles are listed for the simple verb de;?to give?:
de.01 ?to give?
Arg0 the giver
Arg1 thing given
Arg2 recipient
Table 1: A frame file
The labels ARG0, ARG1 and ARG2 are always de-
fined on a verb-by-verb basis. The description at
the verb-specific level gives details about each num-
bered argument. In the example above, the num-
bered arguments correspond to the giver, thing given
and recipient. In the Hindi treebank, which consists
of 400,000 words, there are nearly 37,576 predi-
cates, of which 37% have been identified as complex
predicates at the dependency level. This implies that
a sizeable portion of the predicates are NVCs, which
makes the task of manual frame file creation time
consuming.
In order to reduce the effort required for manual
creation of NVC frame files, we propose a novel au-
tomatic method for generating PropBank semantic
roles. The automatically generated semantic roles
will be used to create frame files for each com-
plex predicate in the corpus. Our method accurately
predicts semantic roles for almost two thirds of
the unique nominal-verb combinations, with around
20% partial predictions, giving us a total of 87% use-
ful predictions.
For our implementation, we use linguistic re-
sources in the form of syntactic dependency labels
from the treebank. In addition we also have manu-
ally created, gold standard frame files for Hindi sim-
ple verbs2. In the following sections we provide lin-
guistic background, followed by a detailed descrip-
tion of our method. We conclude with an error anal-
ysis and evaluation section.
2http://verbs.colorado.edu/propbank/framesets-hindi/
2 The Nominal and the Light Verb
Semantic roles for the arguments of the light verb are
determined jointly by the noun as well as the light
verb. Megerdoomian (2001) showed that the light
verb places some restrictions on the semantic role of
its subject in Persian. A similar phenomenon may
be observed for Hindi. Compare example 1 with ex-
ample 2 below:
(1) Raam-ne
Ram-erg
cycle-kii
cycle-gen
chorii
theft
kii
do.prf
?Ram stole a bicycle?
(2) aaj
Today
cycle-kii
cycle-gen
chorii
theft
huii
be.pres
?Today a bicycle was stolen?
PropBank annotation assumes that sentences in
the corpus have already been parsed. The annotation
task involves identification of arguments for a given
NVC and the labelling of these arguments with se-
mantic roles. In example 1 we get an agentive sub-
ject with the light verb kar ?do?. However, when it
is replaced by the unaccusative ho ?become? in Ex-
ample 2, then the resulting clause has a theme argu-
ment as its subject. Note that the nominal chorii in
both examples remains the same. From the point
of view of PropBank annotation, the NVC chorii
kii will have both ARG0 and ARG1, but chorii huii
will only have ARG1 for its single argument cycle.
Hence, the frame file for a given nominal must make
reference to the type of light verb that occurs with it.
The nominal as the true predicate also contributes
its own arguments. In example 3, which shows a full
(non-light) use of the verb de ?give?, there are three
arguments: giver(agent), thing given(theme) and re-
cipient. In contrast the light verb usage zor de ?em-
phasis give; emphasize?, seen in example 4, has a
locative marked argument baat par ?matter on? con-
tributed by the nominal zor ?emphasis?.
(3) Raam-ne
Ram-erg
Mohan ko
Mohan-dat
kitaab
book
dii
give.prf
?Ram gave Mohan a book?
(4) Ram ne
Ram-erg
is
this
baat
matter
par
loc
zor
emphasis
diyaa
give.prf
?Ram emphasized this matter?
127
As both noun and light verb contribute to the se-
mantic roles of their arguments, we require linguis-
tic knowledge about both parts of the NVC. The
semantic roles for the nominal need to specify the
co-occurring light verb and the nominal?s argument
roles must also be captured. Table 2 describes the
desired representation for a nominal frame file.
Frame file for chorii-n(oun)
chorii.01: theft-n light verb: kar?do; to
steal?
Arg0 person who steals
Arg1 thing stolen
chorii.02 : theft-n light verb: ho
?be/become; to get
stolen?
Arg1 thing stolen
Table 2: Frame file for predicate noun chorii ?theft? with
two frequently occurring light verbs ho and kar. If other
light verbs are found to occur, they are added as addi-
tional rolesets as chorii.03, chorii.04 and so on.
This frame file shows the representation of a nom-
inal chorii ?theft? that can occur in combination with
a light verb kar ?do? or ho ?happen?. For each
combination, we derive a different set of PropBank
roles: agent and patient for chorii.01 and theme for
chorii.02. Note that the nominal?s frame actually
contains the roles for the combination of nominal
and light verb, and not the nominal alone.
Nominal frame files such as these have already
been defined for English PropBank.3 However, for
English, many nominals in NVCs are in fact nom-
inalizations of full verbs, which makes it far easier
to derive their frame files (e.g. walk in take a walk
is a full verb). For Hindi, this is not the case, and
a different strategy needs to be employed to derive
these frames automatically.
3 Generating Semantic Roles
The Hindi Treebank has already identified NVC
cases by using a special label pof or ?part-of?. The
Treebank annotators apply this label on the basis of
native speaker intuition. We use the label given by
the Treebank as a means to extract the NVC cases
(the issues related to complex predicate identifica-
tion are beyond the scope of this paper). Once this
3http://verbs.colorado.edu/propbank/framesets-noun/
extraction step is complete, we have a set of nomi-
nals and a corresponding list of light verbs that occur
with them.
In Section 2, we showed that the noun as well
as the light verb in a sentence influence the type of
semantic roles that will occur. Our method builds
on this idea and uses two resources in order to de-
rive linguistic knowledge about the NVC: PropBank
frame files for simple verbs in Hindi and the Hindi
Treebank, annotated with dependency labels. The
next two sections describe the use of these resources
in some detail.
3.1 Karaka to PropBank Mapping
The annotated Hindi Treebank is based on a depen-
dency framework (Begum et al, 2008) and has a
very rich set of dependency labels. These labels
(also known as karaka labels) represent the relations
between a head (e.g. a verb) and its dependents (e.g.
arguments). Using the Treebank we extract all the
dependency karaka label combinations that occur
with a unique instance of an NVC. We filter them
to include argument labels and discard those labels
that are usually used for adjuncts. We then calculate
the most frequently occurring combination of labels
that will occur with that NVC. Finally, we get a tu-
ple consisting of an NVC, a set of karaka argument
labels that occur with it and a count of the number
of times that NVC has occurred in the corpus. The
karaka labels are then mapped onto PropBank la-
bels. We reproduce in Table 3 the numbered argu-
ments to karaka label mapping found in Vaidya et
al., (2011).
PropBank label Treebank label
Arg0 (agent) k1 (karta); k4a (experiencer)
Arg1 (theme,
patient)
k2 (karma)
Arg2 (beneficiary) k4 (beneficiary)
Arg2-ATR(attribute) k1s (attribute)
Arg2-SOU(source) k5 (source)
Arg2-GOL(goal) k2p (goal)
Arg3 (instrument) k3 (instrument)
Table 3: Mapping from Karaka labels to PropBank
3.2 Verb Frames
Our second resource consists of PropBank frames
for full Hindi verbs. Every light verb that occurs in
128
Hindi is also used as a full verb, e.g. de ?give? in
Table 1 may be used both as a ?full? verb as well as
a ?light? verb. As a full verb, it has a frame file in
Hindi PropBank. The set of roles in the full verb
frame is used to generate a ?canonical? verb frame
for each light verb. The argument structure of the
light verb will change when combined with a nom-
inal, which contributes its own arguments. How-
ever, as a default, the canonical argument structure
list captures the fact that most kar ?do? light verbs
are likely to occur with the roles ARG0 and ARG1
respectively or that ho ?become?, an unaccusative
verb, occurs with only ARG1.
3.3 Procedure
Our procedure integrates the two resources de-
scribed above. First, the tuple consisting of karaka
labels for a particular NVC is mapped to PropBank
labels. But many NVC cases occur just once in the
corpus and the karaka label tuple may not be very
reliable. Hence, the likelihood that the mapped tu-
ple accurately depicts the correct semantic frame is
not very high. Secondly, Hindi can drop manda-
tory subjects or objects in a sentence e.g., (vo) ki-
taab paRegaa; ?(He) will read the book?. These are
not inserted by the dependency annotation (Bhatia
et al, 2010) and are not easy to discover automati-
cally (Vaidya et al, 2012). We cannot afford to ig-
nore any of the low frequency cases as each NVC
in the corpus must be annotated with semantic roles.
In order to get reasonable predictions for each NVC,
we use a simple rule. We carry out a mapping from
karaka to PropBank labels only if the NVC occurs at
least 30 times in the corpus. If the NVC occurs fewer
than 30 times, then we use the ?canonical? verb list.
4 Evaluation
The automatic method described in the previous sec-
tion generated 1942 nominal frame files. In or-
der to evaluate the frame files, we opted for man-
ual checking of the automatically generated frames.
The frame files were checked by three linguists and
the checking focused on the validity of the seman-
tic roles. The linguists also indicated whether an-
notation errors or duplicates were present. There
was some risk that the automatically derived frames
could bias the linguists? choice of roles as it is
quicker to accept a given suggestion than propose
an entirely new set of roles for the NVC. As we
had a very large number of automatically gener-
ated frames, all of which would need to be checked
manually anyway, practical concerns determined the
choice of this evaluation.
After this process of checking, the total number
of frame files stood at 1884. These frame files con-
sisted of 3015 rolesets i.e. individual combinations
of a nominal with a light verb (see Table 2). The
original automatically generated rolesets were com-
pared with their hand corrected counterparts (i.e.
manually checked ?gold? rolesets) and evaluated for
accuracy. We used three parameters to compare the
gold rolesets with the automatically generated ones:
a full match, partial match and no match. Table 4
shows the results derived from each resource (Sec-
tion 3) and the total accuracy.
Type of Match Full Partial None Errors
Karaka Mapping 25 31 4 0
Verbal Frames 1929 642 249 143
Totals 1954 673 245 143
% Overall 65 22 8 5
Table 4: Automatic mapping results, total frames=3015
The results show that almost two thirds of the se-
mantic roles are guessed correctly by the automatic
method, with an additional 22% partial predictions,
giving us a total of 87% useful predictions. Only
8% show no match at all between the automatically
generated labels and the gold labels.
When we compare the contribution of the karaka
labels with the verb frames, we find that the verb
frames contribute to the majority of the full matches.
The karaka mapping contributes relatively less as
only 62 NVC types occur more than 30 times in
the corpus. If we reduce our frequency requirement
from of 30 to 5, the accuracy drops by 5%. The bulk
of the cases are thus derived from the simple verb
frames. We think that the detailed information in
the verb frames, such as unaccusativity contributes
towards generating the correct frame files.
It is interesting to observe that nearly 65% accu-
racy can be achieved from the verbal information
alone. The treebank has two light verbs that occur
with high frequency i.e. kar ?do? and ho ?become?.
These combine with a variety of nominals but per-
129
Light verb Full (%) None (%) Total
Uses*
kar?do? 64 8 1038
ho ?be/become? 81 3 549
de ?give? 55 34 157
A ?come? 31 42 36
Table 5: Light verbs ?do? and ?be/become? vs. ?give? and
?come?. *The unique total light verb usages in the corpus
form more consistently than light verbs such as de
?give? or A ?come?. The light verb kar adds inten-
tionality to the NVC, but appears less often with a
set of semantic roles that are quite different from
its original ?full? verb usage. In comparison, the
light verbs such as de ?give? show far more varia-
tion, and as seen from Table 4, will match with au-
tomatically derived frames to a lesser extent. The
set of nominals that occur in combination with kar,
usually seem to require only a doer and a thing
done. Borrowed English verbs such dijain?design?
or Pona?phone? will appear preferentially with kar
in the corpus and as they are foreign words they do
not add arguments of their own.
One of the advantages of creating this lexical re-
source is the availability of gold standard frame files
for around 3000 NVCs in Hindi. As a next step, it
would be useful to use these frames to make some
higher level generalizations about these NVCs. For
example, much work has already been done on au-
tomatic verb classification for simple predicates e.g.
(Merlo and Stevenson, 2001; Schulte im Walde,
2006), and perhaps such classes can be derived for
NVCs. Also, the frame files do not currently address
the problem of polysemous NVCs which could ap-
pear with a different set of semantic roles, which will
be addressed in future work.
Acknowledgments
I am grateful to Archna Bhatia and Richa Srishti for
their help with evaluating the accuracy of the nom-
inal frames. This work is supported by NSF grants
CNS-0751089, CNS-0751171, CNS-0751202, and
CNS-0751213.
References
Tafseer Ahmed, Miriam Butt, Annette Hautli, and Se-
bastian Sulger. 2012. A reference dependency bank
for analyzing complex predicates. In Proceedings of
the Eight International Conference on Language Re-
sources and Evaluation (LREC?12.
Rafiya Begum, Samar Husain, Arun Dhwaj, Dipti Misra
Sharma, Lakshmi Bai, and Rajeev Sangal. 2008. De-
pendency Annotation Scheme for Indian Languages.
In Proceedings of The Third International Joint Con-
ference on Natural Language Processing (IJCNLP).
Hyderabad, India.
Archna Bhatia, Rajesh Bhatt, Bhuvana Narasimhan,
Martha Palmer, Owen Rambow, Dipti Misra Sharma,
Michael Tepper, Ashwini Vaidya, and Fei Xia. 2010.
Empty Categories in a Hindi Treebank. In Proceed-
ings of the 7th International Conference on Language
Resources and Evaluation (LREC?10), pages 1863?
1870.
Miriam Butt. 1993. The Light Verb Jungle. In G. Aygen,
C. Bowers, and C. Quinn, editors, Harvard Working
Papers in Linguistics: Papers from the GSAS/Dudley
House workshop on light verbs, volume 9.
Afsaneh Fazly and Suzanne Stevenson. 2007. Au-
tomatic Acquisition of Knowledge about Multiword
Predicates. In Proceedings of PACLIC 19, the 19th
Asia-Pacific Conference on Language, Information
and Computation.
Jena D. Hwang, Archna Bhatia, Claire Bonial, Aous
Mansouri, Ashwini Vaidya, Nianwen Xue, and Martha
Palmer. 2010. PropBank Annotation of Multilingual
Light Verb Constructions. In Proceedings of the Lin-
guistic Annotation Workshop held in conjunction with
ACL-2010.
Karine Megerdoomian. 2001. Event Structure and Com-
plex Predicates in Persian. Canadian Journal of Lin-
guistics, 46:97?125.
Paola Merlo and Suzanne Stevenson. 2001. Automatic
Verb Classification Based on Statistical Distributions
of Argument Structure. Computational Linguistics,
27(3):373?408.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
106.
Martha Palmer, Olga Babko-Malaya, Ann Bies, Mona
Diab, Mohammed Maamouri, Aous Mansouri, and
Wajdi Zaghouani. 2008. A pilot Arabic PropBank.
In Proceedings of the 6th International Language Re-
sources and Evaluation.
Martha Palmer, Rajesh Bhatt, Bhuvana Narasimhan,
Owen Rambow, Dipti Misra Sharma, and Fei Xia.
2009. Hindi Syntax: Annotating Dependency, Lexical
130
Predicate-Argument Structure, and Phrase Structure.
In Proceedings of ICON-2009: 7th International Con-
ference on Natural Language Processing, Hyderabad.
Sabine Schulte im Walde. 2006. Experiments on the Au-
tomatic Induction of German Semantic Verb Classes.
Computational Linguistics, 32(2):159?194.
Ashwini Vaidya, Jinho D. Choi, Martha Palmer, and Bhu-
vana Narasimhan. 2011. Analysis of the Hindi propo-
sition bank using dependency structure. In Proceed-
ings of the 5th Linguistic Annotation Workshop - LAW
V ?11.
Ashwini Vaidya, Jinho D. Choi, Martha Palmer, and Bhu-
vana Narasimhan. 2012. Empty Argument Insertion
in the Hindi PropBank. In Proceedings of the Eighth
International Conference on Language Resources and
Evaluation - LREC-12, Istanbul.
Nianwen Xue and Martha Palmer. 2003. Annotating the
Propositions in the Penn Chinese Treebank. In Pro-
ceedings of the 2nd SIGHAN workshop on Chinese
language processing, SIGHAN?03, pages 47?54.
131
Language Technology for Closely Related Languages and Language Variants (LT4CloseLang), pages 47?55,
October 29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Adapting Predicate Frames for Urdu PropBanking
Riyaz Ahmad Bhat
?
, Naman Jain
?
, Dipti Misra Sharma
?
, Ashwini Vaidya
?
,
Martha Palmer
?
, James Babani
?
and Tafseer Ahmed
?
LTRC, IIIT-H, Hyderabad, India
?
University of Colorado, Boulder, CO 80309 USA
?
DHA Suffa University, Karachi, Pakistan
?
{riyaz.bhat, naman.jain}@research.iiit.ac.in, dipti@iiit.ac.in,
{vaidyaa,mpalmer, james.babani}@colorado.edu, tafseer@dsu.edu.pk
Abstract
Hindi and Urdu are two standardized reg-
isters of what has been called the Hindus-
tani language, which belongs to the Indo-
Aryan language family. Although, both
the varieties share a common grammar,
they differ significantly in their vocabulary
to an extent where both become mutually
incomprehensible (Masica, 1993). Hindi
draws its vocabulary from Sanskrit while
Urdu draws its vocabulary from Persian,
Arabic and even Turkish. In this paper,
we present our efforts to adopt frames of
nominal and verbal predicates that Urdu
shares with either Hindi or Arabic for
Urdu PropBanking. We discuss the fea-
sibility of porting such frames from either
of the sources (Arabic or Hindi) and also
present a simple and reasonably accurate
method to automatically identify the ori-
gin of Urdu words which is a necessary
step in the process of porting such frames.
1 Introduction
Hindi and Urdu, spoken primarily in northern In-
dia and Pakistan, are socially and even officially
considered two different language varieties. How-
ever, such a division between the two is not es-
tablished linguistically. They are two standard-
ized registers of what has been called the Hindus-
tani language, which belongs to the Indo-Aryan
language family. Masica (1993) explains that,
while they are different languages officially, they
are not even different dialects or sub-dialects in
a linguistic sense; rather, they are different liter-
ary styles based on the same linguistically defined
sub-dialect. He further explains that at the collo-
quial level, Hindi and Urdu are nearly identical,
both in terms of core vocabulary and grammar.
However, at formal and literary levels, vocabu-
lary differences begin to loom much larger (Hindi
drawing its higher lexicon from Sanskrit and Urdu
from Persian and Arabic) to the point where the
two styles/languages become mutually unintelligi-
ble. In written form, not only the vocabulary but
the way Urdu and Hindi are written makes one be-
lieve that they are two separate languages. They
are written in separate orthographies, Hindi be-
ing written in Devanagari, and Urdu in a modi-
fied Persio-Arabic script. Given such (apparent)
divergences between the two varieties, two paral-
lel treebanks are being built under The Hindi-Urdu
treebanking Project (Bhatt et al., 2009; Xia et al.,
2009). Both the treebanks follow a multi-layered
and multi-representational framework which fea-
tures Dependency, PropBank and Phrase Structure
annotations. Among the two treebanks the Hindi
treebank is ahead of the Urdu treebank across all
layers. In the case of PropBanking, the Hindi tree-
bank has made considerable progress while Urdu
PropBanking has just started.
The creation of predicate frames is the first step
in PropBanking, which is followed by the actual
annotation of verb instances in corpora. In this
paper, we look at the possibility of porting re-
lated frames from Arabic and Hindi PropBanks for
Urdu PropBanking. Given that Urdu shares its vo-
cabulary with Arabic, Hindi and Persian, we look
at verbal and nominal predicates that Urdu shares
with these languages and try to port and adapt their
frames from the respective PropBanks instead of
creating them afresh. This implies that identifi-
cation of the source of Urdu predicates becomes
a necessary step in this process. Thus, in order
to port the relevant frames, we need to first iden-
tify the source of Urdu predicates and then extract
their frames from the related PropBanks. To state
briefly, we present the following as contributions
of this paper:
? Automatic identification of origin or source
of Urdu vocabulary.
47
? Porting and adapting nominal and verbal
predicate frames from the PropBanks of re-
lated languages.
The rest of the paper is organised as follows: In
the next Section we discuss the Hindi-Urdu tree-
banking project with the focus on PropBanking.
In Section 3, we discuss our efforts to automati-
cally identify the source of Urdu vocabulary and
in Section 4, we discuss the process of adapting
and porting Arabic and Hindi frames for Urdu
PropBanking. Finally we conclude with some
future directions in Section 5.
2 A multi-layered,
multi-representational treebank
Compared to other existing treebanks, Hindi/Urdu
Treebanks (HTB/UTB) are unusual in that they are
multi-layered. They contain three layers of anno-
tation: dependency structure (DS) for annotation
of modified-modifier relations, PropBank-style
annotation (PropBank) for predicate-argument
structure, and an independently motivated phrase-
structure (PS). Each layer has its own framework,
annotation scheme, and detailed annotation guide-
lines. Due to lack of space and relevance to our
work, we only look at PropBanking with reference
to Hindi PropBank, here.
2.1 PropBank Annotation
The first PropBank, the English PropBank (Kings-
bury and Palmer, 2002), originated as a one-
million word subset of the Wall Street Journal
(WSJ) portion of Penn Treebank II (an English
phrase structure treebank). The verbs in the Prop-
Bank are annotated with predicate-argument struc-
tures and provide semantic role labels for each
syntactic argument of a verb. Although these
were deliberately chosen to be generic and theory-
neutral (e.g., ARG0, ARG1), they are intended
to consistently annotate the same semantic role
across syntactic variations. For example, in both
the sentences John broke the window and The win-
dow broke, ?the window? is annotated as ARG1
and as bearing the role of ?Patient?. This reflects
the fact that this argument bears the same seman-
tic role in both the cases, even though it is realized
as the structural subject in one sentence and as the
object in the other. This is the primary difference
between PropBank?s approach to semantic role la-
bels and the Paninian approach to karaka labels,
which it otherwise resembles closely. PropBank?s
ARG0 and ARG1 can be thought of as similar
to Dowty?s prototypical ?Agent? and ?Patient?
(Dowty, 1991). PropBank provides, for each sense
of each annotated verb, its ?roleset?, i.e., the possi-
ble arguments of the predicate, their labels and all
possible syntactic realizations. The primary goal
of PropBank is to supply consistent, simple, gen-
eral purpose labeling of semantic roles for a large
quantity of coherent text that can provide training
data for supervised machine learning algorithms,
in the same way that the Penn Treebank supported
the training of statistical syntactic parsers.
2.1.1 Hindi PropBank
The Hindi PropBank project has differed signif-
icantly from other PropBank projects in that the
semantic role labels are annotated on dependency
trees rather than on phrase structure trees. How-
ever, it is similar in that semantic roles are defined
on a verb-by-verb basis and the description at
the verb-specific level is fine-grained; e.g., a
verb like ?hit? will have ?hitter? and ?hittee?.
These verb-specific roles are then grouped into
broader categories using numbered arguments
(ARG). Each verb can also have a set of modifiers
not specific to the verb (ARGM). In Table 1,
PropBank-style semantic roles are listed for
the simple verb de ?to give?. In the table, the
numbered arguments correspond to the giver,
thing given and recipient. Frame file definitions
are created manually and include role information
as well as a unique roleset ID (e.g. de.01 in Table
1), which is assigned to every sense of a verb. In
addition, for Hindi the frame file also includes the
transitive and causative forms of the verb (if any).
Thus, the frame file for de ?give? will include
dilvaa ?cause to give?.
de.01 to give
Arg0 the giver
Arg1 thing given
Arg2 recipient
Table 1: A Frame File
The annotation process for the PropBank takes
place in two stages: the creation of frame files for
individual verb types, and the annotation of pred-
icate argument structures for each verb instance.
The annotation for each predicate in the corpus
is carried out based on its frame file definitions.
48
The PropBank makes use of two annotation tools
viz. Jubilee (Choi et al., 2010b) and Cornerstone
(Choi et al., 2010a) for PropBank instance annota-
tion and PropBank frame file creation respectively.
For annotation of the Hindi and Urdu PropBank,
the Jubilee annotation tool had to be modified to
display dependency trees and also to provide ad-
ditional labels for the annotation of empty argu-
ments.
3 Identifying the source of Urdu
Vocabulary
Predicting the source of a word is similar to lan-
guage identification where the task is to identify
the language a given document is written in. How-
ever, language identification at word level is more
challenging than a typical document level lan-
guage identification problem. The number of fea-
tures available at document level is much higher
than at word level. The available features for word
level identification are word morphology, syllable
structure and phonemic (letter) inventory of the
language(s).
In the case of Urdu, the problem is even more
complex as the borrowed words don?t necessarily
carry the inflections of their source language and
don?t retain their identity as such (they undergo
phonetic changes as well). For example, khabar
?news? which is an Arabic word declines as per
the morphological paradigm of feminine nom-
inals in Hindi and Urdu as shown in Table (2).
However, despite such challenges, if we look at
the character histogram in Figure (1), we can still
identify the source of a sufficiently large portion
of Urdu vocabulary just by using letter-based
heuristics. For example neither Arabic nor Persian
has aspirated consonants like bH, ph Aspirated
Bilabial Plosives; tSh, dZH Aspirated Alveolar
Fricatives; ?H Aspirated Retroflex Plosive; gH, kh
Aspirated Velar Plosives etc. while Hindi does.
Similarly, the following sounds occur only in
Arabic and Persian: Z Fricative Postalveolar; T,
D Fricative Dental; ? Fricative Pharyngeal; X
Fricative Uvular etc. Using these heuristics we
could identify 2,682 types as Indic, and 3,968
as either Persian or Arabic out of 12,223 unique
types in the Urdu treebank (Bhat and Sharma,
2012).
Singular Plural
Direct khabar khabarain
Oblique khabar khabaron
Table 2: Morphological Paradigm of khabar
This explains the efficiency of n-gram based ap-
proaches to either document level or word level
language identification tasks as reported in the re-
cent literature on the problem (Dunning, 1994;
Elfardy and Diab, 2012; King and Abney, 2013;
Nguyen and Dogruoz, 2014; Lui et al., 2014).
In order to predict the source of an Urdu word,
we frame two classification tasks: (1) binary clas-
sification into Indic and Persio-Arabic and, (2) tri-
class classification into Arabic, Indic and Persian.
Both the problems are modeled using smoothed n-
gram based language models.
3.1 N-gram Language Models
Given a word w to classify into one of k classes
c
1
, c
2
, ... , c
k
, we will choose the class with the
maximum conditional probability:
c
?
= argmax
c
i
p(c
i
|w)
= argmax
c
i
p(w|c
i
) ? p(c
i
)
(1)
The prior distribution p(c) of a class is esti-
mated from the respective training sets shown in
Table (3). Each training set is used to train a
separate letter-based language model to estimate
the probability of word w. The language model
p(w) is implemented as an n-gram model using
the IRSTLM-Toolkit (Federico et al., 2008) with
Kneser-Ney smoothing. The language model is
defined as:
p(w) =
n
?
i=1
p(l
i
|l
i?1
i?k
) (2)
where, l is a letter and k is a parameter indicat-
ing the amount of context used (e.g., k = 4 means
5-gram model).
3.2 Etymological Data
In order to prepare training and testing data
marked with etymological information for our
classification experiments, we used the Online
1
http://www.langsci.ucl.ac.uk/ipa/IPA chart %28C%
292005.pdf
49
bH Z T ? X D sQ tQ dQ Q DQ G f tSh q ?H gH khdZH N ? ph S ? th t?h d?H tS b d g H k dZ m l n p s r t t? V j d?
0
5 ? 10
?2
0.1
0.15
0.2
0.25
0.3
0.35
Alphabets in IPA
1
R
e
l
a
t
i
v
e
F
r
e
q
u
e
n
c
y
Arabic
Hindi
Persian
Urdu
Figure 1: Relative Distribution of Arabic, Hindi, Persian and Urdu Alphabets (Consonants only)
Urdu Dictionary
2
(henceforth OUD). OUD has
been prepared under the supervision of the e-
government Directorate of Pakistan
3
. Apart from
basic definition and meaning, it provides etymo-
logical information for more than 120K Urdu
words. Since the dictionary is freely
4
available
and requires no expertise for extraction of word
etymology which is usually the case with manual
annotation, we could mark the etymological infor-
mation on a reasonably sized word list in a limited
time frame. The statistics are provided in Table
(3). We use Indic as a cover term for all the words
that are either from Sanskrit, Prakrit, Hindi or lo-
cal languages.
Language Data Size Average Token Length
Arabic 6,524 6.8
Indic 3,002 5.5
Persian 4,613 6.5
Table 3: Statistics of Etymological Data
2
http://182.180.102.251:8081/oud/default.aspx
3
www.e-government.gov.pk
4
We are not aware of an offline version of OUD.
3.3 Experiments
We carried out a number of experiments in order
to explore the effect of data size and the order of
n-gram models on the classification performance.
By varying the size of training data, we wanted to
identify the lower bound on the training size with
respect to the classification performance. We var-
ied the training size per training iteration by 1%
for n-grams in the order 1-5 for both the classifi-
cation problems. For each n-gram order 100 ex-
periments were carried out, i.e overall 800 exper-
iments for binary and tri-class classification. The
impact of training size on the classification perfor-
mance is shown in Figures (2) and (3) for binary
and tri-class classification respectively. As ex-
pected, at every iteration the additional data points
introduced into the training data increased the per-
formance of the model. With a mere 3% of the
training data, we could reach a reasonable accu-
racy of 0.85 in terms of F-score for binary classi-
fication and for tri-class classification we reached
the same accuracy with 6% of the data.
Similarly, we tried different order n-gram mod-
els to quantify the effect of character context on
50
the classification performance. As with the in-
crease in data size, increasing the n-gram order
profoundly improved the results. In both the clas-
sification tasks, unigram based models converge
faster than the higher order n-gram based models.
The obvious reason for it is the small, finite set of
characters that a language operates with (? 37 in
Arabic, ? 39 in Persian and ? 48 in Hindi). A
small set of words (unique in our case) is probably
enough to capture at least a single instance of each
character. As no new n-gram is introduced with
subsequent additions of new tokens in the training
data, the accuracy stabilizes. However, the accu-
racy with higher order n-grams kept on increas-
ing with an increase in the data size, though it was
marginal after 5-grams. The abrupt increase after
8,000 training instances is probably due to the ad-
dition of an unknown bigram sequence(s) to the
training data. In particular, the Recall of Persio-
Arabic increased by 2.2%.
0 0.2 0.4 0.6 0.8 1 1.2
?10
4
0.5
0.6
0.7
0.8
0.9
1
Training Data Size
F
-
S
c
o
r
e
1-gram
2-gram
3-gram
4-gram
Figure 2: Learning Curves for Binary Classifica-
tion of Urdu Vocabulary
3.4 Results
We performed 10-fold cross validation over all the
instances of the etymological data for both the bi-
nary and tri-class classification tasks. We split the
data into training and testing sets with a ratio of
80:20 using the stratified sampling. Stratified sam-
pling distributes the samples of each class in train-
ing and testing sets with the same percentage as in
the complete set. For all the 10-folds, the order of
0 0.2 0.4 0.6 0.8 1 1.2
?10
4
0.5
0.6
0.7
0.8
0.9
1
Training Data Size
F
-
S
c
o
r
e
1-gram
2-gram
3-gram
4-gram
Figure 3: Learning Curves for Tri-class Classifi-
cation of Urdu Vocabulary
n-gram was varied again from 1-5. Tables (4) and
(5) show the consolidated results for these tasks
with a frequency based baseline to evaluate the
classification performance. In both the tasks, we
achieved highest accuracy with language models
trained with 5-gram letter sequence context. The
best results in terms of F-score are 0.96 and 0.93
for binary and tri-class classification respectively.
Type Precision (P) Recall (R) F1-Score (F)
Baseline 0.40 0.50 0.40
1-gram 0.89 0.89 0.89
2-gram 0.95 0.95 0.95
3-gram 0.96 0.96 0.96
4-gram 0.96 0.96 0.96
5-gram 0.96 0.96 0.96
Table 4: Results of 10-fold Cross Validation on
Binary Classification
Although, we have achieved quite reasonable
accuracies in both the tasks, a closer look at the
confusion matrices shown in Tables (6) and (7)
show that we can still improve the accuracies by
balancing the size of data across classes. In binary
classification our model is more biased towards
Persio-Arabic as the data is highly imbalanced.
Our binary classifier misclassifies 0.86% of Indic
tokens as Persio-Arabic since the prior probability
of the latter is much higher than that of the former.
While in the case of tri-class classification, using
51
Type Precision (P) Recall (R) F1-Score (F)
Baseline 0.15 0.33 0.21
1-gram 0.83 0.83 0.83
2-gram 0.89 0.89 0.89
3-gram 0.91 0.91 0.91
4-gram 0.93 0.93 0.93
5-gram 0.93 0.93 0.93
Table 5: Results of 10-fold Cross Validation on
Tri-Class Classification
higher order n-gram models can resolve the
prominent confusion between Arabic and Persian.
Since both Arabic and Persian share almost the
same phonetic inventory, working with lower
order n-gram models doesn?t seem ideal.
Class Indic Persio-Arabic
Indic 235 60
Persio-Arabic 15 1,057
Table 6: Confusion Matrix of Binary Classifica-
tion
Class Arabic Indic Persian
Arabic 605 5 26
Indic 11 268 18
Persian 22 9 415
Table 7: Confusion Matrix of Tri-class Classifica-
tion
4 Adapting Frames from Arabic and
Hindi PropBanks
As discussed in Section 2.1.1, the creation of pred-
icate frames precedes the actual annotation of verb
instances in a given corpus. In this section, we de-
scribe our approach towards the first stage of Urdu
PropBanking by adapting related predicate frames
from Arabic and Hindi PropBanks (Palmer et al.,
2008; Vaidya et al., 2011). Since a PropBank
is not available for Persian, we could only adapt
those predicate frames which are shared with Ara-
bic and Hindi.
Although, Urdu shares or borrows most of its
literary vocabulary from Arabic and Persian, it re-
tains its simple verb (as opposed to compound or
complex verbs) inventory from Indo-Aryan ances-
try. Verbs from Arabic and Persian are borrowed
less frequently, although there are examples such
as ?khariid? buy, ?farma? say etc.
5
This over-
lap in the verb inventory between Hindi and Urdu
might explain the fact that they share the same
grammar.
The fact that Urdu shares its lexicon with these
languages, prompted us towards exploring the
possibility of using their resources for Urdu Prop-
Banking. We are in the process of adapting frames
for those Urdu predicates that are shared with ei-
ther Arabic or Hindi.
Urdu frame file creation must be carried out for
both simple verbs and complex predicates. Since
Urdu differs very little in simple verb inventory
from Hindi, this simplifies the development pro-
cess as the frames could be ported easily. How-
ever, this is not the case with nominal predicates.
In Urdu, many nominal predicates are borrowed
from Arabic or Persian as shown in Table (8).
Given that a PropBank for Persian is not available,
the task of creating the frames for nominal predi-
cates in Urdu would have been fairly daunting in
the paucity of the Arabic PropBank, as well.
Simple Verbs Nominal Predicates
Language Total Unique Total Unique
Arabic 12 1 6,780 765
Hindi 7,332 441 1,203 258
Persian 69 3 2,276 352
Total 7,413 445 10,259 1,375
Table 8: Urdu Treebank Predicate Statistics
4.1 Simple Verbs
The simple verb inventory of Urdu and Hindi is
almost similar, so the main task was to locate and
extract the relevant frames from Hindi frame files.
Fortunately, with the exception of farmaa ?say?,
all the other simple verbs which Urdu borrows
from Persian or Arabic (cf. Table (8)) were also
borrowed by Hindi. Therefore, the Hindi sim-
ple verb frame files sufficed for porting frames for
Urdu simple verbs.
There were no significant differences found be-
tween the Urdu and Hindi rolesets, which describe
either semantic variants of the same verb or its
causative forms. Further, in order to name the
frame files with their corresponding Urdu lemmas,
we used Konstanz?s Urdu transliteration scheme
5
Borrowed verbs often do not function as simple verbs
rather they are used like nominals in complex predicate con-
structions such as mehsoos in ?mehsoos karnaa? to feel.
52
(Malik et al., 2010) to convert a given lemma into
its romanized form. Since the Hindi frame files
use the WX transliteration scheme
6
, which is not
appropriate for Urdu due to lack of coverage for
Persio-Arabic phonemes or sounds like dQ ?pha-
ryngealized voiced alveolar stop?. The frame files
also contain example sentences for each predicate,
in order to make the PropBank annotation task eas-
ier. While adapting the frame files from Hindi
to Urdu, simply transliterating such examples for
Urdu predicates was not always an option, because
sentences consisting of words with Sanskrit origin
may not be understood by Urdu speakers. Hence,
all the examples in the ported frames have been
replaced with Urdu sentences by an Urdu expert.
In general we find that the Urdu verbs are quite
similar to Hindi verbs, and this simplified our task
of adapting the frames for simple verbs. The
nouns, however, show more variation. Since a
large proportion (up to 50%) of Urdu predicates
are expressed using verb-noun complex predi-
cates, nominal predicates play a crucial role in our
annotation process and must be accounted for.
4.2 Complex Predicates
In the Urdu treebank, there are 17,672 predicates,
of which more than half have been identified as
noun-verb complex predicates (NVC) at the de-
pendency level. Typically, a noun-verb complex
predicate chorii ?theft? karnaa ?to do? has two
components: a noun chorii and a light verb karnaa
giving us the meaning ?steal?. The verbal compo-
nent in NVCs has reduced predicating power (al-
though it is inflected for person, number, and gen-
der agreement as well as tense, aspect and mood)
and its nominal complement is considered the true
predicate. In our annotation of NVCs, we fol-
low a procedure common to all PropBanks, where
we create frame files for the nominal or the ?true?
predicate (Hwang et al., 2010). An example of a
frame file for a noun such as chorii is described in
Table (9).
The creation of a frame file for the set of
true predicates that occur in an NVC is impor-
tant from the point of view of linguistic annota-
tion. Given the large number of NVCs, a semi-
automatic method has been proposed for creating
Hindi nominal frame files, which saves the man-
ual effort required for creating frames for nearly
6
http://en.wikipedia.org/wiki/WX notation
Frame file for chorii-n(oun)
chorii.01: theft-n light verb: kar?do; to steal?
Arg0 person who steals
Arg1 thing stolen
chorii.02 : theft-n light verb: ho ?be/become; to
get stolen?
Arg1 thing stolen
Table 9: Frame file for predicate noun chorii
?theft? with two frequently occurring light verbs
ho and kar. If other light verbs are found to occur,
they are added as additional rolesets as chorii.03,
chorii.04 and so on.
3,015 unique Hindi noun and light verb combina-
tions (Vaidya et al., 2013).
For Urdu, the process of nominal frame file cre-
ation is preceded by the identification of the ety-
mological origin for each nominal. If that nomi-
nal has an Indic or Arabic origin, relevant frames
from Arabic or Hindi PropBanks were adapted for
Urdu. On the other hand, if the Urdu nominal orig-
inates from Persian, then frame creation will be
done either manually or using other available Per-
sian language resources, in the future.
In Table (8), there are around 258 nominal pred-
icates that are common in Hindi and Urdu, so we
directly ported their frames from Hindi PropBank
with minor changes as was done for simple verb
frames. Out of 765 nominal predicates shared with
Arabic, 308 nominal predicate frames have been
ported to Urdu. 98 of these nominal predicate
frames were already present in the Arabic Prop-
Bank and were ported as such. However, for the
remaining 667 unique predicates, frames are be-
ing created manually by Arabic PropBanking ex-
perts and will be ported to Urdu once they become
available.
Porting of Arabic frames to Urdu is not that triv-
ial. We observed that while Urdu borrows vocabu-
lary from Arabic it does not borrow all the senses
for some words. In such cases, the rolesets that are
irrelevant to Urdu have to be discarded manually.
The example sentences for all the frames ported
from Arabic PropBank have to be sourced from
either the web or manually created by an Urdu ex-
pert, as was the case with Hindi simple verbs.
5 Conclusion
In this paper we have exploited the overlap be-
tween the lexicon of Urdu, Arabic and Hindi for
the creation of predicate frames for Urdu Prop-
53
Banking. We presented a simple and accurate clas-
sifier for the identification of source or origin of
Urdu vocabulary which is a necessary step in the
overall process of extraction of predicate frames
from the related PropBanks. In the case of sim-
ple verbs that occur in the Urdu treebank, we have
extracted all the frames from the Hindi PropBank
and adapted them for Urdu PropBanking. Simi-
larly for complex predicates, frames for Urdu tree-
bank nominal predicates are extracted from Hindi
as well as from Arabic PropBanks. Since a Prop-
Bank is not available for Persian, the creation
of frames for shared predicates with Persian is a
prospect for future work. We plan to create these
frames either manually or semi-automatically, us-
ing the available Persian Dependency treebanks
(Rasooli et al., 2011; Rasooli et al., 2013).
Acknowledgments
We would like to thank Himani Chaudhry for her
valuable comments that helped to improve the
quality of this paper.
The work reported in this paper is supported by
the NSF grant (Award Number: CNS 0751202;
CFDA Number: 47.070)
7
.
References
Riyaz Ahmad Bhat and Dipti Misra Sharma. 2012.
A dependency treebank of urdu and its evaluation.
In Proceedings of the Sixth Linguistic Annotation
Workshop, pages 157?165. Association for Compu-
tational Linguistics.
Rajesh Bhatt, Bhuvana Narasimhan, Martha Palmer,
Owen Rambow, Dipti Misra Sharma, and Fei Xia.
2009. A multi-representational and multi-layered
treebank for hindi/urdu. In Proceedings of the Third
Linguistic Annotation Workshop, pages 186?189.
Association for Computational Linguistics.
Jinho D Choi, Claire Bonial, and Martha Palmer.
2010a. Propbank frameset annotation guidelines us-
ing a dedicated editor, cornerstone. In LREC.
Jinho D Choi, Claire Bonial, and Martha Palmer.
2010b. Propbank instance annotation guidelines us-
ing a dedicated editor, jubilee. In LREC.
David Dowty. 1991. Thematic proto-roles and argu-
ment selection. Language, 67(3):547?619.
7
Any opinions, findings, and conclusions or recommen-
dations expressed in this material are those of the author(s)
and do not necessarily reflect the views of the National Sci-
ence Foundation.
Ted Dunning. 1994. Statistical identification of lan-
guage. Computing Research Laboratory, New Mex-
ico State University.
Heba Elfardy and Mona T Diab. 2012. Token level
identification of linguistic code switching. In COL-
ING (Posters), pages 287?296.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. Irstlm: an open source toolkit for han-
dling large scale language models. In Interspeech,
pages 1618?1621.
Jena D Hwang, Archna Bhatia, Clare Bonial, Aous
Mansouri, Ashwini Vaidya, Nianwen Xue, and
Martha Palmer. 2010. Propbank annotation of mul-
tilingual light verb constructions. In Proceedings of
the Fourth Linguistic Annotation Workshop, pages
82?90. Association for Computational Linguistics.
Ben King and Steven P Abney. 2013. Labeling the
languages of words in mixed-language documents
using weakly supervised methods. In HLT-NAACL,
pages 1110?1119.
Paul Kingsbury and Martha Palmer. 2002. From tree-
bank to propbank. In LREC. Citeseer.
Marco Lui, Jey Han Lau, and Timothy Baldwin. 2014.
Automatic detection and language identification of
multilingual documents. Transactions of the Asso-
ciation for Computational Linguistics, 2:27?40.
Muhammad Kamran Malik, Tafseer Ahmed, Sebastian
Sulger, Tina B?ogel, Atif Gulzar, Ghulam Raza, Sar-
mad Hussain, and Miriam Butt. 2010. Transliterat-
ing urdu for a broad-coverage urdu/hindi lfg gram-
mar. In LREC.
Colin P Masica. 1993. The Indo-Aryan Languages.
Cambridge University Press.
Dong Nguyen and A Seza Dogruoz. 2014. Word
level language identification in online multilingual
communication. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing.
Martha Palmer, Olga Babko-Malaya, Ann Bies,
Mona T Diab, Mohamed Maamouri, Aous Man-
souri, and Wajdi Zaghouani. 2008. A pilot arabic
propbank. In LREC.
Mohammad Sadegh Rasooli, Amirsaeid Moloodi,
Manouchehr Kouhestani, and Behrouz Minaei-
Bidgoli. 2011. A syntactic valency lexicon for
persian verbs: The first steps towards persian de-
pendency treebank. In 5th Language & Technology
Conference (LTC): Human Language Technologies
as a Challenge for Computer Science and Linguis-
tics, pages 227?231.
Mohammad Sadegh Rasooli, Manouchehr Kouhestani,
and Amirsaeid Moloodi. 2013. Development of a
persian syntactic dependency treebank. In Proceed-
ings of the 2013 Conference of the North Ameri-
can Chapter of the Association for Computational
54
Linguistics: Human Language Technologies, pages
306?314.
Ashwini Vaidya, Jinho D Choi, Martha Palmer, and
Bhuvana Narasimhan. 2011. Analysis of the hindi
proposition bank using dependency structure. In
Proceedings of the 5th Linguistic Annotation Work-
shop, pages 21?29. Association for Computational
Linguistics.
Ashwini Vaidya, Martha Palmer, and Bhuvana
Narasimhan. 2013. Semantic roles for nominal
predicates: Building a lexical resource. NAACL
HLT 2013, 13:126.
Fei Xia, Owen Rambow, Rajesh Bhatt, Martha Palmer,
and Dipti Misra Sharma. 2009. Towards a multi-
representational treebank. In The 7th International
Workshop on Treebanks and Linguistic Theories.
Groningen, Netherlands.
55
Proceedings of the 5th Workshop on South and Southeast Asian NLP, 25th International Conference on Computational Linguistics, pages 1?10,
Dublin, Ireland, August 23-29 2014.
Towards Identifying Hindi/Urdu Noun Templates in Support of a
Large-Scale LFG Grammar
Sebastian Sulger
Department of Linguistics
University of Konstanz
Germany
sebastian.sulger@uni-konstanz.de
Ashwini Vaidya
University of Colorado
Boulder, CO
80309 USA
vaidyaa@colorado.edu
Abstract
Complex predicates (CPs) are a highly productive predicational phenomenon in Hindi and Urdu
and present a challenge for deep syntactic parsing. For CPs, a combination of a noun and light
verb express a single event. The combinatorial preferences of nouns with one (or more) light
verb is useful for predicting an instance of a CP. In this paper, we present a semi-automatic
method to obtain noun groups based on their co-occurrences with light verbs. These noun groups
represent the likelihood of a particular noun-verb combination in a large corpus. Finally, in
order to encode this in an LFG grammar, we propose linking nouns with templates that describe
preferable combinations with light verbs.
1 Introduction
A problem that crops up repeatedly in shallow and deep syntactic parsing approaches to South Asian lan-
guages like Urdu and Hindi
1
is the proper treatment of complex predicates (CPs). In CPs, combinations
of more than one element are used to express an event (e.g., memory + do = remember). In Urdu/Hindi,
only about 700 simple verbs exist (Humayoun, 2006); the remaining verbal inventory consists of CPs.
CPs are encountered frequently in general language use, as well as in newspaper corpora. Thus, any NLP
application, whether shallow or deep, whether its goal be parsing, generation, question-answering or the
construction of lexical resources like WordNet (Bhattacharyya, 2010) encounters CPs sooner rather than
later.
There is a range of different elements that may combine with verbs to form a CP: verbs, nouns, prepo-
sitions, adjectives all occur in CPs. The constraints and productive mechanisms in verb-verb CPs are
comparatively well-understood (e.g, see Hook (1974), Butt (1995), Butt (2010) and references therein).
The domain of noun-verb CPs (N-V CPs) is less well understood, the standard theoretical reference being
Mohanan (1994). It is only recently that researchers have tried to come up with linguistic generaliza-
tions regarding N-V CPs, some by using manual methods and linguistic introspection (Ahmed and Butt,
2011), others using a combination of manual and statistical methods (Butt et al., 2012).
Ahmed and Butt (2011) have suggested that the combinatory possibilities of N-V combinations are in
part governed by the lexical semantic compatibility of the noun with the verb. Similar observations have
been made for English (Barrett and Davis, 2003; North, 2005). If this is true, then lexical resources
such as WordNet could be augmented with semantic specifications or feature information that can then
be used to determine dynamically whether a given N-V combination is licit or not.
Knowledge about this kind of lexical-semantic information is essential in computational grammars.
For example,lexicon entries and templates are required to define predicational classes. Implementing
such a grammar for a language that makes heavy use of CPs calls for two requirements. First, the lexical
items taking part in CP formation need to be present in the lexicon of the grammar; and second, the
grammar needs to be engineered in a way that represents the correct linguistic generalizations. Any ap-
1
Urdu is an Indo-Aryan language spoken primarily in Pakistan and parts of India, as well as in the South Asian diaspora. It
is structurally almost identical to Hindi, although the lexicon and orthography differs considerably.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
proach that is short of either of these requirements will result either in loss in coverage or overgeneration
of the grammar.
The Hindi/Urdu ParGram Grammar forms part of a larger international research effort, the ParGram
(Parallel Grammars) project (Butt et al., 1999; Butt et al., 2002; Butt and King, 2007). All of the gram-
mars in the ParGram project are couched within the LFG framework and are implemented using the
development platform XLE (Crouch et al., 2012). The grammars are developed manually and not via
learning methods, which allows for a theoretically sound analysis that is also efficient from a computa-
tional point of view. The Hindi/Urdu ParGram Grammar aims at covering both Hindi and Urdu, which is
a design decision that suggests itself due to the many structural conformities of the two languages (Butt
et al., 2002). One weakness of the grammar is its currently relatively small lexicon, compared to other
ParGram grammars. Adding to the lexicon is a critical step in extending the grammar coverage. This
is even more true for N-V CPs due to the high frequency of such constructions in running text. Thus,
we see the Hindi/Urdu ParGram Grammar as an ideal test bed for developing a lexical resource of Hindi
nouns.
This paper is a first step in terms of constructing such a lexical resource for Hindi nouns. Following
up on previous work, we assume that there are distinct groups of nouns; nouns that are part of a certain
group tend to co-occur with the same light verb(s) and differ in their usage from members of other
groups. Contrary to what has been done before, though, we do not dive into available corpora blindly to
identify the groupings. Instead, we make use of a manually annotated treebank for Hindi, the Hindi and
Urdu Treebank (HUTB, Bhatt et al. (2009)). Thus, we construct a seed list of nouns known to partake
in CP formation in the HUTB. Since the HUTB is limited in its coverage, we then turn to a large Hindi
corpus collected specifically for the present study and use clustering algorithms to put the nouns in the
seed list into groups, based on light verb co-occurrence.
2
Our aim is to arrive at a broad notion of noun similarity. If we can find groups of nouns that be-
have alike with respect to their light verbs, these groups can be included in an application such as the
Hindi/Urdu ParGram Grammar to boost coverage as well as precision. Note that this notion of noun
similarity is not the same as semantic classes in the sense of Levin (1993); however, it can serve as input
for future research into semantic noun classification.
2 N-V Complex Predicates in Hindi and Urdu
As mentioned above, CPs are an important means of forming verbal predication in Hindi and Urdu.
There is no single way of forming CPs; it is possible to find V-V CPs (Butt, 1995), ADJ-V combinations,
P-V CPs (Raza, 2011) and N-V CPs (Mohanan, 1994) (see Ahmed et al. (2012) for some examples of
each CP type.). In the present paper, we focus on identifying patterns of N-V CP formation. Here,
the noun contributes the main predicational content. The verb in such constructions is usually called a
light verb (Mohanan, 1994; Butt, 2003). The term represents the fact that these verbs are semantically
bleached and specify additional information about the predication, such as whether the predicate has an
agentive, telic or stative flavor. The light verb also determines the case marking on the subject, controls
agreement patterns and contributes tense and aspect information. This is illustrated in (1).
(1) a. nadya=ne kAhani yad k-i
Nadya.F.Sg=Erg story.F.Sg memory.F.Sg do-Perf.F.Sg
?Nadya remembered a/the story (agentively).? (lit. ?Nadya did memory of a/the story.?)
b. nadya=ko kAhani yad hE
Nadya.F.Sg=Dat story.F.Sg memory.F.Sg be.Pres.3.Sg
?Nadya remembers/knows a/the story.? (lit. ?At Nadya is memory of a/the story.?)
2
Note that despite the many structural conformities between Hindi and Urdu, the main difference between the two languages
is in the lexicon; Modern Standard Hindi vocabulary is based on Sanskrit, while Urdu draws from a Persio-Arabic lexicon.
This means that in principle, the methodology presented in this paper applied to Hindi needs to be applied to both languages
separately. The equivalent Urdu study is pending future work and currently faces two major obstacles. First, the Urdu portion
of the HUTB has not yet been released. Second, there is a major shortage of Urdu resources, with comparatively small corpora
becoming available only recently (Urooj et al., 2012). Readily available Urdu sources (e.g., Wikipedia) are of minor quality.
2
c. nadya=ko kAhani yad a-yi
Nadya.F.Sg=Dat story.F.Sg memory.F.Sg come-Perf.F.Sg
?Nadya remembered a/the story.? (lit. ?The memory of a/the story came to Nadya.?)
In all of the examples in (1), it is evident that the noun and the verb form a single predicational element.
The object kAhani ?story? is thematically licensed by the noun yad ?memory?, but it is not realized as a
genitive, as would be typical for arguments of nouns (and as in the English literal translations). Rather,
kAhani ?story? functions as the syntactic object of the joint predication (see Mohanan (1994) for details
on the argument structure and agreement patterns).
3 Previous Work
A recent study on the semantic classes of Persian N-V CPs using distributional vector-space methods
has shown that verb vectors are a very useful indicator of noun similarity (Taslimipoor et al., 2012). The
reported results are significantly better using the light verb dimension; Taslimipoor et al. (2012) state that
this affirms their original intuition that a verb-based vector space model can better capture similarities
across CPs. This finding is in agreement with our intuition that features based on light verbs best capture
generalizations about N-V CPs.
There have been two studies on noun similarity based on co-occurrence of noun and light verb. Ahmed
and Butt (2011) look at the light verbs kar ?do?, ho ?be?, hu- ?become? and identify three classes of nouns
based on co-occurrence patters. The first consists of psychological nouns that occur with all three light
verbs. The examples shown in (1) represent the class that is compatible with all of the light verbs
surveyed. Other CP classes may only be compatible with a subset of light verbs. The second and third
classes consist of nouns that are classified as more or less agentive in nature- based on their capacity to
form CPs with hu- ?become?. For example, the noun tamir ?construction? is only compatible with the
light verb kar ?do? but disallows hu- ?become?.
(2) a. b?lal=ne mAkan tAmir k?-ya
Bilal.M.Sg=Erg house.M.Sg construction.F.Sg do-Perf.M.Sg
?Bilal built a/the house.?
b. *b?lal=ko mAkan tAmir hE/hu-a
Bilal.M.Sg=Dat house.M.Sg construction.F.Sg be.Pres.3.Sg/be.Part-Perf.M.Sg
?Bilal built a/the house.?
In a follow-up study, Butt et al. (2012) attempted to identify Urdu N-V CPs automatically. After
filtering out the irrelevant combinations, they found that most nouns were either psychological nouns
(and occurred with all three light verbs) or nouns that were highly agentive and disallowed hu- ?become?.
However, one of the drawbacks of their method was the use of an untagged corpus, which required
extensive filtering in order to separate the light and non-light instances of these verbs.
We will draw upon the results of these two studies to motivate this present work. The classes identified
by Ahmed and Butt (2011) seem promising, but the corpus work was done manually, and the total size
of their data set is limited to 45 nouns. This can hardly serve as input to the development of a large-scale
noun lexicon for a grammar. In constructing a lexical resource, we thus take a different route in that we
try to expand the search space by using an external, manually-crafted resource and a larger set of light
verbs to come up with more substantial noun groups.
3
In addition, we circumvent the problems faced in
Butt et al. (2012)?s paper by filtering the list of nominal predicates in advance and by making use of a
tagged corpus.
3
One might argue that there are other features, beyond the light verbs, that one could use in identifying noun classes/groups,
e.g., additional arguments licensed, case marking, etc. The reason why we (and other researchers before us) limit ourselves
to the light verb occurrences is that Hindi and Urdu make rampant use of pro-drop (Kachru, 2006; Schmidt, 1999; Mohanan,
1994), which means that often, not all arguments are present in a sentence. Thus, the only reliable source of information about
the noun is in fact the light verb, since this is the only obligatory element aside from the noun itself.
3
4 Methodology
In order to build a lexical resource of semantically similar nouns, we first need to identify whether these
occur as part of a N-V CP. As we want to improve upon previous work, our aim was to include a large
number of nouns. The Hindi portion of the Hindi and Urdu Treebank (Bhatt et al., 2009) includes N-V
CPs that have been manually tagged with the dependency label POF (which stands for ?part of?). The
diagnostic criteria used for identifying CPs in the treebank is based on native speaker intuition. The POF
label is used for adjectives and adverbs as well as nouns. We extracted only POF cases that were nouns
only. This gave us an initial list of candidate nouns that were further filtered for spelling variations and
annotation errors. After this stage, we had a list of 1207 nouns, which we will refer to as our seed list.
The seed list consists of nouns that are a part of N-V CPs in the treebank.
Our aim was to include nouns that had at least 50 or more occurrences in order to ensure that we were
looking at the most well-attested noun and light verb co-occurrences. For this task, the Hindi Treebank
corpus (400,000 words) by itself would not be sufficient. For instance, if we applied our cutoff of 50
occurrences to the instances in the treebank, we would be left with only 20 nouns, which would not give
us any meaningful groups. Therefore, we chose to use a larger corpus (including the treebank) in order
to give us co-occurrence patterns for a noun from the seedlist. At the same time, we did not look at
co-occurrence patterns with any verb. Instead, we chose a list of the most frequent light verbs from the
treebank. This list is given below:
(3) ho ?be?, kar ?do?, de ?give?, le ?take?, rakh ?put?, lag ?attach?, a ?come?
Given the seed list and a short list of light verbs, our next step was the extraction of co-occurrences
from a larger corpus.
4.1 Extracting Co-occurrences from a Large Corpus
In order to obtain a larger corpus, we scraped two large online sources of Hindi: the BBC Hindi website
4
as well as the Hindi Wikipedia.
5
Along with the Hindi Treebank, this corpus contains about 21 million
tokens (BBC Hindi: ?7 million, Hindi Wikipedia: ?10 million, Hindi Treebank ?4 million); by includ-
ing the Wikipedia part, the resulting corpus extends beyond the newspaper domain. In a second step, the
corpus was automatically POS tagged using the tagger described in Reddy and Sharoff (2011).
We were interested in extracting co-occurrences that had the following pattern: seed list item + light
verb. A match would only occur if one of the light verbs occurred directly to the right of the noun (i.e.,
an item tagged as NN by the POS tagger). Our method therefore did not take into account any N-V CPs
that were syntactically flexible, i.e., when the noun and the light verb did not occur next to each other.
Those cases where the noun may be scrambled away from the light verb (e.g., topicalization of the noun)
are not numerous and occur rarely in the Hindi Treebank (only about 1% of the time).
6
4.2 Clustering & Evaluation
In the next step, a clustering algorithm was applied to the data. This was done using the clustering tool
described in Lamprecht et al. (2013). At the moment, the tool features two clustering algorithms: the
k-means algorithm (MacQueen, 1967) as well as the Greedy Variance Minimization (GVM) algorithm.
7
We made use of an automatic method using Hindi WordNet (Bhattacharyya, 2010) to choose the best
partition value. We followed the technique described in Van de Cruys (2006), which uses WordNet
relations to arrive at the most semantically coherent clusters. We define semantic coherence as the
similarity between items in a cluster, based on an overlap between their WordNet relations. Specifically,
for each k = 2?10, we iterated through the automatically generated clusters and performed the following
steps:
1. Using WordNet, we extracted synonyms, hypernyms and hyponyms for every word in a cluster.
4
http://www.bbc.co.uk/hindi
5
http://dumps.wikimedia.org/hiwiki
6
Mohanan (1994) even goes so far as to call the topicalization of nouns in N-V CPs ungrammatical.
7
http://code.google.com/p/tomgibara/
4
2 4 6 8 10
0.0
2
0.0
4
0.0
6
0.0
8
0.1
0
0.1
2
0.1
4
Results with GVM and Kmeans, with varying cutoffs
Number of clusters
Se
ma
nti
c c
oh
ere
nc
e
Kmeans;50
GVM;50
Kmeans;3
GVM;3
Figure 1: Choosing the best value for k. k-means with a frequency cutoff of 50 gives us the most
semantically coherent clusters for k = 5
2. A word that had the most semantic relations with every word in the cluster was chosen as its cen-
troid.
3. The co-hyponyms i.e., the hyponyms of the hypernyms for this centroid were extracted from Word-
Net (along with its synonyms, hypernyms and hyponyms).
4. In order to calculate precision for each cluster, we counted the number of words in that cluster that
overlapped with the words in the centroid?s relations.
We averaged the precision across all clusters for every k value. We found that precision for each
cluster gradually improved until we got the most semantically coherent partitions for k = 5 using k-
means, for 522 nouns occurring with a frequency of 50 and above. Table 1 shows the values for k using
our WordNet evaluation method, for k = 5? 9.
Frequency = 3 Frequency = 50
Size of k GVM k-means GVM k-means
5 0.049 0.060 0.107 0.122
6 0.066 0.055 0.121 0.119
7 0.089 0.056 0.104 0.110
8 0.084 0.089 0.108 0.109
9 0.082 0.081 0.095 0.097
Table 1: Semantic coherence values for k = 5? 9 for clustering algorithms GVM and k-means
In Figure 1, we have plotted the semantic coherence values against the number of clusters to show
the best results. The k-means algorithm performed only slightly better than GVM, and after k = 5, the
semantic coherence of the clusters declined again. As a point of comparison, we also plotted k-means
and GVM results for nouns that occurred more than 3 times in the data (i.e., using a far smaller cutoff).
In this configuration, the best results are achieved for a higher k value (i.e., between 7 or 8), but we
rejected this on the basis of a better semantic coherence value for k-means with a cutoff of 50.
5
Figure 2: Visualization for k = 5 clusters
5 Analysis
Lamprecht et al. (2013)?s tool is useful for visual cluster inspection; e.g., the tool created the visual
clustering in Figure 2 using the k-means algorithm with k = 5. The visualization enables the user to in-
spect the data points and derive initial generalizations. For example, Figure 2 shows a visualization with
colored circles that encode membership within a cluster. The larger circles represent cluster centroids.
The visualization enables us to see three most frequently occurring light verbs, viz. kar ?do?, ho ?be? and
de ?give?, represented by light green, dark blue and pink respectively. Many nouns alternate with ?do?
and ?be?, hence there is a visible continuum between the light green and dark blue data points. The two
clusters in the centre show a dark green cluster, consisting of only a handful of nouns that alternate with
the light verbs rakh ?keep?, lag ?attach? and a ?come?. The light blue cluster on the other hand is larger
and is dominated by the light verb le ?take?.
In order to further interpret the results of our study, we also referred to a secondary result from our
WordNet evaluation. While extracting the extent of overlap of the semantic relations, we also extracted
the ?semantic? centroids i.e. words that had the most semantic relations with every other word in the
cluster (see Section 4.2). For our best result of k = 5, these centroids also revealed semantic similarities
in the five clusters that we found. For instance, dynamic events that are inanimate and abstract and take
an agentive argument will lend themselves to combinations with kar ?do?. Similarly, events that include
the semantic property of ?transfer? will occur with de ?give? (although there is ostensibly an overlap here,
as these events invariably also require agentive arguments). The light verb ho ?be? occurs often with
nouns that denote mental states, resulting in an experiencer subject ? but this group also includes nouns
that alternate with kar. Less frequently occurring light verbs, especially rakh ?keep?, lag ?to attach? and
a ?come? show fewer alternations, as they do not occur in combination with all nouns and are grouped
together in this result. These light verbs often form N-V CPs with a more idiosyncratic meaning, in fact
Davison (2005) has argued that some of these light verbs may form ?incorporation idioms? (rather than
true N-V CPs).
The average figures of N-V CP co-occurrences for a certain cluster inform us about the likelihood of
a certain group of nouns to co-occur with a certain light verb. For instance, this noun grouping shows a
high likelihood of occurrence with kar ?do? and le ?take?, but not very likely at all with lag ?attach?. We
take this distribution to reflect a difference in the syntactic behavior of the nouns: While the productive
patterns indicate CP formation, the less productive patterns do not represent CPs at all. This is a finding
in line with Butt et al. (2012), who ended up deleting many low-frequency patterns which turned out
to be non-CP combinations. Similar tendencies can be derived for the five groups of nouns derived
6
from our clustering experiment above. This information is useful for a task like lexicon development
for a computational grammar. The following section therefore explores the possibility of encoding noun
group information in a computational Lexical Functional Grammar.
6 Noun Groups in Hindi/Urdu Grammar Development
Our experiments show that nouns appear with several different distributions, often with one dominant
light verb, but also with the possibility of occurring with one or two other light verbs. The clusters do
not represent absolute certainties about N-V CPs, but report tendencies of occurrences; e.g., the relative
frequencies of the cluster centroid for the noun group dominated by de ?give? is shown below.
(4) de ?give? 0.75, kar ?do? 0.08, le ?take? 0.06, ho ?be? 0.06, a ?come? 0.02, rakh ?keep? 0.02, lag
?attach? 0.01
In this section, we discuss the integration of our Hindi noun groupings into the grammar via the
construction of templates that can be augmented to model the relevant linguistic generalizations in terms
of constraints inspired by optimality theory (OT, Prince and Smolensky (2004)). A serious evaluation of
the effect on the grammar of adding in this lexical resource is planned for future work.
6.1 Templates in XLE
In XLE, grammar writers can define templates in a special section of the grammar that can be called
from the lexicon. Templates allow generalizations to be captured and, if necessary, changes to be made
only once, namely to the template itself (Butt et al., 1999; Dalrymple et al., 2004). Consider the template
in (5), which models intransitive verbs in English; these are represented in LFG terms as predicates that
apply to a single grammatical function, a subject. The lexical entry in (6) for the English intransitive
verb laugh calls up the INTRANS template; the argument supplied to the template is substituted for the
P(redicate) value inside the template definition.
(5) INTRANS(P) = (? PRED) = ? P<(? SUBJ)>?
@NOPASS.
(6) laugh V @(INTRANS laugh).
In ParGram grammars, the lexicons are generally organized so that each verb subcategorization frame
corresponds to a different template. Similarly, templates can be defined to encode a given set of general-
izations about how certain groups of nouns combine with different light verbs. Consider the N-V CPs in
(7). The noun ?sharA ?signal? forms part of the cluster dominated by de ?give? (i.e., the cluster with the
frequencies shown in (4)) and thus occurs most frequently with de ?give? as well as kar ?do?.
(7) a. nadya=ne b?lal=ko ?shara d?-ya
Nadya.F.Sg=Erg Bilal.M.Sg=Dat signal.M.Sg give-Perf.M.Sg
?Nadya signaled Bilal.? (lit. ?Nadya gave a signal to Bilal.?)
b. nadya=ne b?lal=ko ?shara k?-ya
Nadya.F.Sg=Erg Bilal.M.Sg=Acc signal.M.Sg do-Perf.M.Sg
?Nadya signaled Bilal.? (lit. ?Nadya made a signal towards Bilal.?)
The lexical entry of the noun ?shara ?signal? is given in (8).
8
The entry points to the template
NVGROUP2 which is defined as in (9). This version of the template constrains the verbal type of the
overall predication to be a CP either with the light verb de ?give? or with the light verb kar ?do?, or to not
be a CP at all. Thus, only light verb options with relative frequencies equaling or above 0.08 (i.e., 8%)
are accepted, an arbitrary threshold.
8
The transliteration scheme employed in the Hindi/Urdu ParGram Grammar is described in Malik et al. (2010).
7
(8) iSArA NOUN-S XLE (? PRED) = ?iSArA<(? OBJ)>?
@NVGROUP2.
(9) NVGROUP2 = { (? VTYPE COMPLEX-PRED-FORM) =c dE
|(? VTYPE COMPLEX-PRED-FORM) =c kar
| ? (? VTYPE COMPLEX-PRED-FORM)}
6.2 Preferred CPs
The template in (9), however, misses out on the fact that for all the groups identified, there are N-V com-
binations that are more productive (and thus more likely to be CP constructions) than other combinations
(which are more likely to be non-CP constructions, e.g., plain objects). In XLE, grammar developers can
model statistical generalizations using special marks that were inspired by Optimality Theory (Prince
and Smolensky, 2004). On top of the classical constraint system of existing LFG grammars, a separate
projection, o-structure, determines a preference ranking on the set of analyses for a given input sentence.
A relative ranking is specified for the constraints that appear in the o-projection, and this ranking serves
to determine the winner among the competing candidates. The constraints are also referred to as OT
marks and are overlaid on the existing grammar (Frank et al., 1998).
OT marks can be added in the appropriate place in the grammar to punish or prefer a certain analysis.
For example, (10) states that Mark1 is a member of the optimality projection. The order of preference of
a sequence of OT marks can be specified in the configuration section of the grammar; an example pref-
erence ordering is given in (11). Here, the list given in OPTIMALITYORDER shows the relative importance
of the marks. In this case Mark5 is the most important, and Mark1 is the least important. Marks that have
a + in front of them are preference marks. The more preference marks that an analysis has, the better. All
other marks are dispreference marks (the fewer, the better).
(10) ... Mark1 $ o::
*
...
(11) OPTIMALITYORDER Mark5 Mark4 Mark3 +Mark2 +Mark1.
Given the relative ordering of light verb tendencies in our noun groups, we can augment the templates
with OT marks that represent such tendencies. The noun template in (9) is changed in two ways. First,
all the light verbs are included; second, each disjunct is extended by two OT marks that represent the
statistical likelihood of this particular combination forming a CP or not.
9
The ordering of the marks is
shown in (13), where the mark cp-dispref is most severely punished, and the mark +cp-pref is most
strongly preferred. With an ordering like this, a CP analysis for (7a) is preferred, while a compositional
analysis is dispreferred by XLE; the inverse will apply to ?shara lAg, which is not a CP.
(12) NVGROUP2 = { { (? VTYPE COMPLEX-PRED-FORM) =c dE
cp-pref $ ::
*
| ? (? VTYPE COMPLEX-PRED-FORM)
non-cp-dispref $ ::
*
}
...
|(? VTYPE COMPLEX-PRED-FORM) =c lag}.
cp-dispref $ o::
*
| ? (? VTYPE COMPLEX-PRED-FORM)
non-cp-pref $ ::
*
} }.
(13) OPTIMALITYORDER cp-dispref non-cp-dispref +cp-pref +non-cp-pref.
9
For space reasons, only the disjuncts for de ?give? as well as lAg ?attach? are shown.
8
7 Conclusion
We have discussed a corpus study of Hindi/Urdu N-V CPs that makes use of a novel methodology in
terms of a noun seed list and an evaluation based on WordNet. We found that the k-means algorithm
with k = 5 and a frequency cutoff of 50 gave us the best result in terms of semantic coherence of the
resulting clusters. We are optimistic that the resulting noun groups can be used in different NLP settings
and have presented one such setting, the Hindi/Urdu ParGram Grammar, where lexical information about
nouns and their combinatory possibilities in CPs are vital for grammar extension.
Acknowledgements
We would like to thank the DAAD (Deutscher Akademischer Austausch Dienst) for sponsoring Ashwini
Vaidya?s research stay at the University of Konstanz and Dr Miriam Butt for hosting her. We are also
thankful to Dr Miriam Butt, Dr Martha Palmer and Christian Rohrdantz for their feedback on this paper.
Any errors that remain are our own.
References
Tafseer Ahmed and Miriam Butt. 2011. Discovering Semantic Classes for Urdu N-V Complex Predicates. In
Proceedings of the International Conference on Computational Semantics (IWCS 2011).
Tafseer Ahmed, Miriam Butt, Annette Hautli, and Sebastian Sulger. 2012. A Reference Dependency Bank for An-
alyzing Complex Predicates. In Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Mehmet U?gur Do?gan,
Bente Maegaard, Joseph Mariani, Jan Odijk, and Stelios Piperidis, editors, Proceedings of the Eight Interna-
tional Conference on Language Resources and Evaluation (LREC?12). European Language Resources Associ-
ation (ELRA), May.
Leslie Barrett and Anthony R Davis. 2003. Diagnostics for determining compatibility in English support-verb-
nominalization pairs. In Proceedings of the 4th international conference on Computational Linguistics and
Intelligent text processing (CICLing 03).
Rajesh Bhatt, Bhuvana Narasimhan, Martha Palmer, Owen Rambow, Dipti Sharma, and Fei Xia. 2009. A Multi-
Representational and Multi-Layered Treebank for Hindi/Urdu. In Proceedings of the Third Linguistic Annota-
tion Workshop, pages 186?189, Suntec, Singapore, August. Association for Computational Linguistics.
Pushpak Bhattacharyya. 2010. IndoWordNet. In Proceedings of the Seventh Conference on International Lan-
guage Resources and Evaluation (LREC?10), pages 3785?3792.
Miriam Butt and Tracy Holloway King. 2007. Urdu in a Parallel Grammar Development Environment. Lan-
guage Resources and Evaluation: Special Issue on Asian Language Processing: State of the Art Resources and
Processing, 41.
Miriam Butt, Tracy Holloway King, Mar??a-Eugenia Ni?no, and Fr?ed?erique Segond. 1999. A Grammar Writer?s
Cookbook. CSLI Publications.
Miriam Butt, Helge Dyvik, Tracy Holloway King, Hiroshi Masuichi, and Christian Rohrer. 2002. The Parallel
Grammar Project. In Proceedings of the COLING-2002 Workshop on Grammar Engineering and Evaluation,
pages 1?7.
Miriam Butt, Tina B?ogel, Annette Hautli, Sebastian Sulger, and Tafseer Ahmed. 2012. Identifying Urdu Complex
Predication via Bigram Extraction. In In Proceedings of COLING 2012, Technical Papers, pages 409 ? 424,
Mumbai, India.
Miriam Butt. 1995. The Structure of Complex Predicates in Urdu. CSLI Publications.
Miriam Butt. 2003. The Light Verb Jungle. Harvard Working Papers in Linguistics, 9.
Miriam Butt. 2010. The Light Verb Jungle: Still Hacking Away. In Mengistu Amberber, Brett Baker, and Mark
Harvey, editors, Complex Predicates in Cross-Linguistic Perspective. Cambridge University Press.
Dick Crouch, Mary Dalrymple, Ronald M. Kaplan, Tracy Holloway King, John T. Maxwell III, and Paula Newman,
2012. XLE Documentation. Palo Alto Research Center.
9
Mary Dalrymple, Ronald M. Kaplan, and Tracy Holloway King. 2004. Linguistic Generalizations over De-
scriptions. In Miriam Butt and Tracy Holloway King, editors, Proceedings of the LFG04 Conference. CSLI
Publications.
Alice Davison. 2005. Phrasal predicates: How N combines with V in Hindi/Urdu. In Tanmoy Bhattacharya,
editor, Yearbook of South Asian Languages and Linguistics, pages 83?116. Mouton de Gruyter.
Anette Frank, Tracy Holloway King, Jonas Kuhn, and John T. Maxwell III. 1998. Optimality Theory Style Con-
straint Ranking in Large-scale LFG Grammars. In Proceedings of the LFG98 Conference. CSLI Publications.
Peter Hook. 1974. The Compound Verb in Hindi. Center for South and Southeast Asian Studies, University of
Michigan.
Muhammad Humayoun. 2006. Urdu Morphology, Orthography and Lexicon Extraction. Master?s thesis, Depart-
ment of Computing Science, Chalmers University of Technology.
Yamuna Kachru. 2006. Hindi. John Benjamins.
Andreas Lamprecht, Annette Hautli, Christian Rohrdantz, and Tina B?ogel. 2013. A Visual Analytics System
for Cluster Exploration. In Proceedings of the 51st Annual Meeting of the Association for Computational
Linguistics: System Demonstrations, pages 109?114, Sofia, Bulgaria, August. Association for Computational
Linguistics.
Beth Levin. 1993. English Verb Classes and Alternations. A Preliminary Investigation. The University of Chicago
Press.
James B. MacQueen. 1967. Some Methods for Classification and Analysis of Multivariate Observations. In
Proceedings of 5th Berkeley Symposium on Mathematical Statistics and Probability, pages 281?297. University
of California Press.
Muhammad Kamran Malik, Tafseer Ahmed, Sebastian Sulger, Tina B?ogel, Atif Gulzar, Ghulam Raza, Sarmad
Hussain, and Miriam Butt. 2010. Transliterating Urdu for a Broad-Coverage Urdu/Hindi LFG Grammar. In
Proceedings of the Seventh Conference on International Language Resources and Evaluation (LREC 2010).
Tara Mohanan. 1994. Argument Structure in Hindi. CSLI Publications.
Ryan North. 2005. Computational Measures of the Acceptability of Light Verb Constructions. Ph.D. thesis,
University of Toronto.
Alan Prince and Paul Smolensky. 2004. Optimality Theory: Constraint Interaction in Generative Grammar.
Blackwell Publishing.
Ghulam Raza. 2011. Subcategorization Acquisition and Classes of Predication in Urdu. Ph.D. thesis, University
of Konstanz.
Siva Reddy and Serge Sharoff. 2011. Cross Language POS Taggers (and other Tools) for Indian Languages:
An Experiment with Kannada using Telugu Resources. In Proceedings of the Fifth International Workshop On
Cross Lingual Information Access, pages 11?19, Chiang Mai, Thailand, November. Asian Federation of Natural
Language Processing.
Ruth Laila Schmidt. 1999. Urdu: An Essential Grammar. Routledge.
Shiva Taslimipoor, Afsaneh Fazly, and Ali Hamzeh. 2012. Using Noun Similarity to Adapt an Acceptability
Measure for Persian Light Verb Constructions. In Nicoletta Calzolari (Conference Chair), Khalid Choukri,
Thierry Declerck, Mehmet U?gur Do?gan, Bente Maegaard, Joseph Mariani, Jan Odijk, and Stelios Piperidis,
editors, Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC?12),
Istanbul, Turkey, may. European Language Resources Association (ELRA).
S. Urooj, F. Jabeen, F. Adeeba, R. Parveen., and S. Hussain. 2012. Urdu Digest Corpus. In Proceedings of the
Conference on Language and Technology 2012, Lahore, Pakistan.
Tim Van de Cruys. 2006. Semantic Clustering in Dutch. In Proceedings of the Sixteenth Computational Linguis-
tics in Netherlands (CLIN), pages 17?32.
10
Proceedings of the Workshop on Lexical and Grammatical Resources for Language Processing, pages 127?136,
Coling 2014, Dublin, Ireland, August 24 2014.
Light verb constructions with ?do? and ?be? in Hindi: A TAG analysis
Ashwini Vaidya
University of Colorado
Boulder, CO
80309, USA
vaidyaa@colorado.edu
Owen Rambow
Columbia University
New York, NY
10115, USA
rambow@ccls.columbia.edu
Martha Palmer
University of Colorado
Boulder, CO
80309, USA
mpalmer@colorado.edu
Abstract
In this paper we present a Lexicalized Feature-based Tree-Adjoining Grammar analysis for a
type of nominal predicate that occurs in combination with the light verbs ?do? and ?be? (Hindi
kar and ho respectively). Light verb constructions are a challenge for computational grammars
because they are a highly productive predicational strategy in Hindi. Such nominals have been
discussed in the literature (Mohanan, 1997; Ahmed and Butt, 2011; Bhatt et al., 2013), but this
work is a first attempt at a Tree-Adjoining Grammar (TAG) representation. We look at three
possibilities for the design of elementary trees in TAG and explore one option in depth using
Hindi data. In this analysis, the nominal is represented with all the arguments of the light verb
construction, while the light verb adjoins into its elementary tree.
1 Introduction
Lexical resource development for computational analyses in Hindi must contend with a large number of
light verb constructions. For instance, in the Hindi Treebank (Palmer et al., 2009), nearly 37% of the
predicates have been annotated as light verb constructions. Hence, the combination of a noun with a light
verb is a productive predicational strategy in Hindi. For example, the noun yaad ?memory? combines
with kar ?do? to form yaad kar ?remember?.
In light verb constructions, the noun is a predicating element along with the light verb. The pres-
ence of two predicating elements representing a single meaning is a challenge for a linguistic theory
that maps between syntax and semantics. Consequently, the argument structure representation for light
verb constructions (LVC) has resulted in two opposing views in syntactic theory. One view supports
a noun-centric analysis of the LVC, where the noun is represented with all the arguments of the LVC
e.g. (Grimshaw and Mester, 1988; Kearns, 1988). The light verb?s only role is to theta-mark the ar-
guments of the LVC, without any semantic contribution. The second view proposes argument sharing
between the noun and the light verb as they both contribute to the argument structure of the LVC (Butt,
1995; Ahmed et al., 2012). We refer to such analyses as verb-centric analyses.
Within the framework of this debate, we propose to use Lexicalized Feature-based Tree Adjoining
Grammar, which is a variant of Tree Adjoining Grammar (TAG). TAG has been used to represent light
verb constructions in French (Abeill?e, 1988) and Korean (Han and Rambow, 2000). The primitive struc-
tures of TAG are its elementary trees, which encapsulate the syntactic and semantic arguments of its
lexical anchor (for a light verb construction, the noun and light verb respectively will be the anchors).
The association of a structural object with a linguistic anchor allows TAG to specify all the linguistic
constraints associated with the anchor over a local domain. This is especially advantageous for compos-
ing the complex argument structure of a LVC. In comparison with other formalisms (e.g. context-free
grammars), this property gives TAG an extended domain of locality.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
127
In this paper, we look at a particular group of nouns that occur with light verbs ?do? and ?be? (kar and
ho) as part of a light verb construction. The same noun alternates with either light verb, resulting in a
change in the argument structure of the verb. For example the noun chorii ?theft? can occur as either chorii
kar ?theft do? or chorii ho ?theft happen?. There are nearly 265 nouns showing this alternation in the Hindi
Treebank (Palmer et al., 2009)
1
. These constitute about 15% of the total light verb constructions in the
Treebank. Note that other light verbs also occur in Hindi e.g. de ?give?, le ?take? etc. but they are not
part of this study.
Section 3 has some examples of these predicating nominals. Before this, Section 2 will introduce the
TAG formalism. Section 4 describes the design of the elementary trees that are the basis of the analysis
and in the final section we summarize our findings and make suggestions for future work.
2 Lexicalized Feature based Tree Adjoining Grammar
Tree-Adjoining Grammar (TAG) is a formal tree-rewriting system that is used to describe the syntax of
natural languages (Joshi and Schabes, 1997). The basic structure of a TAG grammar is an elementary
tree, which is a fragment of a phrase structure tree labelled with both terminal and non-terminal nodes.
The elementary trees are combined by the operations of substitution (where a terminal node is replaced
with a new tree) or adjunction (where an internal node is split to add a new tree).
The elementary trees in TAG can be enriched with feature structures (Vijay-Shanker and Joshi, 1988).
These can capture linguistic descriptions in a more precise manner and also capture adjunction con-
straints. TAG with feature structures is also known as FTAG (Feature-structure based TAG). A TAG can
also be lexicalized i.e., an elementary tree has a lexical item as one of its terminal nodes. Lexicalized
TAG enhanced with feature structures is known as Lexicalized Feature-based Tree-Adjoining Gram-
mar (LF-TAG). This has been used for developing computational grammars for English (XTAG-Group,
2001), French (Abeill?e and Candito, 2000) and Korean (Han et al., 2000). In our analysis, we will also
use LF-TAG, but we will refer to it as LTAG for convenience.
Figure 1 shows the basic steps for composing elementary trees containing feature structures. Each
node has a top and a bottom feature structure. Features can be shared among nodes in an elementary
tree. In the tree for the verb running, the variable
1
is used to show that the verb must share the same
features as the subject NP.
The tree for running is an initial tree with a single terminal for its argument noun phrase (NP). The
tree for is, on the other hand, is a special type of elementary tree called the auxiliary tree. It has a foot
node (marked with an asterisk), which is identical to its root node. The auxiliary tree will adjoin into the
tree for running at the VP node only. The top and bottom feature structures for MODE at the VP node,
have different values (ind and ger), and they cannot unify. This captures an adjunction constraint for
obligatory adjunction and requires adjunction to take place at this node only.
During adjunction, the top of the root of the auxiliary tree (for is) will unify with the top of the
adjunction site. The bottom of the foot of the auxiliary tree will unify with the bottom of the adjunction
site. During substitution, the top node in the tree for Jill unifies with the node at NP.
This results in the second tree in Figure 1, post the operations of substitution and adjunction. In a
final derivation step, top and bottom feature structures at each node will unify, to give the final derived
tree with a single feature structure at each node. The resulting tree is called a derived tree, but another
by-product of the TAG analysis is also the derivation tree. This tree has numbered node labels that record
the history of composition of the elementary trees. For example, the tree for Jill is running can be seen
in Figure 2. The root of this tree is labelled with running, which is an initial tree of the type S.
An important characteristic of lexicalized elementary trees is their correspondence with that lexi-
cal item?s predicate-argument structure. This has sometimes been formalized as the PACP (Predicate-
Argument Co-occurrence Principle) (Frank, 2002). The PACP restricts the structure of the elementary
trees such that they may not be drawn arbitrarily. At the same time, lexicalized TAGs will often have the
1
It is possible that many more nouns occur in this group, but all their alternations are not instantiated in the Treebank
corpus.
128
SVP
[
AGR=
1
MODE=ind
]
[
MODE=ger
]
V
running
NP
[
AGR=
1
]
NP
[]
[
AGR=[PERS=3 num=sg]
]
N
Jill
VP
[
AGR=
2
MODE=
3
]
VP*
[
MODE=ger
]
V
[
MODE=
3
ind
]
[
AGR=
2
[PERS=3 num=sg]
]
is
After substitution
and adjunction:
S
VP
[
AGR=
1
MODE=ind
]
[
AGR=
2
MODE=
3
]
VP
[
MODE=ger
]
[
MODE=ger
]
V
running
V
[
MODE=
3
ind
]
[
AGR=
2
[PERS=3 num=sg]
]
is
NP
[
AGR=
1
]
[
agr=[pers=3 num=sg]
]
Jill
After top-bottom unification:
S
VP
[
AGR=
1
[PERS=3 NUM=sg]
MODE=ind
]
VP
[
MODE=ger
]
V
running
V
[
AGR=
1
MODE=ind
]
is
NP
[
AGR=
1
]
Jill
Figure 1: LTAG showing feature structures and constraints on adjunction (Example adapted from
(Kallmeyer and Osswald, 2013))
running
isJill
1 2
Figure 2: Derivation tree for ?Jill is running?. The dashed node indicates adjunction and the solid node
indicates substitution
129
same lexical item realized as the anchor of varying syntactic realizations. For example a verb such as run
will anchor a different elementary tree for its passive or interrogative variant.
3 Data
In this section, we introduce the nominal predicates that will be the focus of our LTAG analysis. Such
nominals allow an agentive (ergative-marked
2
) subject with the light verb kar ?do?. In contrast, the same
nominal does not have an agentive subject with ho ?be? (Ahmed and Butt, 2011). The alternation with
ho ?be? has an intransitivizing effect. In (1) and (2), a change in the light verb results in the presence or
absence of the agent argument. The nominal chorii is the same, but the LVC in (1) requires only a Theme
argument, whereas (2) needs an Agent and a Theme.
(1) gehene
jewels.M
chorii
theft.F
hue.
be.Perf.MPl
?The jewels got stolen?
(2) Ram-ne
Ram-Erg
gehene
jewels.M.Pl
chorii
theft.F
kiye.
do.Perf.M.Pl
?Ram stole the jewels ?
In English, a similar alternation structure may be found with light verbs in bring to light vs. come to
light (Claridge, 2000). Here, two light verbs bring and come are used to express either a causative or
inchoative reading. In the Hindi examples, the light verb ho ?be? and the light verb kar ?do? are used to
express the inchoative vs. causative reading. In Persian, kardan ?make or do? and ?sodan ?become? are
used in a manner similar (although not identical) to Hindi.
The noun chorii ?theft? belongs to a particular class of nouns where a change in the light verb does
result in a change in the arguments, but the agent argument is always presupposed, irrespective of the
light verb. For instance, the addition of a phrase such as apne-aap ?on its own? is semantically odd with
example 1. This is because the event of ?theft? cannot occur without an agent, although it is unexpressed
with the light verb ho ?be?. Contrast this with 4, where apne-aap is not odd and where the alternation
with kar ?do? is not possible. The non-alternating noun afsos ?regret? occurs with an Experiencer subject,
which can act spontaneously and hence allows the use of apne-aap.
(3) ??aaj
today
apne-aap
own-mine
gehene
jewels.M
chorii
theft.F
hue
be.Pres.MPl
???Today the jewels got stolen by themselves ?
(4) aaj
today
Ram-ko
Ram-Dat
is
this
baat-par
issue-Loc
apne-aap
own-mine
afsos
regret.M
huaa/*kiyaa.
be.Perf.M.Sg/*do.Perf.M.Sg
?Today Ram himself regretted this point/issue ?
In order to model such nominals in TAG we have three options: first, a noun-centric analysis, where the
nominal projects all the arguments of the LVC. In reference to the examples above, this would imply that
the light verb chorii ?theft? would be represented by two trees? i.e., it would appear with two arguments
with kar ?do? and only one with ho ?be?.
The second option is a verb-centric analysis, where the light verb kar ?do? would contribute the agen-
tive argument, and chorii would contribute the object. The nominal?s elementary tree would consist of
only one argument, regardless of whether it combined with kar ?do? or ho ?be?. The third option is to
2
Hindi is a split-ergative language, where ergative case on the subject is found only with transitive verbs in the perfective
aspect. For non-perfective aspect, the subject is nominative.
130
represent the LVC chorii kar ?theft do; steal? as the anchor of a single elementary tree? a single multi-
word expression. While the first two options are worth exploring, we discard the third option for two
reasons: first, the LVC is highly productive in Hindi, which would imply that this would result in too
many elementary trees in the grammar. Second, there is evidence that the LVC forms a phrasal category
in the syntax (Mohanan, 1997; Davison, 2005). This means that individual components of the LVC may
be moved away from each other, emphatic particles or negation may intervene and the noun component
may be independently modified by an adjective. Therefore, the multi-word option would not be the best
approach here. This is in contrast to previous TAG analyses for English LVCs where both nominal and
verb are anchored in the same elementary tree (XTAG-Group, 2001).
Figure 3 shows the derivation trees (cf. Figure 2) for the three different analysis options as described
above for the sentence Ram ne gehene chorii kiye ?Ram stole the jewels?. The LVC in question is chorii
kar ?theft do?. The dashed line indicates adjunction into the elementary tree, whereas the solid line
indicates substitution. In the noun-centric analysis, the light verb adjoins into the nominal?s elementary
tree and contributes no arguments of its own. For the verb-centric analysis, the light verb contributes the
argument Ram, whereas the nominal contributes jewels. Finally, for the multi-word expression tree, theft
and do are both anchors of the elementary tree.
Noun-centric analysis ?
chorii
Ram-ne
gehene
kiye
Verb-centric analysis ? kiye
Ram-ne chorii
gehene
Multi-word analysis ? chorii-kiye
Ram-ne
gehene
Figure 3: Derivation graphs showing three options for the analysis of Ram ne gehene chorii kiye ?Ram
stole the jewels?. The LVC is chorii kiye.
In this paper we explore a noun-centric analysis of Hindi LVCs.
3
In the analysis that follows, we will
describe two elementary trees for a noun like chorii i.e., when it combines with either ho ?be? or kar ?do?.
Making the elementary structures richer and more complex increases ambiguity locally and we then have
more descriptions for the same lexical item. But these structures also capture local dependencies i.e.,
the fact that the lexical item can appear in varying linguistic environments. Second, this is in keeping
with the TAG notion of using complex elementary structures to capture linguistic properties and having
very general operations (substitution and adjunction) to combine these structures. This has been used
effectively in computational applications and is characterised by the slogan complicate locally, simplify
globally (Bangalore and Joshi, 2010).
4 Analysis
In a noun-centric analysis, the light verb does not have arguments of its own. The full array of arguments
for the light verb construction is instead represented in the nominal?s tree. The light verb can only choose
3
Based on the comments of the reviewers we are now considering a revision of the noun-centric analysis in this paper. It
may seem that a verb-centric analysis may be more appropriate for Hindi LVCs. However, due to lack of space, we do not
explore the second option fully in this paper and leave it to future work.
131
the semantic property of the nominal it may combine with (e.g., the light verb ho may combine only with
nominals that have no agentive arguments). Other analyses e.g Ahmed et al. (2012) represent the light
verb kar ?do? with arguments of its own. We discuss this in Section 5.
Our work follows Han and Rambow (2000)?s representation of Sino-Korean LVCs. This work has
also proposed separate trees for the nominal and light verb. The elementary tree of the nominal is an an
initial tree, and as it is considered the true predicate, it also chooses a syntactic structure that will realize
all its arguments. The light verb on the other hand is represented as an auxiliary tree, therefore it is an
adjunct to the nominal?s basic structure. However, as it is a predicate, it is also a special type of auxiliary
tree viz., a predicative auxiliary tree (Abeill?e and Rambow, 2000).
The second feature of this analysis, also based on Han and Rambow (2000)?s work is the idea of the
nominal as an underspecified base form. The nominal?s elementary tree is not specified with respect to
its category, rather, we use the label X, which projects to an XP. We also assume, following Han and
Rambow that each node is specified with the feature CAT which has values like V or N, but the [CAT=N]
feature on the noun is not realized unless the light verb composes with the elementary tree of the nominal.
In addition, although the nominal is not a verb, it has the feature TENSE=- i.e., it is not tensed.
4.1 The light verb
In order to model the light verb kar ?do? in Example 2, we will construct an auxiliary tree with feature
structures, anchored at kar ?do?. Figure 4 shows such an elementary tree. Note that this is a very different
tree from ?full? kar ?do?, which will have all its arguments. The light verb kar is inflected for person,
number, and gender as well as tense and aspect. In this particular example, it is tensed, masculine, plural
and has perfective aspect; therefore it appears as kiye. We assume that morphological analysis has already
taken place in a separate module, such that the correct morphological surface form has been derived for
?do, masculine plural perfective?. In Figure 4, the XP
r
(root) node and its right-branching daughters
are [CAT=V] with linguistic information about gender, number, tense and aspect. The feature AGT=+ at
the top node implies that this auxiliary tree needs to unify with an initial tree that is also [AGT=+]. In
contrast with kar ?do?, the auxiliary tree of the light verb ho ?be? will have [AGT=-].
XP
r
[
cat=v tense=+ perf=+ agt=+ agr=mpl
]
[
cat=v tense=+ perf=+ agt=+ agr=mpl
]
VP
[
cat=v tense=+ perf=+ agt=+ agr=mpl
]
[
cat=v tense=+ perf=+ agt=+ agr=mpl
]
V
[
cat=v tense=+ perf=+ agt=+ agr=mpl
]
[
cat=v tense=+ perf=+ agt=+ agr=mpl
]
kiye
XP
f
[
cat=n tense=- case=nom nagr=-
]
[]
Figure 4: Elementary tree for light verb kar ?do? inflected as kiye?do.masc.pl.perf?
The XP
f
(foot) node has [TENSE=-] and [CAT=N], which will enable it to adjoin into the elementary
tree of a nominal. The CASE value is specified as NOM (nominative) as the light verb will assign nomi-
native case to the noun. The NAGR feature is required when the light verb agrees in number and gender
with the predicative nominal itself (Mohanan, 1997). As this will not occur in the examples we are
working with, the value for NAGR is negative. For other ?standard? cases of agreement, the feature AGR
is used (It is also useful to note that the verbal agreement rule in Hindi differs from English as the verb
agrees with the highest nominative marked argument- and not necessarily the subject (Mohanan, 1995)).
132
S[
cat=v tense=+ perf=+[1] agt=+[2]
]
[
cat=v tense=+ perf=[3] agt=[4]
]
VP
[
cat=v tense=+ perf=[3] agt=[4] agr=[11]
]
[
cat=v tense=+ perf=[5] agt=[6]agr=[10]
]
XP
2
[
cat=v tense=+ perf=[5] agt=[6] agr=[10]
]
[
cat=[19] tense=- nagr=- case=[14]
]
X
[
cat=[19] tense=- nagr=- case=[14]
]
[
cat=[20] tense=- nagr=- case=[15]
]
chorii
NP
2
?
?
?
?
case=nom
cat=n
agr=[11]
?
?
?
NP
1
?
?
?
?
case=erg cat=n
perf=+[1] agt=+[2]
agr=[13]
?
?
?
Figure 5: Tree for nominal chorii ?theft? -agentive, as seen in Ram ne gehene chorii kiye ?Ram stole the
jewels?. The feature clash at XP
2
is marked with a box.
4.2 The nominal
In contrast to the impoverished argument structure of the light verb, the nominal in Figure 5 has the full
array of arguments for chorii ?theft?. The tree is anchored by the lexical item chorii and the non terminals
at NP
1
and NP
2
are marked with a ? for substitution with the actual lexical items.
The position of the arguments roughly follows the configuration described in Bhatt et al. (2013, p. 59)
, where the first position is the ergative-marked argument and is found in a transitive sentence (but only
if the property [PERF=+] is also present.)
The ?second? position is one where the object of the transitive verb is found. In Figure 5, this is
represented as NP
2
and is the nominative marked argument. The elementary tree for the nominal is not
complete, because of the feature clash at XP
2
between [TENSE=+] vs. [TENSE=-]. The feature clash
represents an obligatory adjunction constraint which will require the light verb to adjoin at this node.
The first position in Figure 5 has the features for [PERF=+] and [AGT=+] as a consequence of having
[CASE=ERG]. The agentive argument shares the values for PERF and AGT with the S node. This ensures
that the light verb that adjoins into this tree will match the PERF and AGT values in NP
1
. The argument
in second position NP
2
will share its values for AGR with XP
2
. At XP
2
, the values for PERF, AGT and
AGR should match with the root node of the light verb. Otherwise, adjunction will fail.
The light verb?s tree as shown in Figure 4 will adjoin into the tree of the nominal. Post adjunction and
substitution, we find a composed structure as seen in Figure 7.
The same noun chorii ?theft? may combine with the light verb ho. In that case, non-agentive chorii will
choose an elementary tree such as Figure 6. This elementary tree appears without an agentive argument.
Its single nominative Theme argument has moved to the first position at NP
1
, leaving behind a co-indexed
trace. Figure 6 shows that the site of adjunction into chorii ?theft? (non-agentive) is at XP
1
. Adjunction
cannot take place at XP
2
as the feature clash is higher up at XP
1
. The single nominative argument of
chorii (non-agentive) will move up to NP
1
in order to receive nominative case from the node CAT=V
(Note that the node immediately above NP
2
has an underspecified CAT feature and this requires the
argument to move to a higher position). The tree for non-agentive chorii will always combine with a
light verb that is AGT=-. Its Theme argument will take nominative case irrespective of the tense-aspect
value of the verb.
133
S[
cat=v tense=+ perf=[1] agt=[2] agr=[12]
]
[
cat=v tense=+ perf=[3] agt=[4] agr=[11]
]
XP
1
[
cat=v tense=+ perf=[3] agt=[4] agr=[11]
]
[
cat=[18] tense=- nagr=- case=[14]
]
XP
2
[
cat=[18] tense=- nagr=- case=[14]
]
[
cat=[19] tense=- nagr=- case=[15]
]
X
[
cat=[19] tense=- nagr=- case=[15]
]
[
cat=[20] tense=- nagr=- case=[16]
]
chorii
NP
2
[
cat=n
]
t
i
NP
1
i
?
[
case=nom perf=[1] agt=?[2] agr=[12]
]
Figure 6: Tree for nominal chorii - non agentive as seen in gehene chorii hue ?The jewels were stolen?.
The feature clash this time is higher in the tree at XP
1
and is marked with a box.
S
[
cat=v tense=+ perf=+ agt=+
]
[
cat=v tense=+ perf=+ agt=+
]
VP
[
cat=v tense=+ perf=+ agt=+ agr=mpl
]
[
cat=v tense=+ perf=+ agt=+ agr=mpl
]
XP
2
[
cat=v tense=+ perf=+ agt=+ agr=mpl
]
[
cat=v tense=+ perf=+ agt=+ agr=mpl
]
VP
[
cat=v tense=+ perf=+ agt=+ agr=mpl
]
[
cat=v tense=+ perf=+ agt=+ agr=mpl
]
V
[
cat=v tense=+ perf=+ agt=+ agr=mpl
]
[
cat=v tense=+ perf=+ agt=+ agr=mpl
]
kiye
XP
f
[
cat=n tense=-
case=nom nagr=-
]
[
cat=n tense=-
case=nom nagr=-
]
X
[
cat=n tense=-
case=nom nagr=-
]
[
cat=n tense=-
case=nom nagr=-
]
chorii
NP
[
case=nom cat=n
agr=mpl
]
[
case=nom cat=n
agr=mpl
]
gehene
NP
[
case=erg cat=n
perf=+ agt=+ agr=[13]
]
[
case=erg cat=n
perf=+ agt=+ agr=msg
]
Ram ne
Figure 7: Post adjunction of the light verb?s auxiliary tree into the initial tree chorii ?theft? at XP
2
, we
get the complete argument structure. Substitution at the nodes NP
1
and NP
2
gives us Ram ne gehene
chorii kiye ?Ram stole the jewels?
134
5 Discussion
The elementary trees for chorii ?theft??both agentive and non-agentive are able to capture its alternations
with kar ?do? and ho ?be?. This is in contrast to Ahmed et al. (2012)?s approach in an important way.
They do not consider the nominal?s alternation with the light verb ho ?be? as a light verb construction.
Instead, they maintain that it has a resultative reading and provide a different analysis within the Lexical
Functional Grammar (LFG) framework. In fact, the alternation with ho ?be? provides a useful lexical
alternative to an alternative syntactic structure (such as a passive). The alternation of the light verb ho
?be? and kar ?do? is moreover a characteristic of a certain group of nominals only (not all can show this
alternation e.g., intizar ?waiting? cf. Ahmed and Butt (2011)). Therefore, we maintain that chorii ho
?theft happen? is indeed a light verb construction.
Ahmed and Butt (2011)?s analysis looks at the noun and light verb as co-predicators i.e., it is a verb
centric analysis. While this is different from the proposed analysis here, it is not impossible to construct
elementary trees where the light verb?s elementary tree consists of one argument i.e., the subject and the
nominal (with its own argument) adjoins into it. The pros and cons of these two approaches need to be
explored more thoroughly within the TAG framework and we leave this to future work.
While this work has examined one class of nominals that occur as part of light verb constructions,
it does not complete the analysis of light verb constructions in Hindi. The behaviour of other nominal
classes remains to be explored. There are also nominals that occur with light verbs other than kar ?do?
and ho ?be?. Finally, while the work presented here is mainly theoretical, it is in keeping with recent
proposals for extracting a Hindi TAG grammar from a phrase structure treebank (Bhatt et al., 2012;
Mannem et al., 2009). The algorithm in Bhatt et al. (2012) relies on the annotated Hindi Dependency
Treebank and proposes a rule extraction system for elementary trees. Therefore, the description of Hindi
LVCs in TAG would be a useful addition to the implementation of a grammar extraction task.
Acknowledgements
The first author was supported by DAAD (Deutscher Akademischer Austausch Dienst) for a research
stay at the University of Konstanz in 2013-14s. We are also thankful to Prof Miriam Butt for hosting the
first author at the University of Konstanz. We would like to thank Bhuvana Narasimhan, Miriam Butt
and the anonymous reviewers for their useful remarks on this paper. Any errors that remain are our own.
References
Anne Abeill?e and Marie-H?el`ene Candito. 2000. FTAG: A Lexicalized Tree-Adjoining Grammar for French. In
Tree Adjoining Grammars: Formalisms, Linguistic Analysis and Processing. CSLI Publications.
Anne Abeill?e and Owen Rambow. 2000. Tree Adjoining Grammar: An Overview. In Anne Abeill?e and Owen
Rambow, editors, Tree Adjoining Grammars: Formalisms, Linguistic Analysis and Processing. CSLI Publica-
tions.
Anne Abeill?e. 1988. Light verb constructions and Extraction out of NP in TAG. In Lynn MacLeod, Gary Larson,
and Diane Brentari, editors, Proceedings of the 24th Annual Meeting of the Chicago Linguistics Society.
Tafseer Ahmed and Miriam Butt. 2011. Discovering Semantic Classes for Urdu N-V Complex Predicates. In
Proceedings of the International Conference on Computational Semantics (IWCS 2011), Oxford.
Tafseer Ahmed, Miriam Butt, Annette Hautli, and Sebastian Sulger. 2012. A reference dependency bank for
analyzing complex predicates. In Proceedings of the Eight International Conference on Language Resources
and Evaluation (LREC?12).
Srinivas Bangalore and Aravind Joshi. 2010. Introduction. In Srinivas Bangalore and Aravind Joshi, editors,
Supertagging: Using Complex Lexical Descriptions in Natural Language Processing, pages 1?31. MIT Press,
Cambridge.
Rajesh Bhatt, Owen Rambow, and Fei Xia. 2012. Creating a Tree Adjoining Grammar from a Multilayer Tree-
bank. In Proceedings of the 11th International Workshop on Tree Adjoining Grammars and Related Formalisms
(TAG+11), pages 162?170.
135
Rajesh Bhatt, Annahita Farudi, and Owen Rambow. 2013. Hindi-Urdu Phrase Structure Annotation Guidelines.
http://verbs.colorado.edu/hindiurdu/guidelines docs/PhraseStructureguidelines.pdf, November.
Miriam Butt. 1995. The Structure of Complex Predicates in Urdu. CSLI Publications, Stanford.
Claudia Claridge. 2000. Multi-word Verbs in Early Modern English: A Corpus-based Study. Editions Rodopi B.
V., Amsterdam-Atlanta edition.
Alice Davison. 2005. Phrasal predicates: How N combines with V in Hindi/Urdu. In Tanmoy Bhattacharya,
editor, Yearbook of South Asian Languages and Linguistics, pages 83?116. Mouton de Gruyter.
Robert Frank. 2002. Phrase Structure Composition and Syntactic Dependencies. MIT Press, Cambridge.
Jane Grimshaw and Armin Mester. 1988. Light verbs and theta-marking. Linguistic Inquiry, 9(2):205?232.
Chung-hye Han and Owen Rambow. 2000. The Sino-Korean light verb construction and lexical argument struc-
ture. In Proceedings of the Fifth International Workshop on Tree-Adjoining Grammars and Related Formalisms,
TAG+5.
Chung-hye Han, Juntae Yoon, Nari Kim, and Martha Palmer. 2000. A Feature based Lexicalized Tree Adjoining
Grammar for Korean. Technical report, Institute for Research in Cognitive Science, University of Pennsylvania,
http://www.cis.upenn.edu/?xtag/koreantag.
Aravind Joshi and Y. Schabes. 1997. Tree-adjoining grammars. In G. Rozenburg and A. Salomaa, editors,
Handbook of Formal Languages, volume 3, pages 69?124. Springer.
Laura Kallmeyer and Rainer Osswald. 2013. Syntax-driven semantic frame composition in Lexicalized Tree
Adjoining Grammars. Journal of Language Modelling, 1(2):267?330.
Kate Kearns. 1988. Light verbs in English. Manuscript, MIT (revised 2002).
Prashanth Mannem, Aswarth Abhilash, and Akshar Bharati. 2009. LTAG-spinal Treebank and Parser for Hindi.
In Proceedings of ICON-2009: 7th International Conference on Natural Language Processing.
Tara Mohanan. 1995. Wordhood and Lexicality- Noun Incorporation in Hindi. Natural Language and Linguistic
Theory, 13:75?134.
Tara Mohanan. 1997. Multidimensionality of representation- NV complex predicates in Hindi. In Alex Alsina,
Joan Bresnan, and Peter Sells, editors, Complex Predicates. CSLI Publications, Stanford.
Martha Palmer, Rajesh Bhatt, Bhuvana Narasimhan, Owen Rambow, Dipti Misra Sharma, and Fei Xia. 2009.
Hindi Syntax: Annotating Dependency, Lexical Predicate-Argument Structure, and Phrase Structure. In Pro-
ceedings of ICON-2009: 7th International Conference on Natural Language Processing, Hyderabad.
K. Vijay-Shanker and Aravind Joshi. 1988. Feature structure based Tree Adjoining Grammars. In Proceedings of
COLING 1988.
The XTAG-Group. 2001. A Lexicalized Tree Adjoining Grammar for English. Technical report, IRCS, University
of Pennsylvania.
136
